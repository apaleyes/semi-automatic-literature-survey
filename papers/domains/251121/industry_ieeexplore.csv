doi,title,publisher,content_type,abstract,html_url,publication_title,publication_date,database
10.1109/ICAIIC51459.2021.9415257,A Deep Learning Module Design for Workspace Identification in Manufacturing Industry,IEEE,Conferences,"In this paper, in order to solve various problems occurring in the workspace, a deep learning-based workspace identification module was designed, and the performance was analyzed through an experiment on the recognition accuracy according to the configuration of the training dataset and the number of training. The data model of the designed deep learning module is ResNetl8, and after setting up three dataset strategies, a dataset using five types of workspaces of the manufacturing industry was selected. In terms of the average top 5 and all training, strategy 2 was 81.2% and 76.4%, respectively, confirming that it was the best among the 3 strategies. In the future, after upgrading the designed module, it is planned to implement a module with real-time workspace identification performance level of practical use in a mobile environment with an image input device installed.",https://ieeexplore.ieee.org/document/9415257/,2021 International Conference on Artificial Intelligence in Information and Communication (ICAIIC),13-16 April 2021,ieeexplore
10.1109/IS.2018.8710526,A Digital Twin-based Privacy Enhancement Mechanism for the Automotive Industry,IEEE,Conferences,"This paper discusses a Digital Twin demonstrator for privacy enhancement in the automotive industry. Here, the Digital Twin demonstrator is presented as a method for the design and implementation of privacy enhancement mechanisms, and is used to detect privacy concerns and minimize breaches and associated risks to which smart car drivers can be exposed through connected infotainment applications and services. The Digital Twin-based privacy enhancement demonstrator is designed to simulate variety of conditions that can occur in the smart car ecosystem. We firstly identify the core stakeholders (actors) in the smart car ecosystem, their roles and exposure to privacy vulnerabilities and associated risks. Secondly, we identify assets that consume and generate sensitive privacy data in smart cars, their functionalities, and relevant privacy concerns and risks. Thirdly, we design an infrastructure for collecting (i) real-time sensor data from smart cars and their assets, and (ii) environmental data, road and traffic data, generated through operational driving lifecycle. In order to ensure compliance of the collected data with privacy policies and regulations, e.g. with GDPR requirements for enforcement of the data subject's rights, we design methods for the Digital Twin-based privacy enhancement demonstrator that are based on behavioural analytics informed by GDPR. We also perform data anonymization to minimize privacy risks and enable actions such as sending an automatic informed consent to the stakeholders.",https://ieeexplore.ieee.org/document/8710526/,2018 International Conference on Intelligent Systems (IS),25-27 Sept. 2018,ieeexplore
10.1109/ICMCCE.2018.00050,A Smart Manufacturing Compliance Architecture of Electronic Batch Recording System (eBRS) for Life Sciences Industry,IEEE,Conferences,"The paradigm shift brought about by smart manufacturing or Industrie 4.0 has posed threefold challenges to electronic batch recording system (eBRS) in Life Sciences Industry: 1) the structure of the data should be informative and standard for interoperate using information models, 2) administration of synchronization between physical world and cyber world for smart decision making and optimization using cyber physical system (CPS) and 3) so-called digital manufacturing operations management (digital MOM) characterized by decentralization, comprehensive collaboration and servitization shall be implemented. Under the new situations of smart manufacturing or Industrie 4.0, the requirements from information models, CPS and digital MOM will become the most principal criteria to be considered for future eBRS/MES and other operations management information system in shop floor. To fulfill these demands, an approach combining ISA95/88 hybrid model with activities ontology and variant domain-driven design for SOA-based eBRS development has been presented. An eBRS software platform has been developed on the theoretical basis and applied to a specific application scenario of Lyophilized Injection Production for verifying its feasibility purpose.",https://ieeexplore.ieee.org/document/8537548/,"2018 3rd International Conference on Mechanical, Control and Computer Engineering (ICMCCE)",14-16 Sept. 2018,ieeexplore
10.1109/PacificVis.2018.00026,A Visual Analytics Approach for Equipment Condition Monitoring in Smart Factories of Process Industry,IEEE,Conferences,"Monitoring equipment conditions is of great value in manufacturing, which can not only reduce unplanned downtime by early detecting anomalies of equipment but also avoid unnecessary routine maintenance. With the coming era of Industry 4.0 (or industrial internet), more and more assets and machines in plants are equipped with various sensors and information systems, which brings an unprecedented opportunity to capture large-scale and fine-grained data for effective on-line equipment condition monitoring. However, due to the lack of systematic methods, analysts still find it challenging to carry out efficient analyses and extract valuable information from the mass volume of data collected, especially for process industry (e.g., a petrochemical plant) with complex manufacturing procedures. In this paper, we report the design and implementation of an interactive visual analytics system, which helps managers and operators at manufacturing sites leverage their domain knowledge and apply substantial human judgements to guide the automated analytical approaches, thus generating understandable and trustable results for real-world applications. Our system integrates advanced analytical algorithms (e.g., Gaussian mixture model with a Bayesian framework) and intuitive visualization designs to provide a comprehensive and adaptive semi-supervised solution to equipment condition monitoring. The example use cases based on a real-world manufacturing dataset and interviews with domain experts demonstrate the effectiveness of our system.",https://ieeexplore.ieee.org/document/8365986/,2018 IEEE Pacific Visualization Symposium (PacificVis),10-13 April 2018,ieeexplore
10.1109/BigData.2014.7004408,Advanced planning and control of manufacturing processes in steel industry through big data analytics: Case study and architecture proposal,IEEE,Conferences,"Enterprises in today's globalized world are compelled to react on threats and opportunities in a highly flexible manner. Hence, companies that are able to analyze the current state of their business processes, forecast their most optimal progresses and with this proactively control them will have a decisive competitive advantage. Technological progress in sensor technology has boosted real-time situation awareness, especially in manufacturing operations. The paper at hands examines, based on a case study stemming from the steel manufacturing industry, which production-related data is collectable using state of the art sensors forming a basis for a detailed situation awareness and for deriving accurate forecasts. However, analyses of this data point out that dedicated big data analytics approaches are required to utilize the full potential out of it. By proposing an architecture for predictive process planning and control systems, the paper intends to form a working and discussion basis for further research and implementation efforts in big data analytics.",https://ieeexplore.ieee.org/document/7004408/,2014 IEEE International Conference on Big Data (Big Data),27-30 Oct. 2014,ieeexplore
10.1109/HICSS.2007.52,Agent-based Human-computer-interaction for Real-time Monitoring Systems in the Trucking Industry,IEEE,Conferences,"Auto ID systems can replace time-consuming, costly and error-prone processes of human data entry and produce detailed real time information. However, they would add value only to the extent that data is presented in a user-friendly manner. As model-based decision support is not always adequate, an agent-based approach is often chosen. Real life entities such as orders and trucks are represented by agents, which negotiate in order to solve planning problems. For the respective data representation at least two forms can be distinguished, focusing either on (1) resources (account-based) or (2) orders (order-centric). Applying cognitive fit theory we describe how the different interfaces affect decision making. The hypotheses would be tested in a laboratory experiment. The intended contribution should support that order-centric interfaces have higher user-friendliness and are especially beneficial to low-analytics and planners working under time pressure",https://ieeexplore.ieee.org/document/4076424/,2007 40th Annual Hawaii International Conference on System Sciences (HICSS'07),3-6 Jan. 2007,ieeexplore
10.1109/COASE.2018.8560557,An Industry 4.0 Cyber-Physical Framework for Micro Devices Assembly,IEEE,Conferences,"An advanced cyber manufacturing framework to support the collaborative assembly of micro devices is presented based on Industry 4.0 principles. The distributed cyber and physical components work together to plan, assemble and monitor micro assembly related tasks; micro assembly refers to the assembly of micron sized devices which cannot be manufactured by MEMS technologies. The collaborative framework proposed includes assembly planning and path planning modules, Virtual Reality based assembly simulation environments and physical assembly work cells. An ontology based approach was implemented to address semantic interoperability issues to support formation of temporary partnerships in a Virtual Enterprise context. The key to the design and implementation of this complex framework is an information centric process modeling approach which provides a data/information oriented basis for collaboration. A collaborative cyber physical test bed has been built to demonstrate feasibility of proposed framework and approach.",https://ieeexplore.ieee.org/document/8560557/,2018 IEEE 14th International Conference on Automation Science and Engineering (CASE),20-24 Aug. 2018,ieeexplore
10.1109/SYSCON.2018.8369547,An interactive architecture for industrial scale prediction: Industry 4.0 adaptation of machine learning,IEEE,Conferences,"According to wiki definition, there are four design principles in Industry 4.0. These principles support companies in identifying and implementing Industry 4.0 scenarios, namely, Interoperability, Information transparency, Technical assistance, Decentralized decisions. In this paper we have discussed our work on an implementation of a machine learning based interactive architecture for industrial scale prediction for dynamic distribution of water resources across the continent, keeping the four corners of Industry 4.0 in place. We report the possibility of producing most probable high resolution estimation regarding the water balance in any region within Australia by implementation of an intelligent system that can integrate spatial-temporal data from various independent sensors and models, with the ground truth data produced by 250 practitioners from the irrigation industry across Australia. This architectural implementation on a cloud computing platform linked with a freely distributed mobile application, allowing interactive ground truthing of a machine learning model on a continental scale, shows accuracy of 90% with 85% sensitivity of correct surface soil moisture estimation with end users at its complete control. Along with high level of information transparency and interoperability, providing on-demand technical supports and motivating users by allowing them to customize and control their own local predictive models, show the successfulness of principles in Industry 4.0 in real environmental issues in the future adaptation in various industries starting from resource management to modern generation soft robotics.",https://ieeexplore.ieee.org/document/8369547/,2018 Annual IEEE International Systems Conference (SysCon),23-26 April 2018,ieeexplore
10.1109/PerComWorkshops51409.2021.9431009,Architecture and pervasive platform for machine learning services in Industry 4.0,IEEE,Conferences,"Pervasive computing promotes the integration of smart electronic devices in our living and working spaces in order to provide new, advanced services. Recently many prototype services based on machine learning techniques have been proposed in a number of domains like smart homes, smart buildings or smart plants. However, the number of applications effectively deployed in the real world is still limited. We believe that architectural principles and integrated frameworks are still missing today to successfully and repetitively support application developers and operators. In this paper, we present a novel architecture and a pervasive platform allowing the development of machine learning based applications in smart buildings.",https://ieeexplore.ieee.org/document/9431009/,2021 IEEE International Conference on Pervasive Computing and Communications Workshops and other Affiliated Events (PerCom Workshops),22-26 March 2021,ieeexplore
10.1109/IV47402.2020.9304778,"Autonomous Driving Vehicle Control Auto-Calibration System: An Industry-Level, Data-Driven and Learning-based Vehicle Longitudinal Dynamic Calibrating Algorithm",IEEE,Conferences,"The control module is a crucial part for autonomous driving systems, a typical control algorithm often requires vehicle dynamics (such as longitudinal dynamics) as inputs, which, unfortunately are difficult to calibrate in real time. Further, it is also a challenge to reflect instantaneous changes in longitudinal dynamics (e.g. load changes) using a calibration table. As a result, control performance may deteriorate when load changes considerably (especially for small cargoes). In this paper, we will show how we build a data-driven longitudinal calibration procedure using machine learning techniques to adapt load changes in real time. We first generated offline calibration tables from human driving data. The offline table serves as an initial guess for later uses, and it only requires twenty minutes of data collection and processing. We then used an online learning algorithm to appropriately update the initial table (the offline table) based on real-time performance analysis. Experiments indicated (a) offline auto-calibration leads to a better control accuracy, compared with manual calibration; (b) online auto-calibration is capable to handle load changes and significantly reduce real time control error. This system has been deployed to more than one hundred Baidu self-driving vehicles (both hybrid and electronic vehicles) since April 2018. By January 2019, the system had been tested for more than 2,000 hours and over 10,000 kilometers (6,213 miles) and was still proven to be effective.",https://ieeexplore.ieee.org/document/9304778/,2020 IEEE Intelligent Vehicles Symposium (IV),19 Oct.-13 Nov. 2020,ieeexplore
10.1109/RTAS52030.2021.00041,Brief Industry Paper: Workload-Aware GPU Performance Estimation in the Airborne Embedded System,IEEE,Conferences,"New generation airborne embedded system has deployed Graphical Processing Units (GPUs) to raise processing capability to meet growing computational demands. Applications in the airborne embedded system have strict real-time constraints. Therefore, it is necessary to accurately predict timing behaviors of those applications. Many previous work propose GPU performance models to estimate the execution time of applications. However, most of those models do not consider the impact of co-execution on the GPU performance. In this paper, we propose a workload-aware GPU performance model to predict the execution time of applications executed concurrently on a single GPU. Experimental results illustrate that the proposed model can achieve a 5.1%-11.6% prediction error in a real airborne embedded hardware platform.",https://ieeexplore.ieee.org/document/9470486/,2021 IEEE 27th Real-Time and Embedded Technology and Applications Symposium (RTAS),18-21 May 2021,ieeexplore
10.1109/MED.2017.7984310,Cloud computing for big data analytics in the Process Control Industry,IEEE,Conferences,"The aim of this article is to present an example of a novel cloud computing infrastructure for big data analytics in the Process Control Industry. Latest innovations in the field of Process Analyzer Techniques (PAT), big data and wireless technologies have created a new environment in which almost all stages of the industrial process can be recorded and utilized, not only for safety, but also for real time optimization. Based on analysis of historical sensor data, machine learning based optimization models can be developed and deployed in real time closed control loops. However, still the local implementation of those systems requires a huge investment in hardware and software, as a direct result of the big data nature of sensors data being recorded continuously. The current technological advancements in cloud computing for big data processing, open new opportunities for the industry, while acting as an enabler for a significant reduction in costs, making the technology available to plants of all sizes. The main contribution of this article stems from the presentation for a fist time ever of a pilot cloud based architecture for the application of a data driven modeling and optimal control configuration for the field of Process Control. As it will be presented, these developments have been carried in close relationship with the process industry and pave a way for a generalized application of the cloud based approaches, towards the future of Industry 4.0.",https://ieeexplore.ieee.org/document/7984310/,2017 25th Mediterranean Conference on Control and Automation (MED),3-6 July 2017,ieeexplore
10.1109/WFCS.2019.8757952,Cloud-enabled Smart Data Collection in Shop Floor Environments for Industry 4.0,IEEE,Conferences,"Industry 4.0 transition is producing a remarkable change in the Smart Factories management. Modern companies can provide new services following products inside the shop floors along the entire production chain. To achieve the goal of servitization that the Industry 4.0 world requires, a modernization of current production chains is needed. This common demand comes mostly from manufacturing sector, where complex work machines collaborate with human workers. The data produced by the machines must be processed quickly, to allow the implementation of reactive services such as predictive maintenance, and remote control, always taking care of the safety of nearby people. This paper proposes a multi-layer architecture to monitor legacy production machines during their operations inside customers plants. The platform provides near real-time delivery of data collected from the machines with a high grade of customization according to customer needs. The performed tests show the scalability of the platform for a productive ecosystem with many machines at work, confirming its feasibility within different production facilities with different needs.",https://ieeexplore.ieee.org/document/8757952/,2019 15th IEEE International Workshop on Factory Communication Systems (WFCS),27-29 May 2019,ieeexplore
10.1109/WCNC45663.2020.9120761,Collaborative Learning Model for Cyberattack Detection Systems in IoT Industry 4.0,IEEE,Conferences,"Although the development of IoT Industry 4.0 has brought breakthrough achievements in many sectors, e.g., manufacturing, healthcare, and agriculture, it also raises many security issues to human beings due to a huge of emerging cybersecurity threats recently. In this paper, we propose a novel collaborative learning-based intrusion detection system which can be efficiently implemented in IoT Industry 4.0. In the system under consideration, we develop smart “filters” which can be deployed at the IoT gateways to promptly detect and prevent cyberattacks. In particular, each filter uses the collected data in its network to train its cyberattack detection model based on the deep learning algorithm. After that, the trained model will be shared with other IoT gateways to improve the accuracy in detecting intrusions in the whole system. In this way, not only the detection accuracy is improved, but our proposed system also can significantly reduce the information disclosure as well as network traffic in exchanging data among the IoT gateways. Through thorough simulations on real datasets, we show that the performance obtained by our proposed method can outperform those of the conventional machine learning methods.",https://ieeexplore.ieee.org/document/9120761/,2020 IEEE Wireless Communications and Networking Conference (WCNC),25-28 May 2020,ieeexplore
10.1109/EAEEIE.2013.6576535,Cooperation between industry and university based on the evaluation of the industrial research results in the academic environment,IEEE,Conferences,"Based on the European Structural Funds it was developed the Intelligent Mobile Box, Intelligent Panel Controller with intelligent adaptive controllers within the industrial research and experimental development in the company Kybernetes, s.r.o. Within the frame of the academic-industry cooperation, the intelligent adaptive controller was tested at the Department of Cybernetics and Artificial Intelligence, Technical university of Kosice, Slovakia. The tests of the mobile intelligent adaptive controller were performed on two levels of university study, on the Bachelor level on the exercises from the subject “Control of Technological Processes” and on the Engineering level the exercises from the subject “Intelligent Control Networks” and on one Diploma project. Goals of students of the Control of the technological processes course had two goals, firstly to connect the intelligent adaptive controller to pre-defined controlled system (real plant, real model or simulated model) and next to validate the control results. Students of the Diploma project on the Engineering level had more advanced goals. Tasks defined for engineering students were to connect the intelligent adaptive controller to non-defined controlled system, setup the adaptivity process of the controller regarding the learning error, parameterize the control system, observe and validate the control results. Both sides concluded this cooperation as very valuable. Main contributions for students were (U1) the challenge to apply studied theoretical knowledge on the real industrial controllers, (U2) experience with new research results and technologies deployed in industry and (U3) the implementation of the control and adaptive algorithms from abstract mathematical area to real PLC controller. On side of industry research company the main contributions were (C1) testing of designed algorithms and (C2) user feedback from students to make the application HMI interface more understandable a native.",https://ieeexplore.ieee.org/document/6576535/,2013 24th EAEEIE Annual Conference (EAEEIE 2013),30-31 May 2013,ieeexplore
,Corporate Social Responsibility Challenges and Risks of Industry 4.0 technologies: A review,VDE,Conferences,"The fourth industrial revolution arrived with many enabling technologies that would impact important sociological aspects in the industry. Some of the Industry 4.0 technologies are already running in different industrial application, and other are still as a paradigm state. The social, economic, and environmental acceptance of Industry 4.0 technologies is still under discussion, which open new opportunities to execute various analysis about the possible implications of the implementation of such technologies. This article refers to an exploratory analysis and identification of the different challenges and risks of this new Industry 4.0 paradigm and its related technologies. The technologies under review were Internet of Things, Artificial Intelligence, Cloud Computing, cybersecurity, bid data, blockchain, 5G, robotics, adding manufacturing, unmanned systems, autonomous vehicles, virtual reality, and augmented reality. As a result, different social challenges and risks were identified for each technology, starting from vulnerability, implementation cost, until social aspects such as education and unemployment caused by those new technologies. In conclusion, Industry 4.0 arrived with a lot of benefits to the industry business, but companies should not stop thinking about sustainable development.",https://ieeexplore.ieee.org/document/8835964/,"Smart SysTech 2019; European Conference on Smart Objects, Systems and Technologies",4-5 June 2019,ieeexplore
10.1109/COINS51742.2021.9524088,Cyber Security Risks of Technical Components in Industry 4.0,IEEE,Conferences,"Industry 4.0 is a new concept of automation data exchange in manufacturing. Industry 4.0 consists of various technical components such as Artificial Intelligence (AI), IoT, OPC UA, and cloud. There could be cyber risks when these technical components are used in Industrial Control System (ICS) without security consideration. Therefore cyber risk assessment, reconsideration of implementation, operation are necessary to adopt Industry 4.0. Problems of previous research for ICS risk assessments are some of them are theoretical and conceptual methods such as simulations. Moreover, they do not focus on Industry 4.0 components.The objective of this research is to prove the concrete cyber risks and introduces corresponding secure implementations for Industry 4.0 components such as Artificial Intelligence (AI), the Internet of Things (IoT), OPC Unified Architecture (OPC UA). The proposed method has benefits for clarifying impacts on the real world by cyber-attacks through penetration tests against ICS testbed with actual machines.",https://ieeexplore.ieee.org/document/9524088/,2021 IEEE International Conference on Omni-Layer Intelligent Systems (COINS),23-25 Aug. 2021,ieeexplore
10.1109/IPFA49335.2020.9260582,"Detection and prevention of assembly defects, by machine learning algorithms, in semiconductor industry for automotive",IEEE,Conferences,"Years of experience within semiconductor manufacturing facilities have led to optimize processes to serve both quality and cost. The solution to achieve next generational levels requires a new approach: this one is fitting with implementation of advanced analytics and machine learning algorithms. Applied to manufacturing data which corresponds with a real big data context, these algorithms can provide insights and automate responses to detect, prevent and ultimately eliminate the most severe failure modes. The project described in this paper targets a wafer sawing process. Various challenges that are raised in such a project are of different natures. A first one is the need for a high level of technical expertise in the manufacturing process of focus: this is essential to define the meaningful dataset that represents comprehensively the desired output of the process. Another component is the data collection aspect: many data have to be collected, stored and parsed, and some small signals found will become the leading indicator to an upcoming process degradation and capability of capturing them is essential. Another key data is traceability of the processed material. Additionally, ensuring an informatic technology architecture to support collection, storage, parsing and computation of the datasets is a significant challenge. Lastly, project success is related to the data scientist expertise to build adequate machine learning algorithms. Optimization of the models can take several iterations with back and forth communication between data scientists and process technical experts. This paper describes issues revealed, some solutions found, and future expectations.",https://ieeexplore.ieee.org/document/9260582/,2020 IEEE International Symposium on the Physical and Failure Analysis of Integrated Circuits (IPFA),20-23 July 2020,ieeexplore
10.1109/EuCNC/6GSummit51104.2021.9482590,Empowering Industry 4.0 and Autonomous Drone Scouting use cases through 5G-DIVE Solution,IEEE,Conferences,"The 5G Edge Intelligence for Vertical Experimentation (5G-DIVE) project aims at demonstrating the technical merits and business value proposition of 5G technologies in two vertical pilots, namely the Industry 4.0 (I4.0) and Autonomous Drones Scout (ADS) pilots. This paper presents an overview of the overall 5G-DIVE solution and reports the results of the initial validation campaign of the selected use case, featuring 5G connectivity, distributed Edge computing, and artificial intelligence. The initial results for the I4.0 provide a baseline for next step validation campaign targeting a broader scale 5G implementation, while the ADS results provides promising results for enhancing the autonomous navigation in real-time.",https://ieeexplore.ieee.org/document/9482590/,2021 Joint European Conference on Networks and Communications & 6G Summit (EuCNC/6G Summit),8-11 June 2021,ieeexplore
10.1109/SMC.2019.8913901,Explainable Machine Learning in Industry 4.0: Evaluating Feature Importance in Anomaly Detection to Enable Root Cause Analysis,IEEE,Conferences,"In the past recent years, Machine Learning methodologies have been applied in countless application areas. In particular, they play a key role in enabling Industry 4.0. However, one of the main obstacles to the diffusion of Machine Learning-based applications is related to the lack of interpretability of most of these methods. In this work, we propose an approach for defining a `feature importance' in Anomaly Detection problems. Anomaly Detection is an important Machine Learning task that has an enormous applicability in industrial scenarios. Indeed, it is extremely relevant for the purpose of quality monitoring. Moreover, it is often the first step towards the design of a Machine Learning-based smart monitoring solution because Anomaly Detection can be implemented without the need of labelled data. The proposed feature importance evaluation approach is designed for Isolation Forest, one of the most commonly used algorithm for Anomaly Detection. The efficacy of the proposed method is tested on synthetic and real industrial datasets.",https://ieeexplore.ieee.org/document/8913901/,"2019 IEEE International Conference on Systems, Man and Cybernetics (SMC)",6-9 Oct. 2019,ieeexplore
10.1109/CTIT.2018.8649493,Game Theoretic Approach for Applying Artificial Intelligence in the Credit Industry,IEEE,Conferences,"The law of accelerating returns can be viewed as a concept that describes acceleration of technological progress. The idea is that tools are used for developing more advanced tools that are applied for creating even more advanced tools etc. A similar idea has been implemented in algorithms for advancing artificial intelligence. In this paper, the results of applying these algorithms in games are discussed. Nevertheless, real life tasks seem more complicated. The game theoretic approach can be applied for transition from theoretical and unrealistic games to more complex and practical tasks. Applications of the game theoretic approach to advance artificial intelligence in solving tasks in the credit industry are proposed.",https://ieeexplore.ieee.org/document/8649493/,2018 Fifth HCT Information Technology Trends (ITT),28-29 Nov. 2018,ieeexplore
10.1109/COMPSAC.2018.10204,Indoor Augmented Reality Using Deep Learning for Industry 4.0 Smart Factories,IEEE,Conferences,"This paper proposes to design, develop and implement a fast and markerless mobile augmented reality system to achieve the registration for, the visualization of, and the interaction with machines in indoor smart factories with Industry 4.0 vision. A lightweight deep-learning image detection module based on MobileNets running on mobile devices is used to detect/recognize different machines and different portions of machines. Internet of Things (IoT) networking allows machines and sensors in machines to report data, such as machine settings and machine states, to the cloud-side server. Thus, augmented information associated with a machine portion can be derived from the server and superimposed with the portion image shown on the device display. Furthermore, interaction methods based on touch gestures and distance calculation are also implemented. A prototype system is developed and tested in a mechanical workshop for the purpose of validation and evaluation. The system is shown to achieve high detection accuracy, intuitive visualization, and unique interaction modes.",https://ieeexplore.ieee.org/document/8377831/,2018 IEEE 42nd Annual Computer Software and Applications Conference (COMPSAC),23-27 July 2018,ieeexplore
10.23919/ECC.2003.7085250,Inferential sensor for the olive oil industry,IEEE,Conferences,"This paper shows an inferential sensor that has been developed to be used in the olive oil industry. This sensor has been designed to measure two variables that appear in the elaboration of olive oil in a mill which are very difficult to be measured on line by a physical sensor. The knowledge of these variables on line is crucial for the optimal operation of the process, since they provide the state of the plant, allowing the development of a control strategy that can improve the quality and yield of the product. This sensor measures variables that in other case should come form laboratory analysis with large processing delays or from very expensive and difficult to use on line analysers. The sensor has been devised based upon artificial Neural Networks (NN) and has been implemented as a routine running on a Programmable Logic Controller (PLC) and successfully tested on a real plant.",https://ieeexplore.ieee.org/document/7085250/,2003 European Control Conference (ECC),1-4 Sept. 2003,ieeexplore
10.1109/RTSI.2019.8895598,Intelligent Embedded Load Detection at the Edge on Industry 4.0 Powertrains Applications,IEEE,Conferences,"In the context of Industry 4.0, there has been great focus in developing intelligent sensors. Deploying them, condition monitoring and predictive maintenance have become feasible solutions to minimize operating and maintenance costs while also increasing safety. A critical aspect is the applied load to the supervised machinery system. Vibration data can be used to determine the current condition, but this needs signal processing specially developed and adapted to the monitored machine part for feature extraction. Artificial intelligence (AI) can, on one hand, simplify the development of such special purpose processing and on another hand be used to monitor and classify machine conditions by learning features directly from data. By bringing the AI computation as close as possible to the sensor (Edge-AI), data bandwidth can be minimized, system scalability and responsiveness can be improved and real-time requirements can be fulfilled. This work describes how Edge-AI on a STM32-bit microcontroller can be implemented. Our experimental setup demonstrates how AI can be effectively used to detect and classify the load on a powertrain. In order to do this, we use a MEMS capacity accelerometer to sense vibrations of the system. Also, this work demonstrates how Deep Neural Networks (DNN) for signal classification are build and trained by using an open-source deep learning framework and how the code library for the microcontroller is automatically generated afterwards by using STM32Cube. AI toolchain. We compare the classification accuracy of a memory compressed DNN against an uncompressed DNN.",https://ieeexplore.ieee.org/document/8895598/,2019 IEEE 5th International forum on Research and Technology for Society and Industry (RTSI),9-12 Sept. 2019,ieeexplore
10.1109/AIM46487.2021.9517377,Introducing adaptive mechatronic designs in bulk handling industry,IEEE,Conferences,"The advances of mechatronic system design and system integration have shown improvements in functionality, performance and energy efficiency in many applications across industries, from autonomous ground vehicles and drones to conveyor belts. This trend has been adopted in some industries more than others. The design of equipment to handle granular or bulk material is commonly based on traditional approaches. Therefore, introducing mechatronic concepts in the design procedure can enable new possibilities, such as sensor integration and data analyses, adaptability and control. The efficiency of bulk material handling equipment in ports, agriculture and food processing is heavily influenced by the operational conditions. Typically, a piece of equipment is designed for defined operational conditions when the maximum performance can be achieved. In this work the concept of adaptability to varying operational conditions is explored by understanding the technologies implemented in other industries and the feasibility to be implemented in the bulk handling equipment design. Sensing technology, actuation and adaptability are systematically presented in this work to support the design process of the next generation of bulk handling equipment. This will pave the way for incorporating the technological trends in the design, such as: sustainability, “smartness”, Internet of Things, Industry 4.0, digital twin and machine learning. Adaptive mechatronic solutions will play a crucial role in generating and implementing innovative sustainable solutions for bulk handling equipment.",https://ieeexplore.ieee.org/document/9517377/,2021 IEEE/ASME International Conference on Advanced Intelligent Mechatronics (AIM),12-16 July 2021,ieeexplore
10.1109/ICDE51399.2021.00283,Learning to Optimize Industry-Scale Dynamic Pickup and Delivery Problems,IEEE,Conferences,"The Dynamic Pickup and Delivery Problem (DPDP) is aimed at dynamically scheduling vehicles among multiple sites in order to minimize the cost when delivery orders are not known a priori. Although DPDP plays an important role in modern logistics and supply chain management, state-of-the-art DPDP algorithms are still limited on their solution quality and efficiency. In practice, they fail to provide a scalable solution as the numbers of vehicles and sites become large. In this paper, we propose a data-driven approach, Spatial-Temporal Aided Double Deep Graph Network (ST-DDGN), to solve industry-scale DPDP. In our method, the delivery demands are first forecast using spatial-temporal prediction method, which guides the neural network to perceive spatial-temporal distribution of delivery demand when dispatching vehicles. Besides, the relationships of individuals such as vehicles are modelled by establishing a graph-based value function. ST-DDGN incorporates attention-based graph embedding with Double DQN (DDQN). As such, it can make the inference across vehicles more efficiently compared with traditional methods. Our method is entirely data driven and thus adaptive, i.e., the relational representation of adjacent vehicles can be learned and corrected by ST-DDGN from data periodically. We have conducted extensive experiments over real-world data to evaluate our solution. The results show that ST-DDGN reduces 11.27% number of the used vehicles and decreases 13.12% total transportation cost on average over the strong baselines, including the heuristic algorithm deployed in our UAT (User Acceptance Test) environment and a variety of vanilla DRL methods. We are due to fully deploy our solution into our online logistics system and it is estimated that millions of USD logistics cost can be saved per year.",https://ieeexplore.ieee.org/document/9458860/,2021 IEEE 37th International Conference on Data Engineering (ICDE),19-22 April 2021,ieeexplore
10.1109/ICICTA.2010.569,Model of Viability Prediction Based on Neural Network and Data Mining Technique for Forest Industry Enterprise,IEEE,Conferences,"The operating status of a forest industry enterprise is disclosed periodically for viability. As a result, the manager usually only get information about the operating decision. An employer may be in after the formal financial statement has been published. If the employer executives intentionally package financial statements with the purpose of hiding the actual status of the forestry industry enterprise, then manager will have even less chance of obtaining the real financial information. To improve the accuracy of the viability prediction, viability ratios, non-viability ratios, and factor analysis had been used to extract adaptable variables. Moreover, the neural network and data mining technique were used to build the viability prediction model. The empirical experiment with a total of viability and non-viability ratios and projects as the initial samples obtained a satisfactory result, which testifies for the feasibility and validity of our proposed methods for the viability prediction of forestry industry enterprise.",https://ieeexplore.ieee.org/document/5522660/,2010 International Conference on Intelligent Computation Technology and Automation,11-12 May 2010,ieeexplore
10.1109/SCC47175.2019.9116104,Modeling and management of human resources in the reconfiguration of production system in industry 4.0 by neural networks,IEEE,Conferences,"In Industry 4.0, the role of employees changes significantly. Real-time production line control transforms job content. Work processes affect working conditions. The implementation of a socio-technical approach to the organization of work gives workers the opportunity to adapt their skills. Indeed, production work will become more and more multi-factor, especially with regard to control and decision-making tasks. In this paper, a proposal for an intelligent system for modeling skills and human resource management in the production system chain through the use of two artificial neural networks. The first NN1 network allows for the identification of the human factor, as well as the second NN2 network is reserved for valuing the human skills needed in Industry 4.0.",https://ieeexplore.ieee.org/document/9116104/,"2019 International Conference on Signal, Control and Communication (SCC)",16-18 Dec. 2019,ieeexplore
10.1109/ISCAS.2019.8702575,Multi-View Fusion Neural Network with Application in the Manufacturing Industry,IEEE,Conferences,"In recent years the research community and industry have paid high attention to the field of machine learning, especially deep learning. Nowadays many real-world classification or rather prediction applications are implemented by neural network models. We propose a multi-view fusion neural network with application in the manufacturing industry. Image information of multiple cameras is fused and used by the proposed model to predict the state of a manufacturing machine. Experiments show that the overall classification performance is increased from a baseline of 92.7% to 99.5% by the fusion model.",https://ieeexplore.ieee.org/document/8702575/,2019 IEEE International Symposium on Circuits and Systems (ISCAS),26-29 May 2019,ieeexplore
10.23919/DATE48585.2020.9116407,Network Synthesis for Industry 4.0,IEEE,Conferences,"Today's factory machines are ever more connected with SCADA, MES, ERP applications as well as external systems for data analysis. Different types of network architectures must be used for this purpose. For instance, control applications at the lowest level are susceptible to delays and errors while data analysis with machine learning procedures requires to move a large amount of data without real-time constraints. Standard data formats, like Automation Markup Language (AML), have been established to document factory environment, machine placement and network deployment, however, no automatic technique is currently available in the context of Industry 4.0 to choose the best mix of network architectures according to spacial constraints, cost, and performance. We propose to fill this gap by formulating an optimization problem. First of all, spatial and communication requirements are extracted from the AML description. Then, the optimal interconnection of wired or wireless channels is obtained according to application objectives. Finally, this result is back-annotated to AML to be used in the life cycle of the production system. The proposed methodology is described through a small, but complete, smart production plant.",https://ieeexplore.ieee.org/document/9116407/,"2020 Design, Automation & Test in Europe Conference & Exhibition (DATE)",9-13 March 2020,ieeexplore
10.1109/DSAA53316.2021.9564181,Predictive maintenance based on anomaly detection using deep learning for air production unit in the railway industry,IEEE,Conferences,"Predictive maintenance methods assist early detection of failures and errors in machinery before they reach critical stages. This study proposes a data-driven predictive maintenance framework for the air production unit (APU) system of a train of Metro do Porto by deep learning based on a sparse autoencoder (SAE) network that efficiently detects abnormal data and considerably reduces the false alarm rate. Several analog and digital sensors installed on the APU system allow the detection of behavioral changes and deviations from the normal pattern by analyzing the collected data. We implemented two versions of the SAE network in which we inputted analog sensors data and digital sensors data, and the experimental results show that the failures due to air leakage problems are predicted by analog sensors data while other types of failures are identified by digital sensors data. A low pass filter is applied to the output of the SAE network, and a sequence of abnormal data is used as an alarm for the APU system failure. Performance indicators of the SAE network with digital sensors data, in terms of F1 Score, Recall, and Precision, are respectively, about 33.6%, 42%, and 28% better than those of the SAE network with analog sensors data. For comparison purposes, we also implemented a variational autoencoder (VAE). The results show that SAE performance is better than that of VAE by 14%, 77%, and 37% respectively, for Recall, Precision and F1 Score.",https://ieeexplore.ieee.org/document/9564181/,2021 IEEE 8th International Conference on Data Science and Advanced Analytics (DSAA),6-9 Oct. 2021,ieeexplore
10.1109/ITSC45102.2020.9294450,Predictive maintenance leveraging machine learning for time-series forecasting in the maritime industry,IEEE,Conferences,"One of the key challenges in the maritime industry refers to minimizing the time a vessel cannot be utilized, which has multiple effects. The latter is addressed through maintenance approaches that however in many cases are not efficient in terms of cost and downtime. Predictive maintenance provides optimized maintenance scheduling offering extended vessel lifespan, coupled with reduced maintenance costs. As in several industries, including the maritime domain, an increasing amount of data is made available through the deployment and exploitation of data sources, such as on board sensors that provide real-time information. These data provide the required ground for analysis and thus support for various types of data-driven decision making. In the maritime domain, sensors are deployed on vessels to monitor their engines and data analysis tools are needed to assist engineers towards reduced operational risk through predictive maintenance solutions that are put in place. In this paper, we present an approach for anomaly detection on time-series data, utilizing machine learning on the vessels sensor data, in order to predict the condition of specific parts of the vessel's main engine and thus facilitate predictive maintenance. The novel characteristic of the proposed approach refers both to the inclusion of new innovative models to address the case of predictive maintenance in maritime and the combination of those different models, highlighting an improved result in terms of evaluation metrics.",https://ieeexplore.ieee.org/document/9294450/,2020 IEEE 23rd International Conference on Intelligent Transportation Systems (ITSC),20-23 Sept. 2020,ieeexplore
10.1109/AQTR49680.2020.9129934,Rapid Prototyping of IoT Applications for the Industry,IEEE,Conferences,"In this article a novel approach to rapid IoT application prototype design and development is presented using an existing experimental dataset and a functional model. Using the existing data, we populate our NoSQL Apache Cassandra database cluster with legacy data and generate similar data using a python code, considering the Mosquitto MQTT protocol implementation and Node-RED node.js development environment. Using Node-RED, we display the data already collected, and dynamically create new data that can be monitored in real-time in the provided dashboard. The possibilities and utility of this approach are explored in the article, and a simple prototype application for modeling the open access Combined Cycle Power Plant (CCPP) dataset provided by the UCI Machine Learning Repository is presented to prove the efficiency and rapidity of IoT application development. The presented system development approach can be used in industrial environment for rapid development of IIoT applications.",https://ieeexplore.ieee.org/document/9129934/,"2020 IEEE International Conference on Automation, Quality and Testing, Robotics (AQTR)",21-23 May 2020,ieeexplore
10.1109/IMITEC50163.2020.9334129,Real Time Customer Churn Scoring Model for the Telecommunications Industry,IEEE,Conferences,"There are two types of customers in the telecommunication industry; the pre-paid and the contract customers. In South Africa it is the pre-paid customers that keep telcos constantly worried because such customers do not have anything binding them to the company, they can leave and join a competitor at any time. To retain such customers, telcos need to customise suitable solutions especially for those customers that are agitating and can churn at any time. This needs customer churn prediction models that would take advantage of big data analytics and provide the telco industry with a real time solution. The purpose of this study was to develop a real time customer churn prediction model. The study used the CRISP-DM methodology and the three machine learning algorithms for implementation. Watson Studio software was used for the model prototype deployment. The study used the confusion matrix to unpack a number of performance measures. The results showed that all the models had some degree of misclassification, however the misclassification rate of the Logistic Regression was very minimal (2.2%) as differentiated from the Random Forest and the Decision Tree, which had misclassification rates of 20.8% and 21.7% respectively. The results further showed that both Random Forest and the Decision Tree had good accuracy rates of 78.3% and 79.2% respectively, although they were still not better than that of the Logistic Regression. Despite the two having good accuracy rates, they had the highest rates of misclassification of class events. The conclusion we drew from this was that, accuracy is not a dependable measure for determining model performance.",https://ieeexplore.ieee.org/document/9334129/,2020 2nd International Multidisciplinary Information Technology and Engineering Conference (IMITEC),25-27 Nov. 2020,ieeexplore
10.1109/ICSIMA47653.2019.9057343,Real-Time Wireless Monitoring for Three Phase Motors in Industry: A Cost-Effective Solution using IoT,IEEE,Conferences,"In recent days modern environment industries are facing rapid flourishing for performance capabilities and their requirements for corporate clients and industrial sector. Internet of Things (IoT) is an innovative and rapidly growing field for automation and evaluation in networks, Artificial Intelligence, data sensing, data mining, and big data. These systems have a great tendency to monitor and control different process used in industries. IoT systems have been implemented and have applications in different industries due to their cost-effectiveness and flexibility In this paper we have developed a system which includes real-time monitoring of current reading of three-phase motor through a wireless network. With the help of this system, data can be saved and monitored and then transmitted to cloud storage. This system contains Arduino-UNO board, ACS-712 current sensor, ESP-8266 Wi-Fi module which sends information to an IoT API service THING-SPEAK that behave like a cloud for various sensors to monitor data. The proposed system was successfully deployed in Aisha Steel Mills, Karachi, Pakistan.",https://ieeexplore.ieee.org/document/9057343/,"2019 IEEE International Conference on Smart Instrumentation, Measurement and Application (ICSIMA)",27-29 Aug. 2019,ieeexplore
10.1109/ICMLC.2009.5212284,SVM optimized scheme based PSO in application of engineering industry process,IEEE,Conferences,"Aimed to the problem that it is hardship to get real-time and on-line measuring parameters in wood drying process, a novel PSO-SVM model that hybridized the particle swarm optimization (PSO) and support vector machines (SVM) to improve the nonlinearity caused by ambient temperature and other disturbance factors is presented. Support vector machines (SVM) based on statistical learning theory and structural risk minimization is proposed to deal with these problems. However, the model complexity and generalization performance of support vector machines (SVM) depend on a good setting of the three parameters (epsiv,c,gamma). In this paper, the particle swarm optimization is applied to optimize the parameters (epsiv,c,gamma) at the same time. Based on the proposed method, both PSO-SVM and SVM models are established and implemented to estimate lumber moisture content value in wood drying process. The result of comparative analysis is given. Experimental results show that solutions obtained by PSO-SVM training seem to be more robust and better generalization performance compared to SVM training.",https://ieeexplore.ieee.org/document/5212284/,2009 International Conference on Machine Learning and Cybernetics,12-15 July 2009,ieeexplore
10.1109/I-SMAC49090.2020.9243544,Scalable IoT Solution using Cloud Services – An Automobile Industry Use Case,IEEE,Conferences,"The role of IoT and related internet-based applications in otherwise mechanical devices to monitor, manage and enhance the performance of the same is quite widespread now. Almost all public cloud service providers provide scalable, fully managed and elastic IoT related services. The data flows from these services are essentially streaming and can be consumed for further use in various predictive, descriptive and visualization modules. The cloud platforms enable ingestion, transformation and usage of the data by providing streaming, machine learning and sharable visualization services. This ecosystem greatly reduces the time to create IoT based minimum viable product creation which in turn enhances the business value realization cycle. The effect of cycle time reduction to design, architect and develop IoT solutions leads to a rapid improvement of business lead time and makes it easier for businesses to gain from the data insights and plan the next course of action. In this paper, one such enterprise graded use case is explored, in which the Azure IoT platform in terms of the offerings and associated ecosystem of Azure Stream Analytics and Azure Machine learning services are explained. This paper covers design, architecture, development and deployment of the solution prepared and how the same is monitored once in production. Security is a very important aspect of the same and here the security architecture is being explored. A conclusion is presented with the scope of future enhancements using auto ML services in serverless platforms to enable real-time automated decision making augmented with human expertise and intelligence.",https://ieeexplore.ieee.org/document/9243544/,"2020 Fourth International Conference on I-SMAC (IoT in Social, Mobile, Analytics and Cloud) (I-SMAC)",7-9 Oct. 2020,ieeexplore
10.1109/ICRITO48877.2020.9198036,State of Art: Energy Efficient Protocols for Self-Powered Wireless Sensor Network in IIoT to Support Industry 4.0,IEEE,Conferences,"Up gradation of manufacturing systems in industries by means of implementing innovative manufacturing techniques that captures real time data, applies machine learning algorithms, makes entire system self-decisive and provides inter connectivity to the whole system is the prime focus of the Industry 4.0. It is aimed at bringing new industrial revolution with the help of internet of things technology due to its considerable influence in the industrial manufacturing process. Though impact of internet of things in industrial sector is huge, a practical implementation incorporates challenges in energy efficiency, self-powered sensor nodes and security. For processing of gathered data self-powered sensor nodes may sinks energy from ambient energy sources. A considerable amount of efforts has been put by researchers to address the challenges for development of energy efficient routing protocol for such nodes. The presented survey is aimed at analyzing the protocols for contribution towards the goals of industry 4.0 Energy efficient protocol will support the system to consume least energy for its operation. Henceforth combination of self-powered wireless sensor network and energy efficient protocol will be useful to power up many industrial IoT applications.",https://ieeexplore.ieee.org/document/9198036/,"2020 8th International Conference on Reliability, Infocom Technologies and Optimization (Trends and Future Directions) (ICRITO)",4-5 June 2020,ieeexplore
10.1109/icABCD49160.2020.9183853,The Impact of Smart Manufacturing Approach On The South African Manufacturing Industry,IEEE,Conferences,"SM is a technology-driven approach that mainly utilises machines to monitor the entire production of an organisation. The objective of SM in an organisation is to identify ways to automatize the manufacturing process while using data analytics to optimize the manufacturing performance. This research mitigates the impact of technology, in this case expressed as SM in South African industries. The research followed a quantitative approach whereby 42 respondents from low, medium low and high technology industries took part in the study. Data has been amassed from first-hand experience by mean of an adapted questionnaire constituted of three sections: The first section was about the general demographic information of the respondents. Section two investigates the respondent's awareness on SM. Finally, section three assessed the impact that SM had on the performance of the organisation. The findings of this study revealed that Smart Manufacturing has a positive impact in South African manufacturing organisations as it allows effective operations, fast response to customers demand, real time operations optimisation. Nevertheless, Smart Manufacturing is a new concept under the fourth industrial revolution in South Africa and will need time before being totally implemented in all organisations as it is costly.",https://ieeexplore.ieee.org/document/9183853/,"2020 International Conference on Artificial Intelligence, Big Data, Computing and Data Communication Systems (icABCD)",6-7 Aug. 2020,ieeexplore
10.1109/ICMAE.2017.8038685,Using artificial intelligence based expert system for selection of design subcontractors: A case study in aerospace industry,IEEE,Conferences,"As one of the top expectations for type certification of an aircraft, Aviation Authorities (AA) regulate design organization to establish Design Assurance System (DAS). DAS is composed of design, independent monitoring and airworthiness functions in which these functions are specialized for aerospace industry. Besides, Design Organization Approval (DOA) is a milestone to establish a rigid Design Assurance System. By this way, design organization assures aircraft development life cycle by complying with aviation regulations. To meet requirements of Design Organization Approval, Design Organization transfers its authority and technical signatories to its subcontractors to improve effectiveness of the system. So, performance of design subcontractors shall be traceable and measurable to match capability requirements of main contractor. Thus, subcontractor evaluation is a long and complicated process; survey implementation could be misleading in some cases. The purpose of this study is to propose a novel tool to measure performance of a design subcontractor according to necessities of Design Assurance System. Up to now, there is no tool to evaluate aviation design subcontractors. With this tool, contractor firm can evaluate multiple criteria in a single run. AHP is used to prioritize criteria relative to each other one-by-one. Then, for subcontractor selection and subcontractor monitoring, Artificial Neural Network (ANN) is applied to optimize decision making process. Annual Actual Data is applied in AHP model to assess current performance score of subcontractor. To have a long term judgment of this system, the model shall be applied to a design subcontractor for more than once on fixed periods such as quarterly, yearly etc.",https://ieeexplore.ieee.org/document/8038685/,2017 8th International Conference on Mechanical and Aerospace Engineering (ICMAE),22-25 July 2017,ieeexplore
10.1109/ISWCS.2019.8877305,Visible Light Positioning for Location-Based Services in Industry 4.0,IEEE,Conferences,"Industry 4.0 refers to the evolution in manufacturing from computerization to fully cyberphysical systems that exploit rich sensor data, adaptive real-time safety-critical control, and machine learning. An important aspect of this vision is the sensing and subsequent association of objects in the physical world with their cyber and virtual counterparts. In this paper we propose Visible Light Positioning (VLP) as an enabler for these Industry 4.0 applications. We also explore sensing techniques, including cameras (and depth sensors), and other light-based solutions for object positioning and detection along with their respective limitations. We then demonstrate an application of positioning for real time robot control in an interactive multiparty cyber-physical-virtual deployment. Lastly, based on our experience with this cyberphysical-virtual application, we propose Ray-Surface Positioning (RSP), a novel VLP technique, as a low cost positioning system for Industry 4.0.",https://ieeexplore.ieee.org/document/8877305/,2019 16th International Symposium on Wireless Communication Systems (ISWCS),27-30 Aug. 2019,ieeexplore
10.1109/BDCloud.2018.00136,"iSTEP, an Integrated Self-Tuning Engine for Predictive Maintenance in Industry 4.0",IEEE,Conferences,"The recent expansion of IoT-enabled (Internet of Things) devices in manufacturing contexts and their subsequent data-driven exploitation paved the way to the advent of the Industry 4.0, promoting a full integration of IT services, smart devices, and control systems with physical objects, their electronics and sensors. The real-time transmission and analysis of collected data from factories has the potential to create manufacturing intelligence, of which predictive maintenance is an expression. Hence the need to design new approaches able to manage not only the data volume, but also the variety and velocity, extracting actual value from the humongous amounts of collected data. To this aim, we present iSTEP, an integrated Self-Tuning Engine for Predictive maintenance, based on Big Data technologies and designed for Industry 4.0 applications. The proposed approach targets some of the most common needs of manufacturing enterprises: compatibility with both the on-premises and the in-the-cloud environments, exploitation of reliable and largely supported Big Data platforms, easy deployment through containerized software modules, virtually unlimited horizontal scalability, fault-tolerant self-reconfiguration, flexible yet friendly streaming-KPI computations, and above all, the integrated provisioning of self-tuning machine learning techniques for predictive maintenance. The current implementation of iSTEP exploits a distributed architecture based on Apache Kafka, Spark Streaming, MLlib, and Cassandra; iSTEP provides (i) a specific feature engineering block aimed at automatically extracting metrics from the production monitoring time series, which improves the predictive performance by 77% on average, and (ii) a self-tuning approach that dynamically selects the best prediction algorithm, which improves the predictive performance up to 60%. The iSTEP engine provides transparent predictive models, able to provide end users with insights into the knowledge learned, and it has been experimentally evaluated on a public unbalanced failure dataset, whose extensive results are discussed in the paper.",https://ieeexplore.ieee.org/document/8672266/,"2018 IEEE Intl Conf on Parallel & Distributed Processing with Applications, Ubiquitous Computing & Communications, Big Data & Cloud Computing, Social Computing & Networking, Sustainable Computing & Communications (ISPA/IUCC/BDCloud/SocialCom/SustainCom)",11-13 Dec. 2018,ieeexplore
10.1109/TLA.2021.9480156,A Data Governance Framework for Industry 4.0,IEEE,Journals,"The fourth industrial revolution, or Industry 4.0, represents a new stage of evolution in the organization, management and control of the value chain throughout the product or service life cycle. This digitization of the industrial environment is characterized by the connection of Information Technologies (IT) and Operations Technologies (OT) through cyber-physical systems and the Industrial IoT (IIoT). One of the main consequences of this integration is the increasing amount and variety of data generated in real time from different sources. In this environment of intensive generation of actionable information, data becomes a critical asset for Industry 4.0, at all stages of the value chain. However, in order to data become a competitive advantage for the company, it must be managed and governed like any other strategic asset, and therefore it is necessary to rely on a Data Governance system. Industry 4.0 requires a reformulation of governance since the data is a key element and the backbone of the processes of the organization. This paper proposes a Reference Framework for the implementation of Data Governance Systems for Industry 4.0. Previously, it contextualizes data governance for Industry 4.0 environments and identifies the requirements that this framework must address, which are conditioned by the specific features of Industry 4.0, among others, the intensive use of big data, cloud and edge computing, artificial intelligence and current regulations.",https://ieeexplore.ieee.org/document/9480156/,IEEE Latin America Transactions,Dec. 2021,ieeexplore
10.1109/ACCESS.2021.3121302,A Survey on Industry 4.0 for the Oil and Gas Industry: Upstream Sector,IEEE,Journals,"The market volatility in the oil and gas (O&amp;G) sector, the dwindling demand for oil due to the impact of COVID-19, and the push for alternative greener energy are driving the need for innovation and digitization in the O&amp;G industry. This has attracted research interest from academia and the industry in the application of industry 4.0 (I4.0) technologies in the O&amp;G sector. The application of some of these I4.0 technologies has been presented in the literature, but the domain still lacks a comprehensive survey of the application of I4.0 in the O&amp;G upstream sector. This paper investigates the state-of-the-art efforts directed toward I4.0 technologies in the O&amp;G upstream sector. To achieve this, first, an overview of the I4.0 is discussed followed by a systematic literature review from an integrative perspective for publications between 2012–2021 with 223 analyzed documents. The benefits and challenges of the adoption of I4.0 have been identified. Moreover, the paper adds value by proposing a framework for the implementation of I4.0 in the O&amp;G upstream sector. Finally, future directions and research opportunities such as framework, edge computing, quantum computing, communication technologies, standardization, and innovative areas related to the implementation of I4.0 in the upstream sector are presented. The findings from this review show that I4.0 technologies are currently being explored and deployed for various aspects of the upstream sector. However, some of the I4.0 technologies like additive manufacturing and virtual reality are least explored.",https://ieeexplore.ieee.org/document/9579415/,IEEE Access,2021,ieeexplore
10.1109/TII.2019.2962029,An Entropy-Based Approach to Real-Time Information Extraction for Industry 4.0,IEEE,Journals,"Industry 4.0 has drawn considerable attention from industry and academic research communities. The recent advances in Internet of Things (IoT), Big Data analytics, sensor technology, and artificial intelligence have led to the design and implementation of novel approaches to take full advantage of data-driven solutions applicable to Industry 4.0. With the availability of large datasets, it has become crucially important to identify the appropriate amount of relevant information, which would optimize the overall analysis of the corresponding systems. In this article, specific properties of dynamically evolving data systems are introduced and investigated, which provide framework to assess the appropriate amount of representative information.",https://ieeexplore.ieee.org/document/8941297/,IEEE Transactions on Industrial Informatics,Sept. 2020,ieeexplore
10.1109/TCPMT.2020.3047089,Automatic Industry PCB Board DIP Process Defect Detection System Based on Deep Ensemble Self-Adaption Method,IEEE,Journals,"A deep ensemble convolutional neural network (CNN) model to inspect printed circuit board (PCB) board dual in-line package (DIP) soldering defects with Hybrid-YOLOv2 (YOLOv2 as a foreground detector and ResNet-101 as a classifier) and Faster RCNN with ResNet-101 and Feature Pyramid Network (FPN) (FRRF) achieved a detection rate of 97.45% and a false alarm rate (FAR) of 20%-30% in the previous study [34]. However, applying the method to other production lines, environmental variations, such as lighting, orientations of the sample feeds, and mechanical deviations, led to the degradation in detection performance. This article proposes an effective self-adaption method that collects “exception data” like the samples with which the Artificial Intelligent (AI) model made mistakes from the automated optical inspection inference edge to the training server, retraining with exceptions on the server and deploying back to the edge. The proposed defect detection system has been verified with real tests that achieved a detection rate of 99.99% with an FAR 20%-30% and less than 15 s of inspection time on a resolution $7296 \times 6000$ PCB image. The proposed system has proven capable of shortening inspection and repair time for online operators, where a 33% efficiency boost from the three production lines of the collaborated factory has been reported [6]. The contribution of the proposed retraining mechanism is threefold: 1) because the retraining process directly learns from the exceptions, the model can quickly adapt to the characteristic of each production line, leading to a fast and reliable mass deployment; 2) the proposed retraining mechanism is a necessary self-service for conventional users as it incrementally improves the detection performance without professional guidance or fine-tuning; and 3) the semiautomatic exception data collection method helps to reduce the time-consuming manual labeling during the retraining process.",https://ieeexplore.ieee.org/document/9306873/,"IEEE Transactions on Components, Packaging and Manufacturing Technology",Feb. 2021,ieeexplore
10.1109/TII.2018.2807797,Deep Endoscope: Intelligent Duct Inspection for the Avionic Industry,IEEE,Journals,"We present the first autonomous endoscope for the visual inspection of very small ducts and cavities, up to a 6-mm diameter. The system has been designed, implemented, and tested in a challenging industrial scenario and in strict collaboration with an avionic industry partner. The inspected objects are metallic gearboxes eventually presenting different residuals (e.g., sand, machining swarfs, and metallic dust) inside the oil ducts. The automatic system is actuated by a robotic arm that moves the endoscope with a microcamera inside the gearbox duct, while a deep-learning-based spatio-temporal image analysis module detects, classifies, and localizes defects in real time. Feedback is given to the robotic arm in order to move or extract the endoscope given the detected anomalies. Evaluation provides a detection rate of nearly 98% given different tests with different types of residuals and duct structures.",https://ieeexplore.ieee.org/document/8295126/,IEEE Transactions on Industrial Informatics,April 2018,ieeexplore
10.1109/ACCESS.2020.2998723,"Digital Twin for the Oil and Gas Industry: Overview, Research Trends, Opportunities, and Challenges",IEEE,Journals,"With the emergence of industry 4.0, the oil and gas (O&amp;G) industry is now considering a range of digital technologies to enhance productivity, efficiency, and safety of their operations while minimizing capital and operating costs, health and environment risks, and variability in the O&amp;G project life cycles. The deployment of emerging technologies allows O&amp;G companies to construct digital twins (DT) of their assets. Considering DT adoption, the O&amp;G industry is still at an early stage with implementations limited to isolated and selective applications instead of industry-wide implementation, limiting the benefits from DT implementation. To gain the full potential of DT and related technological adoption, a comprehensive understanding of DT technology, the current status of O&amp;G-related DT research activities, and the opportunities and challenges associated with the deployment of DT in the O&amp;G industry are of paramount importance. In order to develop this understanding, this paper presents a literature review of DT within the context of the O&amp;G industry. The paper follows a systematic approach to select articles for the literature review. First, a keywords-based publication search was performed on the scientific databases such as Elsevier, IEEE Xplore, OnePetro, Scopus, and Springer. The filtered articles were then analyzed using online text analytic software (Voyant Tools) followed by a manual review of the abstract, introduction and conclusion sections to select the most relevant articles for our study. These articles and the industrial publications cited by them were thoroughly reviewed to present a comprehensive overview of DT technology and to identify current research status, opportunities and challenges of DT deployment in the O&amp;G industry. From this literature review, it was found that asset integrity monitoring, project planning, and life cycle management are the key application areas of digital twin in the O&amp;G industry while cyber security, lack of standardization, and uncertainty in scope and focus are the key challenges of DT deployment in the O&amp;G industry. When considering the geographical distribution for the DT related research in the O&amp;G industry, the United States (US) is the leading country, followed by Norway, United Kingdom (UK), Canada, China, Italy, Netherland, Brazil, Germany, and Saudi Arabia. The overall publication rate was less than ten articles (approximately) per year until 2017, and a significant increase occurred in 2018 and 2019. The number of journal publications was noticeably lower than the number of conference publications, and the majority of the publications presented theoretical concepts rather than the industrial implementations. Both these observations suggest that the DT implementation in the O&amp;G industry is still at an early stage.",https://ieeexplore.ieee.org/document/9104682/,IEEE Access,2020,ieeexplore
10.1109/ACCESS.2021.3103680,Human Centric Digital Transformation and Operator 4.0 for the Oil and Gas Industry,IEEE,Journals,"Working at an oil and gas facility, such as a drilling rig, production facility, processing facility, or storage facility, involves various challenges, including health and safety risks. It is possible to leverage emerging digital technologies such as smart sensors, wearable or mobile devices, big data analytics, cloud computing, extended reality technologies, robotic systems, and drones to mitigate the challenges faced by oil and gas workers. While these technologies are not new to the oil and gas industry, most of its existing digital transformation initiatives follow business or process-centric approaches, in which the critical driver of the technology adoption is the enhancement of production, efficiency, and revenue. As a result, they may not address the challenges faced by the workers. As oil and gas workers are among the essential assets in the oil and gas industry, it is vital to address the challenges faced by these workers. This paper proposes a human-centric digital transformational framework for the oil and gas industry to deploy existing digital technologies to enhance their workers' health, safety, and working conditions. The paper outlines the critical challenges faced by oilfield workers, introduces a system architecture to implements a human-centric digital transformation, discusses the opportunities of the proposed framework, and summarizes the key impediment for the proposed framework.",https://ieeexplore.ieee.org/document/9509417/,IEEE Access,2021,ieeexplore
10.1109/JETCAS.2021.3097699,Machine-Learning-Based Microwave Sensing: A Case Study for the Food Industry,IEEE,Journals,"Despite the meticulous attention of food industries to prevent hazards in packaged goods, some contaminants may still elude the controls. Indeed, standard methods, like X-rays, metal detectors and near-infrared imaging, cannot detect low-density materials. Microwave sensing is an alternative method that, combined with machine learning classifiers, can tackle these deficiencies. In this paper we present a design methodology applied to a case study in the food sector. Specifically, we offer a complete flow from microwave dataset acquisition to deployment of the classifiers on real-time hardware and we show the effectiveness of this method in terms of detection accuracy. In the case study, we apply the machine-learning based microwave sensing approach to the case of food jars flowing at high speed on a conveyor belt. First, we collected a dataset from hazelnut-cocoa spread jars which were uncontaminated or contaminated with various intrusions, including low-density plastics. Then, we performed a design space exploration to choose the best MLPs as binary classifiers, which resulted to be exceptionally accurate. Finally, we selected the two most light-weight models for implementation on both an ARM-based CPU and an FPGA SoC, to cover a wide range of possible latency requirements, from loose to strict, to detect contaminants in real-time. The proposed design flow facilitates the design of the FPGA accelerator that might be required to meet the timing requirements by using a high-level approach, which might be suited for the microwave domain experts without specific digital hardware skills.",https://ieeexplore.ieee.org/document/9489295/,IEEE Journal on Emerging and Selected Topics in Circuits and Systems,Sept. 2021,ieeexplore
10.1109/ITCA52113.2020.00084,5G Enabling Technologies in Rail,IEEE,Conferences,"Leveraging recent advances in IoT, blockchain, big data, artificial intelligence, and others, these state-of-art technologies still have difficulty in massive deployment and real fruition of working together in the industry. 5G brings new opportunities through enabling these technologies and thus leads to new developments. This paper introduces 5G and analyzes how 5G enable other technologies. Besides, it slices complicated railway scenarios into three aspects, and then discusses applications and innovations 5G and technologies can bring to rail.",https://ieeexplore.ieee.org/document/9422090/,2020 2nd International Conference on Information Technology and Computer Application (ITCA),18-20 Dec. 2020,ieeexplore
10.1109/ICITAET47105.2019.9170214,A Basic Permanent Magnets Array Interaction Project for Teaching Artificial Intelligence as a Complementary Model,IEEE,Conferences,"There are new algorithms such as artificial intelligence (AI) methodologies that have achieved accurate representation of experimental systems. On the other hand, undergraduate freshmen students must understand AI methodologies since the industry has developed several products based on those and some academic problems also can be solved using AI. If those students do not learn how to model real systems using AI, they will be losing the opportunity of applying this powerful tool for solving several real problems in their professional life. Since the AI model can be a representation for forecasting the performance of the real model, this model can help the design process and provide information during its operation. This paper proposes an engineering project to teach artificial intelligence algorithms using real systems that are non-linear. Since permanent magnets are used in several applications, they can be attractive for modeling those when they are interacting between them; hence, this paper shows the interaction among them when they are deployed as an electrical power source. Moreover, this source could be classified as a renewable energy source. The basic generation of electrical energy is based on changing the magnetic field. Although the operation principle is basic, the electrical source has a non-liner description that is extremely complex so AI could be applied to create a model that represents those non-linear relationships in a precise manner. The main goal of this work is to describe an undergraduate project that can be used for teaching how to model a real system using AI algorithms. The main characteristics and properties of the permanent magnets are studied for the comprehension of how magnets can be implemented. It is also examined the viability for the construction of an electric motor using only permanent magnets, based on the analysis of different designs and materials and finally an AI model is created.",https://ieeexplore.ieee.org/document/9170214/,2019 International Conference on Innovative Trends and Advances in Engineering and Technology (ICITAET),27-28 Dec. 2019,ieeexplore
10.1109/UIC-ATC.2017.8397649,A CNN based bagging learning approach to short-term load forecasting in smart grid,IEEE,Conferences,"Short-term load forecasting in smart grid is key to electricity dispatch scheduling, reliability analysis, and maintenance planning for the generators. In this paper, we present a convolutional neural networks (CNN) based bagging model for forecasting hourly loads. We employ CNN to train forecasting models on big load data sets. Then, we segment a real industry load data set into many subsets, fine-tune the forecasting models on these subsets to learn weak forecasting models, and assemble these weak forecasting models to conduct a bagging forecasting model, where the learning and assembling procedures are implemented on Spark. Specifically, all load samples in those data sets are reorganized as images with respect to similarities between relations of pixels in images and those of features in load samples. Experimental results indicate the effectiveness of the proposed method.",https://ieeexplore.ieee.org/document/8397649/,"2017 IEEE SmartWorld, Ubiquitous Intelligence & Computing, Advanced & Trusted Computed, Scalable Computing & Communications, Cloud & Big Data Computing, Internet of People and Smart City Innovation (SmartWorld/SCALCOM/UIC/ATC/CBDCom/IOP/SCI)",4-8 Aug. 2017,ieeexplore
10.1109/BDCloud.2018.00112,A CTR Prediction Approach for Advertising Based on Embedding Model and Deep Learning,IEEE,Conferences,"In the field of CTR prediction for advertisement, a lot of surface models such as LR, FM have been widely used in industry. Meanwhile, the studies based on deep learning also launched in this field. Because advertisement dataset contains massive multi-field categorical data, this paper proposes a kind of CTR prediction model based on embedding model and deep learning-FMSDAELR. This proposed model can effectively model multi-field categorical data, extract the complex nonlinear correlated relationship, and capture the important features contributing to CTR prediction for advertisement. The result of experiment based on the real dataset shows that the proposed approach can obtain superior performance than other baseline models.",https://ieeexplore.ieee.org/document/8672361/,"2018 IEEE Intl Conf on Parallel & Distributed Processing with Applications, Ubiquitous Computing & Communications, Big Data & Cloud Computing, Social Computing & Networking, Sustainable Computing & Communications (ISPA/IUCC/BDCloud/SocialCom/SustainCom)",11-13 Dec. 2018,ieeexplore
10.1109/GIOTS.2018.8534533,A Car as a Semantic Web Thing: Motivation and Demonstration,IEEE,Conferences,"Car signal data is usually hard to access, understand and integrate for non automotive domain experts. In this paper, we use semantic technologies for enriching signal data in the automotive industry and access it through Web of Things interactions. This combination allows the access and integration of car data from the web. We built VSSo, a Vehicle Signal ontology based on SOSA/SSN Observations and Actuations, and generated WoT Actions, Events and Properties, enriched with domain metadata. We mapped VSSo to a Web of Things ontology and we developed a Web of Things protocol binding with LwM2M, and made an implementation in a real car. This implementation resulted in a first working prototype, and a number of future improvements required in order to be compliant with automotive standards.",https://ieeexplore.ieee.org/document/8534533/,2018 Global Internet of Things Summit (GIoTS),4-7 June 2018,ieeexplore
10.1109/MetroInd4.0IoT51437.2021.9488429,A Case Study on Challenges of Applying Machine Learning for Predictive Drill Bit Sharpness Estimation,IEEE,Conferences,"Tool condition estimation is one of the most crucial aspects of Industry 4.0 and its evolved maintenance paradigm using predictive systems. This paper presents a use case on vibration measurement based Remaining Useful Lifetime (RUL) prediction of drill bits. The scope of this paper is to give insights on the challenges and limitations when building a Machine Learning (ML) based solution. We analyze the captured signals, suitable feature extraction methods and different data-driven / ML models for progressive wear and RUL prediction task. The methods are investigated on a real world drilling experiment with a multi-sensor setup. Our results indicate that during continuous drilling scenarios, where the drilling locations vary, feature extractors, that are normalized by the average signal energy, provide better results in cutting tool condition estimation problem. In addition, we show several challenges that arise during the design phase and need to be addressed in order to build successful solutions.",https://ieeexplore.ieee.org/document/9488429/,2021 IEEE International Workshop on Metrology for Industry 4.0 & IoT (MetroInd4.0&IoT),7-9 June 2021,ieeexplore
10.1109/ICAIIC51459.2021.9415257,A Deep Learning Module Design for Workspace Identification in Manufacturing Industry,IEEE,Conferences,"In this paper, in order to solve various problems occurring in the workspace, a deep learning-based workspace identification module was designed, and the performance was analyzed through an experiment on the recognition accuracy according to the configuration of the training dataset and the number of training. The data model of the designed deep learning module is ResNetl8, and after setting up three dataset strategies, a dataset using five types of workspaces of the manufacturing industry was selected. In terms of the average top 5 and all training, strategy 2 was 81.2% and 76.4%, respectively, confirming that it was the best among the 3 strategies. In the future, after upgrading the designed module, it is planned to implement a module with real-time workspace identification performance level of practical use in a mobile environment with an image input device installed.",https://ieeexplore.ieee.org/document/9415257/,2021 International Conference on Artificial Intelligence in Information and Communication (ICAIIC),13-16 April 2021,ieeexplore
10.1109/IS.2018.8710526,A Digital Twin-based Privacy Enhancement Mechanism for the Automotive Industry,IEEE,Conferences,"This paper discusses a Digital Twin demonstrator for privacy enhancement in the automotive industry. Here, the Digital Twin demonstrator is presented as a method for the design and implementation of privacy enhancement mechanisms, and is used to detect privacy concerns and minimize breaches and associated risks to which smart car drivers can be exposed through connected infotainment applications and services. The Digital Twin-based privacy enhancement demonstrator is designed to simulate variety of conditions that can occur in the smart car ecosystem. We firstly identify the core stakeholders (actors) in the smart car ecosystem, their roles and exposure to privacy vulnerabilities and associated risks. Secondly, we identify assets that consume and generate sensitive privacy data in smart cars, their functionalities, and relevant privacy concerns and risks. Thirdly, we design an infrastructure for collecting (i) real-time sensor data from smart cars and their assets, and (ii) environmental data, road and traffic data, generated through operational driving lifecycle. In order to ensure compliance of the collected data with privacy policies and regulations, e.g. with GDPR requirements for enforcement of the data subject's rights, we design methods for the Digital Twin-based privacy enhancement demonstrator that are based on behavioural analytics informed by GDPR. We also perform data anonymization to minimize privacy risks and enable actions such as sending an automatic informed consent to the stakeholders.",https://ieeexplore.ieee.org/document/8710526/,2018 International Conference on Intelligent Systems (IS),25-27 Sept. 2018,ieeexplore
10.1109/BigData47090.2019.9005598,A Dynamic Neural Network Model for Click-Through Rate Prediction in Real-Time Bidding,IEEE,Conferences,"Real-time bidding (RTB) that features perimpression-level real-time ad auctions has become a popular practice in today's digital advertising industry. In RTB, click-through rate (CTR) prediction is a fundamental problem to ensure the success of an ad campaign and boost revenue. In this paper, we present a dynamic CTR prediction model designed for the Samsung demand-side platform (DSP). From our production data, we identify two key technical challenges that have not been fully addressed by the existing solutions: the dynamic nature of RTB and user information scarcity. To address both challenges, we develop a Dynamic Neural Network model. Our model effectively captures the dynamic evolutions of both users and ads and integrates auxiliary data sources (e.g., installed apps) to better model users' preferences. We put forward a novel interaction layer that fuses both explicit user responses (e.g., clicks on ads) and auxiliary data sources to generate consolidated user preference representations. We evaluate our model using a large amount of data collected from the Samsung advertising platform and compare our method against several state-of-the-art methods that are likely suitable for real-world deployment. The evaluation results demonstrate the effectiveness of our method and the potential for production. In addition, we discuss how to address a few practical engineering challenges caused by big data toward making our model in readiness for deployment.",https://ieeexplore.ieee.org/document/9005598/,2019 IEEE International Conference on Big Data (Big Data),9-12 Dec. 2019,ieeexplore
10.1109/ICCCBDA51879.2021.9442522,A FPGA Deployment System Based on Convolutional Neural Network for Rolling Bearing Diagnosis,IEEE,Conferences,"Real-time fault diagnosis of rolling bearing is a challenging issue for industry. Although artificial intelligence-based technologies could be well used for fault diagnosis of rolling bearing, the factories may not take into account the deployment of diagnosis algorithms. To tackle the issue, this work proposes a flexible deployment system of diagnosis algorithm for rolling bearing, where the required Convolutional Neural Network (CNN) model is deployed on the Field Programmable Gate Array (FPGA) to identify the working conditions of rolling bearing using vibration signals. The experimental results show that the deployed system performs accurately and efficiently on the test set, while a real-time prediction of FPGA could be guaranteed, indicating its potential as a powerful auxiliary system of rotating machinery.",https://ieeexplore.ieee.org/document/9442522/,2021 IEEE 6th International Conference on Cloud Computing and Big Data Analytics (ICCCBDA),24-26 April 2021,ieeexplore
10.1109/CRV.2015.42,A Hidden Markov Model for Vehicle Detection and Counting,IEEE,Conferences,"To reduce roadway congestion and improve traffic safety, accurate traffic metrics, such as number of vehicles travelling through lane-ways, are required. Unfortunately most existing infrastructure, such as loop-detectors and many video detectors, do not feasibly provide accurate vehicle counts. Consequently, a novel method is proposed which models vehicle motion using hidden Markov models (HMM). The proposed method represents a specified small region of the roadway as 'empty', 'vehicle entering', 'vehicle inside', and 'vehicle exiting', and then applies a modified Viterbi algorithm to the HMM sequential estimation framework to initialize and track vehicles. Vehicle observations are obtained using an Adaboost trained Haar-like feature detector. When tested on 88 hours of video, from three distinct locations, the proposed method proved to be robust to changes in lighting conditions, moving shadows, and camera motion, and consistently out-performed Multiple Target Tracking (MTT) and Virtual Detection Line(VDL) implementations. The median vehicle count error of the proposed method is lower than MTT and VDL by 28%, and 70% respectively. As future work, this algorithm will be implemented to provide the traffic industry with improved automated vehicle counting, with the intent to eventually provide real-time counts.",https://ieeexplore.ieee.org/document/7158929/,2015 12th Conference on Computer and Robot Vision,3-5 June 2015,ieeexplore
10.1109/HPCC.and.EUC.2013.124,A Hypervisor for MIPS-Based Architecture Processors - A Case Study in Loongson Processors,IEEE,Conferences,"Loongson is a family of general purpose processors based on MIPS architecture designed and manufactured in Mainland China. With the maturity of Loongson CPUs, applications are widely available with the increasing development of software tools and hardware platforms by research teams in academia and industry. In recent years, products based on Loongson have been mainly used in education, personal computers and server systems. Meanwhile, it is not yet popularly used in industrial real-time control fields, so such products have large room and potential to further development and deployment. The M-Hyper visor discussed in this paper is a real-time hyper visor designed for MIPS architecture and implemented in Loongson2F processor. It is based on the management program of para-virtualization whilst multiple partitions are scheduled to execute according to their priorities. The design and implementation of M-Hyper visor is discussed, along with details as timer, interrupts, memory management, partition loading and scheduling, to enrich real-time virtualized applications for MIPS architecture. Evaluation results show the performance and viability of proposed design, being promising to new deployments.",https://ieeexplore.ieee.org/document/6832006/,2013 IEEE 10th International Conference on High Performance Computing and Communications & 2013 IEEE International Conference on Embedded and Ubiquitous Computing,13-15 Nov. 2013,ieeexplore
10.1109/ETTandGRS.2008.78,A Kind of New Intelligent Monitor and Control System Based on Sensors Technology,IEEE,Conferences,"In heat treatment and other industry detection region, we usually adopt artificial or conventional instrument to control heat treatment furnacepsilas temperature. But due to fluctuation amplitude of furnace temperature is larger, which usually lead to poor production quality appearance. Aiming to this problem, we design an intelligent heat treatment furnacepsilas temperature intelligent monitor and control system based on single chip microprocessor (SCM) AT89C52. Through cold segment concentration technology, and automatic control technology, etc, system can efficiently realize heat treatment furnacepsilas temperature distributed monitor, control and centralized management. Experiment shows that system can accurately control heat treatment furnacepsilas temperature in real time, and temperature monitor error reaches monitor requirement, as well as realizes sound-light alarming once furnacepsilas fact temperature is out of setting limit value.",https://ieeexplore.ieee.org/document/5070180/,2008 International Workshop on Education Technology and Training & 2008 International Workshop on Geoscience and Remote Sensing,21-22 Dec. 2008,ieeexplore
10.23919/DATE48585.2020.9116539,A Machine Learning Based Write Policy for SSD Cache in Cloud Block Storage,IEEE,Conferences,"Nowadays, SSD cache plays an important role in cloud storage systems. The associated write policy, which enforces an admission control policy regarding filling data into the cache, has a significant impact on the performance of the cache system and the amount of write traffic to SSD caches. Based on our analysis on a typical cloud block storage system, approximately 47.09% writes are write-only, i.e., writes to the blocks which have not been read during a certain time window. Naively writing the write-only data to the SSD cache unnecessarily introduces a large number of harmful writes to the SSD cache without any contribution to cache performance. On the other hand, it is a challenging task to identify and filter out those write-only data in a real-time manner, especially in a cloud environment running changing and diverse workloads.In this paper, to alleviate the above cache problem, we propose an ML-WP, Machine Learning Based Write Policy, which reduces write traffic to SSDs by avoiding writing write-only data. The main challenge in this approach is to identify write-only data in a real-time manner. To realize ML-WP and achieve accurate write-only data identification, we use machine learning methods to classify data into two groups (i.e., write-only and normal data). Based on this classification, the write-only data is directly written to backend storage without being cached. Experimental results show that, compared with the industry widely deployed write-back policy, ML-WP decreases write traffic to SSD cache by 41.52%, while improving the hit ratio by 2.61% and reducing the average read latency by 37.52%.",https://ieeexplore.ieee.org/document/9116539/,"2020 Design, Automation & Test in Europe Conference & Exhibition (DATE)",9-13 March 2020,ieeexplore
10.1109/ISCAS.2018.8351785,A Machine Learning-Based Strategy for Efficient Resource Management of Video Encoding on Heterogeneous MPSoCs,IEEE,Conferences,"The design of new streaming systems is becoming a major area of research to deploy services targeted in the Internet-of-Things (IoT) era. In this context, the new High Efficiency Video Coding (HEVC) standard provides high efficiency and scalability of quality at the cost of increased computational complexity for edge nodes, which is a new challenge for the design of IoT systems. The usage of hardware acceleration in conjunction with general-purpose cores in Multiprocessor Systems-on-Chip (MP-SoCs) is a promising solution to create heterogeneous computing systems to manage the complexity of real-time streaming for high-end IoT systems, achieving higher throughput and power efficiency when compared to conventional processors alone. Furthermore, Machine Learning (ML) provides a promising solution to efficiently use this next-generation of heterogeneous MPSoC designs that the EDA industry is developing by dynamically optimizing system performance under diverse requirements such as frame resolution, search area, operating frequency and stream allocation. In this work, we propose an ML-based approach for stream allocation and Dynamic Voltage and Frequency Scaling (DVFS) management on a heterogeneous MPSoC composed of ARM cores and FPGA fabric containing hardware accelerators for the motion estimation of HEVC encoding. Our experiments on a Zynq7000 SoC outline 20% higher throughput when compared to the state-of-the-art streaming systems for next-generation IoT devices.",https://ieeexplore.ieee.org/document/8351785/,2018 IEEE International Symposium on Circuits and Systems (ISCAS),27-30 May 2018,ieeexplore
10.1109/ICCEAI52939.2021.00004,A Method for Designing and Analyzing Automotive Software Architecture: A Case Study for an Autonomous Electric Vehicle,IEEE,Conferences,"Software complexity is increased in automotive systems because many software functions are required for autonomous driving, electrified vehicles, and connected cars. In addition, autonomous driving requires centralized software that generally decreases evolvability with many connections. Thus, the automotive industry adopted the microservice architecture within the service-oriented architecture (SOA), which was already being used in distributed computing environments in the information and communication technology (ICT) industry. However, the software characteristics of an automotive system are different from those of an ICT system. Automotive software generally fulfills safety and real-time requirements that are not required in ICT software. Another challenge is integrating electric control units (ECUs) because software platforms supporting SOA require relatively high computational power and network bandwidth, which increases ECU cost. Thus, the deployment of software functions must be considered before integrating ECUs to find an optimal design solution for evolvability, dependability, real-time performance, cost, etc. However, many OEMs integrate ECUs based on deploying vehicular features without software architecture. It causes optimality problems during integrating ECUs. We propose component-based sensor-process-actuator architectural style for high-level architecture to handle quality attributes. Software architecture for an autonomous electrified vehicle will be presented with the proposed architectural style. The architecture is used to deploy software components and integrated ECUs with empirical quantitative analysis. Four design patterns for dependability with the architectural style will also be introduced.",https://ieeexplore.ieee.org/document/9544320/,2021 International Conference on Computer Engineering and Artificial Intelligence (ICCEAI),27-29 Aug. 2021,ieeexplore
10.1109/EDUCON46332.2021.9454147,"A Mixed Reality Approach Enriching the Agricultural Engineering Education Paradigm, against the COVID-19 Constraints",IEEE,Conferences,"Since the very early beginning of the mankind history, any great difficulty, like wars or diseases, had to be a challenge for progress and innovation, otherwise the game was lost. In this regard, the recent COVID-19 pandemic provides to the learners' and teachers' community a great opportunity to better adapt and enrich their educational practices. Initially, aiming to assist students of agricultural engineering to demystify the innovative technologies of their scientific area, a remotely programmed and controlled robotic arm platform for fruit-picking purposes is deployed. This is just the excuse behind which a colorful bouquet of modern and software and hardware components are glued together to provide the potential for supporting a wide range of modern engineering applications. In an era that the speed of the technological achievements makes difficult to categorize their impact in industry, society or education, the proposed approach can be classified as containing mainly mixed reality, mobile, blended and project-based learning characteristics. A first set of results indicate that the discussed platform can greatly assist the students to tackle the lack of physical presence in the laboratory/classroom providing a quite interesting alternative to full in-vitro educational practices.",https://ieeexplore.ieee.org/document/9454147/,2021 IEEE Global Engineering Education Conference (EDUCON),21-23 April 2021,ieeexplore
10.1109/ISIC.2008.4635950,A Multi-agent System for Integrated Control and Asset Management of Petroleum Production Facilities - Part 1: Prototype Design and Development,IEEE,Conferences,"This three-part paper thoroughly addresses the design and development of multi-agent system for asset management for the petroleum industry, which is crucial for profitable oil and gas facilities operations and maintenance. A research project was initiated to study the feasibility of an intelligent asset management system. Having proposed a conceptual model, architecture, and implementation plan for such a system in previous work [1], [2], [3], defined its autonomy, communications, and artificial intelligence (AI) requirements [4], [5], and initiated the preliminary design of a simple system prototype [6], we are extending the build of a system prototype and simulate it in real-time to validate its logical behavior in normal and abnormal process situations and analyze its performance.",https://ieeexplore.ieee.org/document/4635950/,2008 IEEE International Symposium on Intelligent Control,3-5 Sept. 2008,ieeexplore
10.1109/ISIC.2008.4635951,A Multi-agent System for Integrated Control and Asset Management of Petroleum Production Facilities - Part 2: Prototype Design Verification,IEEE,Conferences,"This three-part paper thoroughly addresses the design and development of multi-agent system for asset management for the petroleum industry, which is crucial for profitable oil and gas facilities operations and maintenance. A research project was initiated to study the feasibility of an intelligent asset management system. Having proposed a conceptual model, architecture, and implementation plan for such a system defined its autonomy, communications, and artificial intelligence (AI) requirements, and initiated the preliminary design of a simple system prototype, we are extending the build of a system prototype and simulate it in real-time to validate its logical behavior in normal and abnormal process situations and analyze its performance. The second-part paper addresses the ICAM system prototype design verification and its logical behavior during sensor faults in the plant.",https://ieeexplore.ieee.org/document/4635951/,2008 IEEE International Symposium on Intelligent Control,3-5 Sept. 2008,ieeexplore
10.1109/ISIC.2008.4635952,A Multi-agent System for Integrated Control and Asset Management of Petroleum Production Facilities - Part 3: Performance Analysis and System Limitations,IEEE,Conferences,"This three-part paper thoroughly addresses the design and development of multi-agent system for asset management for the petroleum industry, which is crucial for profitable oil and gas facilities operations and maintenance. A research project was initiated to study the feasibility of an intelligent asset management system. Having proposed a conceptual model, architecture, and implementation plan for such a system in previous work (J.H. Taylor and A.F. Sayda, 2005), (A.F. Sayda and J.H. Taylor, 2006), defined its autonomy, communications, and artificial intelligence (AI) requirements and initiated the preliminary design of a simple system prototype (J.H. Taylor and A.F. Sayda, 2008), we are extending the build of a system prototype and simulate it in real-time to validate its logical behavior in normal and abnormal process situations and analyze its performance. The third-part paper addresses the ICAM system prototype validation in terms of system performance analysis and system behavior during unexpected situations.",https://ieeexplore.ieee.org/document/4635952/,2008 IEEE International Symposium on Intelligent Control,3-5 Sept. 2008,ieeexplore
10.1109/ISMCR47492.2019.8955729,A Novel Architecture for Condition Based Machinery Health Monitoring on Marine Vessels Using Deep Learning and Edge Computing,IEEE,Conferences,"Condition based machinery health monitoring on marine vessels involves collecting operational sensor data on the vessel using a robust data acquisition system and determining asset health using anomaly detection analytics. Automation and digitalization of marine vessels involve smart digital technologies such as the Internet of Things (IoT) to collect ships' health data and send it over to a central processing location where this data is analyzed. However, it is difficult to apply this to the shipping industry due to offshore data transmission bandwidth challenges. Deep Learning, a technology that can be used to conduct Machinery Health Monitoring (MHM) holds the key to solve the bandwidth problems. In this paper, we investigate the use of Convolutional Neural Networks (CNN) as a practical solution for deploying Smart Health Monitoring on Marine Vessels using the example of electric induction motors. We show a mechanism to develop data-driven deep learning model that can classify if the motor is in a healthy or faulty condition, and propose an architecture to deploy this model on the Marine vessel in real time on an edge computing hardware. While in operation, sensor data from the motor will be fed into the DL Model, and the resulting predictions will be presented in the Vessel Alarm Monitoring System.",https://ieeexplore.ieee.org/document/8955729/,2019 IEEE International Symposium on Measurement and Control in Robotics (ISMCR),19-21 Sept. 2019,ieeexplore
10.1109/DDCLS49620.2020.9275082,A Novel Incremental Gaussian Mixture Regression and Its Application for Time-varying Multimodal Process Quality Prediction,IEEE,Conferences,"Data-driven soft sensor approach has been widely applied on real-time prediction and control of difficult-to-measure quality variables. Among these approaches, the Gaussian mixture regression (GMR) carries the potential of dealing with nonlinear and non-Gaussian industry problems, which has drawn increasing popularity and attentions in recent years. However, the fluctuation of raw materials, change of process environment, aging of instruments and other factors will have an effect on system performances over time. Hence, the lack of adaptive mechanism will make the GMR difficult to suit for time-varying processes and may cause large prediction errors. In order to model time-varying industrial processes and improve the adaptability of the conventional GMR, an adaptive soft sensor based on incremental Gaussian mixture regression (IGMR) is proposed in this paper. The incremental idea is integrated and an adaptive mechanism is added, which endow the proposed IGMR with the capability of adapting to new data in online environment. Compared to the moving window GMR (MWGMR) and the just-in-time learning GMR (JITLGMR), the feasibility and effectiveness of the proposed IGMR are verified both in a numerical simulation and a real-life industrial process experiment.",https://ieeexplore.ieee.org/document/9275082/,2020 IEEE 9th Data Driven Control and Learning Systems Conference (DDCLS),20-22 Nov. 2020,ieeexplore
10.1109/PHM-Paris.2019.00064,A Novel Testability Optimization Algorithm Counting the Reliability of Test Points,IEEE,Conferences,"The traditional testability mathematical model is attributed with inaccurate when applied in real industry occasions for it ignores the reliability of the test points (usually considered fully convinced). In this paper, we devise a novel testability optimization algorithm regarding with the reliability of test points. First, the D-matrix of uncertainty is acquired based on the Bayes-learning. Then, quantizing the loss function with the information entropy and utilizing the global searching ability of Genetic-PSO algorithm and the efficiency of the Greedy algorithm to form the test group. The proposed algorithm is validated with test data of avionics. The experiment result shows the method is able to select the optimal test group considering the uncertainty.",https://ieeexplore.ieee.org/document/8756396/,2019 Prognostics and System Health Management Conference (PHM-Paris),2-5 May 2019,ieeexplore
10.1109/ICIEM51511.2021.9445281,A Novel approach of GUI Mapping with image based widget detection and classification,IEEE,Conferences,"Software testing is vital for the intellectual benefits of software reliability and quality. At present, Graphical user interfaces are the most common and widely used interfaces in the software industry. Furthermore, GUI Testing is an important approach to ensure the quality of software. Automated software testing is a GUI front end applications similar to APP, and WEB, etc and is a vastly time and resource-consuming task. Therefore, this will become even more complex in rapidly updated GUI applications such as Ex: Patches/Version updates of a mobile App, or the product and offer updates in marketing websites like Flipkart, Amazon, etc., in which the GUI components are continuously updated infinitely. Developing a test case scenario whenever a new GUI component is updated will affect the productivity of the application. In our experiment, we found a better way of improving GUI testing by consequently detecting and classifying GUI widgets using machine learning techniques. Additionally, we also found that detecting and classifying GUI objects in screenshots and reports with a position of the widgets (x, y coordinates) and type of the widgets, matches with trained samples, URL links, and screen links. Hence, we in this paper will analyze and devise an efficient automated testing strategy for Web Applications. This is a unique way of web Graphical user interface testing with a computer vision. This paper will also present the parameters used for object detection, classification, and evaluation with image processing using machine learning algorithms with better accuracy.",https://ieeexplore.ieee.org/document/9445281/,2021 2nd International Conference on Intelligent Engineering and Management (ICIEM),28-30 April 2021,ieeexplore
10.1109/APARM49247.2020.9209530,A PdM Framework Through the Event-based Genomics of Machine Breakdown,IEEE,Conferences,"A novel event-based predictive maintenance framework based on sensor signal measurements and regressive predictions to minimise machine breakdown and component failure is proposed. Such capabilities will be complemented by Event-Clustering technique to cluster and remove less impact sensor signals and also build breakdown genomics from the root of a failure in order to predict the upcoming machine breakdowns and components failures. The creation of machine breakdown genomics requires the knowledge of systems state observed as well as the state change at specified time intervals (discretization). The proposed framework is applied to a real application case study. An industrial case study of a continuous compression moulding machine that manufactures the plastic bottle closure (caps) in the beverage industry has been considered as an experiment. The machine breakdown genomics theory is tested in this case to build the sequence of events or the genomics of breakdown, where sequences of contiguous events lead to failure or healthy machine status. This is complemented by the Regression Event-Tracker method to estimates the condition monitoring of the components and provide components real-time remaining useful life estimation. The Weibull failure-rate analysis is carried out on the remaining useful life estimates for each element to understand and estimate the mean time to failure for the manufacturing machine.",https://ieeexplore.ieee.org/document/9209530/,2020 Asia-Pacific International Symposium on Advanced Reliability and Maintenance Modeling (APARM),20-23 Aug. 2020,ieeexplore
10.1109/ITC-Egypt52936.2021.9513888,A Proposed end to end Telemedicine System based on embedded system and mobile application using CMOS wearable sensors,IEEE,Conferences,"Internet of things (IoT) and Embedded systems have extensive applications in healthcare markets. Integration of IoT with healthcare started with wearable smartwatches monitoring some signals and storing this data in the cloud. With 4G/5G and WiFi 6 networks. Healthcare data can be analyzed with Artificial Intelligence providing new era Internet of Medical Things (IoMT) that encompass an array of internet-capable medical devices that are in constant communication with each other or with the cloud; Internet of Healthcare Things (IoHT) that is the digital transformation of the healthcare industry. This article presents an end-to-end architecture with realization of three modules for key IoT aspects for healthcare and telemedicine. Results from a real implementation of application Platform for Data Processing including patient and doctor data base-based web site, MySQL data base, Android based mobile App, and PHP webserver.",https://ieeexplore.ieee.org/document/9513888/,2021 International Telecommunications Conference (ITC-Egypt),13-15 July 2021,ieeexplore
10.1109/CSCI49370.2019.00084,A Real-Time Based Intelligent System for Predicting Equipment Status,IEEE,Conferences,"In manufacturing industry, significant productivity losses arise due to equipment failures. Therefore, it is an important task to prevent the equipment from failure by monitoring each machine's sensor data in advance. However, most of the current developed systems have been only focused on monitoring the sensor data and have a difficulty in applying advanced algorithms to the real-time stream data. To address issues, we implemented an intelligent system that employs real-time streaming engine loaded with the machine learning libraries for predictive maintenance analysis. By applying a deep-learning based model to the real-time streaming data, we can provide not only trends of raw sensor data but also give an indicator representing an equipment's status in real-time. We anticipate that our system contributes to recognize the equipment's status by monitoring the indicator for productivity improvement in manufacturing industry in real-time.",https://ieeexplore.ieee.org/document/9071016/,2019 International Conference on Computational Science and Computational Intelligence (CSCI),5-7 Dec. 2019,ieeexplore
10.1109/WICOM.2010.5600178,A Real-Time Categorization and Clustering Method for Text Data of Laws and Regulations,IEEE,Conferences,"Taking the features of data in low and high frequency texts and the frequencies which such features emerge in a single text into consideration, the paper sets up a vector space model for part of texts of field. Then the paper also establishes a classifying and clustering method with features of classification and clustering by designing and constructing the two-dimensional analytic indexes of similarities and differences between field texts. This method is designed for field texts because it is quite suitable for effective machine learning and can extend the text data and textual categories dynamically in real-time. Meantime, it solves the single-label classification and multi-label classification issues at one time, overcoming the defects of previous text classifying methods which can only expand data instead of capacity. The general text clustering methods have many defects: they are not suitable for high-dimensional data sets or large data sets; they don't have the text data and category expanding function and they can not handle the outlier data problem well. On the contrary, this new method can offer solutions to these defects. According to this method, the corresponding algorithm has been established and the effectiveness of the method has been proven by the experiment on the data sets of laws and regulations of construction industry in Shaanxi Province in China.",https://ieeexplore.ieee.org/document/5600178/,2010 6th International Conference on Wireless Communications Networking and Mobile Computing (WiCOM),23-25 Sept. 2010,ieeexplore
10.1109/DAS.2012.10,A Signature Verification Framework for Digital Pen Applications,IEEE,Conferences,"In this paper we present a framework for real-time online signature verification scenarios. The proposed framework is based on state-of-the-art feature extraction and Gaussian Mixture Model (GMM) classification. While our signature verification library is generally applicable to any input device using digital pens, we have implemented verification scenarios using the Anoto digital pen. As such our automated signature verification framework becomes an interesting commodity for industry, because the Anoto SDK is easy to apply and the GMM-based classification can be seamlessly integrated. The novelty of this work is the application of our framework that takes real-time online signature verification to every scenario where digital pens may potentially be used. In this paper we describe several scenarios where our framework has been applied, including signatures in financial contracts or ordering processes. We also propose a general approach to integrate the GMM-descriptions into electronic ID-cards in order to also store behavioral biometrics on these cards. In experiments we have measured the performance of the signature verification system when skilled forgeries were present. The interest shown by our partner financial institutions and the results of our initial evaluations indicate that our signature verification framework suits exactly the demands of our clients.",https://ieeexplore.ieee.org/document/6195406/,2012 10th IAPR International Workshop on Document Analysis Systems,27-29 March 2012,ieeexplore
10.1109/PCIC31437.2018.9080444,"A Smart Condition Monitoring System for HV Networks with Artificial Intelligence, Augmented Reality and Virtual Reality: Copyright Material IEEE, Paper No. PCIC-2018-37",IEEE,Conferences,"The authors present a conceptual design for a SMART asset monitoring solution for high voltage (HV) networks in the petrochemical industry. The paper discusses the potential for incorporating artificial intelligence (AI), augmented reality (AR) and virtual reality (VR) into an application of the Industrial Internet of Things (IIoT) for condition monitoring. The paper is a continuation of the work presented by the authors at the IEEE-PCIC 2017 conference in Calgary. The proposed asset management system analyses condition monitoring (CM) data and assesses the risk of failure data across complete HV networks. Knowledge of deteriorating asset condition provides the operator with an advanced, early warning of incipient mechanical and electrical faults. With knowledge of the severity and source of such faults, pinpointed preventative maintenance interventions can then made during planned maintenance outages. The complete HV network asset monitoring solution described includes permanent sensors and monitoring nodes deployed at strategic locations across the network. Processed data is passed via a local area network to local servers and then via secure data cloud transmission to a centralized monitoring server located at the CM headquarters. This central server operates a CM database that logs, displays, benchmarks and trends the condition data with comparison to a statistically-significant database of measurements. It is proposed in the IIoT solution proposed that this database will be downloadable to a smartphone/tablet for use by the field engineer. The monitoring technology will likely also incorporate a number of AI machine learning software modules for the de-noising of raw signals and the diagnosis of different types of defects within different types of HV plant items. The proposed SMART CM system includes an advanced graphical user interface (GUI) for viewing HV asset CM data along with operational and maintenance (O&amp;M) data. The GUI will also be able to display both condition criticality and operational criticality (on a color-coded range of 0-100%) for individual HV plant items on a digitized mimic of the HV network's single-line diagram (SLD). This could also be combined with geometric positioning data of assets across the facility (including HV cable routes and lengths) to provide a fully digitized SMART network diagram for use in the IIoT asset management solution. Asset management data, combined with the application of the developing techniques of AI, AR and VR, will greatly help the user to visualize the plant items in 3-D, their position within the network, their condition and operational criticality along with all related asset management information together on one dashboard screen, downloaded onto smartphone/tablet. The paper concludes with a case study showing the development of a specification for a SMART IIoT asset condition monitoring solution suitable for a large petrochemical refining facility.",https://ieeexplore.ieee.org/document/9080444/,2018 IEEE Petroleum and Chemical Industry Technical Conference (PCIC),24-26 Sept. 2018,ieeexplore
10.1109/ICMCCE.2018.00050,A Smart Manufacturing Compliance Architecture of Electronic Batch Recording System (eBRS) for Life Sciences Industry,IEEE,Conferences,"The paradigm shift brought about by smart manufacturing or Industrie 4.0 has posed threefold challenges to electronic batch recording system (eBRS) in Life Sciences Industry: 1) the structure of the data should be informative and standard for interoperate using information models, 2) administration of synchronization between physical world and cyber world for smart decision making and optimization using cyber physical system (CPS) and 3) so-called digital manufacturing operations management (digital MOM) characterized by decentralization, comprehensive collaboration and servitization shall be implemented. Under the new situations of smart manufacturing or Industrie 4.0, the requirements from information models, CPS and digital MOM will become the most principal criteria to be considered for future eBRS/MES and other operations management information system in shop floor. To fulfill these demands, an approach combining ISA95/88 hybrid model with activities ontology and variant domain-driven design for SOA-based eBRS development has been presented. An eBRS software platform has been developed on the theoretical basis and applied to a specific application scenario of Lyophilized Injection Production for verifying its feasibility purpose.",https://ieeexplore.ieee.org/document/8537548/,"2018 3rd International Conference on Mechanical, Control and Computer Engineering (ICMCCE)",14-16 Sept. 2018,ieeexplore
10.23919/CCC50068.2020.9188783,A Smartphone-Based Networked Control Platform: Design and Implementation,IEEE,Conferences,"The rapid development of embedded systems and IT makes electrical devices functional and tiny. This progress is especially reflected in the smartphone industry. In this paper, a novel applicable wireless control platform based on smartphones is proposed. The system is of high value due to the advantages of smartphones such as mobility, resourcefulness, tiny and etc. Mechanisms are designed in this paper to guarantee the real-timeness of the controller. To ease the controller implementation, code auto-generation technique is integrated to the platform, which is able to convert Simulink block diagrams to executable files for Android smartphones. In addition, protocols are designed to transfer data to remote workstations such that the control system can be supervised and monitored on-line. At last, the control platform is tested on a spacecraft simulator. The experimental results validate the effectiveness and reliability of the designed platform.",https://ieeexplore.ieee.org/document/9188783/,2020 39th Chinese Control Conference (CCC),27-29 July 2020,ieeexplore
10.1109/IOLTS52814.2021.9486704,A Suitability Analysis of Software Based Testing Strategies for the On-line Testing of Artificial Neural Networks Applications in Embedded Devices,IEEE,Conferences,"Electronic devices based on artificial intelligence solutions are pervading our everyday life. Nowadays, human decision processes are supported by real-time data gathered from intelligent systems. Artificial Neural Networks (ANNs) are one of the most used deep learning predictive models due to their outstanding computational capabilities. However, assessing their reliability is still an open issue faced by both the academic and industrial worlds, especially when ANNs are deployed on safety-critical systems, such as self-driving cars in the automotive world. In these systems, a strategy for identifying hardware faults is required by industry standards (e.g., ISO26262 for automotive, and DO254 for avionics). Among the existing in-field test strategies, the periodic scheduling of on-line Software Test Library (STL) is a wide strategy adopted; STL allows to reach an acceptable fault coverage without the need for additional hardware. However, when dealing with ANN-based applications, the execution of on-line tests interleaving the ANN inferences may jeopardise the strive for performance maximization. The paper presents a comprehensive analysis of six possible scenarios concerning the execution of on-line self-test programs in embedded devices running ANN-based applications. In the proposed scenarios, the impact of the STL execution on the ANN performance is analyzed; in particular, the execution times of an inference and the Fault Detection Time (FDT) of the STL are discussed and compared. Experimental analyses are provided by relying on: an open-source RISC-V platform running two different convolutional neural networks; a STL for RISC-V cores with a maximum achievable fault coverage of 90%.",https://ieeexplore.ieee.org/document/9486704/,2021 IEEE 27th International Symposium on On-Line Testing and Robust System Design (IOLTS),28-30 June 2021,ieeexplore
10.1109/ICEBE.2010.44,A Support Vector Machine Based Method for Credit Risk Assessment,IEEE,Conferences,"The credit card industry has been growing rapidly in recent years, and credit risk assessment becomes critically important for financial companies. In this paper, a novel support vector machine (SVM) based ensemble model is proposed for credit risk assessment. In the proposed method, principles component analysis (PCA) is firstly employed for credit feature selection. Secondly, SVMs with different kernels are trained by using genetic algorithm (GA) to optimize the parameters, and the corresponding assessment results are obtained. Thirdly, all results produced by different SVMs are combined by several ensemble strategies. Finally, an optimal ensemble strategy is selected for credit scoring. For validation, two real world credit datasets are used to test the effectiveness and efficiency of our proposed method. The experiment results find that our proposed ensemble model outperforms commonly used credit scoring tools. The findings of the study reveal the support vector machine based ensemble method to be a promising alternative for credit scoring.",https://ieeexplore.ieee.org/document/5704298/,2010 IEEE 7th International Conference on E-Business Engineering,10-12 Nov. 2010,ieeexplore
10.1109/ISSE46696.2019.8984442,A Systems Approach to Validating and Analyzing Improvements to Real-Time Image Classification Utilizing Machine Learning,IEEE,Conferences,"This paper delves into the generation and use of image classification models in a real-time environment utilizing machine learning. The ImageAI framework is used to generate a list of models from a set of training images and also for classifying new images using the generated models. Through this paper, previous research projects and industry programs are analyzed for design and operation. The basic implementation results in models that classify new images correctly the majority of the time with a high level of confidence. However, almost a quarter of the time the models classify images incorrectly. This paper attempts to improve the classification accuracy and improve the operational efficiency of the overall system as well.",https://ieeexplore.ieee.org/document/8984442/,2019 International Symposium on Systems Engineering (ISSE),1-3 Oct. 2019,ieeexplore
10.1109/TAAI.2013.73,A Tourist Navigation System in Which a Historical Character Guides to Related Spots by Hide-and-Seek,IEEE,Conferences,"Tourism is an important industry, and various initiatives for the discovery of sightseeing spots are being implemented. Most current sightseeing navigation systems, which are very efficient at enabling immediate acquisition of desired information, lack a sense of fun. Moreover, although some researches have been conducted on the creation of new encounters and discoveries in tourist areas, there is a problem in that the high degree of user freedom does not lead users to spots which tourist area authorities hope to promote. Thus, in this study we propose a system that stimulates rediscovery of sightseeing spots through hide-and-seek with CG characters using augmented reality technology. We intend to verify the anticipated effects of the proposed system in upcoming evaluation experiments.",https://ieeexplore.ieee.org/document/6783892/,2013 Conference on Technologies and Applications of Artificial Intelligence,6-8 Dec. 2013,ieeexplore
10.1109/ICIEVicIVPR52578.2021.9564229,A Vision-Based Lane Detection Approach for Autonomous Vehicles Using a Convolutional Neural Network Architecture,IEEE,Conferences,"Autonomous vehicles no longer belong to the realm of science fiction. They have become a prominent area of research in the last two decades because of the integration of Artificial Intelligence in the automobile industry. Apart from the development of various complex learning algorithms, the advancement of cameras, sensors, and geolocation technology as well as the escalation in the capacity of machines have played a crucial role in bringing this technology into reality. We have had significant breakthroughs in the development of autonomous cars within the last ten years. However, despite the success of multiple prototypes in navigating within the borders of a delimited area, researchers are yet to overcome several drawbacks before embodying them in the transport system; and one of those hurdles lies in the lane detection system of the cars. Therefore, in this article, we present an intelligent lane detection algorithm incorporating fully-connected Neural Networks with a secondary layer protection scheme to detect the borders of a lane. We achieved over 98% classification accuracy using the proposed lane detection model. We also implemented the model in a small prototype to take a look at its performance. Experimental results infer that the algorithm is capable of lane detection and ready for practical use.",https://ieeexplore.ieee.org/document/9564229/,"2021 Joint 10th International Conference on Informatics, Electronics & Vision (ICIEV) and 2021 5th International Conference on Imaging, Vision & Pattern Recognition (icIVPR)",16-20 Aug. 2021,ieeexplore
10.1109/PacificVis.2018.00026,A Visual Analytics Approach for Equipment Condition Monitoring in Smart Factories of Process Industry,IEEE,Conferences,"Monitoring equipment conditions is of great value in manufacturing, which can not only reduce unplanned downtime by early detecting anomalies of equipment but also avoid unnecessary routine maintenance. With the coming era of Industry 4.0 (or industrial internet), more and more assets and machines in plants are equipped with various sensors and information systems, which brings an unprecedented opportunity to capture large-scale and fine-grained data for effective on-line equipment condition monitoring. However, due to the lack of systematic methods, analysts still find it challenging to carry out efficient analyses and extract valuable information from the mass volume of data collected, especially for process industry (e.g., a petrochemical plant) with complex manufacturing procedures. In this paper, we report the design and implementation of an interactive visual analytics system, which helps managers and operators at manufacturing sites leverage their domain knowledge and apply substantial human judgements to guide the automated analytical approaches, thus generating understandable and trustable results for real-world applications. Our system integrates advanced analytical algorithms (e.g., Gaussian mixture model with a Bayesian framework) and intuitive visualization designs to provide a comprehensive and adaptive semi-supervised solution to equipment condition monitoring. The example use cases based on a real-world manufacturing dataset and interviews with domain experts demonstrate the effectiveness of our system.",https://ieeexplore.ieee.org/document/8365986/,2018 IEEE Pacific Visualization Symposium (PacificVis),10-13 April 2018,ieeexplore
10.1109/EDOC49727.2020.00017,A Zone Reference Model for Enterprise-Grade Data Lake Management,IEEE,Conferences,"Data lakes are on the rise as data platforms for any kind of analytics, from data exploration to machine learning. They achieve the required flexibility by storing heterogeneous data in their raw format, and by avoiding the need for pre-defined use cases. However, storing only raw data is inefficient, as for many applications, the same data processing has to be applied repeatedly. To foster the reuse of processing steps, literature proposes to store data in different degrees of processing in addition to their raw format. To this end, data lakes are typically structured in zones. There exists various zone models, but they are varied, vague, and no assessments are given. It is unclear which of these zone models is applicable in a practical data lake implementation in enterprises. In this work, we assess existing zone models using requirements derived from multiple representative data analytics use cases of a real-world industry case. We identify the shortcomings of existing work and develop a zone reference model for enterprise-grade data lake management in a detailed manner. We assess the reference model's applicability through a prototypical implementation for a real-world enterprise data lake use case. This assessment shows that the zone reference model meets the requirements relevant in practice and is ready for industry use.",https://ieeexplore.ieee.org/document/9233155/,2020 IEEE 24th International Enterprise Distributed Object Computing Conference (EDOC),5-8 Oct. 2020,ieeexplore
10.1109/I2MTC.2012.6229541,A hybrid multiple classifier system for recognizing usual and unusual drilling events,IEEE,Conferences,"Up to very recently, the applications of machine learning in the oil &amp; gas industry were limited to using a single machine learning technique to solve problems in-hand. As the complexity of the demanded tasks being increased, the single techniques proved insufficient. This gave rise to intelligent systems that are hybrids of several machine learning techniques to solve the most challenging problems. In this paper we propose a hybrid multiple classifier approach for recognizing usual and unusual drilling events. We suggest using two different information sources namely: (1) real time data collected by sensors located around the drilling rig, and (2) daily morning reports written by drilling personnel to describe the drilling process. Text mining techniques were used to analysis the daily morning reports and to extract textual features that include keywords and phrases, whereas data mining techniques were used to analysis the sensors data and extracting statistical features. Three base classifiers were trained and combined in one ensemble to obtain better predictive performance. Experimental evaluation with real data and reports shows that the ensemble outperforms the base classifiers in every experiment, and the average classification accuracy is about 90% for usual events, and about 75% for unusual events.",https://ieeexplore.ieee.org/document/6229541/,2012 IEEE International Instrumentation and Measurement Technology Conference Proceedings,13-16 May 2012,ieeexplore
10.1109/ICCCBDA.2018.8386514,A method for CIR fault diagnosis based on improved tri-training in big data environment,IEEE,Conferences,"In recent years, China's railway industry has seen a rapid development, people's travel and transportation are increasingly being carried out by railway. How to ensure the safe operation of railway is a very important subject. Cab Integrated Radio communication equipment (CIR) is an important equipment for communication between train and train or between train and ground, the failure of the CIR equipment directly affects the safety of railway operation. This paper focus on the large amount of data generated by CIR devices in real time. Using big data technology to collect, preprocess and store them. Then, a new Tri-training algorithm is reformed according to the actual requirement of CIR fault diagnosis; Finally, a method for CIR fault diagnosis based on improved tri-training algorithm is proposed. Experiment shows that the proposed method can effectively use unlabeled CIR data to improve the accuracy of the model and can ensure the real-time and accuracy of CIR fault diagnosis. It has a high application value.",https://ieeexplore.ieee.org/document/8386514/,2018 IEEE 3rd International Conference on Cloud Computing and Big Data Analysis (ICCCBDA),20-22 April 2018,ieeexplore
10.1109/SSST.1991.138548,A neural network based histogramic procedure for fast image segmentation,IEEE,Conferences,"The determination of the dimension of a lumber board, the location and extent of surface defects on it, are essential in the construction of a visual inspection station for the lumber industry. The paper presents a neural network based histogramic procedure that performs on the image of a board and can be used to determine the board dimension, the location and extent of surface defects on it, in near real time. The method is based on segmentation of the image based on multiple threshold information derived from a multi-layered neural network. Such a scheme can be applied in general to image analysis and the implementation shows fast processing requiring very little control over the environment. The construction of the network and its training are also discussed.&lt;<ETX>&gt;</ETX>",https://ieeexplore.ieee.org/document/138548/,[1991 Proceedings] The Twenty-Third Southeastern Symposium on System Theory,10-12 March 1991,ieeexplore
10.1109/IAS.2005.1518384,A neural network based optimal wide area control scheme for a power system,IEEE,Conferences,"With deregulation of the power industry, many tie lines between control areas are driven to operate near their maximum capacity, especially those serving heavy load centers. Wide area control systems (WACSs) using wide-area or global signals can provide remote auxiliary control signals to local controllers such as automatic voltage regulators, power system stabilizers, etc to damp out inter-area oscillations. This paper presents the design and the DSP implementation of a nonlinear optimal wide area controller based on adaptive critic designs and neural networks for a power system on the real-time digital simulator (RTDS/spl reg/). The performance of the WACS as a power system stability agent is studied using the Kundur's two area power system example. The WACS provides better damping of power system oscillations under small and large disturbances even with the inclusion of local power system stabilizers.",https://ieeexplore.ieee.org/document/1518384/,"Fourtieth IAS Annual Meeting. Conference Record of the 2005 Industry Applications Conference, 2005.",2-6 Oct. 2005,ieeexplore
10.1109/WINCOM50532.2020.9272477,A new middleware for managing heterogeneous robot in ubiquitous environments,IEEE,Conferences,"Heterogeneity is one of the main issues for the deployment of the Industry 4.0. This is due to the diversity in the available robots and the IIoT devices. These equipments use different programming languages and communication protocols. To make the integration of such equipments easy, we propose TalkRoBots, a middleware that allows heterogeneous robots and IIoT devices to communicate together and exchange data in a transparent way. The middleware was experimented in a real scenario with different robots that demonstrate its efficiency.",https://ieeexplore.ieee.org/document/9272477/,2020 8th International Conference on Wireless Networks and Mobile Communications (WINCOM),27-29 Oct. 2020,ieeexplore
10.1109/ICIT.2018.8352510,A real-time smart fruit quality grading system classifying by external appearance and internal flavor factors,IEEE,Conferences,"The fruit grading by visual inspection suffers from the problem of inconsistency in judgment by different persons. There is a need for an automatic fruit classification machine replacing the expensive human labor with a smart fruit quality classification system. This study proposed a practical real-time smart fruits quality grading system classifying by appearance and internal flavor factors in order to decrease human labor cost in fruit industry. The proposed system applies color image processing techniques for the computation of the fruits appearance features and the near-infrared spectroscopy analysis methods for the estimation of internal flavor factors. This study also suggests an artificial neural network model in order to be able to classify fruit grading. The proposed ANN model is trained and tested with 1,900 numbers of pears for grading. It has achieved the classification accuracy rate of 97.4% in our experiment.",https://ieeexplore.ieee.org/document/8352510/,2018 IEEE International Conference on Industrial Technology (ICIT),20-22 Feb. 2018,ieeexplore
10.1109/MFI-2003.2003.1232590,A robust real time position and force (hybrid) control of a robot manipulator in presence of uncertainties,IEEE,Conferences,"We examine the living intelligent biological systems and model the computational system components. We consider the situation of a kind of ""blind-tracking"" with constant force/torque by a human hand. The problem involves hand kinematics, hand motor control, and an adaptive judgment method from the position and force/torque reflection of the uncertain hyper plane. In this study, these control levels were designed using neural networks and fuzzy logic technologies. The control levels are coordinated amongst themselves forming the distributed artificial intelligent (DAI) system. The conclusive characteristic of the proposed controller was a one-step-ahead feedback control. This DAI-based control systems was implemented in the RX-90 industrial robot. Certainly these types of control system will help an industry to be autonomous and increase the productivity as well.",https://ieeexplore.ieee.org/document/1232590/,"Proceedings of IEEE International Conference on Multisensor Fusion and Integration for Intelligent Systems, MFI2003.",1-1 Aug. 2003,ieeexplore
10.1109/ICISC.2018.8398982,A rule-based classification of short message service type,IEEE,Conferences,"Short message service (SMS) is one of the most popular means of communication due to its type of usages like one-time passwords(OTPs), banking transaction alerts, and other promotional messages. SMS is the most time-sensitive channel of communication that demands service providers to send any information globally without any delay in respective time zone. For successful implementation of this service, the Telecom Regulatory of India (TRAI) has recommended certain guidelines that must be followed for the delivery of short messages. The prevalent practice in the industry is to store rules or raw string in a database and to match with incoming SMSs in synchronous mode. However, there are various shortcomings in this traditional approach that have been also discussed in this paper. Further, to address these issues, this research work proposes a novel approach for automated classification of SMSs in real time by the SMS service using a rule-based system of database template matching. Further to validate the proposed approach, SMS database of Netcore Solutions Pvt Limited, India, has been used for training the proposed algorithm. Proposed rule-based classifier keeps learning and updating the training dataset as more SMSs are accumulated and processed through the server. Other advantages, in terms of performance metrics, of the classification using the proposed rule-based database template matching over the traditional database matching approach have been reported in this work as evaluation measures. Reported rule-based SMS classification algorithm shows highest average classification accuracy of 100% which is better as compared to the similar research works already available in the literature.",https://ieeexplore.ieee.org/document/8398982/,2018 2nd International Conference on Inventive Systems and Control (ICISC),19-20 Jan. 2018,ieeexplore
10.1109/ISIE.1998.707755,A self-organization neuro-fuzzy network applied to a seismic signal classification problem,IEEE,Conferences,"The marriage between neural networks and fuzzy logic systems can lead to improved systems in which the best of both worlds are combined, viz. learning capability as well as the ability to handle real-life ambiguities and uncertainties gracefully. In this contribution a heterogeneous neuro-fuzzy network is proposed as a solution for a problem relevant in the mining industry: the classification of seismic signals according to their generating sources. Different stages in the traditional fuzzy system are implemented in consecutive layers in the network, resulting in an architecture reminiscent of radial basis function networks. In contrast to some other neuro-fuzzy networks in which a rule-base is derived by the rule-elimination-algorithm, fuzzy rules are generated with the aid of a fuzzy adaptive resonance theory network. Besides leading to reduced training times, the proposed rule generation algorithm can result in a better understanding of the signals being classified.",https://ieeexplore.ieee.org/document/707755/,IEEE International Symposium on Industrial Electronics. Proceedings. ISIE'98 (Cat. No.98TH8357),7-10 July 1998,ieeexplore
10.1109/CNSC.2014.6906645,A survey of water distribution system and new approach to intelligent water distribution system,IEEE,Conferences,"This paper reviews the challenges in water distribution systems (WDS), development and deployment of intelligent systems to manage WDS efficiently. Recognizing that intelligent systems has the potential to revolutionize management of precious natural resource by utilities, this paper provides a summary of the research work and practices for researchers and industry practitioners to ensure that the technology cultivates sustainable drinking water management. At present a lot technological developments ensures reduction of labor cost for various tasks in WDS. The technical sophistications of intelligent systems to ensure optimal use of precious resources have increased noticeably in recent decades. This paper addresses all concerned issues with WDS such as real time data collection, forecast of future water consumption, labor cost, recovery cost of water treatment and distribution, power supply requirement, operational time, water leakage, remote capturing of meter reading etc. We proposed a new intelligent water distribution system, which overcomes most of the problems in WDS.",https://ieeexplore.ieee.org/document/6906645/,2014 First International Conference on Networks & Soft Computing (ICNSC2014),19-20 Aug. 2014,ieeexplore
10.1109/COMITCon.2019.8862212,ACT Testbot and 4S Quality Metrics in XAAS Framework,IEEE,Conferences,"The purpose of this paper is to analyze all Cloud based Service Models, Continuous Integration, Deployment and Delivery process and propose an Automated Continuous Testing and testing as a service based TestBot and metrics dashboard which will be integrated with all existing automation, bug logging, build management, configuration and test management tools. Recently cloud is being used by organizations to save time, money and efforts required to setup and maintain infrastructure and platform. Continuous Integration and Delivery is in practice nowadays within Agile methodology to give capability of multiple software releases on daily basis and ensuring all the development, test and Production environments could be synched up quickly. In such an agile environment there is need to ramp up testing tools and processes so that overall regression testing including functional, performance and security testing could be done along with build deployments at real time. To support this phenomenon, we researched on Continuous Testing and worked with industry professionals who are involved in architecting, developing and testing the software products. A lot of research has been done towards automating software testing so that testing of software product could be done quickly and overall testing process could be optimized. As part of this paper we have proposed ACT TestBot tool, metrics dashboard and coined 4S quality metrics term to quantify quality of the software product. ACT testbot and metrics dashboard will be integrated with Continuous Integration tools, Bug reporting tools, test management tools and Data Analytics tools to trigger automation scripts, continuously analyze application logs, open defects automatically and generate metrics reports. Defect pattern report will be created to support root cause analysis and to take preventive action.",https://ieeexplore.ieee.org/document/8862212/,"2019 International Conference on Machine Learning, Big Data, Cloud and Parallel Computing (COMITCon)",14-16 Feb. 2019,ieeexplore
10.1109/CloudCom.2019.00037,APEX: Adaptive Ext4 File System for Enhanced Data Recoverability in Edge Devices,IEEE,Conferences,"Recently Edge Computing paradigm has gained significant popularity both in industry and academia. With its increased usage in real-life scenarios, security, privacy and integrity of data in such environments have become critical. Malicious deletion of mission-critical data due to ransomware, trojans and viruses has been a huge menace and recovering such lost data is an active field of research. As most of Edge computing devices have compute and storage limitations, difficult constraints arise in providing an optimal scheme for data protection. These devices mostly use Linux/Unix based operating systems. Hence, this work focuses on extending the Ext4 file system to APEX (Adaptive Ext4): a file system based on novel on-the-fly learning model that provides an Adaptive Recover-ability Aware file allocation platform for efficient post-deletion data recovery and therefore maintaining data integrity. Our recovery model and its lightweight implementation allow significant improvement in recover-ability of lost data with lower compute, space, time, and cost overheads compared to other methods. We demonstrate the effectiveness of APEX through a case study of overwriting surveillance videos by CryPy malware on Raspberry-Pi based Edge deployment and show 678% and 32% higher recovery than Ext4 and current state-of-the-art File Systems. We also evaluate the overhead characteristics and experimentally show that they are lower than other related works.",https://ieeexplore.ieee.org/document/8968863/,2019 IEEE International Conference on Cloud Computing Technology and Science (CloudCom),11-13 Dec. 2019,ieeexplore
10.1109/AIVR46125.2019.00036,"AR Tracking with Hybrid, Agnostic And Browser Based Approach",IEEE,Conferences,"Mobile platform tools are desirable when it comes to practical augmented reality applications. With the convenience and portability that the form factor has to offer, it lays an ideal basic foundation for a feasible use case in industry and commercial applications. Here, we present a novel approach of using the monocular Simultaneous Localization and Mapping (SLAM) information provided by a Cross-Reality (XR) device to augment the linked 3D CAD models. The main objective is to use the tracking technology for an augmented and mixed reality experience by tracking a 3D model and superimposing its respective 3D CAD model data over the images we receive from the camera feed of the XR device without any scene preparation (e.g markers or feature maps). The intent is to conduct a visual analysis and evaluations based on the intrinsic and extrinsic of the model in the visualization system that instant3Dhub has to offer. To achieve this we make use of the Apple's ARKit to obtain the images, sensor data and SLAM heuristic of client XR device, remote marker-less model based 3D object tracking from monocular RGB image data and hybrid client server architecture. Our approach is agnostic of any SLAM system or Augmented Reality (AR) framework. We make use of the Apple's ARKit because of the its ease of use, affordability, stability and maturity as a platform and as an integrated system.",https://ieeexplore.ieee.org/document/8942252/,2019 IEEE International Conference on Artificial Intelligence and Virtual Reality (AIVR),9-11 Dec. 2019,ieeexplore
10.1109/ISCAS45731.2020.9180670,AVAC: A Machine Learning Based Adaptive RRAM Variability-Aware Controller for Edge Devices,IEEE,Conferences,"Recently, the Edge Computing paradigm has gained significant popularity both in industry and academia. Researchers now increasingly target to improve performance and reduce energy consumption of such devices. Some recent efforts focus on using emerging RRAM technologies for improving energy efficiency, thanks to their no leakage property and high integration density. As the complexity and dynamism of applications supported by such devices escalate, it has become difficult to maintain ideal performance by static RRAM controllers. Machine Learning provides a promising solution for this, and hence, this work focuses on extending such controllers to allow dynamic parameter updates. In this work we propose an Adaptive RRAM Variability-Aware Controller, AVAC, which periodically updates Wait Buffer and batch sizes using on-the-fly learning models and gradient ascent. AVAC allows Edge devices to adapt to different applications and their stages, to improve computation performance and reduce energy consumption. Simulations demonstrate that the proposed model can provide up to 29% increase in performance and 19% decrease in energy, compared to static controllers, using traces of real-life healthcare applications on a Raspberry-Pi based Edge deployment.",https://ieeexplore.ieee.org/document/9180670/,2020 IEEE International Symposium on Circuits and Systems (ISCAS),12-14 Oct 2020,ieeexplore
10.1109/INDIN.2017.8104789,Addressing security challenges in industrial augmented reality systems,IEEE,Conferences,"In context of Industry 4.0 Augmented Reality (AR) is frequently mentioned as the upcoming interface technology for human-machine communication and collaboration. Many prototypes have already arisen in both the consumer market and in the industrial sector. According to numerous experts it will take only few years until AR will reach the maturity level to be deployed in productive applications. Especially for industrial usage it is required to assess security risks and challenges this new technology implicates. Thereby we focus on plant operators, Original Equipment Manufacturers (OEMs) and component vendors as stakeholders. Starting from several industrial AR use cases and the structure of contemporary AR applications, in this paper we identify security assets worthy of protection and derive the corresponding security goals. Afterwards we elaborate the threats industrial AR applications are exposed to and develop an edge computing architecture for future AR applications which encompasses various measures to reduce security risks for our stakeholders.",https://ieeexplore.ieee.org/document/8104789/,2017 IEEE 15th International Conference on Industrial Informatics (INDIN),24-26 July 2017,ieeexplore
10.1109/BigData.2014.7004408,Advanced planning and control of manufacturing processes in steel industry through big data analytics: Case study and architecture proposal,IEEE,Conferences,"Enterprises in today's globalized world are compelled to react on threats and opportunities in a highly flexible manner. Hence, companies that are able to analyze the current state of their business processes, forecast their most optimal progresses and with this proactively control them will have a decisive competitive advantage. Technological progress in sensor technology has boosted real-time situation awareness, especially in manufacturing operations. The paper at hands examines, based on a case study stemming from the steel manufacturing industry, which production-related data is collectable using state of the art sensors forming a basis for a detailed situation awareness and for deriving accurate forecasts. However, analyses of this data point out that dedicated big data analytics approaches are required to utilize the full potential out of it. By proposing an architecture for predictive process planning and control systems, the paper intends to form a working and discussion basis for further research and implementation efforts in big data analytics.",https://ieeexplore.ieee.org/document/7004408/,2014 IEEE International Conference on Big Data (Big Data),27-30 Oct. 2014,ieeexplore
10.1109/EISIC.2017.21,Adversarial Machine Learning in Malware Detection: Arms Race between Evasion Attack and Defense,IEEE,Conferences,"Since malware has caused serious damages and evolving threats to computer and Internet users, its detection is of great interest to both anti-malware industry and researchers. In recent years, machine learning-based systems have been successfully deployed in malware detection, in which different kinds of classifiers are built based on the training samples using different feature representations. Unfortunately, as classifiers become more widely deployed, the incentive for defeating them increases. In this paper, we explore the adversarial machine learning in malware detection. In particular, on the basis of a learning-based classifier with the input of Windows Application Programming Interface (API) calls extracted from the Portable Executable (PE) files, we present an effective evasion attack model (named EvnAttack) by considering different contributions of the features to the classification problem. To be resilient against the evasion attack, we further propose a secure-learning paradigm for malware detection (named SecDefender), which not only adopts classifier retraining technique but also introduces the security regularization term which considers the evasion cost of feature manipulations by attackers to enhance the system security. Comprehensive experimental results on the real sample collections from Comodo Cloud Security Center demonstrate the effectiveness of our proposed methods.",https://ieeexplore.ieee.org/document/8240775/,2017 European Intelligence and Security Informatics Conference (EISIC),11-13 Sept. 2017,ieeexplore
10.1109/EEEI.2014.7005849,Adware detection and privacy control in mobile devices,IEEE,Conferences,"In this paper we propose a system and algorithms for detection of Adware in mobile devices that are based on machine learning algorithms and are capable of adapting to the ongoing transformation of Adware and Malware. The system is based on static and dynamic analysis of mobile applications, extraction of useful features and real-time classification. This classification is based on supervised machine learning algorithms with emphasis on fast, linear operations and efficient implementation on mobile devices. The system presented in this paper enables identification of relevant features that are salient in Adware and Malware, useful for further analysis by security researchers. The proposed system exhibits a detection rate of 97%. The system has been tested and verified on known, industry standard, datasets and is superior to state of the art solutions available in the market. These result have been verified by 3<sup>rd</sup> party evaluators.",https://ieeexplore.ieee.org/document/7005849/,2014 IEEE 28th Convention of Electrical & Electronics Engineers in Israel (IEEEI),3-5 Dec. 2014,ieeexplore
10.1109/CEWIT.2013.6713745,Agent-based planning and control for groupage traffic,IEEE,Conferences,"In this research and technology transfer project, the planning and control processes of the industrial partner Hellmann Worldwide Logistics GmbH &amp; Co. KG are analyzed. An agent-based approach is presented to model current processes and to exploit the identified optimization potential. The developed system directly connects the information flow and the material flow as well as their interdependencies in order to optimize the planning and control in groupage traffic. The software system maps current processes to agents as system components and improves the efficiency by intelligent objects. To handle the high complexity and dynamics of logistics autonomous intelligent agents plan and control the way of represented objects through the logistic network by themselves and induce a flexible and reactive system behavior. We evaluate the implemented dispatching application by simulating the groupage traffic processes using effectively transported orders and process data provided by our industrial partner. Moreover, we modeled real world infrastructures and considered also the dynamics by the simulation of unexpected events and process disturbances. The results show that the system significantly decreases daily cost by reducing the required number of transport providers and shifting conventional orders to next days, which need no immediate delivery. Thus the system increases the efficiency and meets the special challenges and requirements of groupage traffic. Moreover, the system supports freight carriers and dispatchers with adequate tour and routing proposals. Computed tours were successfully validated by human dispatchers. Due to the promising results, Hellmann is highly interested in transferring the prototype to an application that optimizes the daily operations in numerous distribution centers. Finally, provide further research perspectives, and emphasize the advantages of the developed system in Industry 4.0 applications.",https://ieeexplore.ieee.org/document/6713745/,2013 10th International Conference and Expo on Emerging Technologies for a Smarter World (CEWIT),21-22 Oct. 2013,ieeexplore
10.1109/NOMS.1998.655214,Alarm correlation engine (ACE),IEEE,Conferences,"Networks are growing in size and complexity, resulting in increased alarm volume and number of unfamiliar alarms. Often, there is no proportional increase in monitoring personnel and response time to faults suffers. GTE deployed Telephone Operations Network Integrated Control System (TONICS) in 1993 to support its network management operations. To stay competitive in the face of continued staff reductions, increase in network size, and monitoring complications related to deregulation of the telephone industry, GTE is introducing artificial intelligence techniques into TONICS. Alarm Correlation Engine (ACE), the system described in this paper, is part of the effort. ACE aids network management by correlating alarms on the basis of common cause to provide alarm compression, filtering, and suppression. In conjunction with its ability to carry out prescribed responses, it improves response times and increases productivity. ACE was developed with the following requirements: reliability, speed, versatility (handle alarms from different switches and networks), ease of knowledge engineering (field technicians must be able to construct, test, and modify correlation patterns), handle in real time multiple network problems, and finally, interface smoothly with GTE's TONICS system. ACE's strength lies in its domain specific correlation language which facilitates knowledge engineering and in its asynchronous processing core that enables integration into a real-time monitoring system.",https://ieeexplore.ieee.org/document/655214/,NOMS 98 1998 IEEE Network Operations and Management Symposium,15-20 Feb. 1998,ieeexplore
10.1109/DCOSS49796.2020.00046,An Agnostic Data-Driven Approach to Predict Stoppages of Industrial Packing Machine in Near,IEEE,Conferences,"As data awareness in manufacturing companies increases with the deployment of sensors and Internet of Things (IoT) devices, data-driven maintenance and prediction have become quite popular in the Industry 4.0 paradigm. Machine Learning (ML) has been recognised as a promising, efficient and reliable tool for fault detection use cases, as it allows to export important knowledge from monitored assets. Scientists deal with issues such as the small amount of data that indicate potential problems, or the imbalance which exists between the standard process data and the data inadequacy of the systems to make a high precision forecast. Currently, in this context, even large industries are not able to effectively predict abnormal behaviors in their tools, processes and equipment, when adopting strategies to anticipate crucial events. In this paper, we propose a methodology to enable prediction of a packing machine's stoppages in manufacturing process of a large industry, by using forecasting techniques based on univariate time series data. There are more than 100 reasons that cause the machine to stop, in a quite big production line length. However, we use a single signal, concerning the machines operational status to make our prediction, without considering other fault or warning signals, hence its characterization as ""agnostic"". A workflow is presented for cleaning and preprocessing the data, and for training and evaluating a predictive model. Two predictive models, namely ARIMA and Prophet, are applied and evaluated on real data from an advanced machining process used for packing. Training and evaluation tests indicate that the results of the applied methods perform well on a daily basis. Our work can be further extended and act as reference for future research activities that could lead to more robust and accurate prediction frameworks.",https://ieeexplore.ieee.org/document/9183540/,2020 16th International Conference on Distributed Computing in Sensor Systems (DCOSS),25-27 May 2020,ieeexplore
10.1109/ISEFS.2006.251169,An Approach to Real-time Color-based Object Tracking,IEEE,Conferences,"Object tracking is of great interest in different areas of industry, security and defense. Tracking moving objects based on color information is more robust than systems utilizing motion cues. In order to maintain the lock on the object as the surrounding conditions vary, the color model needs to be adapted in real-time. In this paper an on-line learning method for the color model is implemented using fuzzy adaptive resonance theory (ART). Fuzzy ART is a type of neural network that is trained based on competitive learning principle. The color model of the target region is regularly updated based on the vigilance criteria (which is a threshold) applied to the pixel color information. The target location in the next frame is predicted using evolving extended Takagi-Sugeno (exTS) model to improve the tracking performance. The results of applying exTS for prediction of the position of the moving target were compared with the usually used solution based on Kalman filter. The experiments with real footage demonstrate over a variety of scenarios the superiority of the exTS as a predictor comparing to the Kalman filter. Further investigation concentrates on using evolving clustering for realizing computationally efficient simultaneous tracking of different segments in the object",https://ieeexplore.ieee.org/document/4016733/,2006 International Symposium on Evolving Fuzzy Systems,7-9 Sept. 2006,ieeexplore
10.1109/ICCC.2018.00010,An Edge Based Smart Parking Solution Using Camera Networks and Deep Learning,IEEE,Conferences,"The smart parking industry continues to evolve as an increasing number of cities struggle with traffic congestion and inadequate parking availability. For urban dwellers, few things are more irritating than anxiously searching for a parking space. Research results show that as much as 30% of traffic is caused by drivers driving around looking for parking spaces in congested city areas. There has been considerable activity among researchers to develop smart technologies that can help drivers find a parking spot with greater ease, not only reducing traffic congestion but also the subsequent air pollution. Many existing solutions deploy sensors in every parking spot to address the automatic parking spot detection problems. However, the device and deployment costs are very high, especially for some large and old parking structures. A wide variety of other technological innovations are beginning to enable more adaptable systems-including license plate number detection, smart parking meter, and vision-based parking spot detection. In this paper, we propose to design a more adaptable and affordable smart parking system via distributed cameras, edge computing, data analytics, and advanced deep learning algorithms. Specifically, we deploy cameras with zoom-lens and motorized head to capture license plate numbers by tracking the vehicles when they enter or leave the parking lot; cameras with wide angle fish-eye lens will monitor the large parking lot via our custom designed deep neural network. We further optimize the algorithm and enable the real-time deep learning inference in an edge device. Through the intelligent algorithm, we can significantly reduce the cost of existing systems, while achieving a more adaptable solution. For example, our system can automatically detect when a car enters the parking space, the location of the parking spot, and precisely charge the parking fee and associate this with the license plate number.",https://ieeexplore.ieee.org/document/8457691/,2018 IEEE International Conference on Cognitive Computing (ICCC),2-7 July 2018,ieeexplore
10.1109/ICMLA.2019.00115,An Edge Computing Visual System for Vegetable Categorization,IEEE,Conferences,"In self-service supermarket and retail industry, efforts to reduce customer wait time using automatic grocery item identification are challenged by low recognition accuracy, long response time and substantial requirement for equipment. In this paper, we propose a novel edge computing system named EdgeVegfru for vegetable and fruit image classification. While existing work on Vegfru dataset shows excellent performance, few of them have been deployed in real-world applications. We adopt an edge computing paradigm, design, implement and evaluate the whole system on the Android devices. The proposed deep learning model and quantization algorithm reduce the model size and inference time significantly. Our system has shown out-standing accuracy within limited time and computation resources, compared with other machine learning methods(such as Support Vector Machine(SVM), Random Forest(RF)), thus providing the potential path for automatic recognition and pricing in self-service retail stores.",https://ieeexplore.ieee.org/document/8999203/,2019 18th IEEE International Conference On Machine Learning And Applications (ICMLA),16-19 Dec. 2019,ieeexplore
10.1109/I2CT45611.2019.9033957,An Efficient Approach to Fruit Classification and Grading using Deep Convolutional Neural Network,IEEE,Conferences,"In India, the agricultural industry has seen a boom in recent years, demanding an increased inclusion of automation in it. An important aspect of this agro-automation is grading and classification of agricultural produce. These labor intensive tasks can be automated by use of Computer Vision and Machine Learning. This paper focuses on developing a standalone system capable of classifying 3 types of fruit and taking apple as test case of grading. The fruit types include apple, orange, pear and lemon. Further, apples have been graded into four grades, Grade 1 being the best quality apple and Grade 4 consisting of the spoilt ones. Input is given in the form of fruit image. The involved methodology is dataset formation, preprocessing, software as well as hardware implementations and classification. Preprocessing consists of background removal and segmentation techniques in order to extract fruit area. Deep Convolutional Neural Network has been chosen for the real time implementation of system and applied on fruit 360 dataset. For that purpose, the Inception V3 model is trained using the transfer training approach, thus enabling it to distinguish fruit images. The results after experimentation show that the Top 5 accuracy on the dataset used is 90% and the Top 1 accuracy is 85% which targets accuracy limitation of previous attempts.",https://ieeexplore.ieee.org/document/9033957/,2019 IEEE 5th International Conference for Convergence in Technology (I2CT),29-31 March 2019,ieeexplore
10.1109/IDAP.2018.8620812,An Embedded Real-Time Object Detection and Measurement of its Size,IEEE,Conferences,"In these days, real-time object detection and dimensioning of objects is an important issue from many areas of industry. This is a vital topic of computer vision problems. This study presents an enhanced technique for detecting objects and computing their measurements in real time from video streams. We suggested an object measurement technique for real-time video by utilizing OpenCV libraries and includes the canny edge detection, dilation, and erosion algorithms. The suggested technique comprises of four stages: (1) identifying an object to be measured by using canny edge detection algorithm, (2) using morphological operators includes dilation and erosion algorithm to close gaps between edges, (3) find and sort contours, (4) measuring the dimensions of objects. In the implementation of the proposed technique, we designed a system that used OpenCV software library, Raspberry Pi 3 and Raspberry Camera. The proposed technique was nearly achieved 98% success in determines the size of the objects.",https://ieeexplore.ieee.org/document/8620812/,2018 International Conference on Artificial Intelligence and Data Processing (IDAP),28-30 Sept. 2018,ieeexplore
10.1109/ICAPAI49758.2021.9462061,An Industrial Assistance System with Manual Assembly Step Recognition in Virtual Reality,IEEE,Conferences,"In the era of Industry 4.0, worker assistance systems are becoming more and more important. In order to assist shop floor workers in manual assembly tasks, we implemented an assistance system in virtual reality. A deep neural network was trained to recognize the current work step in real-time during an assembly process, thus giving the assistance system context-awareness. We defined the problem of assembly step recognition as a multivariate time series classification using the poses of the workers head, both hands and all relevant tools and objects. With this definition, the VR environment's output can also be replaced with data from the real world. For our proof-of-concept assembly step recognition system, we created an assembly process consisting of six different work steps, five movable assembly parts and one tool. We showed that we can train an activity recognition model for assembly steps with only 10 assembly recordings. To achieve this, we used multiple data augmentation techniques and proposed a novel method of synthesizing new training data, which we call Path Joining. With only 10 training recordings, we attain a categorical classification accuracy of 81 percent and with 60 recordings we achieve an accuracy of 89 percent.",https://ieeexplore.ieee.org/document/9462061/,2021 International Conference on Applied Artificial Intelligence (ICAPAI),19-21 May 2021,ieeexplore
10.1109/ICIT46573.2021.9453580,An Industrial HMI Temporal Adaptation based on Operator-Machine Interaction Sequence Similarity,IEEE,Conferences,"The incorporation of Artificial Intelligence (AI) into Industrial Environments has brought about a Smart Industry revolution, improving efficiency and simplifying complex industrial processes. However, these technological advances remain primarily focused on the process, and pay little attention to industrial Human-Machine Interfaces (HMI), the bridge between the operator and the industrial process.Current industrial HMIs have a static design, and are focused exclusively on the control and visualization of process information. They fail to take into account user behaviour and skills, information key to understanding how the operator interacts with the production process. Thus, the potential beneficial outcomes of considering operator-machine interaction in terms of efficiency and productivity, make a compelling case for industrial HMIs that can adapt to different operators based on their skills and process knowledge.This paper proposes a Machine Learning (ML) based method-ology capable of analysing operator-machine interaction and detecting the variability of interaction patterns for repetitive similar sequences in monitoring and control tasks. The method-ology generates a set of adaptation rules that improve Usability and User Experience, and hence operator working performance. To validate the proposed methodology, an experiment with real operators was conducted.",https://ieeexplore.ieee.org/document/9453580/,2021 22nd IEEE International Conference on Industrial Technology (ICIT),10-12 March 2021,ieeexplore
10.1109/COASE.2018.8560557,An Industry 4.0 Cyber-Physical Framework for Micro Devices Assembly,IEEE,Conferences,"An advanced cyber manufacturing framework to support the collaborative assembly of micro devices is presented based on Industry 4.0 principles. The distributed cyber and physical components work together to plan, assemble and monitor micro assembly related tasks; micro assembly refers to the assembly of micron sized devices which cannot be manufactured by MEMS technologies. The collaborative framework proposed includes assembly planning and path planning modules, Virtual Reality based assembly simulation environments and physical assembly work cells. An ontology based approach was implemented to address semantic interoperability issues to support formation of temporary partnerships in a Virtual Enterprise context. The key to the design and implementation of this complex framework is an information centric process modeling approach which provides a data/information oriented basis for collaboration. A collaborative cyber physical test bed has been built to demonstrate feasibility of proposed framework and approach.",https://ieeexplore.ieee.org/document/8560557/,2018 IEEE 14th International Conference on Automation Science and Engineering (CASE),20-24 Aug. 2018,ieeexplore
10.1109/ISSE46696.2019.8984462,An IoT Reconfigurable SoC Platform for Computer Vision Applications,IEEE,Conferences,"The field of Internet of Things (IoT) and smart sensors has expanded rapidly in various fields of research and industrial applications. The area of IoT robotics has become a critical component in the evolution of Industry 4.0 standard. In this paper, we developed an IoT based reconfigurable System on Chip (SoC) robot that is fast and efficient for computer vision applications. It can be deployed in other IoT robotics applications and achieve its intended function. A Terasic Hexapod Spider Robot (TSR) was used with its DE0-Nano SoC board to implement our IoT robotics system. The TSR was designed to provide a competent computer vision application to recognize different shapes using a machine learning classifier. The data processing for image detection was divided into two parts, the first part involves hardware implementation on the SoC board and to provide real-time interaction of the robot with the surrounding environment. The second part of implementation is based on the cloud processing technique, where further data analysis was performed. The image detection algorithm for the computer vision component was tested and successfully implemented to recognize shapes. The TSR moves or reacts based on the detected image. The Field Programmable Gate Array (FPGA) part is programmed to handle the movement of the robot and the Hard Processor System (HPS) handles the shape recognition, Wi-Fi connectivity, and Bluetooth communication. This design is implemented, tested and can be used in real-time applications in harsh environments where movements of other robots are restricted.",https://ieeexplore.ieee.org/document/8984462/,2019 International Symposium on Systems Engineering (ISSE),1-3 Oct. 2019,ieeexplore
10.1109/WCNCW.2019.8902853,An Open 5G NFV Platform for Smart City Applications Using Network Softwarization,IEEE,Conferences,"Advanced wireless communication network testbeds are now widely being deployed around European and cross-continental. This represents an interesting opportunity for vertical industry and academia to perform experimentation and validation before a real deployment. In this paper, we present 5GinFIRE as a suitably flexible platform towards open 5G (Network Function Virtualization (NFV) ecosystem and playground. On top of this platform, we designed and deployed a smart city safety system as a vertical use case, exploring 5G capabilities through a combination of NFV and machine learning to provide end-to-end communication and low latency smart city service. This safety system helps detecting criminals along the city and sending a notification to the security center. A Virtual Network Function (VNF) has been developed to enable video transcoding, face detection and recognition at the cloud or the edge of the network. The validation of the overall system is performed through the deployment of the use case indoor (Smart Internet Lab) and outdoor (Millennium Square Bristol). We show the VNF specification and present a quantitative analysis in terms of bandwidth, response time, processing time and transmission speed in terms of Quality of Experience (QoE).",https://ieeexplore.ieee.org/document/8902853/,2019 IEEE Wireless Communications and Networking Conference Workshop (WCNCW),15-18 April 2019,ieeexplore
10.1109/ICSSSM.2017.7996301,An active learning method based on mistake sampling for large scale imbalanced classification,IEEE,Conferences,"Nowadays, the challenge of learning from large scale and imbalanced data set have attracted a great deal of attention from both industry and academia, which is also deemed to be an important task for fraud detection in telecommunication, finance, online commerce. In general, it's almost impossible to train a classification model on the complete data set, especially in the era of big data, due to the space-time complexity. Thus, how to sample a training set from the original large-scale set that can provide a more accurate prediction result has become a focal point of study. Active learning provides a way to iteratively add a small batch of data to the initial training set at one time, such that a training set can be augmented with informative samples. However, when tackling with extremely imbalanced data, active learning methods can be invalid. To that end, in this paper, we proposed a novel method to sample the training set based on active learning, in order to solve large scale and imbalanced learning problem. Moreover, we exploit SMOTE, one of the most widely used resampling methods to balance the training set. The experiment was conducted on real world data from the industry of telecommunications. As the result presents, our proposed solution showed a steady and better performance compared to those widely used active learning methods.",https://ieeexplore.ieee.org/document/7996301/,2017 International Conference on Service Systems and Service Management,16-18 June 2017,ieeexplore
10.1109/ICPHYS.2018.8390779,An approach for implementing key performance indicators of a discrete manufacturing simulator based on the ISO 22400 standard,IEEE,Conferences,"Performance measurement tools and techniques have become very significant in today's industries for increasing the efficiency of their processes in order to face the competitive market. The first step towards performance measurement is the real-time monitoring and gathering of the data from the manufacturing system. Applying these performance measurement techniques on real-world industry in a way that is more general and efficient is the next challenge. This paper presents a methodology for implementing the key performance indicators defined in the ISO 22400 standard-Automation systems and integration, Key performance indicators (KPIs) for manufacturing operations management. The proposed methodology is implemented on a multi robot line simulator for measuring its performance at runtime. The approach implements a knowledge-based system within an ontology model which describes the environment, the system and the KPIs. In fact, the KPIs semantic descriptions are based on the data models presented in the Key Performance Indicators Markup Language (KPIML), which is an XML implementation of models developed by the Manufacturing Enterprise Solutions Association (MESA) international organization.",https://ieeexplore.ieee.org/document/8390779/,2018 IEEE Industrial Cyber-Physical Systems (ICPS),15-18 May 2018,ieeexplore
10.1109/SYSCON.2018.8369547,An interactive architecture for industrial scale prediction: Industry 4.0 adaptation of machine learning,IEEE,Conferences,"According to wiki definition, there are four design principles in Industry 4.0. These principles support companies in identifying and implementing Industry 4.0 scenarios, namely, Interoperability, Information transparency, Technical assistance, Decentralized decisions. In this paper we have discussed our work on an implementation of a machine learning based interactive architecture for industrial scale prediction for dynamic distribution of water resources across the continent, keeping the four corners of Industry 4.0 in place. We report the possibility of producing most probable high resolution estimation regarding the water balance in any region within Australia by implementation of an intelligent system that can integrate spatial-temporal data from various independent sensors and models, with the ground truth data produced by 250 practitioners from the irrigation industry across Australia. This architectural implementation on a cloud computing platform linked with a freely distributed mobile application, allowing interactive ground truthing of a machine learning model on a continental scale, shows accuracy of 90% with 85% sensitivity of correct surface soil moisture estimation with end users at its complete control. Along with high level of information transparency and interoperability, providing on-demand technical supports and motivating users by allowing them to customize and control their own local predictive models, show the successfulness of principles in Industry 4.0 in real environmental issues in the future adaptation in various industries starting from resource management to modern generation soft robotics.",https://ieeexplore.ieee.org/document/8369547/,2018 Annual IEEE International Systems Conference (SysCon),23-26 April 2018,ieeexplore
10.1109/ROBOT.1992.220085,An optimal scheduling of pick place operations of a robot-vision-tracking system by using back-propagation and Hamming networks,IEEE,Conferences,"The authors present a neural network approach to solve the dynamic scheduling problem for pick-place operations of a robot-vision-tracking system. An optimal scheduling problem is formulated to minimize robot processing time without constraint violations. This is a real-time optimization problem which must be repeated for each group of objects. A scheme which uses neural networks to learn the mapping from object pattern space to optimal order space offline and to recall online what has been learned is presented. The idea was implemented in a real system to solve a problem in large commercial dishwashing operations. Experimental results have been shown that with four different objects, time savings of up to 21% are possible over first-come, first-served schemes currently used in industry.&lt;<ETX>&gt;</ETX>",https://ieeexplore.ieee.org/document/220085/,Proceedings 1992 IEEE International Conference on Robotics and Automation,12-14 May 1992,ieeexplore
10.1109/ESSCIRC.1992.5468456,Analog Electronic Neural Networks,IEEE,Conferences,"The interest in analog circuit techniques for implementing neural nets is undiminished, as is indicated by a large number of recent designs, coming from universities as well as from industry. One group of circuits are networks containing the ""multiply-accumulate"" neurons with a large interconnectivity. The main motivation for using analog circuit techniques is the fact that the multiply-accumulate operation can be implemented compactly, if only a moderate precision of the computation is required. Other types of networks are more algorithm-specific, hard-wired for one function, for example Kohonen networks, or neuromorphic designs implementing functions found in the visual or the auditory system. Most neural nets are built with standard CMOS technology, except for a few designs in CCD technology. A few analog neural net chips are now commercially available, and more and more reports of applications are appearing. This is a significant step in the development of analog neural nets, as now their usefulness is being put to the test in ""real-world"" applications.",https://ieeexplore.ieee.org/document/5468456/,ESSCIRC '92: Eighteenth European Solid-State Circuits conference,21-23 Sept. 1992,ieeexplore
10.1109/SPAC49953.2019.237870,Analysis of Customer Segmentation Based on Broad Learning System,IEEE,Conferences,"In the field of retail industry and marketing, identifying customer segments is one of the most important tasks. A meaningful segmentation is able to help the managers to enhance the quality of products and services for the targeting segments. Most of traditional methods used POS data to classify the customer loyalty as “heavy” segment while others are belonging to “light” segment. Based on the previous studies, this paper presents three improvements. Firstly, in addition to customer purchasing behavior, we also include RFID (Radio Frequency IDentification) data, which can accurately represent the consumers' in-store behavior. Secondly, this paper uses broad learning system (BLS) to analyze the consumer segmentation. BLS is one of the most state-of-the-art machine learning techniques, and quite efficient and effective for classification tasks. Thirdly, the customer behavior data used in this paper are collected from a real-world supermarket in Japan. We also consider the customer segmentation as a multi-label classification problem based on both of POS data and RFID data. In the experiment, the results were compared with other popular classification models, such as neural network and support vector machine, and it was found that BLS greatly reduced training time while guaranteeing accuracy.",https://ieeexplore.ieee.org/document/9084871/,"2019 International Conference on Security, Pattern Analysis, and Cybernetics (SPAC)",20-23 Dec. 2019,ieeexplore
10.1109/CICT.2013.6558219,Analysis of rice granules using image processing and neural network,IEEE,Conferences,"In food handling industry, grading of granular food materials is necessary because samples of material are subjected to adulteration. In the past, food products in the form of particles or granules were passed through sieves or other mechanical means for grading purposes. In this paper, analysis is performed on basmati rice granules; to evaluate the performance using image processing and Neural Network is implemented based on the features extracted from rice granules for classification grades of granules. Digital imaging is recognized as an efficient technique, to extract the features from rice granules in a non-contact manner. Images are acquired for rice using camera. Conversion to gray scale, Median smoothing, Adaptive thresholding, Canny edge detection, Sobel edge Detection, morphological operations, extraction of quantitative information are the checks that are performed on the acquired image using image processing technique through Open source Computer Vision (Open CV) which is a library of functions that aids image processing in real time. The morphological features acquired from the image are given to Neural Network. This work has been done to identify the relevant quality category for a given rice sample based on its parameters. The performance of image processing reduced the time of operation and improved the crop recognition greatly. Grading results obtained from Neural Network system shows greater accuracy when compared with the outputs from human experts.",https://ieeexplore.ieee.org/document/6558219/,2013 IEEE Conference on Information & Communication Technologies,11-12 April 2013,ieeexplore
10.1109/LCN52139.2021.9524931,Anomaly Detection for Discovering Performance Degradation in Cellular IoT Services,IEEE,Conferences,"Connected and automated vehicles (CAVs) are envisioned to revolutionize the transportation industry, enabling autonomous processes and real-time exchange of information among vehicles and infrastructure. To safely navigate the roadways, CAVs rely on sensor readings and data from the surrounding vehicles. Hence, a fault or anomaly arising from the hardware, software, or the network can lead into devastating consequences regarding safety. This study investigates potential performance degradation caused by anomalies, by analyzing real-life vehicles’ sensory and network-related data. The aim is to utilize unsupervised learning for anomaly detection, with a goal to describe the cause and effect of the detected anomalies from a performance perspective. The results show around 93% F1-score when detecting anomalies imposed by the cellular network and the vehicle’s sensors. Moreover, with approximately 90% F1-score we can detect anomalous predictions from a deployed network-related ML model predicting cellular throughput and describe the root-causes behind the detected anomalies.",https://ieeexplore.ieee.org/document/9524931/,2021 IEEE 46th Conference on Local Computer Networks (LCN),4-7 Oct. 2021,ieeexplore
10.1109/ICNSURV.2016.7486356,Anomaly detection in aircraft data using Recurrent Neural Networks (RNN),IEEE,Conferences,"Anomaly Detection in multivariate, time-series data collected from aircraft's Flight Data Recorder (FDR) or Flight Operational Quality Assurance (FOQA) data provide a powerful means for identifying events and trends that reduce safety margins. The industry standard “Exceedance Detection” algorithm uses a list of specified parameters and their thresholds to identify known deviations. In contrast, Machine Learning algorithms detect unknown unusual patterns in the data either through semi-supervised or unsupervised learning. The Multiple Kernel Anomaly Detection (MKAD) algorithm based on One-class SVM identified 6 of 11 canonical anomalies in a large dataset but is limited by the need for dimensionality reduction, poor sensitivity to short term anomalies, and inability to detect anomalies in latent features. This paper describes the application of Recurrent Neural Networks (RNN) with Long Term Short Term Memory (LTSM) and Gated Recurrent Units (GRU) architectures which can overcome the limitations described above. The RNN algorithms detected 9 out the 11 anomalies in the test dataset with Precision = 1, Recall = 0.818 and F1 score = 0.89. RNN architectures, designed for time-series data, are suited for implementation on the flight deck to provide real-time anomaly detection. The implications of these results are discussed.",https://ieeexplore.ieee.org/document/7486356/,2016 Integrated Communications Navigation and Surveillance (ICNS),19-21 April 2016,ieeexplore
10.1109/TAAI.2017.11,Application of List-Wise Rank Learning to Property Search for Renovation,IEEE,Conferences,"Today, artificial Intelligence (AI) technology has been wildly spread to different industries such as advertising, business, and finance industry. On the other hand, there exist industries to which application of AI has not yet been enough despite its potential. Real estate industry is one of such domains. When applying AI technology to the real estate industry, various issues are expected to be solved such as predicting the price of properties, customer reception, floor plan identification, and support of property search. This paper focuses on the support of property search. The difficulty of property search compared with such items as books, movies, and news articles is that we would seldom buy property. It also means that we usually do not have enough knowledge and experience needed to explicitly express information need and to examine retrieved results. Therefore, human experts (salespersons) help such users to find properties which they consider satisfactory. If the tacit knowledge used by human experts to estimate the value of properties could be extracted, it would contribute to support of property search by ordinary users (customers). We think utilizing different individual's strength and experience is one of important topics of community-centric systems. Aiming at extracting expert knowledge about properties suitable for renovation, this paper applies List-wise rank learning to property search. Through an experiment using search log by actual salespersons in the real estate company, the possibility of learning expert knowledge is examined.",https://ieeexplore.ieee.org/document/8356920/,2017 Conference on Technologies and Applications of Artificial Intelligence (TAAI),1-3 Dec. 2017,ieeexplore
,Application of RapidMiner and R environments to dangerous seismic events prediction,IEEE,Conferences,"Underground coal mining is a branch of an industry which safety of operation is very dependent on the natural hazards. A proper seismic event prediction is a significant aspect of building classification models from the real data, which can affect the coal mining safety increase. In this paper four models, built in a well known data mining environments, are presented. The obtained models, depending on a given implementation of popular methods, occurred comparable to the best results from the competition.",https://ieeexplore.ieee.org/document/7733247/,2016 Federated Conference on Computer Science and Information Systems (FedCSIS),11-14 Sept. 2016,ieeexplore
10.1109/CMI.2016.7413757,Application of close loop expert system for heating control of rolling mill furnaces in a steel plant,IEEE,Conferences,"One of the critical impediment faced in hierarchical control in an process industry is unavailability of exact mathematical co-relations, which can precisely define the process behavior. These are primarily due to variable, complex and un-measurable factors and noises influencing the process behavior. However, such cases are most appropriate application areas of Expert Systems. In process industry, Expert Systems are one of the successful application areas of Artificial Intelligence, where expertise and knowledge of a Process Expert or a group of Experts are embedded as computer inference software and database. In a real time situation, these systems can take intelligent decisions as would have been taken by process Expert on a similar situation. Determination of exact Set Process Temperatures or thermal regime on different parts of Rolling Mill Furnaces like Annealing Furnace in a Steel Industry is an intriguing problem. However, this decision is very crucial as final mechanical and metallurgical quality of steel stock significantly depends on fixing and accurate control of these temperatures. But as a irony, no well defined mathematical co-relations are available, which can predict exact thermal regime to be followed to achieve desired quality and properties of steel coils/sheets under heating inside such furnaces. The aforesaid intriguing issue has been successfully resolved by development and implementation of Expert System guided heating control system through prediction and control of optimum furnace temperatures inside Annealing Furnaces at Cold Rolling Mill of Bokaro Steel Plant and Decarburization-Annealing Furnace at Silicon Steel Mill of Rourkela Steel Plant. In both the cases, concepts of hierarchical automation has been used, wherein Expert System comprising Level-II tier of automation predicts most appropriate thermal regime to obtain desired product quality for a given set of steel sheet. A seamlessly dovetailed PLC constitutes Level-I automation layer. PLC monitors and controls the plant as per advice from Expert System. Both the systems have enhanced plants efficiency by improving production, quality and energy conservation.",https://ieeexplore.ieee.org/document/7413757/,"2016 IEEE First International Conference on Control, Measurement and Instrumentation (CMI)",8-10 Jan. 2016,ieeexplore
10.1109/IJCNN.2001.939567,Application of neural networks in online oil content monitors,IEEE,Conferences,"Four major types of oils were examined to obtain both fluorescence and light scattering spectra as a function of oil concentrations. The large variations in fluorescence and scattering intensity with oil types and sub-types make it difficult to calibrate the analytical instrument using traditional methods. We implemented a multivariate, nonlinear calibration of instrumental response through an artificial neural network. We demonstrated that the combined use of fluorescence and scattering data significantly improves the quantitative prediction accuracy. The trained backpropagation neural network was used successfully to predict the concentrations of single oils and their mixtures, and appears well suited for the calibration of an online oil content monitor. The newly developed technique permits the online monitoring of oil concentrations in wastewater discharged from ships and oil refinery industry.",https://ieeexplore.ieee.org/document/939567/,IJCNN'01. International Joint Conference on Neural Networks. Proceedings (Cat. No.01CH37222),15-19 July 2001,ieeexplore
10.1109/CAIBDA53561.2021.00014,Applications of Smart Energy Integrated Service Platform in Optimization of Energy Utilization of Customers,IEEE,Conferences,"In the new era, comprehensive energy service has gradually become an important development direction to promote the high-quality development of energy industry and boost the development of real economy. The construction of smart integrated energy service platform is the internal demand for energy service enterprises to transform into integrated energy service providers, and is a solution to provide comprehensive energy support services for power users. The platform is divided into perception layer, acquisition layer, network layer, platform layer, application layer and display layer from bottom to top. Its main functions include energy regulation, energy efficiency analysis, smart operation, energy trading and smart dispatching. The construction of smart energy comprehensive service platform can provide customers with more high-quality, more convenient and more comprehensive energy value-added services, and constantly create more value for customers. Practice has proved that the implementation of smart energy integrated service platform can greatly reduce the monthly energy cost, monthly power load of the park and monthly coal consumption in park and enhance the indexes of monthly active users and customer satisfaction.",https://ieeexplore.ieee.org/document/9545981/,"2021 International Conference on Artificial Intelligence, Big Data and Algorithms (CAIBDA)",28-30 May 2021,ieeexplore
10.1109/IV.2003.1217948,Applied visual user interface technique in knowledge management,IEEE,Conferences,"Extracting actionable insight from large high-dimensional data sets, and its use for more effective decision-making, has become a pervasive problem across many application fields in both research and industry. The objective of our presentation is to report on some investigations of this problem covering both these areas. Taking as the problem domain the area of ""unsupervised learning"", we show that by tightly coupling statistical analysis technique with combinations of visualization components and techniques for interactivity, real-time analysis of multidimensional data can be efficiently made. We give particular attention to the ways in which dynamic visual representations can be used in these contexts to facilitate shared understanding. Our system is implemented and validated in the context of 3D medical imaging knowledge construction, knowledge management and geovisualisation.",https://ieeexplore.ieee.org/document/1217948/,"Proceedings on Seventh International Conference on Information Visualization, 2003. IV 2003.",18-18 July 2003,ieeexplore
10.1109/NetSoft48620.2020.9165317,Applying Machine Learning to End-to-end Slice SLA Decomposition,IEEE,Conferences,"5G is set to revolutionize the network service industry with unprecedented use-cases in industrial automation, augmented reality, virtual reality and many other domains. Network slicing is a key enabler to realize this concept, and comes with various SLA requirements in terms of latency, throughput, and reliability. Network slicing is typically performed in an end-to-end (e2e) manner across multiple domains, for example, in mobile networks, a slice can span access, transport and core networks. Thus, if an SLA requirement is specified for e2e services, we need to ensure that the total SLA budget is appropriately proportioned to each participating domain in an adaptive manner. Such an SLA decomposition can be extremely useful for network service operators as they can plan accordingly for actual deployment. In this paper we design and implement an SLA decomposition planner for network slicing using supervised machine learning algorithms. Traditional optimization based approaches cannot deal with the dynamic nature of such services. We design machine learning models for SLA decomposition, based on random forest, gradient boosting and neural network. We then evaluate each class of algorithms in terms of accuracy, sample complexity, and model explainability. Our experiments reveal that, in terms of these three requirements, the gradient boosting and neural network algorithms for SLA decomposition out-perform random forest algorithms, given emulated data sets.",https://ieeexplore.ieee.org/document/9165317/,2020 6th IEEE Conference on Network Softwarization (NetSoft),29 June-3 July 2020,ieeexplore
10.1109/ICE/ITMC52061.2021.9570215,Artificial Intelligence as Enabler for Sustainable Development,IEEE,Conferences,"When the UN published the 17 Sustainable Development Goals (SDGs) in 2015, emerging technologies like Artificial Intelligence (AI) were not yet mature. However, through its deployment across industry sectors and verticals, issues related to sustainability, fairness, inclusiveness, efficiency, and usability of these technologies are now priorities for global consumers and producers. This paper discusses what needs to be considered by both policy makers and ‘managers’ in order to exploit the use of AI for SDG achievement. AI can act as a real and meaningful enabler to achieve sustainability goals; however, it may also have negative impacts. Therefore, a carefully balanced approach is required to ensure that Artificial Intelligence systems are employed to help solve sustainability issues without inadvertently affecting other goals.",https://ieeexplore.ieee.org/document/9570215/,"2021 IEEE International Conference on Engineering, Technology and Innovation (ICE/ITMC)",21-23 June 2021,ieeexplore
10.1109/ICAACCA51523.2021.9465297,Assessing Machine learning-based approaches for Silica concentration estimation in Iron Froth flotation,IEEE,Conferences,"In the mining industry, specifically in the flotation process, there is a challenge associated to noninvasive, real-time contaminant and impurities estimation. Achieving predictions on contaminant levels has a high impact on quality insurance and it can help technicians and engineers to make adjustments in advance to improve the quality of the final product, and thus profits. In this paper, exploratory research is performed on the problem of silica concentrate estimation for iron ore froth flotation using machine learning techniques, with the goal to identify algorithms that may be suitable for industry soft sensor development. For this purpose, a public, real process dataset is used and three different machine learning techniques are implemented: Random Forest (RF), Long Short-Term Memory (LSTM) and Gated Recurrent Unit (GRU). The techniques were implemented, tested and compared in terms of their error percentage, mean absolute error, mean square error, and root mean square error. Obtained results show acceptable performance for LTSM and GRU implementations, being LSTM network the out-performer with errors below 9%.",https://ieeexplore.ieee.org/document/9465297/,2021 IEEE International Conference on Automation/XXIV Congress of the Chilean Association of Automatic Control (ICA-ACCA),22-26 March 2021,ieeexplore
10.1109/ISDA.2010.5687116,Associative prediction model and clustering for product forecast data,IEEE,Conferences,"Association rules are adopted to discover the interesting relationship and knowledge in a large dataset. Knowledge may appear in terms of a frequent pattern discovered in a large number of production data. This knowledge can improve or solve production problems to achieve low cost production. To obtain knowledge and quality information, data mining can be applied to the manufacturing industry. In this study, we used one of the association rule approach, i.e. Apriori algorithm to build an associative prediction model for product forecast data. Also, we adopt the simplest method in clustering, k-means algorithm to attain the link between patterns. The real industrial product forecast data for one year duration is used in the experiment. This data consists of 42 products with two important attributes, i.e. time in the week and required quantity. Since the data mining processes need a large amount of data, we simulated these data by using the Monte Carlo technique to obtain another 15 years of simulated forecast data. There are two main experiments for the association rules mining and clustering. As a result, we obtain an associative prediction model and clustering for the forecasting data. The extracted model provides the prediction knowledge about the range of production in a certain period.",https://ieeexplore.ieee.org/document/5687116/,2010 10th International Conference on Intelligent Systems Design and Applications,29 Nov.-1 Dec. 2010,ieeexplore
10.1109/ICSPCC.2018.8567797,Attack Detection for Wireless Enterprise Network: a Machine Learning Approach,IEEE,Conferences,"An increasing number of enterprises are adopting wireless technology to deploy networks. However, wireless enterprise networks are more vulnerable than wired networks because of the broadcast feature. Thus, illegal attacks such as data theft and information forgery seriously threaten the property and information security of users and enterprises; these phenomena are attracting increasing attention from both academia and industry. Additionally, effectively detecting the attacks in the wireless enterprise networks is one of todays most important and challenging problems, especially in Wi-Fi networks, as attacks become increasingly covert and diverse. Fortunately, WiFi networks produce large amounts of data, providing copious big data for researchers. In this paper, using the Aegean Wi-Fi Intrusion Dataset (AWID), which is derived from the real-world Wi-Fi network, we introduce machine learning to detect network attacks. To significantly increase the training and convergence speeds, we deploy two-dimensional data cleaning and select 18 useful attributes from the original set of 154. Then, we introduce support vector machine (SVM) to detect attacks based on the cleaned dataset. The detection accuracy for flooding attacks, injection attacks, and normal data reached 89.18%, 87.34%, and 99.88% respectively. To the best of our knowledge, this is the first study to introduce a two-dimensional data cleaning method with an SVM to improve the detection accuracy for attacks. Finally, our detection results are comparable with the existing studies; however, our method operates with simpler data attributes with faster and more efficient training speed.",https://ieeexplore.ieee.org/document/8567797/,"2018 IEEE International Conference on Signal Processing, Communications and Computing (ICSPCC)",14-16 Sept. 2018,ieeexplore
10.1109/ICIT.2017.7915547,Augmented reality based on edge computing using the example of remote live support,IEEE,Conferences,"Augmented Reality (AR) introduces vast opportunities to the industry in terms of time and therefore cost reduction when utilized in various tasks. The biggest obstacle for a comprehensive deployment of mobile AR is that current devices still leave much to be desired concerning computational and graphical performance. To improve this situation in this paper we introduce an AR Edge Computing architecture with the aim to offload the demanding AR algorithms over the local network to a high-end PC considering the real-time requirements of AR. As an example use case we implemented an AR Remote Live Support application. Applications like this on the one hand are strongly demanded in the industry at present, on the other hand by now mostly do not implement a satisfying tracking algorithm lacking computational resources. In our work we lay the focus on both, the possibilities our architecture offers regarding improvements of tracking and the challenges it implies in respect of real-time. We found that offloading AR algorithms in real-time is possible with available WiFi making use of standard compression techniques like JPEG. However it can be improved by future radio solutions offering higher bandwidth to avoid additional latency contributed by the coding.",https://ieeexplore.ieee.org/document/7915547/,2017 IEEE International Conference on Industrial Technology (ICIT),22-25 March 2017,ieeexplore
10.1109/IJCNN.2016.7727278,Augmenting adaptation with retrospective model correction for non-stationary regression problems,IEEE,Conferences,"Existing adaptive predictive methods often use multiple adaptive mechanisms as part of their coping strategy in non-stationary environments. We address a scenario when selective deployment of these adaptive mechanisms is possible. In this case, deploying each adaptive mechanism results in different candidate models, and only one of these candidates is chosen to make predictions on the subsequent data. After observing the error of each of candidate, it is possible to revert the current model to the one which had the least error. We call this strategy retrospective model correction. In this work we aim to investigate the benefits of such approach. As a vehicle for the investigation we use an adaptive ensemble method for regression in batch learning mode which employs several adaptive mechanisms to react to changes in the data. Using real world data from the process industry we show empirically that the retrospective model correction is indeed beneficial for the predictive accuracy, especially for the weaker adaptive mechanisms.",https://ieeexplore.ieee.org/document/7727278/,2016 International Joint Conference on Neural Networks (IJCNN),24-29 July 2016,ieeexplore
10.1109/WSAI49636.2020.9143279,Automated Analysis of Seizure Behavior in Video: Methods and Challenges,IEEE,Conferences,"Automated analysis of seizure behavior in video using intelligent video analytics technology has significant applications in healthcare industry, since it can provide accurate and quantitative measurement of human seizure behavior for assisting diagnosis. This paper presents a brief survey on intelligent video analytics for automated seizure behavior analysis, including both conventional motion analysis based approaches and the state-of-the-art machine learning based approaches. Furthermore, a new automated video analytics framework is proposed in this paper, by exploiting the machine learning approach to build a seizure motion model and performing automatic detection of seizure events in the surveillance video in real time. This paper also discusses the preliminary experimental results and deployment of the proposed framework, as well as the future research challenges in this area.",https://ieeexplore.ieee.org/document/9143279/,2020 2nd World Symposium on Artificial Intelligence (WSAI),27-29 June 2020,ieeexplore
10.1109/CICommS.2013.6582853,Automated network application classification: A competitive learning approach,IEEE,Conferences,"The design of a sustainable application level classification system has, over the past few years, been the subject of much research by academics and industry alike. The methodologies proposed rely predominantly on predefined signatures for each protocol, applied to each passing flow in order to classify them. These signatures are often static, resulting in inaccuracies during the classification process. This problem is compounded by delays in signature update releases. This paper presents an approach toward automated signature generation, mitigating classification problems experienced with existing systems. A hierarchical system is proposed, where signatures are developed and deployed in real-time. The ideas set forth in this research are evaluated by experimentation in a live network environment. Discriminators of both encrypted and plain-text application protocol samples were recorded and automatically annotated by a Hierarchical Self-Organizing Map (HSOM). The clusters identified by the HSOM were used in a supervised training process that correctly identified protocols with an almost perfect (99% percent) success rate.",https://ieeexplore.ieee.org/document/6582853/,2013 IEEE Symposium on Computational Intelligence for Communication Systems and Networks (CIComms),16-19 April 2013,ieeexplore
10.1109/IJCNN.2010.5596494,Automated texture classification of marble shades with real-time PLC neural network implementation,IEEE,Conferences,"The subjective evaluation of marbles based on their visual appearance could be replaced by an automated texture classification system, intending to achieve high classification accuracy and production effectiveness. The existing marble classification methods from a computational point of view are either too complex or very expensive. Nowadays some inspection systems in marble industry that automates the quality-control tasks and shade classification are too expensive and are compatible only with specific technological equipment. In this paper a new approach for classification of marble tiles with similar shades is proposed. It is based on simple image preprocessing, on training a MLP neural network (MLP NN) with marble histograms and implementation of the algorithm in a Programmable Logic Controller (PLC) for real-time execution. A method for training the MLP NN aiming optimization of MLP parameters and topology is proposed. The designed automated system uses only standard PLC modules and communication interfaces. The experimental test results when recognizing marble textures with added motion blur are represented and discussed. The performance of the modeling technique is assessed with different training and test sets. The classification accuracy results are compared to other results obtained by similar approaches.",https://ieeexplore.ieee.org/document/5596494/,The 2010 International Joint Conference on Neural Networks (IJCNN),18-23 July 2010,ieeexplore
10.1109/VTC2021-Spring51267.2021.9448686,Automatic Generation of Critical Test Cases for the Development of Highly Automated Driving Functions,IEEE,Conferences,"The development of highly automated driving functions is currently one of the key drivers for the automotive industry and research. In addition to the technical constraints in the implementation of these functions, a major challenge is the verification of functional safety. Conventional approaches aiming at statistical validation in the sense of real test drives are reaching their economic limits. On the other hand, there are simulation methods that allow a lot of freedom in test case design, but whose representativeness and relevance must be proven separately. In this paper an approach is presented that allows to generate critical concrete scenarios and test cases for automated driving functions by means of a reinforcement learning based optimization using here the example of an overtaking assistant. For this purpose, a Q-Learning approach is used that automates the parameter generation for the test cases. While pure combinatorics of the variable parameters leads to an unmanageable amount of test cases, the percentage of actually relevant critical test cases is very low. In this work we show how the share of critical and thus relevant test cases can be increased significantly by using the presented method compared to a purely combinatorial parameter variation.",https://ieeexplore.ieee.org/document/9448686/,2021 IEEE 93rd Vehicular Technology Conference (VTC2021-Spring),25-28 April 2021,ieeexplore
10.1109/BigMM.2019.00-26,Automatic Speech Recognition for Real Time Systems,IEEE,Conferences,"Automatic Speech Recognition (ASR) systems have proven to be a useful tool to perform various day to day operations when used along with systems like Personal AI Assistants. Various industries require ASR to be trained in their domains. Music on Demand (MoD), over IVR, is one such industry where the user interacts with the dialogue system to play music using voice commands only. Domain adaptation of the model is expected to perform well on this domain as systems trained on public datasets are very generic in nature and not very domain-specific. To train the ASR for MoD, we experiment with the HMM-based classical approach and DeepSpeech2 on Voxforge dataset. We then fine-tune the DeepSpeech2 model on MoD data. With very limited data and little finetuning of the model, we were able to achieve 14.727% Word Error Rate (WER).",https://ieeexplore.ieee.org/document/8919271/,2019 IEEE Fifth International Conference on Multimedia Big Data (BigMM),11-13 Sept. 2019,ieeexplore
10.1109/ICMLC.2005.1527044,Automatic generating numerical control rule using genetic-based with multiple critical evaluation,IEEE,Conferences,"A method is proposed to automatically extract numerical control rules from the sensor data without the help of experts by means of a genetic algorithms (GA) based on multiplet critical evaluation to meeting different criteria. Every generated numerical rule is accumulated in a control table called a numerical rule-based controller. The numerical control table can be stored into the controller of embedded control system to construct a numerical rule-based embedded controller to meet real-time processing in the industry. The combination of multiple critics applies on the controller as fitness function of genetic learning in order to generate numerical rules to achieve multi-objective genetic process. Moreover, this paper apply an experimental design method which add a 'King strategy' to crossover operator of the standard GA in order to reduce the blindness of GA search processes and raise the convergence speed. An illustrative experiment is successfully made on the computer simulation. The experimental results reveal that the proposed approach is more efficient and more effective than the single objective.",https://ieeexplore.ieee.org/document/1527044/,2005 International Conference on Machine Learning and Cybernetics,18-21 Aug. 2005,ieeexplore
10.1109/ICIT.2018.8352157,Automatic parameter learning for easy instruction of industrial collaborative robots,IEEE,Conferences,"The manufacturing industry faces challenges in meeting requirements of flexibility, product variability and small batch sizes. Automation of high mix, low volume productions requires faster (re)configuration of manufacturing equipment. These demands are to some extend accommodated by collaborative robots. Certain actions can still be hard or impossible to manually adjust due to inherent process uncertainties. This paper proposes a generic iteratively learning approach based on Bayesian Optimisation to efficiently search for the optimal set of process parameters. The approach takes into account the process uncertainties by iteratively making a statistical founded choice on the next parameter-set to examine only based on the prior binomial outcomes. Moreover, our function estimator uses Wilson Score to make proper estimates on the success probability and the associated uncertain measure of sparsely sampled regions. The function estimator also generalises the experiment outcomes to the neighbour region through kernel smoothing by integrating Kernel Density Estimation. Our approach is applied to a real industrial task with significant process uncertainties, where sufficiently robust process parameters cannot intuitively be chosen. Using our approach, a collaborative robot automatically finds a reliable solution.",https://ieeexplore.ieee.org/document/8352157/,2018 IEEE International Conference on Industrial Technology (ICIT),20-22 Feb. 2018,ieeexplore
10.1109/VTCSpring.2019.8746507,Autonomous Driving without a Burden: View from Outside with Elevated LiDAR,IEEE,Conferences,"The current autonomous driving architecture places a heavy burden in signal processing for the graphics processing units (GPUs) in the car. This directly translates into battery drain and lower energy efficiency, crucial factors in electric vehicles. This is due to the high bit rate of the captured video and other sensing inputs, mainly due to Light Detection and Ranging (LiDAR) sensor at the top of the car which is an essential feature in autonomous vehicles. LiDAR is needed to obtain a high precision map for the vehicle AI to make relevant decisions. However, this is still a quite restricted view from the car. This is the same even in the case of cars without a LiDAR such as Tesla. The existing LiDARs and the cameras have limited horizontal and vertical fields of visions. In all cases it can be argued that precision is lower, given the smaller map generated. This also results in the accumulation of a large amount of data in the order of several TBs in a day, the storage of which becomes challenging. If we are to reduce the effort for the processing units inside the car, we need to uplink the data to edge or an appropriately placed cloud. However, the required data rates in the order of several Gbps are difficult to be met even with the advent of 5G. Therefore, we propose to have a coordinated set of LiDAR's outside at an elevation which can provide an integrated view with a much larger field of vision (FoV) to a centralized decision making body which then sends the required control actions to the vehicles with a lower bit rate in the downlink and with the required latency. The calculations we have based on industry standard equipment from several manufacturers show that this is not just a concept but a feasible system which can be implemented.The proposed system can play a supportive role with existing autonomous vehicle architecture and it is easily applicable in an urban area.",https://ieeexplore.ieee.org/document/8746507/,2019 IEEE 89th Vehicular Technology Conference (VTC2019-Spring),28 April-1 May 2019,ieeexplore
10.1109/ICDM50108.2020.00084,Autonomous Graph Mining Algorithm Search with Best Speed/Accuracy Trade-off,IEEE,Conferences,"Graph data is ubiquitous in academia and industry, from social networks to bioinformatics. The pervasiveness of graphs today has raised the demand for algorithms that can answer various questions: Which products would a user like to purchase given her order list? Which users are buying fake followers to increase their public reputation? Myriads of new graph mining algorithms are proposed every year to answer such questions - each with a distinct problem formulation, computational time, and memory footprint. This lack of unity makes it difficult for a practitioner to compare different algorithms and pick the most suitable one for a specific application. These challenges - even more severe for non-experts - create a gap in which state-of-the-art techniques developed in academic settings fail to be optimally deployed in real-world applications. To bridge this gap, we propose AutoGM, an automated system for graph mining algorithm development. We first define a unified framework UnifiedGM that integrates various message-passing based graph algorithms, ranging from conventional algorithms like PageRank to graph neural networks. Then UnifiedGM defines a search space in which five parameters are required to determine a graph algorithm. Under this search space, AutoGM explicitly optimizes for the optimal parameter set of UnifiedGM using Bayesian Optimization. AutoGM defines a novel budget-aware objective function for the optimization to incorporate a practical issue - finding the best speed-accuracy trade-off under a computation budget - into the graph algorithm generation problem. Experiments on real-world benchmark datasets demonstrate that AutoGM generates novel graph mining algorithms with the best speed/accuracy trade-off compared to existing models with heuristic parameters.",https://ieeexplore.ieee.org/document/9338289/,2020 IEEE International Conference on Data Mining (ICDM),17-20 Nov. 2020,ieeexplore
10.1109/ICIII.2008.195,BP Neural Network Optimized with PSO Algorithm for Daily Load Forecasting,IEEE,Conferences,"Accurate forecasting of daily electricity load has been one of the most important issues in the electricity industry. In recent few decades, the artificial neural network has been successfully employed to solve this problem because of the powerful capability to generalize the nonlinear relationships between the inputs and the desired outputs, without considering real problem domain expressions. A short-term load forecasting method based on BP neural network which is optimized by particle swarm optimization (PSO) algorithm is presented in this paper. The PSO is used to optimize the initial parameters of the BP neural network, then based on the optimized result, the BP neural network is used for short-term load forecasting. The experiment results show the method in the paper has greater improvement in both accuracy and velocity of convergence for BP neural network. Consequently, the model is practical and effective and provides a alternative for forecasting electricity load.",https://ieeexplore.ieee.org/document/4737732/,"2008 International Conference on Information Management, Innovation Management and Industrial Engineering",19-21 Dec. 2008,ieeexplore
10.1109/EIT.2018.8500102,Behavioral Cloning for Lateral Motion Control of Autonomous Vehicles Using Deep Learning,IEEE,Conferences,"Current trend of the automotive industry combined with research by the major tech companies has proved that self-driving vehicles are the future. With successful demonstration of neural network based autonomous driving, NVIDIA has introduced a new paradigm for autonomous driving software. The biggest challenge for self-driving cars is autonomous lateral control. An end-to-end model seems very promising in providing a complete software stack for autonomous driving. Although this system is not ready to be provided as a feature in the market today, it is one of the many steps in the right direction to make self-driving cars a reality. The work described in this paper focusses on how an end-to-end model is implemented. The subtleties of training a successful end-to-end model are highlighted with the aim of providing an insight on deep learning and software required for neural network training. Detailed analyses of data acquisition and training systems are provided and installation procedures for all required tools and software discussed. TORCS is used for developing and testing the end-to-end model. Approximately ten hours of driving data was collected from two different tracks. Using four hours of data from a track, we trained a deep neural network to steer a car inside simulation. Even with such a small training set, the end-to-end model developed demonstrated capabilities to maintain lanes and complete laps in different tracks. For a multilane track, like the one used for training, the model demonstrated an autonomy of 96.62%. For single lane unknown tracks, the model steered the vehicle successfully for 89.02% of the time. The results indicate that end-to-end learning and behavioral cloning can be used to drive autonomously in new and unknown scenarios.",https://ieeexplore.ieee.org/document/8500102/,2018 IEEE International Conference on Electro/Information Technology (EIT),3-5 May 2018,ieeexplore
10.1109/NetSoft48620.2020.9165393,Benchmarking and Profiling 5G Verticals' Applications: An Industrial IoT Use Case,IEEE,Conferences,"The Industry 4.0 sector is evolving in a tremendous pace by introducing a set of industrial automation mechanisms tightly coupled with the exploitation of Internet of Things (IoT), 5G and Artificial Intelligence (AI) technologies. By combining such emerging technologies, interconnected sensors, instruments, and other industrial devices are networked together with industrial applications, formulating the Industrial IoT (IIoT) and aiming to improve the efficiency and reliability of the deployed applications and provide Quality of Service (QoS) guarantees. However, in a 5G era, efficient, reliable and highly performant applications' provision has to be combined with exploitation of capabilities offered by 5G networks. Optimal usage of the available resources has to be realised, while guaranteeing strict QoS requirements such as high data rates, ultra-low latency and jitter. The first step towards this direction is based on the accurate profiling of vertical industries' applications in terms of resources usage, capacity limits and reliability characteristics. To achieve so, in this paper we provide an integrated methodology and approach for benchmarking and profiling 5G vertical industries' applications. This approach covers the realisation of benchmarking experiments and the extraction of insights based on the analysis of the collected data. Such insights are considered the cornerstones for the development of AI models that can lead to optimal infrastructure usage along with assurance of high QoS provision. The detailed approach is applied in a real IIoT use case, leading to profiling of a set of 5G network functions.",https://ieeexplore.ieee.org/document/9165393/,2020 6th IEEE Conference on Network Softwarization (NetSoft),29 June-3 July 2020,ieeexplore
10.1109/ICAIIC51459.2021.9415189,Big Data Platform for Intelligence Industrial IoT Sensor Monitoring System Based on Edge Computing and AI,IEEE,Conferences,"The cutting edge of Industry 4.0 has driven everything to be converted to disruptive innovation and digitalized. This digital revolution is imprinted by modern and advanced technology that takes advantage of Big Data and Artificial Intelligence (AI) to nurture from automatic learning systems, smart city, smart energy, smart factory to the edge computing technology, and so on. To harness an appealing, noteworthy, and leading development in smart manufacturing industry, the modern industrial sciences and technologies such as Big Data, Artificial Intelligence, Internet of things, and Edge Computing have to be integrated cooperatively. Accordingly, a suggestion on the integration is presented in this paper. This proposed paper describes the design and implementation of big data platform for intelligence industrial internet of things sensor monitoring system and conveys a prediction of any upcoming errors beforehand. The architecture design is based on edge computing and artificial intelligence. To extend more precisely, industrial internet of things sensor here is about the condition monitoring sensor data - vibration, temperature, related humidity, and barometric pressure inside facility manufacturing factory.",https://ieeexplore.ieee.org/document/9415189/,2021 International Conference on Artificial Intelligence in Information and Communication (ICAIIC),13-16 April 2021,ieeexplore
10.1109/OCEANS.2018.8604865,Binocular Vision of Fish Swarm Detection in Real-time Based on Deep Learning,IEEE,Conferences,"In the field of ocean development, fish swarm detection has significance for AUV's autonomous navigation and fishing industry. Aim to present fish swarm detections are common based on 2D which lack of spatial information, has low accuracy and bad real-time performance, so we proposed systematic fish swarm detection and position method. We used deep learning target detection system to detect fish and used binocular vision position system to position, then fused every fish's 3D information in camera vision to displayed fish swarm spatial information through radar map. Finally, the contrast experiment and is carried out to verify the effectiveness of the proposed method.",https://ieeexplore.ieee.org/document/8604865/,OCEANS 2018 MTS/IEEE Charleston,22-25 Oct. 2018,ieeexplore
10.1109/CCEM.2018.00025,Bodhisattva - Rapid Deployment of AI on Containers,IEEE,Conferences,"Cloud-based machine learning is becoming increasingly important in all verticals of the industry as all organizations want to leverage ML and AI to solve real-world problems of emerging markets. But, incorporating these services into business solutions is a goliath task, mainly due to the sheer effort necessary to go from development to deployment. We present a novel idea that enables users to easily specify, create, train and rapidly deploy machine learning models through a scalable Machine-Learning-as-a-Service (MLaaS) offering. The MLaaS is provided as an end-to-end microservice suite in a container-based PaaS environment for web applications on the cloud. Our implementation provides an intuitive web-based GUI for tenants to consume these services in a few quick steps. The utility of our service is demonstrated by training ML models for various use cases and comparing them on factors like time-to-deploy, resource usage and training metrics.",https://ieeexplore.ieee.org/document/8648632/,2018 IEEE International Conference on Cloud Computing in Emerging Markets (CCEM),23-24 Nov. 2018,ieeexplore
10.1109/ICFSP.2018.8552045,Building an Automatic Defect Verification System Using Deep Neural Network for PCB Defect Classification,IEEE,Conferences,"In the PCB industry, automatic optical inspection (AOI) system takes an important role to increase yield rate. However, the false alarm rate of AOI equipment is high. Therefore, the high cost of human visual inspection at verify and repair system (VRS) station is becoming a problem. Therefore, we propose an automatic defect verification system, called Auto-VRS, to decrease the false alarm rate and reduce operator's workload. The proposed system is composed of two subsystems, referred to fast circuit comparison and deep neural network based defect classification. The fast circuit comparison is to find the accurate defect region of interest (ROI). The deep neural network based defect classification is to verify which is real defect or pseudo defect. The experiment results showed that the Auto-VRS can recognition defects well and has the significant reduction in both false alarm rate and escape rate. With the advantage of the Auto-VRS, it can further improve the VRS operator's efficiency and accuracy in the future.",https://ieeexplore.ieee.org/document/8552045/,2018 4th International Conference on Frontiers of Signal Processing (ICFSP),24-27 Sept. 2018,ieeexplore
10.1109/TEMSCON-EUR52034.2021.9488593,Business Forum Panel: Perspectives and real-life application of AI in Croatia 20-May 2021 9:00-10:30 CET,IEEE,Conferences,"Organized by IEEE Croatia Section and Technology and Engineering Management Chapter Artificial Intelligence is one of the hottest topics at the moment. In recent years it has been attracting the equal attention of the academia, industry and policy makers. The topic itself is far from new, especially scientifically and engineering wise, but its true deployment raises many questions: technological, legal/ethical, economic. AI is without a doubt a technology for the future, but it is reasonable to ask what are its true perspective and application in a foreseeable future in Croatia and world wide. These and many other questions will be discussed on this round table with our distinguished guests.",https://ieeexplore.ieee.org/document/9488593/,2021 IEEE Technology & Engineering Management Conference - Europe (TEMSCON-EUR),17-20 May 2021,ieeexplore
10.1109/ICPR48806.2021.9413181,CARRADA Dataset: Camera and Automotive Radar with Range- Angle- Doppler Annotations,IEEE,Conferences,"High quality perception is essential for autonomous driving (AD) systems. To reach the accuracy and robustness thatare required by such systems, several types of sensors must be combined. Currently, mostly cameras and laser scanners (lidar) are deployed to build a representation of the world around the vehicle. While radar sensors have been used fora long time in the automotive industry, they are still under-used for AD despite their appealing characteristics (notably, their ability to measure the relative speed of obstacles and to operate even in adverse weather conditions). To alarge extent, this situation is due to the relative lack of automotive datasets with real radar signals that are both raw and annotated. In this work, we introduce CARRADA, a dataset of synchronized camera and radar recordings with range-angle-Doppler annotations. We also present a semi-automatic annotation approach, which was used to annotate the dataset, and a radar semantic segmentation baseline, which we evaluate on several metrics. Both our code and dataset are available online.",https://ieeexplore.ieee.org/document/9413181/,2020 25th International Conference on Pattern Recognition (ICPR),10-15 Jan. 2021,ieeexplore
10.1109/I-SMAC49090.2020.9243440,CNN and Data Augmentation Based Fruit Classification Model,IEEE,Conferences,"Identification of different fruits is still an important and tedious task in the food processing industry. Workers not only have to identify the fruit but also have to make decisions to determine if the fruit is edible or not. Classification of fruits into edible and non-edible classes can be proved as a very important aspect in such industry. In our proposed system four fruits are classified namely, Banana, Papaya, Mango, and Guava into three stages raw, ripe, and over-ripe using Convolutional Neural Networks. In the model, a dataset of local fruits is used and studied their life cycle in different stages. In this, an accuracy of 97.74 % in 8 epochs with 0.9833 validation precision was achieved. The same model can be implemented using real-time images captured using cameras to identify the edibility of fruit which can prove to be helpful for everyone.",https://ieeexplore.ieee.org/document/9243440/,"2020 Fourth International Conference on I-SMAC (IoT in Social, Mobile, Analytics and Cloud) (I-SMAC)",7-9 Oct. 2020,ieeexplore
10.1109/I2MTC.2018.8409769,Classification of eye gestures using machine learning for use in embedded switch controller,IEEE,Conferences,"The classification of signals captured by sampling devices through analysis is a powerful tool with application spanning nearly every industry. Using such classification on real time signals to detect different events or anomalies provides a fast and reliable way of implementing monitoring or control systems. Machine learning classification models include support vector machines (SVM), K-nearest neighbors (KNN), decision trees, and many more. These algorithms separate labelled datasets based on features extracted from the inputs. In the assistive technology field, the use of eye tracking technology provides patients of quadriplegia, ALS, or other neurodegenerative disease with the ability to control speech devices using their eyes. To provide a low-cost alternative to the existing costly devices, an electrooculography (EOG) controller was utilized for obtaining signal data and classifying gestures. A dataset consisting of these gestures was collected over several trials and classified using an SVM, KNN, and decision tree's algorithm with a moving window buffer suitable for an embedded device with peak overall accuracies of 96.8%, 96.9%, and 95.4% respectively. The trained models are converted to C code and uploaded to an ATmega328p AVR microcontroller. Using a decision tree implementation, the intentional blink signal classification is successfully predicted with an accuracy of 97.33% accuracy and filters out unintentional blinks with 100% accuracy on the embedded device.",https://ieeexplore.ieee.org/document/8409769/,2018 IEEE International Instrumentation and Measurement Technology Conference (I2MTC),14-17 May 2018,ieeexplore
10.1109/CCNC49032.2021.9369589,Closed Loop Paging Optimization for Efficient Mobility Management,IEEE,Conferences,"The 4G/5G networks deploy conventional Tracking Area Update and multi-step paging procedures for mobility management. The paging procedure consumes significant amount of licensed spectrum resources. The signaling overhead is going to worsen further with the increasing use of small cells and higher user mobility speed. To address this challenge, the telecommunication industry is embracing closed loop approaches to predict user mobility patterns. Though the mobility pattern prediction is a known problem, most of the existing solutions apply it for enhancing the handover management and use academic dataset. Furthermore, limited research has been done on idle-state users. In this paper, we propose a Closed Loop Paging (CLOP) optimization solution using semi-supervised learning model to reduce paging overhead. We harness the real network dataset to analyze the location trail of users in the network to predict a subset of Base Stations for paging to locate idle-state users. Our empirical results demonstrate that Linear Support Vector Machine (L-SVM) classification method excels when compared to other supervised learning models. The L-SVM Classifier saves nearly 43% of the paging overhead with a marginal increase in connection establishment delay by around 7.3%.",https://ieeexplore.ieee.org/document/9369589/,2021 IEEE 18th Annual Consumer Communications & Networking Conference (CCNC),9-12 Jan. 2021,ieeexplore
10.1109/COINS49042.2020.9191386,Closing the loop: Real-time Error Detection and Correction in automotive production using Edge-/Cloud-Architecture and a CNN,IEEE,Conferences,"One challenge faced by the automotive industry is the shift from combustion to electrically powered vehicles. This change strongly impacts on components such as the electric motor and the battery, and hence on production. In this context, the low level of expert knowledge is especially problematic. To meet these new challenges, this paper introduces a data-driven optimization of the production process by integrating a modular edge and cloud computing layer, and advanced data analysis. Defects are classified by a convolutional neural network (CNN) (predictive analytics) and corrected (depending on the defect type) by an automated rework (prescriptive analytics). The architecture of the CNN achieves an accuracy of 99.21% to predict the defect class. The automated rework process is selected through an implemented decision tree. The edge device communicates with a programmable logic controller (PLC) through a cyber physical interface. As an example of its practical application, the method is applied to hairpin welding of the stator of an electric motor with real production data.",https://ieeexplore.ieee.org/document/9191386/,2020 International Conference on Omni-layer Intelligent Systems (COINS),31 Aug.-2 Sept. 2020,ieeexplore
10.1109/MED.2017.7984310,Cloud computing for big data analytics in the Process Control Industry,IEEE,Conferences,"The aim of this article is to present an example of a novel cloud computing infrastructure for big data analytics in the Process Control Industry. Latest innovations in the field of Process Analyzer Techniques (PAT), big data and wireless technologies have created a new environment in which almost all stages of the industrial process can be recorded and utilized, not only for safety, but also for real time optimization. Based on analysis of historical sensor data, machine learning based optimization models can be developed and deployed in real time closed control loops. However, still the local implementation of those systems requires a huge investment in hardware and software, as a direct result of the big data nature of sensors data being recorded continuously. The current technological advancements in cloud computing for big data processing, open new opportunities for the industry, while acting as an enabler for a significant reduction in costs, making the technology available to plants of all sizes. The main contribution of this article stems from the presentation for a fist time ever of a pilot cloud based architecture for the application of a data driven modeling and optimal control configuration for the field of Process Control. As it will be presented, these developments have been carried in close relationship with the process industry and pave a way for a generalized application of the cloud based approaches, towards the future of Industry 4.0.",https://ieeexplore.ieee.org/document/7984310/,2017 25th Mediterranean Conference on Control and Automation (MED),3-6 July 2017,ieeexplore
10.1109/WFCS.2019.8757952,Cloud-enabled Smart Data Collection in Shop Floor Environments for Industry 4.0,IEEE,Conferences,"Industry 4.0 transition is producing a remarkable change in the Smart Factories management. Modern companies can provide new services following products inside the shop floors along the entire production chain. To achieve the goal of servitization that the Industry 4.0 world requires, a modernization of current production chains is needed. This common demand comes mostly from manufacturing sector, where complex work machines collaborate with human workers. The data produced by the machines must be processed quickly, to allow the implementation of reactive services such as predictive maintenance, and remote control, always taking care of the safety of nearby people. This paper proposes a multi-layer architecture to monitor legacy production machines during their operations inside customers plants. The platform provides near real-time delivery of data collected from the machines with a high grade of customization according to customer needs. The performed tests show the scalability of the platform for a productive ecosystem with many machines at work, confirming its feasibility within different production facilities with different needs.",https://ieeexplore.ieee.org/document/8757952/,2019 15th IEEE International Workshop on Factory Communication Systems (WFCS),27-29 May 2019,ieeexplore
10.1109/WCNC45663.2020.9120761,Collaborative Learning Model for Cyberattack Detection Systems in IoT Industry 4.0,IEEE,Conferences,"Although the development of IoT Industry 4.0 has brought breakthrough achievements in many sectors, e.g., manufacturing, healthcare, and agriculture, it also raises many security issues to human beings due to a huge of emerging cybersecurity threats recently. In this paper, we propose a novel collaborative learning-based intrusion detection system which can be efficiently implemented in IoT Industry 4.0. In the system under consideration, we develop smart “filters” which can be deployed at the IoT gateways to promptly detect and prevent cyberattacks. In particular, each filter uses the collected data in its network to train its cyberattack detection model based on the deep learning algorithm. After that, the trained model will be shared with other IoT gateways to improve the accuracy in detecting intrusions in the whole system. In this way, not only the detection accuracy is improved, but our proposed system also can significantly reduce the information disclosure as well as network traffic in exchanging data among the IoT gateways. Through thorough simulations on real datasets, we show that the performance obtained by our proposed method can outperform those of the conventional machine learning methods.",https://ieeexplore.ieee.org/document/9120761/,2020 IEEE Wireless Communications and Networking Conference (WCNC),25-28 May 2020,ieeexplore
10.1109/iSAI-NLP.2018.8692924,"Comparative Assessment of Indoor Positioning Technologies, Techniques, and Algorithms",IEEE,Conferences,"Indoor positioning systems (IPS) are used to locate the position of objects in indoor environments. Due to its many real-world applications, IPS has garnered interest from both academia and industry. Each IPS is made up of three major components: sensor technology, position-finding technique, and operating algorithm. The goal of this paper is to examine and independently compare different types of components with the aim to understand and make useful suggestions for improvement. This paper also presents the past and current trends in IPS and predict the future trends in approaches to the design and implementation of IPS.",https://ieeexplore.ieee.org/document/8692924/,2018 International Joint Symposium on Artificial Intelligence and Natural Language Processing (iSAI-NLP),15-17 Nov. 2018,ieeexplore
10.1109/ICIT.2002.1189341,Computer based robot training in a virtual environment,IEEE,Conferences,"As more market segments are welcoming automation, the robotic field continues to expand. With the accepted breadth of viable industrial robotic applications increasing, the need for flexible robotic training also grows. In the area of simulation and offline programming there have been innovative developments to Computer Aided Robotics (CAR) Systems. New and notable releases have been introduced to the public, especially among the small, affordable, and easy to use systems. These CAR-Systems are mainly aimed at system integrators in general industry business fields to whom the complex, powerful software tools used by the automotive industry (and its suppliers) are oversized. In general, CAR-Systems are used to design robot cells and to create the offline programs necessary to reduce start-up time and to achieve a considerable degree of planning reliability. Another potential yet to be fully considered, is the use of such CAR-Systems as an inexpensive and user-friendly tool for robotics training. This paper will show the educational potential and possibility inherent in simulation and introduce a successful example of this new method of training. Finally, this presentation should be seen as an attempt to outline novel methods for future education in an industrial environment characterized by the increased occurrence and implementation of the virtual factory.",https://ieeexplore.ieee.org/document/1189341/,"2002 IEEE International Conference on Industrial Technology, 2002. IEEE ICIT '02.",11-14 Dec. 2002,ieeexplore
10.1109/ICIVC.2018.8492883,Construction Model Using Machine Learning Techniques for the Prediction of Rice Produce for Farmers,IEEE,Conferences,"Nowadays, the researches in data mining area have been continuous increasing. Appling data mining to agriculture; for example, the prediction of rice produce for farmers is still challenging. The objective of the research is to propose a model using Machine Learning Techniques comparing between Decision Tree Technique and Neural Network Technique (ANN) for the prediction of rice produce for farmers. Farmers can predict volume of rice produce and selling price. It is helpful for farmers to increase their income. The process of the research follows Cross-industry standard process for data mining (CRISP-DM) process. The model pattern is classified by machine learning techniques experiment with a dataset of farmer records. Performance measure of model pattern uses four options such as Test Options, Cross-Validation Folds 10, Split 80-20, and Use Training Set. After that, four options will be averaged for accuracy. The experimental result shows that the best technique which has highest accuracy can be helpful for farmers in real world.",https://ieeexplore.ieee.org/document/8492883/,"2018 IEEE 3rd International Conference on Image, Vision and Computing (ICIVC)",27-29 June 2018,ieeexplore
10.1109/ICAIS50930.2021.9395800,Construction method of “multi-station integration” operation system based on ubiquitous power Internet of Things,IEEE,Conferences,"Construction method of the modern “multi-station integration” operation system based on the ubiquitous power Internet of Things is designed in this paper. In the continuous development of the Internet of Things technology and the power industry, the ubiquitous power Internet of Things and also the intelligent distribution network can be well integrated, which can completely change the power supply situation of today's power distribution system. In the environment of the ubiquitous power Internet of Things, problems such as the injection of power harmonics, the two-way flow of power flow, and the increase in voltage or frequency are very common. For dealing with this challenge, this paper proposes the novel system. The experiment results have proven the robustness.",https://ieeexplore.ieee.org/document/9395800/,2021 International Conference on Artificial Intelligence and Smart Systems (ICAIS),25-27 March 2021,ieeexplore
10.1109/IGSC51522.2020.9291232,Conversion of an Unsupervised Anomaly Detection System to Spiking Neural Network for Car Hacking Identification,IEEE,Conferences,"Across industry, there is an increasing availability of streaming, time-varying data, where it is important to detect anomalous behavior. These data are found in an enormous number of sensor-based applications, in cybersecurity (where anomalous behavior could indicate an attack), and in finance. Spiking Neural Networks (SNNs) have come under the spotlight for machine learning applications due to the extreme energy efficiency of their implementation on neuromorphic processors like the Intel Loihi research chip. In this paper we explore the applicability of spiking neural networks for in vehicle cyberattack detection. We show exemplary results by converting an autoencoder model to spiking form. We present a learning model comparison that shows the proposed SNN autoencoder outperforms a One Class Support Vector Machine and an Isolation Forest. Furthermore, only a slight reduction in accuracy is observed when compared to a traditional autoencoder.",https://ieeexplore.ieee.org/document/9291232/,2020 11th International Green and Sustainable Computing Workshops (IGSC),19-22 Oct. 2020,ieeexplore
10.1109/EAEEIE.2013.6576535,Cooperation between industry and university based on the evaluation of the industrial research results in the academic environment,IEEE,Conferences,"Based on the European Structural Funds it was developed the Intelligent Mobile Box, Intelligent Panel Controller with intelligent adaptive controllers within the industrial research and experimental development in the company Kybernetes, s.r.o. Within the frame of the academic-industry cooperation, the intelligent adaptive controller was tested at the Department of Cybernetics and Artificial Intelligence, Technical university of Kosice, Slovakia. The tests of the mobile intelligent adaptive controller were performed on two levels of university study, on the Bachelor level on the exercises from the subject “Control of Technological Processes” and on the Engineering level the exercises from the subject “Intelligent Control Networks” and on one Diploma project. Goals of students of the Control of the technological processes course had two goals, firstly to connect the intelligent adaptive controller to pre-defined controlled system (real plant, real model or simulated model) and next to validate the control results. Students of the Diploma project on the Engineering level had more advanced goals. Tasks defined for engineering students were to connect the intelligent adaptive controller to non-defined controlled system, setup the adaptivity process of the controller regarding the learning error, parameterize the control system, observe and validate the control results. Both sides concluded this cooperation as very valuable. Main contributions for students were (U1) the challenge to apply studied theoretical knowledge on the real industrial controllers, (U2) experience with new research results and technologies deployed in industry and (U3) the implementation of the control and adaptive algorithms from abstract mathematical area to real PLC controller. On side of industry research company the main contributions were (C1) testing of designed algorithms and (C2) user feedback from students to make the application HMI interface more understandable a native.",https://ieeexplore.ieee.org/document/6576535/,2013 24th EAEEIE Annual Conference (EAEEIE 2013),30-31 May 2013,ieeexplore
10.1109/IEEM45057.2020.9309978,Creating Transparency in the Finished Vehicles Transportation Process Through the Implementation of a Real-Time Decision Support System,IEEE,Conferences,"The complexity of global distribution networks in the automotive industry and likewise the number of disruptions significantly increased throughout the last years. In order to monitor relevant processes and to optimize decision-making in case of disruptions, a concept for a decision support system (DSS) was introduced. For this purpose, the distribution process weaknesses of the German premium automotive company BMW were identified. The method used was a Failure Mode and Effect Analysis with operational managers and relevant process partners interviews. Based on the findings, performance indicators, thresholds, early warnings and options for action were specified. A big data platform supports the processing of the growing number of relevant data in real-time. In the long-term decision-making can be automated using machine learning algorithms. This paper proves that negative impacts of disruptions can be minimized, and the robustness of the process improved by anticipating and identifying deviations beforehand and in real-time. Hence, companies save money while strengthening customer satisfaction. The DSS can be seen as a necessary precursor of a digital twin.",https://ieeexplore.ieee.org/document/9309978/,2020 IEEE International Conference on Industrial Engineering and Engineering Management (IEEM),14-17 Dec. 2020,ieeexplore
10.1109/ICDM.2011.44,Cross Domain Random Walk for Query Intent Pattern Mining from Search Engine Log,IEEE,Conferences,"Understanding search intents of users through their condensed short queries has attracted much attention both in academia and industry. The search intents of users are generally assumed to be associated with various query patterns, such as ""MobileName price"", where ""MobileName"" could be any named entity of mobile phone model and this pattern indicates that the user intends to buy a mobile phone. However, discovering the query intent patterns for general search is challenging mainly due to the difficulty in collecting sufficient training data for learning query patterns across a large number of searchable domains. In this work, we propose Cross Domain Random Walk (CDRW) algorithm, which is semi-supervised, to discover the query intent patterns across different domains from search engine click-through log data. Starting with some manually tagged seed queries in one or more independent domains, CDRW takes the query patterns as bridge and propagates the transition probability across domains to collect the query intent patterns among different domains based on the assumption that ""users who have similar intent in different but similar domains will have high probability to share similar query patterns across domains"". Different from classical random walk algorithms, CDRW walks across different domains to disseminate the shared knowledge in a transfer learning manner. Extensive experiment results on real log data of a commercial search engine well validate the effectiveness and efficiency of the proposed algorithm.",https://ieeexplore.ieee.org/document/6137226/,2011 IEEE 11th International Conference on Data Mining,11-14 Dec. 2011,ieeexplore
10.1049/cp.2018.1731,Current trend in planning and scheduling of construction project using artificial in telligence,IET,Conferences,"During digitalization of Architecture, Engineering and Construction (AEC) industry, one of the influential technologies, Computer-Aided Design can present the state of these digital technologies that assist practitioners to achieve better work. Similar with CAD, digital project planning and schedule control approaches are still mainly relayed on expert experiences which leads to high expert cost in construction project. Researchers and construction-related project participants develops technologies related to Artificial Intelligence (AI) field to decrease the dependence level of expert in construction planning and schedule control. This paper focuses on 1) teasing out the historical trend in planning and scheduling of construction project of AI technologies; 2) using published construction projects and interview data collected by authors, introducing the current trend of implementation of AI technologies in construction planning and schedule control area. Then, this paper identified gaps of AI technologies adoption between academic perspective and real world in both construction scheduling and planning areas. This paper collected totally 1120 journal papers, by keywords as: Artificial Intelligence, Construction. Then the papers are selected following criterion that after 2000 and only related to relevant research fields. After paper selection, totally 383 papers are analysed to identify the historical trend.",https://ieeexplore.ieee.org/document/8826800/,"IET Doctoral Forum on Biomedical Engineering, Healthcare, Robotics and Artificial Intelligence 2018 (BRAIN 2018)",4-4 Nov. 2018,ieeexplore
10.1109/COINS51742.2021.9524088,Cyber Security Risks of Technical Components in Industry 4.0,IEEE,Conferences,"Industry 4.0 is a new concept of automation data exchange in manufacturing. Industry 4.0 consists of various technical components such as Artificial Intelligence (AI), IoT, OPC UA, and cloud. There could be cyber risks when these technical components are used in Industrial Control System (ICS) without security consideration. Therefore cyber risk assessment, reconsideration of implementation, operation are necessary to adopt Industry 4.0. Problems of previous research for ICS risk assessments are some of them are theoretical and conceptual methods such as simulations. Moreover, they do not focus on Industry 4.0 components.The objective of this research is to prove the concrete cyber risks and introduces corresponding secure implementations for Industry 4.0 components such as Artificial Intelligence (AI), the Internet of Things (IoT), OPC Unified Architecture (OPC UA). The proposed method has benefits for clarifying impacts on the real world by cyber-attacks through penetration tests against ICS testbed with actual machines.",https://ieeexplore.ieee.org/document/9524088/,2021 IEEE International Conference on Omni-Layer Intelligent Systems (COINS),23-25 Aug. 2021,ieeexplore
10.1109/GUCON50781.2021.9573994,Cyber Warfare Threat Categorization on CPS by Dark Web Terrorist,IEEE,Conferences,"The Industrial Internet of Things (IIoT) also referred as Cyber Physical Systems (CPS) as critical elements, expected to play a key role in Industry 4.0 and always been vulnerable to cyber-attacks and vulnerabilities. Terrorists use cyber vulnerability as weapons for mass destruction. The dark web's strong transparency and hard-to-track systems offer a safe haven for criminal activity. On the dark web (DW), there is a wide variety of illicit material that is posted regularly. For supervised training, large-scale web pages are used in traditional DW categorization. However, new study is being hampered by the impossibility of gathering sufficiently illicit DW material and the time spent manually tagging web pages. We suggest a system for accurately classifying criminal activity on the DW in this article. Rather than depending on the vast DW training package, we used authorized regulatory to various types of illicit activity for training Machine Learning (ML) classifiers and get appreciable categorization results. Espionage, Sabotage, Electrical power grid, Propaganda and Economic disruption are the cyber warfare motivations and We choose appropriate data from the open source links for supervised Learning and run a categorization experiment on the illicit material obtained from the actual DW. The results shows that in the experimental setting, using TF-IDF function extraction and a AdaBoost classifier, we were able to achieve an accuracy of 0.942. Our method enables the researchers and System authoritarian agency to verify if their DW corpus includes such illicit activity depending on the applicable rules of the illicit categories they are interested in, allowing them to identify and track possible illicit websites in real time. Because broad training set and expert-supplied seed keywords are not required, this categorization approach offers another option for defining illicit activities on the DW.",https://ieeexplore.ieee.org/document/9573994/,"2021 IEEE 4th International Conference on Computing, Power and Communication Technologies (GUCON)",24-26 Sept. 2021,ieeexplore
10.1109/ITHET.2012.6246060,Cybernetic Education Centre: Monitoring and control of learner's e-learning study in the field of Cybernetics and Automation by Coloured Petri Nets model,IEEE,Conferences,"This paper presents the Cybernetic Education Centre (CEDUC) as a hybrid e-learning and training centre for higher education of Cybernetics and Automation fields. If we consider Cybernetics we consider basically (1) controlled systems and (2) control systems. In case of controlled systems learner is focused on the process of analyze, identification, design of the mathematical model and simulation of the controlled system. Therefore this paper deals with controlled models in the laboratories of our department (a) real laboratory models, (b) simulation models and (c) virtual models which creates one integrated hybrid architecture what represents one of new ideas in the paper (Fig. 1). Learner of control systems is focused mainly on design of control parameters, design of control algorithms, design of hardware, software and communication architectures of control systems. Overall control system is represented by Distributed Control System (DCS) which serves for learners to verify designed control systems. The verification of the control systems is very important from safety point of view to prepare learners for real production conditions. Second new idea of the paper is implementation of the Coloured Petri Nets as automata to control access to the resources (not only typical study resources but also access to the components of hybrid DCS architecture) as well as to monitor the learner activities during the study. CEDUC is supported by intensive industry-university partnership. Conclusion of the paper summarizes the results of the study process of learners in DCS environment.",https://ieeexplore.ieee.org/document/6246060/,2012 International Conference on Information Technology Based Higher Education and Training (ITHET),21-23 June 2012,ieeexplore
10.1109/ICIAI.2019.8850773,Data Augmentation for Intelligent Manufacturing with Generative Adversarial Framework,IEEE,Conferences,"The global economy is greatly shaped by the unprecedented booming of ICT and artificial intelligence technologies. Their applications in manufacturing has led to the advent of intelligent manufacturing and industry 4.0. Data has become a precious asset for modern industry. This paper first introduces an energy monitoring and data acquisition system namely the Point Energy Technology, which has been developed by the team and installed in several industrial partners, including a local bakery. The lack of data always exists due to various reasons, such as measurement or transmission errors at data collection and transmission stage, leading to the loss of varied length of data samples that are key for process monitoring and control. To solve this problem, we introduce a generative adversarial framework which is based on a game theory for data augmentation. This framework consists of two multilayer perceptron networks, namely generator and discriminator. An improved framework with Q-net that extracts the latent variables from real data is also proposed, in which the Q-net shares the structure with discriminator except for the last layer. In addition, the two optimization methods, namely mini-batch gradient descent and adaptive moment estimation are adopted to tune the parameters. To evaluate the performance of these algorithms, energy consumption data collected from a bakery process is used in the experiment. The experimental results confirm that the latent generative adversarial framework with adaptive moment estimation could generate good quality data samples to compensate the random loss of samples in time series data.",https://ieeexplore.ieee.org/document/8850773/,2019 1st International Conference on Industrial Artificial Intelligence (IAI),23-27 July 2019,ieeexplore
10.1109/ECCE44975.2020.9235814,Data-Driven Nonparametric Li-Ion Battery Ageing Model Aiming At Learning From Real Operation Data: Holistic Validation With Ev Driving Profiles,IEEE,Conferences,"Conventional Li-ion battery ageing models require a significant amount of time and experimental resources to provide accurate predictions under realistic operating conditions. Furthermore, there is still an uncertainty on the validity of purely laboratory data-based ageing models for the accurate ageing prediction of battery systems deployed in field.At the same time, there is significant interest from industry in the introduction of new data collection telemetry technology. This implies the forthcoming availability of a significant amount of in-field battery operation data. In this context, the development of ageing models able to learn from in-field battery operation data is an interesting solution to mitigate the need for exhaustive laboratory testing, reduce the development cost of ageing models and at the same time ensure the validity of the model for prediction under real operating conditions.In this paper, a holistic data-driven ageing model developed under the Gaussian Process framework is validated with experimental battery ageing data. Both calendar and cycle ageing are considered, to predict the capacity loss within real EV driving scenarios. The model can learn from the driving data progressively observed, improving continuously its performances and providing more accurate and confident predictions.",https://ieeexplore.ieee.org/document/9235814/,2020 IEEE Energy Conversion Congress and Exposition (ECCE),11-15 Oct. 2020,ieeexplore
10.1109/IJCNN52387.2021.9534128,Data-driven Full-waveform Inversion Surrogate using Conditional Generative Adversarial Networks,IEEE,Conferences,"In the Oil and Gas industry, estimating a subsurface velocity field is an essential step in seismic processing, reservoir characterization, and hydrocarbon volume calculation. Full-waveform inversion (FWI) velocity modeling is an iterative advanced technique that provides an accurate and detailed velocity field model, although at a very high computational cost due to the physics-based numerical simulations required at each FWI iteration. In this study, we propose a method of generating velocity field models, as detailed as those obtained through FWI, using a conditional generative adversarial network (cGAN) with multiple inputs. The primary motivation of this approach is to circumvent the extremely high cost of full-waveform inversion velocity modeling. Real-world data were used to train and test the proposed network architecture, and three evaluation metrics (percent error, structural similarity index measure, and visual analysis) were adopted as quality criteria. Based on these metrics, the results evaluated upon the test set suggest that the GAN was able to accurately match real FWI generated outputs, enabling it to extract from input data the main geological structures and lateral velocity variations. Experimental results indicate that the proposed method, when deployed, has the potential to increase the speed of geophysical reservoir characterization processes, saving on time and computational resources.",https://ieeexplore.ieee.org/document/9534128/,2021 International Joint Conference on Neural Networks (IJCNN),18-22 July 2021,ieeexplore
10.1109/AERO.2018.8396547,Data-driven quality prognostics for automated riveting processes,IEEE,Conferences,"Technologies based in robotics and automatics are reshaping the aerospace industry. Aircraft manufacturers and top-tier suppliers now rely on robotics to perform most of its operational tasks. Over the years, a succession of implemented mobile robots has been developed with the mission of automating important industrial processes such as welding, material handling or assembly procedures. However, despite the progress achieved, a major limitation is that the process still requires human supervision and an extensive quality control process. An approach to address this limitation is to integrate machine learning methods within the quality control process. The idea is to develop algorithms that can direct manufacturing experts towards critical areas requiring human supervision and quality control. In this paper we present an application of machine learning to a concrete industrial problem involving the quality control of a riveting machine. The proposal consists of an intelligent predictive model that can be integrated within the existing real time sensing and pre-processing sub-systems at the equipment level. The framework makes use of several data-driven techniques for pre-processing and feature engineering, combined with the most accurate algorithms, validated through k-folds cross validation technique which also estimates prediction errors. The model is able to classify the manufacturing process of the machine as nominal or anomalous according to a real-world data set of design requirements and operational data. Several machine learning algorithms are compared such as linear regression, nearest neighbor, support vector machines, decision trees, random forests and extreme gradient boost. Results obtained from the case study suggest that the proposed model produces accurate predictions which meet industrial standards.",https://ieeexplore.ieee.org/document/8396547/,2018 IEEE Aerospace Conference,3-10 March 2018,ieeexplore
10.1109/IMCEC51613.2021.9482089,Data-driven scheduling for smart shop floor via reinforcement learning with model-based clustering algorithm,IEEE,Conferences,"Various information technologies provide the manufacturing system massive data, which can support the decision optimization of product lifecycle management. However, the lack of effective use for advanced technologies prevents manufacturing industry from being automated and intelligent. Therefore, this paper proposes the smart shop floor and implementation mechanism. Meanwhile, based on the clustering and reinforcement learning, the brain agent and scheduling agent are designed to determine the approriate rule according to the shop floor state information, thus realizing the data-driven real-time scheduling. Experimental results show that the smart shop floor can effectively deal with disturbance events and has better performance compared with composite dispatching rules.",https://ieeexplore.ieee.org/document/9482089/,"2021 IEEE 4th Advanced Information Management, Communicates, Electronic and Automation Control Conference (IMCEC)",18-20 June 2021,ieeexplore
10.1109/ICIAfS52090.2021.9605823,Deep Learning &amp; Computer Vision for IoT based Intelligent Driver Assistant System,IEEE,Conferences,"With the exponential increment of vehicles, roadside accidents also have been increasing rapidly where approximately 80% of these accidents were caused by human error. Therefore, the automobile industry and the government authorities are more focused on accident prevention by introducing improved road safety systems for the general public. Driver assistance system (DAS) is an intelligent road safety development where it senses the surrounding of a moving vehicle which will assist the driver to avoid the dangers and also warn the drivers of immediate dangers. With the current technological advancements, the automobile industry is equipped with the internet of things (IoT) based data transfer mechanisms, wherewith the concept of ‘connected car’ the passengers and the other interconnected vehicles with the internet can share data with back end applications. The data is consisting of the current location, the distance travelled by the vehicle, whether the vehicle requires urgent service. This study is mainly focused on the development of an intelligent driver assistance system based on computer vision and deep learning, which can prevent accidents with early detection of drowsiness, harmful objects and also by alerting the driver with the signboards and the road lines. The system is capable of passing the emergency messages to the driver as well as the other interconnected vehicles through the website by communicating the real-time road map generated within the system. The proposed system has been implemented and tested with multiple detection scenarios where machine learning has been employed to improve the accuracy of the results.",https://ieeexplore.ieee.org/document/9605823/,2021 10th International Conference on Information and Automation for Sustainability (ICIAfS),11-13 Aug. 2021,ieeexplore
10.1109/IMCEC46724.2019.8984019,Deep Learning: Excellent Method at Surface Defect Detection of Industrial Products,IEEE,Conferences,"Surface defect detection of industrial products has always been an important part of the manufacturing industry. At present,there is a high false detection rate and low efficiency problem of traditional image processing algorithms which easy to be disturbed by complex background. Aiming at the above problems, a method for surface defect detection based on deep learning is proposed. YOLOv3 network adopted in this paper has great advantages in small target recognition and location of target in complex background. In addition, the train-set is effectively extended by elastic deformation and thin-plate spline algorithm. The experiment results show that the scratch recognition rate is as high as 95.8%, the over-judgment rate is 5.4%,and the missed rate is 1.3%.The method can identify the surface defects in a short time, and the average detection time does not exceed 0.4s, which can meet the real-time and precision requirements of industrial applications.",https://ieeexplore.ieee.org/document/8984019/,"2019 IEEE 3rd Advanced Information Management, Communicates, Electronic and Automation Control Conference (IMCEC)",11-13 Oct. 2019,ieeexplore
10.1109/SCC.2017.51,Deep and Shallow Model for Insurance Churn Prediction Service,IEEE,Conferences,"Churn prediction is very important to the insurance industry. Therefore, there is a big value to investigate how to improve its performance. More importantly, a good model can be used by a common service provider and benefit many companies. State-of-the-art methods either use 1) shallow models such as logistic regression, with sophisticated feature engineering, or 2) deep models that learn features and classification models simultaneously. In terms of performance, shallow models can memorize better while deep models can generalize better but may under-generalize with insufficient data. Therefore, we propose a combined Deep &amp;, Shallow model (DSM) to take the strengths of both memorization and generalization in one model by jointly training shallow models and deep models. The experiment results show that for insurance churn prediction, joint training can significantly improve the performance and the DSM earns better performance than both shallow-only and deep-only models. In our real-life dataset, the DSM performs better than CNN, LSTM, Stochastic Gradient Descent, Linear Discriminant Analysis, Quadratic Discriminant Analysis, Gaussian Naive Bayes, AdaBoost, Random Forest, and Gradient Tree Boosting. In addition, the DSM can also be applied to other prediction services.",https://ieeexplore.ieee.org/document/8035004/,2017 IEEE International Conference on Services Computing (SCC),25-30 June 2017,ieeexplore
10.1109/ICCE.2018.8326053,Deep learning networks in CE,IEEE,Conferences,"I could probably offer a half-way answer to “what is the next challenge for CE Imaging?”. Reality is that I don't know for sure (there are many), but I know how to solve it: neural networks. The real question becomes how to enable low cost NN implementations and deployments in CE devices without the fear of losing years of work invested in training and optimizing the networks designed to solve specific problems (sound enhancement, better imaging, understanding of the surroundings, easier cooking, better coffee, etc.). This talk will detail the challenges and industry proven solutions for computer vision and computational imaging using a hybrid traditional imaging and deep learning approach. IP protection against intellectual theft once the solutions are deployed is also briefly discussed.",https://ieeexplore.ieee.org/document/8326053/,2018 IEEE International Conference on Consumer Electronics (ICCE),12-14 Jan. 2018,ieeexplore
10.1109/CBMS.2019.00040,Deep-Learning and HPC to Boost Biomedical Applications for Health (DeepHealth),IEEE,Conferences,"This document introduces the DeepHealth project: ""Deep-Learning and HPC to Boost Biomedical Applications for Health"". This project is funded by the European Commission under the H2020 framework program and aims to reduce the gap between the availability of mature enough AI-solutions and their deployment in real scenarios. Several existing software platforms provided by industrial partners will integrate state-of-the-art machine-learning algorithms and will be used for giving support to doctors in diagnosis, increasing their capabilities and efficiency. The DeepHealth consortium is composed by 21 partners from 9 European countries including hospitals, universities, large industry and SMEs.",https://ieeexplore.ieee.org/document/8787438/,2019 IEEE 32nd International Symposium on Computer-Based Medical Systems (CBMS),5-7 June 2019,ieeexplore
10.1109/SANER.2019.8668044,DeepCT: Tomographic Combinatorial Testing for Deep Learning Systems,IEEE,Conferences,"Deep learning (DL) has achieved remarkable progress over the past decade and has been widely applied to many industry domains. However, the robustness of DL systems recently becomes great concerns, where minor perturbation on the input might cause the DL malfunction. These robustness issues could potentially result in severe consequences when a DL system is deployed to safety-critical applications and hinder the real-world deployment of DL systems. Testing techniques enable the robustness evaluation and vulnerable issue detection of a DL system at an early stage. The main challenge of testing a DL system attributes to the high dimensionality of its inputs and large internal latent feature space, which makes testing each state almost impossible. For traditional software, combinatorial testing (CT) is an effective testing technique to balance the testing exploration effort and defect detection capabilities. In this paper, we perform an exploratory study of CT on DL systems. We propose a set of combinatorial testing criteria specialized for DL systems, as well as a CT coverage guided test generation technique. Our evaluation demonstrates that CT provides a promising avenue for testing DL systems.",https://ieeexplore.ieee.org/document/8668044/,"2019 IEEE 26th International Conference on Software Analysis, Evolution and Reengineering (SANER)",24-27 Feb. 2019,ieeexplore
10.1109/INFOCOM42981.2021.9488784,DeepLoRa: Learning Accurate Path Loss Model for Long Distance Links in LPWAN,IEEE,Conferences,"LoRa (Long Range) is an emerging wireless technology that enables long-distance communication and keeps low power consumption. Therefore, LoRa plays a more and more important role in Low-Power Wide-Area Networks (LPWANs), which easily extend many large-scale Internet of Things (IoT) applications in diverse scenarios (e.g., industry, agriculture, city). In lots of environments where various types of land-covers usually exist, it is challenging to precisely predict a LoRa link's path loss. As a result, how to deploy LoRa gateways to ensure reliable coverage and develop precise fingerprint-based localization becomes a difficult issue in practice. In this paper, we propose DeepLoRa, a deep learning-based approach to accurately estimate the path loss of long-distance links in complex environments. Specifically, DeepLoRa relies on remote sensing to automatically recognize land-cover types along a LoRa link. Then, DeepLoRa utilizes Bi-LSTM (Bidirectional Long Short Term Memory) to develop a land-cover aware path loss model. We implement DeepLoRa and use the data gathered from a real LoRaWAN deployment on campus to evaluate its performance extensively in terms of estimation accuracy and model transferability. The results show that DeepLoRa reduces the estimation error to less than 4 dB, which is 2× smaller than state-of-the-art models.",https://ieeexplore.ieee.org/document/9488784/,IEEE INFOCOM 2021 - IEEE Conference on Computer Communications,10-13 May 2021,ieeexplore
10.1109/DASC-PICom-DataCom-CyberSciTec.2017.200,DeepSim: Cluster Level Behavioral Simulation Model for Deep Learning,IEEE,Conferences,"We are witnessing an explosion of AI based use cases driving the computer industry, and especially datacenter and server architectures. As Intel faces fierce competition in this emerging technology space, it is important that architecture definitions and directions are driven with data from proper tools and methodologies, and insights are drawn from end-to-end holistic analysis at the datacenter levels. In this paper, we introduce DeepSim, a cluster-level behavioral simulation model for deep learning. DeepSim, which is based on the Intel CoFluent simulation framework, uses timed behavioral models to simulate complex interworking between compute nodes, networking, and storage at the datacenter level, providing a realistic performance model of a real-world image recognition applications based on the popular Deep Learning Framework Caffe. The end-to-end simulation data from DeepSim provides insight which can be used for architecture analysis driving future datacenter architecture directions. DeepSim enables scalable system design, deployment, and capacity planning through accurate performance insights. Results from preliminary scaling studies (e.g. node scaling and network scaling) and what-if analyses (e.g., Xeon with HBM and Xeon Phi with dual OPA) are presented in this paper. The simulation results are correlated well with empirical measurements, achieving an accuracy of 95%.",https://ieeexplore.ieee.org/document/8328544/,"2017 IEEE 15th Intl Conf on Dependable, Autonomic and Secure Computing, 15th Intl Conf on Pervasive Intelligence and Computing, 3rd Intl Conf on Big Data Intelligence and Computing and Cyber Science and Technology Congress(DASC/PiCom/DataCom/CyberSciTech)",6-10 Nov. 2017,ieeexplore
10.1109/HPEC.2019.8916576,Deploying AI Frameworks on Secure HPC Systems with Containers.,IEEE,Conferences,"The increasing interest in the usage of Artificial Intelligence (AI) techniques from the research community and industry to tackle “real world” problems, requires High Performance Computing (HPC) resources to efficiently compute and scale complex algorithms across thousands of nodes. Unfortunately, typical data scientists are not familiar with the unique requirements and characteristics of HPC environments. They usually develop their applications with high level scripting languages or frameworks such as TensorFlow and the installation processes often require connection to external systems to download open source software during the build. HPC environments, on the other hand, are often based on closed source applications that incorporate parallel and distributed computing API's such as MPI and OpenMP, while users have restricted administrator privileges, and face security restrictions such as not allowing access to external systems. In this paper we discuss the issues associated with the deployment of AI frameworks in a secure HPC environment and how we successfully deploy AI frameworks on SuperMUC-NG with Charliecloud.",https://ieeexplore.ieee.org/document/8916576/,2019 IEEE High Performance Extreme Computing Conference (HPEC),24-26 Sept. 2019,ieeexplore
10.1109/DSN.2019.00043,Deploying Intrusion-Tolerant SCADA for the Power Grid,IEEE,Conferences,"While there has been considerable research on making power grid Supervisory Control and Data Acquisition (SCADA) systems resilient to attacks, the problem of transitioning these technologies into deployed SCADA systems remains largely unaddressed. We describe our experience and lessons learned in deploying an intrusion-tolerant SCADA system in two realistic environments: a red team experiment in 2017 and a power plant test deployment in 2018. These experiences resulted in technical lessons related to developing an intrusion-tolerant system with a real deployable application, preparing a system for deployment in a hostile environment, and supporting protocol assumptions in that hostile environment. We also discuss some meta-lessons regarding the cultural aspects of transitioning academic research into practice in the power industry.",https://ieeexplore.ieee.org/document/8809554/,2019 49th Annual IEEE/IFIP International Conference on Dependable Systems and Networks (DSN),24-27 June 2019,ieeexplore
10.1109/ICBAIE49996.2020.00085,Design and Implementation of Intelligent Tour Guide System in Large Scenic Area Based on Fog Computing,IEEE,Conferences,"In recent years, the tourism industry has developed rapidly, and there are more and more large-scale tourist parks. There is an urgent need for intelligent guide systems that take tourists as the center and integrate multiple functions into one. Based on the basic requirements of smart tourism, this paper proposes an intelligent guide system based on fog computing. The overall architecture and system functions of the system are designed. The tasks undertaken by each level and the implementation methods of each function are analyzed in detail. This intelligent tour guide system can provide high-quality services such as location-based and real-scenery commentary, intelligent tour guides, program reminders, and consulting exchanges to bring tourists a more satisfactory travel experience.",https://ieeexplore.ieee.org/document/9196285/,"2020 International Conference on Big Data, Artificial Intelligence and Internet of Things Engineering (ICBAIE)",12-14 June 2020,ieeexplore
10.1109/AIID51893.2021.9456521,Design and Implementation of Online Monitoring System for Soil Salinity and Alkalinity in Yangtze River Delta Tideland,IEEE,Conferences,"Soil salinity and alkalinity is an important index concerned by planting industry. In order to meet the demand of long-term observation of soil salinity and alkalinity in precision agriculture and eco-environmental protection, and to solve the current pain points of long sampling period and high cost of soil salinity measurement, this paper designs and implements an online monitoring system for soil salinity alkalinity in tideland in the Yangtze River Delta for crop planting and soil remediation. This system uses solar power supply system and maintenance-free digital sensor, which can be arranged in monitoring area for a long time to collect soil temperature, humidity and salinity data. The collected data can be stored in SD card locally and transmitted to cloud server in real time through 4G network. Up to now, the system has been running stably for nearly two years under the condition of unattended and maintenance free. More than 30000 soil salinity data have been collected from 5 monitoring points, which can be used for long-term observation of the interaction between salinity and plant growth, so as to improve the soil and improve the quality of agriculture products.",https://ieeexplore.ieee.org/document/9456521/,2021 IEEE International Conference on Artificial Intelligence and Industrial Design (AIID),28-30 May 2021,ieeexplore
10.1109/AIID51893.2021.9456537,Design and Implementation of Regional Food Distribution Platform Based on Big Data,IEEE,Conferences,"In recent years, the rapid development of big data has made people's daily life very convenient, and at the same time, tasting all kinds of food has become an important activity for people to travel. Due to the vast territory of China, there are many types of cuisines with great differences, it is very important for travelers to understand the special regional cuisines in the area. This project aggregates regional food data based on big data, and provides tourists with efficient, stable and professional data retrieval and analysis services through a visual data interface, and provides intuitive, accurate, and real-time data support for the decision-making of finding characteristic regional food. This thesis first conducted a relevant understanding of big data and the overall situation of regional cuisine, analyzed the distribution of food in the sub-provincial city of Xi'an, explored the research methods and implementation methods of related projects at home and abroad, based on this, summarized the research of this project the goal. At the same time, the focus of this project is to analyze the price, score and popularity of regional food data analysis in my country, and to summarize, proofread and organize the data obtained before into standard and standardized data. Through the classification of basic information, the visual analysis and display of data is realized, and the key data urgently needed by decision makers are extracted from it. To analyze the needs of decision makers, establish corresponding strategies and measures to improve the quality of data services. The establishment of this system provides a useful supplement and improvement to the existing industry data analysis system. The system is mainly divided into six modules, which are data collection, data review, data summary, and visual data display modules. Among them, data collection includes crawling relevant data from the Internet and retrieving key data. The data audit function includes classifying the crawled data and reviewing its effectiveness. Data aggregation includes summarizing the filtered data in an excel table and passing Excel generates a visual chart that you want to know, and at the same time generates a word cloud by associating certain two related items in the data in the excel table. The visual data display module can more clearly show the results of data analysis to users, so that users can make better decisions. The visualization data uses AmChart Flash charts. The implementation of this system adopts Django Python Web framework, the development language chooses Python, the development tool adopted is PyCharm, and the database tool adopts MySQL.",https://ieeexplore.ieee.org/document/9456537/,2021 IEEE International Conference on Artificial Intelligence and Industrial Design (AIID),28-30 May 2021,ieeexplore
10.1109/ICMLC.2017.8108946,Design and implementation of smoke early warning system based on video analysis technology,IEEE,Conferences,"In order to obtain real-time accurate detection of smoke in the panoramic area, a visual -based smoke early warning system is designed. A 180-degree holographic and high speed camera is equipped as the video capture part of the system, and the video acquisition and target lock functions are implemented. The intelligent recognition function is separated into three steps. Firstly, the suspected smoke area is extracted by Vibe algorithm. Secondly, the Camshift method is used to track the suspected area of moving smoke. Thirdly, the smoke is detected by the proposed fusion dynamic features. The interface of system is developed by the software of QT. The designed system can not only realize sensitive area monitoring function in manual mode, but also the intelligent smoke early warning and detection function are achieved in automatic mode. The experimental results showed the designed system obtained the advantages of simple operation, rapid smoke alarm and the satisfied accuracy for industry requirement.",https://ieeexplore.ieee.org/document/8108946/,2017 International Conference on Machine Learning and Cybernetics (ICMLC),9-12 July 2017,ieeexplore
10.1109/ICRA.2014.6907445,Design of driving fatigue detection system based on hybrid measures using wavelet-packets transform,IEEE,Conferences,"With the rapid development of urbanization and motorization in China, fatigue driving has become an increasingly serious road traffic problem. Driving fatigue affects drivers' alertness, decreasing an individual's ability to operate a vehicle safely and increasing the risk of human error that could lead to fatalities, which have been widely recognized as critical safety issues that cut across all modes in the transportation industry. In this paper, firstly, with a virtual driving system we developed, driving simulation experiments were designed to collect subjects' electroencephalogram (EEG) signals and mental fatigue data. To detect drivers' mental state in real time, wavelet-packets transform (WPT) was selected to extract continuous features; then, the subjective evaluation combined with video monitoring was used to evaluate driver's mental state in experiment accurately. At last, with fatigue feature as the input and fatigue state as the output, driving fatigue detection model can be constructed by classification methods. In this paper, Support Vector Machine (SVM) was used to build driving fatigue detection model to estimate mental fatigue state of EEG signal features, and the binary classification accuracy can be achieved up to 88.6207%.",https://ieeexplore.ieee.org/document/6907445/,2014 IEEE International Conference on Robotics and Automation (ICRA),31 May-7 June 2014,ieeexplore
10.1109/CSCI51800.2020.00019,Detection and Defense from False Data Injection Attacks In Aviation Cyber-Physical Systems Using Artificial Immune Systems,IEEE,Conferences,"In recent years, there has been a rapid expansion in the development of Cyber-Physical Systems (CPS), which allows the physical components and the cyber components of a system to be fully integrated and interacted with each other and with the physical world. The commercial aviation industry is shifting towards Aviation Cyber-Physical Systems (ACPS) framework because it allows real-time monitoring and diagnostics, real-time data analytics, and the use of Artificial Intelligent technologies in decision making. Inevitably, ACPS is not immune to cyber-attacks due to integrating a network system, which introduces serious security threats. False Data Injection (FDI) attack is widely used against CPS. It is a serious threat to the integrity of the connected physical components. In this paper, we propose a novel security algorithm for detecting FDI attacks in the communication network of ACPS using Artificial Immune System (AIS). The algorithm was developed based on the negative selection approach. The negative selection algorithm is used to detect malicious network packets and drop them. Then, a Nonlinear Autoregressive Exogenous (NARX) network is used to predict packets that dropped by the negative selection algorithm. The developed algorithm was implemented and tested on a networked control system of commercial aircraft as an Aviation Cyber-physical system.",https://ieeexplore.ieee.org/document/9457964/,2020 International Conference on Computational Science and Computational Intelligence (CSCI),16-18 Dec. 2020,ieeexplore
10.1109/ICICTA.2009.83,Determination of Real Estate Price Based on Principal Component Analysis and Artificial Neural Networks,IEEE,Conferences,"Real estate industry is both capital-intensive, highly related industries and industries essential to provide the daily necessities. However, the real estate pricing models and methods of research rarely receive the critical attention and development it deserves. This paper utilizes the principal components analysis method of multi-dimensional statistical analysis and artificial neural networks to determine the price of real estate. By using principal component method to process a number of listed real estate pricing indices. Firstly, the index system of accident risk was established. Then principal component analysis was applied to eliminate the indexes having the relativities and overlap information. Finally, based on historical data and artificial neural networks, a new real estate pricing models was established. The experiment results show that this method is effective and precise.",https://ieeexplore.ieee.org/document/5287649/,2009 Second International Conference on Intelligent Computation Technology and Automation,10-11 Oct. 2009,ieeexplore
10.1109/RAAI52226.2021.9508033,Development of Gasoline-Electric Hybrid Propulsion Surveillance and Reconnaissance VTOL UAV,IEEE,Conferences,"Vertical Take-Off and Landing (VTOL) Unmanned Aerial Vehicles (UAV) have been a high potential topic in the aerospace industry during the last decades due to its multirotor and fixed-wing nature of the aircraft. Besides, having the ability to rapidly deploy from a tight airstrip and gathering Intelligence, Surveillance, and Reconnaissance (ISR) information is the best way to be one step ahead of the enemy. In this paper, we present the implementation and development of gasoline-electric hybrid propulsion VTOL Unmanned Aerial vehicle respectively. The Hybrid propulsion VTOL UAV offers image and real-time video transmission to the ground station with fully autonomous control to get the best view of the enemy from the sky. The gasoline-electric hybrid propulsion system provides long flight endurance with efficient power consumption. The fundamentals of the multirotor and the conventional fixed-wing aircraft present the theoretical background of the aircraft. The accomplished design consists of high-performance multirotor motors with an efficient gasoline engine. Furthermore, the control system architecture, avionics, and power distribution system presented with addressing cost-effective trending design techniques. The performance of the system has been improved using commercially off-the-shelf (COTS) hardware.",https://ieeexplore.ieee.org/document/9508033/,"2021 IEEE International Conference on Robotics, Automation and Artificial Intelligence (RAAI)",21-23 April 2021,ieeexplore
10.1109/EDPC51184.2020.9388192,Development of a Cloud- and Edge-Architecture for adaptive model weight optimization of a CNN exemplified by optical detection of hairpin welding,IEEE,Conferences,"The beginning of a global reorientation towards an increasingly conscientious approach to nature and the human habitat has been accompanied by changes in industry and society. The automotive industry, where a transition from combustion to electrically powered vehicles is currently underway, is also concerned with this change. In addition to increasing the capacity of the battery, improving the efficiency of the electric motor is essential. To achieve these goals, however, new technologies such as hairpins for the stator are needed. An important process step involves the welding of two pairs of hairpins, which often leads to welding defects. Nevertheless, expert knowledge in this field is limited. Optical monitoring of the welding process with the help of a convolutional neural network (CNN) is a good approach. This approach can compensate for the low level of expert knowledge and detects and classifies welding defects directly in the production line. However, the disadvantage of optical monitoring is that production conditions and the surrounding environment change over time. This has an impact on optical detection and can negatively affect the accuracy of a CNN. For example, the camera perspective can change, which has a negative effect on optical quality monitoring. Therefore, this paper presents an approach for monitoring and evaluating the quality of a CNN in a cloud instance online. If a deteriorating quality is detected, the CNN in the cloud is re-trained by continuously collected data and then automatically deployed to the production line. This allows the CNN to adapt to the changing environmental conditions. The present approach is demonstrated and validated with real data of the stator production process. Compared with the current state-of-the-art, this control loop is highly automated and requires a minimum of human intervention.",https://ieeexplore.ieee.org/document/9388192/,2020 10th International Electric Drives Production Conference (EDPC),8-9 Dec. 2020,ieeexplore
10.1109/EMBC.2016.7591089,Development of a real time activity monitoring Android application utilizing SmartStep,IEEE,Conferences,"Footwear based activity monitoring systems are becoming popular in academic research as well as consumer industry segments. In our previous work, we had presented developmental aspects of an insole based activity and gait monitoring system-SmartStep, which is a socially acceptable, fully wireless and versatile insole. The present work describes the development of an Android application that captures the SmartStep data wirelessly over Bluetooth Low energy (BLE), computes features on the received data, runs activity classification algorithms and provides real time feedback. The development of activity classification methods was based on the the data from a human study involving 4 participants. Participants were asked to perform activities of sitting, standing, walking, and cycling while they wore SmartStep insole system. Multinomial Logistic Discrimination (MLD) was utilized in the development of machine learning model for activity prediction. The resulting classification model was implemented in an Android Smartphone. The Android application was benchmarked for power consumption and CPU loading. Leave one out cross validation resulted in average accuracy of 96.9% during model training phase. The Android application for real time activity classification was tested on a human subject wearing SmartStep resulting in testing accuracy of 95.4%.",https://ieeexplore.ieee.org/document/7591089/,2016 38th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC),16-20 Aug. 2016,ieeexplore
10.1109/ICIP48927.2020.9367369,Development of simulator for efficient aquaculture of Sillago japonica using reinforcement learning,IEEE,Conferences,"Recently, the situation in the Japanese fishing industry has become critical, resulting in one of the most significant food issues in Japan. Aquaculture technology is expected to be a solution to this problem. Sillago japonica is a fish that inhabits shallow waters in parts of Asia, and large Sillago japonica is very expensive. Therefore, we believe that aquaculture of this fish would help revitalize the fishing industry in Japan. However, aquaculture requires considerable manual labor. Hence, we need to introduce new technologies for the aquaculture of Sillago japonica. Specifically, we have been developing two systems to improve efficiency and reduce costs of this aquaculture: an environment control system and an automatic feeding system. The former is to maintain favorable environment conditions for the fish in the aquaculture tank. The latter is for optimal feeding of the fish. In this paper, we describe the development of an automatic feeding system using artificial intelligence (AI). This system includes four processes: image input, image recognition, feeder control, and feeding action. We have adopted AI technologies to assist in the second and third processes. Although these two processes can be implemented together, it is easier for the AI to learn them as two separate processes. In particular, the second (image recognition) process uses supervised learning, and the third (feeder control) process uses reinforcement learning. However, it is impractical to train the AI in the third process in a real-world aquaculture environment that sustains many failures. Therefore, we have developed an aquaculture simulator to facilitate AI learning of the feeder control process. Additionally, we performed an experiment to validate our simulator using the number of feeders and the number of fish as a parameter.",https://ieeexplore.ieee.org/document/9367369/,2020 International Conference on Image Processing and Robotics (ICIP),6-8 March 2020,ieeexplore
10.1109/OCEANS-Genova.2015.7271691,DexROV: Enabling effective dexterous ROV operations in presence of communication latency,IEEE,Conferences,"Subsea interventions in the oil &amp; gas industry as well as in other domains such as archaeology or geological surveys are demanding and costly activities for which robotic solutions are often deployed in addition or in substitution to human divers - contributing to risks and costs cutting. The operation of ROVs (Remotely Operated Vehicles) nevertheless requires significant off-shore dedicated manpower to handle and operate the robotic platform and the supporting vessel. In order to reduce the footprint of operations, DexROV proposes to implement and evaluate novel operation paradigms with safer, more cost effective and time efficient ROV operations. As a keystone of the proposed approach, manned support will in a large extent be delocalized within an onshore ROV control center, possibly at a large distance from the actual operations, relying on satellite communications. The proposed scheme also makes provision for advanced dexterous manipulation and semi-autonomous capabilities, leveraging human expertise when deemed useful. The outcomes of the project will be integrated and evaluated in a series of tests and evaluation campaigns, culminating with a realistic deep sea (1,300 meters) trial in the Mediterranean sea.",https://ieeexplore.ieee.org/document/7271691/,OCEANS 2015 - Genova,18-21 May 2015,ieeexplore
10.1109/IAI50351.2020.9262203,Digital Twin Enabled Smart Control Engineering as an Industrial AI: A New Framework and Case Study,IEEE,Conferences,"In Industry 4.0, the increasing complexity of industrial systems introduces unknown dynamics that affect the performance of manufacturing processes. Thus, Digital Twin appears as a breaking technology to develop virtual representations of any complex system design, analysis, and behavior prediction tasks to enhance the system understanding via enabling capabilities like real-time analytics, or Smart Control Engineering. In this paper, a novel framework is proposed for the design and implementation of Digital Twin applications to the development of Smart Control Engineering. The framework involve the steps of system documentation, Multidomain Simulation, Behavioral Matching, and real-time monitoring, which is applied to develop the Digital Twin for a real-time vision feedback temperature uniformity control. The obtained results show that Digital Twin is a fundamental part of the transformation into Industry 4.0.",https://ieeexplore.ieee.org/document/9262203/,2020 2nd International Conference on Industrial Artificial Intelligence (IAI),23-25 Oct. 2020,ieeexplore
10.1109/DESSERT.2018.8409186,Digitization of the economy of Ukraine: Strategic challenges and implementation technologies,IEEE,Conferences,"The main directions, challenges, threats of digitization of the national economy of Ukraine have been considered in the paper. The attention is focused on the found weaknesses and the imperfection of the strategy and the state policy of digitization of Ukraine's economy. The authors have proven the potential and new possibilities of solving public finance management problems with the usage of blockchain technology. It has been justified that activation of transformation processes in the real economy sector due to the introduction of Industry 4.0 concept is important for Ukraine. The paper reveals basic principles and technologies, the experience of the European Union, and characterizes Industry 4.0 view in Ukraine. The development of the latest financial technologies - FinTech - has been recognized as the driver of digital transformation of financial services. The types of FinTech innovations, the features of increasing competition between FinTech companies and traditional financial intermediaries, the tendencies of FinTech development in Ukraine have been characterized.",https://ieeexplore.ieee.org/document/8409186/,"2018 IEEE 9th International Conference on Dependable Systems, Services and Technologies (DESSERT)",24-27 May 2018,ieeexplore
10.1109/CCECE.2019.8861718,Distributed Optimal Power Flow for Electric Power Systems with High Penetration of Distributed Energy Resources,IEEE,Conferences,"Optimization technology is developing to the point of becoming a cost-effective enabler of increased power transfer asset utilization. This paper presents a smart decomposition technique for the traditional optimal power flow (OPF) algorithm to allow distributed optimal power flow (DOPF) calculations without relying on a centralized controller. Hence, it develops a feasible distributed architectures for the electric power industry. The proposed method is implemented using Monte Carlo Tree Search based reinforcement learning (MCTS-RL) algorithm. This reduces computational complexity and allows to avoid difficulties associated with stochastic modeling often used to capture the random nature of distributed energy resources (DER) units and loads. The efficiency of the optimization process is improved when the DOPF reflects the fast response capability of the optimal solution. This contribution provides results for a real-time dispatchable resource and demonstrates the flexibility of RL to adapt to changes of system states, ultimately reducing the generation cost while maintaining the system security constraints.",https://ieeexplore.ieee.org/document/8861718/,2019 IEEE Canadian Conference of Electrical and Computer Engineering (CCECE),5-8 May 2019,ieeexplore
10.1109/EHB50910.2020.9280165,Drivers’ Drowsiness Detection and Warning Systems for Critical Infrastructures,IEEE,Conferences,"Road traffic accidents, due to driver fatigue, tend to inflict high mortality rates comparing with accidents involving rested drivers. Currently there is an emerging automotive industry trend towards equipping vehicles with various driver-assistance technologies. Third parties also started producing complementary systems, including ones that can detect the driver's degree of fatigue, but this growing field requires further research and development. The main purpose of this paper is the development and implementation of a system capable to detecting and alert, in real-time, the driver's level of fatigue. A system like this is expected to make the driver aware of the assumed danger when his level of driving and taking decisions are reduced and is indicating a sleep break as the necessary approach. By monitoring the state of the human eyes, it is assumed that the signs of driver fatigue can be detected early enough to prevent a possible road accident, which could result in severe injuries or ultimately, in fatalities. Hence, in this work the authors are focused on the video monitoring of the driver face, especially on his eyes position in time, when open or closed, using a machine learning object detection algorithm, the Haar Cascade. Two pretrained Haar classifiers, a face cascade, and an eye cascade were imported from the OpenCV GitHub repository. The OpenCV library, as well as other required packages, were installed on a BeagleBone Black Wireless development board. The software implementation, in order to achieve the driver's drowsiness detection, was made through the Python software program. The proposed system manages to alert if the eyes of the driver are being kept closed for more than a certain amount of time by triggering a set of warning lights and sounds. The large-scale implementation of this type of system will drop the number of road accidents caused by the drivers' fatigue, thus saving countless lives and bringing a reduction of the socio-economic costs associated with these tragic events.",https://ieeexplore.ieee.org/document/9280165/,2020 International Conference on e-Health and Bioengineering (EHB),29-30 Oct. 2020,ieeexplore
10.1109/BigDataCongress.2017.44,Drowsy Driving Warning System Based on GS1 Standards with Machine Learning,IEEE,Conferences,"Drowsy driving is the primary cause of motor vehicle accidents and is a risk factor that leads to the loss of human life, remaining as a challenge for the global automotive industry. Recently, drowsy monitoring system has been actively studied for prediction system based machine learning. However, the challenges of automotive real-time constraints and flexibility should be taken into consideration against a large amount of heterogeneous data from vehicle network and other device. To solve this problem, we propose drowsy monitoring system based machine learning using GS1 standard. First, vehicle motion data is defined and modeled using the GS1 standard language for drowsy predict. Second, we propose an optimal algorithm selection and detail architecture for automotive real-time environments through machine learning algorithms (KNN, Naïve Bayes, Logistic Regression) and deep learning algorithms (RNN-LSTM). Finally, we describe system-wide integration and implementation through the open source hardware Raspberry Pi and the machine learning SW framework. We provide optimal LSTM architecture and implementation that takes into account the real-time environmental conditions and how to improve the readability and usability of the vehicle motion data. We also share the rapid prototyping methodology case of connected car systems without other sensor devices.",https://ieeexplore.ieee.org/document/8029337/,2017 IEEE International Congress on Big Data (BigData Congress),25-30 June 2017,ieeexplore
10.1109/DSAA53316.2021.9564245,Dynamic Graph Convolutional LSTM application for traffic flow estimation from error-prone measurements: results and transferability analysis,IEEE,Conferences,"The technological advances in the transportation and automotive industry led to the use of new types of sensing systems more cost-effective and adapted to large-scale dense deployment. Those sensing techniques allow continuously gathering traffic measurements times series in different geospatial locations. The accuracy of the obtained raw measurements is often hindered by different factors related to the sensing environment and the sensing process itself and thus fail to capture the short-term traffic variations crucial for real-time traffic monitoring. In this paper, we propose the DGC-LSTM model for area-wide traffic estimation from error-prone measurements time series. The backbone of the DGC-LSTM model is a graph convolutional Long Short Term Memory model with a dynamic adjacency matrix. The adjacency matrix is learned and optimized during the model training. The adjacency matrix values are estimated from the set of contextual features that impact the dynamicity of the dependencies in both the spatial and temporal dimensions. Experiments on a realistic synthetic labelled Bluetooth counts dataset is used for model evaluation. Lastly, we highlight the importance of transfer learning methods to improve the model applicability by ensuring model adaptation to the new deployment site while avoiding the extensive data-labelling effort.",https://ieeexplore.ieee.org/document/9564245/,2021 IEEE 8th International Conference on Data Science and Advanced Analytics (DSAA),6-9 Oct. 2021,ieeexplore
10.1109/CloudNet.2014.6968974,Dynamic allocation and efficient distribution of data among multiple clouds using network coding,IEEE,Conferences,"Distributed storage has attracted large interest lately from both industry and researchers as a flexible, cost-efficient, high performance, and potentially secure solution for geographically distributed data centers, edge caching or sharing storage among users. This paper studies the benefits of random linear network coding to exploit multiple commercially available cloud storage providers simultaneously with the possibility to constantly adapt to changing cloud performance in order to optimize data retrieval times. The main contribution of this paper is a new data distribution mechanisms that cleverly stores and moves data among different clouds in order to optimize performance. Furthermore, we investigate the trade-offs among storage space, reliability and data retrieval speed for our proposed scheme. By means of real-world implementation and measurements using well-known and publicly accessible cloud service providers, we can show close to 9x less network use for the adaptation compared to more conventional dense recoding approaches, while maintaining similar download time performance and the same reliability.",https://ieeexplore.ieee.org/document/6968974/,2014 IEEE 3rd International Conference on Cloud Networking (CloudNet),8-10 Oct. 2014,ieeexplore
10.1109/ICCCNT51525.2021.9580072,Early Detection of Disease in Rice Paddy: A Deep Learning based Convolution Neural Networks Approach,IEEE,Conferences,"The agriculture industry faces huge economic losses due to bacterial, viral or fungal infections in the crops due to which farmers lose 15 to 20% of their total profit every year. India is the second largest producer of rice and a leading exporter of the same in the global market. Thus, early detection of diseases in essential crops is a significant area of research in order to prevent further damage to them. The widespread development of Deep Learning makes it possible to achieve the goal of disease detection in crops. The novelty of this work is early detection of Brown spot disease in rice paddy using Convolution Neural Networks. The area of the disease affected was also found to optimize the usage of fertilizers. This work makes use of Image recognition and preprocessing algorithm based on real time data. Data preprocessing and feature extraction has been done using a self-designed image-processing tool. Tensor flow and Keras framework has been implemented on both training and testing data which was collected manually from rice fields. The proposed model achieved an accuracy of 97.32%.",https://ieeexplore.ieee.org/document/9580072/,2021 12th International Conference on Computing Communication and Networking Technologies (ICCCNT),6-8 July 2021,ieeexplore
10.1109/CSIT49958.2020.9321954,Eco-friendly Home Automation System Implemented Using Machine Learning Algorithms,IEEE,Conferences,"This paper presents the exemplary system of house automation implemented with the use of Industry 4.0 inventions. The proposed system tries to benefit from weather conditions to heat or cool the house without any electrical heaters or air conditioners. It is implemented with the aid of Machine Learning algorithms, the Internet of Things, and Cloud technology. The paper contains a technical and practical description of the system, the results of the real use, and proposed extensions that can improve the presented solution.",https://ieeexplore.ieee.org/document/9321954/,2020 IEEE 15th International Conference on Computer Sciences and Information Technologies (CSIT),23-26 Sept. 2020,ieeexplore
10.1109/CSCloud-EdgeCom49738.2020.00050,Edge Computing-based 3D Pose Estimation and Calibration for Robot Arms,IEEE,Conferences,"Industrial robots are widely used in current production lines, and complex pipeline processes, especially those with different assembly requirements, are designed for intelligent manufacturing in the era of industry 4.0. During the new crown epidemic, a large number of car companies used the production line to transform production of medical materials such as masks and protective clothing, which provided a strong guarantee for fighting the epidemic. In this scenario, a pipeline is often assembled from robotic arms from multiple suppliers. The traditional methods is complex and takes a lot of time. In this paper, we propose a novel deep learning based robot arm 3D pose estimation and calibration model with simple Kinect stereo cameras which can be deployed on light-weight edge computing systems. The light-weight deep CNN model can detection 5 predefined key points based on RGB-D data. In this way, when the assembly line composed of different robot arms needs to be reassembled, our model can quickly provide the robot's pose information without additional tuning processes. Testing in Webots with Rokae xb4 robot arm model shows that our model can quickly estimate the key point of the robot arm.",https://ieeexplore.ieee.org/document/9170983/,2020 7th IEEE International Conference on Cyber Security and Cloud Computing (CSCloud)/2020 6th IEEE International Conference on Edge Computing and Scalable Cloud (EdgeCom),1-3 Aug. 2020,ieeexplore
10.1109/ISAECT50560.2020.9523700,Edge-Cloud Architectures Using UAVs Dedicated To Industrial IoT Monitoring And Control Applications,IEEE,Conferences,"The deployment of new technologies to ease the control and management of a massive data volume and its uncertainty is a very significant challenge in the industry. Under the name ""Smart Factory"", the Industrial Internet of Things (IoT) aims to send data from systems that monitor and control the physical world to data processing systems for which cloud computing has proven to be an important tool to meet processing needs. unmanned aerial vehicles (UAVs) are now being introduced as part of IIoT and can perform important tasks. UAVs are now considered one of the best remote sensing techniques for collecting data over large areas. In the field of fog and edge computing, the IoT gateway connects various objects and sensors to the Internet. It function as a common interface for different networks and support different communication protocols. Edge intelligence is expected to replace Deep Learning (DL) computing in the cloud, providing a variety of distributed, low-latency and reliable intelligent services. In this paper, An unmanned aerial vehicle is automatically integrated into an industrial control system through an IoT gateway platform. Rather than sending photos from the UAV to the cloud for processing, an AI cloud trained model is deployed in the IoT gateway and used to process the taken photos. This model is designed to overcome the latency channels of the cloud computing architecture. The results show that the monitoring and tracking process using advanced computing in the IoT gateway is significantly faster than in the cloud.",https://ieeexplore.ieee.org/document/9523700/,2020 International Symposium on Advanced Electrical and Communication Technologies (ISAECT),25-27 Nov. 2020,ieeexplore
10.1109/AIIoT52608.2021.9454247,"Education System for Bangladesh Using Augmented Reality, Virtual Reality and Artificial Intelligence",IEEE,Conferences,"This paper presents an innovative application for students to study and understand their coursework without any external help from a private tutor. The system uses Augmented Reality (AR) to provide hands on experience for the students. The presented system also supports Virtual Reality (VR) that enriches this process and immerses the users into a fun and productive learning experience. Moreover, the system introduces an industry first Artificial intelligence (AI) based study guide that directs students towards necessary topics and advises them on what to improve on. All the core system features are implemented and are accessible via two mediums. First, a standalone mobile phone application. Second, a dedicated web portal.",https://ieeexplore.ieee.org/document/9454247/,2021 IEEE World AI IoT Congress (AIIoT),10-13 May 2021,ieeexplore
10.1109/ICEETS.2016.7583860,Efficiency optimization of induction motor drive using Artificial Neural Network,IEEE,Conferences,"Induction motors are the workhorse of industry, have good efficiency at rated load, but long duration usage of IM at partial load shows poor efficiency which leads to waste in energy and revenue as well. These motors are reliable, robust, high power/mass ratio and economic, hence replaced all other motors in the industry, so even minute increment in induction motor efficiency can have a major impact on consumption of electricity and saving of revenue, globally. This paper utilizes, a combination of two key concepts of efficiency optimization-loss model control (LMC) and search control (SC) for efficient operation of induction motors used in various industrial applications, in aforesaid load condition. At first, to estimate optimal I<sub>ds</sub> values for various load conditions, an optimal I<sub>ds</sub> expression in terms of machine parameters and load parameters, based on machine loss model in d-q frame along with classical optimization technique, is utilized. Secondly, an offline trained artificial neural network (ANN) controller is used to reproduce the optimal I<sub>ds</sub> values, in run-time load condition. This eliminates run-time computations and perturbation for optimal flux, as in conventional SC method. The (ANN) optimal controller is designed for optimal I<sub>ds</sub> as output, while providing load torque and speed information as inputs. The training is performed in MATLAB and good accuracy of the training model is seen. Dynamic and steady-state performances are compared for proposed optimal (optimal I<sub>ds</sub>) operations and conventional vector operations (constant I<sub>ds</sub>), with the help of a simulation model, developed in MATLAB. Excellent dynamic response in load transients as well as superior efficiency performance (1- 18%) at steady-state, for a wide range of speed and torque in simulation is attained. Assimilated with similar earlier work, the proposed methodology offers effortless implementation in real-time industrial facilities, ripple free operations, fast response and higher energy savings.",https://ieeexplore.ieee.org/document/7583860/,2016 International Conference on Energy Efficient Technologies for Sustainability (ICEETS),7-8 April 2016,ieeexplore
10.1109/CASE48305.2020.9249228,Efficiently Learning a Distributed Control Policy in Cyber-Physical Production Systems Via Simulation Optimization,IEEE,Conferences,"The manufacturing industry is becoming more dynamic than ever. The limitations of non-deterministic network delays and real-time requirements call for decentralized control. For such dynamic and complex systems, learning methods stand out as a transformational technology to have a more flexible control solution. Using simulation for learning enables the description of highly dynamic systems and provides samples without occupying a real facility. However, it requires prohibitively expensive computation. In this paper, we argue that simulation optimization is a powerful tool that can be applied to various simulation-based learning processes for tremendous effects. We proposed an efficient policy learning framework, ROSA (Reinforcement-learning enhanced by Optimal Simulation Allocation), with unprecedented integration of learning, control, and simulation optimization techniques, which can drastically improve the efficiency of policy learning in a cyber-physical system. A proof-of-concept is implemented on a conveyer-switch network, demonstrating how ROSA can be applied for efficient policy learning, with an emphasis on the industrial distributed control system.",https://ieeexplore.ieee.org/document/9249228/,2020 IEEE 16th International Conference on Automation Science and Engineering (CASE),20-21 Aug. 2020,ieeexplore
10.1109/VLSI-DAT.2019.8741637,Embedded Memories for Silicon-In-Package: Optimization of Memory Subsystem from IoT to Machine Learning,IEEE,Conferences,"Traditional memory subsystem consisting of SRAM, DRAM and SSD/HDD has served the needs of electronics industry for decades. With the rapid increase in Graphics, IoT and Machine Learning applications, several new memories have been innovated to optimize the memory hierarchy. Optane memory is deployed to close the performance/area gap between DRAM and SSD. Similarly, embedded DRAM inserted in products as L4 Cache to close the gap between SRAM and DRAM. Lately, MRAM and ReRAM are also brought to reality to target wide range of applications, covering embedded Non-Volatile Memory and Flash buffer at platform level. This talk will go through these innovate memories and how one needs to optimize the memory subsystem for the best application. There's no memory that fits all, but design and architecture opportunities exist for targeted applications.",https://ieeexplore.ieee.org/document/8741637/,"2019 International Symposium on VLSI Design, Automation and Test (VLSI-DAT)",22-25 April 2019,ieeexplore
10.1109/VLSI-TSA.2019.8804633,Embedded Memories for Silicon-ln-Package: Optimization of Memory Subsystem from loT to Machine Learning,IEEE,Conferences,"Traditional memory subsystem consisting of SRAM, DRAM and SSD/HDD has served the needs of electronics industry for decades. With the rapid increase in Graphics, loT and Machine Learning applications, several new memories have been innovated to optimize the memory hierarchy. Optane memory is deployed to close the performance/area gap between DRAM and SSD. Similarly, embedded DRAM inserted in products as L4 Cache to close the gap between SRAM and DRAM. Lately, MRAM and ReRAM are also brought to reality to target wide range ofapplications, covering embedded Non-Volatile Memory and Flash buffer at platform level. This talk will go through these innovate memories and how one needs to optimize the memory subsystem for the best application. There's no memory that fits all, but design and architecture opportunities exist for targeted applications.",https://ieeexplore.ieee.org/document/8804633/,"2019 International Symposium on VLSI Technology, Systems and Application (VLSI-TSA)",22-25 April 2019,ieeexplore
10.1109/BigData.2017.8258076,Empirical evaluations of active learning strategies in legal document review,IEEE,Conferences,"One type of machine learning, text classification, is now regularly applied in the legal matters involving voluminous document populations because it can reduce the time and expense associated with the review of those documents. One form of machine learning - Active Learning - has drawn attention from the legal community because it offers the potential to make the machine learning process even more effective. Active Learning, applied to legal documents, is considered a new technology in the legal domain and is continuously applied to all documents in a legal matter until an insignificant number of relevant documents are left for review. This implementation is slightly different than traditional implementations of Active Learning where the process stops once achieving acceptable model performance. The purpose of this paper is twofold: (i) to question whether Active Learning actually is a superior learning methodology and (ii) to highlight the ways that Active Learning can be most effectively applied to real legal industry data. Unlike other studies, our experiments were performed against large data sets taken from recent, real-world legal matters covering a variety of areas. We conclude that, although these experiments show the Active Learning strategy popularly used in legal document review can quickly identify informative training documents, it becomes less effective over time. In particular, our findings suggest this most popular form of Active Learning in the legal arena, where the highest-scoring documents are selected as training examples, is in fact not the most efficient approach in most instances. Ultimately, a different Active Learning strategy may be best suited to initiate the predictive modeling process but not to continue through the entire document review.",https://ieeexplore.ieee.org/document/8258076/,2017 IEEE International Conference on Big Data (Big Data),11-14 Dec. 2017,ieeexplore
10.1109/EuCNC/6GSummit51104.2021.9482590,Empowering Industry 4.0 and Autonomous Drone Scouting use cases through 5G-DIVE Solution,IEEE,Conferences,"The 5G Edge Intelligence for Vertical Experimentation (5G-DIVE) project aims at demonstrating the technical merits and business value proposition of 5G technologies in two vertical pilots, namely the Industry 4.0 (I4.0) and Autonomous Drones Scout (ADS) pilots. This paper presents an overview of the overall 5G-DIVE solution and reports the results of the initial validation campaign of the selected use case, featuring 5G connectivity, distributed Edge computing, and artificial intelligence. The initial results for the I4.0 provide a baseline for next step validation campaign targeting a broader scale 5G implementation, while the ADS results provides promising results for enhancing the autonomous navigation in real-time.",https://ieeexplore.ieee.org/document/9482590/,2021 Joint European Conference on Networks and Communications & 6G Summit (EuCNC/6G Summit),8-11 June 2021,ieeexplore
10.1109/CCGRID.2017.22,Energy Model for Low-Power Cluster,IEEE,Conferences,"Energy efficiency in high performance computing (HPC) systems is a relevant issue nowadays, which is approached from multiple edges and components (network, I/O, resource management, etc). HPC industry turned its focus towards embedded and low-power computational infrastructures (of RISC architecture processors) to improve energy efficiency, therefore, we use an ARM-based cluster, known as millicluster, designed to achieve high energy efficiency with low power. We provide a model for energy consumption estimation based on experimental data, obtained of measurements performed during a benchmarking process that represents a real-world workload, such as scientific computing algorithms of artificial intelligence. The energy model enables power prediction of tasks in low-power nodes with high accuracy, and its implementation in a job scheduling algorithm of HPC, facilitates the optimization of energy consumption and performance metrics at the same time.",https://ieeexplore.ieee.org/document/7973809/,"2017 17th IEEE/ACM International Symposium on Cluster, Cloud and Grid Computing (CCGRID)",14-17 May 2017,ieeexplore
10.1109/ICE/ITMC49519.2020.9198492,Enhancing Cognition for Digital Twins,IEEE,Conferences,"In the era of Industry 4.0, Digital Twins (DTs) pave the way for the creation of the Cognitive Factory. By virtualizing and twinning information stemming from the real and the digital world, it is now possible to connect all parts of the production process by having virtual copies of physical elements interacting with each other in the digital and physical realms. However, this alone does not imply cognition. Cognition requires modelling not only the physical characteristics but also the behavior of production elements and processes. The latter can be founded upon data-driven models produced via Data Analytics and Machine Learning techniques, giving rise to the so-called Cognitive (Digital) Twin. To further enable the Cognitive Factory, a novel concept, dubbed as Enhanced Cognitive Twin (ECT), is proposed in this paper as a way to introduce advanced cognitive capabilities to the DT artefact that enable supporting decisions, with the end goal to enable DTs to react to inner or outer stimuli. The Enhanced Cognitive Twin can be deployed at different hierarchical levels of the production process, i.e., at sensor-, machine-, process-, employee- or even factory-level, aggregated to allow both horizontal and vertical interplay. The ECT notion is proposed in the context of process industries, where cognition is particularly important due to the continuous, non-linear, and varied nature of the respective production processes.",https://ieeexplore.ieee.org/document/9198492/,"2020 IEEE International Conference on Engineering, Technology and Innovation (ICE/ITMC)",15-17 June 2020,ieeexplore
10.1109/BigData.2018.8622583,Ensemble Machine Learning Systems for the Estimation of Steel Quality Control,IEEE,Conferences,"Recent advances in the steel industry have encountered challenges in soliciting decision making solutions for quality control of products based on data mining techniques. In this paper, we present a steel quality control prediction system encompassing with real-world data as well as comprehensive data analysis results. The core process is cautiously designed as a regression problem, which is then best handled by grouping various learning algorithms with their massive resource of historical production datasets. The characteristics of the currently most popular learning models used in regression problem analysis are as well investigated and compared. The performance indicates our steel quality control prediction system based on ensemble machine learning model can offer promising result whilst delivering high usability for local manufacturers to address the production problem by aid of development of machine learning techniques. Furthermore, real-world deployment of this system is demonstrated and discussed. Finally, future directions and the performance expectation are pointed out.",https://ieeexplore.ieee.org/document/8622583/,2018 IEEE International Conference on Big Data (Big Data),10-13 Dec. 2018,ieeexplore
10.1109/CCWC51732.2021.9375834,Evaluation of Machine Learning Architectures in Healthcare,IEEE,Conferences,"Machine Learning (ML) is now influencing every part of the industry. From detecting objects from an image to recommending different items while doing online shopping based on someone's recent browsing history. Now, the ML is touching the healthcare sector. It is now an area of interest for more doctors and scientists to implement different techniques and harvest the power of ML. Over the past few years, there is a race to implement Artificial Intelligence (AI) and ML in this sector. Multiple scholars have presented their approach. Currently, the proposed models are nowhere near the actual implementation of these models in the real world. However, these models are laying down the path to do so in the future. Here is a review of some of the papers discussing different techniques for targeting various diseases using AI/ML. Each paper introduces a method developed based on the separate datasets created from organically collected data.",https://ieeexplore.ieee.org/document/9375834/,2021 IEEE 11th Annual Computing and Communication Workshop and Conference (CCWC),27-30 Jan. 2021,ieeexplore
10.1109/WSC.1989.718775,Expert Simulation For On-line Scheduling,IEEE,Conferences,"In recent years, the automotive industry has realized the importance of speed of new products to market and has mounted efforts for improving it. The Expert System Scheduler (ESS) facilitates these efforts by enabling manufacturing plants to generate viable schedules under increasing constraints and demands for flexibility. The scheduler takes advantage of the Computer Integrated Manufacturing (CIM) environment by utilizing the real-time information from the factory for responsive scheduling. The Expert System Scheduler uses heuristics developed by an experiences factory scheduler. It uses simulation concepts and these heuristics to generate schedules. Forward and ""backward"" simulation are used at different stages of the schedule generation process. The system is used to control parts flow on the factory floor at one automated facility. This highly automated facility is a testbed for implementation of CIM concepts. The scheduler runs on a Texas Instruments (TI) Explorer II computer using software developed inhouse utilizing IntelliCorp's Knowledge Engineering Environment (KEE) shell and the LISP language. The scheduling computer is networked to the factory control computer, which actually controls the plant floor. The TI Explorer II acquires current plant floor information from the factory control system, generates a new schedule and sends it back within a short time. The configuration allows fast response to changes in requirements and plant floor conditions.",https://ieeexplore.ieee.org/document/718775/,1989 Winter Simulation Conference Proceedings,4-6 Dec. 1989,ieeexplore
10.1109/SMC.2019.8913901,Explainable Machine Learning in Industry 4.0: Evaluating Feature Importance in Anomaly Detection to Enable Root Cause Analysis,IEEE,Conferences,"In the past recent years, Machine Learning methodologies have been applied in countless application areas. In particular, they play a key role in enabling Industry 4.0. However, one of the main obstacles to the diffusion of Machine Learning-based applications is related to the lack of interpretability of most of these methods. In this work, we propose an approach for defining a `feature importance' in Anomaly Detection problems. Anomaly Detection is an important Machine Learning task that has an enormous applicability in industrial scenarios. Indeed, it is extremely relevant for the purpose of quality monitoring. Moreover, it is often the first step towards the design of a Machine Learning-based smart monitoring solution because Anomaly Detection can be implemented without the need of labelled data. The proposed feature importance evaluation approach is designed for Isolation Forest, one of the most commonly used algorithm for Anomaly Detection. The efficacy of the proposed method is tested on synthetic and real industrial datasets.",https://ieeexplore.ieee.org/document/8913901/,"2019 IEEE International Conference on Systems, Man and Cybernetics (SMC)",6-9 Oct. 2019,ieeexplore
10.1109/ISMAR50242.2020.00099,Extracting Velocity-Based User-Tracking Features to Predict Learning Gains in a Virtual Reality Training Application,IEEE,Conferences,"Virtual Reality (VR) for training and education of real-world tasks has been researched extensively and has growing use in industry. The data generated by trainees in VR could be leveraged to improve the ability to evaluate learning beyond that which is possible in traditional training scenarios. In this paper, we present a machine learning approach that is able to classify users into participants with low-learning (LL) and high-learning (HL) gains, based on a knowledge test, using only the linear and angular velocities of the head-mounted display (HMD) and handheld controllers. To collect this data, we conduct a VR training user study. We demonstrate that even with a limited data set, it is possible to train a machine learning classifier to predict a trainee's learning performance for a given task with high degrees of accuracy and confidence. We investigate three different sets of velocity-based input features and two feature representations in a machine learning experiment. Our results indicate that all feature combinations resulted in high degrees of accuracy and confidence for predicting learning gains in our testing data. By employing a novel visualization technique, we were able to determine that participants with HL gains moved with greater velocities and fewer changes in direction than those with LL gains. These results indicate that it may be feasible to create VR training applications that can predict a user's learning gains and dynamically adapt the training to better support the user's learning, based on commonly available tracking data.",https://ieeexplore.ieee.org/document/9284660/,2020 IEEE International Symposium on Mixed and Augmented Reality (ISMAR),9-13 Nov. 2020,ieeexplore
10.1109/EURCON.2007.4400425,Fault Diagnosis of AC Squirrel-cage Asynchronous Motors based on Wavelet Packet-Neural Network,IEEE,Conferences,"Squirrel-cage asynchronous motors are used widely in industry production process. It is significant to improve squirrel-cage asynchronous motors diagnosis technique in application. It helps to decrease the occurrence of accident and reduce the cost of maintenance. Based on the wavelet packet-neural network the scheme on the real-time diagnosis of the stator, bearing, and eccentricity fault of squirrel-cage asynchronous motors is presented in this paper. The electric machines stator current signal is analyzed and processed, with wavelet packet decomposition algorithm. By this method the problem that conventional FFT algorithm can't provide any partial information in time domain is solved. The eigenvectors of bearing and eccentricity fault that processed by wavelet packet decomposition algorithm. The eigenvectors are tiny. The neural networks are not convergent easily when they are trained, with these eigenvectors as above. To this problem, this paper gives a novel scheme based on logarithm fault eigenvectors extraction. By the logarithm fault eigenvectors proposed, the fault of squirrel-cage asynchronous motors can be real time diagnosed and categorized precisely. This scheme is feasible, which is proved by the test experiment.",https://ieeexplore.ieee.org/document/4400425/,"EUROCON 2007 - The International Conference on ""Computer as a Tool""",9-12 Sept. 2007,ieeexplore
10.1109/ICAMechS49982.2020.9310117,Fault detection of tank-system using ChangeFinder and SVM,IEEE,Conferences,"Tank-system is used in the process industry and needs to detect fault early to increase productivity. In recent years, research on fault detection by processing real-time sensing data has been actively conducted. The early fault detection method makes the system dependable. In this paper, a fault detection method using Support Vector Machine (SVM) combined with ChangeFinder, both are machine learning methods, is proposed, and the effectiveness is confirmed by simulation and experiment.",https://ieeexplore.ieee.org/document/9310117/,2020 International Conference on Advanced Mechatronic Systems (ICAMechS),10-13 Dec. 2020,ieeexplore
10.1109/ICS51289.2020.00088,Feature Selection for Malicious Traffic Detection with Machine Learning,IEEE,Conferences,"The network technology plays an important role in the emerging industry 4.0. Industrial control systems (ICS) are related to all aspects of human life and have become the target of cyber-attackers. Attacks on ICS may not only cause economic loss, but also damage equipment and hurt staff. The biggest challenges in establishing a secure network communication system is how to effectively detect and prevent malicious network behavior. A Network Intrusion Detection System (NIDS) can be deployed as a defense mechanism for cyberattacks. However, for industrial internet-of-things (IIoT) applications with limited computing resources, designing an effective NIDS is challenging. In this paper, we propose to use machine learning as the core technology to build a compact and effective NIDS for IIoT. The proposed method is validated by using the more recent UNSW-NB 15 dataset to improve the detection capability against new types of attacks in the real world. Furthermore, we demonstrate that the method is also valid for traditional KDD-CUP-99 dataset. Experimental results show that the proposed method achieves better performance than previous methods.",https://ieeexplore.ieee.org/document/9359069/,2020 International Computer Symposium (ICS),17-19 Dec. 2020,ieeexplore
10.1109/ICCST50977.2020.00021,Food object recognition and intelligent billing system based on Cascade R-CNN,IEEE,Conferences,"With the development of information technology and artificial intelligence, using science and technology to change the low efficiency of the catering industry is a very effective means. The existing system of food identification and intelligent billing in the market includes artificial billing, RFID induction, photo recognition, etc. Based on the Cascade R-CNN algorithm and computer vision technology, this paper proposes an intelligent food identification and intelligent billing system. First, the database is created for algorithm training, then the mobile device is used to collect the food image, and the depth neural network is used to identify the food in the image. Finally, the price calculation result of each is found and returned to the user. In this paper, the basic principle and implementation method of the system are described in detail, and the experimental phenomenon is analyzed. The experimental results show that the system has good accuracy.",https://ieeexplore.ieee.org/document/9262826/,2020 International Conference on Culture-oriented Science & Technology (ICCST),28-31 Oct. 2020,ieeexplore
10.1109/R10-HTC.2018.8629835,Foody - Smart Restaurant Management and Ordering System,IEEE,Conferences,"Customers play a vital role in the contemporary food industry when determining the quality of the restaurant and its food. Restaurants give considerable attention to customers' feedback about their service, since the reputation of the business depends on it. Key factors of evaluating customer satisfaction are, being able to deliver the services effectively to lessen the time of consumption, as well as maintaining a high quality of service. In most cases of selecting a prominent restaurant, customers focus on their choice of favorite food in addition to available seating and space options. Long waiting times and serving the wrong order is a common mistake that happens in every restaurant that eventually leads to customer dissatisfaction. Objectives of this online application “Foody” is to address these deficiencies and provide efficient and accurate services to the customer, by providing unique menus to each customer considering their taste. This concept is implemented as a mobile application using latest IT concepts such as Business Intelligence, Data Mining, Predictive Analysis and Artificial Intelligence. This includes graphics and 3D modeling that provide existent physical information related to food such as colors, sizes and further user can view the ingredients of the meal as well as the available tables. In addition, the app shows the real-time map to the restaurant. Current table reservation status is indicated by the color change of the table. Unique food recommendation and it's order for each customer is generated by analyzing their social media information and the system notifies the customer the wait time by calculating it. Preparation of food and allocation is done subjectively. The expected outcome of the research is to develop a fully automated restaurant management system with the mentioned features as well as to avoid confusions between orders, provide better view of food and allow the customer to choose the menu according to their taste in a minimum time.",https://ieeexplore.ieee.org/document/8629835/,2018 IEEE Region 10 Humanitarian Technology Conference (R10-HTC),6-8 Dec. 2018,ieeexplore
10.1109/AIID51893.2021.9456549,Freight positioning technology of high speed railway carriage based on UWB,IEEE,Conferences,"With the rapid development of railway freight transportation industry, it is imperative for information and intelligent logistics management to replace the traditional logistics management technology. This paper proposes the design of high-speed railway material positioning system based on UWB technology. Firstly, the difficulties of UWB implementation in high-speed railway freight positioning are analyzed. In order to solve the characteristics of nonlinear and non Gaussian distribution of multi-path interference, this paper presents a particle filter ranging calibration algorithm for serious multipath interference in train compartment, which has serious multipath interference, processes 25 sets of ranging data with particle filter. Finally, by arranging 6 base stations in the high-speed railway carriage, the multi-base station weighted least square method is used to locate the information of the material location in the carriage. The feasibility of the system is verified by comparing with the real location.",https://ieeexplore.ieee.org/document/9456549/,2021 IEEE International Conference on Artificial Intelligence and Industrial Design (AIID),28-30 May 2021,ieeexplore
10.1109/ICMLC.2017.8108968,Fully content-based movie recommender system with feature extraction using neural network,IEEE,Conferences,"In recent years, movie industry is getting more and more prosperous. There are hundreds of movies released every year. However, it is difficult to notice the releasing of every movie, not to mention actually seeing it. Therefore, movie recommender system has become more and more popular as a research topic. Among a variety of movie recommender systems, content-based methods always ring a bell when it comes to recommending new movies. Content-based method uses the content of the movie as input so that it does not suffer from the ""cold-start"" problem. In this paper, we propose the Fully Content-based Movie Recommender System (FCMR) to recommend movies to users. The proposed method trains a neural network model, Word2Vec CROW, with content information (e.g., cast, crew, etc.) as the training data to obtain vector form features of each element, and then take advantage of the linear relationship of learned feature to calculate the similarity between each movie. In the end, the proposed FCMR recommends movies based on the similarity. The experiments are conducted on a massive real world dataset, and the intuition behind our proposed method has been proven by the experiment results.",https://ieeexplore.ieee.org/document/8108968/,2017 International Conference on Machine Learning and Cybernetics (ICMLC),9-12 July 2017,ieeexplore
10.1109/CTIT.2018.8649493,Game Theoretic Approach for Applying Artificial Intelligence in the Credit Industry,IEEE,Conferences,"The law of accelerating returns can be viewed as a concept that describes acceleration of technological progress. The idea is that tools are used for developing more advanced tools that are applied for creating even more advanced tools etc. A similar idea has been implemented in algorithms for advancing artificial intelligence. In this paper, the results of applying these algorithms in games are discussed. Nevertheless, real life tasks seem more complicated. The game theoretic approach can be applied for transition from theoretical and unrealistic games to more complex and practical tasks. Applications of the game theoretic approach to advance artificial intelligence in solving tasks in the credit industry are proposed.",https://ieeexplore.ieee.org/document/8649493/,2018 Fifth HCT Information Technology Trends (ITT),28-29 Nov. 2018,ieeexplore
10.1109/WCMEIM52463.2020.00032,Garbage Classification and Recognition Based on SqueezeNet,IEEE,Conferences,"In this study, an intelligent garbage classification and recognition system was deployed on the industry integrated computer with the I3-7100U processor and 2G memory. Considering the unit prediction time and prediction accuracy, SqueezeNet was selected as the classification network training model among ResNet, InceptionV3, and SqueezeNet. The pretraining SqueezeNet network model on the ImageNet-1000 dataset was used for transfer learning, and the model predication accuracy was improved by using image enhancement and Adam optimizer. The comparison between the comprehensive test set and the real garbage image showed that the model predication accuracy reached 87.7% after training, and the prediction time in the industrial integrated machine was less than 2 seconds, which met the needs of practical applications.",https://ieeexplore.ieee.org/document/9409502/,2020 3rd World Conference on Mechanical Engineering and Intelligent Manufacturing (WCMEIM),4-6 Dec. 2020,ieeexplore
10.1109/ICNSC.2006.1673254,General Methodology for Action-Oriented Industrial Ecology Complex Systems Approach Applied to the Rotterdam Industrial Cluster,IEEE,Conferences,"A new approach for the understanding and shaping of the evolution of large scale socio-technical systems is presented. A proof-of-concept knowledge application has been developed, based on the industrial Rotterdam-Rijnmond case. The knowledge application includes the design of a model of industry-infrastructure evolution. Such networks are modeled via a system decomposition, formalization in an ontology and implementation of an agent based model. In simulation runs several network metrics are presented. The results provide insights in real world system behavior and show the validity and potential of the approach",https://ieeexplore.ieee.org/document/1673254/,"2006 IEEE International Conference on Networking, Sensing and Control",23-25 April 2006,ieeexplore
10.1109/AI4I.2018.8665690,Genetic Algorithm Based Parallelization Planning for Legacy Real-Time Embedded Programs,IEEE,Conferences,"Multicore platforms are pervasively deployed in many different sectors of industry. Hence, it is appealing to accelerate the execution through adapting the sequential programs to the underlying architecture to efficiently utilize the hardware resources, e.g., the multi-cores. However, the parallelization of legacy sequential programs remains a grand challenge due to the complexity of the program analysis and dynamics of the runtime environment. This paper focuses on parallelization planning in that the best parallelization candidates would be determined after the parallelism discovery in the target large sequential programs. In this endeavor, a genetic algorithm based method is deployed to help find an optimal solution considering different aspects from the task decomposition to solution evaluation while achieving the maximized speedup. We have experimented the proposed approach on industrial real time embedded application to reveal excellent speedup results.",https://ieeexplore.ieee.org/document/8665690/,2018 First International Conference on Artificial Intelligence for Industries (AI4I),26-28 Sept. 2018,ieeexplore
10.1109/ICCCI.2018.8441350,Genetic Approach based Bug Triage for Sequencing the Instance and Features,IEEE,Conferences,"In software industry analyzing bug by various tester and developer is a costly approach. So collecting these bug reports and triage is done manually which consume time with high rate of error. Here proposed work has focus on this triage of the bug reports by reducing the dataset size. In order to reduce cost of bug triage proper sequencing of the instance and feature selection is done. Here instance and feature selection are clustered by using list of words, keywords and bug id as fitness function parameters. Two stage learning genetic algorithm named as teacher learning based optimization was used for clustering. As genetic algorithms are unsupervised learning approach, so new set bug report triage is adopt by the proposed work. Experiment is done on real dataset of bug reports. Result shows that proposed work is better on precision value by 38.5% while execution time was reduce by 29.2% as compared with existing procedures.",https://ieeexplore.ieee.org/document/8441350/,2018 International Conference on Computer Communication and Informatics (ICCCI),4-6 Jan. 2018,ieeexplore
10.1109/CDC.2004.1428748,Grey-box modelling of a motorcycle shock absorber,IEEE,Conferences,"There is an increasing use of virtual prototyping tools in the motorcycle industry, aimed at reducing the development time of new models and speeding up performance optimization, by providing the designer with an in-laboratory virtual test track. Virtual prototyping software is essentially multi body simulation software that requires the availability of models of all the vehicle components. The choice of the model is then of paramount importance, since it heavily affects the accuracy and reliability of the simulation results. Conventional models (like linear models) are often inadequate to describe the behavior of complex nonlinear components, so that it is necessary to appeal to different modelling approaches. This is actually the case when dealing with motorcycle suspension systems, given that their most critical part, the shock absorber, exhibits nonlinear and time-variant behavior. In this paper, a grey-box model of a racing motorcycle mono tube shock absorber is proposed. It consists of a nonlinear parametric model and a black-box, neural network based model. The absorber model has been implemented in a numerical simulation environment, and it has been validated against experimental test data. The results of the validation show that the model is able to reproduce the real behavior of the shock absorber with an accuracy that matches or even beats that of other models previously presented in the literature. The interfacing of the proposed model to the ADAMS virtual prototyping environment is also discussed.",https://ieeexplore.ieee.org/document/1428748/,2004 43rd IEEE Conference on Decision and Control (CDC) (IEEE Cat. No.04CH37601),14-17 Dec. 2004,ieeexplore
10.1109/CAC51589.2020.9326781,Handwritten Digit Recognition Based on Convolutional Neural Network,IEEE,Conferences,"In order to meet the needs of paperless offices and greatly improve work efficiency, it is necessary to research and implement a handwritten digit recognition system. Handwritten digit recognition plays an important role in large-scale data statistics and the financial business, such as industry annual inspection, population census, tax statements and checks, etc. This paper proposes a new type of handwritten digit recognition system based on convolutional neural network (CNN). In order to improve the recognition performance, the network was trained with a large number of standardized pictures to automatically learn the spatial characteristics of handwritten digits. For model training, according to the loss function, the convolutional neural network continuously updates the network parameters with the data set in MNIST, which contains 60,000 examples. For model test, the system uses the camera to capture the pictures composed of the images generated by the test data set of MNIST and the samples written by different people, then continuously processes the captured graphics and refreshes the output every 0.5 seconds. With the trained deep learning model, we got a recognition accuracy of 97.3% in test process. Good performance in this experiment shows that our system can automatically recognize the handwritten digital content appearing in the target area and output the content label in real time.",https://ieeexplore.ieee.org/document/9326781/,2020 Chinese Automation Congress (CAC),6-8 Nov. 2020,ieeexplore
10.1109/ICSCC51209.2021.9528256,Helmet Detection Using Faster Region-Based Convolutional Neural Networks and Single-Shot MultiBox Detector,IEEE,Conferences,"In a country like India, with excessive population density in all big cities, motorcycles have become dominant modes of transport. It is observed that most motorcyclists avoid wearing helmets despite it being an indispensable safety equipment, whose use can significantly reduce the risk of severe head and brain injuries during accidents. Due to violations of most of the traffic and safety rules, motorcycle accidents have been skyrocketing in the recent years. Hence, it’s the need of the hour to build an effective and scalable system capable of automatic helmet detection by analyzing the surveillance camera’s traffic videos. Although several theoretical deep learning-based models have been proposed to detect helmets for the traffic surveillance aspect, an optimal solution for the industry application is less discussed. This paper demonstrates a novel implementation of the Faster R-CNN and SSD framework for accurate helmet detection in real-time low-quality surveillance videos. The experimental results claim that there is a trade-off between accuracy and execution speed. We also present a comprehensive comparative analysis of the two algorithms and determine the best real-time use case scenarios for each of them.",https://ieeexplore.ieee.org/document/9528256/,2021 8th International Conference on Smart Computing and Communications (ICSCC),1-3 July 2021,ieeexplore
10.1109/ASPDAC.2011.5722294,High performance lithographic hotspot detection using hierarchically refined machine learning,IEEE,Conferences,"Under real and continuously improving manufacturing conditions, lithography hotspot detection faces several key challenges. First, real hotspots become less but harder to fix at post-layout stages; second, false alarm rate must be kept low to avoid excessive and expensive post-processing hotspot removal; third, full chip physical verification and optimization require fast turn-around time. To address these issues, we propose a high performance lithographic hotspot detection flow with ultra-fast speed and high fidelity. It consists of a novel set of hotspot signature definitions and a hierarchically refined detection flow with powerful machine learning kernels, ANN (artificial neural network) and SVM (support vector machine). We have implemented our algorithm with industry-strength engine under real manufacturing conditions in 45nm process, and showed that it significantly outperforms previous state-of-the-art algorithms in hotspot detection false alarm rate (2.4X to 2300X reduction) and simulation run-time (5X to 237X reduction), meanwhile archiving similar or slightly better hotspot detection accuracies. Such high performance lithographic hotspot detection under real manufacturing conditions is especially suitable for guiding lithography friendly physical design.",https://ieeexplore.ieee.org/document/5722294/,16th Asia and South Pacific Design Automation Conference (ASP-DAC 2011),25-28 Jan. 2011,ieeexplore
10.15439/2017F253,Human machine synergies in intra-logistics: Creating a hybrid network for research and technologies,IEEE,Conferences,"The purpose of the article is to outline the futuristic vision of Industry 4.0 in intra-logistics by creating a hybrid network for research and technologies thereby providing a detailed account on the research centre, available technologies and their possibilities for collaboration. Scientific challenges in the field of Industry 4.0 and intra-logistics are identified due to the new form of interaction between humans and machines. This kind of collaboration provides new possibilities of materials handling that can be developed with the support of real-time motion data tracking and virtual reality systems. These services will be provided by a new research centre for flexible human-machine cooperation networks in Dortmund. By the use of various reference and experiment systems various real-time scenarios can be emulated including digital twin simulation concepts. Big data emerges as an important paradigm in this research project where all systems are made flexible in terms of networking for all the systems to consume the data produced and also to combine all the data to arrive at new insights using concepts from machine learning and deep learning networks. This leads to the challenge of finding a common syntax for inter-operating systems. This paper describes the design and deployment strategies of research centre with the possibilities and the design insights for a futuristic Industry 4.0 material handling facility.",https://ieeexplore.ieee.org/document/8104684/,2017 Federated Conference on Computer Science and Information Systems (FedCSIS),3-6 Sept. 2017,ieeexplore
10.1109/SMC42975.2020.9283392,Human-in-the-Loop Error Precursor Detection using Language Translation Modeling of HMI States,IEEE,Conferences,"Situational Awareness (SA) is paramount to ensuring operational safety in Nuclear Power Plant (NPP) and Commercial aviation industry. An increase in Human-in-the-loop (HITL) error rate may be indicative of reduced operator SA while undermining safety. In this paper, natural language processing (NLP) is applied for modelling industrial Human Machine Interface (HMI) state transitions as a means to detect operator HITL error precursors in real-time. A custom seq2seq encode-decoder deep-learning model design is implemented and evaluated using real-plant scenario dataset obtained from a NPP Operator training simulator. Results support NLP HMI state model may be employed to detect HITL error precursor within the desired N time-steps prior to an accident event.",https://ieeexplore.ieee.org/document/9283392/,"2020 IEEE International Conference on Systems, Man, and Cybernetics (SMC)",11-14 Oct. 2020,ieeexplore
10.1109/PerComWorkshops48775.2020.9156250,ID Sequence Analysis for Intrusion Detection in the CAN bus using Long Short Term Memory Networks,IEEE,Conferences,"The number of computer controlled vehicles throughout the world is rising at a staggering speed. Even though this enhances the driving experience, it opens a new security hole in the automotive industry. To alleviate this issue, we are proposing an intrusion detection system (IDS) to the controller area network (CAN), which is the de facto communication standard of present-day vehicles. We implemented an IDS based on the analysis of ID sequences. The IDS uses a trained Long-Short Term Memory (LSTM) to predict an arbitration ID that will appear in the future by looking back to the last 20 packet arbitration IDs. The output from the LSTM network is a softmax probability of all the 42 arbitration IDs in our test car. The softmax probability is used in two approaches for IDS. In the first approach, a single arbitration ID is predicted by taking the class which has the highest softmax probability. This method only gave us an accuracy of 0.6. Applying this result in a real vehicle would give us a lot of false negatives, hence we devised a second approach that uses log loss as an anomaly signal. The evaluated log loss is compared with a predefined threshold to see if the result is in the anomaly boundary. Furthermore, We have tested our approach using insertion, drop and illegal ID attacks which greatly outperform the conventional method with practical F1 scores of 0.9, 0.84, and 1.0 respectively.",https://ieeexplore.ieee.org/document/9156250/,2020 IEEE International Conference on Pervasive Computing and Communications Workshops (PerCom Workshops),23-27 March 2020,ieeexplore
10.1109/CIIMA50553.2020.9290302,Implementación de SCADA a través del protocolo MQTT,IEEE,Conferences,"This document describes an implementation of a SCADA system powered by MQTT &amp; OPC-UA protocols and hosted within the Google Cloud Platform system. This combination allows to have integrated, scalable, secure and reliable industrial communications while allowing real-time data acquisition and sensor feed that can then be used in real-time OEE tracking or predictive maintenance models, to name some examples. This in line with the Industry 4.0 initiatives mainly fueled by data and machine learning autonomous systems.",https://ieeexplore.ieee.org/document/9290302/,2020 IX International Congress of Mechatronics Engineering and Automation (CIIMA),4-6 Nov. 2020,ieeexplore
10.1109/COASE.2007.4341740,Implementation Considerations of Various Virtual Metrology Algorithms,IEEE,Conferences,"In the semiconductor industry, run-to-run (R2R) control is an important technique to improve process capability and further enhance the production yield. As the dimension of electronic device shrinks increasingly, wafer-to-wafer (W2W) advanced process control (APC) becomes essential for critical stages. W2W APC needs to obtain the metrology value of each wafer; however, it will be highly time and cost consuming for obtaining actual metrology value of each wafer by physical measurement. Recently, an efficient and cost-effective approach denoted virtual metrology (VM) was proposed to substitute the actual metrology. To implement VM in W2W APC, both conjecture-accuracy and real-time requirements need to be considered. In this paper, various VM algorithms of back-propagation neural network (BPNN), simple recurrent neural network (SRNN) and multiple regression (MR) are evaluated to see whether they can meet the accuracy and real-time requirements of W2W APC or not. The fifth-generation TFT-LCD CVD process is used to test and verify the requirements. Test results show that both one-hidden-layered BPNN and SRNN VM algorithms can achieve acceptable conjecture accuracy and meet the real-time requirements of semiconductor and TFT-LCD W2W APC applications.",https://ieeexplore.ieee.org/document/4341740/,2007 IEEE International Conference on Automation Science and Engineering,22-25 Sept. 2007,ieeexplore
10.1109/ICC.2016.7511226,Implementation and evaluation of adaptive video streaming based on Markov decision process,IEEE,Conferences,"In HTTP-based adaptive streaming systems, media server simply stores video content segmented into a series of small chunks coded in different qualities and sizes. The decision for next chunk's quality level to achieve a high quality viewing experience is left to the client which is a challenging task, especially in mobile environment due to unexpected changes in network bandwidth. Using computer simulations, previous work has demonstrated that Markov decision process (MDP) is very effective for such decision making and that it can reduce video freezing or re-buffering events drastically compared to other methods of adaptation. However, to date there has been no practical implementation and evaluation of MDP-based DASH players. In this work, we extend a publicly available DASH player recently released by DASH industry forum to realise a real DASH player that implements MDP-based video adaptation. We implement two alternative MDP optimisation algorithms, value iteration and Q learning and evaluate their performances in real driving conditions under 300 minutes of video streaming. Our results show that value iteration and Q learning reduce video freezing by a factor of 8 and 11, respectively, compared to the default decision making algorithm implemented in the public DASH player.",https://ieeexplore.ieee.org/document/7511226/,2016 IEEE International Conference on Communications (ICC),22-27 May 2016,ieeexplore
10.1109/LESCPE.2001.941634,Implementation and performance analysis of very short term load forecaster based on the electronic dispatch project in ISO New England,IEEE,Conferences,"With deregulation in the electric power industry, an accurate very short-term load forecasting (VSTLF) function is increasingly important and becomes an integral component of the real-time (spot) market. VSTLF, dealing with the forecast horizon of one to several minutes ahead, is a relatively new division of the load forecasting which is of special importance in resource dispatch function and correction of area control error (ACE). This paper reports upon the implementation and performance analysis of very short term load forecast (VSTLF) in the electronic dispatch project in ISO New England.",https://ieeexplore.ieee.org/document/941634/,LESCOPE 01. 2001 Large Engineering Systems Conference on Power Engineering. Conference Proceedings. Theme: Powering Beyond 2001 (Cat. No.01ex490),11-13 July 2001,ieeexplore
10.1109/AIMS52415.2021.9466068,Implementation of Cloud Based Action Recognition Backend Platform,IEEE,Conferences,"The Internet of Things (IoT) growth are rapidly in various fields such as industry 4.0, smart cities, and smart homes. Implementation of IoT for electronic assistance had been researched to increase the longevity of human life. However, not all IoT implementation as human life assistance provides action recognition monitoring on multiple elderly people, provide information such as real-time action monitoring, and real-time streaming in a mobile application. Therefore, this research intends to create a system that can receive and provide information on each elderly people who registered. The Action Recognition Backend Platform will be working as cloud computing to receive and manage input data from Edge Computing Action Recognition. This platform integrated Deep Learning, Data Analytics, Big Data Warehouse that implemented Extract, Transform, and Load (ETL) methods, communication services with MQTT, and Kafka Streaming Processor. The test result showed that the edge computing action recognition got better model accuracy performance from our last model [1], which can predict with 50,7% accuracy in 0.5 confidence threshold. Moreover, the backend platform had been successfully implemented a simple IoT paradigm and got an average delivery time of MQTT communication at 204ms, for streaming data process took an average delay of 680ms.",https://ieeexplore.ieee.org/document/9466068/,2021 International Conference on Artificial Intelligence and Mechatronics Systems (AIMS),28-30 April 2021,ieeexplore
10.1109/ICPHYS.2019.8780271,Implementation of Industrial Cyber Physical System: Challenges and Solutions,IEEE,Conferences,"The Industry Internet of Things (IIoT) and the Industry Cyber-Physical System (ICPS) for real industry are becoming vitally necessary in the smart manufacturing environment. Very large number of intelligent sensors are being available generating an exploding amount of data. Several issues come with the big data in real industry, including the a grand-scale connected network construction with the data security and access protocol issues, data quality with considerable noise when gathered from industrial factories, efficient data storage, smart interconnection with cloud services, and real-time analytics requirements. This paper proposes an integrated CPS based architecture for smart manufacturing and provides the deployment details, addressing all the potential problems in an appropriate way. It has been successfully implemented in a real industry environment, and won the Best Industry Application of IoT at the BigInsights Data &amp; AI Innovation Awards.",https://ieeexplore.ieee.org/document/8780271/,2019 IEEE International Conference on Industrial Cyber Physical Systems (ICPS),6-9 May 2019,ieeexplore
10.1109/IES53407.2021.9594001,Implementation of SUMO Simulation for Comparison of CVRP,IEEE,Conferences,"With the rapid increase in human density, development, and mobility in urban areas, the need for logistics distribution systems is increasing which is an important part of connecting industries with their consumers. Thus, route planning is an important thing for the industry. Therefore, this paper proposes a comparison of several vehicle routing problems algorithms and test the routes that have been obtained on a simulation system based on real conditions. Our proposed algorithm consists of Mixed Integer Linear Programming (MILP), Clarke-Wright and Reinforcement Learning algorithm using Markov Decision Process. Digital maps, customer data and route planning results will be converted into a SUMO simulation. We compare the performance of the algorithm with parameters consisting of the number of routes, distance traveled, computation time and simulation time. The experimental results show that the MILP algorithm has the best performance with the most optimal route results, but other algorithms have a lower computation time.",https://ieeexplore.ieee.org/document/9594001/,2021 International Electronics Symposium (IES),29-30 Sept. 2021,ieeexplore
10.1109/ICCCWorkshops52231.2021.9538856,Implementation of an intelligent target detection system for edge node,IEEE,Conferences,"With the rapid development of the artificial intelligence industry, the application of object detection technology in real life is becoming increasingly widespread, such as intelligent monitoring, autonomous driving, and augmented reality. In this paper, object detection system is deployed on edge devices which is low complexity and low-cost device such as the Raspberry Pi. Implement Mobilenet-SSD based on the deep learning framework and deploy on edge devices, such as image acquisition, object detection and result display. The results show that the object detection technology can also be achieved on devices with scarce computing resources such as the Raspberry Pi and satisfies actual business requirements.",https://ieeexplore.ieee.org/document/9538856/,2021 IEEE/CIC International Conference on Communications in China (ICCC Workshops),28-30 July 2021,ieeexplore
10.1109/ICCE-TW46550.2019.8991771,Implementation of ransomware prediction system based on weighted-KNN and real-time isolation architecture on SDN Networks,IEEE,Conferences,"In May 2017, hackers used the ransomware WannaCry to launch large-scale attacks on 150 countries, affecting every industry. Therefore, detection and control of the ransomware virus has become an important issue for security experts in recent years. Recently, machine learning, deep learning, and artificial intelligence technologies have become increasingly mature. Many companies (such as Google) have introduced software-defined networking (SDN) to replace the original network architecture, traffic routing, and network configuration control management. Therefore, this paper proposes a ransomware prediction system based on weighted-K-Nearest-Neighbor. This system includes the detection and prediction of ransomware packet traffic and design and the implementation of a dynamic isolation system integrated SDN. The experimental results show that the precision of detecting normal flow and abnormal flow is 99.7 and 97.7, respectively.",https://ieeexplore.ieee.org/document/8991771/,2019 IEEE International Conference on Consumer Electronics - Taiwan (ICCE-TW),20-22 May 2019,ieeexplore
10.1109/RTC.2012.6418168,Implementation of the disruption predictor APODIS in JET real time network using the MARTe framework,IEEE,Conferences,"The evolution in the past years of Machine learning techniques, as well as the technological evolution of computer architectures and operating systems, are enabling new approaches for complex problems in different areas of industry and research, where a classical approach is nonviable due to lack of knowledge of the problem's nature. A typical example of this situation is the prediction of plasma disruptions in Tokamak devices. This paper shows the implementation of a real time disruption predictor. The predictor is based on a support vector machine (SVM). The implementation was done under the MARTe framework on a six core x86 architecture. The system is connected in JET's Real time Data Network (RTDN). Online results show a high degree of successful predictions and a low rate of false alarms thus, confirming its usefulness in a disruption mitigation scheme. The implementation shows a low computational load, which in an immediate future will be exploited to increase the prediction's temporal resolution.",https://ieeexplore.ieee.org/document/6418168/,2012 18th IEEE-NPSS Real Time Conference,9-15 June 2012,ieeexplore
10.1109/ICAML48257.2019.00042,Importance of Cloud Deployment Model and Security Issues of Software as a Service (SaaS) for Cloud Computing,IEEE,Conferences,"Cloud computing, now-a-days has become the jargon in the industry of IT. It is a model that gives worldwide access to shared pools of configurable resources over the internet. That means it run several applications or programs at the same time on more than one computer. The nature and usage of the cloud computing are developing rapidly both virtually and in reality. It is making things easier for the user of internet by many of its attractive features but these features, have not just only challenged the existing security system, but have also revealed new security issues. In this paper the deployment of cloud models are discussed . The cloud models are deployed as public cloud, private cloud and hybrid cloud. Each of these models have their own advantages and disadvantages. Cloud also provides some services like software as a service (SaaS), Platform as a service (PaaS), Infrastructure as a service (IaaS) . This paper is an insightful analysis of the features as well as the securities challenges of Software as a Service (SaaS) for cloud computing.",https://ieeexplore.ieee.org/document/8989204/,2019 International Conference on Applied Machine Learning (ICAML),25-26 May 2019,ieeexplore
10.1109/ICIT52682.2021.9491636,Improvement of personal loans granting methods in banks using machine learning methods and approaches in Palestine,IEEE,Conferences,"For banking organizations, loan approval and risk assessment which is related is a very complex and significant process which is needs a high effort for relevant employee or manager to take a decision, because of manual or traditional methods that used in banks. The banking industry still needs a more precise method of predictive modeling for several problems. In general, for financial institutions and especially for banks forecasting credit defaulters is a hard challenge. The primary role of the current systems is to accept, or sending loan application to a specific level of approval to be studied and it is very difficult to foresee the probability of the borrower for paying the due dues amount without using methods to predict. Machine learning (ML) techniques and the algorithm that belongs to are a very amazing and promising technique in predicting for a large amount of data. Our research proposed to study three machine learning algorithms [1], Decision Tree (DT), Logistic Regression (LR), and Random Forest (RF), by using real data collected from Quds Bank with a variables that cover credit restriction and regulator instructions. The algorithm has been implemented to predict the loan approval of customers and the output tested in terms of the predicted accuracy.",https://ieeexplore.ieee.org/document/9491636/,2021 International Conference on Information Technology (ICIT),14-15 July 2021,ieeexplore
10.1109/COMPSAC.2018.10204,Indoor Augmented Reality Using Deep Learning for Industry 4.0 Smart Factories,IEEE,Conferences,"This paper proposes to design, develop and implement a fast and markerless mobile augmented reality system to achieve the registration for, the visualization of, and the interaction with machines in indoor smart factories with Industry 4.0 vision. A lightweight deep-learning image detection module based on MobileNets running on mobile devices is used to detect/recognize different machines and different portions of machines. Internet of Things (IoT) networking allows machines and sensors in machines to report data, such as machine settings and machine states, to the cloud-side server. Thus, augmented information associated with a machine portion can be derived from the server and superimposed with the portion image shown on the device display. Furthermore, interaction methods based on touch gestures and distance calculation are also implemented. A prototype system is developed and tested in a mechanical workshop for the purpose of validation and evaluation. The system is shown to achieve high detection accuracy, intuitive visualization, and unique interaction modes.",https://ieeexplore.ieee.org/document/8377831/,2018 IEEE 42nd Annual Computer Software and Applications Conference (COMPSAC),23-27 July 2018,ieeexplore
10.1109/GIOTS49054.2020.9119497,Industrial IoT and Digital Twins for a Smart Factory : An open source toolkit for application design and benchmarking,IEEE,Conferences,"The rapid evolution of digital technology and designed intelligence, such as the Internet of Things (IoT), Big data analytics, Artificial Intelligence (AI), Cyber Physical Systems (CPS), has been a catalyst for the 4th industrial revolution (known as industry 4.0). Among other, the two key state-of-the-art concepts in Industry 4.0, are Industrial IoT (IIoT) and digital twins. IIoT facilitates real-time data acquisition, processing and analytics over large amount of sensor data streams produced by sensors installed within a smart factory, while the `digital twin' concept aims to enable smart factories via the digital replication or representation of physical machines, processes, people in cyber-space. This paper explores the capability of present-state open-source platforms to collectively achieve digital twin capabilities, including IoT real-time data acquisition, virtual representation, analytics, and visualisation. The aim of this work is to `close the gap' between research and implementation, through a collective open source IoT and Digital Twin architecture. The performance of the open-source architecture in this work, is demonstrated in a use-case utilising industry `open data', and is bench-marked with universal testing tools.",https://ieeexplore.ieee.org/document/9119497/,2020 Global Internet of Things Summit (GIoTS),3-3 June 2020,ieeexplore
10.23919/ECC.2003.7085250,Inferential sensor for the olive oil industry,IEEE,Conferences,"This paper shows an inferential sensor that has been developed to be used in the olive oil industry. This sensor has been designed to measure two variables that appear in the elaboration of olive oil in a mill which are very difficult to be measured on line by a physical sensor. The knowledge of these variables on line is crucial for the optimal operation of the process, since they provide the state of the plant, allowing the development of a control strategy that can improve the quality and yield of the product. This sensor measures variables that in other case should come form laboratory analysis with large processing delays or from very expensive and difficult to use on line analysers. The sensor has been devised based upon artificial Neural Networks (NN) and has been implemented as a routine running on a Programmable Logic Controller (PLC) and successfully tested on a real plant.",https://ieeexplore.ieee.org/document/7085250/,2003 European Control Conference (ECC),1-4 Sept. 2003,ieeexplore
10.1109/FUZZY.2006.1681917,Intelligent Constant Current Control for Resistance Spot Welding,IEEE,Conferences,"Resistance spot welding is one of the primary means of joining sheet metal in the automotive industry and other industries. The demand for improved corrosion resistance has led the automotive industry to increasingly use zinc coated steel in auto body construction. One of the major concerns associated with welding coated steel is the mushrooming effect (the increase in the electrode diameter due to deposition of copper into the spot surface) resulting in reduced current density and undersized welds (cold welds). The most common approach to this problem is based on the use of simple unconditional incremental algorithms (steppers) for preprogrammed current scheduling. In this paper, an intelligent algorithm is proposed for adjusting the amount of current to compensate for the electrodes degradation. The algorithm works as a fuzzy logic controller using a set of engineering rules with fuzzy predicates that dynamically adapt the secondary current to the state of the weld process. The state is identified by indirectly estimating two of the main process characteristics - weld quality and expulsion rate. A soft sensor for indirect estimation of the weld quality employing a learning vector quantization (LVQ) type classifier is designed to provide a real time approximate assessment of the weld nugget diameter. Another soft sensing algorithm is applied to predict the impact of changes in current on the expulsion rate of the weld process. By maintaining the expulsion rate just below a minimal acceptable level, robust process control performance and satisfactory weld quality are achieved. The intelligent constant current control for resistance spot welding is implemented and validated on a medium frequency direct current (MFDC) constant current weld controller. Results demonstrate a substantial improvement of weld quality and reduction of process variability due to the proposed new control algorithm.",https://ieeexplore.ieee.org/document/1681917/,2006 IEEE International Conference on Fuzzy Systems,16-21 July 2006,ieeexplore
10.1109/RTSI.2019.8895598,Intelligent Embedded Load Detection at the Edge on Industry 4.0 Powertrains Applications,IEEE,Conferences,"In the context of Industry 4.0, there has been great focus in developing intelligent sensors. Deploying them, condition monitoring and predictive maintenance have become feasible solutions to minimize operating and maintenance costs while also increasing safety. A critical aspect is the applied load to the supervised machinery system. Vibration data can be used to determine the current condition, but this needs signal processing specially developed and adapted to the monitored machine part for feature extraction. Artificial intelligence (AI) can, on one hand, simplify the development of such special purpose processing and on another hand be used to monitor and classify machine conditions by learning features directly from data. By bringing the AI computation as close as possible to the sensor (Edge-AI), data bandwidth can be minimized, system scalability and responsiveness can be improved and real-time requirements can be fulfilled. This work describes how Edge-AI on a STM32-bit microcontroller can be implemented. Our experimental setup demonstrates how AI can be effectively used to detect and classify the load on a powertrain. In order to do this, we use a MEMS capacity accelerometer to sense vibrations of the system. Also, this work demonstrates how Deep Neural Networks (DNN) for signal classification are build and trained by using an open-source deep learning framework and how the code library for the microcontroller is automatically generated afterwards by using STM32Cube. AI toolchain. We compare the classification accuracy of a memory compressed DNN against an uncompressed DNN.",https://ieeexplore.ieee.org/document/8895598/,2019 IEEE 5th International forum on Research and Technology for Society and Industry (RTSI),9-12 Sept. 2019,ieeexplore
10.1109/ITST.2007.4295849,Intelligent Fleet Management System with Concurrent GPS &amp; GSM Real-Time Positioning Technology,IEEE,Conferences,"Fleet management system is a rapid growing industry. This system helps institutions to manage vehicle fleet efficiently and effectively through smart allocation of resources. In this project, an intelligent fleet management system which incorporates the power of concurrent Global Positioning System (GPS) and Global System for Mobile Communications (GSM) real-time positioning, front-end intelligent and web-based management software is proposed. In contrast to systems which depend solely on GPS positioning, the proposed system provides higher positioning accuracy and is capable to track the target at areas where GPS signals are weak or unavailable. The terminal is powered by Front-End Intelligent Technology (FEI), a comprehensive embedded technology that is equipped with necessary artificial intelligence to mimic human intelligence in decision-making for quicker response, better accuracy and less dependence on a backend server. With less dependency on the backend, large scale fleet management system can be implemented more effectively. The proposed system is successfully implemented and evaluated on twenty vehicles including buses and cars in Universiti Teknologi Malaysia (UTM). Results from the test-bed shown that user can monitor and track the real-time physical location and conditions of their vehicles via Internet or Short Message Service (SMS). The web-based fleet management software also helped the user to manage fleets more effectively.",https://ieeexplore.ieee.org/document/4295849/,2007 7th International Conference on ITS Telecommunications,6-8 June 2007,ieeexplore
10.1109/ARIS50834.2020.9205772,Intelligent Robot for Worker Safety Surveillance: Deep Learning Perception and Visual Navigation,IEEE,Conferences,"The fatal injury rate for the construction industry is higher than the average for all industries. Recently, researchers have shown an increased interest in occupational safety in the construction industry. However, all the current methods using conventional machine learning with stationary cameras suffer from some severe limitations, perceptual aliasing (e.g., different places/objects can appear identical), occlusion (e.g., place/object appearance changes between visits), seasonal / illumination changes, significant viewpoint changes, etc. This paper proposes a perception module using end-to-end deep-learning and visual SLAM (Simultaneous Localization and Mapping) for an effective and efficient object recognition and navigation using a differential-drive mobile robot. Various deep-learning frameworks and visual navigation strategies with evaluation metrics are implemented and validated for the selection of the best model. The deep-learning model's predictions are evaluated via the metrics (model speed, accuracy, complexity, precision, recall, P-R curve, F1 score). The YOLOv3 shows the best trade-off among all algorithms, 57.9% mean average precision (mAP), in real-world settings, and can process 45 frames per second (FPS) on NVIDIA Jetson TX2 which makes it suitable for real-time detection, as well as a right candidate for deploying the neural network on a mobile robot. The evaluation metrics used for the comparison of laser SLAM are Root Mean Square Error (RMSE). The Google Cartographer SLAM shows the lowest RMSE and acceptable processing time. The experimental results demonstrate that the perception module can meet the requirements of head protection criteria in Occupational Safety and Health Administration (OSHA) standards for construction. To be more precise, this module can effectively detect construction worker's non-hardhat-use in different construction site conditions and can facilitate improved safety inspection and supervision.",https://ieeexplore.ieee.org/document/9205772/,2020 International Conference on Advanced Robotics and Intelligent Systems (ARIS),19-21 Aug. 2020,ieeexplore
10.1109/ICIT.2006.372422,Intelligent Tuned PID Controllers for PMSM Drive - A Critical Analysis,IEEE,Conferences,"This paper presents a critical analysis of intelligent tuned PID controllers for the permanent magnet synchronous motor drive. The PID proportional-integral-derivative (PID) controller is the most popular controller in process industry. The PID algorithm is simple, reliable and robust for the control of first and second order processes and even high order processes with well damped modes. In this paper, PID control strategies, based on fuzzy logic, neural network and genetic algorithms are reviewed and implemented. A mathematical model of the drive is developed to study steady state and transient performance of current regulated voltage source inverter (VSI) fed PMSM in the field oriented mode under different conditions. The drive includes PID controller, vector control structure, the inverter and the machine. Different tuning algorithms and their effect on dynamic performance of PMSM drive in the real time frame are illustrated in terms of starting and speed reversal time, steady state error in speed, overshoot, performance parameters and speed and torque ripple. Good agreements between different methods has been observed and presented.",https://ieeexplore.ieee.org/document/4237744/,2006 IEEE International Conference on Industrial Technology,15-17 Dec. 2006,ieeexplore
10.1109/AIM46487.2021.9517377,Introducing adaptive mechatronic designs in bulk handling industry,IEEE,Conferences,"The advances of mechatronic system design and system integration have shown improvements in functionality, performance and energy efficiency in many applications across industries, from autonomous ground vehicles and drones to conveyor belts. This trend has been adopted in some industries more than others. The design of equipment to handle granular or bulk material is commonly based on traditional approaches. Therefore, introducing mechatronic concepts in the design procedure can enable new possibilities, such as sensor integration and data analyses, adaptability and control. The efficiency of bulk material handling equipment in ports, agriculture and food processing is heavily influenced by the operational conditions. Typically, a piece of equipment is designed for defined operational conditions when the maximum performance can be achieved. In this work the concept of adaptability to varying operational conditions is explored by understanding the technologies implemented in other industries and the feasibility to be implemented in the bulk handling equipment design. Sensing technology, actuation and adaptability are systematically presented in this work to support the design process of the next generation of bulk handling equipment. This will pave the way for incorporating the technological trends in the design, such as: sustainability, “smartness”, Internet of Things, Industry 4.0, digital twin and machine learning. Adaptive mechatronic solutions will play a crucial role in generating and implementing innovative sustainable solutions for bulk handling equipment.",https://ieeexplore.ieee.org/document/9517377/,2021 IEEE/ASME International Conference on Advanced Intelligent Mechatronics (AIM),12-16 July 2021,ieeexplore
10.1109/ICIAICT.2019.8784859,IoT-NDN: An IoT Architecture via Named Data Netwoking (NDN),IEEE,Conferences,"Internet of Things (IoT) systems have become the central part of future internet research. In the IoT, heterogeneous devices are connected to sense the environment or to observe individual tasks. Many research fields use the seamless IoT infrastructure to interact with the integrated devices and diverse services. Furthermore, IoT is a promising technology to increase the comfort and quality of life and opens new ways of interaction between people and things. Real life applications in healthcare sectors, home automation, industry, smart cities, monitoring scenarios, etc. benefit from the low-cost wireless technology in IoT on one hand. On the contrary, IoT system has many challenging features: many devices are resource-constrained with energy and memory, are highly heterogeneous and their applications continuously transmit transient information. Furthermore, Requesting, delivering and updating the information in IoT are challenging because of the resource limitation. Named Data Networking (NDN) is one of the latest and the most important Information-Centric Networking (ICN) approaches which uses named data to deliver data in the network. Based on hierarchically structured names, NDN matches the application pattern of IoT systems and uses its communication concept to optimize the power supply and distribute the data efficiently in the network. This paper discusses the main concepts of NDN including naming, routing, forwarding and caching in IoT infrastructure. To achieve an efficient system in different applications scenario in future IoT, an IoT architecture is proposed via NDN called IoT-NDN. The objective of this research work is the design and development of IoT-NDN for different fields in IoT systems. The deployment of IoT-NDN is challenging and requires proper design choices. The proposed solution and challenges of all mentioned issues are discussed in this paper.",https://ieeexplore.ieee.org/document/8784859/,"2019 IEEE International Conference on Industry 4.0, Artificial Intelligence, and Communications Technology (IAICT)",1-3 July 2019,ieeexplore
10.1109/FMEC49853.2020.9144776,IoT-WLAN Proximity Network for Potentiostats,IEEE,Conferences,"The implementation of potentiostats as portable and communicated devices has reached significant progress to benefit research, industry, and education. The Internet of Things (IoT) is a good opportunity to interconnect devices such as the potentiostats together with electronics, communication technologies, and chemistry into a single system. This work proposes a network for potentiostats using machine-to-machine (M2M) protocols, modifying its functioning mechanism in the broker to check the payload of the message that passes through it and synchronize the sensors depending on its content. Although one sensor can be synchronized directly to another, the broker decides which sensor to pair. This modification was made in the M2M protocol algorithm, both in the Broker and in the Client (sensor). In addition to this, the network uses an interconnection architecture of IoT smart networks of proximity with centralized management. The results of the tests carried out showed that the use of a modified M2M such as the one proposed in the architecture allows synchronization and comparison of the measurements of several sensors in real-time.",https://ieeexplore.ieee.org/document/9144776/,2020 Fifth International Conference on Fog and Mobile Edge Computing (FMEC),20-23 April 2020,ieeexplore
10.1109/IGSC48788.2019.8957164,IoT/CPS Ecosystem for Efficient Electricity Consumption : Invited Paper,IEEE,Conferences,"Modern society relies on smart systems like internet of things (IoT) and cyber physical systems (CPS) to monitor and control physical processes. The widespread deployment of IoT and CPSs result in fast growth of sensor data as physical processes are constantly monitored by billions of IP-enabled sensors (44 zettabytes by 2020). Hence, fog nodes are deployed to make network edge rich in computing resources to enable real-time data analytics using artificial intelligence/machine learning (AI/ML) for Big data generated from IoT and CPSs. This paper proposes IoT/CPS ecosystem for smart grid (SG) utilizing industry 4.0 concept to manage and control the loads using an intelligent predictive controller based on artificial neural network (ANN). The ANN is trained to predict the loads in certain districts based on previous smart meter readings installed at consumers and substations. This is a novel approach which integrates IoT/CPSs ecosystem into electric power system to deliver energy to consumers with high efficiency, reduce the cost, optimize the energy consumption, improve the reliability and enable real-time monitoring of power consumption.",https://ieeexplore.ieee.org/document/8957164/,2019 Tenth International Green and Sustainable Computing Conference (IGSC),21-24 Oct. 2019,ieeexplore
10.1109/SNPD.2017.8022721,Issues with conducting controlled on-line experiments for E-Commerce,IEEE,Conferences,"More and more on-line experiments have been done in E-Commerce in order to understand the behavior of users or customers and then apply the data analysis technique to provide business guidance. One of the techniques is A/B testing. However, there is not clear guidance on the sample size in order for us to have valuable, trustable discovery. The purpose of this work is to find out a way to group customers in the data sample in order to achieve an optimal difference between the buckets. Based on the analysis result of real data collected during joining an industry project, we think the problem is complex and the meaningful conclusions have to be drawn with caution from business experiments such as A/B testing, due to the vast variation in the data. Moreover, if we don't allocate enough samples in the treatment group, the experiment could be inconclusive even if the testing lasts for a longer enough time, such as one month.",https://ieeexplore.ieee.org/document/8022721/,"2017 18th IEEE/ACIS International Conference on Software Engineering, Artificial Intelligence, Networking and Parallel/Distributed Computing (SNPD)",26-28 June 2017,ieeexplore
10.1109/PerComWorkshops51409.2021.9431114,Keynote: Explainable-by-design Deep Learning,IEEE,Conferences,"MACHINE and AI justifiably attract the attention and interest not only of the wider scientific community and industry, but also society and policy makers. However, even the most powerful (in terms of accuracy) algorithms such as deep learning (DL) can give a wrong output, which may be fatal. Due to the opaque and cumbersome model structure used by DL, some authors started to talk about a dystopian “black box” society. Despite the success in this area, the way computers learn is still principally different from the way people acquire new knowledge, recognise objects and make decisions. People do not need a huge amount of annotated data. They learn by example, using similarities to previously acquired prototypes, not by using parametric analytical models. Current ML approaches are focused primarily on accuracy and overlook explainability, the semantic meaning of the internal model representation, reasoning and its link with the problem domain. They also overlook the efforts to collect and label training data and rely on assumptions about the data distribution that are often not satisfied. The ability to detect the unseen and unexpected and start learning this new class/es in real time with no or very little supervision is critically important and is something that no currently existing classifier can offer. The challenge is to fill this gap between high level of accuracy and the semantically meaningful solutions. The most efficient algorithms that have fuelled interest towards ML and AI recently are also computationally very hungry - they require specific hardware accelerators such as GPU, huge amounts of labeled data and time. They produce parametrised models with hundreds of millions of coefficients, which are also impossible to interpret or be manipulated by a human. Once trained, such models are inflexible to new knowledge. They cannot dynamically evolve their internal structure to start recognising new classes. They are good only for what they were originally trained for. They also lack robustness, formal guarantees about their behaviour and explanatory and normative transparency. This makes problematic use of such algorithms in high stake complex problems such as aviation, health, bailing from jail, etc. where the clear rationale for a particular decision is very important and the errors are very costly. All these challenges and identified gaps require a dramatic paradigm shift and a radical new approach. In this talk the speaker will present such a new approach towards the next generation of computationally lean ML and AI algorithms that can learn in real-time using normal CPUs on computers, laptops, smartphones or even be implemented on chip that will change dramatically the way these new technologies are being applied. It is explainable-by-design. It focuses on addressing the open research challenge of developing highly efficient, accurate ML algorithms and AI models that are transparent, interpretable, explainable and fair by design. Such systems are able to self-learn lifelong, and continuously improve without the need for complete retraining, can start learning from few training data samples, explore the data space, detect and learn from unseen data patterns, collaborate with humans or other such algorithms seamlessly.",https://ieeexplore.ieee.org/document/9431114/,2021 IEEE International Conference on Pervasive Computing and Communications Workshops and other Affiliated Events (PerCom Workshops),22-26 March 2021,ieeexplore
10.1109/SMC.2017.8122711,Knowledge extracted from recurrent deep belief network for real time deterministic control,IEEE,Conferences,"Recently, the market on deep learning including not only software but also hardware is developing rapidly. Big data is collected through IoT devices and the industry world will analyze them to improve their manufacturing process. Deep Learning has the hierarchical network architecture to represent the complicated features of input patterns. Although deep learning can show the high capability of classification, prediction, and so on, the implementation on GPU devices are required. We may meet the trade-off between the higher precision by deep learning and the higher cost with GPU devices. We can success the knowledge extraction from the trained deep learning with high classification capability. The knowledge that can realize faster inference of pre-trained deep network is extracted as IF-THEN rules from the network signal flow given input data. Some experiment results with benchmark tests for time series data sets showed the effectiveness of our proposed method related to the computational speed.",https://ieeexplore.ieee.org/document/8122711/,"2017 IEEE International Conference on Systems, Man, and Cybernetics (SMC)",5-8 Oct. 2017,ieeexplore
10.1109/SC41405.2020.00025,Kraken: Memory-Efficient Continual Learning for Large-Scale Real-Time Recommendations,IEEE,Conferences,"Modern recommendation systems in industry often use deep learning (DL) models that achieve better model accuracy with more data and model parameters. However, current opensource DL frameworks, such as TensorFlow and PyTorch, show relatively low scalability on training recommendation models with terabytes of parameters. To efficiently learn large-scale recommendation models from data streams that generate hundreds of terabytes training data daily, we introduce a continual learning system called Kraken. Kraken contains a special parameter server implementation that dynamically adapts to the rapidly changing set of sparse features for the continual training and serving of recommendation models. Kraken provides a sparsity-aware training system that uses different learning optimizers for dense and sparse parameters to reduce memory overhead. Extensive experiments using real-world datasets confirm the effectiveness and scalability of Kraken. Kraken can benefit the accuracy of recommendation tasks with the same memory resources, or trisect the memory usage while keeping model performance.",https://ieeexplore.ieee.org/document/9355295/,"SC20: International Conference for High Performance Computing, Networking, Storage and Analysis",9-19 Nov. 2020,ieeexplore
10.1109/CarpathianCC.2017.7970404,Landing area recognition by image applied to an autonomous control landing of VTOL aircraft,IEEE,Conferences,"The pattern recognition aims to classify objects on different categories based on characteristics analysis. The usage of pattern recognition shows itself more and more frequent and widely used, covering different areas both in industry and research and development of new technologies. With that in mind, this work aims to compare two nonlinear classifiers, the Adaptive Boosting method and the Artificial Neural Network method, applied to the identification of a certain landmark, where the more profitable is inserted in a Vertical Take-Off and Landing (VTOL) aircraft real model to trigger the land action after a demanded mission in the trained pattern presence. It is used as sensing method, computer vision technique, from camera's acquired images the characteristics are extracted by a proceeding based on Viola-Jones technique. To optimize the classification, it is also used the Principal Component Analysis method to uncouple the amount of data in the training stage and optimize the results in both classifiers. To prove the efficiency of the classifier when the aircraft is flying, it is used to test a scenario where it is possible to simulate the landing action with different altitudes. The Adaptive Boosting method proved itself to be more advantageous due to its simple implementation and less computational processing effort, despite the slightly lower performance when it comes to classifying compared to the Artificial Neural Network. The Principal Component Analysis method also shows itself to be a good improvement when applied to both techniques, raising the success rate of the classifiers in all the tested cases. The results obtained in the simulation tests were considered satisfactory as the aircraft lands with great precision over the determined landmark after identifying the landing area used for training.",https://ieeexplore.ieee.org/document/7970404/,2017 18th International Carpathian Control Conference (ICCC),28-31 May 2017,ieeexplore
10.1109/ISGT.2016.7781159,Large-scale detection of non-technical losses in imbalanced data sets,IEEE,Conferences,"Non-technical losses (NTL) such as electricity theft cause significant harm to our economies, as in some countries they may range up to 40% of the total electricity distributed. Detecting NTLs requires costly on-site inspections. Accurate prediction of NTLs for customers using machine learning is therefore crucial. To date, related research largely ignore that the two classes of regular and non-regular customers are highly imbalanced, that NTL proportions may change and mostly consider small data sets, often not allowing to deploy the results in production. In this paper, we present a comprehensive approach to assess three NTL detection models for different NTL proportions in large real world data sets of 100Ks of customers: Boolean rules, fuzzy logic and Support Vector Machine. This work has resulted in appreciable results that are about to be deployed in a leading industry solution. We believe that the considerations and observations made in this contribution are necessary for future smart meter research in order to report their effectiveness on imbalanced and large real world data sets.",https://ieeexplore.ieee.org/document/7781159/,2016 IEEE Power & Energy Society Innovative Smart Grid Technologies Conference (ISGT),6-9 Sept. 2016,ieeexplore
10.1109/NICS.2018.8606862,Lead Engagement by Automated Real Estate Chatbot,IEEE,Conferences,"Recently, automated chatbot has been increasingly applied in real estate industry. Even though chatbots cannot fully replace the traditional relation between agents and home buyers, they can help to engage potential clients (or leads) in meaningful conversations, which is highly useful for lead capture. In this paper, we present an intelligent chatbot for this purpose. Various machine learning techniques, including multi-task deep learning technique for intent identification and frequent itemsets for conversation elaboration, have been employed in our system. Our chatbot has been deployed by CEO K35 GROUP JSC with daily updated data of real estate information at Hanoi and Ho Chi Minh cities, Vietnam.",https://ieeexplore.ieee.org/document/8606862/,2018 5th NAFOSTED Conference on Information and Computer Science (NICS),23-24 Nov. 2018,ieeexplore
10.1109/VPPC49601.2020.9330855,Learning Based Energy Management Strategy Offline Trainers Comparison for Plug-in Hybrid Electric Buses,IEEE,Conferences,"The automotive industry is facing a transformation towards the massive digitalization and data-acquisition of the vehicles operation. The exploitation of operational data opens up new opportunities in the energy efficiency improvement of the vehicles. In this regard, the combination of optimization techniques with neural networks and fuzzy systems in one unified framework, known as learning-based energy management strategies, have been identified as promising methods. These learning-based techniques combine the optimized operation with the IF-THEN human-type reasoning simplicity of a fuzzy system through neural-type of learning. Therefore, fuzzy-neural networks are the bridge that allows to learn offline from the optimal operation and design energy management strategy for real time implementation. In this regard, the main contribution of this paper lies on the comparison of a previously developed ANFIS approach with a simpler Neo-Fuzzy neuron based, with the aim to evaluate the tradeoff between accuracy and computational and structural efficiency. The proposed approach represents a fuzzy-neural structure with less parameters for training that is expected to facilitate its future real time application for energy management strategies for each bus from a fleet operating on a predefined route.",https://ieeexplore.ieee.org/document/9330855/,2020 IEEE Vehicle Power and Propulsion Conference (VPPC),18 Nov.-16 Dec. 2020,ieeexplore
10.1109/ICDE51399.2021.00283,Learning to Optimize Industry-Scale Dynamic Pickup and Delivery Problems,IEEE,Conferences,"The Dynamic Pickup and Delivery Problem (DPDP) is aimed at dynamically scheduling vehicles among multiple sites in order to minimize the cost when delivery orders are not known a priori. Although DPDP plays an important role in modern logistics and supply chain management, state-of-the-art DPDP algorithms are still limited on their solution quality and efficiency. In practice, they fail to provide a scalable solution as the numbers of vehicles and sites become large. In this paper, we propose a data-driven approach, Spatial-Temporal Aided Double Deep Graph Network (ST-DDGN), to solve industry-scale DPDP. In our method, the delivery demands are first forecast using spatial-temporal prediction method, which guides the neural network to perceive spatial-temporal distribution of delivery demand when dispatching vehicles. Besides, the relationships of individuals such as vehicles are modelled by establishing a graph-based value function. ST-DDGN incorporates attention-based graph embedding with Double DQN (DDQN). As such, it can make the inference across vehicles more efficiently compared with traditional methods. Our method is entirely data driven and thus adaptive, i.e., the relational representation of adjacent vehicles can be learned and corrected by ST-DDGN from data periodically. We have conducted extensive experiments over real-world data to evaluate our solution. The results show that ST-DDGN reduces 11.27% number of the used vehicles and decreases 13.12% total transportation cost on average over the strong baselines, including the heuristic algorithm deployed in our UAT (User Acceptance Test) environment and a variety of vanilla DRL methods. We are due to fully deploy our solution into our online logistics system and it is estimated that millions of USD logistics cost can be saved per year.",https://ieeexplore.ieee.org/document/9458860/,2021 IEEE 37th International Conference on Data Engineering (ICDE),19-22 April 2021,ieeexplore
10.1109/ISMAR-Adjunct54149.2021.00036,Learning to Perceive: Perceptual Resolution Enhancement for VR Display with Efficient Neural Network Processing,IEEE,Conferences,"Even though the Virtual Reality (VR) industry is experiencing a rapid growth with ever-expanding demands today, VR applications have yet to provide a fully immersive experience. The insufficient resolution of the VR head-mounted display (HMD) hinders the user from further immersion into the virtual world. In this work, we attempt to enhance the immersive experience by improving the perceptual resolution of VR HMDs. We employ an efficient neural-network-based approach with the proposed temporal integration loss function. By taking the temporal integration mechanism of the Human Visual System (HVS) into account, our network learns the perception process of the human eye, and temporally upsamples a sequence that in turn improves its perceived resolution. Specifically, we discuss a possible scenario where we deploy our approach on a VR system equipped with the eye-tracking technology, which could save up to 75% of the computational load. Compared with the state-of-the-art in terms of the inference time analysis and a user experiment, it shows that our approach runs around 1.89× faster and produces more favorable results.",https://ieeexplore.ieee.org/document/9585895/,2021 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct),4-8 Oct. 2021,ieeexplore
10.23919/DATE.2019.8714959,Learning to infer: RL-based search for DNN primitive selection on Heterogeneous Embedded Systems,IEEE,Conferences,"Deep Learning is increasingly being adopted by industry for computer vision applications running on embedded devices. While Convolutional Neural Networks' accuracy has achieved a mature and remarkable state, inference latency and throughput are a major concern especially when targeting low-cost and low-power embedded platforms. CNNs' inference latency may become a bottleneck for Deep Learning adoption by industry, as it is a crucial specification for many real-time processes. Furthermore, deployment of CNNs across heterogeneous platforms presents major compatibility issues due to vendor-specific technology and acceleration libraries.In this work, we present QS-DNN, a fully automatic search based on Reinforcement Learning which, combined with an inference engine optimizer, efficiently explores through the design space and empirically finds the optimal combinations of libraries and primitives to speed up the inference of CNNs on heterogeneous embedded devices. We show that, an optimized combination can achieve 45x speedup in inference latency on CPU compared to a dependency-free baseline and 2x on average on GPGPU compared to the best vendor library. Further, we demonstrate that, the quality of results and time ""to-solution"" is much better than with Random Search and achieves up to 15x better results for a short-time search.",https://ieeexplore.ieee.org/document/8714959/,"2019 Design, Automation & Test in Europe Conference & Exhibition (DATE)",25-29 March 2019,ieeexplore
10.1109/PDP50117.2020.00041,Lessons learned from comparing C-CUDA and Python-Numba for GPU-Computing,IEEE,Conferences,"Python as programming language is increasingly gaining importance, especially in data science, scientific, and parallel programming. It is faster and easier to learn than classical programming languages such as C. However, usability often comes at the cost of performance and applications written in Python are considered to be much slower than applications written in C or FORTRAN. Further, it does not allow the usage of GPUs-besides of pre-compiled libraries.However, the Numba package promises performance similar to C code for compute intensive parts of a Python application and it supports CUDA, which allows the use of GPUs inside a Python application.In this paper we compare the performance of Numba-CUDA and C -CUDA for different kinds of applications. For compute intensive benchmarks, the performance of the Numba version only reaches between 50% and 85% performance of the CCUDA version, despite the reduction operation, where the Numba version outperforms CUDA. Analyzing the PTX code and CUDA performance counters revealed that index-calculation is one limiting factor in Numba. Another problem is the type interference for single precision computations, as some values are computed in double precision. By optimizing this within the Numba package, the performance of Numba improves. However, C-CUDA applications still outperform the Numba versions. Further analysis with the CloverLeav Mini App shows that Numba performance further decreases for applications with multiple different compute kernels. The non-GPU part slows down these applications, due to the slow Python interpreter. This leads to a worse GPU utilization.Today Python is widely used in industry and academia and has been the first choice of coding languages among software programmers in the last years. Currently, according to the TIOBE index [5], it is the 3rd most popular programming language and the number one in IEEE Spectrum's fifth annual interactive ranking of the top programming languages [4]. One reason for this is that is easier to learn than classical programming languages like C. However, the other reason is the increasing popularity of Data Science, where Python is the most used language. A collection of libraries such as NumPy [22], and Matplotlib [1] or Scipy [8] provide a rich set of functions for scientific computing [16]. Packages like Dask [19], PyCompss [21] and MPI for Python [6] allow running Python applications on large, parallel machines, promising high performance. However, the performance of Python is considered slow compared to compiled languages such as C, C++, and FORTRAN, especially for heavy computations. In recent years, more and more tools have been developed to counter this prejudice. Numpy [22], for example, uses C-like arrays to store data and offers fast functions implemented in C to speed up calculations. The CuPy [14] package provides a similar set of functions, but these functions are implemented for GPUs using CUDA. The SciPy library is based on NumPy and provides a rich set on functionalities for scientific computing. Still, the high performance of these libraries is provided by the underling C-implementations. Internally, they use libraries like OpenBlas or IntelMKL to reach high performance and therefore, they are limited by the functions which are provided by theses libraries. Therefore, a performance problem always arises when the required functionality is not implemented within these libraries. In this case, the application falls back to the Python interpreter. Compared to ""bare metal"" code, interpreted code is slow. In addition, in Python it is not possible to use GPUs or other accelerators directly, as the Python interpreter cannot execute code on these machines. Therefore, the usage is only possible with precompiled libraries. To overcome this limitation, different approaches where developed to mix C, CUDA or OpenCL with Python. Cython [2] allows integrating C-code in Python applications to improve performance of critical sections. It also allows an easy development of wrappers for C-libraries. Similar, packages such as PyCuda and PyOpenCL [9] support wrappers for CUDA or OpenCL code within a Python script. Both approaches require the mixture of different programming languages.Numba [10] follows a different approach. Instead of merging C/CUDA code with Python, it allows the development of efficient applications for both, CPUs and GPUs in Python style. When a Python script using Numba is executed, marked functions are compiled just-in-time (JIT) using the LLVM framework. Using Python for GPU programming can mean a considerable simplification in the development of parallel applications.But often a simplification of comes at the expense of performance, and one expects a performance loss from Python compared to pure C code. In this paper, we want to understand the differences between native C-CUDA code and CUDA-code written in Python with Numba. We also want to share some basic tips how to improve the performance of applications written in Numba.We will first analyse a few micro benchmarks in detail. We are using these simple benchmarks, as it is easier to understand the differences with small code examples. We will use the collected information to derive some optimization for Numba. Finally, we evaluate and compare the performance of a more application like mini-app, written in C-CUDA and Numba accelerated Python. We will evaluate if our insights from the microbenchmarks to real applications.",https://ieeexplore.ieee.org/document/9092407/,"2020 28th Euromicro International Conference on Parallel, Distributed and Network-Based Processing (PDP)",11-13 March 2020,ieeexplore
10.1109/ICPP.2011.40,Location-Aware MapReduce in Virtual Cloud,IEEE,Conferences,"MapReduce is an important programming model for processing and generating large data sets in parallel. It is commonly applied in applications such as web indexing, data mining, machine learning, etc. As an open-source implementation of MapReduce, Hadoop is now widely used in industry. Virtualization, which is easy to configure and economical to use, shows great potential for cloud computing. With the increasing core number in a CPU and involving of virtualization technique, one physical machine can hosts more and more virtual machines, but I/O devices normally do not increase so rapidly. As MapReduce system is often used to running I/O intensive applications, decreasing of data redundancy and load unbalance, which increase I/O interference in virtual cloud, come to be serious problems. This paper builds a model and defines metrics to analyze the data allocation problem in virtual environment theoretically. And we design a location-aware file block allocation strategy that retains compatibility with the native Hadoop. Our model simulation and experiment in real system shows our new strategy can achieve better data redundancy and load balance to reduce I/O interference. Execution time of applications such as RandomWriter, Text Sort and Word Count are reduced by up to 33% and 10% on average.",https://ieeexplore.ieee.org/document/6047196/,2011 International Conference on Parallel Processing,13-16 Sept. 2011,ieeexplore
10.1109/SMC.2017.8123012,Long short term memory networks for short-term electric load forecasting,IEEE,Conferences,"Short-term electricity demand forecasting is critical to utility companies. It plays a key role in the operation of power industry. It becomes all the more important and critical with increasing penetration of renewable energy sources. Short-term load forecasting enables power companies to make informed business decisions in real-time. Demand patterns are extremely complex due to market deregulation and other environmental factors. Although there has been extensive research in the area of short-term electrical load forecasting, difficulties in implementation and lack of transparency in results has been cited as a main challenge. Deep neural architectures have recently shown their ability to mine complex underlying patterns in various domains. In our work, we present a deep recurrent neural architecture to unearth the complex patterns underlying the regional demand profiles without specific insights from the utilities. The model learns from historical data patterns. We show that deep recurrent neural network with long-short term memory architecture presents a robust methodology for accurate short term load forecasting with the ability to adapt and learn the underlying complex features over time. In most cases it matches the performance of the latest state-of-the-art techniques and even supercedes it in a few cases.",https://ieeexplore.ieee.org/document/8123012/,"2017 IEEE International Conference on Systems, Man, and Cybernetics (SMC)",5-8 Oct. 2017,ieeexplore
10.1109/INFOCOM.2018.8485910,MV-Sports: A Motion and Vision Sensor Integration-Based Sports Analysis System,IEEE,Conferences,"Recently, intelligent sports analytics is becoming a hot area in both industry and academia for coaching, practicing tactic and technical analysis. With the growing trend of bringing sports analytics to live broadcasting, sports robots and common playfield, a low cost system that is easy to deploy and performs real-time and accurate sports analytics is very desirable. However, existing systems, such as Hawk-Eye, cannot satisfy these requirements due to various factors. In this paper, we present MV-Sports, a cost-effective system for real-time sports analysis based on motion and vision sensor integration. Taking tennis as a case study, we aim to recognize player shot types and measure ball states. For fine-grained player action recognition, we leverage motion signal for fast action highlighting and propose a long short term memory (LSTM)-based framework to integrate MV data for training and classification. For ball state measurement, we compute the initial ball state via motion sensing and devise an extended kalman filter (EKF)-based approach to combine ball motion physics-based tracking and vision positioning-based tracking to get more accurate ball state. We implement MV-Sports on commercial off-the-shelf (COTS) devices and conduct real-world experiments to evaluate the performance of our system. The results show our approach can achieve accurate player action recognition and ball state measurement with sub-second latency.",https://ieeexplore.ieee.org/document/8485910/,IEEE INFOCOM 2018 - IEEE Conference on Computer Communications,16-19 April 2018,ieeexplore
10.1109/INCET49848.2020.9154161,Machine Learning Approach to Estimation of Internal Parameters of a Single Phase Transformer,IEEE,Conferences,"Transformer is the heart of power industry. So, it is necessary to track its performance every instant. Internal parameters of any transformer helps evaluate the performance and improve output waveform, raising efficiency[1]. They also state the condition of the internal windings[2]. It requires offloading of transformer to evaluate its internal parameters which is very costly since transformer needs to operate always to fully recover the investment. Many iterative analysis techniques have been proposed by many researchers in the past. Machine Learning is the current state of the art technology whose application is proposed in this paper. Various statistical methods are deployed in the training of such algorithms which comprises of Linear Regression, Stochastic Gradient Decent, Support Vector Machine and many more[3]. But, real time applications require minimum latency so Levenberg-Marquardt algorithm has been proposed for evaluation with higher accuracy.",https://ieeexplore.ieee.org/document/9154161/,2020 International Conference for Emerging Technology (INCET),5-7 June 2020,ieeexplore
10.1109/SIMS.2018.8355297,Machine learning algorithms for estimating powder blend composition using near infrared spectroscopy,IEEE,Conferences,"This paper presents a NIRS based real time continuous monitoring of powder blend composition which has widespread applications such as the pharmaceutical industry. The paper extends the implementation of several machine learning methodologies applied to sensor data collected using an NIR spectrometer for a model powder blending process. Several techniques were examined for the development of chemometric models of the multi-sensor data, including Principal Component Analysis (PCA), Partial Least Squares Regression (PLSR), Support Vector Machines (SVM) and Artificial Neural Networks (ANN). The performances of each of the models were compared in terms of accuracy (MSE) in predicting blend composition. The results obtained show that machine learning-based approaches produce process models of similar accuracy and robustness compared to models developed by PLSR while requiring minimal pre-processing and also being more adaptable to new data. The paper also discusses the prospect of using Convolutional Neural Networks (CNN) for NIRS data analysis.",https://ieeexplore.ieee.org/document/8355297/,2018 2nd International Symposium on Small-scale Intelligent Manufacturing Systems (SIMS),16-18 April 2018,ieeexplore
10.1109/WCICSS.2016.7882607,Machine learning algorithms for process analytical technology,IEEE,Conferences,"Increased globalisation and competition are drivers for process analytical technologies (PAT) that enable seamless process control, greater flexibility and cost efficiency in the process industries. The paper will discuss process modelling and control for industrial applications with an emphasis on solutions enabling the real-time data analytics of sensor measurements that PAT demands. This research aims to introduce an integrated process control approach, embedding novel sensors for monitoring in real time the critical control parameters of key processes in the minerals, ceramics, non-ferrous metals, and chemical process industries. The paper presents a comparison of machine learning algorithms applied to sensor data collected for a polymerisation process. Several machine learning algorithms including Adaptive Neuro-Fuzzy Inference Systems, Neural Networks and Genetic Algorithms were implemented using MATLAB® Software and compared in terms of accuracy (MSE) and robustness in modelling process progression. The results obtained show that machine learning-based approaches produce significantly more accurate and robust process models compared to models developed manually while also being more adaptable to new data. The paper presents perspectives on the potential benefits of machine learning algorithms with a view to their future in the industrial process industry.",https://ieeexplore.ieee.org/document/7882607/,2016 World Congress on Industrial Control Systems Security (WCICSS),12-14 Dec. 2016,ieeexplore
10.1109/IntelliSys.2017.8324372,Machine learning and deep neural network — Artificial intelligence core for lab and real-world test and validation for ADAS and autonomous vehicles: AI for efficient and quality test and validation,IEEE,Conferences,"Autonomous vehicles are now the future of automobile industry. Human drivers can be completely taken out of the loop through the implementation of safe and intelligent autonomous vehicles. Although we can say that HW and SW development continues to play a large role in the automotive industry, test and validation of these systems is a must. The ability to test these vehicles thoroughly and efficiently will ensure their proper and flawless operation. When a large number of people with heterogeneous knowledge and skills try to develop an autonomous vehicle together, it is important to use a sensible engineering process. State of the art techniques for such development include Waterfall, Agile &amp; V-model, where test &amp; validation (T&amp;V) process is an integral part of such a development cycle. This paper will propose a new methodology using machine learning &amp; deep neural network (AI-core) for lab &amp; real-world T&amp;V for ADAS (Advanced driver assistance system) and autonomous vehicles. The methodology will initially connect T&amp;V of individual systems in each level of development and that of complete system efficiently, by using the proposed phase methodology, in which autonomous driving functions are grouped under categories, special T&amp;V processes are carried on simulation as well as in HIL systems. The complete transition towards AI in the field of T&amp;V will be a sequence of steps. Initially the AI-core is fed with available test scenarios, boundary conditions for the test cases and scenarios, and examples, the AI-core will conduct virtual tests on simulation environment using available test scenarios and further generates new test cases and scenarios for efficient and precise tests. These test cases and scenarios are meant to cover all available cases and concentrate on the area where bugs or failures occur. The complete surrounding environment in the simulation is also controlled by the AI-core which means that the system can attain endless/all-possible combinations of the surrounding environment which is necessary. Results of the tests are sorted and stored, critical and important tests are again repeated in the real-world environment using automated cars with other real subsystems to depict the surrounding environment, which are all controlled by the AI-core, and meanwhile the AI-core is always in the loop and learning from each and every executed test case and its results/outcomes. The main goal is to achieve efficient and high quality test and validation of systems for automated driving, which can save precious time in the development process. As a future scope of this methodology, we can step-up to make most parts of test and validation completely autonomous.",https://ieeexplore.ieee.org/document/8324372/,2017 Intelligent Systems Conference (IntelliSys),7-8 Sept. 2017,ieeexplore
10.1049/cp.2020.0049,Machine learning based anomaly-based intrusion detection system in a full digital substation,IET,Conferences,"The cyberattacks that occurred in recent years have raised concerns in critical infrastructures, including power system networks. Identifying ongoing attacks is essential to enable the energy industry to respond to adversaries. Many commercial products and research projects include machine learning based intrusion detection systems but there is still a need for understanding the data training requirements for those systems in order to successfully deploy them to protect power systems. This paper presents the development of an anomaly-based Intrusion Detection System (IDS) based on a machine learning methodology to create a whitelist. The system was implemented using GNU Octave. It was trained using traffic flow from real devices generated from a Virtual Site Acceptance Testing and Training (VSATT) platform where multi-vendor secondary devices were set up and communicated to each other. The system was then tested using different datasets which were also generated from the VSATT platform. Results show that the implemented IDS performed correctly under different case studies. The results also indicate that the learned traffic identifies GOOSE and MMS messages based on the normal behaviours from those protocols, but the presence of other messages might require manual inputs to be incorporated in the training dataset.",https://ieeexplore.ieee.org/document/9449350/,15th International Conference on Developments in Power System Protection (DPSP 2020),9-12 March 2020,ieeexplore
10.1109/DTPI52967.2021.9540077,Mechanical Design Paradigm based on ACP Method in Parallel Manufacturing,IEEE,Conferences,"Parallel Manufacturing is a new manufacturing paradigm in industry, deeply integrating informalization, automation, and artificial intelligence. In this paper we propose a new mechanical design paradigm in Parallel Manufacturing based on ACP method. The key is to regard the design procedure based on artificial design and emulation method as two independent procedures, which can be modeled as a parallel system. The design procedure based on ACP method does not include a real system, which is an inventive extension of the traditional parallel system. This method can be implemented with social information by introducing the definition of SDV, SDM, and Intelligent Design Manager, making it highly adaptive for social manufacturing and Parallel Manufacturing.",https://ieeexplore.ieee.org/document/9540077/,2021 IEEE 1st International Conference on Digital Twins and Parallel Intelligence (DTPI),15 July-15 Aug. 2021,ieeexplore
10.1109/ECAI.2017.8166463,Mitigating DoS attacks in publish-subscribe IoT networks,IEEE,Conferences,"Internet of Things (IoT) is an emerging subject which enables multiple applications and requires robust security solutions. IoT architectures contribute to critical aspects of our society like transportation, health-care, industry, telecommunications and many others. Security is difficult to be implemented in the IoT context because the embedded devices are resource constrained and deployed in uncontrolled areas, thus being targeted by various security attacks. In scenarios like smart city, IoT networks are populated by both trusted and untrusted transient devices, thus mechanisms for mitigating complex security attacks must be implemented. Embedded networks with near real-time constraints are subject to denial-of-service attacks which can easily affect the applications availability by injecting malicious packets into the network. This paper proposes a lightweight security mechanism which addresses DoS attacks in IoT publish-subscribe applications.",https://ieeexplore.ieee.org/document/8166463/,"2017 9th International Conference on Electronics, Computers and Artificial Intelligence (ECAI)",29 June-1 July 2017,ieeexplore
10.1109/ICICTA.2010.569,Model of Viability Prediction Based on Neural Network and Data Mining Technique for Forest Industry Enterprise,IEEE,Conferences,"The operating status of a forest industry enterprise is disclosed periodically for viability. As a result, the manager usually only get information about the operating decision. An employer may be in after the formal financial statement has been published. If the employer executives intentionally package financial statements with the purpose of hiding the actual status of the forestry industry enterprise, then manager will have even less chance of obtaining the real financial information. To improve the accuracy of the viability prediction, viability ratios, non-viability ratios, and factor analysis had been used to extract adaptable variables. Moreover, the neural network and data mining technique were used to build the viability prediction model. The empirical experiment with a total of viability and non-viability ratios and projects as the initial samples obtained a satisfactory result, which testifies for the feasibility and validity of our proposed methods for the viability prediction of forestry industry enterprise.",https://ieeexplore.ieee.org/document/5522660/,2010 International Conference on Intelligent Computation Technology and Automation,11-12 May 2010,ieeexplore
10.1109/SCC.2018.00014,Modeling Sentiment Polarity in Support Ticket Data for Predicting Cloud Service Subscription Renewal,IEEE,Conferences,"In the cloud based service provisioning industry, one of the main challenges that providers face involves keeping existing tenants engaged while attracting new ones. To address this, providers need to gain insights about customer satisfaction. In that regards, support ticket data, understood as the main way of communication between both parties, can be mined to obtain an estimation of customer satisfaction by means of the polarity of the sentiment extracted from the report descriptions. To that end, in this work we propose a model which can learn a feature representation for sentiment polarity changes, from the sequence of tickets emitted by a given customer during the period associated with the service subscription term. Then, that resulting feature representation, combined with other handcrafted features related to contract and ticket data, is passed to a classifier which estimates the likelihood of service subscription renewal by the customer. Experiment results using real data from a service provider shows that learned representation of sentiment polarity changes from support ticket data in combination with other handcrafted features improves the accuracy in predicting subscription renewals. Moreover, our architecture is flexible enough incorporate and integrate several feature representations and give more expressive power to the prediction.",https://ieeexplore.ieee.org/document/8456400/,2018 IEEE International Conference on Services Computing (SCC),2-7 July 2018,ieeexplore
10.1109/ICAT.2006.85,Modeling and Application of Virtual Machine Tool,IEEE,Conferences,"The recent years of the 21th Century are associated with the advent of virtual reality technologies for modern industry and manufacturing engineering. Virtual Machine Tool Technology is given to design, test, control and machine parts in a virtual reality environment. This paper presents the methods to model and simulate the virtual machine tools in response to change in the machining requirements. Specifically, a set of module combination rules and a modeling method of the structure of machine tools using connectivity graph are developed. By this way virtual machine tool is implemented. The developed virtual machine tool can be efficiently used for industry training and machine leaning and operating.",https://ieeexplore.ieee.org/document/4089203/,16th International Conference on Artificial Reality and Telexistence--Workshops (ICAT'06),29 Nov.-1 Dec. 2006,ieeexplore
10.1109/ISIE45063.2020.9152407,Modeling and Predicting an Industrial Process Using a Neural Network and Automation Data,IEEE,Conferences,"Production optimization and prevention of faults and unplanned production halts are areas of particular interest in industry. Predictive analysis is commonly implemented with data analytics and machine learning techniques. Usually, the usage of such tools requires knowledge of the machine learning theory and the subject to be studied, e.g. a pumping process. This paper presents a case study on modeling of a pumping process using stored automation data. The model is trained to predict the performance percentage of the process with minimal background knowledge of the process and data analytics. The proposed model is built with IBM SPSS Modeler, a data analysis tool not usually used in real-time industrial predictive analysis as it is not often considered the best tool when working with time series data. The model is deployed in a cloud service to implement a real-time, visualized predictive analysis system. The case study shows that Modeler can be used for data analysis, modeling, and production purposes. Depending on the case, Modeler can provide an alternative tool compared with typical machine learning tools, as models built with Modeler can be deployed into a cloud service for production use. The findings indicate that industrial automation data are a valuable resource, and data analysis can be conducted on various platforms and tools.",https://ieeexplore.ieee.org/document/9152407/,2020 IEEE 29th International Symposium on Industrial Electronics (ISIE),17-19 June 2020,ieeexplore
10.1109/SCC47175.2019.9116104,Modeling and management of human resources in the reconfiguration of production system in industry 4.0 by neural networks,IEEE,Conferences,"In Industry 4.0, the role of employees changes significantly. Real-time production line control transforms job content. Work processes affect working conditions. The implementation of a socio-technical approach to the organization of work gives workers the opportunity to adapt their skills. Indeed, production work will become more and more multi-factor, especially with regard to control and decision-making tasks. In this paper, a proposal for an intelligent system for modeling skills and human resource management in the production system chain through the use of two artificial neural networks. The first NN1 network allows for the identification of the human factor, as well as the second NN2 network is reserved for valuing the human skills needed in Industry 4.0.",https://ieeexplore.ieee.org/document/9116104/,"2019 International Conference on Signal, Control and Communication (SCC)",16-18 Dec. 2019,ieeexplore
10.1109/ASCC.2015.7244400,Modelling of a conveyor-belt grain dryer utilizing a sigmoid network,IEEE,Conferences,"Post-harvest techniques play an important role in modern agricultural industry. One of these essential post-harvest techniques is the grain drying process. However, this process is characterized by its high complexity and nonlinearity due to the effects of several drying parameters. Therefore, conventional modelling approaches cannot produce accurate modelling results to describe the dynamics of this challenging process. This paper presents a nonlinear modelling technique to develop a highly accurate model for a laboratory-scale conveyor-belt grain drying system. In particular, this modelling technique is based on utilizing the sigmoid network as a nonlinearity estimator in a nonlinear autoregressive with exogenous input (NARX) model. As the training samples, a set of experimental input-output data was used in the model development process. This data set was collected from the conveyor-belt grain dryer during a real-time experiment to dry paddy (rough rice) grains. Compared to other previously reported modelling techniques which were applied for the same drying process, the proposed sigmoid-based NARX model has achieved the best modelling accuracy in describing the grain drying process. More precisely, the proposed model has achieved a root mean squared error (RMSE) of 2.776 × 10<sup>-17</sup>. It is worth to highlight that, unlike previous efforts which aimed at modelling conveyor-belt grain drying systems, the advantage of the proposed modelling technique is that it can be directly applied to model the drying system regardless of the dryer shape, and moreover regardless of the size and physical properties of the grains to be dried. In addition, the resulting model can be readily employed in control applications to design suitable dryer controllers.",https://ieeexplore.ieee.org/document/7244400/,2015 10th Asian Control Conference (ASCC),31 May-3 June 2015,ieeexplore
10.1109/WF-IoT.2019.8767291,Mountain Pine Beetle Monitoring with IoT,IEEE,Conferences,"Outbreaks of forest pests cause large-scale damages, which lead to significant impact on the ecosystem as well as the forestry industry. Current methods of monitoring pest outbreaks involve field, aerial and remote sensing surveys. These methods only provide partial spatial coverage and can detect outbreaks only after they have substantially progressed across wide geographic areas. This paper presents an IoT system for real-time insect infestation detection using bioacoustic recognition via machine learning techniques. Specifically, we focus on detecting the Mountain Pine Beetle (MPB), which is the most destructive insect of mature pines in western North American forests. We present the design of the system and describe its various hardware and software components. Experimental results collected from a prototype implementation of the system are presented, which show that the system can detect MPB with 82% accuracy. We also demonstrate the applicability of our system in other noise monitoring applications, and report our experimental results on urban noise detection and classification.",https://ieeexplore.ieee.org/document/8767291/,2019 IEEE 5th World Forum on Internet of Things (WF-IoT),15-18 April 2019,ieeexplore
10.1109/ISCAS.2019.8702575,Multi-View Fusion Neural Network with Application in the Manufacturing Industry,IEEE,Conferences,"In recent years the research community and industry have paid high attention to the field of machine learning, especially deep learning. Nowadays many real-world classification or rather prediction applications are implemented by neural network models. We propose a multi-view fusion neural network with application in the manufacturing industry. Image information of multiple cameras is fused and used by the proposed model to predict the state of a manufacturing machine. Experiments show that the overall classification performance is increased from a baseline of 92.7% to 99.5% by the fusion model.",https://ieeexplore.ieee.org/document/8702575/,2019 IEEE International Symposium on Circuits and Systems (ISCAS),26-29 May 2019,ieeexplore
10.23919/DATE48585.2020.9116407,Network Synthesis for Industry 4.0,IEEE,Conferences,"Today's factory machines are ever more connected with SCADA, MES, ERP applications as well as external systems for data analysis. Different types of network architectures must be used for this purpose. For instance, control applications at the lowest level are susceptible to delays and errors while data analysis with machine learning procedures requires to move a large amount of data without real-time constraints. Standard data formats, like Automation Markup Language (AML), have been established to document factory environment, machine placement and network deployment, however, no automatic technique is currently available in the context of Industry 4.0 to choose the best mix of network architectures according to spacial constraints, cost, and performance. We propose to fill this gap by formulating an optimization problem. First of all, spatial and communication requirements are extracted from the AML description. Then, the optimal interconnection of wired or wireless channels is obtained according to application objectives. Finally, this result is back-annotated to AML to be used in the life cycle of the production system. The proposed methodology is described through a small, but complete, smart production plant.",https://ieeexplore.ieee.org/document/9116407/,"2020 Design, Automation & Test in Europe Conference & Exhibition (DATE)",9-13 March 2020,ieeexplore
10.1109/UralCon52005.2021.9559619,Neural Network for Real-Time Signal Processing: the Nonlinear Distortions Filtering,IEEE,Conferences,"Artificial neural networks, after their training and testing, allow, in the ""if then"" mode, to process signals in real time. This is relevant for signal processing tasks in electrical engineering and the electric power industry, especially for the analysis of nonlinear signal distortions, transients in electrical networks, during switching, emergency modes, and so on. The paper shows the neuro algorithm possibilities based on an elementary perceptron for filtering nonlinear distortions of industrial frequency signals of 50 Hz over a time interval of units of milliseconds. At the same time, the accuracy for the signals’ amplitude, phase, and frequency determining is units of percent. Examples of the fundamental frequency signal selection in the presence of harmonics, the determination of the signal parameters during transients, and the correction of the transformer saturation current are given. It is shown that real-time neural network processing can be carried out in a ""sliding window"" with the duration of units of milliseconds. The estimates made during the implementation of the neuro algorithm in a microprocessor device showed its application possibility in the electric power industry secondary equipment.",https://ieeexplore.ieee.org/document/9559619/,2021 International Ural Conference on Electrical Power Engineering (UralCon),24-26 Sept. 2021,ieeexplore
10.1109/ICIAS.2007.4658352,Neural network multi layer perceptron modeling of surface quality in laser machining,IEEE,Conferences,"Uncertainty is inevitable in problem solving and decision making. One way to reduce it is by seeking the advice of an expert in related field. On the other hand, when we use computers to reduce uncertainty, the computer itself can become an expert in a specific field through a variety of methods. One such method is machine learning, which involves using a computer algorithm to capture hidden knowledge from data. The researchers conducted the prediction of laser machining quality, namely surface roughness with seven significant parameters to obtain singleton output using machine learning techniques based on Quick Back Propagation Algorithm. In this research, we investigated a problem solving scenario for a metal cutting industry which faces some problems in determining the end product quality of Manganese Molybdenum (Mn-Mo) pressure vessel plates. We considered several real life machining scenarios with some expert knowledge input and machine technology features. The input variables are the design parameters which have been selected after a critical parametric investigation of 14 process parameters available on the machine. The elimination of non-significant parameters out of 14 total parameters were carried out by single factor and interaction factor investigation through design of experiment (DOE) analysis. Total number of 128 experiments was conducted based on 2<sup>k</sup> factorial design. This large search space poses a challenge for both human experts and machine learning algorithms in achieving the objectives of the industry to reduce the cost of manufacturing by enabling the off hand prediction of laser cut quality and further increase the production rate and quality.",https://ieeexplore.ieee.org/document/4658352/,2007 International Conference on Intelligent and Advanced Systems,25-28 Nov. 2007,ieeexplore
10.1109/AFRCON.1996.562998,New solutions in the control of the Hungarian power system,IEEE,Conferences,"In the frame of a reconstruction project on the Hungarian Power Control System a lot of intelligent control tools are investigated. Some are accepted, others are rejected. Since 1993 the first Dispatcher Training Simulator of the Hungarian Electric Power Industry has been working in the regional dispatcher center of the North Hungarian Electricity Distribution Company (EMASZ KDSZ). The simulator performs discrete, event-driven simulation of the network, the protection and the telemetry system. A new continuous approach to the power system restoration problem is proposed. The continuous restoration means an on-line function that supports all the dispatcher actions. It is active and alerts not only the restorative network state, but also in the normal state. This real time expert system is strongly based on the co-operation with the SCADA-EMS functions and the different off-line network calculations. The idea is proposed to be implemented in the Hungarian National Control Centre. In 1994-95 a common project ('A Neural Network Application to Power System's Signal Processing') was performed in the institute KFKI MSZKI. We sought for the appropriate neural network structure, which fits the best the problem of event recognition. The research was extended for a sequential pattern matching algorithm too. On the base of the results we are working on an event recognizer application.",https://ieeexplore.ieee.org/document/562998/,Proceedings of IEEE. AFRICON '96,27-27 Sept. 1996,ieeexplore
10.1109/MED.2016.7535908,Nonlinear model predictive control hardware implementation with custom-precision floating point operations,IEEE,Conferences,"Model predictive control (MPC) based techniques have found many applications both in academia and in industry. Its reach, however, may not be compared to classical control techniques due to e.g. the difficulty of solving an optimization problem at each sampling interval with real-time requirements. Most of the efforts to make the application of MPC viable address this problem with more efficient solvers. This paper, in contrast, proposes a new approach for a real-time MPC solution by mapping an approximate off-line solution into an artificial neural network in a FPGA (Field Programmable Gate Array). We implemented a radial basis function artificial neural network on a low cost FPGA using custom precision floating point operations and tested the control on a single-link robotic manipulator. The amount of time used to calculate the control action at each time instant is in around one microsecond. The comparison between the offline and the approximate solution shows the soundness of the idea. We provide an analysis of hardware usage and execution time in order to achieve the best compromise considering the precision for a given application.",https://ieeexplore.ieee.org/document/7535908/,2016 24th Mediterranean Conference on Control and Automation (MED),21-24 June 2016,ieeexplore
10.1109/ICCCNT49239.2020.9225680,"Notice of Violation of IEEE Publication Principles: 6G Wireless Communication: Its Vision, Viability, Application, Requirement, Technologies, Encounters and Research",IEEE,Conferences,"The fast development of multiband ultrafast seamless network and super reliable data transmission system to support heavy traffic applications such as artificial intelligence, machine learning, deep learning, augmented reality, virtual reality, 3D media, Internet of Things, Enterprise Internet of Thing and the Internet of Nano-things that involves with the real time transfer of data, voice and video in terabytes per second (Tb/s), the current cellular network (5G Network is insufficient to meet the growth of usage of triple play services in fraction of time). To meet the expectation of heavy data users is a big challenge in today's generation. To handle the situation of drastic demand of data, the sixth generation of mobile technology (6G) should be deeply studied along with its potential in terms of bandwidth, low latency, channel capacity, channel modeling techniques, loss propagation models, energy spectrum efficiency, faster network connectivity and data security. In this paper the vision in terms intelligent computing and wireless massive connectivity, feasibility, requirement in terms of modifying the existing 5G network, technologies in terms of artificial intelligence, 3D networking, SM-MIMO and optical computing, challenges after deployment, research to promote good health for 6G and application of 6G in the field of industry, automation sector, health, and transport has been studied and presented.",https://ieeexplore.ieee.org/document/9225680/,"2020 11th International Conference on Computing, Communication and Networking Technologies (ICCCNT)",1-3 July 2020,ieeexplore
10.1109/SNPD.2016.7515945,Notice of Violation of IEEE Publication Principles: Adaptive Fuzzy PID speed control of DC belt conveyor system,IEEE,Conferences,Conveyor belt system is one of the most common transfer systems used in industry to transfer goods from one point to another in a limited distance. It is used in industries such as the electromechanical /mechanical assembly manufacturing to transfer work piece from one station to another or one process to another in food industries. The belt conveyor system discussed in this paper is driven by a DC motor and two speed controllers. The PID speed controller is designed to provide comparison to the main controller which is the Adaptive Fuzzy PID Speed controller. Both controllers are implemented in a real hardware where the algorithm will be written in PLC using SCL language. The experimental result shows that Adaptive Fuzzy PID controller performs better and adapted to the changes in load much faster than the conventional PID controller. This project has also proved that PLC is capable of performing high level control system tasks..,https://ieeexplore.ieee.org/document/7515945/,"2016 17th IEEE/ACIS International Conference on Software Engineering, Artificial Intelligence, Networking and Parallel/Distributed Computing (SNPD)",30 May-1 June 2016,ieeexplore
10.1109/RTSI.2017.8065980,OPEB: Open physical environment benchmark for artificial intelligence,IEEE,Conferences,"Artificial Intelligence methods to solve continuous-control tasks have made significant progress in recent years. However, these algorithms have important limitations and still need significant improvement to be used in industry and real-world applications. This means that this area is still in an active research phase. To involve a large number of research groups, standard benchmarks are needed to evaluate and compare proposed algorithms. In this paper, we propose a physical environment benchmark framework to facilitate collaborative research in this area by enabling different research groups to integrate their designed benchmarks in a unified cloud-based repository and also share their actual implemented benchmarks via the cloud. We demonstrate the proposed framework using an actual implementation of the classical mountain-car example and present the results obtained using a Reinforcement Learning algorithm.",https://ieeexplore.ieee.org/document/8065980/,2017 IEEE 3rd International Forum on Research and Technologies for Society and Industry (RTSI),11-13 Sept. 2017,ieeexplore
10.1109/CCIS48116.2019.9073682,On Improving Text Generation Via Integrating Text Coherence,IEEE,Conferences,"Automatic text generation techniques with either extractive-based or generative-based methods are becoming increasingly popular and widely used in industry. In contrast to existing extractive-based text generation approaches that ignore the text coherence, our proposed approach integrates both keyword coverage and text coherence into our optimization framework. In this paper, we employ the semantics-based coherence and syntax-based coherence metrics to evaluate the text coherence. Extensive experiments on a real corpus demonstrate that our method outperforms baselines overall regrading ROUGE and Human evaluation metrics. Our model provides new insights on how to utilize coherence measures to arrange the sentences extracted by keyword covering method. The proposed method has been deployed on a real system to help generate coherent text.",https://ieeexplore.ieee.org/document/9073682/,2019 IEEE 6th International Conference on Cloud Computing and Intelligence Systems (CCIS),19-21 Dec. 2019,ieeexplore
10.1109/NCC52529.2021.9530062,On Traffic Classification in Enterprise Wireless Networks,IEEE,Conferences,"Enterprises today are quickly adopting intelligent, adaptive, and flexible wireless communication technologies in order to become compliant with Industry 4.0. One of the technological challenges related to this is to provide Quality of Services (QoS)-enabled network connectivity to the applications. Diverse QoS demands from the applications intimidate the underlying wireless networks to be agile and adaptive. Since the applications are diverse in nature, there must be a mechanism to learn the application types in near real-time so that the network can be provisioned accordingly. In this paper, we propose a Machine Learning (ML) based method to classify the application traffic. Our method is different from the existing port based and Deep Packet Inspection (DPI) based methods and uses statistical features of the network traffic related to the applications. We validate the performance of the proposed model in a lab based SDNized WiFi set-up. SDNization ensures that the proposed model can be deployed in practice.",https://ieeexplore.ieee.org/document/9530062/,2021 National Conference on Communications (NCC),27-30 July 2021,ieeexplore
10.1109/IJCNN.2015.7280779,On sequences of different adaptive mechanisms in non-stationary regression problems,IEEE,Conferences,"Existing adaptive predictive methods often use multiple adaptive mechanisms as part of their coping strategy in non-stationary environments. These mechanisms are usually deployed in a prescribed order which does not change. In this work we investigate and provide a comparative analysis of the effects of using a flexible order of adaptive mechanisms' deployment resulting in varying adaptation sequences. As a vehicle for this comparison, we use an adaptive ensemble method for regression in batch learning mode which employs several adaptive mechanisms to react to the changes in data. Using real world data from the process industry we demonstrate that such flexible deployment of available adaptive methods embedded in a cross-validatory framework can benefit the predictive accuracy over time.",https://ieeexplore.ieee.org/document/7280779/,2015 International Joint Conference on Neural Networks (IJCNN),12-17 July 2015,ieeexplore
10.1109/ASCC.2015.7244597,Online sequential extreme learning machine algorithm based human activity recognition using inertial data,IEEE,Conferences,"Human activity recognition (HAR) is the basis for many real world applications concerning health care, sports and gaming industry. Different methodological perspectives have been proposed to perform HAR. One appealing methodology is to take an advantage of data that are collected from inertial sensors which are embedded in the individual's smartphone. These data contain rich amount of information about daily activities of the user. However, there is no straightforward analytical mapping between a performed activity and its corresponding data. Besides, online training for the classification in these types of applications is a concern. This paper aims at classifying human activities based on the inertial data collected from a user's smartphone. An Online Sequential Extreme Learning Machine (OSELM) method is implemented to train a single hidden layer feed-forward network (SLFN). Experimental results with an average accuracy of 82.05% are achieved.",https://ieeexplore.ieee.org/document/7244597/,2015 10th Asian Control Conference (ASCC),31 May-3 June 2015,ieeexplore
10.1109/ICIECA.2005.1644370,Optimal approach for enhancement of large and small scale near-infrared and infrared imaging,IEEE,Conferences,"In a broad area of industry such as remote sensing and medical diagnosing, imaging enhancement technology takes a leading role, where energy distribution of the light source depends not only on image coordinate but also on wavelength. Both infrared (IR) and near-infrared (NIR) imaging techniques have a variety of applications in these fields. For instance, satellite images are taken via IR or NIR spectrometer and laser Doppler medical scanning is collaborated with NIR spectrometer. Matrix functions of any image correspond to brightness or energy at each image pixel. The actual decision making must rely on detailed investigation of images being obtained. Therefore, image processing should be taken into account so as to enhance the results from real world. Segmentation is an image analysis approach to clarify feature ambiguity and information noise, which divides an image into separate parts that correlate with the objects or areas of the particular object involved. This procedure can be conducted by clustering, which is a process of partitioning a set of pattern vectors into subsets. Being a simple unsupervised learning algorithm, k-means clustering algorithm has the potential to both simplify the computation and accelerate the convergence. In most cases optimization is closely related to clustering, which gives rise to the best way of problem solving. In this article, optimal approach is proposed to be implemented along with image segmentation. This methodology is to enhance both large scale and small scale IR and NIR image processing",https://ieeexplore.ieee.org/document/1644370/,2005 International Conference on Industrial Electronics and Control Applications,29 Nov.-2 Dec. 2005,ieeexplore
10.1109/ITMS47855.2019.8940643,Optimization of Neural Networks with Response Surface Methodology: Prediction of Cigarette Pressure Drop,IEEE,Conferences,"Neural network is an artificial intelligence technique providing successful results in solving many prediction problems. There are many factors affecting the predictive performance of a neural network such as learning rate, momentum rate, number of neurons etc. In practice, trial-error method is used in the selection of these parameters. However, this approach is a time-consuming process and can only measure the effect of change in one parameter on performance at a time. In recent years, alternative methods such as experimental design, genetic algorithm, simulated annealing have been used to find the optimum neural network topology. In this study, response surface method, which is one of the most common design of experiment, was used to determine the neural network topology that would provide the highest predictive performance of the pressure drop that is a quality parameter in tobacco industry. The parameters affecting the pressure drop parameter are considered as circumference, total weight and ventilation. In this context, number of hidden neurons, learning rate, momentum rate and stop criteria were identified as the experimental factors and the combination that gives the lowest mean absolute deviation has been proposed as neural network model to predict pressure drop parameter. As a real life application, thousand cigarette samples have been processed by multilayer perceptron. Findings revealed that epoch size, learning rate, number of hidden neurons and stop criteria have significant linear impact on mean absolute deviation of neural network. Optimum neural network design has been obtained to predict pressure drop parameter.",https://ieeexplore.ieee.org/document/8940643/,2019 60th International Scientific Conference on Information Technology and Management Science of Riga Technical University (ITMS),10-11 Oct. 2019,ieeexplore
10.23919/SpliTech49282.2020.9243735,PADL: a Language for the Operationalization of Distributed Analytical Pipelines over Edge/Fog Computing Environments,IEEE,Conferences,"In this paper we introduce PADL, a language for modeling and deploying data-based analytical pipelines. The novelty of this language relies on its independence from both the infrastructure and the technologies used on it. Specifically, this descriptive language aims at embracing all the particularities and constraints of high-demanding deployment models, such as critical restrictions regarding latency, privacy and performance, by providing fully-compliant schemas for implementing data analytical workloads. The adoption of PADL provides means for the operationalization of these pipelines in a reproducible and resilient fashion. In addition, PADL is able to fully utilize the benefits of Edge and Fog computing layers. The feasibility of the language has been validated with an analytical pipeline deployed over an Edge computing environment to solve an Industry 4.0 use case. The promising results obtained therefrom pave the way towards the widespread adoption of our proposed language when deploying data analytical pipelines over real application scenarios.",https://ieeexplore.ieee.org/document/9243735/,2020 5th International Conference on Smart and Sustainable Technologies (SpliTech),23-26 Sept. 2020,ieeexplore
10.1109/TAAI.2015.7407079,PCBA demand forecasting using an evolving Takagi-Sugeno system,IEEE,Conferences,"This paper investigates the use of using an evolving fuzzy system for printed circuit board (PCBA) demand forecasting. The algorithm is based on the evolving Takagi-Sugeno (eTS) fuzzy system, which has the ability to incorporate new patterns by changing its internal structure in an on-line fashion. We argue that these capabilities could aid in forecasting dynamic demand patterns such as those experienced in the electronic manufacturing (EMS) industry. An eTS fuzzy system is implemented in the R statistical programming language and is tested on both synthetic and real-world data. To our knowledge, this is one of the first applications of an evolving fuzzy system to forecast product demand. The results indicate that the evolving fuzzy system outperforms competing approaches for the application considered.",https://ieeexplore.ieee.org/document/7407079/,2015 Conference on Technologies and Applications of Artificial Intelligence (TAAI),20-22 Nov. 2015,ieeexplore
10.1109/CIIMA50553.2020.9290289,PI tuning based on Bacterial Foraging Algorithm for flow control,IEEE,Conferences,"In the industrial field, the need has arisen to use more efficient and robust controllers using artificial intelligence techniques that optimize the operation of processes within the industry. In this way, the need arises to employ adaptive controllers such as the BFOA and implement it in real systems in which its functionality can be analyzed. This article presents the implementation and analysis in a fully instrumented functional prototype with industrial sensors. The work methodology is documented from the acquisition of the physical variables through the OPC client-server communication; the synchronization of the excitation of the input variable (variable speed drive) and obtaining the evolution of the flow in time; with the experimental data, the identification methodology by relative least squares was used to obtain the transfer function. Later, the BFOA algorithm was implemented to adjust the constants of a PI controller (Kp and Ki) and analyze the response through simulation using Matlab software, in which satisfactory results were observed based on the analysis of response to disturbances and as an end final part, the controller and the BFOA algorithm were implemented in a PLC-S7-1500 controller in SCL language, and the functionality was validated with the functional prototype, changing the flow setpoints at certain times, observing a behavior according to the simulations carried out. with a minimum overshoot of approximately 5 % and an establishment time of 20s.",https://ieeexplore.ieee.org/document/9290289/,2020 IX International Congress of Mechatronics Engineering and Automation (CIIMA),4-6 Nov. 2020,ieeexplore
10.1109/BigDataCongress.2019.00032,"PREMISES, a Scalable Data-Driven Service to Predict Alarms in Slowly-Degrading Multi-Cycle Industrial Processes",IEEE,Conferences,"In recent years, the number of industry-4.0-enabled manufacturing sites has been continuously growing, and both the quantity and variety of signals and data collected in plants are increasing at an unprecedented rate. At the same time, the demand of Big Data processing platforms and analytical tools tailored to manufacturing environments has become more and more prominent. Manufacturing companies are collecting huge amounts of information during the production process through a plethora of sensors and networks. To extract value and actionable knowledge from such precious repositories, suitable data-driven approaches are required. They are expected to improve the production processes by reducing maintenance costs, reliably predicting equipment failures, and avoiding quality degradation. To this aim, Machine Learning techniques tailored for predictive maintenance analysis have been adopted in PREMISES (PREdictive Maintenance service for Industrial procesSES), an innovative framework providing a scalable Big Data service able to predict alarming conditions in slowly-degrading processes characterized by cyclic procedures. PREMISES has been experimentally tested and validated on a real industrial use case, resulting efficient and effective in predicting alarms. The framework has been designed to address the main Big Data and industrial requirements, by being developed on a solid and scalable processing framework, Apache Spark, and supporting the deployment on modularized containers, specifically upon the Docker technology stack.",https://ieeexplore.ieee.org/document/8818217/,2019 IEEE International Congress on Big Data (BigDataCongress),8-13 July 2019,ieeexplore
10.1109/BigData.2018.8622065,Parallel Large-Scale Neural Network Training For Online Advertising,IEEE,Conferences,"Neural networks have shown great successes in many fields. Due to the complexity of the training pipeline, however, using them in an industrial setting is challenging. In online advertising, the complexity arises from the immense size of the training data, and the dimensionality of the sparse feature space (both can be hundreds of billions). To tackle these challenges, we built TrainSparse (TS), a system that parallelizes the training of neural networks with a focus on efficiently handling large-scale sparse features. In this paper, we present the design and implementation of TS, and show the effectiveness of the system by applying it to predict the ad conversion rate (pCVR), one of the key problems in online advertising. We also compare several methods for dimensionality reduction on sparse features in the pCVR task. Experiments on real-world industry data show that TS achieves outstanding performance and scalability.",https://ieeexplore.ieee.org/document/8622065/,2018 IEEE International Conference on Big Data (Big Data),10-13 Dec. 2018,ieeexplore
10.1109/SMILE.2018.8353980,Parametric study and design of deep learning on leveling system for smart manufacturing,IEEE,Conferences,"Sheet metal is widely used in the industry for metal forming purposes, such as metal stamping and metal cutting. It is often winded and storage in a coil form for transportation purposes. However, before any manufacturing process such as, cutting, or stamping, leveling is required as the residual stress inside coil is present which can cause distortion to the metal forming/cutting process. In conventional coil leveling machines, the machine parameters are often set by machine technicians with many years of experiences. In addition, the optimized machine parameter is achieved by trial and error method or based on experiences. However, the machine parameters are also not exactly trivial due to too many input factors which may cause changes to the outcome result. In the recent years, industry 4.0 and smart manufacturing has been a widely discussed topic in terms of industry manufacturing solutions in many different industrialized countries. In smart manufacturing, communication and interaction between machines have become an important role to improve manufacturing efficiency, flexibility and customization. As smart manufacturing focused on information process through real objects, it is required to digitize the experience through deep learning method. This paper is aimed to describe and study the deep learning application based on coil leveling system. Finally, through this study and experiment verification, analyzes on research directions and prospects of deep learning.",https://ieeexplore.ieee.org/document/8353980/,"2018 IEEE International Conference on Smart Manufacturing, Industrial & Logistics Engineering (SMILE)",8-9 Feb. 2018,ieeexplore
10.1109/FPA.1994.636094,Perception systems implemented in analog VLSI for real-time applications,IEEE,Conferences,"We point out that analog VLSI can now be considered as the ideal medium to implement computational systems intended to carry out real time perceptive or even cognitive tasks that are not well handled by traditional computers. By exploiting the analog features of the transistors, only a few devices are needed to realise most of the elementary functions required to implement perceptive systems, resulting in very dense, sophisticated circuits and low power consumption. Elementary artificial retinas in silicon based on their biological counterparts have already been successfully used in industrial applications. Artificial cochleas and noses are also under development. This new enabling technology is of great interest over a wide range of industrial sectors, including robotics, automotive, surveillance and food industry.",https://ieeexplore.ieee.org/document/636094/,Proceedings of PerAc '94. From Perception to Action,7-9 Sept. 1994,ieeexplore
10.1109/CloudCom.2019.00066,Performance Analysis of Data Parallelism Technique in Machine Learning for Human Activity Recognition Using LSTM,IEEE,Conferences,"Human activity recognition (HAR), driven by large deep learning models, has received a lot of attention in recent years due to its high applicability in diverse application domains, manipulate time-series data to speculate on activities. Meanwhile, the cloud term ""as-a-service"" has essentially revolutionized the information technology industry market over the last ten years. These two trends somehow are incorporating to inspire a new model for the assistive living application: HAR as a service in the cloud. However, with frequently updates deep learning frameworks in open source communities as well as various new hardware features release, which make a significant software management challenge for deep learning model developers. To address this problem, container techniques are widely employed to facilitate the deep learning software development cycle. In addition, models and the available datasets are being larger and more complicated, and so, an expanding amount of computing resources is desired so that these models are trained in a feasible amount of time. This requires an emerging distributed training approach, called data parallelism, to achieve low resource utilization and faster execution in training time. Therefore, in this paper, we apply the data parallelism to build an assistive living HAR application using LSTM model, deploying in containers within a Kubernetes cluster to enable the real-time recognition as well as prediction of changes in human activity patterns. We then systematically measure the influence of this technique on the performance of the HAR application. Firstly, we evaluate our system performance with regard to CPU and GPU when deployed in containers and host environment, then analyze the outcomes to verify the difference in terms of the model learning performance. Through the experiments, we figure out that data parallelism strategy is efficient for improving model learning performance. In addition, this technique helps to increase the scaling efficiency in our system.",https://ieeexplore.ieee.org/document/8968845/,2019 IEEE International Conference on Cloud Computing Technology and Science (CloudCom),11-13 Dec. 2019,ieeexplore
10.1109/IMCOM48794.2020.9001673,Performance Comparison of Machine Learning Algorithms for Imbalanced Class Classification in Hydraulic System,IEEE,Conferences,"Several studies have been conducted over the past decades on fault detection for data-based system monitoring and have also been applied in various industry groups. However, there are various factors that make it difficult to apply in reality, even though algorithms for detecting various faults have been studied. The class imbalance problem is considered one of the most serious problems with fault detection. In many areas of classification, the data collected is largely imbalanced. Thus, in order to actually apply data-based system monitoring, there is a need for a method that can effectively solve such class imbalances. Various methodologies have been proposed to address this class imbalance problem, and good results have been recorded in several studies. In this paper, the overfitting problem caused by class imbalance is verified through experiment, and we find out which methodology is effective in solving class imbalance among several proposed methodologies for hydraulic system.",https://ieeexplore.ieee.org/document/9001673/,2020 14th International Conference on Ubiquitous Information Management and Communication (IMCOM),3-5 Jan. 2020,ieeexplore
10.1109/BigData.2018.8622389,Performance and Memory Trade-offs of Deep Learning Object Detection in Fast Streaming High-Definition Images,IEEE,Conferences,"Deep learning models are associated with various deployment challenges. Inference of such models is typically very compute-intensive and memory-intensive. In this paper, we investigate the performance of deep learning models for a computer vision application used in the automotive manufacturing industry. This application has demanding requirements that are characteristic of Big Data systems, including high volume and high velocity. The application has to process a very large set of high-definition images in real-time with appropriate accuracy requirements using a deep learning-based object detection model. Meeting the run time, accuracy, and resource requirements require a careful consideration of the choice of model, model parameters, hardware, and environmental support. In this paper, we investigate the trade-offs of the most popular deep neural network-based object detection models on four hardware platforms. We report the trade-offs of resource consumption, run time, and accuracy for a realistic real-time application environment.",https://ieeexplore.ieee.org/document/8622389/,2018 IEEE International Conference on Big Data (Big Data),10-13 Dec. 2018,ieeexplore
10.1109/ICTON.2019.8840155,Photonics-Supported 5G Test Facilities for Low Latency Applications,IEEE,Conferences,"Berlin has become one of the EU's strategic spot for the evolution of 5G networks due in part to several ongoing activities supporting the effort. So far, there has been a huge investment from 5G Berlin Innovation Cluster to deliver a unique infrastructure providing opportunities for research centres, SMEs, and start-ups to develop novel solutions and products [1]. In this regard, there has been a complementary activity from Berlin Centre for Digital Transformation to develop a three-node optical network, which connects three Fraunhofer institutes, HHI, FOKUS, and IPK in a metro ring structure [2]. Additionally, there is a recently funded German national project (OTB-5G+) to develop an Open Testbed Berlin for 5G and Beyond, which will deliver a three-node ROADM network. These activities along with some other ones, like the presence of a Berlin node for two of the EU 5G trial projects (i.e., 5Genesis [3] and 5Gvinni [4]), have made Berlin a strategic spot in the evolution of the EU 5G ecosystem. Fraunhofer HHI hosts several aspects of the above-mentioned infrastructure including the 3-node ROADM metro network testbed, which provide a great opportunity to perform real-field experiments with 5G-ready RAN infrastructure and edge compute capability for realizing low-latency end-to-end use-cases. This metro network is SDN controlled and hosts edge compute node capabilities. Additionally, it could offer NFV services and will eventually support network slicing. Regarding the RAN infrastructure, currently, there are LTE base stations installed, which operate at 2.6 GHz. They will be upgraded to 5G base stations operating at 3.7 GHz as soon as they become available. The access segment, which is based on a passive WDM network, is expected to be extended by sophisticated mm-wave and optical-wireless communication links as bridges demonstrating the network extensions where optical fibers cannot be deployed. This network will be an SDN-enabled allowing the inclusion of several street 5G access points to the infrastructure. The three-node ROADM test-bed described above and shown in Fig. 1 will also host several demonstrations, including the final demonstration of metro-haul project [5]. Two of the ROADM nodes will be realized as AMEN (i.e., Access-Metro Edge Node) and the last one is MCEN (i.e., Metro-Core Edge Node), both developed in the framework of Metro-Haul project. This network will be also connected to German metro ring. The German metro ring will provide connectivity to our partners' premises, including ADVA, NOKIA, and Infinera enabling field trials and optical transmission tests.Figure 1.A part of the 5G test facilities hosted in Berlin.In this talk, we will present in detail the facilities hosted in Berlin, covering its geographical expansion as well as the hosted optical and wireless technologies. We then explore several demonstrations planned to run over this infrastructure showcasing the benefits brought about by different technology pillars in the context of high capacity 5G test facilities in Berlin, including network slicing, network function virtualization, distributed computing over edge compute nodes to eventually address the execution of latency-critical and bandwidth- hungry applications in an intelligent, distributed, and resource-efficient way. We will conclude the talk by providing several ways for future collaboration through which research institutes and industry sector across Europe can get access to the infrastructure in order to perform collaborative field-trials, measurement campaign, autonomous networking solutions validation, etc.",https://ieeexplore.ieee.org/document/8840155/,2019 21st International Conference on Transparent Optical Networks (ICTON),9-13 July 2019,ieeexplore
10.1109/INFCOMW.2019.8845276,Poster Abstract: Deep Learning Workloads Scheduling with Reinforcement Learning on GPU Clusters,IEEE,Conferences,"With the recent widespread adoption of deep learning (DL) in academia and industry, more attention are attracted by DL platform, which can support research and development (R&amp;D) of AI firms, institutes and universities. Towards an off-the-shelf distributed GPU cluster, prior work propose prediction-based schedulers to allocate resources for diverse DL workloads. However, the prediction-based schedulers have disadvantages on prediction accuracy and offline-profiling costs. In this paper, we propose a learning-based scheduler, which models the scheduling problem as a reinforcement learning problem, achieving minimum average job completion time and maximum system utilization. The scheduler contains the designs of state space, action space, reward function and update scheme. Furthermore, we will evaluate our proposed scheduler implemented as a plugin of Tensorflow on real cluster and large-scale simulation.",https://ieeexplore.ieee.org/document/8845276/,IEEE INFOCOM 2019 - IEEE Conference on Computer Communications Workshops (INFOCOM WKSHPS),29 April-2 May 2019,ieeexplore
10.1109/SEC50012.2020.00019,Poster: Lambda architecture for robust condition based maintenance with simulated failure modes,IEEE,Conferences,"Condition based maintenance (CBM) is increasingly seen as a promising approach for addressing downtime issues which are a common occurrence in the manufacturing industry and are a major cause of lost productivity. However, it has been a challenge to develop a generic CBM solution that works for all assets since each asset has unique sources of noise. This mandates use of manual diagnostics to custom tailor a solution for each asset for accurate failure mode identification (FMI). This problem is further compounded by the scarcity of failure data. In this paper, we propose a lambda architecture for FMI of industrial assets that achieves low initial deployment cost while securing a reasonable classification accuracy. The lambda architecture consists of a light-compute edge node, such as Raspberry Pi, that processes high-speed vibration data in real-time to extract useful features and applies a deep-learning (DL) engine which is trained in a cloud platform, such as AWS. In addition, we also incorporate a failure modes' feature simulator so that DL models can adapt to different industrial assets without costly failure data collection. Finally, experimental results are provided using the bearings' failures dataset validating the proposed cost-effective CBM architecture with high accuracy and scalability.",https://ieeexplore.ieee.org/document/9355694/,2020 IEEE/ACM Symposium on Edge Computing (SEC),12-14 Nov. 2020,ieeexplore
10.1109/ICECCE52056.2021.9514218,Predicting “Maintenance Priority” with AI,IEEE,Conferences,"In Tiipras oil refineries, an average of 100 thousand maintenance requests are created annually for more than 140 thousand pieces of equipment. These requests are prioritized manually by chief experts with over 25 years of experience and classified as urgent or planned. If maintenance requests that need to be solved urgently in the refining industry are mislabeled and delayed, they may cause process upsets leading to health &amp; safety hazards, environment problems or big asset damage. To minimize this risk, we think that supporting the decision mechanism with algorithms and cross checking/replacing human decisions by using today's AI technologies is the right approach that reduces the possibility of human error. In this study, our main goal is to automate maintenance prioritization process with supervised and unsupervised ML algorithms, deploy an AI system and achieve high accuracy. Our study was carried out basically in 4 main steps: • Exploratory Data Analysis • Clustering - Feature Addition - Feature Selection • Model Selection and Results • Additional Studies With this study, we aim to explain our AI study, share our experience with other partners that have similar needs and provide them an effective tool and systematic approach about management of transition from human to machine with a real industry case. We believe that the transfer of priority selection process from human to algorithms ensure consistent decisions, reduce costs and tolerate experience losses.",https://ieeexplore.ieee.org/document/9514218/,"2021 International Conference on Electrical, Communication, and Computer Engineering (ICECCE)",12-13 June 2021,ieeexplore
10.1109/ICRITO51393.2021.9596436,Prediction of Sensor Faults and Outliers in IoT Devices,IEEE,Conferences,"Internet of Things (IoT) is tremendously growing and interacting with the physical world in the era of Industry 4.0. In near future, billions of companies will have advanced communication technology and it will increase the growth of critical systems. The accuracy measurement of the functionality of a critical system is a challenging job. Fault Tolerance (FT) is a major concern to ensure the dependability, availability and reliability of critical systems. Faults should be predicted and controlled proactively to lessen failure impact on the critical systems. To predict these failures and use the relevant procedure to avoid it before it actually occurs, FT techniques are used. These techniques are implemented in critical systems to avoid failures as the security of systems is more important than the reliability of systems. It minimizes the effect of faults that are being investigated. FT techniques work on a concept that if the system is built differently then it should fail differently. If a redundant variant fails then atleast the other one should give a satisfactory result. This study exhibits an analysis of existing FT techniques like N-version programming, Recovery blocks and N-self-checking programming. A critical study of sensor faults and outliers prediction models in IoT is presented. A bibliometric analysis is also carried out on 716 Scopus indexed publications to analyze the current research trends in this domain.",https://ieeexplore.ieee.org/document/9596436/,"2021 9th International Conference on Reliability, Infocom Technologies and Optimization (Trends and Future Directions) (ICRITO)",3-4 Sept. 2021,ieeexplore
10.1109/ITSC45102.2020.9294450,Predictive maintenance leveraging machine learning for time-series forecasting in the maritime industry,IEEE,Conferences,"One of the key challenges in the maritime industry refers to minimizing the time a vessel cannot be utilized, which has multiple effects. The latter is addressed through maintenance approaches that however in many cases are not efficient in terms of cost and downtime. Predictive maintenance provides optimized maintenance scheduling offering extended vessel lifespan, coupled with reduced maintenance costs. As in several industries, including the maritime domain, an increasing amount of data is made available through the deployment and exploitation of data sources, such as on board sensors that provide real-time information. These data provide the required ground for analysis and thus support for various types of data-driven decision making. In the maritime domain, sensors are deployed on vessels to monitor their engines and data analysis tools are needed to assist engineers towards reduced operational risk through predictive maintenance solutions that are put in place. In this paper, we present an approach for anomaly detection on time-series data, utilizing machine learning on the vessels sensor data, in order to predict the condition of specific parts of the vessel's main engine and thus facilitate predictive maintenance. The novel characteristic of the proposed approach refers both to the inclusion of new innovative models to address the case of predictive maintenance in maritime and the combination of those different models, highlighting an improved result in terms of evaluation metrics.",https://ieeexplore.ieee.org/document/9294450/,2020 IEEE 23rd International Conference on Intelligent Transportation Systems (ITSC),20-23 Sept. 2020,ieeexplore
10.1109/ARITH.2019.00047,Privacy-Preserving Deep Learning via Additively Homomorphic Encryption,IEEE,Conferences,"We aim at creating a society where we can resolve various social challenges by incorporating the innovations of the fourth industrial revolution (e.g. IoT, big data, AI, robot, and the sharing economy) into every industry and social life. By doing so the society of the future will be one in which new values and services are created continuously, making people's lives more conformable and sustainable. This is Society 5.0, a super-smart society. Security and privacy are key issues to be addressed to realize Society 5.0. Privacy-preserving data analytics will play an important role. In this talk we show our recent works on privacy-preserving data analytics such as privacy-preserving logistic regression and privacy-preserving deep learning. Finally, we show our ongoing research project under JST CREST “AI”. In this project we are developing privacy-preserving financial data analytics systems that can detect fraud with high security and accuracy. To validate the systems, we will perform demonstration tests with several financial institutions and solve the problems necessary for their implementation in the real world.",https://ieeexplore.ieee.org/document/8877418/,2019 IEEE 26th Symposium on Computer Arithmetic (ARITH),10-12 June 2019,ieeexplore
10.1109/GloSIC.2018.8570124,Probabilistic Estimations of Increasing Expected Reliability and Safety for Intelligent Manufacturing,IEEE,Conferences,"In the near future the possibilities of the modern probabilistic models, artificial intelligence and machine learning methods can provide an intelligent support of making decisions by an operator in real time. An agile recovery of intelligent manufacturing integrity can be implemented owing to the development of industrial robotics. For intelligent manufacturing it means the expected reliability and safety may be in the near future at the expense of intelligent support of decision making and the agile recovery of integrity. To answer the question “How much essential may be this increasing?” here are proposed: general analytical approaches for a probabilistic estimation of the expected reliability and safety for every monitored element or the system of intelligent manufacturing on a level of probability distribution functions (PDF) of the time between the losses of system integrity; estimations of increasing the expected reliability and safety for intelligent manufacturing at the expense of the intelligent support of decision making and agile recovery of integrity; the comparisons of the estimations on a prognostic period up to 10 years using the identical model in applications to expected reliability and safety. The applications of the proposed approaches allow the customers, designers, developers, users and experts of Industry 4.0 intelligent manufacturing to be guided by the proposed probabilistic estimations for solving problems of reliability and safety in the system life cycle. The results are demonstrated by examples.",https://ieeexplore.ieee.org/document/8570124/,2018 Global Smart Industry Conference (GloSIC),13-15 Nov. 2018,ieeexplore
10.1109/IROS.2016.7759479,Probabilistic approaches for self-tuning path tracking controllers using prior knowledge of the terrain,IEEE,Conferences,"Nowadays, agricultural and mining industry applications require saving energy in mobile robotic tasks. This critical issue encouraged us to enhance the performance of path tracking controllers during manoeuvring over slippery and rough terrains. In this scenario, we propose probabilistic approaches under machine learning schemes in order to optimally self-tune the controller. The approaches are real time implemented and tested in a mining machinery skid steer loader Cat® 262C under gravel and muddy terrains (and their transitions). Finally, experimental results presented in this work show that the performance of the controller enhances up to 20% (average) without compromising saturations in the actuators.",https://ieeexplore.ieee.org/document/7759479/,2016 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),9-14 Oct. 2016,ieeexplore
10.1109/IWOBI47054.2019.9114431,Proof of Concept: Using Reinforcement Learning agent as an adversary in Serious Games,IEEE,Conferences,"This article focuses on simple rehabilitation video-game called Flying with Friends. The rehabilitation industry has been experiencing a boom in recent years, coupled with the growing popularity of virtual reality technology, a drop in prices for these technologies and the expansion entertainment industry in the form of computer games. The goal of this experiment was to provide a proof that such systems are viable option when it comes to artificial intelligence systems in serious video-games, but not limited to only serious ones. The solution described in this article, in cooperation with experts, is going to be deployed in a real rehabilitation environment.",https://ieeexplore.ieee.org/document/9114431/,2019 IEEE International Work Conference on Bioinspired Intelligence (IWOBI),3-5 July 2019,ieeexplore
10.1109/PC.2017.7976254,Proposal of system for automatic weld evaluation,IEEE,Conferences,"The paper deals with the development of a system for automatic weld recognition using new information technologies based on cloud computing and single-board computer in the context of Industry 4.0. The proposed system is based on a visual system for weld recognition, and a neural network based on cloud computing for real-time weld evaluation, both implemented on a single-board low-cost computer. The proposed system was successfully verified on welding samples which correspond to a real welding process in the car production process. The system considerably contributes to the welds diagnostics in industrial processes of small- and medium-sized enterprises.",https://ieeexplore.ieee.org/document/7976254/,2017 21st International Conference on Process Control (PC),6-9 June 2017,ieeexplore
10.1109/ACC.2008.4587179,Prototype design of a multi-agent system for integrated control and asset management of petroleum production facilities,IEEE,Conferences,"This paper addresses a practical intelligent multi- agent system for asset management for the petroleum industry, which is crucial for profitable oil and gas facilities operations and maintenance. A research project was initiated to study the feasibility of an intelligent asset management system. Having proposed a conceptual model, architecture, and implementation plan for such a system in previous work and defined its autonomy, communications, and artificial intelligence (AI) requirements, we are proceeding to build a system prototype and simulate it in real time to validate its logical behavior in normal and abnormal process situations. We also conducted a thorough system performance analysis to detect any computational bottlenecks. Although the preliminary system prototype design has limitations, simulation results have demonstrated an effective system logical behavior and performance.",https://ieeexplore.ieee.org/document/4587179/,2008 American Control Conference,11-13 June 2008,ieeexplore
10.1109/ICCChinaW.2018.8674519,R-CNN Object Detection Inference With Deep Learning Accelerator,IEEE,Conferences,"The explosively increasing demands of high-speed data applications have brought massive access requirements to various mobile devices. As integrating with artificial intelligence and neural network, the mobile device industry is often more concerned with faster inference with lower power consumption, bringing deep learning inference acceleration to the spotlight. In this paper, we perform a neural network inference merging R-CNN, an object detection model, into a deep learning accelerator architecture. It is a brand new implementation of neural network on embedded system hardware IP-cores of edge computation. On one hand, based on the embedded system environment, we implement the image pre-processing with region proposal algorithm and image post-processing with NMS method. On the other hand, we perform the feature computation with the deep learning accelerator through optimized software and hardware configurations. Through this method, we solve the problem of time-consuming in the computation of neural network layers and give a precise and real-time prediction of object detection. Our R-CNN inference achieves impressive results with 1.9 to 2.6 times higher performance compared with other inference processors.",https://ieeexplore.ieee.org/document/8674519/,2018 IEEE/CIC International Conference on Communications in China (ICCC Workshops),16-18 Aug. 2018,ieeexplore
10.1109/ICCICC46617.2019.9146036,RTPA-based Software Generation by AI Programming,IEEE,Conferences,"AI programming toward autonomous software generation is not only a highly demanded technology by the software industry, but also a hard challenge to the theories of software engineering and computational intelligence. A methodology and tool for autonomous program generation (APG) are recently developed based on Real-Time Process Algebra (RTPA). This paper demonstrates an experimental result of autonomous code generation on a digital clock system by the APG tool. The experimental results indicate a novel approach towards AI programming for machine-enabled software generation theories and technologies.",https://ieeexplore.ieee.org/document/9146036/,2019 IEEE 18th International Conference on Cognitive Informatics & Cognitive Computing (ICCI*CC),23-25 July 2019,ieeexplore
10.1109/CSCI51800.2020.00085,Radically Simplifying Game Engines: AI Emotions &amp; Game Self-Evolution,IEEE,Conferences,"Today, video games are a multi-billion-dollar industry, continuously evolving through the incorporation of new technologies and innovative design. However, current video game software content creation requires extensive and often-times ambiguous planning phases for developing aesthetics, online capabilities, and gameplay mechanics. Design elements can vary significantly relative to the expertise of artists, designers, budget, and overall game engine/software features and capabilities. Game development processes are often extensively long coding sessions, usually involving a highly iterative creative process, where user requirements are rarely provided. Therefore, we propose significantly simplifying game design and development with novel Artificial Cognition Architecture real-time scalability and dynamic emotion core. Rather than utilizing more static emotion state weighting emotion engines (e.g. ExAI), we leverage significant ACA research in successful implementation of analog neural learning bots with Maslowan objective function algorithms. We also leverage AI- based Artificial Psychology software which utilizes ACA's fine grained self-evolving emotion modeling in humanistic avatar patients for Psychologist training. An ACA common cognitive core provides the gaming industry with wider applications across video game genres. A modular, scalable, and cognitive emotion game architecture implements Non-Playable Character (NPC) learning and self-evolution. ACA models NPC's with fine grained emotions, providing interactive dynamic personality traits for a more realistic game environment and enables NPC self-evolution under the influence of both other NPC's and players. Furthermore, we explore current video game design engine architecture (e.g. Unity, Unreal Engine) and propose an ACA integration approach. We apply artificial cognition and emotion intelligence modeling to engender video games with more distinct, realistic consumer gaming experiences, while simultaneously minimizing software gaming development efforts and costs.",https://ieeexplore.ieee.org/document/9457899/,2020 International Conference on Computational Science and Computational Intelligence (CSCI),16-18 Dec. 2020,ieeexplore
10.1109/INFOCOM.2019.8737649,ReLeS: A Neural Adaptive Multipath Scheduler based on Deep Reinforcement Learning,IEEE,Conferences,"The Multipath TCP (MPTCP) protocol, featured by its ability of capacity aggregation across multiple links and connectivity maintenance against single-path failure, has been attracting increasing attention from the industry and academy. Multipath packet scheduling is a unique and fundamental mechanism for the design and implementation of MPTCP, which is responsible for distributing the traffic over multiple subflows. The existing multipath schedulers are facing the challenges of network heterogeneities, comprehensive QoS goals, and dynamic environments, etc. To address these challenges, we propose ReLeS, a Reinforcement Learning based Scheduler for MPTCP. ReLeS uses modern deep reinforcement learning (DRL) techniques to learn a neural network to generate the control policy for packet scheduling. It adopts a comprehensive reward function that takes diverse QoS characteristics into consideration to optimize packet scheduling. To support real-time scheduling, we propose an asynchronous training algorithm that enables parallel execution of packet scheduling, data collecting, and neural network training. We implement ReLeS in the Linux kernel and evaluate it over both emulated and real network conditions. Extensive experiments show that ReLeS significantly outperforms the state-of-the-art schedulers.",https://ieeexplore.ieee.org/document/8737649/,IEEE INFOCOM 2019 - IEEE Conference on Computer Communications,29 April-2 May 2019,ieeexplore
10.1109/IMITEC50163.2020.9334129,Real Time Customer Churn Scoring Model for the Telecommunications Industry,IEEE,Conferences,"There are two types of customers in the telecommunication industry; the pre-paid and the contract customers. In South Africa it is the pre-paid customers that keep telcos constantly worried because such customers do not have anything binding them to the company, they can leave and join a competitor at any time. To retain such customers, telcos need to customise suitable solutions especially for those customers that are agitating and can churn at any time. This needs customer churn prediction models that would take advantage of big data analytics and provide the telco industry with a real time solution. The purpose of this study was to develop a real time customer churn prediction model. The study used the CRISP-DM methodology and the three machine learning algorithms for implementation. Watson Studio software was used for the model prototype deployment. The study used the confusion matrix to unpack a number of performance measures. The results showed that all the models had some degree of misclassification, however the misclassification rate of the Logistic Regression was very minimal (2.2%) as differentiated from the Random Forest and the Decision Tree, which had misclassification rates of 20.8% and 21.7% respectively. The results further showed that both Random Forest and the Decision Tree had good accuracy rates of 78.3% and 79.2% respectively, although they were still not better than that of the Logistic Regression. Despite the two having good accuracy rates, they had the highest rates of misclassification of class events. The conclusion we drew from this was that, accuracy is not a dependable measure for determining model performance.",https://ieeexplore.ieee.org/document/9334129/,2020 2nd International Multidisciplinary Information Technology and Engineering Conference (IMITEC),25-27 Nov. 2020,ieeexplore
10.1109/ICCICT.2018.8325873,Real time control of induction motor using neural network,IEEE,Conferences,"Induction Machine being simple in operation and highly reliable equipment with low cost involving minimal maintenance requirements it has become the most popular equipment in industry. With the development of power electronic technology, low cost DSP, micro-controllers and parameter estimation techniques the induction motor an attractive component for the future high performance drives induction motors have many applications in the industries. The PWM technique which drives a Voltage Source Inverter (VSI) in order to apply v/f control is used to control a 3 ph induction motor. In industrial applications the most widely used controllers are PI controllers because of their simple structure and their capability of delivering good performance over a large band of operating condition. PI and ANN controllers have been designed and developed using MATLAB/SIMULINK. Prototype model is developed to validate the effectiveness of the PI and ANN control of induction motor drive using dSPACE DS1104 controller. The performance of the SVPWM based induction motor in open loop and closed loop is presented with simulation. Artificial Neural Network and Conventional PI controllers have been practically implemented using SVPWM based VSI fed induction motor in open loop mode. Hardware set up has been developed using Inverter and dSPACE controller. The real time performance of ANN based induction motor is presented by validating simulation results with the hardware results.",https://ieeexplore.ieee.org/document/8325873/,2018 International Conference on Communication information and Computing Technology (ICCICT),2-3 Feb. 2018,ieeexplore
10.1109/INDUSCON51756.2021.9529474,Real-Time Downhole Geosteering Data Processing Using Deep Neural Networks On FPGA,IEEE,Conferences,"The success of machine learning has spread the deployment of Deep neural Networks (DNNs) in numerous industrial applications. As an essential technique in today’s oilfield industry, geosteering requires performing DNN inference on the hardware devices that operates under the severe down-hole environments. However, it can produce massive power dissipation and cause long delays to execute the computation-intensive DNN inference on the current hardware platforms, e.g., CPU and GPU. In this paper, we propose an FPGA-based hardware design to efficiently conduct the DNN inference for geosteering tasks in downhole environments. At first, a comprehensive analysis is presented to choose the optimal computation mapping method for the target DNN model. A detailed description of the customized hardware implementation is then proposed to accomplish a complete DNN inference on the FPGA board. The experimental results shows that the proposed design achieves 7× (1.4×) improvement on performance and 82× (1.3×) reduction on power consumption compared with CPU(GPU).",https://ieeexplore.ieee.org/document/9529474/,2021 14th IEEE International Conference on Industry Applications (INDUSCON),15-18 Aug. 2021,ieeexplore
10.1109/ICECCE52056.2021.9514169,Real-Time Fraud Prediction On Streaming Customer-Behaviour Data,IEEE,Conferences,"The field of detection and analysis of fraud situations continues to be an essential topic in the telecom industry. This study proposes a methodology that can predict abnormal behavior situations on real-time streaming customer behavior data in the telecommunication domain. The prototype implementation of the proposed method is designed, developed, and applied to the telecommunications dataset. We perform performance tests for the method's prediction success and scalability metrics on the designed prototype application. The results indicate that the proposed method proves to be a promising approach in the telecommunication sector.",https://ieeexplore.ieee.org/document/9514169/,"2021 International Conference on Electrical, Communication, and Computer Engineering (ICECCE)",12-13 June 2021,ieeexplore
10.1109/08IAS.2008.164,Real-Time Implementation of Intelligent Modeling and Control Techniques on a PLC Platform,IEEE,Conferences,"Programmable logic controllers (PLCs) have been used for many decades for standard control in industrial and factory environments. Over the years, PLCs have become computational efficient and powerful, and a robust platform with applications beyond the standard control and factory automation. Due to the new advanced PLC's features and computational power, they are ideal platforms for exploring advanced modeling and control methods, including computational intelligence based techniques such as neural networks, particle swarm optimization (PSO) and many others. Some of these techniques require fast floating-point calculations that are now possible in real-time on the PLC. This paper focuses on the Allen-Bradley ControlLogix brand of PLCs, due to their high performance and extensive use in industry. The design and implementation of a neurocontroller consisting of two neural networks, one for modeling and the other for control, and the training of these neural networks with particle swarm optimization is presented in this paper on a single PLC. The neurocontroller in this study is a power system stabilizer (PSS) that is used for power system oscillation damping. The PLC is interfaced to a power system simulated on the real time digital simulator. Real time results are presented showing that the PLC is a suitable hardware platform for implementing advanced modeling and control techniques for industrial applications.",https://ieeexplore.ieee.org/document/4658952/,2008 IEEE Industry Applications Society Annual Meeting,5-9 Oct. 2008,ieeexplore
10.1109/MobServ.2016.24,Real-Time Privacy Preserving Crowd Estimation Based on Sensor Data,IEEE,Conferences,"As one of the popular topics to ensure public safety, crowd estimation has attracted lots of attentions from both industry and academia. Most of traditional crowd estimation approaches rely on sophisticated computer vision algorithms to estimate crowd based on camera data, therefore suffering from privacy issues and high deployment and data processing cost. In this paper we present a sensor fusion based approach to real-time crowd estimation based on privacy-conscious and inexpensive sensors. The approach has been implemented and verified first by a small scale deployment at our lab, and then tested based on a 3-month trial at a shopping mall in Singapore. A deep analysis has been carried out based on the data sets collected from the trial, showing promising results: (1) the data from CO2, sound pressure and infrared sensors are influential in estimating crowd levels for indoor environments, (2) Random Forest and C4.5 are identified as the more suitable supervised learning models, (3) an accuracy of 95% can be achieved by our crowd estimation system in a real scenario. In contrast to the state of the art, our approach is privacy preserving and can provide comparable estimation accuracy with lower deployment and processing cost and better applicability for large scale setups. It can be used either as an alternative solution when user privacy must be enforced or as a complementary solution to camera-based crowd estimation when privacy is less concerned because of pubic safety.",https://ieeexplore.ieee.org/document/7787060/,2016 IEEE International Conference on Mobile Services (MS),27 June-2 July 2016,ieeexplore
10.1109/ICIS.2018.8466525,Real-Time Tracing Of A Weld Line Using Artificial Neural Networks,IEEE,Conferences,"Robotic Manipulators are becoming increasingly popular nowadays with applications in almost every industry and production line. It is difficult but essential to create a common algorithm for the different types of manipulators present in todays market so that automation can be achieved at a faster rate. This paper aims to present a real time implementation of a method to control a Tal Brabo! Robotic manipulator to move along a given weld line in order to be utilized in factories for increasing production capacity and decreasing production time. The controller used here is provided by Trio, whose ActiveX component is interfaced to MATLAB. Images were captured to identify weld lines in every possible alignment to find points of interest and the neural network was trained in order to follow a given weld line once the work-piece was placed on the work-table.",https://ieeexplore.ieee.org/document/8466525/,2018 IEEE/ACIS 17th International Conference on Computer and Information Science (ICIS),6-8 June 2018,ieeexplore
10.1109/AI4I46381.2019.00022,Real-World Conversational AI for Hotel Bookings,IEEE,Conferences,"In this paper, we present a real-world conversational AI system to search for and book hotels through text messaging. Our architecture consists of a frame-based dialogue management system, which calls machine learning models for intent classification, named entity recognition, and information retrieval subtasks. Our chatbot has been deployed on a commercial scale, handling tens of thousands of hotel searches every day. We describe the various opportunities and challenges of developing a chatbot in the travel industry.",https://ieeexplore.ieee.org/document/9027787/,2019 Second International Conference on Artificial Intelligence for Industries (AI4I),25-27 Sept. 2019,ieeexplore
10.1109/ICMLC.2012.6359610,Real-time dynamic vehicle detection on resource-limited mobile platform,IEEE,Conferences,"Given the rapid expansion of car ownership worldwide, vehicle safety is an increasingly critical issue in the automobile industry. The reduced cost of cameras and optical devices has made it economically feasible to deploy front-mounted intelligent systems for visual-based event detection. Prior to vehicle event detection, detecting vehicles robustly in real time is challenging, especially conducting detection process in images captured by a dynamic camera. Therefore, in this paper, a robust vehicle detector is developed. Our contribution is three-fold. Road modeling is first proposed to confine detection area for maintaining low computation complexity and reducing false alarms as well. Haar-like features and eigencolors are then employed for the vehicle detector. To tackle the occlusion problem, chamfer distance is used to estimate the probability of each individual vehicle. AdaBoost algorithm is used to select critical features from a combined high dimensional feature set. Experiments on an extensive dataset show that our proposed system can effectively detect vehicles under different lighting and traffic conditions, and thus demonstrates its feasibility in real-world environments.",https://ieeexplore.ieee.org/document/6359610/,2012 International Conference on Machine Learning and Cybernetics,15-17 July 2012,ieeexplore
10.1109/SACI51354.2021.9465544,Real-time locating system and digital twin in Lean 4.0,IEEE,Conferences,"Digital twin plays a key role in the current development of smart manufacturing systems. Through simulation in the cyber world, real phenomena in the physical world can be predicted and optimized before the final implementation. The usage of the digital twin is enhanced along with the uprising of Industry 4.0, in which data availability supports the further insight of system status, helping the operation managers understand their system and perform resources adjustment more easily. Based on this digitization mature, Lean 4.0, a new concept elaborated from Lean manufacturing, has been interested recently. There are several technologies constituted digital twin that provide a favourable condition for Lean 4.0, such as augmented reality, cloud computing. In this paper, the integration of the Real-time Locating System (RTLS) into digital twin is proposed, which facilitates the performance of Lean 4.0 in manufacturing operation. Not only gain effective control over the facility's assets, but this integration also enhances the resources utilization, cut down operational wastes, thus brings a better turnover for industrial systems. A case study of successful implementation is shown, which proved the possible advantages of this approach.",https://ieeexplore.ieee.org/document/9465544/,2021 IEEE 15th International Symposium on Applied Computational Intelligence and Informatics (SACI),19-21 May 2021,ieeexplore
10.1109/ICSensT.2017.8304431,Real-time monitoring of powder blend composition using near infrared spectroscopy,IEEE,Conferences,"Near Infrared Spectroscopy (NIRS) is a very powerful utility in a Process Analytical Technology (PAT) system because it can be used to monitor a multitude of process parameters non-invasively, non-destructively in real time and in hazardous environments. A catch to the versatility of NIRS is the requirement for Multi-Variate Data Analysis (MVDA) to calibrate the measurement of the parameter of interest. This paper presents a NIRS based real time continuous monitoring of powder blend composition which has widespread applications such as the pharmaceutical industry. The proposed system design enables reduction of optical path length so that the sensors can be successfully installed into powder conveyance systems. Sensor signal processing techniques were developed in this work to improve accuracy while minimizing pre-processing steps. The paper presents the implementation of several parameter estimation methodologies applied to sensor data collected using MATLAB® software for a model powder blending process. Several techniques were examined for the development of chemometric models of the multi-sensor data, including Principal Component Analysis (PCA), Partial Least Squares Regression (PLSR), Support Vector Machines (SVM) and Artificial Neural Networks (ANN). The performances of each of the models were compared in terms of accuracy (MSE) in predicting blend composition. The results obtained show that machine learning-based approaches produce process models of similar accuracy and robustness compared to models developed by PLSR while requiring minimal pre-processing and also being more adaptable to new data.",https://ieeexplore.ieee.org/document/8304431/,2017 Eleventh International Conference on Sensing Technology (ICST),4-6 Dec. 2017,ieeexplore
10.1109/ICCCN52240.2021.9522281,Realization of an Intrusion Detection use-case in ONAP with Acumos,IEEE,Conferences,"With Software-Defined Networking and Machine Learning/Artificial Intelligence (ML/AI) reaching new paradigms in their corresponding fields, both academia and industry have exhibited interests in discovering unique aspects of intelligent and autonomous communication networks. Transforming such intentions and interests to reality involves software development and deployment, which has its own story of significant evolution. There has been a notable shift in the strategies and approaches to software development. Today, the divergence of tools and technologies as per demand is so substantial that adapting a software application from one environment to another could involve tedious redesign and redevelopment. This implies enormous effort in migrating existing applications and research works to a modern industrial setup. Additionally, the struggles with sustainability maintenance of such applications could be painful. Concerning ML/AI, the capabilities to train, deploy, retrain, and re-deploy AI models as quickly as possible will be crucial for AI-driven network systems. An end-to-end workflow using unified open-source frameworks is the need of the hour to facilitate the integration of ML/AI models into the modern software-driven virtualized communication networks. Hence, in our paper, we present such a prototype by demonstrating the journey of a sample SVM classifier from being a python script to be deployed as a micro-service using ONAP and Acumos. While illustrating various features of Acumos and ONAP, this paper intends to make readers familiar with an end-to-end workflow taking advantage of the integration of both open-source platforms.",https://ieeexplore.ieee.org/document/9522281/,2021 International Conference on Computer Communications and Networks (ICCCN),19-22 July 2021,ieeexplore
10.1109/ISAMSR53229.2021.9567891,Recent and Future Innovative Artificial Intelligence Services and Fields,IEEE,Conferences,"Recent innovative Artificial Intelligence (AI) solutions accelerate digital transformations in different fields. It is important to highlight and explore this innovative AI service in different domains so that digital transformation can be planned, designed, and implemented for maximum society benefits. This paper investigates the different fields of AI services that can be utilized towards achieving highly beneficial digital transformations in different societal domains. This includes marketing, finance and banking, healthcare industry, emotion, and creative AI, as well as recent AI fields as explainable and responsible AI. Exploring and understanding the innovative AI services in these domains widen researcher capabilities to achieve effective and highly beneficial digital transformations.",https://ieeexplore.ieee.org/document/9567891/,"2021 4th International Symposium on Agents, Multi-Agent Systems and Robotics (ISAMSR)",6-8 Sept. 2021,ieeexplore
10.1109/RAMS.2002.981623,Reliability improvement of airport ground transportation vehicles using neural networks to anticipate system failure,IEEE,Conferences,"This paper describes a joint industry/university collaboration to develop a prototype system to provide real time monitoring of an airport ground transportation vehicle with the objectives of improving availability and minimizing field failures by estimating the proper timing for condition-based maintenance. Hardware for the vehicle was designed, developed and tested to monitor door characteristics (voltage and current through the motor that opens and closes the doors and door movement time and position), to quickly predict degraded performance, and to anticipate failures. A combined statistical and neural network approach was implemented. The neural network ""learns"" the differences among door sets and can be tuned quite easily through this learning. Signals are processed in real time and combined with previous monitoring data to estimate, using the neural network, the condition of the door set relative to maintenance needs. The prototype system was installed on several vehicle door sets at the Pittsburgh International Airport and successfully tested for several months under simulated and actual operating conditions. Preliminary results indicate that improved operational reliability and availability can be achieved.",https://ieeexplore.ieee.org/document/981623/,Annual Reliability and Maintainability Symposium. 2002 Proceedings (Cat. No.02CH37318),28-31 Jan. 2002,ieeexplore
10.1109/WINCOM.2018.8629594,"Reliable and cost-effective communication at high seas, for a safe operation of autonomous ship",IEEE,Conferences,"Nowadays, the high automation of ships has resulted in an ever -increasing need for real time data exchange to monitor ships, cargo, and ensure safety and security on board. This evolution in ship technology and management, must be supported by reliable, and cost-effective communication carriers. The satellite communication services can deliver vital real time capability to keep the ship continuously connected to the maritime players while it is trading worldwide. At high seas, the only communication service that can meet the ship need in term of data exchange is the mobile satellite communication. The reliability of these communication services will contribute significantly in the success of the future implementation of the autonomous ship in the maritime industry. This paper consists of the presentation of an overview of the current satellite services and its comparison in term of reliability, cost-effectiveness, and its capability to meet the autonomous ship communication need for its safe operation. This study results in a proposal of the suitable satellite communication services that can support the implementation of autonomous ship.",https://ieeexplore.ieee.org/document/8629594/,2018 6th International Conference on Wireless Networks and Mobile Communications (WINCOM),16-19 Oct. 2018,ieeexplore
10.1109/DIS.2006.63,Remote Programming of Multirobot Systems within the UPC-UJI Telelaboratories: System Architecture and Agent-Based Multirobot Control,IEEE,Conferences,"One of the areas that needs more improvement within the e-learning environments via Internet (in fact they suppose a very big effort to be accomplished) is allowing students to access and practice real experiments is a real laboratory, instead of using simulations in Marin, R. et al. (2003). Real laboratories allow students to acquire methods, skills and experience related to real equipment, in a manner that is very close to the way they are being used in industry. The purpose of the project is the study, development and implementation of an e-learning environment to allow undergraduate students to practice subjects related to Robotics and Artificial Intelligence. The system, which is now at a preliminary stage, will allow the remote experimentation with real robotic devices (i.e. robots, cameras, etc.). It will enable the student to learn in a collaborative manner (remote participation with other students) where it will be possible to combine the on-site activities (performed ""in-situ"" within the real lab during the normal practical sessions), with the ""online"" one (performed remotely from home via the Internet). Moreover, the remote experiments within the e-laboratory to control the real robots can be performed by both, students and even scientist. This project is under development and it is carried out jointly by two Universities (UPC and UJI). In this article we present the system architecture and the way students and researchers have been able to perform a remote programming of multirobot systems via Web",https://ieeexplore.ieee.org/document/1633445/,IEEE Workshop on Distributed Intelligent Systems: Collective Intelligence and Its Applications (DIS'06),15-16 June 2006,ieeexplore
10.1109/DTPI52967.2021.9540104,Research and practice of lightweight digital twin speeding up the implementation of flexible manufacturing systems,IEEE,Conferences,"Parallel manufacturing in Industry 5.0 requires digital twin to digitize physical systems, building virtual models to open up channels connecting physical systems, information systems, and social systems, and transforming the physical models of the existing production environment to achieve two-way feedback of virtual and real is the current research direction. This paper proposes the modeling idea of lightweight digital twin, extracts core dimensions and performs digital virtual simulation, so as to quickly realize the complete process of two-way feedback, and realize a set of chess flexible parallel manufacturing production lines as a practice for the design of complete lightweight digital twin.",https://ieeexplore.ieee.org/document/9540104/,2021 IEEE 1st International Conference on Digital Twins and Parallel Intelligence (DTPI),15 July-15 Aug. 2021,ieeexplore
10.1109/AIEA53260.2021.00017,Research on Textile Defect Detection Based on Improved Cascade R-CNN,IEEE,Conferences,"Textile fabric defects are an important factor affecting the quality of textile fabrics. Traditional manual-based textile defect detection methods are slow and inefficient, and cannot meet the production needs of the modern textile industry. Aiming at the process and technical difficulties of textile defect detection, this paper analyzes different defect detection algorithms based on deep learning through theoretical research and experimental comparison, and finally designs and implements a defect detection algorithm based on improved Cascade R-CNN. Through the introduction of FPN on the basic structure of Cascade R-CNN, the defect detection experiment based on improved Cascade R-CNN is designed and realized. This paper discusses the defect detection model design and network structure based on the improved Cascade R-CNN, and uses the multi-scale detection characteristics of FPN to detect textile defects. The experimental results show that, compared with the defect detection based on Cascade R-CNN, the accuracy of the improved Cascade R-CNN model defect detection is increased by 4.09% to 95.43%, and the improved model has better detection of multi-scale defects effect.",https://ieeexplore.ieee.org/document/9525547/,2021 International Conference on Artificial Intelligence and Electromechanical Automation (AIEA),14-16 May 2021,ieeexplore
10.1109/DSAA.2016.32,Reserve Price Optimization at Scale,IEEE,Conferences,"Online advertising is a multi-billion dollar industry largely responsible for keeping most online content free and content creators (""publishers"") in business. In one aspect of advertising sales, impressions are auctioned off in second price auctions on an auction-by-auction basis through what is known as real-time bidding (RTB). An important mechanism through which publishers can influence how much revenue they earn is reserve pricing in RTB auctions. The optimal reserve price problem is well studied in both applied and academic literatures. However, few solutions are suited to RTB, where billions of auctions for ad space on millions of different sites and Internet users are conducted each day among bidders with heterogenous valuations. In particular, existing solutions are not robust to violations of assumptions common in auction theory and do not scale to processing terabytes of data each hour, a high dimensional feature space, and a fast changing demand landscape. In this paper, we describe a scalable, online, real-time, incrementally updated reserve price optimizer for RTB that is currently implemented as part of the AppNexus Publisher Suite. Our solution applies an online learning approach, maximizing a custom cost function suited to reserve price optimization. We demonstrate the scalability and feasibility with the results from the reserve price optimizer deployed in a production environment. In the production deployed optimizer, the average revenue lift was 34.4% with 95% confidence intervals (33.2%, 35.6%) from more than 8 billion auctions over 46 days, a substantial increase over non-optimized and often manually set rule based reserve prices.",https://ieeexplore.ieee.org/document/7796939/,2016 IEEE International Conference on Data Science and Advanced Analytics (DSAA),17-19 Oct. 2016,ieeexplore
10.1109/ICE/ITMC52061.2021.9570221,Resilient Manufacturing Systems enabled by AI support to AR equipped operator,IEEE,Conferences,"Supply chains and manufacturing systems robustness and resilience are, for many years, but especially nowadays, key features requested to ensure reliable and efficient production processes. Two domains are crucial to achieve such purpose: the former is fast and comprehensive monitoring, efficient and reliable condition detection and effective and explicable support for decision making. The latter refers to the intervention by operators, able to better identify problems and to put in place effective operations aimed at fixing it or, better, to prevent such circumstances. This paper presents an integrated approach encompassing a sophisticated IoT and AI-based approach to monitor and detect critical situations, fully integrated with an AR (Augmented Reality) system supporting operators in the field to take informed actions in bi-directional continuous connection. Activities in the context of EC funded project Qu4lity developed in Politecnico di Milano Industry 4.0 Lab, a test environment implementing the proposed approach and demonstrating in an automated production line the effectiveness of the approach, significantly improving performances. Analysis of performance indicators demonstrates the soundness of the proposed solution and implementation methodology to make the overall production process more resilient, efficient and with product defects reduction.",https://ieeexplore.ieee.org/document/9570221/,"2021 IEEE International Conference on Engineering, Technology and Innovation (ICE/ITMC)",21-23 June 2021,ieeexplore
10.1109/ICECCE49384.2020.9179241,Role of Ubiquitous Computing and Mobile WSN Technologies and Implementation,IEEE,Conferences,"Computing capabilities such as real time data, unlimited connection, data from sensors, environmental analysis, automated decisions (machine learning) are demanded by many areas like industry for example decision making, machine learning, by research and military, for example GPS, sensor data collection. The possibility to make these features compatible with each domain that demands them is known as ubiquitous computing. Ubiquitous computing includes network topologies such as wireless sensor networks (WSN) which can help further improving the existing communication, for example the Internet. Also, ubiquitous computing is included in the Internet of Things (IoT) applications. In this article, it is discussed the mobility of WSN and its advantages and innovations, which make possible implementations for smart home and office. Knowing the growing number of mobile users, we place the mobile phone as the key factor of the future ubiquitous wireless networks. With secure computing, communicating, and storage capacities of mobile devices, they can be taken advantage of in terms of architecture in the sense of scalability, energy efficiency, packet delay, etc. Our work targets to present a structure from a ubiquitous computing point of view for researchers who have an interest in ubiquitous computing and want to research on the analysis, to implement a novel method structure for the ubiquitous computing system in military sectors. Also, this paper presents security and privacy issues in ubiquitous sensor networks (USN).",https://ieeexplore.ieee.org/document/9179241/,"2020 International Conference on Electrical, Communication, and Computer Engineering (ICECCE)",12-13 June 2020,ieeexplore
10.1109/BigDataSecurity-HPSC-IDS49724.2020.00050,SLA as a mechanism to manage risks related to chatbot services.,IEEE,Conferences,Intelligent Chatbot services become one of the mainstream applications in user help and many other areas. Apart from bringing numerous benefits to users these services may bring additional risks to the companies that employ them. The study starts with the review of the scale of chatbot industry and common use cases by focusing on their applications &amp; industry tendencies. Review of functionality and architecture of typical chatbot services shows the potential risks associated with chatbots. Analysis of such risks in the paper helped to build a checklist that security managers can use to assess risks prior to chatbot implementation. The proposed checklist was tested by reviewing a number of Service Level Agreements (SLA) of real chatbot providers.,https://ieeexplore.ieee.org/document/9123051/,"2020 IEEE 6th Intl Conference on Big Data Security on Cloud (BigDataSecurity), IEEE Intl Conference on High Performance and Smart Computing, (HPSC) and IEEE Intl Conference on Intelligent Data and Security (IDS)",25-27 May 2020,ieeexplore
10.1109/WF-IoT.2016.7845468,SQenloT: Semantic query engine for industrial Internet-of-Things gateways,IEEE,Conferences,"The Advent of Internet-of-Things (IoT) paradigm has brought exciting opportunities to solve many real-world problems. IoT in industries is poised to play an important role not only to increase productivity and efficiency but also to improve customer experiences. Two main challenges that are of particular interest to industry include: handling device heterogeneity and getting contextual information to make informed decisions. These challenges can be addressed by IoT along with proven technologies like the Semantic Web. In this paper, we present our work, SQenIoT: a Semantic Query Engine for Industrial IoT. SQenIoT resides on a commercial product and offers query capabilities to retrieve information regarding the connected things in a given facility. We also propose a things query language, targeted for resource-constrained gateways and non-technical personnel such as facility managers. Two other contributions include multi-level ontologies and mechanisms for semantic tagging in our commercial products. The implementation details of SQenIoT and its performance results are also presented.",https://ieeexplore.ieee.org/document/7845468/,2016 IEEE 3rd World Forum on Internet of Things (WF-IoT),12-14 Dec. 2016,ieeexplore
10.1109/FCCM48280.2020.00067,Scalable Full Hardware Logic Architecture for Gradient Boosted Tree Training,IEEE,Conferences,"Gradient Boosted Tree is most effective and standard machine learning algorithm in many fields especially with various type of tabular dataset. Besides, recent industry field and robotics field require high-speed, power efficient and real-time training with enormous data. FPGA is effective device which enable custom domain specific approach to give acceleration as well as power efficiency. We introduce a scalable full hardware implementation of Gradient Boosted Tree training with high performance and flexibility of hyper parameterization. Experimental work shows that our hardware implementation achieved 11-33 times faster than state-of-art GPU acceleration even with small gates and low power FPGA device.",https://ieeexplore.ieee.org/document/9114741/,2020 IEEE 28th Annual International Symposium on Field-Programmable Custom Computing Machines (FCCM),3-6 May 2020,ieeexplore
10.1109/IS3C50286.2020.00134,Screw defect detection system based on AI image recognition technology,IEEE,Conferences,"In the past ten years, smart manufacturing has been widely discussed and gradually introduced into various manufacturing fields. Since Germany proposed the concept of “Industry 4.0” in 2011, it has been spreading and feverish all over the world. For Industry 4.0, information digitization, intelligent defect detection and database platform management are their main core technologies. Aiming at a large screw industry manufacturing field in central and southern Taiwan, this paper proposes a screw defect detection system based on AI image recognition technology to detect damage to the nut during the “molding” process in the screw production process, and it is determined whether the inspected screw passes the inspection. The recognition result is given as shown in Figure 1. This paper uses 500 non-defective screw samples and 20 defective screw samples provided by the screw factory. The above samples collected real-time images through the sampling structure designed in this article, and we adopt Microsoft Corporation's ML.NET suite to model AI images, and uses the following four deep learning models: ResNetV2 50, ResNetV2 101, InceptionV3, MoblieNetV2 for learning; in the process of learning, this article divides the data set into three types of data sets (one is the unknown set that is not used for training but mixed with correct and defective samples, and the other is used for post-training verification of mixed samples with correct and defective samples. The third is a training set for training a mixture of correct and defective samples) This arrangement is used for subsequent verification models; after training, a PC-based screw defect detection system is implemented as shown in Figure 2; finally, with Detect screw defects in the form of instant photography. After the experiment, in 1,000 repeated tests, the success rate of defect detection reached 97%, while the false positive rate was only 2%.",https://ieeexplore.ieee.org/document/9394116/,"2020 International Symposium on Computer, Consumer and Control (IS3C)",13-16 Nov. 2020,ieeexplore
10.1109/CNS48642.2020.9162320,SecureAIS - Securing Pairwise Vessels Communications,IEEE,Conferences,"Modern vessels increasingly rely on the Automatic Identification System (AIS) digital technology to wirelessly broadcast identification and position information to neighboring vessels and ports. AIS is a time-slotted protocol that also provides unicast messages-usually employed to manage self-separation and to exchange safety information. However, AIS does not provide any security feature. The messages are exchanged in clear-text, and they are not authenticated, being vulnerable to several attacks, including forging and replay. Despite the existing contributions in the literature propose some strategies to overcome the exposed weaknesses, some are insecure, others do not comply with the standard, while the remaining few ones would introduce an unacceptable overhead-that is, they would require a relevant number of AIS-time slots, because of the limited payload available for each time-slot. In this paper, we propose SecureAIS, a key agreement scheme that allows any pair of vessels in reach of an AIS radio to agree on a shared session key of the desired length, by requiring just a fraction of the AIS time-slots of competing solutions. Further, the scheme is fully standard compliant and does not require any modification to the available hardware. The proposed scheme integrates the Elliptic Curve Qu-Vanstone (ECQV) implicit certification scheme and the Elliptic Curve Diffie Hellman (ECDH) key agreement technique, requiring moderate computational overhead while enjoying an optimal usage of the available bandwidth. When configured with the highest security level of 256 bits, SecureAIS allows two AIS transceivers to agree on a shared session key in 20 time-slots, against the 96 time-slots required by the competing solution based on traditional X.509 certificates. The proposed solution has been implemented and tested in a real scenario, while its security has been formally evaluated through the ProVerif tool. Finally, the source code of our Proof-of-concept using GNURadio and Ettus Research X310 has been also released as open-source to pave the way to further research by both Industry and Academia in maritime communication security.",https://ieeexplore.ieee.org/document/9162320/,2020 IEEE Conference on Communications and Network Security (CNS),29 June-1 July 2020,ieeexplore
10.1109/ETFA.2014.7005195,Semantic repository for case-based reasoning in CBM services,IEEE,Conferences,"Condition-based maintenance (CBM) has been implemented in industry to arrange the maintenance work as efficiently as possible. Case-based reasoning (CBR) can be used to automate part of the CBM decision process. However, in complex situations the final decisions have to be made by domain maintenance experts based on information gathered from several sources. This paper presents an approach for utilizing Semantic Web technologies and CBR in a knowledge base system supporting CBM services. The case knowledge base (CKB) is built over a semantic repository with an inference engine supporting ontology based information integration and data access using SPARQL queries. The knowledge base model developed for the system contains CBR task ontology and domain ontology for industrial control valves. Feasibility of the prototype CKB system was evaluated in experiments with real industrial case data.",https://ieeexplore.ieee.org/document/7005195/,Proceedings of the 2014 IEEE Emerging Technology and Factory Automation (ETFA),16-19 Sept. 2014,ieeexplore
10.1109/ICCICC46617.2019.9146103,Sequence Learning for Images Recognition in Videos with Differential Neural Networks,IEEE,Conferences,"Sequence learning from real-time videos is one of the hard challenges to current machine learning technologies and classic neural networks. Since existing supervised learning technologies are heavily dependent on intensive data and prior training, new methodologies for learning temporal sequences by unsupervised learning theories and technologies are yet to be developed. This paper presents the design and implementation of a novel Differential Neural Network (∇NN) for unsupervised sequence learning. The methodology is developed with a set of fundamental theories and enabling technologies for solving the problems of visual object recognition, motion detection, and visual semantic analysis in video sequence. A set of experiments on ∇NN for sequence learning is demonstrated. This work has not only led to a theoretical breakthrough to novel machine sequence learning, but also applicable to a wide range of challenging problems in computational intelligence and the AI industry.",https://ieeexplore.ieee.org/document/9146103/,2019 IEEE 18th International Conference on Cognitive Informatics & Cognitive Computing (ICCI*CC),23-25 July 2019,ieeexplore
10.1109/CCGrid.2015.41,Service Clustering for Autonomic Clouds Using Random Forest,IEEE,Conferences,"Managing and optimising cloud services is one of the main challenges faced by industry and academia. A possible solution is resorting to self-management, as fostered by autonomic computing. However, the abstraction layer provided by cloud computing obfuscates several details of the provided services, which, in turn, hinders the effectiveness of autonomic managers. Data-driven approaches, particularly those relying on service clustering based on machine learning techniques, can assist the autonomic management and support decisions concerning, for example, the scheduling and deployment of services. One aspect that complicates this approach is that the information provided by the monitoring contains both continuous (e.g. CPU load) and categorical (e.g. VM instance type) data. Current approaches treat this problem in a heuristic fashion. This paper, instead, proposes an approach, which uses all kinds of data and learns in a data-driven fashion the similarities and resource usage patterns among the services. In particular, we use an unsupervised formulation of the Random Forest algorithm to calculate similarities and provide them as input to a clustering algorithm. For the sake of efficiency and meeting the dynamism requirement of autonomic clouds, our methodology consists of two steps: (i) off-line clustering and (ii) on-line prediction. Using datasets from real-world clouds, we demonstrate the superiority of our solution with respect to others and validate the accuracy of the on-line prediction. Moreover, to show the applicability of our approach, we devise a service scheduler that uses the notion of similarity among services and evaluate it in a cloud test-bed.",https://ieeexplore.ieee.org/document/7152517/,"2015 15th IEEE/ACM International Symposium on Cluster, Cloud and Grid Computing",4-7 May 2015,ieeexplore
10.1109/EEEIC.2019.8783979,Short-term load forecasting for Jordan’s Power System Using Neural Network based Different,IEEE,Conferences,"In recent times, there is a burst of interest in load forecasting. It is considered as a backbone activity in the electric power industry. Load forecasting influence decision both from an engineering perspective as well as a financial perspective. The key success in load forecasting is a partnership with the industry and how much the suitable load forecasting method has been widely used by the industry. These methods should be a simple solution to the real world problem. Although some level of sophistication is necessary to achieve high accuracy, so the successful methods typically find a good balance between simplicity and accuracy. Hence neural networks (NN) has received wide concern because of its understandable model, simple and direct implementation and satisfactory performance. In this paper, a short-term load forecasting is demonstrated based on neural networks (NN) adaption in Jordan's power system. Updated parameters are op-timized using different techniques; particle swarm optimization (PSO), genetic algorithm (GA) and elephant herding optimization (EHO). In addition, the error is calculated before and after optimization techniques. As the initial experimental result, the analysis shows that the prediction obtained using metaheuristic optimization based NNs is competitively suitable for the load forecasting. Whereas, two-layer NN gives better prediction with the least error. Therefore, it is clear from the above studies that two-layer NN is the best for load forecasting. Finally, this work is implemented using neural network toolbox and optimization Matlab codes in MathWorks for developing a new and foreseeable future forecasting model.",https://ieeexplore.ieee.org/document/8783979/,2019 IEEE International Conference on Environment and Electrical Engineering and 2019 IEEE Industrial and Commercial Power Systems Europe (EEEIC / I&CPS Europe),11-14 June 2019,ieeexplore
10.1109/ICCEA53728.2021.00060,ShuffleNet2MC: A method of light weight fault diagnosis,IEEE,Conferences,"Bearing fault diagnosis plays an important role in the field of modern industry. Although convolution neural network achieves good results, large amount of parameters costs a lot of calculation, which brings challenges to the deployment of fault diagnosis tasks in low computational power equipments. To solve the problems, an novel CNN model ShuffleNet2MC based on improved Shufflenetv2 network is proposed. Firstly, Depthwise convolution and Channel Shuffle are used to reduce the computational cost while ensuring the accuracy of diagnosis computation; Secondly, mixed convolution is used to extract the features of different resolutions through multi-scale and multi-channel method, which improves the accuracy of the model; Finally, K-means quantization is applied to the model, which greatly reduces the GFLOPS of the model and further improves the performance of the model while ensuring that the accuracy is basically unchanged. A large number of experiments on the bearing fault dataset of Western Reserve University show that: The times of floating point operation and classification accuracy of ShufflenetV2 are 0.001GFLOPS and 97.9% respectively in the task of fault diagnosis. Compared with other models, it not only reduces the model parameters and compresses the model, but also gets better classification accuracy.",https://ieeexplore.ieee.org/document/9581143/,2021 International Conference on Computer Engineering and Application (ICCEA),25-27 June 2021,ieeexplore
10.1109/ICOEI.2019.8862602,Smart Disaster Management and Prevention using Reinforcement Learning in IoT Environment,IEEE,Conferences,"At starting of the Internet of Things (IoT), it is passing around a world, in which diverse kinds of different objects are there connected to the Internet. It contains the use of smart phones, sensors, cameras, and other devices to make over the actions of people and things into data and link it to the Internet. With its capability to model the real world in digital form and accomplish scrutiny and replication in cyberspace, the IoT is able to reveal new value at an unparalleled rate and deliver it as response to the real world. This is set to convey main changes that will lengthen to the structure of industry in addition to the infrastructure of society itself. Therefore although the occurrence of the IoT contributes rise to new value, it besides means the occurrence of new threats. The proposed work covenant with disaster management as well as prevention to manufacturing industry using IoT. System first investigates the threat scenario during general execution of work, and finds the critical situations. The system processes learning approach for identifying such critical situations and execute the output appliances. System utilized multiple input along with output sensor for experiment. The Q-Learning approach has used for updating the policy which can provide the best result with high accuracy.",https://ieeexplore.ieee.org/document/8862602/,2019 3rd International Conference on Trends in Electronics and Informatics (ICOEI),23-25 April 2019,ieeexplore
10.1109/VLSID.2018.26,Special Session: Design of Energy-Efficient and Reliable VLSI Systems: A Data-Driven Perspective,IEEE,Conferences,"Summary form only given, as follows. The complete presentation was not made available for publication as part of the conference proceedings. The amount of data generated and collected across computing platforms every day is not only enormous, but growing at an exponential rate. Advanced data analytics and machine learning techniques have become increasingly essential to analyze and extract meaning from such “Big Data”. These techniques can be very useful to detect patterns and trends to improve the operational behavior of computing systems, but they also introduce a number of outstanding challenges: (1) How can we design and deploy data analytics mechanisms to improve energy-efficiency and reliability in IoT and mobile devices, without introducing significant software overheads? (2) How to leverage emerging technologies (e.g.,3D integration) to design energy-efficient and reliable manycore systems for big data computing? (3) How to use machine learning and data mining techniques for effective design space exploration of computing systems, and enable adaptive control to improve energy-efficiency? (4) How can data analytics detect anomalies and increase robustness in the network backbone of emerging large-scale networking systems? To address these outstanding challenges, out-of-the-box approaches need to be explored. In this special session, we will discuss these outstanding problems and describe far-reaching solutions applicable across the interconnected ecosystem of IoT and mobile devices, manycore chips, datacenters, and networks. The special session brings together speakers with unique insights on applying data analytics and machine learning to real-world problems to achieve the most sought after features on multi-scale computing platforms, viz. intelligent data mining, energyefficiency, and robustness. By integrating data analytics and machine learning algorithms, statistical modeling, embedded hardware and software design, and cloud computing content, this session will engage a broad section of Embedded and VLSI Design conference attendees. This special session is targeted towards university researchers/professors, students, industry professionals, and embedded/VLSI system designers. This session will attract newcomers who want to learn how to apply data analytics to solve problems in computing systems, as well as experienced researchers looking for exciting new directions in embedded systems, VLSI design, EDA algorithms, and multi-scale computing.",https://ieeexplore.ieee.org/document/8326889/,2018 31st International Conference on VLSI Design and 2018 17th International Conference on Embedded Systems (VLSID),6-10 Jan. 2018,ieeexplore
10.1109/ACMI53878.2021.9528185,Speed Bump &amp; Pothole Detection with Single Shot MultiBox Detector Algorithm &amp; Speed Control for Autonomous Vehicle,IEEE,Conferences,"The development of self-driving cars has always been an extensive research field for the automobile industry. To make a capable self-driving car, many challenges need to be resolved. Detection of the road condition is one of them. This paper focuses on a particular part-detection of speed bumps and potholes using a camera and analyzing the video feed with the help of artificial intelligence. To solve this problem a popular and lightweight algorithm, SSD (Single Shot Multibox Detector) is used. This is an optimal choice because of being lightweight and also accurate enough to run on mobile devices and to use in real-life situations. For detecting speed bumps and potholes, a dataset has been created based on the road structure of Bangladesh as the main priority of this system is to work on the local environment. Raspberry Pi has been used as the main processing unit because of being small but powerful. A warning system has been implemented so that it can warn the onboard driver about the upcoming pothole or speed bump. This system can also send a signal to the speed controller unit of the car to reduce the speed on detection to avoid accidents or damages to the car. The speed control unit is a microcontroller-based system that uses an ATmega328 microcontroller and L298 motor driver. This paper summarizes the combination of an artificial intelligence-based detection system injunction with a microcontroller-based speed control system in a cost-effective way that can be used in building self-driving cars.",https://ieeexplore.ieee.org/document/9528185/,"2021 International Conference on Automation, Control and Mechatronics for Industry 4.0 (ACMI)",8-9 July 2021,ieeexplore
10.1109/ISCA45697.2020.00038,SpinalFlow: An Architecture and Dataflow Tailored for Spiking Neural Networks,IEEE,Conferences,"Spiking neural networks (SNNs) are expected to be part of the future AI portfolio, with heavy investment from industry and government, e.g., IBM TrueNorth, Intel Loihi. While Artificial Neural Network (ANN) architectures have taken large strides, few works have targeted SNN hardware efficiency. Our analysis of SNN baselines shows that at modest spike rates, SNN implementations exhibit significantly lower efficiency than accelerators for ANNs. This is primarily because SNN dataflows must consider neuron potentials for several ticks, introducing a new data structure and a new dimension to the reuse pattern. We introduce a novel SNN architecture, SpinalFlow, that processes a compressed, time-stamped, sorted sequence of input spikes. It adopts an ordering of computations such that the outputs of a network layer are also compressed, time-stamped, and sorted. All relevant computations for a neuron are performed in consecutive steps to eliminate neuron potential storage overheads. Thus, with better data reuse, we advance the energy efficiency of SNN accelerators by an order of magnitude. Even though the temporal aspect in SNNs prevents the exploitation of some reuse patterns that are more easily exploited in ANNs, at 4-bit input resolution and 90% input sparsity, SpinalFlow reduces average energy by 1.8×, compared to a 4-bit Eyeriss baseline. These improvements are seen for a range of networks and sparsity/resolution levels; SpinalFlow consumes 5× less energy and 5.4× less time than an 8-bit version of Eyeriss. We thus show that, depending on the level of observed sparsity, SNN architectures can be competitive with ANN architectures in terms of latency and energy for inference, thus lowering the barrier for practical deployment in scenarios demanding real-time learning.",https://ieeexplore.ieee.org/document/9138926/,2020 ACM/IEEE 47th Annual International Symposium on Computer Architecture (ISCA),30 May-3 June 2020,ieeexplore
10.1109/ASRU.2009.5372951,"Spoken dialogue systems: Challenges, and opportunities for research",IEEE,Conferences,"Summary form only given. Research into spoken dialog systems has yielded some interesting results recently, such as statistical models for improved robustness, and machine learning for optimal control, among others. What are the basic ideas behind these techniques? What opportunities do they exploit? Are they ready to be deployed in real systems? What remains to be done? This talk aims to tackle these questions. First, the task of dialog management and its challenges will be reviewed, including the effects of ASR errors; the curse of history; the lack of a single optimization metric; and the theory of mind problem. Next, current solutions to these problems will be addressed, focusing on learnings from real deployed systems, particularly in industry. Though relatively pervasive, current practices in industry still yield systems with important flaws. Recently, the research community has attempted to advance the state-of-the-art with techniques such as statistical models, machine learning, simulation, and incremental processing. This talk will present the basic ideas of some of these techniques, and examine their prospects for success in real applications - in light of both pragmatic commercial constraints, and also more fundamental properties of dialog. Finally, opportunities for further progress will be suggested.",https://ieeexplore.ieee.org/document/5372951/,2009 IEEE Workshop on Automatic Speech Recognition & Understanding,13 Nov.-17 Dec. 2009,ieeexplore
10.1109/IT48810.2020.9070503,State Detection of Rotary Actuators Using Wavelet Transform and Neural Networks,IEEE,Conferences,"Rotary actuators are among the most commonly used machines in the industry and the algorithm for detecting the level of wear they are subjected to can prevent significant amount of unnecessary maintenance expenses. This paper proposes a new algorithm which can detect the state of the rotating machine using acoustic signals recorded in its vicinity. The algorithm uses a combination of wavelet transform and neural networks and is computationally inexpensive, so it can be implemented on a simple microcontroller. The testing has been done on real acoustic signals recorded in thermal power plant Kostolac in Serbia.",https://ieeexplore.ieee.org/document/9070503/,2020 24th International Conference on Information Technology (IT),18-22 Feb. 2020,ieeexplore
10.1109/ICRITO48877.2020.9198036,State of Art: Energy Efficient Protocols for Self-Powered Wireless Sensor Network in IIoT to Support Industry 4.0,IEEE,Conferences,"Up gradation of manufacturing systems in industries by means of implementing innovative manufacturing techniques that captures real time data, applies machine learning algorithms, makes entire system self-decisive and provides inter connectivity to the whole system is the prime focus of the Industry 4.0. It is aimed at bringing new industrial revolution with the help of internet of things technology due to its considerable influence in the industrial manufacturing process. Though impact of internet of things in industrial sector is huge, a practical implementation incorporates challenges in energy efficiency, self-powered sensor nodes and security. For processing of gathered data self-powered sensor nodes may sinks energy from ambient energy sources. A considerable amount of efforts has been put by researchers to address the challenges for development of energy efficient routing protocol for such nodes. The presented survey is aimed at analyzing the protocols for contribution towards the goals of industry 4.0 Energy efficient protocol will support the system to consume least energy for its operation. Henceforth combination of self-powered wireless sensor network and energy efficient protocol will be useful to power up many industrial IoT applications.",https://ieeexplore.ieee.org/document/9198036/,"2020 8th International Conference on Reliability, Infocom Technologies and Optimization (Trends and Future Directions) (ICRITO)",4-5 June 2020,ieeexplore
10.1109/IPDPS.2019.00113,Stochastic Gradient Descent on Modern Hardware: Multi-core CPU or GPU? Synchronous or Asynchronous?,IEEE,Conferences,"There is an increased interest in building data analytics frameworks with advanced algebraic capabilities both in industry and academia. Many of these frameworks, e.g., TensorFlow, implement their compute-intensive primitives in two flavors-as multi-thread routines for multi-core CPUs and as highly-parallel kernels executed on GPU. Stochastic gradient descent (SGD) is the most popular optimization method for model training implemented extensively on modern data analytics platforms. While the data-intensive properties of SGD are well-known, there is an intense debate on which of the many SGD variants is better in practice. In this paper, we perform a comprehensive experimental study of parallel SGD for training machine learning models. We consider the impact of three factors - computing architecture (multi-core CPU or GPU), synchronous or asynchronous model updates, and data sparsity - on three measures-hardware efficiency, statistical efficiency, and time to convergence. We draw several interesting findings from our experiments with logistic regression (LR), support vector machines (SVM), and deep neural nets (MLP) on five real datasets. As expected, GPU always outperforms parallel CPU for synchronous SGD. The gap is, however, only 2-5X for simple models, and below 7X even for fully-connected deep nets. For asynchronous SGD, CPU is undoubtedly the optimal solution, outperforming GPU in time to convergence even when the GPU has a speedup of 10X or more. The choice between synchronous GPU and asynchronous CPU is not straightforward and depends on the task and the characteristics of the data. Thus, CPU should not be easily discarded for machine learning workloads. We hope that our insights provide a useful guide for applying parallel SGD in practice and - more importantly - choosing the appropriate computing architecture",https://ieeexplore.ieee.org/document/8820776/,2019 IEEE International Parallel and Distributed Processing Symposium (IPDPS),20-24 May 2019,ieeexplore
10.1109/CAC48633.2019.8996594,Study and Generalization on an End-to-End Spatial-temporal Driving Model,IEEE,Conferences,"Autonomous driving has attracted increasing attentions from both academia and industry, among which end-to-end driving model is a nascent research topic. In this work a deep learning based model is studied and generalized, which directly maps raw videos to steering angles without any manual feature extraction procedure. The model is learned and evaluated from human driving videos from both public dataset and self-recorded dataset under real environment, which is more practical and application oriented. Besides, the model combines both the spatial and temporal information by applying 3D convolutional neural network and recurrent unit. For the dataset studied, ablation experiment is performed to further verify the unity of recurrent block. Finally, transfer learning is applied to transfer the domain knowledge learned from the large-scale public dataset to two different small-scale dataset in unseen scenes. Comprehensive quantitative and qualitative evaluations demonstrate that the RMSE on pretrained model is 0.12603, while the transfer learning for the two unseen scenes achieve 0.13079 and 0.13198, respectively, which shows the effectiveness and generalization ability of the model even under different scenarios and abrupt turns.",https://ieeexplore.ieee.org/document/8996594/,2019 Chinese Automation Congress (CAC),22-24 Nov. 2019,ieeexplore
10.1109/ICE/ITMC49519.2020.9198430,Supporting SMEs in the Lake Constance Region in the Implementation of Cyber-Physical-Systems: Framework and Demonstrator,IEEE,Conferences,"With the emergence of the recent Industry 4.0 movement, data integration is now also being driven along the production line, made possible primarily by the use of established concepts of intelligent supply chains, such as the digital avatars. Digital avatars - sometimes also called Digital Twins or more broadly Cyber-Physical Systems (CPS) - are already successfully used in holistic systems for intelligent transport ecosystems, similar to the use of Big Data and artificial intelligence technologies interwoven with modern production and supply chains. The goal of this paper is to describe how data from interwoven, autonomous and intelligent supply chains can be integrated into the diverse data ecosystems of the Industry 4.0, influenced by a multitude of data exchange formats and varied data schemas. In this paper, we describe how a framework for supporting SMEs was established in the Lake Constance region and describe a demonstrator sprung from the framework. The demonstrator project's goal is to exhibit and compare two different approaches towards optimisation of manufacturing lines. The first approach is based upon static optimisation of production demand, i.e. exact or heuristic algorithms are used to plan and optimise the assignment of orders to individual machines. In the second scenario, we use real-time situational awareness - implemented as digital avatar - to assign local intelligence to jobs and raw materials in order to compare the results to the traditional planning methods of scenario one. The results are generated using event-discrete simulation and are compared to common (heuristic) job scheduling algorithms.",https://ieeexplore.ieee.org/document/9198430/,"2020 IEEE International Conference on Engineering, Technology and Innovation (ICE/ITMC)",15-17 June 2020,ieeexplore
10.1109/ICCES45898.2019.9002132,Symptom Based Health Prediction using Data Mining,IEEE,Conferences,"The general day to day health of a person is vital for the efficient functioning of the human body. Taking certain prominent symptoms and their diseases to build a Machine learning model to predict common diseases based on real symptoms is the objective of this research. With the dataset of the most commonly exhibited diseases, we built a relation to predicting the possible disease based on the input of symptoms. The proposed model utilizes the capability of different Machine learning algorithms combined with text processing to achieve accurate prediction. Text processing has been implemented using Tokenization and, is combined with various algorithms to test the similarities and the outputs. In health industry, it provides several benefits such as pre-emptive detection of diseases, faster diagnosis, medical history for review of patients etc.",https://ieeexplore.ieee.org/document/9002132/,2019 International Conference on Communication and Electronics Systems (ICCES),17-19 July 2019,ieeexplore
10.1109/MDM.2019.00-49,Synchronization-Free GPS Spoofing Detection with Crowdsourced Air Traffic Control Data,IEEE,Conferences,"GPS-dependent localization, navigation and air traffic control (ATC) applications have had a significant impact on the modern aviation industry. However, the lack of encryption and authentication makes GPS vulnerable to spoofing attacks with the purpose of hijacking aerial vehicles or threatening air safety. In this paper, we propose GPS-Probe, a GPS spoofing detection algorithm that leverages the ATC messages that are periodically broadcasted by aerial vehicles. By continuously analyzing the received signal strength indicator (RSSI) and the timestamps at server (TSS) of the ATC messages, which are monitored by multiple ground sensors, GPS-Probe constructs a machine learning enabled framework to estimate the real position of the target aerial vehicle and to detect whether or not the position data is compromised by GPS spoofing attacks. Unlike existing techniques, GPS-Probe neither requires any updates of the GPS infrastructure nor updates of the GPS receivers. More importantly, it releases the requirement on time synchronization of the ground sensors distributed around the world. Using the real-world ATC data crowdsourced by the OpenSky Network, our experiment results show that GPS-Probe can achieve the detection accuracy and precision, of 81.7% and 85.3% respectively on average, and up to 89.7% and 91.5% respectively at the best.",https://ieeexplore.ieee.org/document/8788766/,2019 20th IEEE International Conference on Mobile Data Management (MDM),10-13 June 2019,ieeexplore
10.1109/ICCMC51019.2021.9418273,Systematic Study on Hardware Optimization of 5G Communication,IEEE,Conferences,"Systematic study on hardware optimization of 5G communication is conducted in this paper. With the continuous development of the current era, 5G communication has attracted much attention and has become an important carrier of the future communication industry. Therefore, before launching the specific operations, the most critical thing is to improve the application research level of 5G communication technology as much as possible, and to comprehensively deal with the problems presented in the previous stage in the research process. 5G network architecture realizes the transformation from mobile cellular to general distributed and heterogeneous communication methods. This paper considers the deep learning as the basis for the analysis of efficient modelling. The model is designed on FPGA and the optimization is combined with neural networks. The experiment has shown the 5G communication is enhanced with the combination of computational intelligence.",https://ieeexplore.ieee.org/document/9418273/,2021 5th International Conference on Computing Methodologies and Communication (ICCMC),8-10 April 2021,ieeexplore
10.1109/ICRTAC.2018.8679129,TensorFlow Based Website Click through Rate (CTR) Prediction Using Heat maps,IEEE,Conferences,"Web Heat Maps are used to identify the click patterns and activities visited by the users of the website. Using heat maps, one can make a manual decision based on the user's click activity. This paper proposes a framework using TensorFlow to identify and detect the users click activity in real time. Tensor Flow also suggest or take business decisions predicted through users clicks. This paper models Tensor Flow's machine learning library to take automated decisions like placement of suitable products, placement of advertisements and others based on the highest clicks recorded by the users. The results predict that the future businesses like e-commerce, fashion and retail industry can benefit more if this framework is deployed in such applications.",https://ieeexplore.ieee.org/document/8679129/,2018 International Conference on Recent Trends in Advance Computing (ICRTAC),10-11 Sept. 2018,ieeexplore
10.1109/ICSE-Companion.2019.00131,Testing Untestable Neural Machine Translation: An Industrial Case,IEEE,Conferences,"Neural Machine Translation (NMT) has shown great advantages and is becoming increasingly popular. However, in practice, NMT often produces unexpected translation failures in its translations. While reference-based black-box system testing has been a common practice for NMT quality assurance during development, an increasingly critical industrial practice, named in-vivo testing, exposes unseen types or instances of translation failures when real users are using a deployed industrial NMT system. To fill the gap of lacking test oracles for in-vivo testing of NMT systems, we propose a new methodology for automatically identifying translation failures without reference translations. Our evaluation conducted on real-world datasets shows that our methodology effectively detects several targeted types of translation failures. Our experiences on deploying our methodology in both production and development environments of WeChat (a messenger app with over one billion monthly active users) demonstrate high effectiveness of our methodology along with high industry impact.",https://ieeexplore.ieee.org/document/8802818/,2019 IEEE/ACM 41st International Conference on Software Engineering: Companion Proceedings (ICSE-Companion),25-31 May 2019,ieeexplore
10.1109/ICC40277.2020.9148964,The Scalability Analysis of Machine Learning Based Models in Road Traffic Flow Prediction,IEEE,Conferences,"Nowadays, traffic flow prediction, as a vital part of the Intelligent Transportation System (ITS), has attracted considerable attention from both academia and industry. Many prediction methods have been proposed and can be categorized into parametric methods and non-parametric methods. Nonparametric methods, especially Machine Learning (ML)-based methods, compared to parametric methods, need less prior knowledge about the relationship among different traffic patterns and can better fit the non-linear features of traffic data. However, we notice that, due to the complex structure, ML-models require a higher cost of implementation regarding time consumption of training and predicting. Therefore, in this paper, we evaluate not only the accuracy but also the efficiency and scalability of some state-of-the-art ML-models, which is the key to apply a prediction model into the real world. Furthermore, we design an off-line optimization method, Desensitization, to improve the scalability of a given model.",https://ieeexplore.ieee.org/document/9148964/,ICC 2020 - 2020 IEEE International Conference on Communications (ICC),7-11 June 2020,ieeexplore
10.1109/MIPRO.2016.7522210,The challenge of cellular cooperative ITS services based on 5G communications technology,IEEE,Conferences,"We live at a time when the automotive industry is going through a technological revolution with the development of vehicles changing into autonomous moving objects having the properties of artificial intelligence. Additionally, cellular communications networks introduce new technologies and concepts: SDN (Software-defined networking), NFV (Network Functions Virtualization). These advanced software-defined communications networks virtualize network functions, allowing a new way of configuration, control and management. Increasing the speed and automation are key requirements to support the most demanding services such as mobile payment of contextual services, as well as the introduction of new “Machine” users. SDN also assists in the implementation of new infrastructure for the dynamic services that are based on the concepts of IoT, Big Data and Everything-as-a-Service. Using NFV enables the Internet of Things services that provide a new way of connecting people, processes, data and devices. It is precisely thes5G communications networkse requirements that are essential for the introduction of C-ITS systems, i.e., the integration of passengers, drivers, vehicles and transport infrastructure, as well as information, statistics, predictive traffic analytics, all this in real-time. Due to this trend of development of cellular communications networks in the next 5G communications networks, automotive and ITS traffic systems are becoming the most important market for business expansion of telecom operators. Such revolutionary technological changes, together with the integration of various industries entails a series of challenges. The aim of this paper is to define important information, challenges and opportunities for the telecom industry to provide mobility for people and goods.",https://ieeexplore.ieee.org/document/7522210/,"2016 39th International Convention on Information and Communication Technology, Electronics and Microelectronics (MIPRO)",30 May-3 June 2016,ieeexplore
10.1109/IMTC.2001.928200,The development of an artificial neural network embedded automated inspection quality management system,IEEE,Conferences,This paper describes in detail the development of an innovative artificial neural network embedded automated inspection scheme for the manufacturing industry employing digital image processing techniques. Such a system is capable of performing real-time image processing tasks and identifies the size and location of the finished components on manufactured products as well as the flaws and scratches on surface of products during the manufacturing process. The proposed artificial neural network embedded quality management system provides a user-friendly user interface that has been implemented and tested on a case study from a printed circuit board manufacture. The experimental results have demonstrated the functionality and superiority of the developed artificial neural network embedded inspection system.,https://ieeexplore.ieee.org/document/928200/,IMTC 2001. Proceedings of the 18th IEEE Instrumentation and Measurement Technology Conference. Rediscovering Measurement in the Age of Informatics (Cat. No.01CH 37188),21-23 May 2001,ieeexplore
10.1109/AIAWS.1991.236599,The strategic significance of expert systems,IEEE,Conferences,"Expert or knowledge-based systems are being deployed in virtually every industry because of their power to bring strategic competitive advantage to the firms who successfully employ them. The strategic significance of expert systems lies in their ability to improve the effectiveness and efficiency of the decision-making process and, in the long run, improved decision-making leads to growth and increased profitability. This is especially true of organizations, such as brokerage firms, whose 'life's blood' is fruitful, consistent, and reliable advice. Expert system technology is advancing at a great pace as new methods for improved reasoning and automated learning are emerging from research to reality. If firms in the business of offering advice do not continually upgrade their expert systems with the latest conceptual tools, they will find it difficult to grow, even survive, in the 'knowledge age'.&lt;<ETX>&gt;</ETX>",https://ieeexplore.ieee.org/document/236599/,Proceedings First International Conference on Artificial Intelligence Applications on Wall Street,9-11 Oct. 1991,ieeexplore
10.1109/ISIC.2007.4450938,Toward A Practical Multi-agent System for Integrated Control and Asset Management of Petroleum Production Facilities,IEEE,Conferences,"This paper addresses a practical intelligent multi-agent system for asset management for the petroleum industry, which is crucial for profitable oil and gas facilities operations and maintenance. A research project was initiated to study the feasibility of an intelligent asset management system. Having proposed a conceptual model, architecture, and implementation plan for such a system in previous work [1], [2], [3], we define the autonomy, communications, and artificial intelligence (AI) requirements of the component agents of such a system. We also discuss the software implementation of such agents. Furthermore, we describe a simple system prototype, and conduct a real time simulation experiment to analyze the prototype performance. Simulation results reveal that MATLAB can be used to build high performance real-time multi-agent systems, which can be used for many applications.",https://ieeexplore.ieee.org/document/4450938/,2007 IEEE 22nd International Symposium on Intelligent Control,1-3 Oct. 2007,ieeexplore
10.1109/MELCON.2018.8379089,Toward energy saving and environmental protection by implementation of autonomous ship,IEEE,Conferences,"The energy saving and environmental protection remain the main pillars of the maritime industry's sustainability enhancement. This sustainability will become a reality by implementation of autonomous ship (AS), which is considered as an alternative and may carry the potential to increase the profitability of shipping company. The elimination of crew, the introduction of innovative technology and adoption of new ship design on board of this kind of ship will participate positively in energy saving and environmental protection. For an in-situ study, a hundred of conventional ships (CS) have been visited, their energy consumption and environmental impact are assessed taking into consideration the trading area, navigation scenarios, power capacity and number of crew members. In this paper, we assess and quantify the positive impact on energy saving and environmental pollution prevention resulting from the implementation of AS. For this the facilities related to crew living are enumerated and the impact of their elimination on energy saving and environmental pollution prevention is presented. Other ship design concepts are proposed to enhance the energy-saving and environmental protection are proposed. A benchmarking of CS and AS in term of energy saving and environmental pollution prevention is presented.",https://ieeexplore.ieee.org/document/8379089/,2018 19th IEEE Mediterranean Electrotechnical Conference (MELECON),2-7 May 2018,ieeexplore
10.1145/3377811.3380368,Towards Characterizing Adversarial Defects of Deep Learning Software from the Lens of Uncertainty,IEEE,Conferences,"Over the past decade, deep learning (DL) has been successfully applied to many industrial domain-specific tasks. However, the current state-of-the-art DL software still suffers from quality issues, which raises great concern especially in the context of safety- and security-critical scenarios. Adversarial examples (AEs) represent a typical and important type of defects needed to be urgently addressed, on which a DL software makes incorrect decisions. Such defects occur through either intentional attack or physical-world noise perceived by input sensors, potentially hindering further industry deployment. The intrinsic uncertainty nature of deep learning decisions can be a fundamental reason for its incorrect behavior. Although some testing, adversarial attack and defense techniques have been recently proposed, it still lacks a systematic study to uncover the relationship between AEs and DL uncertainty. In this paper, we conduct a large-scale study towards bridging this gap. We first investigate the capability of multiple uncertainty metrics in differentiating benign examples (BEs) and AEs, which enables to characterize the uncertainty patterns of input data. Then, we identify and categorize the uncertainty patterns of BEs and AEs, and find that while BEs and AEs generated by existing methods do follow common uncertainty patterns, some other uncertainty patterns are largely missed. Based on this, we propose an automated testing technique to generate multiple types of uncommon AEs and BEs that are largely missed by existing techniques. Our further evaluation reveals that the uncommon data generated by ourmethod is hard to be defended by the existing defense techniques with the average defense success rate reduced by 35%. Our results call for attention and necessity to generate more diverse data for evaluating quality assurance solutions of DL software.",https://ieeexplore.ieee.org/document/9284139/,2020 IEEE/ACM 42nd International Conference on Software Engineering (ICSE),5-11 Oct. 2020,ieeexplore
10.1109/DDCLS52934.2021.9455448,Transfer Learning Based Rolling Bearing Fault Diagnosis,IEEE,Conferences,"In recent years, transfer learning has been an important method to address the problem that labeled data are rarely in the real world. In many industry scenarios, collected labeled sample signals are usually not in the same data distribution. A major assumption for traditional learning and deep learning based bearing fault diagnosis methods is the training data and testing data must follow the same data distribution. However, this assumption may not hold in reality. To address the different distribution problem, this paper proposed an unsupervised approach for bearing fault diagnosis based on transfer learning. The correlation alignment (CORAL) algorithm is used to align data distribution of domains in the proposed approach, then the proposed approach applies statistical algorithms to extract shallow features and wavelet scattering network to extract deep features. The 1 nearest neighbors (1-NN) classifier is trained with the feature vector matrix of source domain, which is able to classify the unlabeled samples of target domain presenting the effectiveness of the proposed approach. Different experiments were carried out to demonstrate the performance of the proposed approach. The experiment results show that the proposed model is superior to other bearing fault diagnosis methods.",https://ieeexplore.ieee.org/document/9455448/,2021 IEEE 10th Data Driven Control and Learning Systems Conference (DDCLS),14-16 May 2021,ieeexplore
10.1109/TDC.2002.1177707,Transformer monitoring moving forward from monitoring to diagnostics,IEEE,Conferences,"The technologies employed for monitoring transformers have been evolving over the last 10 or more years to the point where they are now commonly accepted, and have been demonstrated to provide useful data on the key parameters and components of critical power transformers. A major aspect of these technologies has been the accumulation of copious amounts of data, and the associated problem, of what to do with it all. With time and resources in short supply to do a proper analysis of this data, to turn it into useful transformer information, there needs to be a new set of technologies and techniques implemented. The advent of new methods of data modeling and interpretation using statistical analysis, rules based, and artificial intelligence systems is now moving from the research stage to practical field implementation. The industry has a very real need to move from ""just monitoring"" equipment to the point of being able to have the knowledge of the operating condition of the equipment and when things begin to go wrong, diagnose the problem to provide a recommended course of action.",https://ieeexplore.ieee.org/document/1177707/,IEEE/PES Transmission and Distribution Conference and Exhibition,6-10 Oct. 2002,ieeexplore
10.1109/TDC.2001.971373,Transformer monitoring-moving forward from monitoring to diagnostics,IEEE,Conferences,"The technologies employed for monitoring transformers have been evolving over the last 10 or more years to the point where they are now commonly accepted, and have been demonstrated to provide useful data on the key parameters and components of critical power transformers. A major aspect of these technologies has been the accumulation of copious amounts of data, and the associated problem, of what to do with it all. With time and resources in short supply to do a proper analysis of this data, to turn it into useful transformer information, there needs to be a new set of technologies and techniques implemented. The advent of new methods of data modeling and interpretation using statistical analysis, rule based, and artificial intelligence systems is now moving from the research stage to practical field implementation. The industry has a very real need to move from ""just monitoring"" equipment to the point of being able to have the knowledge of the operating condition of the equipment and when things begin to go wrong, diagnose the problem to provide a recommended course of action.",https://ieeexplore.ieee.org/document/971373/,2001 IEEE/PES Transmission and Distribution Conference and Exposition. Developing New Perspectives (Cat. No.01CH37294),2-2 Nov. 2001,ieeexplore
10.1109/ISMAR.2015.73,Tutorial 4: AR Implementations in Informal Learning,IEEE,Conferences,"A variety of cases uses of AR in informal learning environments. The cases uses are drawn from a variety of different contexts. There will be examples of AR use in education, tourism, event organizing, and others. This is mainly geared to people creating learning environments in any industry a foundation to start implementation AR. The featured case use will be how AR was used at TEDxKyoto to engage participants. There will also be several student projects that use AR presented and available for demo.",https://ieeexplore.ieee.org/document/7328050/,2015 IEEE International Symposium on Mixed and Augmented Reality,29 Sept.-3 Oct. 2015,ieeexplore
10.1109/CICA.2009.4982774,Tutorial CICA-T Computing with intelligence for identification and control of nonlinear systems,IEEE,Conferences,"System characterization and identification are fundamental problems in systems theory and play a major role in the design of controllers. System identification and nonlinear control has been proposed and implemented using intelligent systems such as neural networks, fuzzy logic, reinforcement learning, artificial immune system and many others using inverse models, direct/indirect adaptive, or cloning a linear controller. Adaptive Critic Designs (ACDs) are neural networks capable of optimization over time under conditions of noise and uncertainty. The ACD technique develops optimal control laws using two networks - critic and action. There are merits for each approach adopted will be presented. The primary aim of this tutorial is to provide control and system engineers/researchers from industry/academia, new to the field of computational intelligence with the fundamentals required to benefit from and contribute to the rapidly growing field of computational intelligence and its real world applications, including identification and control of power and energy systems, unmanned vehicle navigation, signal and image processing, and evolvable and adaptive hardware systems.",https://ieeexplore.ieee.org/document/4982774/,2009 IEEE Symposium on Computational Intelligence in Control and Automation,30 March-2 April 2009,ieeexplore
10.1109/VTCFall.2018.8690788,Ultra-Low Power IoT Traffic Monitoring System,IEEE,Conferences,"Given the sizable anticipated proliferation of Internet of Things (IoT) devices, Forrester Research forecasts that the fleet management and transportation industry sectors will enjoy more growth than others. This may come as no surprise, since infrastructure (e.g., roadways, bridges, airports) is a prime candidate for sensor integration to provide real-time measurements and to support intelligent decisions. The predicted increase of deployed devices makes it difficult to calculate the amount of energy required for these functions. Current estimates suggest that 2 to 4% of worldwide carbon emissions can be attributed to the information and communication industry. This paper presents novel algorithms designed to optimize power consumption of an intelligent vehicle counter and classifier sensors. Each was based on an event-driven methodology wherein a control block orchestrates the work of different components and subsystems. System duty-cycle is reduced through several techniques, and a reinforcement learning algorithm is introduced to control the system power policy, according to the traffic environment. Battery life for a sensor supported by a 2300 mAh battery was extended from 48-hour, adopted all-on policy to more than 400 days when leveraging the algorithms and techniques presented in this work.",https://ieeexplore.ieee.org/document/8690788/,2018 IEEE 88th Vehicular Technology Conference (VTC-Fall),27-30 Aug. 2018,ieeexplore
10.1109/BigData.2016.7840903,Unravelling the Myth of big data and artificial intelligence in sustainable natural resource development,IEEE,Conferences,"Natural Resources businesses span one or more of the following verticals: Explore &amp; Discover, Develop, Extract &amp; Transport, Market &amp; Trade. Natural Gas (NG) is a key component of the Natural Resources industry, which contributes both directly and indirectly to sustain and support the day to day energy needs of humans. Various forums drive key initiatives to maintain the sustainability of natural resources for decades to come. The recent evolution of advanced Information Technology (IT) fields, termed as `Big Data and Artificial Intelligence (BD&amp;AI),' fortify these initiatives by providing anytime, anywhere access of data (for example in remote offshore assets or facilities) from a vast corpus base, in addition to enabling natural human interaction. In this paper we intend to explore the key primitives of BD&amp;AI and cite the method of real-world implementation in the Natural Gas industry to further enable sustainable development.",https://ieeexplore.ieee.org/document/7840903/,2016 IEEE International Conference on Big Data (Big Data),5-8 Dec. 2016,ieeexplore
10.1109/I2MTC43012.2020.9128596,Unsupervised Flight Phase Recognition with Flight Data Clustering based on GMM,IEEE,Conferences,"Currently, with the rapid development of the aviation industry, researchers are paying more attention to the improvement of aviation safety. Aviation safety mainly includes flight safety, aviation ground safety, and air defense safety. In terms of flight safety, the analysis of large amounts of flight data has gradually become a useful tool for timely detection of potential dangers at various stages of flight. As a result, flight data analysis has been one of the hot topics in aviation. However, due to the complexity of the aircraft operating conditions throughout the aircraft, if the data is analyzed at the entire flight phase, it is very difficult and time consuming to identify the problematic fight phase. Therefore, flight phase recognition for civil aircraft is implemented in this study. A flight phase recognition method based on Gaussian Mixture Model (GMM) is proposed in this work, which is the important foundation for timely detecting the abnormal event and improving the system safety and reliability. Firstly, the FDR data are preprocessed by spline interpolation and normalization, and then a GMM-based flight phase clustering is realized. In addition, a set of evaluation method is developed to evaluate the quality of flight phase recognition result. Finally, the effectiveness of the method is verified by using real FDR data from NASA's open database.",https://ieeexplore.ieee.org/document/9128596/,2020 IEEE International Instrumentation and Measurement Technology Conference (I2MTC),25-28 May 2020,ieeexplore
10.1109/RoEduNet51892.2020.9324883,Usage of Asymetric Small Binning to Compute Histogram of Oriented Gradients for Edge Computing Image Sensors,IEEE,Conferences,"In case of multiple imaging sensors used in different networks (home security, surveillance, automotive, industrial), there is a challenge to perform object detection algorithms in real time, even on the cloud, for a large number of sensors. This is why there is an intensive effort in the industry to move object detection processing on the edge, with the benefits of reducing the bandwidth needs and allowing for scalability in large networks. In this paper we present a hardware friendly optimization technique to compute Histogram of Oriented Gradients (HOG) on the edge, by approximating the HOG orientation as a multitude of small bins. The technique is implemented in RTL for FPGA or ASIC and serves as the first step in a standard object detection algorithm (using Histogram of Oriented Gradients as feature extractor and Support Vector Machine as the detection algorithm). We verified the results of the proposed optimizations for errors by comparison to a reference method and the overall object detection algorithm for robustness.",https://ieeexplore.ieee.org/document/9324883/,2020 19th RoEduNet Conference: Networking in Education and Research (RoEduNet),11-12 Dec. 2020,ieeexplore
10.1109/iCECE.2010.1483,Use of Accurate GPS Timing Based on Radial Basis Probabilistic Neural Network in Electric Systems,IEEE,Conferences,"GPS based time reference provides inexpensive but highly-accurate timing and synchronization capability and meets requirements in power system fault location, monitoring, and control. In the present era of restructuring and modernization of electric power utilities, the applications of GIS/GPS technology in power industry are growing and covering several technical and management activities. Because of GPS receivers error sources are time variant, it is necessary to remove the GPS measurement noise. In this paper a Radial Basis Probabilistic Neural Network (RBPNN) has been applied to GPS receivers timing data for modeling of their timing error sources. This method estimates GPS receivers timing errors from their previous values. The efficiency of this algorithm is illustrated by experiments. For real-time restitution of GPS timing accuracy, the proposed method is implemented on designed hardwares. The experimental tests results on collected real data emphasizes that GPS timing RMS errors can be reduced from 300 nsec and 200 nsec to less than 160 nsec and 41 nsec, with and without SA, respectively.",https://ieeexplore.ieee.org/document/5630774/,2010 International Conference on Electrical and Control Engineering,25-27 June 2010,ieeexplore
10.1109/ICMAE.2017.8038685,Using artificial intelligence based expert system for selection of design subcontractors: A case study in aerospace industry,IEEE,Conferences,"As one of the top expectations for type certification of an aircraft, Aviation Authorities (AA) regulate design organization to establish Design Assurance System (DAS). DAS is composed of design, independent monitoring and airworthiness functions in which these functions are specialized for aerospace industry. Besides, Design Organization Approval (DOA) is a milestone to establish a rigid Design Assurance System. By this way, design organization assures aircraft development life cycle by complying with aviation regulations. To meet requirements of Design Organization Approval, Design Organization transfers its authority and technical signatories to its subcontractors to improve effectiveness of the system. So, performance of design subcontractors shall be traceable and measurable to match capability requirements of main contractor. Thus, subcontractor evaluation is a long and complicated process; survey implementation could be misleading in some cases. The purpose of this study is to propose a novel tool to measure performance of a design subcontractor according to necessities of Design Assurance System. Up to now, there is no tool to evaluate aviation design subcontractors. With this tool, contractor firm can evaluate multiple criteria in a single run. AHP is used to prioritize criteria relative to each other one-by-one. Then, for subcontractor selection and subcontractor monitoring, Artificial Neural Network (ANN) is applied to optimize decision making process. Annual Actual Data is applied in AHP model to assess current performance score of subcontractor. To have a long term judgment of this system, the model shall be applied to a design subcontractor for more than once on fixed periods such as quarterly, yearly etc.",https://ieeexplore.ieee.org/document/8038685/,2017 8th International Conference on Mechanical and Aerospace Engineering (ICMAE),22-25 July 2017,ieeexplore
10.1109/RCAR52367.2021.9517624,Vehicle Longitudinal and Lateral Dynamics Modeling by Deep Neural Network,IEEE,Conferences,"Vehicle modeling is an important topic for the automobile industry and recently for autonomous driving. However, most of the traditional dynamic model-based methods require explicit vehicle individual component models, which usually take great effort to conduct bench experiments for model parameter identification. In this paper, a data-driven method based on neural network is proposed to build the vehicle longitudinal and lateral dynamics model, which could utilize vehicle on-board sensor measurements as training data. As the ground truth states are available, the end-to-end model could be learned without the need for individual components model. Different from the existing works, the proposed method could make full use of the vehicle state history to predict the future states, the network could learn the implicit dynamic model from the history data. Using the proposed methodology, the longitudinal and lateral models are constructed and compared with the traditional kinematics or dynamic model. Experiment results show the longitudinal acceleration model could outperform the table-based method, the acceleration modeling precision could be improved by 40%. The lateral two-layer model also has an obvious advantage in modeling precision, the lateral distance mean absolute error of the neural network model is 0.026m while the traditional method is about 0.06m.",https://ieeexplore.ieee.org/document/9517624/,2021 IEEE International Conference on Real-time Computing and Robotics (RCAR),15-19 July 2021,ieeexplore
10.1109/WOSSPA.2011.5931460,Viscosity and gas/oil ratio curves estimation using advances to neural networks,IEEE,Conferences,"In oil and gas industry, prior prediction of certain properties is needed ahead of exploration and facility design. Viscosity and gas/oil ratio (GOR), are among those properties described through curves with their values varying over a specific range of reservoir pressures. However, the usual prediction approach could result into curves that are not consistent, exhibiting scattered behaviour as compared to the real curves. In this paper two advances to artificial neural networks are implemented to solve the problem. These are Support Vector Regressors and Functional Networks. Statistical error measures have been used and showed the high performance of the proposed techniques. Moreover, the predicted curves are consistent with the actual curves.",https://ieeexplore.ieee.org/document/5931460/,"International Workshop on Systems, Signal Processing and their Applications, WOSSPA",9-11 May 2011,ieeexplore
10.1109/ISWCS.2019.8877305,Visible Light Positioning for Location-Based Services in Industry 4.0,IEEE,Conferences,"Industry 4.0 refers to the evolution in manufacturing from computerization to fully cyberphysical systems that exploit rich sensor data, adaptive real-time safety-critical control, and machine learning. An important aspect of this vision is the sensing and subsequent association of objects in the physical world with their cyber and virtual counterparts. In this paper we propose Visible Light Positioning (VLP) as an enabler for these Industry 4.0 applications. We also explore sensing techniques, including cameras (and depth sensors), and other light-based solutions for object positioning and detection along with their respective limitations. We then demonstrate an application of positioning for real time robot control in an interactive multiparty cyber-physical-virtual deployment. Lastly, based on our experience with this cyberphysical-virtual application, we propose Ray-Surface Positioning (RSP), a novel VLP technique, as a low cost positioning system for Industry 4.0.",https://ieeexplore.ieee.org/document/8877305/,2019 16th International Symposium on Wireless Communication Systems (ISWCS),27-30 Aug. 2019,ieeexplore
10.1109/ICSTCC.2019.8885611,Visual Analytics Framework for Condition Monitoring in Cyber-Physical Systems,IEEE,Conferences,"One of the biggest challenges facing the factory of the future today is to reduce the time-to-market access and increase through the improvement of competitiveness and efficiency. In order to achieve this target, data analytics in Industrial Cyber-Physical System becomes a feasible option. In this paper, a visual analytics framework for condition monitoring of the machine tool is presented with the aim to manage events and alarms at factory level. The framework is assessed in a particular use case that consists in a multi-threaded cloud-based solution for the global analysis of the behaviour of variables acquired from PLC, CNC and robot manipulator. A human-machine interface is also designed for the real-time visualization of the key performance indicators according to the user's criteria. This tool implemented is a great solution for condition monitoring and decision-making process based on data analytics from simple statistics to complex machine learning methods. The results achieved are part of the vision and implementation of the industrial test bed of “Industry and Society 5.0” platform.",https://ieeexplore.ieee.org/document/8885611/,"2019 23rd International Conference on System Theory, Control and Computing (ICSTCC)",9-11 Oct. 2019,ieeexplore
10.1109/CASE48305.2020.9216781,Weak Scratch Detection of Optical Components Using Attention Fusion Network,IEEE,Conferences,"Scratches on the optical surface can directly affect the reliability of the optical system. Machine vision-based methods have been widely applied in various industrial surface defect inspection scenarios. Since weak scratches imaging in the dark field has an ambiguous edge and low contrast, which brings difficulty in automatic defect detection. Recently, many existing visual inspection methods based on deep learning cannot effectively inspect weak scratches due to the lack of attention-aware features. To address the problems arising from industry-specific characteristics, this paper proposes “Attention Fusion Network;”, a convolutional neural network using attention mechanism built by hard and soft attention modules to generate attention-aware features. The hard attention module is implemented by integrating the brightness adjustment operation in the network, and the soft attention module is composed of scale attention and channel attention. The proposed model is trained on a real-world industrial scratch dataset and compared with other defect inspection methods. The proposed method can achieve the best performance to detect the weak scratch inspection of optical components compared to the traditional scratch detection methods and other deep learning-based methods.",https://ieeexplore.ieee.org/document/9216781/,2020 IEEE 16th International Conference on Automation Science and Engineering (CASE),20-21 Aug. 2020,ieeexplore
10.1109/SenSysML50931.2020.00007,Welcome Message from SenSys-ML'20 Chairs,IEEE,Conferences,"We are very excited to welcome you to the second ACM/IEEE International Workshop on Machine Learning on Edge in Sensor Systems (SenSys-ML 2020). SenSys-ML’20 will be held in conjunction with the CPS-IoT Week 2020 and focuses on work that combines sensor signals from the physical world with machine learning, particularly in ways that are distributed to the device or use edge and fog computing. The development and deployment of ML at the very edge remains a technological challenge constrained by computing, memory, energy, network bandwidth, and data privacy and security limitations. This is especially true for battery-operated devices and always-on use cases and applications. In recent years this has gained attention from both academia and industry and many TinyML initiatives have been started focusing both in hardware and software advancements. This workshop will provide a forum for sensing, networking and machine learning researchers to present and share their latest research on building machine learning-enabled sensor systems. Sensys-ML focuses on providing extensive feedback on Work-In-Progress papers involving machine learning (TinyML/ UltraML) on sensor systems. Many papers were submitted from multiple countries, and four papers were selected for publication. This year our articles had themes including techniques for collecting low-resolution images from thermal cameras for human activity recognition, VAE-based approach for privacy preservation of sensor data, deep model compression techniques, and novel GRU based shallow Neural Networks. We see advancement and contributions made by research work will have a significant impact on real-world applications and future research directions.",https://ieeexplore.ieee.org/document/9111711/,2020 IEEE Second Workshop on Machine Learning on Edge in Sensor Systems (SenSys-ML),21-21 April 2020,ieeexplore
10.1109/YAC.2018.8406429,Wheel classification using convolutional neural networks,IEEE,Conferences,"With the fast development of automobile wheel industry, many methods of wheel classification have been proposed. The appearance of wheel changes a lot during the process of production, but few of these conventional methods is designed to handle the many challenges of wheel classification for different appearances. This paper studies on wheel classification for different appearances during the process of production, and proposes a wheel classification method using convolutional neural network. Firstly, we implement circle detection, gray value analysis and size normalization on wheel images and collect ten common types of wheel to build a dataset. Based on the features of the wheel images, a convolutional neural network is proposed. Then data augmentation is implemented on the dataset and local response normalization layer is added in the convolutional neural network. The experiment results show that the presented method has a better performance both in classification accuracy and real-time requirement than conventional methods and achieves 95.6% accuracy on wheel classification in the dataset.",https://ieeexplore.ieee.org/document/8406429/,2018 33rd Youth Academic Annual Conference of Chinese Association of Automation (YAC),18-20 May 2018,ieeexplore
10.1109/RTSS.2018.00029,Work-in-Progress: Making Machine Learning Real-Time Predictable,IEEE,Conferences,"Machine learning (ML) on edge computing devices is becoming popular in the industry as a means to make control systems more intelligent and autonomous. The new trend is to utilize embedded edge devices, as they boast higher computational power and larger memories than before, to perform ML tasks that had previously been limited to cloud-hosted deployments. In this work, we assess the real-time predictability and consider data privacy concerns by comparing traditional cloud services with edge-based ones for certain data analytics tasks. We identify the subset of ML problems appropriate for edge devices by investigating if they result in real-time predictable services for a set of widely used ML libraries. We specifically enhance the Caffe library to make it more suitable for real-time predictability. We then deploy ML models with high accuracy scores on an embedded system, exposing it to industry sensor data from the field, to demonstrates its efficacy and suitability for real-time processing.",https://ieeexplore.ieee.org/document/8603205/,2018 IEEE Real-Time Systems Symposium (RTSS),11-14 Dec. 2018,ieeexplore
10.1109/ICITSI.2016.7858181,[Title page],IEEE,Conferences,"The following topics are dealt with: SDLC SPASI v. 4.0. business process; information extraction; statistics indicator tables; rule generalizations; ontology; conventional learning system; ICT-based learning; job training system; time-series data; RAID; software-based accelerator; virtualization environment; enterprise architecture government organization; TOGAF ADM; SONA; e- library; modified quantitative models for performance measurement system method; business process improvement; district government innovation service case study; government organization; m-government implementation evaluation; trusted Big Data; official statistics study case; data profiling; data quality improvement; secure internet access; copyright protection; color images; transform domain; luminance component; information network architecture; local government; software as a service; expert system; meningitis disease; certainty factor method; digital asset management system; broadcasting organizations; e-portofolio definition; system security requirement identification; electronic payment system; Internet-based long distance education; operational model data governance; requirement engineering; open government information network development; process capability assessment; information security management; information security governance; national cyber physical systems; e-learning readiness; remote control system; serial communications mobile; microcontroller; knowledge sharing; indonesia higher educational institutions; cultural heritage metadata; geo linked open data; NUSANTARA: knowledge management system; adaptive personalized learning system; interactive learning media; RPP ICT; government human capital management; knowledge management tools utilization; knowledge management readiness; analytic hierarchy process; government institutions; usability testing; scrum methodology; assistant information system; automatic arowana raiser; pSPEA2; strength Pareto evolutionary algorithm 2; early diagnosis expert system deficiency; digital forensic; user acceptance; human resource information system; automated plasmodium detection; malaria diagnosis; thin blood smear image; 3D virtual game; MOODLE; SLOODLE; open simulator case study; color-based segmentation; feature detection; ball post; goal post; mobile soccer robot game field; smart farming; real time q-log-based feature normalization; distant speech recognition; Monte Carlo localization; robot operating system; finite element method; 3D DC resistivity modeling; multi GPU; breast cancer lesions; adaptive thresholding; morphological operation; gamification framework; online training; collaborative working system; classification breast cancer ultrasound images; posterior features; three-wheeled omnidirectional robot controller; public services satisfaction; sentiment analysis; color blind test quantification; RGB primary color cluster; ERP modules requirement; micro, small and medium enterprise fashion industry; small culinary enterprises; business system requirement; small craft companies ; power analysis attack; DES and IT value model.",https://ieeexplore.ieee.org/document/7858181/,2016 International Conference on Information Technology Systems and Innovation (ICITSI),24-27 Oct. 2016,ieeexplore
10.1109/BDCloud.2018.00136,"iSTEP, an Integrated Self-Tuning Engine for Predictive Maintenance in Industry 4.0",IEEE,Conferences,"The recent expansion of IoT-enabled (Internet of Things) devices in manufacturing contexts and their subsequent data-driven exploitation paved the way to the advent of the Industry 4.0, promoting a full integration of IT services, smart devices, and control systems with physical objects, their electronics and sensors. The real-time transmission and analysis of collected data from factories has the potential to create manufacturing intelligence, of which predictive maintenance is an expression. Hence the need to design new approaches able to manage not only the data volume, but also the variety and velocity, extracting actual value from the humongous amounts of collected data. To this aim, we present iSTEP, an integrated Self-Tuning Engine for Predictive maintenance, based on Big Data technologies and designed for Industry 4.0 applications. The proposed approach targets some of the most common needs of manufacturing enterprises: compatibility with both the on-premises and the in-the-cloud environments, exploitation of reliable and largely supported Big Data platforms, easy deployment through containerized software modules, virtually unlimited horizontal scalability, fault-tolerant self-reconfiguration, flexible yet friendly streaming-KPI computations, and above all, the integrated provisioning of self-tuning machine learning techniques for predictive maintenance. The current implementation of iSTEP exploits a distributed architecture based on Apache Kafka, Spark Streaming, MLlib, and Cassandra; iSTEP provides (i) a specific feature engineering block aimed at automatically extracting metrics from the production monitoring time series, which improves the predictive performance by 77% on average, and (ii) a self-tuning approach that dynamically selects the best prediction algorithm, which improves the predictive performance up to 60%. The iSTEP engine provides transparent predictive models, able to provide end users with insights into the knowledge learned, and it has been experimentally evaluated on a public unbalanced failure dataset, whose extensive results are discussed in the paper.",https://ieeexplore.ieee.org/document/8672266/,"2018 IEEE Intl Conf on Parallel & Distributed Processing with Applications, Ubiquitous Computing & Communications, Big Data & Cloud Computing, Social Computing & Networking, Sustainable Computing & Communications (ISPA/IUCC/BDCloud/SocialCom/SustainCom)",11-13 Dec. 2018,ieeexplore
10.1109/TLA.2021.9480156,A Data Governance Framework for Industry 4.0,IEEE,Journals,"The fourth industrial revolution, or Industry 4.0, represents a new stage of evolution in the organization, management and control of the value chain throughout the product or service life cycle. This digitization of the industrial environment is characterized by the connection of Information Technologies (IT) and Operations Technologies (OT) through cyber-physical systems and the Industrial IoT (IIoT). One of the main consequences of this integration is the increasing amount and variety of data generated in real time from different sources. In this environment of intensive generation of actionable information, data becomes a critical asset for Industry 4.0, at all stages of the value chain. However, in order to data become a competitive advantage for the company, it must be managed and governed like any other strategic asset, and therefore it is necessary to rely on a Data Governance system. Industry 4.0 requires a reformulation of governance since the data is a key element and the backbone of the processes of the organization. This paper proposes a Reference Framework for the implementation of Data Governance Systems for Industry 4.0. Previously, it contextualizes data governance for Industry 4.0 environments and identifies the requirements that this framework must address, which are conditioned by the specific features of Industry 4.0, among others, the intensive use of big data, cloud and edge computing, artificial intelligence and current regulations.",https://ieeexplore.ieee.org/document/9480156/,IEEE Latin America Transactions,Dec. 2021,ieeexplore
10.1109/TPWRS.2019.2922333,A Data-Driven Framework for Assessing Cold Load Pick-Up Demand in Service Restoration,IEEE,Journals,"Cold load pick-up (CLPU) has been a critical concern to utilities. Researchers and industry practitioners have underlined the impact of CLPU on distribution system design and service restoration. The recent large-scale deployment of smart meters has provided the industry with a huge amount of data that are highly granular, both temporally and spatially. In this paper, a data-driven framework is proposed for assessing CLPU demand of residential customers using smart meter data. The proposed framework consists of two interconnected layers: 1) At the feeder level, a nonlinear autoregression model is applied to estimate the diversified demand during the system restoration and calculate the CLPU demand ratio. 2) At the customer level, Gaussian mixture models and probabilistic reasoning are used to quantify the CLPU demand increase. The proposed methodology has been verified using real smart meter data and outage cases.",https://ieeexplore.ieee.org/document/8735925/,IEEE Transactions on Power Systems,Nov. 2019,ieeexplore
10.1109/ACCESS.2019.2927082,A Decade of Internet of Things: Analysis in the Light of Healthcare Applications,IEEE,Journals,"Impressive growth in the number of wearable health monitoring devices has affected global health industry as they provide rapid and intricate details related to physical examinations, such as discomfort, heart rate, and blood glucose level, which enable doctors to efficiently diagnose sensitive heart troubles. The Internet of Medical Things (IoMT) is a phenomenon wherein computer networks and medical equipment are connected through the Internet to provide real-time interaction between physicians and patients. In this article, we present a comprehensive view of the IoMT and its related Machine Learning (ML)-based developed frameworks designed, or being utilized, in the last decade, i.e., from 2010 to 2019. The presented techniques are designed for monitoring limbs, controlling rural healthcare, identifying e-health applications, monitoring health through mobile apps, classifying heart sounds, detecting stress in drivers, monitoring cardiac diseases, making the decision to predict heart attacks, recognizing human activities, and classifying breast cancer. The aim is to provide a clear picture of the existing IoMT environment so that the analysis may pave the way for the diagnosis of critical disorders such as cancer, heart attack, and blood pressure among others. In the end, we also provide some unresolved challenges that are confronted in the deployment of the secure IoMT-based healthcare systems.",https://ieeexplore.ieee.org/document/8755982/,IEEE Access,2019,ieeexplore
10.1109/TII.2019.2915846,A Global Manufacturing Big Data Ecosystem for Fault Detection in Predictive Maintenance,IEEE,Journals,"Artificial intelligence, big data, machine learning, cloud computing, and Internet of Things (IoT) are terms which have driven the fourth industrial revolution. The digital revolution has transformed the manufacturing industry into smart manufacturing through the development of intelligent systems. In this paper, a big data ecosystem is presented for the implementation of fault detection and diagnosis in predictive maintenance with real industrial big data gathered directly from large-scale global manufacturing plants, aiming to provide a complete architecture which could be used in industrial IoT-based smart manufacturing in an industrial 4.0 system. The proposed architecture overcomes multiple challenges including big data ingestion, integration, transformation, storage, analytics, and visualization in a real-time environment using various technologies such as the data lake, NoSQL database, Apache Spark, Apache Drill, Apache Hive, OPC Collector, and other techniques. Transformation protocols, authentication, and data encryption methods are also utilized to address data and network security issues. A MapReduce-based distributed PCA model is designed for fault detection and diagnosis. In a large-scale manufacturing system, not all kinds of failure data are accessible, and the absence of labels precludes all the supervised methods in the predictive phase. Furthermore, the proposed framework takes advantage of some of the characteristics of PCA such as its ease of implementation on Spark, its simple algorithmic structure, and its real-time processing ability. All these elements are essential for smart manufacturing in the evolution to Industry 4.0. The proposed detection system has been implemented into the real-time industrial production system in a cooperated company, running for several years, and the results successfully provide an alarm warning several days before the fault happens. A test case involving several outages in 2014 is reported and analyzed in detail during the experiment section.",https://ieeexplore.ieee.org/document/8710319/,IEEE Transactions on Industrial Informatics,Jan. 2020,ieeexplore
10.1109/TIM.2020.3047194,A High-Precision Diagnosis Method for Damp Status of OIP Bushing,IEEE,Journals,"An accurate assessment of the damp status of oil-impregnated paper (OIP) bushings is crucial for the power industry to make informed decisions on the maintenance and replacement schedule of bushings. This article proposes a hybrid of the convolutional neural network (CNN) and the hidden Markov model (HMM) for estimating the damp status (i.e., moisture level and moisture source) of bushings especially when nonuniform moisture distribution exhibits in the bushing. First, simulation models of moisture diffusion and frequency-domain spectroscopy (FDS) of the OIP bushing were constructed using the finite element modeling (FEM) approach. Then, CNN was employed to extract informative features from FDS results of the OIP bushing, which is sensitive to both concentrations and sources of moisture. Finally, HMMs were further utilized as a strong stability tool to recognize the damp status of OIP bushings. The proposed method was implemented to identify the bushing damp status using both simulation data and real-life measurements. Identification results demonstrate that the proposed method has high accuracy in determining the moisture level and moisture source of the OIP bushing insulation.",https://ieeexplore.ieee.org/document/9306924/,IEEE Transactions on Instrumentation and Measurement,2021,ieeexplore
10.1109/JSEN.2021.3113908,A Lightweight Framework for Human Activity Recognition on Wearable Devices,IEEE,Journals,"Human Activity Recognition (HAR) is the automatic detection and understanding of human motion behavior based on data extracted from video camera, ambient sensors or wearable sensors which particularly has recently attracted increased attention from both researchers and industry. However, for running practical HAR systems on wearable devices, there are some requirements such as design and development of small, lightweight, powerful, and low-cost smart sensors. In this context, data must be continuously collected, and edge computing is a viable solution, which is an energy-efficient technique, offering real-time response and privacy requirements for HAR applications. Rather than sending data to the cloud, edge computing is a local process that minimizes the data transmission time and responds with low latency. Recently, HAR system designers have adopted deep learning techniques inspired by their outstanding performance in many application areas and achieved relevant gain in activity recognition performance, however, these techniques were not demonstrated suitable for running on resource constrained devices. Thus, designing energy-efficient deep learning models is critical for realizing efficient HAR for mobile applications. In this work, we present a lightweight framework for the deployment of low-power but accurate HAR systems for these devices. We also implement and run the system in a microcontroller and analyze computational cost and energy consumption, and how different system configurations and deep learning model complexity influence on that.",https://ieeexplore.ieee.org/document/9541192/,IEEE Sensors Journal,"1 Nov.1, 2021",ieeexplore
10.1109/ACCESS.2020.2996214,A Machine Learning Security Framework for Iot Systems,IEEE,Journals,"Internet of Things security is attracting a growing attention from both academic and industry communities. Indeed, IoT devices are prone to various security attacks varying from Denial of Service (DoS) to network intrusion and data leakage. This paper presents a novel machine learning (ML) based security framework that automatically copes with the expanding security aspects related to IoT domain. This framework leverages both Software Defined Networking (SDN) and Network Function Virtualization (NFV) enablers for mitigating different threats. This AI framework combines monitoring agent and AI-based reaction agent that use ML-Models divided into network patterns analysis, along with anomaly-based intrusion detection in IoT systems. The framework exploits the supervised learning, distributed data mining system and neural network for achieving its goals. Experiments results demonstrate the efficiency of the proposed scheme. In particular, the distribution of the attacks using the data mining approach is highly successful in detecting the attacks with high performance and low cost. Regarding our anomaly-based intrusion detection system (IDS) for IoT, we have evaluated the experiment in a real Smart building scenario using one-class SVM. The detection accuracy of anomalies achieved 99.71%. A feasibility study is conducted to identify the current potential solutions to be adopted and to promote the research towards the open challenges.",https://ieeexplore.ieee.org/document/9097876/,IEEE Access,2020,ieeexplore
10.1109/ACCESS.2020.3000960,A Novel PPA Method for Fluid Pipeline Leak Detection Based on OPELM and Bidirectional LSTM,IEEE,Journals,"Pipeline leak detection has attracted great research interests for years in the energy industry. Continuous pressure monitoring is one of the most straightforward approaches in leak detection which utilizes pressure point analysis (PPA) algorithms to exploit the transient pressure characteristics and identify leak events. However, a critical issue that jeopardizes the deployment of PPA based methods is the high false alarm rate. In this paper, a novel PPA based leak detection method is proposed which can accurately detect the leak events and dramatically decrease the number of false alarms compared to existing methods. Firstly, the proposed method takes advantage of the good approximation ability and fast learning speed of optimally-pruned extreme learning machine (OPELM) to produce a preliminary leak detection result. Then, the strong memorizing ability of bidirectional long-short term memory (BiLSTM) network is exploited to identify the true positive from the preliminary detection result, hence significantly decrease the number of false alarms. Furthermore, a feature extraction mechanism is also proposed to obtain both the dynamic and static characteristics from raw pressure wave. Experiments and verifications are performed on different real world data sets obtained from pipeline leak tests. It shows that the proposed method can achieve higher detection accuracy with significantly less false alarms. It enhances the practicality of pressure monitoring based leak detection schemes.",https://ieeexplore.ieee.org/document/9110829/,IEEE Access,2020,ieeexplore
10.1109/ACCESS.2020.2990190,"A Novel Simulated-Annealing Based Electric Bus System Design, Simulation, and Analysis for Dehradun Smart City",IEEE,Journals,"Smart transportation network development with environmental issues into consideration has brought Industry 4.0 based solutions on priority. In this direction, battery-powered electric bus systems have been considered widely for ensuring flexibility, operation cost, and lesser pollutants emission. Industry 4.0 provides automation through a cyber-physical system (CPS), the interconnection of bus system entities with industrial internet-of-things (IIoT), remote information availability through cloud computing and scientific disciplines (human-computer interaction, artificial intelligence, machine learning etc.) integration. In this work, a discrete event-based simulation-optimization approach is integrated that take care of bus energy consumption according to real-time city's passenger needs and on-road friction levels. The proposed simulation optimization methodology utilizes multi-objective with dependent and independent variables for optimizing the overall system performance. In simulation optimization, objective functions are designed to tackle battery consumption, Internet-of-Thing (IoT) network performance, cloud operations efficiency and smart scientific discipline integration. Simulation parameters are based on a real-time bus system which is further analyzed, filtered and adapted as per the needs of the system. In another analysis, supercharger's capacities are varied to evaluate the performance of the proposed system and identify the low cost and efficient smart transportation system. Simulation results show different scenarios for variations in the number of buses, charging stations, bus-depots, mobile charging facilities, and bus-schedules. Simulation results show that the average passenger's waiting time in the waiting is (after ticket booking) varies between 0.2 minutes to 0.7 minutes in real-time traffic conditions. In similar traffic conditions, total passenger's time in system (ticket booking to travel) varies between 41.6 minutes (for 24 hours) to 45.5 minutes (for 1 year). In the simulation, priorities are given to those dependent and independent variables which save the battery consumption and elongate the utilization of buses. Lastly, it is also observed that the proposed system is suitable for resource-constraint devices because Gate Equivalent (GE) calculation shows that the proposed system can be implemented between 1986 GEs (communicational cost without confidentiality and authentication) and 7939 GEs (computational cost with HMAC for authentication in data storage). This ensures varies security primitivs such as confidentiality, availability and authentication.",https://ieeexplore.ieee.org/document/9078106/,IEEE Access,2020,ieeexplore
10.1109/TSG.2011.2159819,A Resilient Real-Time System Design for a Secure and Reconfigurable Power Grid,IEEE,Journals,"Energy infrastructure is a critical underpinning of modern society that any compromise or sabotage of its secure and reliable operation has an enormous impact on people's daily lives and the national economy. The massive northeastern power blackout of August 2003 and the most recent Florida blackout have both revealed serious defects in both system-level management and device-level designs of the power grid in handling attacks. At the system level, the control area operators lack the capability to 1) obtain real-time status information of the vastly distributed equipment; 2) respond rapidly enough once events start to unravel; and 3) perform coordinated actions autonomously across the region. At the device level, the traditional hardware lacks the capability to 1) provide reliable frequency and voltage control according to system demands and 2) rapidly reconfigure the system to a secure state through switches and power-electronics based devices. These blackouts were a wake-up call for both the industry and academia to consider new techniques and system architecture design that can help assure the security and reliability of the power grid. In this paper, we present a hardware-in-the-loop reconfigurable system design with embedded intelligence and resilient coordination schemes at both local and system levels that would tackle the vulnerabilities of the grid. The new system design consists of five key components: 1) a location-centric hybrid system architecture that facilitates not only distributed processing but also coordination among geographically close devices; 2) the insertion of intelligence into power electronic devices at the lower level of the power grid to enable a more direct reconfiguration of the physical makeup of the grid; 3) the development of a robust collaboration algorithm among neighboring devices to handle possible faulty, missing, or incomplete information; 4) the design of distributed algorithms to better understand the local state of the power grid; and 5) the adoption of a control-theoretic real-time adaptation strategy to guarantee the availability of large distributed systems. Preliminary evaluation results showing the advantages of each component are provided. A phased implementation plan is also suggested at the end of the discussion.",https://ieeexplore.ieee.org/document/6003812/,IEEE Transactions on Smart Grid,Dec. 2011,ieeexplore
10.1109/ACCESS.2020.3001277,"A Survey of Multi-Access Edge Computing in 5G and Beyond: Fundamentals, Technology Integration, and State-of-the-Art",IEEE,Journals,"Driven by the emergence of new compute-intensive applications and the vision of the Internet of Things (IoT), it is foreseen that the emerging 5G network will face an unprecedented increase in traffic volume and computation demands. However, end users mostly have limited storage capacities and finite processing capabilities, thus how to run compute-intensive applications on resource-constrained users has recently become a natural concern. Mobile edge computing (MEC), a key technology in the emerging fifth generation (5G) network, can optimize mobile resources by hosting compute-intensive applications, process large data before sending to the cloud, provide the cloud-computing capabilities within the radio access network (RAN) in close proximity to mobile users, and offer context-aware services with the help of RAN information. Therefore, MEC enables a wide variety of applications, where the real-time response is strictly required, e.g., driverless vehicles, augmented reality, robotics, and immerse media. Indeed, the paradigm shift from 4G to 5G could become a reality with the advent of new technological concepts. The successful realization of MEC in the 5G network is still in its infancy and demands for constant efforts from both academic and industry communities. In this survey, we first provide a holistic overview of MEC technology and its potential use cases and applications. Then, we outline up-to-date researches on the integration of MEC with the new technologies that will be deployed in 5G and beyond. We also summarize testbeds and experimental evaluations, and open source activities, for edge computing. We further summarize lessons learned from state-of-the-art research works as well as discuss challenges and potential future directions for MEC research.",https://ieeexplore.ieee.org/document/9113305/,IEEE Access,2020,ieeexplore
10.1109/ACCESS.2020.2974035,A Survey on Behavioral Pattern Mining From Sensor Data in Internet of Things,IEEE,Journals,"The deployment of large-scale wireless sensor networks (WSNs) for the Internet of Things (IoT) applications is increasing day-by-day, especially with the emergence of smart city services. The sensor data streams generated from these applications are largely dynamic, heterogeneous, and often geographically distributed over large areas. For high-value use in business, industry and services, these data streams must be mined to extract insightful knowledge, such as about monitoring (e.g., discovering certain behaviors over a deployed area) or network diagnostics (e.g., predicting faulty sensor nodes). However, due to the inherent constraints of sensor networks and application requirements, traditional data mining techniques cannot be directly used to mine IoT data streams efficiently and accurately in real-time. In the last decade, a number of works have been reported in the literature proposing behavioral pattern mining algorithms for sensor networks. This paper presents the technical challenges that need to be considered for mining sensor data. It then provides a thorough review of the mining techniques proposed in the recent literature to mine behavioral patterns from sensor data in IoT, and their characteristics and differences are highlighted and compared. We also propose a behavioral pattern mining framework for IoT and discuss possible future research directions in this area.",https://ieeexplore.ieee.org/document/8999541/,IEEE Access,2020,ieeexplore
10.1109/ACCESS.2021.3121302,A Survey on Industry 4.0 for the Oil and Gas Industry: Upstream Sector,IEEE,Journals,"The market volatility in the oil and gas (O&amp;G) sector, the dwindling demand for oil due to the impact of COVID-19, and the push for alternative greener energy are driving the need for innovation and digitization in the O&amp;G industry. This has attracted research interest from academia and the industry in the application of industry 4.0 (I4.0) technologies in the O&amp;G sector. The application of some of these I4.0 technologies has been presented in the literature, but the domain still lacks a comprehensive survey of the application of I4.0 in the O&amp;G upstream sector. This paper investigates the state-of-the-art efforts directed toward I4.0 technologies in the O&amp;G upstream sector. To achieve this, first, an overview of the I4.0 is discussed followed by a systematic literature review from an integrative perspective for publications between 2012–2021 with 223 analyzed documents. The benefits and challenges of the adoption of I4.0 have been identified. Moreover, the paper adds value by proposing a framework for the implementation of I4.0 in the O&amp;G upstream sector. Finally, future directions and research opportunities such as framework, edge computing, quantum computing, communication technologies, standardization, and innovative areas related to the implementation of I4.0 in the upstream sector are presented. The findings from this review show that I4.0 technologies are currently being explored and deployed for various aspects of the upstream sector. However, some of the I4.0 technologies like additive manufacturing and virtual reality are least explored.",https://ieeexplore.ieee.org/document/9579415/,IEEE Access,2021,ieeexplore
10.1109/ACCESS.2017.2678990,Adaptive Scheme for Caching YouTube Content in a Cellular Network: Machine Learning Approach,IEEE,Journals,"Content caching at base stations is a promising solution to address the large demands for mobile data services over cellular networks. Content caching is a challenging problem as it requires predicting the future popularity of the content and the operating characteristics of the cellular networks. In this paper, we focus on constructing an algorithm that improves the users' quality of experience (QoE) and reduces network traffic. The algorithm accounts for users' behavior and properties of the cellular network (e.g. cache size, bandwidth, and load). The constructed content and network aware adaptive caching scheme uses an extreme-learning machine neural network to estimate the popularity of content, and mixed-integer linear programming to compute where to place the content and select the physical cache sizes in the network. The proposed caching scheme simultaneously performs efficient cache deployment and content caching. Additionally, a simultaneous perturbation stochastic approximation method is developed to reduce the number of neurons in the extreme-learning machine method while ensuring a sufficient predictive performance is maintained. Using real-world data from YouTube and a NS-3 simulator, we demonstrate how the caching scheme improves the QoE of users and network performance compared with industry standard caching schemes.",https://ieeexplore.ieee.org/document/7873292/,IEEE Access,2017,ieeexplore
10.1109/ACCESS.2021.3111229,AdaptiveSystems: An Integrated Framework for Adaptive Systems Design and Development Using MPS JetBrains Domain-Specific Modeling Environment,IEEE,Journals,"This paper contains the design and development of an adaptive systems (<italic>AdaptiveSystems</italic> Domain-Specific Language - DSL) framework to assist language developers and data scientists in their attempt to apply Artificial Intelligence (AI) algorithms in several application domains. Big-data processing and AI algorithms are at the heart of autonomics research groups among industry and academia. Major advances in the field have traditionally focused on algorithmic research and increasing the performance of the developed algorithms. However, it has been recently recognized by the AI community that the applicability of these algorithms and their consideration in context is of paramount importance for their adoption. Current approaches to address AI in context lie in two areas: adaptive systems research that mainly focuses on implementing adaptivity mechanisms (technical perspective) and AI in context research that focuses on business aspects (business perspective). There is currently no approach that combines all aspects required from business considerations to an appropriate level of abstraction. In this paper, we attempt to address the problem of designing adaptive systems and therefore providing AI in context by utilising DSL technology. We propose a new DSL (<italic>AdaptiveSystems</italic>) and a methodology to apply this to the creation of a DSL for specific application domains such as <italic>AdaptiveVLE (Adaptive Virtual Learning Environment)</italic> DSL. The language developer will be able to instantiate the <italic>AdaptiveSystems</italic> DSL to any application domain by using the guidelines in this paper with an integrated path from design to implementation. The domain expert will then be able to use the developed DSL (e.g. <italic>AdaptiveVLE</italic> DSL) to design and develop their application. Future work will include extension and experimentation of the applicability of this work to more application domains within British Telecom (BT) and other areas such as health care, finance, etc.",https://ieeexplore.ieee.org/document/9531598/,IEEE Access,2021,ieeexplore
10.1109/ACCESS.2020.2991474,An Adaptive Method for Inspecting Illumination of Color Intensity in Transparent Polyethylene Terephthalate Preforms,IEEE,Journals,"Machine vision systems are applied in industry to control the quality of production while optimizing efficiency. A machine vision and AI-based inspection of color intensity in transparent Polyethylene Terephthalate (PET) preforms is especially sensitive to backgrounds and lighting, therefore, much attention is given to its illumination conditions. The paper examines the adverse factors affecting the quality of image recognition and presents an adaptive method for reducing the influence of changing illumination conditions in the color inspection process of transparent PET preforms. The method is based on predicting measured color intensity correction parameters according to illumination conditions. To test this adaptive method, a hardware and software system for image capture and processing was developed. This system is capable of inspecting large quantities of preforms in real time using a neural network with a modified gradient descent and momentum algorithm. The experiment showed that correction of the measured color intensity value reduced the standard deviation caused by variable and uneven illumination by 61.51%, demonstrating that machine vision color intensity evaluation is a robust and adaptive solution under illuminated conditions for detecting abnormalities in machine-based PET inspection procedures.",https://ieeexplore.ieee.org/document/9082606/,IEEE Access,2020,ieeexplore
10.1109/TITS.2020.2980855,An Efficient and Scalable Simulation Model for Autonomous Vehicles With Economical Hardware,IEEE,Journals,"Autonomous vehicles rely on sophisticated hardware and software technologies for acquiring holistic awareness of their immediate surroundings. Deep learning methods have effectively equipped modern self-driving cars with high levels of such awareness. However, their application requires high-end computational hardware, which makes utilization infeasible for the legacy vehicles that constitute most of today's automotive industry. Hence, it becomes inherently challenging to achieve high performance while at the same time maintaining adequate computational complexity. In this paper, a monocular vision and scalar sensor-based model car is designed and implemented to accomplish autonomous driving on a specified track by employing a lightweight deep learning model. It can identify various traffic signs based on a vision sensor as well as avoid obstacles by using an ultrasonic sensor. The developed car utilizes a single Raspberry Pi as its computational unit. In addition, our work investigates the behavior of economical hardware used to deploy deep learning models. In particular, we herein propose a novel, computationally efficient, and cost-effective approach. The designed system can serve as a platform to facilitate the development of economical technologies for autonomous vehicles that can be used as part of intelligent transportation or advanced driver assistance systems. The experimental results indicate that this model can achieve real-time response on a resource-constrained device without significant overheads, thus making it a suitable candidate for autonomous driving in current intelligent transportation systems.",https://ieeexplore.ieee.org/document/9094331/,IEEE Transactions on Intelligent Transportation Systems,March 2021,ieeexplore
10.1109/TII.2019.2962029,An Entropy-Based Approach to Real-Time Information Extraction for Industry 4.0,IEEE,Journals,"Industry 4.0 has drawn considerable attention from industry and academic research communities. The recent advances in Internet of Things (IoT), Big Data analytics, sensor technology, and artificial intelligence have led to the design and implementation of novel approaches to take full advantage of data-driven solutions applicable to Industry 4.0. With the availability of large datasets, it has become crucially important to identify the appropriate amount of relevant information, which would optimize the overall analysis of the corresponding systems. In this article, specific properties of dynamically evolving data systems are introduced and investigated, which provide framework to assess the appropriate amount of representative information.",https://ieeexplore.ieee.org/document/8941297/,IEEE Transactions on Industrial Informatics,Sept. 2020,ieeexplore
10.1109/TSMC.1977.4309725,An Interactive Program Fo Conversational Elicitation of Decision Structures,IEEE,Journals,"An interactive computer program has been designed and implemented that elicits a decision tree from a decisionmaker in an English-like conversational mode. It emulates a decision analyst who guides the decisionmaker in structuring and organizing his knowledge about a particular problem domain. The objectives of the research were: 1) to provide the decision analysis industry with a practical automated tool for eliciting decision structures where manual elicitation techniques are either infeasible or uneconomical, 2) to cast the decision analyst's behavior into a formal framework in order to examine the principles governing the elicitation procedure and gain a deeper understanding of the analysis process itself, and 3) to provide experimental psychologists with an automated research tool for coding subjects' perception of problem situations into a standard and formal representation. The approach centers on the realization that the process of conducting an elicitation dialogue is structurally identical to conducting a heuristic search on game trees, as is commonly practiced in artificial intelligence programs. Heuristic search techniques, when applied to tree elicitation, permit real-time rollback and sensitivity analysis as the tree is being formulated. Thus it is possible to concentrate effort on expanding those parts of the tree which are crucial for the resolution of the solution plan. The program requires the decisionmaker to provide provisional values at each intermediate stage in the tree construction, which estimate the promise of future opportunities open to him from that stage.",https://ieeexplore.ieee.org/document/4309725/,"IEEE Transactions on Systems, Man, and Cybernetics",May 1977,ieeexplore
10.1109/TCIAIG.2012.2212194,AntBot: Ant Colonies for Video Games,IEEE,Journals,"The video game industry is an emerging market which continues to expand. From its early beginning, developers have focused mainly on sound and graphical applications, paying less attention to developing game bots or other kinds of nonplayer characters (NPCs). However, recent advances in artificial intelligence offer the possibility of developing game bots which are dynamically adjustable to several difficulty levels as well as variable game environments. Previous works reveal a lack of swarm intelligence approaches to develop these kinds of agents. Considering the potential of particle swarm optimization due to its emerging properties and self-adaptation to dynamic environments, further investigation into this field must be undertaken. This research focuses on developing a generic framework based on swarm intelligence, and in particular on ant colony optimization, such as it allows general implementation of real-time bots that work over dynamic game environments. The framework has been adapted to allow the implementation of intelligent agents for the classical game Ms. Pac-Man. These were trialed at the Ms. Pac-Man competitions held during the 2011 International Congress on Evolutionary Computation.",https://ieeexplore.ieee.org/document/6262464/,IEEE Transactions on Computational Intelligence and AI in Games,Dec. 2012,ieeexplore
10.1109/TMECH.2013.2253116,Approaching Servoclass Tracking Performance by a Proportional Valve-Controlled System,IEEE,Journals,"An industry-grade proportional valve is much cheaper and rugged than a servovalve. A feedforward controller for a proportional valved system has been developed here to achieve tracking controls beyond 1 Hz that are usually attained by servovalves. For compensating the higher nonlinearities, feedforward controllers have been designed offline by proposing appropriate static models for friction and valve flow and executing the supporting experiments. These controllers have been implemented with real-time PID feedback of only the piston displacement. Excellent tracking performance has been obtained up to 2 Hz that has deteriorated with an increase in cylinder friction.",https://ieeexplore.ieee.org/document/6494307/,IEEE/ASME Transactions on Mechatronics,Aug. 2013,ieeexplore
10.1109/JAS.2020.1003021,Artificial intelligence applications in the development of autonomous vehicles: a survey,IEEE,Journals,"The advancement of artificial intelligence (AI) has truly stimulated the development and deployment of autonomous vehicles (AVs) in the transportation industry. Fueled by big data from various sensing devices and advanced computing resources, AI has become an essential component of AVs for perceiving the surrounding environment and making appropriate decision in motion. To achieve goal of full automation (i.e., self-driving), it is important to know how AI works in AV systems. Existing research have made great efforts in investigating different aspects of applying AI in AV development. However, few studies have offered the research community a thorough examination of current practices in implementing AI in AVs. Thus, this paper aims to shorten the gap by providing a comprehensive survey of key studies in this research avenue. Specifically, it intends to analyze their use of AIs in supporting the primary applications in AVs: 1) perception; 2) localization and mapping; and 3) decision making. It investigates the current practices to understand how AI can be used and what are the challenges and issues associated with their implementation. Based on the exploration of current practices and technology advances, this paper further provides insights into potential opportunities regarding the use of AI in conjunction with other emerging technologies: 1) high definition maps, big data, and high performance computing; 2) augmented reality (AR)/virtual reality (VR) enhanced simulation platform; and 3) 5G communication for connected AVs. This paper is expected to offer a quick reference for researchers interested in understanding the use of AI in AV research.",https://ieeexplore.ieee.org/document/9016391/,IEEE/CAA Journal of Automatica Sinica,March 2020,ieeexplore
10.1109/TII.2018.2816971,Automatic Selection of Optimal Parameters Based on Simple Soft-Computing Methods: A Case Study of Micromilling Processes,IEEE,Journals,"Nowadays, the application of novel soft-computing methods to new industrial processes is often limited by the actual capacity of the industry to assimilate state-of-the-art computational methods. The selection of optimal parameters for efficient operation is very challenging in microscale manufacturing processes, because of intrinsic nonlinear behavior and reduced dimensions. In this paper, a decision-making system for selecting optimal parameters in micromilling operations is designed and implemented using simple and efficient soft-computing techniques. The procedure primarily consists of four steps: an experimental characterization; the modeling of cutting force and surface roughness by means of a multilayer perceptron; multiobjective optimization using the cross-entropy method, taking into account productivity and surface quality; and a decision-making procedure for selecting the most appropriate parameters using a fuzzy inference system. Finally, two different alloys for micromilling processes are considered, in order to evaluate the proposed system: a titanium-based alloy and a tungsten-copper alloy. The experimental study demonstrated the effectiveness of the proposed solution for automated decision-making, based on simple soft-computing methods, and its successful application to a real-life industrial challenge.",https://ieeexplore.ieee.org/document/8325494/,IEEE Transactions on Industrial Informatics,Feb. 2019,ieeexplore
10.1109/TSMCC.2004.843236,Automatic system for quality-based classification of marble textures,IEEE,Journals,"In this paper, we present an automatic system and algorithms for the classification of marble slabs into different groups in real time in production line, according to slabs quality. The application of the system is aimed at the marble industry, in order to automate and improve the manual classification process of marble slabs carried out at present. The system consists of a mechatronic prototype, which houses all the required physical components for the acquisition of marble slabs images in suitable light conditions, and computational algorithms, which are used to analyze the color texture of the marble surfaces and classify them into their corresponding group. In order to evaluate the color representation influence on the image analysis, four color spaces have been tested: RGB, XYZ, YIQ, and K-L. After the texture analysis performed with the sum and difference histograms algorithm, a feature extraction process has been implemented with principal component analysis. Finally, a multilayer perceptron neural network trained with the backpropagation algorithm with adaptive learning rate is used to classify the marble slabs in three categories, according to their quality. The results (successful classification rate of 98.9%) show very high performance compared with the traditional (manual) system.",https://ieeexplore.ieee.org/document/1522532/,"IEEE Transactions on Systems, Man, and Cybernetics, Part C (Applications and Reviews)",Nov. 2005,ieeexplore
10.1109/ACCESS.2020.2993010,Bearing Intelligent Fault Diagnosis in the Industrial Internet of Things Context: A Lightweight Convolutional Neural Network,IEEE,Journals,"The advancement of Industry 4.0 and Industrial Internet of Things (IIoT) has laid more emphasis on reducing the parameter amount and storage space of the model in addition to the automatic and accurate fault diagnosis. In this case, this paper proposes a lightweight convolutional neural network (LCNN) method for intelligent fault diagnosis of bearing, which can largely satisfy the need of less parameter amount and storage space as well as high accuracy. First, depthiwise separable convolution is adopted, and a LCNN structure is constructed through an inverse residual structure and a linear bottleneck layer operation. Secondly, a novel decomposed Hierarchical Search Space is introduced to automatically explore the optimal LCNN for bearing fault diagnosis in the context of the IIoT. In the meantime, the real-time monitoring and fault diagnosis of the model are also deployed. In order to verify the validity of the designed model, Case Western Reserve University Bearing fault dataset and MFPT bearing fault dataset are adopted. The results demonstrate the great advantages of the model. The LCNN model can automatically learn and select the appropriate features, highly improving the fault diagnosis accuracy. Meanwhile, the computational and storage costs of the model are largely reduced, which contributes to its being applied to the mechanical system in the IIoT context.",https://ieeexplore.ieee.org/document/9088980/,IEEE Access,2020,ieeexplore
10.1109/TCSS.2014.2377811,Behavioral Analysis of Insider Threat: A Survey and Bootstrapped Prediction in Imbalanced Data,IEEE,Journals,"The problem of insider threat is receiving increasing attention both within the computer science community as well as government and industry. This paper starts by presenting a broad, multidisciplinary survey of insider threat capturing contributions from computer scientists, psychologists, criminologists, and security practitioners. Subsequently, we present the behavioral analysis of insider threat (BAIT) framework, in which we conduct a detailed experiment involving 795 subjects on Amazon Mechanical Turk (AMT) in order to gauge the behaviors that real human subjects follow when attempting to exfiltrate data from within an organization. In the real world, the number of actual insiders found is very small, so supervised machine-learning methods encounter a challenge. Unlike past works, we develop bootstrapping algorithms that learn from highly imbalanced data, mostly unlabeled, and almost no history of user behavior from an insider threat perspective. We develop and evaluate seven algorithms using BAIT and show that they can produce a realistic (and acceptable) balance of precision and recall.",https://ieeexplore.ieee.org/document/7010900/,IEEE Transactions on Computational Social Systems,June 2014,ieeexplore
10.1109/ACCESS.2021.3106797,CNC Machine Tool Fault Diagnosis Integrated Rescheduling Approach Supported by Digital Twin-Driven Interaction and Cooperation Framework,IEEE,Journals,"The problems of CNC machine tool (CNCMT) fault diagnosis and production rescheduling have attracted continuous attention because of their great significance to the manufacturing industry. Digital twin is a supporting technology for achieving smart manufacturing and provides a new paradigm for solving these problems. This paper explores a digital twin-driven interaction and cooperation framework and proposes the architecture and implementation mechanism to enable the sharing of data, knowledge, and resource, to realize the fusion of physical space and cyber space, and to improve the accuracy of fault diagnosis. Under this framework, aiming at the influence of CNCMT failure on the initial production planning, a self-adaptation rescheduling method based on Monte Carlo Tree Search (MCTS) algorithm is proposed to provide support for developing more efficient production planning. Finally, the effectiveness of the proposed framework is validated by experimental study. The framework and integrated rescheduling approach can provide guidance for enterprises in implementing CNCMT maintenance and production scheduling to meet high accuracy and reliability requirements.",https://ieeexplore.ieee.org/document/9520390/,IEEE Access,2021,ieeexplore
10.1109/TDEI.2020.009070,Condition Monitoring Based on Partial Discharge Diagnostics Using Machine Learning Methods: A Comprehensive State-of-the-Art Review,IEEE,Journals,"This paper presents a state-of-the-art review on machine learning (ML) based intelligent diagnostics that have been applied for partial discharge (PD) detection, localization, and pattern recognition. ML techniques, particularly those developed in the last five years, are examined and classified as conventional ML or deep learning (DL). Important features of each method, such as types of input signal, sampling rate, core methodology, and accuracy, are summarized and compared in detail. Advantages and disadvantages of different ML algorithms are discussed. Moreover, technical roadblocks preventing intelligent PD diagnostics from being applied to industry are identified, such as insufficient/imbalanced dataset, data inconsistency, and difficulties in cost-effective real-time deployment. Finally, potential solutions are proposed, and future research directions are suggested.",https://ieeexplore.ieee.org/document/9293208/,IEEE Transactions on Dielectrics and Electrical Insulation,December 2020,ieeexplore
10.1109/TASE.2020.3010536,Condition-Driven Data Analytics and Monitoring for Wide-Range Nonstationary and Transient Continuous Processes,IEEE,Journals,"Frequent and wide changes in operation conditions are quite common in real process industry, resulting in typical wide-range nonstationary and transient characteristics along time direction. The considerable challenge is, thus, how to solve the conflict between the learning model accuracy and change complexity for analysis and monitoring of nonstationary and transient continuous processes. In this work, a novel condition-driven data analytics method is developed to handle this problem. A condition-driven data reorganization strategy is designed which can neatly restore the time-wise nonstationary and transient process into different condition slices, revealing similar process characteristics within the same condition slice. Process analytics can then be conducted for the new analysis unit. On the one hand, coarse-grained automatic condition-mode division is implemented with slow feature analysis to track the changing operation characteristics along condition dimension. On the other hand, fine-grained distribution evaluation is performed for each condition mode with Gaussian mixture model. Bayesian inference-based distance (BID) monitoring indices are defined which can clearly indicate the fault effects and distinguish different operation scenarios with meaningful physical interpretation. A case study on a real industrial process shows the feasibility of the proposed method which, thus, can be generalized to other continuous processes with typical wide-range nonstationary and transient characteristics along time direction. <italic>Note to Practitioners</italic>—Industrial processes in general have nonstationary characteristics which are ubiquitous in real world data, often reflected by a time-variant mean, a time-variant autocovariance, or both resulting from various factors. The focus of this study is to develop a universal analytics and monitoring method for wide-range nonstationary and transient continuous processes. Condition-driven concept takes the place of time-driven thought. For the first time, it is recognized that there are similar process characteristics within the same condition slice and changes in the process correlations may relate to its condition modes. Besides, the proposed method can provide enhanced physical interpretation for the monitoring results with concurrent analysis of the static and dynamic information which carry different information, analogous to the concepts of “position” and “velocity” in physics, respectively. The static information can tell the current operation condition, while the dynamic information can clarify whether the process status is switching between different steady states. It is noted that the condition-driven concept is universal and can be extended to other applications for industrial manufacturing applications.",https://ieeexplore.ieee.org/document/9158352/,IEEE Transactions on Automation Science and Engineering,Oct. 2021,ieeexplore
10.1109/JSEN.2008.926923,Data Processing Method Applying Principal Component Analysis and Spectral Angle Mapper for Imaging Spectroscopic Sensors,IEEE,Journals,"A data processing method to classify hyperspectral images from an imaging spectroscopic sensor is evaluated. Each image contains the whole diffuse reflectance spectra of the analyzed material for all the spatial positions along a specific line of vision. The implemented linear algorithm comes to solve real time constrains typical of industrial systems. This processing method is composed of two blocks: data compression is performed by means of principal component analysis (PCA) and the spectral interpretation algorithm for classification is the spectral angle mapper (SAM). This strategy, applying PCA and SAM, has been successfully tested for online raw material sorting in the tobacco industry, where the desired raw material (tobacco leaves) should be discriminated from other unwanted spurious materials, such as plastic, cardboard, leather, feathers, candy paper, etc. Hyperspectral images are recorded by a sensor consisting of a monochromatic camera and a passive prism-grating-prism device. Performance results are compared with a spectral interpretation algorithm based on artificial neural networks (ANN).",https://ieeexplore.ieee.org/document/4567472/,IEEE Sensors Journal,July 2008,ieeexplore
10.1109/ACCESS.2021.3051583,Data-Driven Condition Monitoring of Mining Mobile Machinery in Non-Stationary Operations Using Wireless Accelerometer Sensor Modules,IEEE,Journals,"This paper presents the development of an easy-to-deploy and smart monitoring IoT system that utilizes vibration measurement devices to assess real-time condition of bulldozers, power shovels and backhoes, in non-stationary operations in the mining industry. According to operating experience data and the type of mining machine, total loss failure rates per machine fleet can reach up to 30%. Vibration analysis techniques are commonly used for condition monitoring and early detection of unforeseen failures to generate predictive maintenance plans for heavy machinery. However, this maintenance strategy is intensively used only for stationary machines and/or mobile machinery in stationary operations. Today, there is a lack of proper solutions to detect and prevent critical failures for non-stationary machinery. This paper shows a cost-effective solution proposal for implementing a vibration sensor network with wireless communication and machine learning data-driven capabilities for condition monitoring of non-stationary heavy machinery in mining operations. During the machine operation, 3-axis accelerations were measured using two sensors deployed across the machine. The machine accelerations (amplitudes and frequencies) are measured in two different frequency spectrums to improve each sensing location's time resolution. Multiple machine learning algorithms use this machine data to assess conditions according to manufacturer recommendations and operational benchmarks Proposed data-driven machine learning models classify the machine condition in states according to the ISO 2372 standards for vibration severity: Good, Acceptable, Unsatisfactory, or Unacceptable. After performing field tests with bulldozers and backhoes from different manufacturers, the machine learning algorithms are able to classify machine health status with an accuracy between 85% - 95%. Moreover, the system allows early detection of “Unacceptable” states between 120 to 170 hours prior to critical failure. These results demonstrate that the proposed system will collect relevant data to generate predictive maintenance plans and avoid unplanned downtimes.",https://ieeexplore.ieee.org/document/9324826/,IEEE Access,2021,ieeexplore
10.1109/ACCESS.2020.2998940,Data-Driven Estimation of Heavy-Truck Residual Value at the Buy-Back,IEEE,Journals,"In a context of deep transformation of the entire automotive industry, starting from pervasive and native connectivity, commercial vehicles (heavy, light, and buses) are generating and transmitting much more data than passenger cars, with a much higher expected value, motivated by the higher costs of the vehicles and their added-value related businesses, such as logistics, freight, and transportation management. This paper presents a data-driven and unsupervised methodology to provide a descriptive model assessing the residual value estimates of heavy trucks subject to buy-back. A huge amount of telematics data characterizing the actual usage of commercial vehicles is jointly analyzed with different external conditions (e.g., altimetry), affecting the truck's performance to estimate the devaluation of the vehicle at the buy-back. The proposed approach has been evaluated on a large set of real-world heavy trucks to demonstrate its effectiveness in correctly assessing the real status of wear and residual value at the end of leasing contracts, to provide a few and quantitative insights through an informative, interactive and user-friendly dashboard to make a proper decision on the next business strategies to be adopted. The proposed solution has already been deployed by a private company within its data analytics services since (1) an interpretable descriptive model of the main factors/parameters and corresponding weights affecting the residual value is provided and (2) the experimental results confirmed the promising outcomes of the proposed data-driven methodology.",https://ieeexplore.ieee.org/document/9104708/,IEEE Access,2020,ieeexplore
10.1109/ACCESS.2021.3101284,"Data-Driven Remaining Useful Life Estimation for Milling Process: Sensors, Algorithms, Datasets, and Future Directions",IEEE,Journals,"An increase in unplanned downtime of machines disrupts and degrades the industrial business, which results in substantial credibility damage and monetary loss. The cutting tool is a critical asset of the milling machine; the failure of the cutting tool causes a loss in industrial productivity due to unplanned downtime. In such cases, a proper predictive maintenance strategy by real-time health monitoring of cutting tools becomes essential. Accurately predicting the useful life of equipment plays a vital role in the predictive maintenance arena of industry 4.0. Many active research efforts have been done to estimate tool life in varied directions. However, the consolidated study of the implemented techniques and future pathways is still missing. So, the purpose of this paper is to provide a systematic and comprehensive literature survey on the data-driven approach of Remaining Useful Life (RUL) estimation of cutting tools during the milling process. The authors have summarized different monitoring techniques, feature extraction methods, decision-making models, and available sensors currently used in the data-driven model. The authors have also presented publicly available datasets related to milling under various operating conditions to compare the accuracy of the prediction model for tool wear estimation. Finally, the article concluded with the challenges, limitations, recent advancements in RUL prognostics techniques using Artificial Intelligence (AI), and future research scope to explore more in this area.",https://ieeexplore.ieee.org/document/9502093/,IEEE Access,2021,ieeexplore
10.1109/TII.2018.2807797,Deep Endoscope: Intelligent Duct Inspection for the Avionic Industry,IEEE,Journals,"We present the first autonomous endoscope for the visual inspection of very small ducts and cavities, up to a 6-mm diameter. The system has been designed, implemented, and tested in a challenging industrial scenario and in strict collaboration with an avionic industry partner. The inspected objects are metallic gearboxes eventually presenting different residuals (e.g., sand, machining swarfs, and metallic dust) inside the oil ducts. The automatic system is actuated by a robotic arm that moves the endoscope with a microcamera inside the gearbox duct, while a deep-learning-based spatio-temporal image analysis module detects, classifies, and localizes defects in real time. Feedback is given to the robotic arm in order to move or extract the endoscope given the detected anomalies. Evaluation provides a detection rate of nearly 98% given different tests with different types of residuals and duct structures.",https://ieeexplore.ieee.org/document/8295126/,IEEE Transactions on Industrial Informatics,April 2018,ieeexplore
10.1109/ACCESS.2019.2924030,Demand Response Management for Industrial Facilities: A Deep Reinforcement Learning Approach,IEEE,Journals,"As a major consumer of energy, the industrial sector must assume the responsibility for improving energy efficiency and reducing carbon emissions. However, most existing studies on industrial energy management are suffering from modeling complex industrial processes. To address this issue, a model-free demand response (DR) scheme for industrial facilities was developed. In practical terms, we first formulated the Markov decision process (MDP) for industrial DR, which presents the composition of the state, action, and reward function in detail. Then, we designed an actor-critic-based deep reinforcement learning algorithm to determine the optimal energy management policy, where both the actor (Policy) and the critic (Value function) are implemented by the deep neural network. We then confirmed the validity of our scheme by applying it to a real-world industry. Our algorithm identified an optimal energy consumption schedule, reducing energy costs without compromising production.",https://ieeexplore.ieee.org/document/8742652/,IEEE Access,2019,ieeexplore
10.1109/ACCESS.2020.2980736,Design and Evaluation of a Reliable Low-Cost Atmospheric Pollution Station in Urban Environment,IEEE,Journals,"The pollution of the air constitutes an environmental risk to health, crops, animals, forests and water. There are several policies for reducing air pollution regarding industry, energy, transportation, and agriculture. Unfortunately, there is limited monitoring of the air quality in cities and rural areas for supervising the accomplishment of these policies. Reliable monitoring of air pollutants is, typically, based on expensive fixed stations, which constitutes a barrier to tackle. This research presents the design, implementation and evaluation of a small, low-cost, station for monitoring atmospheric pollution. The prototype registers ozone (O<sub>3</sub>) and carbon monoxide (CO) using inexpensive sensors. To assure high reliability of the measurements obtained by the sensors installed in this station, it is proposed a calibration procedure based on the selection of the best performance analysis of the following machine learning techniques: multiple linear regression, artificial neural networks, and random forest. Additionally, a decision rule is implemented to select an optimal combination of sensors for the estimation models, while the sample timestamp is considered as a temporal heuristic at the input of the system, assuming similarities in the daily environmental dynamics. In order to test the station in a realistic scenario, the calibration and evaluation sets were taken in two different time frames of one and two months, respectively. The overall process was implemented with reference data coming from a certified air quality fixed station in the city of Cuenca - Ecuador. Experimental results showed that the real-time reports of ozone provided by the prototype are quite similar to the fixed station during the evaluation period, with a resulting correlation of up to r = 0.92 and r = 0.91 in the calibration and evaluation set, respectively. However, signal drift and aging in CO<sub>x</sub> sensors diminished the accuracy of carbon monoxide calibration models, resulting in lower correlation (r ≤ 0.76) with the evaluation set.",https://ieeexplore.ieee.org/document/9035500/,IEEE Access,2020,ieeexplore
10.1109/ACCESS.2020.2991225,Design and Performance Evaluation of an AI-Based W-Band Suspicious Object Detection System for Moving Persons in the IoT Paradigm,IEEE,Journals,"The threat of terrorism has spread all over the world, and the situation has become grave. Suspicious object detection in the Internet of Things (IoT) is an effective way to respond to global terrorist attacks. The traditional solution requires performing security checks one by one at the entrance of each gate, resulting in bottlenecks and crowding. In the IoT paradigm, it is necessary to be able to perform suspicious object detection on moving people. Artificial intelligence (AI) and millimeter-wave imaging are advanced technologies in the global security field. However, suspicious object detection for moving persons in the IoT, which requires the integration of many different imaging technologies, is still a challenge in both academia and industry. Furthermore, increasing the recognition rate of suspicious objects and controlling network congestion are two main issues for such a suspicious object detection system. In this paper, an AI-based W-band suspicious object detection system for moving persons in the IoT paradigm is designed and implemented. In this system, we establish a suspicious object database to support AI technology for improving the probability of identifying suspicious objects. Moreover, we propose an efficient transmission mechanism to reduce system network congestion since a massive amount of data will be generated by 4K cameras during real-time monitoring. The evaluation results indicate that the advantages and efficiency of the proposed scheme are significant.",https://ieeexplore.ieee.org/document/9081933/,IEEE Access,2020,ieeexplore
10.1109/ACCESS.2020.2998723,"Digital Twin for the Oil and Gas Industry: Overview, Research Trends, Opportunities, and Challenges",IEEE,Journals,"With the emergence of industry 4.0, the oil and gas (O&amp;G) industry is now considering a range of digital technologies to enhance productivity, efficiency, and safety of their operations while minimizing capital and operating costs, health and environment risks, and variability in the O&amp;G project life cycles. The deployment of emerging technologies allows O&amp;G companies to construct digital twins (DT) of their assets. Considering DT adoption, the O&amp;G industry is still at an early stage with implementations limited to isolated and selective applications instead of industry-wide implementation, limiting the benefits from DT implementation. To gain the full potential of DT and related technological adoption, a comprehensive understanding of DT technology, the current status of O&amp;G-related DT research activities, and the opportunities and challenges associated with the deployment of DT in the O&amp;G industry are of paramount importance. In order to develop this understanding, this paper presents a literature review of DT within the context of the O&amp;G industry. The paper follows a systematic approach to select articles for the literature review. First, a keywords-based publication search was performed on the scientific databases such as Elsevier, IEEE Xplore, OnePetro, Scopus, and Springer. The filtered articles were then analyzed using online text analytic software (Voyant Tools) followed by a manual review of the abstract, introduction and conclusion sections to select the most relevant articles for our study. These articles and the industrial publications cited by them were thoroughly reviewed to present a comprehensive overview of DT technology and to identify current research status, opportunities and challenges of DT deployment in the O&amp;G industry. From this literature review, it was found that asset integrity monitoring, project planning, and life cycle management are the key application areas of digital twin in the O&amp;G industry while cyber security, lack of standardization, and uncertainty in scope and focus are the key challenges of DT deployment in the O&amp;G industry. When considering the geographical distribution for the DT related research in the O&amp;G industry, the United States (US) is the leading country, followed by Norway, United Kingdom (UK), Canada, China, Italy, Netherland, Brazil, Germany, and Saudi Arabia. The overall publication rate was less than ten articles (approximately) per year until 2017, and a significant increase occurred in 2018 and 2019. The number of journal publications was noticeably lower than the number of conference publications, and the majority of the publications presented theoretical concepts rather than the industrial implementations. Both these observations suggest that the DT implementation in the O&amp;G industry is still at an early stage.",https://ieeexplore.ieee.org/document/9104682/,IEEE Access,2020,ieeexplore
10.1109/ACCESS.2021.3120843,Digital Twins From Smart Manufacturing to Smart Cities: A Survey,IEEE,Journals,"Digital twins are quickly becoming a popular tool in several domains, taking advantage of recent advancements in the Internet of Things, Machine Learning and Big Data, while being used by both the industry sector and the research community. In this paper, we review the current research landscape as regards digital twins in the field of smart cities, while also attempting to draw parallels with the application of digital twins in Industry 4.0. Although digital twins have received considerable attention in the Industrial Internet of Things domain, their utilization in smart cities has not been as popular thus far. We discuss here the open challenges in the field and argue that digital twins in smart cities should be treated differently and be considered as cyber-physical “systems of systems”, due to the vastly different system size, complexity and requirements, when compared to other recent applications of digital twins. We also argue that researchers should utilize established tools and methods of the smart city community, such as co-creation, to better handle the specificities of this domain in practice.",https://ieeexplore.ieee.org/document/9576739/,IEEE Access,2021,ieeexplore
10.1109/ACCESS.2020.3036769,Drill Fault Diagnosis Based on the Scalogram and Mel Spectrogram of Sound Signals Using Artificial Intelligence,IEEE,Journals,"In industry, the ability to detect damage or abnormal functioning in machinery is very important. However, manual detection of machine fault sound is economically inefficient and labor-intensive. Hence, automatic machine fault detection (MFD) plays an important role in reducing operating and personnel costs compared to manual machine fault detection. This research aims to develop a drill fault detection system using state-of-the-art artificial intelligence techniques. Many researchers have applied the traditional approach design for an MFD system, including handcrafted feature extraction of the raw sound signal, feature selection, and conventional classification. However, drill sound fault detection based on conventional machine learning methods using the raw sound signal in the time domain faces a number of challenges. For example, it can be difficult to extract and select good features to input in a classifier, and the accuracy of fault detection may not be sufficient to meet industrial requirements. Hence, we propose a method that uses deep learning architecture to extract rich features from the image representation of sound signals combined with machine learning classifiers to classify drill fault sounds of drilling machines. The proposed methods are trained and evaluated using the real sound dataset provided by the factory. The experiment results show a good classification accuracy of 80.25 percent when using Mel spectrogram and scalogram images. The results promise significant potential for using in the fault diagnosis support system based on the sounds of drilling machines.",https://ieeexplore.ieee.org/document/9252126/,IEEE Access,2020,ieeexplore
10.1109/TSE.2013.2295827,Effects of Developer Experience on Learning and Applying Unit Test-Driven Development,IEEE,Journals,"Unit test-driven development (UTDD) is a software development practice where unit test cases are specified iteratively and incrementally before production code. In the last years, researchers have conducted several studies within academia and industry on the effectiveness of this software development practice. They have investigated its utility as compared to other development techniques, focusing mainly on code quality and productivity. This quasi-experiment analyzes the influence of the developers' experience level on the ability to learn and apply UTDD. The ability to apply UTDD is measured in terms of process conformance and development time. From the research point of view, our goal is to evaluate how difficult is learning UTDD by professionals without any prior experience in this technique. From the industrial point of view, the goal is to evaluate the possibility of using this software development practice as an effective solution to take into account in real projects. Our results suggest that skilled developers are able to quickly learn the UTDD concepts and, after practicing them for a short while, become as effective in performing small programming tasks as compared to more traditional test-last development techniques. Junior programmers differ only in their ability to discover the best design, and this translates into a performance penalty since they need to revise their design choices more frequently than senior programmers.",https://ieeexplore.ieee.org/document/6690135/,IEEE Transactions on Software Engineering,April 2014,ieeexplore
10.1109/ACCESS.2020.2998581,Efficiency Improvement of Function Point-Based Software Size Estimation With Deep Learning Model,IEEE,Journals,"Software cost estimation is crucial to software management, which has received considerable attention from both industry and academia. Software size is an important metric that forms the cornerstone of software cost estimation. The function point has been proven to be a useful software size unit for size estimation and has been successfully implemented in many countries. However, in current practice, the rule of function point size method is complicated and performed manually. Consequently, it is costly in both time and resources spent to apply these methods, especially in the scenario of large-scale software development in the industry. In this paper, a deep learning-based named entity recognition (NER) model was designed in place of manual function point recognition. In particular, a BiLSTM-CRF model was trained on previously labeled requirements in the industry to classify the function point type of new requirements in the same domain. The proposed method was verified on 29 real projects provided by our industry partner. A comparative experiment was designed for the quantitative evaluation of efficiency improvement of the proposed NER model aided function point estimation. The result suggests that, for the NER model, the precision and F1 of the BiLSTM-CRF-based function point analysis on test samples achieved 94.5% and 80.3%, respectively. Moreover, the improvement in the efficiency of the software size estimation process achieved an average of 38.6%, which is a significant enhancement for the function point-based software size estimation.",https://ieeexplore.ieee.org/document/9103508/,IEEE Access,2021,ieeexplore
10.1109/ACCESS.2020.3033725,Efficient Deep Learning Bot Detection in Games Using Time Windows and Long Short-Term Memory (LSTM),IEEE,Journals,"Bots in video games has been gaining the interest of industry as well as academia as a problem that has been enabled by the recent advances in deep learning and reinforcement learning. In turn several studies have attempted to establish bot detectors in various video games. In this article, we introduce a bot detection model that can implemented in real-time and provide feedback on whether a player that is being observed is a bot or human. The model uses a limited feature set and amount of time of observation in order to be small and generalize easily to other domains. We trained and tested our model in a series of replays for Starcraft: Brood War and have yielded a higher accuracy than past studies and a fraction of detection time.",https://ieeexplore.ieee.org/document/9239256/,IEEE Access,2020,ieeexplore
10.1109/ACCESS.2021.3101397,Energy-Efficient Edge-Fog-Cloud Architecture for IoT-Based Smart Agriculture Environment,IEEE,Journals,"The current agriculture systems compete to take advantage of industry advanced technologies, including the internet of things (IoT), cloud/fog/edge computing, artificial intelligence, and agricultural robots to monitor, track, analyze and process various functions and services in real-time. Additionally, these technologies can make the agricultural processes smarter and more cost-efficient by using automated systems and eliminating any human interventions, hence enhancing agricultural production to meet future expectations. Although the current agriculture systems that adopt the traditional cloud-based architecture have provided powerful computing infrastructure to distributed IoT sensors. However, the cost of energy consumption associated with transferring heterogeneous data over the multiple network tiers to process, analyze and store the sensor's information in the cloud has created a huge load on information and communication infrastructure. Besides, the energy consumed by cloud data centers has an environmental impact associated with using non-clean fuels, which usually release carbon emissions (CO<sub>2</sub>) to produce electricity. Thus, to tackle these issues, we propose a new integrated edge-fog-cloud architectural paradigm that promises to enhance the energy-efficient of smart agriculture systems and corresponding carbon emissions. This architecture allows data collection from several sensors to process and analyze the agriculture data that require real-time operation (e.g., weather temperature, soil moisture, soil acidity, irrigation, etc.) in several layers (edge, fog, and cloud). Thus, the real-time processing could be held by the edge and fog layers to reduce the load on the cloud layer, which will help to enhance the overall energy consumption and process the agriculture applications/services efficiently. Mathematical modeling is conducted using mixed-integer linear programming (MILP) for a smart agriculture environment, where the proposed architecture is implemented, and results are analyzed and compared to the traditional implementation. According to the results of thousands of agriculture sensors, the proposed architecture outperforms the traditional cloud-based architecture in terms of reducing the overall energy consumption by 36% and the carbon emissions by 43%. In addition to these achievements, the results show that our proposed architecture can reduce network traffic by up to 86%, which can reduce network congestion. Finally, we develop a heuristic algorithm to validate and mimic the presented approach, and it shows comparable results to the MILP model.",https://ieeexplore.ieee.org/document/9502114/,IEEE Access,2021,ieeexplore
10.1109/ACCESS.2020.3047343,Establishing Trust in Online Advertising With Signed Transactions,IEEE,Journals,"Programmatic advertising operates one of the most sophisticated and efficient service platforms on the Internet. However, the complexity of this ecosystem is a direct cause of one of the most important problems in online advertising, the lack of transparency. This lack of transparency enables subsequent problems such as advertising fraud, which causes billions of dollars in losses. In this paper we propose Ads.chain, a technological solution to the lack-of-transparency problem in programmatic advertising. Ads.chain extends the current effort of the Internet Advertising Bureau (IAB) in providing traceability in online advertising through the Ads.txt and Ads.cert solutions, addressing the limitations of these techniques. Ads.chain is (to the best of the authors' knowledge) the first solution that provides end-to-end cryptographic traceability at the ad transaction level. It is a communication protocol that can be seamlessly embedded into ad-tags and the OpenRTB protocol, the de-facto standards for communications in online advertising, allowing an incremental adoption by the industry. We have implemented Ads.chain and made the code publicly available. We assess the performance of Ads.chain through a thorough analysis in a lab environment that emulates a real ad delivery process at real-life throughputs. The obtained results show that Ads.chain can be implemented with limited impact on the hardware resources and marginal delay increments at the publishers lower than 0.20 milliseconds per ad space on webpages and 2.6 milliseconds at the programmatic advertising platforms. These results confirm that Ads.chain's impact on the user experience and the overall operation of the programmatic ad delivery process can be considered negligible.",https://ieeexplore.ieee.org/document/9306812/,IEEE Access,2021,ieeexplore
10.1109/TSP.2014.2339799,Evolutionary Dynamics of Information Diffusion Over Social Networks,IEEE,Journals,"Current social networks are of extremely large-scale generating tremendous information flows at every moment. How information diffuses over social networks has attracted much attention from both industry and academics. Most of the existing works on information diffusion analysis are based on machine learning methods focusing on social network structure analysis and empirical data mining. However, the network users' decisions, actions, and socio-economic interactions are generally ignored by most of existing works. In this paper, we propose an evolutionary game theoretic framework to model the dynamic information diffusion process in social networks. Specifically, we derive the information diffusion dynamics in complete networks, uniform degree, and nonuniform degree networks, with the highlight of two special networks, the Erdös-Rényi random network and the Barabási-Albert scale-free network. We find that the dynamics of information diffusion over these three kinds of networks are scale-free and all the three dynamics are same with each other when the network scale is sufficiently large. To verify our theoretical analysis, we perform simulations for the information diffusion over synthetic networks and real-world Facebook networks. Moreover, we also conduct an experiment on a Twitter hashtags dataset, which shows that the proposed game theoretic model can well fit and predict the information diffusion over real social networks.",https://ieeexplore.ieee.org/document/6856208/,IEEE Transactions on Signal Processing,"Sept.1, 2014",ieeexplore
10.1109/TII.2019.2937876,Federated Tensor Mining for Secure Industrial Internet of Things,IEEE,Journals,"In a vertical industry alliance, Internet of Things (IoT) deployed in different smart factories are similar. For example, most automobile manufacturers have the similar assembly lines and IoT surveillance systems. It is common to observe the industrial knowledge using deep learning and data mining methods based on the IoT data. However, some knowledge is not easy to be mined from only one factory's data because the samples are still few. If multiple factories within an alliance can gather their data together, more knowledge could be mined. However, the key concern of these factories is the data security. Existing matrix-based methods can guarantee the data security inside a factory but do not allow the data sharing among factories, and thus their mining performance is poor due to lack of correlation. To address this concern, in this article we propose the novel federated tensor mining (FTM) framework to federate multisource data together for tensor-based mining while guaranteeing the security. The key contribution of FTM is that every factory only needs to share its ciphertext data for security issue, and these ciphertexts are adequate for tensor-based knowledge mining due to its homomorphic attribution. Real-data-driven simulations demonstrate that FTM not only mines the same knowledge compared with the plaintext mining, but also is enabled to defend the attacks from distributed eavesdroppers and centralized hackers. In our typical experiment, compared with the matrix-based privacy-preserving compressive sensing (PPCS), FTM increases up to 24% on mining accuracy.",https://ieeexplore.ieee.org/document/8815886/,IEEE Transactions on Industrial Informatics,March 2020,ieeexplore
10.1109/ACCESS.2021.3080237,"Fog-Centric IoT Based Framework for Healthcare Monitoring, Management and Early Warning System",IEEE,Journals,"Internet of things (IoT) and machine learning based systems incorporating smart wearable technology are rapidly evolving to monitor and manage healthcare and physical activities. This paper is focused on the proposition of a fog-centric wireless, real-time, smart wearable and IoT-based framework for ubiquitous health and fitness analysis in a smart gym environment. The proposed framework aims to aid in the health and fitness industry based on body vitals, body movement and health related data. The framework is expected to assist athletes, trainers and physicians with the interpretation of multiple physical signs and raise alerts in case of any health hazard. We proposed a method to collect and analyze exercise specific data which can be used to measure exercise intensity and its benefit to athlete's health and serve as recommendation system for upcoming athletes. We determined the validity of the proposed framework by giving a six weeks workout plan with six days a week for workout activity targeting all muscles followed by one day for recovery. We recorded the electrocardiogram, heart rate, heart rate variability, breath rate, and determined athlete's movement using a 3D-acceleration. The collected data in the research is used in two modules. A Health zone module implemented on body vitals data which categorizes athlete's health state into various categories. Hzone module is responsible for health hazards identification and alarming. Outstandingly, the Hzone module is able to identify an athlete's physical state with 97% accuracy. A gym activity recognition (GAR) module is implemented to recognize workout activity in real-time using body movements and body vitals data. The purpose of the GAR module is to collect and analyze exercise specific data. The GAR module achieved an accuracy of above 89% on athlete independent model based on muscle group.",https://ieeexplore.ieee.org/document/9430526/,IEEE Access,2021,ieeexplore
10.1109/ACCESS.2021.3101647,Gas Path Fault Diagnosis of Gas Turbine Engine Based on Knowledge Data-Driven Artificial Intelligence Algorithm,IEEE,Journals,"As the core power for the aviation industry, shipbuilding industry, and power station industry, it is essential to ensure that the gas turbines operate safely, reliably, greenly and efficiently. Learn from the advantages and disadvantages of the thermodynamic model based and data-driven artificial intelligence based gas-path diagnosis methods, a newfangled gas turbine gas-path diagnosis approach on the basis of knowledge data-driven artificial intelligence is proposed. That is a hybrid method of deep learning and gas path analysis. First, gas turbine thermodynamic model of the object to be diagnosed is constructed by adaptation modeling strategy. And the engine thermodynamic model is taken as the basal model to simulate various gas path faults. Secondly, a large number of knowledge data corresponding to component health parameters and gas turbine boundary condition parameters &amp; gas-path measurable parameters are simulated by setting different component health parameter values and different boundary conditions based on this basal model. And next, define the vector composed of the boundary condition parameters &amp; the gas path measurable parameters in the knowledge database as the input vector, and the component health parameter vector as the output vector, and a deep learning model for regression modeling of this knowledge database is designed. At last, along with the gas turbine engine runs, the trained model outputs component health parameters in real time after trained deep learning model is deployed to the corresponding gas turbine power plant. The simulation experiment results show that, accurate and quantified health parameters of each gas path component can be obtained by the proposed method in this paper, and the overall root mean square error does not exceed 0.033%, and the maximum relative error does not exceed 0.36%, which illustrates the proposed method has great application potential.",https://ieeexplore.ieee.org/document/9502707/,IEEE Access,2021,ieeexplore
10.1109/ACCESS.2021.3103680,Human Centric Digital Transformation and Operator 4.0 for the Oil and Gas Industry,IEEE,Journals,"Working at an oil and gas facility, such as a drilling rig, production facility, processing facility, or storage facility, involves various challenges, including health and safety risks. It is possible to leverage emerging digital technologies such as smart sensors, wearable or mobile devices, big data analytics, cloud computing, extended reality technologies, robotic systems, and drones to mitigate the challenges faced by oil and gas workers. While these technologies are not new to the oil and gas industry, most of its existing digital transformation initiatives follow business or process-centric approaches, in which the critical driver of the technology adoption is the enhancement of production, efficiency, and revenue. As a result, they may not address the challenges faced by the workers. As oil and gas workers are among the essential assets in the oil and gas industry, it is vital to address the challenges faced by these workers. This paper proposes a human-centric digital transformational framework for the oil and gas industry to deploy existing digital technologies to enhance their workers' health, safety, and working conditions. The paper outlines the critical challenges faced by oilfield workers, introduces a system architecture to implements a human-centric digital transformation, discusses the opportunities of the proposed framework, and summarizes the key impediment for the proposed framework.",https://ieeexplore.ieee.org/document/9509417/,IEEE Access,2021,ieeexplore
10.1109/TVT.2021.3099129,Hybrid Policy Learning for Energy-Latency Tradeoff in MEC-Assisted VR Video Service,IEEE,Journals,"Virtual reality (VR) is promising to fundamentally transform a broad spectrum of industry sectors and the way humans interact with virtual content. However, despite unprecedented progress, current networking and computing infrastructures are incompetent to unlock VR's full potential. In this paper, we consider delivering the wireless multi-tile VR video service over a mobile edge computing (MEC) network. The primary goal is to minimize the system latency/energy consumption and to arrive at a tradeoff thereof. To this end, we first cast the time-varying view popularity as a model-free Markov chain to effectively capture its dynamic characteristics. After jointly assessing the caching and computing capacities on both the MEC server and the VR playback device, a hybrid policy is then implemented to coordinate the dynamic caching replacement and the deterministic offloading, so as to fully utilize the system resources. The underlying multi-objective problem is reformulated as a partially observable Markov decision process, and a deep deterministic policy gradient algorithm is proposed to iteratively learn its solution, where a long short-term memory neural network is embedded to continuously predict the dynamics of the unobservable popularity. Simulation results demonstrate the superiority of the proposed scheme in achieving a trade-off between the energy efficiency and the latency reduction over the baseline methods.",https://ieeexplore.ieee.org/document/9495190/,IEEE Transactions on Vehicular Technology,Sept. 2021,ieeexplore
10.1109/ACCESS.2017.2783118,IEEE Access Special Section Editorial: Health Informatics for the Developing World,IEEE,Journals,"We live in a world with growing disparity in the quality of life available to people in the developed and developing countries. Healthcare in the developing world is fraught with numerous problems such as the lack of health infrastructure, and human resources, which results in very limited health coverage. The field of health informatics has made great strides in recent years towards improving public health systems in the developing world by augmenting them with state-of-the-art information and communication technologies (ICT). Through real-world deployment of these technologies, there is real hope that the health industry in the developing world will progress from its current, largely dysfunctional state to one that is more effective, personalized, and cost effective. Health informatics can usher a new era of personalized health analytics, with the potential to transform healthcare in the developing world. In conjunction with mHealth and eHealth, many other important health informatics trends—such as artificial intelligence (AI), machine learning (ML), big data, crowdsourcing, cloud computing—are also emerging. Exponentially growing heterogeneous data, with the help of big data analytics, has the potential to provide descriptive, predictive, and prescriptive health insights as well as enable new applications such as telemedicine and remote diagnostics and surgery. Such systems could enhance the overall process of monitoring, diagnosis, and prognosis of diseases.",https://ieeexplore.ieee.org/document/8262687/,IEEE Access,2017,ieeexplore
10.1147/JRD.2016.2630478,IT troubleshooting with drift analysis in the DevOps era,IBM,Journals,"Over the past few years, DevOps practices have led to many changes in the software industry. The need for agility has resulted in continuous development and deployment of frequent small updates in IT production systems. However, the ever-changing applications and their IT operations environments challenge existing IT troubleshooting approaches, which generally depend on prebuilt domain knowledge and ignore the frequent changes in the DevOps era. Moreover, the complexity and diversity of application architectures exacerbate the challenges. In this paper, we propose an unsupervised learning based drift analysis tool named CHASER to detect and analyze abnormal changes (referred to as “drifts,” which include configuration errors, processes hanging, etc.), with learned change models and patterns in real time as well as in the root cause analysis. First, we categorize the changes into two distinct groups (static and dynamic state changes) and periodically collect the finer grained changes. Then, we extract the time-series and structural features from these changes and apply statistical and machine learning algorithms to learn models and patterns from historical data. Furthermore, we apply these models and patterns to detect drifts in real time and infer possible root causes of reported errors based on a multidimensional correlation approach to improve the precision. Through experiments and case studies, we demonstrate the capability of CHASER.",https://ieeexplore.ieee.org/document/7877287/,IBM Journal of Research and Development,1 Jan.-Feb. 2017,ieeexplore
10.1109/ACCESS.2018.2809681,Industrial Big Data Analytics for Prediction of Remaining Useful Life Based on Deep Learning,IEEE,Journals,"Due to the recent development of cyber-physical systems, big data, cloud computing, and industrial wireless networks, a new era of industrial big data is introduced. Deep learning, which brought a revolutionary change in computer vision, natural language processing, and a variety of other applications, has significant potential for solutions providing in sophisticated industrial applications. In this paper, a concept of device electrocardiogram (DECG) is presented, and an algorithm based on deep denoising autoencoder (DDA) and regression operation is proposed for the prediction of the remaining useful life of industrial equipment. First, the concept of electrocardiogram is explained. Then, a problem statement based on manufacturing scenario is presented. Subsequently, the architecture of the proposed algorithm called integrated DDA and the algorithm workflow are provided. Moreover, DECG is compared with traditional factory information system, and the feasibility and effectiveness of the proposed algorithm are validated experimentally. The proposed concept and algorithm combine typical industrial scenario and advance artificial intelligence, which has great potential to accelerate the implementation of industry 4.0.",https://ieeexplore.ieee.org/document/8302913/,IEEE Access,2018,ieeexplore
10.1109/TII.2019.2934901,Intelligent Fault Diagnosis Method Based on Full 1-D Convolutional Generative Adversarial Network,IEEE,Journals,"Data-driven fault diagnosis is essential for the reliability and safety of industry equipment. However, the lack of real labeled fault data make the machine learning-based diagnosis methods difficult to carry out. To solve this problem, this article proposes a new fault diagnosis framework called multilabel one-dimensional (1-D) generation adversarial network (ML1-D-GAN). In our method, Auxiliary Classifier GAN is utilized first for real damage data generation. Then the generated and real damage data are both used to train the fault classifier. Experimental results reveal that the generated data is applicable, and ML1-D-GAN improves the diagnosing accuracy for real bearing faults from 95% to 98% when trained with the generated data. The scalability of the learning model is also proven in the experiment.",https://ieeexplore.ieee.org/document/8794731/,IEEE Transactions on Industrial Informatics,March 2020,ieeexplore
10.1109/ACCESS.2018.2877175,"Internet of Too Many Things in Smart Transport: The Problem, The Side Effects and The Solution",IEEE,Journals,"The Internet of Things (IoT) involves embedding electronics, software, sensors, and actuators into physical devices, such as vehicles, buildings, and a wide range of smart devices. Network connectivity allows IoT devices to collect and exchange data. The prevalence of IoT devices has increased rapidly in last five years, driven by cheaper electronics and a desire to monitor and control the physical world. We introduce the concept of IoT flood to describe the increased use of IoT devices. Just like the data deluge, the IoT flood has potential benefits and risks. This paper focuses on the hidden side effects of the increased usage of IoT, such as energy consumption, physical pollution, radiation, and health issues. We indicate that an evaluation system with carefully designed metrics reflecting the impact of IoT flood with input from academic, industry, and government is required. We propose some practical measures that can reduce the IoT flood, such as common platforms and data sharing to reduce the side effects. This paper demonstrates the IoT flood problem and potential solutions by examining the intelligent transport system domain where IoT is being deployed to solve problems related to time efficiency and energy consumption through smart mobility.",https://ieeexplore.ieee.org/document/8506609/,IEEE Access,2018,ieeexplore
10.1109/ACCESS.2019.2912012,Knowledge Based Recommender System for Academia Using Machine Learning: A Case Study on Higher Education Landscape of Pakistan,IEEE,Journals,"Allocation of courses and research students based on faculty's subject specialization and area of interest has always remained a challenging task for university administration due to the presence of academics' cross-domain interests, stale faculty resumes at university portals and changing the skill set demands from the industry. Collaborative filtering and content-based recommender systems have already been in use by the industry for recommending things, such as movies, news, restaurants, and shopping items to the users, and however, no one has utilized these off-the-shelf models for enhancing the student experience and improving the quality of higher education in academia. This paper presents a case study showcasing the use of probabilistic topic models for generating recommendations to users in academia through appropriate course allocation and supervisor assignment. The proposed system coined as ScholarLite harnesses the power of machine learning to extract research themes from faculty members' past publications, mines research interests from their resumes, and combines it with their educational background to generate recommendations for course teaching, research supervision, and industry-academia collaboration. We have shown the recommendation results on real-world data gathered from the higher education commission of the country and demonstrated that the proposed techniques are scalable across various programs offered by the universities and could be deployed in a small budget by universities for automating course and supervisor allocation procedures. The experiments confirm our performance expectation by showing good relevance and objectivity in results, thus making this decision management system more appealing for large-scale deployment and use by academia.",https://ieeexplore.ieee.org/document/8693719/,IEEE Access,2019,ieeexplore
10.1109/ACCESS.2020.3008289,Learning-Based IoT Data Aggregation for Disaster Scenarios,IEEE,Journals,"Industrial Internet of Everything (IIoE), as the deep integration of industry 6.0, the Internet of Things (IoT) and 6G mobile communication technology, pave the way for intelligent industry, enabling industrial optimization and automation. To ensure the high quality of services (QoS) in IIoE, tremendous real-time information generated by the pervasive smart things needs to be aggregated and processed quickly and reliably. However, a large-scale disaster could damage the entire communication network and cut off data aggregation such that Qos is compromised. In this paper, an Intelligent NIB based Data Aggregation Strategy, named (IDAS), is proposed for after disaster scenarios in IIoE. Specifically, IDAS first applies both iterative cubature kalman filter and radial basis function neural network to predict the data collection rates of survived infrastructures. Then, an energy efficient task distribution mechanism is design. Next, a deep reinforcement learning method is developed for the car-carrying NIB route design to perform corresponding task. Eventually, all data are aggregated toward the rescue headquarter by NIB deployment based on Fermat tree constructions. The theoretical analysis and simulations indicate that IDAS is not only energy efficient for after disaster scenarios but requires the least NIB consumption while compared with contemporary strategies.",https://ieeexplore.ieee.org/document/9137637/,IEEE Access,2020,ieeexplore
10.1109/JSEN.2021.3087537,Long-Distance Pipeline Safety Early Warning: A Distributed Optical Fiber Sensing Semi-Supervised Learning Method,IEEE,Journals,"Pipeline safety early warning (PSEW) systems based on distributed optical fiber sensors are used to recognize and locate third-party events that may damage long-distance energy transportation pipelines and are essential to ensure pipeline safety and energy supply. However, the deployment of PSEW systems in real sites is hindered by the high experimental cost of collecting large real-site data sets for model building and the small percentage of labeled data (typically less than 0.5%). Besides, the optical fiber sensors are sensitive to hardware and the environment, ensuring challenges to directly migrate the old PSEW system for a new deployment. In this study, a novel semi-supervised learning model is proposed to monitor the safety of pipelines in real-time. Concretely, the sparse stacked autoencoder trained with unlabeled data is used to extract more robust features, and the fully-connected network trained with a small amount of labeled data is used for location and identification. Encouraging empirical results on the real-world long-distance energy pipelines of the PipeChina confirm that our method achieves better recognition and localization performance in comparison to the baseline with less labeled data. Further, the model size and recognition latency are reduced by 18.9× and 7.9× of the baseline, respectively. Also, the decoded features have better visualization than the input. This work reduces the cost of PSEW system deployments, improves its performance and portability, and will contribute to the widespread use of PSEW systems in the industry.",https://ieeexplore.ieee.org/document/9448237/,IEEE Sensors Journal,"1 Sept.1, 2021",ieeexplore
10.1109/ACCESS.2019.2942390,"Machine Learning for 5G/B5G Mobile and Wireless Communications: Potential, Limitations, and Future Directions",IEEE,Journals,"Driven by the demand to accommodate today's growing mobile traffic, 5G is designed to be a key enabler and a leading infrastructure provider in the information and communication technology industry by supporting a variety of forthcoming services with diverse requirements. Considering the ever-increasing complexity of the network, and the emergence of novel use cases such as autonomous cars, industrial automation, virtual reality, e-health, and several intelligent applications, machine learning (ML) is expected to be essential to assist in making the 5G vision conceivable. This paper focuses on the potential solutions for 5G from an ML-perspective. First, we establish the fundamental concepts of supervised, unsupervised, and reinforcement learning, taking a look at what has been done so far in the adoption of ML in the context of mobile and wireless communication, organizing the literature in terms of the types of learning. We then discuss the promising approaches for how ML can contribute to supporting each target 5G network requirement, emphasizing its specific use cases and evaluating the impact and limitations they have on the operation of the network. Lastly, this paper investigates the potential features of Beyond 5G (B5G), providing future research directions for how ML can contribute to realizing B5G. This article is intended to stimulate discussion on the role that ML can play to overcome the limitations for a wide deployment of autonomous 5G/B5G mobile and wireless communications.",https://ieeexplore.ieee.org/document/8844682/,IEEE Access,2019,ieeexplore
10.1109/ACCESS.2017.2754507,Model-Based Development of Knowledge-Driven Self-Reconfigurable Machine Control Systems,IEEE,Journals,"To accommodate the trend toward mass customization launched by intelligent manufacturing in the era of Industry 4.0, this paper proposes the combination of model-driven engineering and knowledgedriven engineering during the development process of self-reconfigurable machine control systems. The complete tool chain for model development, execution, and reconfiguration is established. For the design phase, a machine-control-domain-specific modeling language and the supporting design environment are developed. With regard to the execution stage, a runtime framework compliant with the IEC 61499 standard is proposed. On the ground of the modeling environment and the reconfigurable run-time framework, a self-adaptive control module is developed to establish the close-loop self-reconfiguration infrastructure. The ontological representation of knowledge base toward this end is described, along with extendable SQWRL rules specified to automatically initiate the reconfiguration process in the cases of external user demands and internal faults. A prototype motion control kernel in the low-level layer of machine control system architecture is developed with the proposed modeling language and is then deployed to the runtime framework. Two case studies on self-reconfiguration of the proof-of-concept motion control kernel are demonstrated, which prove the feasibility of our proposal.",https://ieeexplore.ieee.org/document/8047091/,IEEE Access,2017,ieeexplore
10.1109/TLA.2021.9477280,Multilayer Extreme Learning Machine as Equalizer in OFDM-based Radio-over-fiber Systems,IEEE,Journals,"Mobile/wireless networks aim to support diverse services with numerous and sophisticated requirements, such as energy efficiency, spectral efficiency, negligible latency, robustness against time and frequency selective channels, low hardware complexity, among others. From the central station to the base stations, radio-over-fiber orthogonal frequency division multiplexing (RoF-OFDM) schemes with direct-detection are then implemented. Unfortunately, laser phase noise, chromatic fiber dispersion, and carrier frequency offset impair the orthogonality of the subcarriers; hence, deteriorating the performance of the RoF-OFDM system. In order to take all the processing tasks to the cognitive level (the last goal in the telecommunication industry), various extreme learning machines (ELMs), composed by only a single hidden layer, have been recently adopted as equalizers. The reason behind this trend comes from the lower computational complexity, higher detection accuracy, and minimum human intervention of the ELM algorithms. In this article, we introduce a multilayer ELM-based receiver for RoF schemes transmitting phase-correlated OFDM signals affected by phase and frequency errors. Results report that by appropriately setting the hyper-parameters of the multilayer ELMs, the ELM with 3 hidden layers outperforms most of the ELMs reported in the literature (the ELM with 2 hidden layers, original ELM, regularized ELM, and 2 fully-independent ELMs defined in the real domain), as well as the benchmark pilot-assisted equalizer in terms of bit error rate. Nevertheless, this benefit comes with excessive computational cost. Finally, we show that the fully-complex ELM is still the best equalizer taking into account several key metrics.",https://ieeexplore.ieee.org/document/9477280/,IEEE Latin America Transactions,Oct. 2021,ieeexplore
10.1109/JIOT.2019.2960099,Multiuser Physical Layer Authentication in Internet of Things With Data Augmentation,IEEE,Journals,"Unlike most of the upper layer authentication mechanisms, the physical (PHY) layer authentication takes advantages of channel impulse response from wireless propagation to identify transmitted packages with low-resource consumption, and machine learning methods are effective ways to improve its implementation. However, the training of the machine-learning-based PHY-layer authentication requires a large number of training samples, which makes the training process time consuming and computationally resource intensive. In this article, we propose a data augmented multiuser PHY-layer authentication scheme to enhance the security of mobile-edge computing system, an emergent architecture in the Internet of Things (IoT). Three data augmentation algorithms are proposed to speed up the establishment of the authentication model and improve the authentication success rate. By combining the deep neural network with data augmentation methods, the performance of the proposed multiuser PHY-layer authentication scheme is improved and the training speed is accelerated, even with fewer training samples. Extensive simulations are conducted under the real industry IoT environment and the figures illustrate the effectiveness of our approach.",https://ieeexplore.ieee.org/document/8935162/,IEEE Internet of Things Journal,March 2020,ieeexplore
10.1109/TASE.2012.2198057,Neural-Network-Based Optimal Control for a Class of Unknown Discrete-Time Nonlinear Systems Using Globalized Dual Heuristic Programming,IEEE,Journals,"In this paper, a neuro-optimal control scheme for a class of unknown discrete-time nonlinear systems with discount factor in the cost function is developed. The iterative adaptive dynamic programming algorithm using globalized dual heuristic programming technique is introduced to obtain the optimal controller with convergence analysis in terms of cost function and control law. In order to carry out the iterative algorithm, a neural network is constructed first to identify the unknown controlled system. Then, based on the learned system model, two other neural networks are employed as parametric structures to facilitate the implementation of the iterative algorithm, which aims at approximating at each iteration the cost function and its derivatives and the control law, respectively. Finally, a simulation example is provided to verify the effectiveness of the proposed optimal control approach. Note to Practitioners-The increasing complexity of the real-world industry processes inevitably leads to the occurrence of nonlinearity and high dimensions, and their mathematical models are often difficult to build. How to design the optimal controller for nonlinear systems without the requirement of knowing the explicit model has become one of the main foci of control practitioners. However, this problem cannot be handled by only relying on the traditional dynamic programming technique because of the ""curse of dimensionality"". To make things worse, the backward direction of solving process of dynamic programming precludes its wide application in practice. Therefore, in this paper, the iterative adaptive dynamic programming algorithm is proposed to deal with the optimal control problem for a class of unknown nonlinear systems forward-in-time. Moreover, the detailed implementation of the iterative ADP algorithm through the globalized dual heuristic programming technique is also presented by using neural networks. Finally, the effectiveness of the control strategy is illustrated via simulation study.",https://ieeexplore.ieee.org/document/6203617/,IEEE Transactions on Automation Science and Engineering,July 2012,ieeexplore
10.1109/ACCESS.2019.2958284,On the Generation of Anomaly Detection Datasets in Industrial Control Systems,IEEE,Journals,"In recent decades, Industrial Control Systems (ICS) have been affected by heterogeneous cyberattacks that have a huge impact on the physical world and the people's safety. Nowadays, the techniques achieving the best performance in the detection of cyber anomalies are based on Machine Learning and, more recently, Deep Learning. Due to the incipient stage of cybersecurity research in ICS, the availability of datasets enabling the evaluation of anomaly detection techniques is insufficient. In this paper, we propose a methodology to generate reliable anomaly detection datasets in ICS that consists of four steps: attacks selection, attacks deployment, traffic capture and features computation. The proposed methodology has been used to generate the Electra Dataset, whose main goal is the evaluation of cybersecurity techniques in an electric traction substation used in the railway industry. Using the Electra dataset, we train several Machine Learning and Deep Learning models to detect anomalies in ICS and the performed experiments show that the models have high precision and, therefore, demonstrate the suitability of our dataset for use in production systems.",https://ieeexplore.ieee.org/document/8926471/,IEEE Access,2019,ieeexplore
10.1364/JOCN.403205,Open whitebox architecture for smart integration of optical networking and data center technology [Invited],IEEE,Journals,"In this paper, we identify challenges in developing future optical network infrastructure for new services based on technologies such as 5G, virtual reality, and artificial intelligence, and we suggest approaches to handling these challenges that include a business model, architecture, and diversity. Through activities in multiservice agreement and de facto standard organizations, we have shown how the hardware abstraction layer interfaces of optical transceivers are implemented for multivendor and heterogeneous environments, coherent digital signal processor interoperability, and optical transport whiteboxes. We have driven the effort to define the transponder abstraction interface with partners. The feasibility of such implementation was verified through demonstrations and trials. In addition, we are constructing an open-transport platform by combining existing open-source software and implementing software components that automate and enhance operations. An open architecture maintains a healthy ecosystem for industry and allows for a flexible, operator-driven network.",https://ieeexplore.ieee.org/document/9275288/,Journal of Optical Communications and Networking,January 2021,ieeexplore
10.1109/ACCESS.2020.3035880,Prioritization Based Taxonomy of DevOps Challenges Using Fuzzy AHP Analysis,IEEE,Journals,"The DevOps (development and operations) is a collaborative software development environment which offers the continues development and deployment of quality software project within short time. The DevOps practices are not yet mature enough, and the software organizations hesitate to adopt it. This study aims: 1) to explore the DevOps challenges by conducting systematic literature review (SLR) and to get the insight of industry experts via questionnaire survey study and 2) to prioritize the investigated challenges using fuzzy analytical hierarchy process (FAHP). The study findings provide the set of critical challenges faced by the software organizations while adopting DevOps and a prioritization-based taxonomy of the DevOps challenges. The application of FAHP is novel in this research area as it assists in addressing the vagueness of practitioners concerning the influencing factors of DevOps. We believe that the finding of this study will serve as a body of knowledge for real world practitioners and researchers to revise and develop the new strategies for the successful implementation of DevOps practices in the software industry.",https://ieeexplore.ieee.org/document/9247948/,IEEE Access,2020,ieeexplore
10.1109/TC.2020.3000051,Priority Assignment on Partitioned Multiprocessor Systems With Shared Resources,IEEE,Journals,"Driven by industry demand, there is an increasing need to develop real-time multiprocessor systems which contain shared resources. The Multiprocessor Stack Resource Policy (MSRP) and Multiprocessor resource sharing Protocol (MrsP) are two major protocols that manage access to shared resources. Both of them can be applied to Fixed-Priority Preemptive Scheduling (FPPS), which is enforced by most commercial real-time systems regulations, and which requires task priorities to be assigned before deployment. Along with MSRP and MrsP, there exist two forms of schedulability tests that bound the worst-case blocking time due to resource accesses: the traditional ones being more widely adopted and the more recently developed holistic ones which deliver tighter analysis. On uniprocessor systems, there are several well-established optimal priority assignment algorithms. Unfortunately, on multiprocessor systems with shared resources, the issue of priority assignment has not been adequately understood. In this article, we investigate three mainstream priority assignment algorithms-Deadline Monotonic Priority Ordering (DMPO), Audsley's Optimal Priority Assignment (OPA), and Robust Priority Assignment (RPA), in the context of partitioned multiprocessor systems with shared resources. Our contributions are multifold: First, we prove that DMPO is optimal with the traditional schedulability tests. Second, two counter examples are given as evidence that DMPO is not optimal with the tighter holistic schedulability tests. Third, we then analyze the pessimism arising from the adoption of OPA and RPA with the holistic tests. Lastly, we propose a Slack-based Priority Ordering (SPO) algorithm that minimises such pessimism, and has polynomial time complexity. Comprehensive experiments show that SPO outperforms (i.e., results in a larger number of schedulable systems) DMPO, OPA, and RPA in general with the holistic schedulability tests, by up to 15 percent. With the theoretical contributions, this paper is a useful guide to priority assignment in real-time partitioned multiprocessor systems with shared resources.",https://ieeexplore.ieee.org/document/9109645/,IEEE Transactions on Computers,1 July 2021,ieeexplore
10.1109/ACCESS.2021.3054122,RMPD: Method for Enhancing the Robustness of Recommendations With Attack Environments,IEEE,Journals,"Personalized item recommendation has become a hot topic research among academic and industry community. But lots of purposeful fraudsters maybe perform different attacks on the recommender system to insert fake ratings, which could reduce the authenticity and reliability of recommendations. For a recommender system with fraudsters, it is crucial to detect malicious ratings and reduce the proportion of fraudster's ratings. This paper presents a method Prediction and Detection of Rating Matrix(RMPD) combining rating prediction and attack detection. The detection results of the attackers are applied to the rating prediction, thereby controlling the contribution and proportion of attackers to the rating prediction component both in training and learning, and then implementing more accurate item rating projections. The method will also solve the problem of data sparsity in the recommender system to some extent. The superiority of the proposed method in predicting recommendation performance compared with other baseline methods is demonstrated on real-world datasets. The ablation experiment proves the necessity of the components.",https://ieeexplore.ieee.org/document/9334992/,IEEE Access,2021,ieeexplore
10.1109/TSG.2017.2783894,Real-Time Energy Management in Microgrids With Reduced Battery Capacity Requirements,IEEE,Journals,"Energy storage units hold promise to transform the electric power industry, since they can supply power to end customers during peak demand times, and operate as customers upon a power surplus. This paper studies online energy management with renewable energy resources and energy storage units. For the problem at hand, the popular approaches rely on stochastic dual (sub)gradient (SDG) iterations for a chosen stepsize μ, which generally require battery capacity O(1/μ) to guarantee an O(μ)-optimal solution. With the goal of achieving optimal energy cost with considerably reduced battery capacity requirements, an online learning-aided management (OLAM) scheme is introduced for energy management, which incorporates statistical learning advances into real-time energy management. To facilitate real-time implementation of the proposed scheme, the alternating direction method of multipliers method is also leveraged to solve the involved subproblems in a distributed fashion. It is analytically established that OLAM incurs an O(μ) optimality gap, while only requiring battery capacity O(log<sup>2</sup>(μ)/√μ). Simulations on the IEEE power grid benchmark corroborate that OLAM incurs similar average cost relative to that of SDG, while requiring markedly lower battery capacity.",https://ieeexplore.ieee.org/document/8214260/,IEEE Transactions on Smart Grid,March 2019,ieeexplore
10.1109/TITS.2019.2906038,Real-Time Sensor Anomaly Detection and Identification in Automated Vehicles,IEEE,Journals,"Connected and automated vehicles (CAVs) are expected to revolutionize the transportation industry, mainly through allowing for a real-time and seamless exchange of information between vehicles and roadside infrastructure. Although connectivity and automation are projected to bring about a vast number of benefits, they can give rise to new challenges in terms of safety, security, and privacy. To navigate roadways, CAVs need to heavily rely on their sensor readings and the information received from other vehicles and roadside units. Hence, anomalous sensor readings caused by either malicious cyber attacks or faulty vehicle sensors can result in disruptive consequences and possibly lead to fatal crashes. As a result, before the mass implementation of CAVs, it is important to develop methodologies that can detect anomalies and identify their sources seamlessly and in real time. In this paper, we develop an anomaly detection approach through combining a deep learning method, namely convolutional neural network (CNN), with a well-established anomaly detection method, and Kalman filtering with a χ<sup>2</sup>-detector, to detect and identify anomalous behavior in CAVs. Our numerical experiments demonstrate that the developed approach can detect anomalies and identify their sources with high accuracy, sensitivity, and F1 score. In addition, this developed approach outperforms the anomaly detection and identification capabilities of both CNNs and Kalman filtering with a χ<sup>2</sup>-detector method alone. It is envisioned that this research will contribute to the development of safer and more resilient CAV systems that implement a holistic view toward intelligent transportation system (ITS) concepts.",https://ieeexplore.ieee.org/document/8684317/,IEEE Transactions on Intelligent Transportation Systems,March 2020,ieeexplore
10.1109/JSEN.2021.3075535,Recent Advancements in the Development of Sensors for the Structural Health Monitoring (SHM) at High-Temperature Environment: A Review,IEEE,Journals,"With Industry 4.0 becoming increasingly pervasive, the importance and usage of sensors has increased several folds. Industry 4.0 refers to a new phase in the industrial revolution that mainly focuses on interconnectivity, automation, machine learning, and real-time data. Real-time structural health monitoring (SHM) of components in the industrial process is one of the crucial and important component of Industry 4.0. SHM of components exposed to high-temperature (<inline-formula> <tex-math notation=""LaTeX"">$\sim 650^{\circ }\text{C}$ </tex-math></inline-formula>) is becoming increasingly important nowadays. However, harsh and high temperature environments impose a great challenge towards their implementation. This review is an attempt to demonstrate the development, application, limitations and recent advancement of the existing sensors used for SHM. Some sensors such as eddy current (EC) sensors and fiber Bragg grating (FBG) sensors have been discussed in detail. A phenomenological study of the electromagnetic sensor for the SHM of engineering components that are exposed to high temperature has been addressed. State-of-the-art fabrication methodologies such as low temperature co-fired ceramic (LTCC) technology for such type of sensors for high temperature SHM applications have been elucidated. Future challenges and opportunities for SHM applications of high temperature sensors have been highlighted.",https://ieeexplore.ieee.org/document/9415648/,IEEE Sensors Journal,"15 July15, 2021",ieeexplore
10.1109/ACCESS.2020.2976513,Resource-Constrained Machine Learning for ADAS: A Systematic Review,IEEE,Journals,"The advent of machine learning (ML) methods for the industry has opened new possibilities in the automotive domain, especially for Advanced Driver Assistance Systems (ADAS). These methods mainly focus on specific problems ranging from traffic sign and light recognition to pedestrian detection. In most cases, the computational resources and power budget found in ADAS systems are constrained while most machine learning methods are computationally intensive. The usual solution consists in adapting the ML models to comply with the memory and real-time (RT) requirements for inference. Some models are easily adapted to resource-constrained hardware, such as Support Vector Machines, while others, like Neural Networks, need more complex processes to fit into the desired hardware. The ADAS hardware (HW platforms) are diverse, from complex MPSoC CPUs down to classical MCUs, DPSs and application-specific FPGAs and ASICs or specific GPU platforms (such as the NVIDIA families Tegra or Jetson). Therefore, there is a tradeoff between the complexity of the ML model implemented and the selected platform that impacts the performance metrics: function results, energy consumption and speed (latency and throughput). In this paper, a survey in the form of systematic review is conducted to analyze the scope of the published research works that embed ML models into resource-constrained implementations for ADAS applications and what are the achievements regarding the ML performance, energy and speed trade-off.",https://ieeexplore.ieee.org/document/9016213/,IEEE Access,2020,ieeexplore
10.1109/ACCESS.2020.3045563,SDN Enhanced Resource Orchestration of Containerized Edge Applications for Industrial IoT,IEEE,Journals,"With the rise of the Industrial Internet of Things (IIoT), there is an intense pressure on resource and performance optimization leveraging on existing technologies, such as Software Defined Networking (SDN), edge computing, and container orchestration. Industry 4.0 emphasizes the importance of lean and efficient operations for sustainable manufacturing. Achieving this goal would require engineers to consider all layers of the system, from hardware to software, and optimizing for resource efficiency at all levels. This emphasizes the need for container-based virtualization tools such as Docker and Kubernetes, offering Platform as a Service (PaaS), while simultaneously leveraging on edge technologies to reduce related latencies. For network management, SDN is poised to offer a cost-effective and dynamic scalability solution by customizing packet handling for various edge applications and services. In this paper, we investigate the energy and latency trade-offs involved in combining these technologies for industrial applications. As a use case, we emulate a 3D-drone-based monitoring system aimed at providing real-time visual monitoring of industrial automation. We compare a native implementation to a containerized implementation where video processing is orchestrated while streaming is handled by an external UE representing the IIoT device. We compare these two scenarios for energy utilization, latency, and responsiveness. Our test results show that only roughly 16 percent of the total power consumption happens on the mobile node when orchestrated. Virtualization adds up about 4.5 percent of the total power consumption while the latency difference between the two approaches becomes negligible after the streaming session is initialized.",https://ieeexplore.ieee.org/document/9296769/,IEEE Access,2020,ieeexplore
10.1109/ACCESS.2020.2994933,Sensor-Driven Learning of Time-Dependent Parameters for Prescriptive Analytics,IEEE,Journals,"Big data analytics is rapidly emerging as a key Internet of Things (IoT) initiative aiming at providing meaningful insights and supporting optimal decision making under time constraints. In this direction, prescriptive analytics has just started to emerge. Prescriptive analytics moves beyond descriptive and predictive analytics aiming at providing adaptive, automated, constrained, time-dependent and optimal decisions. The use of time-dependent parameters in prescriptive analytics models provide a more reliable and realistic representation of the complex and dynamic environment and the associated decision making process; however, their estimation poses significant challenges due to the uncertainty derived from inaccurate user input, noisy data, and non-stationarity of real-world data streams. Since feedback and learning mechanisms for tracking the prescriptive analytics are crucial enablers for self-configuration and self-optimization, this paper proposes an approach for sensor-driven learning of time-dependent parameters for prescriptive analytics models deployed in streaming computational environments. The proposed approach was validated in an Industry 4.0 use case, while it was further evaluated through extensive simulation experiments. The proposed approach overcomes challenges related to uncertainty derived from user's input, non-stationary data and sensor noise and provides estimates of time-dependent parameters that lead to more reliable prescriptions.",https://ieeexplore.ieee.org/document/9094172/,IEEE Access,2020,ieeexplore
10.1109/TII.2020.3047675,Siamese Neural Network Based Few-Shot Learning for Anomaly Detection in Industrial Cyber-Physical Systems,IEEE,Journals,"With the increasing population of Industry 4.0, both AI and smart techniques have been applied and become hotly discussed topics in industrial cyber-physical systems (CPS). Intelligent anomaly detection for identifying cyber-physical attacks to guarantee the work efficiency and safety is still a challenging issue, especially when dealing with few labeled data for cyber-physical security protection. In this article, we propose a few-shot learning model with Siamese convolutional neural network (FSL-SCNN), to alleviate the over-fitting issue and enhance the accuracy for intelligent anomaly detection in industrial CPS. A Siamese CNN encoding network is constructed to measure distances of input samples based on their optimized feature representations. A robust cost function design including three specific losses is then proposed to enhance the efficiency of training process. An intelligent anomaly detection algorithm is developed finally. Experiment results based on a fully labeled public dataset and a few labeled dataset demonstrate that our proposed FSL-SCNN can significantly improve false alarm rate (FAR) and F1 scores when detecting intrusion signals for industrial CPS security protection.",https://ieeexplore.ieee.org/document/9311786/,IEEE Transactions on Industrial Informatics,Aug. 2021,ieeexplore
10.1109/TITS.2020.2980864,Solving the Security Problem of Intelligent Transportation System With Deep Learning,IEEE,Journals,"Objective: the objective of this study is to study deep learning to solve the safety problems of intelligent transportation system. Method: the intelligent transportation system is improved by using the deep learning algorithm, and the improved system is simulated, and the data transmission performance, accuracy prediction performance and path change strategy of the system are statistically analyzed. Results: in the analysis of the data transmission performance of the system, the probability of successful propagation is found to be 100%. When the value of λ is 0.01~0.05, it is the closest to the actual result and the data delay is the smallest. In the analysis of the accuracy prediction of the system, it is found that the system of this study has the best accuracy prediction performance with the increase of the number of iterations compared with other models in different categories. After further analyzing the path induction strategy of the system, it is found that the route guidance strategy of this study can effectively restrain the spread of congestion and achieve the effect of timely evacuation of traffic congestion in the face of congested road sections. Conclusion: it is found that the improvement of the intelligent transportation system by using deep learning can significantly reduce the data transmission delay of the system, improve the prediction accuracy, and effectively change the path in the face of congestion to suppress the congestion spread. Although there are some shortcomings in the experiment, it still provides experimental reference for the development of the transportation industry in the later stage.",https://ieeexplore.ieee.org/document/9043888/,IEEE Transactions on Intelligent Transportation Systems,July 2021,ieeexplore
10.1109/TII.2019.2919268,Stochastic Configuration Networks Based Adaptive Storage Replica Management for Power Big Data Processing,IEEE,Journals,"In the power industry, processing business big data from geographically distributed locations, such as online line-loss analysis, has emerged as an important application. How to achieve highly efficient big data storage to meet the requirements of low latency processing applications is quite challenging. In this paper, we propose a novel adaptive power storage replica management system, named PARMS, based on stochastic configuration networks (SCNs), in which the network traffic and the data center (DC) geodistribution are taken into consideration to improve data real-time processing. First, as a fast learning model with less computation burden and sound prediction performance, the SCN model is employed to estimate the traffic state of power data networks. Then, a series of data replica management algorithms is proposed to lower the effects of limited bandwidths and a fixed underlying infrastructure. Finally, the proposed PARMS is implemented using data-parallel computing frameworks (DCFs) for the power industry. Experiments are carried out in an electric power corporation of 230 million users, China Southern power grid, and the results show that our proposed solution can deal with power big data storage efficiently and the job completion times across geodistributed DCs are reduced by 12.19% on average.",https://ieeexplore.ieee.org/document/8723083/,IEEE Transactions on Industrial Informatics,Jan. 2020,ieeexplore
10.1109/TNSRE.2014.2305111,The Extraction of Neural Information from the Surface EMG for the Control of Upper-Limb Prostheses: Emerging Avenues and Challenges,IEEE,Journals,"Despite not recording directly from neural cells, the surface electromyogram (EMG) signal contains information on the neural drive to muscles, i.e, the spike trains of motor neurons. Using this property, myoelectric control consists of the recording of EMG signals for extracting control signals to command external devices, such as hand prostheses. In commercial control systems, the intensity of muscle activity is extracted from the EMG and used for single degrees of freedom activation (direct control). Over the past 60 years, academic research has progressed to more sophisticated approaches but, surprisingly, none of these academic achievements has been implemented in commercial systems so far. We provide an overview of both commercial and academic myoelectric control systems and we analyze their performance with respect to the characteristics of the ideal myocontroller. Classic and relatively novel academic methods are described, including techniques for simultaneous and proportional control of multiple degrees of freedom and the use of individual motor neuron spike trains for direct control. The conclusion is that the gap between industry and academia is due to the relatively small functional improvement in daily situations that academic systems offer, despite the promising laboratory results, at the expense of a substantial reduction in robustness. None of the systems so far proposed in the literature fulfills all the important criteria needed for widespread acceptance by the patients, i.e. intuitive, closed-loop, adaptive, and robust real-time (&lt;;200 ms delay) control, minimal number of recording electrodes with low sensitivity to repositioning, minimal training, limited complexity and low consumption. Nonetheless, in recent years, important efforts have been invested in matching these criteria, with relevant steps forwards.",https://ieeexplore.ieee.org/document/6737308/,IEEE Transactions on Neural Systems and Rehabilitation Engineering,July 2014,ieeexplore
10.1109/ACCESS.2021.3083499,Towards Secured Online Monitoring for Digitalized GIS Against Cyber-Attacks Based on IoT and Machine Learning,IEEE,Journals,"Recently, the Internet of Things (IoT) has an important role in the growth and development of digitalized electric power stations while offering ambitious opportunities, specifically real-time monitoring and cybersecurity. In this regard, this paper introduces a novel IoT architecture for the online monitoring of the gas-insulated switchgear (GIS) status instead of the traditional observation methods. The proposed IoT architecture is derived from the concept of the cyber-physic system (CPS) in Industry 4.0. However, the cyber-attacks and the classification of the GIS insulation defects represent the main challenges against the implementation of IoT topology for the online monitoring and tracking of the GIS status. For this purpose, advanced machine learning techniques are utilized to detect cyber-attacks to conduct the paradigm and verification. Different test scenarios on various defects in GIS are performed to demonstrate the effectiveness of the proposed IoT architecture. Partial discharge pulse sequence features are extracted for each defect to represent the inputs for IoT architecture. The results confirm that the proposed IoT architecture based on the machine learning technique, that is the extreme gradient boosting (XGBoost), can visualize all defects in the GIS with different alarms, besides showing the cyber-attacks on the networks effectively. Furthermore, the defects of GIS and the fake data due to the cyber-attacks are recognized and presented on the dashboard of the proposed IoT platform with high accuracy and more clarified visualization to enhance the decision-making about the GIS status.",https://ieeexplore.ieee.org/document/9440436/,IEEE Access,2021,ieeexplore
10.1109/TCIAIG.2016.2528499,Using Behavior Objects to Manage Complexity in Virtual Worlds,IEEE,Journals,"The quality of high-level AI of nonplayer characters (NPCs) in commercial open-world games (OWGs) has been increasing during the past years. However, due to constraints specific to the game industry, this increase has been slow and it has been driven by larger budgets rather than adoption of new complex AI techniques. Most of the contemporary AI is still expressed as hard-coded scripts. The complexity and manageability of the script codebase is one of the key limiting factors for further AI improvements. In this paper, we address this issue. We present behavior objects (BO)-a general approach to development of NPC behaviors for large OWGs. BOs are inspired by object-oriented programming and extend the concept of smart objects. Our approach promotes encapsulation of data and code for multiple related behaviors in one place, hiding internal details and embedding intelligence in the environment. BOs are a natural abstraction of five different techniques that we have implemented to manage AI complexity in an upcoming AAA OWG. We report the details of the implementations in the context of behavior trees and the lessons learned during development. Our study should serve as an inspiration for AI architecture designers from both the academia and the industry.",https://ieeexplore.ieee.org/document/7406730/,IEEE Transactions on Computational Intelligence and AI in Games,June 2017,ieeexplore
10.1109/ACCESS.2021.3082934,Visual Product Tracking System Using Siamese Neural Networks,IEEE,Journals,"Management of unstructured production data is a key challenge for Industry 4.0. Effective product tracking endorses data integration and productivity improvements throughout the manufacturing processes. Radio-frequency identification (RFID) tags are used in many tracking cases, but in some manufacturing environments, those cannot be used as they might get damaged or removed during processing. In this paper, we propose an alternative visual product tracking system. The physical system uses two cameras placed at the two ends of the tracked process(es). Product pairs are then matched with a Siamese neural network operating on the product images and trained offline on the problem at hand with labeled data. The proposed system can track products solely based on their visual appearance and without any physical interference with the products or production processes. Unlike other existing image-based methods, the proposed system is invariant to major positional and visual changes in the products. As a proof-of-concept, we tested the proposed system with real plywood factory data and were able to track the products with 98.5 % accuracy in a realistic test scenario. The implementation of the proposed method and the Veneer21 dataset are publicly available at https://github.com/TuomasJalonen/visual-product-tracking-system.",https://ieeexplore.ieee.org/document/9439511/,IEEE Access,2021,ieeexplore
10.1109/TAI.2021.3057027,ZJU-Leaper: A Benchmark Dataset for Fabric Defect Detection and a Comparative Study,IEEE,Journals,"Fabric inspection plays an important role in the process of quality control in textile manufacturing. There is a growing demand in the textile industry to leverage computer vision technology for more efficient quality control in the hope that it will replace the traditional labor-intensive inspection by naked eyes. However, there is an underlying viewpoint in most existing fabric datasets that automatic defect detection is a traditional image classification problem, thus more samples help better, which lacks enough consideration about the problem itself and real application environments. After deep communication with users, we find these facts that an assembly line usually has only a few fixed texture fabrics for a long period, users prefer fast deployment and easily upgradable model to a general model and long-time tuning, and users hope the process of collecting samples, annotating, and deployment affects assembly lines as little as possible. This implies that defect detection is different from popular deep learning problems. Multiple-stage models and fast training become more attractive since users are able to train and deploy models by themselves according to the real conditions of samples that can be obtained. Based on this analysis, we propose a new fabric dataset “ZJU-Leaper”. It provides a series of task settings in accordance with the progressive strategy dealing with the problem, from only normal samples to many defective samples with precise annotations, to facilitate real-world application. To avoid misleading information and inconsistency issues associated with the prior evaluation metrics, we propose a new evaluation protocol by experimental analysis of task-specific indexes, which can tell a truthful comparison between different inspection methods. We also offer some novel solutions to address the new challenges of our dataset, as part of the baseline experiments. It is our hope that ZJU-Leaper can accelerate the research of automated visual inspection and empower the practitioners with more opportunities for manufacturing automation in the textile industry.",https://ieeexplore.ieee.org/document/9346038/,IEEE Transactions on Artificial Intelligence,Dec. 2020,ieeexplore
10.1109/COASE.2018.8560557,An Industry 4.0 Cyber-Physical Framework for Micro Devices Assembly,IEEE,Conferences,"An advanced cyber manufacturing framework to support the collaborative assembly of micro devices is presented based on Industry 4.0 principles. The distributed cyber and physical components work together to plan, assemble and monitor micro assembly related tasks; micro assembly refers to the assembly of micron sized devices which cannot be manufactured by MEMS technologies. The collaborative framework proposed includes assembly planning and path planning modules, Virtual Reality based assembly simulation environments and physical assembly work cells. An ontology based approach was implemented to address semantic interoperability issues to support formation of temporary partnerships in a Virtual Enterprise context. The key to the design and implementation of this complex framework is an information centric process modeling approach which provides a data/information oriented basis for collaboration. A collaborative cyber physical test bed has been built to demonstrate feasibility of proposed framework and approach.",https://ieeexplore.ieee.org/document/8560557/,2018 IEEE 14th International Conference on Automation Science and Engineering (CASE),20-24 Aug. 2018,ieeexplore
10.1109/SYSCON.2018.8369547,An interactive architecture for industrial scale prediction: Industry 4.0 adaptation of machine learning,IEEE,Conferences,"According to wiki definition, there are four design principles in Industry 4.0. These principles support companies in identifying and implementing Industry 4.0 scenarios, namely, Interoperability, Information transparency, Technical assistance, Decentralized decisions. In this paper we have discussed our work on an implementation of a machine learning based interactive architecture for industrial scale prediction for dynamic distribution of water resources across the continent, keeping the four corners of Industry 4.0 in place. We report the possibility of producing most probable high resolution estimation regarding the water balance in any region within Australia by implementation of an intelligent system that can integrate spatial-temporal data from various independent sensors and models, with the ground truth data produced by 250 practitioners from the irrigation industry across Australia. This architectural implementation on a cloud computing platform linked with a freely distributed mobile application, allowing interactive ground truthing of a machine learning model on a continental scale, shows accuracy of 90% with 85% sensitivity of correct surface soil moisture estimation with end users at its complete control. Along with high level of information transparency and interoperability, providing on-demand technical supports and motivating users by allowing them to customize and control their own local predictive models, show the successfulness of principles in Industry 4.0 in real environmental issues in the future adaptation in various industries starting from resource management to modern generation soft robotics.",https://ieeexplore.ieee.org/document/8369547/,2018 Annual IEEE International Systems Conference (SysCon),23-26 April 2018,ieeexplore
10.1109/PerComWorkshops51409.2021.9431009,Architecture and pervasive platform for machine learning services in Industry 4.0,IEEE,Conferences,"Pervasive computing promotes the integration of smart electronic devices in our living and working spaces in order to provide new, advanced services. Recently many prototype services based on machine learning techniques have been proposed in a number of domains like smart homes, smart buildings or smart plants. However, the number of applications effectively deployed in the real world is still limited. We believe that architectural principles and integrated frameworks are still missing today to successfully and repetitively support application developers and operators. In this paper, we present a novel architecture and a pervasive platform allowing the development of machine learning based applications in smart buildings.",https://ieeexplore.ieee.org/document/9431009/,2021 IEEE International Conference on Pervasive Computing and Communications Workshops and other Affiliated Events (PerCom Workshops),22-26 March 2021,ieeexplore
10.1109/WFCS.2019.8757952,Cloud-enabled Smart Data Collection in Shop Floor Environments for Industry 4.0,IEEE,Conferences,"Industry 4.0 transition is producing a remarkable change in the Smart Factories management. Modern companies can provide new services following products inside the shop floors along the entire production chain. To achieve the goal of servitization that the Industry 4.0 world requires, a modernization of current production chains is needed. This common demand comes mostly from manufacturing sector, where complex work machines collaborate with human workers. The data produced by the machines must be processed quickly, to allow the implementation of reactive services such as predictive maintenance, and remote control, always taking care of the safety of nearby people. This paper proposes a multi-layer architecture to monitor legacy production machines during their operations inside customers plants. The platform provides near real-time delivery of data collected from the machines with a high grade of customization according to customer needs. The performed tests show the scalability of the platform for a productive ecosystem with many machines at work, confirming its feasibility within different production facilities with different needs.",https://ieeexplore.ieee.org/document/8757952/,2019 15th IEEE International Workshop on Factory Communication Systems (WFCS),27-29 May 2019,ieeexplore
10.1109/WCNC45663.2020.9120761,Collaborative Learning Model for Cyberattack Detection Systems in IoT Industry 4.0,IEEE,Conferences,"Although the development of IoT Industry 4.0 has brought breakthrough achievements in many sectors, e.g., manufacturing, healthcare, and agriculture, it also raises many security issues to human beings due to a huge of emerging cybersecurity threats recently. In this paper, we propose a novel collaborative learning-based intrusion detection system which can be efficiently implemented in IoT Industry 4.0. In the system under consideration, we develop smart “filters” which can be deployed at the IoT gateways to promptly detect and prevent cyberattacks. In particular, each filter uses the collected data in its network to train its cyberattack detection model based on the deep learning algorithm. After that, the trained model will be shared with other IoT gateways to improve the accuracy in detecting intrusions in the whole system. In this way, not only the detection accuracy is improved, but our proposed system also can significantly reduce the information disclosure as well as network traffic in exchanging data among the IoT gateways. Through thorough simulations on real datasets, we show that the performance obtained by our proposed method can outperform those of the conventional machine learning methods.",https://ieeexplore.ieee.org/document/9120761/,2020 IEEE Wireless Communications and Networking Conference (WCNC),25-28 May 2020,ieeexplore
,Corporate Social Responsibility Challenges and Risks of Industry 4.0 technologies: A review,VDE,Conferences,"The fourth industrial revolution arrived with many enabling technologies that would impact important sociological aspects in the industry. Some of the Industry 4.0 technologies are already running in different industrial application, and other are still as a paradigm state. The social, economic, and environmental acceptance of Industry 4.0 technologies is still under discussion, which open new opportunities to execute various analysis about the possible implications of the implementation of such technologies. This article refers to an exploratory analysis and identification of the different challenges and risks of this new Industry 4.0 paradigm and its related technologies. The technologies under review were Internet of Things, Artificial Intelligence, Cloud Computing, cybersecurity, bid data, blockchain, 5G, robotics, adding manufacturing, unmanned systems, autonomous vehicles, virtual reality, and augmented reality. As a result, different social challenges and risks were identified for each technology, starting from vulnerability, implementation cost, until social aspects such as education and unemployment caused by those new technologies. In conclusion, Industry 4.0 arrived with a lot of benefits to the industry business, but companies should not stop thinking about sustainable development.",https://ieeexplore.ieee.org/document/8835964/,"Smart SysTech 2019; European Conference on Smart Objects, Systems and Technologies",4-5 June 2019,ieeexplore
10.1109/COINS51742.2021.9524088,Cyber Security Risks of Technical Components in Industry 4.0,IEEE,Conferences,"Industry 4.0 is a new concept of automation data exchange in manufacturing. Industry 4.0 consists of various technical components such as Artificial Intelligence (AI), IoT, OPC UA, and cloud. There could be cyber risks when these technical components are used in Industrial Control System (ICS) without security consideration. Therefore cyber risk assessment, reconsideration of implementation, operation are necessary to adopt Industry 4.0. Problems of previous research for ICS risk assessments are some of them are theoretical and conceptual methods such as simulations. Moreover, they do not focus on Industry 4.0 components.The objective of this research is to prove the concrete cyber risks and introduces corresponding secure implementations for Industry 4.0 components such as Artificial Intelligence (AI), the Internet of Things (IoT), OPC Unified Architecture (OPC UA). The proposed method has benefits for clarifying impacts on the real world by cyber-attacks through penetration tests against ICS testbed with actual machines.",https://ieeexplore.ieee.org/document/9524088/,2021 IEEE International Conference on Omni-Layer Intelligent Systems (COINS),23-25 Aug. 2021,ieeexplore
10.1109/EuCNC/6GSummit51104.2021.9482590,Empowering Industry 4.0 and Autonomous Drone Scouting use cases through 5G-DIVE Solution,IEEE,Conferences,"The 5G Edge Intelligence for Vertical Experimentation (5G-DIVE) project aims at demonstrating the technical merits and business value proposition of 5G technologies in two vertical pilots, namely the Industry 4.0 (I4.0) and Autonomous Drones Scout (ADS) pilots. This paper presents an overview of the overall 5G-DIVE solution and reports the results of the initial validation campaign of the selected use case, featuring 5G connectivity, distributed Edge computing, and artificial intelligence. The initial results for the I4.0 provide a baseline for next step validation campaign targeting a broader scale 5G implementation, while the ADS results provides promising results for enhancing the autonomous navigation in real-time.",https://ieeexplore.ieee.org/document/9482590/,2021 Joint European Conference on Networks and Communications & 6G Summit (EuCNC/6G Summit),8-11 June 2021,ieeexplore
10.1109/SMC.2019.8913901,Explainable Machine Learning in Industry 4.0: Evaluating Feature Importance in Anomaly Detection to Enable Root Cause Analysis,IEEE,Conferences,"In the past recent years, Machine Learning methodologies have been applied in countless application areas. In particular, they play a key role in enabling Industry 4.0. However, one of the main obstacles to the diffusion of Machine Learning-based applications is related to the lack of interpretability of most of these methods. In this work, we propose an approach for defining a `feature importance' in Anomaly Detection problems. Anomaly Detection is an important Machine Learning task that has an enormous applicability in industrial scenarios. Indeed, it is extremely relevant for the purpose of quality monitoring. Moreover, it is often the first step towards the design of a Machine Learning-based smart monitoring solution because Anomaly Detection can be implemented without the need of labelled data. The proposed feature importance evaluation approach is designed for Isolation Forest, one of the most commonly used algorithm for Anomaly Detection. The efficacy of the proposed method is tested on synthetic and real industrial datasets.",https://ieeexplore.ieee.org/document/8913901/,"2019 IEEE International Conference on Systems, Man and Cybernetics (SMC)",6-9 Oct. 2019,ieeexplore
10.1109/COMPSAC.2018.10204,Indoor Augmented Reality Using Deep Learning for Industry 4.0 Smart Factories,IEEE,Conferences,"This paper proposes to design, develop and implement a fast and markerless mobile augmented reality system to achieve the registration for, the visualization of, and the interaction with machines in indoor smart factories with Industry 4.0 vision. A lightweight deep-learning image detection module based on MobileNets running on mobile devices is used to detect/recognize different machines and different portions of machines. Internet of Things (IoT) networking allows machines and sensors in machines to report data, such as machine settings and machine states, to the cloud-side server. Thus, augmented information associated with a machine portion can be derived from the server and superimposed with the portion image shown on the device display. Furthermore, interaction methods based on touch gestures and distance calculation are also implemented. A prototype system is developed and tested in a mechanical workshop for the purpose of validation and evaluation. The system is shown to achieve high detection accuracy, intuitive visualization, and unique interaction modes.",https://ieeexplore.ieee.org/document/8377831/,2018 IEEE 42nd Annual Computer Software and Applications Conference (COMPSAC),23-27 July 2018,ieeexplore
10.1109/RTSI.2019.8895598,Intelligent Embedded Load Detection at the Edge on Industry 4.0 Powertrains Applications,IEEE,Conferences,"In the context of Industry 4.0, there has been great focus in developing intelligent sensors. Deploying them, condition monitoring and predictive maintenance have become feasible solutions to minimize operating and maintenance costs while also increasing safety. A critical aspect is the applied load to the supervised machinery system. Vibration data can be used to determine the current condition, but this needs signal processing specially developed and adapted to the monitored machine part for feature extraction. Artificial intelligence (AI) can, on one hand, simplify the development of such special purpose processing and on another hand be used to monitor and classify machine conditions by learning features directly from data. By bringing the AI computation as close as possible to the sensor (Edge-AI), data bandwidth can be minimized, system scalability and responsiveness can be improved and real-time requirements can be fulfilled. This work describes how Edge-AI on a STM32-bit microcontroller can be implemented. Our experimental setup demonstrates how AI can be effectively used to detect and classify the load on a powertrain. In order to do this, we use a MEMS capacity accelerometer to sense vibrations of the system. Also, this work demonstrates how Deep Neural Networks (DNN) for signal classification are build and trained by using an open-source deep learning framework and how the code library for the microcontroller is automatically generated afterwards by using STM32Cube. AI toolchain. We compare the classification accuracy of a memory compressed DNN against an uncompressed DNN.",https://ieeexplore.ieee.org/document/8895598/,2019 IEEE 5th International forum on Research and Technology for Society and Industry (RTSI),9-12 Sept. 2019,ieeexplore
10.1109/SCC47175.2019.9116104,Modeling and management of human resources in the reconfiguration of production system in industry 4.0 by neural networks,IEEE,Conferences,"In Industry 4.0, the role of employees changes significantly. Real-time production line control transforms job content. Work processes affect working conditions. The implementation of a socio-technical approach to the organization of work gives workers the opportunity to adapt their skills. Indeed, production work will become more and more multi-factor, especially with regard to control and decision-making tasks. In this paper, a proposal for an intelligent system for modeling skills and human resource management in the production system chain through the use of two artificial neural networks. The first NN1 network allows for the identification of the human factor, as well as the second NN2 network is reserved for valuing the human skills needed in Industry 4.0.",https://ieeexplore.ieee.org/document/9116104/,"2019 International Conference on Signal, Control and Communication (SCC)",16-18 Dec. 2019,ieeexplore
10.23919/DATE48585.2020.9116407,Network Synthesis for Industry 4.0,IEEE,Conferences,"Today's factory machines are ever more connected with SCADA, MES, ERP applications as well as external systems for data analysis. Different types of network architectures must be used for this purpose. For instance, control applications at the lowest level are susceptible to delays and errors while data analysis with machine learning procedures requires to move a large amount of data without real-time constraints. Standard data formats, like Automation Markup Language (AML), have been established to document factory environment, machine placement and network deployment, however, no automatic technique is currently available in the context of Industry 4.0 to choose the best mix of network architectures according to spacial constraints, cost, and performance. We propose to fill this gap by formulating an optimization problem. First of all, spatial and communication requirements are extracted from the AML description. Then, the optimal interconnection of wired or wireless channels is obtained according to application objectives. Finally, this result is back-annotated to AML to be used in the life cycle of the production system. The proposed methodology is described through a small, but complete, smart production plant.",https://ieeexplore.ieee.org/document/9116407/,"2020 Design, Automation & Test in Europe Conference & Exhibition (DATE)",9-13 March 2020,ieeexplore
10.1109/ICRITO48877.2020.9198036,State of Art: Energy Efficient Protocols for Self-Powered Wireless Sensor Network in IIoT to Support Industry 4.0,IEEE,Conferences,"Up gradation of manufacturing systems in industries by means of implementing innovative manufacturing techniques that captures real time data, applies machine learning algorithms, makes entire system self-decisive and provides inter connectivity to the whole system is the prime focus of the Industry 4.0. It is aimed at bringing new industrial revolution with the help of internet of things technology due to its considerable influence in the industrial manufacturing process. Though impact of internet of things in industrial sector is huge, a practical implementation incorporates challenges in energy efficiency, self-powered sensor nodes and security. For processing of gathered data self-powered sensor nodes may sinks energy from ambient energy sources. A considerable amount of efforts has been put by researchers to address the challenges for development of energy efficient routing protocol for such nodes. The presented survey is aimed at analyzing the protocols for contribution towards the goals of industry 4.0 Energy efficient protocol will support the system to consume least energy for its operation. Henceforth combination of self-powered wireless sensor network and energy efficient protocol will be useful to power up many industrial IoT applications.",https://ieeexplore.ieee.org/document/9198036/,"2020 8th International Conference on Reliability, Infocom Technologies and Optimization (Trends and Future Directions) (ICRITO)",4-5 June 2020,ieeexplore
10.1109/ISWCS.2019.8877305,Visible Light Positioning for Location-Based Services in Industry 4.0,IEEE,Conferences,"Industry 4.0 refers to the evolution in manufacturing from computerization to fully cyberphysical systems that exploit rich sensor data, adaptive real-time safety-critical control, and machine learning. An important aspect of this vision is the sensing and subsequent association of objects in the physical world with their cyber and virtual counterparts. In this paper we propose Visible Light Positioning (VLP) as an enabler for these Industry 4.0 applications. We also explore sensing techniques, including cameras (and depth sensors), and other light-based solutions for object positioning and detection along with their respective limitations. We then demonstrate an application of positioning for real time robot control in an interactive multiparty cyber-physical-virtual deployment. Lastly, based on our experience with this cyberphysical-virtual application, we propose Ray-Surface Positioning (RSP), a novel VLP technique, as a low cost positioning system for Industry 4.0.",https://ieeexplore.ieee.org/document/8877305/,2019 16th International Symposium on Wireless Communication Systems (ISWCS),27-30 Aug. 2019,ieeexplore
10.1109/BDCloud.2018.00136,"iSTEP, an Integrated Self-Tuning Engine for Predictive Maintenance in Industry 4.0",IEEE,Conferences,"The recent expansion of IoT-enabled (Internet of Things) devices in manufacturing contexts and their subsequent data-driven exploitation paved the way to the advent of the Industry 4.0, promoting a full integration of IT services, smart devices, and control systems with physical objects, their electronics and sensors. The real-time transmission and analysis of collected data from factories has the potential to create manufacturing intelligence, of which predictive maintenance is an expression. Hence the need to design new approaches able to manage not only the data volume, but also the variety and velocity, extracting actual value from the humongous amounts of collected data. To this aim, we present iSTEP, an integrated Self-Tuning Engine for Predictive maintenance, based on Big Data technologies and designed for Industry 4.0 applications. The proposed approach targets some of the most common needs of manufacturing enterprises: compatibility with both the on-premises and the in-the-cloud environments, exploitation of reliable and largely supported Big Data platforms, easy deployment through containerized software modules, virtually unlimited horizontal scalability, fault-tolerant self-reconfiguration, flexible yet friendly streaming-KPI computations, and above all, the integrated provisioning of self-tuning machine learning techniques for predictive maintenance. The current implementation of iSTEP exploits a distributed architecture based on Apache Kafka, Spark Streaming, MLlib, and Cassandra; iSTEP provides (i) a specific feature engineering block aimed at automatically extracting metrics from the production monitoring time series, which improves the predictive performance by 77% on average, and (ii) a self-tuning approach that dynamically selects the best prediction algorithm, which improves the predictive performance up to 60%. The iSTEP engine provides transparent predictive models, able to provide end users with insights into the knowledge learned, and it has been experimentally evaluated on a public unbalanced failure dataset, whose extensive results are discussed in the paper.",https://ieeexplore.ieee.org/document/8672266/,"2018 IEEE Intl Conf on Parallel & Distributed Processing with Applications, Ubiquitous Computing & Communications, Big Data & Cloud Computing, Social Computing & Networking, Sustainable Computing & Communications (ISPA/IUCC/BDCloud/SocialCom/SustainCom)",11-13 Dec. 2018,ieeexplore
10.1109/TLA.2021.9480156,A Data Governance Framework for Industry 4.0,IEEE,Journals,"The fourth industrial revolution, or Industry 4.0, represents a new stage of evolution in the organization, management and control of the value chain throughout the product or service life cycle. This digitization of the industrial environment is characterized by the connection of Information Technologies (IT) and Operations Technologies (OT) through cyber-physical systems and the Industrial IoT (IIoT). One of the main consequences of this integration is the increasing amount and variety of data generated in real time from different sources. In this environment of intensive generation of actionable information, data becomes a critical asset for Industry 4.0, at all stages of the value chain. However, in order to data become a competitive advantage for the company, it must be managed and governed like any other strategic asset, and therefore it is necessary to rely on a Data Governance system. Industry 4.0 requires a reformulation of governance since the data is a key element and the backbone of the processes of the organization. This paper proposes a Reference Framework for the implementation of Data Governance Systems for Industry 4.0. Previously, it contextualizes data governance for Industry 4.0 environments and identifies the requirements that this framework must address, which are conditioned by the specific features of Industry 4.0, among others, the intensive use of big data, cloud and edge computing, artificial intelligence and current regulations.",https://ieeexplore.ieee.org/document/9480156/,IEEE Latin America Transactions,Dec. 2021,ieeexplore
10.1109/ACCESS.2021.3121302,A Survey on Industry 4.0 for the Oil and Gas Industry: Upstream Sector,IEEE,Journals,"The market volatility in the oil and gas (O&amp;G) sector, the dwindling demand for oil due to the impact of COVID-19, and the push for alternative greener energy are driving the need for innovation and digitization in the O&amp;G industry. This has attracted research interest from academia and the industry in the application of industry 4.0 (I4.0) technologies in the O&amp;G sector. The application of some of these I4.0 technologies has been presented in the literature, but the domain still lacks a comprehensive survey of the application of I4.0 in the O&amp;G upstream sector. This paper investigates the state-of-the-art efforts directed toward I4.0 technologies in the O&amp;G upstream sector. To achieve this, first, an overview of the I4.0 is discussed followed by a systematic literature review from an integrative perspective for publications between 2012–2021 with 223 analyzed documents. The benefits and challenges of the adoption of I4.0 have been identified. Moreover, the paper adds value by proposing a framework for the implementation of I4.0 in the O&amp;G upstream sector. Finally, future directions and research opportunities such as framework, edge computing, quantum computing, communication technologies, standardization, and innovative areas related to the implementation of I4.0 in the upstream sector are presented. The findings from this review show that I4.0 technologies are currently being explored and deployed for various aspects of the upstream sector. However, some of the I4.0 technologies like additive manufacturing and virtual reality are least explored.",https://ieeexplore.ieee.org/document/9579415/,IEEE Access,2021,ieeexplore
10.1109/TII.2019.2962029,An Entropy-Based Approach to Real-Time Information Extraction for Industry 4.0,IEEE,Journals,"Industry 4.0 has drawn considerable attention from industry and academic research communities. The recent advances in Internet of Things (IoT), Big Data analytics, sensor technology, and artificial intelligence have led to the design and implementation of novel approaches to take full advantage of data-driven solutions applicable to Industry 4.0. With the availability of large datasets, it has become crucially important to identify the appropriate amount of relevant information, which would optimize the overall analysis of the corresponding systems. In this article, specific properties of dynamically evolving data systems are introduced and investigated, which provide framework to assess the appropriate amount of representative information.",https://ieeexplore.ieee.org/document/8941297/,IEEE Transactions on Industrial Informatics,Sept. 2020,ieeexplore
10.1109/MetroInd4.0IoT51437.2021.9488429,A Case Study on Challenges of Applying Machine Learning for Predictive Drill Bit Sharpness Estimation,IEEE,Conferences,"Tool condition estimation is one of the most crucial aspects of Industry 4.0 and its evolved maintenance paradigm using predictive systems. This paper presents a use case on vibration measurement based Remaining Useful Lifetime (RUL) prediction of drill bits. The scope of this paper is to give insights on the challenges and limitations when building a Machine Learning (ML) based solution. We analyze the captured signals, suitable feature extraction methods and different data-driven / ML models for progressive wear and RUL prediction task. The methods are investigated on a real world drilling experiment with a multi-sensor setup. Our results indicate that during continuous drilling scenarios, where the drilling locations vary, feature extractors, that are normalized by the average signal energy, provide better results in cutting tool condition estimation problem. In addition, we show several challenges that arise during the design phase and need to be addressed in order to build successful solutions.",https://ieeexplore.ieee.org/document/9488429/,2021 IEEE International Workshop on Metrology for Industry 4.0 & IoT (MetroInd4.0&IoT),7-9 June 2021,ieeexplore
10.1109/PacificVis.2018.00026,A Visual Analytics Approach for Equipment Condition Monitoring in Smart Factories of Process Industry,IEEE,Conferences,"Monitoring equipment conditions is of great value in manufacturing, which can not only reduce unplanned downtime by early detecting anomalies of equipment but also avoid unnecessary routine maintenance. With the coming era of Industry 4.0 (or industrial internet), more and more assets and machines in plants are equipped with various sensors and information systems, which brings an unprecedented opportunity to capture large-scale and fine-grained data for effective on-line equipment condition monitoring. However, due to the lack of systematic methods, analysts still find it challenging to carry out efficient analyses and extract valuable information from the mass volume of data collected, especially for process industry (e.g., a petrochemical plant) with complex manufacturing procedures. In this paper, we report the design and implementation of an interactive visual analytics system, which helps managers and operators at manufacturing sites leverage their domain knowledge and apply substantial human judgements to guide the automated analytical approaches, thus generating understandable and trustable results for real-world applications. Our system integrates advanced analytical algorithms (e.g., Gaussian mixture model with a Bayesian framework) and intuitive visualization designs to provide a comprehensive and adaptive semi-supervised solution to equipment condition monitoring. The example use cases based on a real-world manufacturing dataset and interviews with domain experts demonstrate the effectiveness of our system.",https://ieeexplore.ieee.org/document/8365986/,2018 IEEE Pacific Visualization Symposium (PacificVis),10-13 April 2018,ieeexplore
10.1109/WINCOM50532.2020.9272477,A new middleware for managing heterogeneous robot in ubiquitous environments,IEEE,Conferences,"Heterogeneity is one of the main issues for the deployment of the Industry 4.0. This is due to the diversity in the available robots and the IIoT devices. These equipments use different programming languages and communication protocols. To make the integration of such equipments easy, we propose TalkRoBots, a middleware that allows heterogeneous robots and IIoT devices to communicate together and exchange data in a transparent way. The middleware was experimented in a real scenario with different robots that demonstrate its efficiency.",https://ieeexplore.ieee.org/document/9272477/,2020 8th International Conference on Wireless Networks and Mobile Communications (WINCOM),27-29 Oct. 2020,ieeexplore
10.1109/INDIN.2017.8104789,Addressing security challenges in industrial augmented reality systems,IEEE,Conferences,"In context of Industry 4.0 Augmented Reality (AR) is frequently mentioned as the upcoming interface technology for human-machine communication and collaboration. Many prototypes have already arisen in both the consumer market and in the industrial sector. According to numerous experts it will take only few years until AR will reach the maturity level to be deployed in productive applications. Especially for industrial usage it is required to assess security risks and challenges this new technology implicates. Thereby we focus on plant operators, Original Equipment Manufacturers (OEMs) and component vendors as stakeholders. Starting from several industrial AR use cases and the structure of contemporary AR applications, in this paper we identify security assets worthy of protection and derive the corresponding security goals. Afterwards we elaborate the threats industrial AR applications are exposed to and develop an edge computing architecture for future AR applications which encompasses various measures to reduce security risks for our stakeholders.",https://ieeexplore.ieee.org/document/8104789/,2017 IEEE 15th International Conference on Industrial Informatics (INDIN),24-26 July 2017,ieeexplore
10.1109/CEWIT.2013.6713745,Agent-based planning and control for groupage traffic,IEEE,Conferences,"In this research and technology transfer project, the planning and control processes of the industrial partner Hellmann Worldwide Logistics GmbH &amp; Co. KG are analyzed. An agent-based approach is presented to model current processes and to exploit the identified optimization potential. The developed system directly connects the information flow and the material flow as well as their interdependencies in order to optimize the planning and control in groupage traffic. The software system maps current processes to agents as system components and improves the efficiency by intelligent objects. To handle the high complexity and dynamics of logistics autonomous intelligent agents plan and control the way of represented objects through the logistic network by themselves and induce a flexible and reactive system behavior. We evaluate the implemented dispatching application by simulating the groupage traffic processes using effectively transported orders and process data provided by our industrial partner. Moreover, we modeled real world infrastructures and considered also the dynamics by the simulation of unexpected events and process disturbances. The results show that the system significantly decreases daily cost by reducing the required number of transport providers and shifting conventional orders to next days, which need no immediate delivery. Thus the system increases the efficiency and meets the special challenges and requirements of groupage traffic. Moreover, the system supports freight carriers and dispatchers with adequate tour and routing proposals. Computed tours were successfully validated by human dispatchers. Due to the promising results, Hellmann is highly interested in transferring the prototype to an application that optimizes the daily operations in numerous distribution centers. Finally, provide further research perspectives, and emphasize the advantages of the developed system in Industry 4.0 applications.",https://ieeexplore.ieee.org/document/6713745/,2013 10th International Conference and Expo on Emerging Technologies for a Smarter World (CEWIT),21-22 Oct. 2013,ieeexplore
10.1109/DCOSS49796.2020.00046,An Agnostic Data-Driven Approach to Predict Stoppages of Industrial Packing Machine in Near,IEEE,Conferences,"As data awareness in manufacturing companies increases with the deployment of sensors and Internet of Things (IoT) devices, data-driven maintenance and prediction have become quite popular in the Industry 4.0 paradigm. Machine Learning (ML) has been recognised as a promising, efficient and reliable tool for fault detection use cases, as it allows to export important knowledge from monitored assets. Scientists deal with issues such as the small amount of data that indicate potential problems, or the imbalance which exists between the standard process data and the data inadequacy of the systems to make a high precision forecast. Currently, in this context, even large industries are not able to effectively predict abnormal behaviors in their tools, processes and equipment, when adopting strategies to anticipate crucial events. In this paper, we propose a methodology to enable prediction of a packing machine's stoppages in manufacturing process of a large industry, by using forecasting techniques based on univariate time series data. There are more than 100 reasons that cause the machine to stop, in a quite big production line length. However, we use a single signal, concerning the machines operational status to make our prediction, without considering other fault or warning signals, hence its characterization as ""agnostic"". A workflow is presented for cleaning and preprocessing the data, and for training and evaluating a predictive model. Two predictive models, namely ARIMA and Prophet, are applied and evaluated on real data from an advanced machining process used for packing. Training and evaluation tests indicate that the results of the applied methods perform well on a daily basis. Our work can be further extended and act as reference for future research activities that could lead to more robust and accurate prediction frameworks.",https://ieeexplore.ieee.org/document/9183540/,2020 16th International Conference on Distributed Computing in Sensor Systems (DCOSS),25-27 May 2020,ieeexplore
10.1109/ICAPAI49758.2021.9462061,An Industrial Assistance System with Manual Assembly Step Recognition in Virtual Reality,IEEE,Conferences,"In the era of Industry 4.0, worker assistance systems are becoming more and more important. In order to assist shop floor workers in manual assembly tasks, we implemented an assistance system in virtual reality. A deep neural network was trained to recognize the current work step in real-time during an assembly process, thus giving the assistance system context-awareness. We defined the problem of assembly step recognition as a multivariate time series classification using the poses of the workers head, both hands and all relevant tools and objects. With this definition, the VR environment's output can also be replaced with data from the real world. For our proof-of-concept assembly step recognition system, we created an assembly process consisting of six different work steps, five movable assembly parts and one tool. We showed that we can train an activity recognition model for assembly steps with only 10 assembly recordings. To achieve this, we used multiple data augmentation techniques and proposed a novel method of synthesizing new training data, which we call Path Joining. With only 10 training recordings, we attain a categorical classification accuracy of 81 percent and with 60 recordings we achieve an accuracy of 89 percent.",https://ieeexplore.ieee.org/document/9462061/,2021 International Conference on Applied Artificial Intelligence (ICAPAI),19-21 May 2021,ieeexplore
10.1109/COASE.2018.8560557,An Industry 4.0 Cyber-Physical Framework for Micro Devices Assembly,IEEE,Conferences,"An advanced cyber manufacturing framework to support the collaborative assembly of micro devices is presented based on Industry 4.0 principles. The distributed cyber and physical components work together to plan, assemble and monitor micro assembly related tasks; micro assembly refers to the assembly of micron sized devices which cannot be manufactured by MEMS technologies. The collaborative framework proposed includes assembly planning and path planning modules, Virtual Reality based assembly simulation environments and physical assembly work cells. An ontology based approach was implemented to address semantic interoperability issues to support formation of temporary partnerships in a Virtual Enterprise context. The key to the design and implementation of this complex framework is an information centric process modeling approach which provides a data/information oriented basis for collaboration. A collaborative cyber physical test bed has been built to demonstrate feasibility of proposed framework and approach.",https://ieeexplore.ieee.org/document/8560557/,2018 IEEE 14th International Conference on Automation Science and Engineering (CASE),20-24 Aug. 2018,ieeexplore
10.1109/ISSE46696.2019.8984462,An IoT Reconfigurable SoC Platform for Computer Vision Applications,IEEE,Conferences,"The field of Internet of Things (IoT) and smart sensors has expanded rapidly in various fields of research and industrial applications. The area of IoT robotics has become a critical component in the evolution of Industry 4.0 standard. In this paper, we developed an IoT based reconfigurable System on Chip (SoC) robot that is fast and efficient for computer vision applications. It can be deployed in other IoT robotics applications and achieve its intended function. A Terasic Hexapod Spider Robot (TSR) was used with its DE0-Nano SoC board to implement our IoT robotics system. The TSR was designed to provide a competent computer vision application to recognize different shapes using a machine learning classifier. The data processing for image detection was divided into two parts, the first part involves hardware implementation on the SoC board and to provide real-time interaction of the robot with the surrounding environment. The second part of implementation is based on the cloud processing technique, where further data analysis was performed. The image detection algorithm for the computer vision component was tested and successfully implemented to recognize shapes. The TSR moves or reacts based on the detected image. The Field Programmable Gate Array (FPGA) part is programmed to handle the movement of the robot and the Hard Processor System (HPS) handles the shape recognition, Wi-Fi connectivity, and Bluetooth communication. This design is implemented, tested and can be used in real-time applications in harsh environments where movements of other robots are restricted.",https://ieeexplore.ieee.org/document/8984462/,2019 International Symposium on Systems Engineering (ISSE),1-3 Oct. 2019,ieeexplore
10.1109/SYSCON.2018.8369547,An interactive architecture for industrial scale prediction: Industry 4.0 adaptation of machine learning,IEEE,Conferences,"According to wiki definition, there are four design principles in Industry 4.0. These principles support companies in identifying and implementing Industry 4.0 scenarios, namely, Interoperability, Information transparency, Technical assistance, Decentralized decisions. In this paper we have discussed our work on an implementation of a machine learning based interactive architecture for industrial scale prediction for dynamic distribution of water resources across the continent, keeping the four corners of Industry 4.0 in place. We report the possibility of producing most probable high resolution estimation regarding the water balance in any region within Australia by implementation of an intelligent system that can integrate spatial-temporal data from various independent sensors and models, with the ground truth data produced by 250 practitioners from the irrigation industry across Australia. This architectural implementation on a cloud computing platform linked with a freely distributed mobile application, allowing interactive ground truthing of a machine learning model on a continental scale, shows accuracy of 90% with 85% sensitivity of correct surface soil moisture estimation with end users at its complete control. Along with high level of information transparency and interoperability, providing on-demand technical supports and motivating users by allowing them to customize and control their own local predictive models, show the successfulness of principles in Industry 4.0 in real environmental issues in the future adaptation in various industries starting from resource management to modern generation soft robotics.",https://ieeexplore.ieee.org/document/8369547/,2018 Annual IEEE International Systems Conference (SysCon),23-26 April 2018,ieeexplore
10.1109/NetSoft48620.2020.9165393,Benchmarking and Profiling 5G Verticals' Applications: An Industrial IoT Use Case,IEEE,Conferences,"The Industry 4.0 sector is evolving in a tremendous pace by introducing a set of industrial automation mechanisms tightly coupled with the exploitation of Internet of Things (IoT), 5G and Artificial Intelligence (AI) technologies. By combining such emerging technologies, interconnected sensors, instruments, and other industrial devices are networked together with industrial applications, formulating the Industrial IoT (IIoT) and aiming to improve the efficiency and reliability of the deployed applications and provide Quality of Service (QoS) guarantees. However, in a 5G era, efficient, reliable and highly performant applications' provision has to be combined with exploitation of capabilities offered by 5G networks. Optimal usage of the available resources has to be realised, while guaranteeing strict QoS requirements such as high data rates, ultra-low latency and jitter. The first step towards this direction is based on the accurate profiling of vertical industries' applications in terms of resources usage, capacity limits and reliability characteristics. To achieve so, in this paper we provide an integrated methodology and approach for benchmarking and profiling 5G vertical industries' applications. This approach covers the realisation of benchmarking experiments and the extraction of insights based on the analysis of the collected data. Such insights are considered the cornerstones for the development of AI models that can lead to optimal infrastructure usage along with assurance of high QoS provision. The detailed approach is applied in a real IIoT use case, leading to profiling of a set of 5G network functions.",https://ieeexplore.ieee.org/document/9165393/,2020 6th IEEE Conference on Network Softwarization (NetSoft),29 June-3 July 2020,ieeexplore
10.1109/ICAIIC51459.2021.9415189,Big Data Platform for Intelligence Industrial IoT Sensor Monitoring System Based on Edge Computing and AI,IEEE,Conferences,"The cutting edge of Industry 4.0 has driven everything to be converted to disruptive innovation and digitalized. This digital revolution is imprinted by modern and advanced technology that takes advantage of Big Data and Artificial Intelligence (AI) to nurture from automatic learning systems, smart city, smart energy, smart factory to the edge computing technology, and so on. To harness an appealing, noteworthy, and leading development in smart manufacturing industry, the modern industrial sciences and technologies such as Big Data, Artificial Intelligence, Internet of things, and Edge Computing have to be integrated cooperatively. Accordingly, a suggestion on the integration is presented in this paper. This proposed paper describes the design and implementation of big data platform for intelligence industrial internet of things sensor monitoring system and conveys a prediction of any upcoming errors beforehand. The architecture design is based on edge computing and artificial intelligence. To extend more precisely, industrial internet of things sensor here is about the condition monitoring sensor data - vibration, temperature, related humidity, and barometric pressure inside facility manufacturing factory.",https://ieeexplore.ieee.org/document/9415189/,2021 International Conference on Artificial Intelligence in Information and Communication (ICAIIC),13-16 April 2021,ieeexplore
10.1109/MED.2017.7984310,Cloud computing for big data analytics in the Process Control Industry,IEEE,Conferences,"The aim of this article is to present an example of a novel cloud computing infrastructure for big data analytics in the Process Control Industry. Latest innovations in the field of Process Analyzer Techniques (PAT), big data and wireless technologies have created a new environment in which almost all stages of the industrial process can be recorded and utilized, not only for safety, but also for real time optimization. Based on analysis of historical sensor data, machine learning based optimization models can be developed and deployed in real time closed control loops. However, still the local implementation of those systems requires a huge investment in hardware and software, as a direct result of the big data nature of sensors data being recorded continuously. The current technological advancements in cloud computing for big data processing, open new opportunities for the industry, while acting as an enabler for a significant reduction in costs, making the technology available to plants of all sizes. The main contribution of this article stems from the presentation for a fist time ever of a pilot cloud based architecture for the application of a data driven modeling and optimal control configuration for the field of Process Control. As it will be presented, these developments have been carried in close relationship with the process industry and pave a way for a generalized application of the cloud based approaches, towards the future of Industry 4.0.",https://ieeexplore.ieee.org/document/7984310/,2017 25th Mediterranean Conference on Control and Automation (MED),3-6 July 2017,ieeexplore
10.1109/WFCS.2019.8757952,Cloud-enabled Smart Data Collection in Shop Floor Environments for Industry 4.0,IEEE,Conferences,"Industry 4.0 transition is producing a remarkable change in the Smart Factories management. Modern companies can provide new services following products inside the shop floors along the entire production chain. To achieve the goal of servitization that the Industry 4.0 world requires, a modernization of current production chains is needed. This common demand comes mostly from manufacturing sector, where complex work machines collaborate with human workers. The data produced by the machines must be processed quickly, to allow the implementation of reactive services such as predictive maintenance, and remote control, always taking care of the safety of nearby people. This paper proposes a multi-layer architecture to monitor legacy production machines during their operations inside customers plants. The platform provides near real-time delivery of data collected from the machines with a high grade of customization according to customer needs. The performed tests show the scalability of the platform for a productive ecosystem with many machines at work, confirming its feasibility within different production facilities with different needs.",https://ieeexplore.ieee.org/document/8757952/,2019 15th IEEE International Workshop on Factory Communication Systems (WFCS),27-29 May 2019,ieeexplore
10.1109/WCNC45663.2020.9120761,Collaborative Learning Model for Cyberattack Detection Systems in IoT Industry 4.0,IEEE,Conferences,"Although the development of IoT Industry 4.0 has brought breakthrough achievements in many sectors, e.g., manufacturing, healthcare, and agriculture, it also raises many security issues to human beings due to a huge of emerging cybersecurity threats recently. In this paper, we propose a novel collaborative learning-based intrusion detection system which can be efficiently implemented in IoT Industry 4.0. In the system under consideration, we develop smart “filters” which can be deployed at the IoT gateways to promptly detect and prevent cyberattacks. In particular, each filter uses the collected data in its network to train its cyberattack detection model based on the deep learning algorithm. After that, the trained model will be shared with other IoT gateways to improve the accuracy in detecting intrusions in the whole system. In this way, not only the detection accuracy is improved, but our proposed system also can significantly reduce the information disclosure as well as network traffic in exchanging data among the IoT gateways. Through thorough simulations on real datasets, we show that the performance obtained by our proposed method can outperform those of the conventional machine learning methods.",https://ieeexplore.ieee.org/document/9120761/,2020 IEEE Wireless Communications and Networking Conference (WCNC),25-28 May 2020,ieeexplore
,Corporate Social Responsibility Challenges and Risks of Industry 4.0 technologies: A review,VDE,Conferences,"The fourth industrial revolution arrived with many enabling technologies that would impact important sociological aspects in the industry. Some of the Industry 4.0 technologies are already running in different industrial application, and other are still as a paradigm state. The social, economic, and environmental acceptance of Industry 4.0 technologies is still under discussion, which open new opportunities to execute various analysis about the possible implications of the implementation of such technologies. This article refers to an exploratory analysis and identification of the different challenges and risks of this new Industry 4.0 paradigm and its related technologies. The technologies under review were Internet of Things, Artificial Intelligence, Cloud Computing, cybersecurity, bid data, blockchain, 5G, robotics, adding manufacturing, unmanned systems, autonomous vehicles, virtual reality, and augmented reality. As a result, different social challenges and risks were identified for each technology, starting from vulnerability, implementation cost, until social aspects such as education and unemployment caused by those new technologies. In conclusion, Industry 4.0 arrived with a lot of benefits to the industry business, but companies should not stop thinking about sustainable development.",https://ieeexplore.ieee.org/document/8835964/,"Smart SysTech 2019; European Conference on Smart Objects, Systems and Technologies",4-5 June 2019,ieeexplore
10.1109/COINS51742.2021.9524088,Cyber Security Risks of Technical Components in Industry 4.0,IEEE,Conferences,"Industry 4.0 is a new concept of automation data exchange in manufacturing. Industry 4.0 consists of various technical components such as Artificial Intelligence (AI), IoT, OPC UA, and cloud. There could be cyber risks when these technical components are used in Industrial Control System (ICS) without security consideration. Therefore cyber risk assessment, reconsideration of implementation, operation are necessary to adopt Industry 4.0. Problems of previous research for ICS risk assessments are some of them are theoretical and conceptual methods such as simulations. Moreover, they do not focus on Industry 4.0 components.The objective of this research is to prove the concrete cyber risks and introduces corresponding secure implementations for Industry 4.0 components such as Artificial Intelligence (AI), the Internet of Things (IoT), OPC Unified Architecture (OPC UA). The proposed method has benefits for clarifying impacts on the real world by cyber-attacks through penetration tests against ICS testbed with actual machines.",https://ieeexplore.ieee.org/document/9524088/,2021 IEEE International Conference on Omni-Layer Intelligent Systems (COINS),23-25 Aug. 2021,ieeexplore
10.1109/GUCON50781.2021.9573994,Cyber Warfare Threat Categorization on CPS by Dark Web Terrorist,IEEE,Conferences,"The Industrial Internet of Things (IIoT) also referred as Cyber Physical Systems (CPS) as critical elements, expected to play a key role in Industry 4.0 and always been vulnerable to cyber-attacks and vulnerabilities. Terrorists use cyber vulnerability as weapons for mass destruction. The dark web's strong transparency and hard-to-track systems offer a safe haven for criminal activity. On the dark web (DW), there is a wide variety of illicit material that is posted regularly. For supervised training, large-scale web pages are used in traditional DW categorization. However, new study is being hampered by the impossibility of gathering sufficiently illicit DW material and the time spent manually tagging web pages. We suggest a system for accurately classifying criminal activity on the DW in this article. Rather than depending on the vast DW training package, we used authorized regulatory to various types of illicit activity for training Machine Learning (ML) classifiers and get appreciable categorization results. Espionage, Sabotage, Electrical power grid, Propaganda and Economic disruption are the cyber warfare motivations and We choose appropriate data from the open source links for supervised Learning and run a categorization experiment on the illicit material obtained from the actual DW. The results shows that in the experimental setting, using TF-IDF function extraction and a AdaBoost classifier, we were able to achieve an accuracy of 0.942. Our method enables the researchers and System authoritarian agency to verify if their DW corpus includes such illicit activity depending on the applicable rules of the illicit categories they are interested in, allowing them to identify and track possible illicit websites in real time. Because broad training set and expert-supplied seed keywords are not required, this categorization approach offers another option for defining illicit activities on the DW.",https://ieeexplore.ieee.org/document/9573994/,"2021 IEEE 4th International Conference on Computing, Power and Communication Technologies (GUCON)",24-26 Sept. 2021,ieeexplore
10.1109/ICIAI.2019.8850773,Data Augmentation for Intelligent Manufacturing with Generative Adversarial Framework,IEEE,Conferences,"The global economy is greatly shaped by the unprecedented booming of ICT and artificial intelligence technologies. Their applications in manufacturing has led to the advent of intelligent manufacturing and industry 4.0. Data has become a precious asset for modern industry. This paper first introduces an energy monitoring and data acquisition system namely the Point Energy Technology, which has been developed by the team and installed in several industrial partners, including a local bakery. The lack of data always exists due to various reasons, such as measurement or transmission errors at data collection and transmission stage, leading to the loss of varied length of data samples that are key for process monitoring and control. To solve this problem, we introduce a generative adversarial framework which is based on a game theory for data augmentation. This framework consists of two multilayer perceptron networks, namely generator and discriminator. An improved framework with Q-net that extracts the latent variables from real data is also proposed, in which the Q-net shares the structure with discriminator except for the last layer. In addition, the two optimization methods, namely mini-batch gradient descent and adaptive moment estimation are adopted to tune the parameters. To evaluate the performance of these algorithms, energy consumption data collected from a bakery process is used in the experiment. The experimental results confirm that the latent generative adversarial framework with adaptive moment estimation could generate good quality data samples to compensate the random loss of samples in time series data.",https://ieeexplore.ieee.org/document/8850773/,2019 1st International Conference on Industrial Artificial Intelligence (IAI),23-27 July 2019,ieeexplore
10.1109/IAI50351.2020.9262203,Digital Twin Enabled Smart Control Engineering as an Industrial AI: A New Framework and Case Study,IEEE,Conferences,"In Industry 4.0, the increasing complexity of industrial systems introduces unknown dynamics that affect the performance of manufacturing processes. Thus, Digital Twin appears as a breaking technology to develop virtual representations of any complex system design, analysis, and behavior prediction tasks to enhance the system understanding via enabling capabilities like real-time analytics, or Smart Control Engineering. In this paper, a novel framework is proposed for the design and implementation of Digital Twin applications to the development of Smart Control Engineering. The framework involve the steps of system documentation, Multidomain Simulation, Behavioral Matching, and real-time monitoring, which is applied to develop the Digital Twin for a real-time vision feedback temperature uniformity control. The obtained results show that Digital Twin is a fundamental part of the transformation into Industry 4.0.",https://ieeexplore.ieee.org/document/9262203/,2020 2nd International Conference on Industrial Artificial Intelligence (IAI),23-25 Oct. 2020,ieeexplore
10.1109/DESSERT.2018.8409186,Digitization of the economy of Ukraine: Strategic challenges and implementation technologies,IEEE,Conferences,"The main directions, challenges, threats of digitization of the national economy of Ukraine have been considered in the paper. The attention is focused on the found weaknesses and the imperfection of the strategy and the state policy of digitization of Ukraine's economy. The authors have proven the potential and new possibilities of solving public finance management problems with the usage of blockchain technology. It has been justified that activation of transformation processes in the real economy sector due to the introduction of Industry 4.0 concept is important for Ukraine. The paper reveals basic principles and technologies, the experience of the European Union, and characterizes Industry 4.0 view in Ukraine. The development of the latest financial technologies - FinTech - has been recognized as the driver of digital transformation of financial services. The types of FinTech innovations, the features of increasing competition between FinTech companies and traditional financial intermediaries, the tendencies of FinTech development in Ukraine have been characterized.",https://ieeexplore.ieee.org/document/8409186/,"2018 IEEE 9th International Conference on Dependable Systems, Services and Technologies (DESSERT)",24-27 May 2018,ieeexplore
10.1109/CSIT49958.2020.9321954,Eco-friendly Home Automation System Implemented Using Machine Learning Algorithms,IEEE,Conferences,"This paper presents the exemplary system of house automation implemented with the use of Industry 4.0 inventions. The proposed system tries to benefit from weather conditions to heat or cool the house without any electrical heaters or air conditioners. It is implemented with the aid of Machine Learning algorithms, the Internet of Things, and Cloud technology. The paper contains a technical and practical description of the system, the results of the real use, and proposed extensions that can improve the presented solution.",https://ieeexplore.ieee.org/document/9321954/,2020 IEEE 15th International Conference on Computer Sciences and Information Technologies (CSIT),23-26 Sept. 2020,ieeexplore
10.1109/CSCloud-EdgeCom49738.2020.00050,Edge Computing-based 3D Pose Estimation and Calibration for Robot Arms,IEEE,Conferences,"Industrial robots are widely used in current production lines, and complex pipeline processes, especially those with different assembly requirements, are designed for intelligent manufacturing in the era of industry 4.0. During the new crown epidemic, a large number of car companies used the production line to transform production of medical materials such as masks and protective clothing, which provided a strong guarantee for fighting the epidemic. In this scenario, a pipeline is often assembled from robotic arms from multiple suppliers. The traditional methods is complex and takes a lot of time. In this paper, we propose a novel deep learning based robot arm 3D pose estimation and calibration model with simple Kinect stereo cameras which can be deployed on light-weight edge computing systems. The light-weight deep CNN model can detection 5 predefined key points based on RGB-D data. In this way, when the assembly line composed of different robot arms needs to be reassembled, our model can quickly provide the robot's pose information without additional tuning processes. Testing in Webots with Rokae xb4 robot arm model shows that our model can quickly estimate the key point of the robot arm.",https://ieeexplore.ieee.org/document/9170983/,2020 7th IEEE International Conference on Cyber Security and Cloud Computing (CSCloud)/2020 6th IEEE International Conference on Edge Computing and Scalable Cloud (EdgeCom),1-3 Aug. 2020,ieeexplore
10.1109/EuCNC/6GSummit51104.2021.9482590,Empowering Industry 4.0 and Autonomous Drone Scouting use cases through 5G-DIVE Solution,IEEE,Conferences,"The 5G Edge Intelligence for Vertical Experimentation (5G-DIVE) project aims at demonstrating the technical merits and business value proposition of 5G technologies in two vertical pilots, namely the Industry 4.0 (I4.0) and Autonomous Drones Scout (ADS) pilots. This paper presents an overview of the overall 5G-DIVE solution and reports the results of the initial validation campaign of the selected use case, featuring 5G connectivity, distributed Edge computing, and artificial intelligence. The initial results for the I4.0 provide a baseline for next step validation campaign targeting a broader scale 5G implementation, while the ADS results provides promising results for enhancing the autonomous navigation in real-time.",https://ieeexplore.ieee.org/document/9482590/,2021 Joint European Conference on Networks and Communications & 6G Summit (EuCNC/6G Summit),8-11 June 2021,ieeexplore
10.1109/ICE/ITMC49519.2020.9198492,Enhancing Cognition for Digital Twins,IEEE,Conferences,"In the era of Industry 4.0, Digital Twins (DTs) pave the way for the creation of the Cognitive Factory. By virtualizing and twinning information stemming from the real and the digital world, it is now possible to connect all parts of the production process by having virtual copies of physical elements interacting with each other in the digital and physical realms. However, this alone does not imply cognition. Cognition requires modelling not only the physical characteristics but also the behavior of production elements and processes. The latter can be founded upon data-driven models produced via Data Analytics and Machine Learning techniques, giving rise to the so-called Cognitive (Digital) Twin. To further enable the Cognitive Factory, a novel concept, dubbed as Enhanced Cognitive Twin (ECT), is proposed in this paper as a way to introduce advanced cognitive capabilities to the DT artefact that enable supporting decisions, with the end goal to enable DTs to react to inner or outer stimuli. The Enhanced Cognitive Twin can be deployed at different hierarchical levels of the production process, i.e., at sensor-, machine-, process-, employee- or even factory-level, aggregated to allow both horizontal and vertical interplay. The ECT notion is proposed in the context of process industries, where cognition is particularly important due to the continuous, non-linear, and varied nature of the respective production processes.",https://ieeexplore.ieee.org/document/9198492/,"2020 IEEE International Conference on Engineering, Technology and Innovation (ICE/ITMC)",15-17 June 2020,ieeexplore
10.1109/SMC.2019.8913901,Explainable Machine Learning in Industry 4.0: Evaluating Feature Importance in Anomaly Detection to Enable Root Cause Analysis,IEEE,Conferences,"In the past recent years, Machine Learning methodologies have been applied in countless application areas. In particular, they play a key role in enabling Industry 4.0. However, one of the main obstacles to the diffusion of Machine Learning-based applications is related to the lack of interpretability of most of these methods. In this work, we propose an approach for defining a `feature importance' in Anomaly Detection problems. Anomaly Detection is an important Machine Learning task that has an enormous applicability in industrial scenarios. Indeed, it is extremely relevant for the purpose of quality monitoring. Moreover, it is often the first step towards the design of a Machine Learning-based smart monitoring solution because Anomaly Detection can be implemented without the need of labelled data. The proposed feature importance evaluation approach is designed for Isolation Forest, one of the most commonly used algorithm for Anomaly Detection. The efficacy of the proposed method is tested on synthetic and real industrial datasets.",https://ieeexplore.ieee.org/document/8913901/,"2019 IEEE International Conference on Systems, Man and Cybernetics (SMC)",6-9 Oct. 2019,ieeexplore
10.1109/ICS51289.2020.00088,Feature Selection for Malicious Traffic Detection with Machine Learning,IEEE,Conferences,"The network technology plays an important role in the emerging industry 4.0. Industrial control systems (ICS) are related to all aspects of human life and have become the target of cyber-attackers. Attacks on ICS may not only cause economic loss, but also damage equipment and hurt staff. The biggest challenges in establishing a secure network communication system is how to effectively detect and prevent malicious network behavior. A Network Intrusion Detection System (NIDS) can be deployed as a defense mechanism for cyberattacks. However, for industrial internet-of-things (IIoT) applications with limited computing resources, designing an effective NIDS is challenging. In this paper, we propose to use machine learning as the core technology to build a compact and effective NIDS for IIoT. The proposed method is validated by using the more recent UNSW-NB 15 dataset to improve the detection capability against new types of attacks in the real world. Furthermore, we demonstrate that the method is also valid for traditional KDD-CUP-99 dataset. Experimental results show that the proposed method achieves better performance than previous methods.",https://ieeexplore.ieee.org/document/9359069/,2020 International Computer Symposium (ICS),17-19 Dec. 2020,ieeexplore
10.15439/2017F253,Human machine synergies in intra-logistics: Creating a hybrid network for research and technologies,IEEE,Conferences,"The purpose of the article is to outline the futuristic vision of Industry 4.0 in intra-logistics by creating a hybrid network for research and technologies thereby providing a detailed account on the research centre, available technologies and their possibilities for collaboration. Scientific challenges in the field of Industry 4.0 and intra-logistics are identified due to the new form of interaction between humans and machines. This kind of collaboration provides new possibilities of materials handling that can be developed with the support of real-time motion data tracking and virtual reality systems. These services will be provided by a new research centre for flexible human-machine cooperation networks in Dortmund. By the use of various reference and experiment systems various real-time scenarios can be emulated including digital twin simulation concepts. Big data emerges as an important paradigm in this research project where all systems are made flexible in terms of networking for all the systems to consume the data produced and also to combine all the data to arrive at new insights using concepts from machine learning and deep learning networks. This leads to the challenge of finding a common syntax for inter-operating systems. This paper describes the design and deployment strategies of research centre with the possibilities and the design insights for a futuristic Industry 4.0 material handling facility.",https://ieeexplore.ieee.org/document/8104684/,2017 Federated Conference on Computer Science and Information Systems (FedCSIS),3-6 Sept. 2017,ieeexplore
10.1109/CIIMA50553.2020.9290302,Implementación de SCADA a través del protocolo MQTT,IEEE,Conferences,"This document describes an implementation of a SCADA system powered by MQTT &amp; OPC-UA protocols and hosted within the Google Cloud Platform system. This combination allows to have integrated, scalable, secure and reliable industrial communications while allowing real-time data acquisition and sensor feed that can then be used in real-time OEE tracking or predictive maintenance models, to name some examples. This in line with the Industry 4.0 initiatives mainly fueled by data and machine learning autonomous systems.",https://ieeexplore.ieee.org/document/9290302/,2020 IX International Congress of Mechatronics Engineering and Automation (CIIMA),4-6 Nov. 2020,ieeexplore
10.1109/AIMS52415.2021.9466068,Implementation of Cloud Based Action Recognition Backend Platform,IEEE,Conferences,"The Internet of Things (IoT) growth are rapidly in various fields such as industry 4.0, smart cities, and smart homes. Implementation of IoT for electronic assistance had been researched to increase the longevity of human life. However, not all IoT implementation as human life assistance provides action recognition monitoring on multiple elderly people, provide information such as real-time action monitoring, and real-time streaming in a mobile application. Therefore, this research intends to create a system that can receive and provide information on each elderly people who registered. The Action Recognition Backend Platform will be working as cloud computing to receive and manage input data from Edge Computing Action Recognition. This platform integrated Deep Learning, Data Analytics, Big Data Warehouse that implemented Extract, Transform, and Load (ETL) methods, communication services with MQTT, and Kafka Streaming Processor. The test result showed that the edge computing action recognition got better model accuracy performance from our last model [1], which can predict with 50,7% accuracy in 0.5 confidence threshold. Moreover, the backend platform had been successfully implemented a simple IoT paradigm and got an average delivery time of MQTT communication at 204ms, for streaming data process took an average delay of 680ms.",https://ieeexplore.ieee.org/document/9466068/,2021 International Conference on Artificial Intelligence and Mechatronics Systems (AIMS),28-30 April 2021,ieeexplore
10.1109/COMPSAC.2018.10204,Indoor Augmented Reality Using Deep Learning for Industry 4.0 Smart Factories,IEEE,Conferences,"This paper proposes to design, develop and implement a fast and markerless mobile augmented reality system to achieve the registration for, the visualization of, and the interaction with machines in indoor smart factories with Industry 4.0 vision. A lightweight deep-learning image detection module based on MobileNets running on mobile devices is used to detect/recognize different machines and different portions of machines. Internet of Things (IoT) networking allows machines and sensors in machines to report data, such as machine settings and machine states, to the cloud-side server. Thus, augmented information associated with a machine portion can be derived from the server and superimposed with the portion image shown on the device display. Furthermore, interaction methods based on touch gestures and distance calculation are also implemented. A prototype system is developed and tested in a mechanical workshop for the purpose of validation and evaluation. The system is shown to achieve high detection accuracy, intuitive visualization, and unique interaction modes.",https://ieeexplore.ieee.org/document/8377831/,2018 IEEE 42nd Annual Computer Software and Applications Conference (COMPSAC),23-27 July 2018,ieeexplore
10.1109/GIOTS49054.2020.9119497,Industrial IoT and Digital Twins for a Smart Factory : An open source toolkit for application design and benchmarking,IEEE,Conferences,"The rapid evolution of digital technology and designed intelligence, such as the Internet of Things (IoT), Big data analytics, Artificial Intelligence (AI), Cyber Physical Systems (CPS), has been a catalyst for the 4th industrial revolution (known as industry 4.0). Among other, the two key state-of-the-art concepts in Industry 4.0, are Industrial IoT (IIoT) and digital twins. IIoT facilitates real-time data acquisition, processing and analytics over large amount of sensor data streams produced by sensors installed within a smart factory, while the `digital twin' concept aims to enable smart factories via the digital replication or representation of physical machines, processes, people in cyber-space. This paper explores the capability of present-state open-source platforms to collectively achieve digital twin capabilities, including IoT real-time data acquisition, virtual representation, analytics, and visualisation. The aim of this work is to `close the gap' between research and implementation, through a collective open source IoT and Digital Twin architecture. The performance of the open-source architecture in this work, is demonstrated in a use-case utilising industry `open data', and is bench-marked with universal testing tools.",https://ieeexplore.ieee.org/document/9119497/,2020 Global Internet of Things Summit (GIoTS),3-3 June 2020,ieeexplore
10.1109/RTSI.2019.8895598,Intelligent Embedded Load Detection at the Edge on Industry 4.0 Powertrains Applications,IEEE,Conferences,"In the context of Industry 4.0, there has been great focus in developing intelligent sensors. Deploying them, condition monitoring and predictive maintenance have become feasible solutions to minimize operating and maintenance costs while also increasing safety. A critical aspect is the applied load to the supervised machinery system. Vibration data can be used to determine the current condition, but this needs signal processing specially developed and adapted to the monitored machine part for feature extraction. Artificial intelligence (AI) can, on one hand, simplify the development of such special purpose processing and on another hand be used to monitor and classify machine conditions by learning features directly from data. By bringing the AI computation as close as possible to the sensor (Edge-AI), data bandwidth can be minimized, system scalability and responsiveness can be improved and real-time requirements can be fulfilled. This work describes how Edge-AI on a STM32-bit microcontroller can be implemented. Our experimental setup demonstrates how AI can be effectively used to detect and classify the load on a powertrain. In order to do this, we use a MEMS capacity accelerometer to sense vibrations of the system. Also, this work demonstrates how Deep Neural Networks (DNN) for signal classification are build and trained by using an open-source deep learning framework and how the code library for the microcontroller is automatically generated afterwards by using STM32Cube. AI toolchain. We compare the classification accuracy of a memory compressed DNN against an uncompressed DNN.",https://ieeexplore.ieee.org/document/8895598/,2019 IEEE 5th International forum on Research and Technology for Society and Industry (RTSI),9-12 Sept. 2019,ieeexplore
10.1109/AIM46487.2021.9517377,Introducing adaptive mechatronic designs in bulk handling industry,IEEE,Conferences,"The advances of mechatronic system design and system integration have shown improvements in functionality, performance and energy efficiency in many applications across industries, from autonomous ground vehicles and drones to conveyor belts. This trend has been adopted in some industries more than others. The design of equipment to handle granular or bulk material is commonly based on traditional approaches. Therefore, introducing mechatronic concepts in the design procedure can enable new possibilities, such as sensor integration and data analyses, adaptability and control. The efficiency of bulk material handling equipment in ports, agriculture and food processing is heavily influenced by the operational conditions. Typically, a piece of equipment is designed for defined operational conditions when the maximum performance can be achieved. In this work the concept of adaptability to varying operational conditions is explored by understanding the technologies implemented in other industries and the feasibility to be implemented in the bulk handling equipment design. Sensing technology, actuation and adaptability are systematically presented in this work to support the design process of the next generation of bulk handling equipment. This will pave the way for incorporating the technological trends in the design, such as: sustainability, “smartness”, Internet of Things, Industry 4.0, digital twin and machine learning. Adaptive mechatronic solutions will play a crucial role in generating and implementing innovative sustainable solutions for bulk handling equipment.",https://ieeexplore.ieee.org/document/9517377/,2021 IEEE/ASME International Conference on Advanced Intelligent Mechatronics (AIM),12-16 July 2021,ieeexplore
10.1109/IGSC48788.2019.8957164,IoT/CPS Ecosystem for Efficient Electricity Consumption : Invited Paper,IEEE,Conferences,"Modern society relies on smart systems like internet of things (IoT) and cyber physical systems (CPS) to monitor and control physical processes. The widespread deployment of IoT and CPSs result in fast growth of sensor data as physical processes are constantly monitored by billions of IP-enabled sensors (44 zettabytes by 2020). Hence, fog nodes are deployed to make network edge rich in computing resources to enable real-time data analytics using artificial intelligence/machine learning (AI/ML) for Big data generated from IoT and CPSs. This paper proposes IoT/CPS ecosystem for smart grid (SG) utilizing industry 4.0 concept to manage and control the loads using an intelligent predictive controller based on artificial neural network (ANN). The ANN is trained to predict the loads in certain districts based on previous smart meter readings installed at consumers and substations. This is a novel approach which integrates IoT/CPSs ecosystem into electric power system to deliver energy to consumers with high efficiency, reduce the cost, optimize the energy consumption, improve the reliability and enable real-time monitoring of power consumption.",https://ieeexplore.ieee.org/document/8957164/,2019 Tenth International Green and Sustainable Computing Conference (IGSC),21-24 Oct. 2019,ieeexplore
10.1109/SCC47175.2019.9116104,Modeling and management of human resources in the reconfiguration of production system in industry 4.0 by neural networks,IEEE,Conferences,"In Industry 4.0, the role of employees changes significantly. Real-time production line control transforms job content. Work processes affect working conditions. The implementation of a socio-technical approach to the organization of work gives workers the opportunity to adapt their skills. Indeed, production work will become more and more multi-factor, especially with regard to control and decision-making tasks. In this paper, a proposal for an intelligent system for modeling skills and human resource management in the production system chain through the use of two artificial neural networks. The first NN1 network allows for the identification of the human factor, as well as the second NN2 network is reserved for valuing the human skills needed in Industry 4.0.",https://ieeexplore.ieee.org/document/9116104/,"2019 International Conference on Signal, Control and Communication (SCC)",16-18 Dec. 2019,ieeexplore
10.23919/DATE48585.2020.9116407,Network Synthesis for Industry 4.0,IEEE,Conferences,"Today's factory machines are ever more connected with SCADA, MES, ERP applications as well as external systems for data analysis. Different types of network architectures must be used for this purpose. For instance, control applications at the lowest level are susceptible to delays and errors while data analysis with machine learning procedures requires to move a large amount of data without real-time constraints. Standard data formats, like Automation Markup Language (AML), have been established to document factory environment, machine placement and network deployment, however, no automatic technique is currently available in the context of Industry 4.0 to choose the best mix of network architectures according to spacial constraints, cost, and performance. We propose to fill this gap by formulating an optimization problem. First of all, spatial and communication requirements are extracted from the AML description. Then, the optimal interconnection of wired or wireless channels is obtained according to application objectives. Finally, this result is back-annotated to AML to be used in the life cycle of the production system. The proposed methodology is described through a small, but complete, smart production plant.",https://ieeexplore.ieee.org/document/9116407/,"2020 Design, Automation & Test in Europe Conference & Exhibition (DATE)",9-13 March 2020,ieeexplore
10.1109/NCC52529.2021.9530062,On Traffic Classification in Enterprise Wireless Networks,IEEE,Conferences,"Enterprises today are quickly adopting intelligent, adaptive, and flexible wireless communication technologies in order to become compliant with Industry 4.0. One of the technological challenges related to this is to provide Quality of Services (QoS)-enabled network connectivity to the applications. Diverse QoS demands from the applications intimidate the underlying wireless networks to be agile and adaptive. Since the applications are diverse in nature, there must be a mechanism to learn the application types in near real-time so that the network can be provisioned accordingly. In this paper, we propose a Machine Learning (ML) based method to classify the application traffic. Our method is different from the existing port based and Deep Packet Inspection (DPI) based methods and uses statistical features of the network traffic related to the applications. We validate the performance of the proposed model in a lab based SDNized WiFi set-up. SDNization ensures that the proposed model can be deployed in practice.",https://ieeexplore.ieee.org/document/9530062/,2021 National Conference on Communications (NCC),27-30 July 2021,ieeexplore
10.23919/SpliTech49282.2020.9243735,PADL: a Language for the Operationalization of Distributed Analytical Pipelines over Edge/Fog Computing Environments,IEEE,Conferences,"In this paper we introduce PADL, a language for modeling and deploying data-based analytical pipelines. The novelty of this language relies on its independence from both the infrastructure and the technologies used on it. Specifically, this descriptive language aims at embracing all the particularities and constraints of high-demanding deployment models, such as critical restrictions regarding latency, privacy and performance, by providing fully-compliant schemas for implementing data analytical workloads. The adoption of PADL provides means for the operationalization of these pipelines in a reproducible and resilient fashion. In addition, PADL is able to fully utilize the benefits of Edge and Fog computing layers. The feasibility of the language has been validated with an analytical pipeline deployed over an Edge computing environment to solve an Industry 4.0 use case. The promising results obtained therefrom pave the way towards the widespread adoption of our proposed language when deploying data analytical pipelines over real application scenarios.",https://ieeexplore.ieee.org/document/9243735/,2020 5th International Conference on Smart and Sustainable Technologies (SpliTech),23-26 Sept. 2020,ieeexplore
10.1109/BigDataCongress.2019.00032,"PREMISES, a Scalable Data-Driven Service to Predict Alarms in Slowly-Degrading Multi-Cycle Industrial Processes",IEEE,Conferences,"In recent years, the number of industry-4.0-enabled manufacturing sites has been continuously growing, and both the quantity and variety of signals and data collected in plants are increasing at an unprecedented rate. At the same time, the demand of Big Data processing platforms and analytical tools tailored to manufacturing environments has become more and more prominent. Manufacturing companies are collecting huge amounts of information during the production process through a plethora of sensors and networks. To extract value and actionable knowledge from such precious repositories, suitable data-driven approaches are required. They are expected to improve the production processes by reducing maintenance costs, reliably predicting equipment failures, and avoiding quality degradation. To this aim, Machine Learning techniques tailored for predictive maintenance analysis have been adopted in PREMISES (PREdictive Maintenance service for Industrial procesSES), an innovative framework providing a scalable Big Data service able to predict alarming conditions in slowly-degrading processes characterized by cyclic procedures. PREMISES has been experimentally tested and validated on a real industrial use case, resulting efficient and effective in predicting alarms. The framework has been designed to address the main Big Data and industrial requirements, by being developed on a solid and scalable processing framework, Apache Spark, and supporting the deployment on modularized containers, specifically upon the Docker technology stack.",https://ieeexplore.ieee.org/document/8818217/,2019 IEEE International Congress on Big Data (BigDataCongress),8-13 July 2019,ieeexplore
10.1109/SMILE.2018.8353980,Parametric study and design of deep learning on leveling system for smart manufacturing,IEEE,Conferences,"Sheet metal is widely used in the industry for metal forming purposes, such as metal stamping and metal cutting. It is often winded and storage in a coil form for transportation purposes. However, before any manufacturing process such as, cutting, or stamping, leveling is required as the residual stress inside coil is present which can cause distortion to the metal forming/cutting process. In conventional coil leveling machines, the machine parameters are often set by machine technicians with many years of experiences. In addition, the optimized machine parameter is achieved by trial and error method or based on experiences. However, the machine parameters are also not exactly trivial due to too many input factors which may cause changes to the outcome result. In the recent years, industry 4.0 and smart manufacturing has been a widely discussed topic in terms of industry manufacturing solutions in many different industrialized countries. In smart manufacturing, communication and interaction between machines have become an important role to improve manufacturing efficiency, flexibility and customization. As smart manufacturing focused on information process through real objects, it is required to digitize the experience through deep learning method. This paper is aimed to describe and study the deep learning application based on coil leveling system. Finally, through this study and experiment verification, analyzes on research directions and prospects of deep learning.",https://ieeexplore.ieee.org/document/8353980/,"2018 IEEE International Conference on Smart Manufacturing, Industrial & Logistics Engineering (SMILE)",8-9 Feb. 2018,ieeexplore
10.1109/ICRITO51393.2021.9596436,Prediction of Sensor Faults and Outliers in IoT Devices,IEEE,Conferences,"Internet of Things (IoT) is tremendously growing and interacting with the physical world in the era of Industry 4.0. In near future, billions of companies will have advanced communication technology and it will increase the growth of critical systems. The accuracy measurement of the functionality of a critical system is a challenging job. Fault Tolerance (FT) is a major concern to ensure the dependability, availability and reliability of critical systems. Faults should be predicted and controlled proactively to lessen failure impact on the critical systems. To predict these failures and use the relevant procedure to avoid it before it actually occurs, FT techniques are used. These techniques are implemented in critical systems to avoid failures as the security of systems is more important than the reliability of systems. It minimizes the effect of faults that are being investigated. FT techniques work on a concept that if the system is built differently then it should fail differently. If a redundant variant fails then atleast the other one should give a satisfactory result. This study exhibits an analysis of existing FT techniques like N-version programming, Recovery blocks and N-self-checking programming. A critical study of sensor faults and outliers prediction models in IoT is presented. A bibliometric analysis is also carried out on 716 Scopus indexed publications to analyze the current research trends in this domain.",https://ieeexplore.ieee.org/document/9596436/,"2021 9th International Conference on Reliability, Infocom Technologies and Optimization (Trends and Future Directions) (ICRITO)",3-4 Sept. 2021,ieeexplore
10.1109/GloSIC.2018.8570124,Probabilistic Estimations of Increasing Expected Reliability and Safety for Intelligent Manufacturing,IEEE,Conferences,"In the near future the possibilities of the modern probabilistic models, artificial intelligence and machine learning methods can provide an intelligent support of making decisions by an operator in real time. An agile recovery of intelligent manufacturing integrity can be implemented owing to the development of industrial robotics. For intelligent manufacturing it means the expected reliability and safety may be in the near future at the expense of intelligent support of decision making and the agile recovery of integrity. To answer the question “How much essential may be this increasing?” here are proposed: general analytical approaches for a probabilistic estimation of the expected reliability and safety for every monitored element or the system of intelligent manufacturing on a level of probability distribution functions (PDF) of the time between the losses of system integrity; estimations of increasing the expected reliability and safety for intelligent manufacturing at the expense of the intelligent support of decision making and agile recovery of integrity; the comparisons of the estimations on a prognostic period up to 10 years using the identical model in applications to expected reliability and safety. The applications of the proposed approaches allow the customers, designers, developers, users and experts of Industry 4.0 intelligent manufacturing to be guided by the proposed probabilistic estimations for solving problems of reliability and safety in the system life cycle. The results are demonstrated by examples.",https://ieeexplore.ieee.org/document/8570124/,2018 Global Smart Industry Conference (GloSIC),13-15 Nov. 2018,ieeexplore
10.1109/PC.2017.7976254,Proposal of system for automatic weld evaluation,IEEE,Conferences,"The paper deals with the development of a system for automatic weld recognition using new information technologies based on cloud computing and single-board computer in the context of Industry 4.0. The proposed system is based on a visual system for weld recognition, and a neural network based on cloud computing for real-time weld evaluation, both implemented on a single-board low-cost computer. The proposed system was successfully verified on welding samples which correspond to a real welding process in the car production process. The system considerably contributes to the welds diagnostics in industrial processes of small- and medium-sized enterprises.",https://ieeexplore.ieee.org/document/7976254/,2017 21st International Conference on Process Control (PC),6-9 June 2017,ieeexplore
10.1109/SACI51354.2021.9465544,Real-time locating system and digital twin in Lean 4.0,IEEE,Conferences,"Digital twin plays a key role in the current development of smart manufacturing systems. Through simulation in the cyber world, real phenomena in the physical world can be predicted and optimized before the final implementation. The usage of the digital twin is enhanced along with the uprising of Industry 4.0, in which data availability supports the further insight of system status, helping the operation managers understand their system and perform resources adjustment more easily. Based on this digitization mature, Lean 4.0, a new concept elaborated from Lean manufacturing, has been interested recently. There are several technologies constituted digital twin that provide a favourable condition for Lean 4.0, such as augmented reality, cloud computing. In this paper, the integration of the Real-time Locating System (RTLS) into digital twin is proposed, which facilitates the performance of Lean 4.0 in manufacturing operation. Not only gain effective control over the facility's assets, but this integration also enhances the resources utilization, cut down operational wastes, thus brings a better turnover for industrial systems. A case study of successful implementation is shown, which proved the possible advantages of this approach.",https://ieeexplore.ieee.org/document/9465544/,2021 IEEE 15th International Symposium on Applied Computational Intelligence and Informatics (SACI),19-21 May 2021,ieeexplore
10.1109/ICE/ITMC52061.2021.9570221,Resilient Manufacturing Systems enabled by AI support to AR equipped operator,IEEE,Conferences,"Supply chains and manufacturing systems robustness and resilience are, for many years, but especially nowadays, key features requested to ensure reliable and efficient production processes. Two domains are crucial to achieve such purpose: the former is fast and comprehensive monitoring, efficient and reliable condition detection and effective and explicable support for decision making. The latter refers to the intervention by operators, able to better identify problems and to put in place effective operations aimed at fixing it or, better, to prevent such circumstances. This paper presents an integrated approach encompassing a sophisticated IoT and AI-based approach to monitor and detect critical situations, fully integrated with an AR (Augmented Reality) system supporting operators in the field to take informed actions in bi-directional continuous connection. Activities in the context of EC funded project Qu4lity developed in Politecnico di Milano Industry 4.0 Lab, a test environment implementing the proposed approach and demonstrating in an automated production line the effectiveness of the approach, significantly improving performances. Analysis of performance indicators demonstrates the soundness of the proposed solution and implementation methodology to make the overall production process more resilient, efficient and with product defects reduction.",https://ieeexplore.ieee.org/document/9570221/,"2021 IEEE International Conference on Engineering, Technology and Innovation (ICE/ITMC)",21-23 June 2021,ieeexplore
10.1109/IS3C50286.2020.00134,Screw defect detection system based on AI image recognition technology,IEEE,Conferences,"In the past ten years, smart manufacturing has been widely discussed and gradually introduced into various manufacturing fields. Since Germany proposed the concept of “Industry 4.0” in 2011, it has been spreading and feverish all over the world. For Industry 4.0, information digitization, intelligent defect detection and database platform management are their main core technologies. Aiming at a large screw industry manufacturing field in central and southern Taiwan, this paper proposes a screw defect detection system based on AI image recognition technology to detect damage to the nut during the “molding” process in the screw production process, and it is determined whether the inspected screw passes the inspection. The recognition result is given as shown in Figure 1. This paper uses 500 non-defective screw samples and 20 defective screw samples provided by the screw factory. The above samples collected real-time images through the sampling structure designed in this article, and we adopt Microsoft Corporation's ML.NET suite to model AI images, and uses the following four deep learning models: ResNetV2 50, ResNetV2 101, InceptionV3, MoblieNetV2 for learning; in the process of learning, this article divides the data set into three types of data sets (one is the unknown set that is not used for training but mixed with correct and defective samples, and the other is used for post-training verification of mixed samples with correct and defective samples. The third is a training set for training a mixture of correct and defective samples) This arrangement is used for subsequent verification models; after training, a PC-based screw defect detection system is implemented as shown in Figure 2; finally, with Detect screw defects in the form of instant photography. After the experiment, in 1,000 repeated tests, the success rate of defect detection reached 97%, while the false positive rate was only 2%.",https://ieeexplore.ieee.org/document/9394116/,"2020 International Symposium on Computer, Consumer and Control (IS3C)",13-16 Nov. 2020,ieeexplore
10.1109/ICRITO48877.2020.9198036,State of Art: Energy Efficient Protocols for Self-Powered Wireless Sensor Network in IIoT to Support Industry 4.0,IEEE,Conferences,"Up gradation of manufacturing systems in industries by means of implementing innovative manufacturing techniques that captures real time data, applies machine learning algorithms, makes entire system self-decisive and provides inter connectivity to the whole system is the prime focus of the Industry 4.0. It is aimed at bringing new industrial revolution with the help of internet of things technology due to its considerable influence in the industrial manufacturing process. Though impact of internet of things in industrial sector is huge, a practical implementation incorporates challenges in energy efficiency, self-powered sensor nodes and security. For processing of gathered data self-powered sensor nodes may sinks energy from ambient energy sources. A considerable amount of efforts has been put by researchers to address the challenges for development of energy efficient routing protocol for such nodes. The presented survey is aimed at analyzing the protocols for contribution towards the goals of industry 4.0 Energy efficient protocol will support the system to consume least energy for its operation. Henceforth combination of self-powered wireless sensor network and energy efficient protocol will be useful to power up many industrial IoT applications.",https://ieeexplore.ieee.org/document/9198036/,"2020 8th International Conference on Reliability, Infocom Technologies and Optimization (Trends and Future Directions) (ICRITO)",4-5 June 2020,ieeexplore
10.1109/ICE/ITMC49519.2020.9198430,Supporting SMEs in the Lake Constance Region in the Implementation of Cyber-Physical-Systems: Framework and Demonstrator,IEEE,Conferences,"With the emergence of the recent Industry 4.0 movement, data integration is now also being driven along the production line, made possible primarily by the use of established concepts of intelligent supply chains, such as the digital avatars. Digital avatars - sometimes also called Digital Twins or more broadly Cyber-Physical Systems (CPS) - are already successfully used in holistic systems for intelligent transport ecosystems, similar to the use of Big Data and artificial intelligence technologies interwoven with modern production and supply chains. The goal of this paper is to describe how data from interwoven, autonomous and intelligent supply chains can be integrated into the diverse data ecosystems of the Industry 4.0, influenced by a multitude of data exchange formats and varied data schemas. In this paper, we describe how a framework for supporting SMEs was established in the Lake Constance region and describe a demonstrator sprung from the framework. The demonstrator project's goal is to exhibit and compare two different approaches towards optimisation of manufacturing lines. The first approach is based upon static optimisation of production demand, i.e. exact or heuristic algorithms are used to plan and optimise the assignment of orders to individual machines. In the second scenario, we use real-time situational awareness - implemented as digital avatar - to assign local intelligence to jobs and raw materials in order to compare the results to the traditional planning methods of scenario one. The results are generated using event-discrete simulation and are compared to common (heuristic) job scheduling algorithms.",https://ieeexplore.ieee.org/document/9198430/,"2020 IEEE International Conference on Engineering, Technology and Innovation (ICE/ITMC)",15-17 June 2020,ieeexplore
10.1109/ISWCS.2019.8877305,Visible Light Positioning for Location-Based Services in Industry 4.0,IEEE,Conferences,"Industry 4.0 refers to the evolution in manufacturing from computerization to fully cyberphysical systems that exploit rich sensor data, adaptive real-time safety-critical control, and machine learning. An important aspect of this vision is the sensing and subsequent association of objects in the physical world with their cyber and virtual counterparts. In this paper we propose Visible Light Positioning (VLP) as an enabler for these Industry 4.0 applications. We also explore sensing techniques, including cameras (and depth sensors), and other light-based solutions for object positioning and detection along with their respective limitations. We then demonstrate an application of positioning for real time robot control in an interactive multiparty cyber-physical-virtual deployment. Lastly, based on our experience with this cyberphysical-virtual application, we propose Ray-Surface Positioning (RSP), a novel VLP technique, as a low cost positioning system for Industry 4.0.",https://ieeexplore.ieee.org/document/8877305/,2019 16th International Symposium on Wireless Communication Systems (ISWCS),27-30 Aug. 2019,ieeexplore
10.1109/BDCloud.2018.00136,"iSTEP, an Integrated Self-Tuning Engine for Predictive Maintenance in Industry 4.0",IEEE,Conferences,"The recent expansion of IoT-enabled (Internet of Things) devices in manufacturing contexts and their subsequent data-driven exploitation paved the way to the advent of the Industry 4.0, promoting a full integration of IT services, smart devices, and control systems with physical objects, their electronics and sensors. The real-time transmission and analysis of collected data from factories has the potential to create manufacturing intelligence, of which predictive maintenance is an expression. Hence the need to design new approaches able to manage not only the data volume, but also the variety and velocity, extracting actual value from the humongous amounts of collected data. To this aim, we present iSTEP, an integrated Self-Tuning Engine for Predictive maintenance, based on Big Data technologies and designed for Industry 4.0 applications. The proposed approach targets some of the most common needs of manufacturing enterprises: compatibility with both the on-premises and the in-the-cloud environments, exploitation of reliable and largely supported Big Data platforms, easy deployment through containerized software modules, virtually unlimited horizontal scalability, fault-tolerant self-reconfiguration, flexible yet friendly streaming-KPI computations, and above all, the integrated provisioning of self-tuning machine learning techniques for predictive maintenance. The current implementation of iSTEP exploits a distributed architecture based on Apache Kafka, Spark Streaming, MLlib, and Cassandra; iSTEP provides (i) a specific feature engineering block aimed at automatically extracting metrics from the production monitoring time series, which improves the predictive performance by 77% on average, and (ii) a self-tuning approach that dynamically selects the best prediction algorithm, which improves the predictive performance up to 60%. The iSTEP engine provides transparent predictive models, able to provide end users with insights into the knowledge learned, and it has been experimentally evaluated on a public unbalanced failure dataset, whose extensive results are discussed in the paper.",https://ieeexplore.ieee.org/document/8672266/,"2018 IEEE Intl Conf on Parallel & Distributed Processing with Applications, Ubiquitous Computing & Communications, Big Data & Cloud Computing, Social Computing & Networking, Sustainable Computing & Communications (ISPA/IUCC/BDCloud/SocialCom/SustainCom)",11-13 Dec. 2018,ieeexplore
10.1109/TLA.2021.9480156,A Data Governance Framework for Industry 4.0,IEEE,Journals,"The fourth industrial revolution, or Industry 4.0, represents a new stage of evolution in the organization, management and control of the value chain throughout the product or service life cycle. This digitization of the industrial environment is characterized by the connection of Information Technologies (IT) and Operations Technologies (OT) through cyber-physical systems and the Industrial IoT (IIoT). One of the main consequences of this integration is the increasing amount and variety of data generated in real time from different sources. In this environment of intensive generation of actionable information, data becomes a critical asset for Industry 4.0, at all stages of the value chain. However, in order to data become a competitive advantage for the company, it must be managed and governed like any other strategic asset, and therefore it is necessary to rely on a Data Governance system. Industry 4.0 requires a reformulation of governance since the data is a key element and the backbone of the processes of the organization. This paper proposes a Reference Framework for the implementation of Data Governance Systems for Industry 4.0. Previously, it contextualizes data governance for Industry 4.0 environments and identifies the requirements that this framework must address, which are conditioned by the specific features of Industry 4.0, among others, the intensive use of big data, cloud and edge computing, artificial intelligence and current regulations.",https://ieeexplore.ieee.org/document/9480156/,IEEE Latin America Transactions,Dec. 2021,ieeexplore
10.1109/TII.2019.2915846,A Global Manufacturing Big Data Ecosystem for Fault Detection in Predictive Maintenance,IEEE,Journals,"Artificial intelligence, big data, machine learning, cloud computing, and Internet of Things (IoT) are terms which have driven the fourth industrial revolution. The digital revolution has transformed the manufacturing industry into smart manufacturing through the development of intelligent systems. In this paper, a big data ecosystem is presented for the implementation of fault detection and diagnosis in predictive maintenance with real industrial big data gathered directly from large-scale global manufacturing plants, aiming to provide a complete architecture which could be used in industrial IoT-based smart manufacturing in an industrial 4.0 system. The proposed architecture overcomes multiple challenges including big data ingestion, integration, transformation, storage, analytics, and visualization in a real-time environment using various technologies such as the data lake, NoSQL database, Apache Spark, Apache Drill, Apache Hive, OPC Collector, and other techniques. Transformation protocols, authentication, and data encryption methods are also utilized to address data and network security issues. A MapReduce-based distributed PCA model is designed for fault detection and diagnosis. In a large-scale manufacturing system, not all kinds of failure data are accessible, and the absence of labels precludes all the supervised methods in the predictive phase. Furthermore, the proposed framework takes advantage of some of the characteristics of PCA such as its ease of implementation on Spark, its simple algorithmic structure, and its real-time processing ability. All these elements are essential for smart manufacturing in the evolution to Industry 4.0. The proposed detection system has been implemented into the real-time industrial production system in a cooperated company, running for several years, and the results successfully provide an alarm warning several days before the fault happens. A test case involving several outages in 2014 is reported and analyzed in detail during the experiment section.",https://ieeexplore.ieee.org/document/8710319/,IEEE Transactions on Industrial Informatics,Jan. 2020,ieeexplore
10.1109/ACCESS.2020.2990190,"A Novel Simulated-Annealing Based Electric Bus System Design, Simulation, and Analysis for Dehradun Smart City",IEEE,Journals,"Smart transportation network development with environmental issues into consideration has brought Industry 4.0 based solutions on priority. In this direction, battery-powered electric bus systems have been considered widely for ensuring flexibility, operation cost, and lesser pollutants emission. Industry 4.0 provides automation through a cyber-physical system (CPS), the interconnection of bus system entities with industrial internet-of-things (IIoT), remote information availability through cloud computing and scientific disciplines (human-computer interaction, artificial intelligence, machine learning etc.) integration. In this work, a discrete event-based simulation-optimization approach is integrated that take care of bus energy consumption according to real-time city's passenger needs and on-road friction levels. The proposed simulation optimization methodology utilizes multi-objective with dependent and independent variables for optimizing the overall system performance. In simulation optimization, objective functions are designed to tackle battery consumption, Internet-of-Thing (IoT) network performance, cloud operations efficiency and smart scientific discipline integration. Simulation parameters are based on a real-time bus system which is further analyzed, filtered and adapted as per the needs of the system. In another analysis, supercharger's capacities are varied to evaluate the performance of the proposed system and identify the low cost and efficient smart transportation system. Simulation results show different scenarios for variations in the number of buses, charging stations, bus-depots, mobile charging facilities, and bus-schedules. Simulation results show that the average passenger's waiting time in the waiting is (after ticket booking) varies between 0.2 minutes to 0.7 minutes in real-time traffic conditions. In similar traffic conditions, total passenger's time in system (ticket booking to travel) varies between 41.6 minutes (for 24 hours) to 45.5 minutes (for 1 year). In the simulation, priorities are given to those dependent and independent variables which save the battery consumption and elongate the utilization of buses. Lastly, it is also observed that the proposed system is suitable for resource-constraint devices because Gate Equivalent (GE) calculation shows that the proposed system can be implemented between 1986 GEs (communicational cost without confidentiality and authentication) and 7939 GEs (computational cost with HMAC for authentication in data storage). This ensures varies security primitivs such as confidentiality, availability and authentication.",https://ieeexplore.ieee.org/document/9078106/,IEEE Access,2020,ieeexplore
10.1109/ACCESS.2021.3121302,A Survey on Industry 4.0 for the Oil and Gas Industry: Upstream Sector,IEEE,Journals,"The market volatility in the oil and gas (O&amp;G) sector, the dwindling demand for oil due to the impact of COVID-19, and the push for alternative greener energy are driving the need for innovation and digitization in the O&amp;G industry. This has attracted research interest from academia and the industry in the application of industry 4.0 (I4.0) technologies in the O&amp;G sector. The application of some of these I4.0 technologies has been presented in the literature, but the domain still lacks a comprehensive survey of the application of I4.0 in the O&amp;G upstream sector. This paper investigates the state-of-the-art efforts directed toward I4.0 technologies in the O&amp;G upstream sector. To achieve this, first, an overview of the I4.0 is discussed followed by a systematic literature review from an integrative perspective for publications between 2012–2021 with 223 analyzed documents. The benefits and challenges of the adoption of I4.0 have been identified. Moreover, the paper adds value by proposing a framework for the implementation of I4.0 in the O&amp;G upstream sector. Finally, future directions and research opportunities such as framework, edge computing, quantum computing, communication technologies, standardization, and innovative areas related to the implementation of I4.0 in the upstream sector are presented. The findings from this review show that I4.0 technologies are currently being explored and deployed for various aspects of the upstream sector. However, some of the I4.0 technologies like additive manufacturing and virtual reality are least explored.",https://ieeexplore.ieee.org/document/9579415/,IEEE Access,2021,ieeexplore
10.1109/TII.2019.2962029,An Entropy-Based Approach to Real-Time Information Extraction for Industry 4.0,IEEE,Journals,"Industry 4.0 has drawn considerable attention from industry and academic research communities. The recent advances in Internet of Things (IoT), Big Data analytics, sensor technology, and artificial intelligence have led to the design and implementation of novel approaches to take full advantage of data-driven solutions applicable to Industry 4.0. With the availability of large datasets, it has become crucially important to identify the appropriate amount of relevant information, which would optimize the overall analysis of the corresponding systems. In this article, specific properties of dynamically evolving data systems are introduced and investigated, which provide framework to assess the appropriate amount of representative information.",https://ieeexplore.ieee.org/document/8941297/,IEEE Transactions on Industrial Informatics,Sept. 2020,ieeexplore
10.1109/ACCESS.2020.2993010,Bearing Intelligent Fault Diagnosis in the Industrial Internet of Things Context: A Lightweight Convolutional Neural Network,IEEE,Journals,"The advancement of Industry 4.0 and Industrial Internet of Things (IIoT) has laid more emphasis on reducing the parameter amount and storage space of the model in addition to the automatic and accurate fault diagnosis. In this case, this paper proposes a lightweight convolutional neural network (LCNN) method for intelligent fault diagnosis of bearing, which can largely satisfy the need of less parameter amount and storage space as well as high accuracy. First, depthiwise separable convolution is adopted, and a LCNN structure is constructed through an inverse residual structure and a linear bottleneck layer operation. Secondly, a novel decomposed Hierarchical Search Space is introduced to automatically explore the optimal LCNN for bearing fault diagnosis in the context of the IIoT. In the meantime, the real-time monitoring and fault diagnosis of the model are also deployed. In order to verify the validity of the designed model, Case Western Reserve University Bearing fault dataset and MFPT bearing fault dataset are adopted. The results demonstrate the great advantages of the model. The LCNN model can automatically learn and select the appropriate features, highly improving the fault diagnosis accuracy. Meanwhile, the computational and storage costs of the model are largely reduced, which contributes to its being applied to the mechanical system in the IIoT context.",https://ieeexplore.ieee.org/document/9088980/,IEEE Access,2020,ieeexplore
10.1109/ACCESS.2021.3101284,"Data-Driven Remaining Useful Life Estimation for Milling Process: Sensors, Algorithms, Datasets, and Future Directions",IEEE,Journals,"An increase in unplanned downtime of machines disrupts and degrades the industrial business, which results in substantial credibility damage and monetary loss. The cutting tool is a critical asset of the milling machine; the failure of the cutting tool causes a loss in industrial productivity due to unplanned downtime. In such cases, a proper predictive maintenance strategy by real-time health monitoring of cutting tools becomes essential. Accurately predicting the useful life of equipment plays a vital role in the predictive maintenance arena of industry 4.0. Many active research efforts have been done to estimate tool life in varied directions. However, the consolidated study of the implemented techniques and future pathways is still missing. So, the purpose of this paper is to provide a systematic and comprehensive literature survey on the data-driven approach of Remaining Useful Life (RUL) estimation of cutting tools during the milling process. The authors have summarized different monitoring techniques, feature extraction methods, decision-making models, and available sensors currently used in the data-driven model. The authors have also presented publicly available datasets related to milling under various operating conditions to compare the accuracy of the prediction model for tool wear estimation. Finally, the article concluded with the challenges, limitations, recent advancements in RUL prognostics techniques using Artificial Intelligence (AI), and future research scope to explore more in this area.",https://ieeexplore.ieee.org/document/9502093/,IEEE Access,2021,ieeexplore
10.1109/ACCESS.2020.2998723,"Digital Twin for the Oil and Gas Industry: Overview, Research Trends, Opportunities, and Challenges",IEEE,Journals,"With the emergence of industry 4.0, the oil and gas (O&amp;G) industry is now considering a range of digital technologies to enhance productivity, efficiency, and safety of their operations while minimizing capital and operating costs, health and environment risks, and variability in the O&amp;G project life cycles. The deployment of emerging technologies allows O&amp;G companies to construct digital twins (DT) of their assets. Considering DT adoption, the O&amp;G industry is still at an early stage with implementations limited to isolated and selective applications instead of industry-wide implementation, limiting the benefits from DT implementation. To gain the full potential of DT and related technological adoption, a comprehensive understanding of DT technology, the current status of O&amp;G-related DT research activities, and the opportunities and challenges associated with the deployment of DT in the O&amp;G industry are of paramount importance. In order to develop this understanding, this paper presents a literature review of DT within the context of the O&amp;G industry. The paper follows a systematic approach to select articles for the literature review. First, a keywords-based publication search was performed on the scientific databases such as Elsevier, IEEE Xplore, OnePetro, Scopus, and Springer. The filtered articles were then analyzed using online text analytic software (Voyant Tools) followed by a manual review of the abstract, introduction and conclusion sections to select the most relevant articles for our study. These articles and the industrial publications cited by them were thoroughly reviewed to present a comprehensive overview of DT technology and to identify current research status, opportunities and challenges of DT deployment in the O&amp;G industry. From this literature review, it was found that asset integrity monitoring, project planning, and life cycle management are the key application areas of digital twin in the O&amp;G industry while cyber security, lack of standardization, and uncertainty in scope and focus are the key challenges of DT deployment in the O&amp;G industry. When considering the geographical distribution for the DT related research in the O&amp;G industry, the United States (US) is the leading country, followed by Norway, United Kingdom (UK), Canada, China, Italy, Netherland, Brazil, Germany, and Saudi Arabia. The overall publication rate was less than ten articles (approximately) per year until 2017, and a significant increase occurred in 2018 and 2019. The number of journal publications was noticeably lower than the number of conference publications, and the majority of the publications presented theoretical concepts rather than the industrial implementations. Both these observations suggest that the DT implementation in the O&amp;G industry is still at an early stage.",https://ieeexplore.ieee.org/document/9104682/,IEEE Access,2020,ieeexplore
10.1109/ACCESS.2021.3120843,Digital Twins From Smart Manufacturing to Smart Cities: A Survey,IEEE,Journals,"Digital twins are quickly becoming a popular tool in several domains, taking advantage of recent advancements in the Internet of Things, Machine Learning and Big Data, while being used by both the industry sector and the research community. In this paper, we review the current research landscape as regards digital twins in the field of smart cities, while also attempting to draw parallels with the application of digital twins in Industry 4.0. Although digital twins have received considerable attention in the Industrial Internet of Things domain, their utilization in smart cities has not been as popular thus far. We discuss here the open challenges in the field and argue that digital twins in smart cities should be treated differently and be considered as cyber-physical “systems of systems”, due to the vastly different system size, complexity and requirements, when compared to other recent applications of digital twins. We also argue that researchers should utilize established tools and methods of the smart city community, such as co-creation, to better handle the specificities of this domain in practice.",https://ieeexplore.ieee.org/document/9576739/,IEEE Access,2021,ieeexplore
10.1109/ACCESS.2018.2809681,Industrial Big Data Analytics for Prediction of Remaining Useful Life Based on Deep Learning,IEEE,Journals,"Due to the recent development of cyber-physical systems, big data, cloud computing, and industrial wireless networks, a new era of industrial big data is introduced. Deep learning, which brought a revolutionary change in computer vision, natural language processing, and a variety of other applications, has significant potential for solutions providing in sophisticated industrial applications. In this paper, a concept of device electrocardiogram (DECG) is presented, and an algorithm based on deep denoising autoencoder (DDA) and regression operation is proposed for the prediction of the remaining useful life of industrial equipment. First, the concept of electrocardiogram is explained. Then, a problem statement based on manufacturing scenario is presented. Subsequently, the architecture of the proposed algorithm called integrated DDA and the algorithm workflow are provided. Moreover, DECG is compared with traditional factory information system, and the feasibility and effectiveness of the proposed algorithm are validated experimentally. The proposed concept and algorithm combine typical industrial scenario and advance artificial intelligence, which has great potential to accelerate the implementation of industry 4.0.",https://ieeexplore.ieee.org/document/8302913/,IEEE Access,2018,ieeexplore
10.1109/ACCESS.2017.2754507,Model-Based Development of Knowledge-Driven Self-Reconfigurable Machine Control Systems,IEEE,Journals,"To accommodate the trend toward mass customization launched by intelligent manufacturing in the era of Industry 4.0, this paper proposes the combination of model-driven engineering and knowledgedriven engineering during the development process of self-reconfigurable machine control systems. The complete tool chain for model development, execution, and reconfiguration is established. For the design phase, a machine-control-domain-specific modeling language and the supporting design environment are developed. With regard to the execution stage, a runtime framework compliant with the IEC 61499 standard is proposed. On the ground of the modeling environment and the reconfigurable run-time framework, a self-adaptive control module is developed to establish the close-loop self-reconfiguration infrastructure. The ontological representation of knowledge base toward this end is described, along with extendable SQWRL rules specified to automatically initiate the reconfiguration process in the cases of external user demands and internal faults. A prototype motion control kernel in the low-level layer of machine control system architecture is developed with the proposed modeling language and is then deployed to the runtime framework. Two case studies on self-reconfiguration of the proof-of-concept motion control kernel are demonstrated, which prove the feasibility of our proposal.",https://ieeexplore.ieee.org/document/8047091/,IEEE Access,2017,ieeexplore
10.1109/JSEN.2021.3075535,Recent Advancements in the Development of Sensors for the Structural Health Monitoring (SHM) at High-Temperature Environment: A Review,IEEE,Journals,"With Industry 4.0 becoming increasingly pervasive, the importance and usage of sensors has increased several folds. Industry 4.0 refers to a new phase in the industrial revolution that mainly focuses on interconnectivity, automation, machine learning, and real-time data. Real-time structural health monitoring (SHM) of components in the industrial process is one of the crucial and important component of Industry 4.0. SHM of components exposed to high-temperature (<inline-formula> <tex-math notation=""LaTeX"">$\sim 650^{\circ }\text{C}$ </tex-math></inline-formula>) is becoming increasingly important nowadays. However, harsh and high temperature environments impose a great challenge towards their implementation. This review is an attempt to demonstrate the development, application, limitations and recent advancement of the existing sensors used for SHM. Some sensors such as eddy current (EC) sensors and fiber Bragg grating (FBG) sensors have been discussed in detail. A phenomenological study of the electromagnetic sensor for the SHM of engineering components that are exposed to high temperature has been addressed. State-of-the-art fabrication methodologies such as low temperature co-fired ceramic (LTCC) technology for such type of sensors for high temperature SHM applications have been elucidated. Future challenges and opportunities for SHM applications of high temperature sensors have been highlighted.",https://ieeexplore.ieee.org/document/9415648/,IEEE Sensors Journal,"15 July15, 2021",ieeexplore
10.1109/ACCESS.2020.3045563,SDN Enhanced Resource Orchestration of Containerized Edge Applications for Industrial IoT,IEEE,Journals,"With the rise of the Industrial Internet of Things (IIoT), there is an intense pressure on resource and performance optimization leveraging on existing technologies, such as Software Defined Networking (SDN), edge computing, and container orchestration. Industry 4.0 emphasizes the importance of lean and efficient operations for sustainable manufacturing. Achieving this goal would require engineers to consider all layers of the system, from hardware to software, and optimizing for resource efficiency at all levels. This emphasizes the need for container-based virtualization tools such as Docker and Kubernetes, offering Platform as a Service (PaaS), while simultaneously leveraging on edge technologies to reduce related latencies. For network management, SDN is poised to offer a cost-effective and dynamic scalability solution by customizing packet handling for various edge applications and services. In this paper, we investigate the energy and latency trade-offs involved in combining these technologies for industrial applications. As a use case, we emulate a 3D-drone-based monitoring system aimed at providing real-time visual monitoring of industrial automation. We compare a native implementation to a containerized implementation where video processing is orchestrated while streaming is handled by an external UE representing the IIoT device. We compare these two scenarios for energy utilization, latency, and responsiveness. Our test results show that only roughly 16 percent of the total power consumption happens on the mobile node when orchestrated. Virtualization adds up about 4.5 percent of the total power consumption while the latency difference between the two approaches becomes negligible after the streaming session is initialized.",https://ieeexplore.ieee.org/document/9296769/,IEEE Access,2020,ieeexplore
10.1109/ACCESS.2020.2994933,Sensor-Driven Learning of Time-Dependent Parameters for Prescriptive Analytics,IEEE,Journals,"Big data analytics is rapidly emerging as a key Internet of Things (IoT) initiative aiming at providing meaningful insights and supporting optimal decision making under time constraints. In this direction, prescriptive analytics has just started to emerge. Prescriptive analytics moves beyond descriptive and predictive analytics aiming at providing adaptive, automated, constrained, time-dependent and optimal decisions. The use of time-dependent parameters in prescriptive analytics models provide a more reliable and realistic representation of the complex and dynamic environment and the associated decision making process; however, their estimation poses significant challenges due to the uncertainty derived from inaccurate user input, noisy data, and non-stationarity of real-world data streams. Since feedback and learning mechanisms for tracking the prescriptive analytics are crucial enablers for self-configuration and self-optimization, this paper proposes an approach for sensor-driven learning of time-dependent parameters for prescriptive analytics models deployed in streaming computational environments. The proposed approach was validated in an Industry 4.0 use case, while it was further evaluated through extensive simulation experiments. The proposed approach overcomes challenges related to uncertainty derived from user's input, non-stationary data and sensor noise and provides estimates of time-dependent parameters that lead to more reliable prescriptions.",https://ieeexplore.ieee.org/document/9094172/,IEEE Access,2020,ieeexplore
10.1109/TII.2020.3047675,Siamese Neural Network Based Few-Shot Learning for Anomaly Detection in Industrial Cyber-Physical Systems,IEEE,Journals,"With the increasing population of Industry 4.0, both AI and smart techniques have been applied and become hotly discussed topics in industrial cyber-physical systems (CPS). Intelligent anomaly detection for identifying cyber-physical attacks to guarantee the work efficiency and safety is still a challenging issue, especially when dealing with few labeled data for cyber-physical security protection. In this article, we propose a few-shot learning model with Siamese convolutional neural network (FSL-SCNN), to alleviate the over-fitting issue and enhance the accuracy for intelligent anomaly detection in industrial CPS. A Siamese CNN encoding network is constructed to measure distances of input samples based on their optimized feature representations. A robust cost function design including three specific losses is then proposed to enhance the efficiency of training process. An intelligent anomaly detection algorithm is developed finally. Experiment results based on a fully labeled public dataset and a few labeled dataset demonstrate that our proposed FSL-SCNN can significantly improve false alarm rate (FAR) and F1 scores when detecting intrusion signals for industrial CPS security protection.",https://ieeexplore.ieee.org/document/9311786/,IEEE Transactions on Industrial Informatics,Aug. 2021,ieeexplore
10.1109/ACCESS.2021.3083499,Towards Secured Online Monitoring for Digitalized GIS Against Cyber-Attacks Based on IoT and Machine Learning,IEEE,Journals,"Recently, the Internet of Things (IoT) has an important role in the growth and development of digitalized electric power stations while offering ambitious opportunities, specifically real-time monitoring and cybersecurity. In this regard, this paper introduces a novel IoT architecture for the online monitoring of the gas-insulated switchgear (GIS) status instead of the traditional observation methods. The proposed IoT architecture is derived from the concept of the cyber-physic system (CPS) in Industry 4.0. However, the cyber-attacks and the classification of the GIS insulation defects represent the main challenges against the implementation of IoT topology for the online monitoring and tracking of the GIS status. For this purpose, advanced machine learning techniques are utilized to detect cyber-attacks to conduct the paradigm and verification. Different test scenarios on various defects in GIS are performed to demonstrate the effectiveness of the proposed IoT architecture. Partial discharge pulse sequence features are extracted for each defect to represent the inputs for IoT architecture. The results confirm that the proposed IoT architecture based on the machine learning technique, that is the extreme gradient boosting (XGBoost), can visualize all defects in the GIS with different alarms, besides showing the cyber-attacks on the networks effectively. Furthermore, the defects of GIS and the fake data due to the cyber-attacks are recognized and presented on the dashboard of the proposed IoT platform with high accuracy and more clarified visualization to enhance the decision-making about the GIS status.",https://ieeexplore.ieee.org/document/9440436/,IEEE Access,2021,ieeexplore
10.1109/ACCESS.2021.3082934,Visual Product Tracking System Using Siamese Neural Networks,IEEE,Journals,"Management of unstructured production data is a key challenge for Industry 4.0. Effective product tracking endorses data integration and productivity improvements throughout the manufacturing processes. Radio-frequency identification (RFID) tags are used in many tracking cases, but in some manufacturing environments, those cannot be used as they might get damaged or removed during processing. In this paper, we propose an alternative visual product tracking system. The physical system uses two cameras placed at the two ends of the tracked process(es). Product pairs are then matched with a Siamese neural network operating on the product images and trained offline on the problem at hand with labeled data. The proposed system can track products solely based on their visual appearance and without any physical interference with the products or production processes. Unlike other existing image-based methods, the proposed system is invariant to major positional and visual changes in the products. As a proof-of-concept, we tested the proposed system with real plywood factory data and were able to track the products with 98.5 % accuracy in a realistic test scenario. The implementation of the proposed method and the Veneer21 dataset are publicly available at https://github.com/TuomasJalonen/visual-product-tracking-system.",https://ieeexplore.ieee.org/document/9439511/,IEEE Access,2021,ieeexplore
10.1109/SMC.2013.819,Automated Sound Signalling Device Quality Assurance Tool for Embedded Industrial Control Applications,IEEE,Conferences,This paper presents a novel system for automatic detection and recognition of faulty audio signaling devices as part of an automated industrial manufacturing process. The system uses historical data labeled by human experts in detecting faulty signaling devices to train an artificial neural network based classifier for modeling their decision making process. The neural network is implemented on a real time embedded micro controller which can be more efficiently incorporated into an automated production line eliminating the need for a manual inspection within the manufacturing process. We present real world experiments based on data pertaining to the production and manufacture of audio signaling components used in car instrument clusters. Our results show that the proposed expert system is able to successfully classify faulty audio signaling devices to a high degree of accuracy. The results can be generalized to other signaling devices where an output signal is represented by a complex and changing frequency spectrum even with significant environmental noise.,https://ieeexplore.ieee.org/document/6722574/,"2013 IEEE International Conference on Systems, Man, and Cybernetics",13-16 Oct. 2013,ieeexplore
10.1109/ICIAICT.2019.8784838,Automated Testing Framework for Geographical Distributed Testing Environment,IEEE,Conferences,"Automated testing is critical for the complex products development and manufacture. In generally, connecting multiple test environments in geographical distributed environment to share resources is a way to reduce cost. However, it conflicts with the principle of “keep test environment clean”. Moreover, high latency between geographical different networks may impact origin automated test activities in reality. In this paper, we propose an automated testing framework for geographical distributed testing environment to solve the problems of sharing resources. In the proposed framework, improved Test Harness (TH) and integrated Test as a Service (TaaS) are proposed to assure automated testing flow and guarantee test integrity. The proposed framework may not impact origin automated test activities, which could reduce the cost of test activities in really. In addition, a basic implementation of Test Harness and TaaS Console in the proposed framework are presented in this paper.",https://ieeexplore.ieee.org/document/8784838/,"2019 IEEE International Conference on Industry 4.0, Artificial Intelligence, and Communications Technology (IAICT)",1-3 July 2019,ieeexplore
10.1109/ICAMIMIA47173.2019.9223365,Autonomous Car Simulation Using Evolutionary Neural Network Algorithm,IEEE,Conferences,"Automation with artificial intelligence (AI) has widely implemented in robotics, transportation and manufacture. AI has become a powerful technology that change human life and help human more flexible doing something. In this paper, it will show a result of simulation from an autonomous car using the evolutionary neural network algorithm which combines genetic algorithm and neural network. The purpose of the simulation is to test the model that we develop to know the right direction based on the track, so the evolutionary neural network that implemented to the autonomous car be able to deliver the best solution before it implements in the real machine or car technology. Genetic algorithm combines with a neural network to reach an evolution condition. The evolution process is achieved through crossover, mutation and selection process, so the algorithm will give the best result from the iteration of the experiment. The result of our experiment shows that evolutionary neural network algorithm give the best result within 3 layer architecture, with iteration average is 14.5 reach finish point (check point) 3 in the track simulation. Based on the simulation, our car model can find out the right direction.",https://ieeexplore.ieee.org/document/9223365/,"2019 International Conference on Advanced Mechatronics, Intelligent Manufacture and Industrial Automation (ICAMIMIA)",9-10 Oct. 2019,ieeexplore
10.1109/ICMLA.2019.00320,Coarse Annotation Refinement for Segmentation of Dot-Matrix Batchcodes,IEEE,Conferences,"Deep Convolutional Neural Networks (CNN) have been extensively applied in various computer vision tasks. Although such approaches have demonstrated exceptionally high performance in various open challenges, adapting them to more specialised tasks can be non-trivial. In this paper we discuss our design and implementation of a batchcode detection system capable of accurate segmentation of batchcode regions within images of consumer products. A batchcode is a unique identifier printed on the packaging of many products that encodes useful information such as date and location of manufacture. Detection of batchcodes in images of products is a useful step in many processes, including quality control, supply chain tracking and counterfeit detection. Beginning with a unique dataset of product images and a set of crowdsourced coarse annotations that roughly correspond to the locations of batchcodes, we demonstrate that such annotations are insufficient for training a reliable model, and subsequently describe a novel label refinement process, which we call the Maximally Stable Global Region (MSGR) method, that we use to generate accurate ground-truth data suitable for training a robust neural network. We also show that detection accuracy can be further improved by applying MSGR to the output of the neural network. We evaluate our approach using a manually labelled test dataset of images of shampoo bottles, and demonstrate the efficacy of the proposed method for accurate real-time batchcode detection.",https://ieeexplore.ieee.org/document/8999037/,2019 18th IEEE International Conference On Machine Learning And Applications (ICMLA),16-19 Dec. 2019,ieeexplore
10.1109/DESSERT50317.2020.9125038,Combination of Digital Twin and Artificial Intelligence in Manufacturing Using Industrial IoT,IEEE,Conferences,"The paper focuses on Digital Twin (DT) in Manufacturing using Artificial Intelligence (AI) and Industrial IoT. According to the concept, the manufacturing includes three main units: equipment, personnel and processes. All data from these units are inherited to manufacture model (DT) and decision support system with the use of AI. DT data technology allows finding the required knowledge that can be interpreted and used to support the process of decision-making in the management of the enterprise. AI applications open up a broad spectrum of opportunities in manufacturing to add value by optimizing processes and generating new business models. The Landscape was described by a formal model to assure the possibility to analyze the state and development of landscape in detail considering DT and other technologies. DT and IIoT implementation for the simulation of real enterprise manufacturing were considered.",https://ieeexplore.ieee.org/document/9125038/,"2020 IEEE 11th International Conference on Dependable Systems, Services and Technologies (DESSERT)",14-18 May 2020,ieeexplore
10.1109/PERCOMW.2018.8480343,Development of Energy-efficient Sensor Networks by Minimizing Sensors Numbers with a Machine Learning Model,IEEE,Conferences,"With the increasing demand to construct sensor networks for a smart IoT (Internet of Things) world, numerous sensors with sensing and communication capabilities are expected to be deployed in the future. Thanks to the development of hardware manufacture technology, relatively small IoT smart sensors are now commercially available and cost-effective. However, the total power required by operating these sensors is expected to be enormous, due to their large number and frequent activity. Removing “unneeded sensors” is the most direct way to reduce the power consumption of sensor networks. Here, “unneeded sensors” refers to those that can be placed in sleep mode, or even be removed from the network topology entirely, without serious impact on the overall networks data processing performance. In this paper, we report the development of an energy-efficient sensor network by using a machine learning model to determine the actual necessity of all the sensors in a sensor network. Machine learning model is introduced to identify unneeded sensors by comparing the data from neighboring sensors to that from the potentially unneeded ones. For identifying unneeded sensors, different strategies with different computational complexity are also proposed. Numerical experiments conducted in two real indoor environments verify that our proposed scheme can reduce the total number of active sensors by around 1/3, while maintaining more than 90% of the original high monitoring performance of the sensor network.",https://ieeexplore.ieee.org/document/8480343/,2018 IEEE International Conference on Pervasive Computing and Communications Workshops (PerCom Workshops),19-23 March 2018,ieeexplore
10.1109/PES.2007.386098,Dynamic Voltage Restorer with Neural Network Controlled Voltage Disturbance Detector and Real-time Digital Voltage Control,IEEE,Conferences,"This paper describes the high power DVR (dynamic voltage restorer) with the new voltage disturbance detection method and the real-time digital PWM voltage control. The new voltage disturbance detector was implemented by using the delta rule of the neural network control. Through the proposed method, we can instantaneously track the peak value of each phase voltage under the severe unbalance voltage conditions. Compared to the conventional synchronous reference frame method, the proposed one shows the minimum time delay to determine the instance of the voltage sag or voltage swell event. Also real-time digital PWM voltage control technique was adopted, where the inverter output filter capacitance voltage, the filter reactor current and the load current are sampled to calculate the inverter PWM command for the next sampling interval. By using digital control, the disturbance voltage can be compensated to the reference voltage level within two sampling intervals. The proposed disturbance detector and the voltage compensator were applied to the high power DVR (440 V/1000 kVA) that was developed for the application of semiconductor manufacture plant. The performances of the proposed DVR control were verified through computer simulation and experimental results. Finally, conclusions are given.",https://ieeexplore.ieee.org/document/4275864/,2007 IEEE Power Engineering Society General Meeting,24-28 June 2007,ieeexplore
10.1109/SUMMA50634.2020.9280823,Intelligent Quality Management System for Casting Gas Turbine Engine Blades,IEEE,Conferences,"This article is devoted to the problem of reducing the number of defects and improving the quality of manufacturing gas turbine engine blades. The process of pressing a casting rod designed to form the inner cavity of the blades during their manufacture by pouring on the smelted models is considered. Information about 400 examples of pressing foundry rods has been collected. Each of the examples contained a set of parameters that characterize the process of obtaining the workpieces and the result of manufacturing. Based on the collected statistical information, the neural network was designed. Using virtual computer experiments, the process of pressing casting rods was studied. The parameters that have the greatest impact on the quality of the resulting products are identified. In the course of computer experiments, it was observed that changes in a number of pressing parameters which lead to a decrease in the probability of defects on one billet do not lead to a similar decrease in the probability of defects on another billet. The method of neural network modeling was able to identify parameters the same change in which leads to a decrease in the probability of any type of defect of all workpieces. For example, the probability of getting defects in all workpieces decreases with increasing the holding time of the rod in the mold without pressure. Thus, it is shown that the developed neural network model allows to control the quality of the obtained blades, select the optimal parameters of the technological process that provide the maximum reduction of defects.",https://ieeexplore.ieee.org/document/9280823/,"2020 2nd International Conference on Control Systems, Mathematical Modeling, Automation and Energy Efficiency (SUMMA)",11-13 Nov. 2020,ieeexplore
10.1109/ICPST.2006.321897,Medium Voltage Dynamic Voltage Restorer with Neural Network Controlled Voltage Disturbance Detector,IEEE,Conferences,"This paper describes the medium voltage high power DVR (dynamic voltage restorer) with the new voltage disturbance detection method and the real-time digital voltage control. The new voltage disturbance detector was implemented by using the delta rule of the neural network control. Through the proposed method, we can instantaneously obtain the peak value of each phase voltage even under the single phase input voltage conditions. Compared to the conventional synchronous reference frame method, the proposed one shows the minimum time delay to determine the instance of the voltage sag or voltage swell event. Also real-time digital voltage control technique was adopted, where the inverter output filter capacitance voltage, the filter reactor current and the load current are sampled to calculate the inverter PWM command for the next sampling interval. By using digital control, the disturbance voltage can be compensated to the reference voltage level within two sampling intervals. The proposed disturbance detector and the voltage compensator were applied to the medium voltage high power DVR (22.9 kV/4 MVA) that was developed for the application of semiconductor manufacture plant. The performances of the proposed DVR control were verified through computer simulation and experimental results. Finally, conclusions are given.",https://ieeexplore.ieee.org/document/4116056/,2006 International Conference on Power System Technology,22-26 Oct. 2006,ieeexplore
10.1109/AIMS52415.2021.9466014,Multi-Pole Road Sign Detection Based on Faster Region-based Convolutional Neural Network (Faster R-CNN),IEEE,Conferences,"Building an approach system that is able to serve various types of traffic signs is a challenge. The important stages in handling an object are finding objects, dividing them into several categories, and marking objects with bounding boxes. However, in reality, monitoring traffic signs objects is quite difficult because it is based on various factors such as; other closed objects, driving times, or traffic sign conditions. This study aims to measure the level of precision in monitoring traffic signs (detection speed of 4-6 frames per second) from video recording (single camera) using the Faster Region based Convolutional Neural Network (Faster R-CNN) algorithm. The traffic sign detection system uses the Faster R-CNN algorithm with Inception v2 model which is implemented in the TensorFlow API framework. The Faster R-CNN consists of 2 different modules. The first module is a deep convolutional neural network which functions to build the area to be detected, which is called the Regional Proposal Network (RPN), and the second module is the Fast R-CNN detector which functions to use the previously proposed area. This system is one unit, a detection network based on the results of the manufacture and testing of a traffic sign detection system based on the Faster R-CNN method, so it can be shown that there is no difference in the results of detection of traffic signs in day and night conditions. Where the precision testing for detection of traffic signs during the day and at night is 100%.",https://ieeexplore.ieee.org/document/9466014/,2021 International Conference on Artificial Intelligence and Mechatronics Systems (AIMS),28-30 April 2021,ieeexplore
10.1109/CIIMA50553.2020.9290291,Open Source Multichannel EMG Armband design,IEEE,Conferences,"This research paper presents the design and implementation of an open source multichannel EMG armband for hand gesture recognition. Initially a brief introduction about electromyography and similar researches is presented. Then, the general structure of the system is explained, after this it can be found the detailed description of the circuits used to acquire the signal, how the acquisition system was configured and how the communication between the EMG device and the computer was handled. Lastly, it is concluded that the device has an immense potential for EMG signal analysis and is cheap, easy to use and manufacture, however, it can be improved through the use of high precision chips like the ADC ADS1298, what gives the device greater acquisition capacities.",https://ieeexplore.ieee.org/document/9290291/,2020 IX International Congress of Mechatronics Engineering and Automation (CIIMA),4-6 Nov. 2020,ieeexplore
10.1109/IMTC.2001.928200,The development of an artificial neural network embedded automated inspection quality management system,IEEE,Conferences,This paper describes in detail the development of an innovative artificial neural network embedded automated inspection scheme for the manufacturing industry employing digital image processing techniques. Such a system is capable of performing real-time image processing tasks and identifies the size and location of the finished components on manufactured products as well as the flaws and scratches on surface of products during the manufacturing process. The proposed artificial neural network embedded quality management system provides a user-friendly user interface that has been implemented and tested on a case study from a printed circuit board manufacture. The experimental results have demonstrated the functionality and superiority of the developed artificial neural network embedded inspection system.,https://ieeexplore.ieee.org/document/928200/,IMTC 2001. Proceedings of the 18th IEEE Instrumentation and Measurement Technology Conference. Rediscovering Measurement in the Age of Informatics (Cat. No.01CH 37188),21-23 May 2001,ieeexplore
10.1109/VLSID.2018.20,Tutorial T2A: Safe Autonomous Systems: Real-Time Error Detection and Correction in Safety-Critical Signal Processing and Control Algorithms,IEEE,Conferences,"While the last two decades have seen revolutions in computing and communications systems, the next few decades will see a revolution in the use of every-day robotics and artificial intelligence in broad societal applications. Examples of such systems include sensor networks, the smart power grid, self-driven cars and autonomous drones. Such systems are driven by signal processing, control and learning algorithms that process sensor data, actuate control functions and learn about the environment in which these systems operate. The trustworthiness and safety of such systems is of paramount importance and has significant impact on the commercial viability of the underlying technology. As a consequence, anomalies in system operation due to computation errors in on-board processors, degradation and failure of embedded sensors, actuators and electro-mechanical subsystems and unforeseen changes in their operation environment need to detected with minimum latency. Such anomalies also need to be mitigated in ways that ensure the safety of such systems under all possible failure scenarios. Many future systems will be selflearning in the field. It is necessary to ensure that such learning does not compromise the safety of all human personnel involved in the operation of such systems. To enable safe operation of such systems, the underlying hardware needs to be tuned in the field to maximize performance, reliability and error-resilience while minimizing power consumption. To enable such dynamic adaptation, device operating conditions and the onset of soft errors are sensed using post-manufacture and real-time checking mechanisms. These mechanisms rely on the use of built-in sensors and/or low-overhead function encoding techniques to detect anomalies in system functions. A key capability is that of being able to deduce multiple performance parameters of the system-under-test using compact optimized stimulus using learning algorithms. The sensors and function encodings assess the loss in performance of the relevant systems due to workload uncertainties, manufacturing process imperfections, soft errors and hardware malfunction and failures induced by electromechanical degradation. These are then mitigated through the use of algorithm-through-circuit level compensation techniques based on pre-deployment simulation and post-deployment self-learning. These techniques continuously trade off performance vs. power of the individual software and hardware modules in such a way as to deliver the end-to-end desired application level Quality of Service (QoS), while minimizing energy/power consumption and maximizing reliability and safety. Applications to signal processing, and control algorithms for example autonomous systems will be discussed.",https://ieeexplore.ieee.org/document/8326883/,2018 31st International Conference on VLSI Design and 2018 17th International Conference on Embedded Systems (VLSID),6-10 Jan. 2018,ieeexplore
10.1109/TADVP.2004.828824,Intelligent SOP manufacturing,IEEE,Journals,"Microsystems packaging is fundamentally dependent on the manufacture of microelectronic, photonic, radio frequency (RF), and MEMS devices. The system-on-package (SOP) approach has been identified as a key strategy for integrating these strategic packaging technologies. Because of rising costs, the challenge before SOP manufacturers is to offset capital investment with greater automation and technological innovation in the fabrication process. To reduce manufacturing cost, several important subtasks have emerged, including increasing fabrication yield, reducing product cycle time, maintaining consistent levels of product quality and performance, and improving the reliability of processing equipment. Because of the large number of steps involved, maintaining product quality in an SOP manufacturing facility requires the control of hundreds of process variables. The interdependent issues of high yield, high quality, and low cycle time are addressed by the ongoing development of several critical capabilities in state-of-the-art computer-integrated manufacturing (CIM) systems: in situ process monitoring, process/equipment modeling, real-time process control, and equipment diagnosis. Recently, the use of computational intelligence in various manufacturing applications has increased, and the SOP manufacturing arena is no exception to this trend. Artificial neural networks, genetic algorithms (GAs), and other techniques have emerged as powerful tools for assisting CIM systems in performing various process monitoring, modeling, and control functions. This paper reviews current research in these areas, as well as the potential for deployment of these capabilities in state-of-the-art SOP manufacturing facilities.",https://ieeexplore.ieee.org/document/1331523/,IEEE Transactions on Advanced Packaging,May 2004,ieeexplore
10.1109/TASE.2017.2783342,MASD: A Multimodal Assembly Skill Decoding System for Robot Programming by Demonstration,IEEE,Journals,"Programming by demonstration (PBD) transforms the robot programming from the code level to automated interface between robot and human, promoting the flexibility of robotized automation. In this paper, we focus on programming the industrial robot for assembly tasks by parsing the human demonstration into a series of assembly skills and compiling the skill to the robot executables. To achieve this goal, an identification system using multimodal information to recognize the assembly skill, called MASD, is proposed including: 1) an initial learning stage using a hierarchical model to recognize the action by considering the features from action-object effect, gesture, and trajectory and 2) a retrospective thinking stage using a segmentation method to cut the continuous demonstrations into multiple assembly skills optimally. Using MASD, the demonstration of assembly tasks can be explained with high accuracy in real time, driving a hypothesis that a PBD system on the top of MASD can be extended to more realistic assembly tasks beyond pure positional moving and picking. In experiments, the skill identification module is used to recognize the five kinds of assembly skills in demonstrations of both single and multiple assembly skills, and outperforms the comparative action identification methods. Besides integrated with the MASD, the PBD system can generate the program based on the demonstration and successfully enable an ABB industrial robotic arm simulator to assemble a flashlight and a switch, verifying the initial hypothesis. Note to Practitioners-In the conventional robotized automation, the key role of the robot mainly owes to its capacity for repeating a wide variety of tasks with high speed and accuracy in long term, with a cost of days to months of programming for deployment. On the other hand, the new trend of customization brings the new characteristics: production in short cycle and small volume. This irreversible momentum urges the robot to switch from task to task efficiently. The biggest bottleneck here is the tedious programming, which also has high prerequisites for most practitioners in manufacturing. This situation motivates the development of a PBD system that can understand the assembly skills performed by the human experts in the demonstration and accordingly generate the program for robot's execution of the taught task. In this paper, we present a skill decoding system to parse the observational raw demonstration into symbolic sequences, which is the crucial bridge to enable the automatic programming. The system achieves high performance in recognition and is tailored for the PBD in assembly tasks by considering both advantages and disadvantages in the background of assembly, such as controllable environment and limited computational resources. It is particularly useful for assembly tasks with modularized actions based on a set of standard parts. At the perspective of industrial application, the PBD upon the proposed system is a promising solution to improve the flexibility of manufacture, which is expected to be true in midterm but an important step toward this goal.",https://ieeexplore.ieee.org/document/8263146/,IEEE Transactions on Automation Science and Engineering,Oct. 2018,ieeexplore
10.1109/WSC48552.2020.9383897,A Case Study of Digital Twin for a Manufacturing Process Involving Human Interactions,IEEE,Conferences,"Current algorithms, computations, and solutions that predict how humans will engage in smart manufacturing are insufficient for real-time activities. In this paper, a digital-twin implementation of a manual, manufacturing process is presented. This work (1) combines simulation with data from the physical world and (2) uses reinforcement learning to improve decision making on the shop floor. An adaptive simulation-based, digital twin is developed for a real manufacturing case. The digital twin demonstrates the improvement in predicting overall production output and solutions to existing problems.",https://ieeexplore.ieee.org/document/9383897/,2020 Winter Simulation Conference (WSC),14-18 Dec. 2020,ieeexplore
10.1109/ICAIIC51459.2021.9415257,A Deep Learning Module Design for Workspace Identification in Manufacturing Industry,IEEE,Conferences,"In this paper, in order to solve various problems occurring in the workspace, a deep learning-based workspace identification module was designed, and the performance was analyzed through an experiment on the recognition accuracy according to the configuration of the training dataset and the number of training. The data model of the designed deep learning module is ResNetl8, and after setting up three dataset strategies, a dataset using five types of workspaces of the manufacturing industry was selected. In terms of the average top 5 and all training, strategy 2 was 81.2% and 76.4%, respectively, confirming that it was the best among the 3 strategies. In the future, after upgrading the designed module, it is planned to implement a module with real-time workspace identification performance level of practical use in a mobile environment with an image input device installed.",https://ieeexplore.ieee.org/document/9415257/,2021 International Conference on Artificial Intelligence in Information and Communication (ICAIIC),13-16 April 2021,ieeexplore
10.1109/ICMCCE.2018.00050,A Smart Manufacturing Compliance Architecture of Electronic Batch Recording System (eBRS) for Life Sciences Industry,IEEE,Conferences,"The paradigm shift brought about by smart manufacturing or Industrie 4.0 has posed threefold challenges to electronic batch recording system (eBRS) in Life Sciences Industry: 1) the structure of the data should be informative and standard for interoperate using information models, 2) administration of synchronization between physical world and cyber world for smart decision making and optimization using cyber physical system (CPS) and 3) so-called digital manufacturing operations management (digital MOM) characterized by decentralization, comprehensive collaboration and servitization shall be implemented. Under the new situations of smart manufacturing or Industrie 4.0, the requirements from information models, CPS and digital MOM will become the most principal criteria to be considered for future eBRS/MES and other operations management information system in shop floor. To fulfill these demands, an approach combining ISA95/88 hybrid model with activities ontology and variant domain-driven design for SOA-based eBRS development has been presented. An eBRS software platform has been developed on the theoretical basis and applied to a specific application scenario of Lyophilized Injection Production for verifying its feasibility purpose.",https://ieeexplore.ieee.org/document/8537548/,"2018 3rd International Conference on Mechanical, Control and Computer Engineering (ICMCCE)",14-16 Sept. 2018,ieeexplore
10.1109/BigData.2017.8258116,A data-driven approach for improving sustainability assessment in advanced manufacturing,IEEE,Conferences,"Sustainability assessment (SA) has been one of the prime contributors to advanced manufacturing analysis, and it traditionally involves life cycle assessment (LCA) techniques for retrospective and prospective evaluations. One big challenge to reach a reliable sustainability assessment comes from the inadequate understandings of the underlying activities related to each of the product lifecycle stages based on expert knowledge. Data-driven modeling, on the other hand, is an emerging approach that takes advantage of machine-learning methods in building models that would complement or replace the knowledge-based models capturing physical behaviors. Incorporating suitable data analytics models to utilize real-time product and process data could significantly improve LCA techniques. To address the complexity and uncertainty involved in multilevel SA decision-making activities, this paper proposes a modular LCA framework to accommodate a hybrid modeling paradigm that includes knowledge-based and data-driven models. We identify and emphasize on two important challenges: (1) Generalizing knowledge-based and data-driven models into analytics models so that they can be uniformly deployed and interchanged, and (2) Modularizing the LCA decision logics and model structures so that the LCA decision process can be streamlined and easily maintained. The issues related to the decomposition, standardization, deployment and execution of analytics models are discussed in this paper. Three well-adopted standards - STEP (Standard for the Exchange of Product model data), DMN (Decision Model and Notation), and PMML (Predictive Model Markup Language) are employed to capture the product-related data/information, the decision logic decomposition of analytics models, and the structure decomposition of analytics models, respectively. The feasibility and benefits of the proposed modular, hybrid sustainability assessment methodology have been illustrated with an injection molding case study, incorporating an overall modular Scorecard-based LCA architecture with a Bayesian Network predictive model.",https://ieeexplore.ieee.org/document/8258116/,2017 IEEE International Conference on Big Data (Big Data),11-14 Dec. 2017,ieeexplore
10.1109/ROBOT.1988.12239,A knowledge-based system linking simulation to real-time control for manufacturing cells,IEEE,Conferences,A description is given of the authors' early work (1987) on the implementation of an experimental knowledge-based discrete-event simulation system. The system is being developed as an extension of the MUSE artificial intelligence (AI) toolkit to allow the reuse of modules of the simulation code to carry out real-time control. The operation of the system is illustrated by applying it to the problem of the control and scheduling of modular flexible machining cells.&lt;<ETX>&gt;</ETX>,https://ieeexplore.ieee.org/document/12239/,Proceedings. 1988 IEEE International Conference on Robotics and Automation,24-29 April 1988,ieeexplore
10.1109/ROBOT.1999.772469,A multi-contract net protocol for dynamic scheduling in flexible manufacturing systems (FMS),IEEE,Conferences,"Deals with a multi-agent architecture and a negotiation protocol for the dynamic scheduling of flexible manufacturing systems. The originality of the multi-agent architecture resides in the existence of task agents and resource agents. The multi-contract net protocol proposed, is an innovation with regard to the contract-net protocol due to the way that it makes it possible to negotiate several tasks concurrently, in real time and with more optimal results taking into account uncertainty and conflict situations in the scheduling of operations. The paper, also, stresses the efficiency and the optimality of the distributed implementation.",https://ieeexplore.ieee.org/document/772469/,Proceedings 1999 IEEE International Conference on Robotics and Automation (Cat. No.99CH36288C),10-15 May 1999,ieeexplore
10.1109/ITAIC.2011.6030279,A solution of dynamic manufacturing resource aggregation in CPS,IEEE,Conferences,"Market diversification and economic globalization bring various uncertainties to the completion of manufacturing task. It requires manufacturing resource aggregation (MRA) could adjust to dynamic changing factors (DCFs) that happen during the life cycle of MRA. The emerging Cyber Physical Systems (CPS), in which the real-time states of physical resources are fully sensed, enable to identify the DCFs in time. This supports to realize the real dynamic MRA. Take CPS as background, the paper focuses on dynamic MRA which includes two key issues, manufacturing resource virtualization model (MRVM) and dynamic service composition. In order to give a complete manufacturing resource view, MRVM which contains both static and dynamic information is built. Based on the analysis towards life cycle of MRA, the paper proposes a dynamic MRA architecture. And as the core parts of it, the implementation process of manufacturing resource service composition and a multi-level dynamic adjustment strategy are described in detail. At last, we make a conclusion.",https://ieeexplore.ieee.org/document/6030279/,2011 6th IEEE Joint International Information Technology and Artificial Intelligence Conference,20-22 Aug. 2011,ieeexplore
10.1109/ICPHYS.2018.8390759,Accented visualization by augmented reality for smart manufacturing aplications,IEEE,Conferences,"Effective application of Augmented Reality user interfaces is one of the challenging trends of industrial cyber-physical systems development and implementation. Despite reach functionality of modern AR devices (like goggles, head mounted displays or tablets) the problem of their comfortable and productive use is not completely solved yet. To cover this gap, it is proposed in this paper to implement a new paradigm of “accented visualization” that allows adapting additional data presented by AR device according to the user's current interest, attention and focus. To provide such context driven functionality there was developed intelligent software based on eye tracking and capturing the user's focus in ontology as a knowledge base. Probation and testing of the proposed approach present 89 % of the solution efficiency.",https://ieeexplore.ieee.org/document/8390759/,2018 IEEE Industrial Cyber-Physical Systems (ICPS),15-18 May 2018,ieeexplore
10.1109/BigData.2014.7004408,Advanced planning and control of manufacturing processes in steel industry through big data analytics: Case study and architecture proposal,IEEE,Conferences,"Enterprises in today's globalized world are compelled to react on threats and opportunities in a highly flexible manner. Hence, companies that are able to analyze the current state of their business processes, forecast their most optimal progresses and with this proactively control them will have a decisive competitive advantage. Technological progress in sensor technology has boosted real-time situation awareness, especially in manufacturing operations. The paper at hands examines, based on a case study stemming from the steel manufacturing industry, which production-related data is collectable using state of the art sensors forming a basis for a detailed situation awareness and for deriving accurate forecasts. However, analyses of this data point out that dedicated big data analytics approaches are required to utilize the full potential out of it. By proposing an architecture for predictive process planning and control systems, the paper intends to form a working and discussion basis for further research and implementation efforts in big data analytics.",https://ieeexplore.ieee.org/document/7004408/,2014 IEEE International Conference on Big Data (Big Data),27-30 Oct. 2014,ieeexplore
,An AI approach to scheduling in flexible manufacturing systems,IET,Conferences,"A description of an integrated intelligent simulation environment has been given. The system considers the use of an expert system (ES) to evaluate FMS simulation model data for the analysis of scheduling experimentation. A FMS model of a real system for manufacturing discrete engine components was created as a test-bed for evaluating the ES. The ES was used to analyse data from the FMS model runs and an attractive integrated environment was developed, whereby users may experiment with general 'what-if' scenarios and test the effects of adopting different scheduling policies within the FMS model framework.&lt;<ETX>&gt;</ETX>",https://ieeexplore.ieee.org/document/171849/,"1992 Third International Conference on Factory 2000, 'Competitive Performance Through Advanced Technology'",27-29 July 1992,ieeexplore
10.1109/ICMLA.2019.00171,An Encoder-Decoder Based Approach for Anomaly Detection with Application in Additive Manufacturing,IEEE,Conferences,"We present a novel unsupervised deep learning approach that utilizes an encoder-decoder architecture for detecting anomalies in sequential sensor data collected during industrial manufacturing. Our approach is designed to not only detect whether there exists an anomaly at a given time step, but also to predict what will happen next in the (sequential) process. We demonstrate our approach on a dataset collected from a real-world Additive Manufacturing (AM) testbed. The dataset contains infrared (IR) images collected under both normal conditions and synthetic anomalies. We show that our encoder-decoder model is able to identify the injected anomalies in a modern AM manufacturing process in an unsupervised fashion. In addition, our approach also gives hints about the temperature non-uniformity of the testbed during manufacturing, which was not previously known prior to the experiment.",https://ieeexplore.ieee.org/document/8999143/,2019 18th IEEE International Conference On Machine Learning And Applications (ICMLA),16-19 Dec. 2019,ieeexplore
10.1109/CSCWD.2012.6221798,An agent-based dynamic scheduling approach for flexible manufacturing systems,IEEE,Conferences,"The paper presents a dynamic scheduling approach for flexible manufacturing systems (FMS). The scheduling approach is implemented based on the negotiation and collaboration between agents in a multi-agent system (MAS) which represents the FMS. Through the collaboration between the agents in the MAS, the system exhibits the behavior that response to the disruption caused by dynamic events arise randomly in FMS such as jobs arrive over time and machines breakdown in real time and globally. The scheduling and controlling process is done on-line, without interrupting the system's operation and without user intervention. An experiment is conducted to evaluate the efficiency of the scheduling strategies exhibited by the proposed agent-based scheduling approach. The results demonstrate the superiority of the suggested scheduling approach as well as its capacity to cope with a fast changing environment.",https://ieeexplore.ieee.org/document/6221798/,Proceedings of the 2012 IEEE 16th International Conference on Computer Supported Cooperative Work in Design (CSCWD),23-25 May 2012,ieeexplore
10.1109/ICPHYS.2018.8390779,An approach for implementing key performance indicators of a discrete manufacturing simulator based on the ISO 22400 standard,IEEE,Conferences,"Performance measurement tools and techniques have become very significant in today's industries for increasing the efficiency of their processes in order to face the competitive market. The first step towards performance measurement is the real-time monitoring and gathering of the data from the manufacturing system. Applying these performance measurement techniques on real-world industry in a way that is more general and efficient is the next challenge. This paper presents a methodology for implementing the key performance indicators defined in the ISO 22400 standard-Automation systems and integration, Key performance indicators (KPIs) for manufacturing operations management. The proposed methodology is implemented on a multi robot line simulator for measuring its performance at runtime. The approach implements a knowledge-based system within an ontology model which describes the environment, the system and the KPIs. In fact, the KPIs semantic descriptions are based on the data models presented in the Key Performance Indicators Markup Language (KPIML), which is an XML implementation of models developed by the Manufacturing Enterprise Solutions Association (MESA) international organization.",https://ieeexplore.ieee.org/document/8390779/,2018 IEEE Industrial Cyber-Physical Systems (ICPS),15-18 May 2018,ieeexplore
10.1109/TENCON.2004.1414983,Certain studies on sample time for a predictive fuzzy logic controller through real time implementation of phenol-formaldehyde manufacturing,IEEE,Conferences,"In polymer manufacturing industries, the automation and control of chemical process incorporating techniques of fuzzy control neural networks, and expert systems had lead to a more secured and stable operation. A sudden and unpredictable heat is often produced by the nonlinear exothermal reaction when phenol and formaldehyde are mixed together. Therefore, the polymerization process has to be controlled with a high level of precision in order to avoid temperature run-away. This paper proposes a design methodology for a sensor based process control system. The duration of ON and OFF time of certain relays are the parameters to be controlled in order to keep the exothermic reaction under control The universe of discourse for the output of the FLC system is the sample time that assigned to the relays where maximum time for heater or valve can be turned on before the next action is applied This paper discusses a detailed real time implementation of the exothermal process control using Matlab-fuzzy logic toolbox. An enhanced predictive FLC structure is developed and compared to a predictive FLC control structure. The obtained practical results thus ensure that the predictive FLC can be enhanced by modifying the rules and the membership Junctions of the universe of discourse, which is proved to be better in controlling the reaction temperature.",https://ieeexplore.ieee.org/document/1414983/,2004 IEEE Region 10 Conference TENCON 2004.,24-24 Nov. 2004,ieeexplore
10.1109/DESSERT50317.2020.9125038,Combination of Digital Twin and Artificial Intelligence in Manufacturing Using Industrial IoT,IEEE,Conferences,"The paper focuses on Digital Twin (DT) in Manufacturing using Artificial Intelligence (AI) and Industrial IoT. According to the concept, the manufacturing includes three main units: equipment, personnel and processes. All data from these units are inherited to manufacture model (DT) and decision support system with the use of AI. DT data technology allows finding the required knowledge that can be interpreted and used to support the process of decision-making in the management of the enterprise. AI applications open up a broad spectrum of opportunities in manufacturing to add value by optimizing processes and generating new business models. The Landscape was described by a formal model to assure the possibility to analyze the state and development of landscape in detail considering DT and other technologies. DT and IIoT implementation for the simulation of real enterprise manufacturing were considered.",https://ieeexplore.ieee.org/document/9125038/,"2020 IEEE 11th International Conference on Dependable Systems, Services and Technologies (DESSERT)",14-18 May 2020,ieeexplore
10.1109/ICIAI.2019.8850773,Data Augmentation for Intelligent Manufacturing with Generative Adversarial Framework,IEEE,Conferences,"The global economy is greatly shaped by the unprecedented booming of ICT and artificial intelligence technologies. Their applications in manufacturing has led to the advent of intelligent manufacturing and industry 4.0. Data has become a precious asset for modern industry. This paper first introduces an energy monitoring and data acquisition system namely the Point Energy Technology, which has been developed by the team and installed in several industrial partners, including a local bakery. The lack of data always exists due to various reasons, such as measurement or transmission errors at data collection and transmission stage, leading to the loss of varied length of data samples that are key for process monitoring and control. To solve this problem, we introduce a generative adversarial framework which is based on a game theory for data augmentation. This framework consists of two multilayer perceptron networks, namely generator and discriminator. An improved framework with Q-net that extracts the latent variables from real data is also proposed, in which the Q-net shares the structure with discriminator except for the last layer. In addition, the two optimization methods, namely mini-batch gradient descent and adaptive moment estimation are adopted to tune the parameters. To evaluate the performance of these algorithms, energy consumption data collected from a bakery process is used in the experiment. The experimental results confirm that the latent generative adversarial framework with adaptive moment estimation could generate good quality data samples to compensate the random loss of samples in time series data.",https://ieeexplore.ieee.org/document/8850773/,2019 1st International Conference on Industrial Artificial Intelligence (IAI),23-27 July 2019,ieeexplore
10.1109/INDIN41052.2019.8972310,Data-driven modeling of semi-batch manufacturing: a rubber compounding test case,IEEE,Conferences,"The continuously growing amount of available data from manufacturing processes supports the development of data-driven models. The typical target application of these models is optimal control and continuous quality management within an objective of zero-defect manufacturing. However, data obtained from batch processes are characterized by its high dimensionality that exceeds the computational capabilities of online applications and data-driven model's reliability must be guaranteed for proper industrial implementation. We explore two approaches to reduce problem's size: feature extraction and feature selection; several multivariate regression methods are also compared regarding it precision and robustness. We base our analysis on an industrial rubber compounding process where natural rubber is blended in a semi-batch mixer with several additives, then it is further mixed up using cylinders and it is conditioned in bands for storing. For this process, real production data is collected and stored in the manufacturing execution system of the company. The objective of the analysis is to predict mechanical properties of the rubber at the end of the processes. Based on the provided data, several data-driven models are built and tested. From the comparison among them it is concluded: models based on feature extraction and artificial neural networks yield the highest accuracy, while feature-selected models provide better physical interpretability and increased robustness regarding industrial deployment.",https://ieeexplore.ieee.org/document/8972310/,2019 IEEE 17th International Conference on Industrial Informatics (INDIN),22-25 July 2019,ieeexplore
10.1109/CACRE52464.2021.9501291,Give Me a Wrench!: Finding Tools for Human Partners in Human-Robot Collaborative Manufacturing Contexts,IEEE,Conferences,"Manufacturing processes can be optimized by enabling human-robot collaboration. A relevant goal in this area is to create a collaborative solution in which robots can provide assisting actions to humans, thereby, reducing menial labor as well as increasing productivity. The solution is based on implementing efficient hand-over of mechanical tools from robots to humans. Hand-over tasks are inevitable in human-robot collaborative manufacturing contexts. These tasks need three-step mechanism: object identification, object grasping, and the actual hand-over. This paper presents an approach for robots to find tools for human partners in human-robot collaboration via deep learning. This is achieved using the object detection system YOLOv3 for identification of commonly used mechanical tools. By training on a custom dataset of 800 images of mechanical tools created for the study, the tool recognition is implemented in realworld human-robot hand-over tasks. Experimental results show that the proposed approach achieves a high accuracy for identification of tools in real-world human-robot collaboration. Future work of this study is also discussed.",https://ieeexplore.ieee.org/document/9501291/,"2021 6th International Conference on Automation, Control and Robotics Engineering (CACRE)",15-17 July 2021,ieeexplore
10.1109/IOLTS50870.2020.9159704,High-level Modeling of Manufacturing Faults in Deep Neural Network Accelerators,IEEE,Conferences,"The advent of data-driven real-time applications requires the implementation of Deep Neural Networks (DNNs) on Machine Learning accelerators. Google's Tensor Processing Unit (TPU) is one such neural network accelerator that uses systolic array-based matrix multiplication hardware for computation in its crux. Manufacturing faults at any state element of the matrix multiplication unit can cause unexpected errors in these inference networks. In this paper, we propose a formal model of permanent faults and their propagation in a TPU using the Discrete-Time Markov Chain (DTMC) formalism. The proposed model is analyzed using the probabilistic model checking technique to reason about the likelihood of faulty outputs. The obtained quantitative results show that the classification accuracy is sensitive to the type of permanent faults as well as their location, bit position and the number of layers in the neural network. The conclusions from our theoretical model have been validated using experiments on a digit recognition-based DNN.",https://ieeexplore.ieee.org/document/9159704/,2020 IEEE 26th International Symposium on On-Line Testing and Robust System Design (IOLTS),13-15 July 2020,ieeexplore
10.1109/ICSMC.2000.886346,Holonic self-organization of multi-agent systems by fuzzy modeling with application to intelligent manufacturing,IEEE,Conferences,"Holonic manufacturing aims to design standardized, modular manufacturing systems made of interchangeable parts, to enable flexibility, online reconfigurability and self-organizing capabilities for the production systems. Recent advances in distributed artificial intelligence and networking technologies have proven that theoretical multi-agent systems (MAS) concepts are very suitable for the real life implementation of holonic concepts. Building on our recent results in the design and implementation of holonic reconfigurable architectures, the paper introduces a novel approach to the online self-organization of distributed systems. By using fuzzy set and uncertainty theoretical concepts, we construct a mathematical foundation for modeling MAS, where appropriate holonic structures are identified for each particular application. This approach opens new possibilities for the design of any distributed system that needs self-organization as an intrinsic property.",https://ieeexplore.ieee.org/document/886346/,"Smc 2000 conference proceedings. 2000 ieee international conference on systems, man and cybernetics. 'cybernetics evolving to systems, humans, organizations, and their complex interactions' (cat. no.0",8-11 Oct. 2000,ieeexplore
10.23919/ACC45564.2020.9147268,Inferential Methods for Additive Manufacturing Feedback,IEEE,Conferences,"Adaptive manufacturing has revolutionized desktop prototyping and the production of physical models for non-load bearing or stress inducing applications. Many extrusion-based printers are available for purchase by entrepreneurial enthusiasts or businesses with manufacturing space limitations. These low-cost printers allow for quick prototyping but are not designed or intended for high quality production or high-cycle production, requiring extensive user tuning and upkeep to maintain the printer in usable condition. In a quest to apply modern deep learning and reinforcement learning based models, this work focuses on the development of control systems and infrastructure needed to resolve many of these intrinsic limitations of desktop 3D printers. A series of real-time agents were designed and deployed to actively monitor the printing of every layer and make continuous corrections in the printing parameters and G-code commands to reduce the variance in the tensile strength of homogeneous parts printed in a large batch.",https://ieeexplore.ieee.org/document/9147268/,2020 American Control Conference (ACC),1-3 July 2020,ieeexplore
10.1109/ICAT.2013.6684074,Intelligent system for inspection and selection of parts in a manufacturing cell,IEEE,Conferences,"This paper addresses the design and implementation of an artificial vision system implemented in a manufacturing cell. The vision system recognizes and selects in an intelligent manner the manufactured parts through a feedforward artificial neural network and the decisions are completely based on the part's color and its geometry. A simple digital camera is used as an image acquisition device. This image is then processed by an artificial neural network, which is able to identify the part's color. Then a Programmable Logic Controller (PLC) drives an electropneumatic system, in order to store the identified part into a corresponding repository. An interface based on power electronic devices and a Data Acquisition Card (DAQ) system implements the communication between the PLC and the computer. The proposed system is completely implemented and tested in a real Flexible Manufacturing System (FMS) of FESTO© showing good results.",https://ieeexplore.ieee.org/document/6684074/,"2013 XXIV International Conference on Information, Communication and Automation Technologies (ICAT)",30 Oct.-1 Nov. 2013,ieeexplore
10.1109/COASE.2016.7743572,Learning-based dynamic scheduling of semiconductor manufacturing system,IEEE,Conferences,"A learning-based scheduling framework for semiconductor manufacturing system is studied in this paper. This framework obtains a dynamic scheduling model by applying machine learning algorithm based on optimal data samples, through which an approximate optimal scheduling strategy under a certain production state can be acquired on time. And then an implementation of a dynamic scheduling model based on extreme learning machine (ELM) is proposed. In order to improve efficiency, a hybrid feature selection and classification algorithm is suggested, which combines filter feature selection method and wrapper feature selection method. Finally, the proposed dynamic scheduling model is tested in a real semiconductor manufacturing system to compare and analysis between the algorithm performance and production performance. The result indicates that the learning-based scheduling method is superior to single scheduling rules and it also meets the requirements of real-time manufacturing scheduling.",https://ieeexplore.ieee.org/document/7743572/,2016 IEEE International Conference on Automation Science and Engineering (CASE),21-25 Aug. 2016,ieeexplore
10.1109/DTPI52967.2021.9540077,Mechanical Design Paradigm based on ACP Method in Parallel Manufacturing,IEEE,Conferences,"Parallel Manufacturing is a new manufacturing paradigm in industry, deeply integrating informalization, automation, and artificial intelligence. In this paper we propose a new mechanical design paradigm in Parallel Manufacturing based on ACP method. The key is to regard the design procedure based on artificial design and emulation method as two independent procedures, which can be modeled as a parallel system. The design procedure based on ACP method does not include a real system, which is an inventive extension of the traditional parallel system. This method can be implemented with social information by introducing the definition of SDV, SDM, and Intelligent Design Manager, making it highly adaptive for social manufacturing and Parallel Manufacturing.",https://ieeexplore.ieee.org/document/9540077/,2021 IEEE 1st International Conference on Digital Twins and Parallel Intelligence (DTPI),15 July-15 Aug. 2021,ieeexplore
10.1109/ISCAS.2019.8702575,Multi-View Fusion Neural Network with Application in the Manufacturing Industry,IEEE,Conferences,"In recent years the research community and industry have paid high attention to the field of machine learning, especially deep learning. Nowadays many real-world classification or rather prediction applications are implemented by neural network models. We propose a multi-view fusion neural network with application in the manufacturing industry. Image information of multiple cameras is fused and used by the proposed model to predict the state of a manufacturing machine. Experiments show that the overall classification performance is increased from a baseline of 92.7% to 99.5% by the fusion model.",https://ieeexplore.ieee.org/document/8702575/,2019 IEEE International Symposium on Circuits and Systems (ISCAS),26-29 May 2019,ieeexplore
10.1109/AQTR.2014.6857897,Multi-agent system for heterarchical product-driven manufacturing,IEEE,Conferences,"Product-driven manufacturing has gained a lot of traction recently among practitioners as it has the potential to take flexibility and agility of the manufacturing system to a new level compared to hierarchical control models. The advances in embedded technology have created the premises for the emergence of truly intelligent products that are capable not only of identification and information storage, but also of complex behavior and local decision making. In this context, this paper proposes a multi-agent control system that aims to solve the new challenges introduced by the shift to product-driven manufacturing, specifically addressing the special needs for information flow between shop floor entities and the MES system. The paper presents the pilot implementation, using the JADE multi-agent platform, a backtracking scheduler, an artificial neural network (ANN) for local decision making and the experimental results outlining the agent processing requirements during the product lifecycle.",https://ieeexplore.ieee.org/document/6857897/,"2014 IEEE International Conference on Automation, Quality and Testing, Robotics",22-24 May 2014,ieeexplore
10.1109/CASE48305.2020.9216979,Online Computation Performance Analysis for Distributed Machine Learning Pipelines in Fog Manufacturing,IEEE,Conferences,"Smart manufacturing enables real-time data streaming from interconnected manufacturing processes to improve manufacturing quality, throughput, flexibility, and cost reduction via computation services. In these computation services, machine learning pipelines integrate various types of computation method options to match the contextualized, on-demand computation needs for the maximum prediction accuracy or the best model structure interpretation. On the other hand, there is a pressing need to integrate Fog computing in manufacturing, which will reduce communication time latency and dependency on connections, improve responsiveness and reliability of the computation services, and maintain data privacy. However, there is a knowledge gap in using machine learning pipelines in Fog manufacturing. Existing offloading strategies are not effective, due to the lack of accurate prediction model for the performance of computation services before the execution of those heterogeneous computation tasks. In this paper, machine learning pipelines are implemented in Fog manufacturing. The computation performance of each sub-step of pipelines is predicted and analyzed via linear regression models and random forest regression models. A Fog manufacturing testbed is adopted to validate the performance of the employed models. The results show that the models can adequately predict the performance of computation services, which can be further integrated into Fog manufacturing to better support offloading strategies for machine learning pipelines.",https://ieeexplore.ieee.org/document/9216979/,2020 IEEE 16th International Conference on Automation Science and Engineering (CASE),20-21 Aug. 2020,ieeexplore
10.1109/INM.2015.7140329,Ontology integration for advanced manufacturing collaboration in cloud platforms,IEEE,Conferences,"Advances in the field of cloud computing and networking have led to rapid development and market growth in areas such as online retail, gaming and healthcare. In the field of advanced manufacturing however, the impact has been significantly lesser than expected due to limitations in cloud platforms for fostering community engagement. To address this problem, we study a new cloud-based architecture that provides Platform-asa-Service (PaaS) management capabilities to the manufacturing community for delivering Software-as-a-Service (SaaS) “Apps” to their customers. Our architecture aims at supporting an “App Marketplace” that thrives on agile development, organic collaboration and scalable sales of next generation manufacturing Apps requiring high-performance simulation and modeling. Towards realizing the vision of the above architecture, our paper involves investigation and implementation of an Ontology Service that interoperates with other common web services related to resource brokering and accounting. Our Ontology Service uses principles of mapping and merging to translate a manufacturing App's collaboration requirements to suitable resource specifications on public cloud platforms. Integrated resultant ontology can be queried to provision the required resource parameters such as amount of memory/storage, number of processing units, and network protocol configurations needed for deployment of an App. We validate the effectiveness of our Ontology Service using the Protégé framework in a pilot testbed of a real-world “WheelSim” App in the NSF GENI Cloud platform. Our ontology integration results show benefits to an App developer in terms of: optimal user experience, lower design time and lower cost/simulation.",https://ieeexplore.ieee.org/document/7140329/,2015 IFIP/IEEE International Symposium on Integrated Network Management (IM),11-15 May 2015,ieeexplore
10.1109/SMILE.2018.8353980,Parametric study and design of deep learning on leveling system for smart manufacturing,IEEE,Conferences,"Sheet metal is widely used in the industry for metal forming purposes, such as metal stamping and metal cutting. It is often winded and storage in a coil form for transportation purposes. However, before any manufacturing process such as, cutting, or stamping, leveling is required as the residual stress inside coil is present which can cause distortion to the metal forming/cutting process. In conventional coil leveling machines, the machine parameters are often set by machine technicians with many years of experiences. In addition, the optimized machine parameter is achieved by trial and error method or based on experiences. However, the machine parameters are also not exactly trivial due to too many input factors which may cause changes to the outcome result. In the recent years, industry 4.0 and smart manufacturing has been a widely discussed topic in terms of industry manufacturing solutions in many different industrialized countries. In smart manufacturing, communication and interaction between machines have become an important role to improve manufacturing efficiency, flexibility and customization. As smart manufacturing focused on information process through real objects, it is required to digitize the experience through deep learning method. This paper is aimed to describe and study the deep learning application based on coil leveling system. Finally, through this study and experiment verification, analyzes on research directions and prospects of deep learning.",https://ieeexplore.ieee.org/document/8353980/,"2018 IEEE International Conference on Smart Manufacturing, Industrial & Logistics Engineering (SMILE)",8-9 Feb. 2018,ieeexplore
10.1109/GloSIC.2018.8570124,Probabilistic Estimations of Increasing Expected Reliability and Safety for Intelligent Manufacturing,IEEE,Conferences,"In the near future the possibilities of the modern probabilistic models, artificial intelligence and machine learning methods can provide an intelligent support of making decisions by an operator in real time. An agile recovery of intelligent manufacturing integrity can be implemented owing to the development of industrial robotics. For intelligent manufacturing it means the expected reliability and safety may be in the near future at the expense of intelligent support of decision making and the agile recovery of integrity. To answer the question “How much essential may be this increasing?” here are proposed: general analytical approaches for a probabilistic estimation of the expected reliability and safety for every monitored element or the system of intelligent manufacturing on a level of probability distribution functions (PDF) of the time between the losses of system integrity; estimations of increasing the expected reliability and safety for intelligent manufacturing at the expense of the intelligent support of decision making and agile recovery of integrity; the comparisons of the estimations on a prognostic period up to 10 years using the identical model in applications to expected reliability and safety. The applications of the proposed approaches allow the customers, designers, developers, users and experts of Industry 4.0 intelligent manufacturing to be guided by the proposed probabilistic estimations for solving problems of reliability and safety in the system life cycle. The results are demonstrated by examples.",https://ieeexplore.ieee.org/document/8570124/,2018 Global Smart Industry Conference (GloSIC),13-15 Nov. 2018,ieeexplore
10.1109/IEMT.1995.526119,Real-time diagnosis of semiconductor manufacturing equipment using neural networks,IEEE,Conferences,"This paper presents a tool for the real-time diagnosis of integrated circuit fabrication equipment. The approach focuses on integrating neural networks into a knowledge-based expert system. The system employs evidential reasoning to identify malfunctions by combining evidence originating from equipment maintenance history, on-line sensor data, and in-line past-process measurements. Neural networks are used in the maintenance phase of diagnosis to approximate the functional form of the failure history distribution of each component. Predicted failure rates are then converted to belief levels. For on-line diagnosis in the case of previously unencountered faults, a CUSUM control chart is implemented on real sensor data to detect very small process shifts and their trends. For the known fault case, hypothesis resting on the statistical mean and variance of the sensor data is performed to search for similar data patterns and assign belief levels. Finally, neural process models of process figures of merit (such as etch uniformity) derived from prior experimentation are used to analyze the in-line measurements, and identify the most suitable candidate among faulty input parameters (such as gas flow) to explain process shifts. A working prototype for this hybrid diagnostic system is being implemented on the Plasma Therm 700 series reactive ion etcher located in the Georgia Tech Microelectronic Research Center.",https://ieeexplore.ieee.org/document/526119/,Seventeenth IEEE/CPMT International Electronics Manufacturing Technology Symposium. 'Manufacturing Technologies - Present and Future',2-4 Oct. 1995,ieeexplore
10.1109/RTOSS.1994.292553,Real-time platforms and environments for time constrained flexible manufacturing,IEEE,Conferences,"The Spring Kernel and associated algorithms, languages, and tools provide system support for static or dynamic real-time applications that require predictable operation. Spring currently consists of two major parts: (1) the development environment, where application and target systems are described, preprocessed and downloaded, and (2) the run-time environment, where the operating system, the Spring Kernel, creates and ensures predictable executions of application tasks. We have integrated our real-time systems technology with component technologies from robotics, computer vision, and real-time artificial intelligence, to develop a test platform for flexible manufacturing. The results being produced are generic so that they should be in many other real-time applications such as air traffic control and chemical plants. We describe this platform, identify new features developed, and comment on some lessons learned to date from this experiment.&lt;<ETX>&gt;</ETX>",https://ieeexplore.ieee.org/document/292553/,Proceedings of 11th IEEE Workshop on Real-Time Operating Systems and Software,18-19 May 1994,ieeexplore
10.1109/DTPI52967.2021.9540104,Research and practice of lightweight digital twin speeding up the implementation of flexible manufacturing systems,IEEE,Conferences,"Parallel manufacturing in Industry 5.0 requires digital twin to digitize physical systems, building virtual models to open up channels connecting physical systems, information systems, and social systems, and transforming the physical models of the existing production environment to achieve two-way feedback of virtual and real is the current research direction. This paper proposes the modeling idea of lightweight digital twin, extracts core dimensions and performs digital virtual simulation, so as to quickly realize the complete process of two-way feedback, and realize a set of chess flexible parallel manufacturing production lines as a practice for the design of complete lightweight digital twin.",https://ieeexplore.ieee.org/document/9540104/,2021 IEEE 1st International Conference on Digital Twins and Parallel Intelligence (DTPI),15 July-15 Aug. 2021,ieeexplore
10.1109/ICAICA52286.2021.9498172,Research on Edge Computing of Automatic Control System of Unattended Intelligent Manufacturing Equipment,IEEE,Conferences,"In order to solve the problem that unattended substations are difficult to monitor in real time due to the large number of points, this paper proposes an intelligent classification edge computing algorithm for real-time monitoring of inbound personnel. The paper uses the continuous-time Markov algorithm to complete the intelligent classification of the personnel entering the station: record the name and time of the entry for the known personnel, and perform the alarm function and other prescribed actions for the stranger. Experimental results in practical applications show that adjusting the hyperparameters of the algorithm will achieve different sensitivity and recognition rates. After fine- tuning the hyperparameters, the accuracy of the algorithm reaches about 90%. The monitoring platform developed based on this algorithm has been deployed on smart terminals, relying on edge computing technology to realize automatic identification of unattended substation personnel entering the station, and achieved expected results in production practice.",https://ieeexplore.ieee.org/document/9498172/,2021 IEEE International Conference on Artificial Intelligence and Computer Applications (ICAICA),28-30 June 2021,ieeexplore
10.1109/ICE/ITMC52061.2021.9570221,Resilient Manufacturing Systems enabled by AI support to AR equipped operator,IEEE,Conferences,"Supply chains and manufacturing systems robustness and resilience are, for many years, but especially nowadays, key features requested to ensure reliable and efficient production processes. Two domains are crucial to achieve such purpose: the former is fast and comprehensive monitoring, efficient and reliable condition detection and effective and explicable support for decision making. The latter refers to the intervention by operators, able to better identify problems and to put in place effective operations aimed at fixing it or, better, to prevent such circumstances. This paper presents an integrated approach encompassing a sophisticated IoT and AI-based approach to monitor and detect critical situations, fully integrated with an AR (Augmented Reality) system supporting operators in the field to take informed actions in bi-directional continuous connection. Activities in the context of EC funded project Qu4lity developed in Politecnico di Milano Industry 4.0 Lab, a test environment implementing the proposed approach and demonstrating in an automated production line the effectiveness of the approach, significantly improving performances. Analysis of performance indicators demonstrates the soundness of the proposed solution and implementation methodology to make the overall production process more resilient, efficient and with product defects reduction.",https://ieeexplore.ieee.org/document/9570221/,"2021 IEEE International Conference on Engineering, Technology and Innovation (ICE/ITMC)",21-23 June 2021,ieeexplore
10.1109/IJCNN.1993.714078,Simulation of manufacturing models and its learning with artificial neural networks,IEEE,Conferences,"Introduces some new faces of simulation for intelligent manufacturing systems. Not only the productive parameters are treated from a multi-level point of view but the economic indicators were calculated with global optimistic features. The general marketing aspects are considered as critical conditions. To solve several decision making problems of a complex manufacturing organization, the authors extract meaningful variables to observe by data analysis and construct an artificial neural net. The authors try to take some advantages of non-symbolic processing such that parallel treatment and its implementation guide toward a real-time system, learning capabilities also can be applied independently to problems.",https://ieeexplore.ieee.org/document/714078/,"Proceedings of 1993 International Conference on Neural Networks (IJCNN-93-Nagoya, Japan)",25-29 Oct. 1993,ieeexplore
10.1109/CSCWD.2005.194338,Study of ASP service lifecycle management technologies for networked manufacturing system,IEEE,Conferences,"Nowadays, networked manufacturing system based on ASP (application service provider) has become one of the hotspots of research and application. How to manage large amount of ASP services, and how to provide better service quality, has become very important. The paper mainly studies the ASP service management technologies. We present the concept of service lifecycle management (SLM), defines ASP service, and set up a state model for service lifecycle. Then, we give several key technologies' solutions for implementation of service management. Finally the application of these technologies in a real networked manufacturing system is introduced.",https://ieeexplore.ieee.org/document/1504245/,"Proceedings of the Ninth International Conference on Computer Supported Cooperative Work in Design, 2005.",24-26 May 2005,ieeexplore
10.1109/icABCD49160.2020.9183853,The Impact of Smart Manufacturing Approach On The South African Manufacturing Industry,IEEE,Conferences,"SM is a technology-driven approach that mainly utilises machines to monitor the entire production of an organisation. The objective of SM in an organisation is to identify ways to automatize the manufacturing process while using data analytics to optimize the manufacturing performance. This research mitigates the impact of technology, in this case expressed as SM in South African industries. The research followed a quantitative approach whereby 42 respondents from low, medium low and high technology industries took part in the study. Data has been amassed from first-hand experience by mean of an adapted questionnaire constituted of three sections: The first section was about the general demographic information of the respondents. Section two investigates the respondent's awareness on SM. Finally, section three assessed the impact that SM had on the performance of the organisation. The findings of this study revealed that Smart Manufacturing has a positive impact in South African manufacturing organisations as it allows effective operations, fast response to customers demand, real time operations optimisation. Nevertheless, Smart Manufacturing is a new concept under the fourth industrial revolution in South Africa and will need time before being totally implemented in all organisations as it is costly.",https://ieeexplore.ieee.org/document/9183853/,"2020 International Conference on Artificial Intelligence, Big Data, Computing and Data Communication Systems (icABCD)",6-7 Aug. 2020,ieeexplore
10.1109/WF-IoT48130.2020.9221078,The Manufacturing Data and Machine Learning Platform: Enabling Real-time Monitoring and Control of Scientific Experiments via IoT,IEEE,Conferences,"IoT devices and sensor networks present new opportunities for measuring, monitoring, and guiding scientific experiments. Sensors, cameras, and instruments can be combined to provide previously unachievable insights into the state of ongoing experiments. However, IoT devices can vary greatly in the type, volume, and velocity of data they generate, making it challenging to fully realize this potential. Indeed, synergizing diverse IoT data streams in near-real time can require the use of machine learning (ML). In addition, new tools and technologies are required to facilitate the collection, aggregation, and manipulation of sensor data in order to simplify the application of ML models and in turn, fully realize the utility of IoT devices in laboratories. Here we will demonstrate how the use of the Argonne-developed Manufacturing Data and Machine Learning (MDML) platform can analyze and use IoT devices in a manufacturing experiment. MDML is designed to standardize the research and operational environment for advanced data analytics and AI-enabled automated process optimization by providing the infrastructure to integrate AI in cyber-physical systems for in situ analysis. We will show that MDML is capable of processing diverse IoT data streams, using multiple computing resources, and integrating ML models to guide an experiment.",https://ieeexplore.ieee.org/document/9221078/,2020 IEEE 6th World Forum on Internet of Things (WF-IoT),2-16 June 2020,ieeexplore
10.1109/IEMC.1998.727776,The importance of artificial intelligence-expert systems in computer integrated manufacturing,IEEE,Conferences,"In order to maintain their competitiveness, companies feel compelled to adopt productivity increasing measures. Yet, they cannot relinquish the flexibility their production cycles need in order to improve their response, and thus, their positioning in the market. To achieve this, companies must combine these two seemingly opposed principles. Thanks to new technological advances, this combination is already a working reality in some companies. It is made possible today by the implementation of computer integrated manufacturing (CIM) and artificial intelligence (AI) techniques, fundamentally by means of expert systems (ES) and robotics. Depending on how these (AI/CIM) techniques contribute to automation, their immediate effects are an increase in productivity and cost reductions. Yet also, the system's flexibility allows for easier adaptation and, as a result, an increased ability to generate value, in other words, competitiveness is improved. The authors have analyzed three studies to identify the possible benefits or advantages, as well as the inconveniences, that this type of technique may bring to companies, specifically in the production field. Although the scope of the studies and their approach differ from one to the other, their joint contribution can be of unquestionable value in order to understand a little better the importance of ES within the production system.",https://ieeexplore.ieee.org/document/727776/,IEMC '98 Proceedings. International Conference on Engineering and Technology Management. Pioneering New Technologies: Management Issues and Challenges in the Third Millennium (Cat. No.98CH36266),11-13 Oct. 1998,ieeexplore
10.1109/AQTR.2018.8402748,Time series forecasting for dynamic scheduling of manufacturing processes,IEEE,Conferences,"Manufacturing control systems evolved in the recent decades from pre-programmed rigid systems to adaptable, data driven, cloud based implementations, capable to respond to environment changes and new requirements in real time. A byproduct of this transformation is represented by large amounts of structured and semi-structured information, both historical and real-time data that is made available on various layers of the system. This accumulation of information brings the opportunity to move from the rule based decision making algorithms used traditionally by these control systems towards more intelligent approaches, driven by modern deep learning mechanisms. This paper proposes a time series forecasting model using recursive neural networks (RNN) for operation scheduling and sequencing in a virtual shop floor environment. The time series aspect of the RNN is novel in manufacturing domain, in the sense that the new best prediction produced considers the previous decisions and outcomes. The proposed implementation explains how the RNN can be mapped to the specifics of a manufacturing control system and introduces a bidding mechanism to allow dynamic evaluation of individual forecasts. The pilot implementation, initial experiments on sample data sets and results presented show how using recursive neural networks can optimize resource utilization and energy consumption.",https://ieeexplore.ieee.org/document/8402748/,"2018 IEEE International Conference on Automation, Quality and Testing, Robotics (AQTR)",24-26 May 2018,ieeexplore
10.1109/DEST.2009.5276766,Transforming SME manufacturing plants into evolvable systems through agents,IEEE,Conferences,"Manufacturing plants need to be flexible and evolvable, which might be achieved by stepping up the level of abstraction at which they are designed. In this work a methodology that starts by creating ontologies of the plants that represent information flows is proposed. Agent-based technology is then used to provide systems with the ability to evolve. The proposal is designed to suit the needs of SME' plants, whose operation cannot be disturbed while progressively undergo a transformation into more advanced ones. The methodology has been based on a partial implementation on a real case that, in its turn, has served to validate the whole procedure.",https://ieeexplore.ieee.org/document/5276766/,2009 3rd IEEE International Conference on Digital Ecosystems and Technologies,1-3 June 2009,ieeexplore
10.1109/ICRA.2019.8794123,Visual Guidance and Automatic Control for Robotic Personalized Stent Graft Manufacturing,IEEE,Conferences,"Personalized stent graft is designed to treat Abdominal Aortic Aneurysms (AAA). Due to the individual difference in arterial structures, stent graft has to be custom made for each AAA patient. Robotic platforms for autonomous personalized stent graft manufacturing have been proposed in recently which rely upon stereo vision systems for coordinating multiple robots for fabricating customized stent grafts. This paper proposes a novel hybrid vision system for real-time visual-sevoing for personalized stent-graft manufacturing. To coordinate the robotic arms, this system is based on projecting a dynamic stereo microscope coordinate system onto a static wide angle view stereo webcam coordinate system. The multiple stereo camera configuration enables accurate localization of the needle in 3D during the sewing process. The scale-invariant feature transform (SIFT) method and color filtering are implemented for stereo matching and feature identifications for object localization. To maintain the clear view of the sewing process, a visual-servoing system is developed for guiding the stereo microscopes for tracking the needle movements. The deep deterministic policy gradient (DDPG) reinforcement learning algorithm is developed for real-time intelligent robotic control. Experimental results have shown that the robotic arm can learn to reach the desired targets autonomously.",https://ieeexplore.ieee.org/document/8794123/,2019 International Conference on Robotics and Automation (ICRA),20-24 May 2019,ieeexplore
10.1109/ECC.2015.7330620,Web tension regulation with partially known periodic disturbances in roll-to-roll manufacturing systems,IEEE,Conferences,"This paper deals with the problem of synthesizing feedforward control to aid the regulation of web tension in the presence of partially known exogenous inputs. The problem appears in many engineering applications including Roll-to-Roll (R2R) manufacturing systems where the governing equation for tension is nonlinear. Currently known methods for the nonlinear output regulation problem either require the solution of a constrained partial differential equation or the preview information of the signal to be tracked. In this paper, we consider the problem of regulating web tension while rejecting periodic disturbances and use a novel approach to synthesize feedforward control as the solution of a system of differential-algebraic equations, which is considerably less complex and suitable for practical implementation. We assume that the disturbance signal is the output of a known exogenous system with unknown initial conditions. A parameter identification scheme to estimate the unknown initial conditions is developed. The proposed technique is successfully applied to web tension regulation in a large R2R machine which contains real-time hardware and software that is used in industrial practice. Extensive experiments were conducted to evaluate the proposed scheme under various experimental conditions, including different web speeds and materials. We will discuss a representative sample of the results with the proposed nonlinear tension regulator and provide a comparison with a well-tuned industrial PI control scheme to highlight the benefits of using the proposed scheme.",https://ieeexplore.ieee.org/document/7330620/,2015 European Control Conference (ECC),15-17 July 2015,ieeexplore
10.1109/TII.2019.2915846,A Global Manufacturing Big Data Ecosystem for Fault Detection in Predictive Maintenance,IEEE,Journals,"Artificial intelligence, big data, machine learning, cloud computing, and Internet of Things (IoT) are terms which have driven the fourth industrial revolution. The digital revolution has transformed the manufacturing industry into smart manufacturing through the development of intelligent systems. In this paper, a big data ecosystem is presented for the implementation of fault detection and diagnosis in predictive maintenance with real industrial big data gathered directly from large-scale global manufacturing plants, aiming to provide a complete architecture which could be used in industrial IoT-based smart manufacturing in an industrial 4.0 system. The proposed architecture overcomes multiple challenges including big data ingestion, integration, transformation, storage, analytics, and visualization in a real-time environment using various technologies such as the data lake, NoSQL database, Apache Spark, Apache Drill, Apache Hive, OPC Collector, and other techniques. Transformation protocols, authentication, and data encryption methods are also utilized to address data and network security issues. A MapReduce-based distributed PCA model is designed for fault detection and diagnosis. In a large-scale manufacturing system, not all kinds of failure data are accessible, and the absence of labels precludes all the supervised methods in the predictive phase. Furthermore, the proposed framework takes advantage of some of the characteristics of PCA such as its ease of implementation on Spark, its simple algorithmic structure, and its real-time processing ability. All these elements are essential for smart manufacturing in the evolution to Industry 4.0. The proposed detection system has been implemented into the real-time industrial production system in a cooperated company, running for several years, and the results successfully provide an alarm warning several days before the fault happens. A test case involving several outages in 2014 is reported and analyzed in detail during the experiment section.",https://ieeexplore.ieee.org/document/8710319/,IEEE Transactions on Industrial Informatics,Jan. 2020,ieeexplore
10.1109/JIOT.2019.2940131,A Two-Stage Transfer Learning-Based Deep Learning Approach for Production Progress Prediction in IoT-Enabled Manufacturing,IEEE,Journals,"In make-to-order manufacturing enterprises, accurate production progress (PP) prediction is an important basis for dynamic production process optimization and on-time delivery of orders. The implementation of Internet of Things (IoT) makes it possible to take real-time production state as an important factor affecting PP. In the IoT-enabled workshop, a two-stage transfer learning-based prediction method using both historical production data and real-time state data is proposed to solve the problem of low-prediction accuracy and poor generalization performance caused by insufficient data of target order. The deep autoencoder (DAE) model with transfer learning is designed to extract the generalized features of target order in the first stage, which uses bootstrap sampling to avoid over fitting. The deep belief network (DBN) model with transfer learning is constructed to fit the nonlinear relation for PP prediction in the second stage. A real case from an IoT enabled machining workshop is taken to validate the performance of the proposed method over the other methods such as DBN, deep neural network.",https://ieeexplore.ieee.org/document/8827506/,IEEE Internet of Things Journal,Dec. 2019,ieeexplore
10.1109/ACCESS.2020.2973336,An Effective Discrete Artificial Bee Colony Algorithm for Scheduling an Automatic-Guided-Vehicle in a Linear Manufacturing Workshop,IEEE,Journals,"This paper deals with a new automatic guided vehicle (AGV) scheduling problem from the material handling process in a linear manufacturing workshop. The problem is to determine a sequence of Cells for AGV to travel to minimize the standard deviation of the waiting time of the Cells and the total travel distance of AGV. For this purpose, we first propose an integer linear programming model based on a comprehensive investigation. Then, we present an improved nearest-neighbor-based heuristic so as to fast generate a good solution in view of the problem-specific characteristics. Next, we propose an effective discrete artificial bee colony algorithm with some novel and advanced techniques including a heuristic-based initialization, six neighborhood structures and a new evolution strategy in the onlooker bee phase. Finally, the proposed algorithms are empirically evaluated based on several typical instances from the real-world linear manufacturing workshop. A comprehensive and thorough experiment shows that the presented algorithm produces superior results which are also demonstrated to be statistically significant than the existing algorithms.",https://ieeexplore.ieee.org/document/8995549/,IEEE Access,2020,ieeexplore
10.1109/TASE.2006.886833,An Intelligent Online Monitoring and Diagnostic System for Manufacturing Automation,IEEE,Journals,"Condition monitoring and fault diagnosis in modern manufacturing automation is of great practical significance. It improves quality and productivity, and prevents damage to machinery. In general, this practice consists of two parts: 1)extracting appropriate features from sensor signals and 2)recognizing possible faulty patterns from the features. Through introducing the concept of marginal energy in signal processing, a new feature representation is developed in this paper. In order to cope with the complex manufacturing operations, three approaches are proposed to develop a feasible system for online applications. This paper develops intelligent learning algorithms using hidden Markov models and the newly developed support vector techniques to model manufacturing operations. The algorithms have been coded in modular architecture and hierarchical architecture for the recognition of multiple faulty conditions. We define a novel similarity measure criterion for the comparison of signal patterns which will be incorporated into a novel condition monitoring system. The sensor-based intelligent system has been implemented in stamping operations as an example. We demonstrate that the proposed method is substantially more effective than the previous approaches. Its unique features benefit various real-world manufacturing automation engineering, and it has great potential for shop floor applications.",https://ieeexplore.ieee.org/document/4358068/,IEEE Transactions on Automation Science and Engineering,Jan. 2008,ieeexplore
10.1109/TASE.2020.3044107,An Online Policy for Energy-Efficient State Control of Manufacturing Equipment,IEEE,Journals,"Machine state control is one of the most promising energy-efficient measures for machining processes. A proper control reduces the energy consumed during idle periods by switching off/on the machines. A critical barrier for practical implementation is related to the knowledge of part arrival process that is affected by uncertainty. The stochastic processes involved in the system are usually assumed to be known. However, real production environments are subject to several sources of randomness that are difficult to model a priori. This work provides an online time-based algorithm that is able to control the machine state. Through a method for the estimation of the stochastic process, the algorithm provides the optimal control parameters based on a collected set of observations. A new policy is formulated to manage the control over time such that changes in the control parameters are applied only under certain conditions. Potential benefits are discussed using realistic numerical cases. Note to Practitioners-This article analyzes the control problem of switching off/on a machine tool for energy saving during machine idle periods. A control policy based on time information is investigated when the machine requires a startup time to resume the service after being switched off. The proposed policy works online while acquiring information from the real system. An algorithm is described for identifying and applying the optimal control parameters. The results of this research will be useful for a practical implementation of a switching policy for energy saving. This implementation requires the estimation of the power adsorbed by the machine in four different states and, therefore, it reduces the implementation effort for practitioners.",https://ieeexplore.ieee.org/document/9308932/,IEEE Transactions on Automation Science and Engineering,April 2021,ieeexplore
10.1109/LRA.2017.2737046,Baxter's Homunculus: Virtual Reality Spaces for Teleoperation in Manufacturing,IEEE,Journals,"We demonstrate a low-cost telerobotic system that leverages commercial virtual reality (VR) technology and integrates it with existing robotics control infrastructure. The system runs on a commercial gaming engine using off-the-shelf VR hardware and can be deployed on multiple network architectures. The system is based on the homunculus model of mind wherein we embed the user in a VR control room. The control room allows for multiple sensor displays, and dynamic mapping between the user and robot. This dynamic mapping allows for selective engagement between the user and the robot. We compared our system with state-of-the-art automation algorithms and standard VR-based telepresence systems by performing a user study. The study showed that new users were faster and more accurate than the automation or a direct telepresence system. We also demonstrate that our system can be used for pick and place, assembly, and manufacturing tasks.",https://ieeexplore.ieee.org/document/8003431/,IEEE Robotics and Automation Letters,Jan. 2018,ieeexplore
10.1109/ACCESS.2021.3120843,Digital Twins From Smart Manufacturing to Smart Cities: A Survey,IEEE,Journals,"Digital twins are quickly becoming a popular tool in several domains, taking advantage of recent advancements in the Internet of Things, Machine Learning and Big Data, while being used by both the industry sector and the research community. In this paper, we review the current research landscape as regards digital twins in the field of smart cities, while also attempting to draw parallels with the application of digital twins in Industry 4.0. Although digital twins have received considerable attention in the Industrial Internet of Things domain, their utilization in smart cities has not been as popular thus far. We discuss here the open challenges in the field and argue that digital twins in smart cities should be treated differently and be considered as cyber-physical “systems of systems”, due to the vastly different system size, complexity and requirements, when compared to other recent applications of digital twins. We also argue that researchers should utilize established tools and methods of the smart city community, such as co-creation, to better handle the specificities of this domain in practice.",https://ieeexplore.ieee.org/document/9576739/,IEEE Access,2021,ieeexplore
10.1109/ACCESS.2021.3120126,Energy-Aware Flowshop Scheduling: A Case for AI-Driven Sustainable Manufacturing,IEEE,Journals,"A fully verifiable and deployable framework for optimizing schedules in a batch-based production system is proposed. The scheduler is designed to control and optimize the flow of batches of material into a network of identical and non-identical parallel and series machines that produce a high variation of complex hard metal products. The proposed multi-objective batch-based flowshop scheduling optimization (MOBS-NET) deploys a fully connected deep neural network (FCDNN) with respect to three performance criteria of energy, cost and makespan. The problem is NP-hard and considers minimizing the energy consumed per unit of product, operations cost, and the makespan. The output of the method has been validated and verified as optimal operational planning and scheduling meeting the business operational objectives. Real-time and look ahead discrete event simulation of the production process provides the feedback and assurance of the robustness and practicality of the optimum schedules prior to implementation.",https://ieeexplore.ieee.org/document/9570368/,IEEE Access,2021,ieeexplore
10.1109/70.63270,Hybrid hierarchical scheduling and control systems in manufacturing,IEEE,Journals,"Some experiments on the integration of algorithmic techniques with knowledge-based ones are discussed. Two case studies are presented: an FMS cell and a press shop. It was found that the algorithmic procedures developed for production scheduling resulted in limiting the ability to cope with the complexity of the real manufacturing world. The scheduling problem, seen as a constraint satisfaction problem, can be approached with rule-based techniques. Nevertheless, algorithmic techniques are found to be valuable for their efficiency and ability to deal with aggregated data. This ability is fundamental for an efficient implementation of hierarchical control systems in general and in the manufacturing context in particular. This suggests that the integration of rule-based techniques with algorithmic ones can increase the efficiency of searching in the space of possible solutions. The ability to deal with aggregated data can have little value when detailed real-time operation scheduling is needed. In this case, simple dispatching rules are often used, and sophisticated operations research methods are not used. In such a dynamic situation, a purely-rule based approach may be more suitable.&lt;<ETX>&gt;</ETX>",https://ieeexplore.ieee.org/document/63270/,IEEE Transactions on Robotics and Automation,Dec. 1990,ieeexplore
10.1109/TADVP.2004.828824,Intelligent SOP manufacturing,IEEE,Journals,"Microsystems packaging is fundamentally dependent on the manufacture of microelectronic, photonic, radio frequency (RF), and MEMS devices. The system-on-package (SOP) approach has been identified as a key strategy for integrating these strategic packaging technologies. Because of rising costs, the challenge before SOP manufacturers is to offset capital investment with greater automation and technological innovation in the fabrication process. To reduce manufacturing cost, several important subtasks have emerged, including increasing fabrication yield, reducing product cycle time, maintaining consistent levels of product quality and performance, and improving the reliability of processing equipment. Because of the large number of steps involved, maintaining product quality in an SOP manufacturing facility requires the control of hundreds of process variables. The interdependent issues of high yield, high quality, and low cycle time are addressed by the ongoing development of several critical capabilities in state-of-the-art computer-integrated manufacturing (CIM) systems: in situ process monitoring, process/equipment modeling, real-time process control, and equipment diagnosis. Recently, the use of computational intelligence in various manufacturing applications has increased, and the SOP manufacturing arena is no exception to this trend. Artificial neural networks, genetic algorithms (GAs), and other techniques have emerged as powerful tools for assisting CIM systems in performing various process monitoring, modeling, and control functions. This paper reviews current research in these areas, as well as the potential for deployment of these capabilities in state-of-the-art SOP manufacturing facilities.",https://ieeexplore.ieee.org/document/1331523/,IEEE Transactions on Advanced Packaging,May 2004,ieeexplore
10.1109/TIA.2019.2940585,Missing Data Imputation With OLS-Based Autoencoder for Intelligent Manufacturing,IEEE,Journals,"Motivated by the global economy that is greatly shaped by the landscape changes in energy and manufacturing where more and more devices and systems are interconnected, intelligent manufacturing in which data mining is of great importance is studied. In this article, an energy monitoring platform for small- and medium-sized enterprises developed by the point energy team (www.pointenergy.org) is first introduced, which monitors and records the energy consumption of manufacturing processes at various levels of granularity. In processing the collected data, the incompleteness in the data due to various factors needs to be addressed first otherwise it may lead to the inaccurate portrayal of the system and poor generalization of the resultant model trained by the data. Hence, a novel orthogonal-least-square-based autoencoder is proposed to generate new samples for the imputation of missing values. This approach is to learn the representative code from the original samples by constructing an improved encoder network in which the hidden neurons are orthogonal with each other. The new samples are then generated through the decoder network. The proposed approach selects the hidden neurons one by one based on the OLS estimation until an adequate network is built. The classical techniques and other generative models are compared to verify the effectiveness of the proposed algorithm. For these methods, the optimal parameters are estimated based on the performance metric of the cross-validation mean square error. In the experiment, two real industrial datasets from a baking process and a polymer extrusion process are adopted and the percentage of missing values varies from 0.02 to 0.25. The experimental results confirm that the proposed method offers stable performance in the presence of different missing ratios, and it outperforms significantly alternative approaches while the missing ratio is greater than 0.05.",https://ieeexplore.ieee.org/document/8828079/,IEEE Transactions on Industry Applications,Nov.-Dec. 2019,ieeexplore
10.1109/TMECH.2014.2366033,Output Regulation of Nonlinear Systems With Application to Roll-to-Roll Manufacturing Systems,IEEE,Journals,"This paper deals with the problem of synthesizing feedforward control to aid the regulation of output of a nonlinear system in the presence of partially known exogenous inputs. The problem appears in many engineering applications including Roll-to-Roll (R2R) manufacturing systems. Currently known methods for this problem either require the solution of a constrained partial differential equation or the preview information of the signal to be tracked. The novelty of this paper lies in synthesizing feedforward control as the solution of a system of differential-algebraic equations, which is considerably less complex and suitable for practical implementation. In this paper, we consider the problem of regulating the output while rejecting the disturbances and apply it to R2R manufacturing systems. The problem of tracking reference signals can also be handled with the suggested technique. We assume that the disturbance signal is the output of a known exogenous system with unknown initial conditions. A parameter identification scheme to estimate the unknown initial conditions is developed. The proposed technique is successfully applied to control of web tension in a large R2R machine which mimics most of the features of industrial R2R machines and contains real-time hardware and software that is used in industrial practice. Extensive experiments were conducted to evaluate the proposed scheme for web tension control under various experimental conditions, including different web speeds and materials. We will present and discuss the representative experimental results with the proposed technique and provide a comparison with an industrial PI control scheme to highlight the benefits of using the proposed scheme.",https://ieeexplore.ieee.org/document/6963413/,IEEE/ASME Transactions on Mechatronics,June 2015,ieeexplore
10.1109/3476.585143,Real-time diagnosis of semiconductor manufacturing equipment using a hybrid neural network expert system,IEEE,Journals,"This paper presents a tool for the real-time diagnosis of integrated circuit fabrication equipment. The approach focuses on integrating neural networks into an expert system. The system employs evidential reasoning to identify malfunctions by combining evidence originating from equipment maintenance history, on-line sensor data, and in-line post-process measurements. Neural networks are used in the maintenance phase of diagnosis to approximate the functional form of the failure history distribution of each component. Predicted failure rates are then converted to belief levels. For on-line diagnosis in the ease of previously unencountered faults, a CUSUM control chart is implemented on real sensor data to detect very small process shifts and their trends. For the known fault case, continuous hypothesis testing on the statistical mean and variance of the sensor data is performed to search for similar data patterns and assign belief levels. Finally, neural process models of process figures of merit (such as etch uniformity) derived from prior experimentation are used to analyze the in-line measurements, and identify the most suitable candidate among faulty input parameters (such as gas flow) to explain process shifts. A working prototype for this hybrid diagnostic system has been implemented on the Plasma Therm 700 series reactive ion etcher located in the Georgia Tech Microelectronics Research Center.",https://ieeexplore.ieee.org/document/585143/,"IEEE Transactions on Components, Packaging, and Manufacturing Technology: Part C",Jan. 1997,ieeexplore
10.1109/ACCESS.2020.3022947,Reliability Evaluation for Manufacturing System Based on Dynamic Adaptive Fuzzy Reasoning Petri Net,IEEE,Journals,"Due to failure, partial failure, or maintenance, the capacity of each machine is multi-state. Therefore, the limited relationship between the capacity of each machine and the input raw materials has to be considered. Additionally, in order to utilize the machine more effectively, the capacity of the buffers cannot be ignored, too. In this paper, a dynamic adaptive fuzzy reasoning Petri net is proposed to evaluate reliability of a manufacturing system with multiple production lines. Firstly, the model of manufacturing system is conducted, and from the perspective of demand, the minimum capacity vector and loading vector of each machine are determined. Secondly, knowledge representation and rules are formulated to establish weighted fuzzy petri nets. And the weighted fuzzy Petri net is adaptive based on the real-time level of buffers, the minimum capacity vector and loading vector. Moreover, the efficiency of product production can be improved while ensuring system reliability by adjusting the buffer level. Finally, a numerical experiment is used to demonstrate the application of our method.",https://ieeexplore.ieee.org/document/9189847/,IEEE Access,2020,ieeexplore
10.1109/TSMCC.2013.2265234,Self-Organized P2P Approach to Manufacturing Service Discovery for Cross-Enterprise Collaboration,IEEE,Journals,"The combination of service-oriented architecture (SOA) and peer-to-peer (P2P) architecture plays a promising role in distributed manufacturing environments in that the peer service can be used to facilitate the integration and discovery of distributed manufacturing resources and achieve communication and collaboration across distributed virtual enterprises. However, the large size, dynamic nature, and heterogeneous expression of distributed manufacturing resources bring forth a serious challenge in scalability and efficiency. This paper presents a self-organized P2P framework that supports scalable and efficient manufacturing service (MS) discovery for cross-enterprise collaboration by forming and maintaining autonomous enterprise peer groups (PG). Each enterprise exhibits as a peer that provides some sharable MSs that are represented comprehensively and formally with a generalized ontology. Each enterprise PG dynamically clusters a set of enterprise peers offering semantically similar MSs, and elects the most reputed peer through multicriteria trust evaluation as its core (i.e., super peer, SP). Then, a MS request can be first routed to the suitable SP and further to its leaf peer in a systematic way, thus supporting efficient service discovery. A prototype system is implemented on JXTA for real application and validated through an experimental case study.",https://ieeexplore.ieee.org/document/6600968/,"IEEE Transactions on Systems, Man, and Cybernetics: Systems",March 2014,ieeexplore
10.1109/ACCESS.2020.3032601,Sliding Mode Observer Based Multi-Layer Metal Plates Core Temperature On-Line Estimation for Semiconductor Intelligence Manufacturing,IEEE,Journals,"This paper focuses on the technological development of core temperature estimation algorithm of multi-layer metal plates. It applies new control technology in the field of real time monitoring of semiconductor producing processes. To achieve real time core temperature estimation in semiconductor equipment processes, this paper will follow the order of: system description, model derivation, parameter identification, and design of a robust core temperature observer of multi-layer metal plates, which will then be cross validated with experimental data. In the metal heating system model proposed in this paper, external cooling is considered as an unknown interference. Since the system contains an unknown interference, the sliding mode observer (SMO) will use the equivalent conversion technique to tackle with the uncertainty. In order to improve the degree of freedom and flexibility in the design of the gain matrix, and considering the effects of parameter identification uncertainty, this paper introduces the multi-objective linear matrix inequality (LMI) in the design of the sliding mode observer to suppress the impacts of the non-matching uncertainty on the system and to reduce the gain matrix which also satisfies the convergence requirement of the designer. In terms of algorithm implementation, the parameters of the thermally processed multilayer plate model are first identified through the minimum difference filter (MDF), the data is then filtered offline, and lastly, the model parameter is derived using the least square (LS) method. The filtering technology can greatly improve the accuracy of parameter identification and improve the SMO estimation precision. Finally, the experimental data of the actual semiconductor machine and the estimation from the proposed method verifies the usefulness of the SMO in core temperature estimation of multi-layer metal plate heating systems.",https://ieeexplore.ieee.org/document/9233428/,IEEE Access,2020,ieeexplore
10.1109/70.508435,Virtual-reality-based point-and-direct robotic inspection in manufacturing,IEEE,Journals,"This paper explores a flexible manufacturing paradigm in which robot grasping is interactively specified and skeletal images are efficiently used in combination to allow rapidly setting up surface flaw identification tasks in small-quantity/large-variety manufacturing. Two complementary technologies are combined to make implementation of inspection as rapid as possible. First, a novel material handling approach is described for robotic picking and placing of parts onto an inspection table using virtual tools. This allows an operator to point and give directives to set up robotic inspection tasks. Second, since specification may be approximate using this method, a fast and flexible means of identifying images of perfect and flawed parts is explored that avoids rotational or translational restrictions on workpiece placement. This is accomplished by using skeleton pixel counts as neural network inputs. The total system, including material handling and skeleton-based inspection, features flexibility during manufacturing set-up, and reduces the process time and memory requirements for workpiece inspection.",https://ieeexplore.ieee.org/document/508435/,IEEE Transactions on Robotics and Automation,Aug. 1996,ieeexplore
10.1109/IESM45758.2019.8948171,A Bi-objective Model for Dual-resource Constrained Job Shop Scheduling with the Consideration of Energy Consumption,IEEE,Conferences,"Dual resource constrained job shop problem (DR-CJSP) is an extension of the job shop scheduling problem such that each job has two available processing resources like machines and workers, each of which is of a non-identical number of operations. In many real manufacturing systems, machines often process in different processing speeds for some special demands. Note that the processing time of each job not only depends on the assigned worker but also on deployed machine. This undoubtedly leads to non-deterministic job processing time, thus resulting in different electricity consumption. We investigate the bi-objective DRCJSP problem with multi-processing speed for minimizing makespan and total electricity consumption in this work. We establish a bi-objective integer programming model and devise an epsilon constraint algorithm together with a non-dominated sorting genetic algorithm II (NSGA-II). The epsilon constraint method produces exact solutions for small job instances while NSGA-II can efficiently solve large job instances. Numerical experiments validate the proposed model and algorithms.",https://ieeexplore.ieee.org/document/8948171/,2019 International Conference on Industrial Engineering and Systems Management (IESM),25-27 Sept. 2019,ieeexplore
10.1109/FUZZ45933.2021.9494540,A Big Bang-Big Crunch Type-2 Fuzzy Logic System for Explainable Predictive Maintenance,IEEE,Conferences,"The role of maintenance in modern manufacturing systems is becoming a more significant contributor to organizational benefit. World-class enterprises are pushing forward with “predict-and prevent” maintenance instead of embracing the drawbacks of reactive maintenance (or a “fail-and fix” approach). The advancement towards Artificial Intelligence (AI), Internet of Things (IoT) and cloud computing has led to a shift in maintenance paradigms with the rising interest in Machine Learning (ML) and in particular deep learning. However, opaque box AI models are complex and difficult to understand and explain to the lay user. This limits the use of these models in predictive maintenance where it is crucial to understand and analyze the model before deployment and it is imperative to understand the logic behind any given decision. This paper introduces a Type-2 Fuzzy Logic System (FLS) optimized by the Big-Bang Big-Crunch algorithm that allows maximizing the interpretability of a model as well as its prediction accuracy for the faults which may occur in future. We tested the proposed type-2 FLS model on water pumps where data was collected in real-time by our proprietary hardware deployed at Aquatronic Group Management Plc. The observations indicate that the proposed system provides a highly interpretable and accurate model for predicting the faults in equipment for building services, process and water industries. The system predictions are used to understand why a particular fault may occur, leading to improved and better-informed service visits for the customers thus reducing the disruptions faced due to equipment failures.",https://ieeexplore.ieee.org/document/9494540/,2021 IEEE International Conference on Fuzzy Systems (FUZZ-IEEE),11-14 July 2021,ieeexplore
10.1109/RTCSA.2018.00012,A Case Study of Cyber-Physical System Design: Autonomous Pick-and-Place Robot,IEEE,Conferences,"Although modern robots in warehousing systems can perform adequately in a goods-to-person model using hand-designed algorithms that are specialized to a particular environment, developing a robotic system that is capable of handling new products at an inexpensive cost remains a challenge. A conspicuous example of this challenge is seen in Amazon's use of autonomous robots to fetch customers' orders in their massive warehouses. To encourage advance in this technology, Amazon organized the competition, Amazon Picking Challenge that asked participants to develop their own hardware and software for the general task of picking a designated set of products from inventory shelves and then placing them at a target location (called a pick-and-place task). Current technology for pick-and-place tasks is still insufficient to meet the demand for low-cost automation. Handling awkward or oddly shaped object must still depend on hand-programming or specialized robotic systems, making manufacturing automation less flexible and expensive. In this paper, we shall present the design and implementation of a software system that is a step in advancing the technology toward full automation at reasonable costs. Our system integrates a set of state-of-the-art techniques in computer vision, deep-learning, trajectory optimization, visual servoing to create a library of skills that can be composed to perform a variety of robotic tasks. We demonstrate the capability of our system for performing autonomous pick-and-place tasks with an implementation using Hoppy, an industrial robotic arm in an environment similar to the Amazon Picking Challenge.",https://ieeexplore.ieee.org/document/8607230/,2018 IEEE 24th International Conference on Embedded and Real-Time Computing Systems and Applications (RTCSA),28-31 Aug. 2018,ieeexplore
10.1109/WSC48552.2020.9383897,A Case Study of Digital Twin for a Manufacturing Process Involving Human Interactions,IEEE,Conferences,"Current algorithms, computations, and solutions that predict how humans will engage in smart manufacturing are insufficient for real-time activities. In this paper, a digital-twin implementation of a manual, manufacturing process is presented. This work (1) combines simulation with data from the physical world and (2) uses reinforcement learning to improve decision making on the shop floor. An adaptive simulation-based, digital twin is developed for a real manufacturing case. The digital twin demonstrates the improvement in predicting overall production output and solutions to existing problems.",https://ieeexplore.ieee.org/document/9383897/,2020 Winter Simulation Conference (WSC),14-18 Dec. 2020,ieeexplore
10.1109/ICAIIC51459.2021.9415257,A Deep Learning Module Design for Workspace Identification in Manufacturing Industry,IEEE,Conferences,"In this paper, in order to solve various problems occurring in the workspace, a deep learning-based workspace identification module was designed, and the performance was analyzed through an experiment on the recognition accuracy according to the configuration of the training dataset and the number of training. The data model of the designed deep learning module is ResNetl8, and after setting up three dataset strategies, a dataset using five types of workspaces of the manufacturing industry was selected. In terms of the average top 5 and all training, strategy 2 was 81.2% and 76.4%, respectively, confirming that it was the best among the 3 strategies. In the future, after upgrading the designed module, it is planned to implement a module with real-time workspace identification performance level of practical use in a mobile environment with an image input device installed.",https://ieeexplore.ieee.org/document/9415257/,2021 International Conference on Artificial Intelligence in Information and Communication (ICAIIC),13-16 April 2021,ieeexplore
10.1109/ICST46873.2019.9047714,A Fundamental Experiment on Contact Position Estimation on Vision based Dome-type Soft Tactile Sensor using Ready-made Medium,IEEE,Conferences,"Tactile sensors are critical components in robotics fields. Recently, soft tactile sensor utilizing vision is actively developed for safe human machine interaction. Some researches use novel custom-made medium in order to achieve tactile sensing. Deep learning can recognize pattern from any vision data when it has sufficient dataset, i.e., the system does not require specific pattern embedded hardware for the pattern recognition. To achieve soft tactile sensor's economical application for robot fingers, this paper presents a fundamental experiment on contract position estimation on vision based dome-type soft tactile sensor utilizing ready-made silicon as a medium and convolutional neural network. In order to estimate and classify the contact position, convolutional neural network (CNN) was applied. The modified VGGNet architecture was coded using Tensorflow and Keras. 1000 images were taken to train the modified VGG network; 200 images were taken for each neutral, left, right, lower, upper direction. For each direction, fingertip, pencil, ruler, and table corner were utilized to capture various situations. After checking the results of the test set, the trained model was applied to the embedded board and checked the contact position estimation in real-time. The experiment showed high accuracy on classifying the con-tact position of the vision based dome-type soft tactile sensor in real time. This contact position estimation system will be critical for the finger-typed robots since the system is reasonably small and it will reduce significant amount of manufacturing cost for the safe human machine interaction system. For the future work, we will acquire more image data and apply more advanced network architecture to improve accuracy.",https://ieeexplore.ieee.org/document/9047714/,2019 13th International Conference on Sensing Technology (ICST),2-4 Dec. 2019,ieeexplore
10.1109/CAMAD50429.2020.9209305,A Joint Decentralized Federated Learning and Communications Framework for Industrial Networks,IEEE,Conferences,"Industrial wireless networks are pushing towards distributed architectures moving beyond traditional server-client transactions. Paired with this trend, new synergies are emerging among sensing, communications and Machine Learning (ML) co-design, where resources need to be distributed across different wireless field devices, acting as both data producers and learners. Considering this landscape, Federated Learning (FL) solutions are suitable for training a ML model in distributed systems. In particular, decentralized FL policies target scenarios where learning operations must be implemented collaboratively, without relying on the server, and by exchanging model parameters updates rather than training data over capacity-constrained radio links. This paper proposes a real-time framework for the analysis of decentralized FL systems running on top of industrial wireless networks rooted in the popular Time Slotted Channel Hopping (TSCH) radio interface of the IEEE 802.15.4e standard. The proposed framework is suitable for neural networks trained via distributed Stochastic Gradient Descent (SGD), it quantifies the effects of model pruning, sparsification and quantization, as well as physical and link layer constraints, on FL convergence time and learning loss. The goal is to set the fundamentals for comprehensive methods and procedures supporting decentralized FL pre-deployment design. The proposed tool can be thus used to optimize the deployment of the wireless network and the ML model before its actual installation. It has been verified based on real data targeting smart robotic-assisted manufacturing.",https://ieeexplore.ieee.org/document/9209305/,2020 IEEE 25th International Workshop on Computer Aided Modeling and Design of Communication Links and Networks (CAMAD),14-16 Sept. 2020,ieeexplore
10.1109/CSCWD.2019.8791879,A Mapping Approach of Virtual-Real UR10 Twins Based on Long Short-Term Memory Neural Net,IEEE,Conferences,"Cyber-physical system integrates physical entity and its virtual model, which facilitates intelligent manufacturing a lot. Due to the geometric and non-geometric factors, transmission delay between actual and virtual environment, errors exist between desired position and actual position in real-time movement. Thus the accuracy and efficiency need to be improved. In this paper, a mapping approach of the actual UR10 robot and its virtual model based on long short-term memory neural network is developed to implement the synchronization of the virtual-real UR10 twins' behaviors in cyber-physical system. The virtual model can reflect and control the behaviors of UR10 in real time and vice versa. This method is based on a time recursive structure thus takes the temporal property of trajectory points into account. A prototype system is developed to validate its effectiveness. Experimental validation is conducted to compare the LSTM based calibration method with existing kinematic methods and multilayer perceptron neural net based methods. As demonstrated in the experiment results, the real-time mapping model of the virtual-real UR1O twins' behaviors can be obtained.",https://ieeexplore.ieee.org/document/8791879/,2019 IEEE 23rd International Conference on Computer Supported Cooperative Work in Design (CSCWD),6-8 May 2019,ieeexplore
10.1109/COASE.2019.8843232,A New Spectral Clustering Based on Particle Swarm Optimization for Unsupervised Fault Diagnosis of Bearings,IEEE,Conferences,"As a powerful data-driven method, machine learning (ML) has been widely applied in smart manufacturing. However, most existing ML methods need labeled data to train models, which is hard for real-world applications. Moreover, almost all manufacturing data are unlabeled. Therefore, the existing ML methods cannot be used to analyze the manufacturing data directly. To deal with this issue, an unsupervised fault diagnosis method based on spectral clustering is proposed in this study. Specifically, a new multi-manifold spectral clustering based on particle swarm optimization (MMSC-PSO) is proposed to handle data located on multi-manifolds by constructing affinity matrix with local tangent space. The proposed method is evaluated on the benchmark motor bearing dataset provided by the Case Western Reserve University. The experiment results show that the proposed MMSC-PSO method can achieve superior performance compared with existing methods.",https://ieeexplore.ieee.org/document/8843232/,2019 IEEE 15th International Conference on Automation Science and Engineering (CASE),22-26 Aug. 2019,ieeexplore
10.1109/APARM49247.2020.9209530,A PdM Framework Through the Event-based Genomics of Machine Breakdown,IEEE,Conferences,"A novel event-based predictive maintenance framework based on sensor signal measurements and regressive predictions to minimise machine breakdown and component failure is proposed. Such capabilities will be complemented by Event-Clustering technique to cluster and remove less impact sensor signals and also build breakdown genomics from the root of a failure in order to predict the upcoming machine breakdowns and components failures. The creation of machine breakdown genomics requires the knowledge of systems state observed as well as the state change at specified time intervals (discretization). The proposed framework is applied to a real application case study. An industrial case study of a continuous compression moulding machine that manufactures the plastic bottle closure (caps) in the beverage industry has been considered as an experiment. The machine breakdown genomics theory is tested in this case to build the sequence of events or the genomics of breakdown, where sequences of contiguous events lead to failure or healthy machine status. This is complemented by the Regression Event-Tracker method to estimates the condition monitoring of the components and provide components real-time remaining useful life estimation. The Weibull failure-rate analysis is carried out on the remaining useful life estimates for each element to understand and estimate the mean time to failure for the manufacturing machine.",https://ieeexplore.ieee.org/document/9209530/,2020 Asia-Pacific International Symposium on Advanced Reliability and Maintenance Modeling (APARM),20-23 Aug. 2020,ieeexplore
10.1109/CSCI49370.2019.00084,A Real-Time Based Intelligent System for Predicting Equipment Status,IEEE,Conferences,"In manufacturing industry, significant productivity losses arise due to equipment failures. Therefore, it is an important task to prevent the equipment from failure by monitoring each machine's sensor data in advance. However, most of the current developed systems have been only focused on monitoring the sensor data and have a difficulty in applying advanced algorithms to the real-time stream data. To address issues, we implemented an intelligent system that employs real-time streaming engine loaded with the machine learning libraries for predictive maintenance analysis. By applying a deep-learning based model to the real-time streaming data, we can provide not only trends of raw sensor data but also give an indicator representing an equipment's status in real-time. We anticipate that our system contributes to recognize the equipment's status by monitoring the indicator for productivity improvement in manufacturing industry in real-time.",https://ieeexplore.ieee.org/document/9071016/,2019 International Conference on Computational Science and Computational Intelligence (CSCI),5-7 Dec. 2019,ieeexplore
,A Smart Camera Architecture with Keypoint Description and Hybrid Processor Population,VDE,Conferences,"Increasing local processing capability of smart camera systems enables better scene analysis and more accurate decisions, and this leads to expand their application areas such as wireless sensor networks. Image understanding in smart cameras has been enhanced by the incorporation of keypoint detection and description mechanisms. Small memory footprint and high matching speed of Local Binary Descriptors (LBD) make them suitable to use in embedded applications and they also have several hardware implementations, leading them to meet real-time constraints. However, the flexibility of such available hardware is quite limited. On the other hand, it is known that biological neural networks which are naturally capable of solving these problems outperform today's electronics systems. Mimicking these structures with Spiking Neural Networks (SNN) gives very promising results, but this implementation requires special manufacturing process and lack of cost-efficiency in the current technology. Alternatively, Cellular Neural Networks (CNN) which have low-cost digital implementations, can boost the existing image analysis capabilities. In this paper, we extend our smart camera architecture with a modified CNN-based structure with 2-different types of cells showing either inhibitory or excitatory behavior. Cell connections are determined by the cell types stored on a binary identity matrix. Thus, this flexible network can be configured to incorparate with existing keypoint detection and description blocks. In addition, it also enables further analysis with complex comparisons and multiple iterations.",https://ieeexplore.ieee.org/document/8470479/,CNNA 2018; The 16th International Workshop on Cellular Nanoscale Networks and their Applications,28-30 Aug. 2018,ieeexplore
10.1109/ICMCCE.2018.00050,A Smart Manufacturing Compliance Architecture of Electronic Batch Recording System (eBRS) for Life Sciences Industry,IEEE,Conferences,"The paradigm shift brought about by smart manufacturing or Industrie 4.0 has posed threefold challenges to electronic batch recording system (eBRS) in Life Sciences Industry: 1) the structure of the data should be informative and standard for interoperate using information models, 2) administration of synchronization between physical world and cyber world for smart decision making and optimization using cyber physical system (CPS) and 3) so-called digital manufacturing operations management (digital MOM) characterized by decentralization, comprehensive collaboration and servitization shall be implemented. Under the new situations of smart manufacturing or Industrie 4.0, the requirements from information models, CPS and digital MOM will become the most principal criteria to be considered for future eBRS/MES and other operations management information system in shop floor. To fulfill these demands, an approach combining ISA95/88 hybrid model with activities ontology and variant domain-driven design for SOA-based eBRS development has been presented. An eBRS software platform has been developed on the theoretical basis and applied to a specific application scenario of Lyophilized Injection Production for verifying its feasibility purpose.",https://ieeexplore.ieee.org/document/8537548/,"2018 3rd International Conference on Mechanical, Control and Computer Engineering (ICMCCE)",14-16 Sept. 2018,ieeexplore
10.1109/PACET48583.2019.8956270,A Smart Recycling Bin for Waste Classification,IEEE,Conferences,"As there is an obvious and increasing need to preserve valuable resources and reduce waste and pollution, several researches are focusing into this area. However, the solutions provided are neither budget-friendly nor effective to be practical in a real-world application. In this paper, we present a Smart Recycling Bin using modern approaches for waste classification. The design of the system permits the low-cost manufacturing of the final product, while uses state of the art technologies such as neural networks and the LoRaWAN protocol. We implemented a low-cost Smart Bin prototype able to classify different types of waste with an accuracy of 92.1%. The system also remotely transmits valuable data to the corresponding authorities, which can increase their effectiveness in waste management. Index Terms-Smartbin, Recycling, LoRa, Raspberry Pi, embedded system.",https://ieeexplore.ieee.org/document/8956270/,2019 Panhellenic Conference on Electronics & Telecommunications (PACET),8-9 Nov. 2019,ieeexplore
10.1109/PTC.2019.8810705,A Smart Voltage Optimization Approach for Industrial Load Demand Response,IEEE,Conferences,"This paper proposes a generic and comprehensive Voltage Optimization (VO) strategy for energy savings by industrial customers, to lower operating expenses through the implementation of an optimal process-based Demand Response (DR) program without affecting the real-time manufacturing process. This strategy takes into account the complex nature of industrial loads and their unique set of operating constraints, to reduce energy demand for industrial customers by means of varying the voltage at the utility service entrance to the plant. The proposed approach utilizes a Neural Network (NN) model of the industrial load, trained using historical operating data, to estimate the real power consumption of the load, based on the bus voltage and overall plant process. The NN load model is incorporated into the proposed VO model, whose objective is the minimization of the energy drawn from the substation and the number of switching operations of Load Tap Changers (LTC). The proposed VO framework is tested on a real plant model developed using actual measured data. The results demonstrate that the proposed technique can be successfully implemented by industrial customers and plant operators to enhance energy savings compared to Conservation Voltage Reduction (CVR) approaches, and also as a DR strategy that effectively manages the dependence of industrial loads on time-sensitive and critical manufacturing processes.",https://ieeexplore.ieee.org/document/8810705/,2019 IEEE Milan PowerTech,23-27 June 2019,ieeexplore
10.1109/SMC.2019.8914195,A Universal Methodology to Create Digital Twins for Serial and Parallel Manipulators,IEEE,Conferences,"With the technological advances in information technology especially in sensorization, artificial intelligence, big data and visualization; smart manufacturing and industrie 4.0 are gradually becoming an implementable reality. Digital twin is one of the pillars of smart manufacturing where by the physical and virtual worlds can by synced and mimic each others' behaviour. In future, assets and products can sense their state and report back any anomalities so that meaningful insights can be drawn and actions can be taken to keep production optimized at all times. This reporting feature can be realized with the help of digital twin technology as the twin keeps record of the physical asset's behavior. A digital twin will play an integral role in defining the concept of an integrated shopfloor. It will assist in viewing holistic behavior of asset, optimizing processes and exerting control over the physical device. To demonstrate the concept of digital twin, an universal methodology was developed and deployed at Model Factory @ ARTC (Advanced Remanufacturing and Technology Centre) program in Singapore.",https://ieeexplore.ieee.org/document/8914195/,"2019 IEEE International Conference on Systems, Man and Cybernetics (SMC)",6-9 Oct. 2019,ieeexplore
10.1109/PacificVis.2018.00026,A Visual Analytics Approach for Equipment Condition Monitoring in Smart Factories of Process Industry,IEEE,Conferences,"Monitoring equipment conditions is of great value in manufacturing, which can not only reduce unplanned downtime by early detecting anomalies of equipment but also avoid unnecessary routine maintenance. With the coming era of Industry 4.0 (or industrial internet), more and more assets and machines in plants are equipped with various sensors and information systems, which brings an unprecedented opportunity to capture large-scale and fine-grained data for effective on-line equipment condition monitoring. However, due to the lack of systematic methods, analysts still find it challenging to carry out efficient analyses and extract valuable information from the mass volume of data collected, especially for process industry (e.g., a petrochemical plant) with complex manufacturing procedures. In this paper, we report the design and implementation of an interactive visual analytics system, which helps managers and operators at manufacturing sites leverage their domain knowledge and apply substantial human judgements to guide the automated analytical approaches, thus generating understandable and trustable results for real-world applications. Our system integrates advanced analytical algorithms (e.g., Gaussian mixture model with a Bayesian framework) and intuitive visualization designs to provide a comprehensive and adaptive semi-supervised solution to equipment condition monitoring. The example use cases based on a real-world manufacturing dataset and interviews with domain experts demonstrate the effectiveness of our system.",https://ieeexplore.ieee.org/document/8365986/,2018 IEEE Pacific Visualization Symposium (PacificVis),10-13 April 2018,ieeexplore
10.1109/ISCAS.2010.5537177,A camera based closed loop control system for keyhole welding processes: Algorithm comparison,IEEE,Conferences,"Real time monitoring of laser welding has a more and more importance in several manufacturing processes ranging from automobile production to precision mechanics. Despite the huge improvement in welding technology, sophisticated image based closed loop control systems have not been integrated in commercially available equipments yet. Due to the high dynamics of laser beam welding (LBW) processes, robust closed loop control systems require fast real time image processing with frame rates in the multi kilo Hertz range. In the last few years, some new high speed Cellular Neural Network (CNN) based algorithms for the full penetration hole detection in keyhole welding processes have been introduced. In particular, they can be distinguished in two categories: Orientation dependent and orientation independent algorithms. The former can be used only for the welding of straight lines, while the latter has been implemented for the control of curved weld seams. Both algorithms have been used to build up a real time closed loop control system for LBW processes. An algorithm comparison by the description of some experimental results is addressed in this paper.",https://ieeexplore.ieee.org/document/5537177/,Proceedings of 2010 IEEE International Symposium on Circuits and Systems,30 May-2 June 2010,ieeexplore
10.1109/BigData.2017.8258116,A data-driven approach for improving sustainability assessment in advanced manufacturing,IEEE,Conferences,"Sustainability assessment (SA) has been one of the prime contributors to advanced manufacturing analysis, and it traditionally involves life cycle assessment (LCA) techniques for retrospective and prospective evaluations. One big challenge to reach a reliable sustainability assessment comes from the inadequate understandings of the underlying activities related to each of the product lifecycle stages based on expert knowledge. Data-driven modeling, on the other hand, is an emerging approach that takes advantage of machine-learning methods in building models that would complement or replace the knowledge-based models capturing physical behaviors. Incorporating suitable data analytics models to utilize real-time product and process data could significantly improve LCA techniques. To address the complexity and uncertainty involved in multilevel SA decision-making activities, this paper proposes a modular LCA framework to accommodate a hybrid modeling paradigm that includes knowledge-based and data-driven models. We identify and emphasize on two important challenges: (1) Generalizing knowledge-based and data-driven models into analytics models so that they can be uniformly deployed and interchanged, and (2) Modularizing the LCA decision logics and model structures so that the LCA decision process can be streamlined and easily maintained. The issues related to the decomposition, standardization, deployment and execution of analytics models are discussed in this paper. Three well-adopted standards - STEP (Standard for the Exchange of Product model data), DMN (Decision Model and Notation), and PMML (Predictive Model Markup Language) are employed to capture the product-related data/information, the decision logic decomposition of analytics models, and the structure decomposition of analytics models, respectively. The feasibility and benefits of the proposed modular, hybrid sustainability assessment methodology have been illustrated with an injection molding case study, incorporating an overall modular Scorecard-based LCA architecture with a Bayesian Network predictive model.",https://ieeexplore.ieee.org/document/8258116/,2017 IEEE International Conference on Big Data (Big Data),11-14 Dec. 2017,ieeexplore
10.1109/AIHAS.1990.93923,A hierarchical and modular structure for FMS control and monitoring,IEEE,Conferences,"A hierarchical modular structure is presented for real-time control and monitoring of flexible manufacturing systems (FMSs), in which a clear distinction between processing of normal and unusual states is made. It is shown that this approach furthers autonomous diagnosis and recovery, because it makes it possible to integrate two techniques well suited in their own domains: Petri nets for the specification of normal sequences of control and AI techniques for dealing with diagnostic problems. The implementation techniques proposed are similar to rapid prototyping by execution of the specification (token player for Petri nets and inference engine for production rules).&lt;<ETX>&gt;</ETX>",https://ieeexplore.ieee.org/document/93923/,"Proceedings [1990]. AI, Simulation and Planning in High Autonomy Systems",26-27 March 1990,ieeexplore
10.1109/ROBOT.1999.772469,A multi-contract net protocol for dynamic scheduling in flexible manufacturing systems (FMS),IEEE,Conferences,"Deals with a multi-agent architecture and a negotiation protocol for the dynamic scheduling of flexible manufacturing systems. The originality of the multi-agent architecture resides in the existence of task agents and resource agents. The multi-contract net protocol proposed, is an innovation with regard to the contract-net protocol due to the way that it makes it possible to negotiate several tasks concurrently, in real time and with more optimal results taking into account uncertainty and conflict situations in the scheduling of operations. The paper, also, stresses the efficiency and the optimality of the distributed implementation.",https://ieeexplore.ieee.org/document/772469/,Proceedings 1999 IEEE International Conference on Robotics and Automation (Cat. No.99CH36288C),10-15 May 1999,ieeexplore
10.1109/ATEE.2013.6563523,A recognition system of components from a production line using neuronal networks,IEEE,Conferences,"We developed and implemented a system for real time automatic recognition of components from a production line using neuronal networks. The capture device (Web camera) is placed over the production line and the system can identify the type of component even if this is not in the correct position or centered. The device sends the information to a recognition system (software) which identifies the type of component and its parts. Once all parts of the component are identified by the system, the operator is informed about the type of component and this is recorded in the fabrication's report. When a manufacturing defect is observed, the system alerts the operator and the component is recorded as inappropriate. This system can be used for real time recognition of the components. In addition, if the component is an electrical board for example, the system can also decide if all components are present in the correct position.",https://ieeexplore.ieee.org/document/6563523/,2013 8TH INTERNATIONAL SYMPOSIUM ON ADVANCED TOPICS IN ELECTRICAL ENGINEERING (ATEE),23-25 May 2013,ieeexplore
10.1109/ETFA.2003.1248742,A reference architecture and functional model for monitoring and diagnosis of large automated systems,IEEE,Conferences,"This research develops a reference architecture and functional model for intelligent monitoring and fault diagnosis of large complex automated systems in manufacturing and logistics. This reference architecture organises the monitoring and diagnosis functions in a modified hierarchical manner with multiple levels, and is therefore easily scalable to meet growing requirements of different application scenarios. The architecture is efficient as it allows problems to be quickly dealt with closer to their sources; therefore minimising intra-level data communication and messaging. Similarly the proposed functional model for monitoring and diagnosis unit can be adopted (i.e., scaled up or down) to suit the needs of the application, and indeed it fits well into the proposed reference architecture. A successful case of applying the proposed architecture and model is presented which serves to illustrate how they can be implemented in real-life to solve a class of monitoring and diagnosis problems for large automated systems typically found in manufacturing and logistics.",https://ieeexplore.ieee.org/document/1248742/,EFTA 2003. 2003 IEEE Conference on Emerging Technologies and Factory Automation. Proceedings (Cat. No.03TH8696),16-19 Sept. 2003,ieeexplore
10.1109/BigData.2015.7364098,A scalable solution for group feature selection,IEEE,Conferences,"In many applications, we may want to build a classifier with high confidence, while reducing the number of features. We consider the case where features are assigned to predefined groups and cannot be removed individually. An additional and important constraint is that the datasets may be very large and may not fit in memory. We use logistic regression with group penalty, which results in sparse solutions at the group level. In our implementation, we apply L-BFGS to approximate the quadratic loss function of logistic regression and use Block Co-ordinate Descent to solve for each group. Our contributions can be summarized as follows: (1) we discuss different scalable approaches, depending on characteristics of the dataset, such as, large number of data points or large number of features or large number of groups; (2) for datasets with large number of data points and few groups of features, we identify the bottlenecks for scalability; (3) we present Spark solutions in Python and discuss the advantages of our solution over alternate solutions; (4) we present the experiments and results on synthetic data and real data from manufacturing applications.",https://ieeexplore.ieee.org/document/7364098/,2015 IEEE International Conference on Big Data (Big Data),29 Oct.-1 Nov. 2015,ieeexplore
10.1109/ITAIC.2011.6030279,A solution of dynamic manufacturing resource aggregation in CPS,IEEE,Conferences,"Market diversification and economic globalization bring various uncertainties to the completion of manufacturing task. It requires manufacturing resource aggregation (MRA) could adjust to dynamic changing factors (DCFs) that happen during the life cycle of MRA. The emerging Cyber Physical Systems (CPS), in which the real-time states of physical resources are fully sensed, enable to identify the DCFs in time. This supports to realize the real dynamic MRA. Take CPS as background, the paper focuses on dynamic MRA which includes two key issues, manufacturing resource virtualization model (MRVM) and dynamic service composition. In order to give a complete manufacturing resource view, MRVM which contains both static and dynamic information is built. Based on the analysis towards life cycle of MRA, the paper proposes a dynamic MRA architecture. And as the core parts of it, the implementation process of manufacturing resource service composition and a multi-level dynamic adjustment strategy are described in detail. At last, we make a conclusion.",https://ieeexplore.ieee.org/document/6030279/,2011 6th IEEE Joint International Information Technology and Artificial Intelligence Conference,20-22 Aug. 2011,ieeexplore
10.1109/ICIAS.2007.4658557,A study on industrial communication networking: Ethernet based implementation,IEEE,Conferences,"Recent enhancement of the industrial communications and networking are possible to apply in Ethernet networks system at all levels of industrial automation, especially in the controller level whereby the data exchanges in real-time communication is mandatory. This paper is about a study on the development of industrial communications network based on the Ethernet protocol and thus implement it into computer integrated manufacturing (CIM) system. The purpose of this paper is to overcome real-time communication in which the accessibility of data exchange is very difficult in terms of retrieving data from other stations and time consuming. The Ethernet module is installed onto supervisory OMRON PLC to integrate several of stations in the CIM-70A system which is located at Robotic Laboratory in Universiti Tun Hussein Onn Malaysia (UTHM). The workability of this communication technique is analyzed and compared with the conventional serial communication which widely used in automation networking systems. It is found that, the Ethernet protocol approach through the communication and integration of CIM system can be accessed easily and available to be upgraded at the management and enterprise levels of industrial automation system.",https://ieeexplore.ieee.org/document/4658557/,2007 International Conference on Intelligent and Advanced Systems,25-28 Nov. 2007,ieeexplore
10.1109/BigData.2014.7004408,Advanced planning and control of manufacturing processes in steel industry through big data analytics: Case study and architecture proposal,IEEE,Conferences,"Enterprises in today's globalized world are compelled to react on threats and opportunities in a highly flexible manner. Hence, companies that are able to analyze the current state of their business processes, forecast their most optimal progresses and with this proactively control them will have a decisive competitive advantage. Technological progress in sensor technology has boosted real-time situation awareness, especially in manufacturing operations. The paper at hands examines, based on a case study stemming from the steel manufacturing industry, which production-related data is collectable using state of the art sensors forming a basis for a detailed situation awareness and for deriving accurate forecasts. However, analyses of this data point out that dedicated big data analytics approaches are required to utilize the full potential out of it. By proposing an architecture for predictive process planning and control systems, the paper intends to form a working and discussion basis for further research and implementation efforts in big data analytics.",https://ieeexplore.ieee.org/document/7004408/,2014 IEEE International Conference on Big Data (Big Data),27-30 Oct. 2014,ieeexplore
10.1109/DCOSS49796.2020.00046,An Agnostic Data-Driven Approach to Predict Stoppages of Industrial Packing Machine in Near,IEEE,Conferences,"As data awareness in manufacturing companies increases with the deployment of sensors and Internet of Things (IoT) devices, data-driven maintenance and prediction have become quite popular in the Industry 4.0 paradigm. Machine Learning (ML) has been recognised as a promising, efficient and reliable tool for fault detection use cases, as it allows to export important knowledge from monitored assets. Scientists deal with issues such as the small amount of data that indicate potential problems, or the imbalance which exists between the standard process data and the data inadequacy of the systems to make a high precision forecast. Currently, in this context, even large industries are not able to effectively predict abnormal behaviors in their tools, processes and equipment, when adopting strategies to anticipate crucial events. In this paper, we propose a methodology to enable prediction of a packing machine's stoppages in manufacturing process of a large industry, by using forecasting techniques based on univariate time series data. There are more than 100 reasons that cause the machine to stop, in a quite big production line length. However, we use a single signal, concerning the machines operational status to make our prediction, without considering other fault or warning signals, hence its characterization as ""agnostic"". A workflow is presented for cleaning and preprocessing the data, and for training and evaluating a predictive model. Two predictive models, namely ARIMA and Prophet, are applied and evaluated on real data from an advanced machining process used for packing. Training and evaluation tests indicate that the results of the applied methods perform well on a daily basis. Our work can be further extended and act as reference for future research activities that could lead to more robust and accurate prediction frameworks.",https://ieeexplore.ieee.org/document/9183540/,2020 16th International Conference on Distributed Computing in Sensor Systems (DCOSS),25-27 May 2020,ieeexplore
10.1109/ECTC32696.2021.00346,An Automated Optical Inspection System for PIP Solder Joint Classification Using Convolutional Neural Networks,IEEE,Conferences,"In the fields of electronics manufacturing, the application of through-hole devices is still required, as heat dissipation and high current carrying capacity plays an important role. To ensure the highest quality standards, these electronics production processes take a multitude of inspection processes into account. For the detection of error patterns regarding the quality of the solder connections, usually, high-end inspection machines are utilized in the industrial application. The Automated Optical Inspection is a commonly conducted process, using visible light and rule-based inspection routines, setup by process experts for the evaluation of the Region of Interest. The high overhead of creating and maintaining product-specific checking routines and machine acquisition leads to increased costs and severe dependency on expert know-how. A flexible inspection algorithm, implemented into low-cost equipment for image generation is expected to reduce acquisition and optimization costs, and lower dependency on expert knowledge and high-end machinery. In this contribution, we present a novel framework for the automatic, near real-time solder joint classification based on Convolutional Neural Networks, flexibly detecting, and classifying solder connections. We utilize existing Deep Learning architectures for detection and classification. The localization model utilizes a YOLO-architecture (you-only-look-once), learning feature inputs based on a supervised learning approach. Pseudo-labeling is carried out automatically by an anomaly detection model. The image generation is executed by an industrial low-cost camera and an industrial rack-PC. The developed prototype is integrated into the existing production infrastructure. The results indicate a satisfactory detection and classification of the investigated solder connections with the proposed system. Hence, this system represent an alternative to commercially available high-end inspection systems being used for an inline control of Pin-in-Paste and through-hole device solder connections.",https://ieeexplore.ieee.org/document/9501916/,2021 IEEE 71st Electronic Components and Technology Conference (ECTC),1 June-4 July 2021,ieeexplore
10.1109/ICMLA.2019.00171,An Encoder-Decoder Based Approach for Anomaly Detection with Application in Additive Manufacturing,IEEE,Conferences,"We present a novel unsupervised deep learning approach that utilizes an encoder-decoder architecture for detecting anomalies in sequential sensor data collected during industrial manufacturing. Our approach is designed to not only detect whether there exists an anomaly at a given time step, but also to predict what will happen next in the (sequential) process. We demonstrate our approach on a dataset collected from a real-world Additive Manufacturing (AM) testbed. The dataset contains infrared (IR) images collected under both normal conditions and synthetic anomalies. We show that our encoder-decoder model is able to identify the injected anomalies in a modern AM manufacturing process in an unsupervised fashion. In addition, our approach also gives hints about the temperature non-uniformity of the testbed during manufacturing, which was not previously known prior to the experiment.",https://ieeexplore.ieee.org/document/8999143/,2019 18th IEEE International Conference On Machine Learning And Applications (ICMLA),16-19 Dec. 2019,ieeexplore
10.1109/CASE48305.2020.9216773,An Enhanced Fault Diagnosis Method with Uncertainty Quantification Using Bayesian Convolutional Neural Network,IEEE,Conferences,"Fault diagnosis is a vital technique to pinpoint the machine malfunctions in manufacturing systems. In recent years, the deep learning techniques greatly improve the fault detection accuracy, but there still remain some problems. If one fault is absent in the training data or the fault signal is disturbed by severe noise interference, the fault classifier may misjudge the health state. This problem limits the reliability of the fault diagnosis in real applications. In this paper, we enhance the fault diagnosis method by using Bayesian Convolutional Neural Network (BCNN). A Shannon entropy-based method is presented to quantify the prediction uncertainty. The BCNN turns the deterministic predictions to probabilistic distributions and enhances the robustness of the fault diagnosis. The uncertainty quantification method helps to indicate the wrong predictions, detect unknown faults, and discover the strong disturbances. Then, a fine-tuning strategy is applied to enhance the model performance further. The potential usability of the proposed method in monitoring the motors of 3D printers is studied. And the experiment is conducted on a motor bearing dataset provided by Case Western Reserve University. The proposed BCNN achieves 99.82% fault classification accuracy over nine health conditions. Its robustness is verified by comparing the testing accuracy with three other methods on the noisy datasets. And the uncertainty quantification method successfully detects the outlier inputs.",https://ieeexplore.ieee.org/document/9216773/,2020 IEEE 16th International Conference on Automation Science and Engineering (CASE),20-21 Aug. 2020,ieeexplore
10.1109/COASE.2018.8560557,An Industry 4.0 Cyber-Physical Framework for Micro Devices Assembly,IEEE,Conferences,"An advanced cyber manufacturing framework to support the collaborative assembly of micro devices is presented based on Industry 4.0 principles. The distributed cyber and physical components work together to plan, assemble and monitor micro assembly related tasks; micro assembly refers to the assembly of micron sized devices which cannot be manufactured by MEMS technologies. The collaborative framework proposed includes assembly planning and path planning modules, Virtual Reality based assembly simulation environments and physical assembly work cells. An ontology based approach was implemented to address semantic interoperability issues to support formation of temporary partnerships in a Virtual Enterprise context. The key to the design and implementation of this complex framework is an information centric process modeling approach which provides a data/information oriented basis for collaboration. A collaborative cyber physical test bed has been built to demonstrate feasibility of proposed framework and approach.",https://ieeexplore.ieee.org/document/8560557/,2018 IEEE 14th International Conference on Automation Science and Engineering (CASE),20-24 Aug. 2018,ieeexplore
10.1109/WCICA.2014.7052883,An adaptive neural network backstepping control method for reticle stage of lithography,IEEE,Conferences,"This paper proposes an adaptive neural network backstepping tracking control method for the long-stroke reticle stage of lithography. The main feature of this method is that it can real-time online estimate the unknown model parameters of long-stroke reticle stage actuator under some nonlinear effects including load disturbance, force ripple and slot effect by radial basis function(RBF) neural network. First, in the study according to the actual system a nominal model of long-stroke reticle stage is established. Then, with this nominal model the derivation of the control method, the design of the adaptive law and the Lyapunov stability proof are made. From the results of the analysis above, it can be seen that this control method can sufficiently guarantee the convergence of the system's position and velocity errors. At last, this control method is verified by a simulation experiment. From the result of fifth-order S-curve tracking test, the tracking accuracy of the position and velocity can meet the design requirements well. Because it doesn't need to model the actual mechanical system and external uncertainties precisely, this method is very suitable for application in the field of ultra-precision manufacturing.",https://ieeexplore.ieee.org/document/7052883/,Proceeding of the 11th World Congress on Intelligent Control and Automation,29 June-4 July 2014,ieeexplore
10.1109/CSCWD.2012.6221798,An agent-based dynamic scheduling approach for flexible manufacturing systems,IEEE,Conferences,"The paper presents a dynamic scheduling approach for flexible manufacturing systems (FMS). The scheduling approach is implemented based on the negotiation and collaboration between agents in a multi-agent system (MAS) which represents the FMS. Through the collaboration between the agents in the MAS, the system exhibits the behavior that response to the disruption caused by dynamic events arise randomly in FMS such as jobs arrive over time and machines breakdown in real time and globally. The scheduling and controlling process is done on-line, without interrupting the system's operation and without user intervention. An experiment is conducted to evaluate the efficiency of the scheduling strategies exhibited by the proposed agent-based scheduling approach. The results demonstrate the superiority of the suggested scheduling approach as well as its capacity to cope with a fast changing environment.",https://ieeexplore.ieee.org/document/6221798/,Proceedings of the 2012 IEEE 16th International Conference on Computer Supported Cooperative Work in Design (CSCWD),23-25 May 2012,ieeexplore
10.1109/ICPHYS.2018.8390779,An approach for implementing key performance indicators of a discrete manufacturing simulator based on the ISO 22400 standard,IEEE,Conferences,"Performance measurement tools and techniques have become very significant in today's industries for increasing the efficiency of their processes in order to face the competitive market. The first step towards performance measurement is the real-time monitoring and gathering of the data from the manufacturing system. Applying these performance measurement techniques on real-world industry in a way that is more general and efficient is the next challenge. This paper presents a methodology for implementing the key performance indicators defined in the ISO 22400 standard-Automation systems and integration, Key performance indicators (KPIs) for manufacturing operations management. The proposed methodology is implemented on a multi robot line simulator for measuring its performance at runtime. The approach implements a knowledge-based system within an ontology model which describes the environment, the system and the KPIs. In fact, the KPIs semantic descriptions are based on the data models presented in the Key Performance Indicators Markup Language (KPIML), which is an XML implementation of models developed by the Manufacturing Enterprise Solutions Association (MESA) international organization.",https://ieeexplore.ieee.org/document/8390779/,2018 IEEE Industrial Cyber-Physical Systems (ICPS),15-18 May 2018,ieeexplore
10.1109/ISWCS.2018.8491060,Analysis of Machine Learning Algorithms for Spectrum Decision in Cognitive Radios,IEEE,Conferences,"Technological advances in recent years have reduced the manufacturing costs of wireless devices, increasing the number of such devices and applications. Most of these applications are supported by ISM (Industrial, Scientific, and Medical) frequencies, which due to their wide use in several types of devices have suffered from harmful interference. To solve this problem, Cognitive Radios paradigm has been proposed to guarantee the quality of communication. Several frameworks were proposed for the development of a Cognitive Radios Networks (CRN), but none of them were effectively implemented in hardware. This paper presents an analysis of machine learning algorithms in architecture for the development of CRN in real hardware. Results demonstrated the feasibility of the architecture and the decision methods based on machine learning algorithms can find the best communication channel.",https://ieeexplore.ieee.org/document/8491060/,2018 15th International Symposium on Wireless Communication Systems (ISWCS),28-31 Aug. 2018,ieeexplore
10.1109/ICEKIM52309.2021.00040,Application of Teaching Innovation Based on robotics engineering,IEEE,Conferences,"As the core major of “Internet + Industrial Intelligence”, robotics engineering is an upgrade and reconstruction of traditional engineering major. The industrial robot course is the professional core course of the Robotics Engineering. It is also a comprehensive course of multi-discipline integration, which involved mechanical engineering, automatic control, computer, sensor, electronic technology, artificial intelligence and other multi-disciplinary content. Robotics Engineering is characterized by broad foundation, great difficulty, emphasis on practice, rapid development and application of new knowledge. In the process of implementation of the teaching innovation, the new concept of engineering education was applied to propose a new form of curriculum system. Taking the projects of engineering as the study objects, disassemble the knowledge points involved in industrial robots, break the course boundaries, reshape the knowledge system, draw knowledge maps and then design teaching activities. In teaching innovation, teachers extend classroom through formation of subject competition teams, promote teaching and promote learning by competition, realize the integration of “teaching, class and competition”, build a bridge between theory and practice, then complete the transformation from knowledge learning to ability training. Besides, they also keep contact with intelligent manufacturing enterprises in Zhuhai and the Bay Area to obtain real-time new developments in enterprises. Thus, the latest information was introduced into classroom. Therefore, the meaning of “production, teaching, research and application” has been deepened. According to the characteristics of the knowledge points of the course, experts were invited to make special lectures for students which can bring them with international perspective and frontier knowledge.",https://ieeexplore.ieee.org/document/9479656/,"2021 2nd International Conference on Education, Knowledge and Information Management (ICEKIM)",29-31 Jan. 2021,ieeexplore
10.1109/ISDA.2010.5687116,Associative prediction model and clustering for product forecast data,IEEE,Conferences,"Association rules are adopted to discover the interesting relationship and knowledge in a large dataset. Knowledge may appear in terms of a frequent pattern discovered in a large number of production data. This knowledge can improve or solve production problems to achieve low cost production. To obtain knowledge and quality information, data mining can be applied to the manufacturing industry. In this study, we used one of the association rule approach, i.e. Apriori algorithm to build an associative prediction model for product forecast data. Also, we adopt the simplest method in clustering, k-means algorithm to attain the link between patterns. The real industrial product forecast data for one year duration is used in the experiment. This data consists of 42 products with two important attributes, i.e. time in the week and required quantity. Since the data mining processes need a large amount of data, we simulated these data by using the Monte Carlo technique to obtain another 15 years of simulated forecast data. There are two main experiments for the association rules mining and clustering. As a result, we obtain an associative prediction model and clustering for the forecasting data. The extracted model provides the prediction knowledge about the range of production in a certain period.",https://ieeexplore.ieee.org/document/5687116/,2010 10th International Conference on Intelligent Systems Design and Applications,29 Nov.-1 Dec. 2010,ieeexplore
10.1109/SMC.2013.819,Automated Sound Signalling Device Quality Assurance Tool for Embedded Industrial Control Applications,IEEE,Conferences,This paper presents a novel system for automatic detection and recognition of faulty audio signaling devices as part of an automated industrial manufacturing process. The system uses historical data labeled by human experts in detecting faulty signaling devices to train an artificial neural network based classifier for modeling their decision making process. The neural network is implemented on a real time embedded micro controller which can be more efficiently incorporated into an automated production line eliminating the need for a manual inspection within the manufacturing process. We present real world experiments based on data pertaining to the production and manufacture of audio signaling components used in car instrument clusters. Our results show that the proposed expert system is able to successfully classify faulty audio signaling devices to a high degree of accuracy. The results can be generalized to other signaling devices where an output signal is represented by a complex and changing frequency spectrum even with significant environmental noise.,https://ieeexplore.ieee.org/document/6722574/,"2013 IEEE International Conference on Systems, Man, and Cybernetics",13-16 Oct. 2013,ieeexplore
10.1109/SMC.2018.00655,Automated Training Plan Generation for Athletes,IEEE,Conferences,"In sports, athletes need detailed and individualised training plans for maintaining and improving their skills in order to achieve their best performance in competitions. This presents a considerable workload for coaches, who besides setting objectives have to formulate extremely detailed training plans. Automated Planning, which has already been successfully deployed in many real-world applications such as space exploration, robotics, and manufacturing processes, embodies a useful mechanism that can be exploited for generating training plans for athletes. In this paper, we propose the use of Automated Planning techniques for generating individual training plans, which consist of exercises the athlete has to perform during training, given the athlete's current performance, period of time, and target performance that should be achieved. Our experimental analysis, which considers general training of kickboxers, shows that apart of considerable less planning time, training plans automatically generated by the proposed approach are more detailed and individualised than plans prepared manually by an expert coach.",https://ieeexplore.ieee.org/document/8616652/,"2018 IEEE International Conference on Systems, Man, and Cybernetics (SMC)",7-10 Oct. 2018,ieeexplore
10.1109/ECAI52376.2021.9515185,Automated ceramic plate defect detection using ScaledYOLOv4-large,IEEE,Conferences,"Automated visual inspection has become a popular topic of research in the last couple of decades, as computation power available grew exponentially. Judging by the fact that visual inspection is a critical task for the quality of the products, it would be highly recommended that people only supervise the system. This paper proposes a low cost, rapid development defect detection system based on the Scaled-YOLOv4 object detection model. The original model achieves almost state of the art detection mAP on the COCO dataset with a mAP(mean average precision) of 56.0 for the largest variant, named YOLOv4-P7. Our version is derived from the ScaledYOLOv4-P5 model and is trained on ceramic plate defects and achieves 87.4 mAP at a intersection of union of 0.5, while comfortably processing a frame in 20ms on a consumer RTX3070 GPU. Thus, the real time constraint for the manufacturing system is fulfilled. Hence, the critical aspects of the development process are the: quick development process, fast training, rapid deployment on the factory floor, quick validation and feedback, using images acquired in the lab - not on the factory floor for first iteration and overall low cost of the automated inspection system.",https://ieeexplore.ieee.org/document/9515185/,"2021 13th International Conference on Electronics, Computers and Artificial Intelligence (ECAI)",1-3 July 2021,ieeexplore
10.1109/ICIT.2018.8352157,Automatic parameter learning for easy instruction of industrial collaborative robots,IEEE,Conferences,"The manufacturing industry faces challenges in meeting requirements of flexibility, product variability and small batch sizes. Automation of high mix, low volume productions requires faster (re)configuration of manufacturing equipment. These demands are to some extend accommodated by collaborative robots. Certain actions can still be hard or impossible to manually adjust due to inherent process uncertainties. This paper proposes a generic iteratively learning approach based on Bayesian Optimisation to efficiently search for the optimal set of process parameters. The approach takes into account the process uncertainties by iteratively making a statistical founded choice on the next parameter-set to examine only based on the prior binomial outcomes. Moreover, our function estimator uses Wilson Score to make proper estimates on the success probability and the associated uncertain measure of sparsely sampled regions. The function estimator also generalises the experiment outcomes to the neighbour region through kernel smoothing by integrating Kernel Density Estimation. Our approach is applied to a real industrial task with significant process uncertainties, where sufficiently robust process parameters cannot intuitively be chosen. Using our approach, a collaborative robot automatically finds a reliable solution.",https://ieeexplore.ieee.org/document/8352157/,2018 IEEE International Conference on Industrial Technology (ICIT),20-22 Feb. 2018,ieeexplore
10.1109/ICICIC.2007.201,BP Network Based Mix Proportion Design of Self-Compacting Concrete,IEEE,Conferences,"It was known that many parameters of raw materials, such as, strength of cement, mud content and modulus of fineness of river sand, maximum size of aggregate, content of' needle-like/sheet-like crushed stone, loss of ignition and fineness of fly ash, may exert significant influence on the theology and mechanical properties of self compacting concrete(SCC). It is a dream of researchers to identify the influencing degree of various factors on performance of SCC so as to obtain optimal properties. By virtue of BP neural network approach, this paper employed strength of cement, mud content and fineness modulus of fineness of river sand, maximum size of aggregate, content of needle-like/sheet-like crushed stone, loss of ignition and fineness of fly ash as the input parameters, and the corresponding optimized mix proportion as the output to describe the nonlinear relationship between them. And the orthogonal experiment was designed for the purpose of training and verification of network. The results demonstrated that the pre-trained BP neural network trained by orthogonal test data may employ to predict the optimal concrete mix proportion. This approach may replace some waste-time and heavy laboratory tests. In addition, such method may real-time optimize mixture proportion. of self-compacting concrete, which has great effect on the quality control of manufacturing self-compacting concrete.",https://ieeexplore.ieee.org/document/4428217/,"Second International Conference on Innovative Computing, Informatio and Control (ICICIC 2007)",5-7 Sept. 2007,ieeexplore
10.1109/ICAIIC51459.2021.9415189,Big Data Platform for Intelligence Industrial IoT Sensor Monitoring System Based on Edge Computing and AI,IEEE,Conferences,"The cutting edge of Industry 4.0 has driven everything to be converted to disruptive innovation and digitalized. This digital revolution is imprinted by modern and advanced technology that takes advantage of Big Data and Artificial Intelligence (AI) to nurture from automatic learning systems, smart city, smart energy, smart factory to the edge computing technology, and so on. To harness an appealing, noteworthy, and leading development in smart manufacturing industry, the modern industrial sciences and technologies such as Big Data, Artificial Intelligence, Internet of things, and Edge Computing have to be integrated cooperatively. Accordingly, a suggestion on the integration is presented in this paper. This proposed paper describes the design and implementation of big data platform for intelligence industrial internet of things sensor monitoring system and conveys a prediction of any upcoming errors beforehand. The architecture design is based on edge computing and artificial intelligence. To extend more precisely, industrial internet of things sensor here is about the condition monitoring sensor data - vibration, temperature, related humidity, and barometric pressure inside facility manufacturing factory.",https://ieeexplore.ieee.org/document/9415189/,2021 International Conference on Artificial Intelligence in Information and Communication (ICAIIC),13-16 April 2021,ieeexplore
10.1109/ICII.2018.00024,Brightics-IoT: Towards Effective Industrial IoT Platforms for Connected Smart Factories,IEEE,Conferences,"Industrial Internet-of-Things (IIoT) supports machines, computers and users to enable intelligent operations using advanced device management and data analytics. In recent years, thanks to standardized IoT platforms and advanced Artificial Intelligence (AI) technologies, there have been great advances in IIoT, and now it promises revolutions on various manufacturing domains such as transport, health, factory and energy. In this paper, based on our experience operating IIoT in various factory applications, we present the technical challenges of manufacturing facilities needed to be dealt with to collect huge amount of data in real-time and counteraction points of an IoT platform regarding these technical challenges and what kinds of features need to be implemented for intelligent services in the smart manufacturing. Finally, we introduce a story of applying industrial IoT platform in production to show how iterative development approaches can achieve business requirements based on elastically scaled-out architecture.",https://ieeexplore.ieee.org/document/8539113/,2018 IEEE International Conference on Industrial Internet (ICII),21-23 Oct. 2018,ieeexplore
10.1109/CASE49439.2021.9551562,Building Skill Learning Systems for Robotics,IEEE,Conferences,"Skill-generating policies have enabled robots to perform a wide range of applications as for example assembly tasks. However, the manual engineering effort for such policies is fairly high and the environment is frequently required to be rather deterministic. For expanding robot deployment to low-volume manufacturing two challenges need to be addressed. First, the robot should acquire the skill-generating policy not from a robot programmer but rather from an expert on the task and second, the robot needs to be able to operate in unstructured environments. In this paper we present a learning approach that combines imitation learning and reinforcement learning to provide a tool for intuitive task teaching followed by self-optimization of the system. The presented approach is applied to a dual-arm assembly task using a real robot and appropriate simulation models. Whereas pure imitation learning does not result in an acceptable success rate for the considered example, after 400 episodes of reinforcement learning the robot can successfully solve the assembly task.",https://ieeexplore.ieee.org/document/9551562/,2021 IEEE 17th International Conference on Automation Science and Engineering (CASE),23-27 Aug. 2021,ieeexplore
10.1109/CNNA.2010.5430300,Cellular Neural Network (CNN) based control algorithms for omnidirectional laser welding processes: Experimental results,IEEE,Conferences,"The high dynamics of laser beam welding (LBW) in several manufacturing processes ranging from automobile production to precision mechanics requires the introduction of new fast real time controls. In the last few years, algorithms for the control of constant-orientation LBW processes have been introduced. Nevertheless, some real life processes are also performed changing the welding orientation during the process. In this paper experimental results obtained by the use of a new CNN based strategy for the control of curved welding seams are discussed. It is based on the real time adjustment of the laser power by the detection of the full penetration hole in process images. The control algorithm has been implemented on the Eye-RIS system v1.2 leading to a visual closed loop control solution with controlling rates up to 6 kHz.",https://ieeexplore.ieee.org/document/5430300/,2010 12th International Workshop on Cellular Nanoscale Networks and their Applications (CNNA 2010),3-5 Feb. 2010,ieeexplore
10.1109/TENCON.2004.1414983,Certain studies on sample time for a predictive fuzzy logic controller through real time implementation of phenol-formaldehyde manufacturing,IEEE,Conferences,"In polymer manufacturing industries, the automation and control of chemical process incorporating techniques of fuzzy control neural networks, and expert systems had lead to a more secured and stable operation. A sudden and unpredictable heat is often produced by the nonlinear exothermal reaction when phenol and formaldehyde are mixed together. Therefore, the polymerization process has to be controlled with a high level of precision in order to avoid temperature run-away. This paper proposes a design methodology for a sensor based process control system. The duration of ON and OFF time of certain relays are the parameters to be controlled in order to keep the exothermic reaction under control The universe of discourse for the output of the FLC system is the sample time that assigned to the relays where maximum time for heater or valve can be turned on before the next action is applied This paper discusses a detailed real time implementation of the exothermal process control using Matlab-fuzzy logic toolbox. An enhanced predictive FLC structure is developed and compared to a predictive FLC control structure. The obtained practical results thus ensure that the predictive FLC can be enhanced by modifying the rules and the membership Junctions of the universe of discourse, which is proved to be better in controlling the reaction temperature.",https://ieeexplore.ieee.org/document/1414983/,2004 IEEE Region 10 Conference TENCON 2004.,24-24 Nov. 2004,ieeexplore
10.1109/BigData.2016.7840831,Cloud-based machine learning for predictive analytics: Tool wear prediction in milling,IEEE,Conferences,"The proliferation of real-time monitoring systems and the advent of Industrial Internet of Things (IIoT) over the past few years necessitates the development of scalable and parallel algorithms that help predict mechanical failures and remaining useful life of a manufacturing system or system components. Classical model-based prognostics require an in-depth physical understanding of the system of interest and oftentimes assume certain stochastic or random processes. To overcome the limitations of model-based methods, data-driven methods such as machine learning have been increasingly applied to prognostics and health management (PHM). While machine learning algorithms are able to build accurate predictive models, large volumes of training data are required. Consequently, machine learning techniques are not computationally efficient for data-driven PHM. The objective of this research is to create a novel approach for machinery prognostics using a cloud-based parallel machine learning algorithm. Specifically, one of the most popular machine learning algorithms (i.e., random forest) is applied to predict tool wear in dry milling operations. In addition, a parallel random forest algorithm is developed using the MapReduce framework and then implemented on the Amazon Elastic Compute Cloud. Experimental results have shown that the random forest algorithm can generate very accurate predictions. Moreover, significant speedup can be achieved by implementing the parallel random forest algorithm.",https://ieeexplore.ieee.org/document/7840831/,2016 IEEE International Conference on Big Data (Big Data),5-8 Dec. 2016,ieeexplore
10.1109/WFCS.2019.8757952,Cloud-enabled Smart Data Collection in Shop Floor Environments for Industry 4.0,IEEE,Conferences,"Industry 4.0 transition is producing a remarkable change in the Smart Factories management. Modern companies can provide new services following products inside the shop floors along the entire production chain. To achieve the goal of servitization that the Industry 4.0 world requires, a modernization of current production chains is needed. This common demand comes mostly from manufacturing sector, where complex work machines collaborate with human workers. The data produced by the machines must be processed quickly, to allow the implementation of reactive services such as predictive maintenance, and remote control, always taking care of the safety of nearby people. This paper proposes a multi-layer architecture to monitor legacy production machines during their operations inside customers plants. The platform provides near real-time delivery of data collected from the machines with a high grade of customization according to customer needs. The performed tests show the scalability of the platform for a productive ecosystem with many machines at work, confirming its feasibility within different production facilities with different needs.",https://ieeexplore.ieee.org/document/8757952/,2019 15th IEEE International Workshop on Factory Communication Systems (WFCS),27-29 May 2019,ieeexplore
10.1109/WCNC45663.2020.9120761,Collaborative Learning Model for Cyberattack Detection Systems in IoT Industry 4.0,IEEE,Conferences,"Although the development of IoT Industry 4.0 has brought breakthrough achievements in many sectors, e.g., manufacturing, healthcare, and agriculture, it also raises many security issues to human beings due to a huge of emerging cybersecurity threats recently. In this paper, we propose a novel collaborative learning-based intrusion detection system which can be efficiently implemented in IoT Industry 4.0. In the system under consideration, we develop smart “filters” which can be deployed at the IoT gateways to promptly detect and prevent cyberattacks. In particular, each filter uses the collected data in its network to train its cyberattack detection model based on the deep learning algorithm. After that, the trained model will be shared with other IoT gateways to improve the accuracy in detecting intrusions in the whole system. In this way, not only the detection accuracy is improved, but our proposed system also can significantly reduce the information disclosure as well as network traffic in exchanging data among the IoT gateways. Through thorough simulations on real datasets, we show that the performance obtained by our proposed method can outperform those of the conventional machine learning methods.",https://ieeexplore.ieee.org/document/9120761/,2020 IEEE Wireless Communications and Networking Conference (WCNC),25-28 May 2020,ieeexplore
10.1109/DESSERT50317.2020.9125038,Combination of Digital Twin and Artificial Intelligence in Manufacturing Using Industrial IoT,IEEE,Conferences,"The paper focuses on Digital Twin (DT) in Manufacturing using Artificial Intelligence (AI) and Industrial IoT. According to the concept, the manufacturing includes three main units: equipment, personnel and processes. All data from these units are inherited to manufacture model (DT) and decision support system with the use of AI. DT data technology allows finding the required knowledge that can be interpreted and used to support the process of decision-making in the management of the enterprise. AI applications open up a broad spectrum of opportunities in manufacturing to add value by optimizing processes and generating new business models. The Landscape was described by a formal model to assure the possibility to analyze the state and development of landscape in detail considering DT and other technologies. DT and IIoT implementation for the simulation of real enterprise manufacturing were considered.",https://ieeexplore.ieee.org/document/9125038/,"2020 IEEE 11th International Conference on Dependable Systems, Services and Technologies (DESSERT)",14-18 May 2020,ieeexplore
10.1109/IECON.1990.149237,Commercial benefits of AI applications in CIM: value analysis approach,IEEE,Conferences,The analysis of benefits gained from the application of artificial intelligence (AI) techniques to real-time supervision and control problems in computer-integrated manufacturing (CIM) is considered. The value of AI in CIM is viewed as enhancing the benefits already achieved through the implementation of CIM systems. A hybrid technique that is an extension of the value analysis method of evaluating the qualitative and quantitative aspects of the application of conventional project deliverables is discussed. Results from a case study based on a value analysis of the benefits of applying AI techniques to CIM within a multinational project are outlined.&lt;<ETX>&gt;</ETX>,https://ieeexplore.ieee.org/document/149237/,[Proceedings] IECON '90: 16th Annual Conference of IEEE Industrial Electronics Society,27-30 Nov. 1990,ieeexplore
10.1109/STSIVA53688.2021.9592013,Compressive Spectral Imaging Fusion Implementation Based on an End-to-End Optimization Design,IEEE,Conferences,"Compressive spectral imaging fusion (CSIF) has recently attracted attention as a popular methodology to improve the spatial and spectral resolution simultaneously. The joint of coded aperture design with the fusion via a deep neural network is state-of-the-art for CSIF. However, the current results are focused on simulation results where implementation complications such as calibration and adjustment processes in the fusion methods are skipped. Therefore, this paper presents an efficient assemble prototype for CSIF. In particular, some implementation details such as pixel mismatch and manufacturing noise are considered during the training to reduce the calibration problems. Furthermore, a re-training of the network using captured ground truth images and the calibrated sensing matrices is presented. Real fusion results of the testbed implementation validated the proposed fusion system.",https://ieeexplore.ieee.org/document/9592013/,"2021 XXIII Symposium on Image, Signal Processing and Artificial Vision (STSIVA)",15-17 Sept. 2021,ieeexplore
10.1109/IECON43393.2020.9255001,Computation Offloading for Machine Learning in Industrial Environments,IEEE,Conferences,"Industrial applications, such as real-time manufacturing, fault classification and inference, autonomous cars, etc., are data-driven applications that require machine learning with a wealth of data generated from industrial Internet of Things (IoT) devices. However, conventional approaches of transmitting this rich data to a remote data center to learn may be undesired due to the non-negligible network transmission delay and the sensitiveness of data privacy. By deploying a number of computing-capable devices at the network edge, edge computing supports the implementation of machine learning close to the industrial environment. Considering the heterogeneous computing capability as well as network location of edge devices, there are two types of feasible edge computing based machine learning models, including the centralized learning and federated learning models. In centralized learning, a resource-rich edge server aggregates the data from different IoT devices and performs machine learning. In federated learning, distributed edge devices and a federated server collaborate to perform machine learning. The features that data should be offloaded in centralized learning while it is locally trained in federated learning make centralized learning and federated learning quite different. We study the computation offloading problem for edge computing based machine learning in an industrial environment, considering the abovementioned machine learning models. We formulate a machine learning-based offloading problem with the goal of minimizing the training delay. Then, an energy-constrained delay-greedy (ECDG) algorithm is designed to solve the problem. Finally, simulation studies based on the MNIST dataset have been conducted to illustrate the efficiency of the proposal.",https://ieeexplore.ieee.org/document/9255001/,IECON 2020 The 46th Annual Conference of the IEEE Industrial Electronics Society,18-21 Oct. 2020,ieeexplore
10.1109/ICSIP49896.2020.9339378,Critical Analysis of Edge Computing,IEEE,Conferences,"The edge computing is facing many problems and efforts are being made to overcome those challenges. Hybrid Mobile Edge Computing is introduced for the mobile devices to overcome the limited battery issues and performance constraints in the devices. There were some difficulties in the integrated development environment of the edge computing, and to overcome those problems, a container-based method is introduced which improves the performance of the coding environment in EC, and also it facilitates in-place debugging. An EC architecture is presented that provides local data processing, management, and quick reaction for the Virtual IoT Devices. With EC's support, a hybrid computing framework is built and an intelligent resource scheduling strategy to fulfill the real-time requirements in smart manufacturing; which showed satisfactory results. A Multi-source Transmission Protocol is presented to counter problems such as the low video streaming and high bandwidth usage. An ECD device is presented for the displaying of results in real-time, with short-time response and also to overcome the network limits. A decentralized resource management technique is introduced along with a technical framework for the latency-sensitive applications' deployment on the edge devices while protecting the privacy of devices. A Dynamic Edge-Fabric Environment is presented; this platform can automatically learn on the basis of past performance of the available resources using machine learning techniques and it decides which task should be best executed where, based on real-time system status and task requirements; the results have proven that it can improve overall performance for the selection of resources.",https://ieeexplore.ieee.org/document/9339378/,2020 IEEE 5th International Conference on Signal and Image Processing (ICSIP),23-25 Oct. 2020,ieeexplore
10.1109/COINS51742.2021.9524088,Cyber Security Risks of Technical Components in Industry 4.0,IEEE,Conferences,"Industry 4.0 is a new concept of automation data exchange in manufacturing. Industry 4.0 consists of various technical components such as Artificial Intelligence (AI), IoT, OPC UA, and cloud. There could be cyber risks when these technical components are used in Industrial Control System (ICS) without security consideration. Therefore cyber risk assessment, reconsideration of implementation, operation are necessary to adopt Industry 4.0. Problems of previous research for ICS risk assessments are some of them are theoretical and conceptual methods such as simulations. Moreover, they do not focus on Industry 4.0 components.The objective of this research is to prove the concrete cyber risks and introduces corresponding secure implementations for Industry 4.0 components such as Artificial Intelligence (AI), the Internet of Things (IoT), OPC Unified Architecture (OPC UA). The proposed method has benefits for clarifying impacts on the real world by cyber-attacks through penetration tests against ICS testbed with actual machines.",https://ieeexplore.ieee.org/document/9524088/,2021 IEEE International Conference on Omni-Layer Intelligent Systems (COINS),23-25 Aug. 2021,ieeexplore
10.1109/ICIAI.2019.8850773,Data Augmentation for Intelligent Manufacturing with Generative Adversarial Framework,IEEE,Conferences,"The global economy is greatly shaped by the unprecedented booming of ICT and artificial intelligence technologies. Their applications in manufacturing has led to the advent of intelligent manufacturing and industry 4.0. Data has become a precious asset for modern industry. This paper first introduces an energy monitoring and data acquisition system namely the Point Energy Technology, which has been developed by the team and installed in several industrial partners, including a local bakery. The lack of data always exists due to various reasons, such as measurement or transmission errors at data collection and transmission stage, leading to the loss of varied length of data samples that are key for process monitoring and control. To solve this problem, we introduce a generative adversarial framework which is based on a game theory for data augmentation. This framework consists of two multilayer perceptron networks, namely generator and discriminator. An improved framework with Q-net that extracts the latent variables from real data is also proposed, in which the Q-net shares the structure with discriminator except for the last layer. In addition, the two optimization methods, namely mini-batch gradient descent and adaptive moment estimation are adopted to tune the parameters. To evaluate the performance of these algorithms, energy consumption data collected from a bakery process is used in the experiment. The experimental results confirm that the latent generative adversarial framework with adaptive moment estimation could generate good quality data samples to compensate the random loss of samples in time series data.",https://ieeexplore.ieee.org/document/8850773/,2019 1st International Conference on Industrial Artificial Intelligence (IAI),23-27 July 2019,ieeexplore
10.1109/INDIN41052.2019.8972310,Data-driven modeling of semi-batch manufacturing: a rubber compounding test case,IEEE,Conferences,"The continuously growing amount of available data from manufacturing processes supports the development of data-driven models. The typical target application of these models is optimal control and continuous quality management within an objective of zero-defect manufacturing. However, data obtained from batch processes are characterized by its high dimensionality that exceeds the computational capabilities of online applications and data-driven model's reliability must be guaranteed for proper industrial implementation. We explore two approaches to reduce problem's size: feature extraction and feature selection; several multivariate regression methods are also compared regarding it precision and robustness. We base our analysis on an industrial rubber compounding process where natural rubber is blended in a semi-batch mixer with several additives, then it is further mixed up using cylinders and it is conditioned in bands for storing. For this process, real production data is collected and stored in the manufacturing execution system of the company. The objective of the analysis is to predict mechanical properties of the rubber at the end of the processes. Based on the provided data, several data-driven models are built and tested. From the comparison among them it is concluded: models based on feature extraction and artificial neural networks yield the highest accuracy, while feature-selected models provide better physical interpretability and increased robustness regarding industrial deployment.",https://ieeexplore.ieee.org/document/8972310/,2019 IEEE 17th International Conference on Industrial Informatics (INDIN),22-25 July 2019,ieeexplore
10.1109/AERO.2018.8396547,Data-driven quality prognostics for automated riveting processes,IEEE,Conferences,"Technologies based in robotics and automatics are reshaping the aerospace industry. Aircraft manufacturers and top-tier suppliers now rely on robotics to perform most of its operational tasks. Over the years, a succession of implemented mobile robots has been developed with the mission of automating important industrial processes such as welding, material handling or assembly procedures. However, despite the progress achieved, a major limitation is that the process still requires human supervision and an extensive quality control process. An approach to address this limitation is to integrate machine learning methods within the quality control process. The idea is to develop algorithms that can direct manufacturing experts towards critical areas requiring human supervision and quality control. In this paper we present an application of machine learning to a concrete industrial problem involving the quality control of a riveting machine. The proposal consists of an intelligent predictive model that can be integrated within the existing real time sensing and pre-processing sub-systems at the equipment level. The framework makes use of several data-driven techniques for pre-processing and feature engineering, combined with the most accurate algorithms, validated through k-folds cross validation technique which also estimates prediction errors. The model is able to classify the manufacturing process of the machine as nominal or anomalous according to a real-world data set of design requirements and operational data. Several machine learning algorithms are compared such as linear regression, nearest neighbor, support vector machines, decision trees, random forests and extreme gradient boost. Results obtained from the case study suggest that the proposed model produces accurate predictions which meet industrial standards.",https://ieeexplore.ieee.org/document/8396547/,2018 IEEE Aerospace Conference,3-10 March 2018,ieeexplore
10.1109/IMCEC51613.2021.9482089,Data-driven scheduling for smart shop floor via reinforcement learning with model-based clustering algorithm,IEEE,Conferences,"Various information technologies provide the manufacturing system massive data, which can support the decision optimization of product lifecycle management. However, the lack of effective use for advanced technologies prevents manufacturing industry from being automated and intelligent. Therefore, this paper proposes the smart shop floor and implementation mechanism. Meanwhile, based on the clustering and reinforcement learning, the brain agent and scheduling agent are designed to determine the approriate rule according to the shop floor state information, thus realizing the data-driven real-time scheduling. Experimental results show that the smart shop floor can effectively deal with disturbance events and has better performance compared with composite dispatching rules.",https://ieeexplore.ieee.org/document/9482089/,"2021 IEEE 4th Advanced Information Management, Communicates, Electronic and Automation Control Conference (IMCEC)",18-20 June 2021,ieeexplore
10.1109/ICRoM48714.2019.9071857,Deep Learning Approach For Object Tracking Of RoboEye,IEEE,Conferences,"RoboEye is a spherical 3RRR parallel robot which has been developed for its high precision. It can provide high speeds, so can be used for fast tracking tasks. To this end, in this paper proper deep learning approaches are combined with classical control methods. Deep learning algorithms are employed to detect an object of interest among various ones in a monocular image, and then obtain an estimatation of the distance to the camera. So, simultaneous depth estimation, and object detection with a monocular camera for real time implementation is proposed here. For fast calculations, also to overcome manufacturing uncertainties, inverse kinematic equations are computed by a multi-layer perceptron (MLP) network based on real data. Finally, a classical PID controller can perform a fast tracking of the object.",https://ieeexplore.ieee.org/document/9071857/,2019 7th International Conference on Robotics and Mechatronics (ICRoM),20-21 Nov. 2019,ieeexplore
10.1109/MSM49833.2020.9201666,Deep Learning-based Algorithm for Mobile Robot Control in Textureless Environment,IEEE,Conferences,"For the implementation of stereo image-based visual servoing algorithm in the eye-in-hand robotics applications, one of the main concerns is the accurate point feature detection and matching algorithm. Since the visual servoing is carried out in the textureless environment, the feature detection process is even more challenging. To fulfill the requirement of a robust and reliable point feature detection process, in this paper we present the novel deep learning-based algorithm. The approach based on convolutional neural networks and algorithm for detection of manufacturing entities is proposed and detected regions of interest are utilized for the improvement of the point feature detection algorithm. The proposed algorithm is experimentally evaluated in real-world settings by using wheeled nonholonomic mobile robot RAICO equipped with stereo vision system. The experimental results show the improvement of 58% in the accuracy of matched point features in the images obtained during the visual servoing process. Moreover, with the implementation of the proposed deep learning-based approach, the number of successful experimental runs has increased by 80%.",https://ieeexplore.ieee.org/document/9201666/,2020 International Conference Mechatronic Systems and Materials (MSM),1-3 July 2020,ieeexplore
10.1109/ICC40277.2020.9148818,Deep Learning-based Human Implantable Nano Molecular Communications,IEEE,Conferences,"In this paper, we propose a novel nano-molecular communication system, including a nano receiver design and detection strategies. We show how machine intelligence can be incorporated into the practical implementation of nano communications. We introduce a testbed employing a biosensor chip as a receiver. The chip is made to be sufficiently small to be implanted under the human skin with no harm while detecting concentrations of glucose molecules over time. Molecules are released by a transmitter, to convey information through a thin pipe. For this configuration, the channel model is unknown, and the sensor dynamics can differ with according to the manufacturing process. Therefore, it is more desirable to find a universal strategy than using closed-form channel expressions so that it can be less sensitive to the channel and sensor variation. Learning-based approaches are likely to solve the problem. Therefore, in this paper, we suggest detection strategies with and without machine learning. We first describe our intuitions of nanomachine design from observations, and we show how the learning-based techniques can benefit the system by reducing the design burden and enhancing the accuracy of data detection. The study concludes by showing sample results of real data transmission.",https://ieeexplore.ieee.org/document/9148818/,ICC 2020 - 2020 IEEE International Conference on Communications (ICC),7-11 June 2020,ieeexplore
10.1109/IMCEC46724.2019.8984019,Deep Learning: Excellent Method at Surface Defect Detection of Industrial Products,IEEE,Conferences,"Surface defect detection of industrial products has always been an important part of the manufacturing industry. At present,there is a high false detection rate and low efficiency problem of traditional image processing algorithms which easy to be disturbed by complex background. Aiming at the above problems, a method for surface defect detection based on deep learning is proposed. YOLOv3 network adopted in this paper has great advantages in small target recognition and location of target in complex background. In addition, the train-set is effectively extended by elastic deformation and thin-plate spline algorithm. The experiment results show that the scratch recognition rate is as high as 95.8%, the over-judgment rate is 5.4%,and the missed rate is 1.3%.The method can identify the surface defects in a short time, and the average detection time does not exceed 0.4s, which can meet the real-time and precision requirements of industrial applications.",https://ieeexplore.ieee.org/document/8984019/,"2019 IEEE 3rd Advanced Information Management, Communicates, Electronic and Automation Control Conference (IMCEC)",11-13 Oct. 2019,ieeexplore
10.1109/COINS51742.2021.9524156,Defective Wafer Detection Using Sensed Numerical Features,IEEE,Conferences,"One of the fundamental processes in semiconductor manufacturing is slicing, which means cutting an ingot into many wafers. During the slicing process, it is possible to produce defective wafers. Unfortunately, the inspection to identify defective wafers is time-consuming and difficult. To solve this problem, we build a system, which uses sensors to collect features (e.g., temperature, thickness, pattern on wafer surface, etc.) during the slicing process to detect if the wafers are defective in the manufacturing process. Two different models, the GRU neural network and XGBoost, are implemented in the proposed system. After fine-tuning both models, experimental results based on real dataset indicate that the GRU neural network outperforms XGBoost for wafer defective detection in both the prediction accuracy and model training time.",https://ieeexplore.ieee.org/document/9524156/,2021 IEEE International Conference on Omni-Layer Intelligent Systems (COINS),23-25 Aug. 2021,ieeexplore
10.1109/ISIE45063.2020.9152441,Deployment of a Smart and Predictive Maintenance System in an Industrial Case Study,IEEE,Conferences,"Industrial manufacturing environments are often characterized as being stochastic, dynamic and chaotic, being crucial the implementation of proper maintenance strategies to ensure the production efficiency, since the machines' breakdown leads to a degradation of the system performance, causing the loss of productivity and business opportunities. In this context, the use of emergent ICT technologies, such as Internet of Things (IoT), machine learning and augmented reality, allows to develop smart and predictive maintenance systems, contributing for the reduction of unplanned machines' downtime by predicting possible failures and recovering faster when they occur. This paper describes the deployment of a smart and predictive maintenance system in an industrial case study, that considers IoT and machine learning technologies to support the online and real-time data collection and analysis for the earlier detection of machine failures, allowing the visualization, monitoring and schedule of maintenance interventions to mitigate the occurrence of such failures. The deployed system also integrates machine learning and augmented reality technologies to support the technicians during the execution of maintenance interventions.",https://ieeexplore.ieee.org/document/9152441/,2020 IEEE 29th International Symposium on Industrial Electronics (ISIE),17-19 June 2020,ieeexplore
10.23919/URSIGASS51995.2021.9560218,Design and Implementation of a Low-Cost Core Board for Mobile IoT Rapid System Prototyping and Service Roll-Out,IEEE,Conferences,"To enable fast development of specific and general mobile Internet of Things (IoT) systems and services, apart from writing cloud-side applications, the requisite hardware production, e.g., procurement of chips and materials, design and manufacturing of printed circuit boards (PCBs), completion of surface mounted technology (SMT) operations, etc., can be a source of delay. Here a low-cost generic core board, which is IoT-service and loT-system prototyping ready, is proposed. Flexible and adaptable to many mobile IoT applications, it is based on a robust low-power consumption microcontroller unit (MCU), with boot-loading and remote software upgrading attributes, and running an open-source real time (RT) operating system (OS), which is well supported with IoT sensor and peripheral sensor drivers. The key hardware, supporting IoT communication protocols embedded, comprises GPS/GPRS, NB-IoT, and LTE modules. The paper sets out the basics of the board design.",https://ieeexplore.ieee.org/document/9560218/,2021 XXXIVth General Assembly and Scientific Symposium of the International Union of Radio Science (URSI GASS),28 Aug.-4 Sept. 2021,ieeexplore
10.1109/AIEA53260.2021.00021,Design and Implementation of the Prototype for Hybrid Production of Multi-Type Products,IEEE,Conferences,"A smart manufacturing prototype called iCandy Box, used for hybrid packing of assorted candies, was designed to study and verify the cyber-physical control methods. The prototype is aimed to provide personalized consumption, as it can perform flexible and customized production. The prototype is powered by a cloud-edge-end enabled collaborative information framework, which can support both industrial big data and artificial intelligence applications. Furthermore, it is characterized by modularization and interdisciplinarity; therefore, it can be used to carry out both experiments and training in several major fields, including smart manufacturing and IoT. The experimental results have shown that the prototype can carry out hybrid production, paving the way for the study and verification of cyber-physical control methods.",https://ieeexplore.ieee.org/document/9525542/,2021 International Conference on Artificial Intelligence and Electromechanical Automation (AIEA),14-16 May 2021,ieeexplore
10.1109/IPFA49335.2020.9260582,"Detection and prevention of assembly defects, by machine learning algorithms, in semiconductor industry for automotive",IEEE,Conferences,"Years of experience within semiconductor manufacturing facilities have led to optimize processes to serve both quality and cost. The solution to achieve next generational levels requires a new approach: this one is fitting with implementation of advanced analytics and machine learning algorithms. Applied to manufacturing data which corresponds with a real big data context, these algorithms can provide insights and automate responses to detect, prevent and ultimately eliminate the most severe failure modes. The project described in this paper targets a wafer sawing process. Various challenges that are raised in such a project are of different natures. A first one is the need for a high level of technical expertise in the manufacturing process of focus: this is essential to define the meaningful dataset that represents comprehensively the desired output of the process. Another component is the data collection aspect: many data have to be collected, stored and parsed, and some small signals found will become the leading indicator to an upcoming process degradation and capability of capturing them is essential. Another key data is traceability of the processed material. Additionally, ensuring an informatic technology architecture to support collection, storage, parsing and computation of the datasets is a significant challenge. Lastly, project success is related to the data scientist expertise to build adequate machine learning algorithms. Optimization of the models can take several iterations with back and forth communication between data scientists and process technical experts. This paper describes issues revealed, some solutions found, and future expectations.",https://ieeexplore.ieee.org/document/9260582/,2020 IEEE International Symposium on the Physical and Failure Analysis of Integrated Circuits (IPFA),20-23 July 2020,ieeexplore
10.1109/IAI50351.2020.9262203,Digital Twin Enabled Smart Control Engineering as an Industrial AI: A New Framework and Case Study,IEEE,Conferences,"In Industry 4.0, the increasing complexity of industrial systems introduces unknown dynamics that affect the performance of manufacturing processes. Thus, Digital Twin appears as a breaking technology to develop virtual representations of any complex system design, analysis, and behavior prediction tasks to enhance the system understanding via enabling capabilities like real-time analytics, or Smart Control Engineering. In this paper, a novel framework is proposed for the design and implementation of Digital Twin applications to the development of Smart Control Engineering. The framework involve the steps of system documentation, Multidomain Simulation, Behavioral Matching, and real-time monitoring, which is applied to develop the Digital Twin for a real-time vision feedback temperature uniformity control. The obtained results show that Digital Twin is a fundamental part of the transformation into Industry 4.0.",https://ieeexplore.ieee.org/document/9262203/,2020 2nd International Conference on Industrial Artificial Intelligence (IAI),23-25 Oct. 2020,ieeexplore
10.1109/AIHAS.1991.138480,"EMM-networking model for FMS modeling, simulation and control",IEEE,Conferences,"The major challenge of implementing FMS (flexible manufacturing systems) at the factory level is to realize automated control. An appropriate FMS model is required for the purpose of control software design and implementation. In this paper, the EMM-networking (extended Moore machine) model based on automata theory for FMS control systems is presented. The basic concept behind the EMM-networking model is to identify the structure of complex systems in a hierarchy of abstractions. By using object-oriented methodology, the EMM-networking model of a FMS can be implemented into object-based software modules, based on which an integrated environment for simulation, software construction and real-time control can be realized. The methodology is demonstrated via the simulation of a prototype FMS.&lt;<ETX>&gt;</ETX>",https://ieeexplore.ieee.org/document/138480/,"[1991] Proceedings. The Second Annual Conference on AI, Simulation and Planning in High Autonomy Systems",1-2 April 1991,ieeexplore
10.1109/CSCloud-EdgeCom49738.2020.00050,Edge Computing-based 3D Pose Estimation and Calibration for Robot Arms,IEEE,Conferences,"Industrial robots are widely used in current production lines, and complex pipeline processes, especially those with different assembly requirements, are designed for intelligent manufacturing in the era of industry 4.0. During the new crown epidemic, a large number of car companies used the production line to transform production of medical materials such as masks and protective clothing, which provided a strong guarantee for fighting the epidemic. In this scenario, a pipeline is often assembled from robotic arms from multiple suppliers. The traditional methods is complex and takes a lot of time. In this paper, we propose a novel deep learning based robot arm 3D pose estimation and calibration model with simple Kinect stereo cameras which can be deployed on light-weight edge computing systems. The light-weight deep CNN model can detection 5 predefined key points based on RGB-D data. In this way, when the assembly line composed of different robot arms needs to be reassembled, our model can quickly provide the robot's pose information without additional tuning processes. Testing in Webots with Rokae xb4 robot arm model shows that our model can quickly estimate the key point of the robot arm.",https://ieeexplore.ieee.org/document/9170983/,2020 7th IEEE International Conference on Cyber Security and Cloud Computing (CSCloud)/2020 6th IEEE International Conference on Edge Computing and Scalable Cloud (EdgeCom),1-3 Aug. 2020,ieeexplore
10.1109/ICICT50816.2021.9358469,Efficient Fault Isolation Method to Monitor Industrial Batch Processes,IEEE,Conferences,Industrial batch processes are very popular manufacturing system with large number of process variables involved. Monitoring of batch processes using statistical process monitoring becomes very difficult in view of the complex correlations between the process variables. This paper focuses on a fault isolation based process monitoring method without prior information of fault where fault isolation problem is converted into a variable selection. Variable selection is a learning algorithm used here to solve the problem of selection and isolation of variables from a model. The method discussed here uses a sparse coefficient based dissimilarity analysis algorithm known as Sparse Dissimilarity Algorithm(SDISSIM) which checks a calculated D-index for identifying fault in the process. A sparse coefficient is tabulated to verify the process variables contributing to the fault and an absolute variance difference is calculated to select the variables for fault isolation. Finally SDISSIM method is explained by successful implementation in MATLAB with real time industrial process data.,https://ieeexplore.ieee.org/document/9358469/,2021 6th International Conference on Inventive Computation Technologies (ICICT),20-22 Jan. 2021,ieeexplore
10.1109/CASE48305.2020.9249228,Efficiently Learning a Distributed Control Policy in Cyber-Physical Production Systems Via Simulation Optimization,IEEE,Conferences,"The manufacturing industry is becoming more dynamic than ever. The limitations of non-deterministic network delays and real-time requirements call for decentralized control. For such dynamic and complex systems, learning methods stand out as a transformational technology to have a more flexible control solution. Using simulation for learning enables the description of highly dynamic systems and provides samples without occupying a real facility. However, it requires prohibitively expensive computation. In this paper, we argue that simulation optimization is a powerful tool that can be applied to various simulation-based learning processes for tremendous effects. We proposed an efficient policy learning framework, ROSA (Reinforcement-learning enhanced by Optimal Simulation Allocation), with unprecedented integration of learning, control, and simulation optimization techniques, which can drastically improve the efficiency of policy learning in a cyber-physical system. A proof-of-concept is implemented on a conveyer-switch network, demonstrating how ROSA can be applied for efficient policy learning, with an emphasis on the industrial distributed control system.",https://ieeexplore.ieee.org/document/9249228/,2020 IEEE 16th International Conference on Automation Science and Engineering (CASE),20-21 Aug. 2020,ieeexplore
10.1109/RO-MAN46459.2019.8956327,End-User Programming of Low-and High-Level Actions for Robotic Task Planning,IEEE,Conferences,"Programming robots for general purpose applications is extremely challenging due to the great diversity of end-user tasks ranging from manufacturing environments to personal homes. Recent work has focused on enabling end-users to program robots using Programming by Demonstration. However, teaching robots new actions from scratch that can be reused for unseen tasks remains a difficult challenge and is generally left up to robotic experts. We propose iRoPro, an interactive Robot Programming framework that allows end-users to teach robots new actions from scratch and reuse them with a task planner. In this work we provide a system implementation on a two-armed Baxter robot that (i) allows simultaneous teaching of low-and high-level actions by demonstration, (ii) includes a user interface for action creation with condition inference and modification, and (iii) allows creating and solving previously unseen problems using a task planner for the robot to execute in real-time. We evaluate the generalisation power of the system on six benchmark tasks and show how taught actions can be easily reused for complex tasks. We further demonstrate its usability with a user study (N=21), where users completed eight tasks to teach the robot new actions that are reused with a task planner. The study demonstrates that users with any programming level and educational background can easily learn and use the system.",https://ieeexplore.ieee.org/document/8956327/,2019 28th IEEE International Conference on Robot and Human Interactive Communication (RO-MAN),14-18 Oct. 2019,ieeexplore
10.1109/CBD.2014.24,Energy Consumption Data Based Machine Anomaly Detection,IEEE,Conferences,"The ever increasing of product development and the scarcity of the energy resources that those manufacturing activities heavily rely on have made it of great significance the study on how to improve the energy efficiency in manufacturing environment. Energy consumption sensing and collection enables the development of effective solutions to higher energy efficiency. Further, it is found that the data on energy consumption of manufacturing machines also contains the information on the conditions of these machines. In this paper, methods of machine anomaly detection based on energy consumption information are developed and applied to cases on our Syil X4 computer numerical control (CNC) milling machine. Further, given massive amount of energy consumption data from large amount machining tasks, the proposed algorithms are being implemented on a Storm and Hadoop based framework aiming at online real-time machine anomaly detection.",https://ieeexplore.ieee.org/document/7176083/,2014 Second International Conference on Advanced Cloud and Big Data,20-22 Nov. 2014,ieeexplore
10.1109/EUROSIM.2013.39,Experimental and Computational Materials Defects Investigation,IEEE,Conferences,"Production of railway axles (i.e., one of the basic material of the modern train) is an elaborate process unfree from faults and problems. Errors during the manufacturing or the plies' overlapping, in fact, can cause particular flaws in the resulting material, so compromising its same integrity. Within this framework, ultrasonic tests could be useful to characterize the presence of defect, depending on its dimensions. On the contrary, the requirement of a perfect state for used materials is unavoidable in order to assure both transport reliability and passenger safety. Therefore, a real-time approach able to recognize and classify the defect starting from the finite element simulated ultrasonic echoes could be very useful in industrial applications. The ill-posedness of the so defined process induces a regularization method. In this paper, a finite element and a heuristic approach are proposed. Particularly, the proposed method is based on the use of a Neural Network approach, the so called ""learning by sample techniques"", and on the use of Support Vector Machines in order to classify the kind of defect. Results assure good performances of the implemented approach, with very interesting applications.",https://ieeexplore.ieee.org/document/7004937/,2013 8th EUROSIM Congress on Modelling and Simulation,10-13 Sept. 2013,ieeexplore
10.1109/WSC.1989.718775,Expert Simulation For On-line Scheduling,IEEE,Conferences,"In recent years, the automotive industry has realized the importance of speed of new products to market and has mounted efforts for improving it. The Expert System Scheduler (ESS) facilitates these efforts by enabling manufacturing plants to generate viable schedules under increasing constraints and demands for flexibility. The scheduler takes advantage of the Computer Integrated Manufacturing (CIM) environment by utilizing the real-time information from the factory for responsive scheduling. The Expert System Scheduler uses heuristics developed by an experiences factory scheduler. It uses simulation concepts and these heuristics to generate schedules. Forward and ""backward"" simulation are used at different stages of the schedule generation process. The system is used to control parts flow on the factory floor at one automated facility. This highly automated facility is a testbed for implementation of CIM concepts. The scheduler runs on a Texas Instruments (TI) Explorer II computer using software developed inhouse utilizing IntelliCorp's Knowledge Engineering Environment (KEE) shell and the LISP language. The scheduling computer is networked to the factory control computer, which actually controls the plant floor. The TI Explorer II acquires current plant floor information from the factory control system, generates a new schedule and sends it back within a short time. The configuration allows fast response to changes in requirements and plant floor conditions.",https://ieeexplore.ieee.org/document/718775/,1989 Winter Simulation Conference Proceedings,4-6 Dec. 1989,ieeexplore
10.1109/IS.2018.8710554,Exploiting the Digital Twin in the Assessment and Optimization of Sustainability Performances,IEEE,Conferences,"Digitalization has shown the potential to disrupt industrial value chains by supporting real-time, risk-free and inexpensive inputs to decision making towards enhanced companies' productivity and value networks flexibility. Developing a reliable and robust digital replica of the physical systems of the value chain is one of the most advanced (and challenging) approaches to digitalization, condensed in the concept of Digital Twin (DT). DT plays a fundamental role in creating a data-rich environment where simulation and optimization procedures can be run. With DT expected to become a commodity in the coming years, simulation and optimization become therefore a more accessible instrument for the improvement of manufacturing and business processes also in small enterprises with limited investment capacity. While scientific literature has analysed the adoption of DT in the optimization of products lifecycle, no contributions have yet focused on the exploitation of DT to improve the sustainability performances of whole value chains. In this paper we propose a reference framework where DTs built upon process and system data gathered from the field, allow to quickly assess the sustainability performances of both existing and planned production mixes and to compare achievable impacts with changing processes and technologies, thus enabling advisory features for sustainability-aware decision making in structured, multi-entity value networks. Internal validation will be deployed referring to real case studies.",https://ieeexplore.ieee.org/document/8710554/,2018 International Conference on Intelligent Systems (IS),25-27 Sept. 2018,ieeexplore
10.1109/INFOCT.2018.8356831,Fault class prediction in unsupervised learning using model-based clustering approach,IEEE,Conferences,"Manufacturing industries have been on a steady path considering for new methods to achieve near-zero downtime to have flexibility in the manufacturing process and being economical. In the last decade with the availability of industrial internet of things (IIoT) devices, this has made it possible to monitor the machine continuously using wireless sensors, assess the degradation and predict the failures of time. Condition-based predictive maintenance has made a significant influence in monitoring the asset and predicting the failure of time. This has minimized the impact on production, quality, and maintenance cost. Numerous approaches have been in proposed over the years and implemented in supervised learning. In this paper, challenges of supervised learning such as need for historical data and incapable of classifying new faults accurately will be overcome with a new methodology using unsupervised learning for rapid implementation of predictive maintenance activity which includes fault prediction and fault class detection for known and unknown faults using density estimation via Gaussian Mixture Model Clustering and K-means algorithm and compare their results with a real case vibration data.",https://ieeexplore.ieee.org/document/8356831/,2018 International Conference on Information and Computer Technologies (ICICT),23-25 March 2018,ieeexplore
10.1109/CNNA.2008.4588677,Feature extraction in laser welding processes,IEEE,Conferences,"There is a rapidly growing demand for laser welding in a wide variety of manufacturing processes ranging from automobile production to precision mechanics. Up to now, the high dynamics of the process has made it impossible to construct a camera based real time quality and process control. Since new pixel parallel architectures are existing, which are now available in systems such as the ACE16k, Q-Eye, and SCAMP-3 (P. Dudek et al., 2006), one has become able to implement a real time laser welding processing. In this paper we will propose a feature extraction algorithm, running at a frame rate of 10 kHz, for a laser welding process. The performance of the algorithm has been studied in detail. In particular, it has been implemented on an Eye-RIS v.1.1 system and has been applied to laser welding processes.",https://ieeexplore.ieee.org/document/4588677/,2008 11th International Workshop on Cellular Neural Networks and Their Applications,14-16 July 2008,ieeexplore
10.1109/ICSGRC.2011.5991842,Feedforward control for high precision linear servo system,IEEE,Conferences,"High precision control has received great attention. Nowadays, as the machines are used for precise operations in manufacturing field, fast and precision control for translational movement are required. In this paper, a linear digital tracking control system is considered to control the position of a non-minimum phase servo system. The system transfer function is obtained via system identification technique using Matlab Toolbox. Feedforward Zero Phase Error Tracking Control (ZPETC) is designed and used in this system. The performance of the controller is analyzed through simulation and real-time experiment by multiple frequency sine wave input. The system is further improved by introducing an error filter to the feedforward controller to increase frequency spectrum. Both controller performances are compared and the result shows significant improvement.",https://ieeexplore.ieee.org/document/5991842/,2011 IEEE Control and System Graduate Research Colloquium,27-28 June 2011,ieeexplore
10.1109/CACRE52464.2021.9501291,Give Me a Wrench!: Finding Tools for Human Partners in Human-Robot Collaborative Manufacturing Contexts,IEEE,Conferences,"Manufacturing processes can be optimized by enabling human-robot collaboration. A relevant goal in this area is to create a collaborative solution in which robots can provide assisting actions to humans, thereby, reducing menial labor as well as increasing productivity. The solution is based on implementing efficient hand-over of mechanical tools from robots to humans. Hand-over tasks are inevitable in human-robot collaborative manufacturing contexts. These tasks need three-step mechanism: object identification, object grasping, and the actual hand-over. This paper presents an approach for robots to find tools for human partners in human-robot collaboration via deep learning. This is achieved using the object detection system YOLOv3 for identification of commonly used mechanical tools. By training on a custom dataset of 800 images of mechanical tools created for the study, the tool recognition is implemented in realworld human-robot hand-over tasks. Experimental results show that the proposed approach achieves a high accuracy for identification of tools in real-world human-robot collaboration. Future work of this study is also discussed.",https://ieeexplore.ieee.org/document/9501291/,"2021 6th International Conference on Automation, Control and Robotics Engineering (CACRE)",15-17 July 2021,ieeexplore
10.1109/ASPDAC.2011.5722294,High performance lithographic hotspot detection using hierarchically refined machine learning,IEEE,Conferences,"Under real and continuously improving manufacturing conditions, lithography hotspot detection faces several key challenges. First, real hotspots become less but harder to fix at post-layout stages; second, false alarm rate must be kept low to avoid excessive and expensive post-processing hotspot removal; third, full chip physical verification and optimization require fast turn-around time. To address these issues, we propose a high performance lithographic hotspot detection flow with ultra-fast speed and high fidelity. It consists of a novel set of hotspot signature definitions and a hierarchically refined detection flow with powerful machine learning kernels, ANN (artificial neural network) and SVM (support vector machine). We have implemented our algorithm with industry-strength engine under real manufacturing conditions in 45nm process, and showed that it significantly outperforms previous state-of-the-art algorithms in hotspot detection false alarm rate (2.4X to 2300X reduction) and simulation run-time (5X to 237X reduction), meanwhile archiving similar or slightly better hotspot detection accuracies. Such high performance lithographic hotspot detection under real manufacturing conditions is especially suitable for guiding lithography friendly physical design.",https://ieeexplore.ieee.org/document/5722294/,16th Asia and South Pacific Design Automation Conference (ASP-DAC 2011),25-28 Jan. 2011,ieeexplore
10.1109/IOLTS50870.2020.9159704,High-level Modeling of Manufacturing Faults in Deep Neural Network Accelerators,IEEE,Conferences,"The advent of data-driven real-time applications requires the implementation of Deep Neural Networks (DNNs) on Machine Learning accelerators. Google's Tensor Processing Unit (TPU) is one such neural network accelerator that uses systolic array-based matrix multiplication hardware for computation in its crux. Manufacturing faults at any state element of the matrix multiplication unit can cause unexpected errors in these inference networks. In this paper, we propose a formal model of permanent faults and their propagation in a TPU using the Discrete-Time Markov Chain (DTMC) formalism. The proposed model is analyzed using the probabilistic model checking technique to reason about the likelihood of faulty outputs. The obtained quantitative results show that the classification accuracy is sensitive to the type of permanent faults as well as their location, bit position and the number of layers in the neural network. The conclusions from our theoretical model have been validated using experiments on a digit recognition-based DNN.",https://ieeexplore.ieee.org/document/9159704/,2020 IEEE 26th International Symposium on On-Line Testing and Robust System Design (IOLTS),13-15 July 2020,ieeexplore
10.1109/ICSMC.2000.886346,Holonic self-organization of multi-agent systems by fuzzy modeling with application to intelligent manufacturing,IEEE,Conferences,"Holonic manufacturing aims to design standardized, modular manufacturing systems made of interchangeable parts, to enable flexibility, online reconfigurability and self-organizing capabilities for the production systems. Recent advances in distributed artificial intelligence and networking technologies have proven that theoretical multi-agent systems (MAS) concepts are very suitable for the real life implementation of holonic concepts. Building on our recent results in the design and implementation of holonic reconfigurable architectures, the paper introduces a novel approach to the online self-organization of distributed systems. By using fuzzy set and uncertainty theoretical concepts, we construct a mathematical foundation for modeling MAS, where appropriate holonic structures are identified for each particular application. This approach opens new possibilities for the design of any distributed system that needs self-organization as an intrinsic property.",https://ieeexplore.ieee.org/document/886346/,"Smc 2000 conference proceedings. 2000 ieee international conference on systems, man and cybernetics. 'cybernetics evolving to systems, humans, organizations, and their complex interactions' (cat. no.0",8-11 Oct. 2000,ieeexplore
10.1109/NDS.2017.8070626,"Image-driven, model-free control of repetitive processes based on machine learning",IEEE,Conferences,"An image-driven, model-free approach to design control systems for a large class of industrial process is proposed. A mathematical model of the process is replaced by sequences of subsequent images which play the role of the process (plant) states. The length of this sequences depends on the speed of the process dynamics and on the frame rate. Firstly, a learning sequence of the system states is collected and then, it is used for classifying (clustering) its states. A decision of the control system is attached by an expert to each class (cluster) of the states. At the implementation stage images from a camera in the loop are collected into subsequences corresponding to the system states, then they are classified and a control action corresponding to a class at hand is undertaken. This general idea is exemplified by the case study of a laser power control in an additive manufacturing, which is a repetitive process. A tree-like, hierarchical classifier is proposed in order to recognize the process states, each consisting of three consecutive images. Its performance is tested on real-life images from the process of laser cladding additive manufacturing.",https://ieeexplore.ieee.org/document/8070626/,2017 10th International Workshop on Multidimensional (nD) Systems (nDS),13-15 Sept. 2017,ieeexplore
10.1109/IAS.2006.256774,Implementation of Emotional Controller for Interior Permanent Magnet Synchronous Motor Drive,IEEE,Conferences,"This paper presents for the first time the real-time implementation of an emotional controller for interior permanent magnet synchronous motor (IPMSM) drives. The proposed novel controller is called brain emotional learning based intelligent controller (BELBIC). The utilization of BELBIC is based on emotion processing mechanism in brain, and is essentially an action, which is based on sensory inputs and emotional cues. The emotional learning occurs mainly in the amygdala. The amygdala is mathematically modeled, and the BELBIC controller is introduced. This type of controller is insensitive to noise and variance of the parameters. The controller is successfully implemented in real-time using a digital signal processor board ds1102 for a laboratory 1-hp IPMSM. The results show superior control characteristics especially very fast response, simple implementation and robustness with respect to disturbances and manufacturing imperfections. The proposed method enables the designer to shape the response in accordance with the multiple objectives of choices",https://ieeexplore.ieee.org/document/4025462/,Conference Record of the 2006 IEEE Industry Applications Conference Forty-First IAS Annual Meeting,8-12 Oct. 2006,ieeexplore
10.1109/ICPHYS.2019.8780271,Implementation of Industrial Cyber Physical System: Challenges and Solutions,IEEE,Conferences,"The Industry Internet of Things (IIoT) and the Industry Cyber-Physical System (ICPS) for real industry are becoming vitally necessary in the smart manufacturing environment. Very large number of intelligent sensors are being available generating an exploding amount of data. Several issues come with the big data in real industry, including the a grand-scale connected network construction with the data security and access protocol issues, data quality with considerable noise when gathered from industrial factories, efficient data storage, smart interconnection with cloud services, and real-time analytics requirements. This paper proposes an integrated CPS based architecture for smart manufacturing and provides the deployment details, addressing all the potential problems in an appropriate way. It has been successfully implemented in a real industry environment, and won the Best Industry Application of IoT at the BigInsights Data &amp; AI Innovation Awards.",https://ieeexplore.ieee.org/document/8780271/,2019 IEEE International Conference on Industrial Cyber Physical Systems (ICPS),6-9 May 2019,ieeexplore
10.1109/ICCSP.2018.8524377,Implementation of Robotic Vision to Perform Threaded Assembly,IEEE,Conferences,"In manufacturing of mechanical parts and assemblies, proper thread-engagement between a bolt and a nut is vital for the performance and reliability of the product. Typically, this is a precision work, requiring repetitive manual operations. In this paper, we explain how such assembly operations can be carried out by collaborative robots (co-bots) by monitoring the position and orientation of the nut and bolt using an image-sensor (camera). The focus of our discussion is the assembly-operation of bolting of a nut by the grippers of a co-bot. Slips and misalignment, leading to wrong positioning of the nut and the bolt, are identified by capturing the images of the two components in real time using Microsoft Kinect camera-sensor. 3D Reconstruction of the image captured by the camera-sensor is carried out using the Kinect Fusion application. The reconstructed image is in the form of a polygonal mesh which is further converted to 3D Point Cloud data which is less sensitive to noise. Thereafter, the Point Cloud is segmented by dividing the entire scene into many clusters in order to distinguish the objects of the scene as grippers and nut and bolt. These clusters can be used for the training of the co-bot for the proposed operation. This method of extracting object-boundaries leading to recognition of objects is a vital operation in the field of robotic vision. We provide baseline description of various machine learning techniques that can be applied to realize proper assembly of a nut and a bolt.",https://ieeexplore.ieee.org/document/8524377/,2018 International Conference on Communication and Signal Processing (ICCSP),3-5 April 2018,ieeexplore
10.1109/CASE48305.2020.9216902,Industrial Robot Grasping with Deep Learning using a Programmable Logic Controller (PLC),IEEE,Conferences,"Universal grasping of a diverse range of previously unseen objects from heaps is a grand challenge in e-commerce order fulfillment, manufacturing, and home service robotics. Recently, deep learning based grasping approaches have demonstrated results that make them increasingly interesting for industrial deployments. This paper explores the problem from an automation systems point-of-view. We develop a robotics grasping system using Dex-Net, which is fully integrated at the controller level. Two neural networks are deployed on a novel industrial AI hardware acceleration module close to a PLC with a power footprint of less than 10 W for the overall system. The software is tightly integrated with the hardware allowing for fast and efficient data processing and real-time communication. The success rate of grasping an object form a bin is up to 95% with more than 350 picks per hour, if object and receptive bins are in close proximity. The system was presented at the Hannover Fair 2019 (world's largest industrial trade fair) and other events, where it performed over 5,000 grasps per event.",https://ieeexplore.ieee.org/document/9216902/,2020 IEEE 16th International Conference on Automation Science and Engineering (CASE),20-21 Aug. 2020,ieeexplore
10.23919/ACC45564.2020.9147268,Inferential Methods for Additive Manufacturing Feedback,IEEE,Conferences,"Adaptive manufacturing has revolutionized desktop prototyping and the production of physical models for non-load bearing or stress inducing applications. Many extrusion-based printers are available for purchase by entrepreneurial enthusiasts or businesses with manufacturing space limitations. These low-cost printers allow for quick prototyping but are not designed or intended for high quality production or high-cycle production, requiring extensive user tuning and upkeep to maintain the printer in usable condition. In a quest to apply modern deep learning and reinforcement learning based models, this work focuses on the development of control systems and infrastructure needed to resolve many of these intrinsic limitations of desktop 3D printers. A series of real-time agents were designed and deployed to actively monitor the printing of every layer and make continuous corrections in the printing parameters and G-code commands to reduce the variance in the tensile strength of homogeneous parts printed in a large batch.",https://ieeexplore.ieee.org/document/9147268/,2020 American Control Conference (ACC),1-3 July 2020,ieeexplore
10.1109/ICYCS.2008.34,Influence Graph based Task Decomposition and State Abstraction in Reinforcement Learning,IEEE,Conferences,"Task decomposition and state abstraction are crucial parts in reinforcement learning. It allows an agent to ignore aspects of its current states that are irrelevant to its current decision, and therefore speeds up dynamic programming and learning. This paper presents the SVI algorithm that uses a dynamic Bayesian network model to construct an influence graph that indicates relationships between state variables. SVI performs state abstraction for each subtask by ignoring irrelevant state variables and lower level subtasks. Experiment results show that the decomposition of tasks introduced by SVI can significantly accelerate constructing a near-optimal policy. This general framework can be applied to a broad spectrum of complex real world problems such as robotics, industrial manufacturing, games and others.",https://ieeexplore.ieee.org/document/4708962/,2008 The 9th International Conference for Young Computer Scientists,18-21 Nov. 2008,ieeexplore
10.1109/SUMMA50634.2020.9280823,Intelligent Quality Management System for Casting Gas Turbine Engine Blades,IEEE,Conferences,"This article is devoted to the problem of reducing the number of defects and improving the quality of manufacturing gas turbine engine blades. The process of pressing a casting rod designed to form the inner cavity of the blades during their manufacture by pouring on the smelted models is considered. Information about 400 examples of pressing foundry rods has been collected. Each of the examples contained a set of parameters that characterize the process of obtaining the workpieces and the result of manufacturing. Based on the collected statistical information, the neural network was designed. Using virtual computer experiments, the process of pressing casting rods was studied. The parameters that have the greatest impact on the quality of the resulting products are identified. In the course of computer experiments, it was observed that changes in a number of pressing parameters which lead to a decrease in the probability of defects on one billet do not lead to a similar decrease in the probability of defects on another billet. The method of neural network modeling was able to identify parameters the same change in which leads to a decrease in the probability of any type of defect of all workpieces. For example, the probability of getting defects in all workpieces decreases with increasing the holding time of the rod in the mold without pressure. Thus, it is shown that the developed neural network model allows to control the quality of the obtained blades, select the optimal parameters of the technological process that provide the maximum reduction of defects.",https://ieeexplore.ieee.org/document/9280823/,"2020 2nd International Conference on Control Systems, Mathematical Modeling, Automation and Energy Efficiency (SUMMA)",11-13 Nov. 2020,ieeexplore
10.1109/ISIC.1990.128585,Intelligent processing in spray forming applications,IEEE,Conferences,The authors describe a program to develop sensor and control technology for real-time implementation with spray forming. The objectives of this effort are to ensure reproducibility and quality of spray-formed products and to expand the capability of manufacturing asymmetric shapes. Sensor data are used by a fuzzy logic intelligent controller to make adjustments to spray-forming process parameters during preform deposition. Advanced manipulation capabilities are required to produce asymmetric components.&lt;<ETX>&gt;</ETX>,https://ieeexplore.ieee.org/document/128585/,Proceedings. 5th IEEE International Symposium on Intelligent Control 1990,5-7 Sept. 1990,ieeexplore
10.1109/ICAT.2013.6684074,Intelligent system for inspection and selection of parts in a manufacturing cell,IEEE,Conferences,"This paper addresses the design and implementation of an artificial vision system implemented in a manufacturing cell. The vision system recognizes and selects in an intelligent manner the manufactured parts through a feedforward artificial neural network and the decisions are completely based on the part's color and its geometry. A simple digital camera is used as an image acquisition device. This image is then processed by an artificial neural network, which is able to identify the part's color. Then a Programmable Logic Controller (PLC) drives an electropneumatic system, in order to store the identified part into a corresponding repository. An interface based on power electronic devices and a Data Acquisition Card (DAQ) system implements the communication between the PLC and the computer. The proposed system is completely implemented and tested in a real Flexible Manufacturing System (FMS) of FESTO© showing good results.",https://ieeexplore.ieee.org/document/6684074/,"2013 XXIV International Conference on Information, Communication and Automation Technologies (ICAT)",30 Oct.-1 Nov. 2013,ieeexplore
10.1109/WF-IoT51360.2021.9595035,Interpretable Multi-Step Production Optimization Utilizing IoT Sensor Data,IEEE,Conferences,"In an industrial manufacturing process, such as petroleum, chemical, and food processing, with the deployment of thousands of sensors in the plants, we have the chance to provide real-time onsite management for the processes. Beyond the real-time status update, utilizing vast IoT data and creating machine learning and optimization models provide us with intelligent business recommendations. Those are used by the site engineers and managers to make real-time decisions in a situation with multiple conflicting operational and business goals. Those goals include maximizing financial gain, minimizing costs, limiting the usage of certain raw materials or additives, decreasing environmental impact, and more. When formalizing these decision-making tasks, often there is no prior knowledge of compromise between the conflicting goals. That poses a challenge to generate a proper objective function. In this paper, we create a Multi-Step optimization process to address this uncertainty of selecting proper objectives and their preferences. Instead of using an explicit trade-off to create a single weighted objective function (as a traditional approach) and rely on a single attempt to find the optimal solution, we decompose this problem into multiple steps. In each step, we optimize only one objective from one KPI with an exact semantic meaning. We demonstrate the usability of the approach using a practical application from an oil sands processing facility, provide modeling results focusing on the response to business priorities, performance, and interpretability. The multi-step approach presents the convergence of the target goal with an outcome KPI with comparison for each step to illustrate the enhanced interpretability.",https://ieeexplore.ieee.org/document/9595035/,2021 IEEE 7th World Forum on Internet of Things (WF-IoT),14 June-31 July 2021,ieeexplore
10.1109/IoTDI49375.2020.00026,IoT-ID: A Novel Device-Specific Identifier Based on Unique Hardware Fingerprints,IEEE,Conferences,"A significant number of IoT devices are being deployed in the wild, mostly in remote locations and in untrusted conditions. This could include monitoring an electronic perimeter fence or a critical infrastructure such as telecom and power grids. Such applications rely on the fidelity of data reported from the IoT devices, and hence it is imperative to identify the trustworthiness of the remote device before taking decisions. Existing approaches use a secret key usually stored in volatile or non-volatile memory for creating an encrypted digital signature. However, these techniques are vulnerable to malicious attacks and have significant computation and energy overhead. This paper presents a novel device-specific identifier, IoT-ID that captures the device characteristics and can be used towards device identification. IoT-ID is based on physically unclonable functions (PUFs), that exploit variations in the manufacturing process to derive a unique fingerprint for integrated circuits. In this work, we design novel PUFs for Commercially Off the Shelf (COTS) components such as clock oscillators and ADC, to derive IoT-ID for a device. Hitherto, system component PUFs are invasive and rely on additional dedicated hardware circuitry to create a unique fingerprint. A highlight of our PUFs is doing away with special hardware. IoT-ID is non-invasive and can be invoked using simple software APIs running on COTS components. IoT-ID has the following key properties viz., constructability, real-time, uniqueness, and reproducibility, making them robust device-specific identifiers. We present detailed experimental results from our live deployment of 50 IoT devices running over a month. Our edge machine learning algorithm has 100% accuracy in uniquely identifying the 50 devices in our deployment and can run locally on the resource-constrained IoT device. We show the scalability of IoT-ID with the help of numerical analysis on 1000s of IoT devices.",https://ieeexplore.ieee.org/document/9097592/,2020 IEEE/ACM Fifth International Conference on Internet-of-Things Design and Implementation (IoTDI),21-24 April 2020,ieeexplore
10.1109/IEEM45057.2020.9309776,Job Shop Scheduling Problem Neural Network Solver with Dispatching Rules,IEEE,Conferences,"Job Shop Scheduling Problem (JSSP) is an optimization problem in computer science and operations research. Many problems in real-world manufacturing processes can be translated into JSSP. In recent years, Machine Learning has shown great promises in solving optimization problems and can be used to solve JSSP instances. In this paper, an Artificial Neural Network (ANN) was designed and trained to solve JSSP instances using the priority of the operations as the learning output. Dispatching rules were implemented to break ties during the decoding of the priorities. Our experiment results showed that a hybrid algorithm that combines the best of ANN with dispatching rules and standalone dispatching rule-based heuristic outperforms previously reported results.",https://ieeexplore.ieee.org/document/9309776/,2020 IEEE International Conference on Industrial Engineering and Engineering Management (IEEM),14-17 Dec. 2020,ieeexplore
10.1109/SMC.2017.8122711,Knowledge extracted from recurrent deep belief network for real time deterministic control,IEEE,Conferences,"Recently, the market on deep learning including not only software but also hardware is developing rapidly. Big data is collected through IoT devices and the industry world will analyze them to improve their manufacturing process. Deep Learning has the hierarchical network architecture to represent the complicated features of input patterns. Although deep learning can show the high capability of classification, prediction, and so on, the implementation on GPU devices are required. We may meet the trade-off between the higher precision by deep learning and the higher cost with GPU devices. We can success the knowledge extraction from the trained deep learning with high classification capability. The knowledge that can realize faster inference of pre-trained deep network is extracted as IF-THEN rules from the network signal flow given input data. Some experiment results with benchmark tests for time series data sets showed the effectiveness of our proposed method related to the computational speed.",https://ieeexplore.ieee.org/document/8122711/,"2017 IEEE International Conference on Systems, Man, and Cybernetics (SMC)",5-8 Oct. 2017,ieeexplore
10.1109/INDIN.2015.7281881,Knowledge-driven finite-state machines. Study case in monitoring industrial equipment,IEEE,Conferences,Traditionally state machines are implemented by coding the desired behavior of a given system. This work proposes the use of ontological models to describe and perform computations on state machines by using SPARQL queries. This approach represents a paradigm shift relating to the customary manner in which state machines are stored and computed. The main contribution of the work is an ontological model to represent state machines and a set of generic queries that can be used in any knowledge-driven state machine to compute valuable information. The approach was tested in a study case were the state machines of industrial robots in a manufacturing line were modeled as ontological models and used for monitoring the behavior of these devices on real time.,https://ieeexplore.ieee.org/document/7281881/,2015 IEEE 13th International Conference on Industrial Informatics (INDIN),22-24 July 2015,ieeexplore
10.1109/WF-IoT48130.2020.9221446,Learner’s Dilemma: IoT Devices Training Strategies in Collaborative Deep Learning,IEEE,Conferences,"With the growth of Internet of Things (IoT) and mobile edge computing, billions of smart devices are interconnected to develop applications used in various domains including smart homes, healthcare and smart manufacturing. Deep learning has been extensively utilized in various IoT applications which require huge amount of data for model training. Due to privacy requirements, smart IoT devices do not release data to a remote third party for their use. To overcome this problem, collaborative approach to deep learning, also known as Collaborative Deep Learning (CDL) has been largely employed in data-driven applications. This approach enables multiple edge IoT devices to train their models locally on mobile edge devices. In this paper, we address IoT device training problem in CDL by analyzing the behavior of mobile edge devices using a game-theoretic model, where each mobile edge device aims at maximizing the accuracy of its local model at the same time limiting the overhead of participating in CDL. We analyze the Nash Equilibrium in an N-player static game model. We further present a novel clusterbased fair strategy to approximately solve the CDL game to enforce mobile edge devices for cooperation. Our experimental results and evaluation analysis in a real-world smart home deployment show that 80% mobile edge devices are ready to cooperate in CDL, while 20% of them do not train their local models collaboratively.",https://ieeexplore.ieee.org/document/9221446/,2020 IEEE 6th World Forum on Internet of Things (WF-IoT),2-16 June 2020,ieeexplore
10.1109/COASE.2016.7743572,Learning-based dynamic scheduling of semiconductor manufacturing system,IEEE,Conferences,"A learning-based scheduling framework for semiconductor manufacturing system is studied in this paper. This framework obtains a dynamic scheduling model by applying machine learning algorithm based on optimal data samples, through which an approximate optimal scheduling strategy under a certain production state can be acquired on time. And then an implementation of a dynamic scheduling model based on extreme learning machine (ELM) is proposed. In order to improve efficiency, a hybrid feature selection and classification algorithm is suggested, which combines filter feature selection method and wrapper feature selection method. Finally, the proposed dynamic scheduling model is tested in a real semiconductor manufacturing system to compare and analysis between the algorithm performance and production performance. The result indicates that the learning-based scheduling method is superior to single scheduling rules and it also meets the requirements of real-time manufacturing scheduling.",https://ieeexplore.ieee.org/document/7743572/,2016 IEEE International Conference on Automation Science and Engineering (CASE),21-25 Aug. 2016,ieeexplore
10.1109/ICC40277.2020.9148684,Machine Learning for Predictive Diagnostics at the Edge: an IIoT Practical Example,IEEE,Conferences,"Edge Computing is becoming more and more essential for the Industrial Internet of Things (IIoT) for data acquisition from shop floors. The shifting from central (cloud) to distributed (edge nodes) approaches will enhance the capabilities of handling real-time big data from IoT. Furthermore, these paradigms allow moving storage and network resources at the edge of the network closer to IoT devices, thus ensuring low latency, high bandwidth, and location-based awareness. This research aims at developing a reference architecture for data collecting, smart processing, and manufacturing control system in an IIoT environment. In particular, our architecture supports data analytics and Artificial Intelligence (AI) techniques, in particular decentralized and distributed hybrid twins, at the edge of the network. In addition, we claim the possibility to have distributed Machine Learning (ML) by enabling edge devices to learn local ML models and to store them at the edge. Furthermore, edges have the possibility of improving the global model (stored at the cloud) by sending the reinforced local models (stored in different shop floors) towards the cloud. In this paper, we describe our architectural proposal and show a predictive diagnostics case study deployed in an edge-enabled IIoT infrastructure. Reported experimental results show the potential advantages of using the proposed approach for dynamic model reinforcement by using real-time data from IoT instead of using an offline approach at the cloud infrastructure.",https://ieeexplore.ieee.org/document/9148684/,ICC 2020 - 2020 IEEE International Conference on Communications (ICC),7-11 June 2020,ieeexplore
10.1109/CASE48305.2020.9216842,Machine Learning to Empower a Cyber-Physical Machine Tool,IEEE,Conferences,"Machine learning is used to empower a machine tool, which gives rise to a new generation machine tool, i.e. cyber-physical machine tool. The use of four sensors to measure the cutting force, vibration, acoustic emission, and spindle motor current of an end milling machine is proposed. Sixty-five cutting tests using an end milling machine were conducted, during which sensor data was recorded. The flank wear exhibited on the tool following each cut was then measured using a microscope. This provided a labelled data set on which to train four machine learning algorithms: Support Vector Regression, Random Forests, Feed-Forward Back-Propagation Artificial Neural Networks, and Polynomial Regression. These were then compared and it was found that an artificial neural network provides the most accurate predictions of tool flank wear, with a mean absolute percentage accuracy of 90.11%. Using this trained neural network model, a real-time tool wear prediction system was implemented in LabVIEW. This tool condition monitoring system can be used to increase efficiency of manufacturing processes",https://ieeexplore.ieee.org/document/9216842/,2020 IEEE 16th International Conference on Automation Science and Engineering (CASE),20-21 Aug. 2020,ieeexplore
10.1109/PHM-Besancon49106.2020.00009,Machine Performance Monitoring and Fault Classification using Vibration Frequency Analysis,IEEE,Conferences,"Machine anomalies in manufacturing directly affect the production yield and factory operation efficiency if such anomalies cannot be detected in time. Real-time monitoring of machine health condition not only improves machine throughput by reducing unplanned downtime caused by machine failure but also saves cost for unnecessary routine maintenance. This paper presents a systematic approach for real-time or near real-time machine performance monitoring solution development from data collection, feature extraction, data analytics to real-time machine fault and machine status classification. Three data-driven machine-learning approaches using one vibration sensor data are proposed to detect two common machine failure modes during machine turning process. To evaluate the the performance of each approach, three machine-learning algorithms (Random Forest, K Nearest Neighborhood, and Support Vector Machine) are implemented and tested. Evaluation results on the actual machine data shows that a two-layered classification structure with random forest algorithm as the base has high classification accuracy on the machine status including machine fault detection. The developed data-driven machine health monitoring solution is deployed in the IoT device for real-time data collection and processing and results are sent data server for data visualization.",https://ieeexplore.ieee.org/document/9115516/,2020 Prognostics and Health Management Conference (PHM-Besançon),4-7 May 2020,ieeexplore
10.1109/ICMNN.1994.593731,Massively parallel VLSI-implementation of a dedicated neural network for anomaly detection in automated visual quality control,IEEE,Conferences,"In this work we will present the VLSI-implementation of a dedicated neural network architecture which we have developed in prior work for anomaly detection in automated visual industrial quality control. The network, denoted as NOVAS performs a filtering of inspection images and highlights defects or anomalies in an isomorphic image representation, allowing the detection and localisation of faults on objects. Training of NOVAS is achieved by simply presenting a set of tolerable objects to the network in a single sweep. NOVAS works with single and with multichannel image representations. The processing principle of NOVAS is closely related to nearest neighbor and hypersphere classifier approaches. We have designed an ASIC for the efficient implementation of the nearest neighbor search. Based on that ASIC we will present an architecture of a modular massively parallel computer suited to meet the real-time constraints of manufacturing processes. Further we will report on the status of a prototype system which is close to completion.",https://ieeexplore.ieee.org/document/593731/,Proceedings of the Fourth International Conference on Microelectronics for Neural Networks and Fuzzy Systems,26-28 Sept. 1994,ieeexplore
10.1109/DTPI52967.2021.9540077,Mechanical Design Paradigm based on ACP Method in Parallel Manufacturing,IEEE,Conferences,"Parallel Manufacturing is a new manufacturing paradigm in industry, deeply integrating informalization, automation, and artificial intelligence. In this paper we propose a new mechanical design paradigm in Parallel Manufacturing based on ACP method. The key is to regard the design procedure based on artificial design and emulation method as two independent procedures, which can be modeled as a parallel system. The design procedure based on ACP method does not include a real system, which is an inventive extension of the traditional parallel system. This method can be implemented with social information by introducing the definition of SDV, SDM, and Intelligent Design Manager, making it highly adaptive for social manufacturing and Parallel Manufacturing.",https://ieeexplore.ieee.org/document/9540077/,2021 IEEE 1st International Conference on Digital Twins and Parallel Intelligence (DTPI),15 July-15 Aug. 2021,ieeexplore
10.1109/ICAT.2006.85,Modeling and Application of Virtual Machine Tool,IEEE,Conferences,"The recent years of the 21th Century are associated with the advent of virtual reality technologies for modern industry and manufacturing engineering. Virtual Machine Tool Technology is given to design, test, control and machine parts in a virtual reality environment. This paper presents the methods to model and simulate the virtual machine tools in response to change in the machining requirements. Specifically, a set of module combination rules and a modeling method of the structure of machine tools using connectivity graph are developed. By this way virtual machine tool is implemented. The developed virtual machine tool can be efficiently used for industry training and machine leaning and operating.",https://ieeexplore.ieee.org/document/4089203/,16th International Conference on Artificial Reality and Telexistence--Workshops (ICAT'06),29 Nov.-1 Dec. 2006,ieeexplore
10.1109/RCAR52367.2021.9517563,Modeling and Implementation of Tacking for Wing Sail Land-Yacht,IEEE,Conferences,"Wing sail land-yacht is one of the promising vehicles which utilizes wind energy. However, its autonomous driving in all directions is challenging, especially the upwind steering (tacking) maneuver. This paper proposes a novel design of low cost three-wheeled land-yacht with a T-frame and a foamed wing sail, which is able to move and tack upwind owing to the lightweight design. To achieve the successful tacking, a model describing the steering process is developed based on the law of energy conservation and used to predict the minimum initial velocity for steering. In the model, an acceleration error function C induced by the environment disturbance and manufacturing error is identified by a series of experiments. Finally, the tacking experiments verify the steering model and show that the land-yacht can achieve the tacking with a high success rate of 94.7%, based on the predicted minimum initial steering velocity.",https://ieeexplore.ieee.org/document/9517563/,2021 IEEE International Conference on Real-time Computing and Robotics (RCAR),15-19 July 2021,ieeexplore
10.1109/ISCAS.2019.8702575,Multi-View Fusion Neural Network with Application in the Manufacturing Industry,IEEE,Conferences,"In recent years the research community and industry have paid high attention to the field of machine learning, especially deep learning. Nowadays many real-world classification or rather prediction applications are implemented by neural network models. We propose a multi-view fusion neural network with application in the manufacturing industry. Image information of multiple cameras is fused and used by the proposed model to predict the state of a manufacturing machine. Experiments show that the overall classification performance is increased from a baseline of 92.7% to 99.5% by the fusion model.",https://ieeexplore.ieee.org/document/8702575/,2019 IEEE International Symposium on Circuits and Systems (ISCAS),26-29 May 2019,ieeexplore
10.1109/IJCNN48605.2020.9207055,Multi-agent system for dynamic scheduling,IEEE,Conferences,"This paper proposes a flexible manufacturing system based on intelligent computational agents. A Multi-Agent System composed of 4 types of reactive agents was designed to control the operation of a real implementation in the Intelligent Automation Lab at Instituto Superior Técnico. This implementation was based and constructed analogously to a known benchmark, AIP-PRIMECA. The agents were modelled using Petri nets and agent communications were defined through the combination of FIPA Interaction Protocols. The system was tested under the conditions of static and dynamic scenarios, having its performance validated whenever possible by comparison with results from a Potential Fields Approach in the same benchmark. Overall, the performance exhibited by the proposed MAS was slightly better and it is worth highlighting the simple behaviour of each agent and ability to respond in real-time to all the dynamic scenarios tested.",https://ieeexplore.ieee.org/document/9207055/,2020 International Joint Conference on Neural Networks (IJCNN),19-24 July 2020,ieeexplore
10.1109/AQTR.2014.6857897,Multi-agent system for heterarchical product-driven manufacturing,IEEE,Conferences,"Product-driven manufacturing has gained a lot of traction recently among practitioners as it has the potential to take flexibility and agility of the manufacturing system to a new level compared to hierarchical control models. The advances in embedded technology have created the premises for the emergence of truly intelligent products that are capable not only of identification and information storage, but also of complex behavior and local decision making. In this context, this paper proposes a multi-agent control system that aims to solve the new challenges introduced by the shift to product-driven manufacturing, specifically addressing the special needs for information flow between shop floor entities and the MES system. The paper presents the pilot implementation, using the JADE multi-agent platform, a backtracking scheduler, an artificial neural network (ANN) for local decision making and the experimental results outlining the agent processing requirements during the product lifecycle.",https://ieeexplore.ieee.org/document/6857897/,"2014 IEEE International Conference on Automation, Quality and Testing, Robotics",22-24 May 2014,ieeexplore
10.1109/ICDMW.2019.00065,Mímir: Building and Deploying an ML Framework for Industrial IoT,IEEE,Conferences,"In this paper we describe Mímir, a production grade cloud and edge spanning ML framework for Industrial IoT applications. We first describe our infrastructure for optimized capture, streaming and multi-resolution storage of manufacturing data and its context. We then describe our workflow for scalable ML model training, validation, and deployment that leverages a manufacturing taxonomy and parameterized ML pipelines to determine the best metrics, hyper-parameters and models to use for a given task. We also discuss our design decisions on model deployment for real-time and batch data in the cloud and at the edge. Finally, we describe the use of the framework in building and deploying an application for Predictive Quality monitoring during a Plastics Extrusion manufacturing process.",https://ieeexplore.ieee.org/document/8955638/,2019 International Conference on Data Mining Workshops (ICDMW),8-11 Nov. 2019,ieeexplore
10.1109/ICIAS.2007.4658352,Neural network multi layer perceptron modeling of surface quality in laser machining,IEEE,Conferences,"Uncertainty is inevitable in problem solving and decision making. One way to reduce it is by seeking the advice of an expert in related field. On the other hand, when we use computers to reduce uncertainty, the computer itself can become an expert in a specific field through a variety of methods. One such method is machine learning, which involves using a computer algorithm to capture hidden knowledge from data. The researchers conducted the prediction of laser machining quality, namely surface roughness with seven significant parameters to obtain singleton output using machine learning techniques based on Quick Back Propagation Algorithm. In this research, we investigated a problem solving scenario for a metal cutting industry which faces some problems in determining the end product quality of Manganese Molybdenum (Mn-Mo) pressure vessel plates. We considered several real life machining scenarios with some expert knowledge input and machine technology features. The input variables are the design parameters which have been selected after a critical parametric investigation of 14 process parameters available on the machine. The elimination of non-significant parameters out of 14 total parameters were carried out by single factor and interaction factor investigation through design of experiment (DOE) analysis. Total number of 128 experiments was conducted based on 2<sup>k</sup> factorial design. This large search space poses a challenge for both human experts and machine learning algorithms in achieving the objectives of the industry to reduce the cost of manufacturing by enabling the off hand prediction of laser cut quality and further increase the production rate and quality.",https://ieeexplore.ieee.org/document/4658352/,2007 International Conference on Intelligent and Advanced Systems,25-28 Nov. 2007,ieeexplore
10.1109/CW.2016.52,Neuroscience Based Design: Fundamentals and Applications,IEEE,Conferences,"Neuroscience-based or neuroscience-informed design is a new application area of Brain-Computer Interaction (BCI). It takes its roots in study of human well-being in architecture, human factors study in engineering and manufacturing including neuroergonomics. In traditional human factors studies and/or well-being study, mental workload, stress, and emotion are obtained through questionnaires that are administered upon completion of some task and/or the whole experiment. Recent advances in BCI research allow for using Electroencephalogram (EEG) based brain state recognition algorithms to assess the interaction between brain and human performance. We propose and develop an EEG-based system CogniMeter to monitor and analyze human factors measurements of newly designed software/hardware systems and/or working places. Machine learning techniques are applied to the EEG data to recognize levels of mental workload, stress and emotions during each task. The EEG is used as a tool to monitor and record the brain states of subjects during human factors study experiments. We describe two applications of CogniMeter system: human performance assessment in maritime simulator and EEG-based human factors evaluation in Air Traffic Control (ATC) workplace. By utilizing the proposed EEG-based system, true understanding of subjects working patterns can be obtained. Based on the analyses of the objective real time EEG-based data together with the subjective feedback from the subjects, we are able to reliably evaluate current systems/hardware and/or working place design and refine new concepts and design of future systems.",https://ieeexplore.ieee.org/document/7756163/,2016 International Conference on Cyberworlds (CW),28-30 Sept. 2016,ieeexplore
10.1109/ISCAS.2009.5118362,New CNN based algorithms for the full penetration hole extraction in laser welding processes,IEEE,Conferences,"In this paper new CNN based visual algorithms for the control of welding processes are proposed. The high dynamics of laser welding in several manufacturing processes ranging from automobile production to precision mechanics requires the introduction of new fast real time controls. In the last few years, analogic circuits like cellular neural networks (CNN) have obtained a primary place in the development of efficient electronic devices because of their real-time signal processing properties. Furthermore, several pixel parallel CNN based architectures are now included within devices like the family of EyeRis systems [1]. In particular, the algorithms proposed in the following have been implemented on the EyeRis system v1.2 with the aim to be run at frame rates up to 20 kHz.",https://ieeexplore.ieee.org/document/5118362/,2009 IEEE International Symposium on Circuits and Systems,24-27 May 2009,ieeexplore
10.1109/MIV.1989.40559,New framework for dynamic scheduling of production systems,IEEE,Conferences,"A concept for dynamic scheduling in manufacturing systems is proposed. The scope of 'dynamic scheduling' treated includes online dynamic change of some scheduling parameters such as rules for part dispatching, machine selection, or routing. IF-THEN-type heuristic operators are utilized to perform this online real-time rule selection, and offline machine learning is used to obtain more detailed and powerful heuristics than those implemented by human experts or programmers. A learning algorithm has been developed to formulate operators that can treat quantity-type as well as quality-type information. A prototype computer program named learning aided dynamic scheduler (LADS) has been developed. A simulation study using LADS indicates good results for dynamic scheduling.&lt;<ETX>&gt;</ETX>",https://ieeexplore.ieee.org/document/40559/,"International Workshop on Industrial Applications of Machine Intelligence and Vision,",10-12 April 1989,ieeexplore
10.1109/SNPD.2016.7515945,Notice of Violation of IEEE Publication Principles: Adaptive Fuzzy PID speed control of DC belt conveyor system,IEEE,Conferences,Conveyor belt system is one of the most common transfer systems used in industry to transfer goods from one point to another in a limited distance. It is used in industries such as the electromechanical /mechanical assembly manufacturing to transfer work piece from one station to another or one process to another in food industries. The belt conveyor system discussed in this paper is driven by a DC motor and two speed controllers. The PID speed controller is designed to provide comparison to the main controller which is the Adaptive Fuzzy PID Speed controller. Both controllers are implemented in a real hardware where the algorithm will be written in PLC using SCL language. The experimental result shows that Adaptive Fuzzy PID controller performs better and adapted to the changes in load much faster than the conventional PID controller. This project has also proved that PLC is capable of performing high level control system tasks..,https://ieeexplore.ieee.org/document/7515945/,"2016 17th IEEE/ACIS International Conference on Software Engineering, Artificial Intelligence, Networking and Parallel/Distributed Computing (SNPD)",30 May-1 June 2016,ieeexplore
10.1109/TAI.1999.809838,On the sufficiency of limited testing for knowledge based systems,IEEE,Conferences,"Knowledge-based engineering and computational intelligence are expected to become core technologies in the design and manufacturing for the next generation of space exploration missions. Yet, if one is concerned with the reliability of knowledge based systems, studies indicate significant disagreement regarding the amount of testing needed for system assessment. The sizes of standard black-box test suites are impracticably large since the black-box approach neglects the internal structure of knowledge-based systems. On the contrary, practical results repeatedly indicate that only a few tests are needed to sample the range of behaviors of a knowledge-based program. In this paper, we model testing as a search process over the internal state space of the knowledge-based system. When comparing different test suites, the test suite that examines larger portion of the state space is considered more complete. Our goal is to investigate the trade-off between the completeness criterion and the size of test suites. The results of testing experiment on tens of thousands of mutants of real-world knowledge based systems indicate that a very limited gain in completeness can be achieved through prolonged testing. The use of simple (or random) search strategies for testing appears to be as powerful as testing by more thorough search algorithms.",https://ieeexplore.ieee.org/document/809838/,Proceedings 11th International Conference on Tools with Artificial Intelligence,9-11 Nov. 1999,ieeexplore
10.1109/CASE48305.2020.9216979,Online Computation Performance Analysis for Distributed Machine Learning Pipelines in Fog Manufacturing,IEEE,Conferences,"Smart manufacturing enables real-time data streaming from interconnected manufacturing processes to improve manufacturing quality, throughput, flexibility, and cost reduction via computation services. In these computation services, machine learning pipelines integrate various types of computation method options to match the contextualized, on-demand computation needs for the maximum prediction accuracy or the best model structure interpretation. On the other hand, there is a pressing need to integrate Fog computing in manufacturing, which will reduce communication time latency and dependency on connections, improve responsiveness and reliability of the computation services, and maintain data privacy. However, there is a knowledge gap in using machine learning pipelines in Fog manufacturing. Existing offloading strategies are not effective, due to the lack of accurate prediction model for the performance of computation services before the execution of those heterogeneous computation tasks. In this paper, machine learning pipelines are implemented in Fog manufacturing. The computation performance of each sub-step of pipelines is predicted and analyzed via linear regression models and random forest regression models. A Fog manufacturing testbed is adopted to validate the performance of the employed models. The results show that the models can adequately predict the performance of computation services, which can be further integrated into Fog manufacturing to better support offloading strategies for machine learning pipelines.",https://ieeexplore.ieee.org/document/9216979/,2020 IEEE 16th International Conference on Automation Science and Engineering (CASE),20-21 Aug. 2020,ieeexplore
10.1109/INM.2015.7140329,Ontology integration for advanced manufacturing collaboration in cloud platforms,IEEE,Conferences,"Advances in the field of cloud computing and networking have led to rapid development and market growth in areas such as online retail, gaming and healthcare. In the field of advanced manufacturing however, the impact has been significantly lesser than expected due to limitations in cloud platforms for fostering community engagement. To address this problem, we study a new cloud-based architecture that provides Platform-asa-Service (PaaS) management capabilities to the manufacturing community for delivering Software-as-a-Service (SaaS) “Apps” to their customers. Our architecture aims at supporting an “App Marketplace” that thrives on agile development, organic collaboration and scalable sales of next generation manufacturing Apps requiring high-performance simulation and modeling. Towards realizing the vision of the above architecture, our paper involves investigation and implementation of an Ontology Service that interoperates with other common web services related to resource brokering and accounting. Our Ontology Service uses principles of mapping and merging to translate a manufacturing App's collaboration requirements to suitable resource specifications on public cloud platforms. Integrated resultant ontology can be queried to provision the required resource parameters such as amount of memory/storage, number of processing units, and network protocol configurations needed for deployment of an App. We validate the effectiveness of our Ontology Service using the Protégé framework in a pilot testbed of a real-world “WheelSim” App in the NSF GENI Cloud platform. Our ontology integration results show benefits to an App developer in terms of: optimal user experience, lower design time and lower cost/simulation.",https://ieeexplore.ieee.org/document/7140329/,2015 IFIP/IEEE International Symposium on Integrated Network Management (IM),11-15 May 2015,ieeexplore
10.1109/BigData.2015.7363882,Open research challenges with Big Data — A data-scientist's perspective,IEEE,Conferences,"In this paper, we discuss data-driven discovery challenges of the Big Data era. We observe that recent innovations in being able to collect, access, organize, integrate, and query massive amounts of data from a wide variety of data sources have brought statistical data mining and machine learning under more scrutiny and evaluation for gleaning insights from the data than ever before. In that context, we pose and debate the question - Are data mining algorithms scaling with the ability to store and compute? If yes, how? If not, why not? We survey recent developments in the state-of-the-art to discuss emerging and outstanding challenges in the design and implementation of machine learning algorithms at scale. We leverage experience from real-world Big Data knowledge discovery projects across domains of national security, healthcare and manufacturing to suggest our efforts be focused along the following axes: (i) the `data science' challenge - designing scalable and flexible computational architectures for machine learning (beyond just data-retrieval); (ii) the ` science of data' challenge - the ability to understand characteristics of data before applying machine learning algorithms and tools; and (iii) the `scalable predictive functions' challenge - the ability to construct, learn and infer with increasing sample size, dimensionality, and categories of labels. We conclude with a discussion of opportunities and directions for future research.",https://ieeexplore.ieee.org/document/7363882/,2015 IEEE International Conference on Big Data (Big Data),29 Oct.-1 Nov. 2015,ieeexplore
10.1109/TAAI.2015.7407079,PCBA demand forecasting using an evolving Takagi-Sugeno system,IEEE,Conferences,"This paper investigates the use of using an evolving fuzzy system for printed circuit board (PCBA) demand forecasting. The algorithm is based on the evolving Takagi-Sugeno (eTS) fuzzy system, which has the ability to incorporate new patterns by changing its internal structure in an on-line fashion. We argue that these capabilities could aid in forecasting dynamic demand patterns such as those experienced in the electronic manufacturing (EMS) industry. An eTS fuzzy system is implemented in the R statistical programming language and is tested on both synthetic and real-world data. To our knowledge, this is one of the first applications of an evolving fuzzy system to forecast product demand. The results indicate that the evolving fuzzy system outperforms competing approaches for the application considered.",https://ieeexplore.ieee.org/document/7407079/,2015 Conference on Technologies and Applications of Artificial Intelligence (TAAI),20-22 Nov. 2015,ieeexplore
10.1109/BigDataCongress.2019.00032,"PREMISES, a Scalable Data-Driven Service to Predict Alarms in Slowly-Degrading Multi-Cycle Industrial Processes",IEEE,Conferences,"In recent years, the number of industry-4.0-enabled manufacturing sites has been continuously growing, and both the quantity and variety of signals and data collected in plants are increasing at an unprecedented rate. At the same time, the demand of Big Data processing platforms and analytical tools tailored to manufacturing environments has become more and more prominent. Manufacturing companies are collecting huge amounts of information during the production process through a plethora of sensors and networks. To extract value and actionable knowledge from such precious repositories, suitable data-driven approaches are required. They are expected to improve the production processes by reducing maintenance costs, reliably predicting equipment failures, and avoiding quality degradation. To this aim, Machine Learning techniques tailored for predictive maintenance analysis have been adopted in PREMISES (PREdictive Maintenance service for Industrial procesSES), an innovative framework providing a scalable Big Data service able to predict alarming conditions in slowly-degrading processes characterized by cyclic procedures. PREMISES has been experimentally tested and validated on a real industrial use case, resulting efficient and effective in predicting alarms. The framework has been designed to address the main Big Data and industrial requirements, by being developed on a solid and scalable processing framework, Apache Spark, and supporting the deployment on modularized containers, specifically upon the Docker technology stack.",https://ieeexplore.ieee.org/document/8818217/,2019 IEEE International Congress on Big Data (BigDataCongress),8-13 July 2019,ieeexplore
10.1109/SMILE.2018.8353980,Parametric study and design of deep learning on leveling system for smart manufacturing,IEEE,Conferences,"Sheet metal is widely used in the industry for metal forming purposes, such as metal stamping and metal cutting. It is often winded and storage in a coil form for transportation purposes. However, before any manufacturing process such as, cutting, or stamping, leveling is required as the residual stress inside coil is present which can cause distortion to the metal forming/cutting process. In conventional coil leveling machines, the machine parameters are often set by machine technicians with many years of experiences. In addition, the optimized machine parameter is achieved by trial and error method or based on experiences. However, the machine parameters are also not exactly trivial due to too many input factors which may cause changes to the outcome result. In the recent years, industry 4.0 and smart manufacturing has been a widely discussed topic in terms of industry manufacturing solutions in many different industrialized countries. In smart manufacturing, communication and interaction between machines have become an important role to improve manufacturing efficiency, flexibility and customization. As smart manufacturing focused on information process through real objects, it is required to digitize the experience through deep learning method. This paper is aimed to describe and study the deep learning application based on coil leveling system. Finally, through this study and experiment verification, analyzes on research directions and prospects of deep learning.",https://ieeexplore.ieee.org/document/8353980/,"2018 IEEE International Conference on Smart Manufacturing, Industrial & Logistics Engineering (SMILE)",8-9 Feb. 2018,ieeexplore
10.1109/BigData.2018.8622389,Performance and Memory Trade-offs of Deep Learning Object Detection in Fast Streaming High-Definition Images,IEEE,Conferences,"Deep learning models are associated with various deployment challenges. Inference of such models is typically very compute-intensive and memory-intensive. In this paper, we investigate the performance of deep learning models for a computer vision application used in the automotive manufacturing industry. This application has demanding requirements that are characteristic of Big Data systems, including high volume and high velocity. The application has to process a very large set of high-definition images in real-time with appropriate accuracy requirements using a deep learning-based object detection model. Meeting the run time, accuracy, and resource requirements require a careful consideration of the choice of model, model parameters, hardware, and environmental support. In this paper, we investigate the trade-offs of the most popular deep neural network-based object detection models on four hardware platforms. We report the trade-offs of resource consumption, run time, and accuracy for a realistic real-time application environment.",https://ieeexplore.ieee.org/document/8622389/,2018 IEEE International Conference on Big Data (Big Data),10-13 Dec. 2018,ieeexplore
10.1109/SEC50012.2020.00019,Poster: Lambda architecture for robust condition based maintenance with simulated failure modes,IEEE,Conferences,"Condition based maintenance (CBM) is increasingly seen as a promising approach for addressing downtime issues which are a common occurrence in the manufacturing industry and are a major cause of lost productivity. However, it has been a challenge to develop a generic CBM solution that works for all assets since each asset has unique sources of noise. This mandates use of manual diagnostics to custom tailor a solution for each asset for accurate failure mode identification (FMI). This problem is further compounded by the scarcity of failure data. In this paper, we propose a lambda architecture for FMI of industrial assets that achieves low initial deployment cost while securing a reasonable classification accuracy. The lambda architecture consists of a light-compute edge node, such as Raspberry Pi, that processes high-speed vibration data in real-time to extract useful features and applies a deep-learning (DL) engine which is trained in a cloud platform, such as AWS. In addition, we also incorporate a failure modes' feature simulator so that DL models can adapt to different industrial assets without costly failure data collection. Finally, experimental results are provided using the bearings' failures dataset validating the proposed cost-effective CBM architecture with high accuracy and scalability.",https://ieeexplore.ieee.org/document/9355694/,2020 IEEE/ACM Symposium on Edge Computing (SEC),12-14 Nov. 2020,ieeexplore
10.1109/GloSIC.2018.8570124,Probabilistic Estimations of Increasing Expected Reliability and Safety for Intelligent Manufacturing,IEEE,Conferences,"In the near future the possibilities of the modern probabilistic models, artificial intelligence and machine learning methods can provide an intelligent support of making decisions by an operator in real time. An agile recovery of intelligent manufacturing integrity can be implemented owing to the development of industrial robotics. For intelligent manufacturing it means the expected reliability and safety may be in the near future at the expense of intelligent support of decision making and the agile recovery of integrity. To answer the question “How much essential may be this increasing?” here are proposed: general analytical approaches for a probabilistic estimation of the expected reliability and safety for every monitored element or the system of intelligent manufacturing on a level of probability distribution functions (PDF) of the time between the losses of system integrity; estimations of increasing the expected reliability and safety for intelligent manufacturing at the expense of the intelligent support of decision making and agile recovery of integrity; the comparisons of the estimations on a prognostic period up to 10 years using the identical model in applications to expected reliability and safety. The applications of the proposed approaches allow the customers, designers, developers, users and experts of Industry 4.0 intelligent manufacturing to be guided by the proposed probabilistic estimations for solving problems of reliability and safety in the system life cycle. The results are demonstrated by examples.",https://ieeexplore.ieee.org/document/8570124/,2018 Global Smart Industry Conference (GloSIC),13-15 Nov. 2018,ieeexplore
10.1109/IWAPS51164.2020.9286803,Real time process monitoring using diffraction-based overlay measurements from YieldStar,IEEE,Conferences,Real-time process monitoring (RTPM) is a method for semiconductor manufacturing monitoring and tuning using a physical prediction model. It is a fast and nondestructive process excursion measurement method which takes inputs from diffraction-based overlay measurements from YieldStar. The prediction model is created by a physical model which receives standard manufacturing information as input. The prediction capability has been validated in a manufacturing environment experiment with thin film thickness prediction difference less than 3%.,https://ieeexplore.ieee.org/document/9286803/,2020 International Workshop on Advanced Patterning Solutions (IWAPS),5-6 Nov. 2020,ieeexplore
10.1109/CAC51589.2020.9327198,Real time production scheduling based on Asynchronous Advanced Actor Critic and composite dispatching rule,IEEE,Conferences,"In the era of smart manufacturing, the requirements of real-time, adaptability and long-term optimization of the semiconductor manufacturing system (SMS) are increased due to the further expanded, more complicated and unpredictable uncertainties. This paper addresses the real time production scheduling of SMS to maximize on productivity (PROD) and average daily movement (AvgMOV), and minimize mean cycle time (MCT). We propose an Asynchronous Advanced Actor Critic and composite dispatching rule based real time production scheduling (A3C-CR2) framework, which involves a scheduling knowledge training module and a deployment module. The action space is designed as a combination of the composite dispatching rule (CDR) based continuous scheduling actions. In terms of various performance indices over a long period, the proposed A3C-CR2 approach outperforms other dispatching rules.",https://ieeexplore.ieee.org/document/9327198/,2020 Chinese Automation Congress (CAC),6-8 Nov. 2020,ieeexplore
10.1109/SACI51354.2021.9465544,Real-time locating system and digital twin in Lean 4.0,IEEE,Conferences,"Digital twin plays a key role in the current development of smart manufacturing systems. Through simulation in the cyber world, real phenomena in the physical world can be predicted and optimized before the final implementation. The usage of the digital twin is enhanced along with the uprising of Industry 4.0, in which data availability supports the further insight of system status, helping the operation managers understand their system and perform resources adjustment more easily. Based on this digitization mature, Lean 4.0, a new concept elaborated from Lean manufacturing, has been interested recently. There are several technologies constituted digital twin that provide a favourable condition for Lean 4.0, such as augmented reality, cloud computing. In this paper, the integration of the Real-time Locating System (RTLS) into digital twin is proposed, which facilitates the performance of Lean 4.0 in manufacturing operation. Not only gain effective control over the facility's assets, but this integration also enhances the resources utilization, cut down operational wastes, thus brings a better turnover for industrial systems. A case study of successful implementation is shown, which proved the possible advantages of this approach.",https://ieeexplore.ieee.org/document/9465544/,2021 IEEE 15th International Symposium on Applied Computational Intelligence and Informatics (SACI),19-21 May 2021,ieeexplore
10.1109/RTOSS.1994.292553,Real-time platforms and environments for time constrained flexible manufacturing,IEEE,Conferences,"The Spring Kernel and associated algorithms, languages, and tools provide system support for static or dynamic real-time applications that require predictable operation. Spring currently consists of two major parts: (1) the development environment, where application and target systems are described, preprocessed and downloaded, and (2) the run-time environment, where the operating system, the Spring Kernel, creates and ensures predictable executions of application tasks. We have integrated our real-time systems technology with component technologies from robotics, computer vision, and real-time artificial intelligence, to develop a test platform for flexible manufacturing. The results being produced are generic so that they should be in many other real-time applications such as air traffic control and chemical plants. We describe this platform, identify new features developed, and comment on some lessons learned to date from this experiment.&lt;<ETX>&gt;</ETX>",https://ieeexplore.ieee.org/document/292553/,Proceedings of 11th IEEE Workshop on Real-Time Operating Systems and Software,18-19 May 1994,ieeexplore
10.1109/CDC.1994.411283,Real-time tool wear identification using sensor integration with neural network,IEEE,Conferences,"Real-time identification of tool wear in shop floor environment is essential for optimization of machining processes and implementation of automated manufacturing systems. In this paper. the signals obtained from acoustic emission and power sensors during machining processes are analyzed and a set of feature parameters characterizing the tool wear condition are extracted. In order to realize the realtime tool wear condition monitoring for different cutting conditions, a sensor integration strategy which combines the information from multiple sensors (acoustic emission sensor and power sensor) and machining parameters is proposed. A neural network based on improved back-propogation algorithm is developed and a prototype scheme for realtime identification of tool wear is implemented. Experiments under different conditions have proved that a higher rate of tool wear identification can be achieved by using the sensor integration model with neural network. The results also indicated that the neural network is a very effective method of sensor integration for online monitoring of tool abnormalities.&lt;<ETX>&gt;</ETX>",https://ieeexplore.ieee.org/document/411283/,Proceedings of 1994 33rd IEEE Conference on Decision and Control,14-16 Dec. 1994,ieeexplore
10.1109/SMC.2019.8914044,Realizing an assembly task through virtual capture,IEEE,Conferences,"Modern manufacturing strategy requires the robotic infrastructure to be able to adapt to new products or to accomplish new tasks quickly. In order to respond to this demand, teaching a robot to realize a task by demonstration has regained popularity in recent years, especially for dual-arm or humanoid robots. One of the main issues using this method is to adapt the captured motion from the human demonstration to the robot's specific kinematics and control. In this paper we present a method where the motion and grasping adaptation is tackled during the capture. We demonstrate the validity of this method with an experiment where a humanoid robot realizes an assembly previously demonstrated by a user wearing a Head Mounted Display (HMD) performing an assembly task in a virtual environment.",https://ieeexplore.ieee.org/document/8914044/,"2019 IEEE International Conference on Systems, Man and Cybernetics (SMC)",6-9 Oct. 2019,ieeexplore
10.1109/INDIN45582.2020.9442114,Reinforcement Learning Driven Adaptive VR Streaming with Optical Flow Based QoE,IEEE,Conferences,"With the merit of containing full panoramic content in one camera, Virtual Reality (VR) and 360° videos have arisen in the field of industrial cloud manufacturing and training. Industrial Internet of Things (IoT), where many VR terminals needed to be online at the same time, can hardly guarantee VR's bandwidth requirement. However, by making use of users' quality of experience (QoE) awareness factors, including the relative moving speed and depth difference between the viewpoint and other content, bandwidth consumption can be reduced. In this paper, we propose Optical Flow Based VR(OFB-VR), an interactive method of VR streaming that can make use of VR users' QoE awareness to ease the bandwidth pressure. The Just-Noticeable Difference through Optical Flow Estimation (JND-OFE) is explored to quantify users' awareness of quality distortion in 360° videos. Accordingly, a novel 360° videos QoE metric based on Peak Signal-to-Noise Ratio and JND-OFE (PSNR-OF) is proposed. With the help of PSNR-OF, OFB-VR proposes a versatile-size tiling scheme to lessen the tiling overhead. A Reinforcement Learning (RL) method is implemented to make use of historical data to perform Adaptive BitRate (ABR). For evaluation, we take two prior VR streaming schemes, Pano and Plato, as baselines. Vast evaluations show that our system can increase the mean PSNR-OF score by 9.5-15.8% while maintaining the same rebuffer ratio compared with Pano and Plato in a fluctuate LTE bandwidth dataset. Evaluation results show that OFB-VR is a promising prototype for actual interactive industrial VR. A prototype of OFB-VR can be found in https://github.com/buptexplorers/OFB-VR.",https://ieeexplore.ieee.org/document/9442114/,2020 IEEE 18th International Conference on Industrial Informatics (INDIN),20-23 July 2020,ieeexplore
10.1109/DTPI52967.2021.9540104,Research and practice of lightweight digital twin speeding up the implementation of flexible manufacturing systems,IEEE,Conferences,"Parallel manufacturing in Industry 5.0 requires digital twin to digitize physical systems, building virtual models to open up channels connecting physical systems, information systems, and social systems, and transforming the physical models of the existing production environment to achieve two-way feedback of virtual and real is the current research direction. This paper proposes the modeling idea of lightweight digital twin, extracts core dimensions and performs digital virtual simulation, so as to quickly realize the complete process of two-way feedback, and realize a set of chess flexible parallel manufacturing production lines as a practice for the design of complete lightweight digital twin.",https://ieeexplore.ieee.org/document/9540104/,2021 IEEE 1st International Conference on Digital Twins and Parallel Intelligence (DTPI),15 July-15 Aug. 2021,ieeexplore
10.1109/ICRA.2019.8794127,Residual Reinforcement Learning for Robot Control,IEEE,Conferences,"Conventional feedback control methods can solve various types of robot control problems very efficiently by capturing the structure with explicit models, such as rigid body equations of motion. However, many control problems in modern manufacturing deal with contacts and friction, which are difficult to capture with first-order physical modeling. Hence, applying control design methodologies to these kinds of problems often results in brittle and inaccurate controllers, which have to be manually tuned for deployment. Reinforcement learning (RL) methods have been demonstrated to be capable of learning continuous robot controllers from interactions with the environment, even for problems that include friction and contacts. In this paper, we study how we can solve difficult control problems in the real world by decomposing them into a part that is solved efficiently by conventional feedback control methods, and the residual which is solved with RL. The final control policy is a superposition of both control signals. We demonstrate our approach by training an agent to successfully perform a real-world block assembly task involving contacts and unstable objects.",https://ieeexplore.ieee.org/document/8794127/,2019 International Conference on Robotics and Automation (ICRA),20-24 May 2019,ieeexplore
10.1109/ICE/ITMC52061.2021.9570221,Resilient Manufacturing Systems enabled by AI support to AR equipped operator,IEEE,Conferences,"Supply chains and manufacturing systems robustness and resilience are, for many years, but especially nowadays, key features requested to ensure reliable and efficient production processes. Two domains are crucial to achieve such purpose: the former is fast and comprehensive monitoring, efficient and reliable condition detection and effective and explicable support for decision making. The latter refers to the intervention by operators, able to better identify problems and to put in place effective operations aimed at fixing it or, better, to prevent such circumstances. This paper presents an integrated approach encompassing a sophisticated IoT and AI-based approach to monitor and detect critical situations, fully integrated with an AR (Augmented Reality) system supporting operators in the field to take informed actions in bi-directional continuous connection. Activities in the context of EC funded project Qu4lity developed in Politecnico di Milano Industry 4.0 Lab, a test environment implementing the proposed approach and demonstrating in an automated production line the effectiveness of the approach, significantly improving performances. Analysis of performance indicators demonstrates the soundness of the proposed solution and implementation methodology to make the overall production process more resilient, efficient and with product defects reduction.",https://ieeexplore.ieee.org/document/9570221/,"2021 IEEE International Conference on Engineering, Technology and Innovation (ICE/ITMC)",21-23 June 2021,ieeexplore
10.1109/ICRA48506.2021.9561020,SQRP: Sensing Quality-aware Robot Programming System for Non-expert Programmers,IEEE,Conferences,"Robot programming typically makes use of a set of mechanical skills that is acquired by machine learning. Because there is in general no guarantee that machine learning produces robot programs that are free of surprising behavior, the safe execution of a robot program must utilize monitoring modules that take sensor data as inputs in real time to ensure the correctness of the skill execution. Owing to the fact that sensors and monitoring algorithms are usually subject to physical restrictions and that effective robot programming is sensitive to the selection of skill parameters, these considerations may lead to different sensor input qualities such as the view coverage of a vision system that determines whether a skill can be successfully deployed in performing a task. Choosing improper skill parameters may cause the monitoring modules to delay or miss the detection of important events such as a mechanical failure. These failures may reduce the throughput in robotic manufacturing and could even cause a destructive system crash. To address above issues, we propose a sensing quality-aware robot programming system that automatically computes the sensing qualities as a function of the robot’s environment and uses the information to guide non-expert users to select proper skill parameters in the programming phase. We demonstrate our system framework on a 6DOF robot arm for an object pick-up task.",https://ieeexplore.ieee.org/document/9561020/,2021 IEEE International Conference on Robotics and Automation (ICRA),30 May-5 June 2021,ieeexplore
10.1109/RoboSoft48309.2020.9116004,Scalable sim-to-real transfer of soft robot designs,IEEE,Conferences,"The manual design of soft robots and their controllers is notoriously challenging, but it could be augmented-or, in some cases, entirely replaced-by automated design tools. Machine learning algorithms can automatically propose, test, and refine designs in simulation, and the most promising ones can then be manufactured in reality (sim2real). However, it is currently not known how to guarantee that behavior generated in simulation can be preserved when deployed in reality. Although many previous studies have devised training protocols that facilitate sim2real transfer of control polices, little to no work has investigated the simulation-reality gap as a function of morphology. This is due in part to an overall lack of tools capable of systematically designing and rapidly manufacturing robots. Here we introduce a low cost, open source, and modular soft robot design and construction kit, and use it to simulate, fabricate, and measure the simulation-reality gap of minimally complex yet soft, locomoting machines. We prove the scalability of this approach by transferring an order of magnitude more robot designs from simulation to reality than any other method. The kit and its instructions can be found here: github.com/skriegman/sim2real4designs.",https://ieeexplore.ieee.org/document/9116004/,2020 3rd IEEE International Conference on Soft Robotics (RoboSoft),15 May-15 July 2020,ieeexplore
10.1109/IS3C50286.2020.00134,Screw defect detection system based on AI image recognition technology,IEEE,Conferences,"In the past ten years, smart manufacturing has been widely discussed and gradually introduced into various manufacturing fields. Since Germany proposed the concept of “Industry 4.0” in 2011, it has been spreading and feverish all over the world. For Industry 4.0, information digitization, intelligent defect detection and database platform management are their main core technologies. Aiming at a large screw industry manufacturing field in central and southern Taiwan, this paper proposes a screw defect detection system based on AI image recognition technology to detect damage to the nut during the “molding” process in the screw production process, and it is determined whether the inspected screw passes the inspection. The recognition result is given as shown in Figure 1. This paper uses 500 non-defective screw samples and 20 defective screw samples provided by the screw factory. The above samples collected real-time images through the sampling structure designed in this article, and we adopt Microsoft Corporation's ML.NET suite to model AI images, and uses the following four deep learning models: ResNetV2 50, ResNetV2 101, InceptionV3, MoblieNetV2 for learning; in the process of learning, this article divides the data set into three types of data sets (one is the unknown set that is not used for training but mixed with correct and defective samples, and the other is used for post-training verification of mixed samples with correct and defective samples. The third is a training set for training a mixture of correct and defective samples) This arrangement is used for subsequent verification models; after training, a PC-based screw defect detection system is implemented as shown in Figure 2; finally, with Detect screw defects in the form of instant photography. After the experiment, in 1,000 repeated tests, the success rate of defect detection reached 97%, while the false positive rate was only 2%.",https://ieeexplore.ieee.org/document/9394116/,"2020 International Symposium on Computer, Consumer and Control (IS3C)",13-16 Nov. 2020,ieeexplore
10.1109/CHICC.2008.4605508,Service-oriented design and implementation strategy of real-time distributed embedded control software,IEEE,Conferences,"Real-time embedded systems are core issue related to information technology and own huge application demand. This paper presents a service-oriented design methodology that reduces complexity by separating data-related computational parts and interaction among components. Architectures are composed hierarchically to manage embedded models and achieve real-time actor and architecture reuse. We introduce a notion of supervisor to manage the architecture and discuss how to aggregate individual componentpsilas computation into a well-defined composite computation from a view of interaction semantics contract. A component-oriented hierarchically real-time quality analysis and processing control system of manufacturing data in Kunming Iron &amp; Steel Co, Ltd.(KISC) is implemented to prove the feasibility and flexibility of our methodology.",https://ieeexplore.ieee.org/document/4605508/,2008 27th Chinese Control Conference,16-18 July 2008,ieeexplore
10.1109/RO-MAN50785.2021.9515431,Simplifying the A.I. Planning modeling for Human-Robot Collaboration,IEEE,Conferences,"For an effective deployment in manufacturing, Collaborative Robots should be capable of adapting their behavior to the state of the environment and to keep the user safe and engaged during the interaction. Artificial Intelligence (AI) enables robots to autonomously operate understanding the environment, planning their tasks and acting to achieve some given goals. However, the effective deployment of AI technologies in real industrial environments is not straightforward. There is a need for engineering tools facilitating communication and interaction between AI engineers and Domain experts. This paper proposes a novel software tool, called TENANT (Tool fostEriNg Ai plaNning in roboTics) whose aim is to facilitate the use of AI planning technologies by providing domain experts like e.g., production engineers, with a graphical software framework to synthesize AI planning models abstracting from syntactic features of the underlying planning formalism.",https://ieeexplore.ieee.org/document/9515431/,2021 30th IEEE International Conference on Robot & Human Interactive Communication (RO-MAN),8-12 Aug. 2021,ieeexplore
10.1109/IJCNN.1993.714078,Simulation of manufacturing models and its learning with artificial neural networks,IEEE,Conferences,"Introduces some new faces of simulation for intelligent manufacturing systems. Not only the productive parameters are treated from a multi-level point of view but the economic indicators were calculated with global optimistic features. The general marketing aspects are considered as critical conditions. To solve several decision making problems of a complex manufacturing organization, the authors extract meaningful variables to observe by data analysis and construct an artificial neural net. The authors try to take some advantages of non-symbolic processing such that parallel treatment and its implementation guide toward a real-time system, learning capabilities also can be applied independently to problems.",https://ieeexplore.ieee.org/document/714078/,"Proceedings of 1993 International Conference on Neural Networks (IJCNN-93-Nagoya, Japan)",25-29 Oct. 1993,ieeexplore
10.1109/ICOEI.2019.8862602,Smart Disaster Management and Prevention using Reinforcement Learning in IoT Environment,IEEE,Conferences,"At starting of the Internet of Things (IoT), it is passing around a world, in which diverse kinds of different objects are there connected to the Internet. It contains the use of smart phones, sensors, cameras, and other devices to make over the actions of people and things into data and link it to the Internet. With its capability to model the real world in digital form and accomplish scrutiny and replication in cyberspace, the IoT is able to reveal new value at an unparalleled rate and deliver it as response to the real world. This is set to convey main changes that will lengthen to the structure of industry in addition to the infrastructure of society itself. Therefore although the occurrence of the IoT contributes rise to new value, it besides means the occurrence of new threats. The proposed work covenant with disaster management as well as prevention to manufacturing industry using IoT. System first investigates the threat scenario during general execution of work, and finds the critical situations. The system processes learning approach for identifying such critical situations and execute the output appliances. System utilized multiple input along with output sensor for experiment. The Q-Learning approach has used for updating the policy which can provide the best result with high accuracy.",https://ieeexplore.ieee.org/document/8862602/,2019 3rd International Conference on Trends in Electronics and Informatics (ICOEI),23-25 April 2019,ieeexplore
10.1109/SysEng.2017.8088320,Software-in-the-loop testbed for multi-agent-systems in a discrete event simulation: Integration of the Java Agent Development Framework into Plant Simulation,IEEE,Conferences,"Today's research projects propose a modular manufacturing environment for production sites, which adapt itself autonomously and makes manufacturing decisions without human interaction. Therefore, it is necessary that the next generations of production lines, especially the intralogistics transportation systems, are designed more adaptable and flexible. The object in this paper is a cyber-physical material flow system with flexible, autonomous and collaborative vehicles combined with centralized sensors to digitize the workspace. For this purpose, an interface was developed which allows a discrete event simulation tool to communicate with a Multi-Agent-System. Thereby, the decision-making of the agents is integrated directly into the simulation process of the discrete event simulation software. The architecture of this interface is presented as well as a test of its functionality. The architecture is implemented with the Java Agent Development Framework and Plant Simulation as the discrete event simulation tool. The result is an interface, which allows to transfer data from the simulation, in case of an event, to the agent platform. The Multi-Agent-System solves the event specific problem due to its ontology and responses it to the simulation. Therefore, it is possible to integrate the ontology implemented in the physical system as software-in-the-loop in the simulation environment. Furthermore, the possibility is given to improve the ontology iteratively based on historical production data. Different strategies of agents can be combined and improved through machine-learning algorithms by using real production data from the task specific hardware. This leads into a continuous improvement process.",https://ieeexplore.ieee.org/document/8088320/,2017 IEEE International Systems Engineering Symposium (ISSE),11-13 Oct. 2017,ieeexplore
10.1109/ICRITO48877.2020.9198036,State of Art: Energy Efficient Protocols for Self-Powered Wireless Sensor Network in IIoT to Support Industry 4.0,IEEE,Conferences,"Up gradation of manufacturing systems in industries by means of implementing innovative manufacturing techniques that captures real time data, applies machine learning algorithms, makes entire system self-decisive and provides inter connectivity to the whole system is the prime focus of the Industry 4.0. It is aimed at bringing new industrial revolution with the help of internet of things technology due to its considerable influence in the industrial manufacturing process. Though impact of internet of things in industrial sector is huge, a practical implementation incorporates challenges in energy efficiency, self-powered sensor nodes and security. For processing of gathered data self-powered sensor nodes may sinks energy from ambient energy sources. A considerable amount of efforts has been put by researchers to address the challenges for development of energy efficient routing protocol for such nodes. The presented survey is aimed at analyzing the protocols for contribution towards the goals of industry 4.0 Energy efficient protocol will support the system to consume least energy for its operation. Henceforth combination of self-powered wireless sensor network and energy efficient protocol will be useful to power up many industrial IoT applications.",https://ieeexplore.ieee.org/document/9198036/,"2020 8th International Conference on Reliability, Infocom Technologies and Optimization (Trends and Future Directions) (ICRITO)",4-5 June 2020,ieeexplore
10.1109/CSCWD.2005.194338,Study of ASP service lifecycle management technologies for networked manufacturing system,IEEE,Conferences,"Nowadays, networked manufacturing system based on ASP (application service provider) has become one of the hotspots of research and application. How to manage large amount of ASP services, and how to provide better service quality, has become very important. The paper mainly studies the ASP service management technologies. We present the concept of service lifecycle management (SLM), defines ASP service, and set up a state model for service lifecycle. Then, we give several key technologies' solutions for implementation of service management. Finally the application of these technologies in a real networked manufacturing system is introduced.",https://ieeexplore.ieee.org/document/1504245/,"Proceedings of the Ninth International Conference on Computer Supported Cooperative Work in Design, 2005.",24-26 May 2005,ieeexplore
10.1109/ICE/ITMC49519.2020.9198430,Supporting SMEs in the Lake Constance Region in the Implementation of Cyber-Physical-Systems: Framework and Demonstrator,IEEE,Conferences,"With the emergence of the recent Industry 4.0 movement, data integration is now also being driven along the production line, made possible primarily by the use of established concepts of intelligent supply chains, such as the digital avatars. Digital avatars - sometimes also called Digital Twins or more broadly Cyber-Physical Systems (CPS) - are already successfully used in holistic systems for intelligent transport ecosystems, similar to the use of Big Data and artificial intelligence technologies interwoven with modern production and supply chains. The goal of this paper is to describe how data from interwoven, autonomous and intelligent supply chains can be integrated into the diverse data ecosystems of the Industry 4.0, influenced by a multitude of data exchange formats and varied data schemas. In this paper, we describe how a framework for supporting SMEs was established in the Lake Constance region and describe a demonstrator sprung from the framework. The demonstrator project's goal is to exhibit and compare two different approaches towards optimisation of manufacturing lines. The first approach is based upon static optimisation of production demand, i.e. exact or heuristic algorithms are used to plan and optimise the assignment of orders to individual machines. In the second scenario, we use real-time situational awareness - implemented as digital avatar - to assign local intelligence to jobs and raw materials in order to compare the results to the traditional planning methods of scenario one. The results are generated using event-discrete simulation and are compared to common (heuristic) job scheduling algorithms.",https://ieeexplore.ieee.org/document/9198430/,"2020 IEEE International Conference on Engineering, Technology and Innovation (ICE/ITMC)",15-17 June 2020,ieeexplore
10.1109/IJCNN.1990.137594,Synergy of artificial neural networks and knowledge-based expert systems for intelligent FMS scheduling,IEEE,Conferences,"A hybrid architecture that integrates artificial neural networks and knowledge-based expert systems to generate solutions for the real-time scheduling of flexible manufacturing systems is described. The artificial neural networks perform pattern recognition and, due to their inherent characteristics, support the implementation of automated knowledge acquisition and refinement schemes through a feedback mechanism. The artificial neural network structures enable the system to recognize patterns in the tasks to be solved in order to select the best scheduling rule according to different demands. The knowledge-based expert systems are the higher-order elements which drive the inference strategy and interpret the constraints and restrictions imposed by the upper levels of the flexible manufacturing system control hierarchy. The level of self-organization achieved provides a system with a higher probability of success than traditional approaches",https://ieeexplore.ieee.org/document/5726554/,1990 IJCNN International Joint Conference on Neural Networks,17-21 June 1990,ieeexplore
10.1109/ICCRD.2011.5764067,Table of contents vol. 01,IEEE,Conferences,The following topics are dealt with: computer research and development; event driven programming; artificial intelligence; expert systems; algorithm analysis; high performance computing; automated software engineering; human computer interaction; bioinformatics; scientific computing; image processing; information retrieval; compilers; interpreters; computational intelligence; computer architecture; embedded systems; computer animation; Internet; Web applications; communication/networking; knowledge data engineering; computer system implementation; logics; VLSI; mathematical software; information systems; computer based education; mathematical logic; mobile computing; computer games; multimedia applications; computer graphics; virtual reality; natural language processing; neural networks; computer modeling; parallel computing; distributed computing; computer networks; pattern recognition; computer security; computer simulation; computer vision; probability; statistics; performance evaluation; computer aided design/manufacturing; computing ethics; programming languages; problem complexity; control systems; physical sciences; engineering; discrete mathematics; reconfigurable computing systems; data communications; robotics; automation; system security; cryptography; data compression; data encryption; data mining; database systems; document processing; text processing; educational technology; digital library; technology management; digital signal processing; theoretical computer science; digital systems; logic design; ubiquitous computing; and visualizations.,https://ieeexplore.ieee.org/document/5764067/,2011 3rd International Conference on Computer Research and Development,11-13 March 2011,ieeexplore
10.1109/TAI.1992.246450,The G2 development and deployment environment,IEEE,Conferences,"G2 is an object-oriented development and deployment environment, combining rule-based and procedural reasoning, user interface graphics, database interface capabilities, dynamic simulation, and real-time execution in a single package. The applications of G2 typically have large knowledge bases, with tens of thousands of objects, and generally real-time execution is required. The online installations span the process industries, discrete manufacturing, aerospace, telecommunications, electric utilities, and others. Primarily these installations have been implemented directly by plant engineers, who have defined the knowledge contained in the applications using G2's structured natural language and active-object graphics. The design of G2 is described.&lt;<ETX>&gt;</ETX>",https://ieeexplore.ieee.org/document/246450/,Proceedings Fourth International Conference on Tools with Artificial Intelligence TAI '92,10-13 Nov. 1992,ieeexplore
10.1109/icABCD49160.2020.9183853,The Impact of Smart Manufacturing Approach On The South African Manufacturing Industry,IEEE,Conferences,"SM is a technology-driven approach that mainly utilises machines to monitor the entire production of an organisation. The objective of SM in an organisation is to identify ways to automatize the manufacturing process while using data analytics to optimize the manufacturing performance. This research mitigates the impact of technology, in this case expressed as SM in South African industries. The research followed a quantitative approach whereby 42 respondents from low, medium low and high technology industries took part in the study. Data has been amassed from first-hand experience by mean of an adapted questionnaire constituted of three sections: The first section was about the general demographic information of the respondents. Section two investigates the respondent's awareness on SM. Finally, section three assessed the impact that SM had on the performance of the organisation. The findings of this study revealed that Smart Manufacturing has a positive impact in South African manufacturing organisations as it allows effective operations, fast response to customers demand, real time operations optimisation. Nevertheless, Smart Manufacturing is a new concept under the fourth industrial revolution in South Africa and will need time before being totally implemented in all organisations as it is costly.",https://ieeexplore.ieee.org/document/9183853/,"2020 International Conference on Artificial Intelligence, Big Data, Computing and Data Communication Systems (icABCD)",6-7 Aug. 2020,ieeexplore
10.1109/WF-IoT48130.2020.9221078,The Manufacturing Data and Machine Learning Platform: Enabling Real-time Monitoring and Control of Scientific Experiments via IoT,IEEE,Conferences,"IoT devices and sensor networks present new opportunities for measuring, monitoring, and guiding scientific experiments. Sensors, cameras, and instruments can be combined to provide previously unachievable insights into the state of ongoing experiments. However, IoT devices can vary greatly in the type, volume, and velocity of data they generate, making it challenging to fully realize this potential. Indeed, synergizing diverse IoT data streams in near-real time can require the use of machine learning (ML). In addition, new tools and technologies are required to facilitate the collection, aggregation, and manipulation of sensor data in order to simplify the application of ML models and in turn, fully realize the utility of IoT devices in laboratories. Here we will demonstrate how the use of the Argonne-developed Manufacturing Data and Machine Learning (MDML) platform can analyze and use IoT devices in a manufacturing experiment. MDML is designed to standardize the research and operational environment for advanced data analytics and AI-enabled automated process optimization by providing the infrastructure to integrate AI in cyber-physical systems for in situ analysis. We will show that MDML is capable of processing diverse IoT data streams, using multiple computing resources, and integrating ML models to guide an experiment.",https://ieeexplore.ieee.org/document/9221078/,2020 IEEE 6th World Forum on Internet of Things (WF-IoT),2-16 June 2020,ieeexplore
10.1109/ICMLC.2017.8108967,The RFID-based real-time monitoring system and the management algorithm of RFIDs,IEEE,Conferences,"The RFID-based real-time monitoring system has become one of the most important system in the factory. As it can provide managers an easy way to monitor the product, the overall equipment effectiveness or even the condition of machines. However, many industries just import this system into their factories in recent years and have no idea how to manage this system. Hence, this paper reports on an approach for monitoring the manufacturing process of a factory by using RFID tags. As using RFID tags may meet the problem of reusing the tags, we also develop in this work a novel algorithm for managing RFIDs. Experiment results demonstrate the validity of the proposed approach.",https://ieeexplore.ieee.org/document/8108967/,2017 International Conference on Machine Learning and Cybernetics (ICMLC),9-12 July 2017,ieeexplore
10.1109/IMTC.2001.928200,The development of an artificial neural network embedded automated inspection quality management system,IEEE,Conferences,This paper describes in detail the development of an innovative artificial neural network embedded automated inspection scheme for the manufacturing industry employing digital image processing techniques. Such a system is capable of performing real-time image processing tasks and identifies the size and location of the finished components on manufactured products as well as the flaws and scratches on surface of products during the manufacturing process. The proposed artificial neural network embedded quality management system provides a user-friendly user interface that has been implemented and tested on a case study from a printed circuit board manufacture. The experimental results have demonstrated the functionality and superiority of the developed artificial neural network embedded inspection system.,https://ieeexplore.ieee.org/document/928200/,IMTC 2001. Proceedings of the 18th IEEE Instrumentation and Measurement Technology Conference. Rediscovering Measurement in the Age of Informatics (Cat. No.01CH 37188),21-23 May 2001,ieeexplore
10.1109/IEMC.1998.727776,The importance of artificial intelligence-expert systems in computer integrated manufacturing,IEEE,Conferences,"In order to maintain their competitiveness, companies feel compelled to adopt productivity increasing measures. Yet, they cannot relinquish the flexibility their production cycles need in order to improve their response, and thus, their positioning in the market. To achieve this, companies must combine these two seemingly opposed principles. Thanks to new technological advances, this combination is already a working reality in some companies. It is made possible today by the implementation of computer integrated manufacturing (CIM) and artificial intelligence (AI) techniques, fundamentally by means of expert systems (ES) and robotics. Depending on how these (AI/CIM) techniques contribute to automation, their immediate effects are an increase in productivity and cost reductions. Yet also, the system's flexibility allows for easier adaptation and, as a result, an increased ability to generate value, in other words, competitiveness is improved. The authors have analyzed three studies to identify the possible benefits or advantages, as well as the inconveniences, that this type of technique may bring to companies, specifically in the production field. Although the scope of the studies and their approach differ from one to the other, their joint contribution can be of unquestionable value in order to understand a little better the importance of ES within the production system.",https://ieeexplore.ieee.org/document/727776/,IEMC '98 Proceedings. International Conference on Engineering and Technology Management. Pioneering New Technologies: Management Issues and Challenges in the Third Millennium (Cat. No.98CH36266),11-13 Oct. 1998,ieeexplore
10.1109/AQTR.2018.8402748,Time series forecasting for dynamic scheduling of manufacturing processes,IEEE,Conferences,"Manufacturing control systems evolved in the recent decades from pre-programmed rigid systems to adaptable, data driven, cloud based implementations, capable to respond to environment changes and new requirements in real time. A byproduct of this transformation is represented by large amounts of structured and semi-structured information, both historical and real-time data that is made available on various layers of the system. This accumulation of information brings the opportunity to move from the rule based decision making algorithms used traditionally by these control systems towards more intelligent approaches, driven by modern deep learning mechanisms. This paper proposes a time series forecasting model using recursive neural networks (RNN) for operation scheduling and sequencing in a virtual shop floor environment. The time series aspect of the RNN is novel in manufacturing domain, in the sense that the new best prediction produced considers the previous decisions and outcomes. The proposed implementation explains how the RNN can be mapped to the specifics of a manufacturing control system and introduces a bidding mechanism to allow dynamic evaluation of individual forecasts. The pilot implementation, initial experiments on sample data sets and results presented show how using recursive neural networks can optimize resource utilization and energy consumption.",https://ieeexplore.ieee.org/document/8402748/,"2018 IEEE International Conference on Automation, Quality and Testing, Robotics (AQTR)",24-26 May 2018,ieeexplore
10.1109/ICMLC.2006.258465,Time-Frequency Analysis for Cutting Tools Wear Characteristics,IEEE,Conferences,"To monitor the tool wear states in drilling, a new method is proposed to obtain the signal characteristics based on the wavelet transformation. The method can reflect the tool wear states by using discrete dyadic wavelet transform. The cutting force signals of cutting process are decomposed; and the values of the decomposed signals in different scales are taken as the feature vectors. The pattern identification is used to monitor the tool wear states in real time. The method can identify the tool wear states correctly by choosing the suitable standard samples. The result shows that the proposed method is suitable for real-time implementation in manufacturing application, and has good identification precision and high efficiency",https://ieeexplore.ieee.org/document/4028638/,2006 International Conference on Machine Learning and Cybernetics,13-16 Aug. 2006,ieeexplore
10.1109/ICPHM51084.2021.9486566,Tool Wear Prediction Under Varying Milling Conditions via Temporal Convolutional Network and Auxiliary Learning,IEEE,Conferences,"Tool wear is an inevitable and critical problem in manufacturing systems. Accurate prediction of tool wear under varying operation conditions is a hot and challenging problem. In this paper, temporal convolutional network (TCN) is employed to predict tool wear using raw sensor signals. Dilated causal convolution and residual connections are used in TCN to form a more flexible structure with stable training gradients for sequential modeling. In addition, auxiliary learning is used in this paper to enhance model performance under varying operation conditions. An auxiliary task of recognizing operating conditions is designed to support the tool wear prediction task. Experiment on a real milling wear dataset illustrates the superiority of TCN for tool wear prediction compared with traditional convolutional neural network (CNN) and long short-term memory (LSTM) network. Experimental results also demonstrate the effectiveness of the proposed auxiliary learning.",https://ieeexplore.ieee.org/document/9486566/,2021 IEEE International Conference on Prognostics and Health Management (ICPHM),7-9 June 2021,ieeexplore
10.1109/BCI51272.2021.9385331,Towards Neurohaptics: Brain-Computer Interfaces for Decoding Intuitive Sense of Touch,IEEE,Conferences,"Noninvasive brain-computer interface (BCI) is widely used to recognize users' intentions. Especially, BCI related to tactile and sensation decoding could provide various effects on many industrial fields such as manufacturing advanced touch displays, controlling robotic devices, and more immersive virtual reality or augmented reality. In this paper, we introduce haptic and sensory perception-based BCI systems called neurohaptics. It is a preliminary study for a variety of scenarios using actual touch and touch imagery paradigms. We designed a novel experimental environment and a device that could acquire brain signals under touching designated materials to generate natural touch and texture sensations. Through the experiment, we collected the electroencephalogram (EEG) signals with respect to four different texture objects. Seven subjects were recruited for the experiment and evaluated classification performances using machine learning and deep learning approaches. Hence, we could confirm the feasibility of decoding actual touch and touch imagery on EEG signals to develop practical neurohaptics.",https://ieeexplore.ieee.org/document/9385331/,2021 9th International Winter Conference on Brain-Computer Interface (BCI),22-24 Feb. 2021,ieeexplore
10.1109/ASMC49169.2020.9185292,Trace Data Analytics with Knowledge Distillation : DM: Big Data Management and Mining,IEEE,Conferences,"In this paper, we propose the “trace data analytics” for classifying fault conditions from multivariate time series sensor signals using well-known deep CNN models. In our approach, multiple sensor signals are converted into two dimensional representations using the proposed conversion methods to optimize the classification performance. Many studies on the prediction of manufacturing results using sensor signals have been conducted in the field of fault detection and classification for display and semiconductor manufacturing processes. It is challenging to apply machine learning to real-life manufacturing problems due to practical limitations, class imbalance and data insufficiency, which also make it difficult to produce a generalized model. To overcome these challenges, we propose using omni-supervised learning but with a new approach to knowledge distillation that ensembles predictions from multiple instantiations of a CNN model of synthetically generated data samples from a deep generative model. Our experiment results show that the fault classification accuracy improves substantially by applying trace data analytics to manufacturing data from display fabrication lines. The results also show that the quality of trained CNN models using the proposed knowledge distillation is maintained steadily and stably.",https://ieeexplore.ieee.org/document/9185292/,2020 31st Annual SEMI Advanced Semiconductor Manufacturing Conference (ASMC),24-26 Aug. 2020,ieeexplore
10.1109/DEST.2009.5276766,Transforming SME manufacturing plants into evolvable systems through agents,IEEE,Conferences,"Manufacturing plants need to be flexible and evolvable, which might be achieved by stepping up the level of abstraction at which they are designed. In this work a methodology that starts by creating ontologies of the plants that represent information flows is proposed. Agent-based technology is then used to provide systems with the ability to evolve. The proposal is designed to suit the needs of SME' plants, whose operation cannot be disturbed while progressively undergo a transformation into more advanced ones. The methodology has been based on a partial implementation on a real case that, in its turn, has served to validate the whole procedure.",https://ieeexplore.ieee.org/document/5276766/,2009 3rd IEEE International Conference on Digital Ecosystems and Technologies,1-3 June 2009,ieeexplore
10.1109/VLSID.2018.20,Tutorial T2A: Safe Autonomous Systems: Real-Time Error Detection and Correction in Safety-Critical Signal Processing and Control Algorithms,IEEE,Conferences,"While the last two decades have seen revolutions in computing and communications systems, the next few decades will see a revolution in the use of every-day robotics and artificial intelligence in broad societal applications. Examples of such systems include sensor networks, the smart power grid, self-driven cars and autonomous drones. Such systems are driven by signal processing, control and learning algorithms that process sensor data, actuate control functions and learn about the environment in which these systems operate. The trustworthiness and safety of such systems is of paramount importance and has significant impact on the commercial viability of the underlying technology. As a consequence, anomalies in system operation due to computation errors in on-board processors, degradation and failure of embedded sensors, actuators and electro-mechanical subsystems and unforeseen changes in their operation environment need to detected with minimum latency. Such anomalies also need to be mitigated in ways that ensure the safety of such systems under all possible failure scenarios. Many future systems will be selflearning in the field. It is necessary to ensure that such learning does not compromise the safety of all human personnel involved in the operation of such systems. To enable safe operation of such systems, the underlying hardware needs to be tuned in the field to maximize performance, reliability and error-resilience while minimizing power consumption. To enable such dynamic adaptation, device operating conditions and the onset of soft errors are sensed using post-manufacture and real-time checking mechanisms. These mechanisms rely on the use of built-in sensors and/or low-overhead function encoding techniques to detect anomalies in system functions. A key capability is that of being able to deduce multiple performance parameters of the system-under-test using compact optimized stimulus using learning algorithms. The sensors and function encodings assess the loss in performance of the relevant systems due to workload uncertainties, manufacturing process imperfections, soft errors and hardware malfunction and failures induced by electromechanical degradation. These are then mitigated through the use of algorithm-through-circuit level compensation techniques based on pre-deployment simulation and post-deployment self-learning. These techniques continuously trade off performance vs. power of the individual software and hardware modules in such a way as to deliver the end-to-end desired application level Quality of Service (QoS), while minimizing energy/power consumption and maximizing reliability and safety. Applications to signal processing, and control algorithms for example autonomous systems will be discussed.",https://ieeexplore.ieee.org/document/8326883/,2018 31st International Conference on VLSI Design and 2018 17th International Conference on Embedded Systems (VLSID),6-10 Jan. 2018,ieeexplore
10.1109/IAI50351.2020.9262158,Virtual Commissioning and Machine Learning of a Reconfigurable Assembly System,IEEE,Conferences,"The digital twin application in manufacturing is mainly based on the virtual simulation model of a digital twin to build a solid model, which is applied to the product processing and assembly to achieve precise production control. This paper presents a virtual commissioning digital twin model for the modularized automatic assembly system running in our lab. First, the Siemens NX MCD software tool is used to develop the virtual commissioning digital twin model for the system. Then the different working scenarios are simulated and implemented in the virtual physical simulation environment. The data from the proposed virtual commissioning digital twin model is collected and trained with 6 different machine learning algorithm such as Logistic Regression (LR), Linear Discriminant Analysis (LDA), K-Nearest Neighbors (KNN), Classification and Regression Trees (CART), Gaussian Naive Bayes (NB) and Support Vector Machines (SVM). The advantage of our newly developed virtual commissioning model is that it is able to simulate different working conditions without risk and cost-free. It is also convenient to mimic the worsening working status and failed operation scenarios which need long time to collect for the real system. We use the collected data as input for the machine learning to implement the system monitoring and predicting. The machine learning results for 6 learning algorithms are presented and it shows the possibilities and advantages of our proposed virtual commissioning digital twin model.",https://ieeexplore.ieee.org/document/9262158/,2020 2nd International Conference on Industrial Artificial Intelligence (IAI),23-25 Oct. 2020,ieeexplore
10.1109/ISWCS.2019.8877305,Visible Light Positioning for Location-Based Services in Industry 4.0,IEEE,Conferences,"Industry 4.0 refers to the evolution in manufacturing from computerization to fully cyberphysical systems that exploit rich sensor data, adaptive real-time safety-critical control, and machine learning. An important aspect of this vision is the sensing and subsequent association of objects in the physical world with their cyber and virtual counterparts. In this paper we propose Visible Light Positioning (VLP) as an enabler for these Industry 4.0 applications. We also explore sensing techniques, including cameras (and depth sensors), and other light-based solutions for object positioning and detection along with their respective limitations. We then demonstrate an application of positioning for real time robot control in an interactive multiparty cyber-physical-virtual deployment. Lastly, based on our experience with this cyberphysical-virtual application, we propose Ray-Surface Positioning (RSP), a novel VLP technique, as a low cost positioning system for Industry 4.0.",https://ieeexplore.ieee.org/document/8877305/,2019 16th International Symposium on Wireless Communication Systems (ISWCS),27-30 Aug. 2019,ieeexplore
10.1109/ICRA.2019.8794123,Visual Guidance and Automatic Control for Robotic Personalized Stent Graft Manufacturing,IEEE,Conferences,"Personalized stent graft is designed to treat Abdominal Aortic Aneurysms (AAA). Due to the individual difference in arterial structures, stent graft has to be custom made for each AAA patient. Robotic platforms for autonomous personalized stent graft manufacturing have been proposed in recently which rely upon stereo vision systems for coordinating multiple robots for fabricating customized stent grafts. This paper proposes a novel hybrid vision system for real-time visual-sevoing for personalized stent-graft manufacturing. To coordinate the robotic arms, this system is based on projecting a dynamic stereo microscope coordinate system onto a static wide angle view stereo webcam coordinate system. The multiple stereo camera configuration enables accurate localization of the needle in 3D during the sewing process. The scale-invariant feature transform (SIFT) method and color filtering are implemented for stereo matching and feature identifications for object localization. To maintain the clear view of the sewing process, a visual-servoing system is developed for guiding the stereo microscopes for tracking the needle movements. The deep deterministic policy gradient (DDPG) reinforcement learning algorithm is developed for real-time intelligent robotic control. Experimental results have shown that the robotic arm can learn to reach the desired targets autonomously.",https://ieeexplore.ieee.org/document/8794123/,2019 International Conference on Robotics and Automation (ICRA),20-24 May 2019,ieeexplore
10.1109/ECC.2015.7330620,Web tension regulation with partially known periodic disturbances in roll-to-roll manufacturing systems,IEEE,Conferences,"This paper deals with the problem of synthesizing feedforward control to aid the regulation of web tension in the presence of partially known exogenous inputs. The problem appears in many engineering applications including Roll-to-Roll (R2R) manufacturing systems where the governing equation for tension is nonlinear. Currently known methods for the nonlinear output regulation problem either require the solution of a constrained partial differential equation or the preview information of the signal to be tracked. In this paper, we consider the problem of regulating web tension while rejecting periodic disturbances and use a novel approach to synthesize feedforward control as the solution of a system of differential-algebraic equations, which is considerably less complex and suitable for practical implementation. We assume that the disturbance signal is the output of a known exogenous system with unknown initial conditions. A parameter identification scheme to estimate the unknown initial conditions is developed. The proposed technique is successfully applied to web tension regulation in a large R2R machine which contains real-time hardware and software that is used in industrial practice. Extensive experiments were conducted to evaluate the proposed scheme under various experimental conditions, including different web speeds and materials. We will discuss a representative sample of the results with the proposed nonlinear tension regulator and provide a comparison with a well-tuned industrial PI control scheme to highlight the benefits of using the proposed scheme.",https://ieeexplore.ieee.org/document/7330620/,2015 European Control Conference (ECC),15-17 July 2015,ieeexplore
10.1109/BDCloud.2018.00136,"iSTEP, an Integrated Self-Tuning Engine for Predictive Maintenance in Industry 4.0",IEEE,Conferences,"The recent expansion of IoT-enabled (Internet of Things) devices in manufacturing contexts and their subsequent data-driven exploitation paved the way to the advent of the Industry 4.0, promoting a full integration of IT services, smart devices, and control systems with physical objects, their electronics and sensors. The real-time transmission and analysis of collected data from factories has the potential to create manufacturing intelligence, of which predictive maintenance is an expression. Hence the need to design new approaches able to manage not only the data volume, but also the variety and velocity, extracting actual value from the humongous amounts of collected data. To this aim, we present iSTEP, an integrated Self-Tuning Engine for Predictive maintenance, based on Big Data technologies and designed for Industry 4.0 applications. The proposed approach targets some of the most common needs of manufacturing enterprises: compatibility with both the on-premises and the in-the-cloud environments, exploitation of reliable and largely supported Big Data platforms, easy deployment through containerized software modules, virtually unlimited horizontal scalability, fault-tolerant self-reconfiguration, flexible yet friendly streaming-KPI computations, and above all, the integrated provisioning of self-tuning machine learning techniques for predictive maintenance. The current implementation of iSTEP exploits a distributed architecture based on Apache Kafka, Spark Streaming, MLlib, and Cassandra; iSTEP provides (i) a specific feature engineering block aimed at automatically extracting metrics from the production monitoring time series, which improves the predictive performance by 77% on average, and (ii) a self-tuning approach that dynamically selects the best prediction algorithm, which improves the predictive performance up to 60%. The iSTEP engine provides transparent predictive models, able to provide end users with insights into the knowledge learned, and it has been experimentally evaluated on a public unbalanced failure dataset, whose extensive results are discussed in the paper.",https://ieeexplore.ieee.org/document/8672266/,"2018 IEEE Intl Conf on Parallel & Distributed Processing with Applications, Ubiquitous Computing & Communications, Big Data & Cloud Computing, Social Computing & Networking, Sustainable Computing & Communications (ISPA/IUCC/BDCloud/SocialCom/SustainCom)",11-13 Dec. 2018,ieeexplore
10.1109/TASE.2019.2947398,A Genetic Programming-Based Scheduling Approach for Hybrid Flow Shop With a Batch Processor and Waiting Time Constraint,IEEE,Journals,"This article investigates a hybrid flow shop scheduling problem that consists of a batch processor in the upstream and a discrete processor in the downstream. Limited waiting time between the batch processor and discrete processor is taken into consideration. Such a scheduling problem is commonly seen as bottlenecks in the production of precision parts, back-end process of semiconductor products, and glass and steel industries. A mixed-integer linear programming model is presented to minimize the makespan. Considering the complexity of this problem and the imperative requirement in real-time optimization, we first develop a constructive heuristic together with the worst case analysis by exploiting the key decision structure of the problem. Based on the decision structure, we then develop a learning-based scheduling approach via customized genetic programming to automatically generate effective heuristics for this problem. Lower bounds are also developed to provide a measurement for the performance of proposed algorithms. Numerical results show that our proposed algorithms outperform the existing metaheuristics and are capable of providing high-quality solutions using less computational time. Note to Practitioners-The production system consisting of a batch processor in the upstream and a discrete processor in the downstream is common in practice. The batch processor first handles a group of jobs simultaneously. Then, the jobs are released to a buffer to wait for the process on the discrete processor one by one. However, the waiting time of the jobs in the buffer is often required to be limited according to the production requirements. For example, after being heated in the heat-treatment oven, the aerospace precision parts have to be processed on the machining equipment in limited waiting time to improve the processability in subsequent manufacturing. The semiconductor chips have to be packed in limited waiting time after baking to avoid getting wet. The incongruous production modes between the batch processor and discrete processor, together with the limited waiting time constraint, make such operations always the bottleneck in manufacturing. Efficient heuristics, providing high-quality solutions with low time complexity, are much preferred in practice for most of the complicated scheduling problems, such as the scenarios described earlier. However, the designing process of an effective heuristic is tedious, and the heuristic is usually deeply customized for a certain production scenario. Genetic programming (GP) provides an inspiring approach to automatically generate sophisticated heuristics for complicated scheduling problems through evolutionary learning processes. By customizing a GP-based approach, the designing process of heuristics is automated, and some undetectable knowledge relations can be obtained to enhance the quality of heuristics. Such an approach facilitates to obtain more sophisticated schedules by analyzing valuable knowledge for smart manufacturing. The superiority of the heuristic learned by GP is shown in the computational experiment, and it has great potential to be applied to the practical scheduling.",https://ieeexplore.ieee.org/document/8896881/,IEEE Transactions on Automation Science and Engineering,Jan. 2021,ieeexplore
10.1109/TII.2019.2915846,A Global Manufacturing Big Data Ecosystem for Fault Detection in Predictive Maintenance,IEEE,Journals,"Artificial intelligence, big data, machine learning, cloud computing, and Internet of Things (IoT) are terms which have driven the fourth industrial revolution. The digital revolution has transformed the manufacturing industry into smart manufacturing through the development of intelligent systems. In this paper, a big data ecosystem is presented for the implementation of fault detection and diagnosis in predictive maintenance with real industrial big data gathered directly from large-scale global manufacturing plants, aiming to provide a complete architecture which could be used in industrial IoT-based smart manufacturing in an industrial 4.0 system. The proposed architecture overcomes multiple challenges including big data ingestion, integration, transformation, storage, analytics, and visualization in a real-time environment using various technologies such as the data lake, NoSQL database, Apache Spark, Apache Drill, Apache Hive, OPC Collector, and other techniques. Transformation protocols, authentication, and data encryption methods are also utilized to address data and network security issues. A MapReduce-based distributed PCA model is designed for fault detection and diagnosis. In a large-scale manufacturing system, not all kinds of failure data are accessible, and the absence of labels precludes all the supervised methods in the predictive phase. Furthermore, the proposed framework takes advantage of some of the characteristics of PCA such as its ease of implementation on Spark, its simple algorithmic structure, and its real-time processing ability. All these elements are essential for smart manufacturing in the evolution to Industry 4.0. The proposed detection system has been implemented into the real-time industrial production system in a cooperated company, running for several years, and the results successfully provide an alarm warning several days before the fault happens. A test case involving several outages in 2014 is reported and analyzed in detail during the experiment section.",https://ieeexplore.ieee.org/document/8710319/,IEEE Transactions on Industrial Informatics,Jan. 2020,ieeexplore
10.1109/ACCESS.2019.2963723,A Smart Collaborative Routing Protocol for Delay Sensitive Applications in Industrial IoT,IEEE,Journals,"In the industrial Internet of things (IIoT), there is always a strong demand for real-time information transfer. Especially when deploying wireless/wired hybrid networks in smart factories, the requirement for low delay interaction is more prominent. Although tree routing protocols have been successfully executed in simple networks, more challenges in transmission speed can be observed in the manufacturing broadband communication system. Motivated by the progresses in deep learning, a smart collaborative routing protocol with low delay and high reliability is proposed to accommodate mixed link scenarios. First, we establish a one-hop delay model to investigate the potential affects of Media Access Control (MAC) layer parameters, which supports the subsequent design. Second, forwarding, maintenance, and efficiency strategies are created to construct the basic functionalities for our routing protocol. Relevant procedures and key approaches are highlighted as well. Third, two sub-protocols are generated and the corresponding implementation steps are described. The experimental results demonstrate that the end-to-end delay can be effectively cut down through comprehensive improvements. Even more sensor nodes and larger network scale are involved, our proposed protocol can still illustrate the advantages comparing with existing solutions within IIoT.",https://ieeexplore.ieee.org/document/8949516/,IEEE Access,2020,ieeexplore
10.1109/ACCESS.2021.3121302,A Survey on Industry 4.0 for the Oil and Gas Industry: Upstream Sector,IEEE,Journals,"The market volatility in the oil and gas (O&amp;G) sector, the dwindling demand for oil due to the impact of COVID-19, and the push for alternative greener energy are driving the need for innovation and digitization in the O&amp;G industry. This has attracted research interest from academia and the industry in the application of industry 4.0 (I4.0) technologies in the O&amp;G sector. The application of some of these I4.0 technologies has been presented in the literature, but the domain still lacks a comprehensive survey of the application of I4.0 in the O&amp;G upstream sector. This paper investigates the state-of-the-art efforts directed toward I4.0 technologies in the O&amp;G upstream sector. To achieve this, first, an overview of the I4.0 is discussed followed by a systematic literature review from an integrative perspective for publications between 2012–2021 with 223 analyzed documents. The benefits and challenges of the adoption of I4.0 have been identified. Moreover, the paper adds value by proposing a framework for the implementation of I4.0 in the O&amp;G upstream sector. Finally, future directions and research opportunities such as framework, edge computing, quantum computing, communication technologies, standardization, and innovative areas related to the implementation of I4.0 in the upstream sector are presented. The findings from this review show that I4.0 technologies are currently being explored and deployed for various aspects of the upstream sector. However, some of the I4.0 technologies like additive manufacturing and virtual reality are least explored.",https://ieeexplore.ieee.org/document/9579415/,IEEE Access,2021,ieeexplore
10.1109/JIOT.2019.2940131,A Two-Stage Transfer Learning-Based Deep Learning Approach for Production Progress Prediction in IoT-Enabled Manufacturing,IEEE,Journals,"In make-to-order manufacturing enterprises, accurate production progress (PP) prediction is an important basis for dynamic production process optimization and on-time delivery of orders. The implementation of Internet of Things (IoT) makes it possible to take real-time production state as an important factor affecting PP. In the IoT-enabled workshop, a two-stage transfer learning-based prediction method using both historical production data and real-time state data is proposed to solve the problem of low-prediction accuracy and poor generalization performance caused by insufficient data of target order. The deep autoencoder (DAE) model with transfer learning is designed to extract the generalized features of target order in the first stage, which uses bootstrap sampling to avoid over fitting. The deep belief network (DBN) model with transfer learning is constructed to fit the nonlinear relation for PP prediction in the second stage. A real case from an IoT enabled machining workshop is taken to validate the performance of the proposed method over the other methods such as DBN, deep neural network.",https://ieeexplore.ieee.org/document/8827506/,IEEE Internet of Things Journal,Dec. 2019,ieeexplore
10.1109/TRO.2004.833801,A hybrid strategy to solve the forward kinematics problem in parallel manipulators,IEEE,Journals,"A parallel manipulator is a closed kinematic structure with the necessary rigidity to provide a high payload to self-weight ratio suitable for many applications in manufacturing, flight simulation systems, and medical robotics. Because of its closed structure, the kinematic control of such a mechanism is difficult. The inverse kinematics problem for such manipulators has a mathematical solution; however, the forward kinematics problem (FKP) is mathematically intractable. This work addresses the FKP and proposes a neural-network-based hybrid strategy that solves the problem to a desired level of accuracy, and can achieve the solution in real time. Two neural-network (NN) concepts using a modified form of multilayered perceptrons with backpropagation learning were implemented. The better performing concept was then combined with a standard Newton-Raphson numerical technique to yield a hybrid solution strategy. Simulation studies were carried out on a flight simulation syystem to check the validity o the approach. Accuracy of close to 0.01 mm and 0.01/spl deg/ in the position and orientation parameters was achieved in less than two iterations and 0.02 s of execution time for the proposed strategy.",https://ieeexplore.ieee.org/document/1391011/,IEEE Transactions on Robotics,Feb. 2005,ieeexplore
10.1109/ACCESS.2020.2973336,An Effective Discrete Artificial Bee Colony Algorithm for Scheduling an Automatic-Guided-Vehicle in a Linear Manufacturing Workshop,IEEE,Journals,"This paper deals with a new automatic guided vehicle (AGV) scheduling problem from the material handling process in a linear manufacturing workshop. The problem is to determine a sequence of Cells for AGV to travel to minimize the standard deviation of the waiting time of the Cells and the total travel distance of AGV. For this purpose, we first propose an integer linear programming model based on a comprehensive investigation. Then, we present an improved nearest-neighbor-based heuristic so as to fast generate a good solution in view of the problem-specific characteristics. Next, we propose an effective discrete artificial bee colony algorithm with some novel and advanced techniques including a heuristic-based initialization, six neighborhood structures and a new evolution strategy in the onlooker bee phase. Finally, the proposed algorithms are empirically evaluated based on several typical instances from the real-world linear manufacturing workshop. A comprehensive and thorough experiment shows that the presented algorithm produces superior results which are also demonstrated to be statistically significant than the existing algorithms.",https://ieeexplore.ieee.org/document/8995549/,IEEE Access,2020,ieeexplore
10.1109/TII.2019.2959021,An Integrated Histogram-Based Vision and Machine-Learning Classification Model for Industrial Emulsion Processing,IEEE,Journals,"Existing techniques in emulsion quality evaluation are found to be highly subjective, time-consuming, and prone to overprocessing. Other conventional droplet analysis techniques such as laser diffraction, which require dilution of samples, introduce an additional complexity to industrial processes. The possibility of developing a fully automated technique for droplet characterization during emulsification holds remarkable potential for overcoming the existing challenges. In this article, a histogram-based image segmentation technique detects droplets from emulsion micrographs. The evolution of droplet characteristics and their significance are studied by performing statistical analysis, and the significant characteristics are selected. The principal component analysis is applied to obtain a reduced set of uncorrelated components from the selected characteristics. The linear discriminant analysis classifies the micrographs into a set of quality categories called target, acceptable, marginal, and unacceptable. The model accuracy is validated using stratified five-fold cross-validation and is successful in classifying the micrographs obtained from two different manufacturing facilities with high accuracy up to 100%. The histogram-based technique is successful in detecting smaller droplets than previously reflected in the literature. The current approach is fully automated and is implemented as a soft-sensor, which supports its real-time deployment into an industrial environment. The entire approach has promising potential in the in-line prediction of emulsion quality leading to more efficient and sustainable manufacturing.",https://ieeexplore.ieee.org/document/8968624/,IEEE Transactions on Industrial Informatics,Sept. 2020,ieeexplore
10.1109/TASE.2006.886833,An Intelligent Online Monitoring and Diagnostic System for Manufacturing Automation,IEEE,Journals,"Condition monitoring and fault diagnosis in modern manufacturing automation is of great practical significance. It improves quality and productivity, and prevents damage to machinery. In general, this practice consists of two parts: 1)extracting appropriate features from sensor signals and 2)recognizing possible faulty patterns from the features. Through introducing the concept of marginal energy in signal processing, a new feature representation is developed in this paper. In order to cope with the complex manufacturing operations, three approaches are proposed to develop a feasible system for online applications. This paper develops intelligent learning algorithms using hidden Markov models and the newly developed support vector techniques to model manufacturing operations. The algorithms have been coded in modular architecture and hierarchical architecture for the recognition of multiple faulty conditions. We define a novel similarity measure criterion for the comparison of signal patterns which will be incorporated into a novel condition monitoring system. The sensor-based intelligent system has been implemented in stamping operations as an example. We demonstrate that the proposed method is substantially more effective than the previous approaches. Its unique features benefit various real-world manufacturing automation engineering, and it has great potential for shop floor applications.",https://ieeexplore.ieee.org/document/4358068/,IEEE Transactions on Automation Science and Engineering,Jan. 2008,ieeexplore
10.1109/TIM.2021.3087826,Auto-Annotated Deep Segmentation for Surface Defect Detection,IEEE,Journals,"This article presents a deep learning scheme for automatic defect detection in material surfaces. The success of deep learning model training is generally determined by the number of representative training samples and the quality of the annotation. It is extremely tedious and tiresome to annotate defects pixel-by-pixel in an image to train a semantic network model for defect segmentation. In this study, we propose a two-stage deep learning scheme to tackle the pixel-wise defect detection in textured surfaces without manual annotation. The first stage of the deep learning scheme uses two cycle-consistent adversarial network (CycleGAN) models to automatically synthesize and annotate defect pixels in an image. The synthesized defect images and their corresponding annotated results from the CycleGAN models are then used as the input-output pairs for training the U-Net semantic network. The proposed scheme requires only a few real defect samples for the training and completely requires no manual annotation work. It is practical and computationally very efficient for the implementation in manufacturing. Experimental results show that the proposed deep learning scheme can be applied for defect detection in a variety of textured and patterned surfaces, and results in high detection accuracy.",https://ieeexplore.ieee.org/document/9449912/,IEEE Transactions on Instrumentation and Measurement,2021,ieeexplore
10.1109/TII.2018.2816971,Automatic Selection of Optimal Parameters Based on Simple Soft-Computing Methods: A Case Study of Micromilling Processes,IEEE,Journals,"Nowadays, the application of novel soft-computing methods to new industrial processes is often limited by the actual capacity of the industry to assimilate state-of-the-art computational methods. The selection of optimal parameters for efficient operation is very challenging in microscale manufacturing processes, because of intrinsic nonlinear behavior and reduced dimensions. In this paper, a decision-making system for selecting optimal parameters in micromilling operations is designed and implemented using simple and efficient soft-computing techniques. The procedure primarily consists of four steps: an experimental characterization; the modeling of cutting force and surface roughness by means of a multilayer perceptron; multiobjective optimization using the cross-entropy method, taking into account productivity and surface quality; and a decision-making procedure for selecting the most appropriate parameters using a fuzzy inference system. Finally, two different alloys for micromilling processes are considered, in order to evaluate the proposed system: a titanium-based alloy and a tungsten-copper alloy. The experimental study demonstrated the effectiveness of the proposed solution for automated decision-making, based on simple soft-computing methods, and its successful application to a real-life industrial challenge.",https://ieeexplore.ieee.org/document/8325494/,IEEE Transactions on Industrial Informatics,Feb. 2019,ieeexplore
10.1109/LRA.2017.2737046,Baxter's Homunculus: Virtual Reality Spaces for Teleoperation in Manufacturing,IEEE,Journals,"We demonstrate a low-cost telerobotic system that leverages commercial virtual reality (VR) technology and integrates it with existing robotics control infrastructure. The system runs on a commercial gaming engine using off-the-shelf VR hardware and can be deployed on multiple network architectures. The system is based on the homunculus model of mind wherein we embed the user in a VR control room. The control room allows for multiple sensor displays, and dynamic mapping between the user and robot. This dynamic mapping allows for selective engagement between the user and the robot. We compared our system with state-of-the-art automation algorithms and standard VR-based telepresence systems by performing a user study. The study showed that new users were faster and more accurate than the automation or a direct telepresence system. We also demonstrate that our system can be used for pick and place, assembly, and manufacturing tasks.",https://ieeexplore.ieee.org/document/8003431/,IEEE Robotics and Automation Letters,Jan. 2018,ieeexplore
10.1109/ACCESS.2021.3106797,CNC Machine Tool Fault Diagnosis Integrated Rescheduling Approach Supported by Digital Twin-Driven Interaction and Cooperation Framework,IEEE,Journals,"The problems of CNC machine tool (CNCMT) fault diagnosis and production rescheduling have attracted continuous attention because of their great significance to the manufacturing industry. Digital twin is a supporting technology for achieving smart manufacturing and provides a new paradigm for solving these problems. This paper explores a digital twin-driven interaction and cooperation framework and proposes the architecture and implementation mechanism to enable the sharing of data, knowledge, and resource, to realize the fusion of physical space and cyber space, and to improve the accuracy of fault diagnosis. Under this framework, aiming at the influence of CNCMT failure on the initial production planning, a self-adaptation rescheduling method based on Monte Carlo Tree Search (MCTS) algorithm is proposed to provide support for developing more efficient production planning. Finally, the effectiveness of the proposed framework is validated by experimental study. The framework and integrated rescheduling approach can provide guidance for enterprises in implementing CNCMT maintenance and production scheduling to meet high accuracy and reliability requirements.",https://ieeexplore.ieee.org/document/9520390/,IEEE Access,2021,ieeexplore
10.1109/TASE.2020.3010536,Condition-Driven Data Analytics and Monitoring for Wide-Range Nonstationary and Transient Continuous Processes,IEEE,Journals,"Frequent and wide changes in operation conditions are quite common in real process industry, resulting in typical wide-range nonstationary and transient characteristics along time direction. The considerable challenge is, thus, how to solve the conflict between the learning model accuracy and change complexity for analysis and monitoring of nonstationary and transient continuous processes. In this work, a novel condition-driven data analytics method is developed to handle this problem. A condition-driven data reorganization strategy is designed which can neatly restore the time-wise nonstationary and transient process into different condition slices, revealing similar process characteristics within the same condition slice. Process analytics can then be conducted for the new analysis unit. On the one hand, coarse-grained automatic condition-mode division is implemented with slow feature analysis to track the changing operation characteristics along condition dimension. On the other hand, fine-grained distribution evaluation is performed for each condition mode with Gaussian mixture model. Bayesian inference-based distance (BID) monitoring indices are defined which can clearly indicate the fault effects and distinguish different operation scenarios with meaningful physical interpretation. A case study on a real industrial process shows the feasibility of the proposed method which, thus, can be generalized to other continuous processes with typical wide-range nonstationary and transient characteristics along time direction. <italic>Note to Practitioners</italic>—Industrial processes in general have nonstationary characteristics which are ubiquitous in real world data, often reflected by a time-variant mean, a time-variant autocovariance, or both resulting from various factors. The focus of this study is to develop a universal analytics and monitoring method for wide-range nonstationary and transient continuous processes. Condition-driven concept takes the place of time-driven thought. For the first time, it is recognized that there are similar process characteristics within the same condition slice and changes in the process correlations may relate to its condition modes. Besides, the proposed method can provide enhanced physical interpretation for the monitoring results with concurrent analysis of the static and dynamic information which carry different information, analogous to the concepts of “position” and “velocity” in physics, respectively. The static information can tell the current operation condition, while the dynamic information can clarify whether the process status is switching between different steady states. It is noted that the condition-driven concept is universal and can be extended to other applications for industrial manufacturing applications.",https://ieeexplore.ieee.org/document/9158352/,IEEE Transactions on Automation Science and Engineering,Oct. 2021,ieeexplore
10.1109/TII.2021.3067915,Diagnosis of Interturn Short-Circuit Faults in Permanent Magnet Synchronous Motors Based on Few-Shot Learning Under a Federated Learning Framework,IEEE,Journals,"A large amount of labeled data are important to enhance the performance of deep-learning-based methods in the area of fault diagnosis. Because it is difficult to obtain high-quality samples in real industrial applications, federated learning is an effective framework for solving the problem of sparse samples by using the distributed data. Its global model is updated by the local client without sharing data at each round. Considering computing resources and communication loss of multiple clients, an efficient method based on stacked sparse autoencoders (SSAEs) and Siamese networks is proposed to detect interturn short-circuit (ITSC) faults in permanent magnet synchronous motors. In this article, to achieve an accurate ITSC fault detection, an SSAE was employed to extract sparse features in a limited number of samples, and Siamese networks were used to determine the similarity between the given samples. The problem of fault diagnosis is transformed into a classification problem under few-shot learning. Furthermore, the proposed method is trained successfully in the frameworks of centralized learning and decentralized structure. The experimental results indicate that the proposed method achieved high fault diagnosis accuracy. Moreover, it is suitable for deployment in smart manufacturing systems.",https://ieeexplore.ieee.org/document/9384245/,IEEE Transactions on Industrial Informatics,Dec. 2021,ieeexplore
10.1109/ACCESS.2021.3079447,ECT-LSTM-RNN: An Electrical Capacitance Tomography Model-Based Long Short-Term Memory Recurrent Neural Networks for Conductive Materials,IEEE,Journals,"Image reconstruction for industrial applications based on Electrical Capacitance Tomography (ECT) has been broadly applied. The goal of image reconstruction based ECT is to locate the distribution of permittivity for the dielectric substances along the cross-section based on the collected capacitance data. In the ECT-based image reconstruction process: (1) the relationship between capacitance measurements and permittivity distribution is nonlinear, (2) the capacitance measurements collected during image reconstruction are inadequate due to the limited number of electrodes, and (3) the reconstruction process is subject to noise leading to an ill-posed problem. Thence, constructing an accurate algorithm for real images is critical to overcoming such restrictions. This paper presents novel image reconstruction methods using Deep Learning for solving the forward and inverse problems of the ECT system for generating high-quality images of conductive materials in the Lost Foam Casting (LFC) process. Here, Long Short-Term Memory Recurrent Neural Network (LSTM-RNN) models were implemented to predict the distribution of metal filling for the LFC process-based ECT. The recurrent connection and the gating mechanism of the LSTM is capable of extracting the contextual information that is repeatedly passing through the neural network while filtering out the noise caused by adverse factors. Experimental results showed that the presented ECT-LSTM-RNN model is highly reliable for industrial applications and can be utilized for other manufacturing processes.",https://ieeexplore.ieee.org/document/9429218/,IEEE Access,2021,ieeexplore
10.1109/TII.2019.2949347,Edge Coordinated Query Configuration for Low-Latency and Accurate Video Analytics,IEEE,Journals,"To develop smart city and intelligent manufacturing, video cameras are being increasingly deployed. In order to achieve fast and accurate response to live video queries (e.g., license plate recording and object tracking), the real-time high-volume video streams should be delivered and analyzed efficiently. In this article, we introduce an end-edge-cloud coordination framework for low-latency and accurate live video analytics. Considering the locality of video queries, edge platform is designated as the system coordinator. It accepts live video queries and configures the related end cameras to generate video frames that meet quality requirements. By taking into account the latency constraint, edge computing resources are subtly distributed to process the live video frames from different sources such that the analytic accuracy of the accepted video queries can be maximized. Since the amount of required edge computing resource and video quality to accurately address different video queries are unknown in advance, we propose an online video quality and computing resource configuration algorithm to gradually learn the optimal configuration strategy. Extensive simulation results show that as compared to other benchmarks, the proposed configuration algorithm can effectively improve the analytic accuracy, while providing low-latency response.",https://ieeexplore.ieee.org/document/8882341/,IEEE Transactions on Industrial Informatics,July 2020,ieeexplore
10.1109/70.63270,Hybrid hierarchical scheduling and control systems in manufacturing,IEEE,Journals,"Some experiments on the integration of algorithmic techniques with knowledge-based ones are discussed. Two case studies are presented: an FMS cell and a press shop. It was found that the algorithmic procedures developed for production scheduling resulted in limiting the ability to cope with the complexity of the real manufacturing world. The scheduling problem, seen as a constraint satisfaction problem, can be approached with rule-based techniques. Nevertheless, algorithmic techniques are found to be valuable for their efficiency and ability to deal with aggregated data. This ability is fundamental for an efficient implementation of hierarchical control systems in general and in the manufacturing context in particular. This suggests that the integration of rule-based techniques with algorithmic ones can increase the efficiency of searching in the space of possible solutions. The ability to deal with aggregated data can have little value when detailed real-time operation scheduling is needed. In this case, simple dispatching rules are often used, and sophisticated operations research methods are not used. In such a dynamic situation, a purely-rule based approach may be more suitable.&lt;<ETX>&gt;</ETX>",https://ieeexplore.ieee.org/document/63270/,IEEE Transactions on Robotics and Automation,Dec. 1990,ieeexplore
10.1109/ACCESS.2018.2809681,Industrial Big Data Analytics for Prediction of Remaining Useful Life Based on Deep Learning,IEEE,Journals,"Due to the recent development of cyber-physical systems, big data, cloud computing, and industrial wireless networks, a new era of industrial big data is introduced. Deep learning, which brought a revolutionary change in computer vision, natural language processing, and a variety of other applications, has significant potential for solutions providing in sophisticated industrial applications. In this paper, a concept of device electrocardiogram (DECG) is presented, and an algorithm based on deep denoising autoencoder (DDA) and regression operation is proposed for the prediction of the remaining useful life of industrial equipment. First, the concept of electrocardiogram is explained. Then, a problem statement based on manufacturing scenario is presented. Subsequently, the architecture of the proposed algorithm called integrated DDA and the algorithm workflow are provided. Moreover, DECG is compared with traditional factory information system, and the feasibility and effectiveness of the proposed algorithm are validated experimentally. The proposed concept and algorithm combine typical industrial scenario and advance artificial intelligence, which has great potential to accelerate the implementation of industry 4.0.",https://ieeexplore.ieee.org/document/8302913/,IEEE Access,2018,ieeexplore
10.1109/TADVP.2004.828824,Intelligent SOP manufacturing,IEEE,Journals,"Microsystems packaging is fundamentally dependent on the manufacture of microelectronic, photonic, radio frequency (RF), and MEMS devices. The system-on-package (SOP) approach has been identified as a key strategy for integrating these strategic packaging technologies. Because of rising costs, the challenge before SOP manufacturers is to offset capital investment with greater automation and technological innovation in the fabrication process. To reduce manufacturing cost, several important subtasks have emerged, including increasing fabrication yield, reducing product cycle time, maintaining consistent levels of product quality and performance, and improving the reliability of processing equipment. Because of the large number of steps involved, maintaining product quality in an SOP manufacturing facility requires the control of hundreds of process variables. The interdependent issues of high yield, high quality, and low cycle time are addressed by the ongoing development of several critical capabilities in state-of-the-art computer-integrated manufacturing (CIM) systems: in situ process monitoring, process/equipment modeling, real-time process control, and equipment diagnosis. Recently, the use of computational intelligence in various manufacturing applications has increased, and the SOP manufacturing arena is no exception to this trend. Artificial neural networks, genetic algorithms (GAs), and other techniques have emerged as powerful tools for assisting CIM systems in performing various process monitoring, modeling, and control functions. This paper reviews current research in these areas, as well as the potential for deployment of these capabilities in state-of-the-art SOP manufacturing facilities.",https://ieeexplore.ieee.org/document/1331523/,IEEE Transactions on Advanced Packaging,May 2004,ieeexplore
10.1109/TASE.2017.2783342,MASD: A Multimodal Assembly Skill Decoding System for Robot Programming by Demonstration,IEEE,Journals,"Programming by demonstration (PBD) transforms the robot programming from the code level to automated interface between robot and human, promoting the flexibility of robotized automation. In this paper, we focus on programming the industrial robot for assembly tasks by parsing the human demonstration into a series of assembly skills and compiling the skill to the robot executables. To achieve this goal, an identification system using multimodal information to recognize the assembly skill, called MASD, is proposed including: 1) an initial learning stage using a hierarchical model to recognize the action by considering the features from action-object effect, gesture, and trajectory and 2) a retrospective thinking stage using a segmentation method to cut the continuous demonstrations into multiple assembly skills optimally. Using MASD, the demonstration of assembly tasks can be explained with high accuracy in real time, driving a hypothesis that a PBD system on the top of MASD can be extended to more realistic assembly tasks beyond pure positional moving and picking. In experiments, the skill identification module is used to recognize the five kinds of assembly skills in demonstrations of both single and multiple assembly skills, and outperforms the comparative action identification methods. Besides integrated with the MASD, the PBD system can generate the program based on the demonstration and successfully enable an ABB industrial robotic arm simulator to assemble a flashlight and a switch, verifying the initial hypothesis. Note to Practitioners-In the conventional robotized automation, the key role of the robot mainly owes to its capacity for repeating a wide variety of tasks with high speed and accuracy in long term, with a cost of days to months of programming for deployment. On the other hand, the new trend of customization brings the new characteristics: production in short cycle and small volume. This irreversible momentum urges the robot to switch from task to task efficiently. The biggest bottleneck here is the tedious programming, which also has high prerequisites for most practitioners in manufacturing. This situation motivates the development of a PBD system that can understand the assembly skills performed by the human experts in the demonstration and accordingly generate the program for robot's execution of the taught task. In this paper, we present a skill decoding system to parse the observational raw demonstration into symbolic sequences, which is the crucial bridge to enable the automatic programming. The system achieves high performance in recognition and is tailored for the PBD in assembly tasks by considering both advantages and disadvantages in the background of assembly, such as controllable environment and limited computational resources. It is particularly useful for assembly tasks with modularized actions based on a set of standard parts. At the perspective of industrial application, the PBD upon the proposed system is a promising solution to improve the flexibility of manufacture, which is expected to be true in midterm but an important step toward this goal.",https://ieeexplore.ieee.org/document/8263146/,IEEE Transactions on Automation Science and Engineering,Oct. 2018,ieeexplore
10.1109/TIA.2019.2940585,Missing Data Imputation With OLS-Based Autoencoder for Intelligent Manufacturing,IEEE,Journals,"Motivated by the global economy that is greatly shaped by the landscape changes in energy and manufacturing where more and more devices and systems are interconnected, intelligent manufacturing in which data mining is of great importance is studied. In this article, an energy monitoring platform for small- and medium-sized enterprises developed by the point energy team (www.pointenergy.org) is first introduced, which monitors and records the energy consumption of manufacturing processes at various levels of granularity. In processing the collected data, the incompleteness in the data due to various factors needs to be addressed first otherwise it may lead to the inaccurate portrayal of the system and poor generalization of the resultant model trained by the data. Hence, a novel orthogonal-least-square-based autoencoder is proposed to generate new samples for the imputation of missing values. This approach is to learn the representative code from the original samples by constructing an improved encoder network in which the hidden neurons are orthogonal with each other. The new samples are then generated through the decoder network. The proposed approach selects the hidden neurons one by one based on the OLS estimation until an adequate network is built. The classical techniques and other generative models are compared to verify the effectiveness of the proposed algorithm. For these methods, the optimal parameters are estimated based on the performance metric of the cross-validation mean square error. In the experiment, two real industrial datasets from a baking process and a polymer extrusion process are adopted and the percentage of missing values varies from 0.02 to 0.25. The experimental results confirm that the proposed method offers stable performance in the presence of different missing ratios, and it outperforms significantly alternative approaches while the missing ratio is greater than 0.05.",https://ieeexplore.ieee.org/document/8828079/,IEEE Transactions on Industry Applications,Nov.-Dec. 2019,ieeexplore
10.1109/ACCESS.2017.2754507,Model-Based Development of Knowledge-Driven Self-Reconfigurable Machine Control Systems,IEEE,Journals,"To accommodate the trend toward mass customization launched by intelligent manufacturing in the era of Industry 4.0, this paper proposes the combination of model-driven engineering and knowledgedriven engineering during the development process of self-reconfigurable machine control systems. The complete tool chain for model development, execution, and reconfiguration is established. For the design phase, a machine-control-domain-specific modeling language and the supporting design environment are developed. With regard to the execution stage, a runtime framework compliant with the IEC 61499 standard is proposed. On the ground of the modeling environment and the reconfigurable run-time framework, a self-adaptive control module is developed to establish the close-loop self-reconfiguration infrastructure. The ontological representation of knowledge base toward this end is described, along with extendable SQWRL rules specified to automatically initiate the reconfiguration process in the cases of external user demands and internal faults. A prototype motion control kernel in the low-level layer of machine control system architecture is developed with the proposed modeling language and is then deployed to the runtime framework. Two case studies on self-reconfiguration of the proof-of-concept motion control kernel are demonstrated, which prove the feasibility of our proposal.",https://ieeexplore.ieee.org/document/8047091/,IEEE Access,2017,ieeexplore
10.1109/ACCESS.2021.3125519,Multi-Objective Optimization of Electric Arc Furnace Using the Non-Dominated Sorting Genetic Algorithm II,IEEE,Journals,"Combining classical technologies with modern intelligent algorithms, this paper introduces a new approach for the optimisation and modelling of the EAF-based steel-making process based on a multi-objective optimisation using evolutionary computing and machine learning. Using a large amount of real-world historical data containing 6423 consecutive EAF heats collected from a melt shop in an established steel plant this work not only creates machine learning models for both EAF and ladle furnaces but also simultaneously minimises the total scrap cost and EAF energy consumption per ton of scrap. In the modelling process, several algorithms are tested, tuned, evaluated and compared before selecting Gradient Boosting as the best option to model the data analysed. A similar approach is followed for the selection of the multi-objective optimisation algorithm. For this task, six techniques are tested and compared based on the hypervolume performance indicator to just then select the Non-dominated Sorting Genetic <xref ref-type=""algorithm"" rid=""alg2"">Algorithm II</xref> (<italic>NSGA-II</italic>) as the best option. Given this applied research focus on a real manufacturing process, real-world constraints and variables such as individual scrap price, scrap availability, tap additives and ambient temperature are used in the models developed here. A comparison with an equivalent EAF model from the literature showed a 13% improvement using the mean absolute error in the EAF energy usage prediction as a comparative metric. The multi-objective optimisation resulted in reductions in the energy consumption costs that ranged from 1.87% up to 8.20% among different steel grades and scrap cost reductions ranging from 1.15% up to 5.2%. The machine learning models and the optimiser were ultimately deployed with a graphical user interface allowing the melt-shop staff members to make informed decisions while controlling the EAF operation.",https://ieeexplore.ieee.org/document/9600818/,IEEE Access,2021,ieeexplore
10.1109/TMECH.2014.2366033,Output Regulation of Nonlinear Systems With Application to Roll-to-Roll Manufacturing Systems,IEEE,Journals,"This paper deals with the problem of synthesizing feedforward control to aid the regulation of output of a nonlinear system in the presence of partially known exogenous inputs. The problem appears in many engineering applications including Roll-to-Roll (R2R) manufacturing systems. Currently known methods for this problem either require the solution of a constrained partial differential equation or the preview information of the signal to be tracked. The novelty of this paper lies in synthesizing feedforward control as the solution of a system of differential-algebraic equations, which is considerably less complex and suitable for practical implementation. In this paper, we consider the problem of regulating the output while rejecting the disturbances and apply it to R2R manufacturing systems. The problem of tracking reference signals can also be handled with the suggested technique. We assume that the disturbance signal is the output of a known exogenous system with unknown initial conditions. A parameter identification scheme to estimate the unknown initial conditions is developed. The proposed technique is successfully applied to control of web tension in a large R2R machine which mimics most of the features of industrial R2R machines and contains real-time hardware and software that is used in industrial practice. Extensive experiments were conducted to evaluate the proposed scheme for web tension control under various experimental conditions, including different web speeds and materials. We will present and discuss the representative experimental results with the proposed technique and provide a comparison with an industrial PI control scheme to highlight the benefits of using the proposed scheme.",https://ieeexplore.ieee.org/document/6963413/,IEEE/ASME Transactions on Mechatronics,June 2015,ieeexplore
10.1109/TII.2019.2940099,Performance Supervised Fault Detection Schemes for Industrial Feedback Control Systems and their Data-Driven Implementation,IEEE,Journals,"This article addresses performance supervised fault detection (PSFD) issues for industrial feedback control systems based on performance degradation prediction. To be specific, three performance indicators are first introduced based on Bellman equation to predict system performance degradations for industrial processes with the aid of machine learning techniques. Based on them, three PSFD schemes are proposed by embedding the performance indicators as supervising information. In this context, the data-driven implementation of PSFD schemes are investigated for linear systems with unmeasurable state variables. A case study on rolling mill process, a typical benchmark in the steel manufacturing processes, is given at the end of this article to illustrate the applications of the proposed fault detection schemes.",https://ieeexplore.ieee.org/document/8827307/,IEEE Transactions on Industrial Informatics,April 2020,ieeexplore
10.1109/ACCESS.2020.3046784,Reinforcement Learning With Composite Rewards for Production Scheduling in a Smart Factory,IEEE,Journals,"Rapid advances of sensing and cloud technologies transform the manufacturing system into a data-rich environment and make production scheduling increasingly complex. Traditional offline scheduling methods are limited in the ability to handle low-volume-high-mix workorders with diverse design specifications. Simulation-based methods show the promise for distributed scheduling of manufacturing jobs but are mostly implemented with historical data and empirical rules in a static manner. Recently, artificial intelligence (AI) algorithms fuel increasing interests to solve dynamic scheduling problems in the manufacturing setting. However, it's difficult to utilize high-dimensional data for production scheduling while considering multiple practical objectives for smart manufacturing (e.g., minimize the makespan, reduce production costs, balance workloads). Therefore, this paper presents a new AI scheduler with composite reward functions for data-driven dynamic scheduling of manufacturing jobs under uncertainty in a smart factory. Internet-enabled sensor networks are deployed in the smart factory to track real-time statuses of workorders, machines, and material handling systems. A novel manufacturing value network is developed to take high-dimensional data as the input and then learn the state-action values for real-time decision making. Based on reinforcement learning (RL), composite rewards help the AI scheduler learn efficiently to achieve multiple objectives for production scheduling in real time. The proposed methodology is evaluated and validated with experimental studies in a smart manufacturing setting. Experimental results show that the new AI scheduler not only improves the multi-objective performance metrics in the production scheduling problem but also effectively copes with unexpected events (e.g., urgent workorders, machine failures) in manufacturing systems.",https://ieeexplore.ieee.org/document/9305707/,IEEE Access,2021,ieeexplore
10.1109/ACCESS.2020.3022947,Reliability Evaluation for Manufacturing System Based on Dynamic Adaptive Fuzzy Reasoning Petri Net,IEEE,Journals,"Due to failure, partial failure, or maintenance, the capacity of each machine is multi-state. Therefore, the limited relationship between the capacity of each machine and the input raw materials has to be considered. Additionally, in order to utilize the machine more effectively, the capacity of the buffers cannot be ignored, too. In this paper, a dynamic adaptive fuzzy reasoning Petri net is proposed to evaluate reliability of a manufacturing system with multiple production lines. Firstly, the model of manufacturing system is conducted, and from the perspective of demand, the minimum capacity vector and loading vector of each machine are determined. Secondly, knowledge representation and rules are formulated to establish weighted fuzzy petri nets. And the weighted fuzzy Petri net is adaptive based on the real-time level of buffers, the minimum capacity vector and loading vector. Moreover, the efficiency of product production can be improved while ensuring system reliability by adjusting the buffer level. Finally, a numerical experiment is used to demonstrate the application of our method.",https://ieeexplore.ieee.org/document/9189847/,IEEE Access,2020,ieeexplore
10.1109/ACCESS.2020.3029868,Research on Adaptive Job Shop Scheduling Problems Based on Dueling Double DQN,IEEE,Journals,"Traditional approaches for job shop scheduling problems are ill-suited to deal with complex and changeable production environments due to their limited real-time responsiveness. Based on disjunctive graph dispatching, this work proposes a deep reinforcement learning (DRL) framework, that combines the advantages of real-time response and flexibility of a deep convolutional neural network (CNN) and reinforcement learning (RL), and learns behavior strategies directly according to the input manufacturing states, thus is more appropriate for practical order-oriented manufacturing problems. In this framework, a scheduling process using a disjunction graph is viewed as a multi-stage sequential decision-making problem and a deep CNN is used to approximate the state-action value. The manufacturing states are expressed as multi-channel images and input into the network. Various heuristic rules are used as available actions. By adopting the dueling double Deep Q-network with prioritized replay (DDDQNPR), the RL agent continually interacts with the scheduling environment through trial and error to obtain the best policy of combined actions for each decision step. Static computational experiments are performed on 85 JSSP instances from the well-known OR-Library. The results indicate that the proposed algorithm can obtain optimal solutions for small scale problems, and performs better than any single heuristic rule for large scale problems, with performances comparable to genetic algorithms. To prove the generalization and robustness of our algorithm, the instances with random initial states are used as validation sets during training to select the model with the best generalization ability, and then the performance of the trained policy on scheduling instances with different initial states is tested. The results show that the agent is able to get better solutions adaptively. Meanwhile, some studies on dynamic instances with random processing time are performed and experiment results indicate that out method can achieve comparable performances in dynamic environment in the short run.",https://ieeexplore.ieee.org/document/9218934/,IEEE Access,2020,ieeexplore
10.1109/TC.2020.3033627,Revealing DRAM Operating GuardBands Through Workload-Aware Error Predictive Modeling,IEEE,Journals,"Improving the energy efficiency of DRAMs becomes very challenging due to the growing demand for storage capacity and failures induced by the manufacturing process. To protect against failures, vendors adopt conservative margins in the refresh period and supply voltage. Previously, it was shown that these margins are too pessimistic and will become impractical due to high-power costs, especially in future DRAM technologies. In this article, we present a new technique for automatic scaling the DRAM refresh period under reduced supply voltage that minimizes the probability of failures. The main idea behind the proposed approach is that DRAM error behavior is workload-dependent and can be predicted based on particular program inherent features. We use a Machine Learning (ML) method to build a workload-aware DRAM error behavior model based on the program features which we extract from real workloads during our DRAM error characterization campaign. With such a model, we identify the marginal value of the DRAM refresh period under relaxed voltage for each DRAM module of a server that enable us to reduce the DRAM power. We implement a temperature-driven OS governor which automatically sets the module-specific marginal DRAM parameters discovered by the ML model. Our governor reduces the DRAM power by 24 percent on average while minimizing the probability of failures. Unlike previous studies, our technique: i) does not require intrusive changes to hardware; ii) is implemented on a real server; iii) uses a mechanism that prevents any abnormal DRAM error behavior; and iv) can be easily deployed in data centers.",https://ieeexplore.ieee.org/document/9239888/,IEEE Transactions on Computers,1 Nov. 2021,ieeexplore
10.1109/ACCESS.2020.3045563,SDN Enhanced Resource Orchestration of Containerized Edge Applications for Industrial IoT,IEEE,Journals,"With the rise of the Industrial Internet of Things (IIoT), there is an intense pressure on resource and performance optimization leveraging on existing technologies, such as Software Defined Networking (SDN), edge computing, and container orchestration. Industry 4.0 emphasizes the importance of lean and efficient operations for sustainable manufacturing. Achieving this goal would require engineers to consider all layers of the system, from hardware to software, and optimizing for resource efficiency at all levels. This emphasizes the need for container-based virtualization tools such as Docker and Kubernetes, offering Platform as a Service (PaaS), while simultaneously leveraging on edge technologies to reduce related latencies. For network management, SDN is poised to offer a cost-effective and dynamic scalability solution by customizing packet handling for various edge applications and services. In this paper, we investigate the energy and latency trade-offs involved in combining these technologies for industrial applications. As a use case, we emulate a 3D-drone-based monitoring system aimed at providing real-time visual monitoring of industrial automation. We compare a native implementation to a containerized implementation where video processing is orchestrated while streaming is handled by an external UE representing the IIoT device. We compare these two scenarios for energy utilization, latency, and responsiveness. Our test results show that only roughly 16 percent of the total power consumption happens on the mobile node when orchestrated. Virtualization adds up about 4.5 percent of the total power consumption while the latency difference between the two approaches becomes negligible after the streaming session is initialized.",https://ieeexplore.ieee.org/document/9296769/,IEEE Access,2020,ieeexplore
10.1109/TSMCC.2013.2265234,Self-Organized P2P Approach to Manufacturing Service Discovery for Cross-Enterprise Collaboration,IEEE,Journals,"The combination of service-oriented architecture (SOA) and peer-to-peer (P2P) architecture plays a promising role in distributed manufacturing environments in that the peer service can be used to facilitate the integration and discovery of distributed manufacturing resources and achieve communication and collaboration across distributed virtual enterprises. However, the large size, dynamic nature, and heterogeneous expression of distributed manufacturing resources bring forth a serious challenge in scalability and efficiency. This paper presents a self-organized P2P framework that supports scalable and efficient manufacturing service (MS) discovery for cross-enterprise collaboration by forming and maintaining autonomous enterprise peer groups (PG). Each enterprise exhibits as a peer that provides some sharable MSs that are represented comprehensively and formally with a generalized ontology. Each enterprise PG dynamically clusters a set of enterprise peers offering semantically similar MSs, and elects the most reputed peer through multicriteria trust evaluation as its core (i.e., super peer, SP). Then, a MS request can be first routed to the suitable SP and further to its leaf peer in a systematic way, thus supporting efficient service discovery. A prototype system is implemented on JXTA for real application and validated through an experimental case study.",https://ieeexplore.ieee.org/document/6600968/,"IEEE Transactions on Systems, Man, and Cybernetics: Systems",March 2014,ieeexplore
,The application of simulation technique in whole CIMS life cycle,BIAI,Journals,"The life cycle of CIMS (Computer Integrated Manufacturing System) includes four phases: requirement analyzing, designing, implementation and running. For reducing the risk of investment and achieving better economic results the simulation technique is needed in the above four phases of CIMS life cycle. Under the support of China 863 / CIMS plan a series of simulation projects are established. Some of them are finished with succeed and have been used in application. In this paper four simulation projects are introduced.(1) The Integrated Manufacturing Simulation Software (IMSS). It is an integrated platform, based on the discrete event simulation principle. It can be used to analyze and design CIMS, especially FMS, and evaluate the daily production plan.(2) The Advanced Hierarchical Control System Emulator (AHCSE), a software system, based on the finite state machine principle. It can be used to analyze and design of CIMS hierarchical control system, and check expanded system performance before expanding.(3) The Factory Scheduling Environment (FASE), a software system based on the discrete event simulation principle and artificial intelligence technology. It can be used for shop floor scheduling.(4) The Machining Process Simulator (MPS). It can simulate the machining process of machining center by computer. It can check the correctness of NC code (including interference and confliction) and replace the real machining center to support the simulation environment for shop floor scheduling and controlling. There are three companies and universities joining in these four projects, they are: Tsinghua University, Huazhong University of Technology, Beijing Institute of Computer Application and Simulation.",https://ieeexplore.ieee.org/document/6074468/,Journal of Systems Engineering and Electronics,Sept. 1994,ieeexplore
10.1109/TII.2020.3038780,Toward Secure Data Fusion in Industrial IoT Using Transfer Learning,IEEE,Journals,"As an emerging technology, the industrial Internet of Things (IIoT) can promote the development of industrial intelligence, improve production efficiency, and reduce manufacturing costs. In IIoT, the improvement and progress of industrial production and applications are inseparable from data fusion, a process that realizes the collection, analysis, and processing of the massive IoT data generated by industrial equipment and applications. IIot demands a real-time, effective, and privacy-preserving data fusion process. However, the existing works need to train different learning models for data analysis, which cannot meet real-time requirements in IIoT. Meanwhile, the lack of defense against internal attacks and the difficulty to balance system performance and privacy protection hinder the effectiveness and privacy protection in the data fusion process. To solve the abovementioned problems, in this article, we propose a new transfer learning-based secure data fusion strategy (TSDF) for IIoT. The proposed TSDF consists of three parts, guidance based deep deterministic policy gradient (GDDPG) algorithm for task classification, transfer learning based GDDPG for grouping of task receivers, and a multiblockchain mechanism for privacy preservation. The experiment results show that TSDF can achieve high system throughput and low latency, providing privacy preservation in data fusion under various IIoT application environments.",https://ieeexplore.ieee.org/document/9262056/,IEEE Transactions on Industrial Informatics,Oct. 2021,ieeexplore
10.1109/70.508435,Virtual-reality-based point-and-direct robotic inspection in manufacturing,IEEE,Journals,"This paper explores a flexible manufacturing paradigm in which robot grasping is interactively specified and skeletal images are efficiently used in combination to allow rapidly setting up surface flaw identification tasks in small-quantity/large-variety manufacturing. Two complementary technologies are combined to make implementation of inspection as rapid as possible. First, a novel material handling approach is described for robotic picking and placing of parts onto an inspection table using virtual tools. This allows an operator to point and give directives to set up robotic inspection tasks. Second, since specification may be approximate using this method, a fast and flexible means of identifying images of perfect and flawed parts is explored that avoids rotational or translational restrictions on workpiece placement. This is accomplished by using skeleton pixel counts as neural network inputs. The total system, including material handling and skeleton-based inspection, features flexibility during manufacturing set-up, and reduces the process time and memory requirements for workpiece inspection.",https://ieeexplore.ieee.org/document/508435/,IEEE Transactions on Robotics and Automation,Aug. 1996,ieeexplore
10.1109/ACCESS.2021.3082934,Visual Product Tracking System Using Siamese Neural Networks,IEEE,Journals,"Management of unstructured production data is a key challenge for Industry 4.0. Effective product tracking endorses data integration and productivity improvements throughout the manufacturing processes. Radio-frequency identification (RFID) tags are used in many tracking cases, but in some manufacturing environments, those cannot be used as they might get damaged or removed during processing. In this paper, we propose an alternative visual product tracking system. The physical system uses two cameras placed at the two ends of the tracked process(es). Product pairs are then matched with a Siamese neural network operating on the product images and trained offline on the problem at hand with labeled data. The proposed system can track products solely based on their visual appearance and without any physical interference with the products or production processes. Unlike other existing image-based methods, the proposed system is invariant to major positional and visual changes in the products. As a proof-of-concept, we tested the proposed system with real plywood factory data and were able to track the products with 98.5 % accuracy in a realistic test scenario. The implementation of the proposed method and the Veneer21 dataset are publicly available at https://github.com/TuomasJalonen/visual-product-tracking-system.",https://ieeexplore.ieee.org/document/9439511/,IEEE Access,2021,ieeexplore
10.1109/TSM.2021.3068974,Wafer Reflectance Prediction for Complex Etching Process Based on <italic>K</italic>-Means Clustering and Neural Network,IEEE,Journals,"In LED semiconductor manufacturing, it is important to evaluate the wafer reflectance in order to validate the quality of wafers. In this work, a learning model based on K-means clustering and neural networks is proposed to analyze the relationship between etching parameters and wafer reflectance under a complex etching environment. The implemented clustering algorithm is used to cluster historical data and reduce the effect caused by different processing environments. Then, for each obtained cluster, a neural network is developed to learn the relationship between etching parameters and wafer reflectance. Finally, a real case study from an LED semiconductor fab is conducted to show the application of the proposed method. Experiments show that the proposed method achieves much better performance in comparison with support vector machine for mapping the relationship between etching parameters and wafer reflectance. Also, by the proposed method, the average prediction accuracy of the wafer reflectance can be improved by up to 9.38%, and the mean square error is reduced by 21.64% compared with the methods without clustering.",https://ieeexplore.ieee.org/document/9387413/,IEEE Transactions on Semiconductor Manufacturing,May 2021,ieeexplore
10.1109/TAI.2021.3057027,ZJU-Leaper: A Benchmark Dataset for Fabric Defect Detection and a Comparative Study,IEEE,Journals,"Fabric inspection plays an important role in the process of quality control in textile manufacturing. There is a growing demand in the textile industry to leverage computer vision technology for more efficient quality control in the hope that it will replace the traditional labor-intensive inspection by naked eyes. However, there is an underlying viewpoint in most existing fabric datasets that automatic defect detection is a traditional image classification problem, thus more samples help better, which lacks enough consideration about the problem itself and real application environments. After deep communication with users, we find these facts that an assembly line usually has only a few fixed texture fabrics for a long period, users prefer fast deployment and easily upgradable model to a general model and long-time tuning, and users hope the process of collecting samples, annotating, and deployment affects assembly lines as little as possible. This implies that defect detection is different from popular deep learning problems. Multiple-stage models and fast training become more attractive since users are able to train and deploy models by themselves according to the real conditions of samples that can be obtained. Based on this analysis, we propose a new fabric dataset “ZJU-Leaper”. It provides a series of task settings in accordance with the progressive strategy dealing with the problem, from only normal samples to many defective samples with precise annotations, to facilitate real-world application. To avoid misleading information and inconsistency issues associated with the prior evaluation metrics, we propose a new evaluation protocol by experimental analysis of task-specific indexes, which can tell a truthful comparison between different inspection methods. We also offer some novel solutions to address the new challenges of our dataset, as part of the baseline experiments. It is our hope that ZJU-Leaper can accelerate the research of automated visual inspection and empower the practitioners with more opportunities for manufacturing automation in the textile industry.",https://ieeexplore.ieee.org/document/9346038/,IEEE Transactions on Artificial Intelligence,Dec. 2020,ieeexplore
10.1109/GIOTS49054.2020.9119497,Industrial IoT and Digital Twins for a Smart Factory : An open source toolkit for application design and benchmarking,IEEE,Conferences,"The rapid evolution of digital technology and designed intelligence, such as the Internet of Things (IoT), Big data analytics, Artificial Intelligence (AI), Cyber Physical Systems (CPS), has been a catalyst for the 4th industrial revolution (known as industry 4.0). Among other, the two key state-of-the-art concepts in Industry 4.0, are Industrial IoT (IIoT) and digital twins. IIoT facilitates real-time data acquisition, processing and analytics over large amount of sensor data streams produced by sensors installed within a smart factory, while the `digital twin' concept aims to enable smart factories via the digital replication or representation of physical machines, processes, people in cyber-space. This paper explores the capability of present-state open-source platforms to collectively achieve digital twin capabilities, including IoT real-time data acquisition, virtual representation, analytics, and visualisation. The aim of this work is to `close the gap' between research and implementation, through a collective open source IoT and Digital Twin architecture. The performance of the open-source architecture in this work, is demonstrated in a use-case utilising industry `open data', and is bench-marked with universal testing tools.",https://ieeexplore.ieee.org/document/9119497/,2020 Global Internet of Things Summit (GIoTS),3-3 June 2020,ieeexplore
10.1109/SAI.2015.7237244,Object oriented eco-simulator system as predictor &amp; exploratory system to track impact of human induced activities on environmental resource: A case study: Evaluation of unsuitable ground water consumption of petrochemical and power factory of Shazand on ecological status of Markazi province,IEEE,Conferences,"Environment system studies regardless of main reasons are very difficult whereas they have been constructed from natural ecosystems, manmade infrastructure, socio-economic virtual objects and also their perplexing inner relations. Furthermore, ecosystems as any other elements, have iterative multi-scale structure which makes it even more sophisticated to study. That's why real compound system studies look breathtaking and also too difficult. Gradual depletion of perched water tables in “Shazand” watershed of “Markazi” province in Iran, which mainly stems from bulky extraction of water resources by huge factories and intensive agriculture e.g., doesn't treated well by policy makers not only due to lack of well-based studies but also via devoid of potent monitoring and prediction simulator system to manifest decision side effects to managers and those whom are responsible to keep the environment system balanced. Although GI functionalities have exhibited wonder abilities to handle spatial problems, they seem weak in processing ecosystem real problems as there is few specified environment modules. While combination of GIS and environment modeling software intensively suffers from integration issue, simulator software are mainly confined due to filed limitation, non-spatial functionalities and spatial ontologies. To present a proper solution which makes up the mentioned deficiencies, first of all, a problem tree was designed to decompose real cybernetic correlations between “Shazand” watershed elements including natural or manmade ones. Then, object oriented concept was deployed to simulate sophisticated real environmental systems and also to simplify programming issues regarding to exact spatial ontology. To complete the solution, a category based on “Category Theorem” was defined to support temporal concepts on the backdrop of real ecosystem phenomena regarding Spatio-temporal modeling concepts and object oriented foundations. Likewise group of functors were designed via functional language to allay process burden and also programming of object counteractions. Emerging spatial simulator system, supports monitoring issues as well as prediction missions in compound environment systems. It has been also indicated how to use it as an exploratory analyzing system which conduct real analytical system consisted of virtual smart analytical objects.",https://ieeexplore.ieee.org/document/7237244/,2015 Science and Information Conference (SAI),28-30 July 2015,ieeexplore
10.1109/ICIAI.2019.8850806,Pulse Interference Resilience of Convolutional Codes in WirelessHP Physical Layer Protocols: Experiment in Real Factory Environments,IEEE,Conferences,"To address the ultra-low latency and high-reliability requirements in critical applications of industrial wireless control, high performance wireless (WirelessHP) communications has demonstrated its superiority in this area. This paper steps further by investigating the pulse interference resilience of WirelessHP physical layer, and convolutional codes (CC) are adopted for short packet transmission. Through the constructed hardware platform, duration of the pulse interference and number of pulses in each transmitted packet are adjustable, based on which the packet error rate (PER) performance is tested in real factory environments. We show 100 ns duration pulse interference has a slight influence on the PER performance if only one pulse exists in the waveform of one packet, but lose this property for longer duration pulse interference. Lower-rated CC outperforms higher-rated CC on condition of the same pulse interference settings, but is more frangible to pulse interference that with the same interference rate.",https://ieeexplore.ieee.org/document/8850806/,2019 1st International Conference on Industrial Artificial Intelligence (IAI),23-27 July 2019,ieeexplore
10.1109/AIVR.2018.00024,The Virtual Factory: Hologram-Enabled Control and Monitoring of Industrial IoT Devices,IEEE,Conferences,"Augmented reality (AR) has been exploited in manifold fields but is yet to be used at its full potential. With the massive diffusion of smart devices, opportunities to build immersive human-computer interfaces are continually expanding. In this study, we conceptualize a virtual factory: an interactive, dynamic, holographic abstraction of the physical machines deployed in a factory. Through our prototype implementation, we conducted a user-study driven evaluation of holographic interfaces compared to traditional interfaces, highlighting its pros and cons. Our study shows that the majority of the participants found holographic manipulation more attractive and natural to interact with. However, current performance characteristics of head-mounted displays must be improved to be applied in production.",https://ieeexplore.ieee.org/document/8613643/,2018 IEEE International Conference on Artificial Intelligence and Virtual Reality (AIVR),10-12 Dec. 2018,ieeexplore
10.1109/ACCESS.2017.2770180,An Integrated Industrial Ethernet Solution for the Implementation of Smart Factory,IEEE,Journals,"Smart factory addresses the vertical integration of physical entities and information systems. Network and cloud are two essential infrastructures to achieve this goal. Among them, the network provides interconnection for communication and data flow, while the cloud provides powerful and elastic computing and storage abilities for big data and intelligent applications. This paper presents a cloud-centric framework for the implementation of the smart factory. Based on this framework, three high leveled protocols viz., EtherCAT, DDS, and OPC UA are selected to implement machine network (controller to sensors/actuators communication), machine to machine communication, and machine to cloud communication respectively, to satisfy diverse communication requirements of the smart factory. An integrated architecture for combining ontology knowledge and semantic data to support intelligent applications is also proposed. These network and data process schemes are verified in a smart factory prototype for personalized candy packing application.",https://ieeexplore.ieee.org/document/8096995/,IEEE Access,2017,ieeexplore
10.1109/ACCESS.2020.3046784,Reinforcement Learning With Composite Rewards for Production Scheduling in a Smart Factory,IEEE,Journals,"Rapid advances of sensing and cloud technologies transform the manufacturing system into a data-rich environment and make production scheduling increasingly complex. Traditional offline scheduling methods are limited in the ability to handle low-volume-high-mix workorders with diverse design specifications. Simulation-based methods show the promise for distributed scheduling of manufacturing jobs but are mostly implemented with historical data and empirical rules in a static manner. Recently, artificial intelligence (AI) algorithms fuel increasing interests to solve dynamic scheduling problems in the manufacturing setting. However, it's difficult to utilize high-dimensional data for production scheduling while considering multiple practical objectives for smart manufacturing (e.g., minimize the makespan, reduce production costs, balance workloads). Therefore, this paper presents a new AI scheduler with composite reward functions for data-driven dynamic scheduling of manufacturing jobs under uncertainty in a smart factory. Internet-enabled sensor networks are deployed in the smart factory to track real-time statuses of workorders, machines, and material handling systems. A novel manufacturing value network is developed to take high-dimensional data as the input and then learn the state-action values for real-time decision making. Based on reinforcement learning (RL), composite rewards help the AI scheduler learn efficiently to achieve multiple objectives for production scheduling in real time. The proposed methodology is evaluated and validated with experimental studies in a smart manufacturing setting. Experimental results show that the new AI scheduler not only improves the multi-objective performance metrics in the production scheduling problem but also effectively copes with unexpected events (e.g., urgent workorders, machine failures) in manufacturing systems.",https://ieeexplore.ieee.org/document/9305707/,IEEE Access,2021,ieeexplore
10.1109/ECTC.2019.00261,A Methodology to Correct in-Fixture Measurement of Impedance by a Machine Learning Model,IEEE,Conferences,"It is usual to measure the characteristic impedance of the transmission line in the factory by the time-domain reflectometer (TDR) with the test fixture, but the insertion loss of the test fixture would raise the measurement error, especially when the transmission line designed with unconventional impedance that is deviated from the system impedance of the measurement instrument. The traditional calibration that requires the standard kits is not generally implemented in production line due to the complicated operation and high cost. Therefore, a method is proposed to correct the measurement by a generalized transformation that is a numeric model trained by the machine learning from the measurements corresponding to the well-chosen features of the test fixture. Based on the precise circuit model of test fixture presented in this paper, a large amount of data used for training can be produced by circuit simulation instead of real measurements. An experimental instance is given to demonstrate that the measurement error could be suppressed from around 20% to below 3%.",https://ieeexplore.ieee.org/document/8811021/,2019 IEEE 69th Electronic Components and Technology Conference (ECTC),28-31 May 2019,ieeexplore
10.1109/SMC.2019.8914195,A Universal Methodology to Create Digital Twins for Serial and Parallel Manipulators,IEEE,Conferences,"With the technological advances in information technology especially in sensorization, artificial intelligence, big data and visualization; smart manufacturing and industrie 4.0 are gradually becoming an implementable reality. Digital twin is one of the pillars of smart manufacturing where by the physical and virtual worlds can by synced and mimic each others' behaviour. In future, assets and products can sense their state and report back any anomalities so that meaningful insights can be drawn and actions can be taken to keep production optimized at all times. This reporting feature can be realized with the help of digital twin technology as the twin keeps record of the physical asset's behavior. A digital twin will play an integral role in defining the concept of an integrated shopfloor. It will assist in viewing holistic behavior of asset, optimizing processes and exerting control over the physical device. To demonstrate the concept of digital twin, an universal methodology was developed and deployed at Model Factory @ ARTC (Advanced Remanufacturing and Technology Centre) program in Singapore.",https://ieeexplore.ieee.org/document/8914195/,"2019 IEEE International Conference on Systems, Man and Cybernetics (SMC)",6-9 Oct. 2019,ieeexplore
10.1109/MetroInd4.0IoT51437.2021.9488498,Adaptive LoRaWAN Transmission exploiting Reinforcement Learning: the Industrial Case,IEEE,Conferences,"Wireless technologies play a key role in the Industrial Internet of Things (IIoT) scenario, for the development of increasingly flexible and interconnected factory systems. A significant opportunity in this context is represented by the advent of Low Power Wide Area Network (LPWAN) wireless technologies, that enable a reliable, secure, and effective transmission of measurement data over long communication ranges and with very low power consumption. Nevertheless, reliability in harsh environments (as typically occurs in the industrial scenario) is a significant issue to deal with. Focusing on LoRaWAN, adaptive strategies can be profitably devised concerning the above tradeoff. To this aim, this paper proposes to exploit Reinforcement Learning (RL) techniques to design an adaptive LoRaWAN strategy for industrial applications. The RL is spreading in many fields since it allows the design of intelligent systems using a stochastic discrete-time system approach. The proposed technique has been implemented within a purposely designed simulator, allowing to draw a preliminary performance assessment in a real-world scenario. A high density of independent nodes per square km has been considered, showing a significant improvement (about 10%) of the overall reliability in terms of data extraction rate (DER) without compromising full compatibility with the standard specifications.",https://ieeexplore.ieee.org/document/9488498/,2021 IEEE International Workshop on Metrology for Industry 4.0 & IoT (MetroInd4.0&IoT),7-9 June 2021,ieeexplore
10.1109/CYBER53097.2021.9588269,Application of YOLO Object Detection Network In Weld Surface Defect Detection,IEEE,Conferences,"As industrial production becomes more modern and intelligent today, the inspection of product quality of the workshop is becoming more and more accustomed to replacing the old manual visual inspection methods with automated inspection systems. In the welding field, automated welding robots are not only used in traditional large-scale automobile assembly lines. In more general welding work, welding robots also plays an important role. The inspection of the welding quality of the welding robot is mainly to detect the four main types of weld defects. Compared to traditional defect classification based on support vector machines and defect detection based on template matching, this paper uses a welding surface defect detection system designed based on deep learning methods. By working with workshop welding experts, a large-scale image of nearly 5000 pictures is built. Large-scale weld defect datasets, while using the real-time and accuracy of the YOLO series of deep learning object detection frameworks, the weld defects detection model reaches 75.5% mean average precision(mAP) in constructed weld defect data set. In addition, the construction cost of the detection model and the deployment time of the detection system are greatly reduced. During the field test of the system in the workshop, among a batch of welding workpieces provided by the factory, the detection accuracy of weld defects reached 71%, which initially met the requirements of the workshop for an automated defect detection system.",https://ieeexplore.ieee.org/document/9588269/,"2021 IEEE 11th Annual International Conference on CYBER Technology in Automation, Control, and Intelligent Systems (CYBER)",27-31 July 2021,ieeexplore
10.1109/ECAI52376.2021.9515185,Automated ceramic plate defect detection using ScaledYOLOv4-large,IEEE,Conferences,"Automated visual inspection has become a popular topic of research in the last couple of decades, as computation power available grew exponentially. Judging by the fact that visual inspection is a critical task for the quality of the products, it would be highly recommended that people only supervise the system. This paper proposes a low cost, rapid development defect detection system based on the Scaled-YOLOv4 object detection model. The original model achieves almost state of the art detection mAP on the COCO dataset with a mAP(mean average precision) of 56.0 for the largest variant, named YOLOv4-P7. Our version is derived from the ScaledYOLOv4-P5 model and is trained on ceramic plate defects and achieves 87.4 mAP at a intersection of union of 0.5, while comfortably processing a frame in 20ms on a consumer RTX3070 GPU. Thus, the real time constraint for the manufacturing system is fulfilled. Hence, the critical aspects of the development process are the: quick development process, fast training, rapid deployment on the factory floor, quick validation and feedback, using images acquired in the lab - not on the factory floor for first iteration and overall low cost of the automated inspection system.",https://ieeexplore.ieee.org/document/9515185/,"2021 13th International Conference on Electronics, Computers and Artificial Intelligence (ECAI)",1-3 July 2021,ieeexplore
10.1109/ICAIIC51459.2021.9415189,Big Data Platform for Intelligence Industrial IoT Sensor Monitoring System Based on Edge Computing and AI,IEEE,Conferences,"The cutting edge of Industry 4.0 has driven everything to be converted to disruptive innovation and digitalized. This digital revolution is imprinted by modern and advanced technology that takes advantage of Big Data and Artificial Intelligence (AI) to nurture from automatic learning systems, smart city, smart energy, smart factory to the edge computing technology, and so on. To harness an appealing, noteworthy, and leading development in smart manufacturing industry, the modern industrial sciences and technologies such as Big Data, Artificial Intelligence, Internet of things, and Edge Computing have to be integrated cooperatively. Accordingly, a suggestion on the integration is presented in this paper. This proposed paper describes the design and implementation of big data platform for intelligence industrial internet of things sensor monitoring system and conveys a prediction of any upcoming errors beforehand. The architecture design is based on edge computing and artificial intelligence. To extend more precisely, industrial internet of things sensor here is about the condition monitoring sensor data - vibration, temperature, related humidity, and barometric pressure inside facility manufacturing factory.",https://ieeexplore.ieee.org/document/9415189/,2021 International Conference on Artificial Intelligence in Information and Communication (ICAIIC),13-16 April 2021,ieeexplore
10.1109/ICII.2018.00024,Brightics-IoT: Towards Effective Industrial IoT Platforms for Connected Smart Factories,IEEE,Conferences,"Industrial Internet-of-Things (IIoT) supports machines, computers and users to enable intelligent operations using advanced device management and data analytics. In recent years, thanks to standardized IoT platforms and advanced Artificial Intelligence (AI) technologies, there have been great advances in IIoT, and now it promises revolutions on various manufacturing domains such as transport, health, factory and energy. In this paper, based on our experience operating IIoT in various factory applications, we present the technical challenges of manufacturing facilities needed to be dealt with to collect huge amount of data in real-time and counteraction points of an IoT platform regarding these technical challenges and what kinds of features need to be implemented for intelligent services in the smart manufacturing. Finally, we introduce a story of applying industrial IoT platform in production to show how iterative development approaches can achieve business requirements based on elastically scaled-out architecture.",https://ieeexplore.ieee.org/document/8539113/,2018 IEEE International Conference on Industrial Internet (ICII),21-23 Oct. 2018,ieeexplore
10.1109/ICIT.2002.1189341,Computer based robot training in a virtual environment,IEEE,Conferences,"As more market segments are welcoming automation, the robotic field continues to expand. With the accepted breadth of viable industrial robotic applications increasing, the need for flexible robotic training also grows. In the area of simulation and offline programming there have been innovative developments to Computer Aided Robotics (CAR) Systems. New and notable releases have been introduced to the public, especially among the small, affordable, and easy to use systems. These CAR-Systems are mainly aimed at system integrators in general industry business fields to whom the complex, powerful software tools used by the automotive industry (and its suppliers) are oversized. In general, CAR-Systems are used to design robot cells and to create the offline programs necessary to reduce start-up time and to achieve a considerable degree of planning reliability. Another potential yet to be fully considered, is the use of such CAR-Systems as an inexpensive and user-friendly tool for robotics training. This paper will show the educational potential and possibility inherent in simulation and introduce a successful example of this new method of training. Finally, this presentation should be seen as an attempt to outline novel methods for future education in an industrial environment characterized by the increased occurrence and implementation of the virtual factory.",https://ieeexplore.ieee.org/document/1189341/,"2002 IEEE International Conference on Industrial Technology, 2002. IEEE ICIT '02.",11-14 Dec. 2002,ieeexplore
10.1109/INES.2017.8118553,Data integration in scalable data analytics platform for process industries,IEEE,Conferences,"The main objective of work presented in this paper is to introduce the architectural overview of the big data analytics platform for support of process industries. Our aim was to design and develop the cross-sectorial scalable environment, which will enable the data collection from different sources and support the development of predictive functions to help the process industries in optimizing of their production processes. This paper introduces the components of Big Data Storage and Analytics platform which is the core component of the developed cross-sectorial environment. Currently, it is built on top of the Apache Hadoop technology stack and relies on Hadoop distributed file system. On the other hand, we present the idea of integration of the data obtained from different production environments. Data integration is implemented using the Apache Nifi and we designed the workflows for processing both interval and real-time data from the production sites. In this case, we consider two pilot cases, an aluminium factory in France and a plastic molding factory in Portugal.",https://ieeexplore.ieee.org/document/8118553/,2017 IEEE 21st International Conference on Intelligent Engineering Systems (INES),20-23 Oct. 2017,ieeexplore
10.23919/ECC.1999.7100000,Dynamic GMDH neural networks and their application in fault detection systems,IEEE,Conferences,"In this paper, the problem of the dynamic GMDH (Group Method and Data Handling) neural networks and their application in fault detection systems is presented. Such networks can be considered as feedforward networks with a growing structure during the training process. The GMDH networks application in fault detection systems improves their efficiency with lack of information regarding the structure and dynamics of the diagnosed system. The proposed networks have been implemented in fault detection systems using the real data from the Lublin sugar factory.",https://ieeexplore.ieee.org/document/7100000/,1999 European Control Conference (ECC),31 Aug.-3 Sept. 1999,ieeexplore
10.1109/AIHAS.1991.138480,"EMM-networking model for FMS modeling, simulation and control",IEEE,Conferences,"The major challenge of implementing FMS (flexible manufacturing systems) at the factory level is to realize automated control. An appropriate FMS model is required for the purpose of control software design and implementation. In this paper, the EMM-networking (extended Moore machine) model based on automata theory for FMS control systems is presented. The basic concept behind the EMM-networking model is to identify the structure of complex systems in a hierarchy of abstractions. By using object-oriented methodology, the EMM-networking model of a FMS can be implemented into object-based software modules, based on which an integrated environment for simulation, software construction and real-time control can be realized. The methodology is demonstrated via the simulation of a prototype FMS.&lt;<ETX>&gt;</ETX>",https://ieeexplore.ieee.org/document/138480/,"[1991] Proceedings. The Second Annual Conference on AI, Simulation and Planning in High Autonomy Systems",1-2 April 1991,ieeexplore
10.1109/ISAECT50560.2020.9523700,Edge-Cloud Architectures Using UAVs Dedicated To Industrial IoT Monitoring And Control Applications,IEEE,Conferences,"The deployment of new technologies to ease the control and management of a massive data volume and its uncertainty is a very significant challenge in the industry. Under the name ""Smart Factory"", the Industrial Internet of Things (IoT) aims to send data from systems that monitor and control the physical world to data processing systems for which cloud computing has proven to be an important tool to meet processing needs. unmanned aerial vehicles (UAVs) are now being introduced as part of IIoT and can perform important tasks. UAVs are now considered one of the best remote sensing techniques for collecting data over large areas. In the field of fog and edge computing, the IoT gateway connects various objects and sensors to the Internet. It function as a common interface for different networks and support different communication protocols. Edge intelligence is expected to replace Deep Learning (DL) computing in the cloud, providing a variety of distributed, low-latency and reliable intelligent services. In this paper, An unmanned aerial vehicle is automatically integrated into an industrial control system through an IoT gateway platform. Rather than sending photos from the UAV to the cloud for processing, an AI cloud trained model is deployed in the IoT gateway and used to process the taken photos. This model is designed to overcome the latency channels of the cloud computing architecture. The results show that the monitoring and tracking process using advanced computing in the IoT gateway is significantly faster than in the cloud.",https://ieeexplore.ieee.org/document/9523700/,2020 International Symposium on Advanced Electrical and Communication Technologies (ISAECT),25-27 Nov. 2020,ieeexplore
10.1109/ICE/ITMC49519.2020.9198492,Enhancing Cognition for Digital Twins,IEEE,Conferences,"In the era of Industry 4.0, Digital Twins (DTs) pave the way for the creation of the Cognitive Factory. By virtualizing and twinning information stemming from the real and the digital world, it is now possible to connect all parts of the production process by having virtual copies of physical elements interacting with each other in the digital and physical realms. However, this alone does not imply cognition. Cognition requires modelling not only the physical characteristics but also the behavior of production elements and processes. The latter can be founded upon data-driven models produced via Data Analytics and Machine Learning techniques, giving rise to the so-called Cognitive (Digital) Twin. To further enable the Cognitive Factory, a novel concept, dubbed as Enhanced Cognitive Twin (ECT), is proposed in this paper as a way to introduce advanced cognitive capabilities to the DT artefact that enable supporting decisions, with the end goal to enable DTs to react to inner or outer stimuli. The Enhanced Cognitive Twin can be deployed at different hierarchical levels of the production process, i.e., at sensor-, machine-, process-, employee- or even factory-level, aggregated to allow both horizontal and vertical interplay. The ECT notion is proposed in the context of process industries, where cognition is particularly important due to the continuous, non-linear, and varied nature of the respective production processes.",https://ieeexplore.ieee.org/document/9198492/,"2020 IEEE International Conference on Engineering, Technology and Innovation (ICE/ITMC)",15-17 June 2020,ieeexplore
10.1109/WSC.1989.718775,Expert Simulation For On-line Scheduling,IEEE,Conferences,"In recent years, the automotive industry has realized the importance of speed of new products to market and has mounted efforts for improving it. The Expert System Scheduler (ESS) facilitates these efforts by enabling manufacturing plants to generate viable schedules under increasing constraints and demands for flexibility. The scheduler takes advantage of the Computer Integrated Manufacturing (CIM) environment by utilizing the real-time information from the factory for responsive scheduling. The Expert System Scheduler uses heuristics developed by an experiences factory scheduler. It uses simulation concepts and these heuristics to generate schedules. Forward and ""backward"" simulation are used at different stages of the schedule generation process. The system is used to control parts flow on the factory floor at one automated facility. This highly automated facility is a testbed for implementation of CIM concepts. The scheduler runs on a Texas Instruments (TI) Explorer II computer using software developed inhouse utilizing IntelliCorp's Knowledge Engineering Environment (KEE) shell and the LISP language. The scheduling computer is networked to the factory control computer, which actually controls the plant floor. The TI Explorer II acquires current plant floor information from the factory control system, generates a new schedule and sends it back within a short time. The configuration allows fast response to changes in requirements and plant floor conditions.",https://ieeexplore.ieee.org/document/718775/,1989 Winter Simulation Conference Proceedings,4-6 Dec. 1989,ieeexplore
10.1109/ATC52653.2021.9598204,Fast and Accurate Fall Detection and Warning System Using Image Processing Technology,IEEE,Conferences,"Accidental falls can cause serious injuries, which can lead to serious medical problems, especially for construction and factory workers. This paper proposes a study on a fall detection system based on computer vision. This system is applied to help detect people falling in harsh working environment such as dust, loud noise, few people working. From the recorded video streams, the data is processed to recognize a person falling, lying motionless. Algorithms for tracking people are implemented on a compact, easy-to-install embedded system. Experimental results show that the system ensures safety and can provide emergency assistance to people who have fallen within the view of the camera.",https://ieeexplore.ieee.org/document/9598204/,2021 International Conference on Advanced Technologies for Communications (ATC),14-16 Oct. 2021,ieeexplore
10.1109/IDAACS-SWS.2018.8525503,Hybrid MAC for Low Latency Wireless Communication Enabling Industrial HMI Applications,IEEE,Conferences,"Wireless technologies are one of the core components of the future industrial applications. They provide flexibility and scalability to the factory floor in parallel with deployment cost reduction. In our paper, we concentrate on future-oriented human-machine interaction (HMI) applications such as augmented reality (AR) or mobile control. Based on their requirements, we provide an investigation of IEEE 802.11 channel access techniques with respect to their suitability for industrial applications.",https://ieeexplore.ieee.org/document/8525503/,2018 IEEE 4th International Symposium on Wireless Systems within the International Conferences on Intelligent Data Acquisition and Advanced Computing Systems (IDAACS-SWS),20-21 Sept. 2018,ieeexplore
10.1109/LISS.2018.8593220,IOT Enabled Smart Logistics Using Smart Contracts,IEEE,Conferences,"Advancements in sensors and devices have enabled Internet of Things (IoT) adoption in various sectors, especially in domains looking to automate and increase their real-time decision making capabilities to improve efficiencies. Supply chain management in logistics is a perfect fit for adoption of IoT, since it involves shipment of assets being moved, tracked and housed by a number of machines, vehicles and people each day. Smart Contracts are terms and conditions parties can specify that assure trust in the enforceability of the contract and provide visibility at every step of a supply chain. IoT devices can write to a smart contract as a product moves from the factory floor to the store shelves, providing real-time visibility of an enterprises entire supply chain. This paper proposes a smart logistics solution encapsulating smart contracts, logistics planner and condition monitoring of the assets in the Supply Chain Management area. A prototype of the solution is implemented which demonstrates accountability, traceability and liability for asset handling across the supply chain by various parties involved in a logistics scenario.",https://ieeexplore.ieee.org/document/8593220/,"2018 8th International Conference on Logistics, Informatics and Service Sciences (LISS)",3-6 Aug. 2018,ieeexplore
10.1109/GIOTS49054.2020.9119497,Industrial IoT and Digital Twins for a Smart Factory : An open source toolkit for application design and benchmarking,IEEE,Conferences,"The rapid evolution of digital technology and designed intelligence, such as the Internet of Things (IoT), Big data analytics, Artificial Intelligence (AI), Cyber Physical Systems (CPS), has been a catalyst for the 4th industrial revolution (known as industry 4.0). Among other, the two key state-of-the-art concepts in Industry 4.0, are Industrial IoT (IIoT) and digital twins. IIoT facilitates real-time data acquisition, processing and analytics over large amount of sensor data streams produced by sensors installed within a smart factory, while the `digital twin' concept aims to enable smart factories via the digital replication or representation of physical machines, processes, people in cyber-space. This paper explores the capability of present-state open-source platforms to collectively achieve digital twin capabilities, including IoT real-time data acquisition, virtual representation, analytics, and visualisation. The aim of this work is to `close the gap' between research and implementation, through a collective open source IoT and Digital Twin architecture. The performance of the open-source architecture in this work, is demonstrated in a use-case utilising industry `open data', and is bench-marked with universal testing tools.",https://ieeexplore.ieee.org/document/9119497/,2020 Global Internet of Things Summit (GIoTS),3-3 June 2020,ieeexplore
10.1109/ICIT.1996.601630,Intelligent monitoring of sheet forming process,IEEE,Conferences,"In the sheet forming process, tight QC requirements and strict economic objectives make it necessary for factory to quickly identify sheet defects and take corrective actions. A framework of sensor-based intelligent monitoring system is suggested to perform online monitoring and control of cold rolled strip forming. For successful implementation, a backpropagation neural monitor is employed to recognize the defects of the cold rolled strip and generate appropriate control strategy. The monitoring system first deals with a large amount of raw process data detected by stress sensors. From such real-time data, interesting and important features, stress series are extracted and normalized. The stress series are then trained by the neural monitor for identifying the defects, such as left slope, right slope, central buckle, edge wave, quarter-wave and compound wave et al. The output of the neural monitor will activate corresponding feedback control actions such as CVC shift, screws, adjustment of bending pressure and selection of cooling sprays. To improve the performance of the neural networks, optimal learning parameters are employed to train the neural networks. The results of a case study have shown that the output of the defect recognition is well matched to the practical situation, and have given encouragement to further improvements of the intelligent monitoring system.",https://ieeexplore.ieee.org/document/601630/,Proceedings of the IEEE International Conference on Industrial Technology (ICIT'96),2-6 Dec. 1996,ieeexplore
10.1109/ICAIIC51459.2021.9415228,IoT-Based Vibration Sensor Data Collection and Emergency Detection Classification using Long Short Term Memory (LSTM),IEEE,Conferences,"In this paper, we used a vibration sensor known as G-Link 200 to collect real time vibration data. The sensor is connected through the internet gateway and Long Short Term Memory (LSTM) used for the classification of sensor data. The classification allows for detecting normal and anomaly activity situation which allows for triggering emergency situation. This is implemented in smart homes where privacy is an issue of concern. Example of such places are toilets, bedrooms and dressing rooms. It can also be applied to smart factory where detecting excessive or abnormal vibration is of critical importance to factory operation. The system eliminates the discomfort for video surveillance to the user. The data collected is also useful for the research community in similar research areas of sensor data enhancement. MATLAB R2019b was used to develop the LSTM. The result showed that the accuracy of the LSTM is 97.39% which outperformed other machine learning algorithm and is reliable for emergency classification.",https://ieeexplore.ieee.org/document/9415228/,2021 International Conference on Artificial Intelligence in Information and Communication (ICAIIC),13-16 April 2021,ieeexplore
10.1109/PHM-Besancon49106.2020.00009,Machine Performance Monitoring and Fault Classification using Vibration Frequency Analysis,IEEE,Conferences,"Machine anomalies in manufacturing directly affect the production yield and factory operation efficiency if such anomalies cannot be detected in time. Real-time monitoring of machine health condition not only improves machine throughput by reducing unplanned downtime caused by machine failure but also saves cost for unnecessary routine maintenance. This paper presents a systematic approach for real-time or near real-time machine performance monitoring solution development from data collection, feature extraction, data analytics to real-time machine fault and machine status classification. Three data-driven machine-learning approaches using one vibration sensor data are proposed to detect two common machine failure modes during machine turning process. To evaluate the the performance of each approach, three machine-learning algorithms (Random Forest, K Nearest Neighborhood, and Support Vector Machine) are implemented and tested. Evaluation results on the actual machine data shows that a two-layered classification structure with random forest algorithm as the base has high classification accuracy on the machine status including machine fault detection. The developed data-driven machine health monitoring solution is deployed in the IoT device for real-time data collection and processing and results are sent data server for data visualization.",https://ieeexplore.ieee.org/document/9115516/,2020 Prognostics and Health Management Conference (PHM-Besançon),4-7 May 2020,ieeexplore
,Motion planning with obstacle avoidance of an UR3 robot using charge system search,IEEE,Conferences,"For a cyber-physical system (CPS) of a future intelligent factory, a robotic manipulator is requested to co-work with human efficiently and safely in an environment with flexible arrangements. Therefore, an autonomous path planning of robotic manipulator is the most necessary issue to be resolved for the factory automation. For the robotic manipulator, optimizations and artificial intelligence (AI) methods are widely used to investigate the autonomous dynamic path-planning tasks with obstacle avoidance. Among these methods, the Rapidly Exploring Random Tree (RRT) algorithm has been widely used in path planning for a complex environment, because the RRT algorithm has the advantages of perfect expansion, probability completeness, and fast exploring speed. However, for some practical cases, the existing RRT algorithm may obtain a discontinuous solution of the angular trajectory. To solve the above problem, we studied a particle swarm optimization with the charge search system (CSS) to find the optimal path planning with obstacle avoidance. The steps of the proposed method are mentioned as follows: (1) establish the configuration space with the obstacle regions, (2) formulate the motion planning with obstacle using the CSS method and (3) use the PSO method to solve the path planning problem. Finally, the simulation of the path-planning task with obstacle avoidance is visually illustrated using the software RoboDK and the proposed method is implemented by the real-time experiments of the UR3 robot.",https://ieeexplore.ieee.org/document/8571928/,"2018 18th International Conference on Control, Automation and Systems (ICCAS)",17-20 Oct. 2018,ieeexplore
10.23919/DATE48585.2020.9116407,Network Synthesis for Industry 4.0,IEEE,Conferences,"Today's factory machines are ever more connected with SCADA, MES, ERP applications as well as external systems for data analysis. Different types of network architectures must be used for this purpose. For instance, control applications at the lowest level are susceptible to delays and errors while data analysis with machine learning procedures requires to move a large amount of data without real-time constraints. Standard data formats, like Automation Markup Language (AML), have been established to document factory environment, machine placement and network deployment, however, no automatic technique is currently available in the context of Industry 4.0 to choose the best mix of network architectures according to spacial constraints, cost, and performance. We propose to fill this gap by formulating an optimization problem. First of all, spatial and communication requirements are extracted from the AML description. Then, the optimal interconnection of wired or wireless channels is obtained according to application objectives. Finally, this result is back-annotated to AML to be used in the life cycle of the production system. The proposed methodology is described through a small, but complete, smart production plant.",https://ieeexplore.ieee.org/document/9116407/,"2020 Design, Automation & Test in Europe Conference & Exhibition (DATE)",9-13 March 2020,ieeexplore
10.1109/ICITACEE.2016.7892481,On the implementation of ZFS (Zettabyte File System) storage system,IEEE,Conferences,"Digital data storage is very critical in computer systems. Storage devices used to store data may at any time suffer from damage caused by its lifetime, resource failure or factory defects. Such damage may lead to loss of important data. The risk of data loss in the event of device damage can be minimized by building a storage system that supports redundancy. The design of storage based on ZFS (Zettabyte File System) aims at building a storage system that supports redundancy and data integrity without requiring additional RAID controllers. When the system fails on one of its hard drive, the stored data remains secure and data integrity is kept assured. In addition to providing redundancy, the ZFS-based storage system also supports data compression for savings on storage space. The results show that the ZFS with LZ4 compression has the highest read and write speed. For real benchmark, there is no significant difference in reading speed for a variety of different variables, whereas a significant increase in speed occurs when writing compressible files on the ZFS system with compression configuration.",https://ieeexplore.ieee.org/document/7892481/,"2016 3rd International Conference on Information Technology, Computer, and Electrical Engineering (ICITACEE)",19-20 Oct. 2016,ieeexplore
10.1109/ICIAI.2019.8850806,Pulse Interference Resilience of Convolutional Codes in WirelessHP Physical Layer Protocols: Experiment in Real Factory Environments,IEEE,Conferences,"To address the ultra-low latency and high-reliability requirements in critical applications of industrial wireless control, high performance wireless (WirelessHP) communications has demonstrated its superiority in this area. This paper steps further by investigating the pulse interference resilience of WirelessHP physical layer, and convolutional codes (CC) are adopted for short packet transmission. Through the constructed hardware platform, duration of the pulse interference and number of pulses in each transmitted packet are adjustable, based on which the packet error rate (PER) performance is tested in real factory environments. We show 100 ns duration pulse interference has a slight influence on the PER performance if only one pulse exists in the waveform of one packet, but lose this property for longer duration pulse interference. Lower-rated CC outperforms higher-rated CC on condition of the same pulse interference settings, but is more frangible to pulse interference that with the same interference rate.",https://ieeexplore.ieee.org/document/8850806/,2019 1st International Conference on Industrial Artificial Intelligence (IAI),23-27 July 2019,ieeexplore
10.1109/08IAS.2008.164,Real-Time Implementation of Intelligent Modeling and Control Techniques on a PLC Platform,IEEE,Conferences,"Programmable logic controllers (PLCs) have been used for many decades for standard control in industrial and factory environments. Over the years, PLCs have become computational efficient and powerful, and a robust platform with applications beyond the standard control and factory automation. Due to the new advanced PLC's features and computational power, they are ideal platforms for exploring advanced modeling and control methods, including computational intelligence based techniques such as neural networks, particle swarm optimization (PSO) and many others. Some of these techniques require fast floating-point calculations that are now possible in real-time on the PLC. This paper focuses on the Allen-Bradley ControlLogix brand of PLCs, due to their high performance and extensive use in industry. The design and implementation of a neurocontroller consisting of two neural networks, one for modeling and the other for control, and the training of these neural networks with particle swarm optimization is presented in this paper on a single PLC. The neurocontroller in this study is a power system stabilizer (PSS) that is used for power system oscillation damping. The PLC is interfaced to a power system simulated on the real time digital simulator. Real time results are presented showing that the PLC is a suitable hardware platform for implementing advanced modeling and control techniques for industrial applications.",https://ieeexplore.ieee.org/document/4658952/,2008 IEEE Industry Applications Society Annual Meeting,5-9 Oct. 2008,ieeexplore
10.1109/ROBIO.2011.6181679,"Robot self-preservation and adaptation to user preferences in game play, a preliminary study",IEEE,Conferences,"It is expected that in a near future, personal robots will be endowed with enough autonomy to function and live in an individual's home. This is while commercial robots are designed with default configuration and factory settings which may often be different to an individual's operating preferences. This paper presents how reinforcement learning is applied and utilised towards personalisation of a robot's behaviour. Two-level reinforcement learning has been implemented: first level is in charge of energy autonomy, i.e. how to survive, and second level is involved in adapting robot's behaviour to user's preferences. In both levels Q-learning algorithm has been applied. First level actions have been learnt in a simulated environment and then the results have been transferred to the real robot. Second level has been fully implemented in the real robot and learnt by human-robot interaction. Finally, experiments showing the performance of the system are presented.",https://ieeexplore.ieee.org/document/6181679/,2011 IEEE International Conference on Robotics and Biomimetics,7-11 Dec. 2011,ieeexplore
10.1109/eScience.2019.00014,SATVAM: Toward an IoT Cyber-Infrastructure for Low-Cost Urban Air Quality Monitoring,IEEE,Conferences,"Air pollution is a public health emergency in large cities. The availability of commodity sensors and the advent of Internet of Things (IoT) enable the deployment of a city-wide network of 1000's of low-cost real-time air quality monitors to help manage this challenge. This needs to be supported by an IoT cyber-infrastructure for reliable and scalable data acquisition from the edge to the Cloud. The low accuracy of such sensors also motivates the need for data-driven calibration models that can accurately predict the science variables from the raw sensor signals. Here, we offer our experiences with designing and deploying such an IoT software platform and calibration models, and validate it through a pilot field deployment at two mega-cities, Delhi and Mumbai. Our edge data service is able to even-out the differential bandwidths from the sensing devices and to the Cloud repository, and recover from transient failures. Our analytical models reduce the errors of the sensors from a best-case of 63% using the factory baseline to as low as 21%, and substantially advances the state-of-the-art in this domain.",https://ieeexplore.ieee.org/document/9041703/,2019 15th International Conference on eScience (eScience),24-27 Sept. 2019,ieeexplore
10.1109/I2MTC50364.2021.9460075,SNR-based Reinforcement Learning Rate Adaptation for Time Critical Wi-Fi Networks: Assessment through a Calibrated Simulator,IEEE,Conferences,"Nowadays, the Internet of Things is spreading in several different research fields, such as factory automation, instrumentation and measurement, and process control, where it is referred to as Industrial Internet of Things. In these scenarios, wireless communication represents a key aspect to guarantee the required pervasive connectivity required. In particular, Wi-Fi networks are revealing ever more attractive also in time- and mission-critical applications, such as distributed measurement systems. Also, the multi-rate support feature of Wi-Fi, which is implemented by rate adaptation (RA) algorithms, demonstrated its effectiveness to improve reliability and timeliness. In this paper, we propose an enhancement of RSIN, which is a RA algorithm specifically conceived for industrial real-time applications. The new algorithm starts from the assumption that an SNR measure has been demonstrated to be effective to perform RA, and bases on Reinforcement Learning techniques. In detail, we start from the design of the algorithm and its implementation on the OmNet++ simulator. Then, the simulation model is adequately calibrated exploiting the results of a measurement campaign, to reflect the channel behavior typical of industrial environments. Finally, we present the results of an extensive performance assessment that demonstrate the effectiveness of the proposed technique.",https://ieeexplore.ieee.org/document/9460075/,2021 IEEE International Instrumentation and Measurement Technology Conference (I2MTC),17-20 May 2021,ieeexplore
10.1109/IS3C50286.2020.00134,Screw defect detection system based on AI image recognition technology,IEEE,Conferences,"In the past ten years, smart manufacturing has been widely discussed and gradually introduced into various manufacturing fields. Since Germany proposed the concept of “Industry 4.0” in 2011, it has been spreading and feverish all over the world. For Industry 4.0, information digitization, intelligent defect detection and database platform management are their main core technologies. Aiming at a large screw industry manufacturing field in central and southern Taiwan, this paper proposes a screw defect detection system based on AI image recognition technology to detect damage to the nut during the “molding” process in the screw production process, and it is determined whether the inspected screw passes the inspection. The recognition result is given as shown in Figure 1. This paper uses 500 non-defective screw samples and 20 defective screw samples provided by the screw factory. The above samples collected real-time images through the sampling structure designed in this article, and we adopt Microsoft Corporation's ML.NET suite to model AI images, and uses the following four deep learning models: ResNetV2 50, ResNetV2 101, InceptionV3, MoblieNetV2 for learning; in the process of learning, this article divides the data set into three types of data sets (one is the unknown set that is not used for training but mixed with correct and defective samples, and the other is used for post-training verification of mixed samples with correct and defective samples. The third is a training set for training a mixture of correct and defective samples) This arrangement is used for subsequent verification models; after training, a PC-based screw defect detection system is implemented as shown in Figure 2; finally, with Detect screw defects in the form of instant photography. After the experiment, in 1,000 repeated tests, the success rate of defect detection reached 97%, while the false positive rate was only 2%.",https://ieeexplore.ieee.org/document/9394116/,"2020 International Symposium on Computer, Consumer and Control (IS3C)",13-16 Nov. 2020,ieeexplore
10.1109/APPEEC.2009.4918747,Similarity Analysis in Condition Evolution Rule of Transformer in Family Based on Clustering,IEEE,Conferences,"In integrated condition assessment, family quality is an factor affecting a transformer's condition. If some devices in family have had default record, then the other transformer in family would have same default in future. And now, the affecting degree by family default factor is subjectively decided by expert's experience. This paper collected power transformer experimental data in same factory and with same type, analyzed condition evolution similarity of power transformer in family based on clustering technology to mine the potential evolution rule. To make the clustering result more accurate, this paper improved the similarity criterion in clustering algorithm, proposed line slope distance of condition evolution as line shape similarity criterion, used both data distance criterion and line slope distance criterion to cluster transformer experiment data with same factory and same type in reality. It then analyzed the condition evolution of a power transformer according to the family condition evolution rule. The result is the same with the reality.",https://ieeexplore.ieee.org/document/4918747/,2009 Asia-Pacific Power and Energy Engineering Conference,27-31 March 2009,ieeexplore
10.1109/ICMLC.2017.8108967,The RFID-based real-time monitoring system and the management algorithm of RFIDs,IEEE,Conferences,"The RFID-based real-time monitoring system has become one of the most important system in the factory. As it can provide managers an easy way to monitor the product, the overall equipment effectiveness or even the condition of machines. However, many industries just import this system into their factories in recent years and have no idea how to manage this system. Hence, this paper reports on an approach for monitoring the manufacturing process of a factory by using RFID tags. As using RFID tags may meet the problem of reusing the tags, we also develop in this work a novel algorithm for managing RFIDs. Experiment results demonstrate the validity of the proposed approach.",https://ieeexplore.ieee.org/document/8108967/,2017 International Conference on Machine Learning and Cybernetics (ICMLC),9-12 July 2017,ieeexplore
10.1109/CSSE.2008.1161,The Research on Data Resources Sharing of Water Resources Domain Based on Proactive Service,IEEE,Conferences,"It is a key issue to the distributed and heterogeneous domain data resource sharing. In this paper a new mode of water resources data sharing is proposed from the point of proactive service. At first, a data sharing framework, called FDSWRDPS, is provided based on SOA and domain ontology. Then service factory is used for a design pattern of data integration subsystem. Furthermore proactive service is implemented based on event-trigger mechanism. Integrated domain data can be pushed to users automatically by using FDSWRDPS.",https://ieeexplore.ieee.org/document/4722706/,2008 International Conference on Computer Science and Software Engineering,12-14 Dec. 2008,ieeexplore
10.1109/AIVR.2018.00024,The Virtual Factory: Hologram-Enabled Control and Monitoring of Industrial IoT Devices,IEEE,Conferences,"Augmented reality (AR) has been exploited in manifold fields but is yet to be used at its full potential. With the massive diffusion of smart devices, opportunities to build immersive human-computer interfaces are continually expanding. In this study, we conceptualize a virtual factory: an interactive, dynamic, holographic abstraction of the physical machines deployed in a factory. Through our prototype implementation, we conducted a user-study driven evaluation of holographic interfaces compared to traditional interfaces, highlighting its pros and cons. Our study shows that the majority of the participants found holographic manipulation more attractive and natural to interact with. However, current performance characteristics of head-mounted displays must be improved to be applied in production.",https://ieeexplore.ieee.org/document/8613643/,2018 IEEE International Conference on Artificial Intelligence and Virtual Reality (AIVR),10-12 Dec. 2018,ieeexplore
10.1109/ROBOT.1990.126044,Towards a real-time architecture for obstacle avoidance and path planning in mobile robots,IEEE,Conferences,"The design and partial implementation of a real-time architecture for a mobile robot, aimed particularly towards a vehicle developed for factory automation, is described. The authors develop a layered design to equip the robot with a number of behavioral competences. They examine sensing and a potential field algorithm especially to achieve modification of behavior at a speed close to the robot's operational speed. It is shown how the layered architecture interfaces to the original onboard architecture, which provided sophisticated localization but no ability to deal with environmental exceptions.&lt;<ETX>&gt;</ETX>",https://ieeexplore.ieee.org/document/126044/,"Proceedings., IEEE International Conference on Robotics and Automation",13-18 May 1990,ieeexplore
10.1109/ICSTCC.2019.8885611,Visual Analytics Framework for Condition Monitoring in Cyber-Physical Systems,IEEE,Conferences,"One of the biggest challenges facing the factory of the future today is to reduce the time-to-market access and increase through the improvement of competitiveness and efficiency. In order to achieve this target, data analytics in Industrial Cyber-Physical System becomes a feasible option. In this paper, a visual analytics framework for condition monitoring of the machine tool is presented with the aim to manage events and alarms at factory level. The framework is assessed in a particular use case that consists in a multi-threaded cloud-based solution for the global analysis of the behaviour of variables acquired from PLC, CNC and robot manipulator. A human-machine interface is also designed for the real-time visualization of the key performance indicators according to the user's criteria. This tool implemented is a great solution for condition monitoring and decision-making process based on data analytics from simple statistics to complex machine learning methods. The results achieved are part of the vision and implementation of the industrial test bed of “Industry and Society 5.0” platform.",https://ieeexplore.ieee.org/document/8885611/,"2019 23rd International Conference on System Theory, Control and Computing (ICSTCC)",9-11 Oct. 2019,ieeexplore
10.1109/TIM.2019.2962565,A Lightweight Appearance Quality Assessment System Based on Parallel Deep Learning for Painted Car Body,IEEE,Journals,"The appearance quality assessment based on defect inspection for painted car-body surfaces is an essential work to monitor and analyze the level of paint appearance quality. In the industrial application, there are some challenges, such as the huge and stereo skeleton of car bodies, a variety of irregular local surface areas, low visibility of defects due to tiny real size, and specular car-body surface. To overcome these problems, a lightweight online appearance quality assessment system (OAQAS) based on parallel deep learning is proposed, it includes two parts: 1) a vision inspection subsystem with distributed multi-camera image acquisition module and 2) an appearance quality evaluation subsystem (AQES) based on parallel TinyDefectRNet for evaluating the proposed painted surface grinding difficulty criteria. TinyDefectRNet is able to inspect relatively accurate defect size, although it is trained on a coarsely annotated data set. The OAQAS is implemented in an actual painting production line of a car factory, and the application results show that our OAQAS is far superior to the manual inspection in evaluation accuracy and time consumption. Moreover, our system is lightweight so that it is easy to be plugged into existing painting production lines without rebuilding or changing the inspection room.",https://ieeexplore.ieee.org/document/8964458/,IEEE Transactions on Instrumentation and Measurement,Aug. 2020,ieeexplore
10.1109/ACCESS.2017.2770180,An Integrated Industrial Ethernet Solution for the Implementation of Smart Factory,IEEE,Journals,"Smart factory addresses the vertical integration of physical entities and information systems. Network and cloud are two essential infrastructures to achieve this goal. Among them, the network provides interconnection for communication and data flow, while the cloud provides powerful and elastic computing and storage abilities for big data and intelligent applications. This paper presents a cloud-centric framework for the implementation of the smart factory. Based on this framework, three high leveled protocols viz., EtherCAT, DDS, and OPC UA are selected to implement machine network (controller to sensors/actuators communication), machine to machine communication, and machine to cloud communication respectively, to satisfy diverse communication requirements of the smart factory. An integrated architecture for combining ontology knowledge and semantic data to support intelligent applications is also proposed. These network and data process schemes are verified in a smart factory prototype for personalized candy packing application.",https://ieeexplore.ieee.org/document/8096995/,IEEE Access,2017,ieeexplore
10.1109/TCPMT.2020.3047089,Automatic Industry PCB Board DIP Process Defect Detection System Based on Deep Ensemble Self-Adaption Method,IEEE,Journals,"A deep ensemble convolutional neural network (CNN) model to inspect printed circuit board (PCB) board dual in-line package (DIP) soldering defects with Hybrid-YOLOv2 (YOLOv2 as a foreground detector and ResNet-101 as a classifier) and Faster RCNN with ResNet-101 and Feature Pyramid Network (FPN) (FRRF) achieved a detection rate of 97.45% and a false alarm rate (FAR) of 20%-30% in the previous study [34]. However, applying the method to other production lines, environmental variations, such as lighting, orientations of the sample feeds, and mechanical deviations, led to the degradation in detection performance. This article proposes an effective self-adaption method that collects “exception data” like the samples with which the Artificial Intelligent (AI) model made mistakes from the automated optical inspection inference edge to the training server, retraining with exceptions on the server and deploying back to the edge. The proposed defect detection system has been verified with real tests that achieved a detection rate of 99.99% with an FAR 20%-30% and less than 15 s of inspection time on a resolution $7296 \times 6000$ PCB image. The proposed system has proven capable of shortening inspection and repair time for online operators, where a 33% efficiency boost from the three production lines of the collaborated factory has been reported [6]. The contribution of the proposed retraining mechanism is threefold: 1) because the retraining process directly learns from the exceptions, the model can quickly adapt to the characteristic of each production line, leading to a fast and reliable mass deployment; 2) the proposed retraining mechanism is a necessary self-service for conventional users as it incrementally improves the detection performance without professional guidance or fine-tuning; and 3) the semiautomatic exception data collection method helps to reduce the time-consuming manual labeling during the retraining process.",https://ieeexplore.ieee.org/document/9306873/,"IEEE Transactions on Components, Packaging and Manufacturing Technology",Feb. 2021,ieeexplore
10.1109/ACCESS.2020.3036769,Drill Fault Diagnosis Based on the Scalogram and Mel Spectrogram of Sound Signals Using Artificial Intelligence,IEEE,Journals,"In industry, the ability to detect damage or abnormal functioning in machinery is very important. However, manual detection of machine fault sound is economically inefficient and labor-intensive. Hence, automatic machine fault detection (MFD) plays an important role in reducing operating and personnel costs compared to manual machine fault detection. This research aims to develop a drill fault detection system using state-of-the-art artificial intelligence techniques. Many researchers have applied the traditional approach design for an MFD system, including handcrafted feature extraction of the raw sound signal, feature selection, and conventional classification. However, drill sound fault detection based on conventional machine learning methods using the raw sound signal in the time domain faces a number of challenges. For example, it can be difficult to extract and select good features to input in a classifier, and the accuracy of fault detection may not be sufficient to meet industrial requirements. Hence, we propose a method that uses deep learning architecture to extract rich features from the image representation of sound signals combined with machine learning classifiers to classify drill fault sounds of drilling machines. The proposed methods are trained and evaluated using the real sound dataset provided by the factory. The experiment results show a good classification accuracy of 80.25 percent when using Mel spectrogram and scalogram images. The results promise significant potential for using in the fault diagnosis support system based on the sounds of drilling machines.",https://ieeexplore.ieee.org/document/9252126/,IEEE Access,2020,ieeexplore
10.1109/TII.2019.2937876,Federated Tensor Mining for Secure Industrial Internet of Things,IEEE,Journals,"In a vertical industry alliance, Internet of Things (IoT) deployed in different smart factories are similar. For example, most automobile manufacturers have the similar assembly lines and IoT surveillance systems. It is common to observe the industrial knowledge using deep learning and data mining methods based on the IoT data. However, some knowledge is not easy to be mined from only one factory's data because the samples are still few. If multiple factories within an alliance can gather their data together, more knowledge could be mined. However, the key concern of these factories is the data security. Existing matrix-based methods can guarantee the data security inside a factory but do not allow the data sharing among factories, and thus their mining performance is poor due to lack of correlation. To address this concern, in this article we propose the novel federated tensor mining (FTM) framework to federate multisource data together for tensor-based mining while guaranteeing the security. The key contribution of FTM is that every factory only needs to share its ciphertext data for security issue, and these ciphertexts are adequate for tensor-based knowledge mining due to its homomorphic attribution. Real-data-driven simulations demonstrate that FTM not only mines the same knowledge compared with the plaintext mining, but also is enabled to defend the attacks from distributed eavesdroppers and centralized hackers. In our typical experiment, compared with the matrix-based privacy-preserving compressive sensing (PPCS), FTM increases up to 24% on mining accuracy.",https://ieeexplore.ieee.org/document/8815886/,IEEE Transactions on Industrial Informatics,March 2020,ieeexplore
10.1109/ACCESS.2018.2809681,Industrial Big Data Analytics for Prediction of Remaining Useful Life Based on Deep Learning,IEEE,Journals,"Due to the recent development of cyber-physical systems, big data, cloud computing, and industrial wireless networks, a new era of industrial big data is introduced. Deep learning, which brought a revolutionary change in computer vision, natural language processing, and a variety of other applications, has significant potential for solutions providing in sophisticated industrial applications. In this paper, a concept of device electrocardiogram (DECG) is presented, and an algorithm based on deep denoising autoencoder (DDA) and regression operation is proposed for the prediction of the remaining useful life of industrial equipment. First, the concept of electrocardiogram is explained. Then, a problem statement based on manufacturing scenario is presented. Subsequently, the architecture of the proposed algorithm called integrated DDA and the algorithm workflow are provided. Moreover, DECG is compared with traditional factory information system, and the feasibility and effectiveness of the proposed algorithm are validated experimentally. The proposed concept and algorithm combine typical industrial scenario and advance artificial intelligence, which has great potential to accelerate the implementation of industry 4.0.",https://ieeexplore.ieee.org/document/8302913/,IEEE Access,2018,ieeexplore
10.1109/ACCESS.2020.3046784,Reinforcement Learning With Composite Rewards for Production Scheduling in a Smart Factory,IEEE,Journals,"Rapid advances of sensing and cloud technologies transform the manufacturing system into a data-rich environment and make production scheduling increasingly complex. Traditional offline scheduling methods are limited in the ability to handle low-volume-high-mix workorders with diverse design specifications. Simulation-based methods show the promise for distributed scheduling of manufacturing jobs but are mostly implemented with historical data and empirical rules in a static manner. Recently, artificial intelligence (AI) algorithms fuel increasing interests to solve dynamic scheduling problems in the manufacturing setting. However, it's difficult to utilize high-dimensional data for production scheduling while considering multiple practical objectives for smart manufacturing (e.g., minimize the makespan, reduce production costs, balance workloads). Therefore, this paper presents a new AI scheduler with composite reward functions for data-driven dynamic scheduling of manufacturing jobs under uncertainty in a smart factory. Internet-enabled sensor networks are deployed in the smart factory to track real-time statuses of workorders, machines, and material handling systems. A novel manufacturing value network is developed to take high-dimensional data as the input and then learn the state-action values for real-time decision making. Based on reinforcement learning (RL), composite rewards help the AI scheduler learn efficiently to achieve multiple objectives for production scheduling in real time. The proposed methodology is evaluated and validated with experimental studies in a smart manufacturing setting. Experimental results show that the new AI scheduler not only improves the multi-objective performance metrics in the production scheduling problem but also effectively copes with unexpected events (e.g., urgent workorders, machine failures) in manufacturing systems.",https://ieeexplore.ieee.org/document/9305707/,IEEE Access,2021,ieeexplore
10.1109/ACCESS.2021.3050484,Research on Detecting Bearing-Cover Defects Based on Improved YOLOv3,IEEE,Journals,"Detecting defects, which is a branch of target detection in the field of computer vision, is widely used in factory production. To solve the problems in existing detection algorithms that relate to their insensitivity to large or medium defect targets on bearing covers, their difficulty in detecting subtle defects effectively and their lack of real-time detection, in this work, we establish a large-scale bearing-cover defect dataset and propose an improved YOLOv3 network model. The proposed model is divided into four submodels: the bottleneck attention network (BNA-Net), the attention prediction subnet model, the defect localization subnet model, and the large-size output feature branch. To test the generality, robustness and practicability of the new model, we design a comparative experiment under abnormal illumination conditions. We design an ablation experiment to verify the validity of the proposed submodules. The experimental results show that our model solves the problem of the YOLOv3 algorithm's insensitivity to medium or large targets and satisfies real-time detection conditions. The mAP result is 69.74%, which is 16.31%, 13.4%, 13%, 10.9%, and 7.2% more than that of YOLOv3, EfficientDet-D2, YOLOv5, YOLOv4, and PP-YOLO, respectively.",https://ieeexplore.ieee.org/document/9319181/,IEEE Access,2021,ieeexplore
10.1109/TCYB.2016.2587673,Road Disturbance Estimation and Cloud-Aided Comfort-Based Route Planning,IEEE,Journals,"This paper investigates a comfort-based route planner that considers both travel time and ride comfort. We first present a framework of simultaneous road profile estimation and anomaly detection with commonly available vehicle sensors. A jump-diffusion process-based state estimator is developed and used along with a multi-input observer for road profile estimation. The estimation framework is evaluated in an experimental test vehicle and promising performance is demonstrated. Second, three objective comfort metrics are developed based on factors such as travel time, road roughness, road anomaly, and intersection. A comfort-based route planning problem is then formulated with these metrics and an extended Dijkstra's algorithm is exploited to solve the problem. A cloud-based implementation of our comfort-based route planning approach is proposed to facilitate information access and fast computation. Finally, a real-world case study, comfort-based route planning from Ford Research and Innovation Center, Michigan to Ford Rouge Factory Tour, Michigan, is presented to illustrate the efficacy of the proposed route planning framework.",https://ieeexplore.ieee.org/document/7514928/,IEEE Transactions on Cybernetics,Nov. 2017,ieeexplore
,The application of simulation technique in whole CIMS life cycle,BIAI,Journals,"The life cycle of CIMS (Computer Integrated Manufacturing System) includes four phases: requirement analyzing, designing, implementation and running. For reducing the risk of investment and achieving better economic results the simulation technique is needed in the above four phases of CIMS life cycle. Under the support of China 863 / CIMS plan a series of simulation projects are established. Some of them are finished with succeed and have been used in application. In this paper four simulation projects are introduced.(1) The Integrated Manufacturing Simulation Software (IMSS). It is an integrated platform, based on the discrete event simulation principle. It can be used to analyze and design CIMS, especially FMS, and evaluate the daily production plan.(2) The Advanced Hierarchical Control System Emulator (AHCSE), a software system, based on the finite state machine principle. It can be used to analyze and design of CIMS hierarchical control system, and check expanded system performance before expanding.(3) The Factory Scheduling Environment (FASE), a software system based on the discrete event simulation principle and artificial intelligence technology. It can be used for shop floor scheduling.(4) The Machining Process Simulator (MPS). It can simulate the machining process of machining center by computer. It can check the correctness of NC code (including interference and confliction) and replace the real machining center to support the simulation environment for shop floor scheduling and controlling. There are three companies and universities joining in these four projects, they are: Tsinghua University, Huazhong University of Technology, Beijing Institute of Computer Application and Simulation.",https://ieeexplore.ieee.org/document/6074468/,Journal of Systems Engineering and Electronics,Sept. 1994,ieeexplore
10.1109/ACCESS.2021.3082934,Visual Product Tracking System Using Siamese Neural Networks,IEEE,Journals,"Management of unstructured production data is a key challenge for Industry 4.0. Effective product tracking endorses data integration and productivity improvements throughout the manufacturing processes. Radio-frequency identification (RFID) tags are used in many tracking cases, but in some manufacturing environments, those cannot be used as they might get damaged or removed during processing. In this paper, we propose an alternative visual product tracking system. The physical system uses two cameras placed at the two ends of the tracked process(es). Product pairs are then matched with a Siamese neural network operating on the product images and trained offline on the problem at hand with labeled data. The proposed system can track products solely based on their visual appearance and without any physical interference with the products or production processes. Unlike other existing image-based methods, the proposed system is invariant to major positional and visual changes in the products. As a proof-of-concept, we tested the proposed system with real plywood factory data and were able to track the products with 98.5 % accuracy in a realistic test scenario. The implementation of the proposed method and the Veneer21 dataset are publicly available at https://github.com/TuomasJalonen/visual-product-tracking-system.",https://ieeexplore.ieee.org/document/9439511/,IEEE Access,2021,ieeexplore
10.1109/ACCC51160.2020.9347897,A Comparative Analysis of Kinematics of Industrial Robot KUKA KR 60–3 Using Scientific Computing Languages,IEEE,Conferences,"In the field of robotics, there are kinematic analysis methods that are responsible for describing the positions and orientations of the end effectors, as well as the angles, velocities and trajectories of industrial robots; such techniques are: forward kinematics, inverse kinematics and velocity kinematics. For the solutions of these complex mathematical calculations, the use of scientific computing languages or programs is required; which more and more algorithms, libraries and complements are implemented, that achieve a reduction in programming hours and result in the creation of better solutions in areas of all kinds. For this reason, the kinematics of the Industrial Robot KUKA KR 60-3 was programmed in the languages and programs most used in scientific computing, with the aim of comparing the performance (real time) when carrying out symbolic and numerical analysis in said studies.",https://ieeexplore.ieee.org/document/9347897/,2020 Asia Conference on Computers and Communications (ACCC),18-20 Sept. 2020,ieeexplore
10.1109/CYBER53097.2021.9588142,A Fast and Energy-Saving Neural Network Inference Method for Fault Diagnosis of Industrial Equipment Based on Edge-End Collaboration,IEEE,Conferences,"Data-driven fault diagnosis algorithms represented by deep learning have been widely used in industrial equipment fault diagnosis. However, the lack of real-time performance has always restricted the development of such methods. With the development of edge computing, many edge and end computing devices are deployed in industrial environments. For this distributed computing environment, we propose a distributed neural network inference method with edge-end collaboration. This method uses an edge server to cooperate with multiple end devices for network inference. In the diagnosis of industrial equipment, it can increase the speed of inference, reduce the traffic of the edge network, and help the application of deep neural networks in industrial environments.",https://ieeexplore.ieee.org/document/9588142/,"2021 IEEE 11th Annual International Conference on CYBER Technology in Automation, Control, and Intelligent Systems (CYBER)",27-31 July 2021,ieeexplore
10.1109/CAMAD50429.2020.9209305,A Joint Decentralized Federated Learning and Communications Framework for Industrial Networks,IEEE,Conferences,"Industrial wireless networks are pushing towards distributed architectures moving beyond traditional server-client transactions. Paired with this trend, new synergies are emerging among sensing, communications and Machine Learning (ML) co-design, where resources need to be distributed across different wireless field devices, acting as both data producers and learners. Considering this landscape, Federated Learning (FL) solutions are suitable for training a ML model in distributed systems. In particular, decentralized FL policies target scenarios where learning operations must be implemented collaboratively, without relying on the server, and by exchanging model parameters updates rather than training data over capacity-constrained radio links. This paper proposes a real-time framework for the analysis of decentralized FL systems running on top of industrial wireless networks rooted in the popular Time Slotted Channel Hopping (TSCH) radio interface of the IEEE 802.15.4e standard. The proposed framework is suitable for neural networks trained via distributed Stochastic Gradient Descent (SGD), it quantifies the effects of model pruning, sparsification and quantization, as well as physical and link layer constraints, on FL convergence time and learning loss. The goal is to set the fundamentals for comprehensive methods and procedures supporting decentralized FL pre-deployment design. The proposed tool can be thus used to optimize the deployment of the wireless network and the ML model before its actual installation. It has been verified based on real data targeting smart robotic-assisted manufacturing.",https://ieeexplore.ieee.org/document/9209305/,2020 IEEE 25th International Workshop on Computer Aided Modeling and Design of Communication Links and Networks (CAMAD),14-16 Sept. 2020,ieeexplore
10.1109/ICICRS46726.2019.9555893,A Novel Approach Towards Industrial Waste Management Using Q-Learning,IEEE,Conferences,"Industrial waste refers to the unwanted solid, liquid and gaseous wastes resulting from an industrial operation. And the collection, transport, processing, disposal and monitoring of these wastes is called as Industrial Waste Management. Industrial Waste has adhered to severe pollution to air, water and soil in the recent years affecting environment and human health. So, its proper and effective management has become important also as the industries’ liability towards the environment. Since many industries don’t have in-house processing plants, they tie-up with industries for the task. This paper focuses on using Q-learning for effective path-planning from generator industries(waste generating) to aggregator industries(waste processing). Q-learning is used for predicting the most efficient path by learning from its own experiences of rewards and penalties and so we don’t need to train the model which therefore increases the efficiency. This work can be implemented at a place where industrial waste is generated and can be very helpful in managing the same. This could be implemented as a service and handed over to the probable customers or government organizations which provide waste management services to the industries. This submission is a step towards automation of the Industrial Waste Management and helps in planning of waste management in real-time.",https://ieeexplore.ieee.org/document/9555893/,2019 International Conference on Intelligent Computing and Remote Sensing (ICICRS),19-20 July 2019,ieeexplore
10.1109/MCDM.2007.369428,A Review of Two Industrial Deployments of Multi-criteria Decision-making Systems at General Electric,IEEE,Conferences,"Two industrial deployments of multi-criteria decision-making systems at General Electric are reviewed from the perspective of their multi-criteria decision-making component similarities and differences. The motivation is to present a framework for multi-criteria decision-making system development and deployment. The first deployment is a financial portfolio management system that integrates hybrid multi-objective optimization and interactive Pareto frontier decision-making techniques to optimally allocate financial assets while considering multiple measures of return and risk, and numerous regulatory constraints. The second deployment is a power plant management system that integrates predictive modeling based on neural networks, optimization based on multi-objective evolutionary algorithms, and automated decision-making based on Pareto frontier techniques. The integrated approach, embedded in a real-time plant optimization and control software environment dynamically optimizes emissions and efficiency while simultaneously meeting load demands and other operational constraints in a complex real-world power plant",https://ieeexplore.ieee.org/document/4222994/,2007 IEEE Symposium on Computational Intelligence in Multi-Criteria Decision-Making,1-5 April 2007,ieeexplore
10.1109/SMARTCOMP52413.2021.00051,A Smart System for Personal Protective Equipment Detection in Industrial Environments Based on Deep Learning,IEEE,Conferences,"The adoption of real-time object detection systems via video streaming analysis is currently exploited in several contexts, from security monitoring to safety prevention. In industrial environments, proper usage of Personal Protective Equipment (PPE) is paramount to ensure workers’ safety. However, the use of some types of PPE, such as helmets, is often neglected by workers, especially in indoor areas. Thus, in order to reduce the risks of accidents, real-time video streaming-based monitoring systems may be used to monitor areas in which workers operate and alert them not to wear PPEs via acoustic alarms or visual signals. In case of a remote analysis, there are potential issues related to the high rate of data streams to be transported and analyzed and workers’ privacy. In this work, we propose an embedded smart system for real-time PPE detection based on video streaming analysis and deep learning models. We discuss the deployment of different versions of the YOLOv4 network fine-tuned using a public PPE dataset. In the end, we assess the performance of the proposed system in terms of accuracy and latency and of the overall PPE detection procedure.",https://ieeexplore.ieee.org/document/9556246/,2021 IEEE International Conference on Smart Computing (SMARTCOMP),23-27 Aug. 2021,ieeexplore
10.1109/PTC.2019.8810705,A Smart Voltage Optimization Approach for Industrial Load Demand Response,IEEE,Conferences,"This paper proposes a generic and comprehensive Voltage Optimization (VO) strategy for energy savings by industrial customers, to lower operating expenses through the implementation of an optimal process-based Demand Response (DR) program without affecting the real-time manufacturing process. This strategy takes into account the complex nature of industrial loads and their unique set of operating constraints, to reduce energy demand for industrial customers by means of varying the voltage at the utility service entrance to the plant. The proposed approach utilizes a Neural Network (NN) model of the industrial load, trained using historical operating data, to estimate the real power consumption of the load, based on the bus voltage and overall plant process. The NN load model is incorporated into the proposed VO model, whose objective is the minimization of the energy drawn from the substation and the number of switching operations of Load Tap Changers (LTC). The proposed VO framework is tested on a real plant model developed using actual measured data. The results demonstrate that the proposed technique can be successfully implemented by industrial customers and plant operators to enhance energy savings compared to Conservation Voltage Reduction (CVR) approaches, and also as a DR strategy that effectively manages the dependence of industrial loads on time-sensitive and critical manufacturing processes.",https://ieeexplore.ieee.org/document/8810705/,2019 IEEE Milan PowerTech,23-27 June 2019,ieeexplore
10.1109/INDIN.2013.6622897,A genetic algorithm for optimizing vector-based paths of industrial manipulators,IEEE,Conferences,"Nowadays there is a vast amount of IT tools specialized in vector graphics. The data generated by those tools could be used to describe the path of industrial manipulators as a set of vectors. The main problem is that the sequence/direction of those vectors is not meant to be executed by a robot and attempting to do it, would result in inefficient cycle times of the robot. Therefore it is necessary to generate an execution plan that minimizes the cost of carrying out the vector-based path. The number of possible execution actions has a factorial growth and it is unfeasible to evaluate each of them. This paper proposes the use of a genetic algorithm to optimize this task. The main contribution of this work is a chromosome encoding structure and modifications to the Partially Mapped Crossover operator in order to comply with the constraints of this optimization problem. The algorithm was implemented and tested in a real industrial manipulator.",https://ieeexplore.ieee.org/document/6622897/,2013 11th IEEE International Conference on Industrial Informatics (INDIN),29-31 July 2013,ieeexplore
,A neural network based optimization for wireless sensor node position estimation in industrial environments,IEEE,Conferences,"The sensor node position estimation is essential in wireless sensor networks. Among many localization schemes, the position estimations based on Received Signal Strength Indicator (RSSI) are mostly used in various systems and applications. However, RSSI data are highly affected from multipath propagation caused by the reflections from walls or objects. These reasons conduct the improper phenomena to radio signals. The significant variation of RSSI influences to the position estimation error especially in industrial environments. In this paper, we present a sensor node position estimation method in industrial environments and its optimization to reduce the error from multipath propagation by using neural networks. An experiment was performed in an electrical machine laboratory to evaluate the designed system in the real environment. The experimental results show that the average position error was reduced to 0.5 m.",https://ieeexplore.ieee.org/document/5491491/,"ECTI-CON2010: The 2010 ECTI International Confernce on Electrical Engineering/Electronics, Computer, Telecommunications and Information Technology",19-21 May 2010,ieeexplore
10.1109/DTPI52967.2021.9540189,A parallel intelligent control system and its industrial application,IEEE,Conferences,"Parallel control refers to the parallel interaction between the actual physical process and the manual calculation process. The ACP method under its theoretical framework includes artificial system, calculation experiment and parallel control. This paper presents a parallel intelligent control system implementation method, parallel control system needs a carrier, including hardware platform and software system, based on this carrier, the system completes artificial intelligence modeling and real-time optimal control. Firstly, the structure of parallel control platform is introduced, which is composed of industrial control computer, server and power supply, The program function of server is the core part of parallel control system;Secondly. The architecture of parallel intelligent control system is given, The artificial system is designed as an object modeling system, Industrial control computer output excitation signal, The server collects the response data and completes the modeling;The calculation experiment is designed as a process of human-computer interaction, which helps to realize control quality judgment and parameter setting;The parallel control is realized by the industrial controller, and the optimal parameters or control algorithm are put into the controller in parallel to realize the real-time control. Finally, an industrial application example is given to prove the effectiveness of this method.",https://ieeexplore.ieee.org/document/9540189/,2021 IEEE 1st International Conference on Digital Twins and Parallel Intelligence (DTPI),15 July-15 Aug. 2021,ieeexplore
10.1109/ICIAS.2007.4658557,A study on industrial communication networking: Ethernet based implementation,IEEE,Conferences,"Recent enhancement of the industrial communications and networking are possible to apply in Ethernet networks system at all levels of industrial automation, especially in the controller level whereby the data exchanges in real-time communication is mandatory. This paper is about a study on the development of industrial communications network based on the Ethernet protocol and thus implement it into computer integrated manufacturing (CIM) system. The purpose of this paper is to overcome real-time communication in which the accessibility of data exchange is very difficult in terms of retrieving data from other stations and time consuming. The Ethernet module is installed onto supervisory OMRON PLC to integrate several of stations in the CIM-70A system which is located at Robotic Laboratory in Universiti Tun Hussein Onn Malaysia (UTHM). The workability of this communication technique is analyzed and compared with the conventional serial communication which widely used in automation networking systems. It is found that, the Ethernet protocol approach through the communication and integration of CIM system can be accessed easily and available to be upgraded at the management and enterprise levels of industrial automation system.",https://ieeexplore.ieee.org/document/4658557/,2007 International Conference on Intelligent and Advanced Systems,25-28 Nov. 2007,ieeexplore
10.1109/INFOC.2017.8001669,A study on the fast system recovery: Selecting the number of surrogate nodes for fast recovery in industrial IoT environment,IEEE,Conferences,This paper is based on the previous research that selects the proper surrogate nodes for fast recovery mechanism in industrial IoT (Internet of Things) Environment which uses a variety of sensors to collect the data and exchange the collected data in real-time for creating added value. We are going to suggest the way that how to decide the number of surrogate node automatically in different deployed industrial IoT Environment so that minimize the system recovery time when the central server likes IoT gateway is in failure. We are going to use the network simulator to measure the recovery time depending on the number of the selected surrogate nodes according to the sub-devices which are connected to the IoT gateway.,https://ieeexplore.ieee.org/document/8001669/,2017 International Conference on Information and Communications (ICIC),26-28 June 2017,ieeexplore
10.1109/MetroInd4.0IoT51437.2021.9488498,Adaptive LoRaWAN Transmission exploiting Reinforcement Learning: the Industrial Case,IEEE,Conferences,"Wireless technologies play a key role in the Industrial Internet of Things (IIoT) scenario, for the development of increasingly flexible and interconnected factory systems. A significant opportunity in this context is represented by the advent of Low Power Wide Area Network (LPWAN) wireless technologies, that enable a reliable, secure, and effective transmission of measurement data over long communication ranges and with very low power consumption. Nevertheless, reliability in harsh environments (as typically occurs in the industrial scenario) is a significant issue to deal with. Focusing on LoRaWAN, adaptive strategies can be profitably devised concerning the above tradeoff. To this aim, this paper proposes to exploit Reinforcement Learning (RL) techniques to design an adaptive LoRaWAN strategy for industrial applications. The RL is spreading in many fields since it allows the design of intelligent systems using a stochastic discrete-time system approach. The proposed technique has been implemented within a purposely designed simulator, allowing to draw a preliminary performance assessment in a real-world scenario. A high density of independent nodes per square km has been considered, showing a significant improvement (about 10%) of the overall reliability in terms of data extraction rate (DER) without compromising full compatibility with the standard specifications.",https://ieeexplore.ieee.org/document/9488498/,2021 IEEE International Workshop on Metrology for Industry 4.0 & IoT (MetroInd4.0&IoT),7-9 June 2021,ieeexplore
10.1109/CIMSA.2006.250758,Adaptive Spatio-Spectral Hyperspectral Image Processing for Online Industrial Classification of Inhomogeneous Materials,IEEE,Conferences,"An approach for considering spatio-spectral information when classifying inhomogeneous materials in industrial environments is proposed. Its main application would be in the inspection and quality control tasks. They system core is an ANN based hyperspectral processing unit able to perform the online determination of the quality of the material based on its composition and grain size. A training adviser is being implemented in the system in order to automate the determination of the optimal spatial window size, as well as to reduce the number of spectral bands used and for determining the optimal spectral combination function through the automatic extraction of the discriminating features. Several tests have been carried out on synthetic and real data sets. In particular, the proposed approach is used to discriminate samples of andalusite having different purities; the results obtained show an accuracy of better than 98%",https://ieeexplore.ieee.org/document/4016840/,2006 IEEE International Conference on Computational Intelligence for Measurement Systems and Applications,12-14 July 2006,ieeexplore
10.1109/INDIN.2017.8104789,Addressing security challenges in industrial augmented reality systems,IEEE,Conferences,"In context of Industry 4.0 Augmented Reality (AR) is frequently mentioned as the upcoming interface technology for human-machine communication and collaboration. Many prototypes have already arisen in both the consumer market and in the industrial sector. According to numerous experts it will take only few years until AR will reach the maturity level to be deployed in productive applications. Especially for industrial usage it is required to assess security risks and challenges this new technology implicates. Thereby we focus on plant operators, Original Equipment Manufacturers (OEMs) and component vendors as stakeholders. Starting from several industrial AR use cases and the structure of contemporary AR applications, in this paper we identify security assets worthy of protection and derive the corresponding security goals. Afterwards we elaborate the threats industrial AR applications are exposed to and develop an edge computing architecture for future AR applications which encompasses various measures to reduce security risks for our stakeholders.",https://ieeexplore.ieee.org/document/8104789/,2017 IEEE 15th International Conference on Industrial Informatics (INDIN),24-26 July 2017,ieeexplore
10.1109/DCOSS49796.2020.00046,An Agnostic Data-Driven Approach to Predict Stoppages of Industrial Packing Machine in Near,IEEE,Conferences,"As data awareness in manufacturing companies increases with the deployment of sensors and Internet of Things (IoT) devices, data-driven maintenance and prediction have become quite popular in the Industry 4.0 paradigm. Machine Learning (ML) has been recognised as a promising, efficient and reliable tool for fault detection use cases, as it allows to export important knowledge from monitored assets. Scientists deal with issues such as the small amount of data that indicate potential problems, or the imbalance which exists between the standard process data and the data inadequacy of the systems to make a high precision forecast. Currently, in this context, even large industries are not able to effectively predict abnormal behaviors in their tools, processes and equipment, when adopting strategies to anticipate crucial events. In this paper, we propose a methodology to enable prediction of a packing machine's stoppages in manufacturing process of a large industry, by using forecasting techniques based on univariate time series data. There are more than 100 reasons that cause the machine to stop, in a quite big production line length. However, we use a single signal, concerning the machines operational status to make our prediction, without considering other fault or warning signals, hence its characterization as ""agnostic"". A workflow is presented for cleaning and preprocessing the data, and for training and evaluating a predictive model. Two predictive models, namely ARIMA and Prophet, are applied and evaluated on real data from an advanced machining process used for packing. Training and evaluation tests indicate that the results of the applied methods perform well on a daily basis. Our work can be further extended and act as reference for future research activities that could lead to more robust and accurate prediction frameworks.",https://ieeexplore.ieee.org/document/9183540/,2020 16th International Conference on Distributed Computing in Sensor Systems (DCOSS),25-27 May 2020,ieeexplore
10.1109/ICAPAI49758.2021.9462061,An Industrial Assistance System with Manual Assembly Step Recognition in Virtual Reality,IEEE,Conferences,"In the era of Industry 4.0, worker assistance systems are becoming more and more important. In order to assist shop floor workers in manual assembly tasks, we implemented an assistance system in virtual reality. A deep neural network was trained to recognize the current work step in real-time during an assembly process, thus giving the assistance system context-awareness. We defined the problem of assembly step recognition as a multivariate time series classification using the poses of the workers head, both hands and all relevant tools and objects. With this definition, the VR environment's output can also be replaced with data from the real world. For our proof-of-concept assembly step recognition system, we created an assembly process consisting of six different work steps, five movable assembly parts and one tool. We showed that we can train an activity recognition model for assembly steps with only 10 assembly recordings. To achieve this, we used multiple data augmentation techniques and proposed a novel method of synthesizing new training data, which we call Path Joining. With only 10 training recordings, we attain a categorical classification accuracy of 81 percent and with 60 recordings we achieve an accuracy of 89 percent.",https://ieeexplore.ieee.org/document/9462061/,2021 International Conference on Applied Artificial Intelligence (ICAPAI),19-21 May 2021,ieeexplore
10.1109/ICIT46573.2021.9453580,An Industrial HMI Temporal Adaptation based on Operator-Machine Interaction Sequence Similarity,IEEE,Conferences,"The incorporation of Artificial Intelligence (AI) into Industrial Environments has brought about a Smart Industry revolution, improving efficiency and simplifying complex industrial processes. However, these technological advances remain primarily focused on the process, and pay little attention to industrial Human-Machine Interfaces (HMI), the bridge between the operator and the industrial process.Current industrial HMIs have a static design, and are focused exclusively on the control and visualization of process information. They fail to take into account user behaviour and skills, information key to understanding how the operator interacts with the production process. Thus, the potential beneficial outcomes of considering operator-machine interaction in terms of efficiency and productivity, make a compelling case for industrial HMIs that can adapt to different operators based on their skills and process knowledge.This paper proposes a Machine Learning (ML) based method-ology capable of analysing operator-machine interaction and detecting the variability of interaction patterns for repetitive similar sequences in monitoring and control tasks. The method-ology generates a set of adaptation rules that improve Usability and User Experience, and hence operator working performance. To validate the proposed methodology, an experiment with real operators was conducted.",https://ieeexplore.ieee.org/document/9453580/,2021 22nd IEEE International Conference on Industrial Technology (ICIT),10-12 March 2021,ieeexplore
10.1109/ICCWorkshops49005.2020.9145434,An Inter-Disciplinary Modelling Approach in Industrial 5G/6G and Machine Learning Era,IEEE,Conferences,"Unlike conventional cellular systems, the fifth generation (5G) and beyond includes intrinsic support for vertical industries with diverse service requirements. Industrial process automation with autonomous fault detection and prediction, optimised operations and proactive control can be considered as one of the key verticals of 5G and beyond. Such applications enable equipping industrial plants with a reasoning sixth sense for optimised operations and fault avoidance. In this direction, we introduce an inter-disciplinary approach integrating wireless sensor networks with machine learning-enabled industrial plants to build a step towards developing this sixth sense technology, i.e., the reasoning ability. We develop a modular-based system that can be adapted to the vertical-specific elements. Without loss of generalisation, exemplary use cases are developed and presented including a fault detection/prediction scheme in a wireless communication network with sensors and actuators to enable the sixth sense technology with guaranteed service load requirements. The proposed schemes and modelling approach are implemented in a real chemical plant for testing purposes, and a high fault detection and prediction accuracy is achieved coupled with optimised sensor density analysis.",https://ieeexplore.ieee.org/document/9145434/,2020 IEEE International Conference on Communications Workshops (ICC Workshops),7-11 June 2020,ieeexplore
10.1109/SPAC53836.2021.9539933,An LSTM based Malicious Traffic Attack Detection in Industrial Internet,IEEE,Conferences,"Current Industrial Internet faces serious threats where attackers propagate malicious flows, resulting in communication failures in the Industrial Internet. In this work, we propose a practical and novel method to detect malicious traffic attack in real time with high accuracy. Our primary idea is to capture network flow, extract adequate network flow features, construct a long short-term memory (LSTM) based deep learning model, and identify the property of the corresponding network flow. Whether the network suffers attack or not is then determined according to the detection results. The corresponding prototype is also implemented in the Industrial Internet which is equipped with Software Defined Networking (SDN). Experimental results validate that the proposed method is effective in defending against malicious traffic attack in real-world network.",https://ieeexplore.ieee.org/document/9539933/,"2021 International Conference on Security, Pattern Analysis, and Cybernetics（SPAC)",18-20 June 2021,ieeexplore
10.1109/ASIC.1994.404614,An analog VLSI neural network for real-time image processing in industrial applications,IEEE,Conferences,"In this paper we present an analog VLSI architecture that implements a neural network for image processing in industrial environment. The analog architecture is highly modular and operates in real time. The circuit implementation is based on simple and effective circuit primitives. Special care has been devoted to the analysis of the linearity and the precision of computation. A test chip, implementing the filtering stage of the architecture, has been designed and realized.&lt;<ETX>&gt;</ETX>",https://ieeexplore.ieee.org/document/404614/,Proceedings Seventh Annual IEEE International ASIC Conference and Exhibit,19-23 Sept. 1994,ieeexplore
10.1109/INDIN.2005.1560420,An analogue recurrent neural network for trajectory learning and other industrial applications,IEEE,Conferences,"A real-time analogue recurrent neural network (RNN) can extract and learn the unknown dynamics (and features) of a typical control system such as a robot manipulator. The task at hand is a tracking problem in the presence of disturbances. With reference to the tasks assigned to an industrial robot, one important issue is to determine the motion of the joints and the effector of the robot. In order to model robot dynamics we use a neural network that can be implemented in hardware. The synaptic weights are modelled as variable gain cells that can be implemented with a few MOS transistors. The network output signals portray the periodicity and other characteristics of the input signal in unsupervised mode. For the specific purpose of demonstrating the trajectory learning capabilities, a periodic signal with varying characteristics is used. The developed architecture, however, allows for more general learning tasks typical in applications of identification and control. The periodicity of the input signal ensures convergence of the output to a limit cycle. Online versions of the synaptic update can be formulated using simple CMOS circuits. Because the architecture depends on the network generating a stable limit cycle, and consequently a periodic solution which is robust over an interval of parameter uncertainties, we currently place the restriction of a periodic format for the input signals. The simulated network contains interconnected recurrent neurons with continuous-time dynamics. The system emulates random-direction descent of the error as a multidimensional extension to the stochastic approximation. To achieve unsupervised learning in recurrent dynamical systems we propose a synapse circuit which has a very simple structure and is suitable for implementation in VLSI.",https://ieeexplore.ieee.org/document/1560420/,"INDIN '05. 2005 3rd IEEE International Conference on Industrial Informatics, 2005.",10-12 Aug. 2005,ieeexplore
10.1109/ICMA.2013.6618173,An intelligent object manipulation framework for industrial tasks,IEEE,Conferences,"This paper presents an intelligent object manipulation framework for industrial tasks, which integrates a sensor-rich multi-fingered robot hand, an industrial robot manipulator, a conveyor belt and employs machine learning algorithms. The framework software architecture is implemented using a Windows 7 operating system with RTX real-time extension for synchronous handling of peripheral devices. The framework uses Scale Invariant Feature Transform (SIFT) image processing algorithm, Support Vector Machine (SVM) machine learning algorithm and 3D point cloud techniques for intelligent object recognition based on RGB camera and laser rangefinder information from the robot hand end effector. The objective is automated manipulation of objects with different shapes and poses with minimum programming effort applied by a user.",https://ieeexplore.ieee.org/document/6618173/,2013 IEEE International Conference on Mechatronics and Automation,4-7 Aug. 2013,ieeexplore
10.1109/SYSCON.2018.8369547,An interactive architecture for industrial scale prediction: Industry 4.0 adaptation of machine learning,IEEE,Conferences,"According to wiki definition, there are four design principles in Industry 4.0. These principles support companies in identifying and implementing Industry 4.0 scenarios, namely, Interoperability, Information transparency, Technical assistance, Decentralized decisions. In this paper we have discussed our work on an implementation of a machine learning based interactive architecture for industrial scale prediction for dynamic distribution of water resources across the continent, keeping the four corners of Industry 4.0 in place. We report the possibility of producing most probable high resolution estimation regarding the water balance in any region within Australia by implementation of an intelligent system that can integrate spatial-temporal data from various independent sensors and models, with the ground truth data produced by 250 practitioners from the irrigation industry across Australia. This architectural implementation on a cloud computing platform linked with a freely distributed mobile application, allowing interactive ground truthing of a machine learning model on a continental scale, shows accuracy of 90% with 85% sensitivity of correct surface soil moisture estimation with end users at its complete control. Along with high level of information transparency and interoperability, providing on-demand technical supports and motivating users by allowing them to customize and control their own local predictive models, show the successfulness of principles in Industry 4.0 in real environmental issues in the future adaptation in various industries starting from resource management to modern generation soft robotics.",https://ieeexplore.ieee.org/document/8369547/,2018 Annual IEEE International Systems Conference (SysCon),23-26 April 2018,ieeexplore
10.1109/ETFA.2019.8869274,Analyzing availability and QoS of service-oriented cloud for industrial IoT applications,IEEE,Conferences,"Internet of Things and cloud services are one of main enablers in fourth industrial revolution. Real-time industrial systems have high availability requirements of 99.9% to 99.999% whereas architectures built on regional cloud services and IoT do not provide similar guarantees or Service Level Agreement. These differences of QoS and SLA availability between Operational Technology and Information Technology has become a main challenge in adoption of Industrial Internet of Things (IIoT) for real-time applications.This work presents an approach to find end-to-end QoS and availability for an IIoT architecture. Device-to-cloud, cloud-to-cloud and inside-cloud experiments have been performed over eight weeks where each experiment have more then four million QoS measurements. Our availability analysis shows that a remote IoT connected to a less busy cloud region gives higher availability as compared to an IoT device inside a busy cloud region. IIoT and regional cloud services provide good QoS with 99% to 99.9% availability for 1sec soft real-time requirements. In 100ms applications, more efforts are required to achieve higher then 95% availability and design industrial SLA. IIoT applications with 10sec latency like machine learning models can get 99.9% availability with cloud. Availability loss due to communication is almost 1% for 100ms applications. These results also provide requirements and future work of industrial edge computing for IIoT on real-time cloud.",https://ieeexplore.ieee.org/document/8869274/,2019 24th IEEE International Conference on Emerging Technologies and Factory Automation (ETFA),10-13 Sept. 2019,ieeexplore
10.1109/ICAC.2017.21,Ananke: A Q-Learning-Based Portfolio Scheduler for Complex Industrial Workflows,IEEE,Conferences,"Complex workflows that process sensor data are useful for industrial infrastructure management and diagnosis. Although running such workflows in clouds promises reduced operational costs, there are still numerous scheduling challenges to overcome. Such complex workflows are dynamic, exhibit periodic patterns, and combine diverse task groupings and requirements. In this work, we propose ANANKE, a scheduling system addressing these challenges. Our approach extends the state-of-the-art in portfolio scheduling for data centers with a reinforcement-learning technique, and proposes various scheduling policies for managing complex workflows. Portfolio scheduling addresses the dynamic aspect of the workload. Q-learning, allows our approach to adapt to the periodic patterns of the workload, and to tune the other configuration parameters. The proposed policies are heuristics that guide the provisioning process, and map workflow tasks to the provisioned cloud resources. Through real-world experiments based on real and synthetic industrial workloads, we analyze and compare our prototype implementation of ANANKE with a system without portfolio scheduling (baseline) and with a system equipped with a standard portfolio scheduler. Overall, our experimental results give evidence that a learning-based portfolio scheduler can perform better and consume fewer resources than state-of-the-art alternatives, in particular for workloads with uniform arrival patterns.",https://ieeexplore.ieee.org/document/8005354/,2017 IEEE International Conference on Autonomic Computing (ICAC),17-21 July 2017,ieeexplore
10.1109/ICOS.2011.6079285,Application of multi-step time series prediction for industrial equipment prognostic,IEEE,Conferences,The use of prognostics is critically to be implemented in industrial. This paper presents an application of multi-step time series prediction to support industrial equipment prognostic. An artificial neural network technique with sliding window is considered for the multi-step prediction which is able to predict the series of future equipment condition. The structure of prognostic application is presented. The feasibility of this prediction application was demonstrated by applying real condition monitoring data of industrial equipment.,https://ieeexplore.ieee.org/document/6079285/,2011 IEEE Conference on Open Systems,25-28 Sept. 2011,ieeexplore
10.1109/USBEREIT51232.2021.9455060,Applying of Recurrent Neural Networks for Industrial Processes Anomaly Detection,IEEE,Conferences,"The paper considers the issue of recurrent neural networks applicability for detecting industrial process anomalies to detect intrusion in Industrial Control Systems. Cyberattack on Industrial Control Systems often leads to appearing of anomalies in industrial process. Thus, it is proposed to detect such anomalies by forecasting the state of an industrial process using a recurrent neural network and comparing the predicted state with actual process' state. In the course of experimental research, a recurrent neural network with one-dimensional convolutional layer was implemented. The Secure Water Treatment dataset was used to train model and assess its quality. The obtained results indicate the possibility of using the proposed method in practice. The proposed method is characterized by the absence of the need to use anomaly data for training. Also, the method has significant interpretability and allows to localize an anomaly by pointing to a sensor or actuator whose signal does not match the model's prediction.",https://ieeexplore.ieee.org/document/9455060/,"2021 Ural Symposium on Biomedical Engineering, Radioelectronics and Information Technology (USBEREIT)",13-14 May 2021,ieeexplore
10.1109/UPEC.2017.8231939,Assessing the impact of load forecasting accuracy on battery dispatching strategies with respect to Peak Shaving and Time-of-Use (TOU) applications for industrial consumers,IEEE,Conferences,"Energy Storage Systems will play crucial role in controlling the grid of the future when increased penetration of renewable energy sources will take place. Especially batteries are expected to occupy a considerable share of the total energy storage market by simultaneously providing services to different stakeholders such as energy producers, transmission/distribution operators, residential, commercial and industrial consumers. Nowadays, Peak shaving and Time-of-Use applications are the most common services that standalone battery storage systems can provide to industrial consumers (without integrated PV-systems and/or wind turbines). A big part of the existing literature addressing such applications aims at developing an offline algorithm for optimal battery deployment based on a known load profile (or accurately predicted) without taking into consideration real time conditions. This paper investigates the impact of industrial load forecasting errors on dispatching strategies of battery storage systems on economically driven peak shaving and Time-of-Use applications. An artificial neural network has been developed and used as a prediction model of an industrial load profile. The neural network was trained, validated and tested on historical load data with time resolution of 15 minutes, provided by the local distribution operator of the Belgian electric grid. The performance of the neural network in terms of output-target regression and mean absolute error is 0.833 and 10.02% respectively. Afterwards, a simulation was carried out comparing four different scenarios of peak shaving. The results show that the prediction accuracy of the presented neural network is not competitive enough. Peak shaving based on predicted profiles becomes reliable for lower forecasting errors. For this purpose, further access into the process and types of loads of the user is required in order to come up with a more sophisticated prediction model.",https://ieeexplore.ieee.org/document/8231939/,2017 52nd International Universities Power Engineering Conference (UPEC),28-31 Aug. 2017,ieeexplore
10.1109/SMC.2013.819,Automated Sound Signalling Device Quality Assurance Tool for Embedded Industrial Control Applications,IEEE,Conferences,This paper presents a novel system for automatic detection and recognition of faulty audio signaling devices as part of an automated industrial manufacturing process. The system uses historical data labeled by human experts in detecting faulty signaling devices to train an artificial neural network based classifier for modeling their decision making process. The neural network is implemented on a real time embedded micro controller which can be more efficiently incorporated into an automated production line eliminating the need for a manual inspection within the manufacturing process. We present real world experiments based on data pertaining to the production and manufacture of audio signaling components used in car instrument clusters. Our results show that the proposed expert system is able to successfully classify faulty audio signaling devices to a high degree of accuracy. The results can be generalized to other signaling devices where an output signal is represented by a complex and changing frequency spectrum even with significant environmental noise.,https://ieeexplore.ieee.org/document/6722574/,"2013 IEEE International Conference on Systems, Man, and Cybernetics",13-16 Oct. 2013,ieeexplore
10.1109/ICIT.2018.8352157,Automatic parameter learning for easy instruction of industrial collaborative robots,IEEE,Conferences,"The manufacturing industry faces challenges in meeting requirements of flexibility, product variability and small batch sizes. Automation of high mix, low volume productions requires faster (re)configuration of manufacturing equipment. These demands are to some extend accommodated by collaborative robots. Certain actions can still be hard or impossible to manually adjust due to inherent process uncertainties. This paper proposes a generic iteratively learning approach based on Bayesian Optimisation to efficiently search for the optimal set of process parameters. The approach takes into account the process uncertainties by iteratively making a statistical founded choice on the next parameter-set to examine only based on the prior binomial outcomes. Moreover, our function estimator uses Wilson Score to make proper estimates on the success probability and the associated uncertain measure of sparsely sampled regions. The function estimator also generalises the experiment outcomes to the neighbour region through kernel smoothing by integrating Kernel Density Estimation. Our approach is applied to a real industrial task with significant process uncertainties, where sufficiently robust process parameters cannot intuitively be chosen. Using our approach, a collaborative robot automatically finds a reliable solution.",https://ieeexplore.ieee.org/document/8352157/,2018 IEEE International Conference on Industrial Technology (ICIT),20-22 Feb. 2018,ieeexplore
10.1109/NetSoft48620.2020.9165393,Benchmarking and Profiling 5G Verticals' Applications: An Industrial IoT Use Case,IEEE,Conferences,"The Industry 4.0 sector is evolving in a tremendous pace by introducing a set of industrial automation mechanisms tightly coupled with the exploitation of Internet of Things (IoT), 5G and Artificial Intelligence (AI) technologies. By combining such emerging technologies, interconnected sensors, instruments, and other industrial devices are networked together with industrial applications, formulating the Industrial IoT (IIoT) and aiming to improve the efficiency and reliability of the deployed applications and provide Quality of Service (QoS) guarantees. However, in a 5G era, efficient, reliable and highly performant applications' provision has to be combined with exploitation of capabilities offered by 5G networks. Optimal usage of the available resources has to be realised, while guaranteeing strict QoS requirements such as high data rates, ultra-low latency and jitter. The first step towards this direction is based on the accurate profiling of vertical industries' applications in terms of resources usage, capacity limits and reliability characteristics. To achieve so, in this paper we provide an integrated methodology and approach for benchmarking and profiling 5G vertical industries' applications. This approach covers the realisation of benchmarking experiments and the extraction of insights based on the analysis of the collected data. Such insights are considered the cornerstones for the development of AI models that can lead to optimal infrastructure usage along with assurance of high QoS provision. The detailed approach is applied in a real IIoT use case, leading to profiling of a set of 5G network functions.",https://ieeexplore.ieee.org/document/9165393/,2020 6th IEEE Conference on Network Softwarization (NetSoft),29 June-3 July 2020,ieeexplore
10.1109/ICAIIC51459.2021.9415189,Big Data Platform for Intelligence Industrial IoT Sensor Monitoring System Based on Edge Computing and AI,IEEE,Conferences,"The cutting edge of Industry 4.0 has driven everything to be converted to disruptive innovation and digitalized. This digital revolution is imprinted by modern and advanced technology that takes advantage of Big Data and Artificial Intelligence (AI) to nurture from automatic learning systems, smart city, smart energy, smart factory to the edge computing technology, and so on. To harness an appealing, noteworthy, and leading development in smart manufacturing industry, the modern industrial sciences and technologies such as Big Data, Artificial Intelligence, Internet of things, and Edge Computing have to be integrated cooperatively. Accordingly, a suggestion on the integration is presented in this paper. This proposed paper describes the design and implementation of big data platform for intelligence industrial internet of things sensor monitoring system and conveys a prediction of any upcoming errors beforehand. The architecture design is based on edge computing and artificial intelligence. To extend more precisely, industrial internet of things sensor here is about the condition monitoring sensor data - vibration, temperature, related humidity, and barometric pressure inside facility manufacturing factory.",https://ieeexplore.ieee.org/document/9415189/,2021 International Conference on Artificial Intelligence in Information and Communication (ICAIIC),13-16 April 2021,ieeexplore
10.1109/ICII.2018.00024,Brightics-IoT: Towards Effective Industrial IoT Platforms for Connected Smart Factories,IEEE,Conferences,"Industrial Internet-of-Things (IIoT) supports machines, computers and users to enable intelligent operations using advanced device management and data analytics. In recent years, thanks to standardized IoT platforms and advanced Artificial Intelligence (AI) technologies, there have been great advances in IIoT, and now it promises revolutions on various manufacturing domains such as transport, health, factory and energy. In this paper, based on our experience operating IIoT in various factory applications, we present the technical challenges of manufacturing facilities needed to be dealt with to collect huge amount of data in real-time and counteraction points of an IoT platform regarding these technical challenges and what kinds of features need to be implemented for intelligent services in the smart manufacturing. Finally, we introduce a story of applying industrial IoT platform in production to show how iterative development approaches can achieve business requirements based on elastically scaled-out architecture.",https://ieeexplore.ieee.org/document/8539113/,2018 IEEE International Conference on Industrial Internet (ICII),21-23 Oct. 2018,ieeexplore
10.1109/ICICTA.2017.93,CNN-Based Model for Pose Detection of Industrial PCB,IEEE,Conferences,"For applications in robot manipulate with object, get the pose of objects is very important for controller's subsequent operations, especially in PCB feeding and blanking field, the grasp success rate will be enhanced if robot can get a exact pose of objects that relative to end manipulator. So in this paper we utilize the CNN model to build on a neural network for 3 tasks: object recognition, location and pose detection. This model treat pose detection as a classification problem and try to combine recognition, location at the same level. To validate the performance of the multi-task detection model, experiments and analysis of the model performance was carried out by the real-time PCB detection test. In the experiment, we use the PCB dataset comprised of 3 types which contains different poses made by ourselves as train/test samples. The number of object pose categories was divided into 8bins, 12bins and 36bins according to pose detection precision. We analysis the effect of the non-uniform datasets on training process and the final detect results shows that this CNN-based detection model can achieve high accuracy on PCB pose detection.",https://ieeexplore.ieee.org/document/8089976/,2017 10th International Conference on Intelligent Computation Technology and Automation (ICICTA),9-10 Oct. 2017,ieeexplore
10.1109/ICNP.2016.7784407,Characterizing industrial control system devices on the Internet,IEEE,Conferences,"Industrial control system (ICS) devices with IP addresses are accessible on the Internet and play a crucial role for critical infrastructures like power grid. However, there is a lack of deep understanding of these devices' characteristics in the cyberspace. In this paper, we take a first step in this direction by investigating these accessible industrial devices on the Internet. Because of critical nature of industrial control systems, the detection of online ICS devices should be done in a real-time and non-intrusive manner. Thus, we first analyze 17 industrial protocols widely used in industrial control systems, and train a probability model through the learning algorithm to improve detection accuracy. Then, we discover online ICS devices in the IPv4 space while reducing the noise of industrial honeypots. To observe the dynamics of ICS devices in a relatively long run, we have deployed our discovery system on Amazon EC2 and detected online ICS devices in the whole IPv4 space for eight times from August 2015 to March 2016. Based on the ICS device data collection, we conduct a comprehensive data analysis to characterize the usage of ICS devices, especially in the answer to the following three questions: (1) what are the distribution features of ICS devices, (2) who use these ICS devices, and (3) what are the functions of these ICS devices.",https://ieeexplore.ieee.org/document/7784407/,2016 IEEE 24th International Conference on Network Protocols (ICNP),8-11 Nov. 2016,ieeexplore
10.1109/DESSERT50317.2020.9125038,Combination of Digital Twin and Artificial Intelligence in Manufacturing Using Industrial IoT,IEEE,Conferences,"The paper focuses on Digital Twin (DT) in Manufacturing using Artificial Intelligence (AI) and Industrial IoT. According to the concept, the manufacturing includes three main units: equipment, personnel and processes. All data from these units are inherited to manufacture model (DT) and decision support system with the use of AI. DT data technology allows finding the required knowledge that can be interpreted and used to support the process of decision-making in the management of the enterprise. AI applications open up a broad spectrum of opportunities in manufacturing to add value by optimizing processes and generating new business models. The Landscape was described by a formal model to assure the possibility to analyze the state and development of landscape in detail considering DT and other technologies. DT and IIoT implementation for the simulation of real enterprise manufacturing were considered.",https://ieeexplore.ieee.org/document/9125038/,"2020 IEEE 11th International Conference on Dependable Systems, Services and Technologies (DESSERT)",14-18 May 2020,ieeexplore
10.1109/MetroInd4.0IoT48571.2020.9138184,Combining exposure indicators and predictive analytics for threats detection in real industrial IoT sensor networks,IEEE,Conferences,"We present a framework able to combine exposure indicators and predictive analytics using AI-tools and big data architectures for threats detection inside a real industrial IoT sensors network. The described framework, able to fill the gaps between these two worlds, provides mechanisms to internally assess and evaluate products, services and share results without disclosing any sensitive and private information. We analyze the actual state of the art and a possible future research on top of a real case scenario implemented into a technological platform being developed under the H2020 ECHO project, for sharing and evaluating cybersecurity relevant informations, increasing trust and transparency among different stakeholders.",https://ieeexplore.ieee.org/document/9138184/,2020 IEEE International Workshop on Metrology for Industry 4.0 & IoT,3-5 June 2020,ieeexplore
10.23919/CNSM46954.2019.9012664,Communication Challenges and Solutions between Heterogeneous Industrial IoT Systems,IEEE,Conferences,"Industrial automation systems require communication technologies with high availability, high security and low latency. Accordingly, the current article addresses industrial-specific communication challenges, presenting some of the relevant solutions. In order to prove the usability of the presented technologies with sound results, this paper utilises a primarily Industrial IoT(IIoT) solution, the Arrowhead Framework, to experiment with communication capabilities between different IIoT clouds. The research is limited to three technologies from the LTE UE categories: Cat 3, Cat Ml and NB-IoT. The novelty of this paper is that it provides a set of experimental studies on applying different mobile networking technologies to support IIoT applications. The studies are based on our current, real-life measurements.",https://ieeexplore.ieee.org/document/9012664/,2019 15th International Conference on Network and Service Management (CNSM),21-25 Oct. 2019,ieeexplore
10.1109/GLOBECOM42002.2020.9348249,Communication-Efficient Federated Learning for Anomaly Detection in Industrial Internet of Things,IEEE,Conferences,"With the rapid development of the Industrial Internet of Things (IIoT), various IoT devices and sensors generate massive industrial sensing data. Sensing big data can be analyzed for insights that lead to better decisions and strategic industrial production by using advanced machine learning technologies. However, vulnerable IoT devices are easy to be compromised thus causing IoT devices failures (i.e., anomalies). The anomalies seriously affect the production of industrial products, thereby, it is increasingly important to accurately and timely detect anomalies. To this end, we first introduce a Federated Learning (FL) framework to enable decentralized edge devices to collaboratively train a Deep Anomaly Detection (DAD) model, which can improve its generalization ability. Second, we propose a Convolutional Neural Network-Long Short Term Memory (CNN-LSTM) model to accurately detect anomalies. The CNN-LSTM model uses CNN units to capture fine-grained features and retains the advantages of LSTM unit in predicting time series data. Third, to achieve real-time and lightweight anomaly detection in the proposed framework, a gradient compression mechanism is applied to reduce communication costs and improve communication efficiency. Extensive experiment results based on realworld datasets demonstrate that the proposed framework and mechanism can accurately and timely detect anomalies, and also reduce about 50% communication overhead when compared with traditional schemes.",https://ieeexplore.ieee.org/document/9348249/,GLOBECOM 2020 - 2020 IEEE Global Communications Conference,7-11 Dec. 2020,ieeexplore
10.1109/ICMLA.2015.183,Comparative Evaluation of Top-N Recommenders in e-Commerce: An Industrial Perspective,IEEE,Conferences,"We experiment on two real e-commerce datasets and survey more than 30 popular e-commerce platforms to reveal what methods work best for product recommendations in industrial settings. Despite recent academic advances in the field, we observe that simple methods such as best-seller lists dominate deployed recommendation engines in e-commerce. We find our empirical findings to be well-aligned with those of the survey, where in both cases simple personalized recommenders achieve higher ranking than more advanced techniques. We also compare the traditional random evaluation protocol to our proposed chronological sampling method, which can be used for determining the optimal time-span of the training history for optimizing the performance of algorithms. This performance is also affected by a proper hyperparameter tuning, for which we propose golden section search as a fast alternative to other optimization techniques.",https://ieeexplore.ieee.org/document/7424455/,2015 IEEE 14th International Conference on Machine Learning and Applications (ICMLA),9-11 Dec. 2015,ieeexplore
10.1109/MI-STA52233.2021.9464484,Comparison of PID and Artificial Neural Network Controller in on line of Real Time Industrial Temperature Process Control System,IEEE,Conferences,"Due to its simple structure and robustness, the traditional proportional-integral-derivative (PID) controller is commonly used in the field of industrial automation and process control, but it does not function well with nonlinear systems, time-delayed linear systems and time-varying systems. A new type of PID controller based on artificial neural networks and evolutionary algorithms is presented in this paper. An powerful instrument for a highly nonlinear system is the Artificial Neural Network. The interest in the study of the nonlinear system has increased through the implementation of a high-speed computer system,. In complex systems such as robotics and process control systems, the Neuro Control Algorithm is often applied. Systems of process management is also nonlinear and hard to control consistently.. This paper presents a comprehensive analysis in Which is offline trained by a multilayered feed forward back propagation neural network to act as a process control system controller, That is to say, a temperature control device without prior knowledge of its dynamics. Via the implementation of a range of input vectors to the neural network, the inverse dynamics model is developed. Based on these input vectors, the output of the neural network It is being studied by explicitly configuring it to monitor the operation. In this paper, based on set-point adjustment, impact of disturbances in load and variable dead time, compassion between the PID controller and ANN is conducted. The outcome shows that ANN outperforms the controller of the PID.",https://ieeexplore.ieee.org/document/9464484/,2021 IEEE 1st International Maghreb Meeting of the Conference on Sciences and Techniques of Automatic Control and Computer Engineering MI-STA,25-27 May 2021,ieeexplore
10.1109/IECON43393.2020.9255001,Computation Offloading for Machine Learning in Industrial Environments,IEEE,Conferences,"Industrial applications, such as real-time manufacturing, fault classification and inference, autonomous cars, etc., are data-driven applications that require machine learning with a wealth of data generated from industrial Internet of Things (IoT) devices. However, conventional approaches of transmitting this rich data to a remote data center to learn may be undesired due to the non-negligible network transmission delay and the sensitiveness of data privacy. By deploying a number of computing-capable devices at the network edge, edge computing supports the implementation of machine learning close to the industrial environment. Considering the heterogeneous computing capability as well as network location of edge devices, there are two types of feasible edge computing based machine learning models, including the centralized learning and federated learning models. In centralized learning, a resource-rich edge server aggregates the data from different IoT devices and performs machine learning. In federated learning, distributed edge devices and a federated server collaborate to perform machine learning. The features that data should be offloaded in centralized learning while it is locally trained in federated learning make centralized learning and federated learning quite different. We study the computation offloading problem for edge computing based machine learning in an industrial environment, considering the abovementioned machine learning models. We formulate a machine learning-based offloading problem with the goal of minimizing the training delay. Then, an energy-constrained delay-greedy (ECDG) algorithm is designed to solve the problem. Finally, simulation studies based on the MNIST dataset have been conducted to illustrate the efficiency of the proposal.",https://ieeexplore.ieee.org/document/9255001/,IECON 2020 The 46th Annual Conference of the IEEE Industrial Electronics Society,18-21 Oct. 2020,ieeexplore
10.1109/AITest.2019.00015,Constraint-Based Testing of An Industrial Multi-Robot Navigation System,IEEE,Conferences,"Intelligent multi-robot systems get more and more deployed in industrial settings to solve complex and repetitive tasks. Due to safety and economic reasons they need to operate dependably. To ensure a high degree of dependability, testing the deployed system has to be done in a rigorous way. Advanced multi-robot systems show a rich set of complex behaviors. Thus, these systems are difficult to test manually. Moreover, the space of potential environments and tasks for such systems is enormous. Therefore, methods that are able to explore this space in a structured way are needed. One way to address these issues is through model-based testing. In this paper we present an approach for testing the navigation system of a fleet of industrial transport robots. We show how all potential environments and navigation behaviors as well as requirements and restrictions can be represented in a formal constraint-based model. Moreover, we present the concept of coverage criteria in order to handle the potentially infinite space of test cases. Finally, we show how test cases can be derived from this model in an efficient way. In order to show the feasibility of the proposed approach we present an empirical evaluation of a prototype implementation using a real industrial use case.",https://ieeexplore.ieee.org/document/8718216/,2019 IEEE International Conference On Artificial Intelligence Testing (AITest),4-9 April 2019,ieeexplore
10.1109/INTERCON50315.2020.9220246,Convolutional neural networks for the Hass avocado classification using LabVIEW in an agro-industrial plant,IEEE,Conferences,"Peru is currently the world's third-largest exporter of Hass avocados according to the latest statistics from FAOSTAT. To classify avocados efficiently in size and maturity, a robust artificial intelligence plant was implemented to classify avocados into 5 categories. This grading technique differs from traditional grading in that it is non-invasive, reducing avocado damage by manually inspecting and grading. The plant comprises the step of hardware, consisting of Aca2500 Basler camera, lens HR 2mm/F1, illuminated, and the conveyor belt 1200. The step s7 PLC software: TIA PORTAL (OPC), a sequential algorithm, and convolutional neural network decision in which the selection parameters size and color of avocado include. The classification process fulfills three main stages: image acquisition, processing, and recognition. Convolutional neural networks were used for image treatment, obtaining an average classification precision of 60% in real-time. From the results obtained, we see that the classification can be improved.",https://ieeexplore.ieee.org/document/9220246/,"2020 IEEE XXVII International Conference on Electronics, Electrical Engineering and Computing (INTERCON)",3-5 Sept. 2020,ieeexplore
10.1109/EAEEIE.2013.6576535,Cooperation between industry and university based on the evaluation of the industrial research results in the academic environment,IEEE,Conferences,"Based on the European Structural Funds it was developed the Intelligent Mobile Box, Intelligent Panel Controller with intelligent adaptive controllers within the industrial research and experimental development in the company Kybernetes, s.r.o. Within the frame of the academic-industry cooperation, the intelligent adaptive controller was tested at the Department of Cybernetics and Artificial Intelligence, Technical university of Kosice, Slovakia. The tests of the mobile intelligent adaptive controller were performed on two levels of university study, on the Bachelor level on the exercises from the subject “Control of Technological Processes” and on the Engineering level the exercises from the subject “Intelligent Control Networks” and on one Diploma project. Goals of students of the Control of the technological processes course had two goals, firstly to connect the intelligent adaptive controller to pre-defined controlled system (real plant, real model or simulated model) and next to validate the control results. Students of the Diploma project on the Engineering level had more advanced goals. Tasks defined for engineering students were to connect the intelligent adaptive controller to non-defined controlled system, setup the adaptivity process of the controller regarding the learning error, parameterize the control system, observe and validate the control results. Both sides concluded this cooperation as very valuable. Main contributions for students were (U1) the challenge to apply studied theoretical knowledge on the real industrial controllers, (U2) experience with new research results and technologies deployed in industry and (U3) the implementation of the control and adaptive algorithms from abstract mathematical area to real PLC controller. On side of industry research company the main contributions were (C1) testing of designed algorithms and (C2) user feedback from students to make the application HMI interface more understandable a native.",https://ieeexplore.ieee.org/document/6576535/,2013 24th EAEEIE Annual Conference (EAEEIE 2013),30-31 May 2013,ieeexplore
10.1109/ETFA46521.2020.9211930,Data-Driven Industrial Human-Machine Interface Temporal Adaptation for Process Optimization,IEEE,Conferences,"The application of Artificial Intelligence (AI) into Industrial Human-Machine Interfaces (HMIs) moved old systems with physical buttons and analogue actuators into adaptive interaction models and context-based self adjusted interfaces. To date, little attention has been paid to industrial Human-Machine Interfaces (HMI) which play a vital role in the communication between operator and complex productive systems. Current industrial HMIs do not take into account operator behaviour, but rather focus on the production process. To enhance User Experience (UX) and improve performance it is necessary to adapt the interface to the needs of the operator. This paper proposes a Machine Learning (ML) based operator interaction Data-Driven methodology to extract a set of interface adaptation rules. The methodology optimizes the interaction by reducing the number of actions and hence the amount of time and possible errors in repetitive monitoring and control tasks. An experiment with real operators was conducted to validate the proposed approach. The system was able to extract their interaction patterns and propose temporal interface adaptations, leading to a personalized, adaptive and more effective interaction.",https://ieeexplore.ieee.org/document/9211930/,2020 25th IEEE International Conference on Emerging Technologies and Factory Automation (ETFA),8-11 Sept. 2020,ieeexplore
10.1109/ICSTCC.2019.8885434,Data–driven Neural Feedforward Controller Design for Industrial Linear Motors,IEEE,Conferences,"In this paper we consider the problem of feedforward controller design for industrial linear motors. These motors are safety-critical high-precision mechatronics systems that pose stringent requirements on the feedforward design: safe and predictable behavior for the desired motion profiles, tracking performance within the 10μm range in the presence of nonlinear friction and real-time implementation within the 1ms range. We investigate and compare several possibilities to design data- driven feedforward controllers using neural networks (NN) and we show that a two-step inverse estimation method is the most suitable approach, due to robustness to noisy data. We also show that basic knowledge about the system dynamics and the friction behavior can be exploited to design neural feedforward controllers with a simple structure, suitable for real-time implementation in industrial linear motors. The developed data-driven neural feedforward controllers are tested and compared with standard mass-acceleration feedforward and iterative learning controllers in realistic simulations.",https://ieeexplore.ieee.org/document/8885434/,"2019 23rd International Conference on System Theory, Control and Computing (ICSTCC)",9-11 Oct. 2019,ieeexplore
10.1109/IMCEC46724.2019.8984019,Deep Learning: Excellent Method at Surface Defect Detection of Industrial Products,IEEE,Conferences,"Surface defect detection of industrial products has always been an important part of the manufacturing industry. At present,there is a high false detection rate and low efficiency problem of traditional image processing algorithms which easy to be disturbed by complex background. Aiming at the above problems, a method for surface defect detection based on deep learning is proposed. YOLOv3 network adopted in this paper has great advantages in small target recognition and location of target in complex background. In addition, the train-set is effectively extended by elastic deformation and thin-plate spline algorithm. The experiment results show that the scratch recognition rate is as high as 95.8%, the over-judgment rate is 5.4%,and the missed rate is 1.3%.The method can identify the surface defects in a short time, and the average detection time does not exceed 0.4s, which can meet the real-time and precision requirements of industrial applications.",https://ieeexplore.ieee.org/document/8984019/,"2019 IEEE 3rd Advanced Information Management, Communicates, Electronic and Automation Control Conference (IMCEC)",11-13 Oct. 2019,ieeexplore
10.1109/ISIE45063.2020.9152441,Deployment of a Smart and Predictive Maintenance System in an Industrial Case Study,IEEE,Conferences,"Industrial manufacturing environments are often characterized as being stochastic, dynamic and chaotic, being crucial the implementation of proper maintenance strategies to ensure the production efficiency, since the machines' breakdown leads to a degradation of the system performance, causing the loss of productivity and business opportunities. In this context, the use of emergent ICT technologies, such as Internet of Things (IoT), machine learning and augmented reality, allows to develop smart and predictive maintenance systems, contributing for the reduction of unplanned machines' downtime by predicting possible failures and recovering faster when they occur. This paper describes the deployment of a smart and predictive maintenance system in an industrial case study, that considers IoT and machine learning technologies to support the online and real-time data collection and analysis for the earlier detection of machine failures, allowing the visualization, monitoring and schedule of maintenance interventions to mitigate the occurrence of such failures. The deployed system also integrates machine learning and augmented reality technologies to support the technicians during the execution of maintenance interventions.",https://ieeexplore.ieee.org/document/9152441/,2020 IEEE 29th International Symposium on Industrial Electronics (ISIE),17-19 June 2020,ieeexplore
10.1109/CDC.2001.980681,Design and implementation of industrial neural network controller using backstepping,IEEE,Conferences,"A novel neural network (NN) backstepping controller is modified for application to an industrial motor drive system. A control system structure and NN tuning algorithms are presented that are shown to guarantee the stability and performance of the closed-loop system. The NN backstepping controller is implemented on an actual motor drive system using a two-PC control system developed at the authors' university. The implementation results show that the NN backstepping controller is highly effective in controlling the industrial motor drive system. It is also shown that the NN controller gives better results on actual systems than a standard backstepping controller developed assuming full knowledge of the dynamics. Moreover, the NN controller does not require the linear-in-the-parameters assumption or the computation of regression matrices required by standard backstepping.",https://ieeexplore.ieee.org/document/980681/,Proceedings of the 40th IEEE Conference on Decision and Control (Cat. No.01CH37228),4-7 Dec. 2001,ieeexplore
10.1109/IAI50351.2020.9262203,Digital Twin Enabled Smart Control Engineering as an Industrial AI: A New Framework and Case Study,IEEE,Conferences,"In Industry 4.0, the increasing complexity of industrial systems introduces unknown dynamics that affect the performance of manufacturing processes. Thus, Digital Twin appears as a breaking technology to develop virtual representations of any complex system design, analysis, and behavior prediction tasks to enhance the system understanding via enabling capabilities like real-time analytics, or Smart Control Engineering. In this paper, a novel framework is proposed for the design and implementation of Digital Twin applications to the development of Smart Control Engineering. The framework involve the steps of system documentation, Multidomain Simulation, Behavioral Matching, and real-time monitoring, which is applied to develop the Digital Twin for a real-time vision feedback temperature uniformity control. The obtained results show that Digital Twin is a fundamental part of the transformation into Industry 4.0.",https://ieeexplore.ieee.org/document/9262203/,2020 2nd International Conference on Industrial Artificial Intelligence (IAI),23-25 Oct. 2020,ieeexplore
10.1109/AIKE.2018.00042,Distributed Osmotic Computing Approach to Implementation of Explainable Predictive Deep Learning at Industrial IoT Network Edges with Real-Time Adaptive Wavelet Graphs,IEEE,Conferences,"Challenges associated with developing analytics solutions at the edge of large scale Industrial Internet of Things (IIoT) networks close to where data is being generated in most cases involves developing analytics solutions from ground up. However, this approach increases IoT development costs and system complexities, delay time to market, and ultimately lowers competitive advantages associated with delivering next-generation IoT designs. To overcome these challenges, existing, widely available, hardware can be utilized to successfully participate in distributed edge computing for IIoT systems. In this paper, an osmotic computing approach is used to illustrate how distributed osmotic computing and existing low-cost hardware may be utilized to solve complex, compute-intensive Explainable Artificial Intelligence (XAI) deep learning problem from the edge, through the fog, to the network cloud layer of IIoT systems. At the edge layer, the C28x digital signal processor (DSP), an existing low-cost, embedded, real-time DSP that has very wide deployment and integration in several IoT industries is used as a case study for constructing real-time graph-based Coiflet wavelets that could be used for several analytic applications including deep learning pre-processing applications at the edge and fog layers of IIoT networks. Our implementation is the first known application of the fixed-point C28x DSP to construct Coiflet wavelets. Coiflet Wavelets are constructed in the form of an osmotic microservice, using embedded low-level machine language to program the C28x at the network edge. With the graph-based approach, it is shown that an entire Coiflet wavelet distribution could be generated from only one wavelet stored in the C28x based edge device, and this could lead to significant savings in memory at the edge of IoT networks. Pearson correlation coefficient is used to select an edge generated Coiflet wavelet and the selected wavelet is used at the fog layer for pre-processing and denoising IIoT data to improve data quality for fog layer based deep learning application. Parameters for implementing deep learning at the fog layer using LSTM networks have been determined in the cloud. For XAI, communication network noise is shown to have significant impact on results of predictive deep learning at IIoT network fog layer.",https://ieeexplore.ieee.org/document/8527474/,2018 IEEE First International Conference on Artificial Intelligence and Knowledge Engineering (AIKE),26-28 Sept. 2018,ieeexplore
10.1109/ISAECT50560.2020.9523700,Edge-Cloud Architectures Using UAVs Dedicated To Industrial IoT Monitoring And Control Applications,IEEE,Conferences,"The deployment of new technologies to ease the control and management of a massive data volume and its uncertainty is a very significant challenge in the industry. Under the name ""Smart Factory"", the Industrial Internet of Things (IoT) aims to send data from systems that monitor and control the physical world to data processing systems for which cloud computing has proven to be an important tool to meet processing needs. unmanned aerial vehicles (UAVs) are now being introduced as part of IIoT and can perform important tasks. UAVs are now considered one of the best remote sensing techniques for collecting data over large areas. In the field of fog and edge computing, the IoT gateway connects various objects and sensors to the Internet. It function as a common interface for different networks and support different communication protocols. Edge intelligence is expected to replace Deep Learning (DL) computing in the cloud, providing a variety of distributed, low-latency and reliable intelligent services. In this paper, An unmanned aerial vehicle is automatically integrated into an industrial control system through an IoT gateway platform. Rather than sending photos from the UAV to the cloud for processing, an AI cloud trained model is deployed in the IoT gateway and used to process the taken photos. This model is designed to overcome the latency channels of the cloud computing architecture. The results show that the monitoring and tracking process using advanced computing in the IoT gateway is significantly faster than in the cloud.",https://ieeexplore.ieee.org/document/9523700/,2020 International Symposium on Advanced Electrical and Communication Technologies (ISAECT),25-27 Nov. 2020,ieeexplore
10.1109/ICICT50816.2021.9358469,Efficient Fault Isolation Method to Monitor Industrial Batch Processes,IEEE,Conferences,Industrial batch processes are very popular manufacturing system with large number of process variables involved. Monitoring of batch processes using statistical process monitoring becomes very difficult in view of the complex correlations between the process variables. This paper focuses on a fault isolation based process monitoring method without prior information of fault where fault isolation problem is converted into a variable selection. Variable selection is a learning algorithm used here to solve the problem of selection and isolation of variables from a model. The method discussed here uses a sparse coefficient based dissimilarity analysis algorithm known as Sparse Dissimilarity Algorithm(SDISSIM) which checks a calculated D-index for identifying fault in the process. A sparse coefficient is tabulated to verify the process variables contributing to the fault and an absolute variance difference is calculated to select the variables for fault isolation. Finally SDISSIM method is explained by successful implementation in MATLAB with real time industrial process data.,https://ieeexplore.ieee.org/document/9358469/,2021 6th International Conference on Inventive Computation Technologies (ICICT),20-22 Jan. 2021,ieeexplore
10.1109/WFCS.2019.8757999,Exploiting localization systems for LoRaWAN transmission scheduling in industrial applications,IEEE,Conferences,"The Internet of Things (IoT) paradigm contaminated the industrial world. Wireless communications seem to be particularly attracting, especially when complement indoor and outdoor Real Time Location Systems (RTLS) for geo-referencing smart objects (e.g. for asset tracking). In this paper, the LoRaWAN solution is considered for long range transmission of RTLS data (LoRaWAN is an example of Low Power Wide Area Network). Given that the RTLSs use time synchronization, this work proposes to opportunistically obtain LoRaWAN Class A node time synchronization using the RTLS ranging devices. Once a common sense of time is shared in the LoRaWAN network, more efficient scheduled medium access strategies can be implemented. The experimental testbed, based on commercially available solutions, demonstrates the affordability and feasibility of the proposed approach. When low-cost GPS (outdoor) and UWB (indoor) ranging devices are considered, synchronization error of few microseconds can be easily obtained. The experimental results show the that time reference pulses disciplined by GPS have a maximum jitter of 180 ns and a standard deviation of 40 ns whereas, if time reference pulses disciplined by UWB are considered, the maximum jitter is 3.3 μs and the standard deviation is 0.7 μs.",https://ieeexplore.ieee.org/document/8757999/,2019 15th IEEE International Workshop on Factory Communication Systems (WFCS),27-29 May 2019,ieeexplore
10.1109/ICAIE50891.2020.00028,Exploration and Practice on Industrial Robot Experimental Teaching Based on Virtuality and Reality Combination,IEEE,Conferences,"In view of the common problems currently existing in the industrial robot experimental teaching in the higher vocational colleges, this paper, through deep integration of the traditional experimental teaching and the virtual simulation technology, proposes the industrial robot experimental teaching mode based on the combination of virtuality and reality. The teaching process is divided into three parts: pre-class, in-class and post-class, and the experimental teaching design is elaborated in details by taking the “material blocks handling experiment” as an example. Practices proved that the experimental teaching mode based on the virtuality and reality combination has improved the efficiency and quality of classroom teaching, enriched the after-school time of students and improved the depth and scope of learning, which played a positive role in promoting the specialty construction and talent cultivation, and could provide reference for the experimental teaching of related specialties in similar colleges and universities.",https://ieeexplore.ieee.org/document/9262587/,2020 International Conference on Artificial Intelligence and Education (ICAIE),26-28 June 2020,ieeexplore
10.1109/I2MTC43012.2020.9129595,Feature Ranking under Industrial Constraints in Continuous Monitoring Applications based on Machine Learning Techniques,IEEE,Conferences,"The design work-flow of machine learning techniques for continuous monitoring or predictive maintenance in an industrial context is usually a two step procedure: the selection of features to be computed from the observed signals and training of a suitable algorithm with real-life meaningful data, that will be next deployed in the second step. Feature selection is a relevant task since it provides a powerful optimisation of the deployed algorithm performance, for the given training data-set. The paper provides a method for feature ranking and selection that embeds constraints coming from real-life applications, including sensing device specifications, environmental noise, available processing resources, being all these latter aspects not considered in the currently available literature methods for feature selection. A practical case-study in the field on anomaly detection of machines is reported and discussed, in order to show the good properties of the provided method.",https://ieeexplore.ieee.org/document/9129595/,2020 IEEE International Instrumentation and Measurement Technology Conference (I2MTC),25-28 May 2020,ieeexplore
10.1109/COASE.2017.8256157,Full automatic path planning of cooperating robots in industrial applications,IEEE,Conferences,"Parts made of carbon fiber reinforced plastics (CFRP) for airplane components can be so huge that a single industrial robot is no longer able to handle them, and cooperating robots are required. Manual programming of cooperating robots is difficult, but with large numbers of different sized and shaped cut-pieces, it is almost impossible. This paper presents an automated production system consisting of a camera for the precise detection of the position of each cut-piece and a collision-free path planner which can dynamically react to different positions for the transfer motions. The path is planned for multiple robots adhering to motion constrains, such as the requirement that the textile cut-piece must form a catenary which can change during transport. Additionally a technique based on machine learning has been implemented which correctly resolves redundancy for a linear axis during planning. Finally, all components are tested on a real robot system in industrial scale.",https://ieeexplore.ieee.org/document/8256157/,2017 13th IEEE Conference on Automation Science and Engineering (CASE),20-23 Aug. 2017,ieeexplore
10.1109/ICNSC.2006.1673254,General Methodology for Action-Oriented Industrial Ecology Complex Systems Approach Applied to the Rotterdam Industrial Cluster,IEEE,Conferences,"A new approach for the understanding and shaping of the evolution of large scale socio-technical systems is presented. A proof-of-concept knowledge application has been developed, based on the industrial Rotterdam-Rijnmond case. The knowledge application includes the design of a model of industry-infrastructure evolution. Such networks are modeled via a system decomposition, formalization in an ontology and implementation of an agent based model. In simulation runs several network metrics are presented. The results provide insights in real world system behavior and show the validity and potential of the approach",https://ieeexplore.ieee.org/document/1673254/,"2006 IEEE International Conference on Networking, Sensing and Control",23-25 April 2006,ieeexplore
10.1109/TSP52935.2021.9522588,Genetic Programming based Identification of an Industrial Process,IEEE,Conferences,"In the field of industrial automation, it is essential to develop and improve mathematical methods that assist in obtaining more accurate models of real-world systems. In the following paper, a machine learning tool is applied to the problem of identifying a model of an industrial process. Symbolic regression and genetic programming are a successful combination of methods using which one can identify a nonlinear model in analytical form based on data collected from a process during routine operation. In this paper, a detailed description of the method implementation as well as necessary data preprocessing steps are presented. Then, the resulting models are validated on an industrial data set and compared on the basis of performance metrics with more classical methods and previous results achieved by the authors. Finally, the encountered problems in the realization of the methods are reflected upon.",https://ieeexplore.ieee.org/document/9522588/,2021 44th International Conference on Telecommunications and Signal Processing (TSP),26-28 July 2021,ieeexplore
10.1109/IDAACS-SWS.2018.8525503,Hybrid MAC for Low Latency Wireless Communication Enabling Industrial HMI Applications,IEEE,Conferences,"Wireless technologies are one of the core components of the future industrial applications. They provide flexibility and scalability to the factory floor in parallel with deployment cost reduction. In our paper, we concentrate on future-oriented human-machine interaction (HMI) applications such as augmented reality (AR) or mobile control. Based on their requirements, we provide an investigation of IEEE 802.11 channel access techniques with respect to their suitability for industrial applications.",https://ieeexplore.ieee.org/document/8525503/,2018 IEEE 4th International Symposium on Wireless Systems within the International Conferences on Intelligent Data Acquisition and Advanced Computing Systems (IDAACS-SWS),20-21 Sept. 2018,ieeexplore
10.1109/ICACC.2013.54,Implementation of CMNN Based Industrial Controller Using VxWorks RTOS Ported to MPC8260,IEEE,Conferences,"With the development of embedded Real Time Operating System (RTOS), dedicated controllers normally used to control single process loops are being replaced by shared controllers which are ported with RTOS running multiple control algorithms parallelly. This work demonstrates a Cerebral Model Neural Network (CMNN) based control algorithm being run as a real time application in MPC8260 (PowerPC) embedded processor with VxWorks RTOS. Process signals from the sensors are given to MPC8260 board through serial port and control signals transmitted to the actuator are displayed on a client system running Hyper-Terminal application.",https://ieeexplore.ieee.org/document/6686379/,2013 Third International Conference on Advances in Computing and Communications,29-31 Aug. 2013,ieeexplore
10.1109/ICPHYS.2019.8780271,Implementation of Industrial Cyber Physical System: Challenges and Solutions,IEEE,Conferences,"The Industry Internet of Things (IIoT) and the Industry Cyber-Physical System (ICPS) for real industry are becoming vitally necessary in the smart manufacturing environment. Very large number of intelligent sensors are being available generating an exploding amount of data. Several issues come with the big data in real industry, including the a grand-scale connected network construction with the data security and access protocol issues, data quality with considerable noise when gathered from industrial factories, efficient data storage, smart interconnection with cloud services, and real-time analytics requirements. This paper proposes an integrated CPS based architecture for smart manufacturing and provides the deployment details, addressing all the potential problems in an appropriate way. It has been successfully implemented in a real industry environment, and won the Best Industry Application of IoT at the BigInsights Data &amp; AI Innovation Awards.",https://ieeexplore.ieee.org/document/8780271/,2019 IEEE International Conference on Industrial Cyber Physical Systems (ICPS),6-9 May 2019,ieeexplore
10.1109/EUSIPCO.2016.7760546,Implementation of efficient real-time industrial wireless interference identification algorithms with fuzzified neural networks,IEEE,Conferences,Real-time industrial wireless systems sharing a crowded spectrum band require active coexistence management measures. Identification of wireless interference is a key issue for this purpose. We propose an efficient implementation of a wireless interference identification (WII) approach called neuro-fuzzy signal classifier (NFSC). The implementation in Matlab / SIMULINK is based upon the wideband software defined radio Ettus USRP N210. The implementation is evaluated in six selected heterogeneous and harsh industrial scenarios within the license-free 2.4-GHz-ISM radio band with variously combined standard wireless technologies IEEE 802.11g-based WLAN and Bluetooth. The evaluation of the NFSC was performed with a binary classification test with the statistical measurement metrics sensitivity and specificity.,https://ieeexplore.ieee.org/document/7760546/,2016 24th European Signal Processing Conference (EUSIPCO),29 Aug.-2 Sept. 2016,ieeexplore
10.1109/IRCE50905.2020.9199256,Industrial Implementation and Performance Evaluation of LSD-SLAM and Map Filtering Algorithms for Obstacles Avoidance in a Cooperative Fleet of Unmanned Aerial Vehicles,IEEE,Conferences,"In this paper we present an industrial implementation and performance evaluation of the problem of obstacles detection by drones using autonomous navigation systems. The software module that has been developed as well as the tests conducted are part of a large industrial R&amp;D Vitrociset project called SWARM: an AI-Enabled Command and Control (C&amp;C) system, able to execute and review ISR missions for mini/micro cooperative fleets of heterogeneous UAVs. The presented software module, that is currently under test, has been developed to recognize obstacles and drive correctly the drones, using images acquired by low cost RGB video cameras, whose features of lightness and reduced size allow them to be installed on mini/micro UAVs. Moreover, this setup does not require special calibration and preconfiguration processes like the ones necessary for example using stereo video camera systems. The real-time recognition of obstacles in the surrounding environment has been obtained and evaluated through the implementation, performance evaluation and tests of the LSD-SLAM and map filtering algorithms; the core of the study has been realized starting from the integration of these algorithms with a simulated drone in a synthetic environment. The areas of interest have been identified through the filtering of a computer generated map: the module was then integrated into the SWARM project platform, allowing the control of a single drone's movement and making it ready for use in a cooperative fleet environment.",https://ieeexplore.ieee.org/document/9199256/,2020 3rd International Conference on Intelligent Robotic and Control Engineering (IRCE),10-12 Aug. 2020,ieeexplore
10.1109/ICAICA52286.2021.9497973,Industrial Internet Security Protection Based on an Industrial Firewall,IEEE,Conferences,"A crucial step in the development of a security system for the industrial Internet is the implementation of an industrial firewall as the first line of defense for the multi-layer defense-in-depth system and an important safeguard for industrial network security. In the design, development, deployment, application, and maintenance of industrial firewalls, the firewall performance and architecture are vital aspects. This thesis focuses on the analysis and discussion of the requirements and abilities of an industrial firewall in terms of adaptability, network isolation, industrial communication protocol identification, filtering and analysis, real-time performance and reliability, and self-protection.",https://ieeexplore.ieee.org/document/9497973/,2021 IEEE International Conference on Artificial Intelligence and Computer Applications (ICAICA),28-30 June 2021,ieeexplore
10.1109/GIOTS49054.2020.9119497,Industrial IoT and Digital Twins for a Smart Factory : An open source toolkit for application design and benchmarking,IEEE,Conferences,"The rapid evolution of digital technology and designed intelligence, such as the Internet of Things (IoT), Big data analytics, Artificial Intelligence (AI), Cyber Physical Systems (CPS), has been a catalyst for the 4th industrial revolution (known as industry 4.0). Among other, the two key state-of-the-art concepts in Industry 4.0, are Industrial IoT (IIoT) and digital twins. IIoT facilitates real-time data acquisition, processing and analytics over large amount of sensor data streams produced by sensors installed within a smart factory, while the `digital twin' concept aims to enable smart factories via the digital replication or representation of physical machines, processes, people in cyber-space. This paper explores the capability of present-state open-source platforms to collectively achieve digital twin capabilities, including IoT real-time data acquisition, virtual representation, analytics, and visualisation. The aim of this work is to `close the gap' between research and implementation, through a collective open source IoT and Digital Twin architecture. The performance of the open-source architecture in this work, is demonstrated in a use-case utilising industry `open data', and is bench-marked with universal testing tools.",https://ieeexplore.ieee.org/document/9119497/,2020 Global Internet of Things Summit (GIoTS),3-3 June 2020,ieeexplore
10.1109/CASE48305.2020.9216902,Industrial Robot Grasping with Deep Learning using a Programmable Logic Controller (PLC),IEEE,Conferences,"Universal grasping of a diverse range of previously unseen objects from heaps is a grand challenge in e-commerce order fulfillment, manufacturing, and home service robotics. Recently, deep learning based grasping approaches have demonstrated results that make them increasingly interesting for industrial deployments. This paper explores the problem from an automation systems point-of-view. We develop a robotics grasping system using Dex-Net, which is fully integrated at the controller level. Two neural networks are deployed on a novel industrial AI hardware acceleration module close to a PLC with a power footprint of less than 10 W for the overall system. The software is tightly integrated with the hardware allowing for fast and efficient data processing and real-time communication. The success rate of grasping an object form a bin is up to 95% with more than 350 picks per hour, if object and receptive bins are in close proximity. The system was presented at the Hannover Fair 2019 (world's largest industrial trade fair) and other events, where it performed over 5,000 grasps per event.",https://ieeexplore.ieee.org/document/9216902/,2020 IEEE 16th International Conference on Automation Science and Engineering (CASE),20-21 Aug. 2020,ieeexplore
10.1109/YAC51587.2020.9337594,"Internet of Mind with General Intelligent Dispatch in Electrical Industrial: The Concept, Framework, Technology, and Future",IEEE,Conferences,"This paper proposes the concept of general intelligent dispatch, which is a novel framework of electrical power dispatch based on modern artificial intelligence (AI). The general intelligent dispatch system is based on knowledge automation, parallel system, ACP method, and fully integrated with cutting-edge technologies such as parallel deep learning, parallel dynamics programming. The general intelligent dispatch system is devised to be a comprehensive framework that can support the applications of various modern AI technologies in electrical power dispatch. General intelligent dispatch can accommodate the rapid development of the future smart grid, especially the extensive implementation of distributed energy resources and renewable energy. General intelligent dispatch also provides the practical technical path and framework for the future smart power dispatch services to achieve a higher level of automation and intelligence. Based on the general intelligent dispatch concept, this paper addresses power grid situational awareness, which provides an example for the applications of general intelligent dispatch.",https://ieeexplore.ieee.org/document/9337594/,2020 35th Youth Academic Annual Conference of Chinese Association of Automation (YAC),16-18 Oct. 2020,ieeexplore
10.1109/CCGrid51090.2021.00075,IoTwins: Design and Implementation of a Platform for the Management of Digital Twins in Industrial Scenarios,IEEE,Conferences,"With the increase of the volume of data produced by IoT devices, there is a growing demand of applications capable of elaborating data anywhere along the IoT-to-Cloud path (Edge/Fog). In industrial environments, strict real-time constraints require computation to run as close to the data origin as possible (e.g., IoT Gateway or Edge nodes), whilst batch-wise tasks such as Big Data analytics and Machine Learning model training are advised to run on the Cloud, where computing resources are abundant. The H2020 IoTwins project leverages the digital twin concept to implement virtual representation of physical assets (e.g., machine parts, machines, production/control processes) and deliver a software platform that will help enterprises, and in particular SMEs, to build highly innovative, AI-based services that exploit the potential of IoT/Edge/Cloud computing paradigms. In this paper, we discuss the design principles of the IoTwins reference architecture, delving into technical details of its components and offered functionalities, and propose an exemplary software implementation.",https://ieeexplore.ieee.org/document/9499575/,"2021 IEEE/ACM 21st International Symposium on Cluster, Cloud and Internet Computing (CCGrid)",10-13 May 2021,ieeexplore
10.1109/PerComWorkshops51409.2021.9430865,Keyword Spotting for Industrial Control using Deep Learning on Edge Devices,IEEE,Conferences,"Spoken commands promise unique advantages for the control of industrial machinery. Operators are enabled to keep their eyes on safety critical aspects of the process at all times and are free to use their hands in other parts of the process, instead of remote control. Current keyword spotting systems are prone to misunderstanding spoken utterances, especially in noisy environments, and are commonly deployed as non-realtime cloud services. Consequently, these systems can not be trusted with safety critical industrial control. We adapt a DS-CNN and a CNN for keyword spotting and use augmented training data, including real industrial noise, to increase their robustness. Furthermore, we apply post-training quantization and analyze the performance of both networks using multiple embedded systems, including a Google Edge TPU. We carry out a systematic analysis of accuracies, memory footprint and inference times using different combinations of data augmentations, hardware platforms, and quantizations. We show that augmented training data increases the inference accuracy in noisy environments by up to 20 %. Among others, this is demonstrated using an integer quantized network with a memory footprint of 0.57 MByte, reaching inference speeds of less than 5 ms on an embedded CPU and less than 1 ms on the Edge TPU. The results show that keyword spotting for industrial control is feasible on embedded systems and that the training data augmentation has a significant impact on the robustness in challenging environments.",https://ieeexplore.ieee.org/document/9430865/,2021 IEEE International Conference on Pervasive Computing and Communications Workshops and other Affiliated Events (PerCom Workshops),22-26 March 2021,ieeexplore
10.1109/INDIN.2015.7281881,Knowledge-driven finite-state machines. Study case in monitoring industrial equipment,IEEE,Conferences,Traditionally state machines are implemented by coding the desired behavior of a given system. This work proposes the use of ontological models to describe and perform computations on state machines by using SPARQL queries. This approach represents a paradigm shift relating to the customary manner in which state machines are stored and computed. The main contribution of the work is an ontological model to represent state machines and a set of generic queries that can be used in any knowledge-driven state machine to compute valuable information. The approach was tested in a study case were the state machines of industrial robots in a manufacturing line were modeled as ontological models and used for monitoring the behavior of these devices on real time.,https://ieeexplore.ieee.org/document/7281881/,2015 IEEE 13th International Conference on Industrial Informatics (INDIN),22-24 July 2015,ieeexplore
10.1109/ETFA.2019.8869172,Learning based Probabilistic Model for Migration of Industrial Control Systems,IEEE,Conferences,"The updating and upgrading of control systems is a cumbersome, expensive and time consuming task. From a software perspective, control system migration is a collective task of migrating the control logic, Human Machine Interface (HMI) and auxiliary software applications. Migrating control logic is the most challenging task owing to constraints on hard real-time behavior and execution order. Control logic typically contains engineering artifacts that specify the functionality of industrial devices taking into account various parameters. Therefore, to migrate from one Distributed Control System (DCS) system to another or to upgrade the existing DCS, one needs to map the source control entity and their parameters to the appropriate control entities in the target DCS.In this paper, we propose a machine learning based suggestion management system that identifies control entities and parameters for a source DCS and suggests the use of similar control entities and corresponding parameters for the target DCS. This in effect saves effort required in mapping of control parameters and reduces the dependence on subject matter experts. Our system uses a probabilistic approach to find these similarity mappings based on meta-data stored in an Ontology. We further describe a case study implemented for mapping heritage and legacy systems to a modern control system to verify and validate our approach.",https://ieeexplore.ieee.org/document/8869172/,2019 24th IEEE International Conference on Emerging Technologies and Factory Automation (ETFA),10-13 Sept. 2019,ieeexplore
10.1109/INDIN45523.2021.9557472,Learning-based Co-Design of Distributed Edge Sensing and Transmission for Industrial Cyber-Physical Systems,IEEE,Conferences,"Industrial cyber-physical systems (ICPS) refer to an emerging generation of intelligent systems, where distributed data acquisition is of great importance and is influenced by data transmission. In the improvement of the overall performance of sensing accuracy and energy efficiency, sensing and transmission are tightly coupled. Due to the unknown transmission channel states in the harsh industrial field environment, intelligently performing sensor scheduling for distributed sensing is challenging. In this paper, edge computing technology is utilized to enhance the level of intelligence at the edge side and deploy advanced scheduling algorithms. We propose a learning-based distributed edge sensing-transmission co-design (LEST) algorithm under the coordination of the sensors and the edge computing unit (ECU). Deep reinforcement learning is applied to perform real-time sensor scheduling under unknown channel states. The conditions for the existence of feasible scheduling policies are analyzed. The proposed algorithm is applied to estimate the slab temperature in the hot rolling process, which is a typical ICPS. The simulation results demonstrate that the overall performance of LEST is better than other suboptimal algorithms.",https://ieeexplore.ieee.org/document/9557472/,2021 IEEE 19th International Conference on Industrial Informatics (INDIN),21-23 July 2021,ieeexplore
10.1109/ISIE45063.2020.9152407,Modeling and Predicting an Industrial Process Using a Neural Network and Automation Data,IEEE,Conferences,"Production optimization and prevention of faults and unplanned production halts are areas of particular interest in industry. Predictive analysis is commonly implemented with data analytics and machine learning techniques. Usually, the usage of such tools requires knowledge of the machine learning theory and the subject to be studied, e.g. a pumping process. This paper presents a case study on modeling of a pumping process using stored automation data. The model is trained to predict the performance percentage of the process with minimal background knowledge of the process and data analytics. The proposed model is built with IBM SPSS Modeler, a data analysis tool not usually used in real-time industrial predictive analysis as it is not often considered the best tool when working with time series data. The model is deployed in a cloud service to implement a real-time, visualized predictive analysis system. The case study shows that Modeler can be used for data analysis, modeling, and production purposes. Depending on the case, Modeler can provide an alternative tool compared with typical machine learning tools, as models built with Modeler can be deployed into a cloud service for production use. The findings indicate that industrial automation data are a valuable resource, and data analysis can be conducted on various platforms and tools.",https://ieeexplore.ieee.org/document/9152407/,2020 IEEE 29th International Symposium on Industrial Electronics (ISIE),17-19 June 2020,ieeexplore
10.1109/AIID51893.2021.9456471,Multi-Scale Tiny Region Gesture Recognition Towards 3D Object Manipulation In Industrial Design,IEEE,Conferences,"This paper proposed a smart 3D virtual object manipulation by gesture recognition with deep network training for multiply scalar tiny region targets. We introduce the famous YOLOv5 based improved and enhanced target detection deep networks, and provide effective and high efficiency gesture recognition method towards 3D object manipulation. Meanwhile, combined with the adaptive anchor frame calculation and target recognition accelerator, our method can capture the tiny gesture region and for multi-scale of image detection. The method can be efficiently applied to the virtual control with different hand gestures. The experiment results are high efficiency and accuracy. It has high application value in the field of industrial design based on gesture manipulation.",https://ieeexplore.ieee.org/document/9456471/,2021 IEEE International Conference on Artificial Intelligence and Industrial Design (AIID),28-30 May 2021,ieeexplore
10.1109/FUZZ.2002.1005041,Multi-axis fuzzy control and performance analysis for an industrial robot,IEEE,Conferences,"Robot control systems can be considered as complex systems, the design of the controller involving the determination of the dynamic model for the system. Fuzzy logic provides functional capability without the use of a system model or the characteristics associated with capturing the approximate, varying values found in real world systems. Development of a multi-axis fuzzy logic control system was implemented on an industrial robot, replacing the existing control and hardware systems with a new developmental system. During robot control no adaptation of the rule base or membership functions was carried out online; only system gain was modified in relation to link speed and joint error within predetermined design parameters. The fuzzy control system had to manage the effects of frictional and gravitational forces whilst compensating for the varying inertia components when each linkage is moving. Testing based on ISO 9283 for path accuracy and repeatability verified that real time control of three axes was achievable with values of 938 /spl mu/m and 864 /spl mu/m recorded for accuracy and repeatability, respectively.",https://ieeexplore.ieee.org/document/1005041/,2002 IEEE World Congress on Computational Intelligence. 2002 IEEE International Conference on Fuzzy Systems. FUZZ-IEEE'02. Proceedings (Cat. No.02CH37291),12-17 May 2002,ieeexplore
10.1109/ICDMW.2019.00065,Mímir: Building and Deploying an ML Framework for Industrial IoT,IEEE,Conferences,"In this paper we describe Mímir, a production grade cloud and edge spanning ML framework for Industrial IoT applications. We first describe our infrastructure for optimized capture, streaming and multi-resolution storage of manufacturing data and its context. We then describe our workflow for scalable ML model training, validation, and deployment that leverages a manufacturing taxonomy and parameterized ML pipelines to determine the best metrics, hyper-parameters and models to use for a given task. We also discuss our design decisions on model deployment for real-time and batch data in the cloud and at the edge. Finally, we describe the use of the framework in building and deploying an application for Predictive Quality monitoring during a Plastics Extrusion manufacturing process.",https://ieeexplore.ieee.org/document/8955638/,2019 International Conference on Data Mining Workshops (ICDMW),8-11 Nov. 2019,ieeexplore
10.1109/PEDES.2006.344292,Neural Approach for Automatic Identification of Induction Motor Load Torque in Real-Time Industrial Applications,IEEE,Conferences,"Induction motors are widely used in several industrial sectors. However, the dimensioning of induction motors is often inaccurate because, in most cases, the load behavior in the shaft is completely unknown. The proposal of this paper is to use artificial neural networks as a tool for dimensioning induction motors rather than conventional methods, which use classical identification techniques and mechanical load modeling. Since the proposed approach uses current, voltage and speed values as the only input parameters, one of its potentialities is related to the facility of hardware implementation for industrial environments and field applications. Simulation results are also presented to validate the proposed approach.",https://ieeexplore.ieee.org/document/4147999/,"2006 International Conference on Power Electronic, Drives and Energy Systems",12-15 Dec. 2006,ieeexplore
10.1109/ICIEAM.2019.8742984,Objects Geometry Comparative Analysis Method for Industrial Robot Vision System,IEEE,Conferences,"At present, in computer vision systems, neural networks are used to process information received by the system from cameras. The recognition of all objects on the image is an extremely resource-intensive task, the solution of which consumes most of the computing power. For that reason, systems based on neural networks cannot be fully utilized for real-time systems due to limited computing resources. To build real-time computer vision systems, the authors suggested using the contour comparison method. The method allows to supervise the geometry of objects, conduct presorting and screen out defective parts, thereby the pressure on neural networks will reduce. The method is implemented in the Java. The created software performs image processing and objects search on it, that are the most similar to the template. The results of the experiment showed that the desired object is correctly determined on a noisy image and the proposed method can be used to solve the problem of pattern recognition in technical vision systems.",https://ieeexplore.ieee.org/document/8742984/,"2019 International Conference on Industrial Engineering, Applications and Manufacturing (ICIEAM)",25-29 March 2019,ieeexplore
10.1109/ICMA.2019.8816298,Online Learning of the Inverse Dynamics with Parallel Drifting Gaussian Processes: Implementation of an Approach for Feedforward Control of a Parallel Kinematic Industrial Robot,IEEE,Conferences,"The present paper deals with an online approach to learn the inverse dynamics of any robot. This is realized by the use of Gaussian Processes drifting parallel along the system data. An extension by a database enables the efficient use of data points from the past. The central component of this work is the implementation of such a method in a controller in order to achieve the actual goal: the feedforward control of an industrial robot by means of machine learning. This is done by splitting the procedure into two threads running parallel so that the prediction is decoupled from the computing-intensive training of the models. Experiments show that the method reduces the tracking errors more clearly than an elaborately identified rigid body model including friction. For a defined trajectory, the squared areas of the tracking errors of all axes are reduced by more than 54% compared to motion without pre-control. In addition, a highly dynamic pick-and-place experiment is used to investigate the possible changes in system dynamics. Compared to an offline trained model, the approximation error of the proposed online approach is smaller for the remaining time of the experiment after an initial phase. Furthermore, this error is smaller throughout the experiment for online learning with parallel drifting Gaussian Processes than when using a single one.",https://ieeexplore.ieee.org/document/8816298/,2019 IEEE International Conference on Mechatronics and Automation (ICMA),4-7 Aug. 2019,ieeexplore
10.23919/ChiCC.2017.8027747,Optimal operational control for industrial processes based on Q-learning method,IEEE,Conferences,"It is difficult to accurately model productive processes and describe relationship between operational indices and controlled variables for complex modem industrial processes. How to design the optimal setpoints by using only data generated by operational processes, without requiring the knowledge of model parameters of operational processes, poses a challenge on designing optimal setpoints. This paper presents a state-observer based Q-learning algorithm to learn the optimal setpoints by utilizing only data, such that the real operational indices can track the desired values in an approximately optimal manner. A simulation experiment in flotation process is implemented to show the effectiveness of the proposed method.",https://ieeexplore.ieee.org/document/8027747/,2017 36th Chinese Control Conference (CCC),26-28 July 2017,ieeexplore
10.1109/BigDataCongress.2019.00032,"PREMISES, a Scalable Data-Driven Service to Predict Alarms in Slowly-Degrading Multi-Cycle Industrial Processes",IEEE,Conferences,"In recent years, the number of industry-4.0-enabled manufacturing sites has been continuously growing, and both the quantity and variety of signals and data collected in plants are increasing at an unprecedented rate. At the same time, the demand of Big Data processing platforms and analytical tools tailored to manufacturing environments has become more and more prominent. Manufacturing companies are collecting huge amounts of information during the production process through a plethora of sensors and networks. To extract value and actionable knowledge from such precious repositories, suitable data-driven approaches are required. They are expected to improve the production processes by reducing maintenance costs, reliably predicting equipment failures, and avoiding quality degradation. To this aim, Machine Learning techniques tailored for predictive maintenance analysis have been adopted in PREMISES (PREdictive Maintenance service for Industrial procesSES), an innovative framework providing a scalable Big Data service able to predict alarming conditions in slowly-degrading processes characterized by cyclic procedures. PREMISES has been experimentally tested and validated on a real industrial use case, resulting efficient and effective in predicting alarms. The framework has been designed to address the main Big Data and industrial requirements, by being developed on a solid and scalable processing framework, Apache Spark, and supporting the deployment on modularized containers, specifically upon the Docker technology stack.",https://ieeexplore.ieee.org/document/8818217/,2019 IEEE International Congress on Big Data (BigDataCongress),8-13 July 2019,ieeexplore
10.1109/ICATE49685.2021.9465023,PV Monitoring System using Industrial Internet of Things Technologies based on Graphical Programming,IEEE,Conferences,"Photovoltaic Panel Systems are already a commodity, the widespread use of such systems being no longer a pioneering topic. Monitoring these power generation systems is not a trivial task since the hunger of real-time data is growing continuously. The advent of IoT technologies, the ubiquitous presence of communication technologies, including remote areas, are making these solutions easier to develop, deploy and use. This will constitute the nurturing bed for optimization processes, which would eventually rely on Artificial Intelligence. This paper is describing how to design and deploy such data collecting system, emphasizing on its Software Architecture based on graphical programming technologies. Ready-to-run virtual instruments are facilitating real-time measurements and analysis, process data being securely stored and visualized using cloud technologies.",https://ieeexplore.ieee.org/document/9465023/,2021 International Conference on Applied and Theoretical Electricity (ICATE),27-29 May 2021,ieeexplore
10.1109/ICCC51575.2020.9345104,Payload-based Anomaly Detection for Industrial Internet Using Encoder Assisted GAN,IEEE,Conferences,"Payload-based anomaly detection has been proved effective in discovering Internet misbehavior and potential intrusions, but highly relies on the unstructured feature engineering to generalize the distribution of normal payloads. This kind of generalization may not adapt well to the emerging industrial Internet, where the normal behaviors are more diverse and usually embedded in the raw payloads' local structures. In this paper, we tackle this generalization problem and propose a very different solution to payload-based anomaly detection without the need of feature engineering. Our basic idea is to learn the raw structures of normal payloads directly by a generative adversarial network (GAN), in which we have a generator (i.e., a reversed convolutional decoder) to sample raw payloads from a latent space as well as a discriminator (i.e., a convolutional classifier) to guide the generator produce raw payloads approximating the normal structures. We also deploy an assisted convolutional encoder to map the true payloads back to the latent space and combine with the GAN's decoder (i.e., generator) to reconstruct the payload structures. We consider anomalies appear in condition the re-constructed payloads are largely deviated from the true ones, since our encoder-decoder architecture is trained able to rebuild only the normal payload structures. We have evaluated our solution using extensive experiments on real-world industrial Internet datasets, and confirmed its effectiveness in detecting industrial Internet anomalies in the raw payloads.",https://ieeexplore.ieee.org/document/9345104/,2020 IEEE 6th International Conference on Computer and Communications (ICCC),11-14 Dec. 2020,ieeexplore
10.1109/RTEICT49044.2020.9315660,Prediction of Air Quality in Industrial Area,IEEE,Conferences,"Air quality monitoring and prediction in many industrial and urban areas, it has become one of the most important activities. Owing to different types of pollution, air quality is heavily affected. With increasing air pollution, efficient air quality monitoring models is to be implemented; these models gather data on the concentration of air pollutants. In a proposed approach, to solve three problems- prediction, interpolation and feature analysis, previously these problems were solved using three different models but now in the proposed system can solve these three problems in one model i.e Air Pollutant Prediction. This approach relates to unlabeled spatiotemporal data to enhance interpolation efficiency and air quality prediction. Experiments to test the proposed solution based on the real-time data sources collected by the Karnataka State Pollution Control Board (KSPCB), India. The goal of this research paper is to explore various strategies based on machine learning techniques for monitoring and predicating the air quality.",https://ieeexplore.ieee.org/document/9315660/,"2020 International Conference on Recent Trends on Electronics, Information, Communication & Technology (RTEICT)",12-13 Nov. 2020,ieeexplore
10.1109/SoCPaR.2011.6089156,QoS-oriented Service Management in clouds for large scale industrial activity recognition,IEEE,Conferences,"Motivated by the need of industrial enterprises for supervision services for quality, security and safety guarantee, we have developed an Activity Recognition Framework based on computer vision and machine learning tools, attaining good recognition rates. However, the deployment of multiple cameras to exploit redundancies, the large training set requirements of our time series classification models, as well as general resource limitations together with the emphasis on real-time performance, pose significant challenges and lead us to consider a decentralized approach. We thus adapt our application to a new and innovative real-time enabled framework for service-based infrastructures, which has developed QoS-oriented Service Management mechanisms in order to allow cloud environments to facilitate real-time and interactivity. Deploying the Activity Recognition Framework in a cloud infrastructure can therefore enable it for large scale industrial environments.",https://ieeexplore.ieee.org/document/6089156/,2011 International Conference of Soft Computing and Pattern Recognition (SoCPaR),14-16 Oct. 2011,ieeexplore
10.1109/COGINF.2010.5599677,Quadratic neural unit is a good compromise between linear models and neural networks for industrial applications,IEEE,Conferences,"The paper discusses the quadratic neural unit (QNU) and highlights its attractiveness for industrial applications such as for plant modeling, control, and time series prediction. Linear systems are still often preferred in industrial control applications for their solvable and single solution nature and for the clarity to the most application engineers. Artificial neural networks are powerful cognitive nonlinear tools, but their nonlinear strength is naturally repaid with the local minima problem, overfitting, and high demands for application-correct neural architecture and optimization technique that often require skilled users. The QNU is the important midpoint between linear systems and highly nonlinear neural networks because the QNU is relatively very strong in nonlinear approximation; however, its optimization and performance have fast and convex-like nature, and its mathematical structure and the derivation of the learning rules is very comprehensible and efficient for implementation.",https://ieeexplore.ieee.org/document/5599677/,9th IEEE International Conference on Cognitive Informatics (ICCI'10),7-9 July 2010,ieeexplore
10.1109/ICIT.2006.372319,Real Time Classifier For Industrial Wireless Sensor Network Using Neural Networks with Wavelet Preprocessors,IEEE,Conferences,"Wireless sensor node is embedded of computation unit, sensing unit and a radio unit for communication. Amongst three units communication is the largest consumer of energy. Energy is the prime source for wireless sensor node to function. Hence every aspects of sensor node are designed with energy constraints. Neural Networks in particular the combination of ART1 and FuzzyART(FA) can be used very efficiently for developing Real time Classifier. Wireless sensor networks demand for the real time classification of sensor data. In this paper classification technique using ART1 and Fuzzy ART is discussed. ART1 and FA have very good architectural strategy, which makes it simple for VLSI implementation. The VLSI implementation of the proposed classifier can be a part of embedded microsensor. The paper discusses classification technique, which can reduce the energy need for communication and improves communications bandwidth. The proposed sensor clustering architecture can give distributed storage space for the sensor networks. Wavelet Transform is used as preprocessor for denoising the real word data from sensor node, this makes it much suitable for industrial environment. Many methods of wavelet transforms are available. Simplest Haar 1D transform is used for preprocessing and smoothing the sensor signals. The discrete wavelet transform implemented here helps to extract important feature in the sensor data like sudden changes at various scales.",https://ieeexplore.ieee.org/document/4237641/,2006 IEEE International Conference on Industrial Technology,15-17 Dec. 2006,ieeexplore
10.1109/IECON.2012.6389018,Recent advances in the application of real-time computational intelligence to industrial electronics,IEEE,Conferences,"The field of computational intelligence [CI] has seen advances in both the theoretical knowledge base of these techniques, and in specific applications of these techniques to real-world problems. This work first attempts to summarize the current trends and definitions in the CI branches of fuzzy systems, artificial neural networks [ANNs], and hybrid neuro-fuzzy systems and their variants. These particular branches of CI are selected for their ability to be implemented in real-time problem solving, whether computation and processing is done in software or implemented in hardware. Then, some current applications of these CI technologies for use in industrial electronics are highlighted and summarized.",https://ieeexplore.ieee.org/document/6389018/,IECON 2012 - 38th Annual Conference on IEEE Industrial Electronics Society,25-28 Oct. 2012,ieeexplore
10.1109/ICIEAM.2017.8076111,Redundant industrial manipulator control system,IEEE,Conferences,"We present the control system synthesis for the multilink redundant manipulator. Our control system is based on the unique algorithm that includes the novel hybrid method for solving the inverse kinematics problem. This method combines ANFIS-network and iterative refinement. As a result, the control system has high integrative capabilities and is easy to modify for another construction. The manipulator design is described by mathematical equations which are used for the workspace construction. These equations are used for creation of the neurofuzzy network and generation database (network training information). Modeling of the industrial manipulator with 5 degrees of freedom as an example of the implementation of our control system is considered in the paper. Virtual environment that displays a model motion in real time using a virtual 3-D model is also presented in the paper. We present the work results applied to the manipulator physical model. This model includes Festo servomotors and the Siemens programmable logical controller.",https://ieeexplore.ieee.org/document/8076111/,"2017 International Conference on Industrial Engineering, Applications and Manufacturing (ICIEAM)",16-19 May 2017,ieeexplore
10.1109/WF-IoT.2016.7845468,SQenloT: Semantic query engine for industrial Internet-of-Things gateways,IEEE,Conferences,"The Advent of Internet-of-Things (IoT) paradigm has brought exciting opportunities to solve many real-world problems. IoT in industries is poised to play an important role not only to increase productivity and efficiency but also to improve customer experiences. Two main challenges that are of particular interest to industry include: handling device heterogeneity and getting contextual information to make informed decisions. These challenges can be addressed by IoT along with proven technologies like the Semantic Web. In this paper, we present our work, SQenIoT: a Semantic Query Engine for Industrial IoT. SQenIoT resides on a commercial product and offers query capabilities to retrieve information regarding the connected things in a given facility. We also propose a things query language, targeted for resource-constrained gateways and non-technical personnel such as facility managers. Two other contributions include multi-level ontologies and mechanisms for semantic tagging in our commercial products. The implementation details of SQenIoT and its performance results are also presented.",https://ieeexplore.ieee.org/document/7845468/,2016 IEEE 3rd World Forum on Internet of Things (WF-IoT),12-14 Dec. 2016,ieeexplore
10.1109/ETFA.2012.6489781,Semantic design and integration of simulation models in the industrial automation area,IEEE,Conferences,"Simulations are software tools approximating and predicting the behavior of real industrial plants. Unlike real plants, the utilization of simulations cannot cause damages and it saves time and costs during series of experiments. A shortcoming of current simulation models is the complicated runtime integration into legacy industrial systems and platforms, as well as ad-hoc design phase, introducing manual and error-prone work. This paper contributes to improve the efficiency of simulation model design and integration. It utilizes a semantic knowledge base, implemented by ontologies and their mappings. The integration uses the Automation Service Bus and the paper explains how to configure the runtime integration level semantically. The main contributions are the concept of semantic configuration of the service bus and the workflows of simulation design and integration.",https://ieeexplore.ieee.org/document/6489781/,Proceedings of 2012 IEEE 17th International Conference on Emerging Technologies & Factory Automation (ETFA 2012),17-21 Sept. 2012,ieeexplore
10.1109/IECON.2016.7793206,Summer school on intelligent agents in automation: Hands-on educational experience on deploying industrial agents,IEEE,Conferences,"Cyber-physical systems constitutes a framework to develop intelligent, distributed, resilient, collaborative and cooperative systems, promoting the fusion of computational entities and physical devices. Agent technology plays a crucial role to develop this kind of systems by offering a decentralized, distributed, modular, robust and reconfigurable control structure. This paper describes the implementation of a summer school aiming to enhance the participants' knowledge in the field of multi-agent systems applied to industrial environments, being able to gain the necessary theoretical and practical skills to develop real industrial agent based applications. This is accomplished in an international framework where individual knowledge and experiences are shared in a complementary level.",https://ieeexplore.ieee.org/document/7793206/,IECON 2016 - 42nd Annual Conference of the IEEE Industrial Electronics Society,23-26 Oct. 2016,ieeexplore
10.1109/ICSE-Companion.2019.00131,Testing Untestable Neural Machine Translation: An Industrial Case,IEEE,Conferences,"Neural Machine Translation (NMT) has shown great advantages and is becoming increasingly popular. However, in practice, NMT often produces unexpected translation failures in its translations. While reference-based black-box system testing has been a common practice for NMT quality assurance during development, an increasingly critical industrial practice, named in-vivo testing, exposes unseen types or instances of translation failures when real users are using a deployed industrial NMT system. To fill the gap of lacking test oracles for in-vivo testing of NMT systems, we propose a new methodology for automatically identifying translation failures without reference translations. Our evaluation conducted on real-world datasets shows that our methodology effectively detects several targeted types of translation failures. Our experiences on deploying our methodology in both production and development environments of WeChat (a messenger app with over one billion monthly active users) demonstrate high effectiveness of our methodology along with high industry impact.",https://ieeexplore.ieee.org/document/8802818/,2019 IEEE/ACM 41st International Conference on Software Engineering: Companion Proceedings (ICSE-Companion),25-31 May 2019,ieeexplore
10.1109/AIVR.2018.00024,The Virtual Factory: Hologram-Enabled Control and Monitoring of Industrial IoT Devices,IEEE,Conferences,"Augmented reality (AR) has been exploited in manifold fields but is yet to be used at its full potential. With the massive diffusion of smart devices, opportunities to build immersive human-computer interfaces are continually expanding. In this study, we conceptualize a virtual factory: an interactive, dynamic, holographic abstraction of the physical machines deployed in a factory. Through our prototype implementation, we conducted a user-study driven evaluation of holographic interfaces compared to traditional interfaces, highlighting its pros and cons. Our study shows that the majority of the participants found holographic manipulation more attractive and natural to interact with. However, current performance characteristics of head-mounted displays must be improved to be applied in production.",https://ieeexplore.ieee.org/document/8613643/,2018 IEEE International Conference on Artificial Intelligence and Virtual Reality (AIVR),10-12 Dec. 2018,ieeexplore
10.1109/CoASE.2014.6899348,Toward safe close-proximity human-robot interaction with standard industrial robots,IEEE,Conferences,"Allowing humans and robots to interact in close proximity to each other has great potential for increasing the effectiveness of human-robot teams across a large variety of domains. However, as we move toward enabling humans and robots to interact at ever-decreasing distances of separation, effective safety technologies must also be developed. While new, inherently human-safe robot designs have been established, millions of industrial robots are already deployed worldwide, which makes it attractive to develop technologies that can turn these standard industrial robots into human-safe platforms. In this work, we present a real-time safety system capable of allowing safe human-robot interaction at very low distances of separation, without the need for robot hardware modification or replacement. By leveraging known robot joint angle values and accurate measurements of human positioning in the workspace, we can achieve precise robot speed adjustment by utilizing real-time measurements of separation distance. This, in turn, allows for collision prevention in a manner comfortable for the human user.We demonstrate our system achieves latencies below 9.64 ms with 95% probability, 11.10 ms with 99% probability, and 14.08 ms with 99.99% probability, resulting in robust real-time performance.",https://ieeexplore.ieee.org/document/6899348/,2014 IEEE International Conference on Automation Science and Engineering (CASE),18-22 Aug. 2014,ieeexplore
10.1109/RTSI50628.2021.9597339,Towards Graph Machine Learning for Smart Grid Knowledge Graphs in Industrial Scenarios,IEEE,Conferences,"Knowledge Graphs (KGs) demonstrated promising application perspective in different scenarios, especially when combined with Graph Machine Learning (GML) techniques able to interpret and infer over facts. Given the natural network structures of Smart Grid equipment and the exponential growth of electric power data, Smart Grid Knowledge Graphs (SGKGs) provides unprecedented opportunities to manage massive power resources and provide intelligent applications. However, a single representation of the SGKGs is never sufficient to properly exploit GML techniques that leverage different aspects of the KG for various objectives. In this work, we provide a methodology to extract various significant views of the SGKG by iteratively applying a series of transformation to the description of the power network in the IEC CIM standard. Our implementation is based on a declarative approach to guarantee easier portability, and we deploy the transformations as a stateless microservice, facilitating modular integration with the rest of the Smart Grid Semantic Platform. Experimental evaluation on two real power distribution networks demonstrates the efficacy of our approach in highlighting important topological information, without discarding precious additional knowledge present in the SGKG.",https://ieeexplore.ieee.org/document/9597339/,2021 IEEE 6th International Forum on Research and Technology for Society and Industry (RTSI),6-9 Sept. 2021,ieeexplore
10.1109/AIKE.2018.00037,"Towards Low-Cost, Real-Time, Distributed Signal and Data Processing for Artificial Intelligence Applications at Edges of Large Industrial and Internet Networks",IEEE,Conferences,"Digital Signal Processors (DSP) are vital system components in industrial Artificial Intelligence (AI) applications. In this paper, FIR filters that could be used for industrial AI applications are designed from the Spline Biorthogonal 1.5 (SB1.5) mother wavelet using a real-time, low-cost, generic industrial IoT (IIoT) hardware: the C28x DSP and low-level, Embedded C, system software. Our contribution in this paper is the first reported application of the C28x for SB1.5 wavelet construction. The significance of this approach is to be able to repurpose low-cost, readily available hardware for distributed AI applications. Our approach is different from the state of the art, in which specialized hardware are always manufactured for implementing AI applications at large network edges. Our approach supports low-cost and fast single-stage FIR implementation suitable for use in real-time, distributed AI application at network edges, since in our case, successive recursion of FIR filters leading to a full implementation of Pyramid Algorithm is not implemented. The designed FIR filter is evaluated and found capable of both low-pass and high pass filtering operations. Results of this paper indicate that the C28x real-time DSP, which exists in many IoT devices, could have improved scalability by being deployed for other important AI and IoT network edge analytic applications, different from its present uses.",https://ieeexplore.ieee.org/document/8527469/,2018 IEEE First International Conference on Artificial Intelligence and Knowledge Engineering (AIKE),26-28 Sept. 2018,ieeexplore
10.1109/COASE.2018.8560470,Training CNNs from Synthetic Data for Part Handling in Industrial Environments,IEEE,Conferences,"As Convolutional Neural Network based models become reliable and efficient, two questions arise in relation to their applications for industrial purposes. The usefulness of these models in industrial environments and their implementation in these settings. This paper describes the autonomous generation of Region based CNN models trained on images from rendered CAD models and examines their applicability and performance for part handling application. The development of the automated synthetic data generation is detailed and two CNN models are trained with the aim to detect a car component and differentiate it against another similar looking part. The performance of these models is tested on real images and it was found that the proposed approach can be easily adopted for detecting a range of parts in arbitrary backgrounds. Moreover, the use of syntheic images for training CNNs automates the process of generating a detector.",https://ieeexplore.ieee.org/document/8560470/,2018 IEEE 14th International Conference on Automation Science and Engineering (CASE),20-24 Aug. 2018,ieeexplore
10.1109/ICAIIC48513.2020.9065203,UAV-assisted Real-time Data Processing using Deep Q-Network for Industrial Internet of Things,IEEE,Conferences,"Industrial internet of things (IIoT) enables edge computing technology to provide communication between the machines that produce a large amount of data and locate at the edge network. A task scheduling is implemented in the edge node. Furthermore, the real-time data can achieve with the lowest latency that allowed by the edge node near the edge network. However, a mobile machine such as an autonomous guided vehicle can interfere in this situation. Because the vehicle also needs service by the edge node. Over that, quality of service (QoS) performance can decrease. Therefore, this paper deploys an unmanned aerial vehicle (UAV) as an edge node to provide service to the edge network through optimizing the trajectory of UAV, where the edge network request task using a Deep Q-Network (DQN) Learning. The result shows that using machine learning, notably the DQN algorithm, can increase the number of the machine that can be provided service. Subsequently, the real-time data can achieve either the interrupt occurs at the edge node.",https://ieeexplore.ieee.org/document/9065203/,2020 International Conference on Artificial Intelligence in Information and Communication (ICAIIC),19-21 Feb. 2020,ieeexplore
10.1049/cp:19940284,Using expert systems for on-line data qualification and state variable estimation for an industrial fermentation process,IET,Conferences,"An industrial fed batch fermentation process is a nonlinear time-varying process. Important internal state variables such as biomass, substrate and product concentrations cannot be measured online and are usually determined by infrequent and time consuming off-line laboratory analysis. The online measurements are usually noisy and sometimes this leads to misinterpretation of the real situation inside the fermenter. These problems can lead to poor control of the batch and low productivity subsequently. To overcome these problems a real time expert system has been proposed which is based on the Poplog Flex real time expert system shell. The system is used to monitor the state variables of the process, diagnose any fault that might occur in the process, estimate the important unmeasurable state variables and to design a controller to control the state around a desired level. A neural network has been adopted for the online estimation of the unmeasurable state variables. Pattern recognition ideas have been used to improve the modelling ability of the neural network. Predictive control techniques have been used to control the state around a desired level. The model and the controller for the process have been designed and implemented within the Poplog Flex environment.&lt;<ETX>&gt;</ETX>",https://ieeexplore.ieee.org/document/327321/,1994 International Conference on Control - Control '94.,21-24 March 1994,ieeexplore
10.1109/DCOSS.2019.00126,Virtual Light Sensors in Industrial Environment Based on Machine Learning Algorithms,IEEE,Conferences,"Internet of Things (IoT) has become the backbone of current and future emerging applications both in the public and the private, industrial sector. The IoT paradigm, enhanced with intelligence and big data analytics, has found applications in a wide range of solutions such as smart home, smart city, industrial IoT etc. Even though IoT implies that cheap motes can conduct a specific task, thus a large number of them can be deployed, we aim to minimize the installed hardware while we still have a high level of quality of service. Machine Learning algorithms can support this challenge by generating virtual data via utilization of real data from a smaller subset of sensors. The generated data can replicate sensor behavior which would otherwise be difficult or impossible to track. It is also possible to use simulation models for data analysis model validation, by generating new data under varying conditions. In this paper, we propose a concept of an IoT testbed which allows virtual IoT resources to be immersed and tested in real life conditions, which are met in everyday life. Additionally, the features of the implemented testbed prototype are discussed while taking into account specific use cases, regarding luminosity scenarios in industrial environments.",https://ieeexplore.ieee.org/document/8804746/,2019 15th International Conference on Distributed Computing in Sensor Systems (DCOSS),29-31 May 2019,ieeexplore
10.1109/TII.2020.3036159,A General Transfer Framework Based on Industrial Process Fault Diagnosis Under Small Samples,IEEE,Journals,"The lack of fault samples is a challenging issue for fault diagnosis in the industrial process. It is difficult for conventional fault diagnosis methods to achieve satisfactory results under small samples. This article proposes a general transfer framework with evolutionary capability to address the above issue. First, a general transfer framework is proposed, in which the transfer learning strategy is applied to guarantee the number and diversity of expanded samples and achieve accurate modeling. Second, the adaptive mixup (Admixup) method is presented, which can adaptively expand the fault samples and make the shared distribution smoother to guarantee the stability and accuracy of the fault diagnosis results. Finally, an optimized evolution strategy is designed, in which the transformation matrix is used as an evolutionary channel to reduce the fault diagnosis errors without retraining the framework as fault samples increase. The presented framework can utilize the generalization of small samples and the knowledge of various working condition samples to achieve accurate modeling. The proposed framework is applied to simulated and real industrial processes. Experiment results illustrate that the fault diagnosis model can be effectively established by the proposed framework under small samples, and the proposed framework is evolutionary capable.",https://ieeexplore.ieee.org/document/9250552/,IEEE Transactions on Industrial Informatics,Sept. 2021,ieeexplore
10.1109/TII.2020.3013277,A Real-Time Defect Detection Method for Digital Signal Processing of Industrial Inspection Applications,IEEE,Journals,"The signal processing of industrial big data (IBD) is a challenging task, owing to the complex working scenarios and the lack of annotations. Defect detection, which is an important subject of IBD research works, has shown its effectiveness in digital signal processing of industrial inspection applications in many previous studies. This article proposes a novel defect detection method based on deep learning for digital signal processing of industrial inspection applications. In our method, a module named feature collection and compression network is applied to merge multiscale feature information. Then, a new pooling method named Gaussian weighted pooling, which provides more precise location information, is used to replace region of interest (ROI) pooling. Experiment results show that our method gets improvements in both accuracy and efficiency, with mAP/AP50 of 41.8/80.2 at 33 fps on NEUDET, which satisfies the requirement of real-time systems.",https://ieeexplore.ieee.org/document/9153815/,IEEE Transactions on Industrial Informatics,May 2021,ieeexplore
10.1109/ACCESS.2019.2963723,A Smart Collaborative Routing Protocol for Delay Sensitive Applications in Industrial IoT,IEEE,Journals,"In the industrial Internet of things (IIoT), there is always a strong demand for real-time information transfer. Especially when deploying wireless/wired hybrid networks in smart factories, the requirement for low delay interaction is more prominent. Although tree routing protocols have been successfully executed in simple networks, more challenges in transmission speed can be observed in the manufacturing broadband communication system. Motivated by the progresses in deep learning, a smart collaborative routing protocol with low delay and high reliability is proposed to accommodate mixed link scenarios. First, we establish a one-hop delay model to investigate the potential affects of Media Access Control (MAC) layer parameters, which supports the subsequent design. Second, forwarding, maintenance, and efficiency strategies are created to construct the basic functionalities for our routing protocol. Relevant procedures and key approaches are highlighted as well. Third, two sub-protocols are generated and the corresponding implementation steps are described. The experimental results demonstrate that the end-to-end delay can be effectively cut down through comprehensive improvements. Even more sensor nodes and larger network scale are involved, our proposed protocol can still illustrate the advantages comparing with existing solutions within IIoT.",https://ieeexplore.ieee.org/document/8949516/,IEEE Access,2020,ieeexplore
10.1109/TAC.1981.1102738,A digital quality control system for an industrial dry process rotary cement kiln,IEEE,Journals,"A multivariate autoregressive moving average (ARMA) model for an industrial dry process rotary cement kiln is identified, from real process data, using the maximum likelihood method. The model obtained is then used in computing a controller for quality control of clinker production. It is shown that it is relevant to compute a minimum variance controller subject to restrictions both in the controller structure and the variances of the control signals. The resulting controller is finally implemented on the cement kiln process, together with a target adaptive controller for automatic adjustment of the clinker quality setpoint, in order to save energy.",https://ieeexplore.ieee.org/document/1102738/,IEEE Transactions on Automatic Control,August 1981,ieeexplore
10.1109/ACCESS.2020.2992249,An Ensemble Deep Learning-Based Cyber-Attack Detection in Industrial Control System,IEEE,Journals,"The integration of communication networks and the Internet of Things (IoT) in Industrial Control Systems (ICSs) increases their vulnerability towards cyber-attacks, causing devastating outcomes. Traditional Intrusion Detection Systems (IDSs), which are mainly developed to support information technology systems, count vastly on predefined models and are trained mostly on specific cyber-attacks. Besides, most IDSs do not consider the imbalanced nature of ICS datasets, thereby suffering from low accuracy and high false-positive when being put to use. In this paper, we propose a deep learning model to construct new balanced representations of the imbalanced datasets. The new representations are fed into an ensemble deep learning attack detection model specifically designed for an ICS environment. The proposed attack detection model leverages Deep Neural Network (DNN) and Decision Tree (DT) classifiers to detect cyber-attacks from the new representations. The performance of the proposed model is evaluated based on 10-fold cross-validation on two real ICS datasets. The results show that the proposed method outperforms conventional classifiers, including Random Forest (RF), DNN, and AdaBoost, as well as recent existing models in the literature. The proposed approach is a generalized technique, which can be implemented in existing ICS infrastructures with minimum effort.",https://ieeexplore.ieee.org/document/9086038/,IEEE Access,2020,ieeexplore
10.1109/TII.2019.2959021,An Integrated Histogram-Based Vision and Machine-Learning Classification Model for Industrial Emulsion Processing,IEEE,Journals,"Existing techniques in emulsion quality evaluation are found to be highly subjective, time-consuming, and prone to overprocessing. Other conventional droplet analysis techniques such as laser diffraction, which require dilution of samples, introduce an additional complexity to industrial processes. The possibility of developing a fully automated technique for droplet characterization during emulsification holds remarkable potential for overcoming the existing challenges. In this article, a histogram-based image segmentation technique detects droplets from emulsion micrographs. The evolution of droplet characteristics and their significance are studied by performing statistical analysis, and the significant characteristics are selected. The principal component analysis is applied to obtain a reduced set of uncorrelated components from the selected characteristics. The linear discriminant analysis classifies the micrographs into a set of quality categories called target, acceptable, marginal, and unacceptable. The model accuracy is validated using stratified five-fold cross-validation and is successful in classifying the micrographs obtained from two different manufacturing facilities with high accuracy up to 100%. The histogram-based technique is successful in detecting smaller droplets than previously reflected in the literature. The current approach is fully automated and is implemented as a soft-sensor, which supports its real-time deployment into an industrial environment. The entire approach has promising potential in the in-line prediction of emulsion quality leading to more efficient and sustainable manufacturing.",https://ieeexplore.ieee.org/document/8968624/,IEEE Transactions on Industrial Informatics,Sept. 2020,ieeexplore
10.1109/ACCESS.2017.2770180,An Integrated Industrial Ethernet Solution for the Implementation of Smart Factory,IEEE,Journals,"Smart factory addresses the vertical integration of physical entities and information systems. Network and cloud are two essential infrastructures to achieve this goal. Among them, the network provides interconnection for communication and data flow, while the cloud provides powerful and elastic computing and storage abilities for big data and intelligent applications. This paper presents a cloud-centric framework for the implementation of the smart factory. Based on this framework, three high leveled protocols viz., EtherCAT, DDS, and OPC UA are selected to implement machine network (controller to sensors/actuators communication), machine to machine communication, and machine to cloud communication respectively, to satisfy diverse communication requirements of the smart factory. An integrated architecture for combining ontology knowledge and semantic data to support intelligent applications is also proposed. These network and data process schemes are verified in a smart factory prototype for personalized candy packing application.",https://ieeexplore.ieee.org/document/8096995/,IEEE Access,2017,ieeexplore
10.1109/ACCESS.2021.3127084,Auto-NAHL: A Neural Network Approach for Condition-Based Maintenance of Complex Industrial Systems,IEEE,Journals,"Nowadays, machine learning has emerged as a promising alternative for condition monitoring of industrial processes, making it indispensable for maintenance planning. Such a learning model is able to assess health states in real time provided that both training and testing samples are complete and have the same probability distribution. However, it is rare and difficult in practical applications to meet these requirements due to the continuous change in working conditions. Besides, conventional hyperparameters tuning via grid search or manual tuning requires a lot of human intervention and becomes inflexible for users. Two objectives are targeted in this work. In an attempt to remedy the data distribution mismatch issue, we firstly introduce a feature extraction and selection approach built upon correlation analysis and dimensionality reduction. Secondly, to diminish human intervention burdens, we propose an Automatic artificial Neural network with an Augmented Hidden Layer (Auto-NAHL) for the classification of health states. Within the designed network, it is worthy to mention that the novelty of the implemented neural architecture is attributed to the new multiple feature mappings of the inputs, where such configuration allows the hidden layer to learn multiple representations from several random linear mappings and produce a single final efficient representation. Hyperparameters tuning including the network architecture, is fully automated by incorporating Particle Swarm Optimization (PSO) technique. The designed learning process is evaluated on a complex industrial plant as well as various classification problems. Based on the obtained results, it can be claimed that our proposal yields better response to new hidden representations by obtaining a higher approximation compared to some previous works.",https://ieeexplore.ieee.org/document/9610082/,IEEE Access,2021,ieeexplore
10.1109/ACCESS.2020.2993010,Bearing Intelligent Fault Diagnosis in the Industrial Internet of Things Context: A Lightweight Convolutional Neural Network,IEEE,Journals,"The advancement of Industry 4.0 and Industrial Internet of Things (IIoT) has laid more emphasis on reducing the parameter amount and storage space of the model in addition to the automatic and accurate fault diagnosis. In this case, this paper proposes a lightweight convolutional neural network (LCNN) method for intelligent fault diagnosis of bearing, which can largely satisfy the need of less parameter amount and storage space as well as high accuracy. First, depthiwise separable convolution is adopted, and a LCNN structure is constructed through an inverse residual structure and a linear bottleneck layer operation. Secondly, a novel decomposed Hierarchical Search Space is introduced to automatically explore the optimal LCNN for bearing fault diagnosis in the context of the IIoT. In the meantime, the real-time monitoring and fault diagnosis of the model are also deployed. In order to verify the validity of the designed model, Case Western Reserve University Bearing fault dataset and MFPT bearing fault dataset are adopted. The results demonstrate the great advantages of the model. The LCNN model can automatically learn and select the appropriate features, highly improving the fault diagnosis accuracy. Meanwhile, the computational and storage costs of the model are largely reduced, which contributes to its being applied to the mechanical system in the IIoT context.",https://ieeexplore.ieee.org/document/9088980/,IEEE Access,2020,ieeexplore
10.1109/TII.2016.2516973,Data-Based Multiobjective Plant-Wide Performance Optimization of Industrial Processes Under Dynamic Environments,IEEE,Journals,"This paper provides a method for automatically selecting optimal operational indices for unit processes in an industrial plant using measured data and without knowing dynamical models of the unit process. A dynamic multiobjective optimization problem is defined to find operational indices that lead to plant-wide production indices close to their target values. A case-based reasoning (CBR) technique is also employed, which uses the stored experience of a human expert to determine appropriate operational indices for given target production indices. The solutions of the optimization problem and CBR technique are combined to form baseline operational indices. The dynamic models of the production indices, however, are time varying and affected by disturbances and online corrections of these baseline operational indices are required. To this end, reinforcement learning (RL) is used to provide a data-driven optimization technique to compensate for disturbances and model approximation errors and variations. The data-driven RL approach is used in two different time scales. The samples of the predicted production indices are used at a fast sampling rate, i.e., at each sample time, and the samples of actual production indices are used at a slower sampling rate, i.e., after each operational run, to correct the baseline operational indices. The effectiveness of this automated decision procedure has been demonstrated by successful implementation of the proposed approach on a large mineral processing plant in Gansu Province, China.",https://ieeexplore.ieee.org/document/7378488/,IEEE Transactions on Industrial Informatics,April 2016,ieeexplore
10.1109/ACCESS.2021.3076783,Data-Driven Fault Diagnostics for Industrial Processes: An Application to Penicillin Fermentation Process,IEEE,Journals,"We consider the problem of fault detection and isolation for the penicillin fermentation process. A penicillin fermentation process is a highly complex and nonlinear dynamic process with batch processing. A data-driven approach is utilized for fault diagnostics due to the availability of huge batch processing data and the unavailability of an analytical model. To address the non-linearity, a subspace-aided parity-based residual generation technique is proposed by using a just-in-time learning approach. For the just-in-time learning approach, the most similar data samples are selected from the database for incoming test samples and a subspace aided parity-based residual is generated using these samples. The designed fault detection technique is implemented for the penicillin fermentation process to demonstrate real-time health monitoring of the process under sensor noise and process disturbances. Two sensor faults and an actuator fault are considered and successfully detected using the proposed technique. Further, a fault isolation approach is developed to isolate these faults and their location has been identified. A case study is given to show the improvement offered by the proposed technique for the fault detection rate and minimization of the false alarm rate as compared to the existing techniques for the penicillin fermentation process.",https://ieeexplore.ieee.org/document/9420045/,IEEE Access,2021,ieeexplore
10.1109/ACCESS.2019.2924030,Demand Response Management for Industrial Facilities: A Deep Reinforcement Learning Approach,IEEE,Journals,"As a major consumer of energy, the industrial sector must assume the responsibility for improving energy efficiency and reducing carbon emissions. However, most existing studies on industrial energy management are suffering from modeling complex industrial processes. To address this issue, a model-free demand response (DR) scheme for industrial facilities was developed. In practical terms, we first formulated the Markov decision process (MDP) for industrial DR, which presents the composition of the state, action, and reward function in detail. Then, we designed an actor-critic-based deep reinforcement learning algorithm to determine the optimal energy management policy, where both the actor (Policy) and the critic (Value function) are implemented by the deep neural network. We then confirmed the validity of our scheme by applying it to a real-world industry. Our algorithm identified an optimal energy consumption schedule, reducing energy costs without compromising production.",https://ieeexplore.ieee.org/document/8742652/,IEEE Access,2019,ieeexplore
10.1109/TCAD.2020.3012648,Exploring Edge Computing for Multitier Industrial Control,IEEE,Journals,"Industrial automation traditionally relies on local controllers implemented on microcontrollers or programmable logic controllers. With the emergence of edge computing, however, industrial automation evolves into a distributed two-tier computing architecture comprising local controllers and edge servers that communicate over wireless networks. Compared to local controllers, edge servers provide larger computing capacity at the cost of data loss over wireless networks. This article presents switching multitier control (SMC) to exploit edge computing for industrial control. SMC dynamically optimizes control performance by switching between local and edge controllers in response to changing network conditions. SMC employs a data-driven approach to derive switching policies based on classification models trained based on simulations while guaranteeing system stability based on an extended Simplex approach tailored for two-tier platforms. To evaluate the performance of industrial control over edge computing platforms, we have developed WCPS-EC, a real-time hybrid simulator that integrates simulated plants, real computing platforms, and real or simulated wireless networks. In a case study of an industrial robotic control system, SMC significantly outperformed both a local controller and an edge controller in face of varying data loss in a wireless network.",https://ieeexplore.ieee.org/document/9211472/,IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems,Nov. 2020,ieeexplore
10.1109/TII.2019.2937876,Federated Tensor Mining for Secure Industrial Internet of Things,IEEE,Journals,"In a vertical industry alliance, Internet of Things (IoT) deployed in different smart factories are similar. For example, most automobile manufacturers have the similar assembly lines and IoT surveillance systems. It is common to observe the industrial knowledge using deep learning and data mining methods based on the IoT data. However, some knowledge is not easy to be mined from only one factory's data because the samples are still few. If multiple factories within an alliance can gather their data together, more knowledge could be mined. However, the key concern of these factories is the data security. Existing matrix-based methods can guarantee the data security inside a factory but do not allow the data sharing among factories, and thus their mining performance is poor due to lack of correlation. To address this concern, in this article we propose the novel federated tensor mining (FTM) framework to federate multisource data together for tensor-based mining while guaranteeing the security. The key contribution of FTM is that every factory only needs to share its ciphertext data for security issue, and these ciphertexts are adequate for tensor-based knowledge mining due to its homomorphic attribution. Real-data-driven simulations demonstrate that FTM not only mines the same knowledge compared with the plaintext mining, but also is enabled to defend the attacks from distributed eavesdroppers and centralized hackers. In our typical experiment, compared with the matrix-based privacy-preserving compressive sensing (PPCS), FTM increases up to 24% on mining accuracy.",https://ieeexplore.ieee.org/document/8815886/,IEEE Transactions on Industrial Informatics,March 2020,ieeexplore
10.1109/ACCESS.2018.2809681,Industrial Big Data Analytics for Prediction of Remaining Useful Life Based on Deep Learning,IEEE,Journals,"Due to the recent development of cyber-physical systems, big data, cloud computing, and industrial wireless networks, a new era of industrial big data is introduced. Deep learning, which brought a revolutionary change in computer vision, natural language processing, and a variety of other applications, has significant potential for solutions providing in sophisticated industrial applications. In this paper, a concept of device electrocardiogram (DECG) is presented, and an algorithm based on deep denoising autoencoder (DDA) and regression operation is proposed for the prediction of the remaining useful life of industrial equipment. First, the concept of electrocardiogram is explained. Then, a problem statement based on manufacturing scenario is presented. Subsequently, the architecture of the proposed algorithm called integrated DDA and the algorithm workflow are provided. Moreover, DECG is compared with traditional factory information system, and the feasibility and effectiveness of the proposed algorithm are validated experimentally. The proposed concept and algorithm combine typical industrial scenario and advance artificial intelligence, which has great potential to accelerate the implementation of industry 4.0.",https://ieeexplore.ieee.org/document/8302913/,IEEE Access,2018,ieeexplore
10.1109/TII.2020.3007407,Industrial Cyber-Physical Systems-Based Cloud IoT Edge for Federated Heterogeneous Distillation,IEEE,Journals,"Deep convoloutional networks have been widely deployed in modern cyber-physical systems performing different visual classification tasks. As the fog and edge devices have different computing capacity and perform different subtasks, models trained for one device may not be deployable on another. Knowledge distillation technique can effectively compress well trained convolutional neural networks into light-weight models suitable to different devices. However, due to privacy issue and transmission cost, manually annotated data for training the deep learning models are usually gradually collected and archived in different sites. Simply training a model on powerful cloud servers and compressing them for particular edge devices failed to use the distributed data stored at different sites. This offline training approach is also inefficient to deal with new data collected from the edge devices. To overcome these obstacles, in this article, we propose the heterogeneous brain storming (HBS) method for object recognition tasks in real-world Internet of Things (IoT) scenarios. Our method enables flexible bidirectional federated learning of heterogeneous models trained on distributed datasets with a new “brain storming” mechanism and optimizable temperature parameters. In our comparison experiments, this HBS method outperformed multiple state-of-the-art single-model compression methods, as well as the newest multinetwork knowledge distillation methods with both homogeneous and heterogeneous classifiers. The ablation experiment results proved that the trainable temperature parameter into the conventional knowledge distillation loss can effectively ease the learning process of student networks in different methods. To the best of authors' knowledge, this is the first IoT-oriented method that allows asynchronous bidirectional heterogeneous knowledge distillation in deep networks.",https://ieeexplore.ieee.org/document/9134802/,IEEE Transactions on Industrial Informatics,Aug. 2021,ieeexplore
10.1109/JIOT.2019.2912022,Machine Learning-Based Network Vulnerability Analysis of Industrial Internet of Things,IEEE,Journals,"It is critical to secure the Industrial Internet of Things (IIoT) devices because of potentially devastating consequences in case of an attack. Machine learning (ML) and big data analytics are the two powerful leverages for analyzing and securing the Internet of Things (IoT) technology. By extension, these techniques can help improve the security of the IIoT systems as well. In this paper, we first present common IIoT protocols and their associated vulnerabilities. Then, we run a cyber-vulnerability assessment and discuss the utilization of ML in countering these susceptibilities. Following that, a literature review of the available intrusion detection solutions using ML models is presented. Finally, we discuss our case study, which includes details of a real-world testbed that we have built to conduct cyber-attacks and to design an intrusion detection system (IDS). We deploy backdoor, command injection, and Structured Query Language (SQL) injection attacks against the system and demonstrate how a ML-based anomaly detection system can perform well in detecting these attacks. We have evaluated the performance through representative metrics to have a fair point of view on the effectiveness of the methods.",https://ieeexplore.ieee.org/document/8693904/,IEEE Internet of Things Journal,Aug. 2019,ieeexplore
10.1109/TII.2021.3050041,Network Traffic Prediction in Industrial Internet of Things Backbone Networks: A Multitask Learning Mechanism,IEEE,Journals,"Industrial Internet of Things (IIoT), as a common industrial application of Internet of Things, has been widely deployed in recent years. End-to-end network traffic is an essential information for many network security and management functions. This article investigates the issues of IIoT-oriented backbone network traffic prediction. Predicting the traffic of IIoT backbone networks is intractable because of the large number of prior network traffic information, which needs to consume expensive network resources for sampling. Motivated by that, we propose an effective prediction mechanism using multitask learning (MTL), which is a special paradigm of transfer learning. A deep learning architecture constructed by MTL and long short-term memory is designed. This deep architecture takes advantage of link loads as additional information to improve prediction accuracy. We provide a theoretical analysis for the MTL mechanism. The effectiveness is evaluated by implementing our mechanism on real network.",https://ieeexplore.ieee.org/document/9316934/,IEEE Transactions on Industrial Informatics,Oct. 2021,ieeexplore
10.1109/TII.2019.2953275,Neural Network-Based Model Predictive Control of a Paste Thickener Over an Industrial Internet Platform,IEEE,Journals,"This article presents a real implementation of a neural network-based model predictive control scheme (NNMPC) to control an industrial paste thickener. The implementation is done over an Industrial Internet of Things (IIoT) platform designed using the seven layer reference model for IIoT systems. Modeling is achieved using an encoder-decoder with attention recurrent neural network, while MPC search is done using particle swarm optimization. An industrial evaluation is presented, which highlights the set-point tracking and disturbance rejection capabilities of the proposed NNMPC technique.",https://ieeexplore.ieee.org/document/8897590/,IEEE Transactions on Industrial Informatics,April 2020,ieeexplore
10.1109/TIE.2013.2266086,Nonlinear Model-Predictive Control for Industrial Processes: An Application to Wastewater Treatment Process,IEEE,Journals,"Because of their complex behavior, wastewater treatment processes (WWTPs) are very difficult to control. In this paper, the design and implementation of a nonlinear model-predictive control (NMPC) system are discussed. The proposed NMPC comprises a self-organizing radial basis function neural network (SORBFNN) identifier and a multiobjective optimization method. The SORBFNN with concurrent structure and parameter learning is developed as a model identifier for approximating the online states of dynamic systems. Then, the solution of the multiobjective optimization is obtained by a gradient method which can shorten the solution time of optimal control problems. Moreover, the conditions for the stability analysis of NMPC are presented. Experiments reveal that the proposed control technique gives satisfactory tracking and disturbance rejection performance for WWTPs. Experimental results on a real WWTP show the efficacy of the proposed NMPC for industrial processes in many applications.",https://ieeexplore.ieee.org/document/6523075/,IEEE Transactions on Industrial Electronics,April 2014,ieeexplore
10.1109/TIE.2011.2161652,Novel Online Speed Profile Generation for Industrial Machine Tool Based on Flexible Neuro-Fuzzy Approximation,IEEE,Journals,"Reference trajectory generation is one of the most important tasks in the control of machine tools. Such a trajectory must guarantee a smooth kinematics profile to avoid exciting the natural frequencies of the mechanical structure or servo control system. Moreover, the trajectory must be generated online to enable some feed rate adaptation mechanism working. This paper presents the online smooth speed profile generator used in trajectory interpolation in milling machines. Smooth kinematic profile is obtained by imposing limit on the jerk-which is the first derivative of acceleration. This generator is based on the neuro-fuzzy system and is able to adapt online the current feed rate to changing external conditions. Such an approach improves the machining quality, reduces the tool wear, and shortens total machining time. The proposed trajectory generation algorithm has been successfully tested and can be implemented on a multiaxis milling machine.",https://ieeexplore.ieee.org/document/5951767/,IEEE Transactions on Industrial Electronics,Feb. 2012,ieeexplore
10.1109/TCYB.2017.2761841,Off-Policy Reinforcement Learning: Optimal Operational Control for Two-Time-Scale Industrial Processes,IEEE,Journals,"Industrial flow lines are composed of unit processes operating on a fast time scale and performance measurements known as operational indices measured at a slower time scale. This paper presents a model-free optimal solution to a class of two time-scale industrial processes using off-policy reinforcement learning (RL). First, the lower-layer unit process control loop with a fast sampling period and the upper-layer operational index dynamics at a slow time scale are modeled. Second, a general optimal operational control problem is formulated to optimally prescribe the set-points for the unit industrial process. Then, a zero-sum game off-policy RL algorithm is developed to find the optimal set-points by using data measured in real-time. Finally, a simulation experiment is employed for an industrial flotation process to show the effectiveness of the proposed method.",https://ieeexplore.ieee.org/document/8100717/,IEEE Transactions on Cybernetics,Dec. 2017,ieeexplore
10.1109/ACCESS.2019.2958284,On the Generation of Anomaly Detection Datasets in Industrial Control Systems,IEEE,Journals,"In recent decades, Industrial Control Systems (ICS) have been affected by heterogeneous cyberattacks that have a huge impact on the physical world and the people's safety. Nowadays, the techniques achieving the best performance in the detection of cyber anomalies are based on Machine Learning and, more recently, Deep Learning. Due to the incipient stage of cybersecurity research in ICS, the availability of datasets enabling the evaluation of anomaly detection techniques is insufficient. In this paper, we propose a methodology to generate reliable anomaly detection datasets in ICS that consists of four steps: attacks selection, attacks deployment, traffic capture and features computation. The proposed methodology has been used to generate the Electra Dataset, whose main goal is the evaluation of cybersecurity techniques in an electric traction substation used in the railway industry. Using the Electra dataset, we train several Machine Learning and Deep Learning models to detect anomalies in ICS and the performed experiments show that the models have high precision and, therefore, demonstrate the suitability of our dataset for use in production systems.",https://ieeexplore.ieee.org/document/8926471/,IEEE Access,2019,ieeexplore
10.1109/ACCESS.2019.2941436,Opportunities and Challenges in Health Sensing for Extreme Industrial Environment: Perspectives From Underground Mines,IEEE,Journals,"Occupational health and safety hazards in the extreme work environment of underground mines remained a serious concern for both mine management and regulatory agency. Miners are often employed to perform different mining activities across the mine and are always exposed to health risks such as lung cancer. With the technology advancements, it is now possible to keep track of mine individuals and their health parameters. The current practice of health analysis is periodic in nature and is highly dependent on voluntary participation. Wearable health sensing system is an alternative solution to overcome these challenges and is able to provide insights on miners' health conditions. Timely analysis of physiological parameters of the miners is immensely helpful to minimize the injuries and can also provide preventive measures for potential health hazards. In this paper, we propose a wireless health monitoring system, especially for underground mines. The contributions of this paper are twofold. First, it presents and discusses our proposed system architecture and solution followed by challenges of such system in the context of underground mines. Second, as a preliminary analysis, detailed discussion on the wireless link behavior for reliable data transmission and communication are presented. We performed real-world experimental measurements in an operational underground coal mine considering several deployment settings in straight, near face and curved mine galleries. The communication metrics (e.g., received signal strength and packet reception rate) are extensively evaluated.",https://ieeexplore.ieee.org/document/8836591/,IEEE Access,2019,ieeexplore
10.1109/JIOT.2020.2994200,Optimization of Edge-PLC-Based Fault Diagnosis With Random Forest in Industrial Internet of Things,IEEE,Journals,"Facing globalized competition, there have been increasing requirements for safety and efficiency in smart factories, where the industrial Internet of Things can enable the monitoring of equipment's status and the detecting of faults before they go critical. Regarding cloud computing, data-driven methods running at clouds are adopted to train the model with a large amount of raw data at the beginning, then end machines upload their real-time readings to the cloud center for processing. However, this incurs considerable computational costs and may sometimes bear a severe delay. In this article, we consider a hierarchical structure where edge-PLCs are employed to gather sensed data locally and reduce communication costs. Since a single fault may be related to multiple influencing features, we want to first minimize the number of features that need to determine a fault, then try to find out the minimal set of edge-PLCs which can cover all key features so as to save the deployment cost. We propose a random-forest-based method to handle the features selection problem, and then the selection of edge-PLCs by solving the set coverage problem. Through the simulation on real data trace, we compare our method with other artificial-intelligence-based methods, such as the logistics regression model and its extensions. The results prove the efficiency and performance of the proposed method, which reaches or even exceeds the accuracy of methods using the full set of data.",https://ieeexplore.ieee.org/document/9091605/,IEEE Internet of Things Journal,Oct. 2020,ieeexplore
10.1109/TIE.2016.2612160,PLC-Based Real-Time Realization of Flatness-Based Feedforward Control for Industrial Compression Systems,IEEE,Journals,"In this paper, we present a novel programmable logic controller (PLC)-based real-time realization of a flatness-based feedforward control (FFC) scheme. The proposed approach is applied to an industrial fuel-gas compression system which is used to supply fuel gas to the gas turbines in combined cycle power plants. Due to the increasing demand for fast operation point transitions with high performance and accuracy requirements, the currently applied decentralized proportional-integral-derivative controllers appear to be not appropriate any more. Hence, by means of system simulations, a new flatness-based FFC design has been shown to provide improved control performance. In this paper, we bridge the gap between simulation-based control design and practical applicability, in that, we present the real-time realization of the approach on a PLC. Furthermore, the PLC-based controller is tested on a hardware-in-the-loop platform running with a complex compression system model in real time. The results reveal that the flatness-based control design can be implemented on a real compressor system.",https://ieeexplore.ieee.org/document/7572893/,IEEE Transactions on Industrial Electronics,Feb. 2017,ieeexplore
10.1109/TII.2019.2940099,Performance Supervised Fault Detection Schemes for Industrial Feedback Control Systems and their Data-Driven Implementation,IEEE,Journals,"This article addresses performance supervised fault detection (PSFD) issues for industrial feedback control systems based on performance degradation prediction. To be specific, three performance indicators are first introduced based on Bellman equation to predict system performance degradations for industrial processes with the aid of machine learning techniques. Based on them, three PSFD schemes are proposed by embedding the performance indicators as supervising information. In this context, the data-driven implementation of PSFD schemes are investigated for linear systems with unmeasurable state variables. A case study on rolling mill process, a typical benchmark in the steel manufacturing processes, is given at the end of this article to illustrate the applications of the proposed fault detection schemes.",https://ieeexplore.ieee.org/document/8827307/,IEEE Transactions on Industrial Informatics,April 2020,ieeexplore
10.1109/TII.2019.2903224,Risk-Based Scheduling of Security Tasks in Industrial Control Systems With Consideration of Safety,IEEE,Journals,"Industrial control systems (ICSs) in networked environments face severe cyber-security risks and challenges. A timely response to cyber-attacks is of paramount importance for mitigating risks. However, the security policy developed for an ICS may be conflicting with the ICS's safety policy, on which much attention has been paid for a long time in industrial control. An inappropriate enforcement of the security policy may deteriorate the ICS performance or even result in severe unexpected consequences. To tackle this problem, a risk-based security task scheduling approach is presented for ICSs with consideration of the safety policy. It ensures a timely response to cyber-attacks without compromising safety. More specifically, the approach reconciles security tasks and safety tasks according to a designed resolution policy, so as to acquire contradiction-free security and safety (S&amp;S) tasks. Then, a real-time risk assessment method is developed to characterize the subtle change of the system risk with the implementation of the reconciled S&amp;S tasks. After that, a task scheduling method is designed with the risk as the optimization objective, i.e., it searches the optimal task scheduling scheme by minimizing the risk posture. The resulting scheduling scheme ensures the smooth implementation of the S&amp;S policy, which reflects the optimal recovery process against the risk. Finally, case studies on a hardware-in-the-loop testbed are conducted to demonstrate the effectiveness of the proposed approach.",https://ieeexplore.ieee.org/document/8661651/,IEEE Transactions on Industrial Informatics,May 2020,ieeexplore
10.1109/ACCESS.2020.3045563,SDN Enhanced Resource Orchestration of Containerized Edge Applications for Industrial IoT,IEEE,Journals,"With the rise of the Industrial Internet of Things (IIoT), there is an intense pressure on resource and performance optimization leveraging on existing technologies, such as Software Defined Networking (SDN), edge computing, and container orchestration. Industry 4.0 emphasizes the importance of lean and efficient operations for sustainable manufacturing. Achieving this goal would require engineers to consider all layers of the system, from hardware to software, and optimizing for resource efficiency at all levels. This emphasizes the need for container-based virtualization tools such as Docker and Kubernetes, offering Platform as a Service (PaaS), while simultaneously leveraging on edge technologies to reduce related latencies. For network management, SDN is poised to offer a cost-effective and dynamic scalability solution by customizing packet handling for various edge applications and services. In this paper, we investigate the energy and latency trade-offs involved in combining these technologies for industrial applications. As a use case, we emulate a 3D-drone-based monitoring system aimed at providing real-time visual monitoring of industrial automation. We compare a native implementation to a containerized implementation where video processing is orchestrated while streaming is handled by an external UE representing the IIoT device. We compare these two scenarios for energy utilization, latency, and responsiveness. Our test results show that only roughly 16 percent of the total power consumption happens on the mobile node when orchestrated. Virtualization adds up about 4.5 percent of the total power consumption while the latency difference between the two approaches becomes negligible after the streaming session is initialized.",https://ieeexplore.ieee.org/document/9296769/,IEEE Access,2020,ieeexplore
10.1109/TVCG.2020.2969007,Security in Process: Visually Supported Triage Analysis in Industrial Process Data,IEEE,Journals,"Operation technology networks, i.e. hard- and software used for monitoring and controlling physical/industrial processes, have been considered immune to cyber attacks for a long time. A recent increase of attacks in these networks proves this assumption wrong. Several technical constraints lead to approaches to detect attacks on industrial processes using available sensor data. This setting differs fundamentally from anomaly detection in IT-network traffic and requires new visualization approaches adapted to the common periodical behavior in OT-network data. We present a tailored visualization system that utilizes inherent features of measurements from industrial processes to full capacity to provide insight into the data and support triage analysis by laymen and experts. The novel combination of spiral plots with results from anomaly detection was implemented in an interactive system. The capabilities of our system are demonstrated using sensor and actuator data from a real-world water treatment process with introduced attacks. Exemplary analysis strategies are presented. Finally, we evaluate effectiveness and usability of our system and perform an expert evaluation.",https://ieeexplore.ieee.org/document/8968740/,IEEE Transactions on Visualization and Computer Graphics,1 April 2020,ieeexplore
10.1109/ACCESS.2020.3020799,Short-Term Industrial Load Forecasting Based on Ensemble Hidden Markov Model,IEEE,Journals,"Short-term load forecasting (STLF) for industrial customers has been an essential task to reduce the cost of energy transaction and promote the stable operation of smart grid throughout the development of the modern power system. Traditional STLF methods commonly focus on establishing the non-linear relationship between loads and features, but ignore the temporal relationship between them. In this paper, an STLF method based on ensemble hidden Markov model (e-HMM) is proposed to track and learn the dynamic characteristics of industrial customer's consumption patterns in correlated multivariate time series, thereby improving the prediction accuracy. Specifically, a novel similarity measurement strategy of log-likelihood space is designed to calculate the log-likelihood value of the multivariate time series in sliding time windows, which can effectively help the hidden Markov model (HMM) to capture the dynamic temporal characteristics from multiple historical sequences in similar patterns, so that the prediction accuracy is greatly improved. In order to improve the generalization ability and stability of a single HMM, we further adopt the framework of Bagging ensemble learning algorithm to reduce the prediction errors of a single model. The experimental study is implemented on a real dataset from a company in Hunan Province, China. We test the model in different forecasting periods. The results of multiple experiments and comparison with several state-of-the-art models show that the proposed approach has higher prediction accuracy.",https://ieeexplore.ieee.org/document/9183956/,IEEE Access,2020,ieeexplore
10.1109/TII.2020.3047675,Siamese Neural Network Based Few-Shot Learning for Anomaly Detection in Industrial Cyber-Physical Systems,IEEE,Journals,"With the increasing population of Industry 4.0, both AI and smart techniques have been applied and become hotly discussed topics in industrial cyber-physical systems (CPS). Intelligent anomaly detection for identifying cyber-physical attacks to guarantee the work efficiency and safety is still a challenging issue, especially when dealing with few labeled data for cyber-physical security protection. In this article, we propose a few-shot learning model with Siamese convolutional neural network (FSL-SCNN), to alleviate the over-fitting issue and enhance the accuracy for intelligent anomaly detection in industrial CPS. A Siamese CNN encoding network is constructed to measure distances of input samples based on their optimized feature representations. A robust cost function design including three specific losses is then proposed to enhance the efficiency of training process. An intelligent anomaly detection algorithm is developed finally. Experiment results based on a fully labeled public dataset and a few labeled dataset demonstrate that our proposed FSL-SCNN can significantly improve false alarm rate (FAR) and F1 scores when detecting intrusion signals for industrial CPS security protection.",https://ieeexplore.ieee.org/document/9311786/,IEEE Transactions on Industrial Informatics,Aug. 2021,ieeexplore
10.1109/JIOT.2019.2963635,Toward Edge-Based Deep Learning in Industrial Internet of Things,IEEE,Journals,"As a typical application of the Internet of Things (IoT), the Industrial IoT (IIoT) connects all the related IoT sensing and actuating devices ubiquitously so that the monitoring and control of numerous industrial systems can be realized. Deep learning, as one viable way to carry out big-data-driven modeling and analysis, could be integrated in IIoT systems to aid the automation and intelligence of IIoT systems. As deep learning requires large computation power, it is commonly deployed in cloud servers. Thus, the data collected by IoT devices must be transmitted to the cloud for training process, contributing to network congestion and affecting the IoT network performance as well as the supported applications. To address this issue, in this article, we leverage the fog/edge computing paradigm and propose an edge computing-based deep learning model, which utilizes edge computing to migrate the deep learning process from cloud servers to edge nodes, reducing data transmission demands in the IIoT network and mitigating network congestion. Since edge nodes have limited computation ability compared to servers, we design a mechanism to optimize the deep learning model so that its requirements for computational power can be reduced. To evaluate our proposed solution, we design a testbed implemented in the Google cloud and deploy the proposed convolutional neural network (CNN) model, utilizing a real-world IIoT data set to evaluate our approach.<sup>1</sup> Our experimental results confirm the effectiveness of our approach, which cannot only reduce the network traffic overhead for IIoT but also maintain the classification accuracy in comparison with several baseline schemes.<sup>1</sup>Certain commercial equipment, instruments, or materials are identified in this article in order to specify the experimental procedure adequately. Such identification is not intended to imply recommendation or endorsement by the National Institute of Standards and Technology, nor is it intended to imply that the materials or equipment identified are necessarily the best available for the purpose.",https://ieeexplore.ieee.org/document/8948000/,IEEE Internet of Things Journal,May 2020,ieeexplore
10.1109/TII.2020.3038780,Toward Secure Data Fusion in Industrial IoT Using Transfer Learning,IEEE,Journals,"As an emerging technology, the industrial Internet of Things (IIoT) can promote the development of industrial intelligence, improve production efficiency, and reduce manufacturing costs. In IIoT, the improvement and progress of industrial production and applications are inseparable from data fusion, a process that realizes the collection, analysis, and processing of the massive IoT data generated by industrial equipment and applications. IIot demands a real-time, effective, and privacy-preserving data fusion process. However, the existing works need to train different learning models for data analysis, which cannot meet real-time requirements in IIoT. Meanwhile, the lack of defense against internal attacks and the difficulty to balance system performance and privacy protection hinder the effectiveness and privacy protection in the data fusion process. To solve the abovementioned problems, in this article, we propose a new transfer learning-based secure data fusion strategy (TSDF) for IIoT. The proposed TSDF consists of three parts, guidance based deep deterministic policy gradient (GDDPG) algorithm for task classification, transfer learning based GDDPG for grouping of task receivers, and a multiblockchain mechanism for privacy preservation. The experiment results show that TSDF can achieve high system throughput and low latency, providing privacy preservation in data fusion under various IIoT application environments.",https://ieeexplore.ieee.org/document/9262056/,IEEE Transactions on Industrial Informatics,Oct. 2021,ieeexplore
10.1109/TNNLS.2019.2935033,Weighted Broad Learning System and Its Application in Nonlinear Industrial Process Modeling,IEEE,Journals,"Broad learning system (BLS) is a novel neural network with effective and efficient learning ability. BLS has attracted increasing attention from many scholars owing to its excellent performance. This article proposes a weighted BLS (WBLS) based on BLS to tackle the noise and outliers in an industrial process. WBLS provides a unified framework for easily using different methods of calculating the weighted penalty factor. Using the weighted penalty factor to constrain the contribution of each sample to modeling, the normal and abnormal samples were allocated higher and lower weights to increase and decrease their contributions, respectively. Hence, the WBLS can eliminate the bad effect of noise and outliers on the modeling. The weighted ridge regression algorithm is used to compute the algorithm solution. Weighted incremental learning algorithms are also developed using the weighted penalty factor to tackle the noise and outliers in the additional samples and quickly increase nodes or samples without retraining. The proposed weighted incremental learning algorithms provide a unified framework for using different methods of computing weights. We test the feasibility of the proposed algorithms on some public data sets and a real-world application. Experiment results show that our method has better generalization and robustness.",https://ieeexplore.ieee.org/document/8833523/,IEEE Transactions on Neural Networks and Learning Systems,Aug. 2020,ieeexplore
10.1109/IJCNN48605.2020.9207496,"""I’m Sorry Dave, I’m Afraid I Can’t Do That"" Deep Q-Learning from Forbidden Actions",IEEE,Conferences,"The use of Reinforcement Learning (RL) is still restricted to simulation or to enhance human-operated systems through recommendations. Real-world environments (e.g. industrial robots or power grids) are generally designed with safety constraints in mind implemented in the shape of valid actions masks or contingency controllers. For example, the range of motion and the angles of the motors of a robot can be limited to physical boundaries. Violating constraints thus results in rejected actions or entering in a safe mode driven by an external controller, making RL agents incapable of learning from their mistakes. In this paper, we propose a simple modification of a state-of-the-art deep RL algorithm (DQN), enabling learning from forbidden actions. To do so, the standard Q-learning update is enhanced with an extra safety loss inspired by structured classification. We empirically show that it reduces the number of hit constraints during the learning phase and accelerates convergence to near-optimal policies compared to using standard DQN. Experiments are done on a Visual Grid World Environment and the TextWorld domain.",https://ieeexplore.ieee.org/document/9207496/,2020 International Joint Conference on Neural Networks (IJCNN),19-24 July 2020,ieeexplore
10.1109/RTCSA.2018.00012,A Case Study of Cyber-Physical System Design: Autonomous Pick-and-Place Robot,IEEE,Conferences,"Although modern robots in warehousing systems can perform adequately in a goods-to-person model using hand-designed algorithms that are specialized to a particular environment, developing a robotic system that is capable of handling new products at an inexpensive cost remains a challenge. A conspicuous example of this challenge is seen in Amazon's use of autonomous robots to fetch customers' orders in their massive warehouses. To encourage advance in this technology, Amazon organized the competition, Amazon Picking Challenge that asked participants to develop their own hardware and software for the general task of picking a designated set of products from inventory shelves and then placing them at a target location (called a pick-and-place task). Current technology for pick-and-place tasks is still insufficient to meet the demand for low-cost automation. Handling awkward or oddly shaped object must still depend on hand-programming or specialized robotic systems, making manufacturing automation less flexible and expensive. In this paper, we shall present the design and implementation of a software system that is a step in advancing the technology toward full automation at reasonable costs. Our system integrates a set of state-of-the-art techniques in computer vision, deep-learning, trajectory optimization, visual servoing to create a library of skills that can be composed to perform a variety of robotic tasks. We demonstrate the capability of our system for performing autonomous pick-and-place tasks with an implementation using Hoppy, an industrial robotic arm in an environment similar to the Amazon Picking Challenge.",https://ieeexplore.ieee.org/document/8607230/,2018 IEEE 24th International Conference on Embedded and Real-Time Computing Systems and Applications (RTCSA),28-31 Aug. 2018,ieeexplore
10.1109/ACCC51160.2020.9347897,A Comparative Analysis of Kinematics of Industrial Robot KUKA KR 60–3 Using Scientific Computing Languages,IEEE,Conferences,"In the field of robotics, there are kinematic analysis methods that are responsible for describing the positions and orientations of the end effectors, as well as the angles, velocities and trajectories of industrial robots; such techniques are: forward kinematics, inverse kinematics and velocity kinematics. For the solutions of these complex mathematical calculations, the use of scientific computing languages or programs is required; which more and more algorithms, libraries and complements are implemented, that achieve a reduction in programming hours and result in the creation of better solutions in areas of all kinds. For this reason, the kinematics of the Industrial Robot KUKA KR 60-3 was programmed in the languages and programs most used in scientific computing, with the aim of comparing the performance (real time) when carrying out symbolic and numerical analysis in said studies.",https://ieeexplore.ieee.org/document/9347897/,2020 Asia Conference on Computers and Communications (ACCC),18-20 Sept. 2020,ieeexplore
10.1109/INOCON50539.2020.9298344,A Comparative Study of Distinct Speed Controllers for a Separately Excited DC Motor (SEDM),IEEE,Conferences,"A Separately Excited DC Motor (SEDM) is most employed machine, especially in the industrial sector. Tractions, conveyors, heavy planners, actuators are some of the most commonly known machines which are dependent on the SEDM for their stable and efficient operation. This paper proposes an application of the Artificial Neural Networks (ANNs), one of the most accurate and efficient techniques for non-linear systems, to achieve a precise trajectory control of the speed for a real-time system. In this paper, a comparison of distinct controllers such as Proportional Integral (PI) and Fuzzy Logic Controller (FLC), ANNs by analyzing the system attributes like peak overshoot time, steady-state time for varying load conditions to determine the most efficient speed controller for SEDM is implemented. The neural control scheme comprises two parts: the neural identifier and the neural controller which are used to regulate the motor speed and trigger the control signal respectively. Known for its self-adapting, learning ability, and super-fast computing features of ANN, the NARMA L-2 controller is well-suited as a speed controller for DC motors.",https://ieeexplore.ieee.org/document/9298344/,2020 IEEE International Conference for Innovation in Technology (INOCON),6-8 Nov. 2020,ieeexplore
10.1109/FDL53530.2021.9568376,A Container-based Design Methodology for Robotic Applications on Kubernetes Edge-Cloud architectures,IEEE,Conferences,"Programming modern Robots' missions and behavior has become a very challenging task. The always increasing level of autonomy of such platforms requires the integration of multi-domain software applications to implement artificial intelligence, cognition, and human-robot/robot-robot interaction applications. In addition, to satisfy both functional and nonfunctional requirements such as reliability and energy efficiency, robotic SW applications have to be properly developed to take advantage of heterogeneous (Edge-Fog-Cloud) architectures. In this context, containerization and orchestration are becoming a standard practice as they allow for better information flow among different network levels as well as increased modularity in the use of software components. Nevertheless, the adoption of such a practice along the design flow, from simulation to the deployment of complex robotic applications by addressing the de-facto development standards (i.e., robotic operating system - ROS - compliancy for robotic applications) is still an open problem. We present a design methodology based on Docker and Kubernetes that enables containerization and orchestration of ROS-based robotic SW applications for heterogeneous and hierarchical HW architectures. The design methodology allows for (i) integration and verification of multi-domain components since early in the design flow, (ii) task-to-container mapping techniques to guarantee minimum overhead in terms of performance and memory footprint, and (iii) multi-domain verification of functional and non-functional constraints before deployment. We present the results obtained in a real case of study, in which the design methodology has been applied to program the mission of a Robotnik RB-Kairos mobile robot in an industrial agile production chain. The source code of the mobile robot is publicly available on GitHub.",https://ieeexplore.ieee.org/document/9568376/,2021 Forum on specification & Design Languages (FDL),8-10 Sept. 2021,ieeexplore
10.1109/AIAM48774.2019.00157,A Digital Twin-Based Approach for Quality Control and Optimization of Complex Product Assembly,IEEE,Conferences,"To address the problems caused by low ability of quality analysis and decision-making in the process of complex product assembly, in this paper, we proposed a digital twin-based approach for quality control and optimization of complex product assembly, by providing a digital twin system to realize the timely and precisely interactive mapping between the physical world and digital world. Specifically, a quality control and optimization mechanism is presented, which provides the theoretical support to the realization of the digital twin-based approach. A data-driven quality control model is introduced to solve the optimization problem by considering the panoramic assembly quality data. A digital twin system for complex product assembly is elaborated by providing detailed deployment and implementation procedures, which includes (1) building of the digital entity of an assembly line, (2) real-time online sensing in multi-source heterogeneous environment, (3) real-time simulation of equipment and assembly process, (4) realization of the intelligent production scheduling under uncertainty conditions, and (5) dynamical adjustment of the assembly process. Finally, the paper presents the validation results considering the practical applications of the proposed approach in real industrial fields.",https://ieeexplore.ieee.org/document/8950866/,2019 International Conference on Artificial Intelligence and Advanced Manufacturing (AIAM),16-18 Oct. 2019,ieeexplore
10.1109/CYBER53097.2021.9588142,A Fast and Energy-Saving Neural Network Inference Method for Fault Diagnosis of Industrial Equipment Based on Edge-End Collaboration,IEEE,Conferences,"Data-driven fault diagnosis algorithms represented by deep learning have been widely used in industrial equipment fault diagnosis. However, the lack of real-time performance has always restricted the development of such methods. With the development of edge computing, many edge and end computing devices are deployed in industrial environments. For this distributed computing environment, we propose a distributed neural network inference method with edge-end collaboration. This method uses an edge server to cooperate with multiple end devices for network inference. In the diagnosis of industrial equipment, it can increase the speed of inference, reduce the traffic of the edge network, and help the application of deep neural networks in industrial environments.",https://ieeexplore.ieee.org/document/9588142/,"2021 IEEE 11th Annual International Conference on CYBER Technology in Automation, Control, and Intelligent Systems (CYBER)",27-31 July 2021,ieeexplore
10.1109/ICRA.2019.8793690,A Fog Robotics Approach to Deep Robot Learning: Application to Object Recognition and Grasp Planning in Surface Decluttering,IEEE,Conferences,"The growing demand of industrial, automotive and service robots presents a challenge to the centralized Cloud Robotics model in terms of privacy, security, latency, bandwidth, and reliability. In this paper, we present a `Fog Robotics' approach to deep robot learning that distributes compute, storage and networking resources between the Cloud and the Edge in a federated manner. Deep models are trained on non-private (public) synthetic images in the Cloud; the models are adapted to the private real images of the environment at the Edge within a trusted network and subsequently, deployed as a service for low-latency and secure inference/prediction for other robots in the network. We apply this approach to surface decluttering, where a mobile robot picks and sorts objects from a cluttered floor by learning a deep object recognition and a grasp planning model. Experiments suggest that Fog Robotics can improve performance by sim-to-real domain adaptation in comparison to exclusively using Cloud or Edge resources, while reducing the inference cycle time by 4× to successfully declutter 86% of objects over 213 attempts.",https://ieeexplore.ieee.org/document/8793690/,2019 International Conference on Robotics and Automation (ICRA),20-24 May 2019,ieeexplore
10.1109/HPCC.and.EUC.2013.124,A Hypervisor for MIPS-Based Architecture Processors - A Case Study in Loongson Processors,IEEE,Conferences,"Loongson is a family of general purpose processors based on MIPS architecture designed and manufactured in Mainland China. With the maturity of Loongson CPUs, applications are widely available with the increasing development of software tools and hardware platforms by research teams in academia and industry. In recent years, products based on Loongson have been mainly used in education, personal computers and server systems. Meanwhile, it is not yet popularly used in industrial real-time control fields, so such products have large room and potential to further development and deployment. The M-Hyper visor discussed in this paper is a real-time hyper visor designed for MIPS architecture and implemented in Loongson2F processor. It is based on the management program of para-virtualization whilst multiple partitions are scheduled to execute according to their priorities. The design and implementation of M-Hyper visor is discussed, along with details as timer, interrupts, memory management, partition loading and scheduling, to enrich real-time virtualized applications for MIPS architecture. Evaluation results show the performance and viability of proposed design, being promising to new deployments.",https://ieeexplore.ieee.org/document/6832006/,2013 IEEE 10th International Conference on High Performance Computing and Communications & 2013 IEEE International Conference on Embedded and Ubiquitous Computing,13-15 Nov. 2013,ieeexplore
10.1109/CAMAD50429.2020.9209305,A Joint Decentralized Federated Learning and Communications Framework for Industrial Networks,IEEE,Conferences,"Industrial wireless networks are pushing towards distributed architectures moving beyond traditional server-client transactions. Paired with this trend, new synergies are emerging among sensing, communications and Machine Learning (ML) co-design, where resources need to be distributed across different wireless field devices, acting as both data producers and learners. Considering this landscape, Federated Learning (FL) solutions are suitable for training a ML model in distributed systems. In particular, decentralized FL policies target scenarios where learning operations must be implemented collaboratively, without relying on the server, and by exchanging model parameters updates rather than training data over capacity-constrained radio links. This paper proposes a real-time framework for the analysis of decentralized FL systems running on top of industrial wireless networks rooted in the popular Time Slotted Channel Hopping (TSCH) radio interface of the IEEE 802.15.4e standard. The proposed framework is suitable for neural networks trained via distributed Stochastic Gradient Descent (SGD), it quantifies the effects of model pruning, sparsification and quantization, as well as physical and link layer constraints, on FL convergence time and learning loss. The goal is to set the fundamentals for comprehensive methods and procedures supporting decentralized FL pre-deployment design. The proposed tool can be thus used to optimize the deployment of the wireless network and the ML model before its actual installation. It has been verified based on real data targeting smart robotic-assisted manufacturing.",https://ieeexplore.ieee.org/document/9209305/,2020 IEEE 25th International Workshop on Computer Aided Modeling and Design of Communication Links and Networks (CAMAD),14-16 Sept. 2020,ieeexplore
10.1109/ICICRS46726.2019.9555893,A Novel Approach Towards Industrial Waste Management Using Q-Learning,IEEE,Conferences,"Industrial waste refers to the unwanted solid, liquid and gaseous wastes resulting from an industrial operation. And the collection, transport, processing, disposal and monitoring of these wastes is called as Industrial Waste Management. Industrial Waste has adhered to severe pollution to air, water and soil in the recent years affecting environment and human health. So, its proper and effective management has become important also as the industries’ liability towards the environment. Since many industries don’t have in-house processing plants, they tie-up with industries for the task. This paper focuses on using Q-learning for effective path-planning from generator industries(waste generating) to aggregator industries(waste processing). Q-learning is used for predicting the most efficient path by learning from its own experiences of rewards and penalties and so we don’t need to train the model which therefore increases the efficiency. This work can be implemented at a place where industrial waste is generated and can be very helpful in managing the same. This could be implemented as a service and handed over to the probable customers or government organizations which provide waste management services to the industries. This submission is a step towards automation of the Industrial Waste Management and helps in planning of waste management in real-time.",https://ieeexplore.ieee.org/document/9555893/,2019 International Conference on Intelligent Computing and Remote Sensing (ICICRS),19-20 July 2019,ieeexplore
10.1109/DDCLS49620.2020.9275082,A Novel Incremental Gaussian Mixture Regression and Its Application for Time-varying Multimodal Process Quality Prediction,IEEE,Conferences,"Data-driven soft sensor approach has been widely applied on real-time prediction and control of difficult-to-measure quality variables. Among these approaches, the Gaussian mixture regression (GMR) carries the potential of dealing with nonlinear and non-Gaussian industry problems, which has drawn increasing popularity and attentions in recent years. However, the fluctuation of raw materials, change of process environment, aging of instruments and other factors will have an effect on system performances over time. Hence, the lack of adaptive mechanism will make the GMR difficult to suit for time-varying processes and may cause large prediction errors. In order to model time-varying industrial processes and improve the adaptability of the conventional GMR, an adaptive soft sensor based on incremental Gaussian mixture regression (IGMR) is proposed in this paper. The incremental idea is integrated and an adaptive mechanism is added, which endow the proposed IGMR with the capability of adapting to new data in online environment. Compared to the moving window GMR (MWGMR) and the just-in-time learning GMR (JITLGMR), the feasibility and effectiveness of the proposed IGMR are verified both in a numerical simulation and a real-life industrial process experiment.",https://ieeexplore.ieee.org/document/9275082/,2020 IEEE 9th Data Driven Control and Learning Systems Conference (DDCLS),20-22 Nov. 2020,ieeexplore
10.1109/ICNSC48988.2020.9238123,A Novel Reinforcement-Learning-Based Approach to Scientific Workflow Scheduling,IEEE,Conferences,"Recently, the Cloud Computing paradigm is becoming increasingly popular in supporting large-scale and complex workflow applications. The workflow scheduling problem, which refers to finding the most suitable resource for each task of the workflow to meet user defined quality of service (QoS), attracts considerable research attention. Multi-objective optimization algorithms in workflow scheduling have many limitations, e.g., the encoding schemes in most existing heuristic-based scheduling algorithms require prior experts' knowledge and thus they can be ineffective when scheduling workflows upon dynamic cloud infrastructures with real-time. To address this problem, we propose a novel Reinforcement-Learning-Based algorithm to multi-workflow scheduling over IaaS clouds. The proposed algorithm aims at optimizing make-span and dwell time and is to achieve a unique set of correlated equilibrium solution. In the experiment, our algorithm is evaluated for famous scientific workflow templates and real-world industrial IaaS cloud platforms by a simulation process and we compare our algorithm to the current state-of-the-art heuristic algorithms, e.g., NSGA-II, MOPSO, GTBGA. The result shows that our algorithm performs better than compared algorithm.",https://ieeexplore.ieee.org/document/9238123/,"2020 IEEE International Conference on Networking, Sensing and Control (ICNSC)",30 Oct.-2 Nov. 2020,ieeexplore
10.1109/APARM49247.2020.9209530,A PdM Framework Through the Event-based Genomics of Machine Breakdown,IEEE,Conferences,"A novel event-based predictive maintenance framework based on sensor signal measurements and regressive predictions to minimise machine breakdown and component failure is proposed. Such capabilities will be complemented by Event-Clustering technique to cluster and remove less impact sensor signals and also build breakdown genomics from the root of a failure in order to predict the upcoming machine breakdowns and components failures. The creation of machine breakdown genomics requires the knowledge of systems state observed as well as the state change at specified time intervals (discretization). The proposed framework is applied to a real application case study. An industrial case study of a continuous compression moulding machine that manufactures the plastic bottle closure (caps) in the beverage industry has been considered as an experiment. The machine breakdown genomics theory is tested in this case to build the sequence of events or the genomics of breakdown, where sequences of contiguous events lead to failure or healthy machine status. This is complemented by the Regression Event-Tracker method to estimates the condition monitoring of the components and provide components real-time remaining useful life estimation. The Weibull failure-rate analysis is carried out on the remaining useful life estimates for each element to understand and estimate the mean time to failure for the manufacturing machine.",https://ieeexplore.ieee.org/document/9209530/,2020 Asia-Pacific International Symposium on Advanced Reliability and Maintenance Modeling (APARM),20-23 Aug. 2020,ieeexplore
10.1109/IRI.2019.00040,A Power Efficient Neural Network Implementation on Heterogeneous FPGA and GPU Devices,IEEE,Conferences,"Deep neural networks (DNNs) have seen tremendous industrial successes in various applications, including image recognition, machine translation, audio processing, etc. However, they require massive amounts of computations and take a lot of time to process. This quickly becomes a problem in mobile and handheld devices where real-time multimedia applications such as face detection, disaster management, and CCTV require lightweight, fast, and effective computing solutions. The objective of this project is to utilize specialized devices such as Field Programmable Gate Arrays (FPGAs) and Graphics Processing Units (GPUs) in a heterogeneous computing environment to accelerate the deep learning computations with the constraints of power efficiency. We investigate an efficient DNN implementation and make use of FPGA for fully-connected layer and GPU for floating-point operations. This requires the deep neural network architecture to be implemented in a model parallelism system where the DNN model is broken down and processed in a distributed fashion. The proposed heterogeneous framework idea is implemented using an Nvidia TX2 GPU and a Xilinx Artix-7 FPGA. Experimental results indicate that the proposed framework can achieve faster computation and much lower power consumption.",https://ieeexplore.ieee.org/document/8843495/,2019 IEEE 20th International Conference on Information Reuse and Integration for Data Science (IRI),30 July-1 Aug. 2019,ieeexplore
10.1109/ASE.2019.00102,A Quantitative Analysis Framework for Recurrent Neural Network,IEEE,Conferences,"Recurrent neural network (RNN) has achieved great success in processing sequential inputs for applications such as automatic speech recognition, natural language processing and machine translation. However, quality and reliability issues of RNNs make them vulnerable to adversarial attacks and hinder their deployment in real-world applications. In this paper, we propose a quantitative analysis framework - DeepStellar - to pave the way for effective quality and security analysis of software systems powered by RNNs. DeepStellar is generic to handle various RNN architectures, including LSTM and GRU, scalable to work on industrial-grade RNN models, and extensible to develop customized analyzers and tools. We demonstrated that, with DeepStellar, users are able to design efficient test generation tools, and develop effective adversarial sample detectors. We tested the developed applications on three real RNN models, including speech recognition and image classification. DeepStellar outperforms existing approaches three hundred times in generating defect-triggering tests and achieves 97% accuracy in detecting adversarial attacks. A video demonstration which shows the main features of DeepStellar is available at: https://sites.google.com/view/deepstellar/tool-demo.",https://ieeexplore.ieee.org/document/8952565/,2019 34th IEEE/ACM International Conference on Automated Software Engineering (ASE),11-15 Nov. 2019,ieeexplore
10.1109/ICRAE50850.2020.9310899,A ROS Based Automatic Control Implementation for Precision Landing on Slow Moving Platforms Using a Cooperative Fleet of Rotary-Wing UAVs,IEEE,Conferences,"In this paper we present an industrial implementation of an efficient method to solve the problem of the automatic precision landing for rotary-wing UAVs, ready to be used inside a cooperative fleet of drones. The realized software module and tests are part of a large industrial R&amp;D Vitrociset project called SWARM: an AI-Enabled Command and Control (C&amp;C) system, able to execute and review ISR missions for mini/micro cooperative fleets of heterogeneous UAVs. Preparatory to the presented results, it was the identification of a non-linear mathematical model as well as the realization of a robust PID-based control system capable of controlling a single drone of the fleet. A discrete-time Kalman filter was integrated and tested to estimate the possible displacement of the landing points, in order to improve the control law through predictive connotations in case of slow moving tags. The presented approach is featured by the balance between computational efficiency and versatility, in particular in the discovering stage of multiple and different AprilTag during the landing phase. The still under test software module uses the Open Source Robotic Operating System (ROS) libraries for both the acquisition of the data necessary to the control laws, and for the execution of the computer vision algorithms implemented for the precision landing. After analyses and simulations campaigns in a synthetic environment and multiple hardware in the loop (HIL) stress tests, the final prototype algorithm was deployed on a commercial-off-the-shelf mini-class UAV, demonstrating landing capacity on a fixed target with an error of less than ten centimeters; moreover, with slow-moving tags, appreciable tracking abilities emerged on sufficiently smooth trajectories. A special interface with the HIL flight controller was then integrated, with the capability of using its telemetry data for distributing them to all the members of the cooperative fleet, making it possible to access the real-time estimate of the states of each single drone, and making each one of them aware of the selected landing areas of the others, by navigation sensors data fusion with a five meters GPS precision.",https://ieeexplore.ieee.org/document/9310899/,2020 5th International Conference on Robotics and Automation Engineering (ICRAE),20-22 Nov. 2020,ieeexplore
10.1109/MCDM.2007.369428,A Review of Two Industrial Deployments of Multi-criteria Decision-making Systems at General Electric,IEEE,Conferences,"Two industrial deployments of multi-criteria decision-making systems at General Electric are reviewed from the perspective of their multi-criteria decision-making component similarities and differences. The motivation is to present a framework for multi-criteria decision-making system development and deployment. The first deployment is a financial portfolio management system that integrates hybrid multi-objective optimization and interactive Pareto frontier decision-making techniques to optimally allocate financial assets while considering multiple measures of return and risk, and numerous regulatory constraints. The second deployment is a power plant management system that integrates predictive modeling based on neural networks, optimization based on multi-objective evolutionary algorithms, and automated decision-making based on Pareto frontier techniques. The integrated approach, embedded in a real-time plant optimization and control software environment dynamically optimizes emissions and efficiency while simultaneously meeting load demands and other operational constraints in a complex real-world power plant",https://ieeexplore.ieee.org/document/4222994/,2007 IEEE Symposium on Computational Intelligence in Multi-Criteria Decision-Making,1-5 April 2007,ieeexplore
10.1109/PCIC31437.2018.9080444,"A Smart Condition Monitoring System for HV Networks with Artificial Intelligence, Augmented Reality and Virtual Reality: Copyright Material IEEE, Paper No. PCIC-2018-37",IEEE,Conferences,"The authors present a conceptual design for a SMART asset monitoring solution for high voltage (HV) networks in the petrochemical industry. The paper discusses the potential for incorporating artificial intelligence (AI), augmented reality (AR) and virtual reality (VR) into an application of the Industrial Internet of Things (IIoT) for condition monitoring. The paper is a continuation of the work presented by the authors at the IEEE-PCIC 2017 conference in Calgary. The proposed asset management system analyses condition monitoring (CM) data and assesses the risk of failure data across complete HV networks. Knowledge of deteriorating asset condition provides the operator with an advanced, early warning of incipient mechanical and electrical faults. With knowledge of the severity and source of such faults, pinpointed preventative maintenance interventions can then made during planned maintenance outages. The complete HV network asset monitoring solution described includes permanent sensors and monitoring nodes deployed at strategic locations across the network. Processed data is passed via a local area network to local servers and then via secure data cloud transmission to a centralized monitoring server located at the CM headquarters. This central server operates a CM database that logs, displays, benchmarks and trends the condition data with comparison to a statistically-significant database of measurements. It is proposed in the IIoT solution proposed that this database will be downloadable to a smartphone/tablet for use by the field engineer. The monitoring technology will likely also incorporate a number of AI machine learning software modules for the de-noising of raw signals and the diagnosis of different types of defects within different types of HV plant items. The proposed SMART CM system includes an advanced graphical user interface (GUI) for viewing HV asset CM data along with operational and maintenance (O&amp;M) data. The GUI will also be able to display both condition criticality and operational criticality (on a color-coded range of 0-100%) for individual HV plant items on a digitized mimic of the HV network's single-line diagram (SLD). This could also be combined with geometric positioning data of assets across the facility (including HV cable routes and lengths) to provide a fully digitized SMART network diagram for use in the IIoT asset management solution. Asset management data, combined with the application of the developing techniques of AI, AR and VR, will greatly help the user to visualize the plant items in 3-D, their position within the network, their condition and operational criticality along with all related asset management information together on one dashboard screen, downloaded onto smartphone/tablet. The paper concludes with a case study showing the development of a specification for a SMART IIoT asset condition monitoring solution suitable for a large petrochemical refining facility.",https://ieeexplore.ieee.org/document/9080444/,2018 IEEE Petroleum and Chemical Industry Technical Conference (PCIC),24-26 Sept. 2018,ieeexplore
10.1109/SMARTCOMP52413.2021.00051,A Smart System for Personal Protective Equipment Detection in Industrial Environments Based on Deep Learning,IEEE,Conferences,"The adoption of real-time object detection systems via video streaming analysis is currently exploited in several contexts, from security monitoring to safety prevention. In industrial environments, proper usage of Personal Protective Equipment (PPE) is paramount to ensure workers’ safety. However, the use of some types of PPE, such as helmets, is often neglected by workers, especially in indoor areas. Thus, in order to reduce the risks of accidents, real-time video streaming-based monitoring systems may be used to monitor areas in which workers operate and alert them not to wear PPEs via acoustic alarms or visual signals. In case of a remote analysis, there are potential issues related to the high rate of data streams to be transported and analyzed and workers’ privacy. In this work, we propose an embedded smart system for real-time PPE detection based on video streaming analysis and deep learning models. We discuss the deployment of different versions of the YOLOv4 network fine-tuned using a public PPE dataset. In the end, we assess the performance of the proposed system in terms of accuracy and latency and of the overall PPE detection procedure.",https://ieeexplore.ieee.org/document/9556246/,2021 IEEE International Conference on Smart Computing (SMARTCOMP),23-27 Aug. 2021,ieeexplore
10.1109/PTC.2019.8810705,A Smart Voltage Optimization Approach for Industrial Load Demand Response,IEEE,Conferences,"This paper proposes a generic and comprehensive Voltage Optimization (VO) strategy for energy savings by industrial customers, to lower operating expenses through the implementation of an optimal process-based Demand Response (DR) program without affecting the real-time manufacturing process. This strategy takes into account the complex nature of industrial loads and their unique set of operating constraints, to reduce energy demand for industrial customers by means of varying the voltage at the utility service entrance to the plant. The proposed approach utilizes a Neural Network (NN) model of the industrial load, trained using historical operating data, to estimate the real power consumption of the load, based on the bus voltage and overall plant process. The NN load model is incorporated into the proposed VO model, whose objective is the minimization of the energy drawn from the substation and the number of switching operations of Load Tap Changers (LTC). The proposed VO framework is tested on a real plant model developed using actual measured data. The results demonstrate that the proposed technique can be successfully implemented by industrial customers and plant operators to enhance energy savings compared to Conservation Voltage Reduction (CVR) approaches, and also as a DR strategy that effectively manages the dependence of industrial loads on time-sensitive and critical manufacturing processes.",https://ieeexplore.ieee.org/document/8810705/,2019 IEEE Milan PowerTech,23-27 June 2019,ieeexplore
10.1109/IOLTS52814.2021.9486704,A Suitability Analysis of Software Based Testing Strategies for the On-line Testing of Artificial Neural Networks Applications in Embedded Devices,IEEE,Conferences,"Electronic devices based on artificial intelligence solutions are pervading our everyday life. Nowadays, human decision processes are supported by real-time data gathered from intelligent systems. Artificial Neural Networks (ANNs) are one of the most used deep learning predictive models due to their outstanding computational capabilities. However, assessing their reliability is still an open issue faced by both the academic and industrial worlds, especially when ANNs are deployed on safety-critical systems, such as self-driving cars in the automotive world. In these systems, a strategy for identifying hardware faults is required by industry standards (e.g., ISO26262 for automotive, and DO254 for avionics). Among the existing in-field test strategies, the periodic scheduling of on-line Software Test Library (STL) is a wide strategy adopted; STL allows to reach an acceptable fault coverage without the need for additional hardware. However, when dealing with ANN-based applications, the execution of on-line tests interleaving the ANN inferences may jeopardise the strive for performance maximization. The paper presents a comprehensive analysis of six possible scenarios concerning the execution of on-line self-test programs in embedded devices running ANN-based applications. In the proposed scenarios, the impact of the STL execution on the ANN performance is analyzed; in particular, the execution times of an inference and the Fault Detection Time (FDT) of the STL are discussed and compared. Experimental analyses are provided by relying on: an open-source RISC-V platform running two different convolutional neural networks; a STL for RISC-V cores with a maximum achievable fault coverage of 90%.",https://ieeexplore.ieee.org/document/9486704/,2021 IEEE 27th International Symposium on On-Line Testing and Robust System Design (IOLTS),28-30 June 2021,ieeexplore
10.1109/PacificVis.2018.00026,A Visual Analytics Approach for Equipment Condition Monitoring in Smart Factories of Process Industry,IEEE,Conferences,"Monitoring equipment conditions is of great value in manufacturing, which can not only reduce unplanned downtime by early detecting anomalies of equipment but also avoid unnecessary routine maintenance. With the coming era of Industry 4.0 (or industrial internet), more and more assets and machines in plants are equipped with various sensors and information systems, which brings an unprecedented opportunity to capture large-scale and fine-grained data for effective on-line equipment condition monitoring. However, due to the lack of systematic methods, analysts still find it challenging to carry out efficient analyses and extract valuable information from the mass volume of data collected, especially for process industry (e.g., a petrochemical plant) with complex manufacturing procedures. In this paper, we report the design and implementation of an interactive visual analytics system, which helps managers and operators at manufacturing sites leverage their domain knowledge and apply substantial human judgements to guide the automated analytical approaches, thus generating understandable and trustable results for real-world applications. Our system integrates advanced analytical algorithms (e.g., Gaussian mixture model with a Bayesian framework) and intuitive visualization designs to provide a comprehensive and adaptive semi-supervised solution to equipment condition monitoring. The example use cases based on a real-world manufacturing dataset and interviews with domain experts demonstrate the effectiveness of our system.",https://ieeexplore.ieee.org/document/8365986/,2018 IEEE Pacific Visualization Symposium (PacificVis),10-13 April 2018,ieeexplore
10.1109/icABCD49160.2020.9183810,A deep learning algorithm for detection of potassium deficiency in a red grapevine and spraying actuation using a raspberry pi3,IEEE,Conferences,"The fourth industrial revolution (4IR) has ushered in technological advancement, which is currently reshaping all sectors of the economy, including the agricultural domain. This paper describes the application of artificial intelligence technique on an embedded device. It involves the smart detection of potassium deficiency in red grape vines using the deep learning algorithm. This was deployed on a raspberry pi-3 for real-time actuation and effective prediction. The light-emitting diode (LED) was lit when a potassium deficient red grapevine leaf was brought close to the pi-camera. Image data obtained was fed as input into the model. Training, validation, and testing accuracies of 89%, 81%, and 80% were obtained respectively for the CNN model which surpassed the performance of the Support Vector Machines (SVM) classifier. This research has demonstrated a paradigm shift from the conventional agricultural method of detecting nutrient deficiency to a more effective real-time deep learning algorithm which prompt a corresponding actuation to effectively spray of fertilizers. This technique in no doubt would lead to tremendous increase in food production.",https://ieeexplore.ieee.org/document/9183810/,"2020 International Conference on Artificial Intelligence, Big Data, Computing and Data Communication Systems (icABCD)",6-7 Aug. 2020,ieeexplore
10.1109/ASMC.2017.7969203,A dynamic sampling strategy based on confidence level of virtual metrology predictions,IEEE,Conferences,"Metrology is a costly and time consuming activity in semiconductor fabrication; for this reason, Dynamic Sampling strategies and Virtual Metrology approaches have proliferated in the past recent years. Both Dynamic Sampling strategies and Virtual Metrology techniques aim at minimizing the amount of performed measures while keeping acceptable levels of production quality. In this work we study a Dynamic Sampling scheme recently proposed in literature that takes into account the availability of a Virtual Metrology module in the advanced process control architecture. The idea supporting the investigated strategy is based on the availability of a confidence level in the Virtual Metrology predictions; in our implementation of this scheme, this is achieved by exploiting a popular Machine Learning approach for supervised learning tasks, called Random Forests. The aforementioned scheme is tested on a real industrial dataset related to Plasma Etching and it is compared with classical metrology strategies.",https://ieeexplore.ieee.org/document/7969203/,2017 28th Annual SEMI Advanced Semiconductor Manufacturing Conference (ASMC),15-18 May 2017,ieeexplore
10.1109/ISIE.1995.497028,A fixed-point DSP based Cantonese recognition system,IEEE,Conferences,"Speech recognition systems have found useful applications in the field of automation. The development and implementation of a practical speech recognition system has aroused many industrial interests. In this experiment, hardware and software have been designed and implemented for a real-time Cantonese word recognition module using a Texas Instruments digital signal processor (DSP). This paper describes the realisation of the acoustic front-end of this system.",https://ieeexplore.ieee.org/document/497028/,1995 Proceedings of the IEEE International Symposium on Industrial Electronics,10-14 July 1995,ieeexplore
10.1109/INDIN.2013.6622897,A genetic algorithm for optimizing vector-based paths of industrial manipulators,IEEE,Conferences,"Nowadays there is a vast amount of IT tools specialized in vector graphics. The data generated by those tools could be used to describe the path of industrial manipulators as a set of vectors. The main problem is that the sequence/direction of those vectors is not meant to be executed by a robot and attempting to do it, would result in inefficient cycle times of the robot. Therefore it is necessary to generate an execution plan that minimizes the cost of carrying out the vector-based path. The number of possible execution actions has a factorial growth and it is unfeasible to evaluate each of them. This paper proposes the use of a genetic algorithm to optimize this task. The main contribution of this work is a chromosome encoding structure and modifications to the Partially Mapped Crossover operator in order to comply with the constraints of this optimization problem. The algorithm was implemented and tested in a real industrial manipulator.",https://ieeexplore.ieee.org/document/6622897/,2013 11th IEEE International Conference on Industrial Informatics (INDIN),29-31 July 2013,ieeexplore
10.1109/IRDS.2002.1043988,A geometrically validated approach to autonomous robotic assembly,IEEE,Conferences,"The paper discusses the employment of different sources of information to support robotic assembly operations. During component interaction, part of the wrist sensed force and torque information is found to be geometrically dependent. This enables the real-time sensorial data retrieved from the assembly scene to be combined with the information on the geometry of the component and the history of the insertion itself. As a result, an intelligent control architecture is developed to perform simple peg-hole assembly operations emphasising the aspects which relate to learning an appropriate state-action mapping without requiring an a priori defined set of manipulative skills. A real time peg in hole experiment involving a PUMA 761 industrial manipulator is detailed to support the theoretical results.",https://ieeexplore.ieee.org/document/1043988/,IEEE/RSJ International Conference on Intelligent Robots and Systems,30 Sept.-4 Oct. 2002,ieeexplore
10.1109/AIIA.1988.13355,A knowledge-based configurer for embedded computer systems software: real life experience,IEEE,Conferences,"The authors present a knowledge-based system that automatically configures complex embedded computer software and hardware in high-volume industrial production. This knowledge-based configurer was implemented to meet two primary goals in large-scale industrial embedded system production: to conform with just-in-time production requirements, and to ensure the efficiency of producing software. The system has been in real-life production use at several sites for more than two years, and the experiences justify both the underlying concepts and the implementation. The authors discuss the application and the implementation aspects, and summarize the experience gained from over two years' maintenance and enhancement. Particular attention is given to application-modelling and knowledge-representation issues.&lt;<ETX>&gt;</ETX>",https://ieeexplore.ieee.org/document/13355/,Proceedings of the International Workshop on Artificial Intelligence for Industrial Applications,25-27 May 1988,ieeexplore
,A machine learning based approach of robust parameter design,IET,Conferences,"Dual response surface methodology (DRSM) and nonparametric methodology (NPM) are main approaches used to achieve robust parameter design (RPD) of industrial processes and products. When the relationship between influential input factors and output quality characteristic of a process is very complex, both approaches have their limitations. For DRSM, it fails to fit the real response surfaces of process mean and variance by using the second order polynomial models. For NPM, it is hard to optimize parameters of fitting equation, and it needs more experiments as well. From a machine learning perspective, this paper generalizes RPD as a restricted active learning problem and proposes a new approach to achieve it. It fits process mean and variance responses by support vector machines (SVM), and then optimizes levels of design parameters by genetic algorithm. In order to reduce experiment times, the influence of priori knowledge on generalized error of fitting model is studied. Then a prior knowledge based experiment design is developed. Moreover, the approach selects the form of kernel function and optimizes parameters in SVM by comparing the upper bounds of generalized error of different SVM models without extra samples. The example given in the paper shows that, the generalized error and the experiment times of the approach decrease by no less than 45% and 39% respectively, compared with traditional approaches. All these results demonstrate the adaptability and superiority of the approach proposed in the paper.",https://ieeexplore.ieee.org/document/4752040/,2006 International Technology and Innovation Conference (ITIC 2006),6-7 Nov. 2006,ieeexplore
10.1109/CDC.2009.5399836,A new PID-type Fuzzy neural network controller based on Genetic Algorithm with improved Smith predictor,IEEE,Conferences,"Owing to the problem of control difficulty for the complex system, which has the characteristics of the large inertia, the pure time-delay and the model uncertainty in the industrial processes, a new PID-type fuzzy neural network controller (FNNC) based on Takagi-Sugeno-Kang (TSK) inference is proposed. Real-coded Chaotic Quantum-inspired Genetic Algorithm (RCQGA) is used to optimize the membership function parameters and TSK parameter sets simultaneously with faster convergence speed and more powerful optimizing ability. The pure time-delay effect of the complex object is compensated by a Smith predictor combined with Radial Basis Function (RBF) neural network identifier. The structure and control tactics of the controller are presented and tested by simulations and experiments in the heating furnace system. The proposed algorithm, as confirmed by the results of simulation and experiment compared with the Smith-Fuzzy-PID controller, exhibits good dynamic adjustment, high steady-state accuracy, strong resistant ability to interference and good robustness.",https://ieeexplore.ieee.org/document/5399836/,Proceedings of the 48h IEEE Conference on Decision and Control (CDC) held jointly with 2009 28th Chinese Control Conference,15-18 Dec. 2009,ieeexplore
10.1109/ITSC.2017.8317664,A new modelling framework over temporal graphs for collaborative mobility recommendation systems,IEEE,Conferences,"Over the years, collaborative mobility proved to be an important but challenging component of the smart cities paradigm. One of the biggest challenges in the smart mobility domain is the use of data science as an enabler for the implementation of large scale transportation sharing solutions. In particular, the next generation of Intelligent Transportation Systems (ITS) requires the combination of artificial intelligence and discrete simulations when exploring the effects of what-if decisions in complex scenarios with millions of users. In this paper, we address this challenge by presenting an innovative data modelling framework that can be used for ITS related problems. We demonstrate that the use of graphs and time series in multi-dimensional data models can satisfy the requirements of descriptive and predictive analytics in real-world case studies with massive amounts of continuously changing data. The features of the framework are explained in a case study of a complex collaborative mobility system that combines carpooling, carsharing and shared parking. The performance of the framework is tested with a large-scale dataset, performing machine learning tasks and interactive realtime data visualization. The outcome is a fast, efficient and complete architecture that can be easily deployed, tested and used for research as well in an industrial environment.",https://ieeexplore.ieee.org/document/8317664/,2017 IEEE 20th International Conference on Intelligent Transportation Systems (ITSC),16-19 Oct. 2017,ieeexplore
10.1049/cp:19940180,A novel neural adaptive controller for robots,IET,Conferences,"Existing industrial robotic manipulators have proven to be limited in many applications, e.g. both their payload capability and manipulation speeds are limited. This paper presents a novel neural adaptive controller-intelligent gain scheduling-(IGS) for robotic manipulators. It advances the idea of mapping the nonlinear relationship between robot working conditions (e.g. payload, speed, etc.) and its controller gains. This scheme is simple, inexpensive, and especially, attractive for its possible implementation in real-time. Simulation has shown promising results.&lt;<ETX>&gt;</ETX>",https://ieeexplore.ieee.org/document/327097/,1994 International Conference on Control - Control '94.,21-24 March 1994,ieeexplore
10.1109/DTPI52967.2021.9540189,A parallel intelligent control system and its industrial application,IEEE,Conferences,"Parallel control refers to the parallel interaction between the actual physical process and the manual calculation process. The ACP method under its theoretical framework includes artificial system, calculation experiment and parallel control. This paper presents a parallel intelligent control system implementation method, parallel control system needs a carrier, including hardware platform and software system, based on this carrier, the system completes artificial intelligence modeling and real-time optimal control. Firstly, the structure of parallel control platform is introduced, which is composed of industrial control computer, server and power supply, The program function of server is the core part of parallel control system;Secondly. The architecture of parallel intelligent control system is given, The artificial system is designed as an object modeling system, Industrial control computer output excitation signal, The server collects the response data and completes the modeling;The calculation experiment is designed as a process of human-computer interaction, which helps to realize control quality judgment and parameter setting;The parallel control is realized by the industrial controller, and the optimal parameters or control algorithm are put into the controller in parallel to realize the real-time control. Finally, an industrial application example is given to prove the effectiveness of this method.",https://ieeexplore.ieee.org/document/9540189/,2021 IEEE 1st International Conference on Digital Twins and Parallel Intelligence (DTPI),15 July-15 Aug. 2021,ieeexplore
10.1109/IPDPSW.2017.44,A pipelined and scalable dataflow implementation of convolutional neural networks on FPGA,IEEE,Conferences,"Convolutional Neural Network (CNN) is a deep learning algorithm extended from Artificial Neural Network (ANN) and widely used for image classification and recognition, thanks to its invariance to distortions. The recent rapid growth of applications based on deep learning algorithms, especially in the context of Big Data analytics, has dramatically improved both industrial and academic research and exploration of optimized implementations of CNNs on accelerators such as GPUs, FPGAs and ASICs, as general purpose processors can hardly meet the ever increasing performance and energy-efficiency requirements. FPGAs in particular are one of the most attractive alternative, as they allow the exploitation of the implicit parallelism of the algorithm and the acceleration of the different layers of a CNN with custom optimizations, while retaining extreme flexibility thanks to their reconfigurability. In this work, we propose a methodology to implement CNNs on FPGAs in a modular, scalable way. This is done by exploiting the dataflow pattern of convolutions, using an approach derived from previous work on the acceleration of Iterative Stencil Loops (ISLs), a computational pattern that shares some characteristics with convolutions. Furthermore, this approach allows the implementation of a high-level pipeline between the different network layers, resulting in an increase of the overall performance when the CNN is employed to process batches of multiple images, as it would happen in real-life scenarios.",https://ieeexplore.ieee.org/document/7965030/,2017 IEEE International Parallel and Distributed Processing Symposium Workshops (IPDPSW),29 May-2 June 2017,ieeexplore
10.1109/SIPNN.1994.344835,A practical object oriented programming approach for implementing real-time speech compression algorithm,IEEE,Conferences,"A practical approach for developing a system incorporated with real-time speech compression are presented. This is a technique used in practice within the industrial sector for selecting and integrating DSP functionality into a large system. A three-stage technique is used to simulate, evaluate, debug and implement the CCITT G.728 low delay code excited linear prediction (LD-CELP) algorithm. In the first stage, the algorithm is evaluated via simulation to determine whether it meets the design criterion. Then, it is implemented in real-time based an object oriented approach. After the algorithm is thoroughly tested, it is further refined to obtain tighter and faster coding. This technique can be applied to other real-time DSP algorithms.&lt;<ETX>&gt;</ETX>",https://ieeexplore.ieee.org/document/344835/,"Proceedings of ICSIPNN '94. International Conference on Speech, Image Processing and Neural Networks",13-16 April 1994,ieeexplore
10.1109/MFI-2003.2003.1232590,A robust real time position and force (hybrid) control of a robot manipulator in presence of uncertainties,IEEE,Conferences,"We examine the living intelligent biological systems and model the computational system components. We consider the situation of a kind of ""blind-tracking"" with constant force/torque by a human hand. The problem involves hand kinematics, hand motor control, and an adaptive judgment method from the position and force/torque reflection of the uncertain hyper plane. In this study, these control levels were designed using neural networks and fuzzy logic technologies. The control levels are coordinated amongst themselves forming the distributed artificial intelligent (DAI) system. The conclusive characteristic of the proposed controller was a one-step-ahead feedback control. This DAI-based control systems was implemented in the RX-90 industrial robot. Certainly these types of control system will help an industry to be autonomous and increase the productivity as well.",https://ieeexplore.ieee.org/document/1232590/,"Proceedings of IEEE International Conference on Multisensor Fusion and Integration for Intelligent Systems, MFI2003.",1-1 Aug. 2003,ieeexplore
10.1109/INDIN.2015.7281903,A self-learning strategy for artificial cognitive control systems,IEEE,Conferences,"This paper presents a self-learning strategy for an artificial cognitive control based on a reinforcement learning strategy, in particular, an on-line version of a Q-learning algorithm. One architecture for artificial cognitive control was initially reported in [1], but without an effective self-learning strategy in order to deal with nonlinear and time variant behavior. The anticipation mode (i.e., inverse model control) and the single loop mode are two operating modes of the artificial cognitive control architecture. The main goal of the Q-learning algorithm is to deal with intrinsic uncertainty, nonlinearities and noisy behavior of processes in run-time. In order to validate the proposed method, experimental works are carried out for measuring and control the microdrilling process. The real-time application to control the drilling force is presented as a proof of concept. The performance of the artificial cognitive control system by means of the reinforcement learning is improved on the basis of good transient responses and acceptable steady-state error. The Q-learning mechanism built into a low-cost computing platform demonstrates the suitability of its implementation in an industrial setup.",https://ieeexplore.ieee.org/document/7281903/,2015 IEEE 13th International Conference on Industrial Informatics (INDIN),22-24 July 2015,ieeexplore
10.1109/ISIE.2010.5637497,A society of agents for service robots,IEEE,Conferences,"This article presents an agent based distributed software architecture for machine and robot control. The functionality of agents of this architecture has been inspired by Marvin Minsky's definition of the term in his book “The Society of Mind” (1986) [1]. Minsky, widely considered to be one of the fathers of artificial intelligence, tried to describe from an engineering point of view, in this book, how he thought the mind works: “I'll call “Society of Mind” this scheme in which each mind is made of many smaller processes. These we'll call agents. Each mental agent by itself can only do some simple thing that needs no mind or thought at all. Yet when we join these agents in societies-in certain very special ways-this leads to true intelligence.” Societies of simple behaving agents have been implemented in Fatronik, in real robots, and have been demonstrated to be able to perform complex tasks in industrial environments. This article explains the features of such societies of agents and presents their implementation in a real robot.",https://ieeexplore.ieee.org/document/5637497/,2010 IEEE International Symposium on Industrial Electronics,4-7 July 2010,ieeexplore
10.1109/ICIAS.2007.4658557,A study on industrial communication networking: Ethernet based implementation,IEEE,Conferences,"Recent enhancement of the industrial communications and networking are possible to apply in Ethernet networks system at all levels of industrial automation, especially in the controller level whereby the data exchanges in real-time communication is mandatory. This paper is about a study on the development of industrial communications network based on the Ethernet protocol and thus implement it into computer integrated manufacturing (CIM) system. The purpose of this paper is to overcome real-time communication in which the accessibility of data exchange is very difficult in terms of retrieving data from other stations and time consuming. The Ethernet module is installed onto supervisory OMRON PLC to integrate several of stations in the CIM-70A system which is located at Robotic Laboratory in Universiti Tun Hussein Onn Malaysia (UTHM). The workability of this communication technique is analyzed and compared with the conventional serial communication which widely used in automation networking systems. It is found that, the Ethernet protocol approach through the communication and integration of CIM system can be accessed easily and available to be upgraded at the management and enterprise levels of industrial automation system.",https://ieeexplore.ieee.org/document/4658557/,2007 International Conference on Intelligent and Advanced Systems,25-28 Nov. 2007,ieeexplore
10.1109/INFOC.2017.8001669,A study on the fast system recovery: Selecting the number of surrogate nodes for fast recovery in industrial IoT environment,IEEE,Conferences,This paper is based on the previous research that selects the proper surrogate nodes for fast recovery mechanism in industrial IoT (Internet of Things) Environment which uses a variety of sensors to collect the data and exchange the collected data in real-time for creating added value. We are going to suggest the way that how to decide the number of surrogate node automatically in different deployed industrial IoT Environment so that minimize the system recovery time when the central server likes IoT gateway is in failure. We are going to use the network simulator to measure the recovery time depending on the number of the selected surrogate nodes according to the sub-devices which are connected to the IoT gateway.,https://ieeexplore.ieee.org/document/8001669/,2017 International Conference on Information and Communications (ICIC),26-28 June 2017,ieeexplore
10.1109/SIPS.1998.715769,A system-on-chip design of a low-power smart vision system,IEEE,Conferences,"A low-power smart imager design is proposed for real-time machine vision applications. It takes advantages of recent advances in integrated sensing/processing designs, electronic neural networks, and sub-micron VLSI technology. The smart vision system integrates an active pixel camera, with a programmable neural computer and an advanced microcomputer. A system-on-a-chip implementation of this smart vision system is shown to be feasible by integrating the whole system into a 3-cm/spl times/3-cm chip design in a 0.18 m CMOS technology. The on-chip neural computer provides one tera-operation-per-second computing power for various parallel vision operations and smart sensor functions. Its high performance is due to massively parallel computing structures, high data throughput rates, fast learning capabilities, and system-on-a-chip implementation. This highly integrated smart imager can be used for various scientific missions and other military, industrial or commercial vision applications.",https://ieeexplore.ieee.org/document/715769/,1998 IEEE Workshop on Signal Processing Systems. SIPS 98. Design and Implementation (Cat. No.98TH8374),10-10 Oct. 1998,ieeexplore
10.1109/ICPHYS.2018.8390759,Accented visualization by augmented reality for smart manufacturing aplications,IEEE,Conferences,"Effective application of Augmented Reality user interfaces is one of the challenging trends of industrial cyber-physical systems development and implementation. Despite reach functionality of modern AR devices (like goggles, head mounted displays or tablets) the problem of their comfortable and productive use is not completely solved yet. To cover this gap, it is proposed in this paper to implement a new paradigm of “accented visualization” that allows adapting additional data presented by AR device according to the user's current interest, attention and focus. To provide such context driven functionality there was developed intelligent software based on eye tracking and capturing the user's focus in ontology as a knowledge base. Probation and testing of the proposed approach present 89 % of the solution efficiency.",https://ieeexplore.ieee.org/document/8390759/,2018 IEEE Industrial Cyber-Physical Systems (ICPS),15-18 May 2018,ieeexplore
10.1109/CSCMP45713.2019.8976591,Adaptive Control System Based on Neural Tuner of DC Drive with Sinamics DCM,IEEE,Conferences,"The purpose of this study is to develop an adaptive control system of a DC electric drive implemented on the basis of industrial DC converter Siemens Sinamics DCM. A neural tuner adjusting speed PI-controller parameters is chosen as an adaptation method. In contrary to classical methods of adaptive control, this tuner does not require an accurate nonlinear model of the drive. Instead of this, it evaluates transients quality in a speed loop and, if it does not follow the requirements, adjusts the corresponding controller parameters. This assessment is made by a developed rule base, which calculates the value of a learning rate of the neural network online training. The network output is the controller parameters. The experiments with a real DC motor are conducted under the following conditions. The motor inertia moment is changed by 50% from its nominal value. As a result, the neural tuner adjusts the parameters of the speed PI-controller and achieves the required quality of transients. At the same time, the overshoot obtained with the help of the classic PI-controller is 11% higher than required.",https://ieeexplore.ieee.org/document/8976591/,2019 XXI International Conference Complex Systems: Control and Modeling Problems (CSCMP),3-6 Sept. 2019,ieeexplore
10.1109/MetroInd4.0IoT51437.2021.9488498,Adaptive LoRaWAN Transmission exploiting Reinforcement Learning: the Industrial Case,IEEE,Conferences,"Wireless technologies play a key role in the Industrial Internet of Things (IIoT) scenario, for the development of increasingly flexible and interconnected factory systems. A significant opportunity in this context is represented by the advent of Low Power Wide Area Network (LPWAN) wireless technologies, that enable a reliable, secure, and effective transmission of measurement data over long communication ranges and with very low power consumption. Nevertheless, reliability in harsh environments (as typically occurs in the industrial scenario) is a significant issue to deal with. Focusing on LoRaWAN, adaptive strategies can be profitably devised concerning the above tradeoff. To this aim, this paper proposes to exploit Reinforcement Learning (RL) techniques to design an adaptive LoRaWAN strategy for industrial applications. The RL is spreading in many fields since it allows the design of intelligent systems using a stochastic discrete-time system approach. The proposed technique has been implemented within a purposely designed simulator, allowing to draw a preliminary performance assessment in a real-world scenario. A high density of independent nodes per square km has been considered, showing a significant improvement (about 10%) of the overall reliability in terms of data extraction rate (DER) without compromising full compatibility with the standard specifications.",https://ieeexplore.ieee.org/document/9488498/,2021 IEEE International Workshop on Metrology for Industry 4.0 & IoT (MetroInd4.0&IoT),7-9 June 2021,ieeexplore
10.1109/CIMCA.2008.66,Adaptive Local Learning Soft Sensor for Inferential Control Support,IEEE,Conferences,"In this work we focused on the development of an adaptive Soft Sensor which may be deployed in a real-life environment, for example as inferential control support. To be able to do this, the Soft Sensor must fulfil certain constraints like being able to deal with data impurities or to adapt itself with changing data. The task is approached by training a set of models with limited validity in the data space and by proposing a statistically-based technique for the combination of the local models. The combination weights are related to the estimated performance of the local models in the neighbourhood of the processed data sample. The performance and other benefits of the proposed Soft Sensor are demonstrated in terms of a case study where the model deals with raw industrial data.",https://ieeexplore.ieee.org/document/5172632/,2008 International Conference on Computational Intelligence for Modelling Control & Automation,10-12 Dec. 2008,ieeexplore
10.1109/CIMSA.2006.250758,Adaptive Spatio-Spectral Hyperspectral Image Processing for Online Industrial Classification of Inhomogeneous Materials,IEEE,Conferences,"An approach for considering spatio-spectral information when classifying inhomogeneous materials in industrial environments is proposed. Its main application would be in the inspection and quality control tasks. They system core is an ANN based hyperspectral processing unit able to perform the online determination of the quality of the material based on its composition and grain size. A training adviser is being implemented in the system in order to automate the determination of the optimal spatial window size, as well as to reduce the number of spectral bands used and for determining the optimal spectral combination function through the automatic extraction of the discriminating features. Several tests have been carried out on synthetic and real data sets. In particular, the proposed approach is used to discriminate samples of andalusite having different purities; the results obtained show an accuracy of better than 98%",https://ieeexplore.ieee.org/document/4016840/,2006 IEEE International Conference on Computational Intelligence for Measurement Systems and Applications,12-14 July 2006,ieeexplore
10.1109/ICIT.1996.601644,Adaptive robust robot control using BP-SMENs,IEEE,Conferences,"This paper presents the development of a new adaptive recurrent neural network for the control of a nonlinear system represented by a two-link SCARA type planar robot manipulator. The standard backpropagation algorithm is used to adjust the weights of the networks. The proposed control system consists of an inverse neural model of robot (INNM), an INNM-based neural controller, a robust controller, a conventional PI controller, and a second order linear filter. To evaluate the performance of the proposed control scheme and neural network, a simulated SCARA type robot was studied and the results showed how well the proposed controller can minimise the error between an actual and desired end-effector trajectory. From simulation examples, the robot trajectory tracking showed superior performance that is very attractive for real-time implementation and application in complex industrial tasks. For comparison, the standard computed torque method is employed for controlling the robot.",https://ieeexplore.ieee.org/document/601644/,Proceedings of the IEEE International Conference on Industrial Technology (ICIT'96),2-6 Dec. 1996,ieeexplore
10.1109/ICMLA.2012.160,Adaptive soft sensor for online prediction based on moving window Gaussian process regression,IEEE,Conferences,"Very often important process variables cannot be measured online due to low sampling rate of sensors or because their values have to be obtained by laboratory analysis. In order to enable continuous process monitoring and efficient process control in such cases, soft sensors are usually used to estimate these difficult-to-measure process variables. Most industrial processes exhibit some kind of time-varying behavior. To ensure that soft sensor retains its precision, adaptation mechanism has to be implemented. In this paper adaptive soft sensor based on Gaussian Process Regression (GPR) is presented. To make GPR model training more efficient, algorithm for variable selection based on Mutual Information is proposed. Prediction capabilities of the proposed method are examined on real industrial data obtained at an oil distillation column.",https://ieeexplore.ieee.org/document/6406773/,2012 11th International Conference on Machine Learning and Applications,12-15 Dec. 2012,ieeexplore
10.1109/INDIN.2017.8104789,Addressing security challenges in industrial augmented reality systems,IEEE,Conferences,"In context of Industry 4.0 Augmented Reality (AR) is frequently mentioned as the upcoming interface technology for human-machine communication and collaboration. Many prototypes have already arisen in both the consumer market and in the industrial sector. According to numerous experts it will take only few years until AR will reach the maturity level to be deployed in productive applications. Especially for industrial usage it is required to assess security risks and challenges this new technology implicates. Thereby we focus on plant operators, Original Equipment Manufacturers (OEMs) and component vendors as stakeholders. Starting from several industrial AR use cases and the structure of contemporary AR applications, in this paper we identify security assets worthy of protection and derive the corresponding security goals. Afterwards we elaborate the threats industrial AR applications are exposed to and develop an edge computing architecture for future AR applications which encompasses various measures to reduce security risks for our stakeholders.",https://ieeexplore.ieee.org/document/8104789/,2017 IEEE 15th International Conference on Industrial Informatics (INDIN),24-26 July 2017,ieeexplore
10.1109/ISPACS.2010.5704595,Advancing multimedia technologies for smart living and learning services,IEEE,Conferences,"The development of ICT technologies to advance smart living products and creative humanity services becomes future global consensus and industrial trends in the world. The user-driven innovations, which bring products and services naturally and smartly close to human needs, should heavily adopt many interactive multimedia technologies. The interactions through the detection of human nature touch, voice, and gestures and the responses of virtually real multimedia could be implemented in highly integrated embedded and cloud computing systems. In the talk, some designs such as 3D interactive sports, Interactive arts, interactive table, and smart living and learning products in the Technologies of Ubiquitous Computing and Humanity (TOUCH) Center, will be introduced. Through smart living and learning clouds, the future plans related to smart living and efficient learning services for the students are addressed. Linked to the city government, the humanity services through living lab open innovations delivered to the citizens will be forecasted finally.",https://ieeexplore.ieee.org/document/5704595/,2010 International Symposium on Intelligent Signal Processing and Communication Systems,6-8 Dec. 2010,ieeexplore
10.1109/CIPLS.2013.6595200,Agent-based dispatching in groupage traffic,IEEE,Conferences,"The complexity and dynamics in group age traffic requires flexible, efficient, and adaptive planning and controlling processes. While the general problem refers to the Vehicle Routing Problem (VRP), additional requirements have to be fulfilled in application. Individual properties and priorities of orders, a heterogeneous fleet of vehicles, dynamically incoming orders, unexpected events etc. require a proactive and reactive system behavior. To enable automated dispatching processes, we have implemented a multiagent system where the decision making is shifted from a central system to autonomous, interacting, intelligent agents. To evaluate the approach we used multi agent-based simulation and modeled several scenarios on real world infrastructures with orders provided by our industrial partner. The results reveal that agent-based dispatching meets the increasing requirements in groupage traffic while supporting the combination of pickup and delivery tours and accommodating request priorities, time-windows, as well as capacity constraints.",https://ieeexplore.ieee.org/document/6595200/,2013 IEEE Symposium on Computational Intelligence in Production and Logistics Systems (CIPLS),16-19 April 2013,ieeexplore
10.1109/CEWIT.2013.6713745,Agent-based planning and control for groupage traffic,IEEE,Conferences,"In this research and technology transfer project, the planning and control processes of the industrial partner Hellmann Worldwide Logistics GmbH &amp; Co. KG are analyzed. An agent-based approach is presented to model current processes and to exploit the identified optimization potential. The developed system directly connects the information flow and the material flow as well as their interdependencies in order to optimize the planning and control in groupage traffic. The software system maps current processes to agents as system components and improves the efficiency by intelligent objects. To handle the high complexity and dynamics of logistics autonomous intelligent agents plan and control the way of represented objects through the logistic network by themselves and induce a flexible and reactive system behavior. We evaluate the implemented dispatching application by simulating the groupage traffic processes using effectively transported orders and process data provided by our industrial partner. Moreover, we modeled real world infrastructures and considered also the dynamics by the simulation of unexpected events and process disturbances. The results show that the system significantly decreases daily cost by reducing the required number of transport providers and shifting conventional orders to next days, which need no immediate delivery. Thus the system increases the efficiency and meets the special challenges and requirements of groupage traffic. Moreover, the system supports freight carriers and dispatchers with adequate tour and routing proposals. Computed tours were successfully validated by human dispatchers. Due to the promising results, Hellmann is highly interested in transferring the prototype to an application that optimizes the daily operations in numerous distribution centers. Finally, provide further research perspectives, and emphasize the advantages of the developed system in Industry 4.0 applications.",https://ieeexplore.ieee.org/document/6713745/,2013 10th International Conference and Expo on Emerging Technologies for a Smarter World (CEWIT),21-22 Oct. 2013,ieeexplore
10.1109/ICRAS52289.2021.9476659,Agroindustrial Plant for the Classification of Hass Avocados in Real-Time with ResNet-18 Architecture,IEEE,Conferences,"The avocado is the fruit with a growing trend in production due to its demand in the world market. Peru currently ranks third in the export of Hass type avocados. For the efficient classification of avocados in good or bad condition, a ResNet-18 algorithm applied to a robust agro-industrial plant was implemented. By using a non-invasive classification we reduce handling damage. The plant consists of a feeder system that continues with a conveyor belt, followed by the image acquisition system with its lighting system, finally, there is the classification system formed by the pneumatic system consisting of pistons that will deposit the avocados in the right containers. The treatment of the images was developed in three stages: acquisition, training, and implementation of the neural network. The Deep Learning algorithm used is ResNet-18, and the hyperparameters of the convolutional network were adjusted to obtain a precision of 98.72%, a specificity of 98.52%, and an F1 score of 98.08%.",https://ieeexplore.ieee.org/document/9476659/,2021 5th International Conference on Robotics and Automation Sciences (ICRAS),11-13 June 2021,ieeexplore
10.1109/GROUP4.2006.1708184,An Active Demodulation Pixel using a Current Assisted Photonic Demodulator Implemented in 0.6μm Standard CMOS,IEEE,Conferences,"With the ever increasing automation of industrial processes, and the growing need for intelligent systems in, for example, auto-motive and safety/security applications, the demand for adequate artificial three-dimensional (3D) vision increases. It is still the weakest link between intelligent systems in general and the real world. Systems based on the Time-of-Flight (TOF) principle provide an elegant solution to these needs. The key structure to enable wide scale use of TOF ranging systems is a photonic demodulator with high responsivity and high demodulation bandwidth. This paper intends to highlight one of the most promising Standard CMOS photonic demodulator architecture",https://ieeexplore.ieee.org/document/1708184/,"3rd IEEE International Conference on Group IV Photonics, 2006.",13-15 Sept. 2006,ieeexplore
10.1109/WiMOB.2019.8923315,An Anomaly Detector for CAN Bus Networks in Autonomous Cars based on Neural Networks,IEEE,Conferences,"The domain of securing in-vehicle networks has attracted both academic and industrial researchers due to high danger of attacks on drivers and passengers. While securing wired and wireless interfaces is important to defend against these threats, detecting attacks is still the critical phase to construct a robust secure system. There are only a few results on securing communication inside vehicles using anomaly-detection techniques despite their efficiencies in systems that need real-time detection. Therefore, we propose an intrusion detection system (IDS) based on Multi-Layer Perceptron (MLP) neural network for Controller Area Networks (CAN) bus. This IDS divides data according to the ID field of CAN packets using K-means clustering algorithm, then it extracts suitable features and uses them to train and construct the neural network. The proposed IDS works for each ID separately and finally it combines their individual decisions to construct the final score and generates alert in the presence of attack. The strength of our intrusion detection method is that it works simultaneously for two types of attacks which will eliminate the use of several separate IDS and thus reduce the complexity and cost of implementation.",https://ieeexplore.ieee.org/document/8923315/,"2019 International Conference on Wireless and Mobile Computing, Networking and Communications (WiMob)",21-23 Oct. 2019,ieeexplore
10.1109/IICPE.2006.4685399,An Ant Colony based Hybrid Intelligent Controller for looper tension in steel rolling mills,IEEE,Conferences,"Strip tension control is crucial in the tandem finishing mills for its safe operation. Most of the industrial applications are of nonlinear. Fuzzy logic controller is the most useful approach to achieve the adaptive ness in the case of nonlinear system. Since fuzzy logic control provides a systematic method of incorporating human expertise and implementing nonlinear system. Neural networks are integrated with fuzzy logic which forms a neuro fuzzy system. To get a desired solution from the vagueness of the problem, an ant colony system (ACS) is implemented. The nature of search of food by the real antpsilas colony to find the food from its source (i.e. nest), to its destination (i.e. food) is known as ACS. The same foraging behavior of ants can be used to determine the optimal solution for the membership functions of FLC and hence form the hybrid intelligent controller (HIC). This paper demonstrates the effectiveness of HIC in optimizing the looper height in steel rolling mills compared with conventional controllers, FLC. The simulation result depicts that HIC quickly restore the speed of the main drive and hence looper height is quickly reduced to its optimal (zero) value which intern ensures the safety working condition of steel rolling mills.",https://ieeexplore.ieee.org/document/4685399/,2006 India International Conference on Power Electronics,19-21 Dec. 2006,ieeexplore
10.1109/ECTC32696.2021.00346,An Automated Optical Inspection System for PIP Solder Joint Classification Using Convolutional Neural Networks,IEEE,Conferences,"In the fields of electronics manufacturing, the application of through-hole devices is still required, as heat dissipation and high current carrying capacity plays an important role. To ensure the highest quality standards, these electronics production processes take a multitude of inspection processes into account. For the detection of error patterns regarding the quality of the solder connections, usually, high-end inspection machines are utilized in the industrial application. The Automated Optical Inspection is a commonly conducted process, using visible light and rule-based inspection routines, setup by process experts for the evaluation of the Region of Interest. The high overhead of creating and maintaining product-specific checking routines and machine acquisition leads to increased costs and severe dependency on expert know-how. A flexible inspection algorithm, implemented into low-cost equipment for image generation is expected to reduce acquisition and optimization costs, and lower dependency on expert knowledge and high-end machinery. In this contribution, we present a novel framework for the automatic, near real-time solder joint classification based on Convolutional Neural Networks, flexibly detecting, and classifying solder connections. We utilize existing Deep Learning architectures for detection and classification. The localization model utilizes a YOLO-architecture (you-only-look-once), learning feature inputs based on a supervised learning approach. Pseudo-labeling is carried out automatically by an anomaly detection model. The image generation is executed by an industrial low-cost camera and an industrial rack-PC. The developed prototype is integrated into the existing production infrastructure. The results indicate a satisfactory detection and classification of the investigated solder connections with the proposed system. Hence, this system represent an alternative to commercially available high-end inspection systems being used for an inline control of Pin-in-Paste and through-hole device solder connections.",https://ieeexplore.ieee.org/document/9501916/,2021 IEEE 71st Electronic Components and Technology Conference (ECTC),1 June-4 July 2021,ieeexplore
10.1109/ASE.2019.00080,An Empirical Study Towards Characterizing Deep Learning Development and Deployment Across Different Frameworks and Platforms,IEEE,Conferences,"Deep Learning (DL) has recently achieved tremendous success. A variety of DL frameworks and platforms play a key role to catalyze such progress. However, the differences in architecture designs and implementations of existing frameworks and platforms bring new challenges for DL software development and deployment. Till now, there is no study on how various mainstream frameworks and platforms influence both DL software development and deployment in practice. To fill this gap, we take the first step towards understanding how the most widely-used DL frameworks and platforms support the DL software development and deployment. We conduct a systematic study on these frameworks and platforms by using two types of DNN architectures and three popular datasets. (1) For development process, we investigate the prediction accuracy under the same runtime training configuration or same model weights/biases. We also study the adversarial robustness of trained models by leveraging the existing adversarial attack techniques. The experimental results show that the computing differences across frameworks could result in an obvious prediction accuracy decline, which should draw the attention of DL developers. (2) For deployment process, we investigate the prediction accuracy and performance (refers to time cost and memory consumption) when the trained models are migrated/quantized from PC to real mobile devices and web browsers. The DL platform study unveils that the migration and quantization still suffer from compatibility and reliability issues. Meanwhile, we find several DL software bugs by using the results as a benchmark. We further validate the results through bug confirmation from stakeholders and industrial positive feedback to highlight the implications of our study. Through our study, we summarize practical guidelines, identify challenges and pinpoint new research directions, such as understanding the characteristics of DL frameworks and platforms, avoiding compatibility and reliability issues, detecting DL software bugs, and reducing time cost and memory consumption towards developing and deploying high quality DL systems effectively.",https://ieeexplore.ieee.org/document/8952401/,2019 34th IEEE/ACM International Conference on Automated Software Engineering (ASE),11-15 Nov. 2019,ieeexplore
10.1109/ICMLA.2019.00171,An Encoder-Decoder Based Approach for Anomaly Detection with Application in Additive Manufacturing,IEEE,Conferences,"We present a novel unsupervised deep learning approach that utilizes an encoder-decoder architecture for detecting anomalies in sequential sensor data collected during industrial manufacturing. Our approach is designed to not only detect whether there exists an anomaly at a given time step, but also to predict what will happen next in the (sequential) process. We demonstrate our approach on a dataset collected from a real-world Additive Manufacturing (AM) testbed. The dataset contains infrared (IR) images collected under both normal conditions and synthetic anomalies. We show that our encoder-decoder model is able to identify the injected anomalies in a modern AM manufacturing process in an unsupervised fashion. In addition, our approach also gives hints about the temperature non-uniformity of the testbed during manufacturing, which was not previously known prior to the experiment.",https://ieeexplore.ieee.org/document/8999143/,2019 18th IEEE International Conference On Machine Learning And Applications (ICMLA),16-19 Dec. 2019,ieeexplore
10.1109/ICETA.2018.8572099,An Implementation of Efficient Hierarchical Access Control Method for VR/AR Platform,IEEE,Conferences,"With the growth of Virtual Reality (VR) and Augmented Reality (AR) in technologies such as artificial intelligence, wireless, 5G, big data, massive compute, industrial 4.0 and virtual stores. An improved secure mechanism which was previously still existed some shortcomings, was presented in this paper. Another new mechanism regards to achieve the decentralized environment access control is introduced to attain the needs of the non-specific internet. Furthermore, it is very important to look attentively to the administrative previleges in VR/AR clouds. The issues which gained from VR and AR could be tackled with the new mechanism. This new studies achieves a higher circumstance. Developer worker's duty can be distributed; the database system can be compatibly coordinates; moreover, the users' information security can be entirely secured.",https://ieeexplore.ieee.org/document/8572099/,2018 16th International Conference on Emerging eLearning Technologies and Applications (ICETA),15-16 Nov. 2018,ieeexplore
10.23919/ChiCC.2018.8483465,An Improved Ensemble Adaptive Kernel PLS Soft Sensor Model and its Application,IEEE,Conferences,"To avoid the disadvantage of traditional PLS model in dealing with nonlinear data, Kernel PLS (KPLS) algorithm has been proposed. By mapping nonlinear data into high-dimensional space with kernel function, the original data set can be processed using linear models in the new space. However, when facing diverse complicated nonlinear features, the simple kernel method also exhibits some limitations. To tackle this problem, an improved k-means based Ensemble Adaptive Kernel Partial Least Squares (EAKPLS) is proposed. Its whole processes are implemented as follows. In the modeling phase, the data is first divided into k sub datasets by k-means clustering algorithm. Then for each subset, different kernels and corresponding kernel parameters are chosen adaptively by introducing PSO algorithm. In the prediction phase, ensemble learning is introduced to obtain the final predictable value where Bayes' theorem is applied, where an improved weights assignment strategy is also presented. Ultimately, numerical and real industrial test cases are both given to demonstrate its feasibility and effectiveness.",https://ieeexplore.ieee.org/document/8483465/,2018 37th Chinese Control Conference (CCC),25-27 July 2018,ieeexplore
10.1109/ICIT46573.2021.9453580,An Industrial HMI Temporal Adaptation based on Operator-Machine Interaction Sequence Similarity,IEEE,Conferences,"The incorporation of Artificial Intelligence (AI) into Industrial Environments has brought about a Smart Industry revolution, improving efficiency and simplifying complex industrial processes. However, these technological advances remain primarily focused on the process, and pay little attention to industrial Human-Machine Interfaces (HMI), the bridge between the operator and the industrial process.Current industrial HMIs have a static design, and are focused exclusively on the control and visualization of process information. They fail to take into account user behaviour and skills, information key to understanding how the operator interacts with the production process. Thus, the potential beneficial outcomes of considering operator-machine interaction in terms of efficiency and productivity, make a compelling case for industrial HMIs that can adapt to different operators based on their skills and process knowledge.This paper proposes a Machine Learning (ML) based method-ology capable of analysing operator-machine interaction and detecting the variability of interaction patterns for repetitive similar sequences in monitoring and control tasks. The method-ology generates a set of adaptation rules that improve Usability and User Experience, and hence operator working performance. To validate the proposed methodology, an experiment with real operators was conducted.",https://ieeexplore.ieee.org/document/9453580/,2021 22nd IEEE International Conference on Industrial Technology (ICIT),10-12 March 2021,ieeexplore
10.1109/ICCWorkshops49005.2020.9145434,An Inter-Disciplinary Modelling Approach in Industrial 5G/6G and Machine Learning Era,IEEE,Conferences,"Unlike conventional cellular systems, the fifth generation (5G) and beyond includes intrinsic support for vertical industries with diverse service requirements. Industrial process automation with autonomous fault detection and prediction, optimised operations and proactive control can be considered as one of the key verticals of 5G and beyond. Such applications enable equipping industrial plants with a reasoning sixth sense for optimised operations and fault avoidance. In this direction, we introduce an inter-disciplinary approach integrating wireless sensor networks with machine learning-enabled industrial plants to build a step towards developing this sixth sense technology, i.e., the reasoning ability. We develop a modular-based system that can be adapted to the vertical-specific elements. Without loss of generalisation, exemplary use cases are developed and presented including a fault detection/prediction scheme in a wireless communication network with sensors and actuators to enable the sixth sense technology with guaranteed service load requirements. The proposed schemes and modelling approach are implemented in a real chemical plant for testing purposes, and a high fault detection and prediction accuracy is achieved coupled with optimised sensor density analysis.",https://ieeexplore.ieee.org/document/9145434/,2020 IEEE International Conference on Communications Workshops (ICC Workshops),7-11 June 2020,ieeexplore
10.1109/ISSE46696.2019.8984462,An IoT Reconfigurable SoC Platform for Computer Vision Applications,IEEE,Conferences,"The field of Internet of Things (IoT) and smart sensors has expanded rapidly in various fields of research and industrial applications. The area of IoT robotics has become a critical component in the evolution of Industry 4.0 standard. In this paper, we developed an IoT based reconfigurable System on Chip (SoC) robot that is fast and efficient for computer vision applications. It can be deployed in other IoT robotics applications and achieve its intended function. A Terasic Hexapod Spider Robot (TSR) was used with its DE0-Nano SoC board to implement our IoT robotics system. The TSR was designed to provide a competent computer vision application to recognize different shapes using a machine learning classifier. The data processing for image detection was divided into two parts, the first part involves hardware implementation on the SoC board and to provide real-time interaction of the robot with the surrounding environment. The second part of implementation is based on the cloud processing technique, where further data analysis was performed. The image detection algorithm for the computer vision component was tested and successfully implemented to recognize shapes. The TSR moves or reacts based on the detected image. The Field Programmable Gate Array (FPGA) part is programmed to handle the movement of the robot and the Hard Processor System (HPS) handles the shape recognition, Wi-Fi connectivity, and Bluetooth communication. This design is implemented, tested and can be used in real-time applications in harsh environments where movements of other robots are restricted.",https://ieeexplore.ieee.org/document/8984462/,2019 International Symposium on Systems Engineering (ISSE),1-3 Oct. 2019,ieeexplore
10.1109/SPAC53836.2021.9539933,An LSTM based Malicious Traffic Attack Detection in Industrial Internet,IEEE,Conferences,"Current Industrial Internet faces serious threats where attackers propagate malicious flows, resulting in communication failures in the Industrial Internet. In this work, we propose a practical and novel method to detect malicious traffic attack in real time with high accuracy. Our primary idea is to capture network flow, extract adequate network flow features, construct a long short-term memory (LSTM) based deep learning model, and identify the property of the corresponding network flow. Whether the network suffers attack or not is then determined according to the detection results. The corresponding prototype is also implemented in the Industrial Internet which is equipped with Software Defined Networking (SDN). Experimental results validate that the proposed method is effective in defending against malicious traffic attack in real-world network.",https://ieeexplore.ieee.org/document/9539933/,"2021 International Conference on Security, Pattern Analysis, and Cybernetics（SPAC)",18-20 June 2021,ieeexplore
10.1109/ASIC.1994.404614,An analog VLSI neural network for real-time image processing in industrial applications,IEEE,Conferences,"In this paper we present an analog VLSI architecture that implements a neural network for image processing in industrial environment. The analog architecture is highly modular and operates in real time. The circuit implementation is based on simple and effective circuit primitives. Special care has been devoted to the analysis of the linearity and the precision of computation. A test chip, implementing the filtering stage of the architecture, has been designed and realized.&lt;<ETX>&gt;</ETX>",https://ieeexplore.ieee.org/document/404614/,Proceedings Seventh Annual IEEE International ASIC Conference and Exhibit,19-23 Sept. 1994,ieeexplore
10.1109/INDIN.2005.1560420,An analogue recurrent neural network for trajectory learning and other industrial applications,IEEE,Conferences,"A real-time analogue recurrent neural network (RNN) can extract and learn the unknown dynamics (and features) of a typical control system such as a robot manipulator. The task at hand is a tracking problem in the presence of disturbances. With reference to the tasks assigned to an industrial robot, one important issue is to determine the motion of the joints and the effector of the robot. In order to model robot dynamics we use a neural network that can be implemented in hardware. The synaptic weights are modelled as variable gain cells that can be implemented with a few MOS transistors. The network output signals portray the periodicity and other characteristics of the input signal in unsupervised mode. For the specific purpose of demonstrating the trajectory learning capabilities, a periodic signal with varying characteristics is used. The developed architecture, however, allows for more general learning tasks typical in applications of identification and control. The periodicity of the input signal ensures convergence of the output to a limit cycle. Online versions of the synaptic update can be formulated using simple CMOS circuits. Because the architecture depends on the network generating a stable limit cycle, and consequently a periodic solution which is robust over an interval of parameter uncertainties, we currently place the restriction of a periodic format for the input signals. The simulated network contains interconnected recurrent neurons with continuous-time dynamics. The system emulates random-direction descent of the error as a multidimensional extension to the stochastic approximation. To achieve unsupervised learning in recurrent dynamical systems we propose a synapse circuit which has a very simple structure and is suitable for implementation in VLSI.",https://ieeexplore.ieee.org/document/1560420/,"INDIN '05. 2005 3rd IEEE International Conference on Industrial Informatics, 2005.",10-12 Aug. 2005,ieeexplore
10.1109/FUZZ45933.2021.9494483,An approach to bridge the gap between ubiquitous embedded devices and JFML: A new module for Internet of Things,IEEE,Conferences,"Internet of Things enables sensors and actuators to share heterogeneous data between different devices. Such data can be used to create intelligent systems to control diverse structures available in houses, cities, or industrial environments among others. In this context, one of the most used approaches to handle these intelligent systems is based on Fuzzy Rule-Based Systems (FRBS) due to their suitability for addressing complex data and managing their imprecision. However, most of the current developments in this area are usually ad-hoc solutions limited by the intercommunication between FRBS and IoT devices. This results into significant challenges in reusing these solutions to solve latent problems. To bridge this gap, a new module for the open source library JFML is proposed to offer a complete implementation of an IoT infrastructure to develop intelligent IoT solutions based on the IEEE std 1855-2016. Moreover, a case study with real IoT devices is presented to showcase the use of the proposed module.",https://ieeexplore.ieee.org/document/9494483/,2021 IEEE International Conference on Fuzzy Systems (FUZZ-IEEE),11-14 July 2021,ieeexplore
10.1109/AIIA.1988.13342,An expert PID controller,IEEE,Conferences,"The authors report on the development of an expert PID (proportional-integral-differential) controller for industrial processes. It uses the optimal property of a certain region in the P, I, and D parameter space providing minimum integrated (average) error when the required damping is ensured otherwise. The expert system uses an algorithm which basically modifies the settings of a conventional real-time PID controller continually until the parameters enter the optimum region mentioned above. The optimum region for a given process depends on the characterization of that process which is carried out in a simple indentification procedure prior to convergence. The authors describe the theory, the algorithm used, the hardware, and the software aspects of the implementation of the expert system as well as the test results obtained on a simulated process control system.&lt;<ETX>&gt;</ETX>",https://ieeexplore.ieee.org/document/13342/,Proceedings of the International Workshop on Artificial Intelligence for Industrial Applications,25-27 May 1988,ieeexplore
10.1109/ICNN.1994.374213,An implementation and evaluation of the ART1 neural network for pattern recognition,IEEE,Conferences,"A key to solving the stability-plasticity dilemma is to add a feedback mechanism between the competitive and the input layer of a network. This feedback mechanism facilitates the learning of new information without destroying old information, automatic switching between stable and plastic modes, and stabilization of the encoding of the classes done by the nodes resulting from this approach we have a neural network architecture that is particularly suited for pattern-classification problems in real world environments. For industrial use, ART1 neural networks have the potential of becoming an important component in a variety of commercial and military systems. Efficient software emulations of these networks are adequate in many of today's low-end applications such as information retrieval or group technology; but for larger applications, special purpose hardware is required to achieve the expected performance requirements.&lt;<ETX>&gt;</ETX>",https://ieeexplore.ieee.org/document/374213/,Proceedings of 1994 IEEE International Conference on Neural Networks (ICNN'94),28 June-2 July 1994,ieeexplore
10.1109/SIPNN.1994.344942,An improved JPEG image coder using the adaptive fast approximate Karhunen-Loeve transform (AKLT),IEEE,Conferences,"A new fast approximate Karhunen-Loeve transform (AKLT) [Lan and Reed, 1993; Reed and Lan, 1993] was discovered. This novel transform is derived by use of the perturbation theory of linear operators. The theoretical advantages of the AKLT over the DCT, the current industrial standard for image compression, are discussed thoroughly in Reed and Lan, and Lan and Reed for the first-order Markov image model. In the present paper, an improved JPEG image coder which uses the adaptive AKLT is presented. An improvement of 5% in compression ratio as compared with the DCT-JPEG system is obtained when the desired nominal data rate is above 0.4 bits/pixel for typical real-life images. The rate-distortion curves indicate a superiority of the adaptive AKLT scheme over the DCT scheme for all ranges of the data rate. It is worthwhile to note that this new adaptive AKLT-JPEG system can be really implemented using the existing JPEG chip set with only a slight modification.&lt;<ETX>&gt;</ETX>",https://ieeexplore.ieee.org/document/344942/,"Proceedings of ICSIPNN '94. International Conference on Speech, Image Processing and Neural Networks",13-16 April 1994,ieeexplore
10.1109/CDC.2012.6426325,An improved Predictive Optimal Controller with elastic search space for steam temperature control of large-scale supercritical power unit,IEEE,Conferences,"Predictive optimal control (POC) combined with artificial neural networks (ANNs) modeling and advanced heuristic optimization is a powerful technique for intelligent control. But actual implementation of the POC in complex industrial processes is limited by its known drawbacks, including the oscillation resulting from random search direction, difficulty in meeting the real-time requirement, and unresolved adaptability and generalization ability of the ANN predictive model. In resolving these problems, an improved Intelligent Predictive Optimal Controller (IPOC) with elastic search space is proposed in this paper. A new simpler and high-efficiency Particle Swarm Optimization (PSO) algorithm is adopted to find the optimal solution in fewer epochs to meet the real-time control requirements. The system output error in each control step is fed back to adjust the search space dynamically to prevent control oscillation and also make it easier to find the optimal solution. An improved recurrent neural network with external delayed inputs and outputs is constructed to model the dynamic response of the highly nonlinear system. The proposed IPOC is used to superheater steam temperature control of a 600MW supercritical power unit. Extensive control simulation tests are made to verify the validity of the new control scheme in a full-scope simulator.",https://ieeexplore.ieee.org/document/6426325/,2012 IEEE 51st IEEE Conference on Decision and Control (CDC),10-13 Dec. 2012,ieeexplore
10.1109/COGINF.2011.6016132,An intelligent fault recognizer for rotating machinery via remote characteristic vibration signal detection,IEEE,Conferences,"Monitoring industrial machine health in real-time is not only highly demanded but also significantly complicated and difficult. Possible reasons for this include: (a) Access to the machines on site is sometimes impracticable; and (b) The environment in which they operate is usually not human-friendly due to pollution, noise, hazardous wastes, etc. Despite the theoretically sound findings on developing intelligent solutions for machine condition based monitoring, there are few commercial tools in the market that can readily be used. This paper reports on the development of an intelligent fault recognition and monitoring system (Melvin I), which detects and diagnoses rotating machine conditions according to changes in fault frequency indicators. The signals and data are remotely collected from designated sections of machines via data acquisition cards. They are processed by a signal processor in order to extract characteristic vibration signals of ten key performance indicators (KPIs). A 3-layer neural network is designed to recognize and classify faults based on the set of KPIs. The system implemented in our laboratory and applied in the field can also incorporate new experiences into the knowledge base without overwriting previous training. Preliminary results have demonstrated that Melvin I is a smart tool for both system vibration analysts and industrial machine operators.",https://ieeexplore.ieee.org/document/6016132/,IEEE 10th International Conference on Cognitive Informatics and Cognitive Computing (ICCI-CC'11),18-20 Aug. 2011,ieeexplore
10.1109/ICIT.2017.7915520,An intelligent maintenance planning framework prototype for production systems,IEEE,Conferences,"The Intelligent Maintenance Planner (IMP) is designed to automate and improve maintenance processes in industrial applications. The system tracks the entire process cycle beginning with data acquisition and management, it then detects and classifies failure states, initializes maintenance cases, and selects and assigns the required resources. IMP guides maintenance work processes, by automatically providing instructions and augmented reality information. Subsequent feedback of the maintenance process and new or updated information is added to the system and used to train selection algorithms. A prototype of IMP was implemented based on an industrial SCADA system and cloud solutions for storage and machine learning capabilities. This report explains the stages of the maintenance process and provides an outline of the implementation and project results.",https://ieeexplore.ieee.org/document/7915520/,2017 IEEE International Conference on Industrial Technology (ICIT),22-25 March 2017,ieeexplore
10.1109/ICMA.2013.6618173,An intelligent object manipulation framework for industrial tasks,IEEE,Conferences,"This paper presents an intelligent object manipulation framework for industrial tasks, which integrates a sensor-rich multi-fingered robot hand, an industrial robot manipulator, a conveyor belt and employs machine learning algorithms. The framework software architecture is implemented using a Windows 7 operating system with RTX real-time extension for synchronous handling of peripheral devices. The framework uses Scale Invariant Feature Transform (SIFT) image processing algorithm, Support Vector Machine (SVM) machine learning algorithm and 3D point cloud techniques for intelligent object recognition based on RGB camera and laser rangefinder information from the robot hand end effector. The objective is automated manipulation of objects with different shapes and poses with minimum programming effort applied by a user.",https://ieeexplore.ieee.org/document/6618173/,2013 IEEE International Conference on Mechatronics and Automation,4-7 Aug. 2013,ieeexplore
10.1109/SYSCON.2018.8369547,An interactive architecture for industrial scale prediction: Industry 4.0 adaptation of machine learning,IEEE,Conferences,"According to wiki definition, there are four design principles in Industry 4.0. These principles support companies in identifying and implementing Industry 4.0 scenarios, namely, Interoperability, Information transparency, Technical assistance, Decentralized decisions. In this paper we have discussed our work on an implementation of a machine learning based interactive architecture for industrial scale prediction for dynamic distribution of water resources across the continent, keeping the four corners of Industry 4.0 in place. We report the possibility of producing most probable high resolution estimation regarding the water balance in any region within Australia by implementation of an intelligent system that can integrate spatial-temporal data from various independent sensors and models, with the ground truth data produced by 250 practitioners from the irrigation industry across Australia. This architectural implementation on a cloud computing platform linked with a freely distributed mobile application, allowing interactive ground truthing of a machine learning model on a continental scale, shows accuracy of 90% with 85% sensitivity of correct surface soil moisture estimation with end users at its complete control. Along with high level of information transparency and interoperability, providing on-demand technical supports and motivating users by allowing them to customize and control their own local predictive models, show the successfulness of principles in Industry 4.0 in real environmental issues in the future adaptation in various industries starting from resource management to modern generation soft robotics.",https://ieeexplore.ieee.org/document/8369547/,2018 Annual IEEE International Systems Conference (SysCon),23-26 April 2018,ieeexplore
10.1109/ISWCS.2018.8491060,Analysis of Machine Learning Algorithms for Spectrum Decision in Cognitive Radios,IEEE,Conferences,"Technological advances in recent years have reduced the manufacturing costs of wireless devices, increasing the number of such devices and applications. Most of these applications are supported by ISM (Industrial, Scientific, and Medical) frequencies, which due to their wide use in several types of devices have suffered from harmful interference. To solve this problem, Cognitive Radios paradigm has been proposed to guarantee the quality of communication. Several frameworks were proposed for the development of a Cognitive Radios Networks (CRN), but none of them were effectively implemented in hardware. This paper presents an analysis of machine learning algorithms in architecture for the development of CRN in real hardware. Results demonstrated the feasibility of the architecture and the decision methods based on machine learning algorithms can find the best communication channel.",https://ieeexplore.ieee.org/document/8491060/,2018 15th International Symposium on Wireless Communication Systems (ISWCS),28-31 Aug. 2018,ieeexplore
10.1109/ETFA.2019.8869274,Analyzing availability and QoS of service-oriented cloud for industrial IoT applications,IEEE,Conferences,"Internet of Things and cloud services are one of main enablers in fourth industrial revolution. Real-time industrial systems have high availability requirements of 99.9% to 99.999% whereas architectures built on regional cloud services and IoT do not provide similar guarantees or Service Level Agreement. These differences of QoS and SLA availability between Operational Technology and Information Technology has become a main challenge in adoption of Industrial Internet of Things (IIoT) for real-time applications.This work presents an approach to find end-to-end QoS and availability for an IIoT architecture. Device-to-cloud, cloud-to-cloud and inside-cloud experiments have been performed over eight weeks where each experiment have more then four million QoS measurements. Our availability analysis shows that a remote IoT connected to a less busy cloud region gives higher availability as compared to an IoT device inside a busy cloud region. IIoT and regional cloud services provide good QoS with 99% to 99.9% availability for 1sec soft real-time requirements. In 100ms applications, more efforts are required to achieve higher then 95% availability and design industrial SLA. IIoT applications with 10sec latency like machine learning models can get 99.9% availability with cloud. Availability loss due to communication is almost 1% for 100ms applications. These results also provide requirements and future work of industrial edge computing for IIoT on real-time cloud.",https://ieeexplore.ieee.org/document/8869274/,2019 24th IEEE International Conference on Emerging Technologies and Factory Automation (ETFA),10-13 Sept. 2019,ieeexplore
10.1109/ICAC.2017.21,Ananke: A Q-Learning-Based Portfolio Scheduler for Complex Industrial Workflows,IEEE,Conferences,"Complex workflows that process sensor data are useful for industrial infrastructure management and diagnosis. Although running such workflows in clouds promises reduced operational costs, there are still numerous scheduling challenges to overcome. Such complex workflows are dynamic, exhibit periodic patterns, and combine diverse task groupings and requirements. In this work, we propose ANANKE, a scheduling system addressing these challenges. Our approach extends the state-of-the-art in portfolio scheduling for data centers with a reinforcement-learning technique, and proposes various scheduling policies for managing complex workflows. Portfolio scheduling addresses the dynamic aspect of the workload. Q-learning, allows our approach to adapt to the periodic patterns of the workload, and to tune the other configuration parameters. The proposed policies are heuristics that guide the provisioning process, and map workflow tasks to the provisioned cloud resources. Through real-world experiments based on real and synthetic industrial workloads, we analyze and compare our prototype implementation of ANANKE with a system without portfolio scheduling (baseline) and with a system equipped with a standard portfolio scheduler. Overall, our experimental results give evidence that a learning-based portfolio scheduler can perform better and consume fewer resources than state-of-the-art alternatives, in particular for workloads with uniform arrival patterns.",https://ieeexplore.ieee.org/document/8005354/,2017 IEEE International Conference on Autonomic Computing (ICAC),17-21 July 2017,ieeexplore
10.1109/ICNC.2008.44,Anomaly Intrusion Detection Methods for Wireless LAN,IEEE,Conferences,"Nowday, wireless LANs are widely deployed in various places such as corporate office conference rooms, industrial warehouses, Internet-ready classrooms, etc. However, new concerns regarding security have been raised. Intrusion detection, as the second line of defense, is an indispensable tool for highly survivable networks. In this paper two anomaly intrusion detection methods are proposed for wireless LANs. One method uses hidden Markov model to check reflector DoS attacks, another based on adaptive resonance theory, which can learn the normal behavior with unsupervised method. The advantages of the methods are that they donpsilat need attack signatures and can detect intrusion in real-time. Experiments exhibit fairly good results, the attacks being collaboratively detected immediately.",https://ieeexplore.ieee.org/document/4667421/,2008 Fourth International Conference on Natural Computation,18-20 Oct. 2008,ieeexplore
10.1109/MIPRO.2015.7160443,Anomaly detection in thermal power plant using probabilistic neural network,IEEE,Conferences,"Anomalies are integral part of every system's behavior and sometimes cannot be avoided. Therefore it is very important to timely detect such anomalies in real-world running power plant system. Artificial neural networks are one of anomaly detection techniques. This paper gives a type of neural network (probabilistic) to solve the problem of anomaly detection in selected sections of thermal power plant. Selected sections are steam superheaters and steam drum. Inputs for neural networks are some of the most important process variables of these sections. It is noteworthy that all of the inputs are observable in the real system installed in thermal power plant, some of which represent normal behavior and some anomalies. In addition to the implementation of this network for anomaly detection, the effect of key parameter change on anomaly detection results is also shown. Results confirm that probabilistic neural network is excellent solution for anomaly detection problem, especially in real-time industrial applications.",https://ieeexplore.ieee.org/document/7160443/,"2015 38th International Convention on Information and Communication Technology, Electronics and Microelectronics (MIPRO)",25-29 May 2015,ieeexplore
10.1109/ICPHM.2019.8819421,Application of Deep Learning for Fault Diagnostic in Induction Machine’s Bearings,IEEE,Conferences,"Recent developments in sensor technologies and advances in communication systems have resulted in deployment of a large number of sensors for collecting condition monitoring (CM) data in order to monitor health condition of a manufac-tring/industrial system. Efficient utilization of sensory data leads to highly accurate results in system fault diagnostics/prognostics. The exponential growth of CM data poses significant analytical challenges, due to their high variety, high dimensionality and high velocity rendering conventional health monitoring tools impractical. In this regard, the paper proposes a deep learning-based framework for fault diagnosis of an induction machine's bearing based on real data set provided by Case Western Reserve University bearing data center. In particular, we focus on deep bidirectional long short-term memory (BiD-LSTM) networks fed with raw signals for fault diagnosis to address drawbacks of conventional machine learning (ML) solutions such as support vector machines. A numerical example is provided to illustrate the complete procedure of the proposed framework, which shows the great potentials of the BiD-LSTM for detection of different types of the bearing fault with high accuracy. The effectiveness of the proposed model is demonstrated through a comparison with a recently developed deep neural network (DNN) that considers temporal coherence for the same data set. The results indicate that the proposed framework provides considerably improved performance in comparison to its counterparts.",https://ieeexplore.ieee.org/document/8819421/,2019 IEEE International Conference on Prognostics and Health Management (ICPHM),17-20 June 2019,ieeexplore
10.1109/ICEKIM52309.2021.00040,Application of Teaching Innovation Based on robotics engineering,IEEE,Conferences,"As the core major of “Internet + Industrial Intelligence”, robotics engineering is an upgrade and reconstruction of traditional engineering major. The industrial robot course is the professional core course of the Robotics Engineering. It is also a comprehensive course of multi-discipline integration, which involved mechanical engineering, automatic control, computer, sensor, electronic technology, artificial intelligence and other multi-disciplinary content. Robotics Engineering is characterized by broad foundation, great difficulty, emphasis on practice, rapid development and application of new knowledge. In the process of implementation of the teaching innovation, the new concept of engineering education was applied to propose a new form of curriculum system. Taking the projects of engineering as the study objects, disassemble the knowledge points involved in industrial robots, break the course boundaries, reshape the knowledge system, draw knowledge maps and then design teaching activities. In teaching innovation, teachers extend classroom through formation of subject competition teams, promote teaching and promote learning by competition, realize the integration of “teaching, class and competition”, build a bridge between theory and practice, then complete the transformation from knowledge learning to ability training. Besides, they also keep contact with intelligent manufacturing enterprises in Zhuhai and the Bay Area to obtain real-time new developments in enterprises. Thus, the latest information was introduced into classroom. Therefore, the meaning of “production, teaching, research and application” has been deepened. According to the characteristics of the knowledge points of the course, experts were invited to make special lectures for students which can bring them with international perspective and frontier knowledge.",https://ieeexplore.ieee.org/document/9479656/,"2021 2nd International Conference on Education, Knowledge and Information Management (ICEKIM)",29-31 Jan. 2021,ieeexplore
10.1109/CYBER53097.2021.9588269,Application of YOLO Object Detection Network In Weld Surface Defect Detection,IEEE,Conferences,"As industrial production becomes more modern and intelligent today, the inspection of product quality of the workshop is becoming more and more accustomed to replacing the old manual visual inspection methods with automated inspection systems. In the welding field, automated welding robots are not only used in traditional large-scale automobile assembly lines. In more general welding work, welding robots also plays an important role. The inspection of the welding quality of the welding robot is mainly to detect the four main types of weld defects. Compared to traditional defect classification based on support vector machines and defect detection based on template matching, this paper uses a welding surface defect detection system designed based on deep learning methods. By working with workshop welding experts, a large-scale image of nearly 5000 pictures is built. Large-scale weld defect datasets, while using the real-time and accuracy of the YOLO series of deep learning object detection frameworks, the weld defects detection model reaches 75.5% mean average precision(mAP) in constructed weld defect data set. In addition, the construction cost of the detection model and the deployment time of the detection system are greatly reduced. During the field test of the system in the workshop, among a batch of welding workpieces provided by the factory, the detection accuracy of weld defects reached 71%, which initially met the requirements of the workshop for an automated defect detection system.",https://ieeexplore.ieee.org/document/9588269/,"2021 IEEE 11th Annual International Conference on CYBER Technology in Automation, Control, and Intelligent Systems (CYBER)",27-31 July 2021,ieeexplore
10.1109/ISIC.2002.1157861,Application of a model-based fault detection and diagnosis using parameter estimation and fuzzy inference to a DC-servomotor,IEEE,Conferences,"Fault detection and diagnosis are very much needed in many industrial applications. One of the most popular scheme is the model-based fault diagnostic. Recently, artificial intelligence techniques have been found to be suitable for fault detection and diagnosis and a variety of techniques have been proposed in this area. However, only few applications and real time implementation of the schemes have been reported. In this paper, we use a fault detection and diagnostic scheme based on the model-based approach using parameter estimation and fuzzy inference and experimented it on a DC motor servo trainer. The model of the plant is obtained using the recursive least squares parameter estimation technique, and fuzzy inference is used for the interpretation of the fault. Several faults have been identified on the system. The faults are then simulated on the motor and experiments are carried out to diagnose the types of faults. Experiments show the effectiveness of the proposed technique for real time applications.",https://ieeexplore.ieee.org/document/1157861/,Proceedings of the IEEE Internatinal Symposium on Intelligent Control,30-30 Oct. 2002,ieeexplore
10.1109/ICOS.2011.6079285,Application of multi-step time series prediction for industrial equipment prognostic,IEEE,Conferences,The use of prognostics is critically to be implemented in industrial. This paper presents an application of multi-step time series prediction to support industrial equipment prognostic. An artificial neural network technique with sliding window is considered for the multi-step prediction which is able to predict the series of future equipment condition. The structure of prognostic application is presented. The feasibility of this prediction application was demonstrated by applying real condition monitoring data of industrial equipment.,https://ieeexplore.ieee.org/document/6079285/,2011 IEEE Conference on Open Systems,25-28 Sept. 2011,ieeexplore
10.1109/NetSoft48620.2020.9165317,Applying Machine Learning to End-to-end Slice SLA Decomposition,IEEE,Conferences,"5G is set to revolutionize the network service industry with unprecedented use-cases in industrial automation, augmented reality, virtual reality and many other domains. Network slicing is a key enabler to realize this concept, and comes with various SLA requirements in terms of latency, throughput, and reliability. Network slicing is typically performed in an end-to-end (e2e) manner across multiple domains, for example, in mobile networks, a slice can span access, transport and core networks. Thus, if an SLA requirement is specified for e2e services, we need to ensure that the total SLA budget is appropriately proportioned to each participating domain in an adaptive manner. Such an SLA decomposition can be extremely useful for network service operators as they can plan accordingly for actual deployment. In this paper we design and implement an SLA decomposition planner for network slicing using supervised machine learning algorithms. Traditional optimization based approaches cannot deal with the dynamic nature of such services. We design machine learning models for SLA decomposition, based on random forest, gradient boosting and neural network. We then evaluate each class of algorithms in terms of accuracy, sample complexity, and model explainability. Our experiments reveal that, in terms of these three requirements, the gradient boosting and neural network algorithms for SLA decomposition out-perform random forest algorithms, given emulated data sets.",https://ieeexplore.ieee.org/document/9165317/,2020 6th IEEE Conference on Network Softwarization (NetSoft),29 June-3 July 2020,ieeexplore
10.1109/USBEREIT51232.2021.9455060,Applying of Recurrent Neural Networks for Industrial Processes Anomaly Detection,IEEE,Conferences,"The paper considers the issue of recurrent neural networks applicability for detecting industrial process anomalies to detect intrusion in Industrial Control Systems. Cyberattack on Industrial Control Systems often leads to appearing of anomalies in industrial process. Thus, it is proposed to detect such anomalies by forecasting the state of an industrial process using a recurrent neural network and comparing the predicted state with actual process' state. In the course of experimental research, a recurrent neural network with one-dimensional convolutional layer was implemented. The Secure Water Treatment dataset was used to train model and assess its quality. The obtained results indicate the possibility of using the proposed method in practice. The proposed method is characterized by the absence of the need to use anomaly data for training. Also, the method has significant interpretability and allows to localize an anomaly by pointing to a sensor or actuator whose signal does not match the model's prediction.",https://ieeexplore.ieee.org/document/9455060/,"2021 Ural Symposium on Biomedical Engineering, Radioelectronics and Information Technology (USBEREIT)",13-14 May 2021,ieeexplore
10.1109/DSMP.2016.7583579,Artificial intelligence methods for data based modeling and analysis of complex processes: Real life examples,IEEE,Conferences,In this paper two computational intelligence methods are considered. In the first one the Neural Network based Controller with combination of Genetic Algorithm network structure optimization is presented. In the second example fuzzy logic control system is developed and implemented on industrial heating plant. Obtained knowledge considered as a part of the modular ICS (Intelligent Control System) software.,https://ieeexplore.ieee.org/document/7583579/,2016 IEEE First International Conference on Data Stream Mining & Processing (DSMP),23-27 Aug. 2016,ieeexplore
10.1109/ICHQP.1998.760178,Artificial neural networks for power systems harmonic estimation,IEEE,Conferences,"An artificial neural network model was developed and implemented for power system harmonics estimation. The model was given the name FNN, which stands for Fourier neural network. It was tested offline under different conditions and was compared with FFT. The results of the offline tests indicate that the FNN has very high estimation accuracy. It has a recursive nature that makes it a candidate for real-time measurements. It also gave good results in a noisy environment where SNR is as low as 10 dB. The FNN model was implemented on a PC using a data acquisition board. The system was used for an online harmonic estimation study. The FNN was able to estimate the harmonic components of voltage and current at various levels. The estimation results were compared with the data obtained using a FLUKE 41 harmonics meter. The comparison revealed that the ANN based harmonic estimation model performs similarly to industrial-approved meters.",https://ieeexplore.ieee.org/document/760178/,8th International Conference on Harmonics and Quality of Power. Proceedings (Cat. No.98EX227),14-16 Oct. 1998,ieeexplore
10.1109/UPEC.2017.8231939,Assessing the impact of load forecasting accuracy on battery dispatching strategies with respect to Peak Shaving and Time-of-Use (TOU) applications for industrial consumers,IEEE,Conferences,"Energy Storage Systems will play crucial role in controlling the grid of the future when increased penetration of renewable energy sources will take place. Especially batteries are expected to occupy a considerable share of the total energy storage market by simultaneously providing services to different stakeholders such as energy producers, transmission/distribution operators, residential, commercial and industrial consumers. Nowadays, Peak shaving and Time-of-Use applications are the most common services that standalone battery storage systems can provide to industrial consumers (without integrated PV-systems and/or wind turbines). A big part of the existing literature addressing such applications aims at developing an offline algorithm for optimal battery deployment based on a known load profile (or accurately predicted) without taking into consideration real time conditions. This paper investigates the impact of industrial load forecasting errors on dispatching strategies of battery storage systems on economically driven peak shaving and Time-of-Use applications. An artificial neural network has been developed and used as a prediction model of an industrial load profile. The neural network was trained, validated and tested on historical load data with time resolution of 15 minutes, provided by the local distribution operator of the Belgian electric grid. The performance of the neural network in terms of output-target regression and mean absolute error is 0.833 and 10.02% respectively. Afterwards, a simulation was carried out comparing four different scenarios of peak shaving. The results show that the prediction accuracy of the presented neural network is not competitive enough. Peak shaving based on predicted profiles becomes reliable for lower forecasting errors. For this purpose, further access into the process and types of loads of the user is required in order to come up with a more sophisticated prediction model.",https://ieeexplore.ieee.org/document/8231939/,2017 52nd International Universities Power Engineering Conference (UPEC),28-31 Aug. 2017,ieeexplore
10.1109/ISDA.2010.5687116,Associative prediction model and clustering for product forecast data,IEEE,Conferences,"Association rules are adopted to discover the interesting relationship and knowledge in a large dataset. Knowledge may appear in terms of a frequent pattern discovered in a large number of production data. This knowledge can improve or solve production problems to achieve low cost production. To obtain knowledge and quality information, data mining can be applied to the manufacturing industry. In this study, we used one of the association rule approach, i.e. Apriori algorithm to build an associative prediction model for product forecast data. Also, we adopt the simplest method in clustering, k-means algorithm to attain the link between patterns. The real industrial product forecast data for one year duration is used in the experiment. This data consists of 42 products with two important attributes, i.e. time in the week and required quantity. Since the data mining processes need a large amount of data, we simulated these data by using the Monte Carlo technique to obtain another 15 years of simulated forecast data. There are two main experiments for the association rules mining and clustering. As a result, we obtain an associative prediction model and clustering for the forecasting data. The extracted model provides the prediction knowledge about the range of production in a certain period.",https://ieeexplore.ieee.org/document/5687116/,2010 10th International Conference on Intelligent Systems Design and Applications,29 Nov.-1 Dec. 2010,ieeexplore
10.1109/SMC.2013.819,Automated Sound Signalling Device Quality Assurance Tool for Embedded Industrial Control Applications,IEEE,Conferences,This paper presents a novel system for automatic detection and recognition of faulty audio signaling devices as part of an automated industrial manufacturing process. The system uses historical data labeled by human experts in detecting faulty signaling devices to train an artificial neural network based classifier for modeling their decision making process. The neural network is implemented on a real time embedded micro controller which can be more efficiently incorporated into an automated production line eliminating the need for a manual inspection within the manufacturing process. We present real world experiments based on data pertaining to the production and manufacture of audio signaling components used in car instrument clusters. Our results show that the proposed expert system is able to successfully classify faulty audio signaling devices to a high degree of accuracy. The results can be generalized to other signaling devices where an output signal is represented by a complex and changing frequency spectrum even with significant environmental noise.,https://ieeexplore.ieee.org/document/6722574/,"2013 IEEE International Conference on Systems, Man, and Cybernetics",13-16 Oct. 2013,ieeexplore
10.1109/RUSAUTOCON.2018.8501681,Automatic Control System of Low-Emission Combustion Chamber Based on Neural Network Emission Model,IEEE,Conferences,"The paper aims to resolve the problem of reliability improvement of aircraft gas turbine engine's automatic control systems (ACS GTE) based on the use of the built-in mathematic model of the control object. The purpose of the study is the development of the on-board real-time monitoring and control of nitrogen and carbon oxide emissions in a modern GTE. The main purpose of the research is compliance with the emission standards while ensuring the stability of the combustion process under the influence of external and internal factors, based on the creation of robust control algorithms for low-emission combustion chambers (LECC). The mathematic model of the emission generation is designed for a work in conjunction with ACS GTE in a real environment and satisfy the requirements for compactness, speed and accuracy of engine parameters' identification in statics and dynamics in a wide range of operating modes, flight and engine conditions. The LECC's emission identification algorithms are based on the artificial neural networks. The data array of a full-scale experiment was obtained for studying the characteristics of emission during the industrial LECC operation. The neural network simulating LECC's emission was designed and trained on the basis of the obtained data. The MATLAB simulation results showed the high accuracy of the developed model. The different factors' importance for the model accuracy was studied. It turned out that the most important are the parameters of temperature and pressure. The obtained results can be used for the improvement of the reliability and environmental attractiveness of ACS GTE.",https://ieeexplore.ieee.org/document/8501681/,2018 International Russian Automation Conference (RusAutoCon),9-16 Sept. 2018,ieeexplore
10.1109/ICASSP39728.2021.9415049,Automatic Fine-Grained Localization of Utility Pole Landmarks on Distributed Acoustic Sensing Traces Based on Bilinear Resnets,IEEE,Conferences,"In distributed acoustic sensing (DAS) on aerial fiber-optic cables, utility pole localization is a prerequisite for any subsequent event detection. Currently, localizing the utility poles on DAS traces relies on human experts who manually label the poles’ locations by examining DAS signal patterns generated in response to hammer knocks on the poles. This process is inefficient, error-prone and expensive, thus impractical and non-scalable for industrial applications. In this paper, we propose two machine learning approaches to automate this procedure for large-scale implementation. In particular, we investigate both unsupervised and supervised methods for fine-grained pole localization. Our methods are tested on two real-world datasets from field trials, and demonstrate successful estimation of pole locations at the same level of accuracy as human experts and strong robustness to label noises.",https://ieeexplore.ieee.org/document/9415049/,"ICASSP 2021 - 2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)",6-11 June 2021,ieeexplore
10.1109/IJCNN.1991.155523,Automatic classification of solder joint images,IEEE,Conferences,"Summary form only given, as follows. Elaborate techniques have been developed to obtain data from specimens in industrial quality control tasks. However, a problem in the field of visual inspection is how to process complex data in real time. An approach to classification of solder joint images by means of a neuronlike binary associative memory has been developed. All the algorithms and architectures considered could easily be implemented with digital VLSI technology to realize an extremely fast classifier.&lt;<ETX>&gt;</ETX>",https://ieeexplore.ieee.org/document/155523/,IJCNN-91-Seattle International Joint Conference on Neural Networks,8-12 July 1991,ieeexplore
10.1109/ICIT.2018.8352157,Automatic parameter learning for easy instruction of industrial collaborative robots,IEEE,Conferences,"The manufacturing industry faces challenges in meeting requirements of flexibility, product variability and small batch sizes. Automation of high mix, low volume productions requires faster (re)configuration of manufacturing equipment. These demands are to some extend accommodated by collaborative robots. Certain actions can still be hard or impossible to manually adjust due to inherent process uncertainties. This paper proposes a generic iteratively learning approach based on Bayesian Optimisation to efficiently search for the optimal set of process parameters. The approach takes into account the process uncertainties by iteratively making a statistical founded choice on the next parameter-set to examine only based on the prior binomial outcomes. Moreover, our function estimator uses Wilson Score to make proper estimates on the success probability and the associated uncertain measure of sparsely sampled regions. The function estimator also generalises the experiment outcomes to the neighbour region through kernel smoothing by integrating Kernel Density Estimation. Our approach is applied to a real industrial task with significant process uncertainties, where sufficiently robust process parameters cannot intuitively be chosen. Using our approach, a collaborative robot automatically finds a reliable solution.",https://ieeexplore.ieee.org/document/8352157/,2018 IEEE International Conference on Industrial Technology (ICIT),20-22 Feb. 2018,ieeexplore
10.1109/INDIN.2012.6301355,Automation of chlorination process for drinking water treatment plant: Control strategies,IEEE,Conferences,"The technological advances of industrial communications in the last decade allowed automatically to control processes that were manually controlled in a separate way. This paper presents the design and implementation for a novel automated chlorination system at the drinking water treatment plant in Sant Joan Despí (Barcelona, Spain). The automation of chlorination process meant two important challenges: firstly, the impact of dead time inherent to the measurement of process' variable of the water and its dosage has been minimized; and second, the system has been provided with the capacity of adaptation to the variability in the water quality and to the appearance of the ammonium in the treated water. The obtained results in this real process by the combination of a feedforward control strategy together with a classical feedback scheme have been largely satisfactory. The establishment of this new control system has represented a significant change in the working model.",https://ieeexplore.ieee.org/document/6301355/,IEEE 10th International Conference on Industrial Informatics,25-27 July 2012,ieeexplore
10.1109/ICATCCT.2016.7912031,Bearing fault diagnosis using discrete Wavelet Transform and Artificial Neural Network,IEEE,Conferences,"Rotating machinery has vast industrial applications in fields of petroleum, automotive, HVAC and food processing. Rotating machineries use bearings to perform rotational or linear movement of various subcomponents while reducing friction and stress. Compared other types of bearing, REBs offer a good balance of key attributes like friction, lifetime, stiffness, speed and cost. Hence, real-time monitoring and diagnosis of bearings is crucial to prevent failures, improve safety, avoid unforeseen downtime of production assembly lines and lower cost. We propose an approach based on Wavelet Transform and ANN for analysis of vibration signals from a rolling element bearing to identify and multi-classify its component defects. The vibration signals from the REB being analyzed are passed over to the software setup consisting of Wavelet Transform and ANN. To remove noise and extract the relevant features from this signal, we pass the vibration signal through a Wavelet transform. These features are retrieved using time domain parameters like Skewness, Kurtosis, RMS and Crest Factor and they are used as an input for ANN classifier. The role of the ANN is to classify the bearing fault features produced by the Wavelet Transform and identify bearing faults, if any. To this end, we have designed a feedforward topology ANN using the sigmoid transfer function. The ANN training methodology uses three learning paradigms - namely, Levenberg-Marquardt, Resilient Back-propogation and Scaled Conjugate method. The learning models generated by each algorithm are tested to find the one which gives better accuracy. The outcome of this experiment indicates that DWT and ANN can together achieve good accuracy and reliability in detection and classification of bearing faults.",https://ieeexplore.ieee.org/document/7912031/,2016 2nd International Conference on Applied and Theoretical Computing and Communication Technology (iCATccT),21-23 July 2016,ieeexplore
10.1109/NetSoft48620.2020.9165393,Benchmarking and Profiling 5G Verticals' Applications: An Industrial IoT Use Case,IEEE,Conferences,"The Industry 4.0 sector is evolving in a tremendous pace by introducing a set of industrial automation mechanisms tightly coupled with the exploitation of Internet of Things (IoT), 5G and Artificial Intelligence (AI) technologies. By combining such emerging technologies, interconnected sensors, instruments, and other industrial devices are networked together with industrial applications, formulating the Industrial IoT (IIoT) and aiming to improve the efficiency and reliability of the deployed applications and provide Quality of Service (QoS) guarantees. However, in a 5G era, efficient, reliable and highly performant applications' provision has to be combined with exploitation of capabilities offered by 5G networks. Optimal usage of the available resources has to be realised, while guaranteeing strict QoS requirements such as high data rates, ultra-low latency and jitter. The first step towards this direction is based on the accurate profiling of vertical industries' applications in terms of resources usage, capacity limits and reliability characteristics. To achieve so, in this paper we provide an integrated methodology and approach for benchmarking and profiling 5G vertical industries' applications. This approach covers the realisation of benchmarking experiments and the extraction of insights based on the analysis of the collected data. Such insights are considered the cornerstones for the development of AI models that can lead to optimal infrastructure usage along with assurance of high QoS provision. The detailed approach is applied in a real IIoT use case, leading to profiling of a set of 5G network functions.",https://ieeexplore.ieee.org/document/9165393/,2020 6th IEEE Conference on Network Softwarization (NetSoft),29 June-3 July 2020,ieeexplore
10.1109/ICAIIC51459.2021.9415189,Big Data Platform for Intelligence Industrial IoT Sensor Monitoring System Based on Edge Computing and AI,IEEE,Conferences,"The cutting edge of Industry 4.0 has driven everything to be converted to disruptive innovation and digitalized. This digital revolution is imprinted by modern and advanced technology that takes advantage of Big Data and Artificial Intelligence (AI) to nurture from automatic learning systems, smart city, smart energy, smart factory to the edge computing technology, and so on. To harness an appealing, noteworthy, and leading development in smart manufacturing industry, the modern industrial sciences and technologies such as Big Data, Artificial Intelligence, Internet of things, and Edge Computing have to be integrated cooperatively. Accordingly, a suggestion on the integration is presented in this paper. This proposed paper describes the design and implementation of big data platform for intelligence industrial internet of things sensor monitoring system and conveys a prediction of any upcoming errors beforehand. The architecture design is based on edge computing and artificial intelligence. To extend more precisely, industrial internet of things sensor here is about the condition monitoring sensor data - vibration, temperature, related humidity, and barometric pressure inside facility manufacturing factory.",https://ieeexplore.ieee.org/document/9415189/,2021 International Conference on Artificial Intelligence in Information and Communication (ICAIIC),13-16 April 2021,ieeexplore
10.1109/ICII.2018.00024,Brightics-IoT: Towards Effective Industrial IoT Platforms for Connected Smart Factories,IEEE,Conferences,"Industrial Internet-of-Things (IIoT) supports machines, computers and users to enable intelligent operations using advanced device management and data analytics. In recent years, thanks to standardized IoT platforms and advanced Artificial Intelligence (AI) technologies, there have been great advances in IIoT, and now it promises revolutions on various manufacturing domains such as transport, health, factory and energy. In this paper, based on our experience operating IIoT in various factory applications, we present the technical challenges of manufacturing facilities needed to be dealt with to collect huge amount of data in real-time and counteraction points of an IoT platform regarding these technical challenges and what kinds of features need to be implemented for intelligent services in the smart manufacturing. Finally, we introduce a story of applying industrial IoT platform in production to show how iterative development approaches can achieve business requirements based on elastically scaled-out architecture.",https://ieeexplore.ieee.org/document/8539113/,2018 IEEE International Conference on Industrial Internet (ICII),21-23 Oct. 2018,ieeexplore
10.1109/ITSC.2016.7795637,CJAMmer - traffic JAM Cause Prediction using Boosted Trees,IEEE,Conferences,"A traffic incident is defined by an event which provokes a disruption on the normal (free) flow condition of any highway. Such incidents must be caused by a recurrent excessive demand or, in alternative, by a series of possible stochastic occurrences which may suddenly reduce the road capacity (e.g. car accidents, extreme weather changes). This paper proposes a novel binary supervised learning method to classify congestion predictions regarding their causes - CJAMmer. It leverages on heterogeneous and ubiquitous data sources - such as weather, flow counts and traffic incident event logs - to generalize decision models able to understand the road congestion nature. CJAMmer settles on boosted decision trees using the well-known C4.5, as well as a straightforward feature generation process. A real world experiment was used to compare this method against other state-of-the-art classifiers. The results uncovered the high potential impact of this methodology on industrial scale traffic control systems.",https://ieeexplore.ieee.org/document/7795637/,2016 IEEE 19th International Conference on Intelligent Transportation Systems (ITSC),1-4 Nov. 2016,ieeexplore
10.1109/ICCE-Berlin.2018.8576251,CNN Inference: Dynamic and Predictive Quantization,IEEE,Conferences,"Deep Learning techniques like Convolutional Neural Networks (CNN) are the de-facto method for image classification with broad usage spanning across automotive, industrial, medicine, robotics etc. Efficient implementation of CNN inference on embedded device requires a quantization method, which minimizes the accuracy loss, ability to generalize across deployment scenarios as well as real-time processing. Existing literature doesn't address all these three requirements simultaneously. In this paper, we propose a novel quantization algorithm to overcome above mentioned challenges. The proposed solution dynamically selects the scale for quantizing activations and uses Kalman filter to predict quantization scale to reduce accuracy loss. The proposed solution exploits the range statistics from previous inference processes to estimate quantization scale, enabling real-time solution. The proposed solution is implemented on TI's TDA family of embedded automotive processors. The proposed solution is running real time semantic segmentation on TDA2x processor within 0.1% accuracy loss compared floating point algorithm. The solution performs well across multiple deployment scenarios (e.g. rain, snow, night etc) demonstrating generalization capability of the solution.",https://ieeexplore.ieee.org/document/8576251/,2018 IEEE 8th International Conference on Consumer Electronics - Berlin (ICCE-Berlin),2-5 Sept. 2018,ieeexplore
10.1109/MetroInd4.0IoT51437.2021.9488536,CNN-LSTM Neural Network Applied for Thermal Infrared Underground Water Leakage,IEEE,Conferences,"In this paper is proposed a methodological approach for the detection of leaks in water pipelines. The approach is based on the use of Infrared Thermography (IRT) for the real time monitoring, and of Artificial Neural Networks (ANNs) for the identification of potential leaks not easy visible by IRT. The input data source consists of radiometric data processed by Convolutional Neural Networks (CNNs) providing as output information about the presence or absence of the water leakages. A preliminary study has been carried out about a fixed monitoring station created to identify leaks in underground pipelines. In addition, has been implemented a platform to remotely acquire the thermograms and to analyze them by CNN networks detecting leakages, combined with Long Short-Term Memory (LSTM) neural networks, and image filtering algorithms, such as image segmentation and active contour snake approach. The LSTM network allows the prediction and calculation of the propagation trend of the leak plume. Finally, image filtering improves the visualization of leaks as it allows to draw the contours of the pixel clusters representing the leakages areas in the thermograms. The work was developed within the research framework of an industrial project. The proposed approach is suitable also for oil spill and gas leakages detections.",https://ieeexplore.ieee.org/document/9488536/,2021 IEEE International Workshop on Metrology for Industry 4.0 & IoT (MetroInd4.0&IoT),7-9 June 2021,ieeexplore
10.1109/IJCNN.1999.832669,Cascade steepest descent learning algorithm for multilayer feedforward neural network,IEEE,Conferences,"In the article, a new and efficient multilayer neural networks learning algorithm is presented. The key concept of this new algorithm is the two-stage implementation of the steepest descent method. At the first stage, it is used to search the optimal learning constant /spl eta/ and momentum term /spl alpha/ for each weights updating process. At the second stage, the Delta learning rule is then employed to modify the connecting weights in terms of the optimal /spl eta/ and /spl alpha/. Computer simulations show that the proposed new algorithm outmatches other learning algorithms both in convergence speed and success rate. On real industrial application, a self-tuning neural-network based PID controller for precise temperature control of an injection mode barrel system by using the developed algorithm is developed. Experiments show that the proposed self-tuning PID controller can precisely control the barrel temperature within /spl plusmn/0.5/spl deg/C.",https://ieeexplore.ieee.org/document/832669/,IJCNN'99. International Joint Conference on Neural Networks. Proceedings (Cat. No.99CH36339),10-16 July 1999,ieeexplore
10.1109/ICNP.2016.7784407,Characterizing industrial control system devices on the Internet,IEEE,Conferences,"Industrial control system (ICS) devices with IP addresses are accessible on the Internet and play a crucial role for critical infrastructures like power grid. However, there is a lack of deep understanding of these devices' characteristics in the cyberspace. In this paper, we take a first step in this direction by investigating these accessible industrial devices on the Internet. Because of critical nature of industrial control systems, the detection of online ICS devices should be done in a real-time and non-intrusive manner. Thus, we first analyze 17 industrial protocols widely used in industrial control systems, and train a probability model through the learning algorithm to improve detection accuracy. Then, we discover online ICS devices in the IPv4 space while reducing the noise of industrial honeypots. To observe the dynamics of ICS devices in a relatively long run, we have deployed our discovery system on Amazon EC2 and detected online ICS devices in the whole IPv4 space for eight times from August 2015 to March 2016. Based on the ICS device data collection, we conduct a comprehensive data analysis to characterize the usage of ICS devices, especially in the answer to the following three questions: (1) what are the distribution features of ICS devices, (2) who use these ICS devices, and (3) what are the functions of these ICS devices.",https://ieeexplore.ieee.org/document/7784407/,2016 IEEE 24th International Conference on Network Protocols (ICNP),8-11 Nov. 2016,ieeexplore
10.1109/ITIA50152.2020.9312273,Cloud Task Scheduling Based on Policy Gradient Algorithm in Heterogeneous Cloud Data Center for Energy Consumption Optimization,IEEE,Conferences,"The appearance of cloud computing has provided an efficient and economical way for users to receive computing resources with a pay-as-you-go model from cloud service provider. Cloud computing has attracted wide attention in both academia and industrial application. However, with the widely deploying of large-scale data center, high energy consumption becomes one of the key challenges and difficulties that restricting the development of cloud computing. Due to the limitations and dynamics of resources, how to map the cloud tasks to the resources aiming at decreasing the energy consumption has remained an essential issue. In this paper, we have made some improvements to the DeepJS and present DeepEnergyJS to minimize energy consumption for data center that receives enormous numbers of tasks arrive dynamically. We firstly introduce a power model to estimate the power of data center of the moment, then design a reward function that reflects the goal of optimizing power consumption and implement the simulation experiment on independent workloads to verity its availability of reducing the energy consumption. Finally, DeepJS considered only the independent workloads, we model inter-task dependencies in jobs and then conduct experiments in a similar way with independent workloads to prove that DeepEnergyJS can be extended for workloads that with dependencies which make it more functional. What's more, our experiments are based on different configured servers while the severs used in DeepJS are all the same, which make our study closer to the real cloud computing environment. The experiments results show that DeepEnergyJS can significantly outperforms three baselines in both independent workloads and workloads with dependencies.",https://ieeexplore.ieee.org/document/9312273/,2020 International Conference on Internet of Things and Intelligent Applications (ITIA),27-29 Nov. 2020,ieeexplore
10.1109/MED.2017.7984310,Cloud computing for big data analytics in the Process Control Industry,IEEE,Conferences,"The aim of this article is to present an example of a novel cloud computing infrastructure for big data analytics in the Process Control Industry. Latest innovations in the field of Process Analyzer Techniques (PAT), big data and wireless technologies have created a new environment in which almost all stages of the industrial process can be recorded and utilized, not only for safety, but also for real time optimization. Based on analysis of historical sensor data, machine learning based optimization models can be developed and deployed in real time closed control loops. However, still the local implementation of those systems requires a huge investment in hardware and software, as a direct result of the big data nature of sensors data being recorded continuously. The current technological advancements in cloud computing for big data processing, open new opportunities for the industry, while acting as an enabler for a significant reduction in costs, making the technology available to plants of all sizes. The main contribution of this article stems from the presentation for a fist time ever of a pilot cloud based architecture for the application of a data driven modeling and optimal control configuration for the field of Process Control. As it will be presented, these developments have been carried in close relationship with the process industry and pave a way for a generalized application of the cloud based approaches, towards the future of Industry 4.0.",https://ieeexplore.ieee.org/document/7984310/,2017 25th Mediterranean Conference on Control and Automation (MED),3-6 July 2017,ieeexplore
10.1109/BigData.2016.7840831,Cloud-based machine learning for predictive analytics: Tool wear prediction in milling,IEEE,Conferences,"The proliferation of real-time monitoring systems and the advent of Industrial Internet of Things (IIoT) over the past few years necessitates the development of scalable and parallel algorithms that help predict mechanical failures and remaining useful life of a manufacturing system or system components. Classical model-based prognostics require an in-depth physical understanding of the system of interest and oftentimes assume certain stochastic or random processes. To overcome the limitations of model-based methods, data-driven methods such as machine learning have been increasingly applied to prognostics and health management (PHM). While machine learning algorithms are able to build accurate predictive models, large volumes of training data are required. Consequently, machine learning techniques are not computationally efficient for data-driven PHM. The objective of this research is to create a novel approach for machinery prognostics using a cloud-based parallel machine learning algorithm. Specifically, one of the most popular machine learning algorithms (i.e., random forest) is applied to predict tool wear in dry milling operations. In addition, a parallel random forest algorithm is developed using the MapReduce framework and then implemented on the Amazon Elastic Compute Cloud. Experimental results have shown that the random forest algorithm can generate very accurate predictions. Moreover, significant speedup can be achieved by implementing the parallel random forest algorithm.",https://ieeexplore.ieee.org/document/7840831/,2016 IEEE International Conference on Big Data (Big Data),5-8 Dec. 2016,ieeexplore
10.1109/MSM49833.2020.9202398,Collaborative Robot System for Playing Chess,IEEE,Conferences,"In recent years, number of collaborative robots industrial applications has made a significant increasment. Implementation of collaborative robots is a safe and effective way for designing robot-human cooperation systems. Combined with constantly developing artificial intelligence, collaborative systems are actually able to solve complex problems that require some sort of intelligence. For humans, board games are a good example of the visualization of robot intelligence. Such systems require estimation and detection of board and pieces in manipulator workspace, some kind of decision-making algorithms and robot control system to move pieces. The flagship of such systems are chess playing robots. The chess game has a defined and easy to understand set of rules which makes it interesting example of intelligent robotics systems application. In this paper, we present an implementation of collaborative robots for chess playing system which was designed to play against human or another robot. The system is able to track state of the game via camera, calculate the optimal move using implemented decision-making algorithm, detect illegal moves and execute pick-and-place task to physically move pieces. We test the developed system in a real-world setup and provide experimental results documenting the performance of proposed approach.",https://ieeexplore.ieee.org/document/9202398/,2020 International Conference Mechatronic Systems and Materials (MSM),1-3 July 2020,ieeexplore
10.1109/ECAI46879.2019.9041923,Color-Based Sorting System for Agriculture Applications,IEEE,Conferences,"This paper presents the design and the implementation of a real-time color based sorting system that is intended to be used in agriculture applications. The control part of the system is realized with the versatile microcontroller Atmega328. The color detection of the analyzed objects is obtained with the specialized digital sensor TCS230 that contain all the circuitry that is necessary for converting the light into frequency signal for convenient acquiring and processing with the microcontroller. The actuation elements of the sorting system are realized with a set of TowerPro SG90 servomotors which have a very lightweight construction but are capable to generate relatively high output power. Being a microcontroller-based system that relies on an advanced color sensor and dedicated proprietary control software, the proposed sorting system is characterized by improved reconfigurability and adaptability. The system can be easily integrated within a very broad spectrum of applications including Internet of Things or quality control in industrial domain.",https://ieeexplore.ieee.org/document/9041923/,"2019 11th International Conference on Electronics, Computers and Artificial Intelligence (ECAI)",27-29 June 2019,ieeexplore
10.1109/DESSERT50317.2020.9125038,Combination of Digital Twin and Artificial Intelligence in Manufacturing Using Industrial IoT,IEEE,Conferences,"The paper focuses on Digital Twin (DT) in Manufacturing using Artificial Intelligence (AI) and Industrial IoT. According to the concept, the manufacturing includes three main units: equipment, personnel and processes. All data from these units are inherited to manufacture model (DT) and decision support system with the use of AI. DT data technology allows finding the required knowledge that can be interpreted and used to support the process of decision-making in the management of the enterprise. AI applications open up a broad spectrum of opportunities in manufacturing to add value by optimizing processes and generating new business models. The Landscape was described by a formal model to assure the possibility to analyze the state and development of landscape in detail considering DT and other technologies. DT and IIoT implementation for the simulation of real enterprise manufacturing were considered.",https://ieeexplore.ieee.org/document/9125038/,"2020 IEEE 11th International Conference on Dependable Systems, Services and Technologies (DESSERT)",14-18 May 2020,ieeexplore
10.1109/MetroInd4.0IoT48571.2020.9138184,Combining exposure indicators and predictive analytics for threats detection in real industrial IoT sensor networks,IEEE,Conferences,"We present a framework able to combine exposure indicators and predictive analytics using AI-tools and big data architectures for threats detection inside a real industrial IoT sensors network. The described framework, able to fill the gaps between these two worlds, provides mechanisms to internally assess and evaluate products, services and share results without disclosing any sensitive and private information. We analyze the actual state of the art and a possible future research on top of a real case scenario implemented into a technological platform being developed under the H2020 ECHO project, for sharing and evaluating cybersecurity relevant informations, increasing trust and transparency among different stakeholders.",https://ieeexplore.ieee.org/document/9138184/,2020 IEEE International Workshop on Metrology for Industry 4.0 & IoT,3-5 June 2020,ieeexplore
10.23919/CNSM46954.2019.9012664,Communication Challenges and Solutions between Heterogeneous Industrial IoT Systems,IEEE,Conferences,"Industrial automation systems require communication technologies with high availability, high security and low latency. Accordingly, the current article addresses industrial-specific communication challenges, presenting some of the relevant solutions. In order to prove the usability of the presented technologies with sound results, this paper utilises a primarily Industrial IoT(IIoT) solution, the Arrowhead Framework, to experiment with communication capabilities between different IIoT clouds. The research is limited to three technologies from the LTE UE categories: Cat 3, Cat Ml and NB-IoT. The novelty of this paper is that it provides a set of experimental studies on applying different mobile networking technologies to support IIoT applications. The studies are based on our current, real-life measurements.",https://ieeexplore.ieee.org/document/9012664/,2019 15th International Conference on Network and Service Management (CNSM),21-25 Oct. 2019,ieeexplore
10.1109/GLOBECOM42002.2020.9348249,Communication-Efficient Federated Learning for Anomaly Detection in Industrial Internet of Things,IEEE,Conferences,"With the rapid development of the Industrial Internet of Things (IIoT), various IoT devices and sensors generate massive industrial sensing data. Sensing big data can be analyzed for insights that lead to better decisions and strategic industrial production by using advanced machine learning technologies. However, vulnerable IoT devices are easy to be compromised thus causing IoT devices failures (i.e., anomalies). The anomalies seriously affect the production of industrial products, thereby, it is increasingly important to accurately and timely detect anomalies. To this end, we first introduce a Federated Learning (FL) framework to enable decentralized edge devices to collaboratively train a Deep Anomaly Detection (DAD) model, which can improve its generalization ability. Second, we propose a Convolutional Neural Network-Long Short Term Memory (CNN-LSTM) model to accurately detect anomalies. The CNN-LSTM model uses CNN units to capture fine-grained features and retains the advantages of LSTM unit in predicting time series data. Third, to achieve real-time and lightweight anomaly detection in the proposed framework, a gradient compression mechanism is applied to reduce communication costs and improve communication efficiency. Extensive experiment results based on realworld datasets demonstrate that the proposed framework and mechanism can accurately and timely detect anomalies, and also reduce about 50% communication overhead when compared with traditional schemes.",https://ieeexplore.ieee.org/document/9348249/,GLOBECOM 2020 - 2020 IEEE Global Communications Conference,7-11 Dec. 2020,ieeexplore
10.23919/ACC.1993.4793202,Comparative Analysis of Control Design Techniques for a Cart-Inverted-Pendulum in Real-Time Implementation,IEEE,Conferences,"Conventional controllers such as PID controllers have a long history of successful industrial applications. However, in recent years, many nonlinear controllers have been applied to deal with nonlinear systems. Sliding mode control has been successfully used for SISO non-linear systems and for certain multivariable systems, fuzzy-logic has been successfully applied in many practical control systems. Meanwhile, there has been interest in developing expert systems for control that involve necessary process knowledge required for good control. Neural network control has been used to determine adaptive laws for the adjustment of the control parameters. This paper will evaluate and compare PD, sliding mode, fuzzy, expert system, and neural network control methods in controlling the cart-inverted-pendulum. Performance is evaluated in terms of control surface, system response, stability, and robustness. Moreover the comparison of these controllers is validated through experimentation. Strengths and weaknesses, in the real-time control are indicated.",https://ieeexplore.ieee.org/document/4793202/,1993 American Control Conference,2-4 June 1993,ieeexplore
10.1109/ICMLA.2015.183,Comparative Evaluation of Top-N Recommenders in e-Commerce: An Industrial Perspective,IEEE,Conferences,"We experiment on two real e-commerce datasets and survey more than 30 popular e-commerce platforms to reveal what methods work best for product recommendations in industrial settings. Despite recent academic advances in the field, we observe that simple methods such as best-seller lists dominate deployed recommendation engines in e-commerce. We find our empirical findings to be well-aligned with those of the survey, where in both cases simple personalized recommenders achieve higher ranking than more advanced techniques. We also compare the traditional random evaluation protocol to our proposed chronological sampling method, which can be used for determining the optimal time-span of the training history for optimizing the performance of algorithms. This performance is also affected by a proper hyperparameter tuning, for which we propose golden section search as a fast alternative to other optimization techniques.",https://ieeexplore.ieee.org/document/7424455/,2015 IEEE 14th International Conference on Machine Learning and Applications (ICMLA),9-11 Dec. 2015,ieeexplore
10.1109/MI-STA52233.2021.9464484,Comparison of PID and Artificial Neural Network Controller in on line of Real Time Industrial Temperature Process Control System,IEEE,Conferences,"Due to its simple structure and robustness, the traditional proportional-integral-derivative (PID) controller is commonly used in the field of industrial automation and process control, but it does not function well with nonlinear systems, time-delayed linear systems and time-varying systems. A new type of PID controller based on artificial neural networks and evolutionary algorithms is presented in this paper. An powerful instrument for a highly nonlinear system is the Artificial Neural Network. The interest in the study of the nonlinear system has increased through the implementation of a high-speed computer system,. In complex systems such as robotics and process control systems, the Neuro Control Algorithm is often applied. Systems of process management is also nonlinear and hard to control consistently.. This paper presents a comprehensive analysis in Which is offline trained by a multilayered feed forward back propagation neural network to act as a process control system controller, That is to say, a temperature control device without prior knowledge of its dynamics. Via the implementation of a range of input vectors to the neural network, the inverse dynamics model is developed. Based on these input vectors, the output of the neural network It is being studied by explicitly configuring it to monitor the operation. In this paper, based on set-point adjustment, impact of disturbances in load and variable dead time, compassion between the PID controller and ANN is conducted. The outcome shows that ANN outperforms the controller of the PID.",https://ieeexplore.ieee.org/document/9464484/,2021 IEEE 1st International Maghreb Meeting of the Conference on Sciences and Techniques of Automatic Control and Computer Engineering MI-STA,25-27 May 2021,ieeexplore
10.1109/RTSS.2009.14,Component-Based Abstraction Refinement for Timed Controller Synthesis,IEEE,Conferences,"We present a novel technique for synthesizing controllers for distributed real-time environments with safety requirements. Our approach is an abstraction refinement extension to the on-the-fly algorithm by Cassez et al. from 2005. Based on partial compositions of some environment components, each refinement cycle constructs a sound abstraction that can be used to obtain under- and over-approximations of all valid controller implementations. This enables (1) early termination if an implementation does not exist in the over-approximation, or, if one does exist in the under-approximation, and (2) pruning of irrelevant moves in subsequent refinement cycles. In our refinement loop, the precision of the abstractions incrementally increases and converges to all specification-critical components. We implemented our approach in a prototype synthesis tool and evaluated it on an industrial benchmark. In comparison with the timed game solver UPPAAL-Tiga, our technique outperforms the nonincremental approach by an order of magnitude.",https://ieeexplore.ieee.org/document/5368182/,2009 30th IEEE Real-Time Systems Symposium,1-4 Dec. 2009,ieeexplore
10.1109/IECON43393.2020.9255001,Computation Offloading for Machine Learning in Industrial Environments,IEEE,Conferences,"Industrial applications, such as real-time manufacturing, fault classification and inference, autonomous cars, etc., are data-driven applications that require machine learning with a wealth of data generated from industrial Internet of Things (IoT) devices. However, conventional approaches of transmitting this rich data to a remote data center to learn may be undesired due to the non-negligible network transmission delay and the sensitiveness of data privacy. By deploying a number of computing-capable devices at the network edge, edge computing supports the implementation of machine learning close to the industrial environment. Considering the heterogeneous computing capability as well as network location of edge devices, there are two types of feasible edge computing based machine learning models, including the centralized learning and federated learning models. In centralized learning, a resource-rich edge server aggregates the data from different IoT devices and performs machine learning. In federated learning, distributed edge devices and a federated server collaborate to perform machine learning. The features that data should be offloaded in centralized learning while it is locally trained in federated learning make centralized learning and federated learning quite different. We study the computation offloading problem for edge computing based machine learning in an industrial environment, considering the abovementioned machine learning models. We formulate a machine learning-based offloading problem with the goal of minimizing the training delay. Then, an energy-constrained delay-greedy (ECDG) algorithm is designed to solve the problem. Finally, simulation studies based on the MNIST dataset have been conducted to illustrate the efficiency of the proposal.",https://ieeexplore.ieee.org/document/9255001/,IECON 2020 The 46th Annual Conference of the IEEE Industrial Electronics Society,18-21 Oct. 2020,ieeexplore
10.1109/ICIT.2002.1189341,Computer based robot training in a virtual environment,IEEE,Conferences,"As more market segments are welcoming automation, the robotic field continues to expand. With the accepted breadth of viable industrial robotic applications increasing, the need for flexible robotic training also grows. In the area of simulation and offline programming there have been innovative developments to Computer Aided Robotics (CAR) Systems. New and notable releases have been introduced to the public, especially among the small, affordable, and easy to use systems. These CAR-Systems are mainly aimed at system integrators in general industry business fields to whom the complex, powerful software tools used by the automotive industry (and its suppliers) are oversized. In general, CAR-Systems are used to design robot cells and to create the offline programs necessary to reduce start-up time and to achieve a considerable degree of planning reliability. Another potential yet to be fully considered, is the use of such CAR-Systems as an inexpensive and user-friendly tool for robotics training. This paper will show the educational potential and possibility inherent in simulation and introduce a successful example of this new method of training. Finally, this presentation should be seen as an attempt to outline novel methods for future education in an industrial environment characterized by the increased occurrence and implementation of the virtual factory.",https://ieeexplore.ieee.org/document/1189341/,"2002 IEEE International Conference on Industrial Technology, 2002. IEEE ICIT '02.",11-14 Dec. 2002,ieeexplore
10.1109/PCCC.1990.101701,Concept for combining features of real-time and expert systems for on-line plant diagnosis,IEEE,Conferences,"A description is presented of an AI implementation in an industrial power plant: malfunction diagnosis based on the evaluation of plant performance. It is intended to support the development and maintenance of a knowledge-based real-time control system. The features of a programming tool which combines characteristics of expert systems and real-time supervision systems are explained. The system is used for online diagnosis of a plant. The real-time properties are achieved by dividing data processing into several processing units which are computed in parallel. Since plant diagnosis differs from most expert system applications, the expert system part has been developed with regard to this. A prototype of this system has been implemented for the domain of thermal spraying.&lt;<ETX>&gt;</ETX>",https://ieeexplore.ieee.org/document/101701/,Ninth Annual International Phoenix Conference on Computers and Communications. 1990 Conference Proceedings,21-23 March 1990,ieeexplore
10.1109/AITest.2019.00015,Constraint-Based Testing of An Industrial Multi-Robot Navigation System,IEEE,Conferences,"Intelligent multi-robot systems get more and more deployed in industrial settings to solve complex and repetitive tasks. Due to safety and economic reasons they need to operate dependably. To ensure a high degree of dependability, testing the deployed system has to be done in a rigorous way. Advanced multi-robot systems show a rich set of complex behaviors. Thus, these systems are difficult to test manually. Moreover, the space of potential environments and tasks for such systems is enormous. Therefore, methods that are able to explore this space in a structured way are needed. One way to address these issues is through model-based testing. In this paper we present an approach for testing the navigation system of a fleet of industrial transport robots. We show how all potential environments and navigation behaviors as well as requirements and restrictions can be represented in a formal constraint-based model. Moreover, we present the concept of coverage criteria in order to handle the potentially infinite space of test cases. Finally, we show how test cases can be derived from this model in an efficient way. In order to show the feasibility of the proposed approach we present an empirical evaluation of a prototype implementation using a real industrial use case.",https://ieeexplore.ieee.org/document/8718216/,2019 IEEE International Conference On Artificial Intelligence Testing (AITest),4-9 April 2019,ieeexplore
10.1109/ISNCC49221.2020.9297198,Construction Safety Surveillance Using Machine Learning,IEEE,Conferences,"Safety has always been a matter of concern in all industrial activities, especially construction. Hard hats or safety helmets act as the first line of protection against serious head injuries. Some workers don't quite follow the instructions and signs that require them to follow safety measures. And in case of any accidents, the workers will not be eligible for insurance, if they were not wearing the safety equipments. This applied research paper mainly focuses to detect persons with and without a helmet in the construction site. The accuracy and performance of Neural Networks were tested and compared with other hand crafted features like Haar and LBP classifiers, Histogram of Oriented Gradients and Sequential Classifiers. These hand crafted features gave more false detections than neural nets, when tested in real conditions. Different Neural Networks were tested on Edge Devices such as Nvidia Jetson TX2 and Jetson Nano, for commercial deployment. Compared to the other neural networks, the SSD MobileNet model showed better performance without considerable drop in accuracy, when tested on edge devices in real-time. This makes it a preferable solution for this application-oriented problem.",https://ieeexplore.ieee.org/document/9297198/,"2020 International Symposium on Networks, Computers and Communications (ISNCC)",20-22 Oct. 2020,ieeexplore
10.1109/IROS40897.2019.8967523,Contact Skill Imitation Learning for Robot-Independent Assembly Programming,IEEE,Conferences,"Robotic automation is a key driver for the advancement of technology. The skills of human workers, however, are difficult to program and seem currently unmatched by technical systems. In this work we present a data-driven approach to extract and learn robot-independent contact skills from human demonstrations in simulation environments, using a Long Short Term Memory (LSTM) network. Our model learns to generate error-correcting sequences of forces and torques in task space from object-relative motion, which industrial robots carry out through a Cartesian force control scheme on the real setup. This scheme uses forward dynamics computation of a virtually conditioned twin of the manipulator to solve the inverse kinematics problem. We evaluate our methods with an assembly experiment, in which our algorithm handles part tilting and jamming in order to succeed. The results show that the skill is robust towards localization uncertainty in task space and across different joint configurations of the robot. With our approach, non-experts can easily program force-sensitive assembly tasks in a robot-independent way.",https://ieeexplore.ieee.org/document/8967523/,2019 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),3-8 Nov. 2019,ieeexplore
10.1109/PHM-Paris.2019.00061,Convolutional Neural Network Based Rolling-Element Bearing Fault Diagnosis for Naturally Occurring and Progressing Defects Using Time-Frequency Domain Features,IEEE,Conferences,"Convolutional Neural Networks (CNN) are becoming increasingly popular for bearing fault diagnosis due to their ability to automatically capture the sensitive fault information without the need for expert knowledge. Most of these applications are developed considering vibration data from artificially induced faults. However, bearing failure in real-life can show huge damage variations even within a single category of failure which artificially induced failures are unable to represent. Thus, in this paper, the performance of classical CNN is evaluated on bearings with naturally occurring and progressing defects from the Paderborn University Dataset. A three-class (Healthy, Inner Race Fault and Outer Race Fault) classification problem is solved considering five bearing conditions within each class. These conditions vary in terms of bearing operating hours, damage mode, damage repetition pattern, the extent of damage, etc. The classification accuracy is evaluated under two cases: (1) at least a portion of data from each bearing condition from all classes is used in training; (2) data from all available conditions are considered for training except from one condition which is used explicitly for testing. Within each case, the effect of changing the domain of the input data is evaluated on the achieved accuracy. Three input signals based on vibration data (raw time domain signal, envelope spectrum, and spectrogram) were explored for their representation effectiveness. The proposed CNN with a spectrogram of the vibration signal as input achieves better results than similar architectures. Finally, the potential challenges that come along with the implementation of Deep Learning technologies for industrial applications are discussed and future research directions are proposed.",https://ieeexplore.ieee.org/document/8756423/,2019 Prognostics and System Health Management Conference (PHM-Paris),2-5 May 2019,ieeexplore
10.1109/EAEEIE.2013.6576535,Cooperation between industry and university based on the evaluation of the industrial research results in the academic environment,IEEE,Conferences,"Based on the European Structural Funds it was developed the Intelligent Mobile Box, Intelligent Panel Controller with intelligent adaptive controllers within the industrial research and experimental development in the company Kybernetes, s.r.o. Within the frame of the academic-industry cooperation, the intelligent adaptive controller was tested at the Department of Cybernetics and Artificial Intelligence, Technical university of Kosice, Slovakia. The tests of the mobile intelligent adaptive controller were performed on two levels of university study, on the Bachelor level on the exercises from the subject “Control of Technological Processes” and on the Engineering level the exercises from the subject “Intelligent Control Networks” and on one Diploma project. Goals of students of the Control of the technological processes course had two goals, firstly to connect the intelligent adaptive controller to pre-defined controlled system (real plant, real model or simulated model) and next to validate the control results. Students of the Diploma project on the Engineering level had more advanced goals. Tasks defined for engineering students were to connect the intelligent adaptive controller to non-defined controlled system, setup the adaptivity process of the controller regarding the learning error, parameterize the control system, observe and validate the control results. Both sides concluded this cooperation as very valuable. Main contributions for students were (U1) the challenge to apply studied theoretical knowledge on the real industrial controllers, (U2) experience with new research results and technologies deployed in industry and (U3) the implementation of the control and adaptive algorithms from abstract mathematical area to real PLC controller. On side of industry research company the main contributions were (C1) testing of designed algorithms and (C2) user feedback from students to make the application HMI interface more understandable a native.",https://ieeexplore.ieee.org/document/6576535/,2013 24th EAEEIE Annual Conference (EAEEIE 2013),30-31 May 2013,ieeexplore
10.1109/WAIN52551.2021.00009,Corner Case Data Description and Detection,IEEE,Conferences,"As the major factors affecting the safety of deep learning models, corner cases and related detection are crucial in AI quality assurance for constructing safety- and security-critical systems. The generic corner case researches involve two interesting topics. One is to enhance DL models' robustness to corner case data via the adjustment on parameters/structure. The other is to generate new corner cases for model retraining and improvement. However, the complex architecture and the huge amount of parameters make the robust adjustment of DL models not easy, meanwhile it is not possible to generate all real-world corner cases for DL training. Therefore, this paper proposes a simple and novel approach aiming at corner case data detection via a specific metric. This metric is developed on surprise adequacy (SA) which has advantages on capture data behaviors. Furthermore, targeting at characteristics of corner case data, three modifications on distanced-based SA are developed for classification applications in this paper. Consequently, through the experiment analysis on MNIST data and industrial data, the feasibility and usefulness of the proposed method on corner case data detection are verified.",https://ieeexplore.ieee.org/document/9474385/,2021 IEEE/ACM 1st Workshop on AI Engineering - Software Engineering for AI (WAIN),30-31 May 2021,ieeexplore
10.1109/BigData.2018.8622004,Correlated Anomaly Detection from Large Streaming Data,IEEE,Conferences,"Correlated anomaly detection (CAD) from streaming data is a type of group anomaly detection and an essential task in useful real-time data mining applications like botnet detection, financial event detection, industrial process monitor, etc. The primary approach for this type of detection in previous researches is based on principal score (PS) of divided batches or sliding windows by computing top eigenvalues of the correlation matrix, e.g. the Lanczos algorithm. However, this paper brings up the phenomenon of principal score degeneration for large data set, and then mathematically and practically prove current PS-based methods are likely to fail for CAD on large-scale streaming data even if the number of correlated anomalies grows with the data size at a reasonable rate; in reality, anomalies tend to be the minority of the data, and this issue can be more serious. We propose a framework with two novel randomized algorithms rPS and gPS for better detection of correlated anomalies from large streaming data of various correlation strength. The experiment shows high and balanced recall and estimated accuracy of our framework for anomaly detection from a large server log data set and a U.S. stock daily price data set in comparison to direct principal score evaluation and some other recent group anomaly detection algorithms. Moreover, our techniques significantly improve the computation efficiency and scalability for principal score calculation.",https://ieeexplore.ieee.org/document/8622004/,2018 IEEE International Conference on Big Data (Big Data),10-13 Dec. 2018,ieeexplore
10.1109/COINS51742.2021.9524088,Cyber Security Risks of Technical Components in Industry 4.0,IEEE,Conferences,"Industry 4.0 is a new concept of automation data exchange in manufacturing. Industry 4.0 consists of various technical components such as Artificial Intelligence (AI), IoT, OPC UA, and cloud. There could be cyber risks when these technical components are used in Industrial Control System (ICS) without security consideration. Therefore cyber risk assessment, reconsideration of implementation, operation are necessary to adopt Industry 4.0. Problems of previous research for ICS risk assessments are some of them are theoretical and conceptual methods such as simulations. Moreover, they do not focus on Industry 4.0 components.The objective of this research is to prove the concrete cyber risks and introduces corresponding secure implementations for Industry 4.0 components such as Artificial Intelligence (AI), the Internet of Things (IoT), OPC Unified Architecture (OPC UA). The proposed method has benefits for clarifying impacts on the real world by cyber-attacks through penetration tests against ICS testbed with actual machines.",https://ieeexplore.ieee.org/document/9524088/,2021 IEEE International Conference on Omni-Layer Intelligent Systems (COINS),23-25 Aug. 2021,ieeexplore
10.1109/GUCON50781.2021.9573994,Cyber Warfare Threat Categorization on CPS by Dark Web Terrorist,IEEE,Conferences,"The Industrial Internet of Things (IIoT) also referred as Cyber Physical Systems (CPS) as critical elements, expected to play a key role in Industry 4.0 and always been vulnerable to cyber-attacks and vulnerabilities. Terrorists use cyber vulnerability as weapons for mass destruction. The dark web's strong transparency and hard-to-track systems offer a safe haven for criminal activity. On the dark web (DW), there is a wide variety of illicit material that is posted regularly. For supervised training, large-scale web pages are used in traditional DW categorization. However, new study is being hampered by the impossibility of gathering sufficiently illicit DW material and the time spent manually tagging web pages. We suggest a system for accurately classifying criminal activity on the DW in this article. Rather than depending on the vast DW training package, we used authorized regulatory to various types of illicit activity for training Machine Learning (ML) classifiers and get appreciable categorization results. Espionage, Sabotage, Electrical power grid, Propaganda and Economic disruption are the cyber warfare motivations and We choose appropriate data from the open source links for supervised Learning and run a categorization experiment on the illicit material obtained from the actual DW. The results shows that in the experimental setting, using TF-IDF function extraction and a AdaBoost classifier, we were able to achieve an accuracy of 0.942. Our method enables the researchers and System authoritarian agency to verify if their DW corpus includes such illicit activity depending on the applicable rules of the illicit categories they are interested in, allowing them to identify and track possible illicit websites in real time. Because broad training set and expert-supplied seed keywords are not required, this categorization approach offers another option for defining illicit activities on the DW.",https://ieeexplore.ieee.org/document/9573994/,"2021 IEEE 4th International Conference on Computing, Power and Communication Technologies (GUCON)",24-26 Sept. 2021,ieeexplore
10.1109/BigData47090.2019.9006096,DSSLP: A Distributed Framework for Semi-supervised Link Prediction,IEEE,Conferences,"Link prediction is widely used in a variety of industrial applications, such as merchant recommendation, fraudulent transaction detection, and so on. However, it's a great challenge to train and deploy a link prediction model on industrial-scale graphs with billions of nodes and edges. In this work, we present a scalable and distributed framework for semi-supervised link prediction problem (named DSSLP), which is able to handle industrial-scale graphs. Instead of training model on the whole graph, DSSLP is proposed to train on the k-hops neighborhood of nodes in a mini-batch setting, which helps reduce the scale of the input graph and distribute the training procedure. In order to generate negative examples effectively, DSSLP contains a distributed batched runtime sampling module. It implements uniform and dynamic sampling approaches, and is able to adaptively construct positive and negative examples to guide the training process. Moreover, DSSLP proposes a model-split strategy to accelerate the speed of inference process of the link prediction task. Experimental results demonstrate that the effectiveness and efficiency of DSSLP in serval public datasets as well as real-world datasets of industrial-scale graphs.",https://ieeexplore.ieee.org/document/9006096/,2019 IEEE International Conference on Big Data (Big Data),9-12 Dec. 2019,ieeexplore
10.1109/ICIAI.2019.8850773,Data Augmentation for Intelligent Manufacturing with Generative Adversarial Framework,IEEE,Conferences,"The global economy is greatly shaped by the unprecedented booming of ICT and artificial intelligence technologies. Their applications in manufacturing has led to the advent of intelligent manufacturing and industry 4.0. Data has become a precious asset for modern industry. This paper first introduces an energy monitoring and data acquisition system namely the Point Energy Technology, which has been developed by the team and installed in several industrial partners, including a local bakery. The lack of data always exists due to various reasons, such as measurement or transmission errors at data collection and transmission stage, leading to the loss of varied length of data samples that are key for process monitoring and control. To solve this problem, we introduce a generative adversarial framework which is based on a game theory for data augmentation. This framework consists of two multilayer perceptron networks, namely generator and discriminator. An improved framework with Q-net that extracts the latent variables from real data is also proposed, in which the Q-net shares the structure with discriminator except for the last layer. In addition, the two optimization methods, namely mini-batch gradient descent and adaptive moment estimation are adopted to tune the parameters. To evaluate the performance of these algorithms, energy consumption data collected from a bakery process is used in the experiment. The experimental results confirm that the latent generative adversarial framework with adaptive moment estimation could generate good quality data samples to compensate the random loss of samples in time series data.",https://ieeexplore.ieee.org/document/8850773/,2019 1st International Conference on Industrial Artificial Intelligence (IAI),23-27 July 2019,ieeexplore
10.1109/ICDMW51313.2020.00106,Data analysis and processing for spatio-temporal forecasting,IEEE,Conferences,"Spatio-temporal forecasting is a research area applicable to many industrial fields, such as forecasting power consumption in real-life and predicting traffic conditions of roads. For example, in the traffic forecasting, it is important to analyze spatial relations and temporal trends in order to predict traffic changes in roads over time. In the spatio-temporal forecasting task, previous studies applied graph modeling to capture spatial relations. However, existing models use only the recently available data to predict traffic conditions, leading to the degraded performance of the model. Further research is necessary for predicting the speed in the far future. As a study to tackle this issue, we aim to improve the performance of the model by providing the model with additional data through time-series segmentation. In order to verify whether the additional data could be meaningful to the model, an experiment was conducted to compare the performance of the model trained with existing data and the model trained with our data and analyze the distribution of the additional data.",https://ieeexplore.ieee.org/document/9346316/,2020 International Conference on Data Mining Workshops (ICDMW),17-20 Nov. 2020,ieeexplore
10.1109/ETFA46521.2020.9211930,Data-Driven Industrial Human-Machine Interface Temporal Adaptation for Process Optimization,IEEE,Conferences,"The application of Artificial Intelligence (AI) into Industrial Human-Machine Interfaces (HMIs) moved old systems with physical buttons and analogue actuators into adaptive interaction models and context-based self adjusted interfaces. To date, little attention has been paid to industrial Human-Machine Interfaces (HMI) which play a vital role in the communication between operator and complex productive systems. Current industrial HMIs do not take into account operator behaviour, but rather focus on the production process. To enhance User Experience (UX) and improve performance it is necessary to adapt the interface to the needs of the operator. This paper proposes a Machine Learning (ML) based operator interaction Data-Driven methodology to extract a set of interface adaptation rules. The methodology optimizes the interaction by reducing the number of actions and hence the amount of time and possible errors in repetitive monitoring and control tasks. An experiment with real operators was conducted to validate the proposed approach. The system was able to extract their interaction patterns and propose temporal interface adaptations, leading to a personalized, adaptive and more effective interaction.",https://ieeexplore.ieee.org/document/9211930/,2020 25th IEEE International Conference on Emerging Technologies and Factory Automation (ETFA),8-11 Sept. 2020,ieeexplore
10.1109/ICTC49870.2020.9289505,Data-driven IoT-based Water Quality Monitoring and Potability Classification System in Rural Areas,IEEE,Conferences,"Access to a safe and sustainable water source is a major problem especially in rural areas of developing countries. Water monitoring in different water resources has been practiced to ensure safe drinking water. However, manual monitoring of safe drinking water is known to be inconvenient since it requires high operational and transportation costs, and time-consuming. This work develops a water quality monitoring and potability classification system utilizing an Internet of Things framework. Portable sensor nodes capable of collecting physicochemical properties of water are deployed in different water sources from rural household areas. Data collected by nodes are being sent out wirelessly to a base station in real-time. The base station performs potability classification using ensemble learning. In addition, the base station sends the result of classification to households using 2G/3G communication. Also, the predicted output and the actual sensor data are being sent to a cloud server for remote monitoring via a web interface. Results show that the system achieves a 93.33% match with conventional industrial water laboratory tests. Moreover, the system is able to communicate the water potability status to households with minimal delay.",https://ieeexplore.ieee.org/document/9289505/,2020 International Conference on Information and Communication Technology Convergence (ICTC),21-23 Oct. 2020,ieeexplore
10.1109/INDIN41052.2019.8972310,Data-driven modeling of semi-batch manufacturing: a rubber compounding test case,IEEE,Conferences,"The continuously growing amount of available data from manufacturing processes supports the development of data-driven models. The typical target application of these models is optimal control and continuous quality management within an objective of zero-defect manufacturing. However, data obtained from batch processes are characterized by its high dimensionality that exceeds the computational capabilities of online applications and data-driven model's reliability must be guaranteed for proper industrial implementation. We explore two approaches to reduce problem's size: feature extraction and feature selection; several multivariate regression methods are also compared regarding it precision and robustness. We base our analysis on an industrial rubber compounding process where natural rubber is blended in a semi-batch mixer with several additives, then it is further mixed up using cylinders and it is conditioned in bands for storing. For this process, real production data is collected and stored in the manufacturing execution system of the company. The objective of the analysis is to predict mechanical properties of the rubber at the end of the processes. Based on the provided data, several data-driven models are built and tested. From the comparison among them it is concluded: models based on feature extraction and artificial neural networks yield the highest accuracy, while feature-selected models provide better physical interpretability and increased robustness regarding industrial deployment.",https://ieeexplore.ieee.org/document/8972310/,2019 IEEE 17th International Conference on Industrial Informatics (INDIN),22-25 July 2019,ieeexplore
10.1109/AERO.2018.8396547,Data-driven quality prognostics for automated riveting processes,IEEE,Conferences,"Technologies based in robotics and automatics are reshaping the aerospace industry. Aircraft manufacturers and top-tier suppliers now rely on robotics to perform most of its operational tasks. Over the years, a succession of implemented mobile robots has been developed with the mission of automating important industrial processes such as welding, material handling or assembly procedures. However, despite the progress achieved, a major limitation is that the process still requires human supervision and an extensive quality control process. An approach to address this limitation is to integrate machine learning methods within the quality control process. The idea is to develop algorithms that can direct manufacturing experts towards critical areas requiring human supervision and quality control. In this paper we present an application of machine learning to a concrete industrial problem involving the quality control of a riveting machine. The proposal consists of an intelligent predictive model that can be integrated within the existing real time sensing and pre-processing sub-systems at the equipment level. The framework makes use of several data-driven techniques for pre-processing and feature engineering, combined with the most accurate algorithms, validated through k-folds cross validation technique which also estimates prediction errors. The model is able to classify the manufacturing process of the machine as nominal or anomalous according to a real-world data set of design requirements and operational data. Several machine learning algorithms are compared such as linear regression, nearest neighbor, support vector machines, decision trees, random forests and extreme gradient boost. Results obtained from the case study suggest that the proposed model produces accurate predictions which meet industrial standards.",https://ieeexplore.ieee.org/document/8396547/,2018 IEEE Aerospace Conference,3-10 March 2018,ieeexplore
10.1109/AITest.2019.00018,Datamorphic Testing: A Method for Testing Intelligent Applications,IEEE,Conferences,"Adequate testing of AI applications is essential to ensure their quality. However, it is often prohibitively difficult to generate realistic test cases or to check software correctness. This paper proposes a new method called datamorphic testing, which consists of three components: a set of seed test cases, a set of datamorphisms for transforming test cases, and a set of metamorphisms for checking test results. With an example of face recognition application, the paper demonstrates how to develop datamorphic test frameworks, and illustrates how to perform testing in various strategies, and validates the approach using an experiment with four real industrial applications of face recognition.",https://ieeexplore.ieee.org/document/8718220/,2019 IEEE International Conference On Artificial Intelligence Testing (AITest),4-9 April 2019,ieeexplore
10.1109/ICSTCC.2019.8885434,Data–driven Neural Feedforward Controller Design for Industrial Linear Motors,IEEE,Conferences,"In this paper we consider the problem of feedforward controller design for industrial linear motors. These motors are safety-critical high-precision mechatronics systems that pose stringent requirements on the feedforward design: safe and predictable behavior for the desired motion profiles, tracking performance within the 10μm range in the presence of nonlinear friction and real-time implementation within the 1ms range. We investigate and compare several possibilities to design data- driven feedforward controllers using neural networks (NN) and we show that a two-step inverse estimation method is the most suitable approach, due to robustness to noisy data. We also show that basic knowledge about the system dynamics and the friction behavior can be exploited to design neural feedforward controllers with a simple structure, suitable for real-time implementation in industrial linear motors. The developed data-driven neural feedforward controllers are tested and compared with standard mass-acceleration feedforward and iterative learning controllers in realistic simulations.",https://ieeexplore.ieee.org/document/8885434/,"2019 23rd International Conference on System Theory, Control and Computing (ICSTCC)",9-11 Oct. 2019,ieeexplore
10.1109/IMCEC46724.2019.8984019,Deep Learning: Excellent Method at Surface Defect Detection of Industrial Products,IEEE,Conferences,"Surface defect detection of industrial products has always been an important part of the manufacturing industry. At present,there is a high false detection rate and low efficiency problem of traditional image processing algorithms which easy to be disturbed by complex background. Aiming at the above problems, a method for surface defect detection based on deep learning is proposed. YOLOv3 network adopted in this paper has great advantages in small target recognition and location of target in complex background. In addition, the train-set is effectively extended by elastic deformation and thin-plate spline algorithm. The experiment results show that the scratch recognition rate is as high as 95.8%, the over-judgment rate is 5.4%,and the missed rate is 1.3%.The method can identify the surface defects in a short time, and the average detection time does not exceed 0.4s, which can meet the real-time and precision requirements of industrial applications.",https://ieeexplore.ieee.org/document/8984019/,"2019 IEEE 3rd Advanced Information Management, Communicates, Electronic and Automation Control Conference (IMCEC)",11-13 Oct. 2019,ieeexplore
10.1109/CBMS.2019.00040,Deep-Learning and HPC to Boost Biomedical Applications for Health (DeepHealth),IEEE,Conferences,"This document introduces the DeepHealth project: ""Deep-Learning and HPC to Boost Biomedical Applications for Health"". This project is funded by the European Commission under the H2020 framework program and aims to reduce the gap between the availability of mature enough AI-solutions and their deployment in real scenarios. Several existing software platforms provided by industrial partners will integrate state-of-the-art machine-learning algorithms and will be used for giving support to doctors in diagnosis, increasing their capabilities and efficiency. The DeepHealth consortium is composed by 21 partners from 9 European countries including hospitals, universities, large industry and SMEs.",https://ieeexplore.ieee.org/document/8787438/,2019 IEEE 32nd International Symposium on Computer-Based Medical Systems (CBMS),5-7 June 2019,ieeexplore
10.1109/ISIE45063.2020.9152441,Deployment of a Smart and Predictive Maintenance System in an Industrial Case Study,IEEE,Conferences,"Industrial manufacturing environments are often characterized as being stochastic, dynamic and chaotic, being crucial the implementation of proper maintenance strategies to ensure the production efficiency, since the machines' breakdown leads to a degradation of the system performance, causing the loss of productivity and business opportunities. In this context, the use of emergent ICT technologies, such as Internet of Things (IoT), machine learning and augmented reality, allows to develop smart and predictive maintenance systems, contributing for the reduction of unplanned machines' downtime by predicting possible failures and recovering faster when they occur. This paper describes the deployment of a smart and predictive maintenance system in an industrial case study, that considers IoT and machine learning technologies to support the online and real-time data collection and analysis for the earlier detection of machine failures, allowing the visualization, monitoring and schedule of maintenance interventions to mitigate the occurrence of such failures. The deployed system also integrates machine learning and augmented reality technologies to support the technicians during the execution of maintenance interventions.",https://ieeexplore.ieee.org/document/9152441/,2020 IEEE 29th International Symposium on Industrial Electronics (ISIE),17-19 June 2020,ieeexplore
10.1109/IPDPSW.2012.178,Deriving a Methodology for Code Deployment on Multi-Core Platforms via Iterative Manual Optimizations,IEEE,Conferences,"In recent years, there has been what can only be described as an explosion in the types of processing devices one can expect to find within a given computer system. These include the multi-core CPU, the General Purpose Graphics Processing Unit (GPGPU) and the Accelerated Processing Unit (APU), to name but a few. The widespread uptake of these systems presents would-be users with at least two problems. Firstly, each device exposes a complex underlying architecture which must be appreciated in order to attain optimal performance. This is coupled with the fact that a single system can support an arbitrary number of such devices. Consequently, fully leveraging the performance capabilities of such a system must come at a cost -- increasingly prolonged development times. Adhering to a methodology will have the significant industrial impact of reducing these development times. This paper describes the continued formulation of such a novel methodology. Two real world scientific programs are optimized for execution on the CUDA platform. Double precision accuracy and optimized speedups (which include PCI-E transfer times) of 15x and 17x are achieved.",https://ieeexplore.ieee.org/document/6270808/,2012 IEEE 26th International Parallel and Distributed Processing Symposium Workshops & PhD Forum,21-25 May 2012,ieeexplore
10.1109/iSAI-NLP48611.2019.9045302,Design and Development of a Low-Cost Indigenous Solar Powered 4-DOF Robotic Manipulator on an Unmanned Ground Vehicle,IEEE,Conferences,"We present in this paper the design, control and implementation of a versatile low cost manipulator with an arm gripper configured on an existing unmanned ground vehicle (UGV) for lifting payload (PL) and performing real world tasks. The major development in this work is the robust and efficient stable customized design of manipulator having four degrees of freedom (DOF) capable of lifting up to 1.5 kg weight for various industrial and non-industrial applications. The communication link is established using two human supervisory controlled wireless four channel 2. 4GHz remote controllers, which are separately used for UGV and manipulator for effective maneuvering and control of a 6-DOF system (UGV and Manipulator) overall. The controlling of RC servo motors is made using Arduino Uno controller board. An on board solar panel is used for charging batteries run time during the day. A50W, 1SV standard solar panel is used to enhance the maneuvering time of UGV and manipulator. The unique feature of the selected UGV is its two rotating head on flippers capable of controlled maneuvering especially in uneven terrain surfaces, stair climbing etc. Trial run experiments have shown the developed system is capable to perform future tasks in human unapproachable situations like contaminated or hazardous areas in several industrial and military applications.",https://ieeexplore.ieee.org/document/9045302/,2019 14th International Joint Symposium on Artificial Intelligence and Natural Language Processing (iSAI-NLP),30 Oct.-1 Nov. 2019,ieeexplore
10.1109/ICIAI.2019.8850785,Design and Implementation of Intelligent Building Control System Based on Real-time Database,IEEE,Conferences,"In this paper, we propose an intelligent building control system based on BrowserIServer(B/S) architecture that allows intelligent control of construction equipment and integrate multiple subsystems. The system is compatible with three communication protocols and intellectualized control is realized by logic control based on Event-Condition-Action(ECA) rules. The system adopts Agilor distributed real-time database system to ensure high real-time performance. The design of distributed system ensures the stability of system by ensuring that the service failure in a single area of system does not affect normal operation of other areas. The system provides six kinds of interfaces, which makes system have strong integration. The research of system architecture provides a general model in the field of intelligent building for enabling communication and computing infrastructure for industrial Artificial Intelligence(AI).",https://ieeexplore.ieee.org/document/8850785/,2019 1st International Conference on Industrial Artificial Intelligence (IAI),23-27 July 2019,ieeexplore
10.1109/INCOS45849.2019.8951312,Design and Implementation of Non-Intrusive Load Monitoring using Machine Learning Algorithm for Appliance Monitoring,IEEE,Conferences,"Energy Conservation and management is gaining popularity in research area due to the increase in energy demand. It also plays a vital role in industrial/commercial/domestic/power sectors for reducing the carbon emission, the energy bill. Thus, increase in conservation would change the economy of the world energy crisis. To conserve energy, it is very important to have a better monitoring and identification system. The existing monitoring technique has conventional energy meter or smart energy meter that gives total energy consumption only. To enhance a better quality of services in existing monitoring technique, there is a need to monitor energy consumption of individual appliances and hence one meter/sensor for each appliance are necessary. Due to more sensors and its associated installation cost, this technique is not a cost effective in nature. To overcome Non-intrusive Load monitoring technique was introduced to disaggregate the total energy consumption from a single meter using Machine learning disaggregation algorithm. Thus, to identify the appliances malfunctioning Non-intrusive load monitoring (NILM) technique can be used as a Real time Monitoring technique. In this paper, it is proposed to use single energy meter for the set of appliances to monitor the status of the individual appliances. Non-intrusive Load Monitoring technique using machine learning algorithms has been discussed for appliances identification and monitoring for energy conservation. The MATLAB/Simulink Software has been used for designing and mathematical modeling of each appliance. The NILM technique mainly involves the three stages via; Data acquisition, feature extraction and training of data under different classification algorithm for appliance identification. Data acquisition used for acquiring the voltage and current from a single phase system. Using the features extracted like active, reactive power the different load patterns of individual appliances can be studied. Training of data under DT and K-NN which are supervised learning techniques are used as disaggregation algorithm. Moreover, the algorithms are compared using the Confusion matrix and ROC curve for the prediction of accuracy. The result shows that the K-NN algorithm is having a better accuracy of performance compared with DT algorithm.",https://ieeexplore.ieee.org/document/8951312/,"2019 IEEE International Conference on Intelligent Techniques in Control, Optimization and Signal Processing (INCOS)",11-13 April 2019,ieeexplore
10.1109/ICETET.2009.71,Design and Implementation of Real Time Neurofuzzy Based pH Controller,IEEE,Conferences,"The quality of an intelligent control system is the ability to control the process with a certain degree of autonomy. These requirements, as an autonomous process controller are ever increasing. Considering the fact that existing algorithm based controllers such as the adaptive PID have their own limitations/are inadequate, the controllers are designed to emulate human mental faculties such as adaptation, learning, and planning under uncertainties and also coping up with large amounts of data by reducing the complexity of their representation. The family of intelligent controllers includes those based on neural nets, fuzzy logic, classic artificial intelligence and the genetic algorithms. This paper describes design and development of real time neurofuzzy based pH controller, which can be used in water treatment processes, laboratory studies and other industrial applications.",https://ieeexplore.ieee.org/document/5395042/,2009 Second International Conference on Emerging Trends in Engineering & Technology,16-18 Dec. 2009,ieeexplore
10.1109/ICSPIS.2018.8700563,Design and Implementation of a Parcel Sorter Using Deep Learning,IEEE,Conferences,"Automation in industrial environment reduces the cost of the operation while increasing the overall performance. Having an automation mechanism in the e-commerce warehouses to sort the parcels based on their destinations or shipping method will reduce the parcel processing time significantly. To automate parcel processing in Digikala's warehouse, a parcel sorter system is designed and implemented. In this system shipment method of the parcel is indicated by a set of markers. A computer vision system is developed to identify these markers using deep learning algorithms. The parcels are identified while they are moving on the conveyor belt in a relatively high speed (1 m/s). The computer vision system is capable of processing 1.3MP pictures in real-time with a rate of 100FPS. To sort the parcels an omni wheel roller mechanism is designed and utilized. To achieve the best results in a practical environment, a gap optimization mechanism and pack positioning conveyor are implemented and placed before the sorter. This system is successfully installed in the Digikala's warehouse.",https://ieeexplore.ieee.org/document/8700563/,2018 4th Iranian Conference on Signal Processing and Intelligent Systems (ICSPIS),25-27 Dec. 2018,ieeexplore
10.1109/AIEA53260.2021.00021,Design and Implementation of the Prototype for Hybrid Production of Multi-Type Products,IEEE,Conferences,"A smart manufacturing prototype called iCandy Box, used for hybrid packing of assorted candies, was designed to study and verify the cyber-physical control methods. The prototype is aimed to provide personalized consumption, as it can perform flexible and customized production. The prototype is powered by a cloud-edge-end enabled collaborative information framework, which can support both industrial big data and artificial intelligence applications. Furthermore, it is characterized by modularization and interdisciplinarity; therefore, it can be used to carry out both experiments and training in several major fields, including smart manufacturing and IoT. The experimental results have shown that the prototype can carry out hybrid production, paving the way for the study and verification of cyber-physical control methods.",https://ieeexplore.ieee.org/document/9525542/,2021 International Conference on Artificial Intelligence and Electromechanical Automation (AIEA),14-16 May 2021,ieeexplore
10.1109/CDC.2001.980681,Design and implementation of industrial neural network controller using backstepping,IEEE,Conferences,"A novel neural network (NN) backstepping controller is modified for application to an industrial motor drive system. A control system structure and NN tuning algorithms are presented that are shown to guarantee the stability and performance of the closed-loop system. The NN backstepping controller is implemented on an actual motor drive system using a two-PC control system developed at the authors' university. The implementation results show that the NN backstepping controller is highly effective in controlling the industrial motor drive system. It is also shown that the NN controller gives better results on actual systems than a standard backstepping controller developed assuming full knowledge of the dynamics. Moreover, the NN controller does not require the linear-in-the-parameters assumption or the computation of regression matrices required by standard backstepping.",https://ieeexplore.ieee.org/document/980681/,Proceedings of the 40th IEEE Conference on Decision and Control (Cat. No.01CH37228),4-7 Dec. 2001,ieeexplore
10.1109/ICCNEA.2017.25,Design of Multi-channel Temperature Control Inspection System Based on PLC,IEEE,Conferences,"The temperature control system is widely used in the field of industrial control, such as the boiler's temperature control system in Steel, chemical plants and thermal power plants. For the requirements of remote centralized management and security monitor in temperature control system, a temperature control inspection system consisted by down-computer clew and up-computer, is designed in this paper. In this system, a programmable logic controller (PLC) is use as up-computer, multiple AI smart meters are use as down-computer clew. The structure of the system hardware and the interconnection of the various parts are introduced simply, the design and implementation of communication system of down-computer is elaborated in detail, and the part of the communication system program is given. The actual operation shows that the remote monitoring function can be realized and design requirements be satisfied by the application of intelligent instruments of real-time collection, processing and feedback on the site temperature, and high efficiency, high universality and reliable stability are the advantages of the system.",https://ieeexplore.ieee.org/document/8128601/,"2017 International Conference on Computer Network, Electronic and Automation (ICCNEA)",23-25 Sept. 2017,ieeexplore
10.1109/HICSS.1999.772817,Design of a vision system for identity verification,IEEE,Conferences,"The use of biometric data for automated identity verification, is one of the major challenges in many application domains. This is certainly a formidable task which requires the development of a complex system including several concurrent agents operating in real time. In this paper a system for automated identity verification (currently under development within an European research project) encompassing the active vision paradigm is described. In our approach the amount of data to be processed is limited by selecting and analysing only few areas within the face image. The number of pixels for each area are also reduced by applying a space-variant conformal mapping. The devised system does not require to use special hardware. On the other hand, robustness can be enforced by performing the final matching with more than a single image. This may require to adopt a simple, coarse scale, multi-processor architecture. The system is conceived for banking applications but can be ported to a variety of industrial applications. Several experiment's on identity verification, performed on real images, are presented.",https://ieeexplore.ieee.org/document/772817/,Proceedings of the 32nd Annual Hawaii International Conference on Systems Sciences. 1999. HICSS-32. Abstracts and CD-ROM of Full Papers,5-8 Jan. 1999,ieeexplore
10.1109/CSNT.2017.8418538,Detection of attacks in IoT based on ontology using SPARQL,IEEE,Conferences,"Nowadays, the concept of Internet of Things (IoT) has been manifested into reality with the help of latest developments or transformations in hardware circuitry, devices and protocols. IoT is such a diversified field in which a lot of challenges are faced during implementations of IoT applications including smart cities, smart homes, industrial sectors etc. The current scenario is highly demanding for deployment of smart sensors into existing applications to deliver a fully automated system. The major issue faced by IoT's existing system is security issue. In this paper, various attacks in IoT systems has discussed and focuses on ontology based model to deal with various attacks.",https://ieeexplore.ieee.org/document/8418538/,2017 7th International Conference on Communication Systems and Network Technologies (CSNT),11-13 Nov. 2017,ieeexplore
10.1109/ICAMechS49982.2020.9310079,Developing Robotic System for Harvesting Pineapples,IEEE,Conferences,"This paper develops a robotic system to harvest pineapple autonomously. The system contains a machine vision unit, two robotic manipulators mounted on a platform, custom end-effectors, and an image-based harvesting control unit. The manipulators with Gantry 3DOF PPP configuration are geometrically optimized to move the end-effectors approaching pineapples. Each end-effector is actuated by pneumatic actuator and equipped with a cage-shaped gripper to fix the selective pineapple inside and a cutting device to cut its stalk. YOLOv3 approach is implemented for detecting and recognizing pineapple fruits that meet requirements for harvest. The experiment results demonstrate the success of pineapple recognition with 90.82% mAP. The 3D position of the recognized pineapples will be calculated and sent to the control system. The control system, including an industrial computer communicating with PLCs to conducts the manipulators and end-effectors to approach and d the recognized pineapples. The complete system has been tested on the experimental field-model. The success rate of pineapple harvesting is 95.55% and the average time is 12 seconds per one fruit. In the future, this system will be improved for automatic harvesting in real pineapple fields.",https://ieeexplore.ieee.org/document/9310079/,2020 International Conference on Advanced Mechatronic Systems (ICAMechS),10-13 Dec. 2020,ieeexplore
10.1109/IAS.2004.1348846,Development of a self-tuned neuro-fuzzy controller for induction motor drives,IEEE,Conferences,"In This work a novel adaptive neuro-fuzzy (NF) based speed control of an induction motor (IM) is presented. The proposed neuro-fuzzy controller (NFC) incorporates fuzzy logic laws with a five-layer artificial neural network (ANN) scheme. In this controller only three membership functions are used for each input keeping in mind for low computational burden, which will be suitable for real-time implementation. Furthermore, for the proposed NFC an improved self-tuning method is developed based on the IM theory and its high performance requirements. The main task of the tuning method is to adjust the parameters of the fuzzy logic controller (FLC) in order to minimize the square of the error between actual and reference outputs. This work also demonstrates how the proposed NFC can easily be adjusted to work with different size of induction motors. A complete simulation model for indirect field oriented control of IM incorporating the proposed NFC is developed. The performance of the proposed NFC based IM drive is investigated extensively at different operating conditions in simulation. In order to prove the superiority of the proposed NFC, the results for the proposed controller are also compared to those obtained by a conventional PI controller. The proposed NFC based IM drive is found to be more robust as compared to conventional PI controller based drive and hence found suitable for high performance industrial drive applications.",https://ieeexplore.ieee.org/document/1348846/,"Conference Record of the 2004 IEEE Industry Applications Conference, 2004. 39th IAS Annual Meeting.",3-7 Oct. 2004,ieeexplore
10.1109/BIGCOMP.2019.8679267,Diagnosis of Corporate Insolvency Using Massive News Articles for Credit Management,IEEE,Conferences,"In the aftermath of the 4th Industrial Revolution, AI and Big data technology have been used in various fields in South Korea, and the techniques are being applied to and complemented in various service fields which were implemented without them before. Especially, in order to secure credit stability for borrowed companies from financial institutions and to preemptively respond to the risks about-by means of online news articles and SNS data-the attempts to forecast the possibility of insolvency and adopt them into actual business are actively conducted by major domestic banks. In this study, we describe several analytical methods, outputs, and problems that are encountered during the processes of developing the unstructured text-based prediction system to detect the possibility of corporate insolvency-which ordered by a national government bank and discuss related issues with a real case. As a result, we have implemented an automatic tagger program for labeling largely unlabeled articles, and newly devised a prediction algorithm of the possibility of corporate insolvency. We achieved the accuracy of 92% (AUC 0.96) in aspect of performance and the hit ratio of 50% among the number of predicted 26 candidates that have the possibility of insolvency. Thus, the result of our study is revealed to be complementary to the financial data analysis sufficiently in performance, but yet have several limitations such as data coverage, reliability, and the characteristics of Korean language.",https://ieeexplore.ieee.org/document/8679267/,2019 IEEE International Conference on Big Data and Smart Computing (BigComp),27 Feb.-2 March 2019,ieeexplore
10.1109/UPCON47278.2019.8980279,Diagnosis of Induction Motor Faults Using Frequency Occurrence Image Plots—A Deep Learning Approach,IEEE,Conferences,"Accurate diagnosis of induction motor faults is important for reliable and safe operation of industrial processes. Majority of the faults which occur in induction motors are mainly diagnosed using motor current signature analysis. However, the accuracy of fault detection depends on selection of suitable features from motor current, the failure of which may result in incorrect interpretation. Considering the aforesaid fact, this paper presents an image processing aided deep learning framework for reliable diagnosis of induction motor faults, which eliminates the need of separate feature extraction stage. To this end, the motor current signals under different types of fault conditions were procured and were subsequently processed into frequency occurrence plots. The frequency occurrence image plots for different fault scenarios were finally used as inputs to a deep convolution neural network for the purpose of classification. Transfer learning technique was adopted to reduce the computation time of Convolution Neural Network and classification of motor faults was done at five different loading conditions. Four types of classification tasks have been addressed here and comprehensive analysis was done using a variety of CNN architectures. It has been observed that the proposed method returns a highest mean classification accuracy of 96.67% in segregating different types of faults which can be implemented in real-life for condition monitoring of induction motors.",https://ieeexplore.ieee.org/document/8980279/,"2019 International Conference on Electrical, Electronics and Computer Engineering (UPCON)",8-10 Nov. 2019,ieeexplore
10.23919/ChiCC.2019.8866554,Diffusion welding furnace temperature controller based on Actor-Critic,IEEE,Conferences,"Based on the basic mechanism of reinforcement learning, this paper proposes an adaptive PID control algorithm based on Actor-Critic according to the nonlinear and large delay characteristics of complex industrial processes, and controls the diffusion welding furnace. The simulation experiment of the effect shows that the algorithm has better real-time and robustness than the traditional PID.",https://ieeexplore.ieee.org/document/8866554/,2019 Chinese Control Conference (CCC),27-30 July 2019,ieeexplore
10.1109/IAI50351.2020.9262203,Digital Twin Enabled Smart Control Engineering as an Industrial AI: A New Framework and Case Study,IEEE,Conferences,"In Industry 4.0, the increasing complexity of industrial systems introduces unknown dynamics that affect the performance of manufacturing processes. Thus, Digital Twin appears as a breaking technology to develop virtual representations of any complex system design, analysis, and behavior prediction tasks to enhance the system understanding via enabling capabilities like real-time analytics, or Smart Control Engineering. In this paper, a novel framework is proposed for the design and implementation of Digital Twin applications to the development of Smart Control Engineering. The framework involve the steps of system documentation, Multidomain Simulation, Behavioral Matching, and real-time monitoring, which is applied to develop the Digital Twin for a real-time vision feedback temperature uniformity control. The obtained results show that Digital Twin is a fundamental part of the transformation into Industry 4.0.",https://ieeexplore.ieee.org/document/9262203/,2020 2nd International Conference on Industrial Artificial Intelligence (IAI),23-25 Oct. 2020,ieeexplore
10.1109/ICVES.2009.5400189,Digital implementation of fuzzy logic controller for wide range speed control of brushless DC motor,IEEE,Conferences,"The brushless DC motors find wide applications such as in battery operated vehicles, wheel chairs, automotive fuel pumps, robotics, machine tools, aerospace and in many industrial applications due to their superior electrical and mechanical characteristics and its capability to operate in hazardous environment. Conventional controllers fail to yield desired performance in BLDC motor control systems due to the non-linearity arising out of variation in the system parameters and change in load. The main focus is now on the application of artificial intelligent techniques such as fuzzy logic to solve this problem. Another great challenge is to reduce the size and cost of the drive system without compromising the performance. In this paper, the design and digital implementation of fuzzy logic controller using a versatile ADUC812 microcontroller, and low-cost, compact, superior performance components are used in order to reduce the cost and size of the drive system. The experimental results are presented to prove the flexibility of the control scheme in real time.",https://ieeexplore.ieee.org/document/5400189/,2009 IEEE International Conference on Vehicular Electronics and Safety (ICVES),11-12 Nov. 2009,ieeexplore
10.1109/AIKE.2018.00042,Distributed Osmotic Computing Approach to Implementation of Explainable Predictive Deep Learning at Industrial IoT Network Edges with Real-Time Adaptive Wavelet Graphs,IEEE,Conferences,"Challenges associated with developing analytics solutions at the edge of large scale Industrial Internet of Things (IIoT) networks close to where data is being generated in most cases involves developing analytics solutions from ground up. However, this approach increases IoT development costs and system complexities, delay time to market, and ultimately lowers competitive advantages associated with delivering next-generation IoT designs. To overcome these challenges, existing, widely available, hardware can be utilized to successfully participate in distributed edge computing for IIoT systems. In this paper, an osmotic computing approach is used to illustrate how distributed osmotic computing and existing low-cost hardware may be utilized to solve complex, compute-intensive Explainable Artificial Intelligence (XAI) deep learning problem from the edge, through the fog, to the network cloud layer of IIoT systems. At the edge layer, the C28x digital signal processor (DSP), an existing low-cost, embedded, real-time DSP that has very wide deployment and integration in several IoT industries is used as a case study for constructing real-time graph-based Coiflet wavelets that could be used for several analytic applications including deep learning pre-processing applications at the edge and fog layers of IIoT networks. Our implementation is the first known application of the fixed-point C28x DSP to construct Coiflet wavelets. Coiflet Wavelets are constructed in the form of an osmotic microservice, using embedded low-level machine language to program the C28x at the network edge. With the graph-based approach, it is shown that an entire Coiflet wavelet distribution could be generated from only one wavelet stored in the C28x based edge device, and this could lead to significant savings in memory at the edge of IoT networks. Pearson correlation coefficient is used to select an edge generated Coiflet wavelet and the selected wavelet is used at the fog layer for pre-processing and denoising IIoT data to improve data quality for fog layer based deep learning application. Parameters for implementing deep learning at the fog layer using LSTM networks have been determined in the cloud. For XAI, communication network noise is shown to have significant impact on results of predictive deep learning at IIoT network fog layer.",https://ieeexplore.ieee.org/document/8527474/,2018 IEEE First International Conference on Artificial Intelligence and Knowledge Engineering (AIKE),26-28 Sept. 2018,ieeexplore
10.1109/ICARCV.2014.7064599,Distributed signature analysis of induction motors using Artificial Neural Networks,IEEE,Conferences,Motor current signature analysis is a modern approach to fault diagnose and classification for induction motors. Many studies reported successful implementation of MCSA in laboratory situations whereas the method was not so successful in real industrial situation due to propagation of neighbor faults and unwanted noise signals. This paper investigate the correlation between different observations of events in order to provide a more accurate estimation of behavior of electrical motors at a given site. An analytical framework has been implemented to correlate and classify independent fault observations and diagnose the type and identify the origin of fault symptoms. The fault diagnosis algorithm has two layers. Initially outputs of all sensors are processed to generate fault indicators. These fault indicators then are to be classified using an Artificial Neural Network. A typical industrial site is taken as a case study and simulated to evaluate the concept of distributed fault analysis.,https://ieeexplore.ieee.org/document/7064599/,2014 13th International Conference on Control Automation Robotics & Vision (ICARCV),10-12 Dec. 2014,ieeexplore
10.1109/PHM-Paris.2019.00054,Domain Adaptive Transfer Learning for Fault Diagnosis,IEEE,Conferences,"Thanks to digitization of industrial assets in fleets, the ambitious goal of transferring fault diagnosis models from one machine to the other has raised great interest. Solving these domain adaptive transfer learning tasks has the potential to save large efforts on manually labeling data and modifying models for new machines in the same fleet. Although data-driven methods have shown great potential in fault diagnosis applications, their ability to generalize on new machines and new working conditions are limited because of their tendency to overfit to the training set in reality. One promising solution to this problem is to use domain adaptation techniques. It aims to improve model performance on the target new machine. Inspired by its successful implementation in computer vision, we introduced Domain-Adversarial Neural Networks (DANN) to our context, along with two other popular methods existing in previous fault diagnosis research. We then carefully justify the applicability of these methods in realistic fault diagnosis settings, and offer a unified experimental protocol for a fair comparison between domain adaptation methods for fault diagnosis problems.",https://ieeexplore.ieee.org/document/8756463/,2019 Prognostics and System Health Management Conference (PHM-Paris),2-5 May 2019,ieeexplore
10.1109/ICIAFS.2007.4544783,Dynamic power management of an embedded sensor network based on actor-critic reinforcement based learning,IEEE,Conferences,"Wireless sensor networks (WSNs) have gained tremendous popularity in recent years due to the wide range of applications envisioned - ranging from aerospace and defense to industrial and commercial. Although limited by communication and energy constraints, the low cost, small sensor nodes lend themselves to be deployed in large numbers to form a network with high spatial distribution. The overall effectiveness of the sensor network depends on how well the mutually contradicting objectives of conserving the limited on-board battery power and keeping the sensors awake for stimuli, are managed. In this paper, we have proposed an actor-critic based reinforcement learning mechanism that can be practically implemented on an embedded sensor with limited memory and processing power. Specifically, the contribution of this paper is the development of the value function (or critic/reinforcement function) that is implemented on each sensor node which aids in dynamic power scheduling based on different situations. The effectiveness of the proposed method has been demonstrated with real world experiments.",https://ieeexplore.ieee.org/document/4544783/,2007 Third International Conference on Information and Automation for Sustainability,4-6 Dec. 2007,ieeexplore
10.1109/ICCT46805.2019.8947193,EDGE AI for Heterogeneous and Massive IoT Networks,IEEE,Conferences,"By combining multiple sensing and wireless access technologies, the Internet of Things (IoT) shall exhibit features with large-scale, massive, and heterogeneous sensors and data. To integrate diverse radio access technologies, we present the architecture of heterogeneous IoT system for smart industrial parks and build an IoT experimental platform. Various sensors are installed on the IoT devices deployed on the experimental platform. To efficiently process the raw sensor data and realize edge artificial intelligence (AI), we describe four statistical features of the raw sensor data that can be effectively extracted and processed at the network edge in real time. The statistical features are calculated and fed into a back-propagation neural network (BPNN) for sensor data classification. By comparing to the k-nearest neighbor classification algorithm, we examine the BPNN-based classification method with a great amount of raw data gathered from various sensors. We evaluate the system performance according to the classification accuracy of BPNN and the performance indicators of the cloud server, which shows that the proposed approach can effectively enable the edge-AI-based heterogeneous IoT system to process the sensor data at the network edge in real time while reducing the demand for computing and network resources of the cloud.",https://ieeexplore.ieee.org/document/8947193/,2019 IEEE 19th International Conference on Communication Technology (ICCT),16-19 Oct. 2019,ieeexplore
10.1109/SECON52354.2021.9491609,EFCam: Configuration-Adaptive Fog-Assisted Wireless Cameras with Reinforcement Learning,IEEE,Conferences,"Visual sensing has been increasingly employed in industrial processes. This paper presents the design and implementation of an industrial wireless camera system, namely, EFCam, which uses low-power wireless communications and edge-fog computing to achieve cordless and energy-efficient visual sensing. The camera performs image pre-processing (i.e., compression or feature extraction) and transmits the data to a resourceful fog node for advanced processing using deep models. EFCam admits dynamic configurations of several parameters that form a configuration space. It aims to adapt the configuration to maintain desired visual sensing performance of the deep model at the fog node with minimum energy consumption of the camera in image capture, pre-processing, and data communications, under dynamic variations of application requirement and wireless channel conditions. However, the adaptation is challenging due primarily to the complex relationships among the involved factors. To address the complexity, we apply deep reinforcement learning to learn the optimal adaptation policy. Extensive evaluation based on trace-driven simulations and experiments show that EFCam complies with the accuracy and latency requirements with lower energy consumption for a real industrial product object tracking application, compared with four baseline approaches incorporating hysteresis-based adaptation.",https://ieeexplore.ieee.org/document/9491609/,"2021 18th Annual IEEE International Conference on Sensing, Communication, and Networking (SECON)",6-9 July 2021,ieeexplore
10.1109/CSCloud-EdgeCom49738.2020.00050,Edge Computing-based 3D Pose Estimation and Calibration for Robot Arms,IEEE,Conferences,"Industrial robots are widely used in current production lines, and complex pipeline processes, especially those with different assembly requirements, are designed for intelligent manufacturing in the era of industry 4.0. During the new crown epidemic, a large number of car companies used the production line to transform production of medical materials such as masks and protective clothing, which provided a strong guarantee for fighting the epidemic. In this scenario, a pipeline is often assembled from robotic arms from multiple suppliers. The traditional methods is complex and takes a lot of time. In this paper, we propose a novel deep learning based robot arm 3D pose estimation and calibration model with simple Kinect stereo cameras which can be deployed on light-weight edge computing systems. The light-weight deep CNN model can detection 5 predefined key points based on RGB-D data. In this way, when the assembly line composed of different robot arms needs to be reassembled, our model can quickly provide the robot's pose information without additional tuning processes. Testing in Webots with Rokae xb4 robot arm model shows that our model can quickly estimate the key point of the robot arm.",https://ieeexplore.ieee.org/document/9170983/,2020 7th IEEE International Conference on Cyber Security and Cloud Computing (CSCloud)/2020 6th IEEE International Conference on Edge Computing and Scalable Cloud (EdgeCom),1-3 Aug. 2020,ieeexplore
10.1109/ISAECT50560.2020.9523700,Edge-Cloud Architectures Using UAVs Dedicated To Industrial IoT Monitoring And Control Applications,IEEE,Conferences,"The deployment of new technologies to ease the control and management of a massive data volume and its uncertainty is a very significant challenge in the industry. Under the name ""Smart Factory"", the Industrial Internet of Things (IoT) aims to send data from systems that monitor and control the physical world to data processing systems for which cloud computing has proven to be an important tool to meet processing needs. unmanned aerial vehicles (UAVs) are now being introduced as part of IIoT and can perform important tasks. UAVs are now considered one of the best remote sensing techniques for collecting data over large areas. In the field of fog and edge computing, the IoT gateway connects various objects and sensors to the Internet. It function as a common interface for different networks and support different communication protocols. Edge intelligence is expected to replace Deep Learning (DL) computing in the cloud, providing a variety of distributed, low-latency and reliable intelligent services. In this paper, An unmanned aerial vehicle is automatically integrated into an industrial control system through an IoT gateway platform. Rather than sending photos from the UAV to the cloud for processing, an AI cloud trained model is deployed in the IoT gateway and used to process the taken photos. This model is designed to overcome the latency channels of the cloud computing architecture. The results show that the monitoring and tracking process using advanced computing in the IoT gateway is significantly faster than in the cloud.",https://ieeexplore.ieee.org/document/9523700/,2020 International Symposium on Advanced Electrical and Communication Technologies (ISAECT),25-27 Nov. 2020,ieeexplore
10.1109/ICEETS.2016.7583860,Efficiency optimization of induction motor drive using Artificial Neural Network,IEEE,Conferences,"Induction motors are the workhorse of industry, have good efficiency at rated load, but long duration usage of IM at partial load shows poor efficiency which leads to waste in energy and revenue as well. These motors are reliable, robust, high power/mass ratio and economic, hence replaced all other motors in the industry, so even minute increment in induction motor efficiency can have a major impact on consumption of electricity and saving of revenue, globally. This paper utilizes, a combination of two key concepts of efficiency optimization-loss model control (LMC) and search control (SC) for efficient operation of induction motors used in various industrial applications, in aforesaid load condition. At first, to estimate optimal I<sub>ds</sub> values for various load conditions, an optimal I<sub>ds</sub> expression in terms of machine parameters and load parameters, based on machine loss model in d-q frame along with classical optimization technique, is utilized. Secondly, an offline trained artificial neural network (ANN) controller is used to reproduce the optimal I<sub>ds</sub> values, in run-time load condition. This eliminates run-time computations and perturbation for optimal flux, as in conventional SC method. The (ANN) optimal controller is designed for optimal I<sub>ds</sub> as output, while providing load torque and speed information as inputs. The training is performed in MATLAB and good accuracy of the training model is seen. Dynamic and steady-state performances are compared for proposed optimal (optimal I<sub>ds</sub>) operations and conventional vector operations (constant I<sub>ds</sub>), with the help of a simulation model, developed in MATLAB. Excellent dynamic response in load transients as well as superior efficiency performance (1- 18%) at steady-state, for a wide range of speed and torque in simulation is attained. Assimilated with similar earlier work, the proposed methodology offers effortless implementation in real-time industrial facilities, ripple free operations, fast response and higher energy savings.",https://ieeexplore.ieee.org/document/7583860/,2016 International Conference on Energy Efficient Technologies for Sustainability (ICEETS),7-8 April 2016,ieeexplore
10.1109/ICICT50816.2021.9358469,Efficient Fault Isolation Method to Monitor Industrial Batch Processes,IEEE,Conferences,Industrial batch processes are very popular manufacturing system with large number of process variables involved. Monitoring of batch processes using statistical process monitoring becomes very difficult in view of the complex correlations between the process variables. This paper focuses on a fault isolation based process monitoring method without prior information of fault where fault isolation problem is converted into a variable selection. Variable selection is a learning algorithm used here to solve the problem of selection and isolation of variables from a model. The method discussed here uses a sparse coefficient based dissimilarity analysis algorithm known as Sparse Dissimilarity Algorithm(SDISSIM) which checks a calculated D-index for identifying fault in the process. A sparse coefficient is tabulated to verify the process variables contributing to the fault and an absolute variance difference is calculated to select the variables for fault isolation. Finally SDISSIM method is explained by successful implementation in MATLAB with real time industrial process data.,https://ieeexplore.ieee.org/document/9358469/,2021 6th International Conference on Inventive Computation Technologies (ICICT),20-22 Jan. 2021,ieeexplore
10.1109/CASE48305.2020.9249228,Efficiently Learning a Distributed Control Policy in Cyber-Physical Production Systems Via Simulation Optimization,IEEE,Conferences,"The manufacturing industry is becoming more dynamic than ever. The limitations of non-deterministic network delays and real-time requirements call for decentralized control. For such dynamic and complex systems, learning methods stand out as a transformational technology to have a more flexible control solution. Using simulation for learning enables the description of highly dynamic systems and provides samples without occupying a real facility. However, it requires prohibitively expensive computation. In this paper, we argue that simulation optimization is a powerful tool that can be applied to various simulation-based learning processes for tremendous effects. We proposed an efficient policy learning framework, ROSA (Reinforcement-learning enhanced by Optimal Simulation Allocation), with unprecedented integration of learning, control, and simulation optimization techniques, which can drastically improve the efficiency of policy learning in a cyber-physical system. A proof-of-concept is implemented on a conveyer-switch network, demonstrating how ROSA can be applied for efficient policy learning, with an emphasis on the industrial distributed control system.",https://ieeexplore.ieee.org/document/9249228/,2020 IEEE 16th International Conference on Automation Science and Engineering (CASE),20-21 Aug. 2020,ieeexplore
10.1109/IWACI.2010.5585154,Estimation of component concentrations of sodium aluminate solution via PLS and Hammerstein recurrent neural networks,IEEE,Conferences,"In this paper, a new on-line soft sensing method is proposed for component concentrations of sodium aluminate solution. With this sensing strategy, real-time control and optimization can be realized in aluminate production plants. Several advance techniques are used, such as PLS (Partial Least Squares), Hammerstein model, recurrent neural networks and least square algorithm. Industrial experiment results show that the proposed soft sensing algorithm is effective.",https://ieeexplore.ieee.org/document/5585154/,Third International Workshop on Advanced Computational Intelligence,25-27 Aug. 2010,ieeexplore
10.1109/IPIN.2018.8533862,Evaluation Criteria for Inside-Out Indoor Positioning Systems Based on Machine Learning,IEEE,Conferences,"Real-time tracking allows to trace goods and enables the optimization of logistics processes in many application areas. Camera-based inside-out tracking that uses an infrastructure of fixed and known markers is costly as the markers need to be installed and maintained in the environment. Instead, systems that use natural markers suffer from changes in the physical environment. Recently a number of approaches based on machine learning (ML) aim to address such issues. This paper proposes evaluation criteria that consider algorithmic properties of ML-based positioning schemes and introduces a dataset from an indoor warehouse scenario to evaluate for them. Our dataset consists of images labeled with millimeter precise positions that allows for a better development and performance evaluation of learning algorithms. This allows an evaluation of machine learning algorithms for monocular optical positioning in a realistic indoor position application for the first time. We also show the feasibility of ML-based positioning schemes for an industrial deployment.",https://ieeexplore.ieee.org/document/8533862/,2018 International Conference on Indoor Positioning and Indoor Navigation (IPIN),24-27 Sept. 2018,ieeexplore
10.1109/WCICSS.2015.7420323,Evolving decision trees to detect anomalies in recurrent ICS networks,IEEE,Conferences,"Researchers have previously attempted to apply machine learning techniques to network anomaly detection problems. Due to the staggering amount of variety that can occur in normal networks, as well as the difficulty in capturing realistic data sets for supervised learning or testing, the results have often been underwhelming. These challenges are far less pronounced when considering industrial control system (ICS) networks. The recurrent nature of these networks results in less noise and more consistent patterns for a machine learning algorithm to recognize. We propose a method of evolving decision trees through genetic programming (GP) in order to detect network anomalies, such as device outages. Our approach extracts over a dozen features from network packet captures and netflows, normalizes them, and relates them in decision trees using fuzzy logic operators. We used the trees to detect three specific network events from three different points on the network across a statistically significant number of runs and achieved 100% accuracy on five of the nine experiments. When the trees attempted to detect more challenging events at points of presence further from the occurrence, the accuracy averaged to above 98%. On cases where the trees were many hops away and not enough information was available, the accuracy dipped to roughly 50%, or that of a random search. Using our method, all of the evolutionary cycles of the GP algorithm are computed a-priori, allowing the best resultant trees to be deployed as semi-real-time sensors with little overhead. In order for the trees to perform optimally, buffered packets and flows need to be ingested at twenty minute intervals.",https://ieeexplore.ieee.org/document/7420323/,2015 World Congress on Industrial Control Systems Security (WCICSS),14-16 Dec. 2015,ieeexplore
10.1109/EUROSIM.2013.39,Experimental and Computational Materials Defects Investigation,IEEE,Conferences,"Production of railway axles (i.e., one of the basic material of the modern train) is an elaborate process unfree from faults and problems. Errors during the manufacturing or the plies' overlapping, in fact, can cause particular flaws in the resulting material, so compromising its same integrity. Within this framework, ultrasonic tests could be useful to characterize the presence of defect, depending on its dimensions. On the contrary, the requirement of a perfect state for used materials is unavoidable in order to assure both transport reliability and passenger safety. Therefore, a real-time approach able to recognize and classify the defect starting from the finite element simulated ultrasonic echoes could be very useful in industrial applications. The ill-posedness of the so defined process induces a regularization method. In this paper, a finite element and a heuristic approach are proposed. Particularly, the proposed method is based on the use of a Neural Network approach, the so called ""learning by sample techniques"", and on the use of Support Vector Machines in order to classify the kind of defect. Results assure good performances of the implemented approach, with very interesting applications.",https://ieeexplore.ieee.org/document/7004937/,2013 8th EUROSIM Congress on Modelling and Simulation,10-13 Sept. 2013,ieeexplore
10.1109/MELCON.1994.380920,Expert system for control purpose based on CLIPS,IEEE,Conferences,"The use of AI techniques in complex process control of industrial environments, introduces some problems. The first is related to which kind of tools can be used and their match to system requirements. The next one is how to incorporate new features if needed and how to integrate them. The last one is related to the final implementation. Expert systems available in public domain source code seems a good solution in order to reach the advantages of an ES developed specially for the application without spending time developing low level ES features. An ES in source code allows us to select some parts of an ES kernel useful for the particular application. Actually we can find commercial ES in source code like CLIPS. We describe the problem from the point of view of developing ES with CLIPS. This tool is a complete ES language written in C code, it uses the RETE fast pattern matching algorithm to implement forward chaining inference engine. The availability of the source code allows us to cut the parts not used in a particular application and recompile it in a specific machine for control purposes. Therefore we can link the modified code with real time run-time libraries, obtaining applications dealing with real time constraints. Based on this tool, an intelligent controller has been developed which uses fuzzy logic for uncertainty handling.&lt;<ETX>&gt;</ETX>",https://ieeexplore.ieee.org/document/380920/,Proceedings of MELECON '94. Mediterranean Electrotechnical Conference,12-14 April 1994,ieeexplore
10.1109/SMC.2019.8913901,Explainable Machine Learning in Industry 4.0: Evaluating Feature Importance in Anomaly Detection to Enable Root Cause Analysis,IEEE,Conferences,"In the past recent years, Machine Learning methodologies have been applied in countless application areas. In particular, they play a key role in enabling Industry 4.0. However, one of the main obstacles to the diffusion of Machine Learning-based applications is related to the lack of interpretability of most of these methods. In this work, we propose an approach for defining a `feature importance' in Anomaly Detection problems. Anomaly Detection is an important Machine Learning task that has an enormous applicability in industrial scenarios. Indeed, it is extremely relevant for the purpose of quality monitoring. Moreover, it is often the first step towards the design of a Machine Learning-based smart monitoring solution because Anomaly Detection can be implemented without the need of labelled data. The proposed feature importance evaluation approach is designed for Isolation Forest, one of the most commonly used algorithm for Anomaly Detection. The efficacy of the proposed method is tested on synthetic and real industrial datasets.",https://ieeexplore.ieee.org/document/8913901/,"2019 IEEE International Conference on Systems, Man and Cybernetics (SMC)",6-9 Oct. 2019,ieeexplore
10.1109/WFCS.2019.8757999,Exploiting localization systems for LoRaWAN transmission scheduling in industrial applications,IEEE,Conferences,"The Internet of Things (IoT) paradigm contaminated the industrial world. Wireless communications seem to be particularly attracting, especially when complement indoor and outdoor Real Time Location Systems (RTLS) for geo-referencing smart objects (e.g. for asset tracking). In this paper, the LoRaWAN solution is considered for long range transmission of RTLS data (LoRaWAN is an example of Low Power Wide Area Network). Given that the RTLSs use time synchronization, this work proposes to opportunistically obtain LoRaWAN Class A node time synchronization using the RTLS ranging devices. Once a common sense of time is shared in the LoRaWAN network, more efficient scheduled medium access strategies can be implemented. The experimental testbed, based on commercially available solutions, demonstrates the affordability and feasibility of the proposed approach. When low-cost GPS (outdoor) and UWB (indoor) ranging devices are considered, synchronization error of few microseconds can be easily obtained. The experimental results show the that time reference pulses disciplined by GPS have a maximum jitter of 180 ns and a standard deviation of 40 ns whereas, if time reference pulses disciplined by UWB are considered, the maximum jitter is 3.3 μs and the standard deviation is 0.7 μs.",https://ieeexplore.ieee.org/document/8757999/,2019 15th IEEE International Workshop on Factory Communication Systems (WFCS),27-29 May 2019,ieeexplore
10.1109/IS.2018.8710554,Exploiting the Digital Twin in the Assessment and Optimization of Sustainability Performances,IEEE,Conferences,"Digitalization has shown the potential to disrupt industrial value chains by supporting real-time, risk-free and inexpensive inputs to decision making towards enhanced companies' productivity and value networks flexibility. Developing a reliable and robust digital replica of the physical systems of the value chain is one of the most advanced (and challenging) approaches to digitalization, condensed in the concept of Digital Twin (DT). DT plays a fundamental role in creating a data-rich environment where simulation and optimization procedures can be run. With DT expected to become a commodity in the coming years, simulation and optimization become therefore a more accessible instrument for the improvement of manufacturing and business processes also in small enterprises with limited investment capacity. While scientific literature has analysed the adoption of DT in the optimization of products lifecycle, no contributions have yet focused on the exploitation of DT to improve the sustainability performances of whole value chains. In this paper we propose a reference framework where DTs built upon process and system data gathered from the field, allow to quickly assess the sustainability performances of both existing and planned production mixes and to compare achievable impacts with changing processes and technologies, thus enabling advisory features for sustainability-aware decision making in structured, multi-entity value networks. Internal validation will be deployed referring to real case studies.",https://ieeexplore.ieee.org/document/8710554/,2018 International Conference on Intelligent Systems (IS),25-27 Sept. 2018,ieeexplore
10.1109/ICAIE50891.2020.00028,Exploration and Practice on Industrial Robot Experimental Teaching Based on Virtuality and Reality Combination,IEEE,Conferences,"In view of the common problems currently existing in the industrial robot experimental teaching in the higher vocational colleges, this paper, through deep integration of the traditional experimental teaching and the virtual simulation technology, proposes the industrial robot experimental teaching mode based on the combination of virtuality and reality. The teaching process is divided into three parts: pre-class, in-class and post-class, and the experimental teaching design is elaborated in details by taking the “material blocks handling experiment” as an example. Practices proved that the experimental teaching mode based on the virtuality and reality combination has improved the efficiency and quality of classroom teaching, enriched the after-school time of students and improved the depth and scope of learning, which played a positive role in promoting the specialty construction and talent cultivation, and could provide reference for the experimental teaching of related specialties in similar colleges and universities.",https://ieeexplore.ieee.org/document/9262587/,2020 International Conference on Artificial Intelligence and Education (ICAIE),26-28 June 2020,ieeexplore
10.1109/ITOEC.2018.8740558,Fast Inter Mode Decision Algorithms for x265,IEEE,Conferences,"The latest High-Efficiency Video Coding (HEVC) standard achieves nearly 50% bit rates reduction for similar quality relative to H.264/Advanced Video Coding(AVC) . However, its complexity is enormously increased ,which becomes one of the most challenges for its deployment in real time applications. The only solution to decrease the coding complexity is to set up different settings by adjusting various coding parameters. Among them, low complexity settings are suitable for industrial applications and conducive to the popularization of HEVC. Traditional fast mode decision algorithms mainly aim at decreasing coding complexity for high complexity settings. In this paper, we propose a fast mode decision method for HEVC with low complexity settings according to machine learning. A decision tree is constructed to decide whether to check 2N×2N mode or the SKIP/MERGE mode by exploiting relevant information from spatiotemporal adjacent Coding Units(CUs). Further mode skipping is performed based on the result of the first step. Experiments show that the proposed scheme can only increase by 1.42% Bjotegaard Delta Bit rate(BDBR) with an average time reduction of 22.45% for HEVC with low complexity settings.",https://ieeexplore.ieee.org/document/8740558/,2018 IEEE 4th Information Technology and Mechatronics Engineering Conference (ITOEC),14-16 Dec. 2018,ieeexplore
10.1109/INFOCT.2018.8356831,Fault class prediction in unsupervised learning using model-based clustering approach,IEEE,Conferences,"Manufacturing industries have been on a steady path considering for new methods to achieve near-zero downtime to have flexibility in the manufacturing process and being economical. In the last decade with the availability of industrial internet of things (IIoT) devices, this has made it possible to monitor the machine continuously using wireless sensors, assess the degradation and predict the failures of time. Condition-based predictive maintenance has made a significant influence in monitoring the asset and predicting the failure of time. This has minimized the impact on production, quality, and maintenance cost. Numerous approaches have been in proposed over the years and implemented in supervised learning. In this paper, challenges of supervised learning such as need for historical data and incapable of classifying new faults accurately will be overcome with a new methodology using unsupervised learning for rapid implementation of predictive maintenance activity which includes fault prediction and fault class detection for known and unknown faults using density estimation via Gaussian Mixture Model Clustering and K-means algorithm and compare their results with a real case vibration data.",https://ieeexplore.ieee.org/document/8356831/,2018 International Conference on Information and Computer Technologies (ICICT),23-25 March 2018,ieeexplore
10.1109/I2MTC43012.2020.9129595,Feature Ranking under Industrial Constraints in Continuous Monitoring Applications based on Machine Learning Techniques,IEEE,Conferences,"The design work-flow of machine learning techniques for continuous monitoring or predictive maintenance in an industrial context is usually a two step procedure: the selection of features to be computed from the observed signals and training of a suitable algorithm with real-life meaningful data, that will be next deployed in the second step. Feature selection is a relevant task since it provides a powerful optimisation of the deployed algorithm performance, for the given training data-set. The paper provides a method for feature ranking and selection that embeds constraints coming from real-life applications, including sensing device specifications, environmental noise, available processing resources, being all these latter aspects not considered in the currently available literature methods for feature selection. A practical case-study in the field on anomaly detection of machines is reported and discussed, in order to show the good properties of the provided method.",https://ieeexplore.ieee.org/document/9129595/,2020 IEEE International Instrumentation and Measurement Technology Conference (I2MTC),25-28 May 2020,ieeexplore
10.1109/ICS51289.2020.00088,Feature Selection for Malicious Traffic Detection with Machine Learning,IEEE,Conferences,"The network technology plays an important role in the emerging industry 4.0. Industrial control systems (ICS) are related to all aspects of human life and have become the target of cyber-attackers. Attacks on ICS may not only cause economic loss, but also damage equipment and hurt staff. The biggest challenges in establishing a secure network communication system is how to effectively detect and prevent malicious network behavior. A Network Intrusion Detection System (NIDS) can be deployed as a defense mechanism for cyberattacks. However, for industrial internet-of-things (IIoT) applications with limited computing resources, designing an effective NIDS is challenging. In this paper, we propose to use machine learning as the core technology to build a compact and effective NIDS for IIoT. The proposed method is validated by using the more recent UNSW-NB 15 dataset to improve the detection capability against new types of attacks in the real world. Furthermore, we demonstrate that the method is also valid for traditional KDD-CUP-99 dataset. Experimental results show that the proposed method achieves better performance than previous methods.",https://ieeexplore.ieee.org/document/9359069/,2020 International Computer Symposium (ICS),17-19 Dec. 2020,ieeexplore
10.1109/ICTTA.2008.4530058,Filters Bank Derived from the Wavelet Transform for Real Time Change Detection in Signal,IEEE,Conferences,"The aim of this paper is to detect the faults in industrial systems, through on-line monitoring. The faults that are concerned correspond to changes in frequency components of the signal. Thus, early fault detection, which reduces the possibility of catastrophic damage, is possible by detecting the changes of characteristic features of the signal. This approach combines the filters bank technique, for extracting frequency and energy characteristic features, and the dynamic cumulative sum method (DCS), which is a recursive calculation of the logarithm of the likelihood ratio between two local hypotheses. The main contribution is to derive the filters coefficients from the wavelet in order to use the filters bank as a wavelet transform. The advantage of our approach is that the filters bank can be hardware implemented and can be used for online detection.",https://ieeexplore.ieee.org/document/4530058/,2008 3rd International Conference on Information and Communication Technologies: From Theory to Applications,7-11 April 2008,ieeexplore
10.1109/ICESC48915.2020.9155625,Fire and Gun Violence based Anomaly Detection System Using Deep Neural Networks,IEEE,Conferences,"Real-time object detection to improve surveillance methods is one of the sought-after applications of Convolutional Neural Networks (CNNs). This research work has approached the detection of fire and handguns in areas monitored by cameras. Home fires, industrial explosions, and wildfires are a huge problem that cause adverse effects on the environment. Gun violence and mass shootings are also on the rise in certain parts of the world. Such incidents are time-sensitive and can cause a huge loss to life and property. Hence, the proposed work has built a deep learning model based on the YOLOv3 algorithm that processes a video frame-by-frame to detect such anomalies in real-time and generate an alert for the concerned authorities. The final model has a validation loss of 0.2864, with a detection rate of 45 frames per second and has been benchmarked on datasets like IMFDB, UGR, and FireNet with accuracies of 89.3%, 82.6% and 86.5% respectively. Experimental result satisfies the goal of the proposed model and also shows a fast detection rate that can be deployed indoor as well as outdoors.",https://ieeexplore.ieee.org/document/9155625/,2020 International Conference on Electronics and Sustainable Communication Systems (ICESC),2-4 July 2020,ieeexplore
10.1109/IECON.2000.973216,Force control in robotic assembly under extreme uncertainty using ANN,IEEE,Conferences,"Robotic assembly operations can be performed by specifying an exact model of the operation. However, the uncertainties involved during assembly make it difficult to conceive such a model In these cases, the use of a connectionist model may be advantageous. In this paper, the design of a robotic cell based on the adaptive resonance theory artificial neural network and a PC host-slave architecture that overcame these uncertainties is presented. Different sources of uncertainty under real conditions are identified and their contribution in a typical assembly operation evaluated. The robotic system is implemented using a PUMA 761 industrial robot with six degrees of freedom (DOF) and a force/torque (F/T) sensor attached to its wrist which conveys force information to the neural network controller (NNC). Results during assembly operations are presented which validate the approach. Furthermore, the method is generic and can be implemented onto other manipulators.",https://ieeexplore.ieee.org/document/973216/,"2000 26th Annual Conference of the IEEE Industrial Electronics Society. IECON 2000. 2000 IEEE International Conference on Industrial Electronics, Control and Instrumentation. 21st Century Technologies",22-28 Oct. 2000,ieeexplore
10.1109/PerComWorkshops51409.2021.9430942,Forecasting Parking Lots Availability: Analysis from a Real-World Deployment,IEEE,Conferences,"Smart parking technologies are rapidly being deployed in cities and public/private places around the world for the sake of enabling users to know in real time the occupancy of parking lots and offer applications and services on top of that information. In this work, we detail a real-world deployment of a full-stack smart parking system based on industrial-grade components. We also propose innovative forecasting models (based on CNN-LSTM) to analyze and predict parking occupancy ahead of time. Experimental results show that our model can predict the number of available parking lots in a ±3% range with about 80% accuracy over the next 1-8 hours. Finally, we describe novel applications and services that can be developed given such forecasts and associated analysis.",https://ieeexplore.ieee.org/document/9430942/,2021 IEEE International Conference on Pervasive Computing and Communications Workshops and other Affiliated Events (PerCom Workshops),22-26 March 2021,ieeexplore
10.1109/SEST.2018.8495711,From M&amp;V to M&amp;T: An artificial intelligence-based framework for real-time performance verification of demand-side energy savings,IEEE,Conferences,"The European Union's Energy Efficiency Directive is placing an increased focus on the measurement and verification (M&amp;V) of demand side energy savings. The objective of M&amp;V is to quantify energy savings with minimum uncertainty. M&amp;V is currently undergoing a transition to practices, known as M&amp;V 2.0, that employ automated advanced analytics to verify performance. This offers the opportunity to effectively manage the transition from short-term M&amp;V to long-term monitoring and targeting (M&amp;T) in industrial facilities. The original contribution of this paper consists of a novel, robust and technology agnostic framework that not only satisfies the requirements of M&amp;V 2.0, but also bridges the gap between M&amp;V and M&amp;T by ensuring persistence of savings. The approach features a unique machine learning-based energy modelling methodology, model deployment and an exception reporting system that ensures early identification of performance degradation. A case study demonstrates the effectiveness of the approach. Savings from a real-world project are found to be 177,962 +/- 12,334 kWh with a 90% confidence interval. The uncertainty associated with the savings is 8.6% of the allowable uncertainty, thus highlighting the viability of the framework as a reliable and effective tool.",https://ieeexplore.ieee.org/document/8495711/,2018 International Conference on Smart Energy Systems and Technologies (SEST),10-12 Sept. 2018,ieeexplore
10.1109/IECON.2013.6699377,Fuel Cells prognostics using echo state network,IEEE,Conferences,"One remaining technological bottleneck to develop industrial Fuel Cell (FC) applications resides in the system limited useful lifetime. Consequently, it is important to develop failure diagnostic and prognostic tools enabling the optimization of the FC. Among all the existing prognostics approaches, datamining methods such as artificial neural networks aim at estimating the process' behavior without huge knowledge about the underlying physical phenomena. Nevertheless, this kind of approach needs huge learning dataset. Also, the deployment of such an approach can be long (trial and error method), which represents a real problem for industrial applications where real-time complying algorithms must be developed. According to this, the aim of this paper is to study the application of a reservoir computing tool (the Echo State Network) as a prognostics system enabling the estimation of the Remaining Useful Life of a Proton Exchange Membrane Fuel Cell. Developments emphasize on the prediction of the mean voltage cells of a degrading FC. Accuracy and time consumption of the approach are studied, as well as sensitivity of several parameters of the ESN. Results appear to be very promising.",https://ieeexplore.ieee.org/document/6699377/,IECON 2013 - 39th Annual Conference of the IEEE Industrial Electronics Society,10-13 Nov. 2013,ieeexplore
10.1109/COASE.2017.8256157,Full automatic path planning of cooperating robots in industrial applications,IEEE,Conferences,"Parts made of carbon fiber reinforced plastics (CFRP) for airplane components can be so huge that a single industrial robot is no longer able to handle them, and cooperating robots are required. Manual programming of cooperating robots is difficult, but with large numbers of different sized and shaped cut-pieces, it is almost impossible. This paper presents an automated production system consisting of a camera for the precise detection of the position of each cut-piece and a collision-free path planner which can dynamically react to different positions for the transfer motions. The path is planned for multiple robots adhering to motion constrains, such as the requirement that the textile cut-piece must form a catenary which can change during transport. Additionally a technique based on machine learning has been implemented which correctly resolves redundancy for a linear axis during planning. Finally, all components are tested on a real robot system in industrial scale.",https://ieeexplore.ieee.org/document/8256157/,2017 13th IEEE Conference on Automation Science and Engineering (CASE),20-23 Aug. 2017,ieeexplore
10.1109/ICMLC.2017.8107749,Fuzzy logic based solar panel and battery control system design,IEEE,Conferences,"Photovoltaic systems, whether they are domestic, commercial or industrial often incorporate some forms of system protection. However, elaborate real-time fault detection is not defined for most such systems. To address this shortfall, a comprehensive photovoltaic installation system fault detection and control strategy is presented in this paper. The designed system is made up of fault detection in any of one of the photovoltaic system components that include the solar panel, charge controller, battery and inverter. The system also includes battery and user load current control. Fuzzy logic principle, due to its powerful non-linear problem solving capabilities is used in formulation of the fault detection and control algorithms as opposed to the classical method. This results in simpler, cheaper and faster hardware, which in this case is implemented on the PIC18F4550 microcontroller.",https://ieeexplore.ieee.org/document/8107749/,2017 International Conference on Machine Learning and Cybernetics (ICMLC),9-12 July 2017,ieeexplore
10.1109/FUZZ-IEEE.2014.6891715,Fuzzy uncertainty assessment in RBF Neural Networks using neutrosophic sets for multiclass classification,IEEE,Conferences,"In this paper we introduce a fuzzy uncertainty assessment methodology based on Neutrosophic Sets (NS). This is achieved via the implementation of a Radial Basis Function Neural-Network (RBF-NN) for multiclass classification that is functionally equivalent to a class of Fuzzy Logic Systems (FLS). Two types of uncertainties are considered: a) fuzziness and b) ambiguity, with both uncertainty types measured in each receptive unit (RU) of the hidden layer of the RBF-NN. The use of NS assists in the quantification of the uncertainty and formation of the rulebase; the resulting RBF-NN modelling structure proves to have enhanced transparency features to interpretation that enables us to understand the influence of each system parameter thorughout the parameter identification. The presented methodology is based on firstly constructing a neutrosophic set by calculating the associated fuzziness in each rule - and then use this information to train the RBF-NN; and secondly, an ambiguity measure that is defined via the truth and falsity measures related to each normalised consequence of the fuzzy rules within the RUs. In order to evaluate the individual ambiguity in the RUs and then the average ambiguity of the whole system, a neutrosophic set is constructed. Finally, the proposed methodology is tested against two case studies: a benchmark dataset problem and a real industrial case study. On both cases we demonstrate the effectiveness of the developed methodology in automatically creating uncertainty measures and utilising this new information to improve the quality of the trained model.",https://ieeexplore.ieee.org/document/6891715/,2014 IEEE International Conference on Fuzzy Systems (FUZZ-IEEE),6-11 July 2014,ieeexplore
10.1109/WCMEIM52463.2020.00032,Garbage Classification and Recognition Based on SqueezeNet,IEEE,Conferences,"In this study, an intelligent garbage classification and recognition system was deployed on the industry integrated computer with the I3-7100U processor and 2G memory. Considering the unit prediction time and prediction accuracy, SqueezeNet was selected as the classification network training model among ResNet, InceptionV3, and SqueezeNet. The pretraining SqueezeNet network model on the ImageNet-1000 dataset was used for transfer learning, and the model predication accuracy was improved by using image enhancement and Adam optimizer. The comparison between the comprehensive test set and the real garbage image showed that the model predication accuracy reached 87.7% after training, and the prediction time in the industrial integrated machine was less than 2 seconds, which met the needs of practical applications.",https://ieeexplore.ieee.org/document/9409502/,2020 3rd World Conference on Mechanical Engineering and Intelligent Manufacturing (WCMEIM),4-6 Dec. 2020,ieeexplore
10.1109/ICECCO.2018.8634719,Gas Turbine Fault Classification Based on Machine Learning Supervised Techniques,IEEE,Conferences,"Nowadays Machinery Diagnostic becomes a major part for many industrial applications. It allows to predict and prevent of breakages. An analysis of the trends in the development of power machines show that the most advanced installations can be created using gas turbine technologies. Quite justified, many energy specialists consider the XXI century - the century of gas turbine technologies. It is very important to prevent gas turbine failure. In this paper investigated machine learning classification techniques with further implementation for fault detection in gas turbine running data trends. Investigation was done for real gas compression station running parameters.",https://ieeexplore.ieee.org/document/8634719/,2018 14th International Conference on Electronics Computer and Computation (ICECCO),29 Nov.-1 Dec. 2018,ieeexplore
10.1109/ICNSC.2006.1673254,General Methodology for Action-Oriented Industrial Ecology Complex Systems Approach Applied to the Rotterdam Industrial Cluster,IEEE,Conferences,"A new approach for the understanding and shaping of the evolution of large scale socio-technical systems is presented. A proof-of-concept knowledge application has been developed, based on the industrial Rotterdam-Rijnmond case. The knowledge application includes the design of a model of industry-infrastructure evolution. Such networks are modeled via a system decomposition, formalization in an ontology and implementation of an agent based model. In simulation runs several network metrics are presented. The results provide insights in real world system behavior and show the validity and potential of the approach",https://ieeexplore.ieee.org/document/1673254/,"2006 IEEE International Conference on Networking, Sensing and Control",23-25 April 2006,ieeexplore
10.1109/MERCon.2019.8818945,Generalised Framework for Automated Conversational Agent Design via QFD,IEEE,Conferences,"Automated conversational agents are often regarded as the most promising method of responding to customer queries with minimum human intervention. Focus of the existing literature is mostly on technological innovations such as artificial intelligence and learning. A generalized text based real-time conversational agent or a chatbot development framework can be both conceptually and practically appealing, as a way to develop technologies to improve the responsiveness and customer friendliness of the chatbots. Different forms of technological advancements could help the firms to deploy right chatbot technology with regard to the user requirements in their organizations. This paper explores some product design ideas such as Analytic Hierarchy Process (AHP) and Quality Function Deployment (QFD) drawn from industrial engineering literature for chatbot development.",https://ieeexplore.ieee.org/document/8818945/,2019 Moratuwa Engineering Research Conference (MERCon),3-5 July 2019,ieeexplore
10.1109/AI4I.2018.8665690,Genetic Algorithm Based Parallelization Planning for Legacy Real-Time Embedded Programs,IEEE,Conferences,"Multicore platforms are pervasively deployed in many different sectors of industry. Hence, it is appealing to accelerate the execution through adapting the sequential programs to the underlying architecture to efficiently utilize the hardware resources, e.g., the multi-cores. However, the parallelization of legacy sequential programs remains a grand challenge due to the complexity of the program analysis and dynamics of the runtime environment. This paper focuses on parallelization planning in that the best parallelization candidates would be determined after the parallelism discovery in the target large sequential programs. In this endeavor, a genetic algorithm based method is deployed to help find an optimal solution considering different aspects from the task decomposition to solution evaluation while achieving the maximized speedup. We have experimented the proposed approach on industrial real time embedded application to reveal excellent speedup results.",https://ieeexplore.ieee.org/document/8665690/,2018 First International Conference on Artificial Intelligence for Industries (AI4I),26-28 Sept. 2018,ieeexplore
10.1109/CEC.2019.8790171,Genetic Algorithm for Topology Optimization of an Artificial Neural Network Applied to Aircraft Turbojet Engine Identification,IEEE,Conferences,"Artificial neural networks (ANN) has attracted attention of the academic community by the current progress that this technique has provided in speech recognition and digital media such as as image, video, audio, and signal processing. Some fields, as industrial process control and product development can be highly benefited by the development of techniques based on the proven potentialities of ANN models, allowing more accurate simulation, better adaptation to changing environments, and greater robustness in model-based fault diagnosis. Along with the advance of ANNs, there is a trend of open-source softwares use for soft computing which facilitates the access of the interested readers to implement their own codes and to explore other applications. Historically evolutionary algorithms such as the Genetic Algorithm (GA) have been implemented to evolve the architectures to search for solutions, in order to solve this fundamental issue that is still an open problem in the general case. Therefore, the present paper investigates the application of ANN to model the nonlinear aircraft turbojet engine through black-box approach. For that purpose it was used real-world measurements of aircraft engine's fuel and rotation as input and output, respectively. In order to facilitate the design, the ANN was optimized aiming to determine the best topology according to the one-step-ahead and free-run simulation. The results obtained encourage the use of automatically generated ANN architectures for dynamic system modeling.",https://ieeexplore.ieee.org/document/8790171/,2019 IEEE Congress on Evolutionary Computation (CEC),10-13 June 2019,ieeexplore
10.1109/TSP52935.2021.9522588,Genetic Programming based Identification of an Industrial Process,IEEE,Conferences,"In the field of industrial automation, it is essential to develop and improve mathematical methods that assist in obtaining more accurate models of real-world systems. In the following paper, a machine learning tool is applied to the problem of identifying a model of an industrial process. Symbolic regression and genetic programming are a successful combination of methods using which one can identify a nonlinear model in analytical form based on data collected from a process during routine operation. In this paper, a detailed description of the method implementation as well as necessary data preprocessing steps are presented. Then, the resulting models are validated on an industrial data set and compared on the basis of performance metrics with more classical methods and previous results achieved by the authors. Finally, the encountered problems in the realization of the methods are reflected upon.",https://ieeexplore.ieee.org/document/9522588/,2021 44th International Conference on Telecommunications and Signal Processing (TSP),26-28 July 2021,ieeexplore
10.1109/FUZZY.2006.1681714,Granular Auto-regressive Moving Average (grARMA) Model for Predicting a Distribution from Other Distributions. Real-world Applications,IEEE,Conferences,"Industrial products are often output in batches at discrete times. A batch gives rise to distributions of measurements, one distribution per variable of interest. There may be a need for modeling to predict a distribution from other distributions. This work represents a distribution by a fuzzy interval number (FIN) interpreted as an information granule. Based on vector lattice theory it is shown that the lattice F<sub>+</sub> of positive FINs is a cone in a non-linearly tunable, metric, linear space. In conclusion, a multivariate granular autoregressive moving average (grARMA) model is proposed for predicting a distribution from other distributions. A recursive neural network implementation is shown. We report preliminary results regarding two real-world applications including, first, industrial fertilizer production and, second, environmental pollution monitoring along seashore in northern Greece. The far-reaching potential of novel techniques is discussed.",https://ieeexplore.ieee.org/document/1681714/,2006 IEEE International Conference on Fuzzy Systems,16-21 July 2006,ieeexplore
10.1109/VETECS.2000.851509,"Graphical control of autonomous, virtual vehicles",IEEE,Conferences,"This paper presents some of the developments we made with the goal of allowing a friendly control and simulation of a large number of autonomous agents based in behavior in interactive real-time systems. Our work has been specially oriented to the simulation and control of autonomous vehicles and pedestrians in the preparation of scenarios to driving simulation experiments in the DriS simulator. Because every element is intrinsically autonomous, only a few of them are usually addressed to implement the desired study event. Also, because our model is autonomous and controllable, we can use the same model in the implementation of both environment traffic and controlled vehicles. Our scripting language is based in Grafcet, a well known graphical language used in the specification and programming of industrial controllers. Our technique allows the imposition of both short time orders and long time goals to each autonomous element. Orders can be triggered reactively using sensors that monitor the state of virtual traffic and configurable timers that generate all the necessary fixed and variable time events.",https://ieeexplore.ieee.org/document/851509/,VTC2000-Spring. 2000 IEEE 51st Vehicular Technology Conference Proceedings (Cat. No.00CH37026),15-18 May 2000,ieeexplore
10.1109/ETFA.2019.8868968,Hard Real-Time Capable OPC UA Server as Hardware Peripheral for Single Chip IoT Systems,IEEE,Conferences,"The fast semantics project examines the use of the OPC Unified Architecture (OPC UA) in embedded industrial systems and proposes the design of a customizable, hard real time capable OPC UA Intellectual Property Core (IP Core) for single chip computing plattforms. This allows using OPC UA in both novel energy efficient sensor applications and in state of the art field devices. These single chip OPC UA servers form the semantic data sources for future applications such as cloud based added value services or machine learning applications. This article presents the design alternatives and first synthesis results for the implementation of OPC UA servers in embedded systems.",https://ieeexplore.ieee.org/document/8868968/,2019 24th IEEE International Conference on Emerging Technologies and Factory Automation (ETFA),10-13 Sept. 2019,ieeexplore
10.1109/ETFA.2018.8502527,Holo Pick'n'Place,IEEE,Conferences,"In this paper we contribute to the research on facilitating industrial robot programming by presenting a concept for intuitive drag and drop like programming of pick and place tasks with Augmented Reality (AR). We propose a service-oriented architecture to achieve easy exchangeability of components and scalability with respect to AR devices and robot workplaces. Our implementation uses a HoloLens and a UR5 robot, which are integrated into a framework of RESTful web services. The user can drag recognized objects and drop them at a desired position to initiate a pick and place task. Although the positioning accuracy is unsatisfactory yet, our implemented prototype achieves most of the desired advantages to proof the concept.",https://ieeexplore.ieee.org/document/8502527/,2018 IEEE 23rd International Conference on Emerging Technologies and Factory Automation (ETFA),4-7 Sept. 2018,ieeexplore
10.1109/SMC42975.2020.9283392,Human-in-the-Loop Error Precursor Detection using Language Translation Modeling of HMI States,IEEE,Conferences,"Situational Awareness (SA) is paramount to ensuring operational safety in Nuclear Power Plant (NPP) and Commercial aviation industry. An increase in Human-in-the-loop (HITL) error rate may be indicative of reduced operator SA while undermining safety. In this paper, natural language processing (NLP) is applied for modelling industrial Human Machine Interface (HMI) state transitions as a means to detect operator HITL error precursors in real-time. A custom seq2seq encode-decoder deep-learning model design is implemented and evaluated using real-plant scenario dataset obtained from a NPP Operator training simulator. Results support NLP HMI state model may be employed to detect HITL error precursor within the desired N time-steps prior to an accident event.",https://ieeexplore.ieee.org/document/9283392/,"2020 IEEE International Conference on Systems, Man, and Cybernetics (SMC)",11-14 Oct. 2020,ieeexplore
10.1109/IDAACS-SWS.2018.8525503,Hybrid MAC for Low Latency Wireless Communication Enabling Industrial HMI Applications,IEEE,Conferences,"Wireless technologies are one of the core components of the future industrial applications. They provide flexibility and scalability to the factory floor in parallel with deployment cost reduction. In our paper, we concentrate on future-oriented human-machine interaction (HMI) applications such as augmented reality (AR) or mobile control. Based on their requirements, we provide an investigation of IEEE 802.11 channel access techniques with respect to their suitability for industrial applications.",https://ieeexplore.ieee.org/document/8525503/,2018 IEEE 4th International Symposium on Wireless Systems within the International Conferences on Intelligent Data Acquisition and Advanced Computing Systems (IDAACS-SWS),20-21 Sept. 2018,ieeexplore
10.1109/ICCCE50029.2021.9467162,ICS Cyber Attack Detection with Ensemble Machine Learning and DPI using Cyber-kit Datasets,IEEE,Conferences,"Digitization has pioneered to drive exceptional changes across all industries in the advancement of analytics, automation, and Artificial Intelligence (AI) and Machine Learning (ML). However, new business requirements associated with the efficiency benefits of digitalization are forcing increased connectivity between IT and OT networks, thereby increasing the attack surface and hence the cyber risk. Cyber threats are on the rise and securing industrial networks are challenging with the shortage of human resource in OT field, with more inclination to IT/OT convergence and the attackers deploy various hi-tech methods to intrude the control systems nowadays. We have developed an innovative real-time ICS cyber test kit to obtain the OT industrial network traffic data with various industrial attack vectors. In this paper, we have introduced the industrial datasets generated from ICS test kit, which incorporate the cyber-physical system of industrial operations. These datasets with a normal baseline along with different industrial hacking scenarios are analyzed for research purposes. Metadata is obtained from Deep packet inspection (DPI) of flow properties of network packets. DPI analysis provides more visibility into the contents of OT traffic based on communication protocols. The advancement in technology has led to the utilization of machine learning/artificial intelligence capability in IDS ICS SCADA. The industrial datasets are pre-processed, profiled and the abnormality is analyzed with DPI. The processed metadata is normalized for the easiness of algorithm analysis and modelled with machine learning-based latest deep learning ensemble LSTM algorithms for anomaly detection. The deep learning approach has been used nowadays for enhanced OT IDS performances.",https://ieeexplore.ieee.org/document/9467162/,2021 8th International Conference on Computer and Communication Engineering (ICCCE),22-23 June 2021,ieeexplore
10.1109/MetroInd4.0IoT51437.2021.9488447,IOT data-driven experimental process optimisation for kevlar fiberglass components for aeronautic,IEEE,Conferences,"This paper describes the work carried out during the PROOF experiment (IOT data-driven experimental PROcess Optimization for kevlar Fiberglass components for aeronautic), winner of the second open call of the MIDIH EU project. The main objectives of the experiment are the integration of smart sensing devices with the Energy@Work IoT gateways and the development of cloudified innovative data-driven methodologies and data analytics tools to support process optimization in the production of hybrid composite material parts for the aeronautical sector. Collection of real-time production-data from multiple sensors with several industrial protocols and data transfer to the MIDIH project platform has been performed adopting the IoT gateway developed by Energy@Work, following MIDIH reference architecture for advanced data processing and visualization (e.g., Fiware Orion Context Broker, Apache Flink and Fiware Knowage) by using MQTT protocol. Then, historical and new acquired data has been analysed using advanced clustering techniques and trends, with the purpose to allow a novel CPS-based predictive system on the production process. Machine-Learning algorithms and visualisations (GUI based on Fiware Knowage) in real operating conditions have been used to validate the performance and assess the outcome. Finally, thanks to the implementation of specific optimization rules, able to process data gathered from the sensor network, a framework for distributed processing engine has been exploited by (i) generating tips for energy efficiency and process optimization and (ii) providing different type of alarms based on expected consumptions, resulting in concrete support to production managers for the improvement of the whole production value chain.",https://ieeexplore.ieee.org/document/9488447/,2021 IEEE International Workshop on Metrology for Industry 4.0 & IoT (MetroInd4.0&IoT),7-9 June 2021,ieeexplore
10.1109/ICA-ACCA.2018.8609763,"Identification and Process Control for MISO systems, with Artificial Neural Networks and PID Controller",IEEE,Conferences,"Industrial processes with multiple input and single manipulated variables are very complex systems to control in automatic models. Such is the case with processes related to gases extraction or transport phenomena. The present research is focused on the development of a control algorithm (automatic control strategy), based on artificial neural networks, to identify an industrial process by using process historical records, as well as knowledge from the operation itself. The output of the identification stage feeds a classic PID controller to perform control actions (hybrid controller). Here, an actuator or final control element is modeled, estimating its space-state dynamic equation. With the estimated model, a local control loop is conformed controlling the main process or manipulated variable. For this, the process of gases transport in a copper smelter plant was chosen, where the necessary data and scenarios for the proposed control algorithm testing was obtained. This application attempts to present a solution to problems inherent to manual control, multiple key variables coexisting in a system, mechanical stress in equipment due to manual actions, etc. The control strategy is based on a computer simulation made with real process data, showing improvement of the transient periods in the final actuators due to control signals, as well as establishing that these kinds of technologies could be implemented in both, an existing plant hardware/software or in a conventional control system.",https://ieeexplore.ieee.org/document/8609763/,2018 IEEE International Conference on Automation/XXIII Congress of the Chilean Association of Automatic Control (ICA-ACCA),17-19 Oct. 2018,ieeexplore
10.1109/ICDMW.2017.40,Identifying Irregular Power Usage by Turning Predictions into Holographic Spatial Visualizations,IEEE,Conferences,"Power grids are critical infrastructure assets that face non-technical losses (NTL) such as electricity theft or faulty meters. NTL may range up to 40% of the total electricity distributed in emerging countries. Industrial NTL detection systems are still largely based on expert knowledge when deciding whether to carry out costly on-site inspections of customers. Electricity providers are reluctant to move to large-scale deployments of automated systems that learn NTL profiles from data due to the latter's propensity to suggest a large number of unnecessary inspections. In this paper, we propose a novel system that combines automated statistical decision making with expert knowledge. First, we propose a machine learning framework that classifies customers into NTL or non-NTL using a variety of features derived from the customers' consumption data. The methodology used is specifically tailored to the level of noise in the data. Second, in order to allow human experts to feed their knowledge in the decision loop, we propose a method for visualizing prediction results at various granularity levels in a spatial hologram. Our approach allows domain experts to put the classification results into the context of the data and to incorporate their knowledge for making the final decisions of which customers to inspect. This work has resulted in appreciable results on a real-world data set of 3.6M customers. Our system is being deployed in a commercial NTL detection software.",https://ieeexplore.ieee.org/document/8215672/,2017 IEEE International Conference on Data Mining Workshops (ICDMW),18-21 Nov. 2017,ieeexplore
10.1109/NDS.2017.8070626,"Image-driven, model-free control of repetitive processes based on machine learning",IEEE,Conferences,"An image-driven, model-free approach to design control systems for a large class of industrial process is proposed. A mathematical model of the process is replaced by sequences of subsequent images which play the role of the process (plant) states. The length of this sequences depends on the speed of the process dynamics and on the frame rate. Firstly, a learning sequence of the system states is collected and then, it is used for classifying (clustering) its states. A decision of the control system is attached by an expert to each class (cluster) of the states. At the implementation stage images from a camera in the loop are collected into subsequences corresponding to the system states, then they are classified and a control action corresponding to a class at hand is undertaken. This general idea is exemplified by the case study of a laser power control in an additive manufacturing, which is a repetitive process. A tree-like, hierarchical classifier is proposed in order to recognize the process states, each consisting of three consecutive images. Its performance is tested on real-life images from the process of laser cladding additive manufacturing.",https://ieeexplore.ieee.org/document/8070626/,2017 10th International Workshop on Multidimensional (nD) Systems (nDS),13-15 Sept. 2017,ieeexplore
10.1109/I2MTC43012.2020.9129119,Impact of Noise on Machine Learning-based Condition Monitoring Applications: a Case Study,IEEE,Conferences,"In the paper, application of Machine Learning (ML) techniques for the continuous monitoring of the health status of mild mission-critical industrial equipment is considered. A meaningful real-life case-study is presented in order to show how acquisition conditions may severely impact on the performance of the system. In particular, it is shown that a wrong estimate of noise effects in the deployed system may induce a wrong choice of the best features feeding the ML monitoring algorithm, hence affecting accuracy of the target devices. The discussed results may provide an useful guidance to the practitioner in the field during the design phase of ML-based devices depending of the equipment specifications and environmental conditions.",https://ieeexplore.ieee.org/document/9129119/,2020 IEEE International Instrumentation and Measurement Technology Conference (I2MTC),25-28 May 2020,ieeexplore
10.1109/CIIMA50553.2020.9290302,Implementación de SCADA a través del protocolo MQTT,IEEE,Conferences,"This document describes an implementation of a SCADA system powered by MQTT &amp; OPC-UA protocols and hosted within the Google Cloud Platform system. This combination allows to have integrated, scalable, secure and reliable industrial communications while allowing real-time data acquisition and sensor feed that can then be used in real-time OEE tracking or predictive maintenance models, to name some examples. This in line with the Industry 4.0 initiatives mainly fueled by data and machine learning autonomous systems.",https://ieeexplore.ieee.org/document/9290302/,2020 IX International Congress of Mechatronics Engineering and Automation (CIIMA),4-6 Nov. 2020,ieeexplore
10.1109/ICPHYS.2019.8780271,Implementation of Industrial Cyber Physical System: Challenges and Solutions,IEEE,Conferences,"The Industry Internet of Things (IIoT) and the Industry Cyber-Physical System (ICPS) for real industry are becoming vitally necessary in the smart manufacturing environment. Very large number of intelligent sensors are being available generating an exploding amount of data. Several issues come with the big data in real industry, including the a grand-scale connected network construction with the data security and access protocol issues, data quality with considerable noise when gathered from industrial factories, efficient data storage, smart interconnection with cloud services, and real-time analytics requirements. This paper proposes an integrated CPS based architecture for smart manufacturing and provides the deployment details, addressing all the potential problems in an appropriate way. It has been successfully implemented in a real industry environment, and won the Best Industry Application of IoT at the BigInsights Data &amp; AI Innovation Awards.",https://ieeexplore.ieee.org/document/8780271/,2019 IEEE International Conference on Industrial Cyber Physical Systems (ICPS),6-9 May 2019,ieeexplore
10.1109/EUSIPCO.2016.7760546,Implementation of efficient real-time industrial wireless interference identification algorithms with fuzzified neural networks,IEEE,Conferences,Real-time industrial wireless systems sharing a crowded spectrum band require active coexistence management measures. Identification of wireless interference is a key issue for this purpose. We propose an efficient implementation of a wireless interference identification (WII) approach called neuro-fuzzy signal classifier (NFSC). The implementation in Matlab / SIMULINK is based upon the wideband software defined radio Ettus USRP N210. The implementation is evaluated in six selected heterogeneous and harsh industrial scenarios within the license-free 2.4-GHz-ISM radio band with variously combined standard wireless technologies IEEE 802.11g-based WLAN and Bluetooth. The evaluation of the NFSC was performed with a binary classification test with the statistical measurement metrics sensitivity and specificity.,https://ieeexplore.ieee.org/document/7760546/,2016 24th European Signal Processing Conference (EUSIPCO),29 Aug.-2 Sept. 2016,ieeexplore
10.1109/ETFA.2005.1612698,Implementation of neural networks in foundation fieldbus environment using standard function blocks,IEEE,Conferences,"This paper presents an implementation approach of artificial neural networks in industrial network environment, through the use of function blocks standardized by field-bus foundation (FF). This enables the implementation of a wide range of applications that involve this mathematical tool, such as intelligent control, failure detection, etc in standard FF system. For validation propose, some examples are presented from of real experiments",https://ieeexplore.ieee.org/document/1612698/,2005 IEEE Conference on Emerging Technologies and Factory Automation,19-22 Sept. 2005,ieeexplore
10.1109/ICPR48806.2021.9412842,Improved anomaly detection by training an autoencoder with skip connections on images corrupted with Stain-shaped noise,IEEE,Conferences,"In industrial vision, the anomaly detection problem can be addressed with an autoencoder trained to map an arbitrary image, i.e. with or without any defect, to a clean image, i.e. without any defect. In this approach, anomaly detection relies conventionally on the reconstruction residual or, alternatively, on the reconstruction uncertainty. To improve the sharpness of the reconstruction, we consider an autoencoder architecture with skip connections. In the common scenario where only clean images are available for training, we propose to corrupt them with a synthetic noise model to prevent the convergence of the network towards the identity mapping, and introduce an original Stain noise model for that purpose. We show that this model favors the reconstruction of clean images from arbitrary real-world images, regardless of the actual defects appearance. In addition to demonstrating the relevance of our approach, our validation provides the first consistent assessment of reconstruction-based methods, by comparing their performance over the MVTec AD dataset [1], both for pixel- and image-wise anomaly detection. Our implementation is available at https://github.com/anncollin/AnomalyDetection-Keras.",https://ieeexplore.ieee.org/document/9412842/,2020 25th International Conference on Pattern Recognition (ICPR),10-15 Jan. 2021,ieeexplore
10.1109/ICAICA50127.2020.9182590,Improvement of Model Simplification Algorithm Based on LOD and Implementation of WebGL,IEEE,Conferences,"With the continuous progress of computer graphics, virtual reality and other technologies, 3D models in the field of industrial production have become more and more sophisticated, and the triangular surfaces need to be rendered are more than one million, which poses a great challenge to the storage, transmission and computing power of computers. Therefore, in order to adapt to the current performance of the computer while taking into account the rendering effect of the model, the Level-of-Detail(LOD) technology has been spawned. Industrial models tend to have complex structures and need to be displayed accurately in the rendering process, and in the case of a large number of holes in the model, the common algorithm is difficult to maintain the topology of the model well. Therefore, the article uses the edge collapse algorithm. To improve it, an algorithm that uses the mean deviation to guide the simplification process of edge collapse is proposed. While ensuring sufficient triangular mesh simplification, the topology of complex industrial models is maintained.",https://ieeexplore.ieee.org/document/9182590/,2020 IEEE International Conference on Artificial Intelligence and Computer Applications (ICAICA),27-29 June 2020,ieeexplore
10.1109/ICIT.2015.7125235,Improving accuracy of long-term prognostics of PEMFC stack to estimate remaining useful life,IEEE,Conferences,"Proton Exchange Membrane Fuel cells (PEMFC) are energy systems that facilitate electrochemical reactions to create electrical energy from chemical energy of hydrogen. PEMFC are promising source of renewable energy that can operate on low temperature and have the advantages of high power density and low pollutant emissions. However, PEMFC technology is still in the developing phase, and its large-scale industrial deployment requires increasing the life span of fuel cells and decreasing their exploitation costs. In this context, Prognostics and Health Management of fuel cells is an emerging field, which aims at identifying degradation at early stages and estimating the Remaining Useful Life (RUL) for life cycle management. Indeed, due to prognostics capability, the accurate estimates of RUL enables safe operation of the equipment and timely decisions to prolong its life span. This paper contributes data-driven prognostics of PEMFC by an ensemble of constraint based Summation Wavelet-Extreme Learning Machine (SW-ELM) algorithm to improve accuracy and robustness of long-term prognostics. The SW-ELM is used for ensemble modeling due to its enhanced applicability for real applications as compared to conventional data-driven algorithms. The proposed prognostics model is validated on run-to-failure data of PEMFC stack, which had the life span of 1750 hours. The results confirm capability of the prognostics model to achieve accurate RUL estimates.",https://ieeexplore.ieee.org/document/7125235/,2015 IEEE International Conference on Industrial Technology (ICIT),17-19 March 2015,ieeexplore
10.23919/FPL.2017.8056761,In-network online data analytics with FPGAs,IEEE,Conferences,"The growth of sensor technology, communication systems and computation have led to vast quantities of data being available for relevant parties to utilise. Applications such as the monitoring and analysis of industrial equipment, smart surveillance, and fraud detection rely on the `real-time' analysis of time sensitive data gathered from distributed sources. A variety of processing tasks, such as filtering, aggregation, machine learning algorithms, or other transformations to be carried out on this data in order to extract value from it. Centralised computation strategies are often deployed in these scenarios, with the majority of the data being forwarded though the network to a datacenter environment, typically due to the lack of required computational or storage resources at the leaves of the network, and data from other sources or historical data being required. This approach has also traditionally been viewed as more scalable, as resources can be augmented through the addition of extra compute hardware and cloud services.",https://ieeexplore.ieee.org/document/8056761/,2017 27th International Conference on Field Programmable Logic and Applications (FPL),4-8 Sept. 2017,ieeexplore
10.1109/IRCE50905.2020.9199256,Industrial Implementation and Performance Evaluation of LSD-SLAM and Map Filtering Algorithms for Obstacles Avoidance in a Cooperative Fleet of Unmanned Aerial Vehicles,IEEE,Conferences,"In this paper we present an industrial implementation and performance evaluation of the problem of obstacles detection by drones using autonomous navigation systems. The software module that has been developed as well as the tests conducted are part of a large industrial R&amp;D Vitrociset project called SWARM: an AI-Enabled Command and Control (C&amp;C) system, able to execute and review ISR missions for mini/micro cooperative fleets of heterogeneous UAVs. The presented software module, that is currently under test, has been developed to recognize obstacles and drive correctly the drones, using images acquired by low cost RGB video cameras, whose features of lightness and reduced size allow them to be installed on mini/micro UAVs. Moreover, this setup does not require special calibration and preconfiguration processes like the ones necessary for example using stereo video camera systems. The real-time recognition of obstacles in the surrounding environment has been obtained and evaluated through the implementation, performance evaluation and tests of the LSD-SLAM and map filtering algorithms; the core of the study has been realized starting from the integration of these algorithms with a simulated drone in a synthetic environment. The areas of interest have been identified through the filtering of a computer generated map: the module was then integrated into the SWARM project platform, allowing the control of a single drone's movement and making it ready for use in a cooperative fleet environment.",https://ieeexplore.ieee.org/document/9199256/,2020 3rd International Conference on Intelligent Robotic and Control Engineering (IRCE),10-12 Aug. 2020,ieeexplore
10.1109/ICAICA52286.2021.9497973,Industrial Internet Security Protection Based on an Industrial Firewall,IEEE,Conferences,"A crucial step in the development of a security system for the industrial Internet is the implementation of an industrial firewall as the first line of defense for the multi-layer defense-in-depth system and an important safeguard for industrial network security. In the design, development, deployment, application, and maintenance of industrial firewalls, the firewall performance and architecture are vital aspects. This thesis focuses on the analysis and discussion of the requirements and abilities of an industrial firewall in terms of adaptability, network isolation, industrial communication protocol identification, filtering and analysis, real-time performance and reliability, and self-protection.",https://ieeexplore.ieee.org/document/9497973/,2021 IEEE International Conference on Artificial Intelligence and Computer Applications (ICAICA),28-30 June 2021,ieeexplore
10.1109/GIOTS49054.2020.9119497,Industrial IoT and Digital Twins for a Smart Factory : An open source toolkit for application design and benchmarking,IEEE,Conferences,"The rapid evolution of digital technology and designed intelligence, such as the Internet of Things (IoT), Big data analytics, Artificial Intelligence (AI), Cyber Physical Systems (CPS), has been a catalyst for the 4th industrial revolution (known as industry 4.0). Among other, the two key state-of-the-art concepts in Industry 4.0, are Industrial IoT (IIoT) and digital twins. IIoT facilitates real-time data acquisition, processing and analytics over large amount of sensor data streams produced by sensors installed within a smart factory, while the `digital twin' concept aims to enable smart factories via the digital replication or representation of physical machines, processes, people in cyber-space. This paper explores the capability of present-state open-source platforms to collectively achieve digital twin capabilities, including IoT real-time data acquisition, virtual representation, analytics, and visualisation. The aim of this work is to `close the gap' between research and implementation, through a collective open source IoT and Digital Twin architecture. The performance of the open-source architecture in this work, is demonstrated in a use-case utilising industry `open data', and is bench-marked with universal testing tools.",https://ieeexplore.ieee.org/document/9119497/,2020 Global Internet of Things Summit (GIoTS),3-3 June 2020,ieeexplore
10.1109/CASE48305.2020.9216902,Industrial Robot Grasping with Deep Learning using a Programmable Logic Controller (PLC),IEEE,Conferences,"Universal grasping of a diverse range of previously unseen objects from heaps is a grand challenge in e-commerce order fulfillment, manufacturing, and home service robotics. Recently, deep learning based grasping approaches have demonstrated results that make them increasingly interesting for industrial deployments. This paper explores the problem from an automation systems point-of-view. We develop a robotics grasping system using Dex-Net, which is fully integrated at the controller level. Two neural networks are deployed on a novel industrial AI hardware acceleration module close to a PLC with a power footprint of less than 10 W for the overall system. The software is tightly integrated with the hardware allowing for fast and efficient data processing and real-time communication. The success rate of grasping an object form a bin is up to 95% with more than 350 picks per hour, if object and receptive bins are in close proximity. The system was presented at the Hannover Fair 2019 (world's largest industrial trade fair) and other events, where it performed over 5,000 grasps per event.",https://ieeexplore.ieee.org/document/9216902/,2020 IEEE 16th International Conference on Automation Science and Engineering (CASE),20-21 Aug. 2020,ieeexplore
10.1109/ICYCS.2008.34,Influence Graph based Task Decomposition and State Abstraction in Reinforcement Learning,IEEE,Conferences,"Task decomposition and state abstraction are crucial parts in reinforcement learning. It allows an agent to ignore aspects of its current states that are irrelevant to its current decision, and therefore speeds up dynamic programming and learning. This paper presents the SVI algorithm that uses a dynamic Bayesian network model to construct an influence graph that indicates relationships between state variables. SVI performs state abstraction for each subtask by ignoring irrelevant state variables and lower level subtasks. Experiment results show that the decomposition of tasks introduced by SVI can significantly accelerate constructing a near-optimal policy. This general framework can be applied to a broad spectrum of complex real world problems such as robotics, industrial manufacturing, games and others.",https://ieeexplore.ieee.org/document/4708962/,2008 The 9th International Conference for Young Computer Scientists,18-21 Nov. 2008,ieeexplore
10.1109/MetroInd4.0IoT48571.2020.9138310,Infrared Thermography and Image Processing applied on Weldings Quality Monitoring,IEEE,Conferences,"The present paper proposes a methodological approach to evaluate welding quality in the context of tank production. In particular, infrared thermography was adopted to control the structural homogeneity achieved after welding. The analysis was implemented by applying the K-Means algorithm, morphological image processing and artificial neural network based on the Long Short Term Memory - LSTM - technique. The adopted approach was chosen to perform different image post-processing analysis and therefore highlights, identifies, and quantifies welding inhomogeneities, as cracks and defects. The work was developed within the research framework of an industrial project. The proposed approach could be implemented in inline production systems which integrate an artificial intelligence processor for real time quality monitoring.",https://ieeexplore.ieee.org/document/9138310/,2020 IEEE International Workshop on Metrology for Industry 4.0 & IoT,3-5 June 2020,ieeexplore
10.1109/MoRSE48060.2019.8998694,Integration of Blockchains with Management Information Systems,IEEE,Conferences,"In the era of the fourth industrial revolution (In-dustry 4.0), many Management Information Systems (MIS) integrate real-time data collection and use technologies such as big data, machine learning, and cloud computing, to foster a wide range of creative innovations, business improvements, and new business models and processes. However, the integration of blockchain with MIS offers the blockchain trilemma of security, decentralisation and scalability. MIS are usually Web 2.0 client-server applications that include the front end web systems and back end databases; while blockchain systems are Web 3.0 decentralised applications. MIS are usually private systems that a single party controls and manages; while blockchain systems are usually public, and any party can join and participate. This paper clarifies the key concepts and illustrates with figures, the implementation of public, private and consortium blockchains on the Ethereum platform. Ultimately, the paper presents a framework for building a private blockchain system on the public Ethereum blockchain. Then, integrating the Web 2.0 client-server applications that are commonly used in MIS with Web 3.0 decentralised blockchain applications.",https://ieeexplore.ieee.org/document/8998694/,"2019 International Conference on Mechatronics, Robotics and Systems Engineering (MoRSE)",4-6 Dec. 2019,ieeexplore
10.1109/MSN48538.2019.00085,Intelligent Log Analysis System for Massive and Multi-Source Security Logs: MMSLAS Design and Implementation Plan,IEEE,Conferences,"In the Internet of Things and industrial controlnetwork servers, a large number of logs will be formed everymoment. This log information, as an important basis for eventrecording and security auditing, provides important informa-tion for identifying threat sources, identifying threat degreeand judging threat impact. However, the current security loganalysis system usually only standardizes the logs separately, and lacks the correlation analysis of the information fromvarious sources. Thus, this paper presents an intelligent loganalysis system for massive and multi-source security logs-MMSLAS(Massive and Multi-Source Security Log AnalysisSystem). In the log analysis module, the system integratesbusiness rule analysis and behavior analysis and additionallyadopts a machine learning-based analysis method, which fullyexploits the correlation between security logs and realizes thecomprehensive analysis of multi-source security logs. At thesame time, the distributed architecture scheme is also sufficientto cope with the system load caused by a large amount ofdata. The final implementation results show that MMSLAScan quickly locate the improper behavior in the log, and detectthe abnormal requests in advance according to the analysis ofthe behavior trajectory.",https://ieeexplore.ieee.org/document/9066044/,2019 15th International Conference on Mobile Ad-Hoc and Sensor Networks (MSN),11-13 Dec. 2019,ieeexplore
10.1109/GCCE.2014.7031179,Intelligent PID control based on PSO type NN for USM,IEEE,Conferences,"Ultrasonic Motors (USMs) are a kind of actuators with attractive features. Owing to the features, they are expected to be applied widely in industrial fields. Especially, since they work well in or near MRI environment, they are expected to play more important roles in medical and welfare area. In this research, for the control of USM, an intelligent PID control method using Neural Network (NN) combined with type Particle Swarm Optimization (PSO) is developed. In the method, the intelligent controller is designed based on variable gain type PID control using NN. The learning of the NN unit is implemented by the PSO. The PID gains are adjusted by the intelligent controller in real-time environment. The effectiveness of the method is confirmed by experiments.",https://ieeexplore.ieee.org/document/7031179/,2014 IEEE 3rd Global Conference on Consumer Electronics (GCCE),7-10 Oct. 2014,ieeexplore
10.1109/CCECE.2009.5090245,Intelligent speed controllers for IPM motor drives,IEEE,Conferences,"In this paper the comparative performances of the interior permanent magnet synchronous motor (IPMSM) drive system using proportional integral (PI) controller, proportional integral derivative (PID) controller, adaptive neural network (NN) controller, and wavelet based multiresolution proportional integral derivative (MRPID) controller are presented. In the proposed wavelet based MRPID controller, the discrete wavelet transform is used to decompose the error between actual and command speeds into different frequency components at various scales. The wavelet transformed coefficients of different scales are scaled by their respective gains, and then are added together to generate the control signal. The performances of the IPMSM drive system are investigated in simulation and experiments at different dynamic operating conditions. The vector control scheme of the conventional and proposed speed controllers based IPMSM drive system is successfully implemented in real-time using the digital signal processor board ds1102 on the laboratory 1-hp IPMSM. The simulation and laboratory test results confirm the superiority of the proposed wavelet based MRPID controller over the conventional speed controllers for wide spread applications in high performance industrial motor drive systems.",https://ieeexplore.ieee.org/document/5090245/,2009 Canadian Conference on Electrical and Computer Engineering,3-6 May 2009,ieeexplore
10.1109/CIMCA.2006.133,International Conference on Computational Inteligence for Modelling Control and Automation and International Conference on Intelligent Agents Web Technologies and International Commerce - Title,IEEE,Conferences,"The following topics are dealt with: intelligent agents and ontologies; data mining, knowledge discovery and decision making; intelligent systems; Web technologies and Web services; virtual reality and games; image processing and image understanding techniques; adaptive control and automation; modelling, prediction and control; multi-agent systems and computational intelligence; agent systems, personal assistant agents and profiling; fuzzy systems for industrial automation; control strategies; neural network applications; clustering, classification, data mining and risk analysis; dynamics systems; innovative control systems, hardware design and implementation; robotics and automation; e-business, e-commerce, innovative Web applications; Web databases; diagnosis and medical applications; learning systems; optimization, hybrid systems, genetic algorithms and evolutionary computation control applications; online learning and ERP; knowledge acquisition and classification; nanomechatronics; simulation and control; mobile network applications; information retrieval; Bayesian networks; human computer interaction; cognitive science; mobile agents; knowledge management; intelligent control; e-search and navigation; security.",https://ieeexplore.ieee.org/document/4052645/,2006 International Conference on Computational Inteligence for Modelling Control and Automation and International Conference on Intelligent Agents Web Technologies and International Commerce (CIMCA'06),28 Nov.-1 Dec. 2006,ieeexplore
10.1109/WF-IoT51360.2021.9595035,Interpretable Multi-Step Production Optimization Utilizing IoT Sensor Data,IEEE,Conferences,"In an industrial manufacturing process, such as petroleum, chemical, and food processing, with the deployment of thousands of sensors in the plants, we have the chance to provide real-time onsite management for the processes. Beyond the real-time status update, utilizing vast IoT data and creating machine learning and optimization models provide us with intelligent business recommendations. Those are used by the site engineers and managers to make real-time decisions in a situation with multiple conflicting operational and business goals. Those goals include maximizing financial gain, minimizing costs, limiting the usage of certain raw materials or additives, decreasing environmental impact, and more. When formalizing these decision-making tasks, often there is no prior knowledge of compromise between the conflicting goals. That poses a challenge to generate a proper objective function. In this paper, we create a Multi-Step optimization process to address this uncertainty of selecting proper objectives and their preferences. Instead of using an explicit trade-off to create a single weighted objective function (as a traditional approach) and rely on a single attempt to find the optimal solution, we decompose this problem into multiple steps. In each step, we optimize only one objective from one KPI with an exact semantic meaning. We demonstrate the usability of the approach using a practical application from an oil sands processing facility, provide modeling results focusing on the response to business priorities, performance, and interpretability. The multi-step approach presents the convergence of the target goal with an outcome KPI with comparison for each step to illustrate the enhanced interpretability.",https://ieeexplore.ieee.org/document/9595035/,2021 IEEE 7th World Forum on Internet of Things (WF-IoT),14 June-31 July 2021,ieeexplore
10.1109/ISSCS52333.2021.9497411,Inverted Pendulum Control with a Robotic Arm using Deep Reinforcement Learning,IEEE,Conferences,"Inverted pendulum control is a benchmark control problem that researchers have used to test the new control strategies over the past 50 years. Deep Reinforcement Learning Algorithm is used recently on the inverted pendulum on a straightforward form. The inverted pendulum had only one degree of freedom and was moving on a plane. This paper demonstrates a successful implementation of a deep reinforcement learning algorithm on an inverted pendulum that rotates freely on a spherical joint with an industrial 6 degrees freedom robot arm. This research used the Deep Reinforcement Learning algorithm in Robot Operating System (ROS) and Gazebo Simulation. Experimental results show that the proposed method achieved promising outputs and reaches the control objectives. We were able to control the inverted pendulum upward for 30 and 20 seconds in two case studies. Two other significant novelties in this research are using an inertial measurement unit (IMU) on the tip of the pendulum, that will facilitate implementation on the real robot for future work and different reward functions in comparing to past publications that enable continuous learning and mastering control in a vertical position",https://ieeexplore.ieee.org/document/9497411/,"2021 International Symposium on Signals, Circuits and Systems (ISSCS)",15-16 July 2021,ieeexplore
10.1109/PARC49193.2020.236616,IoT and Cloud Computing based Smart Water Metering System,IEEE,Conferences,"This paper focuses on the developmental and implementation methodology of smart water meter based on Internet of Things (IoT) and Cloud computing equipped with machine learning algorithms, to differentiate between normal and excessive water usage at industrial, domestic and all other sectors having an abundance of water usage, both for Indian and worldwide context. Recognizing that intelligent metering of water has the potential to alter customer engagement of water usage in urban and rural water supplies, this paper fosters for sustainable water management, a need of the present. With shrinking reserves of clean water resources worldwide, it is becoming cumbersome to cater for this resource to masses in the coming years on a consistent basis. Using our smart water meter, water resources can be managed efficiently and an optimum use could save water for the future generations. Sensors will provide for real time monitoring of hydraulic data, automated control and alarming from Cloud platform in case of events such as water leakages, excessive usage, etc. Analysis of the same will help in taking meaningful actions. Thus we do propose for a smart water metering technology that can be utilized by Indian citizens, and worldwide, to curb wastage of water. With an ease of monitoring and visualization of the data through the Cloud platform combined with machine learning based tools to detect excess water consumption, the server-less architecture we propose can be easily adopted and implemented in a large scale.",https://ieeexplore.ieee.org/document/9087024/,2020 International Conference on Power Electronics & IoT Applications in Renewable Energy and its Control (PARC),28-29 Feb. 2020,ieeexplore
10.1109/CCGrid51090.2021.00075,IoTwins: Design and Implementation of a Platform for the Management of Digital Twins in Industrial Scenarios,IEEE,Conferences,"With the increase of the volume of data produced by IoT devices, there is a growing demand of applications capable of elaborating data anywhere along the IoT-to-Cloud path (Edge/Fog). In industrial environments, strict real-time constraints require computation to run as close to the data origin as possible (e.g., IoT Gateway or Edge nodes), whilst batch-wise tasks such as Big Data analytics and Machine Learning model training are advised to run on the Cloud, where computing resources are abundant. The H2020 IoTwins project leverages the digital twin concept to implement virtual representation of physical assets (e.g., machine parts, machines, production/control processes) and deliver a software platform that will help enterprises, and in particular SMEs, to build highly innovative, AI-based services that exploit the potential of IoT/Edge/Cloud computing paradigms. In this paper, we discuss the design principles of the IoTwins reference architecture, delving into technical details of its components and offered functionalities, and propose an exemplary software implementation.",https://ieeexplore.ieee.org/document/9499575/,"2021 IEEE/ACM 21st International Symposium on Cluster, Cloud and Internet Computing (CCGrid)",10-13 May 2021,ieeexplore
10.1109/PerComWorkshops51409.2021.9430865,Keyword Spotting for Industrial Control using Deep Learning on Edge Devices,IEEE,Conferences,"Spoken commands promise unique advantages for the control of industrial machinery. Operators are enabled to keep their eyes on safety critical aspects of the process at all times and are free to use their hands in other parts of the process, instead of remote control. Current keyword spotting systems are prone to misunderstanding spoken utterances, especially in noisy environments, and are commonly deployed as non-realtime cloud services. Consequently, these systems can not be trusted with safety critical industrial control. We adapt a DS-CNN and a CNN for keyword spotting and use augmented training data, including real industrial noise, to increase their robustness. Furthermore, we apply post-training quantization and analyze the performance of both networks using multiple embedded systems, including a Google Edge TPU. We carry out a systematic analysis of accuracies, memory footprint and inference times using different combinations of data augmentations, hardware platforms, and quantizations. We show that augmented training data increases the inference accuracy in noisy environments by up to 20 %. Among others, this is demonstrated using an integer quantized network with a memory footprint of 0.57 MByte, reaching inference speeds of less than 5 ms on an embedded CPU and less than 1 ms on the Edge TPU. The results show that keyword spotting for industrial control is feasible on embedded systems and that the training data augmentation has a significant impact on the robustness in challenging environments.",https://ieeexplore.ieee.org/document/9430865/,2021 IEEE International Conference on Pervasive Computing and Communications Workshops and other Affiliated Events (PerCom Workshops),22-26 March 2021,ieeexplore
10.23919/ACC.1988.4789792,Knowledge-Based Approach to Real-Time Supervisory Control,IEEE,Conferences,"Increasing complexity of industrial plants necessitates the usage of advanced programming techniques in process control. Artificial Intelligence programming offers new opportunities in constructing complex software systems. This paper describes an approach which has been used to build Intelligent Process Control System (IPCS). IPCS expand the conventional process control systems with an additional, ""knowledge-based layer"", where declarative programming methods are used extensively. The knowledge-based components represent a model of the process from multiple viewpoints and the simulator, monitor, control, diagnostics and operator interface subsystems are coupled to a symbolic process database. A special architecture, the Multigraph Architecture has been used as a general framework for integrating symbolic and numeric programming techniques in a distributed environment. The knowledge-based components have been implemented as ""Autonomous Communicating Objects"". The application of concurrent programming techniques made possible the introduction of new approaches in the design of the monitoring and diagnostic system. A prototype IPCS has been implemented on a network of VAX computers.",https://ieeexplore.ieee.org/document/4789792/,1988 American Control Conference,15-17 June 1988,ieeexplore
10.1109/INDIN.2015.7281881,Knowledge-driven finite-state machines. Study case in monitoring industrial equipment,IEEE,Conferences,Traditionally state machines are implemented by coding the desired behavior of a given system. This work proposes the use of ontological models to describe and perform computations on state machines by using SPARQL queries. This approach represents a paradigm shift relating to the customary manner in which state machines are stored and computed. The main contribution of the work is an ontological model to represent state machines and a set of generic queries that can be used in any knowledge-driven state machine to compute valuable information. The approach was tested in a study case were the state machines of industrial robots in a manufacturing line were modeled as ontological models and used for monitoring the behavior of these devices on real time.,https://ieeexplore.ieee.org/document/7281881/,2015 IEEE 13th International Conference on Industrial Informatics (INDIN),22-24 July 2015,ieeexplore
10.1109/CVPR.2019.00655,L3-Net: Towards Learning Based LiDAR Localization for Autonomous Driving,IEEE,Conferences,"We present L3-Net - a novel learning-based LiDAR localization system that achieves centimeter-level localization accuracy, comparable to prior state-of-the-art systems with hand-crafted pipelines. Rather than relying on these hand-crafted modules, we innovatively implement the use of various deep neural network structures to establish a learning-based approach. L3-Net learns local descriptors specifically optimized for matching in different real-world driving scenarios. 3D convolutions over a cost volume built in the solution space significantly boosts the localization accuracy. RNNs are demonstrated to be effective in modeling the vehicle's dynamics, yielding better temporal smoothness and accuracy. We comprehensively validate the effectiveness of our approach using freshly collected datasets. Multiple trials of repetitive data collection over the same road and areas make our dataset ideal for testing localization systems. The SunnyvaleBigLoop sequences, with a year's time interval between the collected mapping and testing data, made it quite challenging, but the low localization error of our method in these datasets demonstrates its maturity for real industrial implementation.",https://ieeexplore.ieee.org/document/8954371/,2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),15-20 June 2019,ieeexplore
10.1109/ICASSP.2019.8683088,Learning Discriminative Features in Sequence Training without Requiring Framewise Labelled Data,IEEE,Conferences,"In this work, we try to answer two questions: Can deeply learned features with discriminative power benefit an ASR system's robustness to acoustic variability? And how to learn them without requiring framewise labelled sequence training data? As existing methods usually require knowing where the labels occur in the input sequence, they have so far been limited to many real-world sequence learning tasks. We propose a novel method which simultaneously models both the sequence discriminative training and the feature discriminative learning within a single network architecture, so that it can learn discriminative deep features in sequence training that obviates the need for presegmented training data. Our experiment in a realistic industrial ASR task shows that, without requiring any specific fine-tuning or additional complexity, our proposed models have consistently outperformed state-of-the-art models and significantly reduced Word Error Rate (WER) under all test conditions, and especially with highest improvements under unseen noise conditions, by relative 12.94%, 8.66% and 5.80%, showing our proposed models can generalize better to acoustic variability.",https://ieeexplore.ieee.org/document/8683088/,"ICASSP 2019 - 2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)",12-17 May 2019,ieeexplore
10.1109/ETFA.2019.8869172,Learning based Probabilistic Model for Migration of Industrial Control Systems,IEEE,Conferences,"The updating and upgrading of control systems is a cumbersome, expensive and time consuming task. From a software perspective, control system migration is a collective task of migrating the control logic, Human Machine Interface (HMI) and auxiliary software applications. Migrating control logic is the most challenging task owing to constraints on hard real-time behavior and execution order. Control logic typically contains engineering artifacts that specify the functionality of industrial devices taking into account various parameters. Therefore, to migrate from one Distributed Control System (DCS) system to another or to upgrade the existing DCS, one needs to map the source control entity and their parameters to the appropriate control entities in the target DCS.In this paper, we propose a machine learning based suggestion management system that identifies control entities and parameters for a source DCS and suggests the use of similar control entities and corresponding parameters for the target DCS. This in effect saves effort required in mapping of control parameters and reduces the dependence on subject matter experts. Our system uses a probabilistic approach to find these similarity mappings based on meta-data stored in an Ontology. We further describe a case study implemented for mapping heritage and legacy systems to a modern control system to verify and validate our approach.",https://ieeexplore.ieee.org/document/8869172/,2019 24th IEEE International Conference on Emerging Technologies and Factory Automation (ETFA),10-13 Sept. 2019,ieeexplore
10.1109/INDIN45523.2021.9557472,Learning-based Co-Design of Distributed Edge Sensing and Transmission for Industrial Cyber-Physical Systems,IEEE,Conferences,"Industrial cyber-physical systems (ICPS) refer to an emerging generation of intelligent systems, where distributed data acquisition is of great importance and is influenced by data transmission. In the improvement of the overall performance of sensing accuracy and energy efficiency, sensing and transmission are tightly coupled. Due to the unknown transmission channel states in the harsh industrial field environment, intelligently performing sensor scheduling for distributed sensing is challenging. In this paper, edge computing technology is utilized to enhance the level of intelligence at the edge side and deploy advanced scheduling algorithms. We propose a learning-based distributed edge sensing-transmission co-design (LEST) algorithm under the coordination of the sensors and the edge computing unit (ECU). Deep reinforcement learning is applied to perform real-time sensor scheduling under unknown channel states. The conditions for the existence of feasible scheduling policies are analyzed. The proposed algorithm is applied to estimate the slab temperature in the hot rolling process, which is a typical ICPS. The simulation results demonstrate that the overall performance of LEST is better than other suboptimal algorithms.",https://ieeexplore.ieee.org/document/9557472/,2021 IEEE 19th International Conference on Industrial Informatics (INDIN),21-23 July 2021,ieeexplore
10.1109/CVPR46437.2021.01493,LightTrack: Finding Lightweight Neural Networks for Object Tracking via One-Shot Architecture Search,IEEE,Conferences,"Object tracking has achieved significant progress over the past few years. However, state-of-the-art trackers become increasingly heavy and expensive, which limits their deployments in resource-constrained applications. In this work, we present LightTrack, which uses neural architecture search (NAS) to design more lightweight and efficient object trackers. Comprehensive experiments show that our LightTrack is effective. It can find trackers that achieve superior performance compared to handcrafted SOTA trackers, such as SiamRPN++ [30] and Ocean [56], while using much fewer model Flops and parameters. Moreover, when deployed on resource-constrained mobile chipsets, the discovered trackers run much faster. For example, on Snapdragon 845 Adreno GPU, LightTrack runs 12× faster than Ocean, while using 13× fewer parameters and 38× fewer Flops. Such improvements might narrow the gap between academic models and industrial deployments in object tracking task. LightTrack is released at here.",https://ieeexplore.ieee.org/document/9578709/,2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),20-25 June 2021,ieeexplore
10.1109/ICAICA52286.2021.9498178,Linear and nonlinear hierarchical modeling strategy for dynamic soft sensor,IEEE,Conferences,"In real industrial process, linearity and nonlinearity often exist at the same time, which brings difficulty to the modeling of soft sensor in industrial process. In this paper, a linear and nonlinear hierarchical strategy is proposed for soft sensing of dynamic processes. First, a linear identification coefficient (LIC) is designed to measure the degree of linear correlation between input variables and output variables. Process variables are divided into linear variable group and nonlinear variable group. Then, we use dynamic partial least squares (DPLS) to build a linear model. In view of the prediction residuals of linear models, a long short-term memory (LSTM) model is established to fit them, so as to compensate for the failure of linear methods to capture nonlinear relationships. The validity of the method is proved by the experiment of three-phase flow. Compared with other linear and nonlinear models, the proposed method has better accuracy and clearer structure.",https://ieeexplore.ieee.org/document/9498178/,2021 IEEE International Conference on Artificial Intelligence and Computer Applications (ICAICA),28-30 June 2021,ieeexplore
10.1109/ICC40277.2020.9148684,Machine Learning for Predictive Diagnostics at the Edge: an IIoT Practical Example,IEEE,Conferences,"Edge Computing is becoming more and more essential for the Industrial Internet of Things (IIoT) for data acquisition from shop floors. The shifting from central (cloud) to distributed (edge nodes) approaches will enhance the capabilities of handling real-time big data from IoT. Furthermore, these paradigms allow moving storage and network resources at the edge of the network closer to IoT devices, thus ensuring low latency, high bandwidth, and location-based awareness. This research aims at developing a reference architecture for data collecting, smart processing, and manufacturing control system in an IIoT environment. In particular, our architecture supports data analytics and Artificial Intelligence (AI) techniques, in particular decentralized and distributed hybrid twins, at the edge of the network. In addition, we claim the possibility to have distributed Machine Learning (ML) by enabling edge devices to learn local ML models and to store them at the edge. Furthermore, edges have the possibility of improving the global model (stored at the cloud) by sending the reinforced local models (stored in different shop floors) towards the cloud. In this paper, we describe our architectural proposal and show a predictive diagnostics case study deployed in an edge-enabled IIoT infrastructure. Reported experimental results show the potential advantages of using the proposed approach for dynamic model reinforcement by using real-time data from IoT instead of using an offline approach at the cloud infrastructure.",https://ieeexplore.ieee.org/document/9148684/,ICC 2020 - 2020 IEEE International Conference on Communications (ICC),7-11 June 2020,ieeexplore
10.1109/21CW48944.2021.9532537,Machine diagnosis using acoustic analysis: a review,IEEE,Conferences,"Diagnosis or fault identification in real industrial machines using audio or sound signals is a challenging task. Active research has been pursued to determine acoustic features, classification &amp; clustering algorithms that could estimate the state of an industrial machine. Acoustic features &amp; classifiers from different domains have been successfully implemented for fault identification in industrial machines. This paper is a comparative study of propositions, experiments, applications and systems developed by various researchers. Effort has been made to generate a collection of test benches developed, results observed and conclusion arrived. These insights suggest deep learning and anomaly detection techniques as a promising technology for preventive maintenance in real industrial machines.",https://ieeexplore.ieee.org/document/9532537/,2021 IEEE Conference on Norbert Wiener in the 21st Century (21CW),22-25 July 2021,ieeexplore
10.1109/WCICSS.2016.7882607,Machine learning algorithms for process analytical technology,IEEE,Conferences,"Increased globalisation and competition are drivers for process analytical technologies (PAT) that enable seamless process control, greater flexibility and cost efficiency in the process industries. The paper will discuss process modelling and control for industrial applications with an emphasis on solutions enabling the real-time data analytics of sensor measurements that PAT demands. This research aims to introduce an integrated process control approach, embedding novel sensors for monitoring in real time the critical control parameters of key processes in the minerals, ceramics, non-ferrous metals, and chemical process industries. The paper presents a comparison of machine learning algorithms applied to sensor data collected for a polymerisation process. Several machine learning algorithms including Adaptive Neuro-Fuzzy Inference Systems, Neural Networks and Genetic Algorithms were implemented using MATLAB® Software and compared in terms of accuracy (MSE) and robustness in modelling process progression. The results obtained show that machine learning-based approaches produce significantly more accurate and robust process models compared to models developed manually while also being more adaptable to new data. The paper presents perspectives on the potential benefits of machine learning algorithms with a view to their future in the industrial process industry.",https://ieeexplore.ieee.org/document/7882607/,2016 World Congress on Industrial Control Systems Security (WCICSS),12-14 Dec. 2016,ieeexplore
10.1109/CIMSA.2006.250751,Management of Complex Dynamic Systems based on Model-Predictive Multi-objective Optimization,IEEE,Conferences,"Over the past two decades, model predictive control and decision-making strategies have established themselves as powerful methods for optimally managing the behavior of complex dynamic industrial systems and processes. This paper presents a novel model-based multi-objective optimization and decision-making approach to model-predictive decision-making. The approach integrates predictive modeling based on neural networks, optimization based on multi-objective evolutionary algorithms, and decision-making based on Pareto frontier techniques. The predictive models are adaptive, and continually update themselves to reflect with high fidelity the gradually changing underlying system dynamics. The integrated approach, embedded in a real-time plant optimization and control software environment has been deployed to dynamically optimize emissions and efficiency while simultaneously meeting load demands and other operational constraints in a complex real-world power plant. While this approach is described in the context of power plants, the method is adaptable to a wide variety of industrial process control and management applications",https://ieeexplore.ieee.org/document/4016827/,2006 IEEE International Conference on Computational Intelligence for Measurement Systems and Applications,12-14 July 2006,ieeexplore
10.1109/ICMNN.1994.593731,Massively parallel VLSI-implementation of a dedicated neural network for anomaly detection in automated visual quality control,IEEE,Conferences,"In this work we will present the VLSI-implementation of a dedicated neural network architecture which we have developed in prior work for anomaly detection in automated visual industrial quality control. The network, denoted as NOVAS performs a filtering of inspection images and highlights defects or anomalies in an isomorphic image representation, allowing the detection and localisation of faults on objects. Training of NOVAS is achieved by simply presenting a set of tolerable objects to the network in a single sweep. NOVAS works with single and with multichannel image representations. The processing principle of NOVAS is closely related to nearest neighbor and hypersphere classifier approaches. We have designed an ASIC for the efficient implementation of the nearest neighbor search. Based on that ASIC we will present an architecture of a modular massively parallel computer suited to meet the real-time constraints of manufacturing processes. Further we will report on the status of a prototype system which is close to completion.",https://ieeexplore.ieee.org/document/593731/,Proceedings of the Fourth International Conference on Microelectronics for Neural Networks and Fuzzy Systems,26-28 Sept. 1994,ieeexplore
10.1109/IDAP.2019.8875881,Measuring IEEE 802.15.4 Protocol Performance over Embedded Control Systems,IEEE,Conferences,"Using the IEEE 802.15.4 could be notice in various areas such as hospitals, agricultural, industrial, as well as smart homes. All the above-mentioned areas require devices that have a mechanism that consume low energies and low costs for communication. Thus, the purpose of fulfilling the needed requirements, IEEE 802.15.4 could be seen as effective and commercial solution. Various factors exist for studying the efficiency related to the wireless networks. Emulation could be define as a toll of high importance for designing and evaluating the implementation and the efficiency related to protocols, while the majority of real applications are considered to be of high difficulty with regard to the real applications. Concerning the presented paper, the main aim has been showing performance results, which are relates to IEEE 802.15.4 through use of octave simulations. The main task of the presented paper is analyzing the protocol's performance in various conditions as well as having the ability of checking the performance with regard to reliability in real-time. One of the aims of this study is analyzing the performance parameters, that are energy consumption rate, packet received ration as well as the Received Signal Strength Indicator (RSSI). All of the mentioned parameters studied in indoor and outdoor conditions the purpose of verifying the performance of the wireless protocol. The presented paper focus on 2 nodes in for the peer-to-peer communications, where one of the nodes acts as base-station or coordinator, while the other node is considered as router. The router include Xbee with sensors and Arduino, while the coordinator consist of Xbee receiver and adapter.",https://ieeexplore.ieee.org/document/8875881/,2019 International Artificial Intelligence and Data Processing Symposium (IDAP),21-22 Sept. 2019,ieeexplore
10.1109/AIVR50618.2020.00017,Mirrorlabs - creating accessible Digital Twins of robotic production environment with Mixed Reality,IEEE,Conferences,How to visualize recorded production data in Virtual Reality? How to use state of the art Augmented Reality displays that can show robot data? This paper introduces an opensource ICT framework approach for combining Unity-based Mixed Reality applications with robotic production equipment using ROS Industrial. This publication gives details on the implementation and demonstrates the use as a data analysis tool in the context of scientific exchange within the area of Mixed Reality enabled human-robot co-production.,https://ieeexplore.ieee.org/document/9319071/,2020 IEEE International Conference on Artificial Intelligence and Virtual Reality (AIVR),14-18 Dec. 2020,ieeexplore
10.1109/ICMEW46912.2020.9106033,Mobile Centernet for Embedded Deep Learning Object Detection,IEEE,Conferences,"Object detection is a fundamental task in computer vision with wide application prospect. And recent years, many novel methods are proposed to tackle this task. However, most algorithms suffer from high computation cost and long inference time, which makes them impossible to be deployed on embedded devices in real industrial application scenarios. In this paper, we propose the Mobile CenterNet to solve this problem. Our method is based on CenterNet but with some key improvements. To enhance detection performance, we adopt HRNet as a powerful backbone and introduce a category-balanced focal loss to deal with category imbalance problem. Moreover, to compress the model size as well as reduce inference time, knowledge distillation is employed to transfer knowledge from cumbersome model to a compact one. We conduct experiments on a large traffic detection dataset BDD100K and validate the effectiveness of all the modifications. Finally, our method achieves the 1st place in the Embedded Deep Learning Object Detection Model Compression Competition held in ICME 2020.",https://ieeexplore.ieee.org/document/9106033/,2020 IEEE International Conference on Multimedia & Expo Workshops (ICMEW),6-10 July 2020,ieeexplore
10.1109/VRAIS.1995.512486,Model based vision as feedback for virtual reality robotics environments,IEEE,Conferences,"Task definition methods for robotic systems are often difficult to use. The ""on-line"" programming methods are often time expensive or risky for the human operator or the robot itself. On the other hand, ""off-line"" techniques are tedious and complex. In addition operator training is costly and time consuming. In a Virtual Reality Robotics Environment (VRRE), users are not asked to write down complicated functions, but can operate complex robotic systems in an intuitive and cost-effective way. However a VRRE is only effective if all the environment changes and object movements are fed-back to the virtual manipulating system. The paper describes the use of a VRRE for a semi-autonomous robot system comprising an industrial 5-axis robot, its virtual equivalent and a model based vision system used as feed-back. The user is immersed in a 3-D space built out of models of the robot's environment. He directly interacts with the virtual ""components"", defining tasks and dynamically optimizing them. A model based vision system locates objects in the real workspace to update the VRRE through a bi-directional communication link. In order to enhance the capabilities of the VRRE, a reflex-type behavior based on vision has been implemented. By locally (independently of the VRRE) controlling the real robot, the operator is discharged of small environmental changes due to transmission delays. Thus once the tasks have been optimized on the VRRE, they are sent to the real robot and a semi autonomous process ensures their correct execution thanks to a camera directly mounted on the robot's end effector. On the other hand if the environmental changes are too important, the robot stops, re-actualizes the VRRE with the new environmental configuration, and waits for task redesign. Because the operator interacts with the robotic system at a task oriented high level, VRRE systems are easily portable to other robotics environments (mobile robotics and micro assembly).",https://ieeexplore.ieee.org/document/512486/,Proceedings Virtual Reality Annual International Symposium '95,11-15 March 1995,ieeexplore
10.1109/CCA.2009.5281106,Model predictive control for nonlinear affine systems based on the simplified dual neural network,IEEE,Conferences,"Model predictive control (MPC), also known as receding horizon control (RHC), is an advanced control strategy for optimizing the performance of control systems. For nonlinear systems, standard MPC schemes based on linearization would result in poor performance. In this paper, we propose an MPC scheme for nonlinear affine systems based on a recurrent neural network (RNN) called the simplified dual network. The proposed RNN-based approach is efficient and suitable for real-time MPC implementation in industrial applications. Simulation results are provided to demonstrate the effectiveness and efficiency of the proposed MPC scheme.",https://ieeexplore.ieee.org/document/5281106/,"2009 IEEE Control Applications, (CCA) & Intelligent Control, (ISIC)",8-10 July 2009,ieeexplore
10.1109/MODELS-C.2019.00101,Modeling Adaptive Learning Agents for Domain Knowledge Transfer,IEEE,Conferences,"The implementation of intelligent agents in industrial applications is often prevented by the high cost of adopting such a system to a particular problem domain. This paper states the thesis that when learning agents are applied to work environments that require domain-specific experience, the agent benefits if it can be further adapted by a supervising domain expert. Closely interacting with the agent, a domain expert should be able to understand its decisions and update the underlying knowledge base as needed. The result would be an agent with individualized knowledge that comes in part from the domain experts. The model of such an adaptive learning agent must take into account the problem domain, the design of the learning agent and the perception of the domain user. Therefore, already in the modeling phase, more attention must be paid to make the learning element of the agent adaptable by an operator. Domain modeling and meta-modeling methods could help to make inner processes of the agent more accessible. In addition, the knowledge gained should be made reusable for future agents in similar environments. To begin with, the existing methods for modeling agent systems and the underlying concepts will be evaluated, based on the requirements for different industrial scenarios. The methods are then compiled into a framework that allows for the description and modeling of such systems in terms of adaptability to a problem domain. Where necessary, new methods or tools will be introduced to close the gap between inconsistent modeling artifacts. The framework shall then be used to build learning agents for real-life scenarios and observe their application in a case study. The results will be used to assess the quality of the adapted knowledge base and compare it to a manual knowledge modeling process.",https://ieeexplore.ieee.org/document/8904822/,2019 ACM/IEEE 22nd International Conference on Model Driven Engineering Languages and Systems Companion (MODELS-C),15-20 Sept. 2019,ieeexplore
10.1109/ISIE45063.2020.9152407,Modeling and Predicting an Industrial Process Using a Neural Network and Automation Data,IEEE,Conferences,"Production optimization and prevention of faults and unplanned production halts are areas of particular interest in industry. Predictive analysis is commonly implemented with data analytics and machine learning techniques. Usually, the usage of such tools requires knowledge of the machine learning theory and the subject to be studied, e.g. a pumping process. This paper presents a case study on modeling of a pumping process using stored automation data. The model is trained to predict the performance percentage of the process with minimal background knowledge of the process and data analytics. The proposed model is built with IBM SPSS Modeler, a data analysis tool not usually used in real-time industrial predictive analysis as it is not often considered the best tool when working with time series data. The model is deployed in a cloud service to implement a real-time, visualized predictive analysis system. The case study shows that Modeler can be used for data analysis, modeling, and production purposes. Depending on the case, Modeler can provide an alternative tool compared with typical machine learning tools, as models built with Modeler can be deployed into a cloud service for production use. The findings indicate that industrial automation data are a valuable resource, and data analysis can be conducted on various platforms and tools.",https://ieeexplore.ieee.org/document/9152407/,2020 IEEE 29th International Symposium on Industrial Electronics (ISIE),17-19 June 2020,ieeexplore
10.1109/NRSC.2002.1022671,Modeling and automatic control of nuclear reactors,IEEE,Conferences,"This paper presents a developed real time simulator for MPR (multi purpose reactor), on which the Egyptian 2/sup nd/ research reactor is based. A VisSim S/W environment with real-time add-on is employed to achieve this put-pose. VisSim S/W supports a variety of standard and industrial grade I/O cards and uses the block diagram programming technique, which keeps use of specialized code to minimum. All necessary reactivity feedback is taken into consideration and modeled by sufficiently accurate equations. A control rod withdrawal algorithm, which represents the actual one of the MPR, is stated and modeled. Application of traditional control algorithms are implemented and discussed. Advanced control algorithms such as fuzzy, neural network, and genetic are investigated to substitute the moving control rod selection logic and core parameter prediction. The simulator can be distributed with VisSim viewer or through generated C code from VisSim block diagram. The results of the proposed simulator in the power state agree well with the published experimental and analytical results.",https://ieeexplore.ieee.org/document/1022671/,Proceedings of the Nineteenth National Radio Science Conference,21-21 March 2002,ieeexplore
10.1109/AIID51893.2021.9456471,Multi-Scale Tiny Region Gesture Recognition Towards 3D Object Manipulation In Industrial Design,IEEE,Conferences,"This paper proposed a smart 3D virtual object manipulation by gesture recognition with deep network training for multiply scalar tiny region targets. We introduce the famous YOLOv5 based improved and enhanced target detection deep networks, and provide effective and high efficiency gesture recognition method towards 3D object manipulation. Meanwhile, combined with the adaptive anchor frame calculation and target recognition accelerator, our method can capture the tiny gesture region and for multi-scale of image detection. The method can be efficiently applied to the virtual control with different hand gestures. The experiment results are high efficiency and accuracy. It has high application value in the field of industrial design based on gesture manipulation.",https://ieeexplore.ieee.org/document/9456471/,2021 IEEE International Conference on Artificial Intelligence and Industrial Design (AIID),28-30 May 2021,ieeexplore
10.1109/FUZZ.2002.1005041,Multi-axis fuzzy control and performance analysis for an industrial robot,IEEE,Conferences,"Robot control systems can be considered as complex systems, the design of the controller involving the determination of the dynamic model for the system. Fuzzy logic provides functional capability without the use of a system model or the characteristics associated with capturing the approximate, varying values found in real world systems. Development of a multi-axis fuzzy logic control system was implemented on an industrial robot, replacing the existing control and hardware systems with a new developmental system. During robot control no adaptation of the rule base or membership functions was carried out online; only system gain was modified in relation to link speed and joint error within predetermined design parameters. The fuzzy control system had to manage the effects of frictional and gravitational forces whilst compensating for the varying inertia components when each linkage is moving. Testing based on ISO 9283 for path accuracy and repeatability verified that real time control of three axes was achievable with values of 938 /spl mu/m and 864 /spl mu/m recorded for accuracy and repeatability, respectively.",https://ieeexplore.ieee.org/document/1005041/,2002 IEEE World Congress on Computational Intelligence. 2002 IEEE International Conference on Fuzzy Systems. FUZZ-IEEE'02. Proceedings (Cat. No.02CH37291),12-17 May 2002,ieeexplore
10.1109/IJCNN52387.2021.9533598,Multiplicative Attention Mechanism for Multi-horizon Time Series Forecasting,IEEE,Conferences,"Multi-horizon time series forecasting plays an important role in many industrial and business decision processes. To grasp complex and various patterns across different time series is the crucial step in achieving promising performance. However, most deep learning-based forecasting approaches simply take series-specific static (i.e. time-invariant) covariates as input features, which can fail to capture the complex pattern variation for each possible time series. In this paper, we propose a novel multiplicative attention-based architecture to tackle such forecasting problem. Our modification to multi-head attention layers leverages the series-specific covariates to build flexible attention functions for each possible time series. This improvement contributes to greater representation capacity to grasp different patterns across related time series. Experiment results demonstrate that our approach achieves state-of-the-art performance on a variety of real-world datasets.",https://ieeexplore.ieee.org/document/9533598/,2021 International Joint Conference on Neural Networks (IJCNN),18-22 July 2021,ieeexplore
10.1109/ICDMW.2019.00065,Mímir: Building and Deploying an ML Framework for Industrial IoT,IEEE,Conferences,"In this paper we describe Mímir, a production grade cloud and edge spanning ML framework for Industrial IoT applications. We first describe our infrastructure for optimized capture, streaming and multi-resolution storage of manufacturing data and its context. We then describe our workflow for scalable ML model training, validation, and deployment that leverages a manufacturing taxonomy and parameterized ML pipelines to determine the best metrics, hyper-parameters and models to use for a given task. We also discuss our design decisions on model deployment for real-time and batch data in the cloud and at the edge. Finally, we describe the use of the framework in building and deploying an application for Predictive Quality monitoring during a Plastics Extrusion manufacturing process.",https://ieeexplore.ieee.org/document/8955638/,2019 International Conference on Data Mining Workshops (ICDMW),8-11 Nov. 2019,ieeexplore
10.1109/INDIN.2015.7281723,NN-ANARX model based control of liquid level using visual feedback,IEEE,Conferences,"In this paper, the problem of liquid level control based on visual feedback is investigated in application to a real life model of an industrial plant. Visual detection of liquid level is implemented on the Raspberry Pi computer. Computational intelligence based controller uses input-output feedback linearization. Parameters of the controller are provided by the neural network ANARX structure. As communication between Raspberry Pi and control system is provided over WLAN, additional prediction module is used to overcome networking problems. Control of the SISO and MIMO systems is provided.",https://ieeexplore.ieee.org/document/7281723/,2015 IEEE 13th International Conference on Industrial Informatics (INDIN),22-24 July 2015,ieeexplore
10.1109/PEDES.2006.344292,Neural Approach for Automatic Identification of Induction Motor Load Torque in Real-Time Industrial Applications,IEEE,Conferences,"Induction motors are widely used in several industrial sectors. However, the dimensioning of induction motors is often inaccurate because, in most cases, the load behavior in the shaft is completely unknown. The proposal of this paper is to use artificial neural networks as a tool for dimensioning induction motors rather than conventional methods, which use classical identification techniques and mechanical load modeling. Since the proposed approach uses current, voltage and speed values as the only input parameters, one of its potentialities is related to the facility of hardware implementation for industrial environments and field applications. Simulation results are also presented to validate the proposed approach.",https://ieeexplore.ieee.org/document/4147999/,"2006 International Conference on Power Electronic, Drives and Energy Systems",12-15 Dec. 2006,ieeexplore
10.1109/UralCon52005.2021.9559619,Neural Network for Real-Time Signal Processing: the Nonlinear Distortions Filtering,IEEE,Conferences,"Artificial neural networks, after their training and testing, allow, in the ""if then"" mode, to process signals in real time. This is relevant for signal processing tasks in electrical engineering and the electric power industry, especially for the analysis of nonlinear signal distortions, transients in electrical networks, during switching, emergency modes, and so on. The paper shows the neuro algorithm possibilities based on an elementary perceptron for filtering nonlinear distortions of industrial frequency signals of 50 Hz over a time interval of units of milliseconds. At the same time, the accuracy for the signals’ amplitude, phase, and frequency determining is units of percent. Examples of the fundamental frequency signal selection in the presence of harmonics, the determination of the signal parameters during transients, and the correction of the transformer saturation current are given. It is shown that real-time neural network processing can be carried out in a ""sliding window"" with the duration of units of milliseconds. The estimates made during the implementation of the neuro algorithm in a microprocessor device showed its application possibility in the electric power industry secondary equipment.",https://ieeexplore.ieee.org/document/9559619/,2021 International Ural Conference on Electrical Power Engineering (UralCon),24-26 Sept. 2021,ieeexplore
10.1145/2830772.2830789,Neuromorphic accelerators: A comparison between neuroscience and machine-learning approaches,IEEE,Conferences,"A vast array of devices, ranging from industrial robots to self-driven cars or smartphones, require increasingly sophisticated processing of real-world input data (image, voice, radio, ...). Interestingly, hardware neural network accelerators are emerging again as attractive candidate architectures for such tasks. The neural network algorithms considered come from two, largely separate, domains: machine-learning and neuroscience. These neural networks have very different characteristics, so it is unclear which approach should be favored for hardware implementation. Yet, few studies compare them from a hardware perspective. We implement both types of networks down to the layout, and we compare the relative merit of each approach in terms of energy, speed, area cost, accuracy and functionality. Within the limit of our study (current SNN and machine learning NN algorithms, current best effort at hardware implementation efforts, and workloads used in this study), our analysis helps dispel the notion that hardware neural network accelerators inspired from neuroscience, such as SNN+STDP, are currently a competitive alternative to hardware neural networks accelerators inspired from machine-learning, such as MLP+BP: not only in terms of accuracy, but also in terms of hardware cost for realistic implementations, which is less expected. However, we also outline that SNN+STDP carry potential for reduced hardware cost compared to machine-learning networks at very large scales, if accuracy issues can be controlled (or for applications where they are less important). We also identify the key sources of inaccuracy of SNN+STDP which are less related to the loss of information due to spike coding than to the nature of the STDP learning algorithm. Finally, we outline that for the category of applications which require permanent online learning and moderate accuracy, SNN+STDP hardware accelerators could be a very cost-efficient solution.",https://ieeexplore.ieee.org/document/7856622/,2015 48th Annual IEEE/ACM International Symposium on Microarchitecture (MICRO),5-9 Dec. 2015,ieeexplore
10.1109/ICMCS.2011.5945672,Node disjoint multi-path routing for ZigBee cluster-tree wireless sensor networks,IEEE,Conferences,"ZigBee is the emerging industrial standard for Ad hoc networks based on the IEEE 802.15.4 physical and MAC layers standard. It was originally specified for low data rate, low power consumption and low cost wireless personal area networks (WPANs). Due to these characteristics, ZigBee is expected to be used in wireless sensor networks (WSNs) for industrial sensing and control applications. Handling high data rate applications, such video surveillance in WPANs is a real challenge. To increase available bandwidth in a ZigBee network, we explore in this paper, multipath routing where multiple paths are used simultaneously to transfer data between a source and the sink. We propose Z-MHTR, a node disjoint multipath routing extension of the ZigBee hierarchical tree routing protocol in cluster-tree WSN. The aim of this work is to study and evaluate the implemented multipath routing using NS2 simulations. We mainly faced some problems that influence the efficiency of the multipath routing and the overall network performances in terms of throughput, end to end delay data transmission and network lifetime under heavy and low data rates.",https://ieeexplore.ieee.org/document/5945672/,2011 International Conference on Multimedia Computing and Systems,7-9 April 2011,ieeexplore
10.1109/ComPE49325.2020.9200139,Object Detection and Tracking Turret based on Cascade Classifiers and Single Shot Detectors,IEEE,Conferences,"The involvement of embedded systems and computer vision is increasing day by day in various segments of consumer market like industrial automation, traffic monitoring, medical imaging, modern appliance market, augmented reality systems, etc. These technologies are bound to make new developments in the domain of commercial and home security surveillance. Our project aims to make contributions to the domain of video surveillance by making use of embedded computer vision systems. Our implementation, built around the Raspberry Pi 4 SBC aims to utilize computer vision techniques like motion detection, face recognition, object detection, etc to segment the region of interest from the captured video footage. This technique is superior as compared to traditional surveillance systems as it requires minimum human interaction and intervention at the control room of such security systems. The proposed system is capable of sensing suspicious events like detection of an unknown face in the captured video or motion detection/object detection in a closed section of a building. Moreover, with the help of the turret mechanism built using servo motors, the camera integrated in the system is capable of having 360° rotation and can track a detected face or object of interest within its range. Apart from automated tracking, the system can also be manually controlled by the operator.",https://ieeexplore.ieee.org/document/9200139/,2020 International Conference on Computational Performance Evaluation (ComPE),2-4 July 2020,ieeexplore
10.1109/ROBIO.2009.4913200,Object orientation recognition based on SIFT and SVM by using stereo camera,IEEE,Conferences,"The goal of this research is to recognize an object and its orientation in space by using stereo camera. The principle of object orientation recognition in this paper was based on the scale invariant feature transform (SIFT) and support vector machine (SVM). SIFT has been successfully implemented on object recognition but it had a problem recognizing the object orientation. For many autonomous robotics applications, such as using a vision-guided industrial robot to grab a product, not only correct object recognition will be needed in this process but also object orientation recognition is required. In this paper we used SVM to recognize object orientation. SVM has been known as a promising method for classification accuracy and its generalization ability. The stereo camera system adopted in this research provided more useful information compared to single camera one. The object orientation recognition technique was implemented on an industrial robot in a real application. The proposed camera system and recognition algorithms were used to recognize a specific object and its orientation and then guide the industrial robot to perform some alignment operations on the object.",https://ieeexplore.ieee.org/document/4913200/,2008 IEEE International Conference on Robotics and Biomimetics,22-25 Feb. 2009,ieeexplore
10.1109/IECON.2006.347441,Obstacle avoidance algorithm based on biological patterns for anthropomorphic robot manipulator,IEEE,Conferences,"This study addresses the problem of collision-free controlling of 3-DOF (degree of freedom) anthropomorphic manipulators with given a priori unrestricted trajectory. The robot constraints resulting from the physical robot's actuators are also taken into account during the robot movement. Obstacle avoidance algorithm is based on penalty function, which is minimized when collision is predicted. Mathematical construction of penalty function and minimization process allows modeling of variety behaviors of robot elusion moves. Implementation of artificial neural network (ANN) inside the control process gives the additional flexibility needed to remember most important robot behaviors based on biological pattern of human arm moves. Thanks to the fast collisions' detection, the presented algorithm appears to be applicable to the industrial real-time implementations. Numerical simulations of the anthropomorphic manipulator operating in three dimensional space with obstacles is also presented",https://ieeexplore.ieee.org/document/4152937/,IECON 2006 - 32nd Annual Conference on IEEE Industrial Electronics,6-10 Nov. 2006,ieeexplore
10.1109/ISIC.1995.525123,On the design and implementation of a mobile robotic system,IEEE,Conferences,"Describes the effort being carried out in the analysis, design and implementation of the control architecture for a mobile platform for autonomous transportation, surveillance and inspection in structured and semi-structured industrial environments. The control architecture is based in a hierarchical structure organized linguistically permitting the real-time parallel execution of tasks. This architecture is composed of three levels, organization, coordination and functional, structured according to the increasing precision with decreasing intelligence principle.",https://ieeexplore.ieee.org/document/525123/,Proceedings of Tenth International Symposium on Intelligent Control,27-29 Aug. 1995,ieeexplore
10.1109/ICMA.2019.8816298,Online Learning of the Inverse Dynamics with Parallel Drifting Gaussian Processes: Implementation of an Approach for Feedforward Control of a Parallel Kinematic Industrial Robot,IEEE,Conferences,"The present paper deals with an online approach to learn the inverse dynamics of any robot. This is realized by the use of Gaussian Processes drifting parallel along the system data. An extension by a database enables the efficient use of data points from the past. The central component of this work is the implementation of such a method in a controller in order to achieve the actual goal: the feedforward control of an industrial robot by means of machine learning. This is done by splitting the procedure into two threads running parallel so that the prediction is decoupled from the computing-intensive training of the models. Experiments show that the method reduces the tracking errors more clearly than an elaborately identified rigid body model including friction. For a defined trajectory, the squared areas of the tracking errors of all axes are reduced by more than 54% compared to motion without pre-control. In addition, a highly dynamic pick-and-place experiment is used to investigate the possible changes in system dynamics. Compared to an offline trained model, the approximation error of the proposed online approach is smaller for the remaining time of the experiment after an initial phase. Furthermore, this error is smaller throughout the experiment for online learning with parallel drifting Gaussian Processes than when using a single one.",https://ieeexplore.ieee.org/document/8816298/,2019 IEEE International Conference on Mechatronics and Automation (ICMA),4-7 Aug. 2019,ieeexplore
10.1109/ICRA40945.2020.9196769,Online LiDAR-SLAM for Legged Robots with Robust Registration and Deep-Learned Loop Closure,IEEE,Conferences,"In this paper, we present a 3D factor-graph LiDAR-SLAM system which incorporates a state-of-the-art deeply learned feature-based loop closure detector to enable a legged robot to localize and map in industrial environments. Point clouds are accumulated using an inertial-kinematic state estimator before being aligned using ICP registration. To close loops we use a loop proposal mechanism which matches individual segments between clouds. We trained a descriptor offline to match these segments. The efficiency of our method comes from carefully designing the network architecture to minimize the number of parameters such that this deep learning method can be deployed in real-time using only the CPU of a legged robot, a major contribution of this work. The set of odometry and loop closure factors are updated using pose graph optimization. Finally we present an efficient risk alignment prediction method which verifies the reliability of the registrations. Experimental results at an industrial facility demonstrated the robustness and flexibility of our system, including autonomous following paths derived from the SLAM map.",https://ieeexplore.ieee.org/document/9196769/,2020 IEEE International Conference on Robotics and Automation (ICRA),31 May-31 Aug. 2020,ieeexplore
10.1109/ROMA.2017.8231735,Online system modeling of chemical process plant using U-model,IEEE,Conferences,"One of the major challenges in industrial process control is to deal with nonlinearities in the plants. There have been a significant amount of research efforts towards design and development of appropriate, reliable and promising control techniques to deployed on real-time industrial applications. Some of the widely used and acknowledged control methods lack in terms of tuning unknown system parameters. The reason is their unadaptive or fixed nature. This paves the field well for adaptive controllers. Their biggest advantage is their automatic updation of unknown system paramters, that saves quite some resources and manpower and ensures an overall stable control strategy. In this regard, system modeling happens to be the prime and pertinent task so that it can set the basis for a stable control law synthesis. This reserach work proposes a polynomial adaptive model recently introduced called U-Model to be used for online system identification of Chemical Process Plant. U-Model is a simple, stable and reliable which has previously yielded encouraging results when applied to various application in different scenario. The aforementioned plant shall be used for investigation on its Flow process. The modeling results shall be compared and validated by other commonly known and utilized modeling structures.",https://ieeexplore.ieee.org/document/8231735/,2017 IEEE 3rd International Symposium in Robotics and Manufacturing Automation (ROMA),19-21 Sept. 2017,ieeexplore
10.1109/ICCI-CC.2018.8482025,Ontology Faults Diagnosis Model for the Hazardous Chemical Storage Device,IEEE,Conferences,"Due to high temperature, high pressure, high corrosion and many other factors, the hazardous chemical device is facing more severe security challenges than other industries. Now, the monitoring methods have been very mature, which play a basic monitoring role, not a predictive fault diagnosis. In this paper, the hazardous chemical device's status data will been collected from the existing industrial monitoring network, the real-time data will be preprocessed and then stored in a database, and the data will be imported to the real-time data into the ontology model, the data will be performed by big data processing and automatic reasoning. So that real-time status of hazardous chemical device and the warning of security risks predict are easily got at any time. The model is proposed to solving the problem of knowledge representation and reasoning of the hazardous chemical device based on ontology. The model is analyzed and implemented in Protégé software.",https://ieeexplore.ieee.org/document/8482025/,2018 IEEE 17th International Conference on Cognitive Informatics & Cognitive Computing (ICCI*CC),16-18 July 2018,ieeexplore
10.1109/MMSP.2013.6659335,Open collaboration on hybrid video quality models - VQEG joint effort group hybrid,IEEE,Conferences,"Several factors limit the advances on automatizing video quality measurement. Modelling the human visual system requires multi- and interdisciplinary efforts. A joint effort may bridge the large gap between the knowledge required in conducting a psychophysical experiment on isolated visual stimuli to engineering a universal model for video quality estimation under real-time constraints. The verification and validation requires input reaching from professional content production to innovative machine learning algorithms. Our paper aims at highlighting the complex interactions and the multitude of open questions as well as industrial requirements that led to the creation of the Joint Effort Group in the Video Quality Experts Group. The paper will zoom in on the first activity, the creation of a hybrid video quality model.",https://ieeexplore.ieee.org/document/6659335/,2013 IEEE 15th International Workshop on Multimedia Signal Processing (MMSP),30 Sept.-2 Oct. 2013,ieeexplore
10.1109/CAMAD.2019.8858503,Operational Data Based Intrusion Detection System for Smart Grid,IEEE,Conferences,"With the rapid progression of Information and Communication Technology (ICT) and especially of Internet of Things (IoT), the conventional electrical grid is transformed into a new intelligent paradigm, known as Smart Grid (SG). SG provides significant benefits both for utility companies and energy consumers such as the two-way communication (both electricity and information), distributed generation, remote monitoring, self-healing and pervasive control. However, at the same time, this dependence introduces new security challenges, since SG inherits the vulnerabilities of multiple heterogeneous, co-existing legacy and smart technologies, such as IoT and Industrial Control Systems (ICS). An effective countermeasure against the various cyberthreats in SG is the Intrusion Detection System (IDS), informing the operator timely about the possible cyberattacks and anomalies. In this paper, we provide an anomaly-based IDS especially designed for SG utilising operational data from a real power plant. In particular, many machine learning and deep learning models were deployed, introducing novel parameters and feature representations in a comparative study. The evaluation analysis demonstrated the efficacy of the proposed IDS and the improvement due to the suggested complex data representation.",https://ieeexplore.ieee.org/document/8858503/,2019 IEEE 24th International Workshop on Computer Aided Modeling and Design of Communication Links and Networks (CAMAD),11-13 Sept. 2019,ieeexplore
10.1109/ICECCE49384.2020.9179427,Opportunistic Resource Allocation for Narrowband Internet of Things: A Literature Review,IEEE,Conferences,"Nowadays, the growing adoption of the Internet of Things (IoT) is reshaping the telecommunication landscape and has penetrated every aspect of our lives with influential applications on smart health, home automation, smart logistics, smart industries and smart cities. These advanced technologies bring about numerous benefits and has begun to play a major role in daily lives, particularly data mining applied in precision agriculture to discover knowledge. Also in agro-industrial production chain, the combination of wireless and distributed specific sensor devices with the simulation of climatic conditions in order to track the evolution of grapes for wineries is outstanding. Mobile IoT such as narrow-band-IoT (NB-IoT) and the long-term evolution (LTE) for machines (LTE-M) are significant innovations of this accelerating development of IoT technologies. In the era of ubiquitous communication where everything is connected to the internet, NB-IoT systems are expected to offer better quality-of-services (QoS) to end users than the traditional IoT paradigm. However, offering better QoS satisfaction to end users will become a great challenge due to the bottleneck caused by the dual problem of increasing IoT use cases and the shortage of wireless spectrum resources. Whilst discussing the recent innovative solutions of NB-IoT resource allocation, significant challenges and open issues related to the real-time implementation of NBIoT are identified and discussed. Therefore, this paper gives a general overview of the resource allocation solutions in this NB-IoT innovation and further suggests and motivates for the intelligentization of future resource allocation solutions (i.e., the use of artificial intelligence (AI) strategies).",https://ieeexplore.ieee.org/document/9179427/,"2020 International Conference on Electrical, Communication, and Computer Engineering (ICECCE)",12-13 June 2020,ieeexplore
10.23919/ChiCC.2017.8027747,Optimal operational control for industrial processes based on Q-learning method,IEEE,Conferences,"It is difficult to accurately model productive processes and describe relationship between operational indices and controlled variables for complex modem industrial processes. How to design the optimal setpoints by using only data generated by operational processes, without requiring the knowledge of model parameters of operational processes, poses a challenge on designing optimal setpoints. This paper presents a state-observer based Q-learning algorithm to learn the optimal setpoints by utilizing only data, such that the real operational indices can track the desired values in an approximately optimal manner. A simulation experiment in flotation process is implemented to show the effectiveness of the proposed method.",https://ieeexplore.ieee.org/document/8027747/,2017 36th Chinese Control Conference (CCC),26-28 July 2017,ieeexplore
10.1109/ICOIN50884.2021.9334026,Optimization of RSSI based indoor localization and tracking to monitor workers in a hazardous working zone using Machine Learning techniques,IEEE,Conferences,"This paper proposes a method for RSSI based indoor localization and tracking in cluttered environments using Deep Neural Networks. We implemented a real-time system to localize people using wearable active RF tags and RF receivers fixed in an industrial environment with high RF noise. The proposed solution is advantageous in analysing RSSI data in cluttered-indoor environments with the presence of human body attenuation, signal distortion, and environmental noise. Simulations and experiments on a hardware testbed demonstrated that receiver arrangement, number of receivers and amount of line of sight signals captured by receivers are important parameters for improving localization and tracking accuracy. The effect of RF signal attenuation through the person who carries the tag was combined with two neural network models trained with RSSI data pertaining to two walking directions. This method was successful in predicting the walking direction of the person.",https://ieeexplore.ieee.org/document/9334026/,2021 International Conference on Information Networking (ICOIN),13-16 Jan. 2021,ieeexplore
10.1109/CIIMA50553.2020.9290289,PI tuning based on Bacterial Foraging Algorithm for flow control,IEEE,Conferences,"In the industrial field, the need has arisen to use more efficient and robust controllers using artificial intelligence techniques that optimize the operation of processes within the industry. In this way, the need arises to employ adaptive controllers such as the BFOA and implement it in real systems in which its functionality can be analyzed. This article presents the implementation and analysis in a fully instrumented functional prototype with industrial sensors. The work methodology is documented from the acquisition of the physical variables through the OPC client-server communication; the synchronization of the excitation of the input variable (variable speed drive) and obtaining the evolution of the flow in time; with the experimental data, the identification methodology by relative least squares was used to obtain the transfer function. Later, the BFOA algorithm was implemented to adjust the constants of a PI controller (Kp and Ki) and analyze the response through simulation using Matlab software, in which satisfactory results were observed based on the analysis of response to disturbances and as an end final part, the controller and the BFOA algorithm were implemented in a PLC-S7-1500 controller in SCL language, and the functionality was validated with the functional prototype, changing the flow setpoints at certain times, observing a behavior according to the simulations carried out. with a minimum overshoot of approximately 5 % and an establishment time of 20s.",https://ieeexplore.ieee.org/document/9290289/,2020 IX International Congress of Mechatronics Engineering and Automation (CIIMA),4-6 Nov. 2020,ieeexplore
10.1109/CDC.2009.5400848,PLS-based FDI of a Three-Tank laboratory system,IEEE,Conferences,"The problems of fault detection and isolation of dynamic systems has been studied intensively in the recent years and many successful industrial applications have been reported. In the main these studies have been restricted to model based techniques, with few reports of successful implementation of data driven approaches. These data driven approaches have been range from the application of linear regression techniques, to neuro-fuzzy systems. This paper reports on application of, Multivariate Statistical Process Control (MSPC) methodologies, which can provide a diagnostic tool for the on-line or real time monitoring and detection of the process malfunction is proposed. Finally the effectiveness of Partial Least Squares (PLS) in FDI of the three-tank system are represented and discussed through simulation results.",https://ieeexplore.ieee.org/document/5400848/,Proceedings of the 48h IEEE Conference on Decision and Control (CDC) held jointly with 2009 28th Chinese Control Conference,15-18 Dec. 2009,ieeexplore
10.1109/BigDataCongress.2019.00032,"PREMISES, a Scalable Data-Driven Service to Predict Alarms in Slowly-Degrading Multi-Cycle Industrial Processes",IEEE,Conferences,"In recent years, the number of industry-4.0-enabled manufacturing sites has been continuously growing, and both the quantity and variety of signals and data collected in plants are increasing at an unprecedented rate. At the same time, the demand of Big Data processing platforms and analytical tools tailored to manufacturing environments has become more and more prominent. Manufacturing companies are collecting huge amounts of information during the production process through a plethora of sensors and networks. To extract value and actionable knowledge from such precious repositories, suitable data-driven approaches are required. They are expected to improve the production processes by reducing maintenance costs, reliably predicting equipment failures, and avoiding quality degradation. To this aim, Machine Learning techniques tailored for predictive maintenance analysis have been adopted in PREMISES (PREdictive Maintenance service for Industrial procesSES), an innovative framework providing a scalable Big Data service able to predict alarming conditions in slowly-degrading processes characterized by cyclic procedures. PREMISES has been experimentally tested and validated on a real industrial use case, resulting efficient and effective in predicting alarms. The framework has been designed to address the main Big Data and industrial requirements, by being developed on a solid and scalable processing framework, Apache Spark, and supporting the deployment on modularized containers, specifically upon the Docker technology stack.",https://ieeexplore.ieee.org/document/8818217/,2019 IEEE International Congress on Big Data (BigDataCongress),8-13 July 2019,ieeexplore
10.1109/BigData.2018.8622065,Parallel Large-Scale Neural Network Training For Online Advertising,IEEE,Conferences,"Neural networks have shown great successes in many fields. Due to the complexity of the training pipeline, however, using them in an industrial setting is challenging. In online advertising, the complexity arises from the immense size of the training data, and the dimensionality of the sparse feature space (both can be hundreds of billions). To tackle these challenges, we built TrainSparse (TS), a system that parallelizes the training of neural networks with a focus on efficiently handling large-scale sparse features. In this paper, we present the design and implementation of TS, and show the effectiveness of the system by applying it to predict the ad conversion rate (pCVR), one of the key problems in online advertising. We also compare several methods for dimensionality reduction on sparse features in the pCVR task. Experiments on real-world industry data show that TS achieves outstanding performance and scalability.",https://ieeexplore.ieee.org/document/8622065/,2018 IEEE International Conference on Big Data (Big Data),10-13 Dec. 2018,ieeexplore
10.1109/MEMCOD.2015.7340480,Passive testing of production systems based on model inference,IEEE,Conferences,"This paper tackles the problem of testing production systems, i.e. systems that run in industrial environments, and that are distributed over several devices and sensors. Usually, such systems lack of models, or are expressed with models that are not up to date. Without any model, the testing process is often done by hand, and tends to be an heavy and tedious task. This paper contributes to this issue by proposing a framework called Autofunk, which combines different fields such as model inference, expert systems, and machine learning. This framework, designed with the collaboration of our industrial partner Michelin, infers formal models that can be used as specifications to perform offline passive testing. Given a large set of production messages, it infers exact models that only capture the functional behaviours of a system under analysis. Thereafter, inferred models are used as input by a passive tester, which checks whether a system under test conforms to these models. Since inferred models do not express all the possible behaviours that should happen, we define conformance with two implementation relations. We evaluate our framework on real production systems and show that it can be used in practice.",https://ieeexplore.ieee.org/document/7340480/,2015 ACM/IEEE International Conference on Formal Methods and Models for Codesign (MEMOCODE),21-23 Sept. 2015,ieeexplore
10.1109/ICCC51575.2020.9345104,Payload-based Anomaly Detection for Industrial Internet Using Encoder Assisted GAN,IEEE,Conferences,"Payload-based anomaly detection has been proved effective in discovering Internet misbehavior and potential intrusions, but highly relies on the unstructured feature engineering to generalize the distribution of normal payloads. This kind of generalization may not adapt well to the emerging industrial Internet, where the normal behaviors are more diverse and usually embedded in the raw payloads' local structures. In this paper, we tackle this generalization problem and propose a very different solution to payload-based anomaly detection without the need of feature engineering. Our basic idea is to learn the raw structures of normal payloads directly by a generative adversarial network (GAN), in which we have a generator (i.e., a reversed convolutional decoder) to sample raw payloads from a latent space as well as a discriminator (i.e., a convolutional classifier) to guide the generator produce raw payloads approximating the normal structures. We also deploy an assisted convolutional encoder to map the true payloads back to the latent space and combine with the GAN's decoder (i.e., generator) to reconstruct the payload structures. We consider anomalies appear in condition the re-constructed payloads are largely deviated from the true ones, since our encoder-decoder architecture is trained able to rebuild only the normal payload structures. We have evaluated our solution using extensive experiments on real-world industrial Internet datasets, and confirmed its effectiveness in detecting industrial Internet anomalies in the raw payloads.",https://ieeexplore.ieee.org/document/9345104/,2020 IEEE 6th International Conference on Computer and Communications (ICCC),11-14 Dec. 2020,ieeexplore
10.1109/ICCCI49374.2020.9145976,PeTIT: Perceiving the Technological Innovation Trends via the Heuristic Model of Community Detection,IEEE,Conferences,"Patent analysis is widely used in many kinds of research, such as competitive intelligence analysis, technology trends perceiving, industrial distribution planning, macro-economy regulations, and so on. Based on the patent data, this paper proposed a novel algorithm, which named PeTIT, to perceive the technological innovation trends by applying the heuristic community detection model. The PeTIT algorithm included four steps: patent ontology extraction, technological innovation tree construction, technological innovation community detection, and technological innovation trends perceiving. We implemented the PeTIT algorithm on the real dataset of invention patents in the field of artificial intelligence in China, which ranged from Jun 1st, 2000 to May 1st, 2019. The results showed that the innovation situation of artificial intelligent was mainly concentrated in 10 fields. Moreover, the application field of intelligent driving has become one of the fastest-growing industries. Finally, the experimental results demonstrated that the cubic exponential smoothing model had a higher performance by perceiving the technological innovation trends.",https://ieeexplore.ieee.org/document/9145976/,2020 2nd International Conference on Computer Communication and the Internet (ICCCI),26-29 June 2020,ieeexplore
10.1109/FPA.1994.636094,Perception systems implemented in analog VLSI for real-time applications,IEEE,Conferences,"We point out that analog VLSI can now be considered as the ideal medium to implement computational systems intended to carry out real time perceptive or even cognitive tasks that are not well handled by traditional computers. By exploiting the analog features of the transistors, only a few devices are needed to realise most of the elementary functions required to implement perceptive systems, resulting in very dense, sophisticated circuits and low power consumption. Elementary artificial retinas in silicon based on their biological counterparts have already been successfully used in industrial applications. Artificial cochleas and noses are also under development. This new enabling technology is of great interest over a wide range of industrial sectors, including robotics, automotive, surveillance and food industry.",https://ieeexplore.ieee.org/document/636094/,Proceedings of PerAc '94. From Perception to Action,7-9 Sept. 1994,ieeexplore
10.1109/ICIEA.2006.257304,Performance Studies of Fuzzy Logic Based PI-like Controller Designed for Speed Control of Switched Reluctance Motor,IEEE,Conferences,"Switched reluctance motor (SRM) has gained significant interest in the field of industrial drive. The controller used to drive the machine is conventional PI controller. But the machine characteristics are very much nonlinear. This poses a problem for conventional controller design as regards to maintaining steady performance. There is also a need to adapt to the variable operating conditions. Fuzzy logic based heuristics is prospective since the exact analytical modelling of the system is difficult. PC implementation of the controller offers great flexibility in both design and maintenance phase. This work implements a PI like fuzzy logic controller (FLC) for SRM, which is found to work successfully in real time conditions. The work compares the performance of the FLC with respect to the conventional PI controller",https://ieeexplore.ieee.org/document/4025905/,2006 1ST IEEE Conference on Industrial Electronics and Applications,24-26 May 2006,ieeexplore
10.1109/CNSC.2014.6906671,Pixelwise object class segmentation based on synthetic data using an optimized training strategy,IEEE,Conferences,"In this paper we present an approach for low-level body part segmentation based on RGB-D data. The RGB-D sensor is thereby placed at the ceiling and observes a shared workspace for human-robot collaboration in the industrial domain. The pixelwise information about certain body parts of the human worker is used by a cognitive system for the optimization of interaction and collaboration processes. In this context, for rational decision making and planning, the pixelwise predictions must be reliable despite the high variability of the appearance of the human worker. In our approach we treat the problem as a pixelwise classification task, where we train a random decision forest classifier on the information contained in depth frames produced by a synthetic representation of the human body and the ceiling sensor, in a virtual environment. As shown in similar approaches, the samples used for training need to cover a broad spectrum of the geometrical characteristics of the human, and possible transformations of the body in the scene. In order to reduce the number of training samples and the complexity of the classifier training, we therefore apply an elaborated and coupled strategy for randomized training data sampling and feature extraction. This allows us to reduce the training set size and training time, by decreasing the dimensionality of the sampling parameter space. In order to keep the creation of synthetic training samples and real-world ground truth data simple, we use a highly reduced virtual representation of the human body, in combination with KINECT skeleton tracking data from a calibrated multi-sensor setup. The optimized training and simplified sample creation allows us to deploy standard hardware for the realization of the presented approach, while yielding a reliable segmentation in real-time, and high performance scores in the evaluation.",https://ieeexplore.ieee.org/document/6906671/,2014 First International Conference on Networks & Soft Computing (ICNSC2014),19-20 Aug. 2014,ieeexplore
10.1109/IROS.1991.174701,Planning based sensing and task executing in an autonomous machine,IEEE,Conferences,"Implementing a control system for an autonomous machine is a challenging task. Several techniques have to be applied, such as task planning, hierarchical and/or distributed control, and advanced sensing techniques. In addition, to be useful these various techniques have to be integrated into a system that has to operate more or less in real-time. The authors present a control scheme based on hierarchically organized planning-executing-monitoring-cycles which is used to solve some of the problems related to real-time control of an autonomous machine. The implementation is also presented in which the control system is applied in a pilot system based on an industrial robot.&lt;<ETX>&gt;</ETX>",https://ieeexplore.ieee.org/document/174701/,Proceedings IROS '91:IEEE/RSJ International Workshop on Intelligent Robots and Systems '91,3-5 Nov. 1991,ieeexplore
10.1109/INDIN45523.2021.9557519,Platform Generation for Edge AI Devices with Custom Hardware Accelerators,IEEE,Conferences,"In recent years artificial neural networks (NNs) have been at the center of research on data processing. However, their high computational demand often prohibits deployment on resource-constrained Industrial IoT Systems. Custom hardware accelerators can enable real-time NN processing on small-scale edge devices but are generally hard to develop and integrate. In this paper we present a hardware generation approach to rapidly create, test, and deploy entire SoC platforms with application-specific NN hardware accelerators. The feasibility of the approach is demonstrated by the generation of a condition monitoring system for high-speed valves.",https://ieeexplore.ieee.org/document/9557519/,2021 IEEE 19th International Conference on Industrial Informatics (INDIN),21-23 July 2021,ieeexplore
10.1109/SEC50012.2020.00019,Poster: Lambda architecture for robust condition based maintenance with simulated failure modes,IEEE,Conferences,"Condition based maintenance (CBM) is increasingly seen as a promising approach for addressing downtime issues which are a common occurrence in the manufacturing industry and are a major cause of lost productivity. However, it has been a challenge to develop a generic CBM solution that works for all assets since each asset has unique sources of noise. This mandates use of manual diagnostics to custom tailor a solution for each asset for accurate failure mode identification (FMI). This problem is further compounded by the scarcity of failure data. In this paper, we propose a lambda architecture for FMI of industrial assets that achieves low initial deployment cost while securing a reasonable classification accuracy. The lambda architecture consists of a light-compute edge node, such as Raspberry Pi, that processes high-speed vibration data in real-time to extract useful features and applies a deep-learning (DL) engine which is trained in a cloud platform, such as AWS. In addition, we also incorporate a failure modes' feature simulator so that DL models can adapt to different industrial assets without costly failure data collection. Finally, experimental results are provided using the bearings' failures dataset validating the proposed cost-effective CBM architecture with high accuracy and scalability.",https://ieeexplore.ieee.org/document/9355694/,2020 IEEE/ACM Symposium on Edge Computing (SEC),12-14 Nov. 2020,ieeexplore
10.1049/cp:19970735,Pre-processing of acoustic signals by neural networks for fault detection and diagnosis of rolling mill,IET,Conferences,"Incipient faults and changes in the structure of any industrial process may be detected and known by their effects: vibration and/or acoustic signals. We consider some methods for pre-processing the acoustic signal, generated by a rolling mill process, for fault detection and structural classification. The pre-processing methods are based on artificial neural networks. The methods refer to: signal decomposition algorithms, a distances measure for spectral amplitude classification and neural network structures for spectrum compression. For the signal decomposition problem an adaptive neural network algorithm is proposed in which the number of inputs is adapted to the imposed error. When the training error for two successive steps is very little, then the number of inputs in network is increased. If the spectral components are zero for sufficient time, then the number of inputs is decreased. The Hausdorff distance is proposed for spectrum classification as the distance measure for the frequency domain in a pattern recognition context. It shown that the Hausdorff distance has a monotone relationship with the signal-to-noise-ratio. Finally, the possibility of decreasing the number of spectrum components as patterns is presented, by compression with neural networks. Spectral representations of the acoustic source show that signatures collected at rolling mill sensor locations can be successfully used to identify process and structural changes in the rolling mill monitoring system. The results obtained by simulation is encouraging for real-time implementation.",https://ieeexplore.ieee.org/document/607526/,Fifth International Conference on Artificial Neural Networks (Conf. Publ. No. 440),7-9 July 1997,ieeexplore
10.1109/RTEICT49044.2020.9315660,Prediction of Air Quality in Industrial Area,IEEE,Conferences,"Air quality monitoring and prediction in many industrial and urban areas, it has become one of the most important activities. Owing to different types of pollution, air quality is heavily affected. With increasing air pollution, efficient air quality monitoring models is to be implemented; these models gather data on the concentration of air pollutants. In a proposed approach, to solve three problems- prediction, interpolation and feature analysis, previously these problems were solved using three different models but now in the proposed system can solve these three problems in one model i.e Air Pollutant Prediction. This approach relates to unlabeled spatiotemporal data to enhance interpolation efficiency and air quality prediction. Experiments to test the proposed solution based on the real-time data sources collected by the Karnataka State Pollution Control Board (KSPCB), India. The goal of this research paper is to explore various strategies based on machine learning techniques for monitoring and predicating the air quality.",https://ieeexplore.ieee.org/document/9315660/,"2020 International Conference on Recent Trends on Electronics, Information, Communication & Technology (RTEICT)",12-13 Nov. 2020,ieeexplore
10.1109/ARITH.2019.00047,Privacy-Preserving Deep Learning via Additively Homomorphic Encryption,IEEE,Conferences,"We aim at creating a society where we can resolve various social challenges by incorporating the innovations of the fourth industrial revolution (e.g. IoT, big data, AI, robot, and the sharing economy) into every industry and social life. By doing so the society of the future will be one in which new values and services are created continuously, making people's lives more conformable and sustainable. This is Society 5.0, a super-smart society. Security and privacy are key issues to be addressed to realize Society 5.0. Privacy-preserving data analytics will play an important role. In this talk we show our recent works on privacy-preserving data analytics such as privacy-preserving logistic regression and privacy-preserving deep learning. Finally, we show our ongoing research project under JST CREST “AI”. In this project we are developing privacy-preserving financial data analytics systems that can detect fraud with high security and accuracy. To validate the systems, we will perform demonstration tests with several financial institutions and solve the problems necessary for their implementation in the real world.",https://ieeexplore.ieee.org/document/8877418/,2019 IEEE 26th Symposium on Computer Arithmetic (ARITH),10-12 June 2019,ieeexplore
10.1109/GloSIC.2018.8570124,Probabilistic Estimations of Increasing Expected Reliability and Safety for Intelligent Manufacturing,IEEE,Conferences,"In the near future the possibilities of the modern probabilistic models, artificial intelligence and machine learning methods can provide an intelligent support of making decisions by an operator in real time. An agile recovery of intelligent manufacturing integrity can be implemented owing to the development of industrial robotics. For intelligent manufacturing it means the expected reliability and safety may be in the near future at the expense of intelligent support of decision making and the agile recovery of integrity. To answer the question “How much essential may be this increasing?” here are proposed: general analytical approaches for a probabilistic estimation of the expected reliability and safety for every monitored element or the system of intelligent manufacturing on a level of probability distribution functions (PDF) of the time between the losses of system integrity; estimations of increasing the expected reliability and safety for intelligent manufacturing at the expense of the intelligent support of decision making and agile recovery of integrity; the comparisons of the estimations on a prognostic period up to 10 years using the identical model in applications to expected reliability and safety. The applications of the proposed approaches allow the customers, designers, developers, users and experts of Industry 4.0 intelligent manufacturing to be guided by the proposed probabilistic estimations for solving problems of reliability and safety in the system life cycle. The results are demonstrated by examples.",https://ieeexplore.ieee.org/document/8570124/,2018 Global Smart Industry Conference (GloSIC),13-15 Nov. 2018,ieeexplore
10.1109/INDIN41052.2019.8972094,Probabilistic Modelling combined with a CNN for boundary detection of carbon fiber fabrics,IEEE,Conferences,For many industrial machine vision applications it is difficult to acquire good training data to deploy deep learning techniques. In this paper we propose a method based on probabilistic modelling and rendering to generate artificial images of carbon fiber fabrics. We deploy a convolutional neural network (CNN) to learn detection of fabric contours from artificially generated images. Our network largely follows the recently proposed U-Net architecture. We provide results for a set of real images taken under controlled lighting conditions. The method can easily be adapted to similar problems in quality control for composite parts.,https://ieeexplore.ieee.org/document/8972094/,2019 IEEE 17th International Conference on Industrial Informatics (INDIN),22-25 July 2019,ieeexplore
10.1109/IRCE.2018.8492918,Product Surface Defects Detection Based on Multiple-Kernel Learning Feature Fusion Method,IEEE,Conferences,"In order to achieve high accuracy and real-time requirement for product surface defect detection during the process of modern industrial production, a multi-feature fusion method based on Multiple-Kernel -Learning (MKL) is proposed. Firstly, HSV(Hue Saturation and Value) and SIFT(Scale-invariant feature transform) feature are extracted from the real -time acquisition of images, as well as the Multi-scale Equivalent Pattern Local Binary Pattern (MEP-LBP) feature presented in this paper. Secondly, according to the MKL method, three suitable kernel function were selected to train and classify of various defects. At the same time, in the process of detection, the multi-scale sliding window is generated according to the accuracy requirements of different surfaces in the captured images, so as to improve the detection performance. Experiment results show that the proposed method can meet the high accuracy requirements and ensure the real-time demand for industrial production detection.",https://ieeexplore.ieee.org/document/8492918/,2018 IEEE International Conference of Intelligent Robotic and Control Engineering (IRCE),24-27 Aug. 2018,ieeexplore
10.1109/ECCTD.2011.6043628,Production test of an RF receiver chain based on ATM combining RF BIST and machine learning algorithm,IEEE,Conferences,"Testing an RF device in Production is expensive and technically difficult. At Wafer Test level, the RF probing technologies hardly fulfil the industrial test requirements in terms of accuracy, reliability and cost. At Package test level testing the RF parameters requires expensive RF equipments (RF automated test equipments (ATE)) and for complex RF transceivers, which address multi-modes (RF multi-paths and/or requiring different impedance matchings), it usually leads to prohibitive test time. In order to reduce the test costs for RF devices, different methods are proposed and evaluated in NXP and at competitions. These methods mainly target test time reduction (e.g. by testing parts in parallel) or propose ways of limiting the needs of expensive RF tester in Production (e.g. by using Alternate Test Methods, Design For Test, or Built In Test). In the proposed presentation we will focus on ATM and RF BI(s)T, providing some results on DC-RF correlation and an example of a real case BIT implementation into a fully integrated single-chip receiver operating in the sub-GHz ISM bands 315 MHz to 920 MHz.",https://ieeexplore.ieee.org/document/6043628/,2011 20th European Conference on Circuit Theory and Design (ECCTD),29-31 Aug. 2011,ieeexplore
10.1109/ICPHM49022.2020.9187061,Prognostics by classifying degradation stage on Lambda architecture,IEEE,Conferences,"To enhance the reliability and availability of an asset in its life, predicting the remaining useful life of an asset is strongly encouraged by assessing the extent of deviation or degradation of the asset's monitored parameters from its expected normal operating conditions. Although intelligent fault prognostic techniques such as machine learning and artificial neural networks have been applied in modern industries, application in actual industrial conditions requires that the forecasting process is revealed and more descriptive. To investigate the issue and increase the accuracy, this paper proposes an additional technique that can be further applied to any recent intelligent prognostic methods. The proposed method consists of two steps. First, the entire training set is divided into several degradation stages before regression using a heuristic approach and then the regression results are synthesized for each stage. The proposed method will increase the monotonicity of the predictive parameters, thus helping improve the predictive model's accuracy. To demonstrate the hypothesis, real condition monitoring data of high-pressure LNG pump and acceleration experimental data of a rotating machine is used for an experiment. Moreover, a system in which the proposed method can be appropriately executed is introduced with Lambda architecture. Finally, by demonstrating that the proposed method is capable of parallel computing, it is proven suitable for use in the proposed large-scale distributed processing system.",https://ieeexplore.ieee.org/document/9187061/,2020 IEEE International Conference on Prognostics and Health Management (ICPHM),8-10 June 2020,ieeexplore
10.1109/PC.2017.7976254,Proposal of system for automatic weld evaluation,IEEE,Conferences,"The paper deals with the development of a system for automatic weld recognition using new information technologies based on cloud computing and single-board computer in the context of Industry 4.0. The proposed system is based on a visual system for weld recognition, and a neural network based on cloud computing for real-time weld evaluation, both implemented on a single-board low-cost computer. The proposed system was successfully verified on welding samples which correspond to a real welding process in the car production process. The system considerably contributes to the welds diagnostics in industrial processes of small- and medium-sized enterprises.",https://ieeexplore.ieee.org/document/7976254/,2017 21st International Conference on Process Control (PC),6-9 June 2017,ieeexplore
10.1109/ICIAI.2019.8850806,Pulse Interference Resilience of Convolutional Codes in WirelessHP Physical Layer Protocols: Experiment in Real Factory Environments,IEEE,Conferences,"To address the ultra-low latency and high-reliability requirements in critical applications of industrial wireless control, high performance wireless (WirelessHP) communications has demonstrated its superiority in this area. This paper steps further by investigating the pulse interference resilience of WirelessHP physical layer, and convolutional codes (CC) are adopted for short packet transmission. Through the constructed hardware platform, duration of the pulse interference and number of pulses in each transmitted packet are adjustable, based on which the packet error rate (PER) performance is tested in real factory environments. We show 100 ns duration pulse interference has a slight influence on the PER performance if only one pulse exists in the waveform of one packet, but lose this property for longer duration pulse interference. Lower-rated CC outperforms higher-rated CC on condition of the same pulse interference settings, but is more frangible to pulse interference that with the same interference rate.",https://ieeexplore.ieee.org/document/8850806/,2019 1st International Conference on Industrial Artificial Intelligence (IAI),23-27 July 2019,ieeexplore
10.1109/SoCPaR.2011.6089156,QoS-oriented Service Management in clouds for large scale industrial activity recognition,IEEE,Conferences,"Motivated by the need of industrial enterprises for supervision services for quality, security and safety guarantee, we have developed an Activity Recognition Framework based on computer vision and machine learning tools, attaining good recognition rates. However, the deployment of multiple cameras to exploit redundancies, the large training set requirements of our time series classification models, as well as general resource limitations together with the emphasis on real-time performance, pose significant challenges and lead us to consider a decentralized approach. We thus adapt our application to a new and innovative real-time enabled framework for service-based infrastructures, which has developed QoS-oriented Service Management mechanisms in order to allow cloud environments to facilitate real-time and interactivity. Deploying the Activity Recognition Framework in a cloud infrastructure can therefore enable it for large scale industrial environments.",https://ieeexplore.ieee.org/document/6089156/,2011 International Conference of Soft Computing and Pattern Recognition (SoCPaR),14-16 Oct. 2011,ieeexplore
10.1109/COGINF.2010.5599677,Quadratic neural unit is a good compromise between linear models and neural networks for industrial applications,IEEE,Conferences,"The paper discusses the quadratic neural unit (QNU) and highlights its attractiveness for industrial applications such as for plant modeling, control, and time series prediction. Linear systems are still often preferred in industrial control applications for their solvable and single solution nature and for the clarity to the most application engineers. Artificial neural networks are powerful cognitive nonlinear tools, but their nonlinear strength is naturally repaid with the local minima problem, overfitting, and high demands for application-correct neural architecture and optimization technique that often require skilled users. The QNU is the important midpoint between linear systems and highly nonlinear neural networks because the QNU is relatively very strong in nonlinear approximation; however, its optimization and performance have fast and convex-like nature, and its mathematical structure and the derivation of the learning rules is very comprehensible and efficient for implementation.",https://ieeexplore.ieee.org/document/5599677/,9th IEEE International Conference on Cognitive Informatics (ICCI'10),7-9 July 2010,ieeexplore
10.1109/ITherm51669.2021.9503225,"Quantifying Steam Dropwise Condensation Heat Transfer via Experiment, Computer Vision and Machine Learning Algorithms",IEEE,Conferences,"Condensation of water vapor on metal surfaces has a plethora of engineering applications in the energy sector. This study demonstrates an approach of estimating the condensation heat transfer performance of a hydrophobic copper plate utilizing Artificial Intelligence (AI) techniques. The situation under study is an isothermal vertical cooled plate with an attached thin copper sheet exposed to a water-vapor environment devoid of non-condensable gases. Water was chosen due to its high latent heat of vaporization and its extensive industrial use, while copper was selected because of its high thermal conductivity (385 W/m-K). A thin layer of Teflon coating was applied to the copper surface to render it hydrophobic, allowing dropwise condensation (DwC) to be investigated. The experimental setup featured a glass condensation chamber, which allowed visualization of the condensation phenomena on the condenser; a cold plate attached to a chiller, to hold and cool down the condenser; a vacuum pump to maintain low-pressure conditions; a heat source along with pressure and temperature measurement instrumentation. The condensation setup allows to directly calculate the heat transfer coefficient of dropwise condensation and, at the same time, offers visual access to the condenser. The overall efficiency of the dropwise condensation mechanism depends on many key factors, such as nucleation density and rate, maximum droplet size and efficient condensate drainage. The surface-condensation phenomena can be video-recorded while using the instrumentation to measure the heat-transfer efficiency in real time. Computer Vision algorithms were employed to process the raw video files and generate data on the basic aspects of condensing droplet dynamics (number, diameter, area coverage, etc.). The data was evaluated and cross-compared with the heat-transfer coefficients determined from instrumentation, allowing to consider how the droplet characteristics change with experimental operating conditions. Machine learning algorithms, such as Artificial Neural Network (ANN), Multi-Layer Perceptron (MLP), General Regression Neural Network (GRNN), and one ensemble model, Gradient Boosting Regression (GBR) can help to directly determine the heat transfer performance of a specific surface (with known wettability characteristics) from its condensation characteristics. The hypothesis is explored that the visual characteristics of the condensate droplet dynamics can be used in conjunction with trained machine learning models to forecast the DwC heat transfer efficiency of this system without directly measuring the corresponding variables (e.g., temperatures, flow rate, etc.).",https://ieeexplore.ieee.org/document/9503225/,2021 20th IEEE Intersociety Conference on Thermal and Thermomechanical Phenomena in Electronic Systems (iTherm),1-4 June 2021,ieeexplore
10.1109/CONFLUENCE.2019.8776960,Quasi-Automated Firmware in E-Automobiles: Structural Integration,IEEE,Conferences,"Intelligent transit mechanism has become the need of the hour. The topical panacea seems to centre on the development of labyrinthine Automated Vehicles. With the inevitable boom in areas like Machine Learning, Industrial Automation coupled with highly convergent heuristics and the advent of highly efficient low latency communication devices, the idea of Automated Vehicles inches closer to practical realization every passing day. Design for any vehicle can be described as a function of various interdependent parameters, which are generally defined by the level of convolution and the level of automation defined for a system.This study explores the controller system design of Quasi-Automated Electric Vehicles. Categorically, this paper is an attempt at examining novel and innovative ways of designing a schematic for control of speed and navigation subsystems along with an exhaustive feedback capability designed to give users a real time virtual emulation of the vehicle. A wide ranging discussion on possible topologies for effective implementation of feedback have also been depicted in this paper. A column-type power steering system has been investigated as a control system for navigation of direction while an inverter based speed control mechanism has also been proposed. Furthermore, the discussed control algorithms have been rigorously tested and consequently proved capable of providing 1<sup>st</sup> level of autonomy, as defined by Society of Automotive Engineers standards while also reflecting a potential schematic for integration of intelligent learning firmware in the near future.",https://ieeexplore.ieee.org/document/8776960/,"2019 9th International Conference on Cloud Computing, Data Science & Engineering (Confluence)",10-11 Jan. 2019,ieeexplore
10.1109/ISIE.2015.7281681,RFID indoor localization based on support vector regression and k-means,IEEE,Conferences,"Systems need to know the physical locations of objects and people to optimize user experience and solve logistical and security issues. Also, there is a growing demand for applications that need to locate individual assets for industrial automation. This work proposes an indoor positioning system (IPS) able to estimate the item-level location of stationary objects using off-the-shelf equipment. By using RFID technology, a machine learning model based on support vector regression (SVR) is proposed. A multi-frequency technique is developed in order to overcome off-the-shelf equipment constraints. A k-means approach is also applied to improve accuracy. We have implemented our system and evaluated it using real experiments. The localization error is between 17 and 31 cm in 2.25m<sup>2</sup> area coverage.",https://ieeexplore.ieee.org/document/7281681/,2015 IEEE 24th International Symposium on Industrial Electronics (ISIE),3-5 June 2015,ieeexplore
10.1109/AQTR49680.2020.9129934,Rapid Prototyping of IoT Applications for the Industry,IEEE,Conferences,"In this article a novel approach to rapid IoT application prototype design and development is presented using an existing experimental dataset and a functional model. Using the existing data, we populate our NoSQL Apache Cassandra database cluster with legacy data and generate similar data using a python code, considering the Mosquitto MQTT protocol implementation and Node-RED node.js development environment. Using Node-RED, we display the data already collected, and dynamically create new data that can be monitored in real-time in the provided dashboard. The possibilities and utility of this approach are explored in the article, and a simple prototype application for modeling the open access Combined Cycle Power Plant (CCPP) dataset provided by the UCI Machine Learning Repository is presented to prove the efficiency and rapidity of IoT application development. The presented system development approach can be used in industrial environment for rapid development of IIoT applications.",https://ieeexplore.ieee.org/document/9129934/,"2020 IEEE International Conference on Automation, Quality and Testing, Robotics (AQTR)",21-23 May 2020,ieeexplore
10.1109/ICRA48506.2021.9562075,Reaching Pruning Locations in a Vine Using a Deep Reinforcement Learning Policy,IEEE,Conferences,"We outline a neural network-based pipeline for perception, control and planning of a 7 DoF robot for tasks that involve reaching into a dormant grapevine canopy. The proposed system consists of a 6 DoF industrial robot arm and a linear slider that can actuate on an entire grape vine. Our approach uses Convolutional Neural Networks to detect buds in dormant grape vines and a Reinforcement Learning based control strategy to reach desired cut-point locations for pruning tasks. Within this framework, three methodologies are developed and compared to reach the desired locations: the learned policy-based approach (RL), a hybrid method that uses the learned policy and an inverse kinematics solver (RL+IK), and lastly a classical approach commonly used in robotics. We first tested and validated the suitability of the proposed learning methodology in a simulated environment that resembled laboratory conditions. A reaching accuracy of up to 61.90% and 85.71% for the RL and RL+IK approaches respectively was obtained for a vine that the agent observed while learning. When testing in a new vine, the accuracy was up to 66.66% and 76.19% for RL and RL+IK, respectively. The same methods were then deployed on a real system in an end to end procedure: autonomously scan the vine using a vision system, create its model and finally use the learned policy to reach cutting points. The reaching accuracy obtained in these tests was 73.08%.",https://ieeexplore.ieee.org/document/9562075/,2021 IEEE International Conference on Robotics and Automation (ICRA),30 May-5 June 2021,ieeexplore
10.1109/ICIT.2006.372319,Real Time Classifier For Industrial Wireless Sensor Network Using Neural Networks with Wavelet Preprocessors,IEEE,Conferences,"Wireless sensor node is embedded of computation unit, sensing unit and a radio unit for communication. Amongst three units communication is the largest consumer of energy. Energy is the prime source for wireless sensor node to function. Hence every aspects of sensor node are designed with energy constraints. Neural Networks in particular the combination of ART1 and FuzzyART(FA) can be used very efficiently for developing Real time Classifier. Wireless sensor networks demand for the real time classification of sensor data. In this paper classification technique using ART1 and Fuzzy ART is discussed. ART1 and FA have very good architectural strategy, which makes it simple for VLSI implementation. The VLSI implementation of the proposed classifier can be a part of embedded microsensor. The paper discusses classification technique, which can reduce the energy need for communication and improves communications bandwidth. The proposed sensor clustering architecture can give distributed storage space for the sensor networks. Wavelet Transform is used as preprocessor for denoising the real word data from sensor node, this makes it much suitable for industrial environment. Many methods of wavelet transforms are available. Simplest Haar 1D transform is used for preprocessing and smoothing the sensor signals. The discrete wavelet transform implemented here helps to extract important feature in the sensor data like sudden changes at various scales.",https://ieeexplore.ieee.org/document/4237641/,2006 IEEE International Conference on Industrial Technology,15-17 Dec. 2006,ieeexplore
10.1109/ICCICT.2018.8325873,Real time control of induction motor using neural network,IEEE,Conferences,"Induction Machine being simple in operation and highly reliable equipment with low cost involving minimal maintenance requirements it has become the most popular equipment in industry. With the development of power electronic technology, low cost DSP, micro-controllers and parameter estimation techniques the induction motor an attractive component for the future high performance drives induction motors have many applications in the industries. The PWM technique which drives a Voltage Source Inverter (VSI) in order to apply v/f control is used to control a 3 ph induction motor. In industrial applications the most widely used controllers are PI controllers because of their simple structure and their capability of delivering good performance over a large band of operating condition. PI and ANN controllers have been designed and developed using MATLAB/SIMULINK. Prototype model is developed to validate the effectiveness of the PI and ANN control of induction motor drive using dSPACE DS1104 controller. The performance of the SVPWM based induction motor in open loop and closed loop is presented with simulation. Artificial Neural Network and Conventional PI controllers have been practically implemented using SVPWM based VSI fed induction motor in open loop mode. Hardware set up has been developed using Inverter and dSPACE controller. The real time performance of ANN based induction motor is presented by validating simulation results with the hardware results.",https://ieeexplore.ieee.org/document/8325873/,2018 International Conference on Communication information and Computing Technology (ICCICT),2-3 Feb. 2018,ieeexplore
10.1109/ICCCE.2008.4580693,Real time implementation of NARMA L2 feedback linearization and smoothed NARMA L2 controls of a single link manipulator,IEEE,Conferences,"Robotics is a field of modern technology which requires knowledge in vast areas such as electrical engineering, mechanical engineering, computer science as well as finance. Nonlinearities and parametric uncertainties are unavoidable problems faced in controlling robots in industrial plants. Tracking control of a single link manipulator driven by a permanent magnet brushed DC motor is a nonlinear dynamics due to effects of gravitational force, mass of the payload, posture of the manipulator and viscous friction coefficient. Furthermore uncertainties arise because of changes of the rotor resistance with temperature and random variations of friction while operating. Due to this fact classical PID controller can not be used effectively since it is developed based on linear system theory. Neural network control schemes for manipulator control problem have been proposed by researchers; in which their competency is validated through simulation studies. On the other hand, actual real time applications are rarely established. Instead of simulation studies, this paper is aimed to implement neural network controller in real time for controlling a DC motor driven single link manipulator. The work presented in this paper is concentrating on neural NARMA L2 control and its improvement called to as Smoothed NARMA L2 control. As proposed by K. S Narendra and Mukhopadhyay, Narma L2 control is one of the popular neural network architectures for prediction and control. The real time experimentation showed that the Smoothed NARMA L2 is effective for controlling the single link manipulator for both point-to-point and continuous path motion control.",https://ieeexplore.ieee.org/document/4580693/,2008 International Conference on Computer and Communication Engineering,13-15 May 2008,ieeexplore
10.1109/INDUSCON51756.2021.9529474,Real-Time Downhole Geosteering Data Processing Using Deep Neural Networks On FPGA,IEEE,Conferences,"The success of machine learning has spread the deployment of Deep neural Networks (DNNs) in numerous industrial applications. As an essential technique in today’s oilfield industry, geosteering requires performing DNN inference on the hardware devices that operates under the severe down-hole environments. However, it can produce massive power dissipation and cause long delays to execute the computation-intensive DNN inference on the current hardware platforms, e.g., CPU and GPU. In this paper, we propose an FPGA-based hardware design to efficiently conduct the DNN inference for geosteering tasks in downhole environments. At first, a comprehensive analysis is presented to choose the optimal computation mapping method for the target DNN model. A detailed description of the customized hardware implementation is then proposed to accomplish a complete DNN inference on the FPGA board. The experimental results shows that the proposed design achieves 7× (1.4×) improvement on performance and 82× (1.3×) reduction on power consumption compared with CPU(GPU).",https://ieeexplore.ieee.org/document/9529474/,2021 14th IEEE International Conference on Industry Applications (INDUSCON),15-18 Aug. 2021,ieeexplore
10.1109/08IAS.2008.164,Real-Time Implementation of Intelligent Modeling and Control Techniques on a PLC Platform,IEEE,Conferences,"Programmable logic controllers (PLCs) have been used for many decades for standard control in industrial and factory environments. Over the years, PLCs have become computational efficient and powerful, and a robust platform with applications beyond the standard control and factory automation. Due to the new advanced PLC's features and computational power, they are ideal platforms for exploring advanced modeling and control methods, including computational intelligence based techniques such as neural networks, particle swarm optimization (PSO) and many others. Some of these techniques require fast floating-point calculations that are now possible in real-time on the PLC. This paper focuses on the Allen-Bradley ControlLogix brand of PLCs, due to their high performance and extensive use in industry. The design and implementation of a neurocontroller consisting of two neural networks, one for modeling and the other for control, and the training of these neural networks with particle swarm optimization is presented in this paper on a single PLC. The neurocontroller in this study is a power system stabilizer (PSS) that is used for power system oscillation damping. The PLC is interfaced to a power system simulated on the real time digital simulator. Real time results are presented showing that the PLC is a suitable hardware platform for implementing advanced modeling and control techniques for industrial applications.",https://ieeexplore.ieee.org/document/4658952/,2008 IEEE Industry Applications Society Annual Meeting,5-9 Oct. 2008,ieeexplore
10.1109/EDPC51184.2020.9388185,Real-Time Inference of Neural Networks on FPGAs for Motor Control Applications,IEEE,Conferences,"Machine learning algorithms are increasingly used in industrial applications for a multitude of use-cases. However, using them in control tasks is a challenge due to real-time requirements and limited resources. In this paper, an implementation scheme for real-time inference of multilayer perceptron (MLP) neural networks on FPGAs is proposed. Design constraints for using MLPs in reinforcement learning agents for motor control applications are derived and accounted for in the implementation. Two MLP architectures are evaluated on an FPGA, and the timing and resource-usage data are reported. The real-time capability of the implementation for motor control applications is investigated for standard control frequencies. It is shown by experimental validation that real-time interference with an area-efficient implementation for motor control applications is achievable. Therefore, the proposed implementation scheme can be applied to deep reinforcement learning controllers with hard real-time requirements.",https://ieeexplore.ieee.org/document/9388185/,2020 10th International Electric Drives Production Conference (EDPC),8-9 Dec. 2020,ieeexplore
10.1109/I2CACIS.2019.8825093,Real-Time Robotic Grasping and Localization Using Deep Learning-Based Object Detection Technique,IEEE,Conferences,"This work aims to increase the impact of computer vision on robotic positioning and grasping in industrial assembly lines. Real-time object detection and localization problem is addressed for robotic grasp-and-place operation using Selective Compliant Assembly Robot Arm (SCARA). The movement of SCARA robot is guided by deep learning-based object detection for grasp task and edge detection-based position measurement for place task. Deep Convolutional Neural Network (CNN) model, called KSSnet, is developed for object detection based on CNN Alexnet using transfer learning approach. SCARA training dataset with 4000 images of two object categories associated with 20 different positions is created and labeled to train KSSnet model. The position of the detected object is included in prediction result at the output classification layer. This method achieved the state-of-the-art results at 100% precision of object detection, 100% accuracy for robotic positioning and 100% successful real-time robotic grasping within 0.38 seconds as detection time. A combination of Zerocross and Canny edge detectors is implemented on a circular object to simplify the place task. For accurate position measurement, the distortion of camera lens is removed using camera calibration technique where the measured position represents the desired location to place the grasped object. The result showed that the robot successfully moved to the measured position with positioning Root Mean Square Error (0.361, 0.184) mm and 100% for successful place detection.",https://ieeexplore.ieee.org/document/8825093/,2019 IEEE International Conference on Automatic Control and Intelligent Systems (I2CACIS),29-29 June 2019,ieeexplore
10.1109/ICSIMA47653.2019.9057343,Real-Time Wireless Monitoring for Three Phase Motors in Industry: A Cost-Effective Solution using IoT,IEEE,Conferences,"In recent days modern environment industries are facing rapid flourishing for performance capabilities and their requirements for corporate clients and industrial sector. Internet of Things (IoT) is an innovative and rapidly growing field for automation and evaluation in networks, Artificial Intelligence, data sensing, data mining, and big data. These systems have a great tendency to monitor and control different process used in industries. IoT systems have been implemented and have applications in different industries due to their cost-effectiveness and flexibility In this paper we have developed a system which includes real-time monitoring of current reading of three-phase motor through a wireless network. With the help of this system, data can be saved and monitored and then transmitted to cloud storage. This system contains Arduino-UNO board, ACS-712 current sensor, ESP-8266 Wi-Fi module which sends information to an IoT API service THING-SPEAK that behave like a cloud for various sensors to monitor data. The proposed system was successfully deployed in Aisha Steel Mills, Karachi, Pakistan.",https://ieeexplore.ieee.org/document/9057343/,"2019 IEEE International Conference on Smart Instrumentation, Measurement and Application (ICSIMA)",27-29 Aug. 2019,ieeexplore
10.1109/COMPSAC51774.2021.00070,Real-time End-to-End Federated Learning: An Automotive Case Study,IEEE,Conferences,"With the development and the increasing interests in ML/DL fields, companies are eager to apply Machine Learning/Deep Learning approaches to increase service quality and customer experience. Federated Learning was implemented as an effective model training method for distributing and accelerating time-consuming model training while protecting user data privacy. However, common Federated Learning approaches, on the other hand, use a synchronous protocol to conduct model aggregation, which is inflexible and unable to adapt to rapidly changing environments and heterogeneous hardware settings in real-world scenarios. In this paper, we present an approach to real-time end-to-end Federated Learning combined with a novel asynchronous model aggregation protocol. Our method is validated in an industrial use case in the automotive domain, focusing on steering wheel angle prediction for autonomous driving. Our findings show that asynchronous Federated Learning can significantly improve the prediction performance of local edge models while maintaining the same level of accuracy as centralized machine learning. Furthermore, by using a sliding training window, the approach can minimize communication overhead, accelerate model training speed and consume real-time streaming data, proving high efficiency when deploying ML/DL components to heterogeneous real-world embedded systems.",https://ieeexplore.ieee.org/document/9529467/,"2021 IEEE 45th Annual Computers, Software, and Applications Conference (COMPSAC)",12-16 July 2021,ieeexplore
10.1109/CRV50864.2020.00032,Real-time Motion Planning for Robotic Teleoperation Using Dynamic-goal Deep Reinforcement Learning,IEEE,Conferences,"We propose Dynamic-goal Deep Reinforcement Learning (DGDRL) method to address the problem of robot arm motion planning in telemanipulation applications. This method intuitively maps human hand motions to a robot arm in real-time, while avoiding collisions, joint limits and singularities. We further propose a novel hardware setup, based on the HTC VIVE VR system, that enables users to smoothly control the robot tool position and orientation with hand motions, while monitoring its movements in a 3D virtual reality environment. A VIVE controller captures 6D hand movements and gives them as reference trajectories to a deep neural policy network for controlling the robot's joint movements. Our DGDRL method leverages the state-of-art Proximal Policy Optimization (PPO) algorithm for deep reinforcement learning to train the policy network with the robot joint values and reference trajectory observed at each iteration. Since training the network on a real robot is time-consuming and unsafe, we developed a simulation environment called RobotPath which provides kinematic modeling, collision analysis and a 3D VR graphical simulation of industrial robots. The deep neural network trained using RobotPath is then deployed on a physical robot (ABB IRB 120) to evaluate its performance. We show that the policies trained in the simulation environment can be successfully used for trajectory planning on a real robot. The the codes, data and video presenting our experiments are available at https://github.com/kavehkamali/ppoRobotPath.",https://ieeexplore.ieee.org/document/9108691/,2020 17th Conference on Computer and Robot Vision (CRV),13-15 May 2020,ieeexplore
10.1109/IJCNN48605.2020.9207462,"Real-time anomaly intrusion detection for a clean water supply system, utilising machine learning with novel energy-based features",IEEE,Conferences,"Industrial Control Systems have become a priority domain for cybersecurity practitioners due to the number of cyber-attacks against those systems has increased over the past few years. This paper proposes a real-time anomaly intrusion detector for a model of a clean water supply system. A testbed of such system is implemented using the Festo MPA Control Process Rig. A set of attacks to the testbed is conducted during the control process operation. During the attacks, the energy of the components is monitored and recorded to build a novel dataset for training and testing a total of five traditional supervised machine learning algorithms: K-Nearest Neighbour, Support Vector Machine, Decision Tree, Naïve Bayes and Multilayer Perceptron. The trained machine learning algorithms were built and deployed online, during the control system operation, for further testing. The performance obtained from offline and online training and testing steps are compared. The captures results show that KNN and SVM outperformed the rest of the algorithms by achieving high accuracy scores and low false-positive, false-negative alerts.",https://ieeexplore.ieee.org/document/9207462/,2020 International Joint Conference on Neural Networks (IJCNN),19-24 July 2020,ieeexplore
10.1109/ANTHOLOGY.2013.6784984,Real-time cloud computing for web-based searching system of pattern recognition,IEEE,Conferences,"For cross-platform real-time systems, cloud computing technology is an innovative application of pattern recognition method. This study is the use of associative memory of the way to do the work of pattern recognition; this system is real-time client-server type network pattern recognition system. Remote user can operate through the browser to draw the shape or character of industrial components, and recognition system to the database through Internet search. Cloud storage server contains a database of pattern samples. In the training period, the user can specify any of the pattern to what in real-time. Patterns are recorded in the cloud server database. In the recall period, an innovative database matching methods have been proposed. This method can effectively solve the problem of RNN a false state of the database than on the technology to overcome the problem of capacity constraints RNN. In this new approach, CWBPR system partition database in the cloud server, a pattern record set, and then figure out they were separate sections for each value of W and θ. CWBPR system to deal with each of the last segment of the pattern recognition work. Pattern recognition technology for the network, the paper has two simulation experiments are clearly discussed. The first experiment identified a number of characters; the second experiment is the pattern recognition of industrial components. Finally, the paper also put forward innovative pattern recognition method to the traditional text input search method comparison.",https://ieeexplore.ieee.org/document/6784984/,IEEE Conference Anthology,1-8 Jan. 2013,ieeexplore
10.1109/ISIE.2010.5636556,Real-time evaluation of power quality using FPGA based measurement system,IEEE,Conferences,"Real-time evaluation of power quality is a desired feature in research and industrial projects, especially when embedded systems are employed and/or studied. Fast Fourier Transforms FFT is commonly used to evaluate the harmonic content of electric signals. Artificial Neural Networks (ANN) are also employed for harmonics estimation with short processing time and low implementation complexity. Commercial power quality measurement systems are available and offer good performance, communication and storage capabilities, and other special features, however in most of them the real-time information is not available or it is offered with important communication delays. This paper presents the implementation of a measurement system using Xilinx FPGA target and the Adaptive Linear Neuron (ADALINE) algorithm for real-time evaluation of power quality. Experimental results show that the implemented system can be employed for power quality monitoring and embedded control applications.",https://ieeexplore.ieee.org/document/5636556/,2010 IEEE International Symposium on Industrial Electronics,4-7 July 2010,ieeexplore
10.1109/SACI51354.2021.9465544,Real-time locating system and digital twin in Lean 4.0,IEEE,Conferences,"Digital twin plays a key role in the current development of smart manufacturing systems. Through simulation in the cyber world, real phenomena in the physical world can be predicted and optimized before the final implementation. The usage of the digital twin is enhanced along with the uprising of Industry 4.0, in which data availability supports the further insight of system status, helping the operation managers understand their system and perform resources adjustment more easily. Based on this digitization mature, Lean 4.0, a new concept elaborated from Lean manufacturing, has been interested recently. There are several technologies constituted digital twin that provide a favourable condition for Lean 4.0, such as augmented reality, cloud computing. In this paper, the integration of the Real-time Locating System (RTLS) into digital twin is proposed, which facilitates the performance of Lean 4.0 in manufacturing operation. Not only gain effective control over the facility's assets, but this integration also enhances the resources utilization, cut down operational wastes, thus brings a better turnover for industrial systems. A case study of successful implementation is shown, which proved the possible advantages of this approach.",https://ieeexplore.ieee.org/document/9465544/,2021 IEEE 15th International Symposium on Applied Computational Intelligence and Informatics (SACI),19-21 May 2021,ieeexplore
10.1109/PES.2003.1270511,Real-time power quality waveform recognition with a programmable digital signal processor,IEEE,Conferences,"Power quality (PQ) monitoring is an important issue to electric utilities and many industrial power customers. This paper presents a DSP-based hardware monitoring system based on a recently proposed PQ classification algorithm. The algorithm is implemented with a Texas Instruments (TI) TMS320VC5416 digital signal processor (DSP) with the TI THS1206 12-bit 6 MSPS analog to digital converter. A TI TMS320VC5416 DSP starter kit (DSK) is used as the host board with the THS1206 mounted on a daughter card. The implemented PQ classification algorithm is composed of two processes: feature extraction and classification. The feature extraction projects a PQ signal onto a time-frequency representation (TFR), which is designed for maximizing the separability between classes. The classifiers include a Heaviside-function linear classifier and neural networks with feedforward structures. The algorithm is optimized according to the architecture of the DSP to meet the hard realtime constraints of classifying a 5-cycle segment of the 60 Hz sinusoidal voltage/current signals in power systems. The classification output can be transmitted serially to an operator interface or control mechanism for logging and issue resolution.",https://ieeexplore.ieee.org/document/1270511/,2003 IEEE Power Engineering Society General Meeting (IEEE Cat. No.03CH37491),13-17 July 2003,ieeexplore
10.1109/ICCCN52240.2021.9522281,Realization of an Intrusion Detection use-case in ONAP with Acumos,IEEE,Conferences,"With Software-Defined Networking and Machine Learning/Artificial Intelligence (ML/AI) reaching new paradigms in their corresponding fields, both academia and industry have exhibited interests in discovering unique aspects of intelligent and autonomous communication networks. Transforming such intentions and interests to reality involves software development and deployment, which has its own story of significant evolution. There has been a notable shift in the strategies and approaches to software development. Today, the divergence of tools and technologies as per demand is so substantial that adapting a software application from one environment to another could involve tedious redesign and redevelopment. This implies enormous effort in migrating existing applications and research works to a modern industrial setup. Additionally, the struggles with sustainability maintenance of such applications could be painful. Concerning ML/AI, the capabilities to train, deploy, retrain, and re-deploy AI models as quickly as possible will be crucial for AI-driven network systems. An end-to-end workflow using unified open-source frameworks is the need of the hour to facilitate the integration of ML/AI models into the modern software-driven virtualized communication networks. Hence, in our paper, we present such a prototype by demonstrating the journey of a sample SVM classifier from being a python script to be deployed as a micro-service using ONAP and Acumos. While illustrating various features of Acumos and ONAP, this paper intends to make readers familiar with an end-to-end workflow taking advantage of the integration of both open-source platforms.",https://ieeexplore.ieee.org/document/9522281/,2021 International Conference on Computer Communications and Networks (ICCCN),19-22 July 2021,ieeexplore
10.1109/NAFIPS.2005.1548498,Reasoning about uncertainty in prognosis: a confidence prediction neural network approach,IEEE,Conferences,"Uncertainty representation and management is the Achilles heel of fault prognosis in condition based management systems. Long-term prediction of the time to failure of critical military and industrial systems entails large-grain uncertainty that must be represented effectively and managed efficiently, i.e. as more data becomes available, means must be devised to narrow or ""shrink"" the uncertainty bounds. Prediction accuracy and precision are typical performance metrics employed to access the performance of prognostic algorithms. That is, we would like the predicted time to failure to be as close as possible to the real one. Also, the bounds or limits of uncertainty must be as ""narrow"" as possible. This paper introduces a novel confidence prediction neural network construct with a confidence distribution node based on a Parzen estimate to represent uncertainty and a learning algorithm implemented as a lazy or Q-learning routine that improves online prognostics estimates. The approach is illustrated with test and simulation results obtained from a faulty helicopter planetary gear plate.",https://ieeexplore.ieee.org/document/1548498/,NAFIPS 2005 - 2005 Annual Meeting of the North American Fuzzy Information Processing Society,26-28 June 2005,ieeexplore
10.1109/IECON.2012.6389018,Recent advances in the application of real-time computational intelligence to industrial electronics,IEEE,Conferences,"The field of computational intelligence [CI] has seen advances in both the theoretical knowledge base of these techniques, and in specific applications of these techniques to real-world problems. This work first attempts to summarize the current trends and definitions in the CI branches of fuzzy systems, artificial neural networks [ANNs], and hybrid neuro-fuzzy systems and their variants. These particular branches of CI are selected for their ability to be implemented in real-time problem solving, whether computation and processing is done in software or implemented in hardware. Then, some current applications of these CI technologies for use in industrial electronics are highlighted and summarized.",https://ieeexplore.ieee.org/document/6389018/,IECON 2012 - 38th Annual Conference on IEEE Industrial Electronics Society,25-28 Oct. 2012,ieeexplore
10.1109/ICARM52023.2021.9536145,Reducing the Dimension of the Configuration Space with Self Organizing Neural Networks,IEEE,Conferences,"For robotics, especially industrial applications, it is crucial to reactively plan safe motions through efficient algorithms. Planning is more powerful in the configuration space than the task space. However, for robots with many degrees of freedom, this is challenging and computationally expensive. Sophisticated techniques for motion planning such as the Wavefront algorithm are limited by the high dimensionality of the configuration space, especially for robots with many degrees of freedom. For a neural implementation of the Wavefront algorithm in the configuration space, neurons represent discrete configurations and synapses are used for path planning. In order to decrease the complexity, we reduce the search space by pruning superfluous neurons and synapses. We present different models of self-organizing neural networks for this reduction. The approach takes real-life human motion data as input and creates a representation with reduced dimension. We compare six different neural network models and adapt the Wavefront algorithm to the different structures of the reduced output spaces. The method is backed up by an extensive evaluation of the reduced spaces, including their suitability for path planning by the Wavefront algorithm.",https://ieeexplore.ieee.org/document/9536145/,2021 6th IEEE International Conference on Advanced Robotics and Mechatronics (ICARM),3-5 July 2021,ieeexplore
10.1109/ICIEAM.2017.8076111,Redundant industrial manipulator control system,IEEE,Conferences,"We present the control system synthesis for the multilink redundant manipulator. Our control system is based on the unique algorithm that includes the novel hybrid method for solving the inverse kinematics problem. This method combines ANFIS-network and iterative refinement. As a result, the control system has high integrative capabilities and is easy to modify for another construction. The manipulator design is described by mathematical equations which are used for the workspace construction. These equations are used for creation of the neurofuzzy network and generation database (network training information). Modeling of the industrial manipulator with 5 degrees of freedom as an example of the implementation of our control system is considered in the paper. Virtual environment that displays a model motion in real time using a virtual 3-D model is also presented in the paper. We present the work results applied to the manipulator physical model. This model includes Festo servomotors and the Siemens programmable logical controller.",https://ieeexplore.ieee.org/document/8076111/,"2017 International Conference on Industrial Engineering, Applications and Manufacturing (ICIEAM)",16-19 May 2017,ieeexplore
10.1109/INDIN45582.2020.9442114,Reinforcement Learning Driven Adaptive VR Streaming with Optical Flow Based QoE,IEEE,Conferences,"With the merit of containing full panoramic content in one camera, Virtual Reality (VR) and 360° videos have arisen in the field of industrial cloud manufacturing and training. Industrial Internet of Things (IoT), where many VR terminals needed to be online at the same time, can hardly guarantee VR's bandwidth requirement. However, by making use of users' quality of experience (QoE) awareness factors, including the relative moving speed and depth difference between the viewpoint and other content, bandwidth consumption can be reduced. In this paper, we propose Optical Flow Based VR(OFB-VR), an interactive method of VR streaming that can make use of VR users' QoE awareness to ease the bandwidth pressure. The Just-Noticeable Difference through Optical Flow Estimation (JND-OFE) is explored to quantify users' awareness of quality distortion in 360° videos. Accordingly, a novel 360° videos QoE metric based on Peak Signal-to-Noise Ratio and JND-OFE (PSNR-OF) is proposed. With the help of PSNR-OF, OFB-VR proposes a versatile-size tiling scheme to lessen the tiling overhead. A Reinforcement Learning (RL) method is implemented to make use of historical data to perform Adaptive BitRate (ABR). For evaluation, we take two prior VR streaming schemes, Pano and Plato, as baselines. Vast evaluations show that our system can increase the mean PSNR-OF score by 9.5-15.8% while maintaining the same rebuffer ratio compared with Pano and Plato in a fluctuate LTE bandwidth dataset. Evaluation results show that OFB-VR is a promising prototype for actual interactive industrial VR. A prototype of OFB-VR can be found in https://github.com/buptexplorers/OFB-VR.",https://ieeexplore.ieee.org/document/9442114/,2020 IEEE 18th International Conference on Industrial Informatics (INDIN),20-23 July 2020,ieeexplore
10.1109/ICISS.2010.5656975,Research and implementation Of the temperature control system of heat treatment based on .NET and RS-485 bus,IEEE,Conferences,"RS-485 is a widely used industrial field bus. The successful application of AIBUS protocol in AI series display control instrument, make the AIDCS system's cost significantly lower than traditional DCS system. The paper successfully developed a prototype system based on RS-485 bus for high precision temperature control system of heat treatment, and based on the .net and AIBUS protocol, developed it' s system management software. This system have the characteristics of good real-time, high control-precision, high degree of automation, and friendly human-machine interface.",https://ieeexplore.ieee.org/document/5656975/,2010 International Conference on Intelligent Computing and Integrated Systems,22-24 Oct. 2010,ieeexplore
10.1109/RCAR49640.2020.9303282,Robot Programming by Demonstration with Oral Instructions for Assembly,IEEE,Conferences,"Programming by demonstration has been seen as a feasible solution for transferring human's skills to robots without too much time and labor cost. So far, applications of programming by demonstration in industrial assembly have attracted many researchers' attention. In practice, the robot control policy must give consideration to both efficiency and precision. Furthermore, it is difficult for one policy to handle the whole assembly process. To deal with these issues, a programming by demonstration with oral instructions method is developed in this article. With oral instructions, the demonstration data are segmented into pre-assembly phase and precise assembly phase. Moreover two related assembly policies are learned independently. Task-parametrized Gaussian mixture model and dynamic movement primitive are selected to prestructure the assembly policies for the two phases respectively on accounting of their properties. Effectiveness of the proposed method has been demonstrated by an assembly experiment.",https://ieeexplore.ieee.org/document/9303282/,2020 IEEE International Conference on Real-time Computing and Robotics (RCAR),28-29 Sept. 2020,ieeexplore
10.1109/MECHATRONIKA.2014.7018286,Robot imitation of human arm via Artificial Neural Network,IEEE,Conferences,"In this study, a robot arm that can imitate human arm is designed and presented. The potentiometers are located to the joints of the human arm in order to detect movements of human gestures, and data were collected by this way. The collected data named as “movement of human arm” are classified by the help of Artificial Neural Network (ANN). The robot performs its movements according to the classified movements of the human. Real robot and real data are used in this study. Obtained results show that the learning application of imitating human action via the robot was successfully implemented. With this application, the platforms of robot arm in an industrial environment can be controlled more easily; on the other hand, robotic automation systems which have the capability of making a standard movements of a human can become more resistant to the errors.",https://ieeexplore.ieee.org/document/7018286/,Proceedings of the 16th International Conference on Mechatronics - Mechatronika 2014,3-5 Dec. 2014,ieeexplore
10.1109/ICIAS.2012.6306173,SCARA robot control using neural networks,IEEE,Conferences,"A SCARA industrial robot model is identified based on a 4-axis structure using Lagrangian mechanics, also the dynamic model for the electromechanical actuator and motion transmission systems is identified. A conventional PD controller is implemented and compared to neural networks control system to achieve precise position control of SCARA manipulator. The performance of the modeled system is simulated using several desired tracking motion for each joint. Neural networks control method has shown a remarkable improvement of tracking capabilities for the SCARA robot over conventional PD controller. The proposed neural network controller has the potential to accurately control real-time manipulator applications.",https://ieeexplore.ieee.org/document/6306173/,2012 4th International Conference on Intelligent and Advanced Systems (ICIAS2012),12-14 June 2012,ieeexplore
10.1109/I2MTC50364.2021.9460075,SNR-based Reinforcement Learning Rate Adaptation for Time Critical Wi-Fi Networks: Assessment through a Calibrated Simulator,IEEE,Conferences,"Nowadays, the Internet of Things is spreading in several different research fields, such as factory automation, instrumentation and measurement, and process control, where it is referred to as Industrial Internet of Things. In these scenarios, wireless communication represents a key aspect to guarantee the required pervasive connectivity required. In particular, Wi-Fi networks are revealing ever more attractive also in time- and mission-critical applications, such as distributed measurement systems. Also, the multi-rate support feature of Wi-Fi, which is implemented by rate adaptation (RA) algorithms, demonstrated its effectiveness to improve reliability and timeliness. In this paper, we propose an enhancement of RSIN, which is a RA algorithm specifically conceived for industrial real-time applications. The new algorithm starts from the assumption that an SNR measure has been demonstrated to be effective to perform RA, and bases on Reinforcement Learning techniques. In detail, we start from the design of the algorithm and its implementation on the OmNet++ simulator. Then, the simulation model is adequately calibrated exploiting the results of a measurement campaign, to reflect the channel behavior typical of industrial environments. Finally, we present the results of an extensive performance assessment that demonstrate the effectiveness of the proposed technique.",https://ieeexplore.ieee.org/document/9460075/,2021 IEEE International Instrumentation and Measurement Technology Conference (I2MTC),17-20 May 2021,ieeexplore
10.1109/WF-IoT.2016.7845468,SQenloT: Semantic query engine for industrial Internet-of-Things gateways,IEEE,Conferences,"The Advent of Internet-of-Things (IoT) paradigm has brought exciting opportunities to solve many real-world problems. IoT in industries is poised to play an important role not only to increase productivity and efficiency but also to improve customer experiences. Two main challenges that are of particular interest to industry include: handling device heterogeneity and getting contextual information to make informed decisions. These challenges can be addressed by IoT along with proven technologies like the Semantic Web. In this paper, we present our work, SQenIoT: a Semantic Query Engine for Industrial IoT. SQenIoT resides on a commercial product and offers query capabilities to retrieve information regarding the connected things in a given facility. We also propose a things query language, targeted for resource-constrained gateways and non-technical personnel such as facility managers. Two other contributions include multi-level ontologies and mechanisms for semantic tagging in our commercial products. The implementation details of SQenIoT and its performance results are also presented.",https://ieeexplore.ieee.org/document/7845468/,2016 IEEE 3rd World Forum on Internet of Things (WF-IoT),12-14 Dec. 2016,ieeexplore
10.1109/SMARTCOMP52413.2021.00052,SWIMS: the Smart Wastewater Intelligent Management System,IEEE,Conferences,"Wastewater treatment is a critical process in urban and industrial settlements aiming to clean and protect the water as well as the overall environment. Wastewater management systems are conceived explicitly for purifying wastewater, providing clean water efficiently, but this is a hard task due to frequent and quite unpredictable fluctuations of inlet wastewater flows, arising from (random) rain water or (periodical, e.g. day-night) sewage sources, sometimes also leading to failures and outages. To ensure the quality of the clean water out above a threshold and keep the overall system operating, this paper proposes the smart wastewater intelligent management system (SWIMS). It monitors and controls inlet and outlet flows as well as the water quality and parts of the plant as a cyber-physical system (CPS), starting from an Environmental Internet of Things (EIoT) platform. The data generated from the treatment plant is collected in an information system hosted by a server together with an intelligent system that processes this information in a real-time fashion and provides the feedback for optimizing the plant to maintain a good quality of water over time. Such an intelligent system exploits deep learning approaches to control the behaviour of the wastewater treatment system through anomaly detection, supporting decision making on it. SWIMS has been implemented in a real case study deployed in Briatico, Italy. The data and results collected from such a case study are presented, analyzed and discussed in this paper, demonstrating the feasibility and the effectiveness of the SWIMS solution.",https://ieeexplore.ieee.org/document/9556275/,2021 IEEE International Conference on Smart Computing (SMARTCOMP),23-27 Aug. 2021,ieeexplore
10.1109/AVSS.2018.8639339,Scene Adaptation for Semantic Segmentation using Adversarial Learning,IEEE,Conferences,"Semantic Segmentation algorithms based on the deep learning paradigm have reached outstanding performances. However, in order to achieve good results in a new domain, it is generally demanded to fine-tune a pre-trained deep architecture using new labeled data coming from the target application domain. The fine-tuning procedure is also required when the domain application settings change, e. g., when a camera is moved, or a new camera is installed. This implies the collection and pixel-wise la-beling of images to be used for training, which slows down the deployment of semantic segmentation systems in real industrial scenarios and increases the industrial costs. Taking into account the aforementioned issues, in this paper we propose an approach based on Adversarial Learning to perform scene adaptation for semantic segmentation. We frame scene adaptation as the task of predicting semantic segmentation masks for images belonging to a Target Scene Context given labeled images coming from a Source Scene Context and unlabeled images coming from the Target Scene Context. Experiments highlight that the proposed method achieves promising performances both when the two scenes contain similar content (i.e., they are related to two different points of view of the same scene) and when the observed scenes contain unrelated content (i.e., they account to completely different scenes).",https://ieeexplore.ieee.org/document/8639339/,2018 15th IEEE International Conference on Advanced Video and Signal Based Surveillance (AVSS),27-30 Nov. 2018,ieeexplore
10.1109/OCEANS.2016.7761412,Self-tuned PID control based on backpropagation Neural Networks for underwater vehicles,IEEE,Conferences,"For a long time, PID-like controllers have been successfully used in academic and industrial tasks. This is thanks to its simplicity and suitable performance in linear or linearized plants, and under certain conditions, in nonlinear ones. A number of PID controller gains tuning approaches have been proposed in the literature in the last decades; most of them off-line techniques. However, in those cases wherein plants are subject to continuous parametric changes or external disturbances, online gains tuning is a need. This is the case of modular underwater ROVs (Remotely Operated Vehicles) where parameters (weight, buoyancy, added mass, among others) change according to the tool they are fitted with. In practice, some amount of time is dedicated to tune the PID gains of a ROV. Once the best set of gains has been achieved the ROV is ready to work. However, when the vehicle changes its tool or it is subject to ocean currents, its performance deteriorates since the fixed set of gains is no longer valid for the new conditions. Thus, an online PID gains tuning algorithm should be implemented to overcome this problem. In this paper, an auto-tuned PID-like controller based on Neural Networks (NN) is proposed. The NN plays the role of automatically estimating the suitable set of PID gains that achieves stability of the system. The NN adjusts online the controller gains that attain the smaller position tracking error. Simulation results are given considering an underactuated 6 DOF (degrees of freedom) underwater ROV. Real time experiments on an underactuated mini ROV are conducted to show the effectiveness of the proposed scheme.",https://ieeexplore.ieee.org/document/7761412/,OCEANS 2016 MTS/IEEE Monterey,19-23 Sept. 2016,ieeexplore
10.1109/DS-RT.2007.38,Semantic Web Service Architecture for Simulation Model Reuse,IEEE,Conferences,"COTS simulation packages (CSPs) have proved popular in an industrial setting with a number of software vendors. In contrast, options for re-using existing models seem more limited. Re-use of simulation component models by collaborating organizations is restricted by the same semantic issues however that restrict the inter-organization use of web services. The current representations of web components are predominantly syntactic in nature lacking the fundamental semantic underpinning required to support discovery on the emerging semantic web. Semantic models, in the form of ontology, utilized by web service discovery and deployment architecture provide one approach to support simulation model reuse. Semantic interoperation is achieved through the use of simulation component ontology to identify required components at varying levels of granularity (including both abstract and specialized components). Selected simulation components are loaded into a CSP, modified according to the requirements of the new model and executed. The paper presents the development of ontology, connector software and web service discovery architecture in order to understand how such ontology are created, maintained and subsequently used for simulation model reuse. The ontology is extracted from health service simulation - comprising hospitals and the National Blood Service. The ontology engineering framework and discovery architecture provide a novel approach to inter- organization simulation, uncovering domain semantics and adopting a less intrusive interface between participants. Although specific to CSPs the work has wider implications for the simulation community.",https://ieeexplore.ieee.org/document/4384541/,11th IEEE International Symposium on Distributed Simulation and Real-Time Applications (DS-RT'07),22-26 Oct. 2007,ieeexplore
10.1109/ETFA.2012.6489781,Semantic design and integration of simulation models in the industrial automation area,IEEE,Conferences,"Simulations are software tools approximating and predicting the behavior of real industrial plants. Unlike real plants, the utilization of simulations cannot cause damages and it saves time and costs during series of experiments. A shortcoming of current simulation models is the complicated runtime integration into legacy industrial systems and platforms, as well as ad-hoc design phase, introducing manual and error-prone work. This paper contributes to improve the efficiency of simulation model design and integration. It utilizes a semantic knowledge base, implemented by ontologies and their mappings. The integration uses the Automation Service Bus and the paper explains how to configure the runtime integration level semantically. The main contributions are the concept of semantic configuration of the service bus and the workflows of simulation design and integration.",https://ieeexplore.ieee.org/document/6489781/,Proceedings of 2012 IEEE 17th International Conference on Emerging Technologies & Factory Automation (ETFA 2012),17-21 Sept. 2012,ieeexplore
10.1109/ETFA.2014.7005195,Semantic repository for case-based reasoning in CBM services,IEEE,Conferences,"Condition-based maintenance (CBM) has been implemented in industry to arrange the maintenance work as efficiently as possible. Case-based reasoning (CBR) can be used to automate part of the CBM decision process. However, in complex situations the final decisions have to be made by domain maintenance experts based on information gathered from several sources. This paper presents an approach for utilizing Semantic Web technologies and CBR in a knowledge base system supporting CBM services. The case knowledge base (CKB) is built over a semantic repository with an inference engine supporting ontology based information integration and data access using SPARQL queries. The knowledge base model developed for the system contains CBR task ontology and domain ontology for industrial control valves. Feasibility of the prototype CKB system was evaluated in experiments with real industrial case data.",https://ieeexplore.ieee.org/document/7005195/,Proceedings of the 2014 IEEE Emerging Technology and Factory Automation (ETFA),16-19 Sept. 2014,ieeexplore
10.1109/CVPR.2019.00715,ShieldNets: Defending Against Adversarial Attacks Using Probabilistic Adversarial Robustness,IEEE,Conferences,"Defending adversarial attack is a critical step towards reliable deployment of deep learning empowered solutions for industrial applications. Probabilistic adversarial robustness (PAR), as a theoretical framework, is introduced to neutralize adversarial attacks by concentrating sample probability to adversarial-free zones. Distinct to most of the existing defense mechanisms that require modifying the architecture/training of the target classifier which is not feasible in the real-world scenario, e.g., when a model has already been deployed, PAR is designed in the first place to provide proactive protection to an existing fixed model. ShieldNet is implemented as a demonstration of PAR in this work by using PixelCNN. Experimental results show that this approach is generalizable, robust against adversarial transferability and resistant to a wide variety of attacks on the Fashion-MNIST and CIFAR10 datasets, respectively.",https://ieeexplore.ieee.org/document/8953209/,2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),15-20 June 2019,ieeexplore
10.1109/RO-MAN50785.2021.9515431,Simplifying the A.I. Planning modeling for Human-Robot Collaboration,IEEE,Conferences,"For an effective deployment in manufacturing, Collaborative Robots should be capable of adapting their behavior to the state of the environment and to keep the user safe and engaged during the interaction. Artificial Intelligence (AI) enables robots to autonomously operate understanding the environment, planning their tasks and acting to achieve some given goals. However, the effective deployment of AI technologies in real industrial environments is not straightforward. There is a need for engineering tools facilitating communication and interaction between AI engineers and Domain experts. This paper proposes a novel software tool, called TENANT (Tool fostEriNg Ai plaNning in roboTics) whose aim is to facilitate the use of AI planning technologies by providing domain experts like e.g., production engineers, with a graphical software framework to synthesize AI planning models abstracting from syntactic features of the underlying planning formalism.",https://ieeexplore.ieee.org/document/9515431/,2021 30th IEEE International Conference on Robot & Human Interactive Communication (RO-MAN),8-12 Aug. 2021,ieeexplore
10.1109/IntelCIS.2015.7397201,Simulation Model of the Decision-Making Support for Human-Machine Systems Operators,IEEE,Conferences,Simulation Model for the Decision-Making Support of the Human-Machine Systems Operator was developed. The term of operator's professional confidence as an index of the operator's ability to maintain or for a certain time lead to a stabile state of the Human-Machine Systems by performing the motor operator activity as the final component of the implementation of the decision was proposed. The motor operator's activity speed typed classification was developed. Algorithm and tool for the identification of the motor operator's activity speed of the Human-Machine Systems were offered. The Simulation Experiment Results were presented. The ways of the Simulation Model industrial realization and implementation as part of the real Automated Decision Support System were formulated.,https://ieeexplore.ieee.org/document/7397201/,2015 IEEE Seventh International Conference on Intelligent Computing and Information Systems (ICICIS),12-14 Dec. 2015,ieeexplore
10.23919/AEIT.2018.8577226,Smart Farms for a Sustainable and Optimized Model of Agriculture,IEEE,Conferences,"Nowadays, public and private companies, are in a constant race to increase profitability, chasing the costs reduction while facing the market competition. Also in the agriculture an analysis of cost-effectiveness, measuring technological innovation and profitability becomes necessary. The `smart farm' model exploits information coming from technologies like sensors, intelligent systems and the Internet of Things (IoT) paradigm to understand the influential and non-influential factors while considering environmental, productive and structural data coming from a large number of sources. The goal of this work is to design and deploy practical tasks that exploit heterogeneous real datasets with the aim to forecast and reconstruct values using and comparing innovative machine learning techniques with more standard ones. The application of these methodologies, in fields that are only apparently refractory to the technology such as the agricultural one, shows that there are ample margins for innovation and investment while supporting requests and needs coming from companies that wish to employ a sustainable and optimized agricultural industrial business.",https://ieeexplore.ieee.org/document/8577226/,2018 AEIT International Annual Conference,3-5 Oct. 2018,ieeexplore
10.1109/IEMCON.2018.8615072,Smart Mirror - a Secured Application of Artificial Intelligence Recognizing Human Face and Voice,IEEE,Conferences,"Smart mirrors are a brand new addition to the IoT product family that has been obtaining a great deal of attention in recent years by each industrial makers and hobbyists. This paper describes the planning associated implementation of a voice controlled wall mirror, referred to as “Magic Mirror” with Artificial Intelligence for the home environment. It is a mirror, which can display real time content like time, date, weather and news at the same time. The Magic Mirror consists of functionalities like real time information and data updates, voice commands, face recognition. The user can control the magic mirror by voice commands.",https://ieeexplore.ieee.org/document/8615072/,"2018 IEEE 9th Annual Information Technology, Electronics and Mobile Communication Conference (IEMCON)",1-3 Nov. 2018,ieeexplore
10.1109/ICCPEIC.2017.8290335,Smart personalized learning system for energy management in buildings,IEEE,Conferences,"Integration of energy management systems into existing buildings brings in several challenges and financial constraints. Some of the challenges in the existing smart building solutions are that they require large-scale deployment of sensors, high rate of data collection, real-time data analysis in short span of time, and lack of knowledge about the energy usage with respect to the behavior of individuals and groups. This work proposes an affordable wearable device system as an alternative for large-scale deployment of sensors in industrial buildings. For effective energy management in the buildings, a personalized behavior analysis has been done in machine learning and neural networks algorithm and integrated with the proposed system. The complete system is implemented and tested extensively. The results show that the proposed system could provide 85% user comfort and 23% energy savings.",https://ieeexplore.ieee.org/document/8290335/,"2017 International Conference on Computation of Power, Energy Information and Commuincation (ICCPEIC)",22-23 March 2017,ieeexplore
10.1109/IHMSC.2009.155,Soft Sensor Modeling Using SVR Based on Genetic Algorithm and Akaike Information Criterion,IEEE,Conferences,"Support vector regression (SVR) is one of the new methods of soft sensor modeling for estimating the products of metabolism in microorganism fermentations. The accuracy of SVR is mainly impacted by two factors: input variables selection and parameters set in SVR training procedures. But it is difficult to select the input variables and set the parameters. A novel method of soft sensor modeling is proposed based on akaike information criterion (AIC) and genetic algorithm (GA) to overcome the difficulties. Moreover, a real experiment process-erythromycin fermentation process is used to evaluate the performance of the proposed soft sensor modeling method. Results show the accuracy of the estimation is improved and the number of the input variables is reduced by the proposed approach, and the presented method could have a promising application in industrial process.",https://ieeexplore.ieee.org/document/5336030/,2009 International Conference on Intelligent Human-Machine Systems and Cybernetics,26-27 Aug. 2009,ieeexplore
10.1109/CCAA.2016.7813753,Soft computing techniques to address various issues in wireless sensor networks: A survey,IEEE,Conferences,"Wireless sensor network (WSN) is a collection of large number of self-organized types of sensors which chain together to monitor and record physical or environmental conditions (i.e. used to measure temperature, sound, pressure) and passes gathered information to the central location. WSN build bridge between real world and virtual environment, which makes it more utilizable for many applications. Mainly WSN was used for military arena but now a days it is used in various area like industrial applications, consumer applications, health care applications and many more. Despite of having many advantages there are some issues also occurred in WSNs like hotspot problem, energy hole problem, routing, coverage problem, load balancing problem and so on. These issues effect on different factors of WSN named energy consumption, stability, quality, deployment time, lifetime of network, which degrade the performance of the WSN. To solve these issues various researchers develop different mechanisms. Among all of them, in this paper, we survey different kind of soft computing paradigms. Soft computing is a technique to use of improper solutions to solve the complicated problem in robust time. There are various types of soft computing techniques developed: swarm intelligence, fuzzy logic, neural network, reinforcement learning and evolutionary algorithm, which used to solve WSN problems so that performance of the network will be increased.",https://ieeexplore.ieee.org/document/7813753/,"2016 International Conference on Computing, Communication and Automation (ICCCA)",29-30 April 2016,ieeexplore
10.1109/MELCON.2010.5476042,Soft sensors and artificial intelligence for nuclear fusion experiments,IEEE,Conferences,"Soft sensors are mathematical models able to estimate process variables. They can work in parallel with hardware sensors, and can be implemented at a low-cost on existing hardware. They are useful for back-up of measuring devices, reduction of measuring hardware requirements, real-time estimation for monitoring and control, sensor validation, fault detection and diagnosis, what-if analysis. In industrial applications, data-driven approaches, especially based on soft-computing techniques, are very promising. In this paper we review important issues in soft sensor design and applications, especially concerning the applications in the field of nuclear fusion.",https://ieeexplore.ieee.org/document/5476042/,Melecon 2010 - 2010 15th IEEE Mediterranean Electrotechnical Conference,26-28 April 2010,ieeexplore
10.1109/PQ.2016.7724114,Speed control for a permanent magnet synchronous motor based on fuzzy logic with reduced perturbations on the supply network,IEEE,Conferences,"Fuzzy logic controllers, FLCs, are elements of the artificial intelligence, and, today, they are widely use in command and control of many industrial processes. It involves the acquisition of better signal wave forms at the output and the reduction of all oscillations (in case of electric drives, we consider speed variation, torque variation, current variation for all the three phases ore more) in comparison with the classic control procedures. This paper presents two case studies of two FLCs applied for some exterior permanent magnet synchronous machines, PMSM. One fuzzy logic algorithm has 25 rules and the other has 49 rules. These FLCs are developed for less computational issues, which make them suitable for real time implementation. The permanent magnet synchronous machines are used today in state of the art drives due to their dynamic performances. It is known that PMSM has a high torque density, with reduced losses/torque rates, a high power factor and a quick torque and speed time response. All the references concerning the real time implementation are made on the digital signal processor controller board type DS1104.",https://ieeexplore.ieee.org/document/7724114/,2016 Electric Power Quality and Supply Reliability (PQ),29-31 Aug. 2016,ieeexplore
10.1109/ICRITO48877.2020.9198036,State of Art: Energy Efficient Protocols for Self-Powered Wireless Sensor Network in IIoT to Support Industry 4.0,IEEE,Conferences,"Up gradation of manufacturing systems in industries by means of implementing innovative manufacturing techniques that captures real time data, applies machine learning algorithms, makes entire system self-decisive and provides inter connectivity to the whole system is the prime focus of the Industry 4.0. It is aimed at bringing new industrial revolution with the help of internet of things technology due to its considerable influence in the industrial manufacturing process. Though impact of internet of things in industrial sector is huge, a practical implementation incorporates challenges in energy efficiency, self-powered sensor nodes and security. For processing of gathered data self-powered sensor nodes may sinks energy from ambient energy sources. A considerable amount of efforts has been put by researchers to address the challenges for development of energy efficient routing protocol for such nodes. The presented survey is aimed at analyzing the protocols for contribution towards the goals of industry 4.0 Energy efficient protocol will support the system to consume least energy for its operation. Henceforth combination of self-powered wireless sensor network and energy efficient protocol will be useful to power up many industrial IoT applications.",https://ieeexplore.ieee.org/document/9198036/,"2020 8th International Conference on Reliability, Infocom Technologies and Optimization (Trends and Future Directions) (ICRITO)",4-5 June 2020,ieeexplore
10.1109/ICPST.2002.1053583,State self-adaptive monitor and control system for 6 kV/1600 kVA adjustable-speed drive,IEEE,Conferences,"Based on proposed state self-adaptive monitor logic and volts per hertz (V/f) control method with current and voltage limitation, a compact operation monitor and control system (OMCS) is developed for real-time monitoring and control of a 6 kV/1600 kVA adjustable-speed drive (ASD) installation. The hardware of OMCS is based on industrial PC and composed of AI and DI data acquisition cards and TMS320C32 DSP based pulse system with specially designed configuration to exempt from strong electromagnetic interference. Implemented under Chinese Window 98 operating system with VC6.0 programming language, the software of OMCS can provide friendly man-machine interface with multi-functions. Experimental results show that the proposed monitor and control system can reliably supervise the operation conditions of ASD and efficiently control acceleration and deceleration of the induction motor without violating limits of current and voltage.",https://ieeexplore.ieee.org/document/1053583/,Proceedings. International Conference on Power System Technology,13-17 Oct. 2002,ieeexplore
10.1109/IECON.2016.7793206,Summer school on intelligent agents in automation: Hands-on educational experience on deploying industrial agents,IEEE,Conferences,"Cyber-physical systems constitutes a framework to develop intelligent, distributed, resilient, collaborative and cooperative systems, promoting the fusion of computational entities and physical devices. Agent technology plays a crucial role to develop this kind of systems by offering a decentralized, distributed, modular, robust and reconfigurable control structure. This paper describes the implementation of a summer school aiming to enhance the participants' knowledge in the field of multi-agent systems applied to industrial environments, being able to gain the necessary theoretical and practical skills to develop real industrial agent based applications. This is accomplished in an international framework where individual knowledge and experiences are shared in a complementary level.",https://ieeexplore.ieee.org/document/7793206/,IECON 2016 - 42nd Annual Conference of the IEEE Industrial Electronics Society,23-26 Oct. 2016,ieeexplore
10.1109/IJCNN.2006.246929,Synthesis of Pulsed-Coupled Neural Networks in FPGAs for Real-Time Image Segmentation,IEEE,Conferences,"This paper describes the implementation of a system based on Pulse Coupled Neural Networks (PCNNs) and Field Programmable Gate Arrays (FPGAs). The PCNN implemented is oriented to the industrial application of segmentation in sequences of images. The work went through several real physical stages of implementation and optimization to achieve the needed performance. The greatest performance achieved by the digital system was of 250M pixels per second, enough to process a sequence of images in real time. Details of these stages about the neuron implementation with different Altera's FPGAs families are presented. Furthermore, the implementation is compared with previous implemented schemes based on floating point DSP microprocessor.",https://ieeexplore.ieee.org/document/1716657/,The 2006 IEEE International Joint Conference on Neural Network Proceedings,16-21 July 2006,ieeexplore
10.1109/CSR51186.2021.9527936,TRUSTY: A Solution for Threat Hunting Using Data Analysis in Critical Infrastructures,IEEE,Conferences,"The rise of the Industrial Internet of Things (IIoT) plays a crucial role in the era of hyper-connected digital economies. Despite the valuable benefits, such as increased resiliency, self-monitoring and pervasive control, IIoT raises severe cybersecurity and privacy risks, allowing cyberattackers to exploit a plethora of vulnerabilities and weaknesses that can lead to disastrous consequences. Although the Intrusion Detection and Prevention Systems (IDPS) constitute valuable solutions, they suffer from several gaps, such as zero-day attacks, unknown anomalies and false positives. Therefore, the presence of supporting mechanisms is necessary. To this end, honeypots can protect the real assets and trap the cyberattackers. In this paper, we provide a web-based platform called TRUSTY , which is capable of aggregating, storing and analysing the detection results of multiple industrial honeypots related to Modbus/Transmission Control Protocol (TCP), IEC 60870-5-104, BACnet, Message Queuing Telemetry Transport (MQTT) and EtherNet/IP. Based on this analysis, we provide a dataset related to honeypot security events. Moreover, this paper provides a Reinforcement Learning (RL) method, which decides about the number of honeypots that can be deployed in an industrial environment in a strategic way. In particular, this decision is converted into a Multi-Armed Bandit (MAB), which is solved with the Thompson Sampling (TS) method. The evaluation analysis demonstrates the efficiency of the proposed method.",https://ieeexplore.ieee.org/document/9527936/,2021 IEEE International Conference on Cyber Security and Resilience (CSR),26-28 July 2021,ieeexplore
10.1109/ICSE-Companion.2019.00131,Testing Untestable Neural Machine Translation: An Industrial Case,IEEE,Conferences,"Neural Machine Translation (NMT) has shown great advantages and is becoming increasingly popular. However, in practice, NMT often produces unexpected translation failures in its translations. While reference-based black-box system testing has been a common practice for NMT quality assurance during development, an increasingly critical industrial practice, named in-vivo testing, exposes unseen types or instances of translation failures when real users are using a deployed industrial NMT system. To fill the gap of lacking test oracles for in-vivo testing of NMT systems, we propose a new methodology for automatically identifying translation failures without reference translations. Our evaluation conducted on real-world datasets shows that our methodology effectively detects several targeted types of translation failures. Our experiences on deploying our methodology in both production and development environments of WeChat (a messenger app with over one billion monthly active users) demonstrate high effectiveness of our methodology along with high industry impact.",https://ieeexplore.ieee.org/document/8802818/,2019 IEEE/ACM 41st International Conference on Software Engineering: Companion Proceedings (ICSE-Companion),25-31 May 2019,ieeexplore
10.1109/icABCD49160.2020.9183853,The Impact of Smart Manufacturing Approach On The South African Manufacturing Industry,IEEE,Conferences,"SM is a technology-driven approach that mainly utilises machines to monitor the entire production of an organisation. The objective of SM in an organisation is to identify ways to automatize the manufacturing process while using data analytics to optimize the manufacturing performance. This research mitigates the impact of technology, in this case expressed as SM in South African industries. The research followed a quantitative approach whereby 42 respondents from low, medium low and high technology industries took part in the study. Data has been amassed from first-hand experience by mean of an adapted questionnaire constituted of three sections: The first section was about the general demographic information of the respondents. Section two investigates the respondent's awareness on SM. Finally, section three assessed the impact that SM had on the performance of the organisation. The findings of this study revealed that Smart Manufacturing has a positive impact in South African manufacturing organisations as it allows effective operations, fast response to customers demand, real time operations optimisation. Nevertheless, Smart Manufacturing is a new concept under the fourth industrial revolution in South Africa and will need time before being totally implemented in all organisations as it is costly.",https://ieeexplore.ieee.org/document/9183853/,"2020 International Conference on Artificial Intelligence, Big Data, Computing and Data Communication Systems (icABCD)",6-7 Aug. 2020,ieeexplore
10.1109/AIMS52415.2021.9466046,The Implementation of Building Intelligent Smart Energy using LSTM Neural Network,IEEE,Conferences,"Internet of Things (IoT) makes many devices getting smarter and more connected in the 4.0 industrial revolution. One of the implementations of the Internet of Things is smart energy. It allows communication between humans or between things that make a building smarter. This paper proposes the implementation of the MQTT-based smart meter. The smart meter is used to make it easier for users to monitor and manage the energy consumption of buildings in real-time. It is considered as the main component of a smart network to make efficient and manage energy consumption remotely. Taking into account the increasing demand for electricity in Indonesia, smart meters can reduce overall energy use and reduce global warming by optimizing energy utilization through the internet of things and artificial intelligence. This paper proposes the implementation of the MQTT-based smart meter. This smart meter can measure energy consumption, transmit information related to the energy used, and provide an early warning system to stakeholders through the website in real-time analytics with predictive data on the following month and what days are most used to support energy consumption efficiency planning. This study conducted LTSM and ARIMA to determine forecasting energy consumption with 59 epochs, 8 batch sizes, 64 hidden layers with the results of MSE Error, RMSE Error, Mean Accuracy 0.14,0.373, and 95.16%, respectively. This result is better than ARIMA with MSE error results of 0.812 and 0.66 and RMSE error.",https://ieeexplore.ieee.org/document/9466046/,2021 International Conference on Artificial Intelligence and Mechatronics Systems (AIMS),28-30 April 2021,ieeexplore
10.1109/IVS.1994.639471,The development of a fully autonomous ground vehicle (FAGV),IEEE,Conferences,"As a first step toward the creation of a fully autonomous vehicle that operates in a real world environment, we are currently developing a prototype autonomous ground vehicle (AGV) for use in factories and other industrial/business sites based on behavior-based artificial intelligence (AI) control. This flexible and fully autonomous AGV (FAGV) is expected to operate efficiently in a normal industrial environment without any external guidance. The crucial technique employed is a non-Cartesian way of organizing software agents for the creation of a highly responsive control program. The resulting software is considerably reduced in size. Through numerous experiments using mobile robots we confirmed that these new control programs excel in functionality, efficiency, flexibility and robustness. The second key technique in the planning stage is evolutionary computation, of which genetic algorithms are a principal technique. An online, real-time evolution of the control program will be incorporated in later phases of the project to make FAGVs adaptable to any given operational environment after deployment. The first prototype FAGV has an active vision and behaviour-based control system.",https://ieeexplore.ieee.org/document/639471/,Proceedings of the Intelligent Vehicles '94 Symposium,24-26 Oct. 1994,ieeexplore
10.1109/ICACITE51222.2021.9404738,The learning approaches using Augmented Reality in learning environments: Meta-Analysis,IEEE,Conferences,"With the emergence of Industrial Revolution 4.0, the educational settings are changing quickly. Augmented Reality (AR) is one of the upcoming technologies. AR enhances the real world by overlaying/augmenting the virtual/digital information over it. It provides the user with the ability to interact with the created virtual world in real space. The aim of this study is to classify the learning approaches implemented through AR technology. The technique used for the analysis is derived from systematic search of online literature databases like Taylor Francis, Web of Science, Springer, ScienceDirect and Scopus. The keywords used for the search include learning approaches, AR, AR in education, AR in learning and teaching and integration approaches. The findings of this research work highlights 4 categories of educational learning approaches that highlight AR. The approaches are experimental learning, game-based, interactive and collaborative learning. The research findings can be referred by other researchers and educators to identify the potential of AR in education and the learning approaches currently used with AR for their further research on how these approaches can be effectively and efficiently implemented in educational settings.",https://ieeexplore.ieee.org/document/9404738/,2021 International Conference on Advance Computing and Innovative Technologies in Engineering (ICACITE),4-5 March 2021,ieeexplore
10.1109/RTAS48715.2020.000-1,Timing of Autonomous Driving Software: Problem Analysis and Prospects for Future Solutions,IEEE,Conferences,"The software used to implement advanced functionalities in critical domains (e.g. autonomous operation) impairs software timing. This is not only due to the complexity of the underlying high-performance hardware deployed to provide the required levels of computing performance, but also due to the complexity, non-deterministic nature, and huge input space of the artificial intelligence (AI) algorithms used. In this paper, we focus on Apollo, an industrial-quality Autonomous Driving (AD) software framework: we statistically characterize its observed execution time variability and reason on the sources behind it. We discuss the main challenges and limitations in finding a satisfactory software timing analysis solution for Apollo and also show the main traits for the acceptability of statistical timing analysis techniques as a feasible path. While providing a consolidated solution for the software timing analysis of Apollo is a huge effort far beyond the scope of a single research paper, our work aims to set the basis for future and more elaborated techniques for the timing analysis of AD software.",https://ieeexplore.ieee.org/document/9113112/,2020 IEEE Real-Time and Embedded Technology and Applications Symposium (RTAS),21-24 April 2020,ieeexplore
10.1109/IECON.2016.7793038,Tool compensation in walk-through programming for admittance-controlled robots,IEEE,Conferences,"This paper describes a walk-through programming technique, based on admittance control and tool dynamics compensation, to ease and simplify the process of trajectory learning in common industrial setups. In the walk-through programming, the human operator grabs the tool attached at the robot end-effector and “walks” the robot through the desired positions. During the teaching phase, the robot records the positions and then it will be able to interpolate them to reproduce the trajectory back. In the proposed control architecture, the admittance control allows to provide a compliant behavior during the interaction between the human operator and the robot end-effector, while the algorithm of compensation of the tool dynamics allows to directly use the real tool in the teaching phase. In this way, the setup used for the teaching can directly be the one used for performing the reproduction task. Experiments have been performed to validate the proposed control architecture and a pick and place example has been implemented to show a possible application in the industrial field.",https://ieeexplore.ieee.org/document/7793038/,IECON 2016 - 42nd Annual Conference of the IEEE Industrial Electronics Society,23-26 Oct. 2016,ieeexplore
10.1109/CoASE.2014.6899348,Toward safe close-proximity human-robot interaction with standard industrial robots,IEEE,Conferences,"Allowing humans and robots to interact in close proximity to each other has great potential for increasing the effectiveness of human-robot teams across a large variety of domains. However, as we move toward enabling humans and robots to interact at ever-decreasing distances of separation, effective safety technologies must also be developed. While new, inherently human-safe robot designs have been established, millions of industrial robots are already deployed worldwide, which makes it attractive to develop technologies that can turn these standard industrial robots into human-safe platforms. In this work, we present a real-time safety system capable of allowing safe human-robot interaction at very low distances of separation, without the need for robot hardware modification or replacement. By leveraging known robot joint angle values and accurate measurements of human positioning in the workspace, we can achieve precise robot speed adjustment by utilizing real-time measurements of separation distance. This, in turn, allows for collision prevention in a manner comfortable for the human user.We demonstrate our system achieves latencies below 9.64 ms with 95% probability, 11.10 ms with 99% probability, and 14.08 ms with 99.99% probability, resulting in robust real-time performance.",https://ieeexplore.ieee.org/document/6899348/,2014 IEEE International Conference on Automation Science and Engineering (CASE),18-22 Aug. 2014,ieeexplore
10.1145/3377811.3380368,Towards Characterizing Adversarial Defects of Deep Learning Software from the Lens of Uncertainty,IEEE,Conferences,"Over the past decade, deep learning (DL) has been successfully applied to many industrial domain-specific tasks. However, the current state-of-the-art DL software still suffers from quality issues, which raises great concern especially in the context of safety- and security-critical scenarios. Adversarial examples (AEs) represent a typical and important type of defects needed to be urgently addressed, on which a DL software makes incorrect decisions. Such defects occur through either intentional attack or physical-world noise perceived by input sensors, potentially hindering further industry deployment. The intrinsic uncertainty nature of deep learning decisions can be a fundamental reason for its incorrect behavior. Although some testing, adversarial attack and defense techniques have been recently proposed, it still lacks a systematic study to uncover the relationship between AEs and DL uncertainty. In this paper, we conduct a large-scale study towards bridging this gap. We first investigate the capability of multiple uncertainty metrics in differentiating benign examples (BEs) and AEs, which enables to characterize the uncertainty patterns of input data. Then, we identify and categorize the uncertainty patterns of BEs and AEs, and find that while BEs and AEs generated by existing methods do follow common uncertainty patterns, some other uncertainty patterns are largely missed. Based on this, we propose an automated testing technique to generate multiple types of uncommon AEs and BEs that are largely missed by existing techniques. Our further evaluation reveals that the uncommon data generated by ourmethod is hard to be defended by the existing defense techniques with the average defense success rate reduced by 35%. Our results call for attention and necessity to generate more diverse data for evaluating quality assurance solutions of DL software.",https://ieeexplore.ieee.org/document/9284139/,2020 IEEE/ACM 42nd International Conference on Software Engineering (ICSE),5-11 Oct. 2020,ieeexplore
10.1109/AIKE.2018.00037,"Towards Low-Cost, Real-Time, Distributed Signal and Data Processing for Artificial Intelligence Applications at Edges of Large Industrial and Internet Networks",IEEE,Conferences,"Digital Signal Processors (DSP) are vital system components in industrial Artificial Intelligence (AI) applications. In this paper, FIR filters that could be used for industrial AI applications are designed from the Spline Biorthogonal 1.5 (SB1.5) mother wavelet using a real-time, low-cost, generic industrial IoT (IIoT) hardware: the C28x DSP and low-level, Embedded C, system software. Our contribution in this paper is the first reported application of the C28x for SB1.5 wavelet construction. The significance of this approach is to be able to repurpose low-cost, readily available hardware for distributed AI applications. Our approach is different from the state of the art, in which specialized hardware are always manufactured for implementing AI applications at large network edges. Our approach supports low-cost and fast single-stage FIR implementation suitable for use in real-time, distributed AI application at network edges, since in our case, successive recursion of FIR filters leading to a full implementation of Pyramid Algorithm is not implemented. The designed FIR filter is evaluated and found capable of both low-pass and high pass filtering operations. Results of this paper indicate that the C28x real-time DSP, which exists in many IoT devices, could have improved scalability by being deployed for other important AI and IoT network edge analytic applications, different from its present uses.",https://ieeexplore.ieee.org/document/8527469/,2018 IEEE First International Conference on Artificial Intelligence and Knowledge Engineering (AIKE),26-28 Sept. 2018,ieeexplore
10.1109/BCI51272.2021.9385331,Towards Neurohaptics: Brain-Computer Interfaces for Decoding Intuitive Sense of Touch,IEEE,Conferences,"Noninvasive brain-computer interface (BCI) is widely used to recognize users' intentions. Especially, BCI related to tactile and sensation decoding could provide various effects on many industrial fields such as manufacturing advanced touch displays, controlling robotic devices, and more immersive virtual reality or augmented reality. In this paper, we introduce haptic and sensory perception-based BCI systems called neurohaptics. It is a preliminary study for a variety of scenarios using actual touch and touch imagery paradigms. We designed a novel experimental environment and a device that could acquire brain signals under touching designated materials to generate natural touch and texture sensations. Through the experiment, we collected the electroencephalogram (EEG) signals with respect to four different texture objects. Seven subjects were recruited for the experiment and evaluated classification performances using machine learning and deep learning approaches. Hence, we could confirm the feasibility of decoding actual touch and touch imagery on EEG signals to develop practical neurohaptics.",https://ieeexplore.ieee.org/document/9385331/,2021 9th International Winter Conference on Brain-Computer Interface (BCI),22-24 Feb. 2021,ieeexplore
10.1109/SP.2019.00001,Towards Practical Differentially Private Convex Optimization,IEEE,Conferences,"Building useful predictive models often involves learning from sensitive data. Training models with differential privacy can guarantee the privacy of such sensitive data. For convex optimization tasks, several differentially private algorithms are known, but none has yet been deployed in practice. In this work, we make two major contributions towards practical differentially private convex optimization. First, we present Approximate Minima Perturbation, a novel algorithm that can leverage any off-the-shelf optimizer. We show that it can be employed without any hyperparameter tuning, thus making it an attractive technique for practical deployment. Second, we perform an extensive empirical evaluation of the state-of-the-art algorithms for differentially private convex optimization, on a range of publicly available benchmark datasets, and real-world datasets obtained through an industrial collaboration. We release open-source implementations of all the differentially private convex optimization algorithms considered, and benchmarks on as many as nine public datasets, four of which are high-dimensional.",https://ieeexplore.ieee.org/document/8835258/,2019 IEEE Symposium on Security and Privacy (SP),19-23 May 2019,ieeexplore
10.1109/IECON48115.2021.9589594,Towards Sustainable Models of Computation for Artificial Intelligence in Cyber-Physical Systems,IEEE,Conferences,"This paper confronts with a reflection about a deep problem in computational models for cyber-physical systems (CPS). The problem arises in the contact between digital computing and the physical realm, and affects heavily the design, modeling, and implementation of CPS. Problems are exacerbated by the introduction of artificial intelligence and autonomy in industrial applications that have to meet sustainability of solutions, both in technical and societal sense. After a brief review, a new perspective and position on the future of sustainable CPS is addressed, and a pragmatic research path is presented. The RMAS (Relational-model Multi-Agent System) architecture is proposed as a test framework for the deep integration of real-world semantics into the advancements brought about by the digital transformation wave.",https://ieeexplore.ieee.org/document/9589594/,IECON 2021 – 47th Annual Conference of the IEEE Industrial Electronics Society,13-16 Oct. 2021,ieeexplore
10.1109/IS.2006.348456,Towards an Agent-Based Framework for Online After-Sale Services,IEEE,Conferences,"The multi-agent paradigm for building intelligent systems has gradually been accepted by researchers and practitioners in the research field of artificial intelligence. There are also attempts of adapting agents and agent-based systems for creating industrial applications and providing e-services. In this paper, we present an attempt to use agents for constructing an online after-sale services system. The system is decomposed into four major cooperative agents, and in which each agent concentrates on particular aspects in the system and expresses intelligence by using various techniques. The proposed agent-based framework for the system is presented at both the micro-level and the macro-level according to the Gala methodology. UML notations are also used to represent some software design models. As the result of this, agents are implemented into a framework for which exploits case-based reasoning (CBR) technique to fulfil real life on-line services' diagnoses and tasks",https://ieeexplore.ieee.org/document/4155463/,2006 3rd International IEEE Conference Intelligent Systems,4-6 Sept. 2006,ieeexplore
10.23919/SMAGRIMET.2018.8369850,Towards statistical machine learning for edge analytics in large scale networks: Real-time Gaussian function generation with generic DSP,IEEE,Conferences,"The smart grid (SG) is a large-scale network and it is an integral part of the Internet of Things (IoT). For a more effective big data analytic in large-scale IoT networks, reliable solutions are being designed such that many real-time decisions will be taken at the edge of the network close to where data is being generated. Gaussian functions are extensively applied in the field of statistical machine learning, pattern recognition, adaptive algorithms for function approximation, etc. It is envisaged that soon, some of these machine learning solutions and other gaussian function based applications that have low computation and low-memory footprint will be deployed for edge analytics in large-scale IoT networks. Hence, it will be of immense benefit if an adaptive, low-cost, method of designing gaussian functions becomes available. In this paper, gaussian distribution functions are designed using C28x real-time digital signal processor (DSP) that is embedded in the TMS320C2000 modem designed for powerline communication (PLC) at the low voltage distribution end of the smart grid, where numerous devices that generate massive amount of data exist. Open-source embedded C programming language is used to program the C28x for real-time gaussian function generation. The designed gaussian waveforms are stored in lookup tables (LUTs) in the C28x embedded DSP, and could be deployed for a variety of applications at the edge of the SG and IoT network. The novelty of the design is that the gaussian functions are designed with a generic, low-cost, fixed-point DSP, different from state of the art in which gaussian functions are designed using expensive arbitrary waveform generators and other specialized circuits. C28x DSP is selected for this design since it is already existing as an embedded DSP in many smart grid applications and in other numerous industrial systems that are part of the large scale IoT network, hence it is envisaged that integration of any gaussian function based solution using this DSP in the smart grid and other IoT systems may not be too challenging.",https://ieeexplore.ieee.org/document/8369850/,2018 First International Colloquium on Smart Grid Metrology (SmaGriMet),24-27 April 2018,ieeexplore
10.1109/COASE.2018.8560470,Training CNNs from Synthetic Data for Part Handling in Industrial Environments,IEEE,Conferences,"As Convolutional Neural Network based models become reliable and efficient, two questions arise in relation to their applications for industrial purposes. The usefulness of these models in industrial environments and their implementation in these settings. This paper describes the autonomous generation of Region based CNN models trained on images from rendered CAD models and examines their applicability and performance for part handling application. The development of the automated synthetic data generation is detailed and two CNN models are trained with the aim to detect a car component and differentiate it against another similar looking part. The performance of these models is tested on real images and it was found that the proposed approach can be easily adopted for detecting a range of parts in arbitrary backgrounds. Moreover, the use of syntheic images for training CNNs automates the process of generating a detector.",https://ieeexplore.ieee.org/document/8560470/,2018 IEEE 14th International Conference on Automation Science and Engineering (CASE),20-24 Aug. 2018,ieeexplore
10.1109/ISCAS.1992.230637,Trajectory tracking using neural networks,IEEE,Conferences,"Presents a method for tracking prespecified trajectories in industrial drive systems with a multilayered feedforward neural network. The method utilizes the backpropagation technique to learn feedback-error ranges, in which appropriate control actions can be generated from a lookup table. It can follow arbitrarily prescribed trajectories even when they are not present in the training phase. This approach is simple and practical for real-time implementation. Examples are included to demonstrate the effectiveness. The analogy between this scheme and a fuzzy logic control strategy is also investigated.&lt;<ETX>&gt;</ETX>",https://ieeexplore.ieee.org/document/230637/,[Proceedings] 1992 IEEE International Symposium on Circuits and Systems,10-13 May 1992,ieeexplore
10.1109/ICAIIC48513.2020.9065203,UAV-assisted Real-time Data Processing using Deep Q-Network for Industrial Internet of Things,IEEE,Conferences,"Industrial internet of things (IIoT) enables edge computing technology to provide communication between the machines that produce a large amount of data and locate at the edge network. A task scheduling is implemented in the edge node. Furthermore, the real-time data can achieve with the lowest latency that allowed by the edge node near the edge network. However, a mobile machine such as an autonomous guided vehicle can interfere in this situation. Because the vehicle also needs service by the edge node. Over that, quality of service (QoS) performance can decrease. Therefore, this paper deploys an unmanned aerial vehicle (UAV) as an edge node to provide service to the edge network through optimizing the trajectory of UAV, where the edge network request task using a Deep Q-Network (DQN) Learning. The result shows that using machine learning, notably the DQN algorithm, can increase the number of the machine that can be provided service. Subsequently, the real-time data can achieve either the interrupt occurs at the edge node.",https://ieeexplore.ieee.org/document/9065203/,2020 International Conference on Artificial Intelligence in Information and Communication (ICAIIC),19-21 Feb. 2020,ieeexplore
10.1109/ICASSP39728.2021.9414882,Unsupervised Clustering of Time Series Signals Using Neuromorphic Energy-Efficient Temporal Neural Networks,IEEE,Conferences,"Unsupervised time series clustering is a challenging problem with diverse industrial applications such as anomaly detection, bio-wearables, etc. These applications typically involve small, low-power devices on the edge that collect and process real-time sensory signals. State-of-the-art time-series clustering methods perform some form of loss minimization that is extremely computationally intensive from the perspective of edge devices. In this work, we propose a neuromorphic approach to unsupervised time series clustering based on Temporal Neural Networks that is capable of ultra low-power, continuous online learning. We demonstrate its clustering performance on a subset of UCR Time Series Archive datasets. Our results show that the proposed approach either outperforms or performs similarly to most of the existing algorithms while being far more amenable for efficient hardware implementation. Our hardware assessment analysis shows that in 7 nm CMOS the proposed architecture, on average, consumes only about 0.005 mm<sup>2</sup> die area and 22 μW power and can process each signal with about 5 ns latency.",https://ieeexplore.ieee.org/document/9414882/,"ICASSP 2021 - 2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)",6-11 June 2021,ieeexplore
10.1109/RoEduNet51892.2020.9324883,Usage of Asymetric Small Binning to Compute Histogram of Oriented Gradients for Edge Computing Image Sensors,IEEE,Conferences,"In case of multiple imaging sensors used in different networks (home security, surveillance, automotive, industrial), there is a challenge to perform object detection algorithms in real time, even on the cloud, for a large number of sensors. This is why there is an intensive effort in the industry to move object detection processing on the edge, with the benefits of reducing the bandwidth needs and allowing for scalability in large networks. In this paper we present a hardware friendly optimization technique to compute Histogram of Oriented Gradients (HOG) on the edge, by approximating the HOG orientation as a multitude of small bins. The technique is implemented in RTL for FPGA or ASIC and serves as the first step in a standard object detection algorithm (using Histogram of Oriented Gradients as feature extractor and Support Vector Machine as the detection algorithm). We verified the results of the proposed optimizations for errors by comparison to a reference method and the overall object detection algorithm for robustness.",https://ieeexplore.ieee.org/document/9324883/,2020 19th RoEduNet Conference: Networking in Education and Research (RoEduNet),11-12 Dec. 2020,ieeexplore
10.1109/ICVR51878.2021.9483702,User Interface Research in Web Extended Reality,IEEE,Conferences,"With computer technology rapidly expands out in recent years, there is a significant trend that Virtual Reality, Augmented Reality, and Mixed Reality technologies turn into the public. The paper starts from a literature review to discuss Web Extended Reality and its critical characteristics in user interface. Our research discussed the user interface layout and interactive mode by wide research. The paper reveals not only the traditional literature review but also contains a data experiment study. By analyzing how the platform limitation restricts the user interface in Web Extended Reality, we designed a simulated solar system as a case experiment. We summarized its rendering performance data and discussed the limitation in various textures and materials. The paper uses the collected articles and experimental data to discuss industrial developing trends and indicates the possible future view based on the data study result.",https://ieeexplore.ieee.org/document/9483702/,2021 IEEE 7th International Conference on Virtual Reality (ICVR),20-22 May 2021,ieeexplore
10.1109/RIVF.2009.5174663,Using Belief Theory to Diagnose Control Knowledge Quality: Application to Cartographic Generalisation,IEEE,Conferences,"Both humans and artificial systems frequently use trial and error methods to problem solving. In order to be effective, this type of strategy implies having high quality control knowledge to guide the quest for the optimal solution. Unfortunately, this control knowledge is rarely perfect. Moreover, in artificial systems-as in humans-self-evaluation of one's own knowledge is often difficult. Yet, this self-evaluation can be very useful to manage knowledge and to determine when to revise it. The objective of our work is to propose an automated approach to evaluate the quality of control knowledge in artificial systems based on a specific trial and error strategy, namely the informed tree search strategy. Our revision approach consists in analysing the system's execution logs, and in using the belief theory to evaluate the global quality of the knowledge. We present a real-world industrial application in the form of an experiment using this approach in the domain of cartographic generalisation. Thus far, the results of using our appProach have been encouraging.",https://ieeexplore.ieee.org/document/5174663/,2009 IEEE-RIVF International Conference on Computing and Communication Technologies,13-17 July 2009,ieeexplore
10.1109/ICNN.1995.487735,Using CMAC neural networks and optimal control,IEEE,Conferences,"This paper explores the real-time control of an industrial robotic arm to balance a mass at the end of a pole. The 3D inverted pendulum is a MIMO nonlinear inherently unstable system. The control system uses combined optimal and neural network techniques. To provide stable control, an optimal, linear quadratic regulator controller was developed from the linearized system model. When applied to the robotic system, this controller produced a relatively large limit cycle, due primarily to unmodelled system nonlinearities. The CMAC neural network was then introduced into the controller to implement a technique referred to as prediction feedback. The purpose of this adaptive feedback controller was to learn system nonlinearities, reject any residual noise, and reduce the system limit cycle. When applied to the robotic pole-balancer, the addition of adaptive prediction feedback helped to significantly decrease the magnitude and frequency of oscillation. This experiment is a primary example of how an intelligent controller can be developed by combining the strengths of different control techniques.",https://ieeexplore.ieee.org/document/487735/,Proceedings of ICNN'95 - International Conference on Neural Networks,27 Nov.-1 Dec. 1995,ieeexplore
10.1049/cp:19940284,Using expert systems for on-line data qualification and state variable estimation for an industrial fermentation process,IET,Conferences,"An industrial fed batch fermentation process is a nonlinear time-varying process. Important internal state variables such as biomass, substrate and product concentrations cannot be measured online and are usually determined by infrequent and time consuming off-line laboratory analysis. The online measurements are usually noisy and sometimes this leads to misinterpretation of the real situation inside the fermenter. These problems can lead to poor control of the batch and low productivity subsequently. To overcome these problems a real time expert system has been proposed which is based on the Poplog Flex real time expert system shell. The system is used to monitor the state variables of the process, diagnose any fault that might occur in the process, estimate the important unmeasurable state variables and to design a controller to control the state around a desired level. A neural network has been adopted for the online estimation of the unmeasurable state variables. Pattern recognition ideas have been used to improve the modelling ability of the neural network. Predictive control techniques have been used to control the state around a desired level. The model and the controller for the process have been designed and implemented within the Poplog Flex environment.&lt;<ETX>&gt;</ETX>",https://ieeexplore.ieee.org/document/327321/,1994 International Conference on Control - Control '94.,21-24 March 1994,ieeexplore
10.1109/AICCSA50499.2020.9316530,Video Preview Generation Based on Playback Records,IEEE,Conferences,"A video preview is formed by joining a subset of video snippets from a source video. It is expected to be expressive to help enrich users' experience and hence increase the views of the full video. However, it is challenging to identify suitable video snippets to compose such video previews in support of user recommendation of full-length videos. In this paper, we formulate this problem as an optimization problem to maximize the total playback popularity of video segments based on the analysis of a large amount of users' playback records. We design an algorithm for this problem and provide proof of optimality. Furthermore, we conduct an experiment using real industrial data and recruit volunteers to assess the quality of generated video previews. The experimental results show the feasibility and applicability of our methods.",https://ieeexplore.ieee.org/document/9316530/,2020 IEEE/ACS 17th International Conference on Computer Systems and Applications (AICCSA),2-5 Nov. 2020,ieeexplore
10.1109/DCOSS.2019.00126,Virtual Light Sensors in Industrial Environment Based on Machine Learning Algorithms,IEEE,Conferences,"Internet of Things (IoT) has become the backbone of current and future emerging applications both in the public and the private, industrial sector. The IoT paradigm, enhanced with intelligence and big data analytics, has found applications in a wide range of solutions such as smart home, smart city, industrial IoT etc. Even though IoT implies that cheap motes can conduct a specific task, thus a large number of them can be deployed, we aim to minimize the installed hardware while we still have a high level of quality of service. Machine Learning algorithms can support this challenge by generating virtual data via utilization of real data from a smaller subset of sensors. The generated data can replicate sensor behavior which would otherwise be difficult or impossible to track. It is also possible to use simulation models for data analysis model validation, by generating new data under varying conditions. In this paper, we propose a concept of an IoT testbed which allows virtual IoT resources to be immersed and tested in real life conditions, which are met in everyday life. Additionally, the features of the implemented testbed prototype are discussed while taking into account specific use cases, regarding luminosity scenarios in industrial environments.",https://ieeexplore.ieee.org/document/8804746/,2019 15th International Conference on Distributed Computing in Sensor Systems (DCOSS),29-31 May 2019,ieeexplore
10.23919/ChiCC.2019.8865406,Vision-Based Position/Impedance Control for Robotic Assembly Task,IEEE,Conferences,"In robotic assembly processes, detecting target pose is indispensable for robot manipulation algorithms. Such a tedious work hinders the further application of robot manipulation algorithms to real industrial processes. In this paper, a vision-based position/impedance control framework is proposed. Therein, the objective pose is firstly obtained by a motion capture system, where a kinematic equation is solved to Figure out the target assembly pose. Next the robot arm motion is approximated by an impedance model, in this way, the position/impedance control law of the robot is derived, which drives the robot to perform a compliant assembly manipulation. Finally, a memory stick assembly experiment is conducted on a 6-DOF robot arm equipped with a motion capture system and a force-torque sensor. Experiment shows the efficiency of the vision-based position/impedance hybrid controller in terms of autonomous positioning and compliant assembly. Due to its convenience and generality, the present controller can be expected to apply to more general industrial scenarios.",https://ieeexplore.ieee.org/document/8865406/,2019 Chinese Control Conference (CCC),27-30 July 2019,ieeexplore
10.1109/ICSTCC.2019.8885611,Visual Analytics Framework for Condition Monitoring in Cyber-Physical Systems,IEEE,Conferences,"One of the biggest challenges facing the factory of the future today is to reduce the time-to-market access and increase through the improvement of competitiveness and efficiency. In order to achieve this target, data analytics in Industrial Cyber-Physical System becomes a feasible option. In this paper, a visual analytics framework for condition monitoring of the machine tool is presented with the aim to manage events and alarms at factory level. The framework is assessed in a particular use case that consists in a multi-threaded cloud-based solution for the global analysis of the behaviour of variables acquired from PLC, CNC and robot manipulator. A human-machine interface is also designed for the real-time visualization of the key performance indicators according to the user's criteria. This tool implemented is a great solution for condition monitoring and decision-making process based on data analytics from simple statistics to complex machine learning methods. The results achieved are part of the vision and implementation of the industrial test bed of “Industry and Society 5.0” platform.",https://ieeexplore.ieee.org/document/8885611/,"2019 23rd International Conference on System Theory, Control and Computing (ICSTCC)",9-11 Oct. 2019,ieeexplore
10.1109/CLEOE-EQEC.2019.8872523,Wavelength Independent Image Classification through a Multimode Fiber using Deep Neural Networks,IEEE,Conferences,"Deep Neural Networks (DNNs) have been increasingly implemented in different research fields or industrial applications. Large amounts of data are processed daily in order to extract useful information using machine learning techniques. Many research groups have shown impressive results on improving resolution in microscopy and quantitative phase retrieval by training DNNs on real datasets [1,2]. Recently, recovery and reconstruction of images after they have propagated through multimode optical fibers (MMFs) have also been achieved using DNNs [3,4]. When images propagate through MMFs they suffer severe scrambling because the information gets distributed among the different spatial modes that the fiber supports. Furthermore, since the fiber modes propagate with different velocities, the local information of the input decorrelates after a few millimeters along the MMF, thus resulting in the formation of a speckle pattern at the output. Recovery of information from such speckle patterns is of practical interest for integrating the MMFs for endoscopic applications in medicine or for signal recovery in telecommunications.",https://ieeexplore.ieee.org/document/8872523/,2019 Conference on Lasers and Electro-Optics Europe & European Quantum Electronics Conference (CLEO/Europe-EQEC),23-27 June 2019,ieeexplore
10.1109/CASE48305.2020.9216781,Weak Scratch Detection of Optical Components Using Attention Fusion Network,IEEE,Conferences,"Scratches on the optical surface can directly affect the reliability of the optical system. Machine vision-based methods have been widely applied in various industrial surface defect inspection scenarios. Since weak scratches imaging in the dark field has an ambiguous edge and low contrast, which brings difficulty in automatic defect detection. Recently, many existing visual inspection methods based on deep learning cannot effectively inspect weak scratches due to the lack of attention-aware features. To address the problems arising from industry-specific characteristics, this paper proposes “Attention Fusion Network;”, a convolutional neural network using attention mechanism built by hard and soft attention modules to generate attention-aware features. The hard attention module is implemented by integrating the brightness adjustment operation in the network, and the soft attention module is composed of scale attention and channel attention. The proposed model is trained on a real-world industrial scratch dataset and compared with other defect inspection methods. The proposed method can achieve the best performance to detect the weak scratch inspection of optical components compared to the traditional scratch detection methods and other deep learning-based methods.",https://ieeexplore.ieee.org/document/9216781/,2020 IEEE 16th International Conference on Automation Science and Engineering (CASE),20-21 Aug. 2020,ieeexplore
10.1109/ECC.2015.7330620,Web tension regulation with partially known periodic disturbances in roll-to-roll manufacturing systems,IEEE,Conferences,"This paper deals with the problem of synthesizing feedforward control to aid the regulation of web tension in the presence of partially known exogenous inputs. The problem appears in many engineering applications including Roll-to-Roll (R2R) manufacturing systems where the governing equation for tension is nonlinear. Currently known methods for the nonlinear output regulation problem either require the solution of a constrained partial differential equation or the preview information of the signal to be tracked. In this paper, we consider the problem of regulating web tension while rejecting periodic disturbances and use a novel approach to synthesize feedforward control as the solution of a system of differential-algebraic equations, which is considerably less complex and suitable for practical implementation. We assume that the disturbance signal is the output of a known exogenous system with unknown initial conditions. A parameter identification scheme to estimate the unknown initial conditions is developed. The proposed technique is successfully applied to web tension regulation in a large R2R machine which contains real-time hardware and software that is used in industrial practice. Extensive experiments were conducted to evaluate the proposed scheme under various experimental conditions, including different web speeds and materials. We will discuss a representative sample of the results with the proposed nonlinear tension regulator and provide a comparison with a well-tuned industrial PI control scheme to highlight the benefits of using the proposed scheme.",https://ieeexplore.ieee.org/document/7330620/,2015 European Control Conference (ECC),15-17 July 2015,ieeexplore
10.1109/IECON.2012.6389251,ZnO crystalline nanowires array for application in gas ionization sensor,IEEE,Conferences,"The air monitoring becomes daily necessity not only in industrial environment and in aerospace applications but also in a living milieu as a consequence of the gas pollution. For detection of gaseous pollutants gas sensors are employed. In this work the successful p-type and n-type ZnO nanowires (NWs) were accomplished during electrochemical deposition. P-type ZnO NWs doped with Ag dopant were achieved with omitted post annealing procedure. Also, the novel gas ionization sensor (GIS) with integrated p-type ZnO NWs as the anode is proposed. P-type ZnO NWs-based gas sensor's characteristics compared with the gold NWs-based GIS, which was developed and reported by our group previously. It showed the comparable breakdown voltages in inert gas (Ar) atmosphere. P-type ZnO NWs-based GIS demonstrated good repeatability. The practical and low cost p-type ZnO NWs-based gas sensor presented in this article shows potential for future implementation in real world gas sensors' applications.",https://ieeexplore.ieee.org/document/6389251/,IECON 2012 - 38th Annual Conference on IEEE Industrial Electronics Society,25-28 Oct. 2012,ieeexplore
10.1109/RCAR47638.2019.9043946,libSmart: an Open-Source Tool for Simple Integration of Deep Learning into Intelligent Robotic Systems,IEEE,Conferences,"Intelligent robotic systems can be empowered by advanced deep learning techniques. Robotic operations such as object recognition are well investigated by researchers involved in machine learning. However, these solutions have often led to ad-hoc implementation in experimental settings. Less reported is systematic implementation of deep learning models in industrial robots. The lack of standard implementation platforms has impeded widespread use of deep learning modules in industrial robots. It is of great importance to have development platforms that can coordinate several deep learning modules of a complex system. In this paper, a scalable deep-learning friendly robot task organization system named libSmart is introduced. Similar to ROS, the architecture of the proposed system allows users to plug and play various devices but the proposed architecture is also highly compatible with deep learning modules. Specifically, the deployment of deep learning models is handled using a novel data graph method with distributed computing. In this way, the computationally expensive training and inferencing processes of deep learning models can be handled with isolated accelerating hardware to reduce the overall system latency. Successful implementation of simultaneous object recognition and pose estimation by an industrial robot has been presented as a case study. The proposed system is open source for all users to build their own intelligent systems with customized deep-learning models. (https://github.com/RustIron/libSmart.git).",https://ieeexplore.ieee.org/document/9043946/,2019 IEEE International Conference on Real-time Computing and Robotics (RCAR),4-9 Aug. 2019,ieeexplore
10.1109/JIOT.2020.3035431,A Blockchain-Driven IIoT Traffic Classification Service for Edge Computing,IEEE,Journals,"Nowadays, more and more sensors, devices and applications are connected in Industrial Internet of Things (IIoT), producing massive real-time flows which need to be scheduled for Quality-of-Service provision. To realize application-aware and adaptive flow scheduling, the problem of traffic classification must be addressed at first. When edge computing paradigm is introduced into IIoT, the traffic classification service can be deployed on edge node in the near-end. Recently, deep-learning-based IIoT traffic classification methods show better performance, but the computational cost of deep learning model is too high to be deployed on edge node. Moreover, increasingly unknown flows generated by new devices and emerging industrial APPs lead to frequent training of traffic classifiers. It is difficult to migrate the complex process of classifier training from cloud server to edge nodes with limited resources. To address these issues, we take the benefits of hash mechanism and consensus mechanism in blockchain to design a lightweight IIoT traffic classification service, which is more applicable for edge computing paradigm. First, inspired by the hash mechanism in blockchain and the learning to hash for big data, we propose a new learning-to-hash method named extension hashing. By this method, we can build the set of binary coding tress (BCT set), then generating hash table for more efficient k-nearest neighbor-based classification without complex classifier training. Then, we design a new voting-based consensus algorithm to synchronize the BCT sets and the hash tables across edge nodes, thereby providing the traffic classification service. Finally, we conduct data-driven simulations to evaluate the proposed service. By comparing traffic classification results on public data set, we can see that the proposed service achieves the highest classification accuracy with the minimal time cost and memory usage.",https://ieeexplore.ieee.org/document/9247248/,IEEE Internet of Things Journal,"15 Feb.15, 2021",ieeexplore
10.1109/TIM.2006.881034,A Comparison Between Stereo-Vision Techniques for the Reconstruction of 3-D Coordinates of Objects,IEEE,Journals,"This paper deals with the problems in setting up stereo-vision systems for contactless measurement of dimensional parameters in industrial environments. Two implicit calibration algorithms for the reconstruction of three-dimensional (3-D) real-world coordinates of objects from pairs of two-dimensional image coordinates have been implemented and compared. The former is based on a direct linear transformation, while the latter on an Artificial Neural Network (ANN). The results of the comparison made on artificial and real objects are finally reported in terms of statistical analysis of the reconstruction error",https://ieeexplore.ieee.org/document/1703886/,IEEE Transactions on Instrumentation and Measurement,Oct. 2006,ieeexplore
10.1109/TLA.2021.9480156,A Data Governance Framework for Industry 4.0,IEEE,Journals,"The fourth industrial revolution, or Industry 4.0, represents a new stage of evolution in the organization, management and control of the value chain throughout the product or service life cycle. This digitization of the industrial environment is characterized by the connection of Information Technologies (IT) and Operations Technologies (OT) through cyber-physical systems and the Industrial IoT (IIoT). One of the main consequences of this integration is the increasing amount and variety of data generated in real time from different sources. In this environment of intensive generation of actionable information, data becomes a critical asset for Industry 4.0, at all stages of the value chain. However, in order to data become a competitive advantage for the company, it must be managed and governed like any other strategic asset, and therefore it is necessary to rely on a Data Governance system. Industry 4.0 requires a reformulation of governance since the data is a key element and the backbone of the processes of the organization. This paper proposes a Reference Framework for the implementation of Data Governance Systems for Industry 4.0. Previously, it contextualizes data governance for Industry 4.0 environments and identifies the requirements that this framework must address, which are conditioned by the specific features of Industry 4.0, among others, the intensive use of big data, cloud and edge computing, artificial intelligence and current regulations.",https://ieeexplore.ieee.org/document/9480156/,IEEE Latin America Transactions,Dec. 2021,ieeexplore
10.1109/ACCESS.2020.3020906,A Fault Diagnosis Method Based on Improved Adaptive Filtering and Joint Distribution Adaptation,IEEE,Journals,"In the real environment of industrial equipment, the vibration signals of essential components show deviations due to the fault and noise. Notably, the noise in the signal will interfere with the diagnosis process of the signal and reduce the accuracy of fault diagnosis. Based on the above problem, adaptive filtering (AF) is used as an excellent method to attenuate noise without specifying the noise type. However, how to define the most appropriate length and type of morphological filter element is the most inherent problem which needs to be solved first. This paper proposed a cooperative diagnosis method of rolling bearings vibration signal based on improved adaptive filtering and joint distribution adaptation (JDA). First, the kurtosis under different element types and lengths is calculated as an index. The structural element corresponding to maximum kurtosis is selected as the most suitable morphological filter element because the different morphological filter elements reflect the effect of feature extraction. Then, JDA aims to improve both the marginal distribution and the conditional distribution to solve the chaotic distribution of time-domain features under variable working conditions. Finally, the improved least squares support vector machine (LSSVM) verified the effectiveness and improvement of the proposed method under bearing acceleration signal. At the same time, the comparative experiment proved that the proposed method not only directly corrects the most appropriate elements greatly optimizes the feature structure, but also enhances the accuracy of fault diagnosis.",https://ieeexplore.ieee.org/document/9184065/,IEEE Access,2020,ieeexplore
10.1109/TII.2020.3036159,A General Transfer Framework Based on Industrial Process Fault Diagnosis Under Small Samples,IEEE,Journals,"The lack of fault samples is a challenging issue for fault diagnosis in the industrial process. It is difficult for conventional fault diagnosis methods to achieve satisfactory results under small samples. This article proposes a general transfer framework with evolutionary capability to address the above issue. First, a general transfer framework is proposed, in which the transfer learning strategy is applied to guarantee the number and diversity of expanded samples and achieve accurate modeling. Second, the adaptive mixup (Admixup) method is presented, which can adaptively expand the fault samples and make the shared distribution smoother to guarantee the stability and accuracy of the fault diagnosis results. Finally, an optimized evolution strategy is designed, in which the transformation matrix is used as an evolutionary channel to reduce the fault diagnosis errors without retraining the framework as fault samples increase. The presented framework can utilize the generalization of small samples and the knowledge of various working condition samples to achieve accurate modeling. The proposed framework is applied to simulated and real industrial processes. Experiment results illustrate that the fault diagnosis model can be effectively established by the proposed framework under small samples, and the proposed framework is evolutionary capable.",https://ieeexplore.ieee.org/document/9250552/,IEEE Transactions on Industrial Informatics,Sept. 2021,ieeexplore
10.1109/TII.2019.2915846,A Global Manufacturing Big Data Ecosystem for Fault Detection in Predictive Maintenance,IEEE,Journals,"Artificial intelligence, big data, machine learning, cloud computing, and Internet of Things (IoT) are terms which have driven the fourth industrial revolution. The digital revolution has transformed the manufacturing industry into smart manufacturing through the development of intelligent systems. In this paper, a big data ecosystem is presented for the implementation of fault detection and diagnosis in predictive maintenance with real industrial big data gathered directly from large-scale global manufacturing plants, aiming to provide a complete architecture which could be used in industrial IoT-based smart manufacturing in an industrial 4.0 system. The proposed architecture overcomes multiple challenges including big data ingestion, integration, transformation, storage, analytics, and visualization in a real-time environment using various technologies such as the data lake, NoSQL database, Apache Spark, Apache Drill, Apache Hive, OPC Collector, and other techniques. Transformation protocols, authentication, and data encryption methods are also utilized to address data and network security issues. A MapReduce-based distributed PCA model is designed for fault detection and diagnosis. In a large-scale manufacturing system, not all kinds of failure data are accessible, and the absence of labels precludes all the supervised methods in the predictive phase. Furthermore, the proposed framework takes advantage of some of the characteristics of PCA such as its ease of implementation on Spark, its simple algorithmic structure, and its real-time processing ability. All these elements are essential for smart manufacturing in the evolution to Industry 4.0. The proposed detection system has been implemented into the real-time industrial production system in a cooperated company, running for several years, and the results successfully provide an alarm warning several days before the fault happens. A test case involving several outages in 2014 is reported and analyzed in detail during the experiment section.",https://ieeexplore.ieee.org/document/8710319/,IEEE Transactions on Industrial Informatics,Jan. 2020,ieeexplore
10.1109/TIM.2019.2962565,A Lightweight Appearance Quality Assessment System Based on Parallel Deep Learning for Painted Car Body,IEEE,Journals,"The appearance quality assessment based on defect inspection for painted car-body surfaces is an essential work to monitor and analyze the level of paint appearance quality. In the industrial application, there are some challenges, such as the huge and stereo skeleton of car bodies, a variety of irregular local surface areas, low visibility of defects due to tiny real size, and specular car-body surface. To overcome these problems, a lightweight online appearance quality assessment system (OAQAS) based on parallel deep learning is proposed, it includes two parts: 1) a vision inspection subsystem with distributed multi-camera image acquisition module and 2) an appearance quality evaluation subsystem (AQES) based on parallel TinyDefectRNet for evaluating the proposed painted surface grinding difficulty criteria. TinyDefectRNet is able to inspect relatively accurate defect size, although it is trained on a coarsely annotated data set. The OAQAS is implemented in an actual painting production line of a car factory, and the application results show that our OAQAS is far superior to the manual inspection in evaluation accuracy and time consumption. Moreover, our system is lightweight so that it is easy to be plugged into existing painting production lines without rebuilding or changing the inspection room.",https://ieeexplore.ieee.org/document/8964458/,IEEE Transactions on Instrumentation and Measurement,Aug. 2020,ieeexplore
10.1109/JSEN.2020.2980868,A Multisensor Data Fusion Method for Ball Screw Fault Diagnosis Based on Convolutional Neural Network With Selected Channels,IEEE,Journals,"In the real industrial application, the problem of ball screw health condition monitoring and fault diagnosis is still confronted many challenges. In some cases, the rotating machinery has long rotor, it need to arrange multiple sensors at different positions of the system and different faults are located at different positions of the system. The primary difficult issue involved in the task is to recognize the multiple faults at different positions of ball screw with high accuracy and feasibility. In order to overcome the problem, a novel method for ball screw fault diagnosis is proposed. The proposed method combines weighted data of the multiple sensors at different positions with convolutional neural network and it considers the sensitive index of different faults at different sensors for weight assignment. The proposed method mainly contains three steps. Firstly, a new data segmentation algorithm is proposed to obtain the uniform data of the vibration signals. Secondly, a sensitive sensor data selection criterion based on ball screw failure mechanism is developed to obtain the sensor importance factor. Finally, the weighted data is classified by the convolutional neural network. The effectiveness of the proposed method is verified by the experiment on the ball screw test-bed.",https://ieeexplore.ieee.org/document/9037071/,IEEE Sensors Journal,"15 July15, 2020",ieeexplore
10.1109/TIM.2018.2866744,A Multiview Discriminant Feature Fusion-Based Nonlinear Process Assessment and Diagnosis: Application to Medical Diagnosis,IEEE,Journals,"Fusion of large-scale information is the key strategy for the complete understanding of many nonlinear and complicated industrial and medical processes. This paper presents the proposed multiview (MV) feature fusion-based learning generalizing discriminant correlation analysis (DCA) for assessment and diagnosis of nonlinear processes. The core focus of this algorithm is to explore the effectiveness of MV information embedding in learning models and viable implementation in real-time applications. Our method is capable of incorporating high-dimensional information inherent in MV features generated from available inputs using the proposed DCA-based scheme. The algorithm is tested with two real-time electromyogram data sets, which include three categories of nonlinear data-amyotrophic lateral sclerosis, myopathy, and healthy control subjects. A set of MV features is generated in both the time and the wavelet domain for all of the study groups. The features are subjected to DCA projection and optimization and obtained low-order features are concatenated using DCA-based fusion scheme. The discriminant features are applied for the statistical analysis and the model validation. The model achieves an accuracy of 99.03% with a specificity of 99.58% and sensitivities of 98.50% and 97.59%. However, the accuracy over the second data set is 100% with sensitivities of 100% and the specificity of 100%. Results are further compared with the state-of-the-art methods. The proposed scheme is promising and outperforms many state-of-the-art methods, and thus it ensures the faithfulness for industrial applications.",https://ieeexplore.ieee.org/document/8458157/,IEEE Transactions on Instrumentation and Measurement,July 2019,ieeexplore
10.1109/TNSRE.2020.3044113,A Novel Point-in-Polygon-Based sEMG Classifier for Hand Exoskeleton Systems,IEEE,Journals,"In the early 2000s, data from the latest World Health Organization estimates paint a picture where one-seventh of the world population needs at least one assistive device. Fortunately, these years are also characterized by a marked technological drive which takes the name of the Fourth Industrial Revolution. In this terrain, robotics is making its way through more and more aspects of everyday life, and robotics-based assistance/rehabilitation is considered one of the most encouraging applications. Providing high-intensity rehabilitation sessions or home assistance through low-cost robotic devices can be indeed an effective solution to democratize services otherwise not accessible to everyone. However, the identification of an intuitive and reliable real-time control system does arise as one of the critical issues to unravel for this technology in order to land in homes or clinics. Intention recognition techniques from surface ElectroMyoGraphic (sEMG) signals are referred to as one of the main ways-to-go in literature. Nevertheless, even if widely studied, the implementation of such procedures to real-case scenarios is still rarely addressed. In a previous work, the development and implementation of a novel sEMG-based classification strategy to control a fully-wearable Hand Exoskeleton System (HES) have been qualitatively assessed by the authors. This paper aims to furtherly demonstrate the validity of such a classification strategy by giving quantitative evidence about the favourable comparison to some of the standard machine-learning-based methods. Real-time action, computational lightness, and suitability to embedded electronics will emerge as the major characteristics of all the investigated techniques.",https://ieeexplore.ieee.org/document/9291412/,IEEE Transactions on Neural Systems and Rehabilitation Engineering,Dec. 2020,ieeexplore
10.1109/ACCESS.2020.2990190,"A Novel Simulated-Annealing Based Electric Bus System Design, Simulation, and Analysis for Dehradun Smart City",IEEE,Journals,"Smart transportation network development with environmental issues into consideration has brought Industry 4.0 based solutions on priority. In this direction, battery-powered electric bus systems have been considered widely for ensuring flexibility, operation cost, and lesser pollutants emission. Industry 4.0 provides automation through a cyber-physical system (CPS), the interconnection of bus system entities with industrial internet-of-things (IIoT), remote information availability through cloud computing and scientific disciplines (human-computer interaction, artificial intelligence, machine learning etc.) integration. In this work, a discrete event-based simulation-optimization approach is integrated that take care of bus energy consumption according to real-time city's passenger needs and on-road friction levels. The proposed simulation optimization methodology utilizes multi-objective with dependent and independent variables for optimizing the overall system performance. In simulation optimization, objective functions are designed to tackle battery consumption, Internet-of-Thing (IoT) network performance, cloud operations efficiency and smart scientific discipline integration. Simulation parameters are based on a real-time bus system which is further analyzed, filtered and adapted as per the needs of the system. In another analysis, supercharger's capacities are varied to evaluate the performance of the proposed system and identify the low cost and efficient smart transportation system. Simulation results show different scenarios for variations in the number of buses, charging stations, bus-depots, mobile charging facilities, and bus-schedules. Simulation results show that the average passenger's waiting time in the waiting is (after ticket booking) varies between 0.2 minutes to 0.7 minutes in real-time traffic conditions. In similar traffic conditions, total passenger's time in system (ticket booking to travel) varies between 41.6 minutes (for 24 hours) to 45.5 minutes (for 1 year). In the simulation, priorities are given to those dependent and independent variables which save the battery consumption and elongate the utilization of buses. Lastly, it is also observed that the proposed system is suitable for resource-constraint devices because Gate Equivalent (GE) calculation shows that the proposed system can be implemented between 1986 GEs (communicational cost without confidentiality and authentication) and 7939 GEs (computational cost with HMAC for authentication in data storage). This ensures varies security primitivs such as confidentiality, availability and authentication.",https://ieeexplore.ieee.org/document/9078106/,IEEE Access,2020,ieeexplore
10.1109/TII.2020.3013277,A Real-Time Defect Detection Method for Digital Signal Processing of Industrial Inspection Applications,IEEE,Journals,"The signal processing of industrial big data (IBD) is a challenging task, owing to the complex working scenarios and the lack of annotations. Defect detection, which is an important subject of IBD research works, has shown its effectiveness in digital signal processing of industrial inspection applications in many previous studies. This article proposes a novel defect detection method based on deep learning for digital signal processing of industrial inspection applications. In our method, a module named feature collection and compression network is applied to merge multiscale feature information. Then, a new pooling method named Gaussian weighted pooling, which provides more precise location information, is used to replace region of interest (ROI) pooling. Experiment results show that our method gets improvements in both accuracy and efficiency, with mAP/AP50 of 41.8/80.2 at 33 fps on NEUDET, which satisfies the requirement of real-time systems.",https://ieeexplore.ieee.org/document/9153815/,IEEE Transactions on Industrial Informatics,May 2021,ieeexplore
10.1109/ACCESS.2019.2963723,A Smart Collaborative Routing Protocol for Delay Sensitive Applications in Industrial IoT,IEEE,Journals,"In the industrial Internet of things (IIoT), there is always a strong demand for real-time information transfer. Especially when deploying wireless/wired hybrid networks in smart factories, the requirement for low delay interaction is more prominent. Although tree routing protocols have been successfully executed in simple networks, more challenges in transmission speed can be observed in the manufacturing broadband communication system. Motivated by the progresses in deep learning, a smart collaborative routing protocol with low delay and high reliability is proposed to accommodate mixed link scenarios. First, we establish a one-hop delay model to investigate the potential affects of Media Access Control (MAC) layer parameters, which supports the subsequent design. Second, forwarding, maintenance, and efficiency strategies are created to construct the basic functionalities for our routing protocol. Relevant procedures and key approaches are highlighted as well. Third, two sub-protocols are generated and the corresponding implementation steps are described. The experimental results demonstrate that the end-to-end delay can be effectively cut down through comprehensive improvements. Even more sensor nodes and larger network scale are involved, our proposed protocol can still illustrate the advantages comparing with existing solutions within IIoT.",https://ieeexplore.ieee.org/document/8949516/,IEEE Access,2020,ieeexplore
10.1109/JSYST.2019.2921867,A Smart Optimization of Fault Diagnosis in Electrical Grid Using Distributed Software-Defined IoT System,IEEE,Journals,"Electrical power demands have increased significantly over the last years due to rapid increase in air conditioning units and home appliances per domestic unit, particularly in Iraq. Having an uninterrupted power supply is essential for the continuity of power-generated home services and industrial platforms. Currently, in Iraq, electrical power interruption has become a big concern to the utility suppliers. Despite successive attempts to put an end to this dilemma, the issue still prevails. One of the main factors in power outages in local zones is persistent faults in distribution transformers (DTs). DT is considered one of the main elements in the electrical network that is essential for the reliability of the grid supply. Due to the internal lack of monitoring system and periodic maintenance, DT is relentlessly subject to faults due to high overhead utilization. Therefore, in order to enhance the grid reliability, transformer health check, and maintenance practices, we propose a remote condition Internet of Things monitoring and fault prediction system that is based on a customized software-defined networking (SDN) technology. This approach is a transition to smart grid implementation by fusing the power grid with efficient and real-time wireless communication architecture. The SDN implementation is considered in two phases: one is a controller installed per local zone and the other is the main controller that is installed between zones and connected to the core network. The core network consists of redundant links to recover from any future fails. Furthermore, we propose a prediction system based on an artificial neural network algorithm, called distribution transformer fault prediction, that is installed in the management plane for periodic prediction based on real-time sensor traffic to our proposed cloud. Moreover, we propose a communication protocol in the local zone called local SDN-sense. The SDN-sense ensures a reliable communication and local node selection to relay DT sensor data to the main controller. Our proposed architecture showcases an efficient approach to handle future interruption and faults in power grid using cost-effective and reliable infrastructure that can predict and provide real-time health monitoring indices for the Iraqi grid network with minimal power interruptions. After extensive testing, the prediction accuracy was about 96.1%.",https://ieeexplore.ieee.org/document/8746591/,IEEE Systems Journal,June 2020,ieeexplore
10.1109/ACCESS.2021.3068824,A Stochastic Local Search Algorithm for the Partial Max-SAT Problem Based on Adaptive Tuning and Variable Depth Neighborhood Search,IEEE,Journals,"The Partial Max-SAT (PMSAT) problem is an optimization variant of the well-known Propositional Boolean Satisfiability (SAT) problem. It holds an important place in theory and practice, because a huge number of real-world problems, such as timetabling, planning, routing, bioinformatics, fault diagnosis, etc., could be encoded into it. Stochastic local search (SLS) methods can solve many real-world problems that often involve large-scale instances at reasonable computation costs while delivering good-quality solutions. In this work, we propose a novel SLS algorithm called adaptive variable depth SLS for PMSAT problem solving based on a dynamic local search framework. Our algorithm exploits two algorithmic components of an SLS method: parameter tuning and neighborhood search. Our first contribution is the design of an adaptive parameter tuner that searches for the best parameter setting for each instance by considering its features. The second contribution is a variable depth neighborhood search (VDS) algorithm adopted for PMSAT problem, which our empirical evaluation proves is a more efficient w.r.t. single neighborhood search. We conducted our experiments on the PMSAT benchmarks from MaxSAT Evaluation 2014 to 2019, including more than 3600 instances which have been encoded from a broad range of domains such as verification, optimization, graph theory, automated-reasoning, pseudo Boolean, etc. Our experimental evaluation results show that AVD-SLS solver, which is implemented based on our algorithm, outperforms state-of-the-art PMSAT SLS solvers in most benchmark classes, including random, crafted, and industrial instances. Furthermore, AVD-SLS reports remarkably better results on weighted benchmark, and shows competitive results with several well-known hybrid PMSAT solvers.",https://ieeexplore.ieee.org/document/9386095/,IEEE Access,2021,ieeexplore
10.1109/TCIAIG.2017.2755699,A Tool to Design Interactive Characters Based on Embodied Cognition,IEEE,Journals,"Creating interactive characters is one of the most challenging tasks of videogame design. In order to facilitate such an endeavor, we introduce a decisional and behavior synthesis architecture integrated in the game engine Unity3D. A distinguishing feature of this architecture is that it embraces embodied cognition principles and uses them as implementation requirements. From these, we derive an architecture, which is based on a novel combination of previously proposed systems, together with some simplifications. We also argue that the architecture proposed has properties-modularity, scalability, and stability-which can be beneficial for its practical industrial adoption, particularly in the context of the recent improvement in machine learning techniques. Artificial intelligence is a quite technical domain, and we believe such a tool can facilitate interactive character creation by creative minds in industrial applications.",https://ieeexplore.ieee.org/document/8048516/,IEEE Transactions on Games,Dec. 2019,ieeexplore
10.1109/TASE.2020.3032075,A Virtual Mechanism Approach for Exploiting Functional Redundancy in Finishing Operations,IEEE,Journals,"We propose a new approach to programming by the demonstration of finishing operations. Such operations can be carried out by industrial robots in multiple ways because an industrial robot is typically functionally redundant with respect to a finishing task. In the proposed system, a human expert demonstrates a finishing operation, and the demonstrated motion is recorded in the Cartesian space. The robot’s kinematic model is augmented with a virtual mechanism, which is defined according to the applied finishing tool. This way, the kinematic model is expanded with additional degrees of freedom that can be exploited to compute the optimal joint space motion of the robot without altering the essential aspects of the Cartesian space task execution as demonstrated by the human expert. Finishing operations, such as polishing and grinding, occur in contact with the treated workpiece. Since information about the contact point position is needed to control the robot during the operation, we have developed a novel approach for accurate estimation of contact points using the measured forces and torques. Finally, we applied iterative learning control to refine the demonstrated operations and compensate for inaccurate calibration and different dynamics of the robot and human demonstrator. The proposed method was verified on real robots and real polishing and grinding tasks. <italic>Note to Practitioners</italic>—This work was motivated by the need for automation of finishing operations, such as polishing and grinding, on contemporary industrial robots. Existing approaches are both too complex and too time-consuming to be applied in flexible and small-scale production, which often requires the frequent deployment of new applications. Our approach is based on programming by demonstration and enables the programming of finishing operations also for users who are not specialists in robot programming. Programming by demonstration is especially useful for teaching finishing operations because it enables the transfer of expert knowledge about finishing skills to robots without providing lengthy task descriptions or manual coding. Besides the human demonstration of the desired operation, the proposed approach also requires the availability of the kinematic model for the machine tool applied to carry out the finishing operation. We provide several practical examples of grinding and polishing tools and how to integrate them into our approach. Another feature of the proposed system is that user demonstrations of finishing operations can be transferred between different combinations of robots and machine tools.",https://ieeexplore.ieee.org/document/9246671/,IEEE Transactions on Automation Science and Engineering,Oct. 2021,ieeexplore
10.1109/ACCESS.2020.3048707,A Wear Particle Sensor Using Multiple Inductive Coils Under a Toroidal Magnetic Field,IEEE,Journals,"Inductive wear particle sensors have been widely studied due to their ability to monitor the wear status of equipment in real time. To detect wear particles more precisely, a wear particle sensor based on multiple inductive coils under a toroidal magnetic field is proposed in this paper. With inductive coils located near a stronger magnetic field, the sensor can not only ensure the flow rate, but also enhance wear particle detection accuracy. The proportional relationship between the signal and the wear particle velocity and the excitation current is discussed in this paper. The crosstalk phenomenon among different inductive coils was also investigated by single particle experiments and lubricating oil dynamic experiments. In the dynamic oil experiment, the sensor detected 13μm particles under a flow rate of 570 mL/min, which satisfied most industrial online monitoring demands.",https://ieeexplore.ieee.org/document/9312033/,IEEE Access,2021,ieeexplore
10.1109/TAC.1981.1102738,A digital quality control system for an industrial dry process rotary cement kiln,IEEE,Journals,"A multivariate autoregressive moving average (ARMA) model for an industrial dry process rotary cement kiln is identified, from real process data, using the maximum likelihood method. The model obtained is then used in computing a controller for quality control of clinker production. It is shown that it is relevant to compute a minimum variance controller subject to restrictions both in the controller structure and the variances of the control signals. The resulting controller is finally implemented on the cement kiln process, together with a target adaptive controller for automatic adjustment of the clinker quality setpoint, in order to save energy.",https://ieeexplore.ieee.org/document/1102738/,IEEE Transactions on Automatic Control,August 1981,ieeexplore
10.1109/ACCESS.2021.3055257,Adaptive Vision-Based Method for Rotor Dynamic Balance System,IEEE,Journals,"This article presents a new adaptive vision-based method (AVBM) of performing automatic detection for rotor balancing, and the online implementation proved that the method achieved rapid real-time optimization of system balancing configuration for a rotor dynamic balance machine. The proposed AVBM integrated 3D sensors and dynamic balancing platform using 3D computer vision technique and dynamic balance algorithm to improve the efficiency of rotor dynamic balancing. AVBM applied 3D ToF sensors on active rotor dynamic balance machines to grab 3D point cloud of rotor shaft and balance sprues. By 3D depth data, the background noise can be removed to detect the positions of shaft center, key phasor and balance sprues of rotor automatically. After combining with unbalance vector from dynamic balancing machine, the AVBM system calculated the optimal balance configuration by the vector analysis algorithm. Compares to conventional methods, conventional rotor dynamic balancing process relies on technicians to mount washers on particular balance sprues based on their experience, therefore uncertainty causes productivity decline. Experiments in industrial examples showed that the proposed AVBM required fewer rounds to achieve acceptance, whereas the conventional industrial rotor balancing method performed by operators required more than three rounds in average. Consider the overall dynamic balancing process for the motor, the processing time required for each motor without AVBM was 348.9 seconds, and the daily rotor balancing count of each dynamic balancing machine was 83. The processing time with AVBM was shortened from 348.9 to 283.9 seconds, the daily rotor balancing count had increased from 83 to 101, and the production improvement had reached 22 %. That is, the proposed AVBM can accurately analyze the dynamic balance and greatly reduce the redundant dynamic balance operations. The main advantage of the proposed AVBM over conventional methods is its efficiency, effectiveness and robustness in online optimization of rotor dynamic balance.",https://ieeexplore.ieee.org/document/9337863/,IEEE Access,2021,ieeexplore
10.1109/ACCESS.2021.3063676,An Automated Sensor Fusion Approach for the RUL Prediction of Electromagnetic Pumps,IEEE,Journals,"The remaining useful life (RUL) prediction of industrial cyber-physical system components demands the use of reliable prognostics parameters and frameworks. Against the traditional use of a single measure of degradation, data from multiple sensors provide abundant characteristic information for modeling, assessing, and extracting useful parameters via appropriate signal processing and sensor fusion methods. This study introduces a multi-sensor prognostics approach which merges highly prognosible statistical features from vibrational and pressure sensor measurements after a multi-level wavelet decomposition of the signals. The prognostic algorithm presented in this work for solenoid pump RUL prediction is a multi-objective genetic algorithm-optimized long short-term memory (MOGA-LSTM) which accepts the fused sensor features as input and returns the RUL of the pump as output. The framework was tested on a run-to-failure experiment on a VSC63A5 Solenoid pump following a significant pump malfunction caused by a clogged suction filter after the test. Using standard prognostic performance evaluation metrics, the performance of the prognostics framework was compared with other reliable state-of-the-art methods with a remarkable comparative advantage in addition to better automation potentials for real-time condition monitoring and RUL prediction.",https://ieeexplore.ieee.org/document/9367131/,IEEE Access,2021,ieeexplore
10.1109/TIE.2019.2962437,An Efficient Convolutional Neural Network Model Based on Object-Level Attention Mechanism for Casting Defect Detection on Radiography Images,IEEE,Journals,"Automatic detection of casting defects on radiography images is an important technology to automatize digital radiography defect inspection. Traditionally, in an industrial application, conventional methods are inefficient when the detection targets are small, local, and subtle in the complex scenario. Meanwhile, the outperformance of deep learning models, such as the convolutional neural network (CNN), is limited by a huge volume of data with precise annotations. To overcome these challenges, an efficient CNN model, only trained with image-level labels, is first proposed for detection of tiny casting defects in a complicated industrial scene. Then, in this article, we present a novel training strategy which can form a new object-level attention mechanism for the model during the training phase, and bilinear pooling is utilized to improve the model capability of detecting local contrast casting defects. Moreover, to enhance the interpretability, we extend class activation maps (CAM) to bilinear CAM (Bi-CAM) which is adapted to bilinear architectures as a visualization technique to reason about the model output. Experimental results show that the proposed model achieves superior performance in terms of each quantitative metric and is suitable for most actual applications. The real-time defect detection of castings is efficiently implemented in the complex scenario.",https://ieeexplore.ieee.org/document/8948332/,IEEE Transactions on Industrial Electronics,Dec. 2020,ieeexplore
10.1109/TII.2020.2965996,An Efficient Feature Extraction Method for the Detection of Material Rings in Rotary Kilns,IEEE,Journals,"Rotary kilns are important industrial plants to process material. In this article, we address the problem of detecting material rings formed in alumina rotary kilns, which leads to high waste and substantial economic loss. This necessitates the use of artificial intelligence to help kiln operators detect material rings promptly. We describe a recently developed system that extracts useful features and detects the presence of material rings efficiently and accurately in real time. Our major contribution is a novel feature extraction method based on geometric transformation and peak localization exploiting the domain knowledge. We also contribute a large data set covering many thousand labeled frames for the evaluation of ring detection. On this data set, we demonstrate that our novel feature extraction method outperforms other alternatives in the literature. Our method is also demonstrated as favorable over deep learning approaches. Our system produces an overall accuracy of nearly 96%, which is acceptable for deployment.",https://ieeexplore.ieee.org/document/8957316/,IEEE Transactions on Industrial Informatics,Sept. 2020,ieeexplore
10.1109/ACCESS.2021.3065195,An End-to-End Intelligent Fault Diagnosis Application for Rolling Bearing Based on MobileNet,IEEE,Journals,"To find out the hidden danger in the industrial production process in time, it is necessary to monitor the health condition of the key components of the mechanical system in operation. However, traditional fault diagnosis methods usually adopt manual feature extraction, which not only costs expensively and depends on prior knowledge. Therefore, it is of great significance to study the application of automatic fault identification based on the original vibration signals. Recently, existing studies have shown that most of fault diagnoses are implemented by using deep neural network. Although these methods have achieved satisfactory performances, there are obvious limitations in real applications, that is, the complexity of deep neural network requires a lot of hardware computing resources. This hinders the development of online fault diagnosis tools. To solve this problem, this paper proposes a fault diagnosis model based on lightweight convolutional neural network MobileNet, and realizes an end-to-end intelligent fault classification and diagnosis application. We evaluated the proposed method with the rolling bearing dataset from Western Reserve University. The best average precision, recall and F1 score of ten different bearing health conditions are about 96%, 82% and 88%, respectively. In addition, we also compare the accuracy of the rolling bearing fault diagnosis classification model under the standard ReLU and the improved ReLU. Experimental results show that both obtain good performances, but the improved ReLU reaches the over 96% accuracy more rapidly.",https://ieeexplore.ieee.org/document/9374431/,IEEE Access,2021,ieeexplore
10.1109/TIM.2019.2915404,An End-to-End Steel Surface Defect Detection Approach via Fusing Multiple Hierarchical Features,IEEE,Journals,"A complete defect detection task aims to achieve the specific class and precise location of each defect in an image, which makes it still challenging for applying this task in practice. The defect detection is a composite task of classification and location, leading to related methods is often hard to take into account the accuracy of both. The implementation of defect detection depends on a special detection data set that contains expensive manual annotations. In this paper, we proposed a novel defect detection system based on deep learning and focused on a practical industrial application: steel plate defect inspection. In order to achieve strong classification ability, this system employs a baseline convolution neural network (CNN) to generate feature maps at each stage, and then the proposed multilevel feature fusion network (MFN) combines multiple hierarchical features into one feature, which can include more location details of defects. Based on these multilevel features, a region proposal network (RPN) is adopted to generate regions of interest (ROIs). For each ROI, a detector, consisting of a classifier and a bounding box regressor, produces the final detection results. Finally, we set up a defect detection data set NEU-DET for training and evaluating our method. On the NEU-DET, our method achieves 74.8/82.3 mAP with baseline networks ResNet34/50 by using 300 proposals. In addition, by using only 50 proposals, our method can detect at 20 ft/s on a single GPU and reach 92% of the above performance, hence the potential for real-time detection.",https://ieeexplore.ieee.org/document/8709818/,IEEE Transactions on Instrumentation and Measurement,April 2020,ieeexplore
10.1109/ACCESS.2020.2992249,An Ensemble Deep Learning-Based Cyber-Attack Detection in Industrial Control System,IEEE,Journals,"The integration of communication networks and the Internet of Things (IoT) in Industrial Control Systems (ICSs) increases their vulnerability towards cyber-attacks, causing devastating outcomes. Traditional Intrusion Detection Systems (IDSs), which are mainly developed to support information technology systems, count vastly on predefined models and are trained mostly on specific cyber-attacks. Besides, most IDSs do not consider the imbalanced nature of ICS datasets, thereby suffering from low accuracy and high false-positive when being put to use. In this paper, we propose a deep learning model to construct new balanced representations of the imbalanced datasets. The new representations are fed into an ensemble deep learning attack detection model specifically designed for an ICS environment. The proposed attack detection model leverages Deep Neural Network (DNN) and Decision Tree (DT) classifiers to detect cyber-attacks from the new representations. The performance of the proposed model is evaluated based on 10-fold cross-validation on two real ICS datasets. The results show that the proposed method outperforms conventional classifiers, including Random Forest (RF), DNN, and AdaBoost, as well as recent existing models in the literature. The proposed approach is a generalized technique, which can be implemented in existing ICS infrastructures with minimum effort.",https://ieeexplore.ieee.org/document/9086038/,IEEE Access,2020,ieeexplore
10.1109/JSYST.2020.2994548,An Improved Just-in-Time Learning Scheme for Online Fault Detection of Nonlinear Systems,IEEE,Journals,"Just-in-time learning (JITL) scheme has been employed as an efficient tool of online soft sensor. It requires current measured data with high accuracy. However, in real industrial environments, it is difficult to ensure that no disturbance is added to the measurement. To solve this problem, this article proposes an improved JITL scheme, which employs leverage calculation to trim the weight of variables and abate the affect of disturbances. On this basis, an online modeling and output prediction algorithm is further presented. The experiment on a typical nonlinear system shows the better robustness and accuracy of the presented algorithm in comparison with conventional JITL-based approaches. Moreover, an online fault detection strategy for nonlinear systems is proposed based on JITL and partial least squares (PLS), for the purpose of simplifying the parameter setting and reducing the computational load of conventional fault detection approaches for nonlinear systems. Four statistic indexes are designed, including conventional T<sup>2</sup> and SPE for fault detection, T<sup>2</sup><sub>h</sub> and T<sup>2</sup><sub>t</sub> of two orthogonal decomposition input subspaces for fault birth subspace observation. A numerical nonlinear system and an industrial benchmark of Tennessee Eastman process are employed for fault detection experiments, testifying the effectiveness of the proposed strategy.",https://ieeexplore.ieee.org/document/9099847/,IEEE Systems Journal,June 2021,ieeexplore
10.1109/TII.2019.2959021,An Integrated Histogram-Based Vision and Machine-Learning Classification Model for Industrial Emulsion Processing,IEEE,Journals,"Existing techniques in emulsion quality evaluation are found to be highly subjective, time-consuming, and prone to overprocessing. Other conventional droplet analysis techniques such as laser diffraction, which require dilution of samples, introduce an additional complexity to industrial processes. The possibility of developing a fully automated technique for droplet characterization during emulsification holds remarkable potential for overcoming the existing challenges. In this article, a histogram-based image segmentation technique detects droplets from emulsion micrographs. The evolution of droplet characteristics and their significance are studied by performing statistical analysis, and the significant characteristics are selected. The principal component analysis is applied to obtain a reduced set of uncorrelated components from the selected characteristics. The linear discriminant analysis classifies the micrographs into a set of quality categories called target, acceptable, marginal, and unacceptable. The model accuracy is validated using stratified five-fold cross-validation and is successful in classifying the micrographs obtained from two different manufacturing facilities with high accuracy up to 100%. The histogram-based technique is successful in detecting smaller droplets than previously reflected in the literature. The current approach is fully automated and is implemented as a soft-sensor, which supports its real-time deployment into an industrial environment. The entire approach has promising potential in the in-line prediction of emulsion quality leading to more efficient and sustainable manufacturing.",https://ieeexplore.ieee.org/document/8968624/,IEEE Transactions on Industrial Informatics,Sept. 2020,ieeexplore
10.1109/TIFS.2019.2923577,Anomaly Detection in Real-Time Multi-Threaded Processes Using Hardware Performance Counters,IEEE,Journals,"We propose a novel methodology for real-time monitoring of software running on embedded processors in cyber-physical systems (CPS). The approach uses real-time monitoring of hardware performance counters (HPC) and applies to multi-threaded and interrupt-driven processes typical in programmable logic controller (PLC) implementation of real-time controllers. The methodology uses a black-box approach to profile the target process using HPCs. The time series of HPC measurements over a time window under known-good operating conditions is used to train a machine learning classifier. At run-time, this trained classifier classifies the time series of HPC measurements as baseline (i.e., probabilistically corresponding to a model learned from the training data) or anomalous. The baseline versus anomalous labels over successive time windows offer robustness against the stochastic variability of code execution on the embedded processor and detect code modifications. We demonstrate effectiveness of the approach on an embedded PLC in a hardware-in-the-loop (HITL) testbed emulating a benchmark industrial process. In addition, to illustrate the scalability of the approach, we also apply the methodology to a second PLC platform running a representative embedded control process.",https://ieeexplore.ieee.org/document/8737990/,IEEE Transactions on Information Forensics and Security,2020,ieeexplore
10.1109/TSG.2021.3054375,"Anomaly Detection, Localization and Classification Using Drifting Synchrophasor Data Streams",IEEE,Journals,"With ongoing automation and digitization of the electric power system, several Phasor Measurement Units (PMUs) have been deployed for monitoring and control. PMU data can have multiple anomalies, and many of the researchers in the past have concentrated on training machine/deep learning algorithms offline for anomaly detection over PMU data (i.e., not in real-time). These machine/deep learning algorithms, when trained offline on a sample rather than a population of the dataset, fail to consider the dynamic behavior of the power grid in real-time, resulting in low accuracy. Considering the dynamic behavior of the power grid (e.g., change in load, generation, distributed energy resources (DERs) switching, network, controls), the definition of data anomalies varies in time and requires online training. A fundamental challenge is to enable online (i.e., real-time) training of machine/deep learning algorithms for anomaly detection over streaming PMU data. While machine/deep learning is often desirable to manage data streams, training a deep learning algorithm over streaming PMU data is nontrivial due to changes in data statistics caused by dynamic streaming data. This article proposes PMUNET: a novel device-level deep learning-based data-driven approach for anomaly detection, localization, and classification over streaming PMU data, using online learning and multivariate data-drift detection algorithm. Two variants of PMUNET, Dynamic data Change Driven Learning (DCDL) and Continuity Driven Learning (CDL), are proposed and compared. DCDL aims to train the deep learning algorithm whenever the definition of anomaly changes due to the power grid dynamics. On the other hand, CDL continuously trains the deep learning algorithm over the PMU data-stream. The experimental results verify that DCDL outperforms CDL and other efficient anomaly detection methods over multiple events such as faults and load/ generator/capacitor/DERs variations/switching for IEEE 14 and 39 Bus test system as well as real PMU industrial data. The result verifies that DCDL variant of PMUNET improves over existing approach with a gain of 2% - 10% in terms of accuracy, false-positive rate, and false-negative rate.",https://ieeexplore.ieee.org/document/9335975/,IEEE Transactions on Smart Grid,July 2021,ieeexplore
10.1109/ACCESS.2020.2971319,Artificial Generation of Partial Discharge Sources Through an Algorithm Based on Deep Convolutional Generative Adversarial Networks,IEEE,Journals,"The measurement of partial discharges (PD) in electrical equipment or machines subjected to high voltage can be considered as one of the most important indicators when assessing the state of an insulation system. One of the main challenges in monitoring these degradation phenomena is to adequately measure a statistically significant number of signals from each of the sources acting on the asset under test. However, in industrial environments the presence of large amplitude noise sources or the simultaneous presence of multiple PD sources may limit the acquisition of the signals and therefore the final diagnosis of the equipment status may not be the most accurate. Although different procedures and separation and identification techniques have been implemented with very good results, not having a significant number of PD pulses associated with each source can limit the effectiveness of these procedures. Based on the above, this research proposes a new algorithm of artificial generation of PD based on a Deep Convolutional Generative Adversarial Networks (DCGAN) architecture which allows artificially generating different sources of PD from a small group of real PD, in order to complement those sources that during the measurement were poorly represented in terms of signals. According to the results obtained in different experiments, the temporal and spectral behavior of artificially generated PD sources proved to be similar to that of real experimentally obtained sources.",https://ieeexplore.ieee.org/document/8979409/,IEEE Access,2020,ieeexplore
10.1109/LRA.2020.2969927,Augmented LiDAR Simulator for Autonomous Driving,IEEE,Journals,"In Autonomous Driving (AD), detection and tracking of obstacles on the roads is a critical task. Deep-learning based methods using annotated LiDAR data have been the most widely adopted approach for this. Unfortunately, annotating 3D point cloud is a very challenging, time- and money-consuming task. In this letter, we propose a novel LiDAR simulator that augments real point cloud with synthetic obstacles (e.g., vehicles, pedestrians, and other movable objects). Unlike previous simulators that entirely rely on CG (Computer Graphics) models and game engines, our augmented simulator bypasses the requirement to create high-fidelity background CAD (Computer Aided Design) models. Instead, we can deploy a vehicle with a LiDAR scanner to sweep the street of interests to obtain the background points cloud, based on which annotated point cloud can be automatically generated. This “scan-and-simulate” capability makes our approach scalable and practical, ready for large-scale industrial applications. In this letter, we describe our simulator in detail, in particular the placement of obstacles that is critical for performance enhancement. We show that detectors with our simulated LiDAR point cloud alone can perform comparably (within two percentage points) with these trained with real data. Mixing real and simulated data can achieve over 95% accuracy.",https://ieeexplore.ieee.org/document/8972449/,IEEE Robotics and Automation Letters,April 2020,ieeexplore
10.1109/ACCESS.2021.3127084,Auto-NAHL: A Neural Network Approach for Condition-Based Maintenance of Complex Industrial Systems,IEEE,Journals,"Nowadays, machine learning has emerged as a promising alternative for condition monitoring of industrial processes, making it indispensable for maintenance planning. Such a learning model is able to assess health states in real time provided that both training and testing samples are complete and have the same probability distribution. However, it is rare and difficult in practical applications to meet these requirements due to the continuous change in working conditions. Besides, conventional hyperparameters tuning via grid search or manual tuning requires a lot of human intervention and becomes inflexible for users. Two objectives are targeted in this work. In an attempt to remedy the data distribution mismatch issue, we firstly introduce a feature extraction and selection approach built upon correlation analysis and dimensionality reduction. Secondly, to diminish human intervention burdens, we propose an Automatic artificial Neural network with an Augmented Hidden Layer (Auto-NAHL) for the classification of health states. Within the designed network, it is worthy to mention that the novelty of the implemented neural architecture is attributed to the new multiple feature mappings of the inputs, where such configuration allows the hidden layer to learn multiple representations from several random linear mappings and produce a single final efficient representation. Hyperparameters tuning including the network architecture, is fully automated by incorporating Particle Swarm Optimization (PSO) technique. The designed learning process is evaluated on a complex industrial plant as well as various classification problems. Based on the obtained results, it can be claimed that our proposal yields better response to new hidden representations by obtaining a higher approximation compared to some previous works.",https://ieeexplore.ieee.org/document/9610082/,IEEE Access,2021,ieeexplore
10.1109/TII.2018.2816971,Automatic Selection of Optimal Parameters Based on Simple Soft-Computing Methods: A Case Study of Micromilling Processes,IEEE,Journals,"Nowadays, the application of novel soft-computing methods to new industrial processes is often limited by the actual capacity of the industry to assimilate state-of-the-art computational methods. The selection of optimal parameters for efficient operation is very challenging in microscale manufacturing processes, because of intrinsic nonlinear behavior and reduced dimensions. In this paper, a decision-making system for selecting optimal parameters in micromilling operations is designed and implemented using simple and efficient soft-computing techniques. The procedure primarily consists of four steps: an experimental characterization; the modeling of cutting force and surface roughness by means of a multilayer perceptron; multiobjective optimization using the cross-entropy method, taking into account productivity and surface quality; and a decision-making procedure for selecting the most appropriate parameters using a fuzzy inference system. Finally, two different alloys for micromilling processes are considered, in order to evaluate the proposed system: a titanium-based alloy and a tungsten-copper alloy. The experimental study demonstrated the effectiveness of the proposed solution for automated decision-making, based on simple soft-computing methods, and its successful application to a real-life industrial challenge.",https://ieeexplore.ieee.org/document/8325494/,IEEE Transactions on Industrial Informatics,Feb. 2019,ieeexplore
10.1109/ACCESS.2020.2993010,Bearing Intelligent Fault Diagnosis in the Industrial Internet of Things Context: A Lightweight Convolutional Neural Network,IEEE,Journals,"The advancement of Industry 4.0 and Industrial Internet of Things (IIoT) has laid more emphasis on reducing the parameter amount and storage space of the model in addition to the automatic and accurate fault diagnosis. In this case, this paper proposes a lightweight convolutional neural network (LCNN) method for intelligent fault diagnosis of bearing, which can largely satisfy the need of less parameter amount and storage space as well as high accuracy. First, depthiwise separable convolution is adopted, and a LCNN structure is constructed through an inverse residual structure and a linear bottleneck layer operation. Secondly, a novel decomposed Hierarchical Search Space is introduced to automatically explore the optimal LCNN for bearing fault diagnosis in the context of the IIoT. In the meantime, the real-time monitoring and fault diagnosis of the model are also deployed. In order to verify the validity of the designed model, Case Western Reserve University Bearing fault dataset and MFPT bearing fault dataset are adopted. The results demonstrate the great advantages of the model. The LCNN model can automatically learn and select the appropriate features, highly improving the fault diagnosis accuracy. Meanwhile, the computational and storage costs of the model are largely reduced, which contributes to its being applied to the mechanical system in the IIoT context.",https://ieeexplore.ieee.org/document/9088980/,IEEE Access,2020,ieeexplore
10.1109/TSMCC.2009.2013816,Cascaded and Hierarchical Neural Networks for Classifying Surface Images of Marble Slabs,IEEE,Journals,"Marble quality classification is an important procedure generally performed by human experts. However, using human experts for classification is error prone and subjective. Therefore, automatic and computerized methods are needed in order to obtain reproducible and objective results. Although several methods are proposed for this purpose, we demonstrate that their performance is limited when dealing with diverse datasets containing a large number of quality groups. In this work, we test several feature sets and neural network topologies to obtain a better classification performance. During these tests, it is observed that different feature sets represent different subgroup(s) in a quality group rather than representing the whole group. Therefore, our approach is to use these features in a cascaded manner in which a quality group is classified by classifying all of its subgroups. We first realize this approach by using a two-stage cascaded network. Then, we design a hierarchical radial basis function network (HRBFN) in which correctly classified marble samples are taken out of the dataset and a different feature extraction method is applied to the remaining samples at each network level. The HRBFN system produces successful results for industrial applications and facilitates the desirable property of implementation in a quasi real-time manner.",https://ieeexplore.ieee.org/document/4812089/,"IEEE Transactions on Systems, Man, and Cybernetics, Part C (Applications and Reviews)",July 2009,ieeexplore
10.1109/TASE.2020.3010536,Condition-Driven Data Analytics and Monitoring for Wide-Range Nonstationary and Transient Continuous Processes,IEEE,Journals,"Frequent and wide changes in operation conditions are quite common in real process industry, resulting in typical wide-range nonstationary and transient characteristics along time direction. The considerable challenge is, thus, how to solve the conflict between the learning model accuracy and change complexity for analysis and monitoring of nonstationary and transient continuous processes. In this work, a novel condition-driven data analytics method is developed to handle this problem. A condition-driven data reorganization strategy is designed which can neatly restore the time-wise nonstationary and transient process into different condition slices, revealing similar process characteristics within the same condition slice. Process analytics can then be conducted for the new analysis unit. On the one hand, coarse-grained automatic condition-mode division is implemented with slow feature analysis to track the changing operation characteristics along condition dimension. On the other hand, fine-grained distribution evaluation is performed for each condition mode with Gaussian mixture model. Bayesian inference-based distance (BID) monitoring indices are defined which can clearly indicate the fault effects and distinguish different operation scenarios with meaningful physical interpretation. A case study on a real industrial process shows the feasibility of the proposed method which, thus, can be generalized to other continuous processes with typical wide-range nonstationary and transient characteristics along time direction. <italic>Note to Practitioners</italic>—Industrial processes in general have nonstationary characteristics which are ubiquitous in real world data, often reflected by a time-variant mean, a time-variant autocovariance, or both resulting from various factors. The focus of this study is to develop a universal analytics and monitoring method for wide-range nonstationary and transient continuous processes. Condition-driven concept takes the place of time-driven thought. For the first time, it is recognized that there are similar process characteristics within the same condition slice and changes in the process correlations may relate to its condition modes. Besides, the proposed method can provide enhanced physical interpretation for the monitoring results with concurrent analysis of the static and dynamic information which carry different information, analogous to the concepts of “position” and “velocity” in physics, respectively. The static information can tell the current operation condition, while the dynamic information can clarify whether the process status is switching between different steady states. It is noted that the condition-driven concept is universal and can be extended to other applications for industrial manufacturing applications.",https://ieeexplore.ieee.org/document/9158352/,IEEE Transactions on Automation Science and Engineering,Oct. 2021,ieeexplore
10.1109/ACCESS.2020.2986356,Data Fusion Generative Adversarial Network for Multi-Class Imbalanced Fault Diagnosis of Rotating Machinery,IEEE,Journals,"For the fault diagnosis problems of rotating machinery in the real industrial practice, measurement data with imbalanced class distributions negatively affect the diagnostic performance of most conventional machine learning classification algorithms since equal cost weights are assigned to different fault classes. Meanwhile, the widely used traditional data generation methods for the imbalanced data problem are limited by data dependencies over time continuity. To fill this research gap, this paper develops a new diagnostic framework based on the adversarial neural networks (GAN) and multi-sensor data fusion technique to generate new synthetic data for data compensation purpose. Two different practice modes are designed based on this framework according to the position logic of the data fusion, namely a Pre-fusion GAN mode and a Post-fusion GAN mode. More concretely, without data pre-processing, the designed generator generates synthetic data to puzzle the discriminator and the synthetic data that out-trick the discriminator can be used to compensate the minor class. To avoid data dependency and to ensure the generality of the proposed framework, the network modelling are trained with a more practical approach where the training and test data are obtained under different rotating speeds. Two imbalanced data sets on the rotating machinery, one benchmark public rolling bearing data set and another gear box data set acquired in our lab, are used to validate the proposed method. The performance is examined through a wide range of data imbalanced ratios (as high as 30:1), and compared with other state-of-the-art methods. The experiment results conclude that the proposed Pre-fusion GAN and Post-fusion GAN frameworks both have good performance on the imbalanced fault diagnosis of rotating machinery.",https://ieeexplore.ieee.org/document/9058665/,IEEE Access,2020,ieeexplore
10.1109/JSEN.2008.926923,Data Processing Method Applying Principal Component Analysis and Spectral Angle Mapper for Imaging Spectroscopic Sensors,IEEE,Journals,"A data processing method to classify hyperspectral images from an imaging spectroscopic sensor is evaluated. Each image contains the whole diffuse reflectance spectra of the analyzed material for all the spatial positions along a specific line of vision. The implemented linear algorithm comes to solve real time constrains typical of industrial systems. This processing method is composed of two blocks: data compression is performed by means of principal component analysis (PCA) and the spectral interpretation algorithm for classification is the spectral angle mapper (SAM). This strategy, applying PCA and SAM, has been successfully tested for online raw material sorting in the tobacco industry, where the desired raw material (tobacco leaves) should be discriminated from other unwanted spurious materials, such as plastic, cardboard, leather, feathers, candy paper, etc. Hyperspectral images are recorded by a sensor consisting of a monochromatic camera and a passive prism-grating-prism device. Performance results are compared with a spectral interpretation algorithm based on artificial neural networks (ANN).",https://ieeexplore.ieee.org/document/4567472/,IEEE Sensors Journal,July 2008,ieeexplore
10.1109/TII.2016.2516973,Data-Based Multiobjective Plant-Wide Performance Optimization of Industrial Processes Under Dynamic Environments,IEEE,Journals,"This paper provides a method for automatically selecting optimal operational indices for unit processes in an industrial plant using measured data and without knowing dynamical models of the unit process. A dynamic multiobjective optimization problem is defined to find operational indices that lead to plant-wide production indices close to their target values. A case-based reasoning (CBR) technique is also employed, which uses the stored experience of a human expert to determine appropriate operational indices for given target production indices. The solutions of the optimization problem and CBR technique are combined to form baseline operational indices. The dynamic models of the production indices, however, are time varying and affected by disturbances and online corrections of these baseline operational indices are required. To this end, reinforcement learning (RL) is used to provide a data-driven optimization technique to compensate for disturbances and model approximation errors and variations. The data-driven RL approach is used in two different time scales. The samples of the predicted production indices are used at a fast sampling rate, i.e., at each sample time, and the samples of actual production indices are used at a slower sampling rate, i.e., after each operational run, to correct the baseline operational indices. The effectiveness of this automated decision procedure has been demonstrated by successful implementation of the proposed approach on a large mineral processing plant in Gansu Province, China.",https://ieeexplore.ieee.org/document/7378488/,IEEE Transactions on Industrial Informatics,April 2016,ieeexplore
10.1109/ACCESS.2019.2894956,Data-Driven Dynamic Active Node Selection for Event Localization in IoT Applications - A Case Study of Radiation Localization,IEEE,Journals,"In this paper, the problem of active node selection for localization tasks, on the Internet of Things (IoT) sensing applications, is addressed. IoT plays a significant role in realizing the concept of smart environments, such as in environmental, infrastructural, industrial, disaster, or threat monitoring. Several IoT sensing nodes can be deployed within an area to collect regional information for the purpose of achieving a common contextual goal. Active node selection proves useful in mitigating common IoT-related issues like resource allocation, network lifetime, and the confidence in the collected data, by having the right sensors active at a given time. Current active node selection schemes prove inefficient when adapted to localization tasks, as they- 1) are usually designed for general monitoring, not localization, 2) do not dynamically exploit data readings in the selection process, and 3) are mostly designed for systems with nodes having sensing ranges. To address these challenges, we propose a novel Data-driven active node selection approach that- 1) dynamically uses data readings from current active nodes to select future ones, 2) assesses the area coverage achieved by a group of nodes while considering range-free sensors, 3) considers parameters like residual energy, power cost, and data confidence levels in the selection process, and 4) combines group-based and individual-based selection mechanisms to enhance the localization process in terms of time and power consumption. These considerations are integrated into a two-phase active node selection mechanism that uses genetic and greedy algorithms to select optimum groups for localization tasks. The efficacy of the proposed approach is validated through an example of radioactive source localization by using real-life and synthetic datasets, and by comparing the proposed approach to existing benchmarks. The results demonstrate the ability of the proposed approach to performing faster localization at low energy cost, even with a smaller number of active nodes.",https://ieeexplore.ieee.org/document/8625410/,IEEE Access,2019,ieeexplore
10.1109/ACCESS.2017.2764474,Data-Driven Inter-Turn Short Circuit Fault Detection in Induction Machines,IEEE,Journals,"Inter-turn short circuit (ITSC) fault is one of the critical electrical faults in induction motors that affects the reliability of many industrial applications. Although the use of data-driven fault detection techniques have gained much interest, the main deterrent in using these approaches in detecting ITSC faults is in the generalization and robustness of the diagnosis. In this paper, a data-driven on-line fault detection framework, incorporated with multi-feature extraction/selection and multi-classifier ensemble is proposed, capable of detecting ITSC faults in induction motors (IMs) that subjected to variable operating conditions. By using the synchronous time series signals collected from the machines, multiple feature extraction/selection is explored to find the sensitive faulty features, and the different types of classification strategies is used to increase the diversity of single based models. With the increased diversity of the base learners, the fault detection accuracy is expected to be enhanced and the robustness can be guaranteed. The framework was implemented and tested using real data collected from a designed test bed, with the experimental results showing the effectiveness of the framework in detecting ITSC faults in IMs.",https://ieeexplore.ieee.org/document/8081735/,IEEE Access,2017,ieeexplore
10.1109/TIE.2019.2927197,"Data-Driven Modeling Based on Two-Stream <inline-formula><tex-math notation=""LaTeX"">${\rm{\lambda }}$</tex-math></inline-formula> Gated Recurrent Unit Network With Soft Sensor Application",IEEE,Journals,"Data-driven soft sensors, estimating the pivotal quality variables, have been widely employed in industrial process. This paper proposes a novel soft sensor modeling approach based on a two-stream λ gated recurrent unit (T S - λ GRU) network. First, factors λ<sub>1</sub> and λ<sub>2</sub> are implemented to alter the linear constraint existing in the original GRU unit, enriching the information passing through. Then, a two-stream network structure is designed, equipped with some advanced network parameter adjustment techniques, such as batch normalization and dropout rate, to learn diverse features of the various process data. Finally, the learned features from the two streams are fused and a supervised learning regression layer is employed to decrease the error between the output and label. The application in melt viscosity index estimation for a real polymerization industrial process has demonstrated that the proposed T S - λ GRUs algorithm for soft sensor modeling is more accurate and promising than other existing methods.",https://ieeexplore.ieee.org/document/8763912/,IEEE Transactions on Industrial Electronics,Aug. 2020,ieeexplore
10.1109/ACCESS.2021.3101284,"Data-Driven Remaining Useful Life Estimation for Milling Process: Sensors, Algorithms, Datasets, and Future Directions",IEEE,Journals,"An increase in unplanned downtime of machines disrupts and degrades the industrial business, which results in substantial credibility damage and monetary loss. The cutting tool is a critical asset of the milling machine; the failure of the cutting tool causes a loss in industrial productivity due to unplanned downtime. In such cases, a proper predictive maintenance strategy by real-time health monitoring of cutting tools becomes essential. Accurately predicting the useful life of equipment plays a vital role in the predictive maintenance arena of industry 4.0. Many active research efforts have been done to estimate tool life in varied directions. However, the consolidated study of the implemented techniques and future pathways is still missing. So, the purpose of this paper is to provide a systematic and comprehensive literature survey on the data-driven approach of Remaining Useful Life (RUL) estimation of cutting tools during the milling process. The authors have summarized different monitoring techniques, feature extraction methods, decision-making models, and available sensors currently used in the data-driven model. The authors have also presented publicly available datasets related to milling under various operating conditions to compare the accuracy of the prediction model for tool wear estimation. Finally, the article concluded with the challenges, limitations, recent advancements in RUL prognostics techniques using Artificial Intelligence (AI), and future research scope to explore more in this area.",https://ieeexplore.ieee.org/document/9502093/,IEEE Access,2021,ieeexplore
10.1109/TII.2011.2158839,Decentralized Reconfiguration of a Flexible Transportation System,IEEE,Journals,"This paper presents a decentralized approach for the local reconfiguration of control software, which is based on a multiagent system with ontology-driven reasoning. We apply this approach to a transportation system and demonstrate improvements on efficiency, fault tolerance and stability with several experiments. One key element to achieve these results is the use of ontologies to ensure the consistency of local reconfiguration of the control software with the desired global behavior of the system. To show the feasibility of our approach in a realistic industrial setting, we implemented the multiagent system on the “Testbed for Distributed Holonic Control” at the Automation and Control Institute. We also used simulation to analyze its impact on the system performance. The simulation results as well as the real system experiments indicate that our approach is able to cope with the dynamic nature of the transportation domain thereby enhancing reconfigurability, robustness, and fault tolerance.",https://ieeexplore.ieee.org/document/5928365/,IEEE Transactions on Industrial Informatics,Aug. 2011,ieeexplore
10.1109/TII.2018.2807797,Deep Endoscope: Intelligent Duct Inspection for the Avionic Industry,IEEE,Journals,"We present the first autonomous endoscope for the visual inspection of very small ducts and cavities, up to a 6-mm diameter. The system has been designed, implemented, and tested in a challenging industrial scenario and in strict collaboration with an avionic industry partner. The inspected objects are metallic gearboxes eventually presenting different residuals (e.g., sand, machining swarfs, and metallic dust) inside the oil ducts. The automatic system is actuated by a robotic arm that moves the endoscope with a microcamera inside the gearbox duct, while a deep-learning-based spatio-temporal image analysis module detects, classifies, and localizes defects in real time. Feedback is given to the robotic arm in order to move or extract the endoscope given the detected anomalies. Evaluation provides a detection rate of nearly 98% given different tests with different types of residuals and duct structures.",https://ieeexplore.ieee.org/document/8295126/,IEEE Transactions on Industrial Informatics,April 2018,ieeexplore
10.1109/ACCESS.2021.3072916,Deep Learning Anomaly Detection for Cellular IoT With Applications in Smart Logistics,IEEE,Journals,"The number of connected Internet of Things (IoT) devices within cyber-physical infrastructure systems grows at an increasing rate. This poses significant device management and security challenges to current IoT networks. Among several approaches to cope with these challenges, data-based methods rooted in deep learning (DL) are receiving an increased interest. In this paper, motivated by the upcoming surge of 5G IoT connectivity in industrial environments, we propose to integrate a DL-based anomaly detection (AD) as a service into the 3GPP mobile cellular IoT architecture. The proposed architecture embeds autoencoder based anomaly detection modules both at the IoT devices (ADM-EDGE) and in the mobile core network (ADM-FOG), thereby balancing between the system responsiveness and accuracy. We design, integrate, demonstrate and evaluate a testbed that implements the above service in a real-world deployment integrated within the 3GPP Narrow-Band IoT (NB-IoT) mobile operator network.",https://ieeexplore.ieee.org/document/9402912/,IEEE Access,2021,ieeexplore
10.1109/ACCESS.2019.2924030,Demand Response Management for Industrial Facilities: A Deep Reinforcement Learning Approach,IEEE,Journals,"As a major consumer of energy, the industrial sector must assume the responsibility for improving energy efficiency and reducing carbon emissions. However, most existing studies on industrial energy management are suffering from modeling complex industrial processes. To address this issue, a model-free demand response (DR) scheme for industrial facilities was developed. In practical terms, we first formulated the Markov decision process (MDP) for industrial DR, which presents the composition of the state, action, and reward function in detail. Then, we designed an actor-critic-based deep reinforcement learning algorithm to determine the optimal energy management policy, where both the actor (Policy) and the critic (Value function) are implemented by the deep neural network. We then confirmed the validity of our scheme by applying it to a real-world industry. Our algorithm identified an optimal energy consumption schedule, reducing energy costs without compromising production.",https://ieeexplore.ieee.org/document/8742652/,IEEE Access,2019,ieeexplore
10.1109/JSEN.2020.3041668,Design of a Pose and Force Controller for a Robotized Ultrasonic Probe Based on Neural Networks and Stochastic Gradient Approximation,IEEE,Journals,"In medicine and engineering, the implementation of a diagnostic test using an ultrasonic sensor requires suitable contact conditions, and a correct pose to attain the best signal transmission settings. A soft sensor probe provides a good surface adaptation and forces transfer, but it introduces nonlinearities and noisy measurements, making it difficult to control the probe during a real time test by conventional algorithms. In this work, a data driven controller is developed to control force and pose of a soft contact ultrasound sensor. The adaptive controller is based on a fuzzy-rules emulated network structure with the learning algorithm using a stochastic gradient approximation. The proposed control algorithm overcomes the noise environment conditions and nonlinearities of the unknown nonlinear discrete-time system. This was numerically validated and then, experimentally tested with an industrial robotic system using an ultrasonic probe designed in our lab. The results show that the proposed controller performs well under the contact-force regulation and can find the correct contact orientation with a fast convergence.",https://ieeexplore.ieee.org/document/9274482/,IEEE Sensors Journal,"1 March1, 2021",ieeexplore
10.1109/TITS.2020.3018259,Detecting Anomalies in Intelligent Vehicle Charging and Station Power Supply Systems With Multi-Head Attention Models,IEEE,Journals,"Safe and reliable intelligent charging stations are imperative in an intelligent transportation infrastructure. Over the past few years, a big number of smart charging stations have been deployed, and most of them are online and connected, resulting in potential risks of threats. Although there exists related work on securing intelligent vehicles, very little work focused on the security of charging devices. Unlike traditional network systems, these power-related Industrial Control Systems (ICSs) use many different proprietary protocols and diverse interactions. Traditional anomaly detection methods based on network traffic are thus not suitable for these systems. In this work, we propose an anomaly detection method in real vehicle power supply systems based on a deep architecture model. In particular, we propose a novel traffic anomaly detection model based on Multi-Head Attentions (MHA) that take into account the inherent correlations of traffic generated by ICSs. The MHA model is employed to substitute the traditional feature extraction and rule making process with an acceptable computational cost for classifying traffic data. It is an attention-based model that employs Google Transformer encoder architecture to extract recessive features of traffic for anomaly detection. The effectiveness of the model is demonstrated by experiments on two real-world power ICS testbeds including a substation with a slave charging station and a power generation simulation platform based on a distributed control system. Comprehensive experimental results indicate that the MHA model outperforms the Convolutional Neural Networks (CNN)-based and classical machine learning detection models with an accuracy rate of 99.86%.",https://ieeexplore.ieee.org/document/9184272/,IEEE Transactions on Intelligent Transportation Systems,Jan. 2021,ieeexplore
10.1109/72.641456,Development and application of an integrated neural system for an HDCL,IEEE,Journals,"This study presents the development and industrial application of an integrated neural system in coating weight control for a modern hot dip coating line (HDCL) in a steel mill. The neural system consists of two multilayered feedforward neural networks and a neural adaptive controller. They perform coating weight real-time prediction, feedforward control (FFC), and adaptive feedback control (FBC), respectively. The production line analysis, neural system architecture, learning, associative memories, generalization and real-time applications are addressed in this paper. This integrated neural system has been successfully implemented and applied to an HDCL at Burns Harbor Division, Bethlehem Steel Co., Chesterton, IN. The industrial application results have shown significant improvements in reduction of coating weight transitional footage, variation of the error between the target and actual coating weight, and the coating material used. Some practical aspects for applying a neural system to industrial control are discussed as concluding remarks.",https://ieeexplore.ieee.org/document/641456/,IEEE Transactions on Neural Networks,Nov. 1997,ieeexplore
10.1109/TIA.2007.900472,Development of a Self-Tuned Neuro-Fuzzy Controller for Induction Motor Drives,IEEE,Journals,"In this paper, a novel adaptive neuro-fuzzy (NF)-based speed control of an induction motor (IM) is presented. The proposed NF controller (NFC) incorporates fuzzy logic laws with a five-layer artificial neural network scheme. In this controller, only three membership functions are used for each input for low computational burden, which will be suitable for real-time implementation. Furthermore, for the proposed NFC, an improved self-tuning method is developed based on the knowledge of intelligent algorithms and high-performance requirements of motor drives. The main task of the tuning method is to adjust the parameters of the fuzzy logic controller (FLC) in order to minimize the square of the error between actual and reference outputs. A complete model for indirect field-oriented control of IM incorporating the proposed NFC is developed. The performance of the proposed NFC-based IM drive is investigated extensively both in simulation and in experiment at different operating conditions. In order to prove the superiority of the proposed NFC, the results for the proposed controller are also compared to those obtained by conventional proportional-integral (PI) and FLC controllers. The proposed NFC-based IM drive is found to be more robust as compared to conventional PI and FLC controllers and, hence, suitable for high-performance industrial drive applications.",https://ieeexplore.ieee.org/document/4276869/,IEEE Transactions on Industry Applications,July-aug. 2007,ieeexplore
10.1109/TSMC.2019.2933161,Device-Free Orientation Detection Based on CSI and Visibility Graph,IEEE,Journals,"Nonintrusive orientation detection is an important yet largely unaddressed area. It can be used in many important applications, e.g., interactive games, medical care, and various industrial scenarios. Moreover, our novel techniques can be readily implemented in other critical areas, such as indoor localization, objective tracking, movement detection, etc. Many factors can limit the performance of detection algorithms in real-word applications, an important one of which is the negligence of subcarrier correlations. Therefore, we propose to build our system based on existing WiFi infrastructure and its channel state information (CSI). To explore the correlations of adjacent subcarriers, we apply the visibility graph (VG)-based network analysis method to process the CSI data. Specifically, in this article we make the following contributions: 1) we design a CSI-based orientation detection system; 2) we model the correlations between subcarriers using complex network and propose a VG-based feature extraction technique; and 3) we demonstrate the performance and effectiveness of our system with commercial products in real-world deployments. The experimental results show that our technique can achieve more than 98% accuracy and at least 26% better than the baseline approaches.",https://ieeexplore.ieee.org/document/8828088/,"IEEE Transactions on Systems, Man, and Cybernetics: Systems",July 2021,ieeexplore
10.1109/TII.2021.3067915,Diagnosis of Interturn Short-Circuit Faults in Permanent Magnet Synchronous Motors Based on Few-Shot Learning Under a Federated Learning Framework,IEEE,Journals,"A large amount of labeled data are important to enhance the performance of deep-learning-based methods in the area of fault diagnosis. Because it is difficult to obtain high-quality samples in real industrial applications, federated learning is an effective framework for solving the problem of sparse samples by using the distributed data. Its global model is updated by the local client without sharing data at each round. Considering computing resources and communication loss of multiple clients, an efficient method based on stacked sparse autoencoders (SSAEs) and Siamese networks is proposed to detect interturn short-circuit (ITSC) faults in permanent magnet synchronous motors. In this article, to achieve an accurate ITSC fault detection, an SSAE was employed to extract sparse features in a limited number of samples, and Siamese networks were used to determine the similarity between the given samples. The problem of fault diagnosis is transformed into a classification problem under few-shot learning. Furthermore, the proposed method is trained successfully in the frameworks of centralized learning and decentralized structure. The experimental results indicate that the proposed method achieved high fault diagnosis accuracy. Moreover, it is suitable for deployment in smart manufacturing systems.",https://ieeexplore.ieee.org/document/9384245/,IEEE Transactions on Industrial Informatics,Dec. 2021,ieeexplore
10.1109/ACCESS.2020.2998723,"Digital Twin for the Oil and Gas Industry: Overview, Research Trends, Opportunities, and Challenges",IEEE,Journals,"With the emergence of industry 4.0, the oil and gas (O&amp;G) industry is now considering a range of digital technologies to enhance productivity, efficiency, and safety of their operations while minimizing capital and operating costs, health and environment risks, and variability in the O&amp;G project life cycles. The deployment of emerging technologies allows O&amp;G companies to construct digital twins (DT) of their assets. Considering DT adoption, the O&amp;G industry is still at an early stage with implementations limited to isolated and selective applications instead of industry-wide implementation, limiting the benefits from DT implementation. To gain the full potential of DT and related technological adoption, a comprehensive understanding of DT technology, the current status of O&amp;G-related DT research activities, and the opportunities and challenges associated with the deployment of DT in the O&amp;G industry are of paramount importance. In order to develop this understanding, this paper presents a literature review of DT within the context of the O&amp;G industry. The paper follows a systematic approach to select articles for the literature review. First, a keywords-based publication search was performed on the scientific databases such as Elsevier, IEEE Xplore, OnePetro, Scopus, and Springer. The filtered articles were then analyzed using online text analytic software (Voyant Tools) followed by a manual review of the abstract, introduction and conclusion sections to select the most relevant articles for our study. These articles and the industrial publications cited by them were thoroughly reviewed to present a comprehensive overview of DT technology and to identify current research status, opportunities and challenges of DT deployment in the O&amp;G industry. From this literature review, it was found that asset integrity monitoring, project planning, and life cycle management are the key application areas of digital twin in the O&amp;G industry while cyber security, lack of standardization, and uncertainty in scope and focus are the key challenges of DT deployment in the O&amp;G industry. When considering the geographical distribution for the DT related research in the O&amp;G industry, the United States (US) is the leading country, followed by Norway, United Kingdom (UK), Canada, China, Italy, Netherland, Brazil, Germany, and Saudi Arabia. The overall publication rate was less than ten articles (approximately) per year until 2017, and a significant increase occurred in 2018 and 2019. The number of journal publications was noticeably lower than the number of conference publications, and the majority of the publications presented theoretical concepts rather than the industrial implementations. Both these observations suggest that the DT implementation in the O&amp;G industry is still at an early stage.",https://ieeexplore.ieee.org/document/9104682/,IEEE Access,2020,ieeexplore
10.1109/ACCESS.2019.2928141,Digital Twin-Based Optimization for Ultraprecision Motion Systems With Backlash and Friction,IEEE,Journals,"A digital twin-based optimization procedure is presented for an ultraprecision motion system with a flexible shaft connecting the motor to the (elastic) load, which is subject to both backlash and friction. The main contributions of the study are the design of the digital twin and its implementation, assuming a two-mass drive system. The procedure includes the virtual representation of mechanical and electrical components, non-linearities (backlash and friction), and the corresponding control system. A procedure for digital twin-based optimization is also presented, in which the maximum absolute position error is minimized while maintaining accuracy with no significant increase in the control effort. The optimal settings for the controller parameters and for the backlash peak amplitude, the backlash peak time, and the hysteresis amplitude are then determined, in order to guarantee an appropriate dynamic response in the presence of backlash and friction. The surface quality of certain manufactured components, such as hip and knee implants, depends on the smoothness and the accuracy of the real trajectory produced in the cutting process that is strongly influenced by the maximum position error. The simulations and experimental studies are presented using a real platform and two references for trajectory control, and a comparison of four digital twin-based optimization methods. The simulation study and the real-time experiments demonstrate the suitability of the digital twin-based optimization procedure and lay the foundations for the implementation of the proposed method at an industrial level.",https://ieeexplore.ieee.org/document/8759853/,IEEE Access,2019,ieeexplore
10.1109/ACCESS.2021.3120843,Digital Twins From Smart Manufacturing to Smart Cities: A Survey,IEEE,Journals,"Digital twins are quickly becoming a popular tool in several domains, taking advantage of recent advancements in the Internet of Things, Machine Learning and Big Data, while being used by both the industry sector and the research community. In this paper, we review the current research landscape as regards digital twins in the field of smart cities, while also attempting to draw parallels with the application of digital twins in Industry 4.0. Although digital twins have received considerable attention in the Industrial Internet of Things domain, their utilization in smart cities has not been as popular thus far. We discuss here the open challenges in the field and argue that digital twins in smart cities should be treated differently and be considered as cyber-physical “systems of systems”, due to the vastly different system size, complexity and requirements, when compared to other recent applications of digital twins. We also argue that researchers should utilize established tools and methods of the smart city community, such as co-creation, to better handle the specificities of this domain in practice.",https://ieeexplore.ieee.org/document/9576739/,IEEE Access,2021,ieeexplore
10.1109/ACCESS.2020.3036769,Drill Fault Diagnosis Based on the Scalogram and Mel Spectrogram of Sound Signals Using Artificial Intelligence,IEEE,Journals,"In industry, the ability to detect damage or abnormal functioning in machinery is very important. However, manual detection of machine fault sound is economically inefficient and labor-intensive. Hence, automatic machine fault detection (MFD) plays an important role in reducing operating and personnel costs compared to manual machine fault detection. This research aims to develop a drill fault detection system using state-of-the-art artificial intelligence techniques. Many researchers have applied the traditional approach design for an MFD system, including handcrafted feature extraction of the raw sound signal, feature selection, and conventional classification. However, drill sound fault detection based on conventional machine learning methods using the raw sound signal in the time domain faces a number of challenges. For example, it can be difficult to extract and select good features to input in a classifier, and the accuracy of fault detection may not be sufficient to meet industrial requirements. Hence, we propose a method that uses deep learning architecture to extract rich features from the image representation of sound signals combined with machine learning classifiers to classify drill fault sounds of drilling machines. The proposed methods are trained and evaluated using the real sound dataset provided by the factory. The experiment results show a good classification accuracy of 80.25 percent when using Mel spectrogram and scalogram images. The results promise significant potential for using in the fault diagnosis support system based on the sounds of drilling machines.",https://ieeexplore.ieee.org/document/9252126/,IEEE Access,2020,ieeexplore
10.1109/ACCESS.2021.3079447,ECT-LSTM-RNN: An Electrical Capacitance Tomography Model-Based Long Short-Term Memory Recurrent Neural Networks for Conductive Materials,IEEE,Journals,"Image reconstruction for industrial applications based on Electrical Capacitance Tomography (ECT) has been broadly applied. The goal of image reconstruction based ECT is to locate the distribution of permittivity for the dielectric substances along the cross-section based on the collected capacitance data. In the ECT-based image reconstruction process: (1) the relationship between capacitance measurements and permittivity distribution is nonlinear, (2) the capacitance measurements collected during image reconstruction are inadequate due to the limited number of electrodes, and (3) the reconstruction process is subject to noise leading to an ill-posed problem. Thence, constructing an accurate algorithm for real images is critical to overcoming such restrictions. This paper presents novel image reconstruction methods using Deep Learning for solving the forward and inverse problems of the ECT system for generating high-quality images of conductive materials in the Lost Foam Casting (LFC) process. Here, Long Short-Term Memory Recurrent Neural Network (LSTM-RNN) models were implemented to predict the distribution of metal filling for the LFC process-based ECT. The recurrent connection and the gating mechanism of the LSTM is capable of extracting the contextual information that is repeatedly passing through the neural network while filtering out the noise caused by adverse factors. Experimental results showed that the presented ECT-LSTM-RNN model is highly reliable for industrial applications and can be utilized for other manufacturing processes.",https://ieeexplore.ieee.org/document/9429218/,IEEE Access,2021,ieeexplore
10.1109/TSE.2013.2295827,Effects of Developer Experience on Learning and Applying Unit Test-Driven Development,IEEE,Journals,"Unit test-driven development (UTDD) is a software development practice where unit test cases are specified iteratively and incrementally before production code. In the last years, researchers have conducted several studies within academia and industry on the effectiveness of this software development practice. They have investigated its utility as compared to other development techniques, focusing mainly on code quality and productivity. This quasi-experiment analyzes the influence of the developers' experience level on the ability to learn and apply UTDD. The ability to apply UTDD is measured in terms of process conformance and development time. From the research point of view, our goal is to evaluate how difficult is learning UTDD by professionals without any prior experience in this technique. From the industrial point of view, the goal is to evaluate the possibility of using this software development practice as an effective solution to take into account in real projects. Our results suggest that skilled developers are able to quickly learn the UTDD concepts and, after practicing them for a short while, become as effective in performing small programming tasks as compared to more traditional test-last development techniques. Junior programmers differ only in their ability to discover the best design, and this translates into a performance penalty since they need to revise their design choices more frequently than senior programmers.",https://ieeexplore.ieee.org/document/6690135/,IEEE Transactions on Software Engineering,April 2014,ieeexplore
10.1109/ACCESS.2021.3111783,Emerging Tools for Link Adaptation on 5G NR and Beyond: Challenges and Opportunities,IEEE,Journals,"With the speeding up of the fifth generation (5G) new radio (NR) worldwide commercialization, one of the paramount questions for operators and vendors is how to optimize the radio links, considering the widely diverse scenarios envisioned. One of the key pillars of 5G has been an unprecedented flexibility on the configuration of the radio access network (RAN) on scenarios that include cellular, vehicular, and industrial networks among others. This flexibility has its main exponent on link adaptation (LA), which has evolved into a multi-domain technique where a plethora of parameters, like numerology, bandwidth part, radio frequency beam, power, modulation and coding scheme (MCS) or multiple antenna precoding can be adapted to the instantaneous link conditions. Although such enhancements open the door to a significant performance improvement, they also pose many challenges to LA optimization. In this article, we first present the signaling aspects of NR technology for multi-domain LA and the challenges that need to be faced. Then, we explore the latest advances on LA for wireless networks. We envision a combination of machine learning (ML) tools with multi-domain LA as a key enabler for 5G and beyond networks. Finally, we investigate emerging ML approaches for LA and present a promising application of ML for LA that is assessed with simulations. With this scheme, the training is performed at the network side to relieve the user equipment (UE) to do such a complex task. It is shown with simulations that our ML approach outperforms the well-known outer loop link adaptation (OLLA) algorithm in terms of instantaneous block error rate (BLER), while reaching the same average spectral efficiency (SE). Interestingly, it is shown that the proposed scheme only requires 4 bits to represent the features used to train the model, which makes it suitable for implementation in real systems with limited feedback.",https://ieeexplore.ieee.org/document/9534770/,IEEE Access,2021,ieeexplore
10.1109/JSTARS.2015.2442584,Estimation of Seismic Vulnerability Levels of Urban Structures With Multisensor Remote Sensing,IEEE,Journals,"The ongoing global transformation of human habitats from rural villages to ever growing urban agglomerations induces unprecedented seismic risks in earthquake prone regions. To mitigate affiliated perils requires the seismic assessment of built environments. Numerous studies emphasize that remote sensing can play a valuable role in supporting the extraction of relevant features for preevent vulnerability analysis. However, the majority of approaches operate on building level. This induces the deployment of very high spatial resolution remote sensing data, which hampers, nowadays, utilization capabilities for larger areas due to data costs and processing requirements. In this paper, we alter the spatial scale of analysis and propose concepts and methods to estimate the seismic vulnerability level of homogeneous urban structures. A procedure is designed, which comprises four main steps dedicated to: 1) delineation of urban structures by means of a tailored unsupervised data segmentation procedure with scale optimization; 2) characterization of urban structures by a joint exploitation of multisensor data; 3) selection of most feasible features under consideration of in situ vulnerability information; and 4) estimation of seismic vulnerability levels of urban structures within a supervised learning framework. We render the prediction problem in three ways to address operational requirements that can evolve in real-life situations. 1) To discriminate two or more classes based on labeled samples of all classes present in the data under investigation, we use the framework of soft margin support vector machines (C-SVM). 2) To consider situations, where solely labeled samples are available for the class(es) of interest and not for all classes present in the data, we deploy ensembles of ν-one-class SVM (ν-OC-SVM). and 3) To fit data with a higher statistical level of measurement (interval or ratio scale), we utilize a support vector regression (SVR) approach to estimate a regression function from the training samples. Experimental results are obtained for the earthquake-prone mega city Istanbul, Turkey. We use multispectral data from the RapidEye constellation, elevation measurements from the TanDEM-X mission, and spatiotemporal analyses based on data from the Landsat archive to characterize the urban environment. In addition, different in situ data sets are incorporated for Istanbul's district Zeytinburnu and the residual settlement area of Istanbul. When estimating damage grades for Zeytinburnu with SVR, best models are characterized by mean absolute percentage errors less than 11%, and fairly strong goodness of fit (R &gt; 0.75). When aiming to identify different types of urban structures for the remaining settlement area of Istanbul (i.e., urban structures determined by large industrial/commercial buildings and tall detached residential buildings, which can be considered here as highly and slightly vulnerable, respectively), results obtained with C-SVM show a distinctive increase of accuracy compared to results obtained with ensembles of ν-OC-SVM. The latter were not able to exceed moderate agreements, with κ statistics slightly above 0.45. Instead, C-SVM allowed obtaining ν statistics expressing substantial and even excellent agreements (κ &gt; 0.6 up to κ &gt; 0.8). Overall, analyzes provide very promising empirical evidence, which confirms the potential of remote sensing to support seismic vulnerability assessment.",https://ieeexplore.ieee.org/document/7150321/,IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing,May 2016,ieeexplore
10.1109/TII.2018.2850001,Event-Triggered Globalized Dual Heuristic Programming and Its Application to Networked Control Systems,IEEE,Journals,"Networked control systems (NCSs) provide many benefits, such as higher control accuracy and better robustness with the successively increasing computational complexity and communication burden. This results in the traditional adaptive dynamic programming control method having difficulty meeting the real-time requirements of industrial systems. In this paper, a novel event-triggered globalized dual heuristic programming method is proposed to reduce the required samples while guaranteeing the stability of the system. In the proposed method, the NCSs can communicate and update the control law only when the designed event-triggered condition is violated. Furthermore, the Elman neural network, which is a dynamic feedback network with a memory function is implemented to reconstruct the state variables as an approximator, and it depends only on the input and output data. To obtain fewer event-triggered times, two optimization methods, i.e., the unscented Kalman filter and the multiobjective quantum particle swarm optimization, are used to optimize the initial weights of the networks and the positive constant in the event-triggered condition, respectively. The simulation results on industrial system of aluminum electrolysis production are included to verify the performance of the controller.",https://ieeexplore.ieee.org/document/8395070/,IEEE Transactions on Industrial Informatics,March 2019,ieeexplore
10.1109/TCAD.2020.3012648,Exploring Edge Computing for Multitier Industrial Control,IEEE,Journals,"Industrial automation traditionally relies on local controllers implemented on microcontrollers or programmable logic controllers. With the emergence of edge computing, however, industrial automation evolves into a distributed two-tier computing architecture comprising local controllers and edge servers that communicate over wireless networks. Compared to local controllers, edge servers provide larger computing capacity at the cost of data loss over wireless networks. This article presents switching multitier control (SMC) to exploit edge computing for industrial control. SMC dynamically optimizes control performance by switching between local and edge controllers in response to changing network conditions. SMC employs a data-driven approach to derive switching policies based on classification models trained based on simulations while guaranteeing system stability based on an extended Simplex approach tailored for two-tier platforms. To evaluate the performance of industrial control over edge computing platforms, we have developed WCPS-EC, a real-time hybrid simulator that integrates simulated plants, real computing platforms, and real or simulated wireless networks. In a case study of an industrial robotic control system, SMC significantly outperformed both a local controller and an edge controller in face of varying data loss in a wireless network.",https://ieeexplore.ieee.org/document/9211472/,IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems,Nov. 2020,ieeexplore
10.1109/TIM.2021.3089240,Fault Diagnosis of Harmonic Drive With Imbalanced Data Using Generative Adversarial Network,IEEE,Journals,"Harmonic drive is the core component of the industrial robot, and its fault diagnosis is crucial to the reliability and performance of the equipment. Most machine learning methods achieve good results based on the assumption of data balance. However, the scarce fault data of harmonic drive is difficult to collect, resulting in various imbalanced health status samples, which has an adverse effect on fault diagnosis. In this article, we propose a data generation method based on generative adversarial networks (GANs) to solve the problem of data imbalance and utilize the multiscale convolutional neural network (MSCNN) to realize the fault diagnosis of the harmonic drive. First, the data collected from three vibration acceleration sensors are preprocessed by fast Fourier transform (FFT) to obtain the frequency spectrum of the vibration signal. Second, multiple GANs were adopted to generate various fault spectrum data and the data selection module (DSM) is elaborately designed to filter and purify these data. Third, the filtered generated data will be combined with the real data to form a balanced dataset, and then the MSCNN is used to achieve multiclassification of the health status of the harmonic drive. Finally, the experiments have been implemented on an industrial robot vibration test bench to validate the effectiveness of our approach. The results have shown the fault multiclassification accuracy as 98.49% under imbalanced fault data conditions, which outperforms that of the other compared methods.",https://ieeexplore.ieee.org/document/9454583/,IEEE Transactions on Instrumentation and Measurement,2021,ieeexplore
10.1109/TII.2019.2937876,Federated Tensor Mining for Secure Industrial Internet of Things,IEEE,Journals,"In a vertical industry alliance, Internet of Things (IoT) deployed in different smart factories are similar. For example, most automobile manufacturers have the similar assembly lines and IoT surveillance systems. It is common to observe the industrial knowledge using deep learning and data mining methods based on the IoT data. However, some knowledge is not easy to be mined from only one factory's data because the samples are still few. If multiple factories within an alliance can gather their data together, more knowledge could be mined. However, the key concern of these factories is the data security. Existing matrix-based methods can guarantee the data security inside a factory but do not allow the data sharing among factories, and thus their mining performance is poor due to lack of correlation. To address this concern, in this article we propose the novel federated tensor mining (FTM) framework to federate multisource data together for tensor-based mining while guaranteeing the security. The key contribution of FTM is that every factory only needs to share its ciphertext data for security issue, and these ciphertexts are adequate for tensor-based knowledge mining due to its homomorphic attribution. Real-data-driven simulations demonstrate that FTM not only mines the same knowledge compared with the plaintext mining, but also is enabled to defend the attacks from distributed eavesdroppers and centralized hackers. In our typical experiment, compared with the matrix-based privacy-preserving compressive sensing (PPCS), FTM increases up to 24% on mining accuracy.",https://ieeexplore.ieee.org/document/8815886/,IEEE Transactions on Industrial Informatics,March 2020,ieeexplore
10.1109/TCST.2017.2756962,Full-State Tracking Control for Flexible Joint Robots With Singular Perturbation Techniques,IEEE,Journals,"This paper proposes a practical method to realize multivariable full-state tracking control for industrial robots with elastic joints. Unlike existing methods, the proposed method does not require high-order derivatives of the link states such as acceleration and jerk. Therefore, the proposed method does not suffer from chatter related to inaccurate estimation of high-order derivatives. The method is derived by adopting a singular perturbation technique. A decoupled error dynamics is achieved by two decoupling control loops: a fast loop that controls the deflection error and a slow loop for tracking control on the link side. Our stability analysis based on a linear system shows that the proposed control system is stable as long as the fast system is at least twice as fast as the slow system. A practical method to select the gain is also presented such that the closed-loop poles are placed at the desired locations. In simulation, we compare the proposed method with feedback linearization. The results indicate that in an ideal scenario the proposed method can obtain a similar performance as feedback linearization. However, the proposed method obtains a superior performance in a realistic scenario. A real-world experiment with a six degree-of-freedom commercial industrial robot is carried out to further validate our approach.",https://ieeexplore.ieee.org/document/8065027/,IEEE Transactions on Control Systems Technology,Jan. 2019,ieeexplore
10.1109/TCAD.2018.2884992,GPGPU-Based ATPG System: Myth or Reality?,IEEE,Journals,"General-purpose computing on graphics processing units (GPGPUs) is a programming model that uses graphics cards to perform computations traditionally done by CPU. It began to become practical with the advent of programmable shaders and floating-point support on GPU in around 2001. The spread of GPGPU has been accelerated with introduction of CUDA from NVIDIA in 2006 and later OpenCL in 2009. Nowadays GPGPU is widely deployed in various applications, such as data mining, artificial intelligence, and many scientific computations. GPGPU seemingly promises immense parallelism with massive concurrent cores, and thus much shorter run times. This is true for algorithms that bear intrinsic data and task parallelism, such as image and video processing. For an ATPG system where some algorithms are sequential in nature, the speedup is not easy to achieve in the real world. Flaws in setting up speedup evaluation can lead to false promises. Will GPGPU-based ATPG system become a reality? Or it is just a myth. In this paper, we try to provide an answer by surveying state-of-the-art works and by analyzing practical aspects of today's industrial designs.",https://ieeexplore.ieee.org/document/8558526/,IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems,Jan. 2020,ieeexplore
10.1109/ACCESS.2020.2976808,Graph-Based Method for Fault Detection in the Iron-Making Process,IEEE,Journals,"Since the iron-making process is performed in complicated environments and controlled by operators, observation labeling is difficult and time-consuming. Therefore, unsupervised fault detection methods are a promising research topic. Recently, an unsupervised graph-based change point detection method has been introduced, and the graph of observations is constructed by the minimum spanning tree. In this paper, a novel fault detection method based on the graph for an iron-making process is proposed, and a weight calculation method for constructing the minimum spanning tree is introduced. The Euclidean distance and Mahalanobis distance are combined to calculate the weights in the minimum spanning tree, which contain important relations of variables. The distance calculation method is determined by the correlation coefficients of variables. Each testing observation is set as a change point candidate, and a change point candidate divides the observations into two groups. The number of a special type of edge in the minimum spanning tree is used as a fault detection statistic. That special edge connects two observations from two different groups. The minimum number of that type of edge corresponding to the change point candidate is a true change point. Finally, numerical simulation is used to test the power of the proposed method, and a real iron-making process including low stock, cooling, and slip faults is implemented to illustrate the effectiveness of fault detection in industrial processes.",https://ieeexplore.ieee.org/document/9015995/,IEEE Access,2020,ieeexplore
10.1109/TVT.2021.3084829,Guest EditorialIntroduction to the Special Section on Vehicular Networks in the Era of 6G: End-Edge-Cloud Orchestrated Intelligence,IEEE,Journals,"The articles in this special section focus on vehicular networks in the era of 6G mobile communication. With the growth of the vehicle population, vehicular networks play a key role in building safe, efficient, and intelligent transport systems and has been attracting a lot of attention from both academic and industrial communities around the world. The rise of autonomous driving technology and the prosperity of mobile applications, e.g., real-time video analytic, image-aided navigation, natural language processing, and etc, have brought tremendous pressure on current vehicular networks, e.g., high bandwidth, ultra-low latency, high reliability, high security, powerful computation capability, and massive connections. It is necessary to continue to develop vehicular networks by combining the latest research intends in other fields to meet quickly rising communication and computation demands. The upcoming 6G technology, which provides Holographic and Artificial Intelligence (AI) enabled communications, together with the increasing implementation of artificial intelligence in mobile devices, will lead to a new research trend to end-edge-cloud orchestrated computing with intelligence. It means that, not only the intelligent communication protocols, but also the intelligent computing resource management and machine learning algorithms among the mobile vehicles, the edge and the cloud, should be redesigned to support the development of vehicular networks.",https://ieeexplore.ieee.org/document/9477551/,IEEE Transactions on Vehicular Technology,June 2021,ieeexplore
10.1109/TSMCB.2003.822956,Hardware implementation of fuzzy Petri net as a controller,IEEE,Journals,"The paper presents a new approach to fuzzy Petri net (FPN) and its hardware implementation. The authors' motivation is as follows. Complex industrial processes can be often decomposed into many parallelly working subprocesses, which can, in turn, be modeled using Petri nets. If all the process variables (or events) are assumed to be two-valued signals, then it is possible to obtain a hardware or software control device, which works according to the algorithm described by conventional Petri net. However, the values of real signals are contained in some bounded interval and can be interpreted as events which are not only true or false, but rather true in some degree from the interval [0, 1]. Such a natural interpretation from multivalued logic (fuzzy logic) point of view, concerns sensor outputs, control signals, time expiration, etc. It leads to the idea of FPN as a controller, which one can rather simply obtain, and which would be able to process both analog, and binary signals. In the paper both graphical, and algebraic representations of the proposed FPN are given. The conditions under which transitions can be fired are described. The algebraic description of the net and a theorem which enables computation of new marking in the net, based on current marking, are formulated. Hardware implementation of the FPN, which uses fuzzy JK flip-flops and fuzzy gates, are proposed. An example illustrating usefulness of the proposed FPN for control algorithm description and its synthesis as a controller device for the concrete production process are presented.",https://ieeexplore.ieee.org/document/1298882/,"IEEE Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics)",June 2004,ieeexplore
10.1109/ACCESS.2021.3098163,Hierarchical Control of Microgrid Using IoT and Machine Learning Based Islanding Detection,IEEE,Journals,"Due to the increase in penetration of renewable energy sources, the control technique plays a vital role to determine the performance of Microgrid (MG). Recently, the Internet of Things (IoT) and cloud computing has gained significance in solving various industrial problems. Robust and scalable Information Communication Technology (ICT) infrastructure is critical for efficient control of MG. IoT Devices with efficient measurement and control capability can play a key role in the MG environment. In this paper three layers hierarchical control of inverter based MG was developed using cloud-based IoT infrastructure and machine learning (ML) based islanding detection scheme. MG was operated in both island and grid connected mode. In the Primary layer, a voltage frequency (V-F) droop control with virtual impedance control was applied to avoid the disturbances in island mode. Moreover, Active Reactive (P-Q) power control was used for grid connected mode. In the secondary layer voltage and frequency deviations were removed by using the decentralized averaging based method. Voltage and frequency from each distributed generator (DG) were communicated by using a lightweight IoT-based protocol through an edge device (ED). Context-aware policy (CAP) was adopted in ED to optimize traffic flow over a communication network (CN) by comparing the difference in the present and previous data values. In the tertiary layer, a cloud-based ML model was developed using an artificial neural network (ANN) for islanding detection. ANN model was trained by data produced by simulating islanding scenarios in Matlab. Phasor measurement unit (PMU) data was communicated to the cloud for island prediction. The Proposed scheme was implemented on a modified IEEE-13 bus system with four inverter-based distributed generators (DGs) in Matlab, and Microsoft cloud services were used. The successful implementation of MG hierarchical control using an IoT feedback network with less data traffic along with cloud-based islanding detection using machine learning are the main contributions in this work. The whole system achieves stability within 2 seconds of islanding according to IEEE 1547 standards.",https://ieeexplore.ieee.org/document/9490217/,IEEE Access,2021,ieeexplore
10.1109/TMECH.2013.2245337,Image-Based Visual Servoing of a 7-DOF Robot Manipulator Using an Adaptive Distributed Fuzzy PD Controller,IEEE,Journals,"This paper is concerned with the design and implementation of a distributed proportional-derivative (PD) controller of a 7-degrees of freedom (DOF) robot manipulator using the Takagi-Sugeno (T-S) fuzzy framework. Existing machine learning approaches to visual servoing involve system identification of image and kinematic Jacobians. In contrast, the proposed approach actuates a control signal primarily as a function of the error and derivative of the error in the desired visual feature space. This approach leads to a significant reduction in the computational burden as compared to model-based approaches, as well as existing learning approaches to model inverse kinematics. The simplicity of the controller structure will make it attractive in industrial implementations where PD/PID type schemes are in common use. While the initial values of PD gain are learned with the help of model-based controller, an online adaptation scheme has been proposed that is capable of compensating for local uncertainties associated with the system and its environment. Rigorous experiments have been performed to show that visual servoing tasks such as reaching a static target and tracking of a moving target can be achieved using the proposed distributed PD controller. It is shown that the proposed adaptive scheme can dynamically tune the controller parameters during visual servoing, so as to improve its initial performance based on parameters obtained while mimicking the model-based controller. The proposed control scheme is applied and assessed in real-time experiments using an uncalibrated eye-in-hand robotic system with a 7-DOF PowerCube robot manipulator.",https://ieeexplore.ieee.org/document/6471828/,IEEE/ASME Transactions on Mechatronics,April 2014,ieeexplore
10.1109/TNSM.2020.3044415,In-Band Network Monitoring Technique to Support SDN-Based Wireless Networks,IEEE,Journals,"Most industrial applications demand determinism in terms of latency, reliability, and throughput. This goes hand in hand with the increased complexity of real-time network programability possibilities. To ensure network performance low-overhead, high-granularity, and timely network verification techniques need to be deployed. The first cornerstone of network verification ability is to enable end-to-end network monitoring, including end devices too. To achieve this, this article shows a novel and low overhead in-band network telemetry and monitoring technique for wireless networks focusing on IEEE 802.11 networks. A design of in-band network telemetry enabled node architecture is proposed and its proof of concept implementation is realized. The PoC realization is used to monitor a real-life SDN-based wireless network, enabling on-the-fly (re)configuration capabilities based on monitoring data. In addition, the proposed monitoring technique is validated in terms of monitoring accuracy, monitoring overhead, and network (re)configuration accuracy. It is shown that the proposed in-band monitoring technique has 6 times lower overhead than other active monitoring techniques on a single-hop link. Besides this, it is demonstrated that (re)configuration decisions taken based on monitored data fulfill targeted application requirements, validating the suitability of the proposed monitoring technique.",https://ieeexplore.ieee.org/document/9292999/,IEEE Transactions on Network and Service Management,March 2021,ieeexplore
10.1109/ACCESS.2018.2809681,Industrial Big Data Analytics for Prediction of Remaining Useful Life Based on Deep Learning,IEEE,Journals,"Due to the recent development of cyber-physical systems, big data, cloud computing, and industrial wireless networks, a new era of industrial big data is introduced. Deep learning, which brought a revolutionary change in computer vision, natural language processing, and a variety of other applications, has significant potential for solutions providing in sophisticated industrial applications. In this paper, a concept of device electrocardiogram (DECG) is presented, and an algorithm based on deep denoising autoencoder (DDA) and regression operation is proposed for the prediction of the remaining useful life of industrial equipment. First, the concept of electrocardiogram is explained. Then, a problem statement based on manufacturing scenario is presented. Subsequently, the architecture of the proposed algorithm called integrated DDA and the algorithm workflow are provided. Moreover, DECG is compared with traditional factory information system, and the feasibility and effectiveness of the proposed algorithm are validated experimentally. The proposed concept and algorithm combine typical industrial scenario and advance artificial intelligence, which has great potential to accelerate the implementation of industry 4.0.",https://ieeexplore.ieee.org/document/8302913/,IEEE Access,2018,ieeexplore
10.1109/TIE.2018.2873546,Information Fusion and Semi-Supervised Deep Learning Scheme for Diagnosing Gear Faults in Induction Machine Systems,IEEE,Journals,"There has been an increasing interest in the design of intelligent diagnostic systems for industrial applications. The key requirement in the design of practical diagnostic systems is the ability for decision making in high-dimensional feature spaces, where the prior knowledge about the system states in terms of labels is very limited. Moreover, the problem of diagnosing simultaneous defects is rarely addressed on real industrial applications. This paper aims to develop a semi-supervised deep-learning scheme for diagnosing multiple defects including simultaneous ones in a gearbox directly connected to an induction machine shaft. This scheme consists of two main modules: information fusion and decision making. The former integrates captured multiple sensory streams into a very high dimensional feature space. The latter uses a semi-supervised deep learning procedure to minimize the human interaction during the training and maximize the diagnostic efficiency. This scheme facilitates learning and diagnosing defects under harshest conditions 1) where only a few number of labeled samples are collected together with a large number of unlabeled samples, and 2) in a very high-dimensional feature space. Several state-of-the-art semi-supervised and supervised learners have also been included in the scheme, enabling a comparative experiment for diagnosing simultaneous defects.",https://ieeexplore.ieee.org/document/8488676/,IEEE Transactions on Industrial Electronics,Aug. 2019,ieeexplore
10.1109/TASE.2018.2847222,Integration of Robotic Vision and Tactile Sensing for Wire-Terminal Insertion Tasks,IEEE,Journals,"This paper reports the development of a manipulation system for electric wires, implemented by means of a commercial gripper installed on an industrial manipulator and equipped with cameras and suitably designed tactile sensors. The purpose of this system is the execution of wire insertion on commercial electromechanical components. The synergy between computer vision and tactile sensing is necessary because, in a real environment, the tight spaces very often prevent the possibility to use the vision system, also when the same task is performed by a human being. A novel technique to speed up the generation of training data sets for convolutional neural networks (CNNs) is proposed. Therefore, this technique is used to train a CNN in order to detect small objects (such as wire terminals). Moreover, aiming to prevent faults during the task and to interact with the environment safely, several machine learning approaches are used to produce an affordable output from the tactile sensor. The proposed approach shows how a cheap sensor embedded with suitable intelligence can provide information comparable to a more expensive force sensor.",https://ieeexplore.ieee.org/document/8395267/,IEEE Transactions on Automation Science and Engineering,April 2019,ieeexplore
10.1109/TIE.2021.3057030,KDnet-RUL: A Knowledge Distillation Framework to Compress Deep Neural Networks for Machine Remaining Useful Life Prediction,IEEE,Journals,"Machine remaining useful life (RUL) prediction is vital in improving the reliability of industrial systems and reducing maintenance cost. Recently, long short-term memory (LSTM) based algorithms have achieved state-of-the-art performance for RUL prediction due to their strong capability of modeling sequential sensory data. In many cases, the RUL prediction algorithms are required to be deployed on edge devices to support real-time decision making, reduce the data communication cost, and preserve the data privacy. However, the powerful LSTM-based methods which have high complexity cannot be deployed to edge devices with limited computational power and memory. To solve this problem, we propose a knowledge distillation framework, entitled KDnet-RUL, to compress a complex LSTM-based method for RUL prediction. Specifically, it includes a generative adversarial network based knowledge distillation (GAN-KD) for disparate architecture knowledge transfer, a learning-during-teaching based knowledge distillation (LDT-KD) for identical architecture knowledge transfer, and a sequential distillation upon LDT-KD for complicated datasets. We leverage simple and complicated datasets to verify the effectiveness of the proposed KDnet-RUL. The results demonstrate that the proposed method significantly outperforms state-of-the-art KD methods. The compressed model with 12.8 times less weights and 46.2 times less total float point operations even achieves a comparable performance with the complex LSTM model for RUL prediction.",https://ieeexplore.ieee.org/document/9351733/,IEEE Transactions on Industrial Electronics,Feb. 2022,ieeexplore
10.1109/LCOMM.2020.3039251,LOS/NLOS Identification for Indoor UWB Positioning Based on Morlet Wavelet Transform and Convolutional Neural Networks,IEEE,Journals,"In indoor ultra-wideband (UWB) positioning systems, positioning accuracy can be improved by determining the conditions of line-of-sight (LOS) and non-line-of-sight (NLOS) propagation and taking appropriate measures. The existing methods, such as support vector machine (SVM), decision tree (DT), k-Nearest Neighbor (KNN), identify LOS/NLOS mainly using time-domain characteristics. However, using only time-domain characteristics cannot achieve satisfactory performance. In this letter, we propose a LOS/NLOS identification method based on Morlet wave transform and convolutional neural networks (MWT-CNN). MWT-CNN is capable of identifying LOS/NLOS in the time-frequency domain. Our simulations are based on the 802.15.4a UWB model and an open-source dataset. The simulation results show that MWT-CNN achieves an accuracy of 100% in the office scenario, 99.89% in the industrial scenario, 96.10% in the residential scenario, and 98.84% in a real experimental scenario. Further simulation results show that MWT-CNN is more suitable to be deployed in static scenarios.",https://ieeexplore.ieee.org/document/9264213/,IEEE Communications Letters,March 2021,ieeexplore
10.1109/LRA.2021.3061374,Learning Variable Impedance Control via Inverse Reinforcement Learning for Force-Related Tasks,IEEE,Journals,"Many manipulation tasks require robots to interact with unknown environments. In such applications, the ability to adapt the impedance according to different task phases and environment constraints is crucial for safety and performance. Although many approaches based on deep reinforcement learning (RL) and learning from demonstration (LfD) have been proposed to obtain variable impedance skills on contact-rich manipulation tasks, these skills are typically task-specific and could be sensitive to changes in task settings. This letter proposes an inverse reinforcement learning (IRL) based approach to recover both the variable impedance policy and reward function from expert demonstrations. We explore different action space of the reward functions to achieve a more general representation of expert variable impedance skills. Experiments on two variable impedance tasks (Peg-in-Hole and Cup-on-Plate) were conducted in both simulations and on a real FANUC LR Mate 200iD/7 L industrial robot. The comparison results with behavior cloning and force-based IRL proved that the learned reward function in the gain action space has better transferability than in the force space. Experiment videos are available at https://msc.berkeley.edu/research/impedance-irl.html.",https://ieeexplore.ieee.org/document/9361101/,IEEE Robotics and Automation Letters,April 2021,ieeexplore
10.1109/ACCESS.2020.3008289,Learning-Based IoT Data Aggregation for Disaster Scenarios,IEEE,Journals,"Industrial Internet of Everything (IIoE), as the deep integration of industry 6.0, the Internet of Things (IoT) and 6G mobile communication technology, pave the way for intelligent industry, enabling industrial optimization and automation. To ensure the high quality of services (QoS) in IIoE, tremendous real-time information generated by the pervasive smart things needs to be aggregated and processed quickly and reliably. However, a large-scale disaster could damage the entire communication network and cut off data aggregation such that Qos is compromised. In this paper, an Intelligent NIB based Data Aggregation Strategy, named (IDAS), is proposed for after disaster scenarios in IIoE. Specifically, IDAS first applies both iterative cubature kalman filter and radial basis function neural network to predict the data collection rates of survived infrastructures. Then, an energy efficient task distribution mechanism is design. Next, a deep reinforcement learning method is developed for the car-carrying NIB route design to perform corresponding task. Eventually, all data are aggregated toward the rescue headquarter by NIB deployment based on Fermat tree constructions. The theoretical analysis and simulations indicate that IDAS is not only energy efficient for after disaster scenarios but requires the least NIB consumption while compared with contemporary strategies.",https://ieeexplore.ieee.org/document/9137637/,IEEE Access,2020,ieeexplore
10.1109/ACCESS.2019.2956556,Light-Weight Spliced Convolution Network-Based Automatic Water Meter Reading in Smart City,IEEE,Journals,"Automatic reading for water meter is one of the practical demands in smart city applications. Due to the high cost, it is not feasible to replace the old mechanical water meter with a new embedded electronic device. Recently, image recognition based meter reading methods have become research hotspots. However, illumination, occlusion, energy and computational consuming in IoT environment bring challenges to these methods. In this paper, we design and implement a smart water meter reading system to handle this issue. Specifically, we first propose a novel light-weight spliced convolution network to recognize the meter number, which simplifies standard 3 x 3 convolutions by splicing a certain number of 1 x 1 and 3 x 3 size kernel. We then prove the superiority of our network by theoretical analysis. Second, we have implemented the prototype which can handle huge real-time data base on the distributed cloud platform. Base on this system, our system can provide industrial service. Finally, we conduct real-world dataset to verify the performance of the system. The experimental results demonstrate that our proposed light-weight spliced convolution network can reduce nearly 10x computational consuming, 7x model space, and save 3x running time comparing with standard convolution network.",https://ieeexplore.ieee.org/document/8917620/,IEEE Access,2019,ieeexplore
10.1109/TASE.2017.2783342,MASD: A Multimodal Assembly Skill Decoding System for Robot Programming by Demonstration,IEEE,Journals,"Programming by demonstration (PBD) transforms the robot programming from the code level to automated interface between robot and human, promoting the flexibility of robotized automation. In this paper, we focus on programming the industrial robot for assembly tasks by parsing the human demonstration into a series of assembly skills and compiling the skill to the robot executables. To achieve this goal, an identification system using multimodal information to recognize the assembly skill, called MASD, is proposed including: 1) an initial learning stage using a hierarchical model to recognize the action by considering the features from action-object effect, gesture, and trajectory and 2) a retrospective thinking stage using a segmentation method to cut the continuous demonstrations into multiple assembly skills optimally. Using MASD, the demonstration of assembly tasks can be explained with high accuracy in real time, driving a hypothesis that a PBD system on the top of MASD can be extended to more realistic assembly tasks beyond pure positional moving and picking. In experiments, the skill identification module is used to recognize the five kinds of assembly skills in demonstrations of both single and multiple assembly skills, and outperforms the comparative action identification methods. Besides integrated with the MASD, the PBD system can generate the program based on the demonstration and successfully enable an ABB industrial robotic arm simulator to assemble a flashlight and a switch, verifying the initial hypothesis. Note to Practitioners-In the conventional robotized automation, the key role of the robot mainly owes to its capacity for repeating a wide variety of tasks with high speed and accuracy in long term, with a cost of days to months of programming for deployment. On the other hand, the new trend of customization brings the new characteristics: production in short cycle and small volume. This irreversible momentum urges the robot to switch from task to task efficiently. The biggest bottleneck here is the tedious programming, which also has high prerequisites for most practitioners in manufacturing. This situation motivates the development of a PBD system that can understand the assembly skills performed by the human experts in the demonstration and accordingly generate the program for robot's execution of the taught task. In this paper, we present a skill decoding system to parse the observational raw demonstration into symbolic sequences, which is the crucial bridge to enable the automatic programming. The system achieves high performance in recognition and is tailored for the PBD in assembly tasks by considering both advantages and disadvantages in the background of assembly, such as controllable environment and limited computational resources. It is particularly useful for assembly tasks with modularized actions based on a set of standard parts. At the perspective of industrial application, the PBD upon the proposed system is a promising solution to improve the flexibility of manufacture, which is expected to be true in midterm but an important step toward this goal.",https://ieeexplore.ieee.org/document/8263146/,IEEE Transactions on Automation Science and Engineering,Oct. 2018,ieeexplore
10.1109/ACCESS.2019.2942390,"Machine Learning for 5G/B5G Mobile and Wireless Communications: Potential, Limitations, and Future Directions",IEEE,Journals,"Driven by the demand to accommodate today's growing mobile traffic, 5G is designed to be a key enabler and a leading infrastructure provider in the information and communication technology industry by supporting a variety of forthcoming services with diverse requirements. Considering the ever-increasing complexity of the network, and the emergence of novel use cases such as autonomous cars, industrial automation, virtual reality, e-health, and several intelligent applications, machine learning (ML) is expected to be essential to assist in making the 5G vision conceivable. This paper focuses on the potential solutions for 5G from an ML-perspective. First, we establish the fundamental concepts of supervised, unsupervised, and reinforcement learning, taking a look at what has been done so far in the adoption of ML in the context of mobile and wireless communication, organizing the literature in terms of the types of learning. We then discuss the promising approaches for how ML can contribute to supporting each target 5G network requirement, emphasizing its specific use cases and evaluating the impact and limitations they have on the operation of the network. Lastly, this paper investigates the potential features of Beyond 5G (B5G), providing future research directions for how ML can contribute to realizing B5G. This article is intended to stimulate discussion on the role that ML can play to overcome the limitations for a wide deployment of autonomous 5G/B5G mobile and wireless communications.",https://ieeexplore.ieee.org/document/8844682/,IEEE Access,2019,ieeexplore
10.1109/JIOT.2019.2912022,Machine Learning-Based Network Vulnerability Analysis of Industrial Internet of Things,IEEE,Journals,"It is critical to secure the Industrial Internet of Things (IIoT) devices because of potentially devastating consequences in case of an attack. Machine learning (ML) and big data analytics are the two powerful leverages for analyzing and securing the Internet of Things (IoT) technology. By extension, these techniques can help improve the security of the IIoT systems as well. In this paper, we first present common IIoT protocols and their associated vulnerabilities. Then, we run a cyber-vulnerability assessment and discuss the utilization of ML in countering these susceptibilities. Following that, a literature review of the available intrusion detection solutions using ML models is presented. Finally, we discuss our case study, which includes details of a real-world testbed that we have built to conduct cyber-attacks and to design an intrusion detection system (IDS). We deploy backdoor, command injection, and Structured Query Language (SQL) injection attacks against the system and demonstrate how a ML-based anomaly detection system can perform well in detecting these attacks. We have evaluated the performance through representative metrics to have a fair point of view on the effectiveness of the methods.",https://ieeexplore.ieee.org/document/8693904/,IEEE Internet of Things Journal,Aug. 2019,ieeexplore
10.1109/TSE.2018.2844788,Machine Learning-Based Prototyping of Graphical User Interfaces for Mobile Apps,IEEE,Journals,"It is common practice for developers of user-facing software to transform a mock-up of a graphical user interface (GUI) into code. This process takes place both at an application's inception and in an evolutionary context as GUI changes keep pace with evolving features. Unfortunately, this practice is challenging and time-consuming. In this paper, we present an approach that automates this process by enabling accurate prototyping of GUIs via three tasks: detection, classification, and assembly. First, logical components of a GUI are detected from a mock-up artifact using either computer vision techniques or mock-up metadata. Then, software repository mining, automated dynamic analysis, and deep convolutional neural networks are utilized to accurately classify GUI-components into domain-specific types (e.g., toggle-button). Finally, a data-driven, K-nearest-neighbors algorithm generates a suitable hierarchical GUI structure from which a prototype application can be automatically assembled. We implemented this approach for Android in a system called ReDraw. Our evaluation illustrates that ReDraw achieves an average GUI-component classification accuracy of 91 percent and assembles prototype applications that closely mirror target mock-ups in terms of visual affinity while exhibiting reasonable code structure. Interviews with industrial practitioners illustrate ReDraw's potential to improve real development workflows.",https://ieeexplore.ieee.org/document/8374985/,IEEE Transactions on Software Engineering,1 Feb. 2020,ieeexplore
10.1109/TIA.2019.2940585,Missing Data Imputation With OLS-Based Autoencoder for Intelligent Manufacturing,IEEE,Journals,"Motivated by the global economy that is greatly shaped by the landscape changes in energy and manufacturing where more and more devices and systems are interconnected, intelligent manufacturing in which data mining is of great importance is studied. In this article, an energy monitoring platform for small- and medium-sized enterprises developed by the point energy team (www.pointenergy.org) is first introduced, which monitors and records the energy consumption of manufacturing processes at various levels of granularity. In processing the collected data, the incompleteness in the data due to various factors needs to be addressed first otherwise it may lead to the inaccurate portrayal of the system and poor generalization of the resultant model trained by the data. Hence, a novel orthogonal-least-square-based autoencoder is proposed to generate new samples for the imputation of missing values. This approach is to learn the representative code from the original samples by constructing an improved encoder network in which the hidden neurons are orthogonal with each other. The new samples are then generated through the decoder network. The proposed approach selects the hidden neurons one by one based on the OLS estimation until an adequate network is built. The classical techniques and other generative models are compared to verify the effectiveness of the proposed algorithm. For these methods, the optimal parameters are estimated based on the performance metric of the cross-validation mean square error. In the experiment, two real industrial datasets from a baking process and a polymer extrusion process are adopted and the percentage of missing values varies from 0.02 to 0.25. The experimental results confirm that the proposed method offers stable performance in the presence of different missing ratios, and it outperforms significantly alternative approaches while the missing ratio is greater than 0.05.",https://ieeexplore.ieee.org/document/8828079/,IEEE Transactions on Industry Applications,Nov.-Dec. 2019,ieeexplore
10.1109/TCST.2011.2159219,Modeling Component Concentrations of Sodium Aluminate Solution Via Hammerstein Recurrent Neural Networks,IEEE,Journals,"The component concentrations of sodium aluminate solution are important indices in alumina processing. At present, they are obtained by laboratory titration on samples taken from the production process. Due to the delays in taking and testing samples, they cannot be used for real-time control and optimization. Existing online measurements are not adopted because of the characteristics of the sodium aluminate solution such as high viscosity and the ease of precipitation which leads to pipeline blocking and decreased precision. In this paper, a new modeling method is proposed to measure the component concentrations online using the measurements of conductivity and temperature. The method combines the partial least squares (PLS) technique and the Hammerstein recurrent neural networks (HRNN), where a stable learning algorithm with theoretical analysis is given for the HRNN model. For this PLS-based HRNN, the PLS technique is used to solve the high dimensional and correlated data. Meanwhile, the HRNN technique is used to fit the nonlinear and dynamic characters of the process. An industrial experimental study on a sodium aluminate solution is described. The experiment results show that the proposed method is sufficient to warrant further evaluation in industrial scale experiments.",https://ieeexplore.ieee.org/document/5948397/,IEEE Transactions on Control Systems Technology,July 2012,ieeexplore
10.1109/ACCESS.2019.2916938,NASM: Nonlinearly Attentive Similarity Model for Recommendation System via Locally Attentive Embedding,IEEE,Journals,"Recommendation system, as a core service for many customer-oriented online services, is employed to predict the personalized rating of users on their potentially preferable items. In modern industrial settings, an item-based collaborative filtering (item-based CF) method has been long popular owing to its excellent interpretability and high efficiency in the real-time personalized recommendation. In this model, the current target item is recommended according to the interacted similarity from the user's profile, which implies that the key of item-based CF is in the estimation of historical item similarity. Early studies usually utilize statistical measures including cosine similarity and Pearson correlation coefficient to estimate similarity with low accuracy caused by lacking optimization tailed. Recently, there are some learning-based models attempting to learn item similarity by optimizing a recommendation-aware loss function. However, these efforts are mainly concentrated on the application of the shallow linear model, and relative works that deploy some deep learning components for item-based CF are scarce. In this paper, we propose a nonlinearly attentive similarity model (NASM) for item-based CF via locally attentive embedding by introducing local attention and novel nonlinear attention to capture local and global item information, simultaneously. The NASM is based on a neural attentive item similarity (NAIS) model and further achieves significantly superior performance. The experimental results demonstrate that the NASM achieves more competitive recommendation performance in terms of hit ration (HR) and the normalized discounted cumulative gain (NDGC) in comparison with other state-of-the-art recommendation models.",https://ieeexplore.ieee.org/document/8715403/,IEEE Access,2019,ieeexplore
10.1109/JPROC.2018.2856739,Navigating the Landscape for Real-Time Localization and Mapping for Robotics and Virtual and Augmented Reality,IEEE,Journals,"Visual understanding of 3-D environments in real time, at low power, is a huge computational challenge. Often referred to as simultaneous localization and mapping (SLAM), it is central to applications spanning domestic and industrial robotics, autonomous vehicles, and virtual and augmented reality. This paper describes the results of a major research effort to assemble the algorithms, architectures, tools, and systems software needed to enable delivery of SLAM, by supporting applications specialists in selecting and configuring the appropriate algorithm and the appropriate hardware, and compilation pathway, to meet their performance, accuracy, and energy consumption goals. The major contributions we present are: 1) tools and methodology for systematic quantitative evaluation of SLAM algorithms; 2) automated, machine-learning-guided exploration of the algorithmic and implementation design space with respect to multiple objectives; 3) end-to-end simulation tools to enable optimization of heterogeneous, accelerated architectures for the specific algorithmic requirements of the various SLAM algorithmic approaches; and 4) tools for delivering, where appropriate, accelerated, adaptive SLAM solutions in a managed, JIT-compiled, adaptive runtime context.",https://ieeexplore.ieee.org/document/8436423/,Proceedings of the IEEE,Nov. 2018,ieeexplore
10.1109/TII.2021.3050041,Network Traffic Prediction in Industrial Internet of Things Backbone Networks: A Multitask Learning Mechanism,IEEE,Journals,"Industrial Internet of Things (IIoT), as a common industrial application of Internet of Things, has been widely deployed in recent years. End-to-end network traffic is an essential information for many network security and management functions. This article investigates the issues of IIoT-oriented backbone network traffic prediction. Predicting the traffic of IIoT backbone networks is intractable because of the large number of prior network traffic information, which needs to consume expensive network resources for sampling. Motivated by that, we propose an effective prediction mechanism using multitask learning (MTL), which is a special paradigm of transfer learning. A deep learning architecture constructed by MTL and long short-term memory is designed. This deep architecture takes advantage of link loads as additional information to improve prediction accuracy. We provide a theoretical analysis for the MTL mechanism. The effectiveness is evaluated by implementing our mechanism on real network.",https://ieeexplore.ieee.org/document/9316934/,IEEE Transactions on Industrial Informatics,Oct. 2021,ieeexplore
10.1109/TII.2019.2953275,Neural Network-Based Model Predictive Control of a Paste Thickener Over an Industrial Internet Platform,IEEE,Journals,"This article presents a real implementation of a neural network-based model predictive control scheme (NNMPC) to control an industrial paste thickener. The implementation is done over an Industrial Internet of Things (IIoT) platform designed using the seven layer reference model for IIoT systems. Modeling is achieved using an encoder-decoder with attention recurrent neural network, while MPC search is done using particle swarm optimization. An industrial evaluation is presented, which highlights the set-point tracking and disturbance rejection capabilities of the proposed NNMPC technique.",https://ieeexplore.ieee.org/document/8897590/,IEEE Transactions on Industrial Informatics,April 2020,ieeexplore
10.1109/TIE.2013.2266086,Nonlinear Model-Predictive Control for Industrial Processes: An Application to Wastewater Treatment Process,IEEE,Journals,"Because of their complex behavior, wastewater treatment processes (WWTPs) are very difficult to control. In this paper, the design and implementation of a nonlinear model-predictive control (NMPC) system are discussed. The proposed NMPC comprises a self-organizing radial basis function neural network (SORBFNN) identifier and a multiobjective optimization method. The SORBFNN with concurrent structure and parameter learning is developed as a model identifier for approximating the online states of dynamic systems. Then, the solution of the multiobjective optimization is obtained by a gradient method which can shorten the solution time of optimal control problems. Moreover, the conditions for the stability analysis of NMPC are presented. Experiments reveal that the proposed control technique gives satisfactory tracking and disturbance rejection performance for WWTPs. Experimental results on a real WWTP show the efficacy of the proposed NMPC for industrial processes in many applications.",https://ieeexplore.ieee.org/document/6523075/,IEEE Transactions on Industrial Electronics,April 2014,ieeexplore
10.1109/TCYB.2017.2761841,Off-Policy Reinforcement Learning: Optimal Operational Control for Two-Time-Scale Industrial Processes,IEEE,Journals,"Industrial flow lines are composed of unit processes operating on a fast time scale and performance measurements known as operational indices measured at a slower time scale. This paper presents a model-free optimal solution to a class of two time-scale industrial processes using off-policy reinforcement learning (RL). First, the lower-layer unit process control loop with a fast sampling period and the upper-layer operational index dynamics at a slow time scale are modeled. Second, a general optimal operational control problem is formulated to optimally prescribe the set-points for the unit industrial process. Then, a zero-sum game off-policy RL algorithm is developed to find the optimal set-points by using data measured in real-time. Finally, a simulation experiment is employed for an industrial flotation process to show the effectiveness of the proposed method.",https://ieeexplore.ieee.org/document/8100717/,IEEE Transactions on Cybernetics,Dec. 2017,ieeexplore
10.1109/ACCESS.2019.2958284,On the Generation of Anomaly Detection Datasets in Industrial Control Systems,IEEE,Journals,"In recent decades, Industrial Control Systems (ICS) have been affected by heterogeneous cyberattacks that have a huge impact on the physical world and the people's safety. Nowadays, the techniques achieving the best performance in the detection of cyber anomalies are based on Machine Learning and, more recently, Deep Learning. Due to the incipient stage of cybersecurity research in ICS, the availability of datasets enabling the evaluation of anomaly detection techniques is insufficient. In this paper, we propose a methodology to generate reliable anomaly detection datasets in ICS that consists of four steps: attacks selection, attacks deployment, traffic capture and features computation. The proposed methodology has been used to generate the Electra Dataset, whose main goal is the evaluation of cybersecurity techniques in an electric traction substation used in the railway industry. Using the Electra dataset, we train several Machine Learning and Deep Learning models to detect anomalies in ICS and the performed experiments show that the models have high precision and, therefore, demonstrate the suitability of our dataset for use in production systems.",https://ieeexplore.ieee.org/document/8926471/,IEEE Access,2019,ieeexplore
10.1109/JIOT.2020.2994200,Optimization of Edge-PLC-Based Fault Diagnosis With Random Forest in Industrial Internet of Things,IEEE,Journals,"Facing globalized competition, there have been increasing requirements for safety and efficiency in smart factories, where the industrial Internet of Things can enable the monitoring of equipment's status and the detecting of faults before they go critical. Regarding cloud computing, data-driven methods running at clouds are adopted to train the model with a large amount of raw data at the beginning, then end machines upload their real-time readings to the cloud center for processing. However, this incurs considerable computational costs and may sometimes bear a severe delay. In this article, we consider a hierarchical structure where edge-PLCs are employed to gather sensed data locally and reduce communication costs. Since a single fault may be related to multiple influencing features, we want to first minimize the number of features that need to determine a fault, then try to find out the minimal set of edge-PLCs which can cover all key features so as to save the deployment cost. We propose a random-forest-based method to handle the features selection problem, and then the selection of edge-PLCs by solving the set coverage problem. Through the simulation on real data trace, we compare our method with other artificial-intelligence-based methods, such as the logistics regression model and its extensions. The results prove the efficiency and performance of the proposed method, which reaches or even exceeds the accuracy of methods using the full set of data.",https://ieeexplore.ieee.org/document/9091605/,IEEE Internet of Things Journal,Oct. 2020,ieeexplore
10.1109/TMECH.2014.2366033,Output Regulation of Nonlinear Systems With Application to Roll-to-Roll Manufacturing Systems,IEEE,Journals,"This paper deals with the problem of synthesizing feedforward control to aid the regulation of output of a nonlinear system in the presence of partially known exogenous inputs. The problem appears in many engineering applications including Roll-to-Roll (R2R) manufacturing systems. Currently known methods for this problem either require the solution of a constrained partial differential equation or the preview information of the signal to be tracked. The novelty of this paper lies in synthesizing feedforward control as the solution of a system of differential-algebraic equations, which is considerably less complex and suitable for practical implementation. In this paper, we consider the problem of regulating the output while rejecting the disturbances and apply it to R2R manufacturing systems. The problem of tracking reference signals can also be handled with the suggested technique. We assume that the disturbance signal is the output of a known exogenous system with unknown initial conditions. A parameter identification scheme to estimate the unknown initial conditions is developed. The proposed technique is successfully applied to control of web tension in a large R2R machine which mimics most of the features of industrial R2R machines and contains real-time hardware and software that is used in industrial practice. Extensive experiments were conducted to evaluate the proposed scheme for web tension control under various experimental conditions, including different web speeds and materials. We will present and discuss the representative experimental results with the proposed technique and provide a comparison with an industrial PI control scheme to highlight the benefits of using the proposed scheme.",https://ieeexplore.ieee.org/document/6963413/,IEEE/ASME Transactions on Mechatronics,June 2015,ieeexplore
10.1109/TIE.2016.2612160,PLC-Based Real-Time Realization of Flatness-Based Feedforward Control for Industrial Compression Systems,IEEE,Journals,"In this paper, we present a novel programmable logic controller (PLC)-based real-time realization of a flatness-based feedforward control (FFC) scheme. The proposed approach is applied to an industrial fuel-gas compression system which is used to supply fuel gas to the gas turbines in combined cycle power plants. Due to the increasing demand for fast operation point transitions with high performance and accuracy requirements, the currently applied decentralized proportional-integral-derivative controllers appear to be not appropriate any more. Hence, by means of system simulations, a new flatness-based FFC design has been shown to provide improved control performance. In this paper, we bridge the gap between simulation-based control design and practical applicability, in that, we present the real-time realization of the approach on a PLC. Furthermore, the PLC-based controller is tested on a hardware-in-the-loop platform running with a complex compression system model in real time. The results reveal that the flatness-based control design can be implemented on a real compressor system.",https://ieeexplore.ieee.org/document/7572893/,IEEE Transactions on Industrial Electronics,Feb. 2017,ieeexplore
10.1109/TII.2019.2940099,Performance Supervised Fault Detection Schemes for Industrial Feedback Control Systems and their Data-Driven Implementation,IEEE,Journals,"This article addresses performance supervised fault detection (PSFD) issues for industrial feedback control systems based on performance degradation prediction. To be specific, three performance indicators are first introduced based on Bellman equation to predict system performance degradations for industrial processes with the aid of machine learning techniques. Based on them, three PSFD schemes are proposed by embedding the performance indicators as supervising information. In this context, the data-driven implementation of PSFD schemes are investigated for linear systems with unmeasurable state variables. A case study on rolling mill process, a typical benchmark in the steel manufacturing processes, is given at the end of this article to illustrate the applications of the proposed fault detection schemes.",https://ieeexplore.ieee.org/document/8827307/,IEEE Transactions on Industrial Informatics,April 2020,ieeexplore
10.1109/TIM.2021.3092518,Pipeline Safety Early Warning by Multifeature-Fusion CNN and LightGBM Analysis of Signals From Distributed Optical Fiber Sensors,IEEE,Journals,"Energy pipelines are the backbones of global energy systems. Monitoring their safety and automatically identifying and locating third-party damage events are crucial to energy supply. However, most traditional methods lack in-depth consideration of distributed fiber signals and have not been tested on real-world long-distance pipelines, making it difficult to deploy them in operating long-distance pipelines. In this study, we utilize a novel real-time machine-learning method based on phase-sensitive optical time domain reflectometer technology to monitor the safety of oil and gas pipelines. Specifically, we build a multifeature-fusion convolutional neural network and LightGBM fusion model based on two novel complementary spatiotemporal features. The method was applied to a large amount of data collected from real-world oil–gas transportation pipelines of the China National Petroleum Corporation. The proposed method could accurately locate and identify third-party damage events in real-time under conditions of strong noise and various types of system hardware, and could effectively handle signal drift in the time and space dimensions. Our methodology has been deployed at real long-distance energy pipeline sites and our work will contribute to energy pipeline safety and energy supply security. Furthermore, the proposed solution could be generalized to other fields, such as industrial inspection, measurement, and monitoring.",https://ieeexplore.ieee.org/document/9541184/,IEEE Transactions on Instrumentation and Measurement,2021,ieeexplore
10.1109/TII.2015.2426012,PolyNet: A Polynomial-Based Learning Machine for Universal Approximation,IEEE,Journals,"Currently, there is a need in all disciplines for efficient and powerful machine learning (ML) algorithms for handling offline and real-time nonlinear data. Industrial applications abound from real-time control systems to modeling and simulation of complex systems and processes. Certain ML methods have become popular with researchers and engineers. Such techniques include fuzzy systems (FSs), artificial neural networks (ANNs), radial basis function (RBF) networks, and support vector regression (SVR) machines. Historically, polynomial-based learning machines (PLMs) based on the group method of data handling (GMDH) model have enjoyed usage similar to that of these other methods. However, unwieldy kernel functions in the form of large high-order polynomials, and relatively limited computer speed and capacity, have limited the use of PLMs to comparatively small problems with low dimensionality and simple functional relationships. Thus, true polynomial-based ML solutions have drifted out of vogue for at least two decades. This work attempts to reinvigorate the interest in PLMs by introducing a novel practical implementation called PolyNet. It will be shown that once certain algorithms are applied to the generation, training, and functional operation of PLMs, they can compete on par with or better than methods currently in use.",https://ieeexplore.ieee.org/document/7095595/,IEEE Transactions on Industrial Informatics,June 2015,ieeexplore
10.1109/TASE.2019.2909043,RFID-Driven Energy-Efficient Control Approach of CNC Machine Tools Using Deep Belief Networks,IEEE,Journals,"Under the consideration of massive energy consumption of machine tools, many approaches have been proposed, and state control method of machine tools has proved its effectiveness. In order to satisfy the demand of real-time production control, a deep learning methodology for energy-efficient control of CNC machine tools is proposed in RFID-enabled ubiquitous environment. First, the energy-efficient control strategies for multiple machine tools are proposed to reduce the carbon emission of the machining process. Then, through evaluating the process progress in the RFID-enabled environment, a deep learning methodology for energy-efficient strategies selection of CNC machine tools using deep belief networks (DBNs) is established to realize the real-time and accurate control of machine tools. Finally, comparisons between the proposed approach and some state-of-the-art ones are given, and the experiment results indicate that the proposed method is effective and efficient for the energy-efficient control problem of machine tools. The proposed method can realize the real-time control of CNC machine tools based on the interaction information in Industrial 4.0. Furthermore, the machine tools will be converted to smart machines, which can complete self-perception and self-adjustment automatically.",https://ieeexplore.ieee.org/document/8694932/,IEEE Transactions on Automation Science and Engineering,Jan. 2020,ieeexplore
10.1109/81.747195,Reaction-diffusion CNN algorithms to generate and control artificial locomotion,IEEE,Journals,"In this paper a physiological-behavioral approach to neural processing is used to realize artificial locomotion in mechatronic devices. The task has been realized by using a particular model of reaction-diffusion cellular neural networks (RD-CNN's) generating autowave fronts as well as Turing patterns. Moreover a programmable hardware cellular neural network structure is presented in order to model, generate, and control in real time some biorobots. The programmable hardware implementation gives the possibility of generating locomotion in real time and also to control the transition among several types of locomotion, with particular attention to hexapodes. The approach proposed allows not only the design of walking robots, but also the ability to build structures able to efficiently solve typical problems in industrial automation, such as online routing of objects moved on conveyor belts.",https://ieeexplore.ieee.org/document/747195/,IEEE Transactions on Circuits and Systems I: Fundamental Theory and Applications,Feb. 1999,ieeexplore
10.1109/ACCESS.2021.3082641,Real-Time Facial Expression Recognition Based on Edge Computing,IEEE,Journals,"In recent years, many large-scale information systems in the Internet of Things (IoT) can be converted into interdependent sensor networks, such as smart cities, smart medical systems, and industrial Internet systems. The successful application of edge computing in the IoT will make our algorithms faster, more convenient, lower overall costs, providing better business practices, and enhance sustainability. Facial action unit (AU) detection recognizes facial expressions by analyzing cues about the movement of certain atomic muscles in the local facial area. According to the detected facial feature points, we could calculate the values of AU, and then use classification algorithms for emotion recognition. In edge devices, using optimized and custom algorithms to directly process the raw image data from each camera, the detected emotions can be more easily transmitted to the end-user. Due to the tremendous network overhead of transferring the facial action unit feature data, it poses challenges of a real-time facial expression recognition system being deployed in a distributed manner while running in production. Therefore, we designed a lightweight edge computing-based distributed system using Raspberry Pi tailed for this need, and we optimized the data transfer and components deployment. In the vicinity, the front-end and back-end processing modes are separated to reduce round-trip delay, thereby completing complex computing tasks and providing high-reliability, large-scale connection services. For IoT or smart city applications and services, they can be made into smart sensing systems that can be deployed anywhere with network connections.",https://ieeexplore.ieee.org/document/9438728/,IEEE Access,2021,ieeexplore
10.1109/TNS.2021.3090670,Real-Time Implementation of the Neutron/Gamma Discrimination in an FPGA-Based DAQ MTCA Platform Using a Convolutional Neural Network,IEEE,Journals,"These days, research on the classification of neutron/gamma waveforms in scintillators using pulse shape discrimination (PSD) techniques is a highly studied topic. Numerous methods have been explored to optimize this classification, with some of the most recent research being focused on machine learning techniques with excellent results. These approaches are mainly based on the use of 1-D convolutional neural networks (CNNs). In this field, field-programmable gate arrays (FPGAs) with high-sampling rate analog to digital converters (ADCs) have been used to perform this classification in real-time. In this work, we select a potential architecture and implement it with the help of the IntelFPGA OpenCL SDK environment. A shorter and C-like development of OpenCL enables a more straightforward modification and optimization of the network architecture. The main goal of this work is the evaluation of the needed resources and the obtained performance to prototype a complete solution in the FPGA. The FPGA design is generated as if it was connected to an ADC module streaming the data samples with the help of a Board Support Package developed for an IntelFPGA ARRIA10 available in an Advanced Mezzanine Card (AMC) module in an Micro Telecommunications Computing Architecture (MTCA.4) platform. The prototyped solution has been integrated into Experimental Physics and Industrial Control System (EPICS) using the nominal device support (NDS) model currently being developed by ITER.",https://ieeexplore.ieee.org/document/9459756/,IEEE Transactions on Nuclear Science,Aug. 2021,ieeexplore
10.1109/83.791960,Real-time DSP implementation for MRF-based video motion detection,IEEE,Journals,"This paper describes the real time implementation of a simple and robust motion detection algorithm based on Markov random field (MRF) modeling, MRF-based algorithms often require a significant amount of computations. The intrinsic parallel property of MRF modeling has led most of implementations toward parallel machines and neural networks, but none of these approaches offers an efficient solution for real-world (i.e., industrial) applications. Here, an alternative implementation for the problem at hand is presented yielding a complete, efficient and autonomous real-time system for motion detection. This system is based on a hybrid architecture, associating pipeline modules with one asynchronous module to perform the whole process, from video acquisition to moving object masks visualization. A board prototype is presented and a processing rate of 15 images/s is achieved, showing the validity of the approach.",https://ieeexplore.ieee.org/document/791960/,IEEE Transactions on Image Processing,Oct. 1999,ieeexplore
10.1109/JSEN.2021.3075535,Recent Advancements in the Development of Sensors for the Structural Health Monitoring (SHM) at High-Temperature Environment: A Review,IEEE,Journals,"With Industry 4.0 becoming increasingly pervasive, the importance and usage of sensors has increased several folds. Industry 4.0 refers to a new phase in the industrial revolution that mainly focuses on interconnectivity, automation, machine learning, and real-time data. Real-time structural health monitoring (SHM) of components in the industrial process is one of the crucial and important component of Industry 4.0. SHM of components exposed to high-temperature (<inline-formula> <tex-math notation=""LaTeX"">$\sim 650^{\circ }\text{C}$ </tex-math></inline-formula>) is becoming increasingly important nowadays. However, harsh and high temperature environments impose a great challenge towards their implementation. This review is an attempt to demonstrate the development, application, limitations and recent advancement of the existing sensors used for SHM. Some sensors such as eddy current (EC) sensors and fiber Bragg grating (FBG) sensors have been discussed in detail. A phenomenological study of the electromagnetic sensor for the SHM of engineering components that are exposed to high temperature has been addressed. State-of-the-art fabrication methodologies such as low temperature co-fired ceramic (LTCC) technology for such type of sensors for high temperature SHM applications have been elucidated. Future challenges and opportunities for SHM applications of high temperature sensors have been highlighted.",https://ieeexplore.ieee.org/document/9415648/,IEEE Sensors Journal,"15 July15, 2021",ieeexplore
10.1109/TII.2017.2752709,Recursive Total Principle Component Regression Based Fault Detection and Its Application to Vehicular Cyber-Physical Systems,IEEE,Journals,"The cyber-physical systems (CPSs) are the central research topic in the era of Industrial 4.0. Such systems interact intensively between physical entities and abstract information, and commonly exist in the industrial processes and people's daily lives. This paper investigates the practical difficulties of the vehicular CPSs online implementation, and based on that proposes a fault diagnosis and control architecture with modular units and reserved extendibility. It is elaborated that the systems' adaptability could be enhanced by either the online tracking techniques or the ensemble learning schemes. For the onboard deployment of automobile CPSs, the requirement of real-time capacity is in focus. A new recursive total principle component regression based design and implementation approach is proposed for efficient data-driven fault detection. Simulation tests were carried out on the Carsim to compare the proposed approach with multiple existing methods.",https://ieeexplore.ieee.org/document/8038833/,IEEE Transactions on Industrial Informatics,April 2018,ieeexplore
10.1109/JIOT.2020.3028325,Reinforcement-Learning-Enabled Partial Confident Information Coverage for IoT-Based Bridge Structural Health Monitoring,IEEE,Journals,"Internet-of-Things (IoT)-based bridge structural health monitoring (BSHM) has recently attracted considerable attention from both academic and industrial communities of civil engineering and computer science. In conjunction with researchers from civil engineering and computer science, this article studied a fundamental problem motivated from practical IoT-based BSHM: how to effectively prolong network lifetime while guaranteeing desired coverage. Integrating a promising reinforcement learning model named learning automata (LA) with confident information coverage (CIC) model, this article presented an energy-efficient sensor scheduling strategy for partial CIC coverage in IoT-based BSHM system to guarantee network coverage and prolong network lifetime. The proposed scheme fully exploits cooperation among deployed nodes and alternatively schedules the wake/sleep status of nodes while satisfying network connectivity and partial coverage ratio. Especially, the proposed scheme takes full advantage of the LA model to adaptively learn the optimal sensor scheduling strategy and significantly extend network lifetime. A series of comparison simulations using real data sets collected by a practical BSHM system strongly verify the effectiveness and energy efficiency of the proposed algorithm. To the best of our knowledge, this is the first study on how to combine the reinforcement learning mechanism with partial coverage for maximizing the network lifetime of the IoT-based BSHM.",https://ieeexplore.ieee.org/document/9211720/,IEEE Internet of Things Journal,"1 March1, 2021",ieeexplore
10.1109/ACCESS.2020.3043817,Research on Double Edge Detection Method of Midsole Based on Improved Otsu Method,IEEE,Journals,"The midsole is an important part of the shoe, but in its industrial production, the surface quality of the midsole currently relies on manual testing. It cost high and cannot meet the needs of industrial online real-time detection. To realize online surface defect detection, extracting the double edges of the midsole resulting from its special structure becomes an indispensable pre-work. This paper proposes a two-step Otsu method (TT-Otsu) to extract double edges of products. This method adopts the improved Otsu method to process the midsole image in two steps. It respectively combines with the Weighted Object Variance method (WOV) and the Neighborhood Valley-Emphasis method (NVE) to calculate the optimal threshold. Then the image is segmented to extract the edges and the misclassification error (ME) is 0.0007. To ensure the accuracy of the edge detection, the neighborhood gradient extreme value discrimination method is used to realize the local self-checking and make appropriate adjustments to the deviated edge. The false positive rate (FPR) and the false negative rate (FNR) of TT-Otsu are approximately equal to 5%. This method can effectively and clearly extract the double edges of the midsole. The precision rate is 95.61%, the average running time is 1.8s. The experiment demonstrates that the proposed method in this paper has good detection performance and good applicability.",https://ieeexplore.ieee.org/document/9290011/,IEEE Access,2020,ieeexplore
10.1109/ACCESS.2021.3085338,Research on a Product Quality Monitoring Method Based on Multi Scale PP-YOLO,IEEE,Journals,"To monitor product quality in the production process in real time, this thesis proposes a quality monitoring model based on PaddlePaddle You Only Look Once (PP-YOLO). First, in the preprocessing stage, the data enhancement method and the K-means++ method are used to improve the robustness of the algorithm, and the generated anchor box can screen more refined features earlier. Second, ResNet50-vd with the deformable convolution idea is selected as the backbone of the detection model, the feature pyramid network structure and the composition of the loss function are improved, and the feature learning ability of the model is enhanced to enable it to detect multiple scales of defects. Finally, pruning is performed on the basis of the trained model to reduce the number of model parameters so that it can be deployed in industrial scenarios with limited hardware conditions. Experimental results show that the proposed quality monitoring model can meet the requirements for detection speed and accuracy in actual production, providing a new concept for the deployment of deep learning models in the industrial field.",https://ieeexplore.ieee.org/document/9445109/,IEEE Access,2021,ieeexplore
10.1109/ACCESS.2020.2980162,Research on the Feature Selection Approach Based on IFDS and DPSO With Variable Thresholds in Complex Data Environments,IEEE,Journals,"Neighborhood rough model is widely used in feature selection with high dimension, fuzzy, continuous and discrete attributes, incomplete data and so on, and the application of neighborhood rough model depends on neighborhood threshold. In the application of the model, the point-value neighborhood threshold is not adaptive, which leads to low classification accuracy and high time complexity of the algorithm. In order to solve the above problems, a feature selection approach based on IFDS (Incomplete Fuzzy Hybrid Decision System) and DPSO (Discrete Particle Swarm Optimization Algorithm) with variable thresholds is proposed. Firstly, a neighborhood rough model capable of simultaneously processing fuzzy, hybrid and incomplete data was established. The average reachable distance was introduced to construct the attribute neighborhood threshold set and reduce the interference of noise data on classification accuracy. Secondly, we constructed the DPSO particle fitness function using the feature subset length, the significance of the attribute and the negative domain of the neighborhood, and improved the inertia weight computing method, so as to enhance the feature selection speed and the feature subset quality. Finally, the simulation experiment was performed using the real industrial production data. The experiment effect shows that this method has obvious advantages in improving the classification accuracy, optimizing the search speed and the optimal feature subset quality.",https://ieeexplore.ieee.org/document/9032173/,IEEE Access,2020,ieeexplore
10.1109/JSAC.2019.2951932,Residential Customer Baseline Load Estimation Using Stacked Autoencoder With Pseudo-Load Selection,IEEE,Journals,"Accurate estimation of customer baseline load (CBL) is a key factor in the successful implementation of demand response (DR). CBL technologies implemented at utilities currently are primarily designed for large industrial and commercial customers. The U.S. Federal Energy Regulatory Commission (FERC) order 745 states that DR owners, including residential customers, can sell their load reduction in the wholesale market. However, since residential load is random and un-schedulable, this tends to inherently degrade the effectiveness of existing CBL technologies. In this paper, a novel SAE based CBL method for residential customers that uses the data reconstruction capability of a stacked autoencoder (SAE) is described. In the model, two SAEs are synchronously trained-one SAE generates a pseudo-load pool and the second one is used to select a pseudo-load to reconstruct a residential CBL. A support vector machine (SVM) classifier is self-trained to conduct the pseudo-load selection. The proposed strategy is validated using a real data set consisting of 328 residential customers' smart meter readings. Benchmarks from other machine learning techniques and existing CBL methods are compared with the proposed method. Test results show that the accuracy of the residential CBL reconstruction significantly improves when compared with existing methods, such as HighXofY and exponential moving average.",https://ieeexplore.ieee.org/document/8892548/,IEEE Journal on Selected Areas in Communications,Jan. 2020,ieeexplore
10.1109/TII.2019.2903224,Risk-Based Scheduling of Security Tasks in Industrial Control Systems With Consideration of Safety,IEEE,Journals,"Industrial control systems (ICSs) in networked environments face severe cyber-security risks and challenges. A timely response to cyber-attacks is of paramount importance for mitigating risks. However, the security policy developed for an ICS may be conflicting with the ICS's safety policy, on which much attention has been paid for a long time in industrial control. An inappropriate enforcement of the security policy may deteriorate the ICS performance or even result in severe unexpected consequences. To tackle this problem, a risk-based security task scheduling approach is presented for ICSs with consideration of the safety policy. It ensures a timely response to cyber-attacks without compromising safety. More specifically, the approach reconciles security tasks and safety tasks according to a designed resolution policy, so as to acquire contradiction-free security and safety (S&amp;S) tasks. Then, a real-time risk assessment method is developed to characterize the subtle change of the system risk with the implementation of the reconciled S&amp;S tasks. After that, a task scheduling method is designed with the risk as the optimization objective, i.e., it searches the optimal task scheduling scheme by minimizing the risk posture. The resulting scheduling scheme ensures the smooth implementation of the S&amp;S policy, which reflects the optimal recovery process against the risk. Finally, case studies on a hardware-in-the-loop testbed are conducted to demonstrate the effectiveness of the proposed approach.",https://ieeexplore.ieee.org/document/8661651/,IEEE Transactions on Industrial Informatics,May 2020,ieeexplore
10.1109/ACCESS.2020.3045563,SDN Enhanced Resource Orchestration of Containerized Edge Applications for Industrial IoT,IEEE,Journals,"With the rise of the Industrial Internet of Things (IIoT), there is an intense pressure on resource and performance optimization leveraging on existing technologies, such as Software Defined Networking (SDN), edge computing, and container orchestration. Industry 4.0 emphasizes the importance of lean and efficient operations for sustainable manufacturing. Achieving this goal would require engineers to consider all layers of the system, from hardware to software, and optimizing for resource efficiency at all levels. This emphasizes the need for container-based virtualization tools such as Docker and Kubernetes, offering Platform as a Service (PaaS), while simultaneously leveraging on edge technologies to reduce related latencies. For network management, SDN is poised to offer a cost-effective and dynamic scalability solution by customizing packet handling for various edge applications and services. In this paper, we investigate the energy and latency trade-offs involved in combining these technologies for industrial applications. As a use case, we emulate a 3D-drone-based monitoring system aimed at providing real-time visual monitoring of industrial automation. We compare a native implementation to a containerized implementation where video processing is orchestrated while streaming is handled by an external UE representing the IIoT device. We compare these two scenarios for energy utilization, latency, and responsiveness. Our test results show that only roughly 16 percent of the total power consumption happens on the mobile node when orchestrated. Virtualization adds up about 4.5 percent of the total power consumption while the latency difference between the two approaches becomes negligible after the streaming session is initialized.",https://ieeexplore.ieee.org/document/9296769/,IEEE Access,2020,ieeexplore
10.1109/ACCESS.2021.3102157,Scalable and Multifaceted Search and Its Application for Binary Malware Files,IEEE,Journals,"Malicious binary files are a serious threat to industrial information systems. Because of their large number, an automatic assistant tool becomes essential for analysis, and finding similar files would be a great help. In this paper, we present a fast, scalable, and multifaceted search scheme to find similar binary malware files. We use a content-defined chunking algorithm to convert a file into a feature set for the first time. The proposed scheme uses MinHash to reduce any feature set of any file to a fixed size, which significantly improves search accuracy, processing speed, and space utilization. We theoretically prove that the new scheme returns similar files in jaccard index order. Through implementation and experiments with 12 million malicious files, we confirm that the search speed is increased by 600%, space is reduced by 90%, and the accuracy is increased by 400% at least, compared with the state-of-the-art of Elasticsearch.",https://ieeexplore.ieee.org/document/9504570/,IEEE Access,2021,ieeexplore
10.1109/TVCG.2020.2969007,Security in Process: Visually Supported Triage Analysis in Industrial Process Data,IEEE,Journals,"Operation technology networks, i.e. hard- and software used for monitoring and controlling physical/industrial processes, have been considered immune to cyber attacks for a long time. A recent increase of attacks in these networks proves this assumption wrong. Several technical constraints lead to approaches to detect attacks on industrial processes using available sensor data. This setting differs fundamentally from anomaly detection in IT-network traffic and requires new visualization approaches adapted to the common periodical behavior in OT-network data. We present a tailored visualization system that utilizes inherent features of measurements from industrial processes to full capacity to provide insight into the data and support triage analysis by laymen and experts. The novel combination of spiral plots with results from anomaly detection was implemented in an interactive system. The capabilities of our system are demonstrated using sensor and actuator data from a real-world water treatment process with introduced attacks. Exemplary analysis strategies are presented. Finally, we evaluate effectiveness and usability of our system and perform an expert evaluation.",https://ieeexplore.ieee.org/document/8968740/,IEEE Transactions on Visualization and Computer Graphics,1 April 2020,ieeexplore
10.1109/ACCESS.2020.3020799,Short-Term Industrial Load Forecasting Based on Ensemble Hidden Markov Model,IEEE,Journals,"Short-term load forecasting (STLF) for industrial customers has been an essential task to reduce the cost of energy transaction and promote the stable operation of smart grid throughout the development of the modern power system. Traditional STLF methods commonly focus on establishing the non-linear relationship between loads and features, but ignore the temporal relationship between them. In this paper, an STLF method based on ensemble hidden Markov model (e-HMM) is proposed to track and learn the dynamic characteristics of industrial customer's consumption patterns in correlated multivariate time series, thereby improving the prediction accuracy. Specifically, a novel similarity measurement strategy of log-likelihood space is designed to calculate the log-likelihood value of the multivariate time series in sliding time windows, which can effectively help the hidden Markov model (HMM) to capture the dynamic temporal characteristics from multiple historical sequences in similar patterns, so that the prediction accuracy is greatly improved. In order to improve the generalization ability and stability of a single HMM, we further adopt the framework of Bagging ensemble learning algorithm to reduce the prediction errors of a single model. The experimental study is implemented on a real dataset from a company in Hunan Province, China. We test the model in different forecasting periods. The results of multiple experiments and comparison with several state-of-the-art models show that the proposed approach has higher prediction accuracy.",https://ieeexplore.ieee.org/document/9183956/,IEEE Access,2020,ieeexplore
10.1109/TII.2020.3047675,Siamese Neural Network Based Few-Shot Learning for Anomaly Detection in Industrial Cyber-Physical Systems,IEEE,Journals,"With the increasing population of Industry 4.0, both AI and smart techniques have been applied and become hotly discussed topics in industrial cyber-physical systems (CPS). Intelligent anomaly detection for identifying cyber-physical attacks to guarantee the work efficiency and safety is still a challenging issue, especially when dealing with few labeled data for cyber-physical security protection. In this article, we propose a few-shot learning model with Siamese convolutional neural network (FSL-SCNN), to alleviate the over-fitting issue and enhance the accuracy for intelligent anomaly detection in industrial CPS. A Siamese CNN encoding network is constructed to measure distances of input samples based on their optimized feature representations. A robust cost function design including three specific losses is then proposed to enhance the efficiency of training process. An intelligent anomaly detection algorithm is developed finally. Experiment results based on a fully labeled public dataset and a few labeled dataset demonstrate that our proposed FSL-SCNN can significantly improve false alarm rate (FAR) and F1 scores when detecting intrusion signals for industrial CPS security protection.",https://ieeexplore.ieee.org/document/9311786/,IEEE Transactions on Industrial Informatics,Aug. 2021,ieeexplore
10.23919/JCN.2020.100039,Special issue on 6G wireless systems,KICS,Journals,"While 5G is currently being deployed around the globe, research on 6G is under way aiming at addressing the coming challenges of drastic increase of wireless data traffic and support of other usage scenarios. 6G is expected to extend 5G capabilities even further. Higher bitrates (up to Tbps) and lower latency (less than 1ms) will allow introducing new services — such as pervasive edge intelligence, ultra-massive machine-type communications, extremely reliable low-latency communications, holographic rendering and high-precision communications — and meet more stringent requirements, especially in the following dimensions: energy efficiency; intelligence; spectral efficiency; security, secrecy and privacy; affordability; and customization. Artificial intelligence approaches and techniques, such as machine learning (of which deep learning and reinforcement learning are specific examples), and machine reasoning (which includes planning, scheduling, knowledge representation and reasoning, search and optimization), are the new fundamental enablers to operate networks more efficiently, enhance the overall end user experience and provide innovative service applications. Quantum Optics Computing (QOC) and Quantum Key Distribution (QKD) are almost ready for industrial applications. In particular, massive Internet of Things (mIoT), Industrial IoT (IloT), fully automated robotic platforms (which include control, perception, sensors and actuators, as well as the integration of other techniques into cyber-physical systems), vehicles and multisensory extended reality are examples of the new data-demanding applications, which will impose new performance targets and motivate 6G design and deployment.",https://ieeexplore.ieee.org/document/9321190/,Journal of Communications and Networks,Dec. 2020,ieeexplore
10.1109/TIA.2011.2125710,Speed and Flux Control of Induction Motors Using Emotional Intelligent Controller,IEEE,Journals,"This paper presents a real-time implementation of an improved emotional controller for induction motor (IM) drives. The proposed controller is called brain-emotional-learning-based intelligent controller. The utilization of the new controller is based on the emotion-processing mechanism in the brain and is essentially an action selection, which is based on sensory inputs and emotional cues. This intelligent control is based on the limbic system of the mammalian brain. The controller is successfully implemented in real time using a PC-based three-phase 2.5-kW laboratory squirrel-cage IM. In this paper, a novel but simple model of the IM drive system is achieved by using the intelligent controller, which simultaneously controls the motor flux and speed. This emotional intelligent controller has a simple computational structure with high auto learning features. The proposed emotional controller has been experimentally implemented in a laboratory IM drive, and it shows good promise for niche industrial-scale utilization.",https://ieeexplore.ieee.org/document/5728907/,IEEE Transactions on Industry Applications,May-June 2011,ieeexplore
10.1109/TPDS.2021.3104255,Taskflow: A Lightweight Parallel and Heterogeneous Task Graph Computing System,IEEE,Journals,"Taskflow aims to streamline the building of parallel and heterogeneous applications using a lightweight task graph-based approach. Taskflow introduces an expressive task graph programming model to assist developers in the implementation of parallel and heterogeneous decomposition strategies on a heterogeneous computing platform. Our programming model distinguishes itself as a very general class of task graph parallelism with in-graph control flow to enable end-to-end parallel optimization. To support our model with high performance, we design an efficient system runtime that solves many of the new scheduling challenges arising out of our models and optimizes the performance across latency, energy efficiency, and throughput. We have demonstrated the promising performance of Taskflow in real-world applications. As an example, Taskflow solves a large-scale machine learning workload up to 29% faster, 1.5× less memory, and 1.9× higher throughput than the industrial system, oneTBB, on a machine of 40 CPUs and 4 GPUs. We have opened the source of Taskflow and deployed it to large numbers of users in the open-source community.",https://ieeexplore.ieee.org/document/9511796/,IEEE Transactions on Parallel and Distributed Systems,1 June 2022,ieeexplore
10.1109/JIOT.2019.2963635,Toward Edge-Based Deep Learning in Industrial Internet of Things,IEEE,Journals,"As a typical application of the Internet of Things (IoT), the Industrial IoT (IIoT) connects all the related IoT sensing and actuating devices ubiquitously so that the monitoring and control of numerous industrial systems can be realized. Deep learning, as one viable way to carry out big-data-driven modeling and analysis, could be integrated in IIoT systems to aid the automation and intelligence of IIoT systems. As deep learning requires large computation power, it is commonly deployed in cloud servers. Thus, the data collected by IoT devices must be transmitted to the cloud for training process, contributing to network congestion and affecting the IoT network performance as well as the supported applications. To address this issue, in this article, we leverage the fog/edge computing paradigm and propose an edge computing-based deep learning model, which utilizes edge computing to migrate the deep learning process from cloud servers to edge nodes, reducing data transmission demands in the IIoT network and mitigating network congestion. Since edge nodes have limited computation ability compared to servers, we design a mechanism to optimize the deep learning model so that its requirements for computational power can be reduced. To evaluate our proposed solution, we design a testbed implemented in the Google cloud and deploy the proposed convolutional neural network (CNN) model, utilizing a real-world IIoT data set to evaluate our approach.<sup>1</sup> Our experimental results confirm the effectiveness of our approach, which cannot only reduce the network traffic overhead for IIoT but also maintain the classification accuracy in comparison with several baseline schemes.<sup>1</sup>Certain commercial equipment, instruments, or materials are identified in this article in order to specify the experimental procedure adequately. Such identification is not intended to imply recommendation or endorsement by the National Institute of Standards and Technology, nor is it intended to imply that the materials or equipment identified are necessarily the best available for the purpose.",https://ieeexplore.ieee.org/document/8948000/,IEEE Internet of Things Journal,May 2020,ieeexplore
10.1109/TII.2020.3038780,Toward Secure Data Fusion in Industrial IoT Using Transfer Learning,IEEE,Journals,"As an emerging technology, the industrial Internet of Things (IIoT) can promote the development of industrial intelligence, improve production efficiency, and reduce manufacturing costs. In IIoT, the improvement and progress of industrial production and applications are inseparable from data fusion, a process that realizes the collection, analysis, and processing of the massive IoT data generated by industrial equipment and applications. IIot demands a real-time, effective, and privacy-preserving data fusion process. However, the existing works need to train different learning models for data analysis, which cannot meet real-time requirements in IIoT. Meanwhile, the lack of defense against internal attacks and the difficulty to balance system performance and privacy protection hinder the effectiveness and privacy protection in the data fusion process. To solve the abovementioned problems, in this article, we propose a new transfer learning-based secure data fusion strategy (TSDF) for IIoT. The proposed TSDF consists of three parts, guidance based deep deterministic policy gradient (GDDPG) algorithm for task classification, transfer learning based GDDPG for grouping of task receivers, and a multiblockchain mechanism for privacy preservation. The experiment results show that TSDF can achieve high system throughput and low latency, providing privacy preservation in data fusion under various IIoT application environments.",https://ieeexplore.ieee.org/document/9262056/,IEEE Transactions on Industrial Informatics,Oct. 2021,ieeexplore
10.1109/ACCESS.2021.3080517,Towards Open and Expandable Cognitive AI Architectures for Large-Scale Multi-Agent Human-Robot Collaborative Learning,IEEE,Journals,"Learning from Demonstration (LfD) constitutes one of the most robust methodologies for constructing efficient cognitive robotic systems. Despite the large body of research works already reported, current key technological challenges include those of multi-agent learning and long-term autonomy. Towards this direction, a novel cognitive architecture for multi-agent LfD robotic learning is introduced in this paper, targeting to enable the reliable deployment of open, scalable and expandable robotic systems in large-scale and complex environments. In particular, the designed architecture capitalizes on the recent advances in the Artificial Intelligence (AI) (and especially the Deep Learning (DL)) field, by establishing a Federated Learning (FL)-based framework for incarnating a multi-human multi-robot collaborative learning environment. The fundamental conceptualization relies on employing multiple AI-empowered cognitive processes (implementing various robotic tasks) that operate at the edge nodes of a network of robotic platforms, while global AI models (underpinning the aforementioned robotic tasks) are collectively created and shared among the network, by elegantly combining information from a large number of human-robot interaction instances. Regarding pivotal novelties, the designed cognitive architecture a) introduces a new FL-based formalism that extends the conventional LfD learning paradigm to support large-scale multi-agent operational settings, b) elaborates previous FL-based self-learning robotic schemes so as to incorporate the human in the learning loop and c) consolidates the fundamental principles of FL with additional sophisticated AI-enabled learning methodologies for modelling the multi-level inter-dependencies among the robotic tasks. The applicability of the proposed framework is explained using an example of a real-world industrial case study (subject to ongoing research activities) for agile production-based Critical Raw Materials (CRM) recovery.",https://ieeexplore.ieee.org/document/9431107/,IEEE Access,2021,ieeexplore
10.1109/ACCESS.2021.3104472,Transfer Learning Strategies for Credit Card Fraud Detection,IEEE,Journals,"Credit card fraud jeopardizes the trust of customers in e-commerce transactions. This led in recent years to major advances in the design of automatic Fraud Detection Systems (FDS) able to detect fraudulent transactions with short reaction time and high precision. Nevertheless, the heterogeneous nature of the fraud behavior makes it difficult to tailor existing systems to different contexts (e.g. new payment systems, different countries and/or population segments). Given the high cost (research, prototype development, and implementation in production) of designing data-driven FDSs, it is crucial for transactional companies to define procedures able to adapt existing pipelines to new challenges. From an AI/machine learning perspective, this is known as the problem of <italic>transfer learning</italic>. This paper discusses the design and implementation of transfer learning approaches for e-commerce credit card fraud detection and their assessment in a real setting. The case study, based on a six-month dataset (more than 200 million e-commerce transactions) provided by the industrial partner, relates to the transfer of detection models developed for a European country to another country. In particular, we present and discuss 15 transfer learning techniques (ranging from naive baselines to state-of-the-art and new approaches), making a critical and quantitative comparison in terms of precision for different transfer scenarios. Our contributions are twofold: (i) we show that the accuracy of many transfer methods is strongly dependent on the number of labeled samples in the target domain and (ii) we propose an ensemble solution to this problem based on self-supervised and semi-supervised domain adaptation classifiers. The thorough experimental assessment shows that this solution is both highly accurate and hardly sensitive to the number of labeled samples.",https://ieeexplore.ieee.org/document/9512084/,IEEE Access,2021,ieeexplore
10.26599/TST.2020.9010055,Underground pipeline surveillance with an algorithm based on statistical time-frequency acoustic features,TUP,Journals,"Underground pipeline networks suffer from severe damage by earth-moving devices due to rapid urbanization. Thus, designing a round-the-clock intelligent surveillance system has become crucial and urgent. In this study, we develop an acoustic signal-based excavation device recognition system for underground pipeline protection. The front-end hardware system is equipped with an acoustic sensor array, an Analog-to-Digital Converter (ADC) module (ADS1274), and an industrial processor Advanced RISC Machine (ARM) cortex-A8 for signal collection and algorithm implementation. Then, a novel Statistical Time-Frequency acoustic Feature (STFF) is proposed, and a fast Extreme Learning Machine (ELM) is adopted as the classifier. Experiments on real recorded data show that the proposed STFF achieves better discriminative capability than the conventional acoustic cepstrum features. In addition, the surveillance platform is applicable for encountering big data owing to the fast learning speed of ELM.",https://ieeexplore.ieee.org/document/9552663/,Tsinghua Science and Technology,April 2022,ieeexplore
10.1109/JSEN.2020.3033754,Virtual Sensors for Fault Diagnosis: A Case of Induction Motor Broken Rotor Bar,IEEE,Journals,"This article presents an industrial implementation of a virtual sensor in the process of fault detection of an induction motor. An ensemble-learning soft-sensor is developed to detect broken rotor bar that is essential to prevent irreparable damage. Most of the existing diagnostic methods assume that the data distribution is static and that all data is available during the training, while in real applications, the data become available as data streams. The proposed method is inspired by the ensemble learning algorithm, which is combined with a new drift detection mechanism. The advantages of the proposed approach are three-fold. First, a fair comparison with other algorithms show the effectiveness of the soft sensor scheme. Second, the presented concept change detection algorithm is capable of detecting a new class in the data stream as well as data distribution change, and last but not least, the efficacy of the proposed algorithm is demonstrated using benchmark concept drift data streams.",https://ieeexplore.ieee.org/document/9245589/,IEEE Sensors Journal,"15 Feb.15, 2021",ieeexplore
10.1109/ACCESS.2019.2931194,Vison-Based 3D Shape Measurement System for Transparent Microdefect Characterization,IEEE,Journals,"The main task of vision-based industrial defect inspection is to implement efficient non-contact visual quality control, i.e., to detect if there is a defect and to achieve an accurate 3D shape measurement of such a defect, and this kind of vision defect inspection system has been widely applied in various industrial application. However, it is still not the case in the inspection of transparent microdefect on the polarizer (which is the most important part of an LCD screen). Optical measurement devices (such as confocal microscopy) are often utilized to fulfil this task. To solve problems lied in the current confocal microscopy inspection system, such as expensive and non-real-time processing, this research aims to develop a novel vision-based 3D shape measurement system for polarizer transparent microdefect characterization. The innovation of this system, which has been verified by our optical model simulation, is that the 3D sizes of microdefect have a monotonically relation to the grayscale of the microdefect image. Hence, a microdefect imaging system, which could acquire defect image accurately, is first well designed and implemented. Then, a support vector regression (SVR) algorithm is derived by the trained data, i.e., 100 acquired defect images and its corresponding 3D shape value by confocal microscopy. Characterized 3D measurement of microdefect is thereby obtained by this SVR algorithm. 30 polarizer microdefect samples have been imaged and measured by our proposed system, and several important performance indicators, including processing speed, accuracy and system reproducibility, have been elaborately tested. The experimental results show that the proposed system could achieve a high-accuracy measurement but in a much faster and more efficient way than the confocal microscopy. Besides, this developed imaging system has been evaluated in real applications, and over 300 samples have been detected, which also validate the effectiveness of the proposed system.",https://ieeexplore.ieee.org/document/8777162/,IEEE Access,2019,ieeexplore
10.1109/TNNLS.2019.2935033,Weighted Broad Learning System and Its Application in Nonlinear Industrial Process Modeling,IEEE,Journals,"Broad learning system (BLS) is a novel neural network with effective and efficient learning ability. BLS has attracted increasing attention from many scholars owing to its excellent performance. This article proposes a weighted BLS (WBLS) based on BLS to tackle the noise and outliers in an industrial process. WBLS provides a unified framework for easily using different methods of calculating the weighted penalty factor. Using the weighted penalty factor to constrain the contribution of each sample to modeling, the normal and abnormal samples were allocated higher and lower weights to increase and decrease their contributions, respectively. Hence, the WBLS can eliminate the bad effect of noise and outliers on the modeling. The weighted ridge regression algorithm is used to compute the algorithm solution. Weighted incremental learning algorithms are also developed using the weighted penalty factor to tackle the noise and outliers in the additional samples and quickly increase nodes or samples without retraining. The proposed weighted incremental learning algorithms provide a unified framework for using different methods of computing weights. We test the feasibility of the proposed algorithms on some public data sets and a real-world application. Experiment results show that our method has better generalization and robustness.",https://ieeexplore.ieee.org/document/8833523/,IEEE Transactions on Neural Networks and Learning Systems,Aug. 2020,ieeexplore
10.1109/TIFS.2019.2945619,Wiretap Code Design by Neural Network Autoencoders,IEEE,Journals,"In industrial machine type communications, an increasing number of wireless devices communicate under reliability, latency, and confidentiality constraints, simultaneously. From information theory, it is known that wiretap codes can asymptotically achieve reliability (vanishing block error rate (BLER) at the legitimate receiver Bob) while also achieving secrecy (vanishing information leakage (IL) to an eavesdropper Eve). However, under finite block length, there exists a tradeoff between the BLER at Bob and the IL at Eve. In this work, we propose a flexible wiretap code design for degraded Gaussian wiretap channels under finite block length, which can change the operating point on the Pareto boundary of the tradeoff between BLER and IL given specific code parameters. To attain this goal, we formulate a multi-objective programming problem, which takes the BLER at Bob and the IL at Eve into account. During training, we approximate the BLER by the mean square error and the IL by schemes based on Jensen's inequality and the Taylor expansion and then solve the optimization problem by neural network autoencoders. Simulation results show that the proposed scheme can find codes outperforming polar wiretap codes (PWC) with respect to both BLER and IL simultaneously. We show that the codes found by the autoencoders could be implemented with real modulation schemes with only small losses in performance.",https://ieeexplore.ieee.org/document/8859269/,IEEE Transactions on Information Forensics and Security,2020,ieeexplore
10.1109/JSEN.2020.3024094,k-Nearest Neighbor Classification for Pattern Recognition of a Reference Source Light for Machine Vision System,IEEE,Journals,"The design of machine vision applications allows automatic inspection, measuring systems, and robot guidance. Typical applications of industrial robots are based on no-contact sensors to give the robot information about the environment. Robot's machine vision requires photosensors or video cameras to make intelligent decisions about its localization. Video cameras used as image-capturing equipment are too costly in comparison with optical scanning systems (OSS). The OSS system provides spatial coordinates measurements that can be exploited to solve a wide variety of structural problems in real-time. Localization and guidance using machine learning (ML) techniques offer advantages due to signals captured can be transformed and be reduced for processing, storage, and displaying. The use of algorithms of ML enhances the performance of the optical system based on localization and guidance. Feature extraction represents an important part of ML techniques to transform the original raw data onto a low-dimensional subspace and holding relevant information. This work presents an improvement of an optical system based on <i>k</i>-nearest neighbor ( <i>k</i>-NN) technique to solve the object detection and localization problem. The utility of this improvement allows the optical system can discriminate between the reference source and the optical noise or interference. The OSS system presented in this article has been implemented in structural health monitoring to measure the angular position even under “lighting and weather conditions”. The feature extraction techniques used in this article were linear predictive coding (LPC), quartiles ( <i>Q</i><sub>iquartile</sub>), and autocorrelation coefficients (ACC). The results of using <i>k</i>-NN and autocorrelation coefficients and quartiles predicted more than 98% of correct classification by using a reference source light as a class 1 and a light bulb as an optical noise and called class 2.",https://ieeexplore.ieee.org/document/9195874/,IEEE Sensors Journal,"15 May15, 2021",ieeexplore
