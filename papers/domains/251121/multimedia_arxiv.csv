id,updated,published,title,summary,database
http://arxiv.org/abs/2001.05703v1,2020-01-16T09:13:31Z,2020-01-16T09:13:31Z,"A Markerless Deep Learning-based 6 Degrees of Freedom PoseEstimation for
  with Mobile Robots using RGB Data","Augmented Reality has been subject to various integration efforts within
industries due to its ability to enhance human machine interaction and
understanding. Neural networks have achieved remarkable results in areas of
computer vision, which bear great potential to assist and facilitate an
enhanced Augmented Reality experience. However, most neural networks are
computationally intensive and demand huge processing power thus, are not
suitable for deployment on Augmented Reality devices. In this work we propose a
method to deploy state of the art neural networks for real time 3D object
localization on augmented reality devices. As a result, we provide a more
automated method of calibrating the AR devices with mobile robotic systems. To
accelerate the calibration process and enhance user experience, we focus on
fast 2D detection approaches which are extracting the 3D pose of the object
fast and accurately by using only 2D input. The results are implemented into an
Augmented Reality application for intuitive robot control and sensor data
visualization. For the 6D annotation of 2D images, we developed an annotation
tool, which is, to our knowledge, the first open source tool to be available.
We achieve feasible results which are generally applicable to any AR device
thus making this work promising for further research in combining high
demanding neural networks with Internet of Things devices.",arxiv
http://arxiv.org/abs/1912.06321v2,2020-08-17T03:26:55Z,2019-12-13T04:29:38Z,"Sim2Real Predictivity: Does Evaluation in Simulation Predict Real-World
  Performance?","Does progress in simulation translate to progress on robots? If one method
outperforms another in simulation, how likely is that trend to hold in reality
on a robot? We examine this question for embodied PointGoal navigation,
developing engineering tools and a research paradigm for evaluating a simulator
by its sim2real predictivity. First, we develop Habitat-PyRobot Bridge (HaPy),
a library for seamless execution of identical code on simulated agents and
robots, transferring simulation-trained agents to a LoCoBot platform with a
one-line code change. Second, we investigate the sim2real predictivity of
Habitat-Sim for PointGoal navigation. We 3D-scan a physical lab space to create
a virtualized replica, and run parallel tests of 9 different models in reality
and simulation. We present a new metric called Sim-vs-Real Correlation
Coefficient (SRCC) to quantify predictivity. We find that SRCC for Habitat as
used for the CVPR19 challenge is low (0.18 for the success metric), suggesting
that performance differences in this simulator-based challenge do not persist
after physical deployment. This gap is largely due to AI agents learning to
exploit simulator imperfections, abusing collision dynamics to 'slide' along
walls, leading to shortcuts through otherwise non-navigable space. Naturally,
such exploits do not work in the real world. Our experiments show that it is
possible to tune simulation parameters to improve sim2real predictivity (e.g.
improving $SRCC_{Succ}$ from 0.18 to 0.844), increasing confidence that
in-simulation comparisons will translate to deployed systems in reality.",arxiv
http://arxiv.org/abs/1805.10604v2,2018-06-27T14:04:07Z,2018-05-27T11:01:30Z,"Deployment of Customized Deep Learning based Video Analytics On
  Surveillance Cameras","This paper demonstrates the effectiveness of our customized deep learning
based video analytics system in various applications focused on security,
safety, customer analytics and process compliance. We describe our video
analytics system comprising of Search, Summarize, Statistics and real-time
alerting, and outline its building blocks. These building blocks include object
detection, tracking, face detection and recognition, human and face
sub-attribute analytics. In each case, we demonstrate how custom models trained
using data from the deployment scenarios provide considerably superior
accuracies than off-the-shelf models. Towards this end, we describe our data
processing and model training pipeline, which can train and fine-tune models
from videos with a quick turnaround time. Finally, since most of these models
are deployed on-site, it is important to have resource constrained models which
do not require GPUs. We demonstrate how we custom train resource constrained
models and deploy them on embedded devices without significant loss in
accuracy. To our knowledge, this is the first work which provides a
comprehensive evaluation of different deep learning models on various
real-world customer deployment scenarios of surveillance video analytics. By
sharing our implementation details and the experiences learned from deploying
customized deep learning models for various customers, we hope that customized
deep learning based video analytics is widely incorporated in commercial
products around the world.",arxiv
http://arxiv.org/abs/1807.08775v1,2018-07-23T18:19:57Z,2018-07-23T18:19:57Z,CNN-based Facial Affect Analysis on Mobile Devices,"This paper focuses on the design, deployment and evaluation of Convolutional
Neural Network (CNN) architectures for facial affect analysis on mobile
devices. Unlike traditional CNN approaches, models deployed to mobile devices
must minimise storage requirements while retaining high performance. We
therefore propose three variants of established CNN architectures and
comparatively evaluate them on a large, in-the-wild benchmark dataset of facial
images. Our results show that the proposed architectures retain similar
performance to the dataset baseline while minimising storage requirements:
achieving 58% accuracy for eight-class emotion classification and average RMSE
of 0.39 for valence/arousal prediction. To demonstrate the feasibility of
deploying these models for real-world applications, we implement a music
recommendation interface based on predicted user affect. Although the CNN
models were not trained in the context of music recommendation, our case study
shows that: (i) the trained models achieve similar prediction performance to
the benchmark dataset, and (ii) users tend to positively rate the song
recommendations provided by the interface. Average runtime of the deployed
models on an iPhone 6S equates to ~45 fps, suggesting that the proposed
architectures are also well suited for real-time deployment on video streams.",arxiv
http://arxiv.org/abs/1705.00346v1,2017-04-30T17:17:44Z,2017-04-30T17:17:44Z,Deep Learning in the Automotive Industry: Applications and Tools,"Deep Learning refers to a set of machine learning techniques that utilize
neural networks with many hidden layers for tasks, such as image
classification, speech recognition, language understanding. Deep learning has
been proven to be very effective in these domains and is pervasively used by
many Internet services. In this paper, we describe different automotive uses
cases for deep learning in particular in the domain of computer vision. We
surveys the current state-of-the-art in libraries, tools and infrastructures
(e.\,g.\ GPUs and clouds) for implementing, training and deploying deep neural
networks. We particularly focus on convolutional neural networks and computer
vision use cases, such as the visual inspection process in manufacturing plants
and the analysis of social media data. To train neural networks, curated and
labeled datasets are essential. In particular, both the availability and scope
of such datasets is typically very limited. A main contribution of this paper
is the creation of an automotive dataset, that allows us to learn and
automatically recognize different vehicle properties. We describe an end-to-end
deep learning application utilizing a mobile app for data collection and
process support, and an Amazon-based cloud backend for storage and training.
For training we evaluate the use of cloud and on-premises infrastructures
(including multiple GPUs) in conjunction with different neural network
architectures and frameworks. We assess both the training times as well as the
accuracy of the classifier. Finally, we demonstrate the effectiveness of the
trained classifier in a real world setting during manufacturing process.",arxiv
http://arxiv.org/abs/1810.07829v1,2018-10-17T23:06:06Z,2018-10-17T23:06:06Z,"Quality 4.0: Let's Get Digital - The many ways the fourth industrial
  revolution is reshaping the way we think about quality","The technology landscape is richer and more promising than ever before. In
many ways, cloud computing, big data, virtual reality (VR), augmented reality
(AR), blockchain, additive manufacturing, artificial intelligence (AI), machine
learning (ML), Internet Protocol Version 6 (IPv6), cyber-physical systems and
the Internet of Things (IoT) all represent new frontiers. These technologies
can help improve product and service quality, and organizational performance.
In many regions, the internet is now as ubiquitous as electricity. Components
are relatively cheap. A robust ecosystem of open-source software libraries
means that engineers can solve problems 100 times faster than just two decades
ago. This digital transformation is leading us toward connected intelligent
automation: smart, hyperconnected agents deployed in environments where humans
and machines cooperate, and leverage data, to achieve shared goals. This is not
the worlds first industrial revolution. In fact, it is its fourth, and the
disruptive changes it will bring suggest we will need a fresh perspective on
quality to adapt to it.",arxiv
http://arxiv.org/abs/1710.09860v2,2018-04-12T13:20:15Z,2017-10-26T18:40:45Z,DoShiCo Challenge: Domain Shift in Control Prediction,"Training deep neural network policies end-to-end for real-world applications
so far requires big demonstration datasets in the real world or big sets
consisting of a large variety of realistic and closely related 3D CAD models.
These real or virtual data should, moreover, have very similar characteristics
to the conditions expected at test time. These stringent requirements and the
time consuming data collection processes that they entail, are currently the
most important impediment that keeps deep reinforcement learning from being
deployed in real-world applications. Therefore, in this work we advocate an
alternative approach, where instead of avoiding any domain shift by carefully
selecting the training data, the goal is to learn a policy that can cope with
it. To this end, we propose the DoShiCo challenge: to train a model in very
basic synthetic environments, far from realistic, in a way that it can be
applied in more realistic environments as well as take the control decisions on
real-world data. In particular, we focus on the task of collision avoidance for
drones. We created a set of simulated environments that can be used as
benchmark and implemented a baseline method, exploiting depth prediction as an
auxiliary task to help overcome the domain shift. Even though the policy is
trained in very basic environments, it can learn to fly without collisions in a
very different realistic simulated environment. Of course several benchmarks
for reinforcement learning already exist - but they never include a large
domain shift. On the other hand, several benchmarks in computer vision focus on
the domain shift, but they take the form of a static datasets instead of
simulated environments. In this work we claim that it is crucial to take the
two challenges together in one benchmark.",arxiv
http://arxiv.org/abs/2105.05873v1,2021-05-12T18:00:14Z,2021-05-12T18:00:14Z,Out of the Box: Embodied Navigation in the Real World,"The research field of Embodied AI has witnessed substantial progress in
visual navigation and exploration thanks to powerful simulating platforms and
the availability of 3D data of indoor and photorealistic environments. These
two factors have opened the doors to a new generation of intelligent agents
capable of achieving nearly perfect PointGoal Navigation. However, such
architectures are commonly trained with millions, if not billions, of frames
and tested in simulation. Together with great enthusiasm, these results yield a
question: how many researchers will effectively benefit from these advances? In
this work, we detail how to transfer the knowledge acquired in simulation into
the real world. To that end, we describe the architectural discrepancies that
damage the Sim2Real adaptation ability of models trained on the Habitat
simulator and propose a novel solution tailored towards the deployment in
real-world scenarios. We then deploy our models on a LoCoBot, a Low-Cost Robot
equipped with a single Intel RealSense camera. Different from previous work,
our testing scene is unavailable to the agent in simulation. The environment is
also inaccessible to the agent beforehand, so it cannot count on scene-specific
semantic priors. In this way, we reproduce a setting in which a research group
(potentially from other fields) needs to employ the agent visual navigation
capabilities as-a-Service. Our experiments indicate that it is possible to
achieve satisfying results when deploying the obtained model in the real world.
Our code and models are available at https://github.com/aimagelab/LoCoNav.",arxiv
http://arxiv.org/abs/2104.09164v1,2021-04-19T09:41:32Z,2021-04-19T09:41:32Z,"HEAR: Human Action Recognition via Neural Networks on Homomorphically
  Encrypted Data","Remote monitoring to support ""aging in place"" is an active area of research.
Advanced computer vision technology based on deep learning can provide near
real-time home monitoring to detect falling and symptoms related to seizure,
and stroke. Affordable webcams, together with cloud computing services (to run
machine learning algorithms), can potentially bring significant social and
health benefits. However, it has not been deployed in practice because of
privacy and security concerns. People may feel uncomfortable sending their
videos of daily activities (with potentially sensitive private information) to
a computing service provider (e.g., on a commercial cloud). In this paper, we
propose a novel strategy to resolve this dilemma by applying fully homomorphic
encryption (FHE) to an alternative representation of human actions (i.e.,
skeleton joints), which guarantees information confidentiality while retaining
high-performance action detection at a low cost. We design an FHE-friendly
neural network for action recognition and present a secure neural network
evaluation strategy to achieve near real-time action detection. Our framework
for private inference achieves an 87.99% recognition accuracy (86.21%
sensitivity and 99.14% specificity in detecting falls) with a latency of 3.1
seconds on real-world datasets. Our evaluation shows that our elaborated and
fine-tuned method reduces the inference latency by 23.81%~74.67% over a
straightforward implementation.",arxiv
http://arxiv.org/abs/1808.02134v1,2018-08-06T22:13:33Z,2018-08-06T22:13:33Z,"Kerman: A Hybrid Lightweight Tracking Algorithm to Enable Smart
  Surveillance as an Edge Service","Edge computing pushes the cloud computing boundaries beyond uncertain network
resource by leveraging computational processes close to the source and target
of data. Time-sensitive and data-intensive video surveillance applications
benefit from on-site or near-site data mining. In recent years, many smart
video surveillance approaches are proposed for object detection and tracking by
using Artificial Intelligence (AI) and Machine Learning (ML) algorithms.
However, it is still hard to migrate those computing and data-intensive tasks
from Cloud to Edge due to the high computational requirement. In this paper, we
envision to achieve intelligent surveillance as an edge service by proposing a
hybrid lightweight tracking algorithm named Kerman (Kernelized Kalman filter).
Kerman is a decision tree based hybrid Kernelized Correlation Filter (KCF)
algorithm proposed for human object tracking, which is coupled with a
lightweight Convolutional Neural Network (L-CNN) for high performance. The
proposed Kerman algorithm has been implemented on a couple of single board
computers (SBC) as edge devices and validated using real-world surveillance
video streams. The experimental results are promising that the Kerman algorithm
is able to track the object of interest with a decent accuracy at a resource
consumption affordable by edge devices.",arxiv
http://arxiv.org/abs/2002.09821v1,2020-02-23T03:51:08Z,2020-02-23T03:51:08Z,"A Multi-view CNN-based Acoustic Classification System for Automatic
  Animal Species Identification","Automatic identification of animal species by their vocalization is an
important and challenging task. Although many kinds of audio monitoring system
have been proposed in the literature, they suffer from several disadvantages
such as non-trivial feature selection, accuracy degradation because of
environmental noise or intensive local computation. In this paper, we propose a
deep learning based acoustic classification framework for Wireless Acoustic
Sensor Network (WASN). The proposed framework is based on cloud architecture
which relaxes the computational burden on the wireless sensor node. To improve
the recognition accuracy, we design a multi-view Convolution Neural Network
(CNN) to extract the short-, middle-, and long-term dependencies in parallel.
The evaluation on two real datasets shows that the proposed architecture can
achieve high accuracy and outperforms traditional classification systems
significantly when the environmental noise dominate the audio signal (low SNR).
Moreover, we implement and deploy the proposed system on a testbed and analyse
the system performance in real-world environments. Both simulation and
real-world evaluation demonstrate the accuracy and robustness of the proposed
acoustic classification system in distinguishing species of animals.",arxiv
http://arxiv.org/abs/2012.05410v1,2020-12-10T02:08:47Z,2020-12-10T02:08:47Z,Artificial Intelligence at the Edge,"The Internet of Things (IoT) and edge computing applications aim to support a
variety of societal needs, including the global pandemic situation that the
entire world is currently experiencing and responses to natural disasters.
  The need for real-time interactive applications such as immersive video
conferencing, augmented/virtual reality, and autonomous vehicles, in education,
healthcare, disaster recovery and other domains, has never been higher. At the
same time, there have been recent technological breakthroughs in highly
relevant fields such as artificial intelligence (AI)/machine learning (ML),
advanced communication systems (5G and beyond), privacy-preserving
computations, and hardware accelerators. 5G mobile communication networks
increase communication capacity, reduce transmission latency and error, and
save energy -- capabilities that are essential for new applications. The
envisioned future 6G technology will integrate many more technologies,
including for example visible light communication, to support groundbreaking
applications, such as holographic communications and high precision
manufacturing. Many of these applications require computations and analytics
close to application end-points: that is, at the edge of the network, rather
than in a centralized cloud. AI techniques applied at the edge have tremendous
potential both to power new applications and to need more efficient operation
of edge infrastructure. However, it is critical to understand where to deploy
AI systems within complex ecosystems consisting of advanced applications and
the specific real-time requirements towards AI systems.",arxiv
http://arxiv.org/abs/2106.15021v1,2021-06-21T11:23:12Z,2021-06-21T11:23:12Z,"How to Reach Real-Time AI on Consumer Devices? Solutions for
  Programmable and Custom Architectures","The unprecedented performance of deep neural networks (DNNs) has led to large
strides in various Artificial Intelligence (AI) inference tasks, such as object
and speech recognition. Nevertheless, deploying such AI models across commodity
devices faces significant challenges: large computational cost, multiple
performance objectives, hardware heterogeneity and a common need for high
accuracy, together pose critical problems to the deployment of DNNs across the
various embedded and mobile devices in the wild. As such, we have yet to
witness the mainstream usage of state-of-the-art deep learning algorithms
across consumer devices. In this paper, we provide preliminary answers to this
potentially game-changing question by presenting an array of design techniques
for efficient AI systems. We start by examining the major roadblocks when
targeting both programmable processors and custom accelerators. Then, we
present diverse methods for achieving real-time performance following a
cross-stack approach. These span model-, system- and hardware-level techniques,
and their combination. Our findings provide illustrative examples of AI systems
that do not overburden mobile hardware, while also indicating how they can
improve inference accuracy. Moreover, we showcase how custom ASIC- and
FPGA-based accelerators can be an enabling factor for next-generation AI
applications, such as multi-DNN systems. Collectively, these results highlight
the critical need for further exploration as to how the various cross-stack
solutions can be best combined in order to bring the latest advances in deep
learning close to users, in a robust and efficient manner.",arxiv
http://arxiv.org/abs/1905.04127v1,2019-05-10T12:43:52Z,2019-05-10T12:43:52Z,"Design of Artificial Intelligence Agents for Games using Deep
  Reinforcement Learning","In order perform a large variety of tasks and to achieve human-level
performance in complex real-world environments, Artificial Intelligence (AI)
Agents must be able to learn from their past experiences and gain both
knowledge and an accurate representation of their environment from raw sensory
inputs. Traditionally, AI agents have suffered from difficulties in using only
sensory inputs to obtain a good representation of their environment and then
mapping this representation to an efficient control policy. Deep reinforcement
learning algorithms have provided a solution to this issue. In this study, the
performance of different conventional and novel deep reinforcement learning
algorithms was analysed. The proposed method utilises two types of algorithms,
one trained with a variant of Q-learning (DQN) and another trained with SARSA
learning (DSN) to assess the feasibility of using direct feedback alignment, a
novel biologically plausible method for back-propagating the error. These novel
agents, alongside two similar agents trained with the conventional
backpropagation algorithm, were tested by using the OpenAI Gym toolkit on
several classic control theory problems and Atari 2600 video games. The results
of this investigation open the way into new, biologically-inspired deep
reinforcement learning algorithms, and their implementation on neuromorphic
hardware.",arxiv
http://arxiv.org/abs/2104.12462v1,2021-04-26T10:44:01Z,2021-04-26T10:44:01Z,Points2Sound: From mono to binaural audio using 3D point cloud scenes,"Binaural sound that matches the visual counterpart is crucial to bring
meaningful and immersive experiences to people in augmented reality (AR) and
virtual reality (VR) applications. Recent works have shown the possibility to
generate binaural audio from mono using 2D visual information as guidance.
Using 3D visual information may allow for a more accurate representation of a
virtual audio scene for VR/AR applications. This paper proposes Points2Sound, a
multi-modal deep learning model which generates a binaural version from mono
audio using 3D point cloud scenes. Specifically, Points2Sound consist of a
vision network which extracts visual features from the point cloud scene to
condition an audio network, which operates in the waveform domain, to
synthesize the binaural version. Both quantitative and perceptual evaluations
indicate that our proposed model is preferred over a reference case, based on a
recent 2D mono-to-binaural model.",arxiv
http://arxiv.org/abs/2012.01913v1,2020-12-03T13:51:05Z,2020-12-03T13:51:05Z,Transfer Learning as an Enabler of the Intelligent Digital Twin,"Digital Twins have been described as beneficial in many areas, such as
virtual commissioning, fault prediction or reconfiguration planning. Equipping
Digital Twins with artificial intelligence functionalities can greatly expand
those beneficial applications or open up altogether new areas of application,
among them cross-phase industrial transfer learning. In the context of machine
learning, transfer learning represents a set of approaches that enhance
learning new tasks based upon previously acquired knowledge. Here, knowledge is
transferred from one lifecycle phase to another in order to reduce the amount
of data or time needed to train a machine learning algorithm. Looking at common
challenges in developing and deploying industrial machinery with deep learning
functionalities, embracing this concept would offer several advantages: Using
an intelligent Digital Twin, learning algorithms can be designed, configured
and tested in the design phase before the physical system exists and real data
can be collected. Once real data becomes available, the algorithms must merely
be fine-tuned, significantly speeding up commissioning and reducing the
probability of costly modifications. Furthermore, using the Digital Twin's
simulation capabilities virtually injecting rare faults in order to train an
algorithm's response or using reinforcement learning, e.g. to teach a robot,
become practically feasible. This article presents several cross-phase
industrial transfer learning use cases utilizing intelligent Digital Twins. A
real cyber physical production system consisting of an automated welding
machine and an automated guided vehicle equipped with a robot arm is used to
illustrate the respective benefits.",arxiv
http://arxiv.org/abs/1905.03418v2,2019-06-07T15:31:48Z,2019-05-09T02:39:37Z,"Deep Learning Acceleration Techniques for Real Time Mobile Vision
  Applications","Deep Learning (DL) has become a crucial technology for Artificial
Intelligence (AI). It is a powerful technique to automatically extract
high-level features from complex data which can be exploited for applications
such as computer vision, natural language processing, cybersecurity,
communications, and so on. For the particular case of computer vision, several
algorithms like object detection in real time videos have been proposed and
they work well on Desktop GPUs and distributed computing platforms. However
these algorithms are still heavy for mobile and embedded visual applications.
The rapid spreading of smart portable devices and the emerging 5G network are
introducing new smart multimedia applications in mobile environments. As a
consequence, the possibility of implementing deep neural networks to mobile
environments has attracted a lot of researchers. This paper presents emerging
deep learning acceleration techniques that can enable the delivery of real time
visual recognition into the hands of end users, anytime and anywhere.",arxiv
http://arxiv.org/abs/2005.01557v1,2020-05-04T15:16:30Z,2020-05-04T15:16:30Z,"Off-the-shelf deep learning is not enough: parsimony, Bayes and
  causality","Deep neural networks (""deep learning"") have emerged as a technology of choice
to tackle problems in natural language processing, computer vision, speech
recognition and gameplay, and in just a few years has led to superhuman level
performance and ushered in a new wave of ""AI."" Buoyed by these successes,
researchers in the physical sciences have made steady progress in incorporating
deep learning into their respective domains. However, such adoption brings
substantial challenges that need to be recognized and confronted. Here, we
discuss both opportunities and roadblocks to implementation of deep learning
within materials science, focusing on the relationship between correlative
nature of machine learning and causal hypothesis driven nature of physical
sciences. We argue that deep learning and AI are now well positioned to
revolutionize fields where causal links are known, as is the case for
applications in theory. When confounding factors are frozen or change only
weakly, this leaves open the pathway for effective deep learning solutions in
experimental domains. Similarly, these methods offer a pathway towards
understanding the physics of real-world systems, either via deriving reduced
representations, deducing algorithmic complexity, or recovering generative
physical models. However, extending deep learning and ""AI"" for models with
unclear causal relationship can produce misleading and potentially incorrect
results. Here, we argue the broad adoption of Bayesian methods incorporating
prior knowledge, development of DL solutions with incorporated physical
constraints, and ultimately adoption of causal models, offers a path forward
for fundamental and applied research. Most notably, while these advances can
change the way science is carried out in ways we cannot imagine, machine
learning is not going to substitute science any time soon.",arxiv
http://arxiv.org/abs/2102.09360v1,2021-02-18T14:12:24Z,2021-02-18T14:12:24Z,"All-optical spiking neurosynaptic networks with self-learning
  capabilities","Software-implementation, via neural networks, of brain-inspired computing
approaches underlie many important modern-day computational tasks, from image
processing to speech recognition, artificial intelligence and deep learning
applications. Yet, differing from real neural tissue, traditional computing
architectures physically separate the core computing functions of memory and
processing, making fast, efficient and low-energy brain-like computing
difficult to achieve. To overcome such limitations, an attractive and
alternative goal is to design direct hardware mimics of brain neurons and
synapses which, when connected in appropriate networks (or neuromorphic
systems), process information in a way more fundamentally analogous to that of
real brains. Here we present an all-optical approach to achieving such a goal.
Specifically, we demonstrate an all-optical spiking neuron device and connect
it, via an integrated photonics network, to photonic synapses to deliver a
small-scale all-optical neurosynaptic system capable of supervised and
unsupervised learning. Moreover, we exploit wavelength division multiplexing
techniques to implement a scalable circuit architecture for photonic neural
networks, successfully demonstrating pattern recognition directly in the
optical domain using a photonic system comprising 140 elements. Such optical
implementations of neurosynaptic networks promise access to the high speed and
bandwidth inherent to optical systems, which would be very attractive for the
direct processing of telecommunication and visual data in the optical domain.",arxiv
http://arxiv.org/abs/2005.13857v1,2020-05-28T09:15:14Z,2020-05-28T09:15:14Z,"Deep Reinforcement learning for real autonomous mobile robot navigation
  in indoor environments","Deep Reinforcement Learning has been successfully applied in various computer
games [8]. However, it is still rarely used in real-world applications,
especially for the navigation and continuous control of real mobile robots
[13]. Previous approaches lack safety and robustness and/or need a structured
environment. In this paper we present our proof of concept for autonomous
self-learning robot navigation in an unknown environment for a real robot
without a map or planner. The input for the robot is only the fused data from a
2D laser scanner and a RGB-D camera as well as the orientation to the goal. The
map of the environment is unknown. The output actions of an Asynchronous
Advantage Actor-Critic network (GA3C) are the linear and angular velocities for
the robot. The navigator/controller network is pretrained in a high-speed,
parallel, and self-implemented simulation environment to speed up the learning
process and then deployed to the real robot. To avoid overfitting, we train
relatively small networks, and we add random Gaussian noise to the input laser
data. The sensor data fusion with the RGB-D camera allows the robot to navigate
in real environments with real 3D obstacle avoidance and without the need to
fit the environment to the sensory capabilities of the robot. To further
increase the robustness, we train on environments of varying difficulties and
run 32 training instances simultaneously. Video: supplementary File / YouTube,
Code: GitHub",arxiv
http://arxiv.org/abs/2008.12858v1,2020-08-28T21:44:24Z,2020-08-28T21:44:24Z,Real-world Video Adaptation with Reinforcement Learning,"Client-side video players employ adaptive bitrate (ABR) algorithms to
optimize user quality of experience (QoE). We evaluate recently proposed
RL-based ABR methods in Facebook's web-based video streaming platform.
Real-world ABR contains several challenges that requires customized designs
beyond off-the-shelf RL algorithms -- we implement a scalable neural network
architecture that supports videos with arbitrary bitrate encodings; we design a
training method to cope with the variance resulting from the stochasticity in
network conditions; and we leverage constrained Bayesian optimization for
reward shaping in order to optimize the conflicting QoE objectives. In a
week-long worldwide deployment with more than 30 million video streaming
sessions, our RL approach outperforms the existing human-engineered ABR
algorithms.",arxiv
http://arxiv.org/abs/1803.10760v1,2018-03-28T17:54:01Z,2018-03-28T17:54:01Z,Unsupervised Predictive Memory in a Goal-Directed Agent,"Animals execute goal-directed behaviours despite the limited range and scope
of their sensors. To cope, they explore environments and store memories
maintaining estimates of important information that is not presently available.
Recently, progress has been made with artificial intelligence (AI) agents that
learn to perform tasks from sensory input, even at a human level, by merging
reinforcement learning (RL) algorithms with deep neural networks, and the
excitement surrounding these results has led to the pursuit of related ideas as
explanations of non-human animal learning. However, we demonstrate that
contemporary RL algorithms struggle to solve simple tasks when enough
information is concealed from the sensors of the agent, a property called
""partial observability"". An obvious requirement for handling partially observed
tasks is access to extensive memory, but we show memory is not enough; it is
critical that the right information be stored in the right format. We develop a
model, the Memory, RL, and Inference Network (MERLIN), in which memory
formation is guided by a process of predictive modeling. MERLIN facilitates the
solution of tasks in 3D virtual reality environments for which partial
observability is severe and memories must be maintained over long durations.
Our model demonstrates a single learning agent architecture that can solve
canonical behavioural tasks in psychology and neurobiology without strong
simplifying assumptions about the dimensionality of sensory input or the
duration of experiences.",arxiv
http://arxiv.org/abs/2109.09828v1,2021-09-20T20:17:40Z,2021-09-20T20:17:40Z,iRNN: Integer-only Recurrent Neural Network,"Recurrent neural networks (RNN) are used in many real-world text and speech
applications. They include complex modules such as recurrence,
exponential-based activation, gate interaction, unfoldable normalization,
bi-directional dependence, and attention. The interaction between these
elements prevents running them on integer-only operations without a significant
performance drop. Deploying RNNs that include layer normalization and attention
on integer-only arithmetic is still an open problem. We present a
quantization-aware training method for obtaining a highly accurate integer-only
recurrent neural network (iRNN). Our approach supports layer normalization,
attention, and an adaptive piecewise linear approximation of activations, to
serve a wide range of RNNs on various applications. The proposed method is
proven to work on RNN-based language models and automatic speech recognition.
Our iRNN maintains similar performance as its full-precision counterpart, their
deployment on smartphones improves the runtime performance by $2\times$, and
reduces the model size by $4\times$.",arxiv
http://arxiv.org/abs/1909.11145v2,2019-10-01T09:02:01Z,2019-09-24T19:29:30Z,"Brain-Inspired Hardware for Artificial Intelligence: Accelerated
  Learning in a Physical-Model Spiking Neural Network","Future developments in artificial intelligence will profit from the existence
of novel, non-traditional substrates for brain-inspired computing. Neuromorphic
computers aim to provide such a substrate that reproduces the brain's
capabilities in terms of adaptive, low-power information processing. We present
results from a prototype chip of the BrainScaleS-2 mixed-signal neuromorphic
system that adopts a physical-model approach with a 1000-fold acceleration of
spiking neural network dynamics relative to biological real time. Using the
embedded plasticity processor, we both simulate the Pong arcade video game and
implement a local plasticity rule that enables reinforcement learning, allowing
the on-chip neural network to learn to play the game. The experiment
demonstrates key aspects of the employed approach, such as accelerated and
flexible learning, high energy efficiency and resilience to noise.",arxiv
http://arxiv.org/abs/2103.13997v1,2021-03-25T17:34:59Z,2021-03-25T17:34:59Z,Real-time low-resource phoneme recognition on edge devices,"While speech recognition has seen a surge in interest and research over the
last decade, most machine learning models for speech recognition either require
large training datasets or lots of storage and memory. Combined with the
prominence of English as the number one language in which audio data is
available, this means most other languages currently lack good speech
recognition models.
  The method presented in this paper shows how to create and train models for
speech recognition in any language which are not only highly accurate, but also
require very little storage, memory and training data when compared with
traditional models. This allows training models to recognize any language and
deploying them on edge devices such as mobile phones or car displays for fast
real-time speech recognition.",arxiv
http://arxiv.org/abs/1910.01918v1,2019-10-03T04:47:40Z,2019-10-03T04:47:40Z,Convolutional Neural Networks for Speech Controlled Prosthetic Hands,"Speech recognition is one of the key topics in artificial intelligence, as it
is one of the most common forms of communication in humans. Researchers have
developed many speech-controlled prosthetic hands in the past decades,
utilizing conventional speech recognition systems that use a combination of
neural network and hidden Markov model. Recent advancements in general-purpose
graphics processing units (GPGPUs) enable intelligent devices to run deep
neural networks in real-time. Thus, state-of-the-art speech recognition systems
have rapidly shifted from the paradigm of composite subsystems optimization to
the paradigm of end-to-end optimization. However, a low-power embedded GPGPU
cannot run these speech recognition systems in real-time. In this paper, we
show the development of deep convolutional neural networks (CNN) for speech
control of prosthetic hands that run in real-time on a NVIDIA Jetson TX2
developer kit. First, the device captures and converts speech into 2D features
(like spectrogram). The CNN receives the 2D features and classifies the hand
gestures. Finally, the hand gesture classes are sent to the prosthetic hand
motion control system. The whole system is written in Python with Keras, a deep
learning library that has a TensorFlow backend. Our experiments on the CNN
demonstrate the 91% accuracy and 2ms running time of hand gestures (text
output) from speech commands, which can be used to control the prosthetic hands
in real-time.",arxiv
http://arxiv.org/abs/1812.00825v2,2018-12-04T05:36:36Z,2018-11-21T21:02:50Z,"Microscope 2.0: An Augmented Reality Microscope with Real-time
  Artificial Intelligence Integration","The brightfield microscope is instrumental in the visual examination of both
biological and physical samples at sub-millimeter scales. One key clinical
application has been in cancer histopathology, where the microscopic assessment
of the tissue samples is used for the diagnosis and staging of cancer and thus
guides clinical therapy. However, the interpretation of these samples is
inherently subjective, resulting in significant diagnostic variability.
Moreover, in many regions of the world, access to pathologists is severely
limited due to lack of trained personnel. In this regard, Artificial
Intelligence (AI) based tools promise to improve the access and quality of
healthcare. However, despite significant advances in AI research, integration
of these tools into real-world cancer diagnosis workflows remains challenging
because of the costs of image digitization and difficulties in deploying AI
solutions. Here we propose a cost-effective solution to the integration of AI:
the Augmented Reality Microscope (ARM). The ARM overlays AI-based information
onto the current view of the sample through the optical pathway in real-time,
enabling seamless integration of AI into the regular microscopy workflow. We
demonstrate the utility of ARM in the detection of lymph node metastases in
breast cancer and the identification of prostate cancer with a latency that
supports real-time workflows. We anticipate that ARM will remove barriers
towards the use of AI in microscopic analysis and thus improve the accuracy and
efficiency of cancer diagnosis. This approach is applicable to other microscopy
tasks and AI algorithms in the life sciences and beyond.",arxiv
http://arxiv.org/abs/1909.06493v1,2019-09-14T00:35:21Z,2019-09-14T00:35:21Z,Flight Controller Synthesis Via Deep Reinforcement Learning,"Traditional control methods are inadequate in many deployment settings
involving control of Cyber-Physical Systems (CPS). In such settings, CPS
controllers must operate and respond to unpredictable interactions, conditions,
or failure modes. Dealing with such unpredictability requires the use of
executive and cognitive control functions that allow for planning and
reasoning. Motivated by the sport of drone racing, this dissertation addresses
these concerns for state-of-the-art flight control by investigating the use of
deep neural networks to bring essential elements of higher-level cognition for
constructing low level flight controllers.
  This thesis reports on the development and release of an open source, full
solution stack for building neuro-flight controllers. This stack consists of
the methodology for constructing a multicopter digital twin for synthesize the
flight controller unique to a specific aircraft, a tuning framework for
implementing training environments (GymFC), and a firmware for the world's
first neural network supported flight controller (Neuroflight). GymFC's novel
approach fuses together the digital twinning paradigm for flight control
training to provide seamless transfer to hardware. Additionally, this thesis
examines alternative reward system functions as well as changes to the software
environment to bridge the gap between the simulation and real world deployment
environments.
  Work summarized in this thesis demonstrates that reinforcement learning is
able to be leveraged for training neural network controllers capable, not only
of maintaining stable flight, but also precision aerobatic maneuvers in real
world settings. As such, this work provides a foundation for developing the
next generation of flight control systems.",arxiv
http://arxiv.org/abs/2106.14739v1,2021-06-28T14:11:48Z,2021-06-28T14:11:48Z,"Real-Time Human Pose Estimation on a Smart Walker using Convolutional
  Neural Networks","Rehabilitation is important to improve quality of life for mobility-impaired
patients. Smart walkers are a commonly used solution that should embed
automatic and objective tools for data-driven human-in-the-loop control and
monitoring. However, present solutions focus on extracting few specific metrics
from dedicated sensors with no unified full-body approach. We investigate a
general, real-time, full-body pose estimation framework based on two RGB+D
camera streams with non-overlapping views mounted on a smart walker equipment
used in rehabilitation. Human keypoint estimation is performed using a
two-stage neural network framework. The 2D-Stage implements a detection module
that locates body keypoints in the 2D image frames. The 3D-Stage implements a
regression module that lifts and relates the detected keypoints in both cameras
to the 3D space relative to the walker. Model predictions are low-pass filtered
to improve temporal consistency. A custom acquisition method was used to obtain
a dataset, with 14 healthy subjects, used for training and evaluating the
proposed framework offline, which was then deployed on the real walker
equipment. An overall keypoint detection error of 3.73 pixels for the 2D-Stage
and 44.05mm for the 3D-Stage were reported, with an inference time of 26.6ms
when deployed on the constrained hardware of the walker. We present a novel
approach to patient monitoring and data-driven human-in-the-loop control in the
context of smart walkers. It is able to extract a complete and compact body
representation in real-time and from inexpensive sensors, serving as a common
base for downstream metrics extraction solutions, and Human-Robot interaction
applications. Despite promising results, more data should be collected on users
with impairments, to assess its performance as a rehabilitation tool in
real-world scenarios.",arxiv
http://arxiv.org/abs/2011.14966v1,2020-11-30T16:38:18Z,2020-11-30T16:38:18Z,"Depression Status Estimation by Deep Learning based Hybrid Multi-Modal
  Fusion Model","Preliminary detection of mild depression could immensely help in effective
treatment of the common mental health disorder. Due to the lack of proper
awareness and the ample mix of stigmas and misconceptions present within the
society, mental health status estimation has become a truly difficult task. Due
to the immense variations in character level traits from person to person,
traditional deep learning methods fail to generalize in a real world setting.
In our study we aim to create a human allied AI workflow which could
efficiently adapt to specific users and effectively perform in real world
scenarios. We propose a Hybrid deep learning approach that combines the essence
of one shot learning, classical supervised deep learning methods and human
allied interactions for adaptation. In order to capture maximum information and
make efficient diagnosis video, audio, and text modalities are utilized. Our
Hybrid Fusion model achieved a high accuracy of 96.3% on the Dataset; and
attained an AUC of 0.9682 which proves its robustness in discriminating classes
in complex real-world scenarios making sure that no cases of mild depression
are missed during diagnosis. The proposed method is deployed in a cloud-based
smartphone application for robust testing. With user-specific adaptations and
state of the art methodologies, we present a state-of-the-art model with user
friendly experience.",arxiv
http://arxiv.org/abs/2009.10679v1,2020-09-22T16:55:44Z,2020-09-22T16:55:44Z,"An embedded deep learning system for augmented reality in firefighting
  applications","Firefighting is a dynamic activity, in which numerous operations occur
simultaneously. Maintaining situational awareness (i.e., knowledge of current
conditions and activities at the scene) is critical to the accurate
decision-making necessary for the safe and successful navigation of a fire
environment by firefighters. Conversely, the disorientation caused by hazards
such as smoke and extreme heat can lead to injury or even fatality. This
research implements recent advancements in technology such as deep learning,
point cloud and thermal imaging, and augmented reality platforms to improve a
firefighter's situational awareness and scene navigation through improved
interpretation of that scene. We have designed and built a prototype embedded
system that can leverage data streamed from cameras built into a firefighter's
personal protective equipment (PPE) to capture thermal, RGB color, and depth
imagery and then deploy already developed deep learning models to analyze the
input data in real time. The embedded system analyzes and returns the processed
images via wireless streaming, where they can be viewed remotely and relayed
back to the firefighter using an augmented reality platform that visualizes the
results of the analyzed inputs and draws the firefighter's attention to objects
of interest, such as doors and windows otherwise invisible through smoke and
flames.",arxiv
http://arxiv.org/abs/2012.12104v1,2020-12-09T05:08:41Z,2020-12-09T05:08:41Z,"A Deep Reinforcement Learning Approach for Ramp Metering Based on
  Traffic Video Data","Ramp metering that uses traffic signals to regulate vehicle flows from the
on-ramps has been widely implemented to improve vehicle mobility of the
freeway. Previous studies generally update signal timings in real-time based on
predefined traffic measures collected by point detectors, such as traffic
volumes and occupancies. Comparing with point detectors, traffic cameras-which
have been increasingly deployed on road networks-could cover larger areas and
provide more detailed traffic information. In this work, we propose a deep
reinforcement learning (DRL) method to explore the potential of traffic video
data in improving the efficiency of ramp metering. The proposed method uses
traffic video frames as inputs and learns the optimal control strategies
directly from the high-dimensional visual inputs. A real-world case study
demonstrates that, in comparison with a state-of-the-practice method, the
proposed DRL method results in 1) lower travel times in the mainline, 2)
shorter vehicle queues at the on-ramp, and 3) higher traffic flows downstream
of the merging area. The results suggest that the proposed method is able to
extract useful information from the video data for better ramp metering
controls.",arxiv
http://arxiv.org/abs/2101.02000v1,2021-01-06T13:15:21Z,2021-01-06T13:15:21Z,Weakly-Supervised Multi-Face 3D Reconstruction,"3D face reconstruction plays a very important role in many real-world
multimedia applications, including digital entertainment, social media,
affection analysis, and person identification. The de-facto pipeline for
estimating the parametric face model from an image requires to firstly detect
the facial regions with landmarks, and then crop each face to feed the deep
learning-based regressor. Comparing to the conventional methods performing
forward inference for each detected instance independently, we suggest an
effective end-to-end framework for multi-face 3D reconstruction, which is able
to predict the model parameters of multiple instances simultaneously using
single network inference. Our proposed approach not only greatly reduces the
computational redundancy in feature extraction but also makes the deployment
procedure much easier using the single network model. More importantly, we
employ the same global camera model for the reconstructed faces in each image,
which makes it possible to recover the relative head positions and orientations
in the 3D scene. We have conducted extensive experiments to evaluate our
proposed approach on the sparse and dense face alignment tasks. The
experimental results indicate that our proposed approach is very promising on
face alignment tasks without fully-supervision and pre-processing like
detection and crop. Our implementation is publicly available at
\url{https://github.com/kalyo-zjl/WM3DR}.",arxiv
http://arxiv.org/abs/2111.01777v1,2021-11-02T17:53:54Z,2021-11-02T17:53:54Z,"A Framework for Real-World Multi-Robot Systems Running Decentralized
  GNN-Based Policies","Graph Neural Networks (GNNs) are a paradigm-shifting neural architecture to
facilitate the learning of complex multi-agent behaviors. Recent work has
demonstrated remarkable performance in tasks such as flocking, multi-agent path
planning and cooperative coverage. However, the policies derived through
GNN-based learning schemes have not yet been deployed to the real-world on
physical multi-robot systems. In this work, we present the design of a system
that allows for fully decentralized execution of GNN-based policies. We create
a framework based on ROS2 and elaborate its details in this paper. We
demonstrate our framework on a case-study that requires tight coordination
between robots, and present first-of-a-kind results that show successful
real-world deployment of GNN-based policies on a decentralized multi-robot
system relying on Adhoc communication. A video demonstration of this case-study
can be found online. https://www.youtube.com/watch?v=COh-WLn4iO4",arxiv
http://arxiv.org/abs/2006.15350v1,2020-06-27T12:13:22Z,2020-06-27T12:13:22Z,"MiniNet: An extremely lightweight convolutional neural network for
  real-time unsupervised monocular depth estimation","Predicting depth from a single image is an attractive research topic since it
provides one more dimension of information to enable machines to better
perceive the world. Recently, deep learning has emerged as an effective
approach to monocular depth estimation. As obtaining labeled data is costly,
there is a recent trend to move from supervised learning to unsupervised
learning to obtain monocular depth. However, most unsupervised learning methods
capable of achieving high depth prediction accuracy will require a deep network
architecture which will be too heavy and complex to run on embedded devices
with limited storage and memory spaces. To address this issue, we propose a new
powerful network with a recurrent module to achieve the capability of a deep
network while at the same time maintaining an extremely lightweight size for
real-time high performance unsupervised monocular depth prediction from video
sequences. Besides, a novel efficient upsample block is proposed to fuse the
features from the associated encoder layer and recover the spatial size of
features with the small number of model parameters. We validate the
effectiveness of our approach via extensive experiments on the KITTI dataset.
Our new model can run at a speed of about 110 frames per second (fps) on a
single GPU, 37 fps on a single CPU, and 2 fps on a Raspberry Pi 3. Moreover, it
achieves higher depth accuracy with nearly 33 times fewer model parameters than
state-of-the-art models. To the best of our knowledge, this work is the first
extremely lightweight neural network trained on monocular video sequences for
real-time unsupervised monocular depth estimation, which opens up the
possibility of implementing deep learning-based real-time unsupervised
monocular depth prediction on low-cost embedded devices.",arxiv
http://arxiv.org/abs/2005.08076v1,2020-05-16T19:42:16Z,2020-05-16T19:42:16Z,"A Deep Learning based Wearable Healthcare IoT Device for AI-enabled
  Hearing Assistance Automation","With the recent booming of artificial intelligence (AI), particularly deep
learning techniques, digital healthcare is one of the prevalent areas that
could gain benefits from AI-enabled functionality. This research presents a
novel AI-enabled Internet of Things (IoT) device operating from the ESP-8266
platform capable of assisting those who suffer from impairment of hearing or
deafness to communicate with others in conversations. In the proposed solution,
a server application is created that leverages Google's online speech
recognition service to convert the received conversations into texts, then
deployed to a micro-display attached to the glasses to display the conversation
contents to deaf people, to enable and assist conversation as normal with the
general population. Furthermore, in order to raise alert of traffic or
dangerous scenarios, an 'urban-emergency' classifier is developed using a deep
learning model, Inception-v4, with transfer learning to detect/recognize
alerting/alarming sounds, such as a horn sound or a fire alarm, with texts
generated to alert the prospective user. The training of Inception-v4 was
carried out on a consumer desktop PC and then implemented into the AI based IoT
application. The empirical results indicate that the developed prototype system
achieves an accuracy rate of 92% for sound recognition and classification with
real-time performance.",arxiv
http://arxiv.org/abs/1605.02097v2,2016-09-20T19:12:49Z,2016-05-06T20:46:34Z,"ViZDoom: A Doom-based AI Research Platform for Visual Reinforcement
  Learning","The recent advances in deep neural networks have led to effective
vision-based reinforcement learning methods that have been employed to obtain
human-level controllers in Atari 2600 games from pixel data. Atari 2600 games,
however, do not resemble real-world tasks since they involve non-realistic 2D
environments and the third-person perspective. Here, we propose a novel
test-bed platform for reinforcement learning research from raw visual
information which employs the first-person perspective in a semi-realistic 3D
world. The software, called ViZDoom, is based on the classical first-person
shooter video game, Doom. It allows developing bots that play the game using
the screen buffer. ViZDoom is lightweight, fast, and highly customizable via a
convenient mechanism of user scenarios. In the experimental part, we test the
environment by trying to learn bots for two scenarios: a basic move-and-shoot
task and a more complex maze-navigation problem. Using convolutional deep
neural networks with Q-learning and experience replay, for both scenarios, we
were able to train competent bots, which exhibit human-like behaviors. The
results confirm the utility of ViZDoom as an AI research platform and imply
that visual reinforcement learning in 3D realistic first-person perspective
environments is feasible.",arxiv
http://arxiv.org/abs/2010.11884v1,2020-10-22T17:20:38Z,2020-10-22T17:20:38Z,"AEGIS: A real-time multimodal augmented reality computer vision based
  system to assist facial expression recognition for individuals with autism
  spectrum disorder","The ability to interpret social cues comes naturally for most people, but for
those living with Autism Spectrum Disorder (ASD), some experience a deficiency
in this area. This paper presents the development of a multimodal augmented
reality (AR) system which combines the use of computer vision and deep
convolutional neural networks (CNN) in order to assist individuals with the
detection and interpretation of facial expressions in social settings. The
proposed system, which we call AEGIS (Augmented-reality Expression Guided
Interpretation System), is an assistive technology deployable on a variety of
user devices including tablets, smartphones, video conference systems, or
smartglasses, showcasing its extreme flexibility and wide range of use cases,
to allow integration into daily life with ease. Given a streaming video camera
source, each real-world frame is passed into AEGIS, processed for facial
bounding boxes, and then fed into our novel deep convolutional time windowed
neural network (TimeConvNet). We leverage both spatial and temporal information
in order to provide an accurate expression prediction, which is then converted
into its corresponding visualization and drawn on top of the original video
frame. The system runs in real-time, requires minimal set up and is simple to
use. With the use of AEGIS, we can assist individuals living with ASD to learn
to better identify expressions and thus improve their social experiences.",arxiv
http://arxiv.org/abs/2110.04697v1,2021-10-10T03:51:39Z,2021-10-10T03:51:39Z,"An Augmented Reality Platform for Introducing Reinforcement Learning to
  K-12 Students with Robots","Interactive reinforcement learning, where humans actively assist during an
agent's learning process, has the promise to alleviate the sample complexity
challenges of practical algorithms. However, the inner workings and state of
the robot are typically hidden from the teacher when humans provide feedback.
To create a common ground between the human and the learning robot, in this
paper, we propose an Augmented Reality (AR) system that reveals the hidden
state of the learning to the human users. This paper describes our system's
design and implementation and concludes with a discussion on two directions for
future work which we are pursuing: 1) use of our system in AI education
activities at the K-12 level; and 2) development of a framework for an AR-based
human-in-the-loop reinforcement learning, where the human teacher can see
sensory and cognitive representations of the robot overlaid in the real world.",arxiv
http://arxiv.org/abs/2107.05989v1,2021-07-13T11:17:00Z,2021-07-13T11:17:00Z,"Emotion Recognition for Healthcare Surveillance Systems Using Neural
  Networks: A Survey","Recognizing the patient's emotions using deep learning techniques has
attracted significant attention recently due to technological advancements.
Automatically identifying the emotions can help build smart healthcare centers
that can detect depression and stress among the patients in order to start the
medication early. Using advanced technology to identify emotions is one of the
most exciting topics as it defines the relationships between humans and
machines. Machines learned how to predict emotions by adopting various methods.
In this survey, we present recent research in the field of using neural
networks to recognize emotions. We focus on studying emotions' recognition from
speech, facial expressions, and audio-visual input and show the different
techniques of deploying these algorithms in the real world. These three emotion
recognition techniques can be used as a surveillance system in healthcare
centers to monitor patients. We conclude the survey with a presentation of the
challenges and the related future work to provide an insight into the
applications of using emotion recognition.",arxiv
http://arxiv.org/abs/2007.10629v1,2020-07-21T07:22:33Z,2020-07-21T07:22:33Z,"SLNSpeech: solving extended speech separation problem by the help of
  sign language","A speech separation task can be roughly divided into audio-only separation
and audio-visual separation. In order to make speech separation technology
applied in the real scenario of the disabled, this paper presents an extended
speech separation problem which refers in particular to sign language assisted
speech separation. However, most existing datasets for speech separation are
audios and videos which contain audio and/or visual modalities. To address the
extended speech separation problem, we introduce a large-scale dataset named
Sign Language News Speech (SLNSpeech) dataset in which three modalities of
audio, visual, and sign language are coexisted. Then, we design a general deep
learning network for the self-supervised learning of three modalities,
particularly, using sign language embeddings together with audio or
audio-visual information for better solving the speech separation task.
Specifically, we use 3D residual convolutional network to extract sign language
features and use pretrained VGGNet model to exact visual features. After that,
an improved U-Net with skip connections in feature extraction stage is applied
for learning the embeddings among the mixed spectrogram transformed from source
audios, the sign language features and visual features. Experiments results
show that, besides visual modality, sign language modality can also be used
alone to supervise speech separation task. Moreover, we also show the
effectiveness of sign language assisted speech separation when the visual
modality is disturbed. Source code will be released in
http://cheertt.top/homepage/",arxiv
http://arxiv.org/abs/1910.11450v1,2019-10-24T23:00:12Z,2019-10-24T23:00:12Z,An Empirical Study of Efficient ASR Rescoring with Transformers,"Neural language models (LMs) have been proved to significantly outperform
classical n-gram LMs for language modeling due to their superior abilities to
model long-range dependencies in text and handle data sparsity problems. And
recently, well configured deep Transformers have exhibited superior performance
over shallow stack of recurrent neural network layers for language modeling.
However, these state-of-the-art deep Transformer models were mostly engineered
to be deep with high model capacity, which makes it computationally inefficient
and challenging to be deployed into large-scale real-world applications.
Therefore, it is important to develop Transformer LMs that have relatively
small model sizes, while still retaining good performance of those much larger
models. In this paper, we aim to conduct empirical study on training
Transformers with small parameter sizes in the context of ASR rescoring. By
combining techniques including subword units, adaptive softmax, large-scale
model pre-training, and knowledge distillation, we show that we are able to
successfully train small Transformer LMs with significant relative word error
rate reductions (WERR) through n-best rescoring. In particular, our experiments
on a video speech recognition dataset show that we are able to achieve WERRs
ranging from 6.46% to 7.17% while only with 5.5% to 11.9% parameter sizes of
the well-known large GPT model [1], whose WERR with rescoring on the same
dataset is 7.58%.",arxiv
http://arxiv.org/abs/2103.16938v1,2021-03-31T09:43:38Z,2021-03-31T09:43:38Z,"Unpaired Single-Image Depth Synthesis with cycle-consistent Wasserstein
  GANs","Real-time estimation of actual environment depth is an essential module for
various autonomous system tasks such as localization, obstacle detection and
pose estimation. During the last decade of machine learning, extensive
deployment of deep learning methods to computer vision tasks yielded successful
approaches for realistic depth synthesis out of a simple RGB modality. While
most of these models rest on paired depth data or availability of video
sequences and stereo images, there is a lack of methods facing single-image
depth synthesis in an unsupervised manner. Therefore, in this study, latest
advancements in the field of generative neural networks are leveraged to fully
unsupervised single-image depth synthesis. To be more exact, two
cycle-consistent generators for RGB-to-depth and depth-to-RGB transfer are
implemented and simultaneously optimized using the Wasserstein-1 distance. To
ensure plausibility of the proposed method, we apply the models to a self
acquised industrial data set as well as to the renown NYU Depth v2 data set,
which allows comparison with existing approaches. The observed success in this
study suggests high potential for unpaired single-image depth estimation in
real world applications.",arxiv
http://arxiv.org/abs/2002.08242v1,2020-02-11T08:23:14Z,2020-02-11T08:23:14Z,AI Online Filters to Real World Image Recognition,"Deep artificial neural networks, trained with labeled data sets are widely
used in numerous vision and robotics applications today. In terms of AI, these
are called reflex models, referring to the fact that they do not self-evolve or
actively adapt to environmental changes. As demand for intelligent robot
control expands to many high level tasks, reinforcement learning and state
based models play an increasingly important role. Herein, in computer vision
and robotics domain, we study a novel approach to add reinforcement controls
onto the image recognition reflex models to attain better overall performance,
specifically to a wider environment range beyond what is expected of the task
reflex models. Follow a common infrastructure with environment sensing and AI
based modeling of self-adaptive agents, we implement multiple types of AI
control agents. To the end, we provide comparative results of these agents with
baseline, and an insightful analysis of their benefit to improve overall image
recognition performance in real world.",arxiv
http://arxiv.org/abs/2110.09625v1,2021-10-18T21:21:23Z,2021-10-18T21:21:23Z,Personalized Speech Enhancement: New Models and Comprehensive Evaluation,"Personalized speech enhancement (PSE) models utilize additional cues, such as
speaker embeddings like d-vectors, to remove background noise and interfering
speech in real-time and thus improve the speech quality of online video
conferencing systems for various acoustic scenarios. In this work, we propose
two neural networks for PSE that achieve superior performance to the previously
proposed VoiceFilter. In addition, we create test sets that capture a variety
of scenarios that users can encounter during video conferencing. Furthermore,
we propose a new metric to measure the target speaker over-suppression (TSOS)
problem, which was not sufficiently investigated before despite its critical
importance in deployment. Besides, we propose multi-task training with a speech
recognition back-end. Our results show that the proposed models can yield
better speech recognition accuracy, speech intelligibility, and perceptual
quality than the baseline models, and the multi-task training can alleviate the
TSOS issue in addition to improving the speech recognition accuracy.",arxiv
http://arxiv.org/abs/1510.03727v1,2015-10-13T15:06:03Z,2015-10-13T15:06:03Z,SemanticPaint: A Framework for the Interactive Segmentation of 3D Scenes,"We present an open-source, real-time implementation of SemanticPaint, a
system for geometric reconstruction, object-class segmentation and learning of
3D scenes. Using our system, a user can walk into a room wearing a depth camera
and a virtual reality headset, and both densely reconstruct the 3D scene and
interactively segment the environment into object classes such as 'chair',
'floor' and 'table'. The user interacts physically with the real-world scene,
touching objects and using voice commands to assign them appropriate labels.
These user-generated labels are leveraged by an online random forest-based
machine learning algorithm, which is used to predict labels for previously
unseen parts of the scene. The entire pipeline runs in real time, and the user
stays 'in the loop' throughout the process, receiving immediate feedback about
the progress of the labelling and interacting with the scene as necessary to
refine the predicted segmentation.",arxiv
http://arxiv.org/abs/2108.01704v1,2021-08-03T18:58:39Z,2021-08-03T18:58:39Z,"Bifocal Neural ASR: Exploiting Keyword Spotting for Inference
  Optimization","We present Bifocal RNN-T, a new variant of the Recurrent Neural Network
Transducer (RNN-T) architecture designed for improved inference time latency on
speech recognition tasks. The architecture enables a dynamic pivot for its
runtime compute pathway, namely taking advantage of keyword spotting to select
which component of the network to execute for a given audio frame. To
accomplish this, we leverage a recurrent cell we call the Bifocal LSTM
(BFLSTM), which we detail in the paper. The architecture is compatible with
other optimization strategies such as quantization, sparsification, and
applying time-reduction layers, making it especially applicable for deployed,
real-time speech recognition settings. We present the architecture and report
comparative experimental results on voice-assistant speech recognition tasks.
Specifically, we show our proposed Bifocal RNN-T can improve inference cost by
29.1% with matching word error rates and only a minor increase in memory size.",arxiv
http://arxiv.org/abs/1812.03519v1,2018-12-09T16:44:56Z,2018-12-09T16:44:56Z,Deep-Net: Deep Neural Network for Cyber Security Use Cases,"Deep neural networks (DNNs) have witnessed as a powerful approach in this
year by solving long-standing Artificial intelligence (AI) supervised and
unsupervised tasks exists in natural language processing, speech processing,
computer vision and others. In this paper, we attempt to apply DNNs on three
different cyber security use cases: Android malware classification, incident
detection and fraud detection. The data set of each use case contains real
known benign and malicious activities samples. The efficient network
architecture for DNN is chosen by conducting various trails of experiments for
network parameters and network structures. The experiments of such chosen
efficient configurations of DNNs are run up to 1000 epochs with learning rate
set in the range [0.01-0.5]. Experiments of DNN performed well in comparison to
the classical machine learning algorithms in all cases of experiments of cyber
security use cases. This is due to the fact that DNNs implicitly extract and
build better features, identifies the characteristics of the data that lead to
better accuracy. The best accuracy obtained by DNN and XGBoost on Android
malware classification 0.940 and 0.741, incident detection 1.00 and 0.997 fraud
detection 0.972 and 0.916 respectively.",arxiv
http://arxiv.org/abs/1607.06854v3,2016-09-30T17:41:41Z,2016-07-22T22:13:04Z,"Unsupervised Learning from Continuous Video in a Scalable Predictive
  Recurrent Network","Understanding visual reality involves acquiring common-sense knowledge about
countless regularities in the visual world, e.g., how illumination alters the
appearance of objects in a scene, and how motion changes their apparent spatial
relationship. These regularities are hard to label for training supervised
machine learning algorithms; consequently, algorithms need to learn these
regularities from the real world in an unsupervised way. We present a novel
network meta-architecture that can learn world dynamics from raw, continuous
video. The components of this network can be implemented using any algorithm
that possesses three key capabilities: prediction of a signal over time,
reduction of signal dimensionality (compression), and the ability to use
supplementary contextual information to inform the prediction. The presented
architecture is highly-parallelized and scalable, and is implemented using
localized connectivity, processing, and learning. We demonstrate an
implementation of this architecture where the components are built from
multi-layer perceptrons. We apply the implementation to create a system capable
of stable and robust visual tracking of objects as seen by a moving camera.
Results show performance on par with or exceeding state-of-the-art tracking
algorithms. The tracker can be trained in either fully supervised or
unsupervised-then-briefly-supervised regimes. Success of the briefly-supervised
regime suggests that the unsupervised portion of the model extracts useful
information about visual reality. The results suggest a new class of AI
algorithms that uniquely combine prediction and scalability in a way that makes
them suitable for learning from and --- and eventually acting within --- the
real world.",arxiv
http://arxiv.org/abs/2105.13331v2,2021-09-23T16:04:49Z,2021-05-27T17:39:06Z,Quantization and Deployment of Deep Neural Networks on Microcontrollers,"Embedding Artificial Intelligence onto low-power devices is a challenging
task that has been partly overcome with recent advances in machine learning and
hardware design. Presently, deep neural networks can be deployed on embedded
targets to perform different tasks such as speech recognition,object detection
or Human Activity Recognition. However, there is still room for optimization of
deep neural networks onto embedded devices. These optimizations mainly address
power consumption,memory and real-time constraints, but also an easier
deployment at the edge. Moreover, there is still a need for a better
understanding of what can be achieved for different use cases. This work
focuses on quantization and deployment of deep neural networks onto low-power
32-bit microcontrollers. The quantization methods, relevant in the context of
an embedded execution onto a microcontroller, are first outlined. Then, a new
framework for end-to-end deep neural networks training, quantization and
deployment is presented. This framework, called MicroAI, is designed as an
alternative to existing inference engines (TensorFlow Lite for Microcontrollers
and STM32CubeAI). Our framework can indeed be easily adjusted and/or extended
for specific use cases. Execution using single precision 32-bit floating-point
as well as fixed-point on 8- and 16-bit integers are supported. The proposed
quantization method is evaluated with three different datasets (UCI-HAR, Spoken
MNIST and GTSRB). Finally, a comparison study between MicroAI and both existing
embedded inference engines is provided in terms of memory and power efficiency.
On-device evaluation is done using ARM Cortex-M4F-based microcontrollers (Ambiq
Apollo3 and STM32L452RE).",arxiv
http://arxiv.org/abs/1804.09997v1,2018-04-26T11:37:03Z,2018-04-26T11:37:03Z,PANDA: Facilitating Usable AI Development,"Recent advances in artificial intelligence (AI) and machine learning have
created a general perception that AI could be used to solve complex problems,
and in some situations over-hyped as a tool that can be so easily used.
Unfortunately, the barrier to realization of mass adoption of AI on various
business domains is too high because most domain experts have no background in
AI. Developing AI applications involves multiple phases, namely data
preparation, application modeling, and product deployment. The effort of AI
research has been spent mostly on new AI models (in the model training stage)
to improve the performance of benchmark tasks such as image recognition. Many
other factors such as usability, efficiency and security of AI have not been
well addressed, and therefore form a barrier to democratizing AI. Further, for
many real world applications such as healthcare and autonomous driving,
learning via huge amounts of possibility exploration is not feasible since
humans are involved. In many complex applications such as healthcare, subject
matter experts (e.g. Clinicians) are the ones who appreciate the importance of
features that affect health, and their knowledge together with existing
knowledge bases are critical to the end results. In this paper, we take a new
perspective on developing AI solutions, and present a solution for making AI
usable. We hope that this resolution will enable all subject matter experts
(eg. Clinicians) to exploit AI like data scientists.",arxiv
http://arxiv.org/abs/1706.08658v1,2017-06-27T03:21:47Z,2017-06-27T03:21:47Z,Fast and accurate classification of echocardiograms using deep learning,"Echocardiography is essential to modern cardiology. However, human
interpretation limits high throughput analysis, limiting echocardiography from
reaching its full clinical and research potential for precision medicine. Deep
learning is a cutting-edge machine-learning technique that has been useful in
analyzing medical images but has not yet been widely applied to
echocardiography, partly due to the complexity of echocardiograms' multi view,
multi modality format. The essential first step toward comprehensive computer
assisted echocardiographic interpretation is determining whether computers can
learn to recognize standard views. To this end, we anonymized 834,267
transthoracic echocardiogram (TTE) images from 267 patients (20 to 96 years, 51
percent female, 26 percent obese) seen between 2000 and 2017 and labeled them
according to standard views. Images covered a range of real world clinical
variation. We built a multilayer convolutional neural network and used
supervised learning to simultaneously classify 15 standard views. Eighty
percent of data used was randomly chosen for training and 20 percent reserved
for validation and testing on never seen echocardiograms. Using multiple images
from each clip, the model classified among 12 video views with 97.8 percent
overall test accuracy without overfitting. Even on single low resolution
images, test accuracy among 15 views was 91.7 percent versus 70.2 to 83.5
percent for board-certified echocardiographers. Confusional matrices, occlusion
experiments, and saliency mapping showed that the model finds recognizable
similarities among related views and classifies using clinically relevant image
features. In conclusion, deep neural networks can classify essential
echocardiographic views simultaneously and with high accuracy. Our results
provide a foundation for more complex deep learning assisted echocardiographic
interpretation.",arxiv
http://arxiv.org/abs/2103.10804v1,2021-03-19T13:48:25Z,2021-03-19T13:48:25Z,"Enhancing Human-in-the-Loop Adaptive Systems through Digital Twins and
  VR Interfaces","Self-adaptation approaches usually rely on closed-loop controllers that avoid
human intervention from adaptation. While such fully automated approaches have
proven successful in many application domains, there are situations where human
involvement in the adaptation process is beneficial or even necessary. For such
""human-in-the-loop"" adaptive systems, two major challenges, namely transparency
and controllability, have to be addressed to include the human in the
self-adaptation loop. Transparency means that relevant context information
about the adaptive systems and its context is represented based on a digital
twin enabling the human an immersive and realistic view. Concerning
controllability, the decision-making and adaptation operations should be
managed in a natural and interactive way. As existing human-in-the-loop
adaptation approaches do not fully cover these aspects, we investigate
alternative human-in-the-loop strategies by using a combination of digital
twins and virtual reality (VR) interfaces. Based on the concept of the digital
twin, we represent a self-adaptive system and its respective context in a
virtual environment. With the help of a VR interface, we support an immersive
and realistic human involvement in the self-adaptation loop by mirroring the
physical entities of the real world to the VR interface. For integrating the
human in the decision-making and adaptation process, we have implemented and
analyzed two different human-in-the-loop strategies in VR: a procedural control
where the human can control the decision making-process and adaptations through
VR interactions (human-controlled) and a declarative control where the human
specifies the goal state and the configuration is delegated to an AI planner
(mixed-initiative). We illustrate and evaluate our approach based on an
autonomic robot system that is accessible and controlled through a VR
interface.",arxiv
http://arxiv.org/abs/2102.07955v1,2021-02-16T04:27:19Z,2021-02-16T04:27:19Z,"Deep Learning based Multi-Source Localization with Source Splitting and
  its Effectiveness in Multi-Talker Speech Recognition","Multi-source localization is an important and challenging technique for
multi-talker conversation analysis. This paper proposes a novel supervised
learning method using deep neural networks to estimate the direction of arrival
(DOA) of all the speakers simultaneously from the audio mixture. At the heart
of the proposal is a source splitting mechanism that creates source-specific
intermediate representations inside the network. This allows our model to give
source-specific posteriors as the output unlike the traditional multi-label
classification approach. Existing deep learning methods perform a frame level
prediction, whereas our approach performs an utterance level prediction by
incorporating temporal selection and averaging inside the network to avoid
post-processing. We also experiment with various loss functions and show that a
variant of earth mover distance (EMD) is very effective in classifying DOA at a
very high resolution by modeling inter-class relationships. In addition to
using the prediction error as a metric for evaluating our localization model,
we also establish its potency as a frontend with automatic speech recognition
(ASR) as the downstream task. We convert the estimated DOAs into a feature
suitable for ASR and pass it as an additional input feature to a strong
multi-channel and multi-talker speech recognition baseline. This added input
feature drastically improves the ASR performance and gives a word error rate
(WER) of 6.3% on the evaluation data of our simulated noisy two speaker
mixtures, while the baseline which doesn't use explicit localization input has
a WER of 11.5%. We also perform ASR evaluation on real recordings with the
overlapped set of the MC-WSJ-AV corpus in addition to simulated mixtures.",arxiv
http://arxiv.org/abs/2107.14569v2,2021-10-23T14:56:56Z,2021-07-30T12:08:16Z,Can You Hear It? Backdoor Attacks via Ultrasonic Triggers,"Deep neural networks represent a powerful approach for many real-world
applications due to their ability to model even complex data relations.
However, such neural networks can also be prohibitively expensive to train,
making it common to either outsource the training process to third parties or
use pretrained neural networks. Unfortunately, such practices make neural
networks vulnerable to various attacks, where one attack is the backdoor
attack. In such an attack, the third party training the model may maliciously
inject hidden behaviors into the model. Then, if a particular input (called
trigger) is fed into a neural network, the network will respond with a wrong
result.
  In this work, we explore backdoor attacks for automatic speech recognition
systems where we inject inaudible triggers. By doing so, we make the backdoor
attack challenging to detect for legitimate users, and thus, potentially more
dangerous. We conduct experiments on two versions of a dataset and three neural
networks and explore the performance of our attack concerning the duration,
position, and type of the trigger. Our results indicate that less than 1% of
poisoned data is sufficient to deploy a backdoor attack and reach a 100% attack
success rate. Since the trigger is inaudible, it makes it without limitations
with respect to the duration of the signal, and we observed that even short,
non-continuous triggers result in highly successful attacks. Finally, we
conducted our attack in actual hardware and saw that a malicious party could
manipulate inference in an Android application by playing the inaudible trigger
over the air.",arxiv
http://arxiv.org/abs/2003.01668v3,2020-03-11T23:30:56Z,2020-03-03T17:49:49Z,Model Assertions for Monitoring and Improving ML Models,"ML models are increasingly deployed in settings with real world interactions
such as vehicles, but unfortunately, these models can fail in systematic ways.
To prevent errors, ML engineering teams monitor and continuously improve these
models. We propose a new abstraction, model assertions, that adapts the
classical use of program assertions as a way to monitor and improve ML models.
Model assertions are arbitrary functions over a model's input and output that
indicate when errors may be occurring, e.g., a function that triggers if an
object rapidly changes its class in a video. We propose methods of using model
assertions at all stages of ML system deployment, including runtime monitoring,
validating labels, and continuously improving ML models. For runtime
monitoring, we show that model assertions can find high confidence errors,
where a model returns the wrong output with high confidence, which
uncertainty-based monitoring techniques would not detect. For training, we
propose two methods of using model assertions. First, we propose a bandit-based
active learning algorithm that can sample from data flagged by assertions and
show that it can reduce labeling costs by up to 40% over traditional
uncertainty-based methods. Second, we propose an API for generating
""consistency assertions"" (e.g., the class change example) and weak labels for
inputs where the consistency assertions fail, and show that these weak labels
can improve relative model quality by up to 46%. We evaluate model assertions
on four real-world tasks with video, LIDAR, and ECG data.",arxiv
http://arxiv.org/abs/2010.13114v1,2020-10-25T13:26:48Z,2020-10-25T13:26:48Z,"Empowering Knowledge Distillation via Open Set Recognition for Robust 3D
  Point Cloud Classification","Real-world scenarios pose several challenges to deep learning based computer
vision techniques despite their tremendous success in research. Deeper models
provide better performance, but are challenging to deploy and knowledge
distillation allows us to train smaller models with minimal loss in
performance. The model also has to deal with open set samples from classes
outside the ones it was trained on and should be able to identify them as
unknown samples while classifying the known ones correctly. Finally, most
existing image recognition research focuses only on using two-dimensional
snapshots of the real world three-dimensional objects. In this work, we aim to
bridge these three research fields, which have been developed independently
until now, despite being deeply interrelated. We propose a joint Knowledge
Distillation and Open Set recognition training methodology for
three-dimensional object recognition. We demonstrate the effectiveness of the
proposed method via various experiments on how it allows us to obtain a much
smaller model, which takes a minimal hit in performance while being capable of
open set recognition for 3D point cloud data.",arxiv
http://arxiv.org/abs/1802.00265v4,2019-01-16T09:04:06Z,2018-02-01T12:42:02Z,VR-Goggles for Robots: Real-to-sim Domain Adaptation for Visual Control,"In this paper, we deal with the reality gap from a novel perspective,
targeting transferring Deep Reinforcement Learning (DRL) policies learned in
simulated environments to the real-world domain for visual control tasks.
Instead of adopting the common solutions to the problem by increasing the
visual fidelity of synthetic images output from simulators during the training
phase, we seek to tackle the problem by translating the real-world image
streams back to the synthetic domain during the deployment phase, to make the
robot feel at home. We propose this as a lightweight, flexible, and efficient
solution for visual control, as 1) no extra transfer steps are required during
the expensive training of DRL agents in simulation; 2) the trained DRL agents
will not be constrained to being deployable in only one specific real-world
environment; 3) the policy training and the transfer operations are decoupled,
and can be conducted in parallel. Besides this, we propose a simple yet
effective shift loss that is agnostic to the downstream task, to constrain the
consistency between subsequent frames which is important for consistent policy
outputs. We validate the shift loss for artistic style transfer for videos and
domain adaptation, and validate our visual control approach in indoor and
outdoor robotics experiments.",arxiv
http://arxiv.org/abs/1908.06752v1,2019-08-16T14:49:30Z,2019-08-16T14:49:30Z,Towards Generating Ambisonics Using Audio-Visual Cue for Virtual Reality,"Ambisonics i.e., a full-sphere surround sound, is quintessential with
360-degree visual content to provide a realistic virtual reality (VR)
experience. While 360-degree visual content capture gained a tremendous boost
recently, the estimation of corresponding spatial sound is still challenging
due to the required sound-field microphones or information about the
sound-source locations. In this paper, we introduce a novel problem of
generating Ambisonics in 360-degree videos using the audio-visual cue. With
this aim, firstly, a novel 360-degree audio-visual video dataset of 265 videos
is introduced with annotated sound-source locations. Secondly, a pipeline is
designed for an automatic Ambisonic estimation problem. Benefiting from the
deep learning-based audio-visual feature-embedding and prediction modules, our
pipeline estimates the 3D sound-source locations and further use such locations
to encode to the B-format. To benchmark our dataset and pipeline, we
additionally propose evaluation criteria to investigate the performance using
different 360-degree input representations. Our results demonstrate the
efficacy of the proposed pipeline and open up a new area of research in
360-degree audio-visual analysis for future investigations.",arxiv
http://arxiv.org/abs/1710.07368v1,2017-10-19T23:03:33Z,2017-10-19T23:03:33Z,"SqueezeSeg: Convolutional Neural Nets with Recurrent CRF for Real-Time
  Road-Object Segmentation from 3D LiDAR Point Cloud","In this paper, we address semantic segmentation of road-objects from 3D LiDAR
point clouds. In particular, we wish to detect and categorize instances of
interest, such as cars, pedestrians and cyclists. We formulate this problem as
a point- wise classification problem, and propose an end-to-end pipeline called
SqueezeSeg based on convolutional neural networks (CNN): the CNN takes a
transformed LiDAR point cloud as input and directly outputs a point-wise label
map, which is then refined by a conditional random field (CRF) implemented as a
recurrent layer. Instance-level labels are then obtained by conventional
clustering algorithms. Our CNN model is trained on LiDAR point clouds from the
KITTI dataset, and our point-wise segmentation labels are derived from 3D
bounding boxes from KITTI. To obtain extra training data, we built a LiDAR
simulator into Grand Theft Auto V (GTA-V), a popular video game, to synthesize
large amounts of realistic training data. Our experiments show that SqueezeSeg
achieves high accuracy with astonishingly fast and stable runtime (8.7 ms per
frame), highly desirable for autonomous driving applications. Furthermore,
additionally training on synthesized data boosts validation accuracy on
real-world data. Our source code and synthesized data will be open-sourced.",arxiv
http://arxiv.org/abs/2110.08307v1,2021-10-15T18:29:46Z,2021-10-15T18:29:46Z,GrowSpace: Learning How to Shape Plants,"Plants are dynamic systems that are integral to our existence and survival.
Plants face environment changes and adapt over time to their surrounding
conditions. We argue that plant responses to an environmental stimulus are a
good example of a real-world problem that can be approached within a
reinforcement learning (RL)framework. With the objective of controlling a plant
by moving the light source, we propose GrowSpace, as a new RL benchmark. The
back-end of the simulator is implemented using the Space Colonisation
Algorithm, a plant growing model based on competition for space. Compared to
video game RL environments, this simulator addresses a real-world problem and
serves as a test bed to visualize plant growth and movement in a faster way
than physical experiments. GrowSpace is composed of a suite of challenges that
tackle several problems such as control, multi-stage learning,fairness and
multi-objective learning. We provide agent baselines alongside case studies to
demonstrate the difficulty of the proposed benchmark.",arxiv
http://arxiv.org/abs/2105.11216v1,2021-05-24T11:51:46Z,2021-05-24T11:51:46Z,"CONECT4: Desarrollo de componentes basados en Realidad Mixta, Realidad
  Virtual Y Conocimiento Experto para generación de entornos de aprendizaje
  Hombre-Máquina","This work presents the results of project CONECT4, which addresses the
research and development of new non-intrusive communication methods for the
generation of a human-machine learning ecosystem oriented to predictive
maintenance in the automotive industry. Through the use of innovative
technologies such as Augmented Reality, Virtual Reality, Digital Twin and
expert knowledge, CONECT4 implements methodologies that allow improving the
efficiency of training techniques and knowledge management in industrial
companies. The research has been supported by the development of content and
systems with a low level of technological maturity that address solutions for
the industrial sector applied in training and assistance to the operator. The
results have been analyzed in companies in the automotive sector, however, they
are exportable to any other type of industrial sector. -- --
  En esta publicaci\'on se presentan los resultados del proyecto CONECT4, que
aborda la investigaci\'on y desarrollo de nuevos m\'etodos de comunicaci\'on no
intrusivos para la generaci\'on de un ecosistema de aprendizaje
hombre-m\'aquina orientado al mantenimiento predictivo en la industria de
automoci\'on. A trav\'es del uso de tecnolog\'ias innovadoras como la Realidad
Aumentada, la Realidad Virtual, el Gemelo Digital y conocimiento experto,
CONECT4 implementa metodolog\'ias que permiten mejorar la eficiencia de las
t\'ecnicas de formaci\'on y gesti\'on de conocimiento en las empresas
industriales. La investigaci\'on se ha apoyado en el desarrollo de contenidos y
sistemas con un nivel de madurez tecnol\'ogico bajo que abordan soluciones para
el sector industrial aplicadas en la formaci\'on y asistencia al operario. Los
resultados han sido analizados en empresas del sector de automoci\'on, no
obstante, son exportables a cualquier otro tipo de sector industrial.",arxiv
http://arxiv.org/abs/2004.05740v2,2020-11-03T03:35:35Z,2020-04-13T01:46:29Z,"Deep-Edge: An Efficient Framework for Deep Learning Model Update on
  Heterogeneous Edge","Deep Learning (DL) model-based AI services are increasingly offered in a
variety of predictive analytics services such as computer vision, natural
language processing, speech recognition. However, the quality of the DL models
can degrade over time due to changes in the input data distribution, thereby
requiring periodic model updates. Although cloud data-centers can meet the
computational requirements of the resource-intensive and time-consuming model
update task, transferring data from the edge devices to the cloud incurs a
significant cost in terms of network bandwidth and are prone to data privacy
issues. With the advent of GPU-enabled edge devices, the DL model update can be
performed at the edge in a distributed manner using multiple connected edge
devices. However, efficiently utilizing the edge resources for the model update
is a hard problem due to the heterogeneity among the edge devices and the
resource interference caused by the co-location of the DL model update task
with latency-critical tasks running in the background. To overcome these
challenges, we present Deep-Edge, a load- and interference-aware,
fault-tolerant resource management framework for performing model update at the
edge that uses distributed training. This paper makes the following
contributions. First, it provides a unified framework for monitoring,
profiling, and deploying the DL model update tasks on heterogeneous edge
devices. Second, it presents a scheduler that reduces the total re-training
time by appropriately selecting the edge devices and distributing data among
them such that no latency-critical applications experience deadline violations.
Finally, we present empirical results to validate the efficacy of the framework
using a real-world DL model update case-study based on the Caltech dataset and
an edge AI cluster testbed.",arxiv
http://arxiv.org/abs/2002.06637v1,2020-02-16T18:18:19Z,2020-02-16T18:18:19Z,Real-time binaural speech separation with preserved spatial cues,"Deep learning speech separation algorithms have achieved great success in
improving the quality and intelligibility of separated speech from mixed audio.
Most previous methods focused on generating a single-channel output for each of
the target speakers, hence discarding the spatial cues needed for the
localization of sound sources in space. However, preserving the spatial
information is important in many applications that aim to accurately render the
acoustic scene such as in hearing aids and augmented reality (AR). Here, we
propose a speech separation algorithm that preserves the interaural cues of
separated sound sources and can be implemented with low latency and high
fidelity, therefore enabling a real-time modification of the acoustic scene.
Based on the time-domain audio separation network (TasNet), a single-channel
time-domain speech separation system that can be implemented in real-time, we
propose a multi-input-multi-output (MIMO) end-to-end extension of TasNet that
takes binaural mixed audio as input and simultaneously separates target
speakers in both channels. Experimental results show that the proposed
end-to-end MIMO system is able to significantly improve the separation
performance and keep the perceived location of the modified sources intact in
various acoustic scenes.",arxiv
http://arxiv.org/abs/1804.09364v3,2018-12-13T15:42:35Z,2018-04-25T06:20:12Z,Driving Policy Transfer via Modularity and Abstraction,"End-to-end approaches to autonomous driving have high sample complexity and
are difficult to scale to realistic urban driving. Simulation can help
end-to-end driving systems by providing a cheap, safe, and diverse training
environment. Yet training driving policies in simulation brings up the problem
of transferring such policies to the real world. We present an approach to
transferring driving policies from simulation to reality via modularity and
abstraction. Our approach is inspired by classic driving systems and aims to
combine the benefits of modular architectures and end-to-end deep learning
approaches. The key idea is to encapsulate the driving policy such that it is
not directly exposed to raw perceptual input or low-level vehicle dynamics. We
evaluate the presented approach in simulated urban environments and in the real
world. In particular, we transfer a driving policy trained in simulation to a
1/5-scale robotic truck that is deployed in a variety of conditions, with no
finetuning, on two continents. The supplementary video can be viewed at
https://youtu.be/BrMDJqI6H5U",arxiv
http://arxiv.org/abs/1810.01140v2,2018-10-08T08:40:40Z,2018-10-02T09:45:15Z,"Training compact deep learning models for video classification using
  circulant matrices","In real world scenarios, model accuracy is hardly the only factor to
consider. Large models consume more memory and are computationally more
intensive, which makes them difficult to train and to deploy, especially on
mobile devices. In this paper, we build on recent results at the crossroads of
Linear Algebra and Deep Learning which demonstrate how imposing a structure on
large weight matrices can be used to reduce the size of the model. We propose
very compact models for video classification based on state-of-the-art network
architectures such as Deep Bag-of-Frames, NetVLAD and NetFisherVectors. We then
conduct thorough experiments using the large YouTube-8M video classification
dataset. As we will show, the circulant DBoF embedding achieves an excellent
trade-off between size and accuracy.",arxiv
http://arxiv.org/abs/2005.13601v1,2020-05-27T19:19:57Z,2020-05-27T19:19:57Z,"The Adversarial Resilience Learning Architecture for AI-based Modelling,
  Exploration, and Operation of Complex Cyber-Physical Systems","Modern algorithms in the domain of Deep Reinforcement Learning (DRL)
demonstrated remarkable successes; most widely known are those in game-based
scenarios, from ATARI video games to Go and the StarCraft~\textsc{II} real-time
strategy game. However, applications in the domain of modern Cyber-Physical
Systems (CPS) that take advantage a vast variety of DRL algorithms are few. We
assume that the benefits would be considerable: Modern CPS have become
increasingly complex and evolved beyond traditional methods of modelling and
analysis. At the same time, these CPS are confronted with an increasing amount
of stochastic inputs, from volatile energy sources in power grids to broad user
participation stemming from markets. Approaches of system modelling that use
techniques from the domain of Artificial Intelligence (AI) do not focus on
analysis and operation. In this paper, we describe the concept of Adversarial
Resilience Learning (ARL) that formulates a new approach to complex environment
checking and resilient operation: It defines two agent classes, attacker and
defender agents. The quintessence of ARL lies in both agents exploring the
system and training each other without any domain knowledge. Here, we introduce
the ARL software architecture that allows to use a wide range of model-free as
well as model-based DRL-based algorithms, and document results of concrete
experiment runs on a complex power grid.",arxiv
http://arxiv.org/abs/1810.10206v1,2018-10-24T06:23:46Z,2018-10-24T06:23:46Z,"Immercity: a curation content application in Virtual and Augmented
  reality","When working with emergent and appealing technologies as Virtual Reality,
Mixed Reality and Augmented Reality, the issue of definitions appear very
often. Indeed, our experience with various publics allows us to notice that
technology definitions pose ambiguity and representation problems for informed
as well as novice users. In this paper we present Immercity, a content curation
system designed in the context of a collaboration between the University of
Montpellier and CapGemi-ni, to deliver a technology watch. It is also used as a
testbed for our experiences with Virtual, Mixed and Augmented reality to
explore new interaction techniques and devices, artificial intelligence
integration, visual affordances, performance , etc. But another, very
interesting goal appeared: use Immercity to communicate about Virtual, Mixed
and Augmented Reality by using them as a support.",arxiv
http://arxiv.org/abs/1810.07167v1,2018-10-16T17:49:43Z,2018-10-16T17:49:43Z,"Composable Action-Conditioned Predictors: Flexible Off-Policy Learning
  for Robot Navigation","A general-purpose intelligent robot must be able to learn autonomously and be
able to accomplish multiple tasks in order to be deployed in the real world.
However, standard reinforcement learning approaches learn separate
task-specific policies and assume the reward function for each task is known a
priori. We propose a framework that learns event cues from off-policy data, and
can flexibly combine these event cues at test time to accomplish different
tasks. These event cue labels are not assumed to be known a priori, but are
instead labeled using learned models, such as computer vision detectors, and
then `backed up' in time using an action-conditioned predictive model. We show
that a simulated robotic car and a real-world RC car can gather data and train
fully autonomously without any human-provided labels beyond those needed to
train the detectors, and then at test-time be able to accomplish a variety of
different tasks. Videos of the experiments and code can be found at
https://github.com/gkahn13/CAPs",arxiv
http://arxiv.org/abs/2011.04424v2,2020-11-10T13:07:19Z,2020-11-05T13:49:55Z,"Playing optical tweezers with deep reinforcement learning: in virtual,
  physical and augmented environments","Reinforcement learning was carried out in a simulated environment to learn
continuous velocity control over multiple motor axes. This was then applied to
a real-world optical tweezers experiment with the objective of moving a
laser-trapped microsphere to a target location whilst avoiding collisions with
other free-moving microspheres. The concept of training a neural network in a
virtual environment has significant potential in the application of machine
learning for experimental optimization and control, as the neural network can
discover optimal methods for problem solving without the risk of damage to
equipment, and at a speed not limited by movement in the physical environment.
As the neural network treats both virtual and physical environments
equivalently, we show that the network can also be applied to an augmented
environment, where a virtual environment is combined with the physical
environment. This technique may have the potential to unlock capabilities
associated with mixed and augmented reality, such as enforcing safety limits
for machine motion or as a method of inputting observations from additional
sensors.",arxiv
http://arxiv.org/abs/2009.05835v3,2021-04-03T15:08:50Z,2020-09-12T17:37:36Z,"How Much Can We Really Trust You? Towards Simple, Interpretable Trust
  Quantification Metrics for Deep Neural Networks","A critical step to building trustworthy deep neural networks is trust
quantification, where we ask the question: How much can we trust a deep neural
network? In this study, we take a step towards simple, interpretable metrics
for trust quantification by introducing a suite of metrics for assessing the
overall trustworthiness of deep neural networks based on their behaviour when
answering a set of questions. We conduct a thought experiment and explore two
key questions about trust in relation to confidence: 1) How much trust do we
have in actors who give wrong answers with great confidence? and 2) How much
trust do we have in actors who give right answers hesitantly? Based on insights
gained, we introduce the concept of question-answer trust to quantify
trustworthiness of an individual answer based on confident behaviour under
correct and incorrect answer scenarios, and the concept of trust density to
characterize the distribution of overall trust for an individual answer
scenario. We further introduce the concept of trust spectrum for representing
overall trust with respect to the spectrum of possible answer scenarios across
correctly and incorrectly answered questions. Finally, we introduce
NetTrustScore, a scalar metric summarizing overall trustworthiness. The suite
of metrics aligns with past social psychology studies that study the
relationship between trust and confidence. Leveraging these metrics, we
quantify the trustworthiness of several well-known deep neural network
architectures for image recognition to get a deeper understanding of where
trust breaks down. The proposed metrics are by no means perfect, but the hope
is to push the conversation towards better metrics to help guide practitioners
and regulators in producing, deploying, and certifying deep learning solutions
that can be trusted to operate in real-world, mission-critical scenarios.",arxiv
http://arxiv.org/abs/2003.04956v1,2020-03-10T20:26:26Z,2020-03-10T20:26:26Z,"SQUIRL: Robust and Efficient Learning from Video Demonstration of
  Long-Horizon Robotic Manipulation Tasks","Recent advances in deep reinforcement learning (RL) have demonstrated its
potential to learn complex robotic manipulation tasks. However, RL still
requires the robot to collect a large amount of real-world experience. To
address this problem, recent works have proposed learning from expert
demonstrations (LfD), particularly via inverse reinforcement learning (IRL),
given its ability to achieve robust performance with only a small number of
expert demonstrations. Nevertheless, deploying IRL on real robots is still
challenging due to the large number of robot experiences it requires. This
paper aims to address this scalability challenge with a robust,
sample-efficient, and general meta-IRL algorithm, SQUIRL, that performs a new
but related long-horizon task robustly given only a single video demonstration.
First, this algorithm bootstraps the learning of a task encoder and a
task-conditioned policy using behavioral cloning (BC). It then collects
real-robot experiences and bypasses reward learning by directly recovering a
Q-function from the combined robot and expert trajectories. Next, this
algorithm uses the Q-function to re-evaluate all cumulative experiences
collected by the robot to improve the policy quickly. In the end, the policy
performs more robustly (90%+ success) than BC on new tasks while requiring no
trial-and-errors at test time. Finally, our real-robot and simulated
experiments demonstrate our algorithm's generality across different state
spaces, action spaces, and vision-based manipulation tasks, e.g.,
pick-pour-place and pick-carry-drop.",arxiv
http://arxiv.org/abs/1908.01853v1,2019-08-02T01:13:50Z,2019-08-02T01:13:50Z,DELTA: A DEep learning based Language Technology plAtform,"In this paper we present DELTA, a deep learning based language technology
platform. DELTA is an end-to-end platform designed to solve industry level
natural language and speech processing problems. It integrates most popular
neural network models for training as well as comprehensive deployment tools
for production. DELTA aims to provide easy and fast experiences for using,
deploying, and developing natural language processing and speech models for
both academia and industry use cases. We demonstrate the reliable performance
with DELTA on several natural language processing and speech tasks, including
text classification, named entity recognition, natural language inference,
speech recognition, speaker verification, etc. DELTA has been used for
developing several state-of-the-art algorithms for publications and delivering
real production to serve millions of users.",arxiv
http://arxiv.org/abs/1804.03313v1,2018-04-10T02:33:47Z,2018-04-10T02:33:47Z,Cortex Neural Network: learning with Neural Network groups,"Neural Network has been successfully applied to many real-world problems,
such as image recognition and machine translation. However, for the current
architecture of neural networks, it is hard to perform complex cognitive tasks,
for example, to process the image and audio inputs together. Cortex, as an
important architecture in the brain, is important for animals to perform the
complex cognitive task. We view the architecture of Cortex in the brain as a
missing part in the design of the current artificial neural network. In this
paper, we purpose Cortex Neural Network (CrtxNN). The Cortex Neural Network is
an upper architecture of neural networks which motivated from cerebral cortex
in the brain to handle different tasks in the same learning system. It is able
to identify different tasks and solve them with different methods. In our
implementation, the Cortex Neural Network is able to process different
cognitive tasks and perform reflection to get a higher accuracy. We provide a
series of experiments to examine the capability of the cortex architecture on
traditional neural networks. Our experiments proved its ability on the Cortex
Neural Network can reach accuracy by 98.32% on MNIST and 62% on CIFAR10 at the
same time, which can promisingly reduce the loss by 40%.",arxiv
http://arxiv.org/abs/1904.07633v1,2019-04-16T13:02:01Z,2019-04-16T13:02:01Z,"HARK Side of Deep Learning -- From Grad Student Descent to Automated
  Machine Learning","Recent advancements in machine learning research, i.e., deep learning,
introduced methods that excel conventional algorithms as well as humans in
several complex tasks, ranging from detection of objects in images and speech
recognition to playing difficult strategic games. However, the current
methodology of machine learning research and consequently, implementations of
the real-world applications of such algorithms, seems to have a recurring
HARKing (Hypothesizing After the Results are Known) issue. In this work, we
elaborate on the algorithmic, economic and social reasons and consequences of
this phenomenon. We present examples from current common practices of
conducting machine learning research (e.g. avoidance of reporting negative
results) and failure of generalization ability of the proposed algorithms and
datasets in actual real-life usage. Furthermore, a potential future trajectory
of machine learning research and development from the perspective of
accountable, unbiased, ethical and privacy-aware algorithmic decision making is
discussed. We would like to emphasize that with this discussion we neither
claim to provide an exhaustive argumentation nor blame any specific institution
or individual on the raised issues. This is simply a discussion put forth by
us, insiders of the machine learning field, reflecting on us.",arxiv
http://arxiv.org/abs/1905.07082v6,2021-06-26T12:14:13Z,2019-05-17T01:35:26Z,"The Audio Auditor: User-Level Membership Inference in Internet of Things
  Voice Services","With the rapid development of deep learning techniques, the popularity of
voice services implemented on various Internet of Things (IoT) devices is ever
increasing. In this paper, we examine user-level membership inference in the
problem space of voice services, by designing an audio auditor to verify
whether a specific user had unwillingly contributed audio used to train an
automatic speech recognition (ASR) model under strict black-box access. With
user representation of the input audio data and their corresponding translated
text, our trained auditor is effective in user-level audit. We also observe
that the auditor trained on specific data can be generalized well regardless of
the ASR model architecture. We validate the auditor on ASR models trained with
LSTM, RNNs, and GRU algorithms on two state-of-the-art pipelines, the hybrid
ASR system and the end-to-end ASR system. Finally, we conduct a real-world
trial of our auditor on iPhone Siri, achieving an overall accuracy exceeding
80\%. We hope the methodology developed in this paper and findings can inform
privacy advocates to overhaul IoT privacy.",arxiv
http://arxiv.org/abs/2006.02230v2,2020-11-17T15:43:42Z,2020-06-02T06:44:09Z,"PolyDL: Polyhedral Optimizations for Creation of High Performance DL
  primitives","Deep Neural Networks (DNNs) have revolutionized many aspects of our lives.
The use of DNNs is becoming ubiquitous including in softwares for image
recognition, speech recognition, speech synthesis, language translation, to
name a few. he training of DNN architectures however is computationally
expensive. Once the model is created, its use in the intended application - the
inference task, is computationally heavy too and the inference needs to be fast
for real time use. For obtaining high performance today, the code of Deep
Learning (DL) primitives optimized for specific architectures by expert
programmers exposed via libraries is the norm. However, given the constant
emergence of new DNN architectures, creating hand optimized code is expensive,
slow and is not scalable.
  To address this performance-productivity challenge, in this paper we present
compiler algorithms to automatically generate high performance implementations
of DL primitives that closely match the performance of hand optimized
libraries. We develop novel data reuse analysis algorithms using the polyhedral
model to derive efficient execution schedules automatically. In addition,
because most DL primitives use some variant of matrix multiplication at their
core, we develop a flexible framework where it is possible to plug in library
implementations of the same in lieu of a subset of the loops. We show that such
a hybrid compiler plus a minimal library-use approach results in
state-of-the-art performance. We develop compiler algorithms to also perform
operator fusions that reduce data movement through the memory hierarchy of the
computer system.",arxiv
http://arxiv.org/abs/1910.03227v1,2019-10-08T06:12:53Z,2019-10-08T06:12:53Z,Identifying Candidate Spaces for Advert Implantation,"Virtual advertising is an important and promising feature in the area of
online advertising. It involves integrating adverts onto live or recorded
videos for product placements and targeted advertisements. Such integration of
adverts is primarily done by video editors in the post-production stage, which
is cumbersome and time-consuming. Therefore, it is important to automatically
identify candidate spaces in a video frame, wherein new adverts can be
implanted. The candidate space should match the scene perspective, and also
have a high quality of experience according to human subjective judgment. In
this paper, we propose the use of a bespoke neural net that can assist the
video editors in identifying candidate spaces. We benchmark our approach
against several deep-learning architectures on a large-scale image dataset of
candidate spaces of outdoor scenes. Our work is the first of its kind in this
area of multimedia and augmented reality applications, and achieves the best
results.",arxiv
http://arxiv.org/abs/2012.06753v2,2020-12-20T13:23:54Z,2020-12-12T08:08:47Z,"Towards Neurohaptics: Brain-Computer Interfaces for Decoding Intuitive
  Sense of Touch","Noninvasive brain-computer interface (BCI) is widely used to recognize users'
intentions. Especially, BCI related to tactile and sensation decoding could
provide various effects on many industrial fields such as manufacturing
advanced touch displays, controlling robotic devices, and more immersive
virtual reality or augmented reality. In this paper, we introduce haptic and
sensory perception-based BCI systems called neurohaptics. It is a preliminary
study for a variety of scenarios using actual touch and touch imagery
paradigms. We designed a novel experimental environment and a device that could
acquire brain signals under touching designated materials to generate natural
touch and texture sensations. Through the experiment, we collected the
electroencephalogram (EEG) signals with respect to four different texture
objects. Seven subjects were recruited for the experiment and evaluated
classification performances using machine learning and deep learning
approaches. Hence, we could confirm the feasibility of decoding actual touch
and touch imagery on EEG signals to develop practical neurohaptics.",arxiv
http://arxiv.org/abs/1906.06969v1,2019-06-17T11:44:15Z,2019-06-17T11:44:15Z,Robotic Navigation using Entropy-Based Exploration,"Robotic navigation concerns the task in which a robot should be able to find
a safe and feasible path and traverse between two points in a complex
environment. We approach the problem of robotic navigation using reinforcement
learning and use deep $Q$-networks to train agents to solve the task of robotic
navigation. We compare the Entropy-Based Exploration (EBE) with the widely used
$\epsilon$-greedy exploration strategy by training agents using both of them in
simulation. The trained agents are then tested on different versions of the
environment to test the generalization ability of the learned policies. We also
implement the learned policies on a real robot in complex real environment
without any fine tuning and compare the effectiveness of the above-mentioned
exploration strategies in the real world setting. Video showing experiments on
TurtleBot3 platform is available at \url{https://youtu.be/NHT-EiN_4n8}.",arxiv
http://arxiv.org/abs/2007.08501v1,2020-07-16T17:53:02Z,2020-07-16T17:53:02Z,Accelerating 3D Deep Learning with PyTorch3D,"Deep learning has significantly improved 2D image recognition. Extending into
3D may advance many new applications including autonomous vehicles, virtual and
augmented reality, authoring 3D content, and even improving 2D recognition.
However despite growing interest, 3D deep learning remains relatively
underexplored. We believe that some of this disparity is due to the engineering
challenges involved in 3D deep learning, such as efficiently processing
heterogeneous data and reframing graphics operations to be differentiable. We
address these challenges by introducing PyTorch3D, a library of modular,
efficient, and differentiable operators for 3D deep learning. It includes a
fast, modular differentiable renderer for meshes and point clouds, enabling
analysis-by-synthesis approaches. Compared with other differentiable renderers,
PyTorch3D is more modular and efficient, allowing users to more easily extend
it while also gracefully scaling to large meshes and images. We compare the
PyTorch3D operators and renderer with other implementations and demonstrate
significant speed and memory improvements. We also use PyTorch3D to improve the
state-of-the-art for unsupervised 3D mesh and point cloud prediction from 2D
images on ShapeNet. PyTorch3D is open-source and we hope it will help
accelerate research in 3D deep learning.",arxiv
http://arxiv.org/abs/1610.01239v4,2017-04-27T17:25:30Z,2016-10-05T00:46:03Z,"Adversary Resistant Deep Neural Networks with an Application to Malware
  Detection","Beyond its highly publicized victories in Go, there have been numerous
successful applications of deep learning in information retrieval, computer
vision and speech recognition. In cybersecurity, an increasing number of
companies have become excited about the potential of deep learning, and have
started to use it for various security incidents, the most popular being
malware detection. These companies assert that deep learning (DL) could help
turn the tide in the battle against malware infections. However, deep neural
networks (DNNs) are vulnerable to adversarial samples, a flaw that plagues most
if not all statistical learning models. Recent research has demonstrated that
those with malicious intent can easily circumvent deep learning-powered malware
detection by exploiting this flaw.
  In order to address this problem, previous work has developed various defense
mechanisms that either augmenting training data or enhance model's complexity.
However, after a thorough analysis of the fundamental flaw in DNNs, we discover
that the effectiveness of current defenses is limited and, more importantly,
cannot provide theoretical guarantees as to their robustness against
adversarial sampled-based attacks. As such, we propose a new adversary
resistant technique that obstructs attackers from constructing impactful
adversarial samples by randomly nullifying features within samples. In this
work, we evaluate our proposed technique against a real world dataset with
14,679 malware variants and 17,399 benign programs. We theoretically validate
the robustness of our technique, and empirically show that our technique
significantly boosts DNN robustness to adversarial samples while maintaining
high accuracy in classification. To demonstrate the general applicability of
our proposed method, we also conduct experiments using the MNIST and CIFAR-10
datasets, generally used in image recognition research.",arxiv
http://arxiv.org/abs/2010.01217v1,2020-10-02T22:28:02Z,2020-10-02T22:28:02Z,Artificial Intelligence Enabled Traffic Monitoring System,"Manual traffic surveillance can be a daunting task as Traffic Management
Centers operate a myriad of cameras installed over a network. Injecting some
level of automation could help lighten the workload of human operators
performing manual surveillance and facilitate making proactive decisions which
would reduce the impact of incidents and recurring congestion on roadways. This
article presents a novel approach to automatically monitor real time traffic
footage using deep convolutional neural networks and a stand-alone graphical
user interface. The authors describe the results of research received in the
process of developing models that serve as an integrated framework for an
artificial intelligence enabled traffic monitoring system. The proposed system
deploys several state-of-the-art deep learning algorithms to automate different
traffic monitoring needs. Taking advantage of a large database of annotated
video surveillance data, deep learning-based models are trained to detect
queues, track stationary vehicles, and tabulate vehicle counts. A pixel-level
segmentation approach is applied to detect traffic queues and predict severity.
Real-time object detection algorithms coupled with different tracking systems
are deployed to automatically detect stranded vehicles as well as perform
vehicular counts. At each stages of development, interesting experimental
results are presented to demonstrate the effectiveness of the proposed system.
Overall, the results demonstrate that the proposed framework performs
satisfactorily under varied conditions without being immensely impacted by
environmental hazards such as blurry camera views, low illumination, rain, or
snow.",arxiv
http://arxiv.org/abs/1906.05925v1,2019-06-13T20:53:33Z,2019-06-13T20:53:33Z,Deep Learning Development Environment in Virtual Reality,"Virtual reality (VR) offers immersive visualization and intuitive
interaction. We leverage VR to enable any biomedical professional to deploy a
deep learning (DL) model for image classification. While DL models can be
powerful tools for data analysis, they are also challenging to understand and
develop. To make deep learning more accessible and intuitive, we have built a
virtual reality-based DL development environment. Within our environment, the
user can move tangible objects to construct a neural network only using their
hands. Our software automatically translates these configurations into a
trainable model and then reports its resulting accuracy on a test dataset in
real-time. Furthermore, we have enriched the virtual objects with
visualizations of the model's components such that users can achieve insight
about the DL models that they are developing. With this approach, we bridge the
gap between professionals in different fields of expertise while offering a
novel perspective for model analysis and data interaction. We further suggest
that techniques of development and visualization in deep learning can benefit
by integrating virtual reality.",arxiv
http://arxiv.org/abs/1612.05571v1,2016-12-16T17:57:15Z,2016-12-16T17:57:15Z,Delta Networks for Optimized Recurrent Network Computation,"Many neural networks exhibit stability in their activation patterns over time
in response to inputs from sensors operating under real-world conditions. By
capitalizing on this property of natural signals, we propose a Recurrent Neural
Network (RNN) architecture called a delta network in which each neuron
transmits its value only when the change in its activation exceeds a threshold.
The execution of RNNs as delta networks is attractive because their states must
be stored and fetched at every timestep, unlike in convolutional neural
networks (CNNs). We show that a naive run-time delta network implementation
offers modest improvements on the number of memory accesses and computes, but
optimized training techniques confer higher accuracy at higher speedup. With
these optimizations, we demonstrate a 9X reduction in cost with negligible loss
of accuracy for the TIDIGITS audio digit recognition benchmark. Similarly, on
the large Wall Street Journal speech recognition benchmark even existing
networks can be greatly accelerated as delta networks, and a 5.7x improvement
with negligible loss of accuracy can be obtained through training. Finally, on
an end-to-end CNN trained for steering angle prediction in a driving dataset,
the RNN cost can be reduced by a substantial 100X.",arxiv
http://arxiv.org/abs/2107.04566v1,2021-07-09T17:34:42Z,2021-07-09T17:34:42Z,"Multi-level Stress Assessment from ECG in a Virtual Reality Environment
  using Multimodal Fusion","ECG is an attractive option to assess stress in serious Virtual Reality (VR)
applications due to its non-invasive nature. However, the existing Machine
Learning (ML) models perform poorly. Moreover, existing studies only perform a
binary stress assessment, while to develop a more engaging biofeedback-based
application, multi-level assessment is necessary. Existing studies annotate and
classify a single experience (e.g. watching a VR video) to a single stress
level, which again prevents design of dynamic experiences where real-time
in-game stress assessment can be utilized. In this paper, we report our
findings on a new study on VR stress assessment, where three stress levels are
assessed. ECG data was collected from 9 users experiencing a VR roller coaster.
The VR experience was then manually labeled in 10-seconds segments to three
stress levels by three raters. We then propose a novel multimodal deep fusion
model utilizing spectrogram and 1D ECG that can provide a stress prediction
from just a 1-second window. Experimental results demonstrate that the proposed
model outperforms the classical HRV-based ML models (9% increase in accuracy)
and baseline deep learning models (2.5% increase in accuracy). We also report
results on the benchmark WESAD dataset to show the supremacy of the model.",arxiv
http://arxiv.org/abs/1604.06195v1,2016-04-21T06:55:42Z,2016-04-21T06:55:42Z,Articulated Hand Pose Estimation Review,"With the increase number of companies focusing on commercializing Augmented
Reality (AR), Virtual Reality (VR) and wearable devices, the need for a hand
based input mechanism is becoming essential in order to make the experience
natural, seamless and immersive. Hand pose estimation has progressed
drastically in recent years due to the introduction of commodity depth cameras.
  Hand pose estimation based on vision is still a challenging problem due to
its complexity from self-occlusion (between fingers), close similarity between
fingers, dexterity of the hands, speed of the pose and the high dimension of
the hand kinematic parameters. Articulated hand pose estimation is still an
open problem and under intensive research from both academia and industry.
  The 2 approaches used for hand pose estimation are: discriminative and
generative. Generative approach is a model based that tries to fit a hand model
to the observed data. Discriminative approach is appearance based, usually
implemented with machine learning (ML) and require a large amount of training
data. Recent hand pose estimation uses hybrid approach by combining both
discriminative and generative methods into a single hand pipeline.
  In this paper, we focus on reviewing recent progress of hand pose estimation
from depth sensor. We will survey discriminative methods, generative methods
and hybrid methods. This paper is not a comprehensive review of all hand pose
estimation techniques, it is a subset of some of the recent state-of-the-art
techniques.",arxiv
http://arxiv.org/abs/1810.11359v4,2020-10-09T07:51:26Z,2018-10-26T15:05:04Z,"gpuRIR: A Python Library for Room Impulse Response Simulation with GPU
  Acceleration","The Image Source Method (ISM) is one of the most employed techniques to
calculate acoustic Room Impulse Responses (RIRs), however, its computational
complexity grows fast with the reverberation time of the room and its
computation time can be prohibitive for some applications where a huge number
of RIRs are needed. In this paper, we present a new implementation that
dramatically improves the computation speed of the ISM by using Graphic
Processing Units (GPUs) to parallelize both the simulation of multiple RIRs and
the computation of the images inside each RIR. Additional speedups were
achieved by exploiting the mixed precision capabilities of the newer GPUs and
by using lookup tables. We provide a Python library under GNU license that can
be easily used without any knowledge about GPU programming and we show that it
is about 100 times faster than other state of the art CPU libraries. It may
become a powerful tool for many applications that need to perform a large
number of acoustic simulations, such as training machine learning systems for
audio signal processing, or for real-time room acoustics simulations for
immersive multimedia systems, such as augmented or virtual reality.",arxiv
http://arxiv.org/abs/1911.10751v1,2019-11-25T08:02:17Z,2019-11-25T08:02:17Z,"Deep Image-to-Video Adaptation and Fusion Networks for Action
  Recognition","Existing deep learning methods for action recognition in videos require a
large number of labeled videos for training, which is labor-intensive and
time-consuming. For the same action, the knowledge learned from different media
types, e.g., videos and images, may be related and complementary. However, due
to the domain shifts and heterogeneous feature representations between videos
and images, the performance of classifiers trained on images may be
dramatically degraded when directly deployed to videos. In this paper, we
propose a novel method, named Deep Image-to-Video Adaptation and Fusion
Networks (DIVAFN), to enhance action recognition in videos by transferring
knowledge from images using video keyframes as a bridge. The DIVAFN is a
unified deep learning model, which integrates domain-invariant representations
learning and cross-modal feature fusion into a unified optimization framework.
Specifically, we design an efficient cross-modal similarities metric to reduce
the modality shift among images, keyframes and videos. Then, we adopt an
autoencoder architecture, whose hidden layer is constrained to be the semantic
representations of the action class names. In this way, when the autoencoder is
adopted to project the learned features from different domains to the same
space, more compact, informative and discriminative representations can be
obtained. Finally, the concatenation of the learned semantic feature
representations from these three autoencoders are used to train the classifier
for action recognition in videos. Comprehensive experiments on four real-world
datasets show that our method outperforms some state-of-the-art domain
adaptation and action recognition methods.",arxiv
http://arxiv.org/abs/2012.10342v3,2021-09-14T20:26:13Z,2020-12-18T16:40:32Z,Exploring and Interrogating Astrophysical Data in Virtual Reality,"Scientists across all disciplines increasingly rely on machine learning
algorithms to analyse and sort datasets of ever increasing volume and
complexity. Although trends and outliers are easily extracted, careful and
close inspection will still be necessary to explore and disentangle detailed
behavior, as well as identify systematics and false positives. We must
therefore incorporate new technologies to facilitate scientific analysis and
exploration. Astrophysical data is inherently multi-parameter, with the
spatial-kinematic dimensions at the core of observations and simulations. The
arrival of mainstream virtual-reality (VR) headsets and increased GPU power, as
well as the availability of versatile development tools for video games, has
enabled scientists to deploy such technology to effectively interrogate and
interact with complex data. In this paper we present development and results
from custom-built interactive VR tools, called the iDaVIE suite, that are
informed and driven by research on galaxy evolution, cosmic large-scale
structure, galaxy-galaxy interactions, and gas/kinematics of nearby galaxies in
survey and targeted observations. In the new era of Big Data ushered in by
major facilities such as the SKA and LSST that render past analysis and
refinement methods highly constrained, we believe that a paradigm shift to new
software, technology and methods that exploit the power of visual perception,
will play an increasingly important role in bridging the gap between
statistical metrics and new discovery. We have released a beta version of the
iDaVIE software system that is free and open to the community.",arxiv
http://arxiv.org/abs/2104.01384v2,2021-08-08T22:28:56Z,2021-04-03T12:16:19Z,"ExKaldi-RT: A Real-Time Automatic Speech Recognition Extension Toolkit
  of Kaldi","This paper describes the ExKaldi-RT online automatic speech recognition (ASR)
toolkit that is implemented based on the Kaldi ASR toolkit and Python language.
ExKaldi-RT provides tools for building online recognition pipelines. While
similar tools are available built on Kaldi, a key feature of ExKaldi-RT that it
works on Python, which has an easy-to-use interface that allows online ASR
system developers to develop original research, such as by applying neural
network-based signal processing and by decoding model trained with deep
learning frameworks. We performed benchmark experiments on the minimum
LibriSpeech corpus, and it showed that ExKaldi-RT could achieve competitive ASR
performance in real-time recognition.",arxiv
http://arxiv.org/abs/2007.00641v1,2020-07-01T17:46:08Z,2020-07-01T17:46:08Z,Democratizing the Edge: A Pervasive Edge Computing Framework,"The needs of emerging applications, such as augmented and virtual reality,
federated machine learning, and autonomous driving, have motivated edge
computing--the push of computation capabilities to the edge. Various edge
computing architectures have emerged, including multi-access edge computing and
edge-cloud, all with the premise of reducing communication latency and
augmenting privacy. However, these architectures rely on static and
pre-deployed infrastructure, falling short in harnessing the abundant resources
at the network's edge. In this paper, we discuss the design of Pervasive Edge
Computing (PEC)--a democratized edge computing framework, which enables
end-user devices (e.g., smartphones, IoT devices, and vehicles) to dynamically
participate in a large-scale computing ecosystem. Our vision of the
democratized edge involves the real-time composition of services using
available edge resources like data, software, and compute-hardware from
multiple stakeholders. We discuss how the novel Named-Data Networking
architecture can facilitate service deployment, discovery, invocation, and
migration. We also discuss the economic models critical to the adoption of PEC
and the outstanding challenges for its full realization.",arxiv
http://arxiv.org/abs/2103.00001v3,2021-10-19T18:50:02Z,2021-02-26T21:47:14Z,"Three-dimensional Coherent X-ray Diffraction Imaging via Deep
  Convolutional Neural Networks","As a critical component of coherent X-ray diffraction imaging (CDI), phase
retrieval has been extensively applied in X-ray structural science to recover
the 3D morphological information inside measured particles. Despite meeting all
the oversampling requirements of Sayre and Shannon, current phase retrieval
approaches still have trouble achieving a unique inversion of experimental data
in the presence of noise. Here, we propose to overcome this limitation by
incorporating a 3D Machine Learning (ML) model combining (optional) supervised
learning with transfer learning. The trained ML model can rapidly provide an
immediate result with high accuracy which could benefit real-time experiments,
and the predicted result can be further refined with transfer learning. More
significantly, the proposed ML model can be used without any prior training to
learn the missing phases of an image based on minimization of an appropriate
'loss function' alone. We demonstrate significantly improved performance with
experimental Bragg CDI data over traditional iterative phase retrieval
algorithms.",arxiv
http://arxiv.org/abs/1904.02579v2,2019-10-15T15:45:57Z,2019-04-04T14:30:09Z,"Can a Robot Become a Movie Director? Learning Artistic Principles for
  Aerial Cinematography","Aerial filming is constantly gaining importance due to the recent advances in
drone technology. It invites many intriguing, unsolved problems at the
intersection of aesthetical and scientific challenges. In this work, we propose
a deep reinforcement learning agent which supervises motion planning of a
filming drone by making desirable shot mode selections based on aesthetical
values of video shots. Unlike most of the current state-of-the-art approaches
that require explicit guidance by a human expert, our drone learns how to make
favorable viewpoint selections by experience. We propose a learning scheme that
exploits aesthetical features of retrospective shots in order to extract a
desirable policy for better prospective shots. We train our agent in realistic
AirSim simulations using both a hand-crafted reward function as well as reward
from direct human input. We then deploy the same agent on a real DJI M210 drone
in order to test the generalization capability of our approach to real world
conditions. To evaluate the success of our approach in the end, we conduct a
comprehensive user study in which participants rate the shot quality of our
methods. Videos of the system in action can be seen at
https://youtu.be/qmVw6mfyEmw.",arxiv
http://arxiv.org/abs/2110.07742v1,2021-10-14T21:53:03Z,2021-10-14T21:53:03Z,"Beyond Classification: Directly Training Spiking Neural Networks for
  Semantic Segmentation","Spiking Neural Networks (SNNs) have recently emerged as the low-power
alternative to Artificial Neural Networks (ANNs) because of their sparse,
asynchronous, and binary event-driven processing. Due to their energy
efficiency, SNNs have a high possibility of being deployed for real-world,
resource-constrained systems such as autonomous vehicles and drones. However,
owing to their non-differentiable and complex neuronal dynamics, most previous
SNN optimization methods have been limited to image recognition. In this paper,
we explore the SNN applications beyond classification and present semantic
segmentation networks configured with spiking neurons. Specifically, we first
investigate two representative SNN optimization techniques for recognition
tasks (i.e., ANN-SNN conversion and surrogate gradient learning) on semantic
segmentation datasets. We observe that, when converted from ANNs, SNNs suffer
from high latency and low performance due to the spatial variance of features.
Therefore, we directly train networks with surrogate gradient learning,
resulting in lower latency and higher performance than ANN-SNN conversion.
Moreover, we redesign two fundamental ANN segmentation architectures (i.e.,
Fully Convolutional Networks and DeepLab) for the SNN domain. We conduct
experiments on two public semantic segmentation benchmarks including the PASCAL
VOC2012 dataset and the DDD17 event-based dataset. In addition to showing the
feasibility of SNNs for semantic segmentation, we show that SNNs can be more
robust and energy-efficient compared to their ANN counterparts in this domain.",arxiv
http://arxiv.org/abs/2009.05533v1,2020-09-11T17:09:35Z,2020-09-11T17:09:35Z,Deep Learning Interference Cancellation in Wireless Networks,"With the crowding of the electromagnetic spectrum and the shrinking cell size
in wireless networks, crosstalk between base stations and users is a major
problem. Although hand-crafted functional blocks and coding schemes are proven
effective to guarantee reliable data transfer, currently deep learning-based
approaches have drawn increasing attention in the communication system
modeling. In this paper, we propose a Neural Network (NN) based signal
processing technique that works with traditional DSP algorithms to overcome the
interference problem in realtime. This technique doesn't require any feedback
protocol between the receiver and transmitter which makes it very suitable for
low-latency and high data-rate applications such as autonomy and augmented
reality. While there has been recent work on the use of Reinforcement Learning
(RL) in the control layer to manage and control the interference, our approach
is novel in the sense that it introduces a neural network for signal processing
at baseband data rate and in the physical layer. We demonstrate this ""Deep
Interference Cancellation"" technique using a convolutional LSTM autoencoder.
When applied to QAM-OFDM modulated data, the network produces significant
improvement in the symbol error rate (SER). We further discuss the hardware
implementation including latency, power consumption, memory requirements, and
chip area.",arxiv
http://arxiv.org/abs/2109.09929v1,2021-09-21T03:12:23Z,2021-09-21T03:12:23Z,"A Unified Approach of Detecting Misleading Images via Tracing its
  Instances on Web and Analysing its Past Context for the Verification of
  Content","The verification of multimedia content over social media is one of the
challenging and crucial issues in the current scenario and gaining prominence
in an age where user-generated content and online social web platforms are the
leading sources in shaping and propagating news stories. As these sources allow
users to share their opinions without restriction, opportunistic users often
post misleading/ unreliable content on social media such as Twitter, Facebook,
etc. At present, to lure users towards the news story, the text is often
attached with some multimedia content (images/videos/audios). Verifying these
contents to maintain the credibility and reliability of social media
information is of paramount importance. Motivated by this, we proposed a
generalized system that supports the automatic classification of images into
credible or misleading. In this paper, we investigated machine learning-based
as well as deep learning-based approaches utilized to verify misleading
multimedia content, where the available image traces are used to identify the
credibility of the content. The experiment is performed on the real-world
dataset (Media-eval-2015 dataset) collected from Twitter. It also demonstrates
the efficiency of our proposed approach and features using both Machine and
Deep Learning Model (Bi-directional LSTM). The experiment result reveals that
the Microsoft bings image search engine is quite effective in retrieving titles
and performs better than our study's Google image search engine. It also shows
that gathering clues from attached multimedia content (image) is more effective
than detecting only posted content-based features.",arxiv
http://arxiv.org/abs/2103.03928v1,2021-03-05T20:22:52Z,2021-03-05T20:22:52Z,Accelerator Real-time Edge AI for Distributed Systems (READS) Proposal,"Our objective will be to integrate ML into Fermilab accelerator operations
and furthermore provide an accessible framework which can also be used by a
broad range of other accelerator systems with dynamic tuning needs. We will
develop of real-time accelerator control using embedded ML on-chip hardware and
fast communication between distributed systems in this proposal. We will
demonstrate this technology for the Mu2e experiment by increasing the overall
duty factor and uptime of the experiment through two synergistic projects.
First, we will use deep reinforcement learning techniques to improve the
performance of the regulation loop through guided optimization to provide
stable proton beams extracted from the Delivery Ring to the Mu2e experiment.
This requires the development of a digital twin of the system to model the
accelerator and develop real-time ML algorithms. Second, we will use
de-blending techniques to disentangle and classify overlapping beam losses in
the Main Injector and Recycler Ring to reduce overall beam downtime in each
machine. This ML model will be deployed within a semi-autonomous operational
mode. Both applications require processing at the millisecond scale and will
share similar ML-in-hardware techniques and beam instrumentation readout
technology. A collaboration between Fermilab and Northwestern University will
pull together the talents and resources of accelerator physicists, beam
instrumentation engineers, embedded system architects, FPGA board design
experts, and ML experts to solve complex real-time accelerator controls
challenges which will enhance the physics program. More broadly, the framework
developed for Accelerator Real-time Edge AI Distributed Systems (READS) can be
applied to future projects as the accelerator complex is upgraded for the
PIP-II and DUNE era.",arxiv
http://arxiv.org/abs/1904.13255v2,2019-06-10T18:34:54Z,2019-04-30T13:53:29Z,"Generative Adversarial Imagination for Sample Efficient Deep
  Reinforcement Learning","Reinforcement learning has seen great advancements in the past five years.
The successful introduction of deep learning in place of more traditional
methods allowed reinforcement learning to scale to very complex domains
achieving super-human performance in environments like the game of Go or
numerous video games. Despite great successes in multiple domains, these new
methods suffer from their own issues that make them often inapplicable to the
real world problems. Extreme lack of data efficiency, together with huge
variance and difficulty in enforcing safety constraints, is one of the three
most prominent issues in the field. Usually, millions of data points sampled
from the environment are necessary for these algorithms to converge to
acceptable policies.
  This thesis proposes novel Generative Adversarial Imaginative Reinforcement
Learning algorithm. It takes advantage of the recent introduction of highly
effective generative adversarial models, and Markov property that underpins
reinforcement learning setting, to model dynamics of the real environment
within the internal imagination module. Rollouts from the imagination are then
used to artificially simulate the real environment in a standard reinforcement
learning process to avoid, often expensive and dangerous, trial and error in
the real environment. Experimental results show that the proposed algorithm
more economically utilises experience from the real environment than the
current state-of-the-art Rainbow DQN algorithm, and thus makes an important
step towards sample efficient deep reinforcement learning.",arxiv
http://arxiv.org/abs/1910.09281v2,2019-11-11T14:18:31Z,2019-10-21T12:06:28Z,Dealing with Sparse Rewards in Reinforcement Learning,"Successfully navigating a complex environment to obtain a desired outcome is
a difficult task, that up to recently was believed to be capable only by
humans. This perception has been broken down over time, especially with the
introduction of deep reinforcement learning, which has greatly increased the
difficulty of tasks that can be automated. However, for traditional
reinforcement learning agents this requires an environment to be able to
provide frequent extrinsic rewards, which are not known or accessible for many
real-world environments. This project aims to explore and contrast existing
reinforcement learning solutions that circumnavigate the difficulties of an
environment that provide sparse rewards. Different reinforcement solutions will
be implemented over a several video game environments with varying difficulty
and varying frequency of rewards, as to properly investigate the applicability
of these solutions. This project introduces a novel reinforcement learning
solution by combining aspects of two existing state of the art sparse reward
solutions, curiosity driven exploration and unsupervised auxiliary tasks.",arxiv
http://arxiv.org/abs/1905.00741v2,2020-03-23T11:39:51Z,2019-05-02T13:42:51Z,From Video Game to Real Robot: The Transfer between Action Spaces,"Deep reinforcement learning has proven to be successful for learning tasks in
simulated environments, but applying same techniques for robots in real-world
domain is more challenging, as they require hours of training. To address this,
transfer learning can be used to train the policy first in a simulated
environment and then transfer it to physical agent. As the simulation never
matches reality perfectly, the physics, visuals and action spaces by necessity
differ between these environments to some degree. In this work, we study how
general video games can be directly used instead of fine-tuned simulations for
the sim-to-real transfer. Especially, we study how the agent can learn the new
action space autonomously, when the game actions do not match the robot
actions. Our results show that the different action space can be learned by
re-training only part of neural network and we obtain above 90% mean success
rate in simulation and robot experiments.",arxiv
http://arxiv.org/abs/2104.02541v1,2021-04-06T14:31:23Z,2021-04-06T14:31:23Z,"Instantaneous Stereo Depth Estimation of Real-World Stimuli with a
  Neuromorphic Stereo-Vision Setup","The stereo-matching problem, i.e., matching corresponding features in two
different views to reconstruct depth, is efficiently solved in biology. Yet, it
remains the computational bottleneck for classical machine vision approaches.
By exploiting the properties of event cameras, recently proposed Spiking Neural
Network (SNN) architectures for stereo vision have the potential of simplifying
the stereo-matching problem. Several solutions that combine event cameras with
spike-based neuromorphic processors already exist. However, they are either
simulated on digital hardware or tested on simplified stimuli. In this work, we
use the Dynamic Vision Sensor 3D Human Pose Dataset (DHP19) to validate a
brain-inspired event-based stereo-matching architecture implemented on a
mixed-signal neuromorphic processor with real-world data. Our experiments show
that this SNN architecture, composed of coincidence detectors and disparity
sensitive neurons, is able to provide a coarse estimate of the input disparity
instantaneously, thereby detecting the presence of a stimulus moving in depth
in real-time.",arxiv
http://arxiv.org/abs/2006.05669v1,2020-06-10T06:08:20Z,2020-06-10T06:08:20Z,"Interpretable Multimodal Learning for Intelligent Regulation in Online
  Payment Systems","With the explosive growth of transaction activities in online payment
systems, effective and realtime regulation becomes a critical problem for
payment service providers. Thanks to the rapid development of artificial
intelligence (AI), AI-enable regulation emerges as a promising solution. One
main challenge of the AI-enabled regulation is how to utilize multimedia
information, i.e., multimodal signals, in Financial Technology (FinTech).
Inspired by the attention mechanism in nature language processing, we propose a
novel cross-modal and intra-modal attention network (CIAN) to investigate the
relation between the text and transaction. More specifically, we integrate the
text and transaction information to enhance the text-trade jointembedding
learning, which clusters positive pairs and push negative pairs away from each
other. Another challenge of intelligent regulation is the interpretability of
complicated machine learning models. To sustain the requirements of financial
regulation, we design a CIAN-Explainer to interpret how the attention mechanism
interacts the original features, which is formulated as a low-rank matrix
approximation problem. With the real datasets from the largest online payment
system, WeChat Pay of Tencent, we conduct experiments to validate the practical
application value of CIAN, where our method outperforms the state-of-the-art
methods.",arxiv
http://arxiv.org/abs/2104.06106v1,2021-04-13T11:23:39Z,2021-04-13T11:23:39Z,"Level Generation for Angry Birds with Sequential VAE and Latent Variable
  Evolution","Video game level generation based on machine learning (ML), in particular,
deep generative models, has attracted attention as a technique to automate
level generation. However, applications of existing ML-based level generations
are mostly limited to tile-based level representation. When ML techniques are
applied to game domains with non-tile-based level representation, such as Angry
Birds, where objects in a level are specified by real-valued parameters, ML
often fails to generate playable levels. In this study, we develop a
deep-generative-model-based level generation for the game domain of Angry
Birds. To overcome these drawbacks, we propose a sequential encoding of a level
and process it as text data, whereas existing approaches employ a tile-based
encoding and process it as an image. Experiments show that the proposed level
generator drastically improves the stability and diversity of generated levels
compared with existing approaches. We apply latent variable evolution with the
proposed generator to control the feature of a generated level computed through
an AI agent's play, while keeping the level stable and natural.",arxiv
http://arxiv.org/abs/2110.01493v1,2021-10-04T15:02:23Z,2021-10-04T15:02:23Z,"Exploiting Pre-Trained ASR Models for Alzheimer's Disease Recognition
  Through Spontaneous Speech","Alzheimer's disease (AD) is a progressive neurodegenerative disease and
recently attracts extensive attention worldwide. Speech technology is
considered a promising solution for the early diagnosis of AD and has been
enthusiastically studied. Most recent works concentrate on the use of advanced
BERT-like classifiers for AD detection. Input to these classifiers are speech
transcripts produced by automatic speech recognition (ASR) models. The major
challenge is that the quality of transcription could degrade significantly
under complex acoustic conditions in the real world. The detection performance,
in consequence, is largely limited. This paper tackles the problem via
tailoring and adapting pre-trained neural-network based ASR model for the
downstream AD recognition task. Only bottom layers of the ASR model are
retained. A simple fully-connected neural network is added on top of the
tailored ASR model for classification. The heavy BERT classifier is discarded.
The resulting model is light-weight and can be fine-tuned in an end-to-end
manner for AD recognition. Our proposed approach takes only raw speech as
input, and no extra transcription process is required. The linguistic
information of speech is implicitly encoded in the tailored ASR model and
contributes to boosting the performance. Experiments show that our proposed
approach outperforms the best manual transcript-based RoBERTa by an absolute
margin of 4.6% in terms of accuracy. Our best-performing models achieve the
accuracy of 83.2% and 78.0% in the long-audio and short-audio competition
tracks of the 2021 NCMMSC Alzheimer's Disease Recognition Challenge,
respectively.",arxiv
http://arxiv.org/abs/1901.04985v1,2019-01-12T06:00:05Z,2019-01-12T06:00:05Z,"NNStreamer: Stream Processing Paradigm for Neural Networks, Toward
  Efficient Development and Execution of On-Device AI Applications","We propose nnstreamer, a software system that handles neural networks as
filters of stream pipelines, applying the stream processing paradigm to neural
network applications. A new trend with the wide-spread of deep neural network
applications is on-device AI; i.e., processing neural networks directly on
mobile devices or edge/IoT devices instead of cloud servers. Emerging privacy
issues, data transmission costs, and operational costs signifies the need for
on-device AI especially when a huge number of devices with real-time data
processing are deployed. Nnstreamer efficiently handles neural networks with
complex data stream pipelines on devices, improving the overall performance
significantly with minimal efforts. Besides, nnstreamer simplifies the neural
network pipeline implementations and allows reusing off-shelf multimedia stream
filters directly; thus it reduces the developmental costs significantly.
Nnstreamer is already being deployed with a product releasing soon and is open
source software applicable to a wide range of hardware architectures and
software platforms.",arxiv
http://arxiv.org/abs/2110.04170v1,2021-10-08T14:53:06Z,2021-10-08T14:53:06Z,Multi-fidelity information fusion with concatenated neural networks,"Recently, computational modeling has shifted towards the use of deep
learning, and other data-driven modeling frameworks. Although this shift in
modeling holds promise in many applications like design optimization and
real-time control by lowering the computational burden, training deep learning
models needs a huge amount of data. This big data is not always available for
scientific problems and leads to poorly generalizable data-driven models. This
gap can be furnished by leveraging information from physics-based models.
Exploiting prior knowledge about the problem at hand, this study puts forth a
concatenated neural network approach to build more tailored, effective, and
efficient machine learning models. For our analysis, without losing its
generalizability and modularity, we focus on the development of predictive
models for laminar and turbulent boundary layer flows. In particular, we
combine the self-similarity solution and power-law velocity profile
(low-fidelity models) with the noisy data obtained either from experiments or
computational fluid dynamics simulations (high-fidelity models) through a
concatenated neural network. We illustrate how the knowledge from these
simplified models results in reducing uncertainties associated with deep
learning models. The proposed framework produces physically consistent models
that attempt to achieve better generalization than data-driven models obtained
purely based on data. While we demonstrate our framework for a problem relevant
to fluid mechanics, its workflow and principles can be adopted for many
scientific problems where empirical models are prevalent. In line with grand
demands in novel physics-guided machine learning principles, this work builds a
bridge between extensive physics-based theories and data-driven modeling
paradigms and paves the way for using hybrid modeling approaches for
next-generation digital twin technologies.",arxiv
http://arxiv.org/abs/1712.01192v1,2017-12-04T16:54:12Z,2017-12-04T16:54:12Z,"Mixed-precision training of deep neural networks using computational
  memory","Deep neural networks have revolutionized the field of machine learning by
providing unprecedented human-like performance in solving many real-world
problems such as image and speech recognition. Training of large DNNs, however,
is a computationally intensive task, and this necessitates the development of
novel computing architectures targeting this application. A computational
memory unit where resistive memory devices are organized in crossbar arrays can
be used to locally store the synaptic weights in their conductance states. The
expensive multiply accumulate operations can be performed in place using
Kirchhoff's circuit laws in a non-von Neumann manner. However, a key challenge
remains the inability to alter the conductance states of the devices in a
reliable manner during the weight update process. We propose a mixed-precision
architecture that combines a computational memory unit storing the synaptic
weights with a digital processing unit and an additional memory unit
accumulating weight updates in high precision. The new architecture delivers
classification accuracies comparable to those of floating-point implementations
without being constrained by challenges associated with the non-ideal weight
update characteristics of emerging resistive memories. A two layer neural
network in which the computational memory unit is realized using non-linear
stochastic models of phase-change memory devices achieves a test accuracy of
97.40% on the MNIST handwritten digit classification problem.",arxiv
http://arxiv.org/abs/1806.04533v2,2018-06-21T02:20:59Z,2018-06-11T03:40:06Z,"Cross-dataset Person Re-Identification Using Similarity Preserved
  Generative Adversarial Networks","Person re-identification (Re-ID) aims to match the image frames which contain
the same person in the surveillance videos. Most of the Re-ID algorithms
conduct supervised training in some small labeled datasets, so directly
deploying these trained models to the real-world large camera networks may lead
to a poor performance due to underfitting. The significant difference between
the source training dataset and the target testing dataset makes it challenging
to incrementally optimize the model. To address this challenge, we propose a
novel solution by transforming the unlabeled images in the target domain to fit
the original classifier by using our proposed similarity preserved generative
adversarial networks model, SimPGAN. Specifically, SimPGAN adopts the
generative adversarial networks with the cycle consistency constraint to
transform the unlabeled images in the target domain to the style of the source
domain. Meanwhile, SimPGAN uses the similarity consistency loss, which is
measured by a siamese deep convolutional neural network, to preserve the
similarity of the transformed images of the same person. Comprehensive
experiments based on multiple real surveillance datasets are conducted, and the
results show that our algorithm is better than the state-of-the-art
cross-dataset unsupervised person Re-ID algorithms.",arxiv
http://arxiv.org/abs/2109.00984v1,2021-09-02T14:36:55Z,2021-09-02T14:36:55Z,CrypTen: Secure Multi-Party Computation Meets Machine Learning,"Secure multi-party computation (MPC) allows parties to perform computations
on data while keeping that data private. This capability has great potential
for machine-learning applications: it facilitates training of machine-learning
models on private data sets owned by different parties, evaluation of one
party's private model using another party's private data, etc. Although a range
of studies implement machine-learning models via secure MPC, such
implementations are not yet mainstream. Adoption of secure MPC is hampered by
the absence of flexible software frameworks that ""speak the language"" of
machine-learning researchers and engineers. To foster adoption of secure MPC in
machine learning, we present CrypTen: a software framework that exposes popular
secure MPC primitives via abstractions that are common in modern
machine-learning frameworks, such as tensor computations, automatic
differentiation, and modular neural networks. This paper describes the design
of CrypTen and measure its performance on state-of-the-art models for text
classification, speech recognition, and image classification. Our benchmarks
show that CrypTen's GPU support and high-performance communication between (an
arbitrary number of) parties allows it to perform efficient private evaluation
of modern machine-learning models under a semi-honest threat model. For
example, two parties using CrypTen can securely predict phonemes in speech
recordings using Wav2Letter faster than real-time. We hope that CrypTen will
spur adoption of secure MPC in the machine-learning community.",arxiv
http://arxiv.org/abs/2008.02604v2,2021-03-25T05:28:18Z,2020-08-06T12:25:18Z,"Deep Learning Based Defect Detection for Solder Joints on Industrial
  X-Ray Circuit Board Images","Quality control is of vital importance during electronics production. As the
methods of producing electronic circuits improve, there is an increasing chance
of solder defects during assembling the printed circuit board (PCB). Many
technologies have been incorporated for inspecting failed soldering, such as
X-ray imaging, optical imaging, and thermal imaging. With some advanced
algorithms, the new technologies are expected to control the production quality
based on the digital images. However, current algorithms sometimes are not
accurate enough to meet the quality control. Specialists are needed to do a
follow-up checking. For automated X-ray inspection, joint of interest on the
X-ray image is located by region of interest (ROI) and inspected by some
algorithms. Some incorrect ROIs deteriorate the inspection algorithm. The high
dimension of X-ray images and the varying sizes of image dimensions also
challenge the inspection algorithms. On the other hand, recent advances on deep
learning shed light on image-based tasks and are competitive to human levels.
In this paper, deep learning is incorporated in X-ray imaging based quality
control during PCB quality inspection. Two artificial intelligence (AI) based
models are proposed and compared for joint defect detection. The noised ROI
problem and the varying sizes of imaging dimension problem are addressed. The
efficacy of the proposed methods are verified through experimenting on a
real-world 3D X-ray dataset. By incorporating the proposed methods, specialist
inspection workload is largely saved.",arxiv
http://arxiv.org/abs/2110.03660v1,2021-08-17T18:01:12Z,2021-08-17T18:01:12Z,"Developing Medical AI : a cloud-native audio-visual data collection
  study","Designing Artificial Intelligence (AI) solutions that can operate in
real-world situations is a highly complex task. Deploying such solutions in the
medical domain is even more challenging. The promise of using AI to improve
patient care and reduce cost has encouraged many companies to undertake such
endeavours. For our team, the goal has been to improve early identification of
deteriorating patients in the hospital. Identifying patient deterioration in
lower acuity wards relies, to a large degree on the attention and intuition of
clinicians, rather than on the presence of physiological monitoring devices. In
these care areas, an automated tool which could continuously observe patients
and notify the clinical staff of suspected deterioration, would be extremely
valuable. In order to develop such an AI-enabled tool, a large collection of
patient images and audio correlated with corresponding vital signs, past
medical history and clinical outcome would be indispensable. To the best of our
knowledge, no such public or for-pay data set currently exists. This lack of
audio-visual data led to the decision to conduct exactly such study. The main
contributions of this paper are, the description of a protocol for audio-visual
data collection study, a cloud-architecture for efficiently processing and
consuming such data, and the design of a specific data collection device.",arxiv
http://arxiv.org/abs/2106.02964v1,2021-06-05T21:15:34Z,2021-06-05T21:15:34Z,"A Review of Machine Learning Classification Using Quantum Annealing for
  Real-world Applications","Optimizing the training of a machine learning pipeline helps in reducing
training costs and improving model performance. One such optimizing strategy is
quantum annealing, which is an emerging computing paradigm that has shown
potential in optimizing the training of a machine learning model. The
implementation of a physical quantum annealer has been realized by D-Wave
systems and is available to the research community for experiments. Recent
experimental results on a variety of machine learning applications using
quantum annealing have shown interesting results where the performance of
classical machine learning techniques is limited by limited training data and
high dimensional features. This article explores the application of D-Wave's
quantum annealer for optimizing machine learning pipelines for real-world
classification problems. We review the application domains on which a physical
quantum annealer has been used to train machine learning classifiers. We
discuss and analyze the experiments performed on the D-Wave quantum annealer
for applications such as image recognition, remote sensing imagery,
computational biology, and particle physics. We discuss the possible advantages
and the problems for which quantum annealing is likely to be advantageous over
classical computation.",arxiv
http://arxiv.org/abs/2008.13222v2,2020-12-09T08:19:43Z,2020-08-30T17:29:19Z,Improved Lite Audio-Visual Speech Enhancement,"Numerous studies have investigated the effectiveness of audio-visual
multimodal learning for speech enhancement (AVSE) tasks, seeking a solution
that uses visual data as auxiliary and complementary input to reduce the noise
of noisy speech signals. Recently, we proposed a lite audio-visual speech
enhancement (LAVSE) algorithm. Compared to conventional AVSE systems, LAVSE
requires less online computation and moderately solves the user privacy problem
on facial data. In this study, we extend LAVSE to improve its ability to
address three practical issues often encountered in implementing AVSE systems,
namely, the requirement for additional visual data, audio-visual
asynchronization, and low-quality visual data. The proposed system is termed
improved LAVSE (iLAVSE), which uses a convolutional recurrent neural network
architecture as the core AVSE model. We evaluate iLAVSE on the Taiwan Mandarin
speech with video dataset. Experimental results confirm that compared to
conventional AVSE systems, iLAVSE can effectively overcome the aforementioned
three practical issues and can improve enhancement performance. The results
also confirm that iLAVSE is suitable for real-world scenarios, where
high-quality audio-visual sensors may not always be available.",arxiv
http://arxiv.org/abs/2109.09165v1,2021-09-19T16:59:01Z,2021-09-19T16:59:01Z,Traffic-Net: 3D Traffic Monitoring Using a Single Camera,"Computer Vision has played a major role in Intelligent Transportation Systems
(ITS) and traffic surveillance. Along with the rapidly growing automated
vehicles and crowded cities, the automated and advanced traffic management
systems (ATMS) using video surveillance infrastructures have been evolved by
the implementation of Deep Neural Networks. In this research, we provide a
practical platform for real-time traffic monitoring, including 3D
vehicle/pedestrian detection, speed detection, trajectory estimation,
congestion detection, as well as monitoring the interaction of vehicles and
pedestrians, all using a single CCTV traffic camera. We adapt a custom YOLOv5
deep neural network model for vehicle/pedestrian detection and an enhanced SORT
tracking algorithm. For the first time, a hybrid satellite-ground based inverse
perspective mapping (SG-IPM) method for camera auto-calibration is also
developed which leads to an accurate 3D object detection and visualisation. We
also develop a hierarchical traffic modelling solution based on short- and
long-term temporal video data stream to understand the traffic flow,
bottlenecks, and risky spots for vulnerable road users. Several experiments on
real-world scenarios and comparisons with state-of-the-art are conducted using
various traffic monitoring datasets, including MIO-TCD, UA-DETRAC and GRAM-RTM
collected from highways, intersections, and urban areas under different
lighting and weather conditions.",arxiv
http://arxiv.org/abs/1707.06600v2,2017-09-06T17:32:44Z,2017-07-20T16:35:02Z,"A multi-agent reinforcement learning model of common-pool resource
  appropriation","Humanity faces numerous problems of common-pool resource appropriation. This
class of multi-agent social dilemma includes the problems of ensuring
sustainable use of fresh water, common fisheries, grazing pastures, and
irrigation systems. Abstract models of common-pool resource appropriation based
on non-cooperative game theory predict that self-interested agents will
generally fail to find socially positive equilibria---a phenomenon called the
tragedy of the commons. However, in reality, human societies are sometimes able
to discover and implement stable cooperative solutions. Decades of behavioral
game theory research have sought to uncover aspects of human behavior that make
this possible. Most of that work was based on laboratory experiments where
participants only make a single choice: how much to appropriate. Recognizing
the importance of spatial and temporal resource dynamics, a recent trend has
been toward experiments in more complex real-time video game-like environments.
However, standard methods of non-cooperative game theory can no longer be used
to generate predictions for this case. Here we show that deep reinforcement
learning can be used instead. To that end, we study the emergent behavior of
groups of independently learning agents in a partially observed Markov game
modeling common-pool resource appropriation. Our experiments highlight the
importance of trial-and-error learning in common-pool resource appropriation
and shed light on the relationship between exclusion, sustainability, and
inequality.",arxiv
http://arxiv.org/abs/2010.08833v1,2020-10-17T17:48:04Z,2020-10-17T17:48:04Z,"Efficient and Compact Convolutional Neural Network Architectures for
  Non-temporal Real-time Fire Detection","Automatic visual fire detection is used to complement traditional fire
detection sensor systems (smoke/heat). In this work, we investigate different
Convolutional Neural Network (CNN) architectures and their variants for the
non-temporal real-time bounds detection of fire pixel regions in video (or
still) imagery. Two reduced complexity compact CNN architectures
(NasNet-A-OnFire and ShuffleNetV2-OnFire) are proposed through experimental
analysis to optimise the computational efficiency for this task. The results
improve upon the current state-of-the-art solution for fire detection,
achieving an accuracy of 95% for full-frame binary classification and 97% for
superpixel localisation. We notably achieve a classification speed up by a
factor of 2.3x for binary classification and 1.3x for superpixel localisation,
with runtime of 40 fps and 18 fps respectively, outperforming prior work in the
field presenting an efficient, robust and real-time solution for fire region
detection. Subsequent implementation on low-powered devices (Nvidia Xavier-NX,
achieving 49 fps for full-frame classification via ShuffleNetV2-OnFire)
demonstrates our architectures are suitable for various real-world deployment
applications.",arxiv
http://arxiv.org/abs/2009.06875v2,2021-02-02T18:57:46Z,2020-09-15T05:50:13Z,Optical Gaze Tracking with Spatially-Sparse Single-Pixel Detectors,"Gaze tracking is an essential component of next generation displays for
virtual reality and augmented reality applications. Traditional camera-based
gaze trackers used in next generation displays are known to be lacking in one
or multiple of the following metrics: power consumption, cost, computational
complexity, estimation accuracy, latency, and form-factor. We propose the use
of discrete photodiodes and light-emitting diodes (LEDs) as an alternative to
traditional camera-based gaze tracking approaches while taking all of these
metrics into consideration. We begin by developing a rendering-based simulation
framework for understanding the relationship between light sources and a
virtual model eyeball. Findings from this framework are used for the placement
of LEDs and photodiodes. Our first prototype uses a neural network to obtain an
average error rate of 2.67{\deg} at 400Hz while demanding only 16mW. By
simplifying the implementation to using only LEDs, duplexed as light
transceivers, and more minimal machine learning model, namely a light-weight
supervised Gaussian process regression algorithm, we show that our second
prototype is capable of an average error rate of 1.57{\deg} at 250 Hz using 800
mW.",arxiv
http://arxiv.org/abs/1901.08097v1,2019-01-23T19:43:58Z,2019-01-23T19:43:58Z,"Can Adversarial Networks Hallucinate Occluded People With a Plausible
  Aspect?","When you see a person in a crowd, occluded by other persons, you miss visual
information that can be used to recognize, re-identify or simply classify him
or her. You can imagine its appearance given your experience, nothing more.
Similarly, AI solutions can try to hallucinate missing information with
specific deep learning architectures, suitably trained with people with and
without occlusions. The goal of this work is to generate a complete image of a
person, given an occluded version in input, that should be a) without occlusion
b) similar at pixel level to a completely visible people shape c) capable to
conserve similar visual attributes (e.g. male/female) of the original one. For
the purpose, we propose a new approach by integrating the state-of-the-art of
neural network architectures, namely U-nets and GANs, as well as discriminative
attribute classification nets, with an architecture specifically designed to
de-occlude people shapes. The network is trained to optimize a Loss function
which could take into account the aforementioned objectives. As well we propose
two datasets for testing our solution: the first one, occluded RAP, created
automatically by occluding real shapes of the RAP dataset (which collects also
attributes of the people aspect); the second is a large synthetic dataset, AiC,
generated in computer graphics with data extracted from the GTA video game,
that contains 3D data of occluded objects by construction. Results are
impressive and outperform any other previous proposal. This result could be an
initial step to many further researches to recognize people and their behavior
in an open crowded world.",arxiv
http://arxiv.org/abs/1807.07769v2,2018-10-05T18:07:23Z,2018-07-20T10:14:27Z,Physical Adversarial Examples for Object Detectors,"Deep neural networks (DNNs) are vulnerable to adversarial
examples-maliciously crafted inputs that cause DNNs to make incorrect
predictions. Recent work has shown that these attacks generalize to the
physical domain, to create perturbations on physical objects that fool image
classifiers under a variety of real-world conditions. Such attacks pose a risk
to deep learning models used in safety-critical cyber-physical systems. In this
work, we extend physical attacks to more challenging object detection models, a
broader class of deep learning algorithms widely used to detect and label
multiple objects within a scene. Improving upon a previous physical attack on
image classifiers, we create perturbed physical objects that are either ignored
or mislabeled by object detection models. We implement a Disappearance Attack,
in which we cause a Stop sign to ""disappear"" according to the detector-either
by covering thesign with an adversarial Stop sign poster, or by adding
adversarial stickers onto the sign. In a video recorded in a controlled lab
environment, the state-of-the-art YOLOv2 detector failed to recognize these
adversarial Stop signs in over 85% of the video frames. In an outdoor
experiment, YOLO was fooled by the poster and sticker attacks in 72.5% and
63.5% of the video frames respectively. We also use Faster R-CNN, a different
object detection model, to demonstrate the transferability of our adversarial
perturbations. The created poster perturbation is able to fool Faster R-CNN in
85.9% of the video frames in a controlled lab environment, and 40.2% of the
video frames in an outdoor environment. Finally, we present preliminary results
with a new Creation Attack, where in innocuous physical stickers fool a model
into detecting nonexistent objects.",arxiv
http://arxiv.org/abs/2107.11750v2,2021-07-30T08:42:18Z,2021-07-25T07:52:53Z,"Improving Variational Autoencoder based Out-of-Distribution Detection
  for Embedded Real-time Applications","Uncertainties in machine learning are a significant roadblock for its
application in safety-critical cyber-physical systems (CPS). One source of
uncertainty arises from distribution shifts in the input data between training
and test scenarios. Detecting such distribution shifts in real-time is an
emerging approach to address the challenge. The high dimensional input space in
CPS applications involving imaging adds extra difficulty to the task.
Generative learning models are widely adopted for the task, namely
out-of-distribution (OoD) detection. To improve the state-of-the-art, we
studied existing proposals from both machine learning and CPS fields. In the
latter, safety monitoring in real-time for autonomous driving agents has been a
focus. Exploiting the spatiotemporal correlation of motion in videos, we can
robustly detect hazardous motion around autonomous driving agents. Inspired by
the latest advances in the Variational Autoencoder (VAE) theory and practice,
we tapped into the prior knowledge in data to further boost OoD detection's
robustness. Comparison studies over nuScenes and Synthia data sets show our
methods significantly improve detection capabilities of OoD factors unique to
driving scenarios, 42% better than state-of-the-art approaches. Our model also
generalized near-perfectly, 97% better than the state-of-the-art across the
real-world and simulation driving data sets experimented. Finally, we
customized one proposed method into a twin-encoder model that can be deployed
to resource limited embedded devices for real-time OoD detection. Its execution
time was reduced over four times in low-precision 8-bit integer inference,
while detection capability is comparable to its corresponding floating-point
model.",arxiv
http://arxiv.org/abs/2008.13369v1,2020-08-31T05:12:57Z,2020-08-31T05:12:57Z,"Introducing Representations of Facial Affect in Automated Multimodal
  Deception Detection","Automated deception detection systems can enhance health, justice, and
security in society by helping humans detect deceivers in high-stakes
situations across medical and legal domains, among others. This paper presents
a novel analysis of the discriminative power of dimensional representations of
facial affect for automated deception detection, along with interpretable
features from visual, vocal, and verbal modalities. We used a video dataset of
people communicating truthfully or deceptively in real-world, high-stakes
courtroom situations. We leveraged recent advances in automated emotion
recognition in-the-wild by implementing a state-of-the-art deep neural network
trained on the Aff-Wild database to extract continuous representations of
facial valence and facial arousal from speakers. We experimented with unimodal
Support Vector Machines (SVM) and SVM-based multimodal fusion methods to
identify effective features, modalities, and modeling approaches for detecting
deception. Unimodal models trained on facial affect achieved an AUC of 80%, and
facial affect contributed towards the highest-performing multimodal approach
(adaptive boosting) that achieved an AUC of 91% when tested on speakers who
were not part of training sets. This approach achieved a higher AUC than
existing automated machine learning approaches that used interpretable visual,
vocal, and verbal features to detect deception in this dataset, but did not use
facial affect. Across all videos, deceptive and truthful speakers exhibited
significant differences in facial valence and facial arousal, contributing
computational support to existing psychological theories on affect and
deception. The demonstrated importance of facial affect in our models informs
and motivates the future development of automated, affect-aware machine
learning approaches for modeling and detecting deception and other social
behaviors in-the-wild.",arxiv
http://arxiv.org/abs/2105.01636v1,2021-05-04T17:27:59Z,2021-05-04T17:27:59Z,Learning 3D Granular Flow Simulations,"Recently, the application of machine learning models has gained momentum in
natural sciences and engineering, which is a natural fit due to the abundance
of data in these fields. However, the modeling of physical processes from
simulation data without first principle solutions remains difficult. Here, we
present a Graph Neural Networks approach towards accurate modeling of complex
3D granular flow simulation processes created by the discrete element method
LIGGGHTS and concentrate on simulations of physical systems found in real world
applications like rotating drums and hoppers. We discuss how to implement Graph
Neural Networks that deal with 3D objects, boundary conditions, particle -
particle, and particle - boundary interactions such that an accurate modeling
of relevant physical quantities is made possible. Finally, we compare the
machine learning based trajectories to LIGGGHTS trajectories in terms of
particle flows and mixing entropies.",arxiv
http://arxiv.org/abs/1601.06473v2,2016-01-26T04:43:20Z,2016-01-25T03:31:24Z,Teaching Robots to Do Object Assembly using Multi-modal 3D Vision,"The motivation of this paper is to develop a smart system using multi-modal
vision for next-generation mechanical assembly. It includes two phases where in
the first phase human beings teach the assembly structure to a robot and in the
second phase the robot finds objects and grasps and assembles them using AI
planning. The crucial part of the system is the precision of 3D visual
detection and the paper presents multi-modal approaches to meet the
requirements: AR markers are used in the teaching phase since human beings can
actively control the process. Point cloud matching and geometric constraints
are used in the robot execution phase to avoid unexpected noises. Experiments
are performed to examine the precision and correctness of the approaches. The
study is practical: The developed approaches are integrated with graph
model-based motion planning, implemented on an industrial robots and applicable
to real-world scenarios.",arxiv
http://arxiv.org/abs/1811.02320v1,2018-11-06T12:32:27Z,2018-11-06T12:32:27Z,Hierarchical Neural Network Architecture In Keyword Spotting,"Keyword Spotting (KWS) provides the start signal of ASR problem, and thus it
is essential to ensure a high recall rate. However, its real-time property
requires low computation complexity. This contradiction inspires people to find
a suitable model which is small enough to perform well in multi environments.
To deal with this contradiction, we implement the Hierarchical Neural
Network(HNN), which is proved to be effective in many speech recognition
problems. HNN outperforms traditional DNN and CNN even though its model size
and computation complexity are slightly less. Also, its simple topology
structure makes easy to deploy on any device.",arxiv
http://arxiv.org/abs/2007.08224v2,2020-07-20T15:42:02Z,2020-07-16T09:50:23Z,SAILenv: Learning in Virtual Visual Environments Made Simple,"Recently, researchers in Machine Learning algorithms, Computer Vision
scientists, engineers and others, showed a growing interest in 3D simulators as
a mean to artificially create experimental settings that are very close to
those in the real world. However, most of the existing platforms to interface
algorithms with 3D environments are often designed to setup navigation-related
experiments, to study physical interactions, or to handle ad-hoc cases that are
not thought to be customized, sometimes lacking a strong photorealistic
appearance and an easy-to-use software interface. In this paper, we present a
novel platform, SAILenv, that is specifically designed to be simple and
customizable, and that allows researchers to experiment visual recognition in
virtual 3D scenes. A few lines of code are needed to interface every algorithm
with the virtual world, and non-3D-graphics experts can easily customize the 3D
environment itself, exploiting a collection of photorealistic objects. Our
framework yields pixel-level semantic and instance labeling, depth, and, to the
best of our knowledge, it is the only one that provides motion-related
information directly inherited from the 3D engine. The client-server
communication operates at a low level, avoiding the overhead of HTTP-based data
exchanges. We perform experiments using a state-of-the-art object detector
trained on real-world images, showing that it is able to recognize the
photorealistic 3D objects of our environment. The computational burden of the
optical flow compares favourably with the estimation performed using modern
GPU-based convolutional networks or more classic implementations. We believe
that the scientific community will benefit from the easiness and high-quality
of our framework to evaluate newly proposed algorithms in their own customized
realistic conditions.",arxiv
http://arxiv.org/abs/2009.09906v1,2020-09-21T14:26:12Z,2020-09-21T14:26:12Z,End-to-End Speaker-Dependent Voice Activity Detection,"Voice activity detection (VAD) is an essential pre-processing step for tasks
such as automatic speech recognition (ASR) and speaker recognition. A basic
goal is to remove silent segments within an audio, while a more general VAD
system could remove all the irrelevant segments such as noise and even unwanted
speech from non-target speakers. We define the task, which only detects the
speech from the target speaker, as speaker-dependent voice activity detection
(SDVAD). This task is quite common in real applications and usually implemented
by performing speaker verification (SV) on audio segments extracted from VAD.
In this paper, we propose an end-to-end neural network based approach to
address this problem, which explicitly takes the speaker identity into the
modeling process. Moreover, inference can be performed in an online fashion,
which leads to low system latency. Experiments are carried out on a
conversational telephone dataset generated from the Switchboard corpus. Results
show that our proposed online approach achieves significantly better
performance than the usual VAD/SV system in terms of both frame accuracy and
F-score. We also used our previously proposed segment-level metric for a more
comprehensive analysis.",arxiv
http://arxiv.org/abs/2012.08637v1,2020-12-15T21:59:58Z,2020-12-15T21:59:58Z,"Multi-Modal Anomaly Detection for Unstructured and Uncertain
  Environments","To achieve high-levels of autonomy, modern robots require the ability to
detect and recover from anomalies and failures with minimal human supervision.
Multi-modal sensor signals could provide more information for such anomaly
detection tasks; however, the fusion of high-dimensional and heterogeneous
sensor modalities remains a challenging problem. We propose a deep learning
neural network: supervised variational autoencoder (SVAE), for failure
identification in unstructured and uncertain environments. Our model leverages
the representational power of VAE to extract robust features from
high-dimensional inputs for supervised learning tasks. The training objective
unifies the generative model and the discriminative model, thus making the
learning a one-stage procedure. Our experiments on real field robot data
demonstrate superior failure identification performance than baseline methods,
and that our model learns interpretable representations. Videos of our results
are available on our website:
https://sites.google.com/illinois.edu/supervised-vae .",arxiv
http://arxiv.org/abs/1702.01182v1,2017-02-03T21:57:13Z,2017-02-03T21:57:13Z,Uncertainty-Aware Reinforcement Learning for Collision Avoidance,"Reinforcement learning can enable complex, adaptive behavior to be learned
automatically for autonomous robotic platforms. However, practical deployment
of reinforcement learning methods must contend with the fact that the training
process itself can be unsafe for the robot. In this paper, we consider the
specific case of a mobile robot learning to navigate an a priori unknown
environment while avoiding collisions. In order to learn collision avoidance,
the robot must experience collisions at training time. However, high-speed
collisions, even at training time, could damage the robot. A successful
learning method must therefore proceed cautiously, experiencing only low-speed
collisions until it gains confidence. To this end, we present an
uncertainty-aware model-based learning algorithm that estimates the probability
of collision together with a statistical estimate of uncertainty. By
formulating an uncertainty-dependent cost function, we show that the algorithm
naturally chooses to proceed cautiously in unfamiliar environments, and
increases the velocity of the robot in settings where it has high confidence.
Our predictive model is based on bootstrapped neural networks using dropout,
allowing it to process raw sensory inputs from high-bandwidth sensors such as
cameras. Our experimental evaluation demonstrates that our method effectively
minimizes dangerous collisions at training time in an obstacle avoidance task
for a simulated and real-world quadrotor, and a real-world RC car. Videos of
the experiments can be found at https://sites.google.com/site/probcoll.",arxiv
http://arxiv.org/abs/2012.15638v2,2021-05-03T07:50:28Z,2020-12-31T14:55:51Z,"CorrNet3D: Unsupervised End-to-end Learning of Dense Correspondence for
  3D Point Clouds","Motivated by the intuition that one can transform two aligned point clouds to
each other more easily and meaningfully than a misaligned pair, we propose
CorrNet3D -- the first unsupervised and end-to-end deep learning-based
framework -- to drive the learning of dense correspondence between 3D shapes by
means of deformation-like reconstruction to overcome the need for annotated
data. Specifically, CorrNet3D consists of a deep feature embedding module and
two novel modules called correspondence indicator and symmetric deformer.
Feeding a pair of raw point clouds, our model first learns the pointwise
features and passes them into the indicator to generate a learnable
correspondence matrix used to permute the input pair. The symmetric deformer,
with an additional regularized loss, transforms the two permuted point clouds
to each other to drive the unsupervised learning of the correspondence. The
extensive experiments on both synthetic and real-world datasets of rigid and
non-rigid 3D shapes show our CorrNet3D outperforms state-of-the-art methods to
a large extent, including those taking meshes as input. CorrNet3D is a flexible
framework in that it can be easily adapted to supervised learning if annotated
data are available. The source code and pre-trained model will be available at
https://github.com/ZENGYIMING-EAMON/CorrNet3D.git.",arxiv
http://arxiv.org/abs/2001.06202v1,2020-01-17T09:02:36Z,2020-01-17T09:02:36Z,"FedVision: An Online Visual Object Detection Platform Powered by
  Federated Learning","Visual object detection is a computer vision-based artificial intelligence
(AI) technique which has many practical applications (e.g., fire hazard
monitoring). However, due to privacy concerns and the high cost of transmitting
video data, it is highly challenging to build object detection models on
centrally stored large training datasets following the current approach.
Federated learning (FL) is a promising approach to resolve this challenge.
Nevertheless, there currently lacks an easy to use tool to enable computer
vision application developers who are not experts in federated learning to
conveniently leverage this technology and apply it in their systems. In this
paper, we report FedVision - a machine learning engineering platform to support
the development of federated learning powered computer vision applications. The
platform has been deployed through a collaboration between WeBank and Extreme
Vision to help customers develop computer vision-based safety monitoring
solutions in smart city applications. Over four months of usage, it has
achieved significant efficiency improvement and cost reduction while removing
the need to transmit sensitive data for three major corporate customers. To the
best of our knowledge, this is the first real application of FL in computer
vision-based tasks.",arxiv
http://arxiv.org/abs/2108.08516v1,2021-08-19T06:14:33Z,2021-08-19T06:14:33Z,Retrieval and Localization with Observation Constraints,"Accurate visual re-localization is very critical to many artificial
intelligence applications, such as augmented reality, virtual reality, robotics
and autonomous driving. To accomplish this task, we propose an integrated
visual re-localization method called RLOCS by combining image retrieval,
semantic consistency and geometry verification to achieve accurate estimations.
The localization pipeline is designed as a coarse-to-fine paradigm. In the
retrieval part, we cascade the architecture of ResNet101-GeM-ArcFace and employ
DBSCAN followed by spatial verification to obtain a better initial coarse pose.
We design a module called observation constraints, which combines geometry
information and semantic consistency for filtering outliers. Comprehensive
experiments are conducted on open datasets, including retrieval on R-Oxford5k
and R-Paris6k, semantic segmentation on Cityscapes, localization on Aachen
Day-Night and InLoc. By creatively modifying separate modules in the total
pipeline, our method achieves many performance improvements on the challenging
localization benchmarks.",arxiv
http://arxiv.org/abs/1903.06539v2,2019-04-28T19:40:48Z,2019-03-13T03:32:53Z,Multi-Geometry Spatial Acoustic Modeling for Distant Speech Recognition,"The use of spatial information with multiple microphones can improve
far-field automatic speech recognition (ASR) accuracy. However, conventional
microphone array techniques degrade speech enhancement performance when there
is an array geometry mismatch between design and test conditions. Moreover,
such speech enhancement techniques do not always yield ASR accuracy improvement
due to the difference between speech enhancement and ASR optimization
objectives. In this work, we propose to unify an acoustic model framework by
optimizing spatial filtering and long short-term memory (LSTM) layers from
multi-channel (MC) input. Our acoustic model subsumes beamformers with multiple
types of array geometry. In contrast to deep clustering methods that treat a
neural network as a black box tool, the network encoding the spatial filters
can process streaming audio data in real time without the accumulation of
target signal statistics. We demonstrate the effectiveness of such MC neural
networks through ASR experiments on the real-world far-field data. We show that
our two-channel acoustic model can on average reduce word error rates (WERs)
by~13.4 and~12.7% compared to a single channel ASR system with the log-mel
filter bank energy (LFBE) feature under the matched and mismatched microphone
placement conditions, respectively. Our result also shows that our two-channel
network achieves a relative WER reduction of over~7.0% compared to conventional
beamforming with seven microphones overall.",arxiv
http://arxiv.org/abs/1905.12340v1,2019-05-29T11:35:18Z,2019-05-29T11:35:18Z,Rethinking Full Connectivity in Recurrent Neural Networks,"Recurrent neural networks (RNNs) are omnipresent in sequence modeling tasks.
Practical models usually consist of several layers of hundreds or thousands of
neurons which are fully connected. This places a heavy computational and memory
burden on hardware, restricting adoption in practical low-cost and low-power
devices. Compared to fully convolutional models, the costly sequential
operation of RNNs severely hinders performance on parallel hardware. This paper
challenges the convention of full connectivity in RNNs. We study structurally
sparse RNNs, showing that they are well suited for acceleration on parallel
hardware, with a greatly reduced cost of the recurrent operations as well as
orders of magnitude less recurrent weights. Extensive experiments on
challenging tasks ranging from language modeling and speech recognition to
video action recognition reveal that structurally sparse RNNs achieve
competitive performance as compared to fully-connected networks. This allows
for using large sparse RNNs for a wide range of real-world tasks that
previously were too costly with fully connected networks.",arxiv
http://arxiv.org/abs/2010.01388v1,2020-10-03T16:55:59Z,2020-10-03T16:55:59Z,Online Neural Networks for Change-Point Detection,"Moments when a time series changes its behaviour are called change points.
Detection of such points is a well-known problem, which can be found in many
applications: quality monitoring of industrial processes, failure detection in
complex systems, health monitoring, speech recognition and video analysis.
Occurrence of change point implies that the state of the system is altered and
its timely detection might help to prevent unwanted consequences. In this
paper, we present two online change-point detection approaches based on neural
networks. These algorithms demonstrate linear computational complexity and are
suitable for change-point detection in large time series. We compare them with
the best known algorithms on various synthetic and real world data sets.
Experiments show that the proposed methods outperform known approaches.",arxiv
http://arxiv.org/abs/1808.04752v2,2018-12-16T08:26:57Z,2018-08-13T14:11:43Z,A Survey on Methods and Theories of Quantized Neural Networks,"Deep neural networks are the state-of-the-art methods for many real-world
tasks, such as computer vision, natural language processing and speech
recognition. For all its popularity, deep neural networks are also criticized
for consuming a lot of memory and draining battery life of devices during
training and inference. This makes it hard to deploy these models on mobile or
embedded devices which have tight resource constraints. Quantization is
recognized as one of the most effective approaches to satisfy the extreme
memory requirements that deep neural network models demand. Instead of adopting
32-bit floating point format to represent weights, quantized representations
store weights using more compact formats such as integers or even binary
numbers. Despite a possible degradation in predictive performance, quantization
provides a potential solution to greatly reduce the model size and the energy
consumption. In this survey, we give a thorough review of different aspects of
quantized neural networks. Current challenges and trends of quantized neural
networks are also discussed.",arxiv
http://arxiv.org/abs/2102.02638v1,2021-02-02T18:50:06Z,2021-02-02T18:50:06Z,"Autodidactic Neurosurgeon: Collaborative Deep Inference for Mobile Edge
  Intelligence via Online Learning","Recent breakthroughs in deep learning (DL) have led to the emergence of many
intelligent mobile applications and services, but in the meanwhile also pose
unprecedented computing challenges on resource-constrained mobile devices. This
paper builds a collaborative deep inference system between a
resource-constrained mobile device and a powerful edge server, aiming at
joining the power of both on-device processing and computation offloading. The
basic idea of this system is to partition a deep neural network (DNN) into a
front-end part running on the mobile device and a back-end part running on the
edge server, with the key challenge being how to locate the optimal partition
point to minimize the end-to-end inference delay. Unlike existing efforts on
DNN partitioning that rely heavily on a dedicated offline profiling stage to
search for the optimal partition point, our system has a built-in online
learning module, called Autodidactic Neurosurgeon (ANS), to automatically learn
the optimal partition point on-the-fly. Therefore, ANS is able to closely
follow the changes of the system environment by generating new knowledge for
adaptive decision making. The core of ANS is a novel contextual bandit learning
algorithm, called $\mu$LinUCB, which not only has provable theoretical learning
performance guarantee but also is ultra-lightweight for easy real-world
implementation. We implement our system on a video stream object detection
testbed to validate the design of ANS and evaluate its performance. The
experiments show that ANS significantly outperforms state-of-the-art benchmarks
in terms of tracking system changes and reducing the end-to-end inference
delay.",arxiv
http://arxiv.org/abs/2107.13619v1,2021-07-28T19:53:05Z,2021-07-28T19:53:05Z,"A Deep Graph Reinforcement Learning Model for Improving User Experience
  in Live Video Streaming","In this paper we present a deep graph reinforcement learning model to predict
and improve the user experience during a live video streaming event,
orchestrated by an agent/tracker. We first formulate the user experience
prediction problem as a classification task, accounting for the fact that most
of the viewers at the beginning of an event have poor quality of experience due
to low-bandwidth connections and limited interactions with the tracker. In our
model we consider different factors that influence the quality of user
experience and train the proposed model on diverse state-action transitions
when viewers interact with the tracker. In addition, provided that past events
have various user experience characteristics we follow a gradient boosting
strategy to compute a global model that learns from different events. Our
experiments with three real-world datasets of live video streaming events
demonstrate the superiority of the proposed model against several baseline
strategies. Moreover, as the majority of the viewers at the beginning of an
event has poor experience, we show that our model can significantly increase
the number of viewers with high quality experience by at least 75% over the
first streaming minutes. Our evaluation datasets and implementation are
publicly available at https://publicresearch.z13.web.core.windows.net",arxiv
http://arxiv.org/abs/1911.00638v1,2019-11-02T03:41:37Z,2019-11-02T03:41:37Z,"Thompson Sampling for Contextual Bandit Problems with Auxiliary Safety
  Constraints","Recent advances in contextual bandit optimization and reinforcement learning
have garnered interest in applying these methods to real-world sequential
decision making problems. Real-world applications frequently have constraints
with respect to a currently deployed policy. Many of the existing
constraint-aware algorithms consider problems with a single objective (the
reward) and a constraint on the reward with respect to a baseline policy.
However, many important applications involve multiple competing objectives and
auxiliary constraints. In this paper, we propose a novel Thompson sampling
algorithm for multi-outcome contextual bandit problems with auxiliary
constraints. We empirically evaluate our algorithm on a synthetic problem.
Lastly, we apply our method to a real world video transcoding problem and
provide a practical way for navigating the trade-off between safety and
performance using Bayesian optimization.",arxiv
http://arxiv.org/abs/2101.04401v2,2021-02-04T13:39:00Z,2021-01-12T10:49:30Z,"Robustness of on-device Models: Adversarial Attack to Deep Learning
  Models on Android Apps","Deep learning has shown its power in many applications, including object
detection in images, natural-language understanding, and speech recognition. To
make it more accessible to end users, many deep learning models are now
embedded in mobile apps. Compared to offloading deep learning from smartphones
to the cloud, performing machine learning on-device can help improve latency,
connectivity, and power consumption. However, most deep learning models within
Android apps can easily be obtained via mature reverse engineering, while the
models' exposure may invite adversarial attacks. In this study, we propose a
simple but effective approach to hacking deep learning models using adversarial
attacks by identifying highly similar pre-trained models from TensorFlow Hub.
All 10 real-world Android apps in the experiment are successfully attacked by
our approach. Apart from the feasibility of the model attack, we also carry out
an empirical study that investigates the characteristics of deep learning
models used by hundreds of Android apps on Google Play. The results show that
many of them are similar to each other and widely use fine-tuning techniques to
pre-trained models on the Internet.",arxiv
http://arxiv.org/abs/2105.01777v1,2021-05-04T21:48:18Z,2021-05-04T21:48:18Z,"PathBench: A Benchmarking Platform for Classical and Learned Path
  Planning Algorithms","Path planning is a key component in mobile robotics. A wide range of path
planning algorithms exist, but few attempts have been made to benchmark the
algorithms holistically or unify their interface. Moreover, with the recent
advances in deep neural networks, there is an urgent need to facilitate the
development and benchmarking of such learning-based planning algorithms. This
paper presents PathBench, a platform for developing, visualizing, training,
testing, and benchmarking of existing and future, classical and learned 2D and
3D path planning algorithms, while offering support for Robot Oper-ating System
(ROS). Many existing path planning algorithms are supported; e.g. A*,
wavefront, rapidly-exploring random tree, value iteration networks, gated path
planning networks; and integrating new algorithms is easy and clearly
specified. We demonstrate the benchmarking capability of PathBench by comparing
implemented classical and learned algorithms for metrics, such as path length,
success rate, computational time and path deviation. These evaluations are done
on built-in PathBench maps and external path planning environments from video
games and real world databases. PathBench is open source.",arxiv
http://arxiv.org/abs/2005.02153v1,2020-04-29T08:46:38Z,2020-04-29T08:46:38Z,"Improving Target-driven Visual Navigation with Attention on 3D Spatial
  Relationships","Embodied artificial intelligence (AI) tasks shift from tasks focusing on
internet images to active settings involving embodied agents that perceive and
act within 3D environments. In this paper, we investigate the target-driven
visual navigation using deep reinforcement learning (DRL) in 3D indoor scenes,
whose navigation task aims to train an agent that can intelligently make a
series of decisions to arrive at a pre-specified target location from any
possible starting positions only based on egocentric views. However, most
navigation methods currently struggle against several challenging problems,
such as data efficiency, automatic obstacle avoidance, and generalization.
Generalization problem means that agent does not have the ability to transfer
navigation skills learned from previous experience to unseen targets and
scenes. To address these issues, we incorporate two designs into classic DRL
framework: attention on 3D knowledge graph (KG) and target skill extension
(TSE) module. On the one hand, our proposed method combines visual features and
3D spatial representations to learn navigation policy. On the other hand, TSE
module is used to generate sub-targets which allow agent to learn from
failures. Specifically, our 3D spatial relationships are encoded through
recently popular graph convolutional network (GCN). Considering the real world
settings, our work also considers open action and adds actionable targets into
conventional navigation situations. Those more difficult settings are applied
to test whether DRL agent really understand its task, navigating environment,
and can carry out reasoning. Our experiments, performed in the AI2-THOR, show
that our model outperforms the baselines in both SR and SPL metrics, and
improves generalization ability across targets and scenes.",arxiv
http://arxiv.org/abs/1805.08692v1,2018-05-04T15:07:29Z,2018-05-04T15:07:29Z,"Assessing a mobile-based deep learning model for plant disease
  surveillance","Convolutional neural network models (CNNs) have made major advances in
computer vision tasks in the last five years. Given the challenge in collecting
real world datasets, most studies report performance metrics based on available
research datasets. In scenarios where CNNs are to be deployed on images or
videos from mobile devices, models are presented with new challenges due to
lighting, angle, and camera specifications, which are not accounted for in
research datasets. It is essential for assessment to also be conducted on real
world datasets if such models are to be reliably integrated with products and
services in society. Plant disease datasets can be used to test CNNs in real
time and gain insight into real world performance. We train a CNN object
detection model to identify foliar symptoms of diseases (or lack thereof) in
cassava (Manihot esculenta Crantz). We then deploy the model on a mobile app
and test its performance on mobile images and video of 720 diseased leaflets in
an agricultural field in Tanzania. Within each disease category we test two
levels of severity of symptoms - mild and pronounced, to assess the model
performance for early detection of symptoms. In both severities we see a
decrease in the F-1 score for real world images and video. The F-1 score
dropped by 32% for pronounced symptoms in real world images (the closest data
to the training data) due to a drop in model recall. If the potential of
smartphone CNNs are to be realized our data suggest it is crucial to consider
tuning precision and recall performance in order to achieve the desired
performance in real world settings. In addition, the varied performance related
to different input data (image or video) is an important consideration for the
design of CNNs in real world applications.",arxiv
http://arxiv.org/abs/2006.11305v2,2021-02-09T21:51:41Z,2020-06-18T04:35:22Z,"Generalization of Agent Behavior through Explicit Representation of
  Context","In order to deploy autonomous agents in digital interactive environments,
they must be able to act robustly in unseen situations. The standard machine
learning approach is to include as much variation as possible into training
these agents. The agents can then interpolate within their training, but they
cannot extrapolate much beyond it. This paper proposes a principled approach
where a context module is coevolved with a skill module in the game. The
context module recognizes the temporal variation in the game and modulates the
outputs of the skill module so that the action decisions can be made robustly
even in previously unseen situations. The approach is evaluated in the Flappy
Bird and LunarLander video games, as well as in the CARLA autonomous driving
simulation. The Context+Skill approach leads to significantly more robust
behavior in environments that require extrapolation beyond training. Such a
principled generalization ability is essential in deploying autonomous agents
in real-world tasks, and can serve as a foundation for continual adaptation as
well.",arxiv
http://arxiv.org/abs/2110.10888v1,2021-10-21T04:19:41Z,2021-10-21T04:19:41Z,"Evaluation of Various Open-Set Medical Imaging Tasks with Deep Neural
  Networks","The current generation of deep neural networks has achieved close-to-human
results on ""closed-set"" image recognition; that is, the classes being evaluated
overlap with the training classes. Many recent methods attempt to address the
importance of the unknown, which are termed ""open-set"" recognition algorithms,
try to reject unknown classes as well as maintain high recognition accuracy on
known classes. However, it is still unclear how different general
domain-trained open-set methods from ImageNet would perform on a different but
more specific domain, such as the medical domain. Without principled and formal
evaluations to measure the effectiveness of those general open-set methods,
artificial intelligence (AI)-based medical diagnostics would experience
ineffective adoption and increased risks of bad decision making. In this paper,
we conduct rigorous evaluations amongst state-of-the-art open-set methods,
exploring different open-set scenarios from ""similar-domain"" to
""different-domain"" scenarios and comparing them on various general and medical
domain datasets. We summarise the results and core ideas and explain how the
models react to various degrees of openness and different distributions of open
classes. We show the main difference between general domain-trained and medical
domain-trained open-set models with our quantitative and qualitative analysis
of the results. We also identify aspects of model robustness in real clinical
workflow usage according to confidence calibration and the inference
efficiency.",arxiv
http://arxiv.org/abs/1811.02566v1,2018-11-06T21:17:34Z,2018-11-06T21:17:34Z,"Bidirectional Quaternion Long-Short Term Memory Recurrent Neural
  Networks for Speech Recognition","Recurrent neural networks (RNN) are at the core of modern automatic speech
recognition (ASR) systems. In particular, long-short term memory (LSTM)
recurrent neural networks have achieved state-of-the-art results in many speech
recognition tasks, due to their efficient representation of long and short term
dependencies in sequences of inter-dependent features. Nonetheless, internal
dependencies within the element composing multidimensional features are weakly
considered by traditional real-valued representations. We propose a novel
quaternion long-short term memory (QLSTM) recurrent neural network that takes
into account both the external relations between the features composing a
sequence, and these internal latent structural dependencies with the quaternion
algebra. QLSTMs are compared to LSTMs during a memory copy-task and a realistic
application of speech recognition on the Wall Street Journal (WSJ) dataset.
QLSTM reaches better performances during the two experiments with up to $2.8$
times less learning parameters, leading to a more expressive representation of
the information.",arxiv
http://arxiv.org/abs/2111.09412v1,2021-10-03T14:03:22Z,2021-10-03T14:03:22Z,"Meta-Reinforcement Learning via Buffering Graph Signatures for Live
  Video Streaming Events","In this study, we present a meta-learning model to adapt the predictions of
the network's capacity between viewers who participate in a live video
streaming event. We propose the MELANIE model, where an event is formulated as
a Markov Decision Process, performing meta-learning on reinforcement learning
tasks. By considering a new event as a task, we design an actor-critic learning
scheme to compute the optimal policy on estimating the viewers' high-bandwidth
connections. To ensure fast adaptation to new connections or changes among
viewers during an event, we implement a prioritized replay memory buffer based
on the Kullback-Leibler divergence of the reward/throughput of the viewers'
connections. Moreover, we adopt a model-agnostic meta-learning framework to
generate a global model from past events. As viewers scarcely participate in
several events, the challenge resides on how to account for the low structural
similarity of different events. To combat this issue, we design a graph
signature buffer to calculate the structural similarities of several streaming
events and adjust the training of the global model accordingly. We evaluate the
proposed model on the link weight prediction task on three real-world datasets
of live video streaming events. Our experiments demonstrate the effectiveness
of our proposed model, with an average relative gain of 25% against
state-of-the-art strategies. For reproduction purposes, our evaluation datasets
and implementation are publicly available at
https://github.com/stefanosantaris/melanie",arxiv
http://arxiv.org/abs/2011.09902v1,2020-11-17T04:11:31Z,2020-11-17T04:11:31Z,"Low-latency Federated Learning and Blockchain for Edge Association in
  Digital Twin empowered 6G Networks","Emerging technologies such as digital twins and 6th Generation mobile
networks (6G) have accelerated the realization of edge intelligence in
Industrial Internet of Things (IIoT). The integration of digital twin and 6G
bridges the physical system with digital space and enables robust instant
wireless connectivity. With increasing concerns on data privacy, federated
learning has been regarded as a promising solution for deploying distributed
data processing and learning in wireless networks. However, unreliable
communication channels, limited resources, and lack of trust among users,
hinder the effective application of federated learning in IIoT. In this paper,
we introduce the Digital Twin Wireless Networks (DTWN) by incorporating digital
twins into wireless networks, to migrate real-time data processing and
computation to the edge plane. Then, we propose a blockchain empowered
federated learning framework running in the DTWN for collaborative computing,
which improves the reliability and security of the system, and enhances data
privacy. Moreover, to balance the learning accuracy and time cost of the
proposed scheme, we formulate an optimization problem for edge association by
jointly considering digital twin association, training data batch size, and
bandwidth allocation. We exploit multi-agent reinforcement learning to find an
optimal solution to the problem. Numerical results on real-world dataset show
that the proposed scheme yields improved efficiency and reduced cost compared
to benchmark learning method.",arxiv
http://arxiv.org/abs/2104.12359v1,2021-04-26T06:04:27Z,2021-04-26T06:04:27Z,"Complex Neural Spatial Filter: Enhancing Multi-channel Target Speech
  Separation in Complex Domain","To date, mainstream target speech separation (TSS) approaches are formulated
to estimate the complex ratio mask (cRM) of the target speech in time-frequency
domain under supervised deep learning framework. However, the existing deep
models for estimating cRM are designed in the way that the real and imaginary
parts of the cRM are separately modeled using real-valued training data pairs.
The research motivation of this study is to design a deep model that fully
exploits the temporal-spectral-spatial information of multi-channel signals for
estimating cRM directly and efficiently in complex domain. As a result, a novel
TSS network is designed consisting of two modules, a complex neural spatial
filter (cNSF) and an MVDR. Essentially, cNSF is a cRM estimation model and an
MVDR module is cascaded to the cNSF module to reduce the nonlinear speech
distortions introduced by neural network. Specifically, to fit the cRM target,
all input features of cNSF are reformulated into complex-valued representations
following the supervised learning paradigm. Then, to achieve good hierarchical
feature abstraction, a complex deep neural network (cDNN) is delicately
designed with U-Net structure. Experiments conducted on simulated multi-channel
speech data demonstrate the proposed cNSF outperforms the baseline NSF by 12.1%
scale-invariant signal-to-distortion ratio and 33.1% word error rate.",arxiv
http://arxiv.org/abs/1709.04909v1,2017-09-14T17:54:05Z,2017-09-14T17:54:05Z,Shared Learning : Enhancing Reinforcement in $Q$-Ensembles,"Deep Reinforcement Learning has been able to achieve amazing successes in a
variety of domains from video games to continuous control by trying to maximize
the cumulative reward. However, most of these successes rely on algorithms that
require a large amount of data to train in order to obtain results on par with
human-level performance. This is not feasible if we are to deploy these systems
on real world tasks and hence there has been an increased thrust in exploring
data efficient algorithms. To this end, we propose the Shared Learning
framework aimed at making $Q$-ensemble algorithms data-efficient. For achieving
this, we look into some principles of transfer learning which aim to study the
benefits of information exchange across tasks in reinforcement learning and
adapt transfer to learning our value function estimates in a novel manner. In
this paper, we consider the special case of transfer between the value function
estimates in the $Q$-ensemble architecture of BootstrappedDQN. We further
empirically demonstrate how our proposed framework can help in speeding up the
learning process in $Q$-ensembles with minimum computational overhead on a
suite of Atari 2600 Games.",arxiv
http://arxiv.org/abs/2105.03688v1,2021-05-08T12:48:08Z,2021-05-08T12:48:08Z,"HamNet: Conformation-Guided Molecular Representation with Hamiltonian
  Neural Networks","Well-designed molecular representations (fingerprints) are vital to combine
medical chemistry and deep learning. Whereas incorporating 3D geometry of
molecules (i.e. conformations) in their representations seems beneficial,
current 3D algorithms are still in infancy. In this paper, we propose a novel
molecular representation algorithm which preserves 3D conformations of
molecules with a Molecular Hamiltonian Network (HamNet). In HamNet, implicit
positions and momentums of atoms in a molecule interact in the Hamiltonian
Engine following the discretized Hamiltonian equations. These implicit
coordinations are supervised with real conformations with translation- &
rotation-invariant losses, and further used as inputs to the Fingerprint
Generator, a message-passing neural network. Experiments show that the
Hamiltonian Engine can well preserve molecular conformations, and that the
fingerprints generated by HamNet achieve state-of-the-art performances on
MoleculeNet, a standard molecular machine learning benchmark.",arxiv
http://arxiv.org/abs/2107.01001v2,2021-07-08T13:19:43Z,2021-06-03T08:35:10Z,"Feeling of Presence Maximization: mmWave-Enabled Virtual Reality Meets
  Deep Reinforcement Learning","This paper investigates the problem of providing ultra-reliable and
energy-efficient virtual reality (VR) experiences for wireless mobile users. To
ensure reliable ultra-high-definition (UHD) video frame delivery to mobile
users and enhance their immersive visual experiences, a coordinated multipoint
(CoMP) transmission technique and millimeter wave (mmWave) communications are
exploited. Owing to user movement and time-varying wireless channels, the
wireless VR experience enhancement problem is formulated as a
sequence-dependent and mixed-integer problem with a goal of maximizing users'
feeling of presence (FoP) in the virtual world, subject to power consumption
constraints on access points (APs) and users' head-mounted displays (HMDs). The
problem, however, is hard to be directly solved due to the lack of users'
accurate tracking information and the sequence-dependent and mixed-integer
characteristics. To overcome this challenge, we develop a parallel echo state
network (ESN) learning method to predict users' tracking information by
training fresh and historical tracking samples separately collected by APs.
With the learnt results, we propose a deep reinforcement learning (DRL) based
optimization algorithm to solve the formulated problem. In this algorithm, we
implement deep neural networks (DNNs) as a scalable solution to produce integer
decision variables and solving a continuous power control problem to criticize
the integer decision variables. Finally, the performance of the proposed
algorithm is compared with various benchmark algorithms, and the impact of
different design parameters is also discussed. Simulation results demonstrate
that the proposed algorithm is more 4.14% energy-efficient than the benchmark
algorithms.",arxiv
http://arxiv.org/abs/2005.08184v1,2020-05-17T08:01:27Z,2020-05-17T08:01:27Z,Voice Activity Detection Scheme by Combining DNN Model with GMM Model,"Due to the superior modeling ability of deep neural network (DNN), it is
widely used in voice activity detection (VAD). However, the performance may
degrade if no sufficient data especially for practical data could be used for
training, thus, leading to inferior ability of adaption to environment.
Moreover, large model structure could not always be used in practical,
especially for low cost devices where restricted hardware is used. This is on
the contrary for Gaussian mixture model (GMM) where model parameters can be
updated in real-time, but, with low modeling ability. In this paper, deeply
integrated scheme combining these two models are proposed to improve
adaptability and modeling ability. This is done by directly combining the
results of models and feeding it back, together with the result of the DNN
model, to update the GMM model. Besides, a control scheme is elaborately
designed to detect the endpoints of speech. The superior performance by
employing this scheme is validated through experiments in practical, which give
an insight into the advantage of combining supervised learning and unsupervised
learning.",arxiv
http://arxiv.org/abs/2108.05045v2,2021-09-09T02:54:03Z,2021-08-11T06:08:25Z,Semi-Supervised Domain Generalizable Person Re-Identification,"Existing person re-identification (re-id) methods are stuck when deployed to
a new unseen scenario despite the success in cross-camera person matching.
Recent efforts have been substantially devoted to domain adaptive person re-id
where extensive unlabeled data in the new scenario are utilized in a
transductive learning manner. However, for each scenario, it is required to
first collect enough data and then train such a domain adaptive re-id model,
thus restricting their practical application. Instead, we aim to explore
multiple labeled datasets to learn generalized domain-invariant representations
for person re-id, which is expected universally effective for each new-coming
re-id scenario. To pursue practicability in real-world systems, we collect all
the person re-id datasets (20 datasets) in this field and select the three most
frequently used datasets (i.e., Market1501, DukeMTMC, and MSMT17) as unseen
target domains. In addition, we develop DataHunter that collects over 300K+
weak annotated images named YouTube-Human from YouTube street-view videos,
which joins 17 remaining full labeled datasets to form multiple source domains.
On such a large and challenging benchmark called FastHuman (~440K+ labeled
images), we further propose a simple yet effective Semi-Supervised Knowledge
Distillation (SSKD) framework. SSKD effectively exploits the weakly annotated
data by assigning soft pseudo labels to YouTube-Human to improve models'
generalization ability. Experiments on several protocols verify the
effectiveness of the proposed SSKD framework on domain generalizable person
re-id, which is even comparable to supervised learning on the target domains.
Lastly, but most importantly, we hope the proposed benchmark FastHuman could
bring the next development of domain generalizable person re-id algorithms.",arxiv
http://arxiv.org/abs/2110.06428v1,2021-10-13T01:24:32Z,2021-10-13T01:24:32Z,All-neural beamformer for continuous speech separation,"Continuous speech separation (CSS) aims to separate overlapping voices from a
continuous influx of conversational audio containing an unknown number of
utterances spoken by an unknown number of speakers. A common application
scenario is transcribing a meeting conversation recorded by a microphone array.
Prior studies explored various deep learning models for time-frequency mask
estimation, followed by a minimum variance distortionless response (MVDR)
filter to improve the automatic speech recognition (ASR) accuracy. The
performance of these methods is fundamentally upper-bounded by MVDR's spatial
selectivity. Recently, the all deep learning MVDR (ADL-MVDR) model was proposed
for neural beamforming and demonstrated superior performance in a target speech
extraction task using pre-segmented input. In this paper, we further adapt
ADL-MVDR to the CSS task with several enhancements to enable end-to-end neural
beamforming. The proposed system achieves significant word error rate reduction
over a baseline spectral masking system on the LibriCSS dataset. Moreover, the
proposed neural beamformer is shown to be comparable to a state-of-the-art
MVDR-based system in real meeting transcription tasks, including AMI, while
showing potentials to further simplify the runtime implementation and reduce
the system latency with frame-wise processing.",arxiv
http://arxiv.org/abs/1808.04929v1,2018-08-15T00:20:35Z,2018-08-15T00:20:35Z,"Holographic Visualisation of Radiology Data and Automated Machine
  Learning-based Medical Image Segmentation","Within this thesis we propose a platform for combining Augmented Reality (AR)
hardware with machine learning in a user-oriented pipeline, offering to the
medical staff an intuitive 3D visualization of volumetric Computed Tomography
(CT) and Magnetic Resonance Imaging (MRI) medical image segmentations inside
the AR headset, that does not need human intervention for loading, processing
and segmentation of medical images. The AR visualization, based on Microsoft
HoloLens, employs a modular and thus scalable frontend-backend architecture for
real-time visualizations on multiple AR headsets. As Convolutional Neural
Networks (CNNs) have lastly demonstrated superior performance for the machine
learning task of image semantic segmentation, the pipeline also includes a
fully automated CNN algorithm for the segmentation of the liver from CT scans.
The model is based on the Deep Retinal Image Understanding (DRIU) model which
is a Fully Convolutional Network with side outputs from feature maps with
different resolution, extracted at different stages of the network. The
algorithm is 2.5D which means that the input is a set of consecutive scan
slices. The experiments have been performed on the Liver Tumor Segmentation
Challenge (LiTS) dataset for liver segmentation and demonstrated good results
and flexibility. While multiple approaches exist in the domain, only few of
them have focused on overcoming the practical aspects which still largely hold
this technology away from the operating rooms. In line with this, we also are
next planning an evaluation from medical doctors and radiologists in a
real-world environment.",arxiv
http://arxiv.org/abs/1908.09186v1,2019-08-24T18:53:52Z,2019-08-24T18:53:52Z,Efficient Learning on Point Clouds with Basis Point Sets,"With the increased availability of 3D scanning technology, point clouds are
moving into the focus of computer vision as a rich representation of everyday
scenes. However, they are hard to handle for machine learning algorithms due to
their unordered structure. One common approach is to apply occupancy grid
mapping, which dramatically increases the amount of data stored and at the same
time loses details through discretization. Recently, deep learning models were
proposed to handle point clouds directly and achieve input permutation
invariance. However, these architectures often use an increased number of
parameters and are computationally inefficient. In this work, we propose basis
point sets (BPS) as a highly efficient and fully general way to process point
clouds with machine learning algorithms. The basis point set representation is
a residual representation that can be computed efficiently and can be used with
standard neural network architectures and other machine learning algorithms.
Using the proposed representation as the input to a simple fully connected
network allows us to match the performance of PointNet on a shape
classification task while using three orders of magnitude less floating-point
operations. In a second experiment, we show how the proposed representation can
be used for registering high-resolution meshes to noisy 3D scans. Here, we
present the first method for single-pass high-resolution mesh registration,
avoiding time-consuming per-scan optimization and allowing real-time execution.",arxiv
http://arxiv.org/abs/1909.13330v1,2019-09-29T17:51:52Z,2019-09-29T17:51:52Z,Neural Hybrid Recommender: Recommendation needs collaboration,"In recent years, deep learning has gained an indisputable success in computer
vision, speech recognition, and natural language processing. After its rising
success on these challenging areas, it has been studied on recommender systems
as well, but mostly to include content features into traditional methods. In
this paper, we introduce a generalized neural network-based recommender
framework that is easily extendable by additional networks. This framework
named NHR, short for Neural Hybrid Recommender allows us to include more
elaborate information from the same and different data sources. We have worked
on item prediction problems, but the framework can be used for rating
prediction problems as well with a single change on the loss function. To
evaluate the effect of such a framework, we have tested our approach on
benchmark and not yet experimented datasets. The results in these real-world
datasets show the superior performance of our approach in comparison with the
state-of-the-art methods.",arxiv
http://arxiv.org/abs/1608.04983v1,2016-08-17T14:43:17Z,2016-08-17T14:43:17Z,"Ensemble of Jointly Trained Deep Neural Network-Based Acoustic Models
  for Reverberant Speech Recognition","Distant speech recognition is a challenge, particularly due to the corruption
of speech signals by reverberation caused by large distances between the
speaker and microphone. In order to cope with a wide range of reverberations in
real-world situations, we present novel approaches for acoustic modeling
including an ensemble of deep neural networks (DNNs) and an ensemble of jointly
trained DNNs. First, multiple DNNs are established, each of which corresponds
to a different reverberation time 60 (RT60) in a setup step. Also, each model
in the ensemble of DNN acoustic models is further jointly trained, including
both feature mapping and acoustic modeling, where the feature mapping is
designed for the dereverberation as a front-end. In a testing phase, the two
most likely DNNs are chosen from the DNN ensemble using maximum a posteriori
(MAP) probabilities, computed in an online fashion by using maximum likelihood
(ML)-based blind RT60 estimation and then the posterior probability outputs
from two DNNs are combined using the ML-based weights as a simple average.
Extensive experiments demonstrate that the proposed approach leads to
substantial improvements in speech recognition accuracy over the conventional
DNN baseline systems under diverse reverberant conditions.",arxiv
http://arxiv.org/abs/1708.05031v2,2017-08-26T01:37:09Z,2017-08-16T18:30:09Z,Neural Collaborative Filtering,"In recent years, deep neural networks have yielded immense success on speech
recognition, computer vision and natural language processing. However, the
exploration of deep neural networks on recommender systems has received
relatively less scrutiny. In this work, we strive to develop techniques based
on neural networks to tackle the key problem in recommendation -- collaborative
filtering -- on the basis of implicit feedback. Although some recent work has
employed deep learning for recommendation, they primarily used it to model
auxiliary information, such as textual descriptions of items and acoustic
features of musics. When it comes to model the key factor in collaborative
filtering -- the interaction between user and item features, they still
resorted to matrix factorization and applied an inner product on the latent
features of users and items. By replacing the inner product with a neural
architecture that can learn an arbitrary function from data, we present a
general framework named NCF, short for Neural network-based Collaborative
Filtering. NCF is generic and can express and generalize matrix factorization
under its framework. To supercharge NCF modelling with non-linearities, we
propose to leverage a multi-layer perceptron to learn the user-item interaction
function. Extensive experiments on two real-world datasets show significant
improvements of our proposed NCF framework over the state-of-the-art methods.
Empirical evidence shows that using deeper layers of neural networks offers
better recommendation performance.",arxiv
http://arxiv.org/abs/2010.00432v1,2020-10-01T14:27:28Z,2020-10-01T14:27:28Z,"The RFML Ecosystem: A Look at the Unique Challenges of Applying Deep
  Learning to Radio Frequency Applications","While deep machine learning technologies are now pervasive in
state-of-the-art image recognition and natural language processing
applications, only in recent years have these technologies started to
sufficiently mature in applications related to wireless communications. In
particular, recent research has shown deep machine learning to be an enabling
technology for cognitive radio applications as well as a useful tool for
supplementing expertly defined algorithms for spectrum sensing applications
such as signal detection, estimation, and classification (termed here as Radio
Frequency Machine Learning, or RFML). A major driver for the usage of deep
machine learning in the context of wireless communications is that little, to
no, a priori knowledge of the intended spectral environment is required, given
that there is an abundance of representative data to facilitate training and
evaluation. However, in addition to this fundamental need for sufficient data,
there are other key considerations, such as trust, security, and
hardware/software issues, that must be taken into account before deploying deep
machine learning systems in real-world wireless communication applications.
This paper provides an overview and survey of prior work related to these major
research considerations. In particular, we present their unique considerations
in the RFML application space, which are not generally present in the image,
audio, and/or text application spaces.",arxiv
http://arxiv.org/abs/1707.06265v2,2017-09-22T16:31:05Z,2017-07-19T19:10:44Z,"Unsupervised Domain Adaptation for Robust Speech Recognition via
  Variational Autoencoder-Based Data Augmentation","Domain mismatch between training and testing can lead to significant
degradation in performance in many machine learning scenarios. Unfortunately,
this is not a rare situation for automatic speech recognition deployments in
real-world applications. Research on robust speech recognition can be regarded
as trying to overcome this domain mismatch issue. In this paper, we address the
unsupervised domain adaptation problem for robust speech recognition, where
both source and target domain speech are presented, but word transcripts are
only available for the source domain speech. We present novel
augmentation-based methods that transform speech in a way that does not change
the transcripts. Specifically, we first train a variational autoencoder on both
source and target domain data (without supervision) to learn a latent
representation of speech. We then transform nuisance attributes of speech that
are irrelevant to recognition by modifying the latent representations, in order
to augment labeled training data with additional data whose distribution is
more similar to the target domain. The proposed method is evaluated on the
CHiME-4 dataset and reduces the absolute word error rate (WER) by as much as
35% compared to the non-adapted baseline.",arxiv
http://arxiv.org/abs/2007.03578v2,2020-07-08T22:53:16Z,2020-07-07T15:55:50Z,"A Vision-based Social Distancing and Critical Density Detection System
  for COVID-19","Social distancing has been proven as an effective measure against the spread
of the infectious COronaVIrus Disease 2019 (COVID-19). However, individuals are
not used to tracking the required 6-feet (2-meters) distance between themselves
and their surroundings. An active surveillance system capable of detecting
distances between individuals and warning them can slow down the spread of the
deadly disease. Furthermore, measuring social density in a region of interest
(ROI) and modulating inflow can decrease social distancing violation occurrence
chance.
  On the other hand, recording data and labeling individuals who do not follow
the measures will breach individuals' rights in free-societies. Here we propose
an Artificial Intelligence (AI) based real-time social distancing detection and
warning system considering four important ethical factors: (1) the system
should never record/cache data, (2) the warnings should not target the
individuals, (3) no human supervisor should be in the detection/warning loop,
and (4) the code should be open-source and accessible to the public. Against
this backdrop, we propose using a monocular camera and deep learning-based
real-time object detectors to measure social distancing. If a violation is
detected, a non-intrusive audio-visual warning signal is emitted without
targeting the individual who breached the social distancing measure. Also, if
the social density is over a critical value, the system sends a control signal
to modulate inflow into the ROI. We tested the proposed method across
real-world datasets to measure its generality and performance. The proposed
method is ready for deployment, and our code is open-sourced.",arxiv
http://arxiv.org/abs/2011.06144v1,2020-11-12T01:06:17Z,2020-11-12T01:06:17Z,I-POST: Intelligent Point of Sale and Transaction System,"We propose a novel solution for the cashier problem. Current cashier
system/Point of Sale (POS) terminals can be inefficient, cumbersome and
time-consuming for the users. There is a need for a solution dependent on
modern technology and ubiquitous computing resources. We present I-POST
(Intelligent Point of Sale and Transaction) as a software system that uses
smart devices, mobile phone and state of the art machine learning algorithms to
process the user transactions in automated and real time manner. I-POST is an
automated checkout system that allows the user to walk in a store, collect his
items and exit the store. There is no need to stand and wait in a queue. The
system uses object detection and facial recognition algorithm to process the
authentication of the client and the state of the object. At point of exit, the
classifier sends the data to the backend server which execute the payments. The
system uses Convolution Neural Network (CNN) for the image recognition and
processing. CNN is a supervised learning model that has found major application
in pattern recognition problem. The current implementation uses two classifiers
that work intrinsically to authenticate the user and track the items. The model
accuracy for object recognition is 97%, the loss is 9.3%. We expect that such
systems can bring efficiency to the market and has the potential for broad and
diverse applications.",arxiv
http://arxiv.org/abs/2005.07424v1,2020-05-15T09:05:17Z,2020-05-15T09:05:17Z,"Exploring the Capabilities and Limits of 3D Monocular Object Detection
  -- A Study on Simulation and Real World Data","3D object detection based on monocular camera data is a key enabler for
autonomous driving. The task however, is ill-posed due to lack of depth
information in 2D images. Recent deep learning methods show promising results
to recover depth information from single images by learning priors about the
environment. Several competing strategies tackle this problem. In addition to
the network design, the major difference of these competing approaches lies in
using a supervised or self-supervised optimization loss function, which require
different data and ground truth information. In this paper, we evaluate the
performance of a 3D object detection pipeline which is parameterizable with
different depth estimation configurations. We implement a simple distance
calculation approach based on camera intrinsics and 2D bounding box size, a
self-supervised, and a supervised learning approach for depth estimation.
  Ground truth depth information cannot be recorded reliable in real world
scenarios. This shifts our training focus to simulation data. In simulation,
labeling and ground truth generation can be automatized. We evaluate the
detection pipeline on simulator data and a real world sequence from an
autonomous vehicle on a race track. The benefit of simulation training to real
world application is investigated. Advantages and drawbacks of the different
depth estimation strategies are discussed.",arxiv
http://arxiv.org/abs/2106.10305v1,2021-06-18T18:30:22Z,2021-06-18T18:30:22Z,"Multi-Task Learning for User Engagement and Adoption in Live Video
  Streaming Events","Nowadays, live video streaming events have become a mainstay in viewer's
communication in large international enterprises. Provided that viewers are
distributed worldwide, the main challenge resides on how to schedule the
optimal event's time so as to improve both the viewer's engagement and
adoption. In this paper we present a multi-task deep reinforcement learning
model to select the time of a live video streaming event, aiming to optimize
the viewer's engagement and adoption at the same time. We consider the
engagement and adoption of the viewers as independent tasks and formulate a
unified loss function to learn a common policy. In addition, we account for the
fact that each task might have different contribution to the training strategy
of the agent. Therefore, to determine the contribution of each task to the
agent's training, we design a Transformer's architecture for the state-action
transitions of each task. We evaluate our proposed model on four real-world
datasets, generated by the live video streaming events of four large
enterprises spanning from January 2019 until March 2021. Our experiments
demonstrate the effectiveness of the proposed model when compared with several
state-of-the-art strategies. For reproduction purposes, our evaluation datasets
and implementation are publicly available at
https://github.com/stefanosantaris/merlin.",arxiv
http://arxiv.org/abs/2108.10972v1,2021-08-24T22:02:27Z,2021-08-24T22:02:27Z,Domain Adaptation for Real-World Single View 3D Reconstruction,"Deep learning-based object reconstruction algorithms have shown remarkable
improvements over classical methods. However, supervised learning based methods
perform poorly when the training data and the test data have different
distributions. Indeed, most current works perform satisfactorily on the
synthetic ShapeNet dataset, but dramatically fail in when presented with real
world images. To address this issue, unsupervised domain adaptation can be used
transfer knowledge from the labeled synthetic source domain and learn a
classifier for the unlabeled real target domain. To tackle this challenge of
single view 3D reconstruction in the real domain, we experiment with a variety
of domain adaptation techniques inspired by the maximum mean discrepancy (MMD)
loss, Deep CORAL, and the domain adversarial neural network (DANN). From these
findings, we additionally propose a novel architecture which takes advantage of
the fact that in this setting, target domain data is unsupervised with regards
to the 3D model but supervised for class labels. We base our framework off a
recent network called pix2vox. Results are performed with ShapeNet as the
source domain and domains within the Object Dataset Domain Suite (ODDS) dataset
as the target, which is a real world multiview, multidomain image dataset. The
domains in ODDS vary in difficulty, allowing us to assess notions of domain gap
size. Our results are the first in the multiview reconstruction literature
using this dataset.",arxiv
http://arxiv.org/abs/1903.05757v1,2019-03-13T23:31:21Z,2019-03-13T23:31:21Z,"VRKitchen: an Interactive 3D Virtual Environment for Task-oriented
  Learning","One of the main challenges of advancing task-oriented learning such as visual
task planning and reinforcement learning is the lack of realistic and
standardized environments for training and testing AI agents. Previously,
researchers often relied on ad-hoc lab environments. There have been recent
advances in virtual systems built with 3D physics engines and photo-realistic
rendering for indoor and outdoor environments, but the embodied agents in those
systems can only conduct simple interactions with the world (e.g., walking
around, moving objects, etc.). Most of the existing systems also do not allow
human participation in their simulated environments. In this work, we design
and implement a virtual reality (VR) system, VRKitchen, with integrated
functions which i) enable embodied agents powered by modern AI methods (e.g.,
planning, reinforcement learning, etc.) to perform complex tasks involving a
wide range of fine-grained object manipulations in a realistic environment, and
ii) allow human teachers to perform demonstrations to train agents (i.e.,
learning from demonstration). We also provide standardized evaluation
benchmarks and data collection tools to facilitate a broad use in research on
task-oriented learning and beyond.",arxiv
http://arxiv.org/abs/1805.07925v3,2019-04-25T08:02:35Z,2018-05-21T07:30:26Z,"Batch-Instance Normalization for Adaptively Style-Invariant Neural
  Networks","Real-world image recognition is often challenged by the variability of visual
styles including object textures, lighting conditions, filter effects, etc.
Although these variations have been deemed to be implicitly handled by more
training data and deeper networks, recent advances in image style transfer
suggest that it is also possible to explicitly manipulate the style
information. Extending this idea to general visual recognition problems, we
present Batch-Instance Normalization (BIN) to explicitly normalize unnecessary
styles from images. Considering certain style features play an essential role
in discriminative tasks, BIN learns to selectively normalize only disturbing
styles while preserving useful styles. The proposed normalization module is
easily incorporated into existing network architectures such as Residual
Networks, and surprisingly improves the recognition performance in various
scenarios. Furthermore, experiments verify that BIN effectively adapts to
completely different tasks like object classification and style transfer, by
controlling the trade-off between preserving and removing style variations. BIN
can be implemented with only a few lines of code using popular deep learning
frameworks.",arxiv
http://arxiv.org/abs/1604.03393v4,2017-08-07T16:25:04Z,2016-04-12T13:11:12Z,"Robust coherence-based spectral enhancement for speech recognition in
  adverse real-world environments","Speech recognition in adverse real-world environments is highly affected by
reverberation and nonstationary background noise. A well-known strategy to
reduce such undesired signal components in multi-microphone scenarios is
spatial filtering of the microphone signals. In this article, we demonstrate
that an additional coherence-based postfilter, which is applied to the
beamformer output signal to remove diffuse interference components from the
latter, is an effective means to further improve the recognition accuracy of
modern deep learning speech recognition systems. To this end, the recently
updated 3rd CHiME Speech Separation and Recognition Challenge (CHiME-3)
baseline speech recognition system is extended by a coherence-based postfilter
and the postfilter's impact on the word error rates is investigated for the
noisy environments provided by CHiME-3. To determine the time- and
frequency-dependent postfilter gains, we use a Direction-of-Arrival
(DOA)-dependent and a DOA-independent estimator of the coherent-to-diffuse
power ratio as an approximation of the short-time signal-to-noise ratio. Our
experiments show that incorporating coherence-based postfiltering into the
CHiME-3 baseline speech recognition system leads to a significant reduction of
the word error rate scores for the noisy and reverberant environments provided
as part of CHiME-3.",arxiv
http://arxiv.org/abs/2101.00793v2,2021-01-05T07:28:38Z,2021-01-04T06:16:52Z,"A Framework for Fast Scalable BNN Inference using Googlenet and Transfer
  Learning","Efficient and accurate object detection in video and image analysis is one of
the major beneficiaries of the advancement in computer vision systems with the
help of deep learning. With the aid of deep learning, more powerful tools
evolved, which are capable to learn high-level and deeper features and thus can
overcome the existing problems in traditional architectures of object detection
algorithms. The work in this thesis aims to achieve high accuracy in object
detection with good real-time performance.
  In the area of computer vision, a lot of research is going into the area of
detection and processing of visual information, by improving the existing
algorithms. The binarized neural network has shown high performance in various
vision tasks such as image classification, object detection, and semantic
segmentation. The Modified National Institute of Standards and Technology
database (MNIST), Canadian Institute for Advanced Research (CIFAR), and Street
View House Numbers (SVHN) datasets are used which is implemented using a
pre-trained convolutional neural network (CNN) that is 22 layers deep.
Supervised learning is used in the work, which classifies the particular
dataset with the proper structure of the model. In still images, to improve
accuracy, Googlenet is used. The final layer of the Googlenet is replaced with
the transfer learning to improve the accuracy of the Googlenet. At the same
time, the accuracy in moving images can be maintained by transfer learning
techniques. Hardware is the main backbone for any model to obtain faster
results with a large number of datasets. Here, Nvidia Jetson Nano is used which
is a graphics processing unit (GPU), that can handle a large number of
computations in the process of object detection. Results show that the accuracy
of objects detected by the transfer learning method is more when compared to
the existing methods.",arxiv
http://arxiv.org/abs/2109.10460v1,2021-09-21T23:46:37Z,2021-09-21T23:46:37Z,"Graph-based Cluttered Scene Generation and Interactive Exploration using
  Deep Reinforcement Learning","We introduce a novel method to teach a robotic agent to interactively explore
cluttered yet structured scenes, such as kitchen pantries and grocery shelves,
by leveraging the physical plausibility of the scene. We propose a novel
learning framework to train an effective scene exploration policy to discover
hidden objects with minimal interactions. First, we define a novel scene
grammar to represent structured clutter. Then we train a Graph Neural Network
(GNN) based Scene Generation agent using deep reinforcement learning (deep RL),
to manipulate this Scene Grammar to create a diverse set of stable scenes, each
containing multiple hidden objects. Given such cluttered scenes, we then train
a Scene Exploration agent, using deep RL, to uncover hidden objects by
interactively rearranging the scene. We show that our learned agents hide and
discover significantly more objects than the baselines. We present quantitative
results that prove the generalization capabilities of our agents. We also
demonstrate sim-to-real transfer by successfully deploying the learned policy
on a real UR10 robot to explore real-world cluttered scenes. The supplemental
video can be found at https://www.youtube.com/watch?v=T2Jo7wwaXss.",arxiv
http://arxiv.org/abs/2111.09838v1,2021-11-11T22:24:15Z,2021-11-11T22:24:15Z,"On Efficient Uncertainty Estimation for Resource-Constrained Mobile
  Applications","Deep neural networks have shown great success in prediction quality while
reliable and robust uncertainty estimation remains a challenge. Predictive
uncertainty supplements model predictions and enables improved functionality of
downstream tasks including embedded and mobile applications, such as virtual
reality, augmented reality, sensor fusion, and perception. These applications
often require a compromise in complexity to obtain uncertainty estimates due to
very limited memory and compute resources. We tackle this problem by building
upon Monte Carlo Dropout (MCDO) models using the Axolotl framework;
specifically, we diversify sampled subnetworks, leverage dropout patterns, and
use a branching technique to improve predictive performance while maintaining
fast computations. We conduct experiments on (1) a multi-class classification
task using the CIFAR10 dataset, and (2) a more complex human body segmentation
task. Our results show the effectiveness of our approach by reaching close to
Deep Ensemble prediction quality and uncertainty estimation, while still
achieving faster inference on resource-limited mobile platforms.",arxiv
http://arxiv.org/abs/1807.00560v3,2018-07-09T01:35:50Z,2018-07-02T09:34:34Z,Weight-importance sparse training in keyword spotting,"Large size models are implemented in recently ASR system to deal with complex
speech recognition problems. The num- ber of parameters in these models makes
them hard to deploy, especially on some resource-short devices such as car
tablet. Besides this, at most of time, ASR system is used to deal with
real-time problem such as keyword spotting (KWS). It is contradictory to the
fact that large model requires long com- putation time. To deal with this
problem, we apply some sparse algo- rithms to reduces number of parameters in
some widely used models, Deep Neural Network (DNN) KWS, which requires real
short computation time. We can prune more than 90 % even 95% of parameters in
the model with tiny effect decline. And the sparse model performs better than
baseline models which has same order number of parameters. Besides this, sparse
algorithm can lead us to find rational model size au- tomatically for certain
problem without concerning choosing an original model size.",arxiv
http://arxiv.org/abs/1508.01774v2,2016-02-11T12:59:35Z,2015-08-07T18:16:32Z,An End-to-End Neural Network for Polyphonic Piano Music Transcription,"We present a supervised neural network model for polyphonic piano music
transcription. The architecture of the proposed model is analogous to speech
recognition systems and comprises an acoustic model and a music language model.
The acoustic model is a neural network used for estimating the probabilities of
pitches in a frame of audio. The language model is a recurrent neural network
that models the correlations between pitch combinations over time. The proposed
model is general and can be used to transcribe polyphonic music without
imposing any constraints on the polyphony. The acoustic and language model
predictions are combined using a probabilistic graphical model. Inference over
the output variables is performed using the beam search algorithm. We perform
two sets of experiments. We investigate various neural network architectures
for the acoustic models and also investigate the effect of combining acoustic
and music language model predictions using the proposed architecture. We
compare performance of the neural network based acoustic models with two
popular unsupervised acoustic models. Results show that convolutional neural
network acoustic models yields the best performance across all evaluation
metrics. We also observe improved performance with the application of the music
language models. Finally, we present an efficient variant of beam search that
improves performance and reduces run-times by an order of magnitude, making the
model suitable for real-time applications.",arxiv
http://arxiv.org/abs/2002.08333v1,2020-02-11T15:32:28Z,2020-02-11T15:32:28Z,"Towards Intelligent Pick and Place Assembly of Individualized Products
  Using Reinforcement Learning","Individualized manufacturing is becoming an important approach as a means to
fulfill increasingly diverse and specific consumer requirements and
expectations. While there are various solutions to the implementation of the
manufacturing process, such as additive manufacturing, the subsequent automated
assembly remains a challenging task. As an approach to this problem, we aim to
teach a collaborative robot to successfully perform pick and place tasks by
implementing reinforcement learning. For the assembly of an individualized
product in a constantly changing manufacturing environment, the simulated
geometric and dynamic parameters will be varied. Using reinforcement learning
algorithms capable of meta-learning, the tasks will first be trained in
simulation. They will then be performed in a real-world environment where new
factors are introduced that were not simulated in training to confirm the
robustness of the algorithms. The robot will gain its input data from tactile
sensors, area scan cameras, and 3D cameras used to generate heightmaps of the
environment and the objects. The selection of machine learning algorithms and
hardware components as well as further research questions to realize the
outlined production scenario are the results of the presented work.",arxiv
http://arxiv.org/abs/1905.10906v1,2019-05-26T23:55:35Z,2019-05-26T23:55:35Z,Non-Determinism in Neural Networks for Adversarial Robustness,"Recent breakthroughs in the field of deep learning have led to advancements
in a broad spectrum of tasks in computer vision, audio processing, natural
language processing and other areas. In most instances where these tasks are
deployed in real-world scenarios, the models used in them have been shown to be
susceptible to adversarial attacks, making it imperative for us to address the
challenge of their adversarial robustness. Existing techniques for adversarial
robustness fall into three broad categories: defensive distillation techniques,
adversarial training techniques, and randomized or non-deterministic model
based techniques. In this paper, we propose a novel neural network paradigm
that falls under the category of randomized models for adversarial robustness,
but differs from all existing techniques under this category in that it models
each parameter of the network as a statistical distribution with learnable
parameters. We show experimentally that this framework is highly robust to a
variety of white-box and black-box adversarial attacks, while preserving the
task-specific performance of the traditional neural network model.",arxiv
http://arxiv.org/abs/1705.06599v1,2017-05-17T03:04:43Z,2017-05-17T03:04:43Z,Localized LRR on Grassmann Manifolds: An Extrinsic View,"Subspace data representation has recently become a common practice in many
computer vision tasks. It demands generalizing classical machine learning
algorithms for subspace data. Low-Rank Representation (LRR) is one of the most
successful models for clustering vectorial data according to their subspace
structures. This paper explores the possibility of extending LRR for subspace
data on Grassmann manifolds. Rather than directly embedding the Grassmann
manifolds into the symmetric matrix space, an extrinsic view is taken to build
the LRR self-representation in the local area of the tangent space at each
Grassmannian point, resulting in a localized LRR method on Grassmann manifolds.
A novel algorithm for solving the proposed model is investigated and
implemented. The performance of the new clustering algorithm is assessed
through experiments on several real-world datasets including MNIST handwritten
digits, ballet video clips, SKIG action clips, DynTex++ dataset and highway
traffic video clips. The experimental results show the new method outperforms a
number of state-of-the-art clustering methods",arxiv
http://arxiv.org/abs/2108.05962v1,2021-08-12T21:03:44Z,2021-08-12T21:03:44Z,DRQN-based 3D Obstacle Avoidance with a Limited Field of View,"In this paper, we propose a map-based end-to-end DRL approach for
three-dimensional (3D) obstacle avoidance in a partially observed environment,
which is applied to achieve autonomous navigation for an indoor mobile robot
using a depth camera with a narrow field of view. We first train a neural
network with LSTM units in a 3D simulator of mobile robots to approximate the
Q-value function in double DRQN. We also use a curriculum learning strategy to
accelerate and stabilize the training process. Then we deploy the trained model
to a real robot to perform 3D obstacle avoidance in its navigation. We evaluate
the proposed approach both in the simulated environment and on a robot in the
real world. The experimental results show that the approach is efficient and
easy to be deployed, and it performs well for 3D obstacle avoidance with a
narrow observation angle, which outperforms other existing DRL-based models by
15.5% on success rate.",arxiv
http://arxiv.org/abs/2102.12911v1,2021-02-25T15:02:47Z,2021-02-25T15:02:47Z,"Blocks World Revisited: The Effect of Self-Occlusion on Classification
  by Convolutional Neural Networks","Despite the recent successes in computer vision, there remain new avenues to
explore. In this work, we propose a new dataset to investigate the effect of
self-occlusion on deep neural networks. With TEOS (The Effect of
Self-Occlusion), we propose a 3D blocks world dataset that focuses on the
geometric shape of 3D objects and their omnipresent challenge of
self-occlusion. We designed TEOS to investigate the role of self-occlusion in
the context of object classification. Even though remarkable progress has been
seen in object classification, self-occlusion is a challenge. In the
real-world, self-occlusion of 3D objects still presents significant challenges
for deep learning approaches. However, humans deal with this by deploying
complex strategies, for instance, by changing the viewpoint or manipulating the
scene to gather necessary information. With TEOS, we present a dataset of two
difficulty levels (L1 and L2 ), containing 36 and 12 objects, respectively. We
provide 738 uniformly sampled views of each object, their mask, object and
camera position, orientation, amount of self-occlusion, as well as the CAD
model of each object. We present baseline evaluations with five well-known
classification deep neural networks and show that TEOS poses a significant
challenge for all of them. The dataset, as well as the pre-trained models, are
made publicly available for the scientific community under
https://nvision2.data.eecs.yorku.ca/TEOS.",arxiv
http://arxiv.org/abs/1908.07517v1,2019-08-20T03:50:57Z,2019-08-20T03:50:57Z,AI for Earth: Rainforest Conservation by Acoustic Surveillance,"Saving rainforests is a key to halting adverse climate changes. In this
paper, we introduce an innovative solution built on acoustic surveillance and
machine learning technologies to help rainforest conservation. In particular,
We propose new convolutional neural network (CNN) models for environmental
sound classification and achieved promising preliminary results on two
datasets, including a public audio dataset and our real rainforest sound
dataset. The proposed audio classification models can be easily extended in an
automated machine learning paradigm and integrated in cloud-based services for
real world deployment.",arxiv
http://arxiv.org/abs/2102.05334v2,2021-09-02T07:50:28Z,2021-02-10T09:16:09Z,"Enhancing Real-World Adversarial Patches through 3D Modeling of Complex
  Target Scenes","Adversarial examples have proven to be a concerning threat to deep learning
models, particularly in the image domain. However, while many studies have
examined adversarial examples in the real world, most of them relied on 2D
photos of the attack scene. As a result, the attacks proposed may have limited
effectiveness when implemented in realistic environments with 3D objects or
varied conditions. There are few studies on adversarial learning that use 3D
objects, and in many cases, other researchers are unable to replicate the
real-world evaluation process. In this study, we present a framework that uses
3D modeling to craft adversarial patches for an existing real-world scene. Our
approach uses a 3D digital approximation of the scene as a simulation of the
real world. With the ability to add and manipulate any element in the digital
scene, our framework enables the attacker to improve the adversarial patch's
impact in real-world settings. We use the framework to create a patch for an
everyday scene and evaluate its performance using a novel evaluation process
that ensures that our results are reproducible in both the digital space and
the real world. Our evaluation results show that the framework can generate
adversarial patches that are robust to different settings in the real world.",arxiv
http://arxiv.org/abs/2103.03898v2,2021-11-17T10:45:24Z,2021-03-05T19:06:15Z,Reducing cybersickness in 360-degree virtual reality,"Despite the technological advancements in Virtual Reality (VR), users are
constantly combating feelings of nausea and disorientation, the so called
cybersickness. Cybersickness symptoms cause severe discomfort and hinder the
immersive VR experience. Here we investigated cybersickness in 360-degree
head-mounted display VR. In traditional 360-degree VR experiences,
translational movement in the real world is not reflected in the virtual world,
and therefore self-motion information is not corroborated by matching visual
and vestibular cues, which may trigger symptoms of cybersickness. We have
evaluated whether a new Artificial Intelligence (AI) software designed to
supplement the 360-degree VR experience with artificial 6-degrees-of-freedom
motion may reduce cybersickness. Explicit (simulator sickness questionnaire and
fast motion sickness rating) and implicit (heart rate) measurements were used
to evaluate cybersickness symptoms during and after 360-degree VR exposure.
Simulator sickness scores showed a significant reduction in feelings of nausea
during the AI supplemented 6-degrees-of-freedom motion VR compared to
traditional 360-degree VR. However, 6-degrees-of-freedom motion VR did not
reduce oculomotor or disorientation measures of sickness. No changes have been
observed in fast motion sickness and heart rate measures. Improving the
congruency between visual and vestibular cues in 360-degree VR, as provided by
the AI supplemented 6-degrees-of-freedom motion system considered, is essential
to provide a more engaging, immersive and safe VR, which is critical for
educational, cultural and entertainment applications.",arxiv
http://arxiv.org/abs/1808.00286v1,2018-08-01T12:08:02Z,2018-08-01T12:08:02Z,Energy-based Tuning of Convolutional Neural Networks on Multi-GPUs,"Deep Learning (DL) applications are gaining momentum in the realm of
Artificial Intelligence, particularly after GPUs have demonstrated remarkable
skills for accelerating their challenging computational requirements. Within
this context, Convolutional Neural Network (CNN) models constitute a
representative example of success on a wide set of complex applications,
particularly on datasets where the target can be represented through a
hierarchy of local features of increasing semantic complexity. In most of the
real scenarios, the roadmap to improve results relies on CNN settings involving
brute force computation, and researchers have lately proven Nvidia GPUs to be
one of the best hardware counterparts for acceleration. Our work complements
those findings with an energy study on critical parameters for the deployment
of CNNs on flagship image and video applications: object recognition and people
identification by gait, respectively. We evaluate energy consumption on four
different networks based on the two most popular ones (ResNet/AlexNet): ResNet
(167 layers), a 2D CNN (15 layers), a CaffeNet (25 layers) and a ResNetIm (94
layers) using batch sizes of 64, 128 and 256, and then correlate those with
speed-up and accuracy to determine optimal settings. Experimental results on a
multi-GPU server endowed with twin Maxwell and twin Pascal Titan X GPUs
demonstrate that energy correlates with performance and that Pascal may have up
to 40% gains versus Maxwell. Larger batch sizes extend performance gains and
energy savings, but we have to keep an eye on accuracy, which sometimes shows a
preference for small batches. We expect this work to provide a preliminary
guidance for a wide set of CNN and DL applications in modern HPC times, where
the GFLOPS/w ratio constitutes the primary goal.",arxiv
http://arxiv.org/abs/1803.01122v1,2018-03-03T08:17:12Z,2018-03-03T08:17:12Z,"An Ensemble Framework of Voice-Based Emotion Recognition System for
  Films and TV Programs","Employing voice-based emotion recognition function in artificial intelligence
(AI) product will improve the user experience. Most of researches that have
been done only focus on the speech collected under controlled conditions. The
scenarios evaluated in these research were well controlled. The conventional
approach may fail when background noise or nonspeech filler exist. In this
paper, we propose an ensemble framework combining several aspects of features
from audio. The framework incorporates gender and speaker information relying
on multi-task learning. Therefore it is able to dig and capture emotional
information as much as possible. This framework is evaluated on multimodal
emotion challenge (MEC) 2017 corpus which is close to real world. The proposed
framework outperformed the best baseline system by 29.5% (relative
improvement).",arxiv
http://arxiv.org/abs/1911.04469v1,2019-11-09T19:59:17Z,2019-11-09T19:59:17Z,"A Proposed Artificial intelligence Model for Real-Time Human Action
  Localization and Tracking","In recent years, artificial intelligence (AI) based on deep learning (DL) has
sparked tremendous global interest. DL is widely used today and has expanded
into various interesting areas. It is becoming more popular in cross-subject
research, such as studies of smart city systems, which combine computer science
with engineering applications. Human action detection is one of these areas.
Human action detection is an interesting challenge due to its stringent
requirements in terms of computing speed and accuracy. High-accuracy real-time
object tracking is also considered a significant challenge. This paper
integrates the YOLO detection network, which is considered a state-of-the-art
tool for real-time object detection, with motion vectors and the Coyote
Optimization Algorithm (COA) to construct a real-time human action localization
and tracking system. The proposed system starts with the extraction of motion
information from a compressed video stream and the extraction of appearance
information from RGB frames using an object detector. Then, a fusion step
between the two streams is performed, and the results are fed into the proposed
action tracking model. The COA is used in object tracking due to its accuracy
and fast convergence. The basic foundation of the proposed model is the
utilization of motion vectors, which already exist in a compressed video bit
stream and provide sufficient information to improve the localization of the
target action without requiring high consumption of computational resources
compared with other popular methods of extracting motion information, such as
optical flows. This advantage allows the proposed approach to be implemented in
challenging environments where the computational resources are limited, such as
Internet of Things (IoT) systems.",arxiv
http://arxiv.org/abs/2109.02915v1,2021-09-07T08:04:02Z,2021-09-07T08:04:02Z,"Few-shot Learning in Emotion Recognition of Spontaneous Speech Using a
  Siamese Neural Network with Adaptive Sample Pair Formation","Speech-based machine learning (ML) has been heralded as a promising solution
for tracking prosodic and spectrotemporal patterns in real-life that are
indicative of emotional changes, providing a valuable window into one's
cognitive and mental state. Yet, the scarcity of labelled data in ambulatory
studies prevents the reliable training of ML models, which usually rely on
""data-hungry"" distribution-based learning. Leveraging the abundance of labelled
speech data from acted emotions, this paper proposes a few-shot learning
approach for automatically recognizing emotion in spontaneous speech from a
small number of labelled samples. Few-shot learning is implemented via a metric
learning approach through a siamese neural network, which models the relative
distance between samples rather than relying on learning absolute patterns of
the corresponding distributions of each emotion. Results indicate the
feasibility of the proposed metric learning in recognizing emotions from
spontaneous speech in four datasets, even with a small amount of labelled
samples. They further demonstrate superior performance of the proposed metric
learning compared to commonly used adaptation methods, including network
fine-tuning and adversarial learning. Findings from this work provide a
foundation for the ambulatory tracking of human emotion in spontaneous speech
contributing to the real-life assessment of mental health degradation.",arxiv
http://arxiv.org/abs/2010.03625v1,2020-10-07T19:54:01Z,2020-10-07T19:54:01Z,Online Safety Assurance for Deep Reinforcement Learning,"Recently, deep learning has been successfully applied to a variety of
networking problems. A fundamental challenge is that when the operational
environment for a learning-augmented system differs from its training
environment, such systems often make badly informed decisions, leading to bad
performance. We argue that safely deploying learning-driven systems requires
being able to determine, in real time, whether system behavior is coherent, for
the purpose of defaulting to a reasonable heuristic when this is not so. We
term this the online safety assurance problem (OSAP). We present three
approaches to quantifying decision uncertainty that differ in terms of the
signal used to infer uncertainty. We illustrate the usefulness of online safety
assurance in the context of the proposed deep reinforcement learning (RL)
approach to video streaming. While deep RL for video streaming bests other
approaches when the operational and training environments match, it is
dominated by simple heuristics when the two differ. Our preliminary findings
suggest that transitioning to a default policy when decision uncertainty is
detected is key to enjoying the performance benefits afforded by leveraging ML
without compromising on safety.",arxiv
http://arxiv.org/abs/2109.05542v2,2021-10-26T14:36:25Z,2021-09-12T15:51:41Z,"Unsupervised Domain Adaptive Learning via Synthetic Data for Person
  Re-identification","Person re-identification (re-ID) has gained more and more attention due to
its widespread applications in intelligent video surveillance. Unfortunately,
the mainstream deep learning methods still need a large quantity of labeled
data to train models, and annotating data is an expensive work in real-world
scenarios. In addition, due to domain gaps between different datasets, the
performance is dramatically decreased when re-ID models pre-trained on
label-rich datasets (source domain) are directly applied to other unlabeled
datasets (target domain). In this paper, we attempt to remedy these problems
from two aspects, namely data and methodology. Firstly, we develop a data
collector to automatically generate synthetic re-ID samples in a computer game,
and construct a data labeler to simultaneously annotate them, which free humans
from heavy data collections and annotations. Based on them, we build two
synthetic person re-ID datasets with different scales, ""GSPR"" and ""mini-GSPR""
datasets. Secondly, we propose a synthesis-based multi-domain collaborative
refinement (SMCR) network, which contains a synthetic pretraining module and
two collaborative-refinement modules to implement sufficient learning for the
valuable knowledge from multiple domains. Extensive experiments show that our
proposed framework obtains significant performance improvements over the
state-of-the-art methods on multiple unsupervised domain adaptation tasks of
person re-ID.",arxiv
http://arxiv.org/abs/1906.08864v1,2019-06-01T18:49:57Z,2019-06-01T18:49:57Z,"Accurate and Energy-Efficient Classification with Spiking Random Neural
  Network: Corrected and Expanded Version","Artificial Neural Network (ANN) based techniques have dominated
state-of-the-art results in most problems related to computer vision, audio
recognition, and natural language processing in the past few years, resulting
in strong industrial adoption from all leading technology companies worldwide.
One of the major obstacles that have historically delayed large scale adoption
of ANNs is the huge computational and power costs associated with training and
testing (deploying) them. In the mean-time, Neuromorphic Computing platforms
have recently achieved remarkable performance running more bio-realistic
Spiking Neural Networks at high throughput and very low power consumption
making them a natural alternative to ANNs. Here, we propose using the Random
Neural Network (RNN), a spiking neural network with both theoretical and
practical appealing properties, as a general purpose classifier that can match
the classification power of ANNs on a number of tasks while enjoying all the
features of a spiking neural network. This is demonstrated on a number of
real-world classification datasets.",arxiv
http://arxiv.org/abs/1909.02511v2,2019-09-27T21:48:31Z,2019-09-05T16:31:40Z,"CT Data Curation for Liver Patients: Phase Recognition in Dynamic
  Contrast-Enhanced CT","As the demand for more descriptive machine learning models grows within
medical imaging, bottlenecks due to data paucity will exacerbate. Thus,
collecting enough large-scale data will require automated tools to harvest
data/label pairs from messy and real-world datasets, such as hospital PACS.
This is the focus of our work, where we present a principled data curation tool
to extract multi-phase CT liver studies and identify each scan's phase from a
real-world and heterogenous hospital PACS dataset. Emulating a typical
deployment scenario, we first obtain a set of noisy labels from our
institutional partners that are text mined using simple rules from DICOM tags.
We train a deep learning system, using a customized and streamlined 3D SE
architecture, to identify non-contrast, arterial, venous, and delay phase
dynamic CT liver scans, filtering out anything else, including other types of
liver contrast studies. To exploit as much training data as possible, we also
introduce an aggregated cross entropy loss that can learn from scans only
identified as ""contrast"". Extensive experiments on a dataset of 43K scans of
7680 patient imaging studies demonstrate that our 3DSE architecture, armed with
our aggregated loss, can achieve a mean F1 of 0.977 and can correctly harvest
up to 92.7% of studies, which significantly outperforms the text-mined and
standard-loss approach, and also outperforms other, and more complex, model
architectures.",arxiv
http://arxiv.org/abs/2110.01863v1,2021-10-05T07:55:19Z,2021-10-05T07:55:19Z,"DeepEdge: A Deep Reinforcement Learning based Task Orchestrator for Edge
  Computing","The improvements in the edge computing technology pave the road for
diversified applications that demand real-time interaction. However, due to the
mobility of the end-users and the dynamic edge environment, it becomes
challenging to handle the task offloading with high performance. Moreover,
since each application in mobile devices has different characteristics, a task
orchestrator must be adaptive and have the ability to learn the dynamics of the
environment. For this purpose, we develop a deep reinforcement learning based
task orchestrator, DeepEdge, which learns to meet different task requirements
without needing human interaction even under the heavily-loaded stochastic
network conditions in terms of mobile users and applications. Given the dynamic
offloading requests and time-varying communication conditions, we successfully
model the problem as a Markov process and then apply the Double Deep Q-Network
(DDQN) algorithm to implement DeepEdge. To evaluate the robustness of DeepEdge,
we experiment with four different applications including image rendering,
infotainment, pervasive health, and augmented reality in the network under
various loads. Furthermore, we compare the performance of our agent with the
four different task offloading approaches in the literature. Our results show
that DeepEdge outperforms its competitors in terms of the percentage of
satisfactorily completed tasks.",arxiv
http://arxiv.org/abs/2102.03207v3,2021-06-23T03:08:42Z,2021-02-05T14:46:41Z,Real-time Denoising and Dereverberation with Tiny Recurrent U-Net,"Modern deep learning-based models have seen outstanding performance
improvement with speech enhancement tasks. The number of parameters of
state-of-the-art models, however, is often too large to be deployed on devices
for real-world applications. To this end, we propose Tiny Recurrent U-Net
(TRU-Net), a lightweight online inference model that matches the performance of
current state-of-the-art models. The size of the quantized version of TRU-Net
is 362 kilobytes, which is small enough to be deployed on edge devices. In
addition, we combine the small-sized model with a new masking method called
phase-aware $\beta$-sigmoid mask, which enables simultaneous denoising and
dereverberation. Results of both objective and subjective evaluations have
shown that our model can achieve competitive performance with the current
state-of-the-art models on benchmark datasets using fewer parameters by orders
of magnitude.",arxiv
http://arxiv.org/abs/2007.05832v1,2020-07-11T19:02:33Z,2020-07-11T19:02:33Z,Optimizing Prediction Serving on Low-Latency Serverless Dataflow,"Prediction serving systems are designed to provide large volumes of
low-latency inferences machine learning models. These systems mix data
processing and computationally intensive model inference and benefit from
multiple heterogeneous processors and distributed computing resources. In this
paper, we argue that a familiar dataflow API is well-suited to this
latency-sensitive task, and amenable to optimization even with unmodified
black-box ML models. We present the design of Cloudflow, a system that provides
this API and realizes it on an autoscaling serverless backend. Cloudflow
transparently implements performance-critical optimizations including operator
fusion and competitive execution. Our evaluation shows that Cloudflow's
optimizations yield significant performance improvements on synthetic workloads
and that Cloudflow outperforms state-of-the-art prediction serving systems by
as much as 2x on real-world prediction pipelines, meeting latency goals of
demanding applications like real-time video analysis.",arxiv
http://arxiv.org/abs/2107.08325v1,2021-07-18T00:00:48Z,2021-07-18T00:00:48Z,"Vision-Based Autonomous Car Racing Using Deep Imitative Reinforcement
  Learning","Autonomous car racing is a challenging task in the robotic control area.
Traditional modular methods require accurate mapping, localization and
planning, which makes them computationally inefficient and sensitive to
environmental changes. Recently, deep-learning-based end-to-end systems have
shown promising results for autonomous driving/racing. However, they are
commonly implemented by supervised imitation learning (IL), which suffers from
the distribution mismatch problem, or by reinforcement learning (RL), which
requires a huge amount of risky interaction data. In this work, we present a
general deep imitative reinforcement learning approach (DIRL), which
successfully achieves agile autonomous racing using visual inputs. The driving
knowledge is acquired from both IL and model-based RL, where the agent can
learn from human teachers as well as perform self-improvement by safely
interacting with an offline world model. We validate our algorithm both in a
high-fidelity driving simulation and on a real-world 1/20-scale RC-car with
limited onboard computation. The evaluation results demonstrate that our method
outperforms previous IL and RL methods in terms of sample efficiency and task
performance. Demonstration videos are available at
https://caipeide.github.io/autorace-dirl/",arxiv
http://arxiv.org/abs/2102.04932v1,2021-02-09T16:41:31Z,2021-02-09T16:41:31Z,Sparsification via Compressed Sensing for Automatic Speech Recognition,"In order to achieve high accuracy for machine learning (ML) applications, it
is essential to employ models with a large number of parameters. Certain
applications, such as Automatic Speech Recognition (ASR), however, require
real-time interactions with users, hence compelling the model to have as low
latency as possible. Deploying large scale ML applications thus necessitates
model quantization and compression, especially when running ML models on
resource constrained devices. For example, by forcing some of the model weight
values into zero, it is possible to apply zero-weight compression, which
reduces both the model size and model reading time from the memory. In the
literature, such methods are referred to as sparse pruning. The fundamental
questions are when and which weights should be forced to zero, i.e. be pruned.
In this work, we propose a compressed sensing based pruning (CSP) approach to
effectively address those questions. By reformulating sparse pruning as a
sparsity inducing and compression-error reduction dual problem, we introduce
the classic compressed sensing process into the ML model training process.
Using ASR task as an example, we show that CSP consistently outperforms
existing approaches in the literature.",arxiv
http://arxiv.org/abs/2109.14549v1,2021-09-29T16:48:05Z,2021-09-29T16:48:05Z,"Vision-Guided Quadrupedal Locomotion in the Wild with Multi-Modal Delay
  Randomization","Developing robust vision-guided controllers for quadrupedal robots in complex
environments, with various obstacles, dynamical surroundings and uneven
terrains, is very challenging. While Reinforcement Learning (RL) provides a
promising paradigm for agile locomotion skills with vision inputs in
simulation, it is still very challenging to deploy the RL policy in the real
world. Our key insight is that aside from the discrepancy in the domain gap, in
visual appearance between the simulation and the real world, the latency from
the control pipeline is also a major cause of difficulty. In this paper, we
propose Multi-Modal Delay Randomization (MMDR) to address this issue when
training RL agents. Specifically, we simulate the latency of real hardware by
using past observations, sampled with randomized periods, for both
proprioception and vision. We train the RL policy for end-to-end control in a
physical simulator without any predefined controller or reference motion, and
directly deploy it on the real A1 quadruped robot running in the wild. We
evaluate our method in different outdoor environments with complex terrains and
obstacles. We demonstrate the robot can smoothly maneuver at a high speed,
avoid the obstacles, and show significant improvement over the baselines. Our
project page with videos is at https://mehooz.github.io/mmdr-wild/.",arxiv
http://arxiv.org/abs/2105.02613v1,2021-05-06T12:40:28Z,2021-05-06T12:40:28Z,"Challenges and Obstacles Towards Deploying Deep Learning Models on
  Mobile Devices","From computer vision and speech recognition to forecasting trajectories in
autonomous vehicles, deep learning approaches are at the forefront of so many
domains. Deep learning models are developed using plethora of high-level,
generic frameworks and libraries. Running those models on the mobile devices
require hardware-aware optimizations and in most cases converting the models to
other formats or using a third-party framework. In reality, most of the
developed models need to undergo a process of conversion, adaptation, and, in
some cases, full retraining to match the requirements and features of the
framework that is deploying the model on the target platform. Variety of
hardware platforms with heterogeneous computing elements, from wearable devices
to high-performance GPU clusters are used to run deep learning models. In this
paper, we present the existing challenges, obstacles, and practical solutions
towards deploying deep learning models on mobile devices.",arxiv
http://arxiv.org/abs/2109.08710v1,2021-09-17T18:31:31Z,2021-09-17T18:31:31Z,On-device neural speech synthesis,"Recent advances in text-to-speech (TTS) synthesis, such as Tacotron and
WaveRNN, have made it possible to construct a fully neural network based TTS
system, by coupling the two components together. Such a system is conceptually
simple as it only takes grapheme or phoneme input, uses Mel-spectrogram as an
intermediate feature, and directly generates speech samples. The system
achieves quality equal or close to natural speech. However, the high
computational cost of the system and issues with robustness have limited their
usage in real-world speech synthesis applications and products. In this paper,
we present key modeling improvements and optimization strategies that enable
deploying these models, not only on GPU servers, but also on mobile devices.
The proposed system can generate high-quality 24 kHz speech at 5x faster than
real time on server and 3x faster than real time on mobile devices.",arxiv
http://arxiv.org/abs/1609.06612v1,2016-09-21T15:59:59Z,2016-09-21T15:59:59Z,Multimedia Communication Quality Assessment Testbeds,"We make an intensive use of multimedia frameworks in our research on modeling
the perceived quality estimation in streaming services and real-time
communications. In our preliminary work, we have used the VLC VOD software to
generate reference audiovisual files with various degree of coding and network
degradations. We have successfully built machine learning based models on the
subjective quality dataset we have generated using these files. However,
imperfections in the dataset introduced by the multimedia framework we have
used prevented us from achieving the full potential of these models.
  In order to develop better models, we have re-created our end-to-end
multimedia pipeline using the GStreamer framework for audio and video
streaming. A GStreamer based pipeline proved to be significantly more robust to
network degradations than the VLC VOD framework and allowed us to stream a
video flow at a loss rate up to 5\% packet very easily. GStreamer has also
enabled us to collect the relevant RTCP statistics that proved to be more
accurate than network-deduced information. This dataset is free to the public.
The accuracy of the statistics eventually helped us to generate better
performing perceived quality estimation models.
  In this paper, we present the implementation of these VLC and GStreamer-based
multimedia communication quality assessment testbeds with the references to
their publicly available code bases.",arxiv
http://arxiv.org/abs/2107.13782v1,2021-07-29T07:25:21Z,2021-07-29T07:25:21Z,"Multimodal Co-learning: Challenges, Applications with Datasets, Recent
  Advances and Future Directions","Multimodal deep learning systems which employ multiple modalities like text,
image, audio, video, etc., are showing better performance in comparison with
individual modalities (i.e., unimodal) systems. Multimodal machine learning
involves multiple aspects: representation, translation, alignment, fusion, and
co-learning. In the current state of multimodal machine learning, the
assumptions are that all modalities are present, aligned, and noiseless during
training and testing time. However, in real-world tasks, typically, it is
observed that one or more modalities are missing, noisy, lacking annotated
data, have unreliable labels, and are scarce in training or testing and or
both. This challenge is addressed by a learning paradigm called multimodal
co-learning. The modeling of a (resource-poor) modality is aided by exploiting
knowledge from another (resource-rich) modality using transfer of knowledge
between modalities, including their representations and predictive models.
Co-learning being an emerging area, there are no dedicated reviews explicitly
focusing on all challenges addressed by co-learning. To that end, in this work,
we provide a comprehensive survey on the emerging area of multimodal
co-learning that has not been explored in its entirety yet. We review
implementations that overcome one or more co-learning challenges without
explicitly considering them as co-learning challenges. We present the
comprehensive taxonomy of multimodal co-learning based on the challenges
addressed by co-learning and associated implementations. The various techniques
employed to include the latest ones are reviewed along with some of the
applications and datasets. Our final goal is to discuss challenges and
perspectives along with the important ideas and directions for future work that
we hope to be beneficial for the entire research community focusing on this
exciting domain.",arxiv
http://arxiv.org/abs/2110.02405v1,2021-10-05T23:23:51Z,2021-10-05T23:23:51Z,Echo-Reconstruction: Audio-Augmented 3D Scene Reconstruction,"Reflective and textureless surfaces such as windows, mirrors, and walls can
be a challenge for object and scene reconstruction. These surfaces are often
poorly reconstructed and filled with depth discontinuities and holes, making it
difficult to cohesively reconstruct scenes that contain these planar
discontinuities. We propose Echoreconstruction, an audio-visual method that
uses the reflections of sound to aid in geometry and audio reconstruction for
virtual conferencing, teleimmersion, and other AR/VR experience. The mobile
phone prototype emits pulsed audio, while recording video for RGB-based 3D
reconstruction and audio-visual classification. Reflected sound and images from
the video are input into our audio (EchoCNN-A) and audio-visual (EchoCNN-AV)
convolutional neural networks for surface and sound source detection, depth
estimation, and material classification. The inferences from these
classifications enhance scene 3D reconstructions containing open spaces and
reflective surfaces by depth filtering, inpainting, and placement of unmixed
sound sources in the scene. Our prototype, VR demo, and experimental results
from real-world and virtual scenes with challenging surfaces and sound indicate
high success rates on classification of material, depth estimation, and
closed/open surfaces, leading to considerable visual and audio improvement in
3D scenes (see Figure 1).",arxiv
http://arxiv.org/abs/2012.05214v2,2020-12-10T12:26:59Z,2020-12-09T18:23:21Z,E3D: Event-Based 3D Shape Reconstruction,"3D shape reconstruction is a primary component of augmented/virtual reality.
Despite being highly advanced, existing solutions based on RGB, RGB-D and Lidar
sensors are power and data intensive, which introduces challenges for
deployment in edge devices. We approach 3D reconstruction with an event camera,
a sensor with significantly lower power, latency and data expense while
enabling high dynamic range. While previous event-based 3D reconstruction
methods are primarily based on stereo vision, we cast the problem as multi-view
shape from silhouette using a monocular event camera. The output from a moving
event camera is a sparse point set of space-time gradients, largely sketching
scene/object edges and contours. We first introduce an event-to-silhouette
(E2S) neural network module to transform a stack of event frames to the
corresponding silhouettes, with additional neural branches for camera pose
regression. Second, we introduce E3D, which employs a 3D differentiable
renderer (PyTorch3D) to enforce cross-view 3D mesh consistency and fine-tune
the E2S and pose network. Lastly, we introduce a 3D-to-events simulation
pipeline and apply it to publicly available object datasets and generate
synthetic event/silhouette training pairs for supervised learning.",arxiv
http://arxiv.org/abs/1904.01698v1,2019-04-02T22:55:43Z,2019-04-02T22:55:43Z,VRGym: A Virtual Testbed for Physical and Interactive AI,"We propose VRGym, a virtual reality testbed for realistic human-robot
interaction. Different from existing toolkits and virtual reality environments,
the VRGym emphasizes on building and training both physical and interactive
agents for robotics, machine learning, and cognitive science. VRGym leverages
mechanisms that can generate diverse 3D scenes with high realism through
physics-based simulation. We demonstrate that VRGym is able to (i) collect
human interactions and fine manipulations, (ii) accommodate various robots with
a ROS bridge, (iii) support experiments for human-robot interaction, and (iv)
provide toolkits for training the state-of-the-art machine learning algorithms.
We hope VRGym can help to advance general-purpose robotics and machine learning
agents, as well as assisting human studies in the field of cognitive science.",arxiv
http://arxiv.org/abs/2005.08332v1,2020-05-17T18:17:46Z,2020-05-17T18:17:46Z,"Learning-based Prediction, Rendering and Association Optimization for
  MEC-enabled Wireless Virtual Reality (VR) Network","Wireless-connected Virtual Reality (VR) provides immersive experience for VR
users from any-where at anytime. However, providing wireless VR users with
seamless connectivity and real-time VR video with high quality is challenging
due to its requirements in high Quality of Experience (QoE) and low VR
interaction latency under limited computation capability of VR device. To
address these issues,we propose a MEC-enabled wireless VR network, where the
field of view (FoV) of each VR user can be real-time predicted using Recurrent
Neural Network (RNN), and the rendering of VR content is moved from VR device
to MEC server with rendering model migration capability. Taking into account
the geographical and FoV request correlation, we propose centralized and
distributed decoupled Deep Reinforcement Learning (DRL) strategies to maximize
the long-term QoE of VR users under the VR interaction latency constraint.
Simulation results show that our proposed MEC rendering schemes and DRL
algorithms substantially improve the long-term QoE of VR users and reduce the
VR interaction latency compared to rendering at VR devices",arxiv
http://arxiv.org/abs/2103.16511v1,2021-03-30T17:13:29Z,2021-03-30T17:13:29Z,"Flatland Competition 2020: MAPF and MARL for Efficient Train
  Coordination on a Grid World","The Flatland competition aimed at finding novel approaches to solve the
vehicle re-scheduling problem (VRSP). The VRSP is concerned with scheduling
trips in traffic networks and the re-scheduling of vehicles when disruptions
occur, for example the breakdown of a vehicle. While solving the VRSP in
various settings has been an active area in operations research (OR) for
decades, the ever-growing complexity of modern railway networks makes dynamic
real-time scheduling of traffic virtually impossible. Recently, multi-agent
reinforcement learning (MARL) has successfully tackled challenging tasks where
many agents need to be coordinated, such as multiplayer video games. However,
the coordination of hundreds of agents in a real-life setting like a railway
network remains challenging and the Flatland environment used for the
competition models these real-world properties in a simplified manner.
Submissions had to bring as many trains (agents) to their target stations in as
little time as possible. While the best submissions were in the OR category,
participants found many promising MARL approaches. Using both centralized and
decentralized learning based approaches, top submissions used graph
representations of the environment to construct tree-based observations.
Further, different coordination mechanisms were implemented, such as
communication and prioritization between agents. This paper presents the
competition setup, four outstanding solutions to the competition, and a
cross-comparison between them.",arxiv
http://arxiv.org/abs/1710.00920v2,2017-12-07T21:13:47Z,2017-10-02T21:44:32Z,End-to-end Learning for 3D Facial Animation from Raw Waveforms of Speech,"We present a deep learning framework for real-time speech-driven 3D facial
animation from just raw waveforms. Our deep neural network directly maps an
input sequence of speech audio to a series of micro facial action unit
activations and head rotations to drive a 3D blendshape face model. In
particular, our deep model is able to learn the latent representations of
time-varying contextual information and affective states within the speech.
Hence, our model not only activates appropriate facial action units at
inference to depict different utterance generating actions, in the form of lip
movements, but also, without any assumption, automatically estimates emotional
intensity of the speaker and reproduces her ever-changing affective states by
adjusting strength of facial unit activations. For example, in a happy speech,
the mouth opens wider than normal, while other facial units are relaxed; or in
a surprised state, both eyebrows raise higher. Experiments on a diverse
audiovisual corpus of different actors across a wide range of emotional states
show interesting and promising results of our approach. Being
speaker-independent, our generalized model is readily applicable to various
tasks in human-machine interaction and animation.",arxiv
http://arxiv.org/abs/1910.07083v1,2019-09-29T22:04:19Z,2019-09-29T22:04:19Z,"Occurence of A Cyber Security Eco-System: A Nature Oriented Project and
  Evaluation of An Indirect Social Experiment","Because of todays technological developments and the influence of digital
systems into every aspect of our lives, importance of cyber security improves
more and more day-by-day. Projects, educational processes and seminars realized
for this aim create and improve awareness among individuals and provide useful
tools for growing equipped generations. The aim of this study is to focus on a
cyber security eco-system, which was self-occurred within the interactive
educational environment designed under the scope of TUBITAK 4004 Nature
Education and Science Schools Projects (with the name of A Cyber Security
Adventure) with the use of important technologies such as virtual reality,
augmented reality, and artificial intelligence. The eco-system occurred within
the interactive educational process where high school students took place
caused both students and the project team to experience an indirect social
experiment environment. In this sense, it is thought that the findings and
comments presented in the study will give important ideas to everyone involved
in cyber security education, life-long learning processes, and the technology
use in software oriented educational tools.",arxiv
http://arxiv.org/abs/1608.01745v2,2016-08-15T19:41:47Z,2016-08-05T03:16:07Z,Play and Learn: Using Video Games to Train Computer Vision Models,"Video games are a compelling source of annotated data as they can readily
provide fine-grained groundtruth for diverse tasks. However, it is not clear
whether the synthetically generated data has enough resemblance to the
real-world images to improve the performance of computer vision models in
practice. We present experiments assessing the effectiveness on real-world data
of systems trained on synthetic RGB images that are extracted from a video
game. We collected over 60000 synthetic samples from a modern video game with
similar conditions to the real-world CamVid and Cityscapes datasets. We provide
several experiments to demonstrate that the synthetically generated RGB images
can be used to improve the performance of deep neural networks on both image
segmentation and depth estimation. These results show that a convolutional
network trained on synthetic data achieves a similar test error to a network
that is trained on real-world data for dense image classification. Furthermore,
the synthetically generated RGB images can provide similar or better results
compared to the real-world datasets if a simple domain adaptation technique is
applied. Our results suggest that collaboration with game developers for an
accessible interface to gather data is potentially a fruitful direction for
future work in computer vision.",arxiv
http://arxiv.org/abs/2105.05092v1,2021-05-11T14:44:12Z,2021-05-11T14:44:12Z,"DeepLight: Robust & Unobtrusive Real-time Screen-Camera Communication
  for Real-World Displays","The paper introduces a novel, holistic approach for robust Screen-Camera
Communication (SCC), where video content on a screen is visually encoded in a
human-imperceptible fashion and decoded by a camera capturing images of such
screen content. We first show that state-of-the-art SCC techniques have two key
limitations for in-the-wild deployment: (a) the decoding accuracy drops rapidly
under even modest screen extraction errors from the captured images, and (b)
they generate perceptible flickers on common refresh rate screens even with
minimal modulation of pixel intensity. To overcome these challenges, we
introduce DeepLight, a system that incorporates machine learning (ML) models in
the decoding pipeline to achieve humanly-imperceptible, moderately high SCC
rates under diverse real-world conditions. Deep-Light's key innovation is the
design of a Deep Neural Network (DNN) based decoder that collectively decodes
all the bits spatially encoded in a display frame, without attempting to
precisely isolate the pixels associated with each encoded bit. In addition,
DeepLight supports imperceptible encoding by selectively modulating the
intensity of only the Blue channel, and provides reasonably accurate screen
extraction (IoU values >= 83%) by using state-of-the-art object detection DNN
pipelines. We show that a fully functional DeepLight system is able to robustly
achieve high decoding accuracy (frame error rate < 0.2) and moderately-high
data goodput (>=0.95Kbps) using a human-held smartphone camera, even over
larger screen-camera distances (approx =2m).",arxiv
http://arxiv.org/abs/2108.05080v3,2021-09-06T04:15:53Z,2021-08-11T07:49:36Z,FakeAVCeleb: A Novel Audio-Video Multimodal Deepfake Dataset,"While significant advancements have been made in the generation of deepfakes
using deep learning technologies, its misuse is a well-known issue now.
Deepfakes can cause severe security and privacy issues as they can be used to
impersonate a person's identity in a video by replacing his/her face with
another person's face. Recently, a new problem of generating synthesized human
voice of a person is emerging, where AI-based deep learning models can
synthesize any person's voice requiring just a few seconds of audio. With the
emerging threat of impersonation attacks using deepfake audios and videos, a
new generation of deepfake detectors is needed to focus on both video and audio
collectively. A large amount of good quality datasets is typically required to
capture the real-world scenarios to develop a competent deepfake detector.
Existing deepfake datasets either contain deepfake videos or audios, which are
racially biased as well. Hence, there is a crucial need for creating a good
video as well as an audio deepfake dataset, which can be used to detect audio
and video deepfake simultaneously. To fill this gap, we propose a novel
Audio-Video Deepfake dataset (FakeAVCeleb) that contains not only deepfake
videos but also respective synthesized lip-synced fake audios. We generate this
dataset using the current most popular deepfake generation methods. We selected
real YouTube videos of celebrities with four racial backgrounds (Caucasian,
Black, East Asian, and South Asian) to develop a more realistic multimodal
dataset that addresses racial bias and further help develop multimodal deepfake
detectors. We performed several experiments using state-of-the-art detection
methods to evaluate our deepfake dataset and demonstrate the challenges and
usefulness of our multimodal Audio-Video deepfake dataset.",arxiv
http://arxiv.org/abs/1810.00162v2,2018-10-02T20:07:32Z,2018-09-29T06:56:33Z,"NICE: Noise Injection and Clamping Estimation for Neural Network
  Quantization","Convolutional Neural Networks (CNN) are very popular in many fields including
computer vision, speech recognition, natural language processing, to name a
few. Though deep learning leads to groundbreaking performance in these domains,
the networks used are very demanding computationally and are far from real-time
even on a GPU, which is not power efficient and therefore does not suit low
power systems such as mobile devices. To overcome this challenge, some
solutions have been proposed for quantizing the weights and activations of
these networks, which accelerate the runtime significantly. Yet, this
acceleration comes at the cost of a larger error. The \uniqname method proposed
in this work trains quantized neural networks by noise injection and a learned
clamping, which improve the accuracy. This leads to state-of-the-art results on
various regression and classification tasks, e.g., ImageNet classification with
architectures such as ResNet-18/34/50 with low as 3-bit weights and
activations. We implement the proposed solution on an FPGA to demonstrate its
applicability for low power real-time applications. The implementation of the
paper is available at https://github.com/Lancer555/NICE",arxiv
http://arxiv.org/abs/2104.03838v2,2021-09-19T08:11:33Z,2021-04-08T15:27:49Z,Speech Denoising Without Clean Training Data: A Noise2Noise Approach,"This paper tackles the problem of the heavy dependence of clean speech data
required by deep learning based audio-denoising methods by showing that it is
possible to train deep speech denoising networks using only noisy speech
samples. Conventional wisdom dictates that in order to achieve good speech
denoising performance, there is a requirement for a large quantity of both
noisy speech samples and perfectly clean speech samples, resulting in a need
for expensive audio recording equipment and extremely controlled soundproof
recording studios. These requirements pose significant challenges in data
collection, especially in economically disadvantaged regions and for low
resource languages. This work shows that speech denoising deep neural networks
can be successfully trained utilizing only noisy training audio. Furthermore it
is revealed that such training regimes achieve superior denoising performance
over conventional training regimes utilizing clean training audio targets, in
cases involving complex noise distributions and low Signal-to-Noise ratios
(high noise environments). This is demonstrated through experiments studying
the efficacy of our proposed approach over both real-world noises and synthetic
noises using the 20 layered Deep Complex U-Net architecture.",arxiv
http://arxiv.org/abs/2110.14139v1,2021-10-27T03:01:44Z,2021-10-27T03:01:44Z,"Closing the Gap Between Time-Domain Multi-Channel Speech Enhancement on
  Real and Simulation Conditions","The deep learning based time-domain models, e.g. Conv-TasNet, have shown
great potential in both single-channel and multi-channel speech enhancement.
However, many experiments on the time-domain speech enhancement model are done
in simulated conditions, and it is not well studied whether the good
performance can generalize to real-world scenarios. In this paper, we aim to
provide an insightful investigation of applying multi-channel Conv-TasNet based
speech enhancement to both simulation and real data. Our preliminary
experiments show a large performance gap between the two conditions in terms of
the ASR performance. Several approaches are applied to close this gap,
including the integration of multi-channel Conv-TasNet into the beamforming
model with various strategies, and the joint training of speech enhancement and
speech recognition models. Our experiments on the CHiME-4 corpus show that our
proposed approaches can greatly reduce the speech recognition performance
discrepancy between simulation and real data, while preserving the strong
speech enhancement capability in the frontend.",arxiv
http://arxiv.org/abs/2002.06016v2,2020-03-16T15:58:55Z,2020-02-13T11:08:00Z,"DNN-Based Distributed Multichannel Mask Estimation for Speech
  Enhancement in Microphone Arrays","Multichannel processing is widely used for speech enhancement but several
limitations appear when trying to deploy these solutions to the real-world.
Distributed sensor arrays that consider several devices with a few microphones
is a viable alternative that allows for exploiting the multiple devices
equipped with microphones that we are using in our everyday life. In this
context, we propose to extend the distributed adaptive node-specific signal
estimation approach to a neural networks framework. At each node, a local
filtering is performed to send one signal to the other nodes where a mask is
estimated by a neural network in order to compute a global multi-channel Wiener
filter. In an array of two nodes, we show that this additional signal can be
efficiently taken into account to predict the masks and leads to better speech
enhancement performances than when the mask estimation relies only on the local
signals.",arxiv
http://arxiv.org/abs/2011.05705v1,2020-11-11T11:16:52Z,2020-11-11T11:16:52Z,"EGAD: Evolving Graph Representation Learning with Self-Attention and
  Knowledge Distillation for Live Video Streaming Events","In this study, we present a dynamic graph representation learning model on
weighted graphs to accurately predict the network capacity of connections
between viewers in a live video streaming event. We propose EGAD, a neural
network architecture to capture the graph evolution by introducing a
self-attention mechanism on the weights between consecutive graph convolutional
networks. In addition, we account for the fact that neural architectures
require a huge amount of parameters to train, thus increasing the online
inference latency and negatively influencing the user experience in a live
video streaming event. To address the problem of the high online inference of a
vast number of parameters, we propose a knowledge distillation strategy. In
particular, we design a distillation loss function, aiming to first pretrain a
teacher model on offline data, and then transfer the knowledge from the teacher
to a smaller student model with less parameters. We evaluate our proposed model
on the link prediction task on three real-world datasets, generated by live
video streaming events. The events lasted 80 minutes and each viewer exploited
the distribution solution provided by the company Hive Streaming AB. The
experiments demonstrate the effectiveness of the proposed model in terms of
link prediction accuracy and number of required parameters, when evaluated
against state-of-the-art approaches. In addition, we study the distillation
performance of the proposed model in terms of compression ratio for different
distillation strategies, where we show that the proposed model can achieve a
compression ratio up to 15:100, preserving high link prediction accuracy. For
reproduction purposes, our evaluation datasets and implementation are publicly
available at https://stefanosantaris.github.io/EGAD.",arxiv
http://arxiv.org/abs/1811.07388v2,2020-01-10T17:27:06Z,2018-11-18T19:43:19Z,"Taming the latency in multi-user VR 360$^\circ$: A QoE-aware deep
  learning-aided multicast framework","Immersive virtual reality (VR) applications require ultra-high data rate and
low-latency for smooth operation. Hence in this paper, aiming to improve VR
experience in multi-user VR wireless video streaming, a deep-learning aided
scheme for maximizing the quality of the delivered video chunks with
low-latency is proposed. Therein the correlations in the predicted field of
view (FoV) and locations of viewers watching 360$^\circ$ HD VR videos are
capitalized on to realize a proactive FoV-centric millimeter wave (mmWave)
physical-layer multicast transmission. The problem is cast as a frame quality
maximization problem subject to tight latency constraints and network
stability. The problem is then decoupled into an HD frame request admission and
scheduling subproblems and a matching theory game is formulated to solve the
scheduling subproblem by associating requests from clusters of users to mmWave
small cell base stations (SBSs) for their unicast/multicast transmission.
Furthermore, for realistic modeling and simulation purposes, a real VR
head-tracking dataset and a deep recurrent neural network (DRNN) based on gated
recurrent units (GRUs) are leveraged. Extensive simulation results show how the
content-reuse for clusters of users with highly overlapping FoVs brought in by
multicasting reduces the VR frame delay in 12\%. This reduction is further
boosted by proactiveness that cuts by half the average delays of both reactive
unicast and multicast baselines while preserving HD delivery rates above 98\%.
Finally, enforcing tight latency bounds shortens the delay-tail as evinced by
13\% lower delays in the 99th percentile.",arxiv
http://arxiv.org/abs/2012.07657v3,2021-08-15T18:33:04Z,2020-12-14T15:53:56Z,"Lips Don't Lie: A Generalisable and Robust Approach to Face Forgery
  Detection","Although current deep learning-based face forgery detectors achieve
impressive performance in constrained scenarios, they are vulnerable to samples
created by unseen manipulation methods. Some recent works show improvements in
generalisation but rely on cues that are easily corrupted by common
post-processing operations such as compression. In this paper, we propose
LipForensics, a detection approach capable of both generalising to novel
manipulations and withstanding various distortions. LipForensics targets
high-level semantic irregularities in mouth movements, which are common in many
generated videos. It consists in first pretraining a spatio-temporal network to
perform visual speech recognition (lipreading), thus learning rich internal
representations related to natural mouth motion. A temporal network is
subsequently finetuned on fixed mouth embeddings of real and forged data in
order to detect fake videos based on mouth movements without overfitting to
low-level, manipulation-specific artefacts. Extensive experiments show that
this simple approach significantly surpasses the state-of-the-art in terms of
generalisation to unseen manipulations and robustness to perturbations, as well
as shed light on the factors responsible for its performance. Code is available
on GitHub.",arxiv
http://arxiv.org/abs/2108.04465v1,2021-08-10T06:20:18Z,2021-08-10T06:20:18Z,"Industrial Digital Twins at the Nexus of NextG Wireless Networks and
  Computational Intelligence: A Survey","By amalgamating recent communication and control technologies, computing and
data analytics techniques, and modular manufacturing, Industry~4.0 promotes
integrating cyber-physical worlds through cyber-physical systems (CPS) and
digital twin (DT) for monitoring, optimization, and prognostics of industrial
processes. A DT is an emerging but conceptually different construct than CPS.
Like CPS, DT relies on communication to create a highly-consistent,
synchronized digital mirror image of the objects or physical processes. DT, in
addition, uses built-in models on this precise image to simulate, analyze,
predict, and optimize their real-time operation using feedback. DT is rapidly
diffusing in the industries with recent advances in the industrial Internet of
things (IIoT), edge and cloud computing, machine learning, artificial
intelligence, and advanced data analytics. However, the existing literature
lacks in identifying and discussing the role and requirements of these
technologies in DT-enabled industries from the communication and computing
perspective. In this article, we first present the functional aspects, appeal,
and innovative use of DT in smart industries. Then, we elaborate on this
perspective by systematically reviewing and reflecting on recent research in
next-generation (NextG) wireless technologies (e.g., 5G and beyond networks),
various tools (e.g., age of information, federated learning, data analytics),
and other promising trends in networked computing (e.g., edge and cloud
computing). Moreover, we discuss the DT deployment strategies at different
industrial communication layers to meet the monitoring and control requirements
of industrial applications. We also outline several key reflections and future
research challenges and directions to facilitate industrial DT's adoption.",arxiv
http://arxiv.org/abs/2105.08205v1,2021-05-18T00:01:27Z,2021-05-18T00:01:27Z,Reinforcement Learning for Adaptive Video Compressive Sensing,"We apply reinforcement learning to video compressive sensing to adapt the
compression ratio. Specifically, video snapshot compressive imaging (SCI),
which captures high-speed video using a low-speed camera is considered in this
work, in which multiple (B) video frames can be reconstructed from a snapshot
measurement. One research gap in previous studies is how to adapt B in the
video SCI system for different scenes. In this paper, we fill this gap
utilizing reinforcement learning (RL). An RL model, as well as various
convolutional neural networks for reconstruction, are learned to achieve
adaptive sensing of video SCI systems. Furthermore, the performance of an
object detection network using directly the video SCI measurements without
reconstruction is also used to perform RL-based adaptive video compressive
sensing. Our proposed adaptive SCI method can thus be implemented in low cost
and real time. Our work takes the technology one step further towards real
applications of video SCI.",arxiv
http://arxiv.org/abs/2006.01250v6,2021-06-21T18:21:58Z,2020-05-09T09:41:46Z,RUHSNet: 3D Object Detection Using Lidar Data in Real Time,"In this work, we address the problem of 3D object detection from point cloud
data in real time. For autonomous vehicles to work, it is very important for
the perception component to detect the real world objects with both high
accuracy and fast inference. We propose a novel neural network architecture
along with the training and optimization details for detecting 3D objects in
point cloud data. We compare the results with different backbone architectures
including the standard ones like VGG, ResNet, Inception with our backbone. Also
we present the optimization and ablation studies including designing an
efficient anchor. We use the Kitti 3D Birds Eye View dataset for benchmarking
and validating our results. Our work surpasses the state of the art in this
domain both in terms of average precision and speed running at > 30 FPS. This
makes it a feasible option to be deployed in real time applications including
self driving cars.",arxiv
http://arxiv.org/abs/2107.12943v1,2021-07-27T16:59:00Z,2021-07-27T16:59:00Z,"Learning-based Prediction, Rendering and Transmission for Interactive
  Virtual Reality in RIS-Assisted Terahertz Networks","The quality of experience (QoE) requirements of wireless Virtual Reality (VR)
can only be satisfied with high data rate, high reliability, and low VR
interaction latency. This high data rate over short transmission distances may
be achieved via abundant bandwidth in the terahertz (THz) band. However, THz
waves suffer from severe signal attenuation, which may be compensated by the
reconfigurable intelligent surface (RIS) technology with programmable
reflecting elements. Meanwhile, the low VR interaction latency may be achieved
with the mobile edge computing (MEC) network architecture due to its high
computation capability. Motivated by these considerations, in this paper, we
propose a MEC-enabled and RIS-assisted THz VR network in an indoor scenario, by
taking into account the uplink viewpoint prediction and position transmission,
MEC rendering, and downlink transmission. We propose two methods, which are
referred to as centralized online Gated Recurrent Unit (GRU) and distributed
Federated Averaging (FedAvg), to predict the viewpoints of VR users. In the
uplink, an algorithm that integrates online Long-short Term Memory (LSTM) and
Convolutional Neural Networks (CNN) is deployed to predict the locations and
the line-of-sight and non-line-of-sight statuses of the VR users over time. In
the downlink, we further develop a constrained deep reinforcement learning
algorithm to select the optimal phase shifts of the RIS under latency
constraints. Simulation results show that our proposed learning architecture
achieves near-optimal QoE as that of the genie-aided benchmark algorithm, and
about two times improvement in QoE compared to the random phase shift selection
scheme.",arxiv
http://arxiv.org/abs/2011.01143v2,2021-05-30T03:47:08Z,2020-11-02T17:36:13Z,"Into the Wild with AudioScope: Unsupervised Audio-Visual Separation of
  On-Screen Sounds","Recent progress in deep learning has enabled many advances in sound
separation and visual scene understanding. However, extracting sound sources
which are apparent in natural videos remains an open problem. In this work, we
present AudioScope, a novel audio-visual sound separation framework that can be
trained without supervision to isolate on-screen sound sources from real
in-the-wild videos. Prior audio-visual separation work assumed artificial
limitations on the domain of sound classes (e.g., to speech or music),
constrained the number of sources, and required strong sound separation or
visual segmentation labels. AudioScope overcomes these limitations, operating
on an open domain of sounds, with variable numbers of sources, and without
labels or prior visual segmentation. The training procedure for AudioScope uses
mixture invariant training (MixIT) to separate synthetic mixtures of mixtures
(MoMs) into individual sources, where noisy labels for mixtures are provided by
an unsupervised audio-visual coincidence model. Using the noisy labels, along
with attention between video and audio features, AudioScope learns to identify
audio-visual similarity and to suppress off-screen sounds. We demonstrate the
effectiveness of our approach using a dataset of video clips extracted from
open-domain YFCC100m video data. This dataset contains a wide diversity of
sound classes recorded in unconstrained conditions, making the application of
previous methods unsuitable. For evaluation and semi-supervised experiments, we
collected human labels for presence of on-screen and off-screen sounds on a
small subset of clips.",arxiv
http://arxiv.org/abs/1811.07807v2,2018-11-20T09:43:21Z,2018-11-19T17:10:44Z,Deeper Interpretability of Deep Networks,"Deep Convolutional Neural Networks (CNNs) have been one of the most
influential recent developments in computer vision, particularly for
categorization. There is an increasing demand for explainable AI as these
systems are deployed in the real world. However, understanding the information
represented and processed in CNNs remains in most cases challenging. Within
this paper, we explore the use of new information theoretic techniques
developed in the field of neuroscience to enable novel understanding of how a
CNN represents information. We trained a 10-layer ResNet architecture to
identify 2,000 face identities from 26M images generated using a rigorously
controlled 3D face rendering model that produced variations of intrinsic (i.e.
face morphology, gender, age, expression and ethnicity) and extrinsic factors
(i.e. 3D pose, illumination, scale and 2D translation). With our methodology,
we demonstrate that unlike human's network overgeneralizes face identities even
with extreme changes of face shape, but it is more sensitive to changes of
texture. To understand the processing of information underlying these
counterintuitive properties, we visualize the features of shape and texture
that the network processes to identify faces. Then, we shed a light into the
inner workings of the black box and reveal how hidden layers represent these
features and whether the representations are invariant to pose. We hope that
our methodology will provide an additional valuable tool for interpretability
of CNNs.",arxiv
http://arxiv.org/abs/2007.11794v1,2020-07-23T05:15:14Z,2020-07-23T05:15:14Z,"Applying GPGPU to Recurrent Neural Network Language Model based Fast
  Network Search in the Real-Time LVCSR","Recurrent Neural Network Language Models (RNNLMs) have started to be used in
various fields of speech recognition due to their outstanding performance.
However, the high computational complexity of RNNLMs has been a hurdle in
applying the RNNLM to a real-time Large Vocabulary Continuous Speech
Recognition (LVCSR). In order to accelerate the speed of RNNLM-based network
searches during decoding, we apply the General Purpose Graphic Processing Units
(GPGPUs). This paper proposes a novel method of applying GPGPUs to RNNLM-based
graph traversals. We have achieved our goal by reducing redundant computations
on CPUs and amount of transfer between GPGPUs and CPUs. The proposed approach
was evaluated on both WSJ corpus and in-house data. Experiments shows that the
proposed approach achieves the real-time speed in various circumstances while
maintaining the Word Error Rate (WER) to be relatively 10% lower than that of
n-gram models.",arxiv
http://arxiv.org/abs/1907.06968v1,2019-07-16T12:50:42Z,2019-07-16T12:50:42Z,"A Unified Deep Framework for Joint 3D Pose Estimation and Action
  Recognition from a Single RGB Camera","We present a deep learning-based multitask framework for joint 3D human pose
estimation and action recognition from RGB video sequences. Our approach
proceeds along two stages. In the first, we run a real-time 2D pose detector to
determine the precise pixel location of important keypoints of the body. A
two-stream neural network is then designed and trained to map detected 2D
keypoints into 3D poses. In the second, we deploy the Efficient Neural
Architecture Search (ENAS) algorithm to find an optimal network architecture
that is used for modeling the spatio-temporal evolution of the estimated 3D
poses via an image-based intermediate representation and performing action
recognition. Experiments on Human3.6M, MSR Action3D and SBU Kinect Interaction
datasets verify the effectiveness of the proposed method on the targeted tasks.
Moreover, we show that our method requires a low computational budget for
training and inference.",arxiv
http://arxiv.org/abs/1706.06551v2,2017-06-26T09:47:36Z,2017-06-20T17:09:29Z,Grounded Language Learning in a Simulated 3D World,"We are increasingly surrounded by artificially intelligent technology that
takes decisions and executes actions on our behalf. This creates a pressing
need for general means to communicate with, instruct and guide artificial
agents, with human language the most compelling means for such communication.
To achieve this in a scalable fashion, agents must be able to relate language
to the world and to actions; that is, their understanding of language must be
grounded and embodied. However, learning grounded language is a notoriously
challenging problem in artificial intelligence research. Here we present an
agent that learns to interpret language in a simulated 3D environment where it
is rewarded for the successful execution of written instructions. Trained via a
combination of reinforcement and unsupervised learning, and beginning with
minimal prior knowledge, the agent learns to relate linguistic symbols to
emergent perceptual representations of its physical surroundings and to
pertinent sequences of actions. The agent's comprehension of language extends
beyond its prior experience, enabling it to apply familiar language to
unfamiliar situations and to interpret entirely novel instructions. Moreover,
the speed with which this agent learns new words increases as its semantic
knowledge grows. This facility for generalising and bootstrapping semantic
knowledge indicates the potential of the present approach for reconciling
ambiguous natural language with the complexity of the physical world.",arxiv
http://arxiv.org/abs/2102.10635v2,2021-06-28T16:23:46Z,2021-02-21T16:15:40Z,"AI-Augmented Behavior Analysis for Children with Developmental
  Disabilities: Building Towards Precision Treatment","Autism spectrum disorder is a developmental disorder characterized by
significant social, communication, and behavioral challenges. Individuals
diagnosed with autism, intellectual, and developmental disabilities (AUIDD)
typically require long-term care and targeted treatment and teaching. Effective
treatment of AUIDD relies on efficient and careful behavioral observations done
by trained applied behavioral analysts (ABAs). However, this process
overburdens ABAs by requiring the clinicians to collect and analyze data,
identify the problem behaviors, conduct pattern analysis to categorize and
predict categorical outcomes, hypothesize responsiveness to treatments, and
detect the effects of treatment plans. Successful integration of digital
technologies into clinical decision-making pipelines and the advancements in
automated decision-making using Artificial Intelligence (AI) algorithms
highlights the importance of augmenting teaching and treatments using novel
algorithms and high-fidelity sensors. In this article, we present an
AI-Augmented Learning and Applied Behavior Analytics (AI-ABA) platform to
provide personalized treatment and learning plans to AUIDD individuals. By
defining systematic experiments along with automated data collection and
analysis, AI-ABA can promote self-regulative behavior using reinforcement-based
augmented or virtual reality and other mobile platforms. Thus, AI-ABA could
assist clinicians to focus on making precise data-driven decisions and increase
the quality of individualized interventions for individuals with AUIDD.",arxiv
http://arxiv.org/abs/1912.11474v3,2020-08-21T18:00:31Z,2019-12-24T18:59:50Z,SoundSpaces: Audio-Visual Navigation in 3D Environments,"Moving around in the world is naturally a multisensory experience, but
today's embodied agents are deaf---restricted to solely their visual perception
of the environment. We introduce audio-visual navigation for complex,
acoustically and visually realistic 3D environments. By both seeing and
hearing, the agent must learn to navigate to a sounding object. We propose a
multi-modal deep reinforcement learning approach to train navigation policies
end-to-end from a stream of egocentric audio-visual observations, allowing the
agent to (1) discover elements of the geometry of the physical space indicated
by the reverberating audio and (2) detect and follow sound-emitting targets. We
further introduce SoundSpaces: a first-of-its-kind dataset of audio renderings
based on geometrical acoustic simulations for two sets of publicly available 3D
environments (Matterport3D and Replica), and we instrument Habitat to support
the new sensor, making it possible to insert arbitrary sound sources in an
array of real-world scanned environments. Our results show that audio greatly
benefits embodied visual navigation in 3D spaces, and our work lays groundwork
for new research in embodied AI with audio-visual perception.",arxiv
http://arxiv.org/abs/2006.11751v2,2020-06-23T00:41:58Z,2020-06-21T10:00:23Z,"Sample Factory: Egocentric 3D Control from Pixels at 100000 FPS with
  Asynchronous Reinforcement Learning","Increasing the scale of reinforcement learning experiments has allowed
researchers to achieve unprecedented results in both training sophisticated
agents for video games, and in sim-to-real transfer for robotics. Typically
such experiments rely on large distributed systems and require expensive
hardware setups, limiting wider access to this exciting area of research. In
this work we aim to solve this problem by optimizing the efficiency and
resource utilization of reinforcement learning algorithms instead of relying on
distributed computation. We present the ""Sample Factory"", a high-throughput
training system optimized for a single-machine setting. Our architecture
combines a highly efficient, asynchronous, GPU-based sampler with off-policy
correction techniques, allowing us to achieve throughput higher than $10^5$
environment frames/second on non-trivial control problems in 3D without
sacrificing sample efficiency. We extend Sample Factory to support self-play
and population-based training and apply these techniques to train highly
capable agents for a multiplayer first-person shooter game. The source code is
available at https://github.com/alex-petrenko/sample-factory",arxiv
http://arxiv.org/abs/1712.09347v1,2017-12-25T02:08:39Z,2017-12-25T02:08:39Z,"Smart Fog: Fog Computing Framework for Unsupervised Clustering Analytics
  in Wearable Internet of Things","The increasing use of wearables in smart telehealth generates heterogeneous
medical big data. Cloud and fog services process these data for assisting
clinical procedures. IoT based ehealthcare have greatly benefited from
efficient data processing. This paper proposed and evaluated use of low
resource machine learning on Fog devices kept close to the wearables for smart
healthcare. In state of the art telecare systems, the signal processing and
machine learning modules are deployed in the cloud for processing physiological
data. We developed a prototype of Fog-based unsupervised machine learning big
data analysis for discovering patterns in physiological data. We employed Intel
Edison and Raspberry Pi as Fog computer in proposed architecture. We performed
validation studies on real-world pathological speech data from in home
monitoring of patients with Parkinson's disease (PD). Proposed architecture
employed machine learning for analysis of pathological speech data obtained
from smartwatches worn by the patients with PD. Results showed that proposed
architecture is promising for low-resource clinical machine learning. It could
be useful for other applications within wearable IoT for smart telehealth
scenarios by translating machine learning approaches from the cloud backend to
edge computing devices such as Fog.",arxiv
http://arxiv.org/abs/2011.12715v1,2020-11-23T00:34:56Z,2020-11-23T00:34:56Z,"Resonance: Replacing Software Constants with Context-Aware Models in
  Real-time Communication","Large software systems tune hundreds of 'constants' to optimize their runtime
performance. These values are commonly derived through intuition, lab tests, or
A/B tests. A 'one-size-fits-all' approach is often sub-optimal as the best
value depends on runtime context. In this paper, we provide an experimental
approach to replace constants with learned contextual functions for Skype - a
widely used real-time communication (RTC) application. We present Resonance, a
system based on contextual bandits (CB). We describe experiences from three
real-world experiments: applying it to the audio, video, and transport
components in Skype. We surface a unique and practical challenge of performing
machine learning (ML) inference in large software systems written using
encapsulation principles. Finally, we open-source FeatureBroker, a library to
reduce the friction in adopting ML models in such development environments",arxiv
http://arxiv.org/abs/1809.06244v2,2019-01-25T11:15:56Z,2018-09-14T14:29:47Z,"A Virtual Testbed for Critical Incident Investigation with Autonomous
  Remote Aerial Vehicle Surveying, Artificial Intelligence, and Decision
  Support","Autonomous robotics and artificial intelligence techniques can be used to
support human personnel in the event of critical incidents. These incidents can
pose great danger to human life. Some examples of such assistance include:
multi-robot surveying of the scene; collection of sensor data and scene
imagery, real-time risk assessment and analysis; object identification and
anomaly detection; and retrieval of relevant supporting documentation such as
standard operating procedures (SOPs). These incidents, although often rare, can
involve chemical, biological, radiological/nuclear or explosive (CBRNE)
substances and can be of high consequence. Real-world training and deployment
of these systems can be costly and sometimes not feasible. For this reason, we
have developed a realistic 3D model of a CBRNE scenario to act as a testbed for
an initial set of assisting AI tools that we have developed.",arxiv
http://arxiv.org/abs/2009.06816v1,2020-09-15T01:44:39Z,2020-09-15T01:44:39Z,Microscope Based HER2 Scoring System,"The overexpression of human epidermal growth factor receptor 2 (HER2) has
been established as a therapeutic target in multiple types of cancers, such as
breast and gastric cancers. Immunohistochemistry (IHC) is employed as a basic
HER2 test to identify the HER2-positive, borderline, and HER2-negative
patients. However, the reliability and accuracy of HER2 scoring are affected by
many factors, such as pathologists' experience. Recently, artificial
intelligence (AI) has been used in various disease diagnosis to improve
diagnostic accuracy and reliability, but the interpretation of diagnosis
results is still an open problem. In this paper, we propose a real-time HER2
scoring system, which follows the HER2 scoring guidelines to complete the
diagnosis, and thus each step is explainable. Unlike the previous scoring
systems based on whole-slide imaging, our HER2 scoring system is integrated
into an augmented reality (AR) microscope that can feedback AI results to the
pathologists while reading the slide. The pathologists can help select
informative fields of view (FOVs), avoiding the confounding regions, such as
DCIS. Importantly, we illustrate the intermediate results with membrane
staining condition and cell classification results, making it possible to
evaluate the reliability of the diagnostic results. Also, we support the
interactive modification of selecting regions-of-interest, making our system
more flexible in clinical practice. The collaboration of AI and pathologists
can significantly improve the robustness of our system. We evaluate our system
with 285 breast IHC HER2 slides, and the classification accuracy of 95\% shows
the effectiveness of our HER2 scoring system.",arxiv
http://arxiv.org/abs/2007.13678v1,2020-07-10T20:55:11Z,2020-07-10T20:55:11Z,"Cloud Detection through Wavelet Transforms in Machine Learning and Deep
  Learning","Cloud detection is a specialized application of image recognition and object
detection using remotely sensed data. The task presents a number of challenges,
including analyzing images obtained in visible, infrared and multi-spectral
frequencies, usually without ground truth data for comparison. Moreover,
machine learning and deep learning (MLDL) algorithms applied to this task are
required to be computationally efficient, as they are typically deployed in
low-power devices and called to operate in real-time.
  This paper explains Wavelet Transform (WT) theory, comparing it to more
widely used image and signal processing transforms, and explores the use of WT
as a powerful signal compressor and feature extractor for MLDL classifiers.",arxiv
http://arxiv.org/abs/1510.02055v1,2015-10-07T18:34:36Z,2015-10-07T18:34:36Z,"Diverse Large-Scale ITS Dataset Created from Continuous Learning for
  Real-Time Vehicle Detection","In traffic engineering, vehicle detectors are trained on limited datasets
resulting in poor accuracy when deployed in real world applications. Annotating
large-scale high quality datasets is challenging. Typically, these datasets
have limited diversity; they do not reflect the real-world operating
environment. There is a need for a large-scale, cloud based positive and
negative mining (PNM) process and a large-scale learning and evaluation system
for the application of traffic event detection. The proposed positive and
negative mining process addresses the quality of crowd sourced ground truth
data through machine learning review and human feedback mechanisms. The
proposed learning and evaluation system uses a distributed cloud computing
framework to handle data-scaling issues associated with large numbers of
samples and a high-dimensional feature space. The system is trained using
AdaBoost on $1,000,000$ Haar-like features extracted from $70,000$ annotated
video frames. The trained real-time vehicle detector achieves an accuracy of at
least $95\%$ for $1/2$ and about $78\%$ for $19/20$ of the time when tested on
approximately $7,500,000$ video frames. At the end of 2015, the dataset is
expect to have over one billion annotated video frames.",arxiv
http://arxiv.org/abs/1804.08264v1,2018-04-23T07:07:20Z,2018-04-23T07:07:20Z,To Create What You Tell: Generating Videos from Captions,"We are creating multimedia contents everyday and everywhere. While automatic
content generation has played a fundamental challenge to multimedia community
for decades, recent advances of deep learning have made this problem feasible.
For example, the Generative Adversarial Networks (GANs) is a rewarding approach
to synthesize images. Nevertheless, it is not trivial when capitalizing on GANs
to generate videos. The difficulty originates from the intrinsic structure
where a video is a sequence of visually coherent and semantically dependent
frames. This motivates us to explore semantic and temporal coherence in
designing GANs to generate videos. In this paper, we present a novel Temporal
GANs conditioning on Captions, namely TGANs-C, in which the input to the
generator network is a concatenation of a latent noise vector and caption
embedding, and then is transformed into a frame sequence with 3D
spatio-temporal convolutions. Unlike the naive discriminator which only judges
pairs as fake or real, our discriminator additionally notes whether the video
matches the correct caption. In particular, the discriminator network consists
of three discriminators: video discriminator classifying realistic videos from
generated ones and optimizes video-caption matching, frame discriminator
discriminating between real and fake frames and aligning frames with the
conditioning caption, and motion discriminator emphasizing the philosophy that
the adjacent frames in the generated videos should be smoothly connected as in
real ones. We qualitatively demonstrate the capability of our TGANs-C to
generate plausible videos conditioning on the given captions on two synthetic
datasets (SBMG and TBMG) and one real-world dataset (MSVD). Moreover,
quantitative experiments on MSVD are performed to validate our proposal via
Generative Adversarial Metric and human study.",arxiv
http://arxiv.org/abs/2108.08762v1,2021-08-19T16:06:16Z,2021-08-19T16:06:16Z,"Dynamic Difficulty Adjustment in Virtual Reality Exergames through
  Experience-driven Procedural Content Generation","Virtual Reality (VR) games that feature physical activities have been shown
to increase players' motivation to do physical exercise. However, for such
exercises to have a positive healthcare effect, they have to be repeated
several times a week. To maintain player motivation over longer periods of
time, games often employ Dynamic Difficulty Adjustment (DDA) to adapt the
game's challenge according to the player's capabilities. For exercise games,
this is mostly done by tuning specific in-game parameters like the speed of
objects. In this work, we propose to use experience-driven Procedural Content
Generation for DDA in VR exercise games by procedurally generating levels that
match the player's current capabilities. Not only finetuning specific
parameters but creating completely new levels has the potential to decrease
repetition over longer time periods and allows for the simultaneous adaptation
of the cognitive and physical challenge of the exergame. As a proof-of-concept,
we implement an initial prototype in which the player must traverse a maze that
includes several exercise rooms, whereby the generation of the maze is realized
by a neural network. Passing those exercise rooms requires the player to
perform physical activities. To match the player's capabilities, we use Deep
Reinforcement Learning to adjust the structure of the maze and to decide which
exercise rooms to include in the maze. We evaluate our prototype in an
exploratory user study utilizing both biodata and subjective questionnaires.",arxiv
http://arxiv.org/abs/1610.00552v1,2016-09-30T10:44:32Z,2016-09-30T10:44:32Z,FPGA-Based Low-Power Speech Recognition with Recurrent Neural Networks,"In this paper, a neural network based real-time speech recognition (SR)
system is developed using an FPGA for very low-power operation. The implemented
system employs two recurrent neural networks (RNNs); one is a
speech-to-character RNN for acoustic modeling (AM) and the other is for
character-level language modeling (LM). The system also employs a statistical
word-level LM to improve the recognition accuracy. The results of the AM, the
character-level LM, and the word-level LM are combined using a fairly simple
N-best search algorithm instead of the hidden Markov model (HMM) based network.
The RNNs are implemented using massively parallel processing elements (PEs) for
low latency and high throughput. The weights are quantized to 6 bits to store
all of them in the on-chip memory of an FPGA. The proposed algorithm is
implemented on a Xilinx XC7Z045, and the system can operate much faster than
real-time.",arxiv
http://arxiv.org/abs/2103.02644v2,2021-07-14T23:14:36Z,2021-03-03T19:16:53Z,Compute and memory efficient universal sound source separation,"Recent progress in audio source separation lead by deep learning has enabled
many neural network models to provide robust solutions to this fundamental
estimation problem. In this study, we provide a family of efficient neural
network architectures for general purpose audio source separation while
focusing on multiple computational aspects that hinder the application of
neural networks in real-world scenarios. The backbone structure of this
convolutional network is the SUccessive DOwnsampling and Resampling of
Multi-Resolution Features (SuDoRM-RF) as well as their aggregation which is
performed through simple one-dimensional convolutions. This mechanism enables
our models to obtain high fidelity signal separation in a wide variety of
settings where variable number of sources are present and with limited
computational resources (e.g. floating point operations, memory footprint,
number of parameters and latency). Our experiments show that SuDoRM-RF models
perform comparably and even surpass several state-of-the-art benchmarks with
significantly higher computational resource requirements. The causal variation
of SuDoRM-RF is able to obtain competitive performance in real-time speech
separation of around 10dB scale-invariant signal-to-distortion ratio
improvement (SI-SDRi) while remaining up to 20 times faster than real-time on a
laptop device.",arxiv
http://arxiv.org/abs/1808.06352v1,2018-08-20T09:06:21Z,2018-08-20T09:06:21Z,"Navigating the Landscape for Real-time Localisation and Mapping for
  Robotics and Virtual and Augmented Reality","Visual understanding of 3D environments in real-time, at low power, is a huge
computational challenge. Often referred to as SLAM (Simultaneous Localisation
and Mapping), it is central to applications spanning domestic and industrial
robotics, autonomous vehicles, virtual and augmented reality. This paper
describes the results of a major research effort to assemble the algorithms,
architectures, tools, and systems software needed to enable delivery of SLAM,
by supporting applications specialists in selecting and configuring the
appropriate algorithm and the appropriate hardware, and compilation pathway, to
meet their performance, accuracy, and energy consumption goals. The major
contributions we present are (1) tools and methodology for systematic
quantitative evaluation of SLAM algorithms, (2) automated,
machine-learning-guided exploration of the algorithmic and implementation
design space with respect to multiple objectives, (3) end-to-end simulation
tools to enable optimisation of heterogeneous, accelerated architectures for
the specific algorithmic requirements of the various SLAM algorithmic
approaches, and (4) tools for delivering, where appropriate, accelerated,
adaptive SLAM solutions in a managed, JIT-compiled, adaptive runtime context.",arxiv
http://arxiv.org/abs/2105.00187v1,2021-05-01T08:02:59Z,2021-05-01T08:02:59Z,"One Detector to Rule Them All: Towards a General Deepfake Attack
  Detection Framework","Deep learning-based video manipulation methods have become widely accessible
to the masses. With little to no effort, people can quickly learn how to
generate deepfake (DF) videos. While deep learning-based detection methods have
been proposed to identify specific types of DFs, their performance suffers for
other types of deepfake methods, including real-world deepfakes, on which they
are not sufficiently trained. In other words, most of the proposed deep
learning-based detection methods lack transferability and generalizability.
Beyond detecting a single type of DF from benchmark deepfake datasets, we focus
on developing a generalized approach to detect multiple types of DFs, including
deepfakes from unknown generation methods such as DeepFake-in-the-Wild (DFW)
videos. To better cope with unknown and unseen deepfakes, we introduce a
Convolutional LSTM-based Residual Network (CLRNet), which adopts a unique model
training strategy and explores spatial as well as the temporal information in
deepfakes. Through extensive experiments, we show that existing defense methods
are not ready for real-world deployment. Whereas our defense method (CLRNet)
achieves far better generalization when detecting various benchmark deepfake
methods (97.57% on average). Furthermore, we evaluate our approach with a
high-quality DeepFake-in-the-Wild dataset, collected from the Internet
containing numerous videos and having more than 150,000 frames. Our CLRNet
model demonstrated that it generalizes well against high-quality DFW videos by
achieving 93.86% detection accuracy, outperforming existing state-of-the-art
defense methods by a considerable margin.",arxiv
http://arxiv.org/abs/2110.04934v1,2021-10-11T00:08:48Z,2021-10-11T00:08:48Z,"Wav2vec-Switch: Contrastive Learning from Original-noisy Speech Pairs
  for Robust Speech Recognition","The goal of self-supervised learning (SSL) for automatic speech recognition
(ASR) is to learn good speech representations from a large amount of unlabeled
speech for the downstream ASR task. However, most SSL frameworks do not
consider noise robustness which is crucial for real-world applications. In this
paper we propose wav2vec-Switch, a method to encode noise robustness into
contextualized representations of speech via contrastive learning.
Specifically, we feed original-noisy speech pairs simultaneously into the
wav2vec 2.0 network. In addition to the existing contrastive learning task, we
switch the quantized representations of the original and noisy speech as
additional prediction targets of each other. By doing this, it enforces the
network to have consistent predictions for the original and noisy speech, thus
allows to learn contextualized representation with noise robustness. Our
experiments on synthesized and real noisy data show the effectiveness of our
method: it achieves 2.9--4.9% relative word error rate (WER) reduction on the
synthesized noisy LibriSpeech data without deterioration on the original data,
and 5.7% on CHiME-4 real 1-channel noisy data compared to a data augmentation
baseline even with a strong language model for decoding. Our results on CHiME-4
can match or even surpass those with well-designed speech enhancement
components.",arxiv
http://arxiv.org/abs/2103.14749v4,2021-11-07T13:04:04Z,2021-03-26T21:54:36Z,"Pervasive Label Errors in Test Sets Destabilize Machine Learning
  Benchmarks","We identify label errors in the test sets of 10 of the most commonly-used
computer vision, natural language, and audio datasets, and subsequently study
the potential for these label errors to affect benchmark results. Errors in
test sets are numerous and widespread: we estimate an average of at least 3.3%
errors across the 10 datasets, where for example label errors comprise at least
6% of the ImageNet validation set. Putative label errors are identified using
confident learning algorithms and then human-validated via crowdsourcing (51%
of the algorithmically-flagged candidates are indeed erroneously labeled, on
average across the datasets). Traditionally, machine learning practitioners
choose which model to deploy based on test accuracy - our findings advise
caution here, proposing that judging models over correctly labeled test sets
may be more useful, especially for noisy real-world datasets. Surprisingly, we
find that lower capacity models may be practically more useful than higher
capacity models in real-world datasets with high proportions of erroneously
labeled data. For example, on ImageNet with corrected labels: ResNet-18
outperforms ResNet-50 if the prevalence of originally mislabeled test examples
increases by just 6%. On CIFAR-10 with corrected labels: VGG-11 outperforms
VGG-19 if the prevalence of originally mislabeled test examples increases by
just 5%. Test set errors across the 10 datasets can be viewed at
https://labelerrors.com and all label errors can be reproduced by
https://github.com/cleanlab/label-errors.",arxiv
http://arxiv.org/abs/2104.05828v1,2021-04-12T21:30:53Z,2021-04-12T21:30:53Z,"Evidence-based Prescriptive Analytics, CAUSAL Digital Twin and a
  Learning Estimation Algorithm","Evidence-based Prescriptive Analytics (EbPA) is necessary to determine
optimal operational set-points that will improve business productivity. EbPA
results from what-if analysis and counterfactual experimentation on CAUSAL
Digital Twins (CDTs) that quantify cause-effect relationships in the DYNAMICS
of a system of connected assets. We describe the basics of Causality and Causal
Graphs and develop a Learning Causal Digital Twin (LCDT) solution; our
algorithm uses a simple recurrent neural network with some innovative
modifications incorporating Causal Graph simulation. Since LCDT is a learning
digital twin where parameters are learned online in real-time with minimal
pre-configuration, the work of deploying digital twins will be significantly
simplified. A proof-of-principle of LCDT was conducted using real vibration
data from a system of bearings; results of causal factor estimation, what-if
analysis study and counterfactual experiment are very encouraging.",arxiv
http://arxiv.org/abs/2009.10283v1,2020-09-22T02:31:00Z,2020-09-22T02:31:00Z,End-to-End Learning of Speech 2D Feature-Trajectory for Prosthetic Hands,"Speech is one of the most common forms of communication in humans. Speech
commands are essential parts of multimodal controlling of prosthetic hands. In
the past decades, researchers used automatic speech recognition systems for
controlling prosthetic hands by using speech commands. Automatic speech
recognition systems learn how to map human speech to text. Then, they used
natural language processing or a look-up table to map the estimated text to a
trajectory. However, the performance of conventional speech-controlled
prosthetic hands is still unsatisfactory. Recent advancements in
general-purpose graphics processing units (GPGPUs) enable intelligent devices
to run deep neural networks in real-time. Thus, architectures of intelligent
systems have rapidly transformed from the paradigm of composite subsystems
optimization to the paradigm of end-to-end optimization. In this paper, we
propose an end-to-end convolutional neural network (CNN) that maps speech 2D
features directly to trajectories for prosthetic hands. The proposed
convolutional neural network is lightweight, and thus it runs in real-time in
an embedded GPGPU. The proposed method can use any type of speech 2D feature
that has local correlations in each dimension such as spectrogram, MFCC, or
PNCC. We omit the speech to text step in controlling the prosthetic hand in
this paper. The network is written in Python with Keras library that has a
TensorFlow backend. We optimized the CNN for NVIDIA Jetson TX2 developer kit.
Our experiment on this CNN demonstrates a root-mean-square error of 0.119 and
20ms running time to produce trajectory outputs corresponding to the voice
input data. To achieve a lower error in real-time, we can optimize a similar
CNN for a more powerful embedded GPGPU such as NVIDIA AGX Xavier.",arxiv
http://arxiv.org/abs/2006.11021v2,2020-11-05T14:41:47Z,2020-06-19T08:54:46Z,"Boosting Active Learning for Speech Recognition with Noisy
  Pseudo-labeled Samples","The cost of annotating transcriptions for large speech corpora becomes a
bottleneck to maximally enjoy the potential capacity of deep neural
network-based automatic speech recognition models. In this paper, we present a
new training pipeline boosting the conventional active learning approach
targeting label-efficient learning to resolve the mentioned problem. Existing
active learning methods only focus on selecting a set of informative samples
under a labeling budget. One step further, we suggest that the training
efficiency can be further improved by utilizing the unlabeled samples,
exceeding the labeling budget, by introducing sophisticatedly configured
unsupervised loss complementing supervised loss effectively. We propose new
unsupervised loss based on consistency regularization, and we configure
appropriate augmentation techniques for utterances to adopt consistency
regularization in the automatic speech recognition task. From the qualitative
and quantitative experiments on the real-world dataset and under real-usage
scenarios, we show that the proposed training pipeline can boost the efficacy
of active learning approaches, thus successfully reducing a sustainable amount
of human labeling cost.",arxiv
http://arxiv.org/abs/1802.05383v1,2018-02-15T02:00:54Z,2018-02-15T02:00:54Z,Deep Learning Based Speech Beamforming,"Multi-channel speech enhancement with ad-hoc sensors has been a challenging
task. Speech model guided beamforming algorithms are able to recover natural
sounding speech, but the speech models tend to be oversimplified or the
inference would otherwise be too complicated. On the other hand, deep learning
based enhancement approaches are able to learn complicated speech distributions
and perform efficient inference, but they are unable to deal with variable
number of input channels. Also, deep learning approaches introduce a lot of
errors, particularly in the presence of unseen noise types and settings. We
have therefore proposed an enhancement framework called DEEPBEAM, which
combines the two complementary classes of algorithms. DEEPBEAM introduces a
beamforming filter to produce natural sounding speech, but the filter
coefficients are determined with the help of a monaural speech enhancement
neural network. Experiments on synthetic and real-world data show that DEEPBEAM
is able to produce clean, dry and natural sounding speech, and is robust
against unseen noise.",arxiv
http://arxiv.org/abs/2104.14870v1,2021-04-30T09:53:28Z,2021-04-30T09:53:28Z,"Action in Mind: A Neural Network Approach to Action Recognition and
  Segmentation","Recognizing and categorizing human actions is an important task with
applications in various fields such as human-robot interaction, video analysis,
surveillance, video retrieval, health care system and entertainment industry.
This thesis presents a novel computational approach for human action
recognition through different implementations of multi-layer architectures
based on artificial neural networks. Each system level development is designed
to solve different aspects of the action recognition problem including online
real-time processing, action segmentation and the involvement of objects. The
analysis of the experimental results are illustrated and described in six
articles. The proposed action recognition architecture of this thesis is
composed of several processing layers including a preprocessing layer, an
ordered vector representation layer and three layers of neural networks. It
utilizes self-organizing neural networks such as Kohonen feature maps and
growing grids as the main neural network layers. Thus the architecture presents
a biological plausible approach with certain features such as topographic
organization of the neurons, lateral interactions, semi-supervised learning and
the ability to represent high dimensional input space in lower dimensional
maps. For each level of development the system is trained with the input data
consisting of consecutive 3D body postures and tested with generalized input
data that the system has never met before. The experimental results of
different system level developments show that the system performs well with
quite high accuracy for recognizing human actions.",arxiv
http://arxiv.org/abs/1911.13068v1,2019-11-29T11:45:20Z,2019-11-29T11:45:20Z,Sparsely Grouped Input Variables for Neural Networks,"In genomic analysis, biomarker discovery, image recognition, and other
systems involving machine learning, input variables can often be organized into
different groups by their source or semantic category. Eliminating some groups
of variables can expedite the process of data acquisition and avoid
over-fitting. Researchers have used the group lasso to ensure group sparsity in
linear models and have extended it to create compact neural networks in
meta-learning. Different from previous studies, we use multi-layer non-linear
neural networks to find sparse groups for input variables. We propose a new
loss function to regularize parameters for grouped input variables, design a
new optimization algorithm for this loss function, and test these methods in
three real-world settings. We achieve group sparsity for three datasets,
maintaining satisfying results while excluding one nucleotide position from an
RNA splicing experiment, excluding 89.9% of stimuli from an eye-tracking
experiment, and excluding 60% of image rows from an experiment on the MNIST
dataset.",arxiv
http://arxiv.org/abs/2105.08886v2,2021-10-30T14:21:58Z,2021-05-19T02:11:43Z,"Towards Trusted and Intelligent Cyber-Physical Systems: A
  Security-by-Design Approach","The complexity of cyberattacks in Cyber-Physical Systems (CPSs) calls for a
mechanism that can evaluate the operational behaviour and security without
negatively affecting the operation of live systems. In this regard, Digital
Twins (DTs) are revolutionizing the CPSs. DTs strengthen the security of CPSs
throughout the product lifecycle, while assuming that the DT data is trusted,
providing agility to predict and respond to real-time changes. However,
existing DTs solutions in CPS are constrained with untrustworthy data
dissemination among multiple stakeholders and timely course correction. Such
limitations reinforce the significance of designing trustworthy distributed
solutions with the ability to create actionable insights in real-time. To do
so, we propose a framework that focuses on trusted and intelligent DT by
integrating blockchain and Artificial Intelligence (AI). Following a hybrid
approach, the proposed framework not only acquires process knowledge from the
specifications of the CPS, but also relies on AI to learn security threats
based on sensor data. Furthermore, we integrate blockchain to safeguard product
lifecycle data. We discuss the applicability of the proposed framework for the
automotive industry as a CPS use case. Finally, we identify the open challenges
that impede the implementation of intelligence-driven architectures in CPSs.",arxiv
http://arxiv.org/abs/2007.13404v2,2020-10-29T16:23:12Z,2020-07-27T09:50:11Z,"YOLOpeds: Efficient Real-Time Single-Shot Pedestrian Detection for Smart
  Camera Applications","Deep Learning-based object detectors can enhance the capabilities of smart
camera systems in a wide spectrum of machine vision applications including
video surveillance, autonomous driving, robots and drones, smart factory, and
health monitoring. Pedestrian detection plays a key role in all these
applications and deep learning can be used to construct accurate
state-of-the-art detectors. However, such complex paradigms do not scale easily
and are not traditionally implemented in resource-constrained smart cameras for
on-device processing which offers significant advantages in situations when
real-time monitoring and robustness are vital. Efficient neural networks can
not only enable mobile applications and on-device experiences but can also be a
key enabler of privacy and security allowing a user to gain the benefits of
neural networks without needing to send their data to the server to be
evaluated. This work addresses the challenge of achieving a good trade-off
between accuracy and speed for efficient deployment of deep-learning-based
pedestrian detection in smart camera applications. A computationally efficient
architecture is introduced based on separable convolutions and proposes
integrating dense connections across layers and multi-scale feature fusion to
improve representational capacity while decreasing the number of parameters and
operations. In particular, the contributions of this work are the following: 1)
An efficient backbone combining multi-scale feature operations, 2) a more
elaborate loss function for improved localization, 3) an anchor-less approach
for detection, The proposed approach called YOLOpeds is evaluated using the
PETS2009 surveillance dataset on 320x320 images. Overall, YOLOpeds provides
real-time sustained operation of over 30 frames per second with detection rates
in the range of 86% outperforming existing deep learning models.",arxiv
http://arxiv.org/abs/2004.09215v1,2020-04-20T11:36:02Z,2020-04-20T11:36:02Z,"CatNet: Class Incremental 3D ConvNets for Lifelong Egocentric Gesture
  Recognition","Egocentric gestures are the most natural form of communication for humans to
interact with wearable devices such as VR/AR helmets and glasses. A major issue
in such scenarios for real-world applications is that may easily become
necessary to add new gestures to the system e.g., a proper VR system should
allow users to customize gestures incrementally. Traditional deep learning
methods require storing all previous class samples in the system and training
the model again from scratch by incorporating previous samples and new samples,
which costs humongous memory and significantly increases computation over time.
In this work, we demonstrate a lifelong 3D convolutional framework --
c(C)la(a)ss increment(t)al net(Net)work (CatNet), which considers temporal
information in videos and enables lifelong learning for egocentric gesture
video recognition by learning the feature representation of an exemplar set
selected from previous class samples. Importantly, we propose a two-stream
CatNet, which deploys RGB and depth modalities to train two separate networks.
We evaluate CatNets on a publicly available dataset -- EgoGesture dataset, and
show that CatNets can learn many classes incrementally over a long period of
time. Results also demonstrate that the two-stream architecture achieves the
best performance on both joint training and class incremental training compared
to 3 other one-stream architectures. The codes and pre-trained models used in
this work are provided at https://github.com/villawang/CatNet.",arxiv
http://arxiv.org/abs/2012.04746v2,2021-04-02T22:28:33Z,2020-12-08T21:20:54Z,"Robust Neural Routing Through Space Partitions for Camera Relocalization
  in Dynamic Indoor Environments","Localizing the camera in a known indoor environment is a key building block
for scene mapping, robot navigation, AR, etc. Recent advances estimate the
camera pose via optimization over the 2D/3D-3D correspondences established
between the coordinates in 2D/3D camera space and 3D world space. Such a
mapping is estimated with either a convolution neural network or a decision
tree using only the static input image sequence, which makes these approaches
vulnerable to dynamic indoor environments that are quite common yet challenging
in the real world. To address the aforementioned issues, in this paper, we
propose a novel outlier-aware neural tree which bridges the two worlds, deep
learning and decision tree approaches. It builds on three important blocks: (a)
a hierarchical space partition over the indoor scene to construct the decision
tree; (b) a neural routing function, implemented as a deep classification
network, employed for better 3D scene understanding; and (c) an outlier
rejection module used to filter out dynamic points during the hierarchical
routing process. Our proposed algorithm is evaluated on the RIO-10 benchmark
developed for camera relocalization in dynamic indoor environments. It achieves
robust neural routing through space partitions and outperforms the
state-of-the-art approaches by around 30% on camera pose accuracy, while
running comparably fast for evaluation.",arxiv
http://arxiv.org/abs/1804.10822v1,2018-04-28T15:37:41Z,2018-04-28T15:37:41Z,"A Bimodal Learning Approach to Assist Multi-sensory Effects
  Synchronization","In mulsemedia applications, traditional media content (text, image, audio,
video, etc.) can be related to media objects that target other human senses
(e.g., smell, haptics, taste). Such applications aim at bridging the virtual
and real worlds through sensors and actuators. Actuators are responsible for
the execution of sensory effects (e.g., wind, heat, light), which produce
sensory stimulations on the users. In these applications sensory stimulation
must happen in a timely manner regarding the other traditional media content
being presented. For example, at the moment in which an explosion is presented
in the audiovisual content, it may be adequate to activate actuators that
produce heat and light. It is common to use some declarative multimedia
authoring language to relate the timestamp in which each media object is to be
presented to the execution of some sensory effect. One problem in this setting
is that the synchronization of media objects and sensory effects is done
manually by the author(s) of the application, a process which is time-consuming
and error prone. In this paper, we present a bimodal neural network
architecture to assist the synchronization task in mulsemedia applications. Our
approach is based on the idea that audio and video signals can be used
simultaneously to identify the timestamps in which some sensory effect should
be executed. Our learning architecture combines audio and video signals for the
prediction of scene components. For evaluation purposes, we construct a dataset
based on Google's AudioSet. We provide experiments to validate our bimodal
architecture. Our results show that the bimodal approach produces better
results when compared to several variants of unimodal architectures.",arxiv
http://arxiv.org/abs/1912.03647v2,2020-08-11T03:42:21Z,2019-12-08T09:51:08Z,Compressing 3DCNNs Based on Tensor Train Decomposition,"Three dimensional convolutional neural networks (3DCNNs) have been applied in
many tasks, e.g., video and 3D point cloud recognition. However, due to the
higher dimension of convolutional kernels, the space complexity of 3DCNNs is
generally larger than that of traditional two dimensional convolutional neural
networks (2DCNNs). To miniaturize 3DCNNs for the deployment in confining
environments such as embedded devices, neural network compression is a
promising approach. In this work, we adopt the tensor train (TT) decomposition,
a straightforward and simple in situ training compression method, to shrink the
3DCNN models. Through proposing tensorizing 3D convolutional kernels in TT
format, we investigate how to select appropriate TT ranks for achieving higher
compression ratio. We have also discussed the redundancy of 3D convolutional
kernels for compression, core significance and future directions of this work,
as well as the theoretical computation complexity versus practical executing
time of convolution in TT. In the light of multiple contrast experiments based
on VIVA challenge, UCF11, and UCF101 datasets, we conclude that TT
decomposition can compress 3DCNNs by around one hundred times without
significant accuracy loss, which will enable its applications in extensive real
world scenarios.",arxiv
http://arxiv.org/abs/2011.03790v1,2020-11-07T15:14:03Z,2020-11-07T15:14:03Z,"Rapid Pose Label Generation through Sparse Representation of Unknown
  Objects","Deep Convolutional Neural Networks (CNNs) have been successfully deployed on
robots for 6-DoF object pose estimation through visual perception. However,
obtaining labeled data on a scale required for the supervised training of CNNs
is a difficult task - exacerbated if the object is novel and a 3D model is
unavailable. To this end, this work presents an approach for rapidly generating
real-world, pose-annotated RGB-D data for unknown objects. Our method not only
circumvents the need for a prior 3D object model (textured or otherwise) but
also bypasses complicated setups of fiducial markers, turntables, and sensors.
With the help of a human user, we first source minimalistic labelings of an
ordered set of arbitrarily chosen keypoints over a set of RGB-D videos. Then,
by solving an optimization problem, we combine these labels under a world frame
to recover a sparse, keypoint-based representation of the object. The sparse
representation leads to the development of a dense model and the pose labels
for each image frame in the set of scenes. We show that the sparse model can
also be efficiently used for scaling to a large number of new scenes. We
demonstrate the practicality of the generated labeled dataset by training a
pipeline for 6-DoF object pose estimation and a pixel-wise segmentation
network.",arxiv
http://arxiv.org/abs/1912.03455v1,2019-12-07T07:36:10Z,2019-12-07T07:36:10Z,Digital Twin: Acquiring High-Fidelity 3D Avatar from a Single Image,"We present an approach to generate high fidelity 3D face avatar with a
high-resolution UV texture map from a single image. To estimate the face
geometry, we use a deep neural network to directly predict vertex coordinates
of the 3D face model from the given image. The 3D face geometry is further
refined by a non-rigid deformation process to more accurately capture facial
landmarks before texture projection. A key novelty of our approach is to train
the shape regression network on facial images synthetically generated using a
high-quality rendering engine. Moreover, our shape estimator fully leverages
the discriminative power of deep facial identity features learned from millions
of facial images. We have conducted extensive experiments to demonstrate the
superiority of our optimized 2D-to-3D rendering approach, especially its
excellent generalization property on real-world selfie images. Our proposed
system of rendering 3D avatars from 2D images has a wide range of applications
from virtual/augmented reality (VR/AR) and telepsychiatry to human-computer
interaction and social networks.",arxiv
http://arxiv.org/abs/1811.09678v1,2018-11-21T10:27:02Z,2018-11-21T10:27:02Z,Speech recognition with quaternion neural networks,"Neural network architectures are at the core of powerful automatic speech
recognition systems (ASR). However, while recent researches focus on novel
model architectures, the acoustic input features remain almost unchanged.
Traditional ASR systems rely on multidimensional acoustic features such as the
Mel filter bank energies alongside with the first, and second order derivatives
to characterize time-frames that compose the signal sequence. Considering that
these components describe three different views of the same element, neural
networks have to learn both the internal relations that exist within these
features, and external or global dependencies that exist between the
time-frames. Quaternion-valued neural networks (QNN), recently received an
important interest from researchers to process and learn such relations in
multidimensional spaces. Indeed, quaternion numbers and QNNs have shown their
efficiency to process multidimensional inputs as entities, to encode internal
dependencies, and to solve many tasks with up to four times less learning
parameters than real-valued models. We propose to investigate modern
quaternion-valued models such as convolutional and recurrent quaternion neural
networks in the context of speech recognition with the TIMIT dataset. The
experiments show that QNNs always outperform real-valued equivalent models with
way less free parameters, leading to a more efficient, compact, and expressive
representation of the relevant information.",arxiv
http://arxiv.org/abs/1909.06993v2,2020-03-08T13:22:41Z,2019-09-16T05:23:14Z,"Learning Visuomotor Policies for Aerial Navigation Using Cross-Modal
  Representations","Machines are a long way from robustly solving open-world perception-control
tasks, such as first-person view (FPV) aerial navigation. While recent advances
in end-to-end Machine Learning, especially Imitation and Reinforcement Learning
appear promising, they are constrained by the need of large amounts of
difficult-to-collect labeled real-world data. Simulated data, on the other
hand, is easy to generate, but generally does not render safe behaviors in
diverse real-life scenarios. In this work we propose a novel method for
learning robust visuomotor policies for real-world deployment which can be
trained purely with simulated data. We develop rich state representations that
combine supervised and unsupervised environment data. Our approach takes a
cross-modal perspective, where separate modalities correspond to the raw camera
data and the system states relevant to the task, such as the relative pose of
gates to the drone in the case of drone racing. We feed both data modalities
into a novel factored architecture, which learns a joint low-dimensional
embedding via Variational Auto Encoders. This compact representation is then
fed into a control policy, which we trained using imitation learning with
expert trajectories in a simulator. We analyze the rich latent spaces learned
with our proposed representations, and show that the use of our cross-modal
architecture significantly improves control policy performance as compared to
end-to-end learning or purely unsupervised feature extractors. We also present
real-world results for drone navigation through gates in different track
configurations and environmental conditions. Our proposed method, which runs
fully onboard, can successfully generalize the learned representations and
policies across simulation and reality, significantly outperforming baseline
approaches.
  Supplementary video: https://youtu.be/VKc3A5HlUU8",arxiv
http://arxiv.org/abs/1711.11141v1,2017-11-29T22:45:05Z,2017-11-29T22:45:05Z,Stream Attention for far-field multi-microphone ASR,"A stream attention framework has been applied to the posterior probabilities
of the deep neural network (DNN) to improve the far-field automatic speech
recognition (ASR) performance in the multi-microphone configuration. The stream
attention scheme has been realized through an attention vector, which is
derived by predicting the ASR performance from the phoneme posterior
distribution of individual microphone stream, focusing the recognizer's
attention to more reliable microphones. Investigation on the various ASR
performance measures has been carried out using the real recorded dataset.
Experiments results show that the proposed framework has yielded substantial
improvements in word error rate (WER).",arxiv
http://arxiv.org/abs/2004.13172v1,2020-04-27T20:58:03Z,2020-04-27T20:58:03Z,Autoencoding Neural Networks as Musical Audio Synthesizers,"A method for musical audio synthesis using autoencoding neural networks is
proposed. The autoencoder is trained to compress and reconstruct magnitude
short-time Fourier transform frames. The autoencoder produces a spectrogram by
activating its smallest hidden layer, and a phase response is calculated using
real-time phase gradient heap integration. Taking an inverse short-time Fourier
transform produces the audio signal. Our algorithm is light-weight when
compared to current state-of-the-art audio-producing machine learning
algorithms. We outline our design process, produce metrics, and detail an
open-source Python implementation of our model.",arxiv
http://arxiv.org/abs/1902.11245v1,2019-02-28T17:48:20Z,2019-02-28T17:48:20Z,"Incorporating End-to-End Speech Recognition Models for Sentiment
  Analysis","Previous work on emotion recognition demonstrated a synergistic effect of
combining several modalities such as auditory, visual, and transcribed text to
estimate the affective state of a speaker. Among these, the linguistic modality
is crucial for the evaluation of an expressed emotion. However, manually
transcribed spoken text cannot be given as input to a system practically. We
argue that using ground-truth transcriptions during training and evaluation
phases leads to a significant discrepancy in performance compared to real-world
conditions, as the spoken text has to be recognized on the fly and can contain
speech recognition mistakes. In this paper, we propose a method of integrating
an automatic speech recognition (ASR) output with a character-level recurrent
neural network for sentiment recognition. In addition, we conduct several
experiments investigating sentiment recognition for human-robot interaction in
a noise-realistic scenario which is challenging for the ASR systems. We
quantify the improvement compared to using only the acoustic modality in
sentiment recognition. We demonstrate the effectiveness of this approach on the
Multimodal Corpus of Sentiment Intensity (MOSI) by achieving 73,6% accuracy in
a binary sentiment classification task, exceeding previously reported results
that use only acoustic input. In addition, we set a new state-of-the-art
performance on the MOSI dataset (80.4% accuracy, 2% absolute improvement).",arxiv
http://arxiv.org/abs/2010.06152v1,2020-10-13T03:53:07Z,2020-10-13T03:53:07Z,"Real-Time Detection of Simulator Sickness in Virtual Reality Games Based
  on Players' Psychophysiological Data during Gameplay","Virtual Reality (VR) technology has been proliferating in the last decade,
especially in the last few years. However, Simulator Sickness (SS) still
represents a significant problem for its wider adoption. Currently, the most
common way to detect SS is using the Simulator Sickness Questionnaire (SSQ).
SSQ is a subjective measurement and is inadequate for real-time applications
such as VR games. This research aims to investigate how to use machine learning
techniques to detect SS based on in-game characters' and users' physiological
data during gameplay in VR games. To achieve this, we designed an experiment to
collect such data with three types of games. We trained a Long Short-Term
Memory neural network with the dataset eye-tracking and character movement data
to detect SS in real-time. Our results indicate that, in VR games, our model is
an accurate and efficient way to detect SS in real-time.",arxiv
http://arxiv.org/abs/2012.02417v2,2020-12-07T02:34:05Z,2020-12-04T06:02:26Z,"Autonomous Navigation with Mobile Robots using Deep Learning and the
  Robot Operating System","Autonomous navigation is a long-standing field of robotics research, which
provides an essential capability for mobile robots to execute a series of tasks
on the same environments performed by human everyday. In this chapter, we
present a set of algorithms to train and deploy deep networks for autonomous
navigation of mobile robots using the Robot Operation System (ROS). We describe
three main steps to tackle this problem: i) collecting data in simulation
environments using ROS and Gazebo; ii) designing deep network for autonomous
navigation, and iii) deploying the learned policy on mobile robots in both
simulation and real-world. Theoretically, we present deep learning
architectures for robust navigation in normal environments (e.g., man-made
houses, roads) and complex environments (e.g., collapsed cities, or natural
caves). We further show that the use of visual modalities such as RGB, Lidar,
and point cloud is essential to improve the autonomy of mobile robots. Our
project website and demonstration video can be found at
https://sites.google.com/site/autonomousnavigationros.",arxiv
http://arxiv.org/abs/1610.07862v2,2016-10-26T02:32:30Z,2016-10-24T02:15:46Z,Intelligence in Artificial Intelligence,"The elusive quest for intelligence in artificial intelligence prompts us to
consider that instituting human-level intelligence in systems may be (still) in
the realm of utopia. In about a quarter century, we have witnessed the winter
of AI (1990) being transformed and transported to the zenith of tabloid fodder
about AI (2015). The discussion at hand is about the elements that constitute
the canonical idea of intelligence. The delivery of intelligence as a
pay-per-use-service, popping out of an app or from a shrink-wrapped software
defined point solution, is in contrast to the bio-inspired view of intelligence
as an outcome, perhaps formed from a tapestry of events, cross-pollinated by
instances, each with its own microcosm of experiences and learning, which may
not be discrete all-or-none functions but continuous, over space and time. The
enterprise world may not require, aspire or desire such an engaged solution to
improve its services for enabling digital transformation through the deployment
of digital twins, for example. One might ask whether the ""work-flow on
steroids"" version of decision support may suffice for intelligence? Are we
harking back to the era of rule based expert systems? The image conjured by the
publicity machines offers deep solutions with human-level AI and preposterous
claims about capturing the ""brain in a box"" by 2020. Even emulating insects may
be difficult in terms of real progress. Perhaps we can try to focus on worms
(Caenorhabditis elegans) which may be better suited for what business needs to
quench its thirst for so-called intelligence in AI.",arxiv
http://arxiv.org/abs/1811.01106v1,2018-11-02T22:02:40Z,2018-11-02T22:02:40Z,"Machine learning architectures to predict motion sickness using a
  Virtual Reality rollercoaster simulation tool","Virtual Reality (VR) can cause an unprecedented immersion and feeling of
presence yet a lot of users experience motion sickness when moving through a
virtual environment. Rollercoaster rides are popular in Virtual Reality but
have to be well designed to limit the amount of nausea the user may feel. This
paper describes a novel framework to get automated ratings on motion sickness
using Neural Networks. An application that lets users create rollercoasters
directly in VR, share them with other users and ride and rate them is used to
gather real-time data related to the in-game behaviour of the player, the track
itself and users' ratings based on a Simulator Sickness Questionnaire (SSQ)
integrated into the application. Machine learning architectures based on deep
neural networks are trained using this data aiming to predict motion sickness
levels. While this paper focuses on rollercoasters this framework could help to
rate any VR application on motion sickness and intensity that involves camera
movement. A new well defined dataset is provided in this paper and the
performance of the proposed architectures are evaluated in a comparative study.",arxiv
http://arxiv.org/abs/1905.12612v2,2019-10-15T08:10:25Z,2019-05-29T17:50:19Z,Learning Navigation Subroutines from Egocentric Videos,"Planning at a higher level of abstraction instead of low level torques
improves the sample efficiency in reinforcement learning, and computational
efficiency in classical planning. We propose a method to learn such
hierarchical abstractions, or subroutines from egocentric video data of experts
performing tasks. We learn a self-supervised inverse model on small amounts of
random interaction data to pseudo-label the expert egocentric videos with agent
actions. Visuomotor subroutines are acquired from these pseudo-labeled videos
by learning a latent intent-conditioned policy that predicts the inferred
pseudo-actions from the corresponding image observations. We demonstrate our
proposed approach in context of navigation, and show that we can successfully
learn consistent and diverse visuomotor subroutines from passive egocentric
videos. We demonstrate the utility of our acquired visuomotor subroutines by
using them as is for exploration, and as sub-policies in a hierarchical RL
framework for reaching point goals and semantic goals. We also demonstrate
behavior of our subroutines in the real world, by deploying them on a real
robotic platform. Project website:
https://ashishkumar1993.github.io/subroutines/.",arxiv
http://arxiv.org/abs/1802.02138v3,2018-03-21T16:21:24Z,2018-02-05T19:32:35Z,"Musical Chair: Efficient Real-Time Recognition Using Collaborative IoT
  Devices","The prevalence of Internet of things (IoT) devices and abundance of sensor
data has created an increase in real-time data processing such as recognition
of speech, image, and video. While currently such processes are offloaded to
the computationally powerful cloud system, a localized and distributed approach
is desirable because (i) it preserves the privacy of users and (ii) it omits
the dependency on cloud services. However, IoT networks are usually composed of
resource-constrained devices, and a single device is not powerful enough to
process real-time data. To overcome this challenge, we examine data and model
parallelism for such devices in the context of deep neural networks. We propose
Musical Chair to enable efficient, localized, and dynamic real-time recognition
by harvesting the aggregated computational power from the resource-constrained
devices in the same IoT network as input sensors. Musical chair adapts to the
availability of computing devices at runtime and adjusts to the inherit
dynamics of IoT networks. To demonstrate Musical Chair, on a network of
Raspberry PIs (up to 12) each connected to a camera, we implement a
state-of-the-art action recognition model for videos and two recognition models
for images. Compared to the Tegra TX2, an embedded low-power platform with a
six-core CPU and a GPU, our distributed action recognition system achieves not
only similar energy consumption but also twice the performance of the TX2.
Furthermore, in image recognition, Musical Chair achieves similar performance
and saves dynamic energy.",arxiv
http://arxiv.org/abs/1907.10148v3,2019-09-19T18:52:58Z,2019-07-23T21:41:54Z,"Conf-Net: Toward High-Confidence Dense 3D Point-Cloud with Error-Map
  Prediction","This work proposes a method for depth completion of sparse LiDAR data using a
convolutional neural network which can be used to generate semi-dense depth
maps and ""almost"" full 3D point-clouds with significantly lower root mean
squared error (RMSE) over state-of-the-art methods. We add an ""Error
Prediction"" unit to our network and present a novel and simple end-to-end
method that learns to predict an error-map of depth regression task. An
""almost"" dense high-confidence/low-variance point-cloud is more valuable for
safety-critical applications specifically real-world autonomous driving than a
full point-cloud with high error rate and high error variance. Using our
predicted error-map, we demonstrate that by up-filling a LiDAR point cloud from
18,000 points to 285,000 points, versus 300,000 points for full depth, we can
reduce the RMSE error from 1004 to 399. This error is approximately 60% less
than the state-of-the-art and 50% less than the state-of-the-art with RGB
guidance (we did not use RGB guidance in our algorithm). In addition to
analyzing our results on Kitti depth completion dataset, we also demonstrate
the ability of our proposed method to extend to new tasks by deploying our
""Error Prediction"" unit to improve upon the state-of-the-art for monocular
depth estimation. Codes and demo videos are available at
http://github.com/hekmak/Conf-net.",arxiv
http://arxiv.org/abs/2005.07115v6,2021-06-29T15:20:19Z,2020-05-14T16:33:13Z,CoSimGNN: Towards Large-scale Graph Similarity Computation,"The ability to compute similarity scores between graphs based on metrics such
as Graph Edit Distance (GED) is important in many real-world applications, such
as 3D action recognition and biological molecular identification. Computing
exact GED values is typically an NP-hard problem and traditional algorithms
usually achieve an unsatisfactory trade-off between accuracy and efficiency.
Recently, Graph Neural Networks (GNNs) provide a data-driven solution for this
task, which is more efficient while maintaining prediction accuracy in small
graph (around 10 nodes per graph) similarity computation. Existing GNN-based
methods, which either respectively embed two graphs (lack of low-level
cross-graph interactions) or deploy cross-graph interactions for whole graph
pairs (redundant and time-consuming), are still not able to achieve competitive
results when the number of nodes in graphs increases. In this paper, we focus
on similarity computation for large-scale graphs and propose the
""embedding-coarsening-matching"" framework, which first embeds and coarsens
large graphs to coarsened graphs with denser local topology and then deploys
fine-grained interactions on the coarsened graphs for the final similarity
scores.",arxiv
http://arxiv.org/abs/2103.08364v2,2021-06-30T21:56:49Z,2021-03-15T13:09:00Z,"Towards the evaluation of automatic simultaneous speech translation from
  a communicative perspective","In recent years, automatic speech-to-speech and speech-to-text translation
has gained momentum thanks to advances in artificial intelligence, especially
in the domains of speech recognition and machine translation. The quality of
such applications is commonly tested with automatic metrics, such as BLEU,
primarily with the goal of assessing improvements of releases or in the context
of evaluation campaigns. However, little is known about how the output of such
systems is perceived by end users or how they compare to human performances in
similar communicative tasks.
  In this paper, we present the results of an experiment aimed at evaluating
the quality of a real-time speech translation engine by comparing it to the
performance of professional simultaneous interpreters. To do so, we adopt a
framework developed for the assessment of human interpreters and use it to
perform a manual evaluation on both human and machine performances. In our
sample, we found better performance for the human interpreters in terms of
intelligibility, while the machine performs slightly better in terms of
informativeness. The limitations of the study and the possible enhancements of
the chosen framework are discussed. Despite its intrinsic limitations, the use
of this framework represents a first step towards a user-centric and
communication-oriented methodology for evaluating real-time automatic speech
translation.",arxiv
http://arxiv.org/abs/1809.04966v1,2018-09-13T13:53:01Z,2018-09-13T13:53:01Z,"Real-Time Lightweight Chaotic Encryption for 5G IoT Enabled Lip-Reading
  Driven Secure Hearing-Aid","Existing audio-only hearing-aids are known to perform poorly in noisy
situations where overwhelming noise is present. Next-generation audio-visual
(lip-reading driven) hearing-aids stand as a major enabler to realise more
intelligible audio. However, high data rate, low latency, low computational
complexity, and privacy are some of the major bottlenecks to the successful
deployment of such advanced hearing aids. To address these challenges, we
envision an integration of 5G Cloud-Radio Access Network, Internet of Things
(IoT), and strong privacy algorithms to fully benefit from the possibilities
these technologies have to offer. The envisioned 5G IoT enabled secure
audio-visual (AV) hearing-aid transmits the encrypted compressed AV information
and receives encrypted enhanced reconstructed speech in real-time which fully
addresses cybersecurity attacks such as location privacy and eavesdropping. For
security implementation, a real-time lightweight AV encryption is utilized. For
speech enhancement, the received AV information in the cloud is used to filter
noisy audio using both deep learning and analytical acoustic modelling
(filtering based approach). To offload the computational complexity and
real-time optimization issues, the framework runs deep learning and big data
optimization processes in the background on the cloud. Specifically, in this
work, three key contributions are reported: (1) 5G IoT enabled secure
audio-visual hearing-aid framework that aims to achieve a round-trip latency up
to 5ms with 100 Mbps datarate (2) Real-time lightweight audio-visual encryption
(3) Lip-reading driven deep learning approach for speech enhancement in the
cloud. The critical analysis in terms of both speech enhancement and AV
encryption demonstrate the potential of the envisioned technology in acquiring
high-quality speech reconstruction and secure mobile AV hearing aid
communication.",arxiv
http://arxiv.org/abs/1904.07537v1,2019-04-16T08:49:06Z,2019-04-16T08:49:06Z,"Complexer-YOLO: Real-Time 3D Object Detection and Tracking on Semantic
  Point Clouds","Accurate detection of 3D objects is a fundamental problem in computer vision
and has an enormous impact on autonomous cars, augmented/virtual reality and
many applications in robotics. In this work we present a novel fusion of neural
network based state-of-the-art 3D detector and visual semantic segmentation in
the context of autonomous driving. Additionally, we introduce
Scale-Rotation-Translation score (SRTs), a fast and highly parameterizable
evaluation metric for comparison of object detections, which speeds up our
inference time up to 20\% and halves training time. On top, we apply
state-of-the-art online multi target feature tracking on the object
measurements to further increase accuracy and robustness utilizing temporal
information. Our experiments on KITTI show that we achieve same results as
state-of-the-art in all related categories, while maintaining the performance
and accuracy trade-off and still run in real-time. Furthermore, our model is
the first one that fuses visual semantic with 3D object detection.",arxiv
http://arxiv.org/abs/1812.07221v1,2018-12-18T08:11:00Z,2018-12-18T08:11:00Z,"Continuous Trajectory Planning Based on Learning Optimization in High
  Dimensional Input Space for Serial Manipulators","To continuously generate trajectories for serial manipulators with high
dimensional degrees of freedom (DOF) in the dynamic environment, a real-time
optimal trajectory generation method based on machine learning aiming at high
dimensional inputs is presented in this paper. First, a learning optimization
(LO) framework is established, and implementations with different sub-methods
are discussed. Additionally, multiple criteria are defined to evaluate the
performance of LO models. Furthermore, aiming at high dimensional inputs, a
database generation method based on input space dimension-reducing mapping is
proposed. At last, this method is validated on motion planning for haptic
feedback manipulators (HFM) in virtual reality systems. Results show that the
input space dimension-reducing method can significantly elevate the efficiency
and quality of database generation and consequently improve the performance of
the LO. Moreover, using this LO method, real-time trajectory generation with
high dimensional inputs can be achieved, which lays a foundation for continuous
trajectory planning for high-DOF-robots in complex environments.",arxiv
http://arxiv.org/abs/1812.01202v2,2019-09-13T02:23:54Z,2018-12-04T03:44:31Z,"Federated Echo State Learning for Minimizing Breaks in Presence in
  Wireless Virtual Reality Networks","In this paper, the problem of enhancing the virtual reality (VR) experience
for wireless users is investigated by minimizing the occurrence of breaks in
presence (BIP) that can detach the users from their virtual world. To measure
the BIP for wireless VR users, a novel model that jointly considers the VR
application type, transmission delay, VR video quality, and users' awareness of
the virtual environment is proposed. In the developed model, the base stations
(BSs) transmit VR videos to the wireless VR users using directional
transmission links so as to provide high data rates for the VR users, thus,
reducing the number of BIP for each user. Since the body movements of a VR user
may result in a blockage of its wireless link, the location and orientation of
VR users must also be considered when minimizing BIP. The BIP minimization
problem is formulated as an optimization problem which jointly considers the
predictions of users' locations, orientations, and their BS association. To
predict the orientation and locations of VR users, a distributed learning
algorithm based on the machine learning framework of deep (ESNs) is proposed.
The proposed algorithm uses concept from federated learning to enable multiple
BSs to locally train their deep ESNs using their collected data and
cooperatively build a learning model to predict the entire users' locations and
orientations. Using these predictions, the user association policy that
minimizes BIP is derived. Simulation results demonstrate that the developed
algorithm reduces the users' BIP by up to 16% and 26%, respectively, compared
to centralized ESN and deep learning algorithms.",arxiv
http://arxiv.org/abs/1801.09866v1,2018-01-30T06:58:50Z,2018-01-30T06:58:50Z,"Accelerating recurrent neural network language model based online speech
  recognition system","This paper presents methods to accelerate recurrent neural network based
language models (RNNLMs) for online speech recognition systems. Firstly, a
lossy compression of the past hidden layer outputs (history vector) with
caching is introduced in order to reduce the number of LM queries. Next, RNNLM
computations are deployed in a CPU-GPU hybrid manner, which computes each layer
of the model on a more advantageous platform. The added overhead by data
exchanges between CPU and GPU is compensated through a frame-wise batching
strategy. The performance of the proposed methods evaluated on LibriSpeech test
sets indicates that the reduction in history vector precision improves the
average recognition speed by 1.23 times with minimum degradation in accuracy.
On the other hand, the CPU-GPU hybrid parallelization enables RNNLM based
real-time recognition with a four times improvement in speed.",arxiv
http://arxiv.org/abs/1812.07106v1,2018-12-12T22:22:16Z,2018-12-12T22:22:16Z,"E-RNN: Design Optimization for Efficient Recurrent Neural Networks in
  FPGAs","Recurrent Neural Networks (RNNs) are becoming increasingly important for time
series-related applications which require efficient and real-time
implementations. The two major types are Long Short-Term Memory (LSTM) and
Gated Recurrent Unit (GRU) networks. It is a challenging task to have
real-time, efficient, and accurate hardware RNN implementations because of the
high sensitivity to imprecision accumulation and the requirement of special
activation function implementations.
  A key limitation of the prior works is the lack of a systematic design
optimization framework of RNN model and hardware implementations, especially
when the block size (or compression ratio) should be jointly optimized with RNN
type, layer size, etc. In this paper, we adopt the block-circulant matrix-based
framework, and present the Efficient RNN (E-RNN) framework for FPGA
implementations of the Automatic Speech Recognition (ASR) application. The
overall goal is to improve performance/energy efficiency under accuracy
requirement. We use the alternating direction method of multipliers (ADMM)
technique for more accurate block-circulant training, and present two design
explorations providing guidance on block size and reducing RNN training trials.
Based on the two observations, we decompose E-RNN in two phases: Phase I on
determining RNN model to reduce computation and storage subject to accuracy
requirement, and Phase II on hardware implementations given RNN model,
including processing element design/optimization, quantization, activation
implementation, etc. Experimental results on actual FPGA deployments show that
E-RNN achieves a maximum energy efficiency improvement of 37.4$\times$ compared
with ESE, and more than 2$\times$ compared with C-LSTM, under the same
accuracy.",arxiv
http://arxiv.org/abs/2005.01802v1,2020-05-04T19:20:09Z,2020-05-04T19:20:09Z,Learning-based Tracking of Fast Moving Objects,"Tracking fast moving objects, which appear as blurred streaks in video
sequences, is a difficult task for standard trackers as the object position
does not overlap in consecutive video frames and texture information of the
objects is blurred. Up-to-date approaches tuned for this task are based on
background subtraction with static background and slow deblurring algorithms.
In this paper, we present a tracking-by-segmentation approach implemented using
state-of-the-art deep learning methods that performs near-realtime tracking on
real-world video sequences. We implemented a physically plausible FMO sequence
generator to be a robust foundation for our training pipeline and demonstrate
the ease of fast generator and network adaptation for different FMO scenarios
in terms of foreground variations.",arxiv
http://arxiv.org/abs/2105.02957v1,2021-04-27T10:08:40Z,2021-04-27T10:08:40Z,"VID-WIN: Fast Video Event Matching with Query-Aware Windowing at the
  Edge for the Internet of Multimedia Things","Efficient video processing is a critical component in many IoMT applications
to detect events of interest. Presently, many window optimization techniques
have been proposed in event processing with an underlying assumption that the
incoming stream has a structured data model. Videos are highly complex due to
the lack of any underlying structured data model. Video stream sources such as
CCTV cameras and smartphones are resource-constrained edge nodes. At the same
time, video content extraction is expensive and requires computationally
intensive Deep Neural Network (DNN) models that are primarily deployed at
high-end (or cloud) nodes. This paper presents VID-WIN, an adaptive 2-stage
allied windowing approach to accelerate video event analytics in an edge-cloud
paradigm. VID-WIN runs parallelly across edge and cloud nodes and performs the
query and resource-aware optimization for state-based complex event matching.
VID-WIN exploits the video content and DNN input knobs to accelerate the video
inference process across nodes. The paper proposes a novel content-driven
micro-batch resizing, queryaware caching and micro-batch based utility
filtering strategy of video frames under resource-constrained edge nodes to
improve the overall system throughput, latency, and network usage. Extensive
evaluations are performed over five real-world datasets. The experimental
results show that VID-WIN video event matching achieves ~2.3X higher throughput
with minimal latency and ~99% bandwidth reduction compared to other baselines
while maintaining query-level accuracy and resource bounds.",arxiv
http://arxiv.org/abs/1706.09453v2,2017-07-04T17:03:20Z,2017-06-28T19:41:25Z,"Toward Computation and Memory Efficient Neural Network Acoustic Models
  with Binary Weights and Activations","Neural network acoustic models have significantly advanced state of the art
speech recognition over the past few years. However, they are usually
computationally expensive due to the large number of matrix-vector
multiplications and nonlinearity operations. Neural network models also require
significant amounts of memory for inference because of the large model size.
For these two reasons, it is challenging to deploy neural network based speech
recognizers on resource-constrained platforms such as embedded devices. This
paper investigates the use of binary weights and activations for computation
and memory efficient neural network acoustic models. Compared to real-valued
weight matrices, binary weights require much fewer bits for storage, thereby
cutting down the memory footprint. Furthermore, with binary weights or
activations, the matrix-vector multiplications are turned into addition and
subtraction operations, which are computationally much faster and more energy
efficient for hardware platforms. In this paper, we study the applications of
binary weights and activations for neural network acoustic modeling, reporting
encouraging results on the WSJ and AMI corpora.",arxiv
http://arxiv.org/abs/2006.09675v1,2020-06-17T06:30:43Z,2020-06-17T06:30:43Z,"A Real-time Action Representation with Temporal Encoding and Deep
  Compression","Deep neural networks have achieved remarkable success for video-based action
recognition. However, most of existing approaches cannot be deployed in
practice due to the high computational cost. To address this challenge, we
propose a new real-time convolutional architecture, called Temporal
Convolutional 3D Network (T-C3D), for action representation. T-C3D learns video
action representations in a hierarchical multi-granularity manner while
obtaining a high process speed. Specifically, we propose a residual 3D
Convolutional Neural Network (CNN) to capture complementary information on the
appearance of a single frame and the motion between consecutive frames. Based
on this CNN, we develop a new temporal encoding method to explore the temporal
dynamics of the whole video. Furthermore, we integrate deep compression
techniques with T-C3D to further accelerate the deployment of models via
reducing the size of the model. By these means, heavy calculations can be
avoided when doing the inference, which enables the method to deal with videos
beyond real-time speed while keeping promising performance. Our method achieves
clear improvements on UCF101 action recognition benchmark against
state-of-the-art real-time methods by 5.4% in terms of accuracy and 2 times
faster in terms of inference speed with a less than 5MB storage model. We
validate our approach by studying its action representation performance on four
different benchmarks over three different tasks. Extensive experiments
demonstrate comparable recognition performance to the state-of-the-art methods.
The source code and the pre-trained models are publicly available at
https://github.com/tc3d.",arxiv
http://arxiv.org/abs/2012.09812v2,2021-03-26T11:12:28Z,2020-12-17T18:22:32Z,ViNG: Learning Open-World Navigation with Visual Goals,"We propose a learning-based navigation system for reaching visually indicated
goals and demonstrate this system on a real mobile robot platform. Learning
provides an appealing alternative to conventional methods for robotic
navigation: instead of reasoning about environments in terms of geometry and
maps, learning can enable a robot to learn about navigational affordances,
understand what types of obstacles are traversable (e.g., tall grass) or not
(e.g., walls), and generalize over patterns in the environment. However, unlike
conventional planning algorithms, it is harder to change the goal for a learned
policy during deployment. We propose a method for learning to navigate towards
a goal image of the desired destination. By combining a learned policy with a
topological graph constructed out of previously observed data, our system can
determine how to reach this visually indicated goal even in the presence of
variable appearance and lighting. Three key insights, waypoint proposal, graph
pruning and negative mining, enable our method to learn to navigate in
real-world environments using only offline data, a setting where prior methods
struggle. We instantiate our method on a real outdoor ground robot and show
that our system, which we call ViNG, outperforms previously-proposed methods
for goal-conditioned reinforcement learning, including other methods that
incorporate reinforcement learning and search. We also study how \sysName
generalizes to unseen environments and evaluate its ability to adapt to such an
environment with growing experience. Finally, we demonstrate ViNG on a number
of real-world applications, such as last-mile delivery and warehouse
inspection. We encourage the reader to visit the project website for videos of
our experiments and demonstrations sites.google.com/view/ving-robot.",arxiv
http://arxiv.org/abs/2002.05123v4,2021-06-04T22:11:54Z,2020-02-12T17:58:12Z,"Over-the-Air Adversarial Flickering Attacks against Video Recognition
  Networks","Deep neural networks for video classification, just like image classification
networks, may be subjected to adversarial manipulation. The main difference
between image classifiers and video classifiers is that the latter usually use
temporal information contained within the video. In this work we present a
manipulation scheme for fooling video classifiers by introducing a flickering
temporal perturbation that in some cases may be unnoticeable by human observers
and is implementable in the real world. After demonstrating the manipulation of
action classification of single videos, we generalize the procedure to make
universal adversarial perturbation, achieving high fooling ratio. In addition,
we generalize the universal perturbation and produce a temporal-invariant
perturbation, which can be applied to the video without synchronizing the
perturbation to the input. The attack was implemented on several target models
and the transferability of the attack was demonstrated. These properties allow
us to bridge the gap between simulated environment and real-world application,
as will be demonstrated in this paper for the first time for an over-the-air
flickering attack.",arxiv
http://arxiv.org/abs/2004.11838v1,2020-04-14T19:36:11Z,2020-04-14T19:36:11Z,"Analysis of Social Media Data using Multimodal Deep Learning for
  Disaster Response","Multimedia content in social media platforms provides significant information
during disaster events. The types of information shared include reports of
injured or deceased people, infrastructure damage, and missing or found people,
among others. Although many studies have shown the usefulness of both text and
image content for disaster response purposes, the research has been mostly
focused on analyzing only the text modality in the past. In this paper, we
propose to use both text and image modalities of social media data to learn a
joint representation using state-of-the-art deep learning techniques.
Specifically, we utilize convolutional neural networks to define a multimodal
deep learning architecture with a modality-agnostic shared representation.
Extensive experiments on real-world disaster datasets show that the proposed
multimodal architecture yields better performance than models trained using a
single modality (e.g., either text or image).",arxiv
http://arxiv.org/abs/1708.01167v1,2017-08-03T14:50:02Z,2017-08-03T14:50:02Z,"Applying advanced machine learning models to classify
  electro-physiological activity of human brain for use in biometric
  identification","In this article we present the results of our research related to the study
of correlations between specific visual stimulation and the elicited brain's
electro-physiological response collected by EEG sensors from a group of
participants. We will look at how the various characteristics of visual
stimulation affect the measured electro-physiological response of the brain and
describe the optimal parameters found that elicit a steady-state visually
evoked potential (SSVEP) in certain parts of the cerebral cortex where it can
be reliably perceived by the electrode of the EEG device. After that, we
continue with a description of the advanced machine learning pipeline model
that can perform confident classification of the collected EEG data in order to
(a) reliably distinguish signal from noise (about 85% validation score) and (b)
reliably distinguish between EEG records collected from different human
participants (about 80% validation score). Finally, we demonstrate that the
proposed method works reliably even with an inexpensive (less than $100)
consumer-grade EEG sensing device and with participants who do not have
previous experience with EEG technology (EEG illiterate). All this in
combination opens up broad prospects for the development of new types of
consumer devices, [e.g.] based on virtual reality helmets or augmented reality
glasses where EEG sensor can be easily integrated. The proposed method can be
used to improve an online user experience by providing [e.g.] password-less
user identification for VR / AR applications. It can also find a more advanced
application in intensive care units where collected EEG data can be used to
classify the level of conscious awareness of patients during anesthesia or to
automatically detect hardware failures by classifying the input signal as
noise.",arxiv
http://arxiv.org/abs/1612.06299v1,2016-12-19T18:12:20Z,2016-12-19T18:12:20Z,Simple Black-Box Adversarial Perturbations for Deep Networks,"Deep neural networks are powerful and popular learning models that achieve
state-of-the-art pattern recognition performance on many computer vision,
speech, and language processing tasks. However, these networks have also been
shown susceptible to carefully crafted adversarial perturbations which force
misclassification of the inputs. Adversarial examples enable adversaries to
subvert the expected system behavior leading to undesired consequences and
could pose a security risk when these systems are deployed in the real world.
  In this work, we focus on deep convolutional neural networks and demonstrate
that adversaries can easily craft adversarial examples even without any
internal knowledge of the target network. Our attacks treat the network as an
oracle (black-box) and only assume that the output of the network can be
observed on the probed inputs. Our first attack is based on a simple idea of
adding perturbation to a randomly selected single pixel or a small set of them.
We then improve the effectiveness of this attack by carefully constructing a
small set of pixels to perturb by using the idea of greedy local-search. Our
proposed attacks also naturally extend to a stronger notion of
misclassification. Our extensive experimental results illustrate that even
these elementary attacks can reveal a deep neural network's vulnerabilities.
The simplicity and effectiveness of our proposed schemes mean that they could
serve as a litmus test for designing robust networks.",arxiv
http://arxiv.org/abs/1907.11546v3,2020-07-13T08:23:37Z,2019-07-26T12:55:55Z,Compressing deep quaternion neural networks with targeted regularization,"In recent years, hyper-complex deep networks (such as complex-valued and
quaternion-valued neural networks) have received a renewed interest in the
literature. They find applications in multiple fields, ranging from image
reconstruction to 3D audio processing. Similar to their real-valued
counterparts, quaternion neural networks (QVNNs) require custom regularization
strategies to avoid overfitting. In addition, for many real-world applications
and embedded implementations, there is the need of designing sufficiently
compact networks, with few weights and neurons. However, the problem of
regularizing and/or sparsifying QVNNs has not been properly addressed in the
literature as of now. In this paper, we show how to address both problems by
designing targeted regularization strategies, which are able to minimize the
number of connections and neurons of the network during training. To this end,
we investigate two extensions of l1 and structured regularization to the
quaternion domain. In our experimental evaluation, we show that these tailored
strategies significantly outperform classical (real-valued) regularization
approaches, resulting in small networks especially suitable for low-power and
real-time applications.",arxiv
http://arxiv.org/abs/1703.03078v3,2017-06-18T23:06:17Z,2017-03-08T23:58:56Z,"Combining Model-Based and Model-Free Updates for Trajectory-Centric
  Reinforcement Learning","Reinforcement learning (RL) algorithms for real-world robotic applications
need a data-efficient learning process and the ability to handle complex,
unknown dynamical systems. These requirements are handled well by model-based
and model-free RL approaches, respectively. In this work, we aim to combine the
advantages of these two types of methods in a principled manner. By focusing on
time-varying linear-Gaussian policies, we enable a model-based algorithm based
on the linear quadratic regulator (LQR) that can be integrated into the
model-free framework of path integral policy improvement (PI2). We can further
combine our method with guided policy search (GPS) to train arbitrary
parameterized policies such as deep neural networks. Our simulation and
real-world experiments demonstrate that this method can solve challenging
manipulation tasks with comparable or better performance than model-free
methods while maintaining the sample efficiency of model-based methods. A video
presenting our results is available at
https://sites.google.com/site/icml17pilqr",arxiv
http://arxiv.org/abs/2003.07583v1,2020-03-17T08:47:34Z,2020-03-17T08:47:34Z,"Reinforcement Learning Driven Adaptive VR Streaming with Optical Flow
  Based QoE","With the merit of containing full panoramic content in one camera, Virtual
Reality (VR) and 360-degree videos have attracted more and more attention in
the field of industrial cloud manufacturing and training. Industrial Internet
of Things (IoT), where many VR terminals needed to be online at the same time,
can hardly guarantee VR's bandwidth requirement. However, by making use of
users' quality of experience (QoE) awareness factors, including the relative
moving speed and depth difference between the viewpoint and other content,
bandwidth consumption can be reduced. In this paper, we propose OFB-VR (Optical
Flow Based VR), an interactive method of VR streaming that can make use of VR
users' QoE awareness to ease the bandwidth pressure. The Just-Noticeable
Difference through Optical Flow Estimation (JND-OFE) is explored to quantify
users' awareness of quality distortion in 360-degree videos. Accordingly, a
novel 360-degree videos QoE metric based on PSNR and JND-OFE (PSNR-OF) is
proposed. With the help of PSNR-OF, OFB-VR proposes a versatile-size tiling
scheme to lessen the tiling overhead. A Reinforcement Learning(RL) method is
implemented to make use of historical data to perform Adaptive BitRate(ABR).
For evaluation, we take two prior VR streaming schemes, Pano and Plato, as
baselines. Vast evaluations show that our system can increase the mean PSNR-OF
score by 9.5-15.8% while maintaining the same rebuffer ratio compared with Pano
and Plato in a fluctuate LTE bandwidth dataset. Evaluation results show that
OFB-VR is a promising prototype for actual interactive industrial VR. A
prototype of OFB-VR can be found in https://github.com/buptexplorers/OFB-VR.",arxiv
http://arxiv.org/abs/2109.10813v2,2021-09-23T17:34:23Z,2021-09-22T16:03:29Z,A Workflow for Offline Model-Free Robotic Reinforcement Learning,"Offline reinforcement learning (RL) enables learning control policies by
utilizing only prior experience, without any online interaction. This can allow
robots to acquire generalizable skills from large and diverse datasets, without
any costly or unsafe online data collection. Despite recent algorithmic
advances in offline RL, applying these methods to real-world problems has
proven challenging. Although offline RL methods can learn from prior data,
there is no clear and well-understood process for making various design
choices, from model architecture to algorithm hyperparameters, without actually
evaluating the learned policies online. In this paper, our aim is to develop a
practical workflow for using offline RL analogous to the relatively
well-understood workflows for supervised learning problems. To this end, we
devise a set of metrics and conditions that can be tracked over the course of
offline training, and can inform the practitioner about how the algorithm and
model architecture should be adjusted to improve final performance. Our
workflow is derived from a conceptual understanding of the behavior of
conservative offline RL algorithms and cross-validation in supervised learning.
We demonstrate the efficacy of this workflow in producing effective policies
without any online tuning, both in several simulated robotic learning scenarios
and for three tasks on two distinct real robots, focusing on learning
manipulation skills with raw image observations with sparse binary rewards.
Explanatory video and additional results can be found at
sites.google.com/view/offline-rl-workflow",arxiv
http://arxiv.org/abs/2010.09810v1,2020-10-19T19:40:29Z,2020-10-19T19:40:29Z,"Connections between Relational Event Model and Inverse Reinforcement
  Learning for Characterizing Group Interaction Sequences","In this paper we explore previously unidentified connections between
relational event model (REM) from the field of network science and inverse
reinforcement learning (IRL) from the field of machine learning with respect to
their ability to characterize sequences of directed social interaction events
in group settings. REM is a conventional approach to tackle such a problem
whereas the application of IRL is a largely unbeaten path. We begin by
examining the mathematical components of both REM and IRL and find
straightforward analogies between the two methods as well as unique
characteristics of the IRL approach. We demonstrate the special utility of IRL
in characterizing group social interactions with an empirical experiment, in
which we use IRL to infer individual behavioral preferences based on a sequence
of directed communication events from a group of virtual-reality game players
interacting and cooperating to accomplish a shared goal. Our comparison and
experiment introduce fresh perspectives for social behavior analytics and help
inspire new research opportunities at the nexus of social network analysis and
machine learning.",arxiv
http://arxiv.org/abs/2007.09521v1,2020-07-18T21:52:21Z,2020-07-18T21:52:21Z,Tomography Based Learning for Load Distribution through Opaque Networks,"Applications such as virtual reality and online gaming require low delays for
acceptable user experience. A key task for over-the-top (OTT) service providers
who provide these applications is sending traffic through the networks to
minimize delays. OTT traffic is typically generated from multiple data centers
which are multi-homed to several network ingresses. However, information about
the path characteristics of the underlying network from the ingresses to
destinations is not explicitly available to OTT services. These can only be
inferred from external probing. In this paper, we combine network tomography
with machine learning to minimize delays. We consider this problem in a general
setting where traffic sources can choose a set of ingresses through which their
traffic enter a black box network. The problem in this setting can be viewed as
a reinforcement learning problem with constraints on a continuous action space,
which to the best of our knowledge have not been investigated by the machine
learning community. Key technical challenges to solving this problem include
the high dimensionality of the problem and handling constraints that are
intrinsic to networks. Evaluation results show that our methods achieve up to
60% delay reductions in comparison to standard heuristics. Moreover, the
methods we develop can be used in a centralized manner or in a distributed
manner by multiple independent agents.",arxiv
http://arxiv.org/abs/2103.12321v1,2021-03-23T05:33:59Z,2021-03-23T05:33:59Z,Learning 6DoF Grasping Using Reward-Consistent Demonstration,"As the number of the robot's degrees of freedom increases, the implementation
of robot motion becomes more complex and difficult. In this study, we focus on
learning 6DOF-grasping motion and consider dividing the grasping motion into
multiple tasks. We propose to combine imitation and reinforcement learning in
order to facilitate a more efficient learning of the desired motion. In order
to collect demonstration data as teacher data for the imitation learning, we
created a virtual reality (VR) interface that allows humans to operate the
robot intuitively. Moreover, by dividing the motion into simpler tasks, we
simplify the design of reward functions for reinforcement learning and show in
our experiments a reduction in the steps required to learn the grasping motion.",arxiv
http://arxiv.org/abs/2104.11462v2,2021-06-10T07:30:30Z,2021-04-23T08:27:09Z,"LeBenchmark: A Reproducible Framework for Assessing Self-Supervised
  Representation Learning from Speech","Self-Supervised Learning (SSL) using huge unlabeled data has been
successfully explored for image and natural language processing. Recent works
also investigated SSL from speech. They were notably successful to improve
performance on downstream tasks such as automatic speech recognition (ASR).
While these works suggest it is possible to reduce dependence on labeled data
for building efficient speech systems, their evaluation was mostly made on ASR
and using multiple and heterogeneous experimental settings (most of them for
English). This questions the objective comparison of SSL approaches and the
evaluation of their impact on building speech systems. In this paper, we
propose LeBenchmark: a reproducible framework for assessing SSL from speech. It
not only includes ASR (high and low resource) tasks but also spoken language
understanding, speech translation and emotion recognition. We also focus on
speech technologies in a language different than English: French. SSL models of
different sizes are trained from carefully sourced and documented datasets.
Experiments show that SSL is beneficial for most but not all tasks which
confirms the need for exhaustive and reliable benchmarks to evaluate its real
impact. LeBenchmark is shared with the scientific community for reproducible
research in SSL from speech.",arxiv
http://arxiv.org/abs/2012.07938v1,2020-12-14T20:55:48Z,2020-12-14T20:55:48Z,NVIDIA SimNet^{TM}: an AI-accelerated multi-physics simulation framework,"We present SimNet, an AI-driven multi-physics simulation framework, to
accelerate simulations across a wide range of disciplines in science and
engineering. Compared to traditional numerical solvers, SimNet addresses a wide
range of use cases - coupled forward simulations without any training data,
inverse and data assimilation problems. SimNet offers fast turnaround time by
enabling parameterized system representation that solves for multiple
configurations simultaneously, as opposed to the traditional solvers that solve
for one configuration at a time. SimNet is integrated with parameterized
constructive solid geometry as well as STL modules to generate point clouds.
Furthermore, it is customizable with APIs that enable user extensions to
geometry, physics and network architecture. It has advanced network
architectures that are optimized for high-performance GPU computing, and offers
scalable performance for multi-GPU and multi-Node implementation with
accelerated linear algebra as well as FP32, FP64 and TF32 computations. In this
paper we review the neural network solver methodology, the SimNet architecture,
and the various features that are needed for effective solution of the PDEs. We
present real-world use cases that range from challenging forward multi-physics
simulations with turbulence and complex 3D geometries, to industrial design
optimization and inverse problems that are not addressed efficiently by the
traditional solvers. Extensive comparisons of SimNet results with open source
and commercial solvers show good correlation.",arxiv
http://arxiv.org/abs/1904.05734v1,2019-03-18T20:10:13Z,2019-03-18T20:10:13Z,"Practical Hidden Voice Attacks against Speech and Speaker Recognition
  Systems","Voice Processing Systems (VPSes), now widely deployed, have been made
significantly more accurate through the application of recent advances in
machine learning. However, adversarial machine learning has similarly advanced
and has been used to demonstrate that VPSes are vulnerable to the injection of
hidden commands - audio obscured by noise that is correctly recognized by a VPS
but not by human beings. Such attacks, though, are often highly dependent on
white-box knowledge of a specific machine learning model and limited to
specific microphones and speakers, making their use across different acoustic
hardware platforms (and thus their practicality) limited. In this paper, we
break these dependencies and make hidden command attacks more practical through
model-agnostic (blackbox) attacks, which exploit knowledge of the signal
processing algorithms commonly used by VPSes to generate the data fed into
machine learning systems. Specifically, we exploit the fact that multiple
source audio samples have similar feature vectors when transformed by acoustic
feature extraction algorithms (e.g., FFTs). We develop four classes of
perturbations that create unintelligible audio and test them against 12 machine
learning models, including 7 proprietary models (e.g., Google Speech API, Bing
Speech API, IBM Speech API, Azure Speaker API, etc), and demonstrate successful
attacks against all targets. Moreover, we successfully use our maliciously
generated audio samples in multiple hardware configurations, demonstrating
effectiveness across both models and real systems. In so doing, we demonstrate
that domain-specific knowledge of audio signal processing represents a
practical means of generating successful hidden voice command attacks.",arxiv
http://arxiv.org/abs/1707.04610v2,2018-04-15T17:48:20Z,2017-07-14T19:05:50Z,Cloud-based or On-device: An Empirical Study of Mobile Deep Inference,"Modern mobile applications are benefiting significantly from the advancement
in deep learning, e.g., implementing real-time image recognition and
conversational system. Given a trained deep learning model, applications
usually need to perform a series of matrix operations based on the input data,
in order to infer possible output values. Because of computational complexity
and size constraints, these trained models are often hosted in the cloud. To
utilize these cloud-based models, mobile apps will have to send input data over
the network. While cloud-based deep learning can provide reasonable response
time for mobile apps, it restricts the use case scenarios, e.g. mobile apps
need to have network access. With mobile specific deep learning optimizations,
it is now possible to employ on-device inference. However, because mobile
hardware, such as GPU and memory size, can be very limited when compared to its
desktop counterpart, it is important to understand the feasibility of this new
on-device deep learning inference architecture. In this paper, we empirically
evaluate the inference performance of three Convolutional Neural Networks
(CNNs) using a benchmark Android application we developed. Our measurement and
analysis suggest that on-device inference can cost up to two orders of
magnitude greater response time and energy when compared to cloud-based
inference, and that loading model and computing probability are two performance
bottlenecks for on-device deep inferences.",arxiv
http://arxiv.org/abs/1910.12750v1,2019-10-28T15:21:48Z,2019-10-28T15:21:48Z,"Deep-Learning-Based Image Segmentation Integrated with Optical
  Microscopy for Automatically Searching for Two-Dimensional Materials","Deep-learning algorithms enable precise image recognition based on
high-dimensional hierarchical image features. Here, we report the development
and implementation of a deep-learning-based image segmentation algorithm in an
autonomous robotic system to search for two-dimensional (2D) materials. We
trained the neural network based on Mask-RCNN on annotated optical microscope
images of 2D materials (graphene, hBN, MoS2, and WTe2). The inference algorithm
is run on a 1024 x 1024 px2 optical microscope images for 200 ms, enabling the
real-time detection of 2D materials. The detection process is robust against
changes in the microscopy conditions, such as illumination and color balance,
which obviates the parameter-tuning process required for conventional
rule-based detection algorithms. Integrating the algorithm with a motorized
optical microscope enables the automated searching and cataloging of 2D
materials. This development will allow researchers to utilize unlimited amounts
of 2D materials simply by exfoliating and running the automated searching
process.",arxiv
http://arxiv.org/abs/2002.10137v2,2020-03-05T10:06:22Z,2020-02-24T10:02:10Z,"Audio-driven Talking Face Video Generation with Learning-based
  Personalized Head Pose","Real-world talking faces often accompany with natural head movement. However,
most existing talking face video generation methods only consider facial
animation with fixed head pose. In this paper, we address this problem by
proposing a deep neural network model that takes an audio signal A of a source
person and a very short video V of a target person as input, and outputs a
synthesized high-quality talking face video with personalized head pose (making
use of the visual information in V), expression and lip synchronization (by
considering both A and V). The most challenging issue in our work is that
natural poses often cause in-plane and out-of-plane head rotations, which makes
synthesized talking face video far from realistic. To address this challenge,
we reconstruct 3D face animation and re-render it into synthesized frames. To
fine tune these frames into realistic ones with smooth background transition,
we propose a novel memory-augmented GAN module. By first training a general
mapping based on a publicly available dataset and fine-tuning the mapping using
the input short video of target person, we develop an effective strategy that
only requires a small number of frames (about 300 frames) to learn personalized
talking behavior including head pose. Extensive experiments and two user
studies show that our method can generate high-quality (i.e., personalized head
movements, expressions and good lip synchronization) talking face videos, which
are naturally looking with more distinguishing head movement effects than the
state-of-the-art methods.",arxiv
http://arxiv.org/abs/2105.01852v2,2021-10-22T15:54:55Z,2021-05-05T03:28:57Z,Deep Learning for Needle Detection in a Cannulation Simulator,"Cannulation for hemodialysis is the act of inserting a needle into a
surgically created vascular access (e.g., an arteriovenous fistula) for the
purpose of dialysis. The main risk associated with cannulation is infiltration,
the puncture of the wall of the vascular access after entry, which can cause
medical complications. Simulator-based training allows clinicians to gain
cannulation experience without putting patients at risk. In this paper, we
propose to use deep-learning-based techniques for detecting, based on video,
whether the needle tip is in or has infiltrated the simulated fistula. Three
categories of deep neural networks are investigated in this work: modified
pre-trained models based on VGG-16 and ResNet-50, light convolutional neural
networks (light CNNs), and convolutional recurrent neural networks (CRNNs).
CRNNs consist of convolutional layers and a long short-term memory (LSTM)
layer. A data set of cannulation experiments was collected and analyzed. The
results show that both the light CNN and the CRNN achieve better performance
than the pre-trained baseline models. The CRNN was implemented in real time on
commodity hardware for use in the cannulation simulator, and the performance
was verified. Deep-learning video analysis is a viable method for detecting
needle state in a low cost cannulation simulator. Our data sets and code are
released at https://github.com/axin233/DL_for_Needle_Detection_Cannulation",arxiv
http://arxiv.org/abs/2010.08600v2,2020-11-16T06:26:16Z,2020-10-16T19:40:08Z,"Robot Navigation in Constrained Pedestrian Environments using
  Reinforcement Learning","Navigating fluently around pedestrians is a necessary capability for mobile
robots deployed in human environments, such as buildings and homes. While
research on social navigation has focused mainly on the scalability with the
number of pedestrians in open spaces, typical indoor environments present the
additional challenge of constrained spaces such as corridors and doorways that
limit maneuverability and influence patterns of pedestrian interaction. We
present an approach based on reinforcement learning (RL) to learn policies
capable of dynamic adaptation to the presence of moving pedestrians while
navigating between desired locations in constrained environments. The policy
network receives guidance from a motion planner that provides waypoints to
follow a globally planned trajectory, whereas RL handles the local
interactions. We explore a compositional principle for multi-layout training
and find that policies trained in a small set of geometrically simple layouts
successfully generalize to more complex unseen layouts that exhibit composition
of the structural elements available during training. Going beyond walls-world
like domains, we show transfer of the learned policy to unseen 3D
reconstructions of two real environments. These results support the
applicability of the compositional principle to navigation in real-world
buildings and indicate promising usage of multi-agent simulation within
reconstructed environments for tasks that involve interaction.",arxiv
http://arxiv.org/abs/2108.00150v1,2021-07-31T05:02:52Z,2021-07-31T05:02:52Z,Scene Inference for Object Illumination Editing,"The seamless illumination integration between a foreground object and a
background scene is an important but challenging task in computer vision and
augmented reality community. However, to our knowledge, there is no publicly
available high-quality dataset that meets the illumination seamless integration
task, which greatly hinders the development of this research direction. To this
end, we apply a physically-based rendering method to create a large-scale,
high-quality dataset, named IH dataset, which provides rich illumination
information for seamless illumination integration task. In addition, we propose
a deep learning-based SI-GAN method, a multi-task collaborative network, which
makes full use of the multi-scale attention mechanism and adversarial learning
strategy to directly infer mapping relationship between the inserted foreground
object and corresponding background environment, and edit object illumination
according to the proposed illumination exchange mechanism in parallel network.
By this means, we can achieve the seamless illumination integration without
explicit estimation of 3D geometric information. Comprehensive experiments on
both our dataset and real-world images collected from the Internet show that
our proposed SI-GAN provides a practical and effective solution for image-based
object illumination editing, and validate the superiority of our method against
state-of-the-art methods.",arxiv
http://arxiv.org/abs/1909.07096v3,2020-08-03T07:33:28Z,2019-09-16T09:58:42Z,Virtual Reality for Robots,"This paper applies the principles of Virtual Reality (VR) to robots, rather
than living organisms. A simulator, of either physical states or information
states, renders outputs to custom displays that fool the robot's sensors. This
enables a robot to experience a combination of real and virtual sensor inputs,
combining the efficiency of simulation and the benefits of real world sensor
inputs. Thus, the robot can be taken through targeted experiences that are more
realistic than pure simulation, yet more feasible and controllable than pure
real-world experiences. We define two distinctive methods for applying VR to
robots, namely black box and white box; based on these methods we identify
potential applications, such as testing and verification procedures that are
better than simulation, the study of spoofing attacks and anti-spoofing
techniques, and sample generation for machine learning. A general mathematical
framework is presented, along with a simple experiment, detailed examples, and
discussion of the implications.",arxiv
http://arxiv.org/abs/2107.12137v2,2021-08-11T06:54:54Z,2021-07-26T12:18:23Z,AA3DNet: Attention Augmented Real Time 3D Object Detection,"In this work, we address the problem of 3D object detection from point cloud
data in real time. For autonomous vehicles to work, it is very important for
the perception component to detect the real world objects with both high
accuracy and fast inference. We propose a novel neural network architecture
along with the training and optimization details for detecting 3D objects using
point cloud data. We present anchor design along with custom loss functions
used in this work. A combination of spatial and channel wise attention module
is used in this work. We use the Kitti 3D Birds Eye View dataset for
benchmarking and validating our results. Our method surpasses previous state of
the art in this domain both in terms of average precision and speed running at
> 30 FPS. Finally, we present the ablation study to demonstrate that the
performance of our network is generalizable. This makes it a feasible option to
be deployed in real time applications like self driving cars.",arxiv
http://arxiv.org/abs/2002.04597v1,2020-02-11T18:41:57Z,2020-02-11T18:41:57Z,WatchDog: Real-time Vehicle Tracking on Geo-distributed Edge Nodes,"Vehicle tracking, a core application to smart city video analytics, is
becoming more widely deployed than ever before thanks to the increasing number
of traffic cameras and recent advances of computer vision and machine learning.
Due to the constraints of bandwidth, latency, and privacy concerns, tracking
tasks are more preferable to run on edge devices sitting close to the cameras.
However, edge devices are provisioned with a fixed amount of compute budget,
making them incompetent to adapt to time-varying tracking workloads caused by
traffic dynamics. In coping with this challenge, we propose WatchDog, a
real-time vehicle tracking system fully utilizes edge nodes across the road
network. WatchDog leverages computer vision tasks with different
resource-accuracy trade-offs, and decompose and schedule tracking tasks
judiciously across edge devices based on the current workload to maximize the
number of tasks while ensuring a provable response time bound at each edge
device. Extensive evaluations have been conducted using real-world city-wide
vehicle trajectory datasets, showing a 100% tracking coverage with real-time
guarantee.",arxiv
http://arxiv.org/abs/1905.05316v1,2019-05-13T23:25:10Z,2019-05-13T23:25:10Z,Wireless Edge Computing with Latency and Reliability Guarantees,"Edge computing is an emerging concept based on distributing computing,
storage, and control services closer to end network nodes. Edge computing lies
at the heart of the fifth generation (5G) wireless systems and beyond. While
current state-of-the-art networks communicate, compute, and process data in a
centralized manner (at the cloud), for latency and compute-centric
applications, both radio access and computational resources must be brought
closer to the edge, harnessing the availability of computing and
storage-enabled small cell base stations in proximity to the end devices.
Furthermore, the network infrastructure must enable a distributed edge
decision-making service that learns to adapt to the network dynamics with
minimal latency and optimize network deployment and operation accordingly. This
article will provide a fresh look to the concept of edge computing by first
discussing the applications that the network edge must provide, with a special
emphasis on the ensuing challenges in enabling ultra-reliable and low-latency
edge computing services for mission-critical applications such as virtual
reality (VR), vehicle-to-everything (V2X), edge artificial intelligence (AI),
and so forth. Furthermore, several case studies where the edge is key are
explored followed by insights and prospect for future work.",arxiv
http://arxiv.org/abs/2010.12570v3,2021-07-14T12:51:12Z,2020-10-23T17:54:38Z,"Eye Tracking Data Collection Protocol for VR for Remotely Located
  Subjects using Blockchain and Smart Contracts","Eye tracking data collection in the virtual reality context is typically
carried out in laboratory settings, which usually limits the number of
participants or consumes at least several months of research time. In addition,
under laboratory settings, subjects may not behave naturally due to being
recorded in an uncomfortable environment. In this work, we propose a
proof-of-concept eye tracking data collection protocol and its implementation
to collect eye tracking data from remotely located subjects, particularly for
virtual reality using Ethereum blockchain and smart contracts. With the
proposed protocol, data collectors can collect high quality eye tracking data
from a large number of human subjects with heterogeneous socio-demographic
characteristics. The quality and the amount of data can be helpful for various
tasks in data-driven human-computer interaction and artificial intelligence.",arxiv
http://arxiv.org/abs/2103.07220v1,2021-03-12T11:49:51Z,2021-03-12T11:49:51Z,Real-time Timbre Transfer and Sound Synthesis using DDSP,"Neural audio synthesis is an actively researched topic, having yielded a wide
range of techniques that leverages machine learning architectures. Google
Magenta elaborated a novel approach called Differential Digital Signal
Processing (DDSP) that incorporates deep neural networks with preconditioned
digital signal processing techniques, reaching state-of-the-art results
especially in timbre transfer applications. However, most of these techniques,
including the DDSP, are generally not applicable in real-time constraints,
making them ineligible in a musical workflow. In this paper, we present a
real-time implementation of the DDSP library embedded in a virtual synthesizer
as a plug-in that can be used in a Digital Audio Workstation. We focused on
timbre transfer from learned representations of real instruments to arbitrary
sound inputs as well as controlling these models by MIDI. Furthermore, we
developed a GUI for intuitive high-level controls which can be used for
post-processing and manipulating the parameters estimated by the neural
network. We have conducted a user experience test with seven participants
online. The results indicated that our users found the interface appealing,
easy to understand, and worth exploring further. At the same time, we have
identified issues in the timbre transfer quality, in some components we did not
implement, and in installation and distribution of our plugin. The next
iteration of our design will address these issues. Our real-time MATLAB and
JUCE implementations are available at https://github.com/SMC704/juce-ddsp and
https://github.com/SMC704/matlab-ddsp , respectively.",arxiv
http://arxiv.org/abs/1903.02219v1,2019-03-06T07:39:11Z,2019-03-06T07:39:11Z,Training in Task Space to Speed Up and Guide Reinforcement Learning,"Recent breakthroughs in the reinforcement learning (RL) community have made
significant advances towards learning and deploying policies on real world
robotic systems. However, even with the current state-of-the-art algorithms and
computational resources, these algorithms are still plagued with high sample
complexity, and thus long training times, especially for high degree of freedom
(DOF) systems. There are also concerns arising from lack of perceived stability
or robustness guarantees from emerging policies. This paper aims at mitigating
these drawbacks by: (1) modeling a complex, high DOF system with a
representative simple one, (2) making explicit use of forward and inverse
kinematics without forcing the RL algorithm to ""learn"" them on its own, and (3)
learning locomotion policies in Cartesian space instead of joint space. In this
paper these methods are applied to JPL's Robosimian, but can be readily used on
any system with a base and end effector(s). These locomotion policies can be
produced in just a few minutes, trained on a single laptop. We compare the
robustness of the resulting learned policies to those of other control methods.
An accompanying video for this paper can be found at
https://youtu.be/xDxxSw5ahnc .",arxiv
http://arxiv.org/abs/1611.04687v2,2016-12-05T05:04:35Z,2016-11-15T03:17:15Z,"Intrinsic Geometric Information Transfer Learning on Multiple
  Graph-Structured Datasets","Graphs provide a powerful means for representing complex interactions between
entities. Recently, deep learning approaches are emerging for representing and
modeling graph-structured data, although the conventional deep learning methods
(such as convolutional neural networks and recurrent neural networks) have
mainly focused on grid-structured inputs (image and audio). Leveraged by the
capability of representation learning, deep learning based techniques are
reporting promising results for graph applications by detecting structural
characteristics of graphs in an automated fashion. In this paper, we attempt to
advance deep learning for graph-structured data by incorporating another
component, transfer learning. By transferring the intrinsic geometric
information learned in the source domain, our approach can help us to construct
a model for a new but related task in the target domain without collecting new
data and without training a new model from scratch. We thoroughly test our
approach with large-scale real corpora and confirm the effectiveness of the
proposed transfer learning framework for deep learning on graphs. According to
our experiments, transfer learning is most effective when the source and target
domains bear a high level of structural similarity in their graph
representations.",arxiv
http://arxiv.org/abs/1911.06636v2,2020-06-16T09:13:58Z,2019-11-15T13:57:35Z,"Catch & Carry: Reusable Neural Controllers for Vision-Guided Whole-Body
  Tasks","We address the longstanding challenge of producing flexible, realistic
humanoid character controllers that can perform diverse whole-body tasks
involving object interactions. This challenge is central to a variety of
fields, from graphics and animation to robotics and motor neuroscience. Our
physics-based environment uses realistic actuation and first-person perception
-- including touch sensors and egocentric vision -- with a view to producing
active-sensing behaviors (e.g. gaze direction), transferability to real robots,
and comparisons to the biology. We develop an integrated neural-network based
approach consisting of a motor primitive module, human demonstrations, and an
instructed reinforcement learning regime with curricula and task variations. We
demonstrate the utility of our approach for several tasks, including
goal-conditioned box carrying and ball catching, and we characterize its
behavioral robustness. The resulting controllers can be deployed in real-time
on a standard PC. See overview video, https://youtu.be/2rQAW-8gQQk .",arxiv
http://arxiv.org/abs/2008.04572v1,2020-08-11T08:10:58Z,2020-08-11T08:10:58Z,"An Empirical Analysis of Backward Compatibility in Machine Learning
  Systems","In many applications of machine learning (ML), updates are performed with the
goal of enhancing model performance. However, current practices for updating
models rely solely on isolated, aggregate performance analyses, overlooking
important dependencies, expectations, and needs in real-world deployments. We
consider how updates, intended to improve ML models, can introduce new errors
that can significantly affect downstream systems and users. For example,
updates in models used in cloud-based classification services, such as image
recognition, can cause unexpected erroneous behavior in systems that make calls
to the services. Prior work has shown the importance of ""backward
compatibility"" for maintaining human trust. We study challenges with backward
compatibility across different ML architectures and datasets, focusing on
common settings including data shifts with structured noise and ML employed in
inferential pipelines. Our results show that (i) compatibility issues arise
even without data shift due to optimization stochasticity, (ii) training on
large-scale noisy datasets often results in significant decreases in backward
compatibility even when model accuracy increases, and (iii) distributions of
incompatible points align with noise bias, motivating the need for
compatibility aware de-noising and robustness methods.",arxiv
http://arxiv.org/abs/1908.11588v1,2019-08-30T08:13:22Z,2019-08-30T08:13:22Z,Generating Persuasive Visual Storylines for Promotional Videos,"Video contents have become a critical tool for promoting products in
E-commerce. However, the lack of automatic promotional video generation
solutions makes large-scale video-based promotion campaigns infeasible. The
first step of automatically producing promotional videos is to generate visual
storylines, which is to select the building block footage and place them in an
appropriate order. This task is related to the subjective viewing experience.
It is hitherto performed by human experts and thus, hard to scale. To address
this problem, we propose WundtBackpack, an algorithmic approach to generate
storylines based on available visual materials, which can be video clips or
images. It consists of two main parts, 1) the Learnable Wundt Curve to evaluate
the perceived persuasiveness based on the stimulus intensity of a sequence of
visual materials, which only requires a small volume of data to train; and 2) a
clustering-based backpacking algorithm to generate persuasive sequences of
visual materials while considering video length constraints. In this way, the
proposed approach provides a dynamic structure to empower artificial
intelligence (AI) to organize video footage in order to construct a sequence of
visual stimuli with persuasive power. Extensive real-world experiments show
that our approach achieves close to 10% higher perceived persuasiveness scores
by human testers, and 12.5% higher expected revenue compared to the best
performing state-of-the-art approach.",arxiv
http://arxiv.org/abs/1711.05734v2,2018-02-20T21:43:55Z,2017-11-15T10:15:44Z,"Chipmunk: A Systolically Scalable 0.9 mm${}^2$, 3.08 Gop/s/mW @ 1.2 mW
  Accelerator for Near-Sensor Recurrent Neural Network Inference","Recurrent neural networks (RNNs) are state-of-the-art in voice
awareness/understanding and speech recognition. On-device computation of RNNs
on low-power mobile and wearable devices would be key to applications such as
zero-latency voice-based human-machine interfaces. Here we present Chipmunk, a
small (<1 mm${}^2$) hardware accelerator for Long-Short Term Memory RNNs in UMC
65 nm technology capable to operate at a measured peak efficiency up to 3.08
Gop/s/mW at 1.24 mW peak power. To implement big RNN models without incurring
in huge memory transfer overhead, multiple Chipmunk engines can cooperate to
form a single systolic array. In this way, the Chipmunk architecture in a 75
tiles configuration can achieve real-time phoneme extraction on a demanding RNN
topology proposed by Graves et al., consuming less than 13 mW of average power.",arxiv
http://arxiv.org/abs/1904.01578v2,2019-04-08T12:00:46Z,2019-04-02T12:10:23Z,Unsupervised training of neural mask-based beamforming,"We present an unsupervised training approach for a neural network-based mask
estimator in an acoustic beamforming application. The network is trained to
maximize a likelihood criterion derived from a spatial mixture model of the
observations. It is trained from scratch without requiring any parallel data
consisting of degraded input and clean training targets. Thus, training can be
carried out on real recordings of noisy speech rather than simulated ones. In
contrast to previous work on unsupervised training of neural mask estimators,
our approach avoids the need for a possibly pre-trained teacher model entirely.
We demonstrate the effectiveness of our approach by speech recognition
experiments on two different datasets: one mainly deteriorated by noise (CHiME
4) and one by reverberation (REVERB). The results show that the performance of
the proposed system is on par with a supervised system using oracle target
masks for training and with a system trained using a model-based teacher.",arxiv
http://arxiv.org/abs/1705.09792v4,2018-02-25T23:42:06Z,2017-05-27T09:04:55Z,Deep Complex Networks,"At present, the vast majority of building blocks, techniques, and
architectures for deep learning are based on real-valued operations and
representations. However, recent work on recurrent neural networks and older
fundamental theoretical analysis suggests that complex numbers could have a
richer representational capacity and could also facilitate noise-robust memory
retrieval mechanisms. Despite their attractive properties and potential for
opening up entirely new neural architectures, complex-valued deep neural
networks have been marginalized due to the absence of the building blocks
required to design such models. In this work, we provide the key atomic
components for complex-valued deep neural networks and apply them to
convolutional feed-forward networks and convolutional LSTMs. More precisely, we
rely on complex convolutions and present algorithms for complex
batch-normalization, complex weight initialization strategies for
complex-valued neural nets and we use them in experiments with end-to-end
training schemes. We demonstrate that such complex-valued models are
competitive with their real-valued counterparts. We test deep complex models on
several computer vision tasks, on music transcription using the MusicNet
dataset and on Speech Spectrum Prediction using the TIMIT dataset. We achieve
state-of-the-art performance on these audio-related tasks.",arxiv
http://arxiv.org/abs/2001.00269v2,2020-04-01T05:40:23Z,2020-01-01T21:21:21Z,"A Smart, Efficient, and Reliable Parking Surveillance System with Edge
  Artificial Intelligence on IoT Devices","Cloud computing has been a main-stream computing service for years. Recently,
with the rapid development in urbanization, massive video surveillance data are
produced at an unprecedented speed. A traditional solution to deal with the big
data would require a large amount of computing and storage resources. With the
advances in Internet of things (IoT), artificial intelligence, and
communication technologies, edge computing offers a new solution to the problem
by processing the data partially or wholly on the edge of a surveillance
system. In this study, we investigate the feasibility of using edge computing
for smart parking surveillance tasks, which is a key component of Smart City.
The system processing pipeline is carefully designed with the consideration of
flexibility, online surveillance, data transmission, detection accuracy, and
system reliability. It enables artificial intelligence at the edge by
implementing an enhanced single shot multibox detector (SSD). A few more
algorithms are developed on both the edge and the server targeting optimal
system efficiency and accuracy. Thorough field tests were conducted in the
Angle Lake parking garage for three months. The experimental results are
promising that the final detection method achieves over 95% accuracy in
real-world scenarios with high efficiency and reliability. The proposed smart
parking surveillance system can be a solid foundation for future applications
of intelligent transportation systems.",arxiv
http://arxiv.org/abs/1711.09312v1,2017-11-26T00:12:43Z,2017-11-26T00:12:43Z,"Unsupervised 3D Reconstruction from a Single Image via Adversarial
  Learning","Recent advancements in deep learning opened new opportunities for learning a
high-quality 3D model from a single 2D image given sufficient training on
large-scale data sets. However, the significant imbalance between available
amount of images and 3D models, and the limited availability of labeled 2D
image data (i.e. manually annotated pairs between images and their
corresponding 3D models), severely impacts the training of most supervised deep
learning methods in practice. In this paper, driven by a novel design of
adversarial networks, we have developed an unsupervised learning paradigm to
reconstruct 3D models from a single 2D image, which is free of manually
annotated pairwise input image and its associated 3D model. Particularly, the
paradigm begins with training an adaption network via autoencoder with
adversarial loss, which embeds unpaired 2D synthesized image domain with real
world image domain to a shared latent vector space. Then, we jointly train a 3D
deconvolutional network to transform the latent vector space to the 3D object
space together with the embedding process. Our experiments verify our network's
robust and superior performance in handling 3D volumetric object generation
from a single 2D image.",arxiv
http://arxiv.org/abs/2006.04001v2,2020-06-09T12:25:49Z,2020-06-07T00:11:42Z,Real-Time Model Calibration with Deep Reinforcement Learning,"The dynamic, real-time, and accurate inference of model parameters from
empirical data is of great importance in many scientific and engineering
disciplines that use computational models (such as a digital twin) for the
analysis and prediction of complex physical processes. However, fast and
accurate inference for processes with large and high dimensional datasets
cannot easily be achieved with state-of-the-art methods under noisy real-world
conditions. The primary reason is that the inference of model parameters with
traditional techniques based on optimisation or sampling often suffers from
computational and statistical challenges, resulting in a trade-off between
accuracy and deployment time. In this paper, we propose a novel framework for
inference of model parameters based on reinforcement learning. The contribution
of the paper is twofold: 1) We reformulate the inference problem as a tracking
problem with the objective of learning a policy that forces the response of the
physics-based model to follow the observations; 2) We propose the constrained
Lyapunov-based actor-critic (CLAC) algorithm to enable the robust and accurate
inference of physics-based model parameters in real time under noisy real-world
conditions. The proposed methodology is demonstrated and evaluated on two
model-based diagnostics test cases utilizing two different physics-based models
of turbofan engines. The performance of the methodology is compared to that of
two alternative approaches: a state update method (unscented Kalman filter) and
a supervised end-to-end mapping with deep neural networks. The experimental
results demonstrate that the proposed methodology outperforms all other tested
methods in terms of speed and robustness, with high inference accuracy.",arxiv
http://arxiv.org/abs/1511.03990v1,2015-11-12T17:54:46Z,2015-11-12T17:54:46Z,Automatic Inference of the Quantile Parameter,"Supervised learning is an active research area, with numerous applications in
diverse fields such as data analytics, computer vision, speech and audio
processing, and image understanding. In most cases, the loss functions used in
machine learning assume symmetric noise models, and seek to estimate the
unknown function parameters. However, loss functions such as quantile and
quantile Huber generalize the symmetric $\ell_1$ and Huber losses to the
asymmetric setting, for a fixed quantile parameter. In this paper, we propose
to jointly infer the quantile parameter and the unknown function parameters,
for the asymmetric quantile Huber and quantile losses. We explore various
properties of the quantile Huber loss and implement a convexity certificate
that can be used to check convexity in the quantile parameter. When the loss if
convex with respect to the parameter of the function, we prove that it is
biconvex in both the function and the quantile parameters, and propose an
algorithm to jointly estimate these. Results with synthetic and real data
demonstrate that the proposed approach can automatically recover the quantile
parameter corresponding to the noise and also provide an improved recovery of
function parameters. To illustrate the potential of the framework, we extend
the gradient boosting machines with quantile losses to automatically estimate
the quantile parameter at each iteration.",arxiv
http://arxiv.org/abs/1903.05299v2,2019-04-28T20:42:39Z,2019-03-13T03:11:39Z,"Frequency Domain Multi-channel Acoustic Modeling for Distant Speech
  Recognition","Conventional far-field automatic speech recognition (ASR) systems typically
employ microphone array techniques for speech enhancement in order to improve
robustness against noise or reverberation. However, such speech enhancement
techniques do not always yield ASR accuracy improvement because the
optimization criterion for speech enhancement is not directly relevant to the
ASR objective. In this work, we develop new acoustic modeling techniques that
optimize spatial filtering and long short-term memory (LSTM) layers from
multi-channel (MC) input based on an ASR criterion directly. In contrast to
conventional methods, we incorporate array processing knowledge into the
acoustic model. Moreover, we initialize the network with beamformers'
coefficients. We investigate effects of such MC neural networks through ASR
experiments on the real-world far-field data where users are interacting with
an ASR system in uncontrolled acoustic environments. We show that our MC
acoustic model can reduce a word error rate (WER) by~16.5\% compared to a
single channel ASR system with the traditional log-mel filter bank energy
(LFBE) feature on average. Our result also shows that our network with the
spatial filtering layer on two-channel input achieves a relative WER reduction
of~9.5\% compared to conventional beamforming with seven microphones.",arxiv
http://arxiv.org/abs/1908.02999v1,2019-08-08T10:19:48Z,2019-08-08T10:19:48Z,Learning Vision-based Flight in Drone Swarms by Imitation,"Decentralized drone swarms deployed today either rely on sharing of positions
among agents or detecting swarm members with the help of visual markers. This
work proposes an entirely visual approach to coordinate markerless drone swarms
based on imitation learning. Each agent is controlled by a small and efficient
convolutional neural network that takes raw omnidirectional images as inputs
and predicts 3D velocity commands that match those computed by a flocking
algorithm. We start training in simulation and propose a simple yet effective
unsupervised domain adaptation approach to transfer the learned controller to
the real world. We further train the controller with data collected in our
motion capture hall. We show that the convolutional neural network trained on
the visual inputs of the drone can learn not only robust inter-agent collision
avoidance but also cohesion of the swarm in a sample-efficient manner. The
neural controller effectively learns to localize other agents in the visual
input, which we show by visualizing the regions with the most influence on the
motion of an agent. We remove the dependence on sharing positions among swarm
members by taking only local visual information into account for control. Our
work can therefore be seen as the first step towards a fully decentralized,
vision-based swarm without the need for communication or visual markers.",arxiv
http://arxiv.org/abs/2006.05011v2,2020-08-05T20:41:29Z,2020-06-09T01:55:48Z,RGB-D-E: Event Camera Calibration for Fast 6-DOF Object Tracking,"Augmented reality devices require multiple sensors to perform various tasks
such as localization and tracking. Currently, popular cameras are mostly
frame-based (e.g. RGB and Depth) which impose a high data bandwidth and power
usage. With the necessity for low power and more responsive augmented reality
systems, using solely frame-based sensors imposes limits to the various
algorithms that needs high frequency data from the environement. As such,
event-based sensors have become increasingly popular due to their low power,
bandwidth and latency, as well as their very high frequency data acquisition
capabilities. In this paper, we propose, for the first time, to use an
event-based camera to increase the speed of 3D object tracking in 6 degrees of
freedom. This application requires handling very high object speed to convey
compelling AR experiences. To this end, we propose a new system which combines
a recent RGB-D sensor (Kinect Azure) with an event camera (DAVIS346). We
develop a deep learning approach, which combines an existing RGB-D network
along with a novel event-based network in a cascade fashion, and demonstrate
that our approach significantly improves the robustness of a state-of-the-art
frame-based 6-DOF object tracker using our RGB-D-E pipeline.",arxiv
http://arxiv.org/abs/1909.06526v1,2019-09-14T04:02:45Z,2019-09-14T04:02:45Z,FfDL : A Flexible Multi-tenant Deep Learning Platform,"Deep learning (DL) is becoming increasingly popular in several application
domains and has made several new application features involving computer
vision, speech recognition and synthesis, self-driving automobiles, drug
design, etc. feasible and accurate. As a result, large scale on-premise and
cloud-hosted deep learning platforms have become essential infrastructure in
many organizations. These systems accept, schedule, manage and execute DL
training jobs at scale.
  This paper describes the design, implementation and our experiences with
FfDL, a DL platform used at IBM. We describe how our design balances
dependability with scalability, elasticity, flexibility and efficiency. We
examine FfDL qualitatively through a retrospective look at the lessons learned
from building, operating, and supporting FfDL; and quantitatively through a
detailed empirical evaluation of FfDL, including the overheads introduced by
the platform for various deep learning models, the load and performance
observed in a real case study using FfDL within our organization, the frequency
of various faults observed including unanticipated faults, and experiments
demonstrating the benefits of various scheduling policies. FfDL has been
open-sourced.",arxiv
http://arxiv.org/abs/1710.07563v1,2017-10-20T15:05:41Z,2017-10-20T15:05:41Z,SEGCloud: Semantic Segmentation of 3D Point Clouds,"3D semantic scene labeling is fundamental to agents operating in the real
world. In particular, labeling raw 3D point sets from sensors provides
fine-grained semantics. Recent works leverage the capabilities of Neural
Networks (NNs), but are limited to coarse voxel predictions and do not
explicitly enforce global consistency. We present SEGCloud, an end-to-end
framework to obtain 3D point-level segmentation that combines the advantages of
NNs, trilinear interpolation(TI) and fully connected Conditional Random Fields
(FC-CRF). Coarse voxel predictions from a 3D Fully Convolutional NN are
transferred back to the raw 3D points via trilinear interpolation. Then the
FC-CRF enforces global consistency and provides fine-grained semantics on the
points. We implement the latter as a differentiable Recurrent NN to allow joint
optimization. We evaluate the framework on two indoor and two outdoor 3D
datasets (NYU V2, S3DIS, KITTI, Semantic3D.net), and show performance
comparable or superior to the state-of-the-art on all datasets.",arxiv
http://arxiv.org/abs/1909.10976v1,2019-09-24T14:58:07Z,2019-09-24T14:58:07Z,"Synthetic dataset generation for object-to-model deep learning in
  industrial applications","The availability of large image data sets has been a crucial factor in the
success of deep learning-based classification and detection methods. While data
sets for everyday objects are widely available, data for specific industrial
use-cases (e.g. identifying packaged products in a warehouse) remains scarce.
In such cases, the data sets have to be created from scratch, placing a crucial
bottleneck on the deployment of deep learning techniques in industrial
applications.
  We present work carried out in collaboration with a leading UK online
supermarket, with the aim of creating a computer vision system capable of
detecting and identifying unique supermarket products in a warehouse setting.
To this end, we demonstrate a framework for using synthetic data to create an
end-to-end deep learning pipeline, beginning with real-world objects and
culminating in a trained model.
  Our method is based on the generation of a synthetic dataset from 3D models
obtained by applying photogrammetry techniques to real-world objects. Using
100k synthetic images generated from 60 real images per class, an InceptionV3
convolutional neural network (CNN) was trained, which achieved classification
accuracy of 95.8% on a separately acquired test set of real supermarket product
images. The image generation process supports automatic pixel annotation. This
eliminates the prohibitively expensive manual annotation typically required for
detection tasks. Based on this readily available data, a one-stage RetinaNet
detector was trained on the synthetic, annotated images to produce a detector
that can accurately localize and classify the specimen products in real-time.",arxiv
http://arxiv.org/abs/2107.07502v2,2021-11-10T07:31:56Z,2021-07-15T17:54:36Z,MultiBench: Multiscale Benchmarks for Multimodal Representation Learning,"Learning multimodal representations involves integrating information from
multiple heterogeneous sources of data. It is a challenging yet crucial area
with numerous real-world applications in multimedia, affective computing,
robotics, finance, human-computer interaction, and healthcare. Unfortunately,
multimodal research has seen limited resources to study (1) generalization
across domains and modalities, (2) complexity during training and inference,
and (3) robustness to noisy and missing modalities. In order to accelerate
progress towards understudied modalities and tasks while ensuring real-world
robustness, we release MultiBench, a systematic and unified large-scale
benchmark spanning 15 datasets, 10 modalities, 20 prediction tasks, and 6
research areas. MultiBench provides an automated end-to-end machine learning
pipeline that simplifies and standardizes data loading, experimental setup, and
model evaluation. To enable holistic evaluation, MultiBench offers a
comprehensive methodology to assess (1) generalization, (2) time and space
complexity, and (3) modality robustness. MultiBench introduces impactful
challenges for future research, including scalability to large-scale multimodal
datasets and robustness to realistic imperfections. To accompany this
benchmark, we also provide a standardized implementation of 20 core approaches
in multimodal learning. Simply applying methods proposed in different research
areas can improve the state-of-the-art performance on 9/15 datasets. Therefore,
MultiBench presents a milestone in unifying disjoint efforts in multimodal
research and paves the way towards a better understanding of the capabilities
and limitations of multimodal models, all the while ensuring ease of use,
accessibility, and reproducibility. MultiBench, our standardized code, and
leaderboards are publicly available, will be regularly updated, and welcomes
inputs from the community.",arxiv
http://arxiv.org/abs/2005.08337v1,2020-05-17T18:46:23Z,2020-05-17T18:46:23Z,A Survey on Unknown Presentation Attack Detection for Fingerprint,"Fingerprint recognition systems are widely deployed in various real-life
applications as they have achieved high accuracy. The widely used applications
include border control, automated teller machine (ATM), and attendance
monitoring systems. However, these critical systems are prone to spoofing
attacks (a.k.a presentation attacks (PA)). PA for fingerprint can be performed
by presenting gummy fingers made from different materials such as silicone,
gelatine, play-doh, ecoflex, 2D printed paper, 3D printed material, or latex.
Biometrics Researchers have developed Presentation Attack Detection (PAD)
methods as a countermeasure to PA. PAD is usually done by training a machine
learning classifier for known attacks for a given dataset, and they achieve
high accuracy in this task. However, generalizing to unknown attacks is an
essential problem from applicability to real-world systems, mainly because
attacks cannot be exhaustively listed in advance. In this survey paper, we
present a comprehensive survey on existing PAD algorithms for fingerprint
recognition systems, specifically from the standpoint of detecting unknown PAD.
We categorize PAD algorithms, point out their advantages/disadvantages, and
future directions for this area.",arxiv
http://arxiv.org/abs/1910.05376v2,2019-12-13T22:58:20Z,2019-10-11T18:57:18Z,AffWild Net and Aff-Wild Database,"Emotions recognition is the task of recognizing people's emotions. Usually it
is achieved by analyzing expression of peoples faces. There are two ways for
representing emotions: The categorical approach and the dimensional approach by
using valence and arousal values. Valence shows how negative or positive an
emotion is and arousal shows how much it is activated. Recent deep learning
models, that have to do with emotions recognition, are using the second
approach, valence and arousal. Moreover, a more interesting concept, which is
useful in real life is the ""in the wild"" emotions recognition. ""In the wild""
means that the images analyzed for the recognition task, come from from real
life sources(online videos, online photos, etc.) and not from staged
experiments. So, they introduce unpredictable situations in the images, that
have to be modeled. The purpose of this project is to study the previous work
that was done for the ""in the wild"" emotions recognition concept, design a new
dataset which has as a standard the ""Aff-wild"" database, implement new deep
learning models and evaluate the results. First, already existing databases and
deep learning models are presented. Then, inspired by them a new database is
created which includes 507.208 frames in total from 106 videos, which were
gathered from online sources. Then, the data are tested in a CNN model based on
CNN-M architecture, in order to be sure about their usability. Next, the main
model of this project is implemented. That is a Regression GAN which can
execute unsupervised and supervised learning at the same time. More
specifically, it keeps the main functionality of GANs, which is to produce fake
images that look as good as the real ones, while it can also predict valence
and arousal values for both real and fake images. Finally, the database created
earlier is applied to this model and the results are presented and evaluated.",arxiv
http://arxiv.org/abs/2111.02156v1,2021-11-03T11:53:57Z,2021-11-03T11:53:57Z,"Continual Learning of Semantic Segmentation using Complementary 2D-3D
  Data Representations","Semantic segmentation networks are usually pre-trained and not updated during
deployment. As a consequence, misclassifications commonly occur if the
distribution of the training data deviates from the one encountered during the
robot's operation. We propose to mitigate this problem by adapting the neural
network to the robot's environment during deployment, without any need for
external supervision. Leveraging complementary data representations, we
generate a supervision signal, by probabilistically accumulating consecutive 2D
semantic predictions in a volumetric 3D map. We then retrain the network on
renderings of the accumulated semantic map, effectively resolving ambiguities
and enforcing multi-view consistency through the 3D representation. To preserve
the previously-learned knowledge while performing network adaptation, we employ
a continual learning strategy based on experience replay. Through extensive
experimental evaluation, we show successful adaptation to real-world indoor
scenes both on the ScanNet dataset and on in-house data recorded with an RGB-D
sensor. Our method increases the segmentation performance on average by 11.8%
compared to the fixed pre-trained neural network, while effectively retaining
knowledge from the pre-training dataset.",arxiv
http://arxiv.org/abs/2111.08973v1,2021-11-17T08:30:18Z,2021-11-17T08:30:18Z,Generating Unrestricted 3D Adversarial Point Clouds,"Utilizing 3D point cloud data has become an urgent need for the deployment of
artificial intelligence in many areas like facial recognition and self-driving.
However, deep learning for 3D point clouds is still vulnerable to adversarial
attacks, e.g., iterative attacks, point transformation attacks, and generative
attacks. These attacks need to restrict perturbations of adversarial examples
within a strict bound, leading to the unrealistic adversarial 3D point clouds.
In this paper, we propose an Adversarial Graph-Convolutional Generative
Adversarial Network (AdvGCGAN) to generate visually realistic adversarial 3D
point clouds from scratch. Specifically, we use a graph convolutional generator
and a discriminator with an auxiliary classifier to generate realistic point
clouds, which learn the latent distribution from the real 3D data. The
unrestricted adversarial attack loss is incorporated in the special adversarial
training of GAN, which enables the generator to generate the adversarial
examples to spoof the target network. Compared with the existing state-of-art
attack methods, the experiment results demonstrate the effectiveness of our
unrestricted adversarial attack methods with a higher attack success rate and
visual quality. Additionally, the proposed AdvGCGAN can achieve better
performance against defense models and better transferability than existing
attack methods with strong camouflage.",arxiv
http://arxiv.org/abs/1705.08918v1,2017-05-24T18:22:41Z,2017-05-24T18:22:41Z,Unsupervised Learning Layers for Video Analysis,"This paper presents two unsupervised learning layers (UL layers) for
label-free video analysis: one for fully connected layers, and the other for
convolutional ones. The proposed UL layers can play two roles: they can be the
cost function layer for providing global training signal; meanwhile they can be
added to any regular neural network layers for providing local training signals
and combined with the training signals backpropagated from upper layers for
extracting both slow and fast changing features at layers of different depths.
Therefore, the UL layers can be used in either pure unsupervised or
semi-supervised settings. Both a closed-form solution and an online learning
algorithm for two UL layers are provided. Experiments with unlabeled synthetic
and real-world videos demonstrated that the neural networks equipped with UL
layers and trained with the proposed online learning algorithm can extract
shape and motion information from video sequences of moving objects. The
experiments demonstrated the potential applications of UL layers and online
learning algorithm to head orientation estimation and moving object
localization.",arxiv
http://arxiv.org/abs/2006.01674v2,2020-06-07T10:04:40Z,2020-06-02T14:48:34Z,"A network paradigm for very high capacity mobile and fixed
  telecommunications ecosystem sustainable evolution","For very high capacity networks (VHC), the main objective is to improve the
quality of the end-user experience. This implies compliance with key
performance indicators (KPIs) required by applications. Key performance
indicators at the application level are throughput, download time, round trip
time, and video delay. They depend on the end-to-end connection between the
server and the end-user device. For VHC networks, Telco operators must provide
the required application quality. Moreover, they must meet the objectives of
economic sustainability. Today, Telco operators rarely achieve the above
objectives, mainly due to the push to increase the bit-rate of access networks
without considering the end-to-end KPIs of the applications. The main
contribution of this paper concerns the definition of a deployment framework to
address performance and cost issues for VHC networks. We show three actions on
which it is necessary to focus. First, limiting bit-rate through video
compression. Second, contain the rate of packet loss through artificial
intelligence algorithms for line stabilization. Third, reduce latency (i.e.,
round-trip time) with edge-cloud computing. The concerted and gradual
application of these measures can allow a Telco to get out of the
ultra-broadband ""trap"" of the access network, as defined in the paper. We
propose to work on end-to-end optimization of the bandwidth utilization ratio.
This leads to a better performance experienced by the end-user. It also allows
a Telco operator to create new business models and obtain new revenue streams
at a sustainable cost. To give a clear example, we describe how to realize
mobile virtual and augmented reality, which is one of the most challenging
future services.",arxiv
http://arxiv.org/abs/1901.01211v1,2019-01-04T17:00:54Z,2019-01-04T17:00:54Z,"Fully Convolutional Deep Network Architectures for Automatic Short Glass
  Fiber Semantic Segmentation from CT scans","We present the first attempt to perform short glass fiber semantic
segmentation from X-ray computed tomography volumetric datasets at medium (3.9
{\mu}m isotropic) and low (8.3 {\mu}m isotropic) resolution using deep learning
architectures. We performed experiments on both synthetic and real CT scans and
evaluated deep fully convolutional architectures with both 2D and 3D kernels.
Our artificial neural networks outperform existing methods at both medium and
low resolution scans.",arxiv
http://arxiv.org/abs/2005.09237v1,2020-05-19T06:25:52Z,2020-05-19T06:25:52Z,"Acoustic Echo Cancellation by Combining Adaptive Digital Filter and
  Recurrent Neural Network","Acoustic Echo Cancellation (AEC) plays a key role in voice interaction. Due
to the explicit mathematical principle and intelligent nature to accommodate
conditions, adaptive filters with different types of implementations are always
used for AEC, giving considerable performance. However, there would be some
kinds of residual echo in the results, including linear residue introduced by
mismatching between estimation and the reality and non-linear residue mostly
caused by non-linear components on the audio devices. The linear residue can be
reduced with elaborate structure and methods, leaving the non-linear residue
intractable for suppression. Though, some non-linear processing methods have
already be raised, they are complicated and inefficient for suppression, and
would bring damage to the speech audio. In this paper, a fusion scheme by
combining adaptive filter and neural network is proposed for AEC. The echo
could be reduced in a large scale by adaptive filtering, resulting in little
residual echo. Though it is much smaller than speech audio, it could also be
perceived by human ear and would make communication annoy. The neural network
is elaborately designed and trained for suppressing such residual echo.
Experiments compared with prevailing methods are conducted, validating the
effectiveness and superiority of the proposed combination scheme.",arxiv
http://arxiv.org/abs/2003.13652v1,2020-03-18T01:26:36Z,2020-03-18T01:26:36Z,"Machine Learning enabled Spectrum Sharing in Dense LTE-U/Wi-Fi
  Coexistence Scenarios","The application of Machine Learning (ML) techniques to complex engineering
problems has proved to be an attractive and efficient solution. ML has been
successfully applied to several practical tasks like image recognition,
automating industrial operations, etc. The promise of ML techniques in solving
non-linear problems influenced this work which aims to apply known ML
techniques and develop new ones for wireless spectrum sharing between Wi-Fi and
LTE in the unlicensed spectrum. In this work, we focus on the LTE-Unlicensed
(LTE-U) specification developed by the LTE-U Forum, which uses the duty-cycle
approach for fair coexistence. The specification suggests reducing the duty
cycle at the LTE-U base-station (BS) when the number of co-channel Wi-Fi basic
service sets (BSSs) increases from one to two or more. However, without
decoding the Wi-Fi packets, detecting the number of Wi-Fi BSSs operating on the
channel in real-time is a challenging problem. In this work, we demonstrate a
novel ML-based approach which solves this problem by using energy values
observed during the LTE-U OFF duration. It is relatively straightforward to
observe only the energy values during the LTE-U BS OFF time compared to
decoding the entire Wi-Fi packet, which would require a full Wi-Fi receiver at
the LTE-U base-station. We implement and validate the proposed ML-based
approach by real-time experiments and demonstrate that there exist distinct
patterns between the energy distributions between one and many Wi-Fi AP
transmissions. The proposed ML-based approach results in a higher accuracy
(close to 99\% in all cases) as compared to the existing auto-correlation (AC)
and energy detection (ED) approaches.",arxiv
http://arxiv.org/abs/1804.00175v4,2019-10-02T00:54:47Z,2018-03-31T14:02:25Z,DeepIM: Deep Iterative Matching for 6D Pose Estimation,"Estimating the 6D pose of objects from images is an important problem in
various applications such as robot manipulation and virtual reality. While
direct regression of images to object poses has limited accuracy, matching
rendered images of an object against the observed image can produce accurate
results. In this work, we propose a novel deep neural network for 6D pose
matching named DeepIM. Given an initial pose estimation, our network is able to
iteratively refine the pose by matching the rendered image against the observed
image. The network is trained to predict a relative pose transformation using
an untangled representation of 3D location and 3D orientation and an iterative
training process. Experiments on two commonly used benchmarks for 6D pose
estimation demonstrate that DeepIM achieves large improvements over
state-of-the-art methods. We furthermore show that DeepIM is able to match
previously unseen objects.",arxiv
http://arxiv.org/abs/2010.13070v1,2020-10-25T08:55:40Z,2020-10-25T08:55:40Z,Dynamic Adversarial Patch for Evading Object Detection Models,"Recent research shows that neural networks models used for computer vision
(e.g., YOLO and Fast R-CNN) are vulnerable to adversarial evasion attacks. Most
of the existing real-world adversarial attacks against object detectors use an
adversarial patch which is attached to the target object (e.g., a carefully
crafted sticker placed on a stop sign). This method may not be robust to
changes in the camera's location relative to the target object; in addition, it
may not work well when applied to nonplanar objects such as cars. In this
study, we present an innovative attack method against object detectors applied
in a real-world setup that addresses some of the limitations of existing
attacks. Our method uses dynamic adversarial patches which are placed at
multiple predetermined locations on a target object. An adversarial learning
algorithm is applied in order to generate the patches used. The dynamic attack
is implemented by switching between optimized patches dynamically, according to
the camera's position (i.e., the object detection system's position). In order
to demonstrate our attack in a real-world setup, we implemented the patches by
attaching flat screens to the target object; the screens are used to present
the patches and switch between them, depending on the current camera location.
Thus, the attack is dynamic and adjusts itself to the situation to achieve
optimal results. We evaluated our dynamic patch approach by attacking the
YOLOv2 object detector with a car as the target object and succeeded in
misleading it in up to 90% of the video frames when filming the car from a wide
viewing angle range. We improved the attack by generating patches that consider
the semantic distance between the target object and its classification. We also
examined the attack's transferability among different car models and were able
to mislead the detector 71% of the time.",arxiv
http://arxiv.org/abs/2008.11331v1,2020-08-26T01:34:19Z,2020-08-26T01:34:19Z,Synthetic Sample Selection via Reinforcement Learning,"Synthesizing realistic medical images provides a feasible solution to the
shortage of training data in deep learning based medical image recognition
systems. However, the quality control of synthetic images for data augmentation
purposes is under-investigated, and some of the generated images are not
realistic and may contain misleading features that distort data distribution
when mixed with real images. Thus, the effectiveness of those synthetic images
in medical image recognition systems cannot be guaranteed when they are being
added randomly without quality assurance. In this work, we propose a
reinforcement learning (RL) based synthetic sample selection method that learns
to choose synthetic images containing reliable and informative features. A
transformer based controller is trained via proximal policy optimization (PPO)
using the validation classification accuracy as the reward. The selected images
are mixed with the original training data for improved training of image
recognition systems. To validate our method, we take the pathology image
recognition as an example and conduct extensive experiments on two
histopathology image datasets. In experiments on a cervical dataset and a lymph
node dataset, the image classification performance is improved by 8.1% and
2.3%, respectively, when utilizing high-quality synthetic images selected by
our RL framework. Our proposed synthetic sample selection method is general and
has great potential to boost the performance of various medical image
recognition systems given limited annotation.",arxiv
http://arxiv.org/abs/2011.10363v1,2020-11-20T12:05:50Z,2020-11-20T12:05:50Z,SophiaPop: Experiments in Human-AI Collaboration on Popular Music,"A diverse team of engineers, artists, and algorithms, collaborated to create
songs for SophiaPop, via various neural networks, robotics technologies, and
artistic tools, and animated the results on Sophia the Robot, a robotic
celebrity and animated character. Sophia is a platform for arts, research, and
other uses. To advance the art and technology of Sophia, we combine various AI
with a fictional narrative of her burgeoning career as a popstar. Her actual
AI-generated pop lyrics, music, and paintings, and animated conversations
wherein she interacts with humans real-time in narratives that discuss her
experiences. To compose the music, SophiaPop team built corpora from human and
AI-generated Sophia character personality content, along with pop music song
forms, to train and provide seeds for a number of AI algorithms including
expert models, and custom-trained transformer neural networks, which then
generated original pop-song lyrics and melodies. Our musicians including
Frankie Storm, Adam Pickrell, and Tiger Darrow, then performed interpretations
of the AI-generated musical content, including singing and instrumentation. The
human-performed singing data then was processed by a neural-network-based
Sophia voice, which was custom-trained from human performances by Cereproc.
This AI then generated the unique Sophia voice singing of the songs. Then we
animated Sophia to sing the songs in music videos, using a variety of animation
generators and human-generated animations. Being algorithms and humans, working
together, SophiaPop represents a human-AI collaboration, aspiring toward human
AI symbiosis. We believe that such a creative convergence of multiple
disciplines with humans and AI working together, can make AI relevant to human
culture in new and exciting ways, and lead to a hopeful vision for the future
of human-AI relations.",arxiv
http://arxiv.org/abs/1804.00209v2,2019-07-10T14:21:12Z,2018-03-31T20:07:37Z,"Human-in-the-Loop Wireless Communications: Machine Learning and
  Brain-Aware Resource Management","Human-centric applications such as virtual reality and immersive gaming will
be central to the future wireless networks. Common features of such services
include: a) their dependence on the human user's behavior and state, and b)
their need for more network resources compared to conventional cellular
applications. To successfully deploy such applications over wireless and
cellular systems, the network must be made cognizant of not only the
quality-of-service (QoS) needs of the applications, but also of the perceptions
of the human users on this QoS. In this paper, by explicitly modeling the
limitations of the human brain, a concrete measure for the delay perception of
human users in a wireless network is introduced. Then, a novel learning method,
called probability distribution identification, is proposed to find a
probabilistic model for this delay perception based on the brain features of a
human user. The proposed learning method uses both supervised and unsupervised
learning techniques to build a Gaussian mixture model of the human brain
features. Given a model for the delay perception of the human brain, a novel
brain-aware resource management algorithm based on Lyapunov optimization is
proposed for allocating radio resources to human users while minimizing the
transmit power and taking into account the reliability of both machine type
devices and human users. The proposed algorithm is shown to have a low
complexity. Moreover, a closed-form relationship between the reliability
measure and wireless physical layer metrics of the network is derived.
Simulation results using real data from actual human users show that a
brain-aware approach can yield savings of up to 78% in power compared to the
system",arxiv
http://arxiv.org/abs/1912.11350v1,2019-12-22T22:22:55Z,2019-12-22T22:22:55Z,Atmospheric turbulence removal using convolutional neural network,"This paper describes a novel deep learning-based method for mitigating the
effects of atmospheric distortion. We have built an end-to-end supervised
convolutional neural network (CNN) to reconstruct turbulence-corrupted video
sequence. Our framework has been developed on the residual learning concept,
where the spatio-temporal distortions are learnt and predicted. Our experiments
demonstrate that the proposed method can deblur, remove ripple effect and
enhance contrast of the video sequences simultaneously. Our model was trained
and tested with both simulated and real distortions. Experimental results of
the real distortions show that our method outperforms the existing ones by up
to 3.8% in term of the quality of restored images, and it achieves faster speed
than the state-of-the-art methods by up to 23 times with GPU implementation.",arxiv
http://arxiv.org/abs/2004.09548v1,2020-04-20T18:07:55Z,2020-04-20T18:07:55Z,AANet: Adaptive Aggregation Network for Efficient Stereo Matching,"Despite the remarkable progress made by learning based stereo matching
algorithms, one key challenge remains unsolved. Current state-of-the-art stereo
models are mostly based on costly 3D convolutions, the cubic computational
complexity and high memory consumption make it quite expensive to deploy in
real-world applications. In this paper, we aim at completely replacing the
commonly used 3D convolutions to achieve fast inference speed while maintaining
comparable accuracy. To this end, we first propose a sparse points based
intra-scale cost aggregation method to alleviate the well-known edge-fattening
issue at disparity discontinuities. Further, we approximate traditional
cross-scale cost aggregation algorithm with neural network layers to handle
large textureless regions. Both modules are simple, lightweight, and
complementary, leading to an effective and efficient architecture for cost
aggregation. With these two modules, we can not only significantly speed up
existing top-performing models (e.g., $41\times$ than GC-Net, $4\times$ than
PSMNet and $38\times$ than GA-Net), but also improve the performance of fast
stereo models (e.g., StereoNet). We also achieve competitive results on Scene
Flow and KITTI datasets while running at 62ms, demonstrating the versatility
and high efficiency of the proposed method. Our full framework is available at
https://github.com/haofeixu/aanet .",arxiv
http://arxiv.org/abs/2107.09822v2,2021-07-22T05:48:12Z,2021-07-21T00:43:32Z,"Bayesian Controller Fusion: Leveraging Control Priors in Deep
  Reinforcement Learning for Robotics","We present Bayesian Controller Fusion (BCF): a hybrid control strategy that
combines the strengths of traditional hand-crafted controllers and model-free
deep reinforcement learning (RL). BCF thrives in the robotics domain, where
reliable but suboptimal control priors exist for many tasks, but RL from
scratch remains unsafe and data-inefficient. By fusing uncertainty-aware
distributional outputs from each system, BCF arbitrates control between them,
exploiting their respective strengths. We study BCF on two real-world robotics
tasks involving navigation in a vast and long-horizon environment, and a
complex reaching task that involves manipulability maximisation. For both these
domains, there exist simple handcrafted controllers that can solve the task at
hand in a risk-averse manner but do not necessarily exhibit the optimal
solution given limitations in analytical modelling, controller miscalibration
and task variation. As exploration is naturally guided by the prior in the
early stages of training, BCF accelerates learning, while substantially
improving beyond the performance of the control prior, as the policy gains more
experience. More importantly, given the risk-aversity of the control prior, BCF
ensures safe exploration and deployment, where the control prior naturally
dominates the action distribution in states unknown to the policy. We
additionally show BCF's applicability to the zero-shot sim-to-real setting and
its ability to deal with out-of-distribution states in the real-world. BCF is a
promising approach for combining the complementary strengths of deep RL and
traditional robotic control, surpassing what either can achieve independently.
The code and supplementary video material are made publicly available at
https://krishanrana.github.io/bcf.",arxiv
http://arxiv.org/abs/2104.09886v1,2021-04-20T10:41:50Z,2021-04-20T10:41:50Z,"Lighting, Reflectance and Geometry Estimation from 360$^{\circ}$
  Panoramic Stereo","We propose a method for estimating high-definition spatially-varying
lighting, reflectance, and geometry of a scene from 360$^{\circ}$ stereo
images. Our model takes advantage of the 360$^{\circ}$ input to observe the
entire scene with geometric detail, then jointly estimates the scene's
properties with physical constraints. We first reconstruct a near-field
environment light for predicting the lighting at any 3D location within the
scene. Then we present a deep learning model that leverages the stereo
information to infer the reflectance and surface normal. Lastly, we incorporate
the physical constraints between lighting and geometry to refine the
reflectance of the scene. Both quantitative and qualitative experiments show
that our method, benefiting from the 360$^{\circ}$ observation of the scene,
outperforms prior state-of-the-art methods and enables more augmented reality
applications such as mirror-objects insertion.",arxiv
http://arxiv.org/abs/1805.06374v2,2018-05-19T15:10:37Z,2018-05-16T15:42:37Z,"Fast Retinomorphic Event Stream for Video Recognition and Reinforcement
  Learning","Good temporal representations are crucial for video understanding, and the
state-of-the-art video recognition framework is based on two-stream networks.
In such framework, besides the regular ConvNets responsible for RGB frame
inputs, a second network is introduced to handle the temporal representation,
usually the optical flow (OF). However, OF or other task-oriented flow is
computationally costly, and is thus typically pre-computed. Critically, this
prevents the two-stream approach from being applied to reinforcement learning
(RL) applications such as video game playing, where the next state depends on
current state and action choices. Inspired by the early vision systems of
mammals and insects, we propose a fast event-driven representation (EDR) that
models several major properties of early retinal circuits: (1) logarithmic
input response, (2) multi-timescale temporal smoothing to filter noise, and (3)
bipolar (ON/OFF) pathways for primitive event detection[12]. Trading off the
directional information for fast speed (> 9000 fps), EDR en-ables fast
real-time inference/learning in video applications that require interaction
between an agent and the world such as game-playing, virtual robotics, and
domain adaptation. In this vein, we use EDR to demonstrate performance
improvements over state-of-the-art reinforcement learning algorithms for Atari
games, something that has not been possible with pre-computed OF. Moreover,
with UCF-101 video action recognition experiments, we show that EDR performs
near state-of-the-art in accuracy while achieving a 1,500x speedup in input
representation processing, as compared to optical flow.",arxiv
http://arxiv.org/abs/1808.06170v1,2018-08-19T06:21:58Z,2018-08-19T06:21:58Z,Linked Recurrent Neural Networks,"Recurrent Neural Networks (RNNs) have been proven to be effective in modeling
sequential data and they have been applied to boost a variety of tasks such as
document classification, speech recognition and machine translation. Most of
existing RNN models have been designed for sequences assumed to be identically
and independently distributed (i.i.d). However, in many real-world
applications, sequences are naturally linked. For example, web documents are
connected by hyperlinks; and genes interact with each other. On the one hand,
linked sequences are inherently not i.i.d., which poses tremendous challenges
to existing RNN models. On the other hand, linked sequences offer link
information in addition to the sequential information, which enables
unprecedented opportunities to build advanced RNN models. In this paper, we
study the problem of RNN for linked sequences. In particular, we introduce a
principled approach to capture link information and propose a linked Recurrent
Neural Network (LinkedRNN), which models sequential and link information
coherently. We conduct experiments on real-world datasets from multiple domains
and the experimental results validate the effectiveness of the proposed
framework.",arxiv
http://arxiv.org/abs/2104.14392v3,2021-07-09T13:08:48Z,2021-04-29T15:09:44Z,"COSCO: Container Orchestration using Co-Simulation and Gradient Based
  Optimization for Fog Computing Environments","Intelligent task placement and management of tasks in large-scale fog
platforms is challenging due to the highly volatile nature of modern workload
applications and sensitive user requirements of low energy consumption and
response time. Container orchestration platforms have emerged to alleviate this
problem with prior art either using heuristics to quickly reach scheduling
decisions or AI driven methods like reinforcement learning and evolutionary
approaches to adapt to dynamic scenarios. The former often fail to quickly
adapt in highly dynamic environments, whereas the latter have run-times that
are slow enough to negatively impact response time. Therefore, there is a need
for scheduling policies that are both reactive to work efficiently in volatile
environments and have low scheduling overheads. To achieve this, we propose a
Gradient Based Optimization Strategy using Back-propagation of gradients with
respect to Input (GOBI). Further, we leverage the accuracy of predictive
digital-twin models and simulation capabilities by developing a Coupled
Simulation and Container Orchestration Framework (COSCO). Using this, we create
a hybrid simulation driven decision approach, GOBI*, to optimize Quality of
Service (QoS) parameters. Co-simulation and the back-propagation approaches
allow these methods to adapt quickly in volatile environments. Experiments
conducted using real-world data on fog applications using the GOBI and GOBI*
methods, show a significant improvement in terms of energy consumption,
response time, Service Level Objective and scheduling time by up to 15, 40, 4,
and 82 percent respectively when compared to the state-of-the-art algorithms.",arxiv
http://arxiv.org/abs/2005.06616v1,2020-05-06T02:45:43Z,2020-05-06T02:45:43Z,"A Large-Scale, Open-Domain, Mixed-Interface Dialogue-Based ITS for STEM","We present Korbit, a large-scale, open-domain, mixed-interface,
dialogue-based intelligent tutoring system (ITS). Korbit uses machine learning,
natural language processing and reinforcement learning to provide interactive,
personalized learning online. Korbit has been designed to easily scale to
thousands of subjects, by automating, standardizing and simplifying the content
creation process. Unlike other ITS, a teacher can develop new learning modules
for Korbit in a matter of hours. To facilitate learning across a widerange of
STEM subjects, Korbit uses a mixed-interface, which includes videos,
interactive dialogue-based exercises, question-answering, conceptual diagrams,
mathematical exercises and gamification elements. Korbit has been built to
scale to millions of students, by utilizing a state-of-the-art cloud-based
micro-service architecture. Korbit launched its first course in 2019 on machine
learning, and since then over 7,000 students have enrolled. Although Korbit was
designed to be open-domain and highly scalable, A/B testing experiments with
real-world students demonstrate that both student learning outcomes and student
motivation are substantially improved compared to typical online courses.",arxiv
http://arxiv.org/abs/2106.12605v1,2021-06-23T18:08:07Z,2021-06-23T18:08:07Z,Deep Fake Detection: Survey of Facial Manipulation Detection Solutions,"Deep Learning as a field has been successfully used to solve a plethora of
complex problems, the likes of which we could not have imagined a few decades
back. But as many benefits as it brings, there are still ways in which it can
be used to bring harm to our society. Deep fakes have been proven to be one
such problem, and now more than ever, when any individual can create a fake
image or video simply using an application on the smartphone, there need to be
some countermeasures, with which we can detect if the image or video is a fake
or real and dispose of the problem threatening the trustworthiness of online
information. Although the Deep fakes created by neural networks, may seem to be
as real as a real image or video, it still leaves behind spatial and temporal
traces or signatures after moderation, these signatures while being invisible
to a human eye can be detected with the help of a neural network trained to
specialize in Deep fake detection. In this paper, we analyze several such
states of the art neural networks (MesoNet, ResNet-50, VGG-19, and Xception
Net) and compare them against each other, to find an optimal solution for
various scenarios like real-time deep fake detection to be deployed in online
social media platforms where the classification should be made as fast as
possible or for a small news agency where the classification need not be in
real-time but requires utmost accuracy.",arxiv
http://arxiv.org/abs/1808.07301v1,2018-08-22T10:16:43Z,2018-08-22T10:16:43Z,"Deep Association Learning for Unsupervised Video Person
  Re-identification","Deep learning methods have started to dominate the research progress of
video-based person re-identification (re-id). However, existing methods mostly
consider supervised learning, which requires exhaustive manual efforts for
labelling cross-view pairwise data. Therefore, they severely lack scalability
and practicality in real-world video surveillance applications. In this work,
to address the video person re-id task, we formulate a novel Deep Association
Learning (DAL) scheme, the first end-to-end deep learning method using none of
the identity labels in model initialisation and training. DAL learns a deep
re-id matching model by jointly optimising two margin-based association losses
in an end-to-end manner, which effectively constrains the association of each
frame to the best-matched intra-camera representation and cross-camera
representation. Existing standard CNNs can be readily employed within our DAL
scheme. Experiment results demonstrate that our proposed DAL significantly
outperforms current state-of-the-art unsupervised video person re-id methods on
three benchmarks: PRID 2011, iLIDS-VID and MARS.",arxiv
http://arxiv.org/abs/1903.09254v4,2019-04-05T22:52:17Z,2019-03-21T22:03:25Z,"CityFlow: A City-Scale Benchmark for Multi-Target Multi-Camera Vehicle
  Tracking and Re-Identification","Urban traffic optimization using traffic cameras as sensors is driving the
need to advance state-of-the-art multi-target multi-camera (MTMC) tracking.
This work introduces CityFlow, a city-scale traffic camera dataset consisting
of more than 3 hours of synchronized HD videos from 40 cameras across 10
intersections, with the longest distance between two simultaneous cameras being
2.5 km. To the best of our knowledge, CityFlow is the largest-scale dataset in
terms of spatial coverage and the number of cameras/videos in an urban
environment. The dataset contains more than 200K annotated bounding boxes
covering a wide range of scenes, viewing angles, vehicle models, and urban
traffic flow conditions. Camera geometry and calibration information are
provided to aid spatio-temporal analysis. In addition, a subset of the
benchmark is made available for the task of image-based vehicle
re-identification (ReID). We conducted an extensive experimental evaluation of
baselines/state-of-the-art approaches in MTMC tracking, multi-target
single-camera (MTSC) tracking, object detection, and image-based ReID on this
dataset, analyzing the impact of different network architectures, loss
functions, spatio-temporal models and their combinations on task effectiveness.
An evaluation server is launched with the release of our benchmark at the 2019
AI City Challenge (https://www.aicitychallenge.org/) that allows researchers to
compare the performance of their newest techniques. We expect this dataset to
catalyze research in this field, propel the state-of-the-art forward, and lead
to deployed traffic optimization(s) in the real world.",arxiv
http://arxiv.org/abs/2001.00487v1,2020-01-02T15:22:36Z,2020-01-02T15:22:36Z,"Using CNNs For Users Segmentation In Video See-Through Augmented
  Virtuality","In this paper, we present preliminary results on the use of deep learning
techniques to integrate the users self-body and other participants into a
head-mounted video see-through augmented virtuality scenario. It has been
previously shown that seeing users bodies in such simulations may improve the
feeling of both self and social presence in the virtual environment, as well as
user performance. We propose to use a convolutional neural network for real
time semantic segmentation of users bodies in the stereoscopic RGB video
streams acquired from the perspective of the user. We describe design issues as
well as implementation details of the system and demonstrate the feasibility of
using such neural networks for merging users bodies in an augmented virtuality
simulation.",arxiv
http://arxiv.org/abs/2104.09056v1,2021-04-19T05:26:11Z,2021-04-19T05:26:11Z,"RingCNN: Exploiting Algebraically-Sparse Ring Tensors for
  Energy-Efficient CNN-Based Computational Imaging","In the era of artificial intelligence, convolutional neural networks (CNNs)
are emerging as a powerful technique for computational imaging. They have shown
superior quality for reconstructing fine textures from badly-distorted images
and have potential to bring next-generation cameras and displays to our daily
life. However, CNNs demand intensive computing power for generating
high-resolution videos and defy conventional sparsity techniques when rendering
dense details. Therefore, finding new possibilities in regular sparsity is
crucial to enable large-scale deployment of CNN-based computational imaging.
  In this paper, we consider a fundamental but yet well-explored approach --
algebraic sparsity -- for energy-efficient CNN acceleration. We propose to
build CNN models based on ring algebra that defines multiplication, addition,
and non-linearity for n-tuples properly. Then the essential sparsity will
immediately follow, e.g. n-times reduction for the number of real-valued
weights. We define and unify several variants of ring algebras into a modeling
framework, RingCNN, and make comparisons in terms of image quality and hardware
complexity. On top of that, we further devise a novel ring algebra which
minimizes complexity with component-wise product and achieves the best quality
using directional ReLU. Finally, we implement an accelerator, eRingCNN, in two
settings, n=2 and 4 (50% and 75% sparsity), with 40 nm technology to support
advanced denoising and super-resolution at up to 4K UHD 30 fps. Layout results
show that they can deliver equivalent 41 TOPS using 3.76 W and 2.22 W,
respectively. Compared to the real-valued counterpart, our ring convolution
engines for n=2 achieve 2.00x energy efficiency and 2.08x area efficiency with
similar or even better image quality. With n=4, the efficiency gains of energy
and area are further increased to 3.84x and 3.77x with 0.11 dB drop of PSNR.",arxiv
http://arxiv.org/abs/2010.05502v2,2020-10-13T05:58:43Z,2020-10-12T07:56:03Z,A Lightweight Speaker Recognition System Using Timbre Properties,"Speaker recognition is an active research area that contains notable usage in
biometric security and authentication system. Currently, there exist many
well-performing models in the speaker recognition domain. However, most of the
advanced models implement deep learning that requires GPU support for real-time
speech recognition, and it is not suitable for low-end devices. In this paper,
we propose a lightweight text-independent speaker recognition model based on
random forest classifier. It also introduces new features that are used for
both speaker verification and identification tasks. The proposed model uses
human speech based timbral properties as features that are classified using
random forest. Timbre refers to the very basic properties of sound that allow
listeners to discriminate among them. The prototype uses seven most actively
searched timbre properties, boominess, brightness, depth, hardness, roughness,
sharpness, and warmth as features of our speaker recognition model. The
experiment is carried out on speaker verification and speaker identification
tasks and shows the achievements and drawbacks of the proposed model. In the
speaker identification phase, it achieves a maximum accuracy of 78%. On the
contrary, in the speaker verification phase, the model maintains an accuracy of
80% having an equal error rate (ERR) of 0.24.",arxiv
http://arxiv.org/abs/1604.03276v1,2016-04-12T07:52:27Z,2016-04-12T07:52:27Z,"Noise Robust Speech Recognition Using Multi-Channel Based Channel
  Selection And ChannelWeighting","In this paper, we study several microphone channel selection and weighting
methods for robust automatic speech recognition (ASR) in noisy conditions. For
channel selection, we investigate two methods based on the maximum likelihood
(ML) criterion and minimum autoencoder reconstruction criterion, respectively.
For channel weighting, we produce enhanced log Mel filterbank coefficients as a
weighted sum of the coefficients of all channels. The weights of the channels
are estimated by using the ML criterion with constraints. We evaluate the
proposed methods on the CHiME-3 noisy ASR task. Experiments show that channel
weighting significantly outperforms channel selection due to its higher
flexibility. Furthermore, on real test data in which different channels have
different gains of the target signal, the channel weighting method performs
equally well or better than the MVDR beamforming, despite the fact that the
channel weighting does not make use of the phase delay information which is
normally used in beamforming.",arxiv
http://arxiv.org/abs/2006.13378v2,2020-08-02T16:06:12Z,2020-06-23T23:11:30Z,A Benchmarking Framework for Interactive 3D Applications in the Cloud,"With the growing popularity of cloud gaming and cloud virtual reality (VR),
interactive 3D applications have become a major type of workloads for the
cloud. However, despite their growing importance, there is limited public
research on how to design cloud systems to efficiently support these
applications, due to the lack of an open and reliable research infrastructure,
including benchmarks and performance analysis tools. The challenges of
generating human-like inputs under various system/application randomness and
dissecting the performance of complex graphics systems make it very difficult
to design such an infrastructure. In this paper, we present the design of a
novel cloud graphics rendering research infrastructure, Pictor. Pictor employs
AI to mimic human interactions with complex 3D applications. It can also
provide in-depth performance measurements for the complex software and hardware
stack used for cloud 3D graphics rendering. With Pictor, we designed a
benchmark suite with six interactive 3D applications. Performance analyses were
conducted with these benchmarks to characterize 3D applications in the cloud
and reveal new performance bottlenecks. To demonstrate the effectiveness of
Pictor, we also implemented two optimizations to address two performance
bottlenecks discovered in a state-of-the-art cloud 3D-graphics rendering
system, which improved the frame rate by 57.7% on average.",arxiv
http://arxiv.org/abs/2103.11799v1,2021-03-14T16:11:30Z,2021-03-14T16:11:30Z,DeepHate: Hate Speech Detection via Multi-Faceted Text Representations,"Online hate speech is an important issue that breaks the cohesiveness of
online social communities and even raises public safety concerns in our
societies. Motivated by this rising issue, researchers have developed many
traditional machine learning and deep learning methods to detect hate speech in
online social platforms automatically. However, most of these methods have only
considered single type textual feature, e.g., term frequency, or using word
embeddings. Such approaches neglect the other rich textual information that
could be utilized to improve hate speech detection. In this paper, we propose
DeepHate, a novel deep learning model that combines multi-faceted text
representations such as word embeddings, sentiments, and topical information,
to detect hate speech in online social platforms. We conduct extensive
experiments and evaluate DeepHate on three large publicly available real-world
datasets. Our experiment results show that DeepHate outperforms the
state-of-the-art baselines on the hate speech detection task. We also perform
case studies to provide insights into the salient features that best aid in
detecting hate speech in online social platforms.",arxiv
http://arxiv.org/abs/2007.07132v1,2020-07-14T15:51:52Z,2020-07-14T15:51:52Z,"A Deep Learning Approach for Low-Latency Packet Loss Concealment of
  Audio Signals in Networked Music Performance Applications","Networked Music Performance (NMP) is envisioned as a potential game changer
among Internet applications: it aims at revolutionizing the traditional concept
of musical interaction by enabling remote musicians to interact and perform
together through a telecommunication network. Ensuring realistic conditions for
music performance, however, constitutes a significant engineering challenge due
to extremely strict requirements in terms of audio quality and, most
importantly, network delay. To minimize the end-to-end delay experienced by the
musicians, typical implementations of NMP applications use un-compressed,
bidirectional audio streams and leverage UDP as transport protocol. Being
connection less and unreliable,audio packets transmitted via UDP which become
lost in transit are not re-transmitted and thus cause glitches in the receiver
audio playout. This article describes a technique for predicting lost packet
content in real-time using a deep learning approach. The ability of concealing
errors in real time can help mitigate audio impairments caused by packet
losses, thus improving the quality of audio playout in real-world scenarios.",arxiv
http://arxiv.org/abs/2105.11494v1,2021-05-24T18:40:18Z,2021-05-24T18:40:18Z,3D-Aware Ellipse Prediction for Object-Based Camera Pose Estimation,"In this paper, we propose a method for coarse camera pose computation which
is robust to viewing conditions and does not require a detailed model of the
scene. This method meets the growing need of easy deployment of robotics or
augmented reality applications in any environments, especially those for which
no accurate 3D model nor huge amount of ground truth data are available. It
exploits the ability of deep learning techniques to reliably detect objects
regardless of viewing conditions. Previous works have also shown that
abstracting the geometry of a scene of objects by an ellipsoid cloud allows to
compute the camera pose accurately enough for various application needs. Though
promising, these approaches use the ellipses fitted to the detection bounding
boxes as an approximation of the imaged objects. In this paper, we go one step
further and propose a learning-based method which detects improved elliptic
approximations of objects which are coherent with the 3D ellipsoid in terms of
perspective projection. Experiments prove that the accuracy of the computed
pose significantly increases thanks to our method and is more robust to the
variability of the boundaries of the detection boxes. This is achieved with
very little effort in terms of training data acquisition -- a few hundred
calibrated images of which only three need manual object annotation. Code and
models are released at
https://github.com/zinsmatt/3D-Aware-Ellipses-for-Visual-Localization.",arxiv
http://arxiv.org/abs/2108.03173v1,2021-08-04T09:03:53Z,2021-08-04T09:03:53Z,"Incremental learning of LSTM framework for sensor fusion in attitude
  estimation","This paper presents a novel method for attitude estimation of an object in 3D
space by incremental learning of the Long-Short Term Memory (LSTM) network.
Gyroscope, accelerometer, and magnetometer are few widely used sensors in
attitude estimation applications. Traditionally, multi-sensor fusion methods
such as the Extended Kalman Filter and Complementary Filter are employed to
fuse the measurements from these sensors. However, these methods exhibit
limitations in accounting for the uncertainty, unpredictability, and dynamic
nature of the motion in real-world situations. In this paper, the inertial
sensors data are fed to the LSTM network which are then updated incrementally
to incorporate the dynamic changes in motion occurring in the run time. The
robustness and efficiency of the proposed framework is demonstrated on the
dataset collected from a commercially available inertial measurement unit. The
proposed framework offers a significant improvement in the results compared to
the traditional method, even in the case of a highly dynamic environment. The
LSTM framework-based attitude estimation approach can be deployed on a standard
AI-supported processing module for real-time applications.",arxiv
http://arxiv.org/abs/1808.03841v1,2018-08-11T17:33:40Z,2018-08-11T17:33:40Z,"Fully Distributed Multi-Robot Collision Avoidance via Deep Reinforcement
  Learning for Safe and Efficient Navigation in Complex Scenarios","In this paper, we present a decentralized sensor-level collision avoidance
policy for multi-robot systems, which shows promising results in practical
applications. In particular, our policy directly maps raw sensor measurements
to an agent's steering commands in terms of the movement velocity. As a first
step toward reducing the performance gap between decentralized and centralized
methods, we present a multi-scenario multi-stage training framework to learn an
optimal policy. The policy is trained over a large number of robots in rich,
complex environments simultaneously using a policy gradient based reinforcement
learning algorithm. The learning algorithm is also integrated into a hybrid
control framework to further improve the policy's robustness and effectiveness.
  We validate the learned sensor-level collision avoidance policy in a variety
of simulated and real-world scenarios with thorough performance evaluations for
large-scale multi-robot systems. The generalization of the learned policy is
verified in a set of unseen scenarios including the navigation of a group of
heterogeneous robots and a large-scale scenario with 100 robots. Although the
policy is trained using simulation data only, we have successfully deployed it
on physical robots with shapes and dynamics characteristics that are different
from the simulated agents, in order to demonstrate the controller's robustness
against the sim-to-real modeling error. Finally, we show that the
collision-avoidance policy learned from multi-robot navigation tasks provides
an excellent solution to the safe and effective autonomous navigation for a
single robot working in a dense real human crowd. Our learned policy enables a
robot to make effective progress in a crowd without getting stuck. Videos are
available at https://sites.google.com/view/hybridmrca",arxiv
http://arxiv.org/abs/2007.01793v1,2020-07-03T16:32:14Z,2020-07-03T16:32:14Z,"CacheNet: A Model Caching Framework for Deep Learning Inference on the
  Edge","The success of deep neural networks (DNN) in machine perception applications
such as image classification and speech recognition comes at the cost of high
computation and storage complexity. Inference of uncompressed large scale DNN
models can only run in the cloud with extra communication latency back and
forth between cloud and end devices, while compressed DNN models achieve
real-time inference on end devices at the price of lower predictive accuracy.
In order to have the best of both worlds (latency and accuracy), we propose
CacheNet, a model caching framework. CacheNet caches low-complexity models on
end devices and high-complexity (or full) models on edge or cloud servers. By
exploiting temporal locality in streaming data, high cache hit and consequently
shorter latency can be achieved with no or only marginal decrease in prediction
accuracy. Experiments on CIFAR-10 and FVG have shown CacheNet is 58-217% faster
than baseline approaches that run inference tasks on end devices or edge
servers alone.",arxiv
http://arxiv.org/abs/2107.11646v2,2021-07-28T07:16:15Z,2021-07-24T16:28:06Z,Hand Image Understanding via Deep Multi-Task Learning,"Analyzing and understanding hand information from multimedia materials like
images or videos is important for many real world applications and remains
active in research community. There are various works focusing on recovering
hand information from single image, however, they usually solve a single task,
for example, hand mask segmentation, 2D/3D hand pose estimation, or hand mesh
reconstruction and perform not well in challenging scenarios. To further
improve the performance of these tasks, we propose a novel Hand Image
Understanding (HIU) framework to extract comprehensive information of the hand
object from a single RGB image, by jointly considering the relationships
between these tasks. To achieve this goal, a cascaded multi-task learning (MTL)
backbone is designed to estimate the 2D heat maps, to learn the segmentation
mask, and to generate the intermediate 3D information encoding, followed by a
coarse-to-fine learning paradigm and a self-supervised learning strategy.
Qualitative experiments demonstrate that our approach is capable of recovering
reasonable mesh representations even in challenging situations. Quantitatively,
our method significantly outperforms the state-of-the-art approaches on various
widely-used datasets, in terms of diverse evaluation metrics.",arxiv
http://arxiv.org/abs/2009.08044v2,2020-12-03T20:51:47Z,2020-09-17T03:38:28Z,Large-Scale Intelligent Microservices,"Deploying Machine Learning (ML) algorithms within databases is a challenge
due to the varied computational footprints of modern ML algorithms and the
myriad of database technologies each with its own restrictive syntax. We
introduce an Apache Spark-based micro-service orchestration framework that
extends database operations to include web service primitives. Our system can
orchestrate web services across hundreds of machines and takes full advantage
of cluster, thread, and asynchronous parallelism. Using this framework, we
provide large scale clients for intelligent services such as speech, vision,
search, anomaly detection, and text analysis. This allows users to integrate
ready-to-use intelligence into any datastore with an Apache Spark connector. To
eliminate the majority of overhead from network communication, we also
introduce a low-latency containerized version of our architecture. Finally, we
demonstrate that the services we investigate are competitive on a variety of
benchmarks, and present two applications of this framework to create
intelligent search engines, and real-time auto race analytics systems.",arxiv
http://arxiv.org/abs/1904.00923v1,2019-04-01T15:51:12Z,2019-04-01T15:51:12Z,Robustness of 3D Deep Learning in an Adversarial Setting,"Understanding the spatial arrangement and nature of real-world objects is of
paramount importance to many complex engineering tasks, including autonomous
navigation. Deep learning has revolutionized state-of-the-art performance for
tasks in 3D environments; however, relatively little is known about the
robustness of these approaches in an adversarial setting. The lack of
comprehensive analysis makes it difficult to justify deployment of 3D deep
learning models in real-world, safety-critical applications. In this work, we
develop an algorithm for analysis of pointwise robustness of neural networks
that operate on 3D data. We show that current approaches presented for
understanding the resilience of state-of-the-art models vastly overestimate
their robustness. We then use our algorithm to evaluate an array of
state-of-the-art models in order to demonstrate their vulnerability to
occlusion attacks. We show that, in the worst case, these networks can be
reduced to 0% classification accuracy after the occlusion of at most 6.5% of
the occupied input space.",arxiv
http://arxiv.org/abs/1603.07341v1,2016-03-23T20:13:11Z,2016-03-23T20:13:11Z,"Acceleration of Deep Neural Network Training with Resistive Cross-Point
  Devices","In recent years, deep neural networks (DNN) have demonstrated significant
business impact in large scale analysis and classification tasks such as speech
recognition, visual object detection, pattern extraction, etc. Training of
large DNNs, however, is universally considered as time consuming and
computationally intensive task that demands datacenter-scale computational
resources recruited for many days. Here we propose a concept of resistive
processing unit (RPU) devices that can potentially accelerate DNN training by
orders of magnitude while using much less power. The proposed RPU device can
store and update the weight values locally thus minimizing data movement during
training and allowing to fully exploit the locality and the parallelism of the
training algorithm. We identify the RPU device and system specifications for
implementation of an accelerator chip for DNN training in a realistic
CMOS-compatible technology. For large DNNs with about 1 billion weights this
massively parallel RPU architecture can achieve acceleration factors of 30,000X
compared to state-of-the-art microprocessors while providing power efficiency
of 84,000 GigaOps/s/W. Problems that currently require days of training on a
datacenter-size cluster with thousands of machines can be addressed within
hours on a single RPU accelerator. A system consisted of a cluster of RPU
accelerators will be able to tackle Big Data problems with trillions of
parameters that is impossible to address today like, for example, natural
speech recognition and translation between all world languages, real-time
analytics on large streams of business and scientific data, integration and
analysis of multimodal sensory data flows from massive number of IoT (Internet
of Things) sensors.",arxiv
http://arxiv.org/abs/2007.06343v2,2020-08-01T11:10:52Z,2020-07-13T12:30:31Z,"AirCapRL: Autonomous Aerial Human Motion Capture using Deep
  Reinforcement Learning","In this letter, we introduce a deep reinforcement learning (RL) based
multi-robot formation controller for the task of autonomous aerial human motion
capture (MoCap). We focus on vision-based MoCap, where the objective is to
estimate the trajectory of body pose and shape of a single moving person using
multiple micro aerial vehicles. State-of-the-art solutions to this problem are
based on classical control methods, which depend on hand-crafted system and
observation models. Such models are difficult to derive and generalize across
different systems. Moreover, the non-linearity and non-convexities of these
models lead to sub-optimal controls. In our work, we formulate this problem as
a sequential decision making task to achieve the vision-based motion capture
objectives, and solve it using a deep neural network-based RL method. We
leverage proximal policy optimization (PPO) to train a stochastic decentralized
control policy for formation control. The neural network is trained in a
parallelized setup in synthetic environments. We performed extensive simulation
experiments to validate our approach. Finally, real-robot experiments
demonstrate that our policies generalize to real world conditions. Video Link:
https://bit.ly/38SJfjo Supplementary: https://bit.ly/3evfo1O",arxiv
http://arxiv.org/abs/2011.10834v1,2020-11-21T18:00:28Z,2020-11-21T18:00:28Z,"Exploring the multimodal information from video content using deep
  learning features of appearance, audio and action for video recommendation","Following the popularisation of media streaming, a number of video streaming
services are continuously buying new video content to mine the potential profit
from them. As such, the newly added content has to be handled well to be
recommended to suitable users. In this paper, we address the new item
cold-start problem by exploring the potential of various deep learning features
to provide video recommendations. The deep learning features investigated
include features that capture the visual-appearance, audio and motion
information from video content. We also explore different fusion methods to
evaluate how well these feature modalities can be combined to fully exploit the
complementary information captured by them. Experiments on a real-world video
dataset for movie recommendations show that deep learning features outperform
hand-crafted features. In particular, recommendations generated with deep
learning audio features and action-centric deep learning features are superior
to MFCC and state-of-the-art iDT features. In addition, the combination of
various deep learning features with hand-crafted features and textual metadata
yields significant improvement in recommendations compared to combining only
the former.",arxiv
http://arxiv.org/abs/2011.09081v1,2020-11-18T04:17:30Z,2020-11-18T04:17:30Z,Multi-Channel Automatic Speech Recognition Using Deep Complex Unet,"The front-end module in multi-channel automatic speech recognition (ASR)
systems mainly use microphone array techniques to produce enhanced signals in
noisy conditions with reverberation and echos. Recently, neural network (NN)
based front-end has shown promising improvement over the conventional signal
processing methods. In this paper, we propose to adopt the architecture of deep
complex Unet (DCUnet) - a powerful complex-valued Unet-structured speech
enhancement model - as the front-end of the multi-channel acoustic model, and
integrate them in a multi-task learning (MTL) framework along with cascaded
framework for comparison. Meanwhile, we investigate the proposed methods with
several training strategies to improve the recognition accuracy on the
1000-hours real-world XiaoMi smart speaker data with echos. Experiments show
that our proposed DCUnet-MTL method brings about 12.2% relative character error
rate (CER) reduction compared with the traditional approach with array
processing plus single-channel acoustic model. It also achieves superior
performance than the recently proposed neural beamforming method.",arxiv
http://arxiv.org/abs/1906.01113v2,2019-09-23T21:25:56Z,2019-06-03T22:45:44Z,Learning in situ: a randomized experiment in video streaming,"We describe the results of a randomized controlled trial of video-streaming
algorithms for bitrate selection and network prediction. Over the last eight
months, we have streamed 14.2 years of video to 56,000 users across the
Internet. Sessions are randomized in blinded fashion among algorithms, and
client telemetry is recorded for analysis.
  We found that in this real-world setting, it is difficult for sophisticated
or machine-learned control schemes to outperform a ""simple"" scheme
(buffer-based control), notwithstanding good performance in network emulators
or simulators. We performed a statistical analysis and found that the
variability and heavy-tailed nature of network and algorithm behavior create
hurdles for robust learned algorithms in this area.
  We developed an ABR algorithm that robustly outperforms other schemes in
practice, by combining classical control with a learned network predictor,
trained with supervised learning in situ on data from the real deployment
environment.
  To support further investigation, we are publishing an archive of traces and
results each day, and will open our ongoing study to the community. We welcome
other researchers to use this platform to develop and validate new algorithms
for bitrate selection, network prediction, and congestion control.",arxiv
http://arxiv.org/abs/1802.03515v5,2018-11-11T04:53:47Z,2018-02-10T03:56:19Z,Vehicle Pose and Shape Estimation through Multiple Monocular Vision,"In this paper, we present an accurate approach to estimate vehicles' pose and
shape from off-board multiview images. The images are taken by monocular
cameras and have small overlaps. We utilize state-of-the-art convolutional
neural networks (CNNs) to extract vehicles' semantic keypoints and introduce a
Cross Projection Optimization (CPO) method to estimate the 3D pose. During the
iterative CPO process, an adaptive shape adjustment method named Hierarchical
Wireframe Constraint (HWC) is implemented to estimate the shape. Our approach
is evaluated under both simulated and real-world scenes for performance
verification. It's shown that our algorithm outperforms other existing
monocular and stereo methods for vehicles' pose and shape estimation. This
approach provides a new and robust solution for off-board visual vehicle
localization and tracking, which can be applied to massive surveillance camera
networks for intelligent transportation.",arxiv
http://arxiv.org/abs/1808.02992v1,2018-08-09T01:38:01Z,2018-08-09T01:38:01Z,"Controllable Image-to-Video Translation: A Case Study on Facial
  Expression Generation","The recent advances in deep learning have made it possible to generate
photo-realistic images by using neural networks and even to extrapolate video
frames from an input video clip. In this paper, for the sake of both furthering
this exploration and our own interest in a realistic application, we study
image-to-video translation and particularly focus on the videos of facial
expressions. This problem challenges the deep neural networks by another
temporal dimension comparing to the image-to-image translation. Moreover, its
single input image fails most existing video generation methods that rely on
recurrent models. We propose a user-controllable approach so as to generate
video clips of various lengths from a single face image. The lengths and types
of the expressions are controlled by users. To this end, we design a novel
neural network architecture that can incorporate the user input into its skip
connections and propose several improvements to the adversarial training method
for the neural network. Experiments and user studies verify the effectiveness
of our approach. Especially, we would like to highlight that even for the face
images in the wild (downloaded from the Web and the authors' own photos), our
model can generate high-quality facial expression videos of which about 50\%
are labeled as real by Amazon Mechanical Turk workers.",arxiv
http://arxiv.org/abs/1805.00313v1,2018-04-17T01:26:48Z,2018-04-17T01:26:48Z,Neural Compatibility Modeling with Attentive Knowledge Distillation,"Recently, the booming fashion sector and its huge potential benefits have
attracted tremendous attention from many research communities. In particular,
increasing research efforts have been dedicated to the complementary clothing
matching as matching clothes to make a suitable outfit has become a daily
headache for many people, especially those who do not have the sense of
aesthetics. Thanks to the remarkable success of neural networks in various
applications such as image classification and speech recognition, the
researchers are enabled to adopt the data-driven learning methods to analyze
fashion items. Nevertheless, existing studies overlook the rich valuable
knowledge (rules) accumulated in fashion domain, especially the rules regarding
clothing matching. Towards this end, in this work, we shed light on
complementary clothing matching by integrating the advanced deep neural
networks and the rich fashion domain knowledge. Considering that the rules can
be fuzzy and different rules may have different confidence levels to different
samples, we present a neural compatibility modeling scheme with attentive
knowledge distillation based on the teacher-student network scheme. Extensive
experiments on the real-world dataset show the superiority of our model over
several state-of-the-art baselines. Based upon the comparisons, we observe
certain fashion insights that add value to the fashion matching study. As a
byproduct, we released the codes, and involved parameters to benefit other
researchers.",arxiv
http://arxiv.org/abs/2011.02833v3,2020-12-04T13:34:23Z,2020-11-02T19:08:49Z,"Digital Twins: State of the Art Theory and Practice, Challenges, and
  Open Research Questions","Digital Twin was introduced over a decade ago, as an innovative
all-encompassing tool, with perceived benefits including real-time monitoring,
simulation and forecasting. However, the theoretical framework and practical
implementations of digital twins (DT) are still far from this vision. Although
successful implementations exist, sufficient implementation details are not
publicly available, therefore it is difficult to assess their effectiveness,
draw comparisons and jointly advance the DT methodology. This work explores the
various DT features and current approaches, the shortcomings and reasons behind
the delay in the implementation and adoption of digital twin. Advancements in
machine learning, internet of things and big data have contributed hugely to
the improvements in DT with regards to its real-time monitoring and forecasting
properties. Despite this progress and individual company-based efforts, certain
research gaps exist in the field, which have caused delay in the widespread
adoption of this concept. We reviewed relevant works and identified that the
major reasons for this delay are the lack of a universal reference framework,
domain dependence, security concerns of shared data, reliance of digital twin
on other technologies, and lack of quantitative metrics. We define the
necessary components of a digital twin required for a universal reference
framework, which also validate its uniqueness as a concept compared to similar
concepts like simulation, autonomous systems, etc. This work further assesses
the digital twin applications in different domains and the current state of
machine learning and big data in it. It thus answers and identifies novel
research questions, both of which will help to better understand and advance
the theory and practice of digital twins.",arxiv
http://arxiv.org/abs/2001.00391v1,2020-01-02T11:12:50Z,2020-01-02T11:12:50Z,"Temporal-Spatial Neural Filter: Direction Informed End-to-End
  Multi-channel Target Speech Separation","Target speech separation refers to extracting the target speaker's speech
from mixed signals. Despite the recent advances in deep learning based
close-talk speech separation, the applications to real-world are still an open
issue. Two main challenges are the complex acoustic environment and the
real-time processing requirement. To address these challenges, we propose a
temporal-spatial neural filter, which directly estimates the target speech
waveform from multi-speaker mixture in reverberant environments, assisted with
directional information of the speaker(s). Firstly, against variations brought
by complex environment, the key idea is to increase the acoustic representation
completeness through the jointly modeling of temporal, spectral and spatial
discriminability between the target and interference source. Specifically,
temporal, spectral, spatial along with the designed directional features are
integrated to create a joint acoustic representation. Secondly, to reduce the
latency, we design a fully-convolutional autoencoder framework, which is purely
end-to-end and single-pass. All the feature computation is implemented by the
network layers and operations to speed up the separation procedure. Evaluation
is conducted on simulated reverberant dataset WSJ0-2mix and WSJ0-3mix under
speaker-independent scenario. Experimental results demonstrate that the
proposed method outperforms state-of-the-art deep learning based multi-channel
approaches with fewer parameters and faster processing speed. Furthermore, the
proposed temporal-spatial neural filter can handle mixtures with varying and
unknown number of speakers and exhibits persistent performance even when
existing a direction estimation error. Codes and models will be released soon.",arxiv
http://arxiv.org/abs/1611.02695v1,2016-11-08T09:50:30Z,2016-11-08T09:50:30Z,"Automatic recognition of child speech for robotic applications in noisy
  environments","Automatic speech recognition (ASR) allows a natural and intuitive interface
for robotic educational applications for children. However there are a number
of challenges to overcome to allow such an interface to operate robustly in
realistic settings, including the intrinsic difficulties of recognising child
speech and high levels of background noise often present in classrooms. As part
of the EU EASEL project we have provided several contributions to address these
challenges, implementing our own ASR module for use in robotics applications.
We used the latest deep neural network algorithms which provide a leap in
performance over the traditional GMM approach, and apply data augmentation
methods to improve robustness to noise and speaker variation. We provide a
close integration between the ASR module and the rest of the dialogue system,
allowing the ASR to receive in real-time the language models relevant to the
current section of the dialogue, greatly improving the accuracy. We integrated
our ASR module into an interactive, multimodal system using a small humanoid
robot to help children learn about exercise and energy. The system was
installed at a public museum event as part of a research study where 320
children (aged 3 to 14) interacted with the robot, with our ASR achieving 90%
accuracy for fluent and near-fluent speech.",arxiv
http://arxiv.org/abs/2108.00105v1,2021-07-30T23:24:29Z,2021-07-30T23:24:29Z,"Deep Feature Tracker: A Novel Application for Deep Convolutional Neural
  Networks","Feature tracking is the building block of many applications such as visual
odometry, augmented reality, and target tracking. Unfortunately, the
state-of-the-art vision-based tracking algorithms fail in surgical images due
to the challenges imposed by the nature of such environments. In this paper, we
proposed a novel and unified deep learning-based approach that can learn how to
track features reliably as well as learn how to detect such reliable features
for tracking purposes. The proposed network dubbed as Deep-PT, consists of a
tracker network which is a convolutional neural network simulating
cross-correlation in terms of deep learning and two fully connected networks
that operate on the output of intermediate layers of the tracker to detect
features and predict trackability of the detected points. The ability to detect
features based on the capabilities of the tracker distinguishes the proposed
method from previous algorithms used in this area and improves the robustness
of the algorithms against dynamics of the scene. The network is trained using
multiple datasets due to the lack of specialized dataset for feature tracking
datasets and extensive comparisons are conducted to compare the accuracy of
Deep-PT against recent pixel tracking algorithms. As the experiments suggest,
the proposed deep architecture deliberately learns what to track and how to
track and outperforms the state-of-the-art methods.",arxiv
http://arxiv.org/abs/1809.10842v1,2018-09-28T03:30:37Z,2018-09-28T03:30:37Z,Learning and Planning with a Semantic Model,"Building deep reinforcement learning agents that can generalize and adapt to
unseen environments remains a fundamental challenge for AI. This paper
describes progresses on this challenge in the context of man-made environments,
which are visually diverse but contain intrinsic semantic regularities. We
propose a hybrid model-based and model-free approach, LEArning and Planning
with Semantics (LEAPS), consisting of a multi-target sub-policy that acts on
visual inputs, and a Bayesian model over semantic structures. When placed in an
unseen environment, the agent plans with the semantic model to make high-level
decisions, proposes the next sub-target for the sub-policy to execute, and
updates the semantic model based on new observations. We perform experiments in
visual navigation tasks using House3D, a 3D environment that contains diverse
human-designed indoor scenes with real-world objects. LEAPS outperforms strong
baselines that do not explicitly plan using the semantic content.",arxiv
http://arxiv.org/abs/1906.04591v4,2021-08-13T11:04:40Z,2019-06-10T09:32:41Z,"CNN depth analysis with different channel inputs for Acoustic Scene
  Classification","Acoustic scene classification (ASC) has been approached in the last years
using deep learning techniques such as convolutional neural networks or
recurrent neural networks. Many state-of-the-art solutions are based on image
classification frameworks and, as such, a 2D representation of the audio signal
is considered for training these networks. Finding the most suitable audio
representation is still a research area of interest. In this paper, different
log-Mel representations and combinations are analyzed. Experiments show that
the best results are obtained using the harmonic and percussive components plus
the difference between left and right stereo channels, (L-R). On the other
hand, it is a common strategy to ensemble different models in order to increase
the final accuracy. Even though averaging different model predictions is a
common choice, an exhaustive analysis of different ensemble techniques has not
been presented in ASC problems. In this paper, geometric and arithmetic mean
plus the Ordered Weighted Averaging (OWA) operator are studied as aggregation
operators for the output of the different models of the ensemble. Finally, the
work carried out in this paper is highly oriented towards real-time
implementations. In this context, as the number of applications for audio
classification on edge devices is increasing exponentially, we also analyze
different network depths and efficient solutions for aggregating ensemble
predictions.",arxiv
http://arxiv.org/abs/2002.09052v1,2020-02-20T22:41:10Z,2020-02-20T22:41:10Z,"Risk-Based Optimization of Virtual Reality over Terahertz Reconfigurable
  Intelligent Surfaces","In this paper, the problem of associating reconfigurable intelligent surfaces
(RISs) to virtual reality (VR) users is studied for a wireless VR network. In
particular, this problem is considered within a cellular network that employs
terahertz (THz) operated RISs acting as base stations. To provide a seamless VR
experience, high data rates and reliable low latency need to be continuously
guaranteed. To address these challenges, a novel risk-based framework based on
the entropic value-at-risk is proposed for rate optimization and reliability
performance. Furthermore, a Lyapunov optimization technique is used to
reformulate the problem as a linear weighted function, while ensuring that
higher order statistics of the queue length are maintained under a threshold.
To address this problem, given the stochastic nature of the channel, a
policy-based reinforcement learning (RL) algorithm is proposed. Since the state
space is extremely large, the policy is learned through a deep-RL algorithm. In
particular, a recurrent neural network (RNN) RL framework is proposed to
capture the dynamic channel behavior and improve the speed of conventional RL
policy-search algorithms. Simulation results demonstrate that the maximal queue
length resulting from the proposed approach is only within 1% of the optimal
solution. The results show a high accuracy and fast convergence for the RNN
with a validation accuracy of 91.92%.",arxiv
http://arxiv.org/abs/1809.01697v1,2018-09-04T00:47:50Z,2018-09-04T00:47:50Z,"HASP: A High-Performance Adaptive Mobile Security Enhancement Against
  Malicious Speech Recognition","Nowadays, machine learning based Automatic Speech Recognition (ASR) technique
has widely spread in smartphones, home devices, and public facilities. As
convenient as this technology can be, a considerable security issue also raises
-- the users' speech content might be exposed to malicious ASR monitoring and
cause severe privacy leakage. In this work, we propose HASP -- a
high-performance security enhancement approach to solve this security issue on
mobile devices. Leveraging ASR systems' vulnerability to the adversarial
examples, HASP is designed to cast human imperceptible adversarial noises to
real-time speech and effectively perturb malicious ASR monitoring by increasing
the Word Error Rate (WER). To enhance the practical performance on mobile
devices, HASP is also optimized for effective adaptation to the human speech
characteristics, environmental noises, and mobile computation scenarios. The
experiments show that HASP can achieve optimal real-time security enhancement:
it can lead an average WER of 84.55% for perturbing the malicious ASR
monitoring, and the data processing speed is 15x to 40x faster compared to the
state-of-the-art methods. Moreover, HASP can effectively perturb various ASR
systems, demonstrating a strong transferability.",arxiv
http://arxiv.org/abs/2006.07644v2,2021-05-17T13:59:45Z,2020-06-13T14:12:23Z,"RoadNet-RT: High Throughput CNN Architecture and SoC Design for
  Real-Time Road Segmentation","In recent years, convolutional neural network has gained popularity in many
engineering applications especially for computer vision. In order to achieve
better performance, often more complex structures and advanced operations are
incorporated into the neural networks, which results very long inference time.
For time-critical tasks such as autonomous driving and virtual reality,
real-time processing is fundamental. In order to reach real-time process speed,
a light-weight, high-throughput CNN architecture namely RoadNet-RT is proposed
for road segmentation in this paper. It achieves 90.33% MaxF score on test set
of KITTI road segmentation task and 8 ms per frame when running on GTX 1080
GPU. Comparing to the state-of-the-art network, RoadNet-RT speeds up the
inference time by a factor of 20 at the cost of only 6.2% accuracy loss. For
hardware design optimization, several techniques such as depthwise separable
convolution and non-uniformed kernel size convolution are customized designed
to further reduce the processing time. The proposed CNN architecture has been
successfully implemented on an FPGA ZCU102 MPSoC platform that achieves the
computation capability of 83.05 GOPS. The system throughput reaches 327.9
frames per second with image size 1216x176.",arxiv
http://arxiv.org/abs/2108.00620v2,2021-10-14T07:08:59Z,2021-08-02T03:54:39Z,Investigating Attention Mechanism in 3D Point Cloud Object Detection,"Object detection in three-dimensional (3D) space attracts much interest from
academia and industry since it is an essential task in AI-driven applications
such as robotics, autonomous driving, and augmented reality. As the basic
format of 3D data, the point cloud can provide detailed geometric information
about the objects in the original 3D space. However, due to 3D data's sparsity
and unorderedness, specially designed networks and modules are needed to
process this type of data. Attention mechanism has achieved impressive
performance in diverse computer vision tasks; however, it is unclear how
attention modules would affect the performance of 3D point cloud object
detection and what sort of attention modules could fit with the inherent
properties of 3D data. This work investigates the role of the attention
mechanism in 3D point cloud object detection and provides insights into the
potential of different attention modules. To achieve that, we comprehensively
investigate classical 2D attentions, novel 3D attentions, including the latest
point cloud transformers on SUN RGB-D and ScanNetV2 datasets. Based on the
detailed experiments and analysis, we conclude the effects of different
attention modules. This paper is expected to serve as a reference source for
benefiting attention-embedded 3D point cloud object detection. The code and
trained models are available at:
https://github.com/ShiQiu0419/attentions_in_3D_detection.",arxiv
http://arxiv.org/abs/1912.02340v2,2019-12-16T00:55:58Z,2019-12-05T01:39:56Z,"Static and Dynamic Fusion for Multi-modal Cross-ethnicity Face
  Anti-spoofing","Regardless of the usage of deep learning and handcrafted methods, the dynamic
information from videos and the effect of cross-ethnicity are rarely considered
in face anti-spoofing. In this work, we propose a static-dynamic fusion
mechanism for multi-modal face anti-spoofing. Inspired by motion divergences
between real and fake faces, we incorporate the dynamic image calculated by
rank pooling with static information into a conventional neural network (CNN)
for each modality (i.e., RGB, Depth and infrared (IR)). Then, we develop a
partially shared fusion method to learn complementary information from multiple
modalities. Furthermore, in order to study the generalization capability of the
proposal in terms of cross-ethnicity attacks and unknown spoofs, we introduce
the largest public cross-ethnicity Face Anti-spoofing (CASIA-CeFA) dataset,
covering 3 ethnicities, 3 modalities, 1607 subjects, and 2D plus 3D attack
types. Experiments demonstrate that the proposed method achieves
state-of-the-art results on CASIA-CeFA, CASIA-SURF, OULU-NPU and SiW.",arxiv
http://arxiv.org/abs/2107.00359v1,2021-07-01T10:49:55Z,2021-07-01T10:49:55Z,"Model Mediated Teleoperation with a Hand-Arm Exoskeleton in Long Time
  Delays Using Reinforcement Learning","Telerobotic systems must adapt to new environmental conditions and deal with
high uncertainty caused by long-time delays. As one of the best alternatives to
human-level intelligence, Reinforcement Learning (RL) may offer a solution to
cope with these issues. This paper proposes to integrate RL with the Model
Mediated Teleoperation (MMT) concept. The teleoperator interacts with a
simulated virtual environment, which provides instant feedback. Whereas
feedback from the real environment is delayed, feedback from the model is
instantaneous, leading to high transparency. The MMT is realized in combination
with an intelligent system with two layers. The first layer utilizes Dynamic
Movement Primitives (DMP) which accounts for certain changes in the avatar
environment. And, the second layer addresses the problems caused by uncertainty
in the model using RL methods. Augmented reality was also provided to fuse the
avatar device and virtual environment models for the teleoperator. Implemented
on DLR's Exodex Adam hand-arm haptic exoskeleton, the results show RL methods
are able to find different solutions when changes are applied to the object
position after the demonstration. The results also show DMPs to be effective at
adapting to new conditions where there is no uncertainty involved.",arxiv
http://arxiv.org/abs/1802.01218v1,2018-02-04T23:53:58Z,2018-02-04T23:53:58Z,Efficient Video Object Segmentation via Network Modulation,"Video object segmentation targets at segmenting a specific object throughout
a video sequence, given only an annotated first frame. Recent deep learning
based approaches find it effective by fine-tuning a general-purpose
segmentation model on the annotated frame using hundreds of iterations of
gradient descent. Despite the high accuracy these methods achieve, the
fine-tuning process is inefficient and fail to meet the requirements of real
world applications. We propose a novel approach that uses a single forward pass
to adapt the segmentation model to the appearance of a specific object.
Specifically, a second meta neural network named modulator is learned to
manipulate the intermediate layers of the segmentation network given limited
visual and spatial information of the target object. The experiments show that
our approach is 70times faster than fine-tuning approaches while achieving
similar accuracy.",arxiv
http://arxiv.org/abs/2105.10707v1,2021-05-22T12:19:03Z,2021-05-22T12:19:03Z,"Adversarial Attacks and Mitigation for Anomaly Detectors of
  Cyber-Physical Systems","The threats faced by cyber-physical systems (CPSs) in critical infrastructure
have motivated research into a multitude of attack detection mechanisms,
including anomaly detectors based on neural network models. The effectiveness
of anomaly detectors can be assessed by subjecting them to test suites of
attacks, but less consideration has been given to adversarial attackers that
craft noise specifically designed to deceive them. While successfully applied
in domains such as images and audio, adversarial attacks are much harder to
implement in CPSs due to the presence of other built-in defence mechanisms such
as rule checkers(or invariant checkers). In this work, we present an
adversarial attack that simultaneously evades the anomaly detectors and rule
checkers of a CPS. Inspired by existing gradient-based approaches, our
adversarial attack crafts noise over the sensor and actuator values, then uses
a genetic algorithm to optimise the latter, ensuring that the neural network
and the rule checking system are both deceived.We implemented our approach for
two real-world critical infrastructure testbeds, successfully reducing the
classification accuracy of their detectors by over 50% on average, while
simultaneously avoiding detection by rule checkers. Finally, we explore whether
these attacks can be mitigated by training the detectors on adversarial
samples.",arxiv
http://arxiv.org/abs/1803.02665v4,2018-09-25T09:00:03Z,2018-03-07T14:16:59Z,"A Neural Network Approach to Missing Marker Reconstruction in Human
  Motion Capture","Optical motion capture systems have become a widely used technology in
various fields, such as augmented reality, robotics, movie production, etc.
Such systems use a large number of cameras to triangulate the position of
optical markers.The marker positions are estimated with high accuracy. However,
especially when tracking articulated bodies, a fraction of the markers in each
timestep is missing from the reconstruction. In this paper, we propose to use a
neural network approach to learn how human motion is temporally and spatially
correlated, and reconstruct missing markers positions through this model. We
experiment with two different models, one LSTM-based and one time-window-based.
Both methods produce state-of-the-art results, while working online, as opposed
to most of the alternative methods, which require the complete sequence to be
known. The implementation is publicly available at
https://github.com/Svito-zar/NN-for-Missing-Marker-Reconstruction .",arxiv
http://arxiv.org/abs/1810.10090v1,2018-10-23T21:07:42Z,2018-10-23T21:07:42Z,"NestDNN: Resource-Aware Multi-Tenant On-Device Deep Learning for
  Continuous Mobile Vision","Mobile vision systems such as smartphones, drones, and augmented-reality
headsets are revolutionizing our lives. These systems usually run multiple
applications concurrently and their available resources at runtime are dynamic
due to events such as starting new applications, closing existing applications,
and application priority changes. In this paper, we present NestDNN, a
framework that takes the dynamics of runtime resources into account to enable
resource-aware multi-tenant on-device deep learning for mobile vision systems.
NestDNN enables each deep learning model to offer flexible resource-accuracy
trade-offs. At runtime, it dynamically selects the optimal resource-accuracy
trade-off for each deep learning model to fit the model's resource demand to
the system's available runtime resources. In doing so, NestDNN efficiently
utilizes the limited resources in mobile vision systems to jointly maximize the
performance of all the concurrently running applications. Our experiments show
that compared to the resource-agnostic status quo approach, NestDNN achieves as
much as 4.2% increase in inference accuracy, 2.0x increase in video frame
processing rate and 1.7x reduction on energy consumption.",arxiv
http://arxiv.org/abs/2008.12444v3,2020-09-03T02:33:25Z,2020-08-28T02:22:07Z,"Pixel-Face: A Large-Scale, High-Resolution Benchmark for 3D Face
  Reconstruction","3D face reconstruction is a fundamental task that can facilitate numerous
applications such as robust facial analysis and augmented reality. It is also a
challenging task due to the lack of high-quality datasets that can fuel current
deep learning-based methods. However, existing datasets are limited in
quantity, realisticity and diversity. To circumvent these hurdles, we introduce
Pixel-Face, a large-scale, high-resolution and diverse 3D face dataset with
massive annotations. Specifically, Pixel-Face contains 855 subjects aging from
18 to 80. Each subject has more than 20 samples with various expressions. Each
sample is composed of high-resolution multi-view RGB images and 3D meshes with
various expressions. Moreover, we collect precise landmarks annotation and 3D
registration result for each data. To demonstrate the advantages of Pixel-Face,
we re-parameterize the 3D Morphable Model (3DMM) into Pixel-3DM using the
collected data. We show that the obtained Pixel-3DM is better in modeling a
wide range of face shapes and expressions. We also carefully benchmark existing
3D face reconstruction methods on our dataset. Moreover, Pixel-Face serves as
an effective training source. We observe that the performance of current face
reconstruction models significantly improves both on existing benchmarks and
Pixel-Face after being fine-tuned using our newly collected data. Extensive
experiments demonstrate the effectiveness of Pixel-3DM and the usefulness of
Pixel-Face.",arxiv
http://arxiv.org/abs/1201.4339v1,2012-01-20T17:04:18Z,2012-01-20T17:04:18Z,"Recognizing recurrent neural networks (rRNN): Bayesian inference for
  recurrent neural networks","Recurrent neural networks (RNNs) are widely used in computational
neuroscience and machine learning applications. In an RNN, each neuron computes
its output as a nonlinear function of its integrated input. While the
importance of RNNs, especially as models of brain processing, is undisputed, it
is also widely acknowledged that the computations in standard RNN models may be
an over-simplification of what real neuronal networks compute. Here, we suggest
that the RNN approach may be made both neurobiologically more plausible and
computationally more powerful by its fusion with Bayesian inference techniques
for nonlinear dynamical systems. In this scheme, we use an RNN as a generative
model of dynamic input caused by the environment, e.g. of speech or kinematics.
Given this generative RNN model, we derive Bayesian update equations that can
decode its output. Critically, these updates define a 'recognizing RNN' (rRNN),
in which neurons compute and exchange prediction and prediction error messages.
The rRNN has several desirable features that a conventional RNN does not have,
for example, fast decoding of dynamic stimuli and robustness to initial
conditions and noise. Furthermore, it implements a predictive coding scheme for
dynamic inputs. We suggest that the Bayesian inversion of recurrent neural
networks may be useful both as a model of brain function and as a machine
learning tool. We illustrate the use of the rRNN by an application to the
online decoding (i.e. recognition) of human kinematics.",arxiv
http://arxiv.org/abs/2104.01036v1,2021-04-02T13:17:11Z,2021-04-02T13:17:11Z,"Hybrid Policy Learning for Energy-Latency Tradeoff in MEC-Assisted VR
  Video Service","Virtual reality (VR) is promising to fundamentally transform a broad spectrum
of industry sectors and the way humans interact with virtual content. However,
despite unprecedented progress, current networking and computing
infrastructures are incompetent to unlock VR's full potential. In this paper,
we consider delivering the wireless multi-tile VR video service over a mobile
edge computing (MEC) network. The primary goal is to minimize the system
latency/energy consumption and to arrive at a tradeoff thereof. To this end, we
first cast the time-varying view popularity as a model-free Markov chain to
effectively capture its dynamic characteristics. After jointly assessing the
caching and computing capacities on both the MEC server and the VR playback
device, a hybrid policy is then implemented to coordinate the dynamic caching
replacement and the deterministic offloading, so as to fully utilize the system
resources. The underlying multi-objective problem is reformulated as a
partially observable Markov decision process, and a deep deterministic policy
gradient algorithm is proposed to iteratively learn its solution, where a long
short-term memory neural network is embedded to continuously predict the
dynamics of the unobservable popularity. Simulation results demonstrate the
superiority of the proposed scheme in achieving a trade-off between the energy
efficiency and the latency reduction over the baseline methods.",arxiv
http://arxiv.org/abs/2107.11355v2,2021-08-18T03:25:24Z,2021-07-23T17:19:23Z,Unsupervised Domain Adaptive 3D Detection with Multi-Level Consistency,"Deep learning-based 3D object detection has achieved unprecedented success
with the advent of large-scale autonomous driving datasets. However, drastic
performance degradation remains a critical challenge for cross-domain
deployment. In addition, existing 3D domain adaptive detection methods often
assume prior access to the target domain annotations, which is rarely feasible
in the real world. To address this challenge, we study a more realistic
setting, unsupervised 3D domain adaptive detection, which only utilizes source
domain annotations. 1) We first comprehensively investigate the major
underlying factors of the domain gap in 3D detection. Our key insight is that
geometric mismatch is the key factor of domain shift. 2) Then, we propose a
novel and unified framework, Multi-Level Consistency Network (MLC-Net), which
employs a teacher-student paradigm to generate adaptive and reliable
pseudo-targets. MLC-Net exploits point-, instance- and neural statistics-level
consistency to facilitate cross-domain transfer. Extensive experiments
demonstrate that MLC-Net outperforms existing state-of-the-art methods
(including those using additional target domain information) on standard
benchmarks. Notably, our approach is detector-agnostic, which achieves
consistent gains on both single- and two-stage 3D detectors.",arxiv
http://arxiv.org/abs/2006.05734v2,2021-06-06T13:00:47Z,2020-06-10T08:50:53Z,3D Human Mesh Regression with Dense Correspondence,"Estimating 3D mesh of the human body from a single 2D image is an important
task with many applications such as augmented reality and Human-Robot
interaction. However, prior works reconstructed 3D mesh from global image
feature extracted by using convolutional neural network (CNN), where the dense
correspondences between the mesh surface and the image pixels are missing,
leading to suboptimal solution. This paper proposes a model-free 3D human mesh
estimation framework, named DecoMR, which explicitly establishes the dense
correspondence between the mesh and the local image features in the UV space
(i.e. a 2D space used for texture mapping of 3D mesh). DecoMR first predicts
pixel-to-surface dense correspondence map (i.e., IUV image), with which we
transfer local features from the image space to the UV space. Then the
transferred local image features are processed in the UV space to regress a
location map, which is well aligned with transferred features. Finally we
reconstruct 3D human mesh from the regressed location map with a predefined
mapping function. We also observe that the existing discontinuous UV map are
unfriendly to the learning of network. Therefore, we propose a novel UV map
that maintains most of the neighboring relations on the original mesh surface.
Experiments demonstrate that our proposed local feature alignment and
continuous UV map outperforms existing 3D mesh based methods on multiple public
benchmarks. Code will be made available at
https://github.com/zengwang430521/DecoMR",arxiv
http://arxiv.org/abs/2008.05563v1,2020-08-12T20:25:07Z,2020-08-12T20:25:07Z,"Facial Expression Recognition Under Partial Occlusion from Virtual
  Reality Headsets based on Transfer Learning","Facial expressions of emotion are a major channel in our daily
communications, and it has been subject of intense research in recent years. To
automatically infer facial expressions, convolutional neural network based
approaches has become widely adopted due to their proven applicability to
Facial Expression Recognition (FER) task.On the other hand Virtual Reality (VR)
has gained popularity as an immersive multimedia platform, where FER can
provide enriched media experiences. However, recognizing facial expression
while wearing a head-mounted VR headset is a challenging task due to the upper
half of the face being completely occluded. In this paper we attempt to
overcome these issues and focus on facial expression recognition in presence of
a severe occlusion where the user is wearing a head-mounted display in a VR
setting. We propose a geometric model to simulate occlusion resulting from a
Samsung Gear VR headset that can be applied to existing FER datasets. Then, we
adopt a transfer learning approach, starting from two pretrained networks,
namely VGG and ResNet. We further fine-tune the networks on FER+ and RAF-DB
datasets. Experimental results show that our approach achieves comparable
results to existing methods while training on three modified benchmark datasets
that adhere to realistic occlusion resulting from wearing a commodity VR
headset. Code for this paper is available at:
https://github.com/bita-github/MRP-FER",arxiv
http://arxiv.org/abs/2109.07735v1,2021-09-16T05:59:01Z,2021-09-16T05:59:01Z,"Decentralized Control of Quadrotor Swarms with End-to-end Deep
  Reinforcement Learning","We demonstrate the possibility of learning drone swarm controllers that are
zero-shot transferable to real quadrotors via large-scale multi-agent
end-to-end reinforcement learning. We train policies parameterized by neural
networks that are capable of controlling individual drones in a swarm in a
fully decentralized manner. Our policies, trained in simulated environments
with realistic quadrotor physics, demonstrate advanced flocking behaviors,
perform aggressive maneuvers in tight formations while avoiding collisions with
each other, break and re-establish formations to avoid collisions with moving
obstacles, and efficiently coordinate in pursuit-evasion tasks. We analyze, in
simulation, how different model architectures and parameters of the training
regime influence the final performance of neural swarms. We demonstrate the
successful deployment of the model learned in simulation to highly
resource-constrained physical quadrotors performing stationkeeping and goal
swapping behaviors. Code and video demonstrations are available at the project
website https://sites.google.com/view/swarm-rl.",arxiv
http://arxiv.org/abs/1912.07791v2,2020-04-02T12:47:39Z,2019-12-17T02:46:54Z,Quaternion Product Units for Deep Learning on 3D Rotation Groups,"We propose a novel quaternion product unit (QPU) to represent data on 3D
rotation groups. The QPU leverages quaternion algebra and the law of 3D
rotation group, representing 3D rotation data as quaternions and merging them
via a weighted chain of Hamilton products. We prove that the representations
derived by the proposed QPU can be disentangled into ""rotation-invariant""
features and ""rotation-equivariant"" features, respectively, which supports the
rationality and the efficiency of the QPU in theory. We design quaternion
neural networks based on our QPUs and make our models compatible with existing
deep learning models. Experiments on both synthetic and real-world data show
that the proposed QPU is beneficial for the learning tasks requiring rotation
robustness.",arxiv
http://arxiv.org/abs/2104.10715v1,2021-04-21T18:28:13Z,2021-04-21T18:28:13Z,Uncertainty-Aware Boosted Ensembling in Multi-Modal Settings,"Reliability of machine learning (ML) systems is crucial in safety-critical
applications such as healthcare, and uncertainty estimation is a widely
researched method to highlight the confidence of ML systems in deployment.
Sequential and parallel ensemble techniques have shown improved performance of
ML systems in multi-modal settings by leveraging the feature sets together. We
propose an uncertainty-aware boosting technique for multi-modal ensembling in
order to focus on the data points with higher associated uncertainty estimates,
rather than the ones with higher loss values. We evaluate this method on
healthcare tasks related to Dementia and Parkinson's disease which involve
real-world multi-modal speech and text data, wherein our method shows an
improved performance. Additional analysis suggests that introducing
uncertainty-awareness into the boosted ensembles decreases the overall entropy
of the system, making it more robust to heteroscedasticity in the data, as well
as better calibrating each of the modalities along with high quality prediction
intervals. We open-source our entire codebase at
https://github.com/usarawgi911/Uncertainty-aware-boosting",arxiv
http://arxiv.org/abs/2110.02653v1,2021-10-06T11:08:50Z,2021-10-06T11:08:50Z,Proactive Scheduling and Caching for Wireless VR Viewport Streaming,"Virtual Reality (VR) applications require high data rate for a high-quality
immersive experience, in addition to low latency to avoid dizziness and motion
sickness. One of the key wireless VR challenges is providing seamless
connectivity and meeting the stringent latency and bandwidth requirements. This
work proposes a proactive wireless VR system that utilizes information about
the user's future orientation for proactive scheduling and caching. This is
achieved by leveraging deep neural networks to predict users' orientation
trained on a real dataset. The 360{\deg} scene is then partitioned using an
overlapping viewports scheme so that only portions of the scene covered by the
users' perceptive field-of-view are streamed. Furthermore, to minimize the
backhaul latency, popular viewports are cached at the edge cloud based on
spatial popularity profiles. Through extensive simulations, we show that the
proposed system provides significant latency and throughput performance
improvement, especially in fluctuating channels and heavy load conditions. The
proactive scheduling enabled by the combination of machine learning prediction
and the proposed viewport scheme reduces the mean latency by more than 80%
while achieving successful delivery rate close to 100%.",arxiv
http://arxiv.org/abs/2003.13586v3,2020-05-29T22:41:21Z,2020-03-30T16:03:03Z,Squeezed Deep 6DoF Object Detection Using Knowledge Distillation,"The detection of objects considering a 6DoF pose is a common requirement to
build virtual and augmented reality applications. It is usually a complex task
which requires real-time processing and high precision results for adequate
user experience. Recently, different deep learning techniques have been
proposed to detect objects in 6DoF in RGB images. However, they rely on high
complexity networks, requiring a computational power that prevents them from
working on mobile devices. In this paper, we propose an approach to reduce the
complexity of 6DoF detection networks while maintaining accuracy. We used
Knowledge Distillation to teach portables Convolutional Neural Networks (CNN)
to learn from a real-time 6DoF detection CNN. The proposed method allows
real-time applications using only RGB images while decreasing the hardware
requirements. We used the LINEMOD dataset to evaluate the proposed method, and
the experimental results show that the proposed method reduces the memory
requirement by almost 99\% in comparison to the original architecture with the
cost of reducing half the accuracy in one of the metrics. Code is available at
https://github.com/heitorcfelix/singleshot6Dpose.",arxiv
http://arxiv.org/abs/1506.02438v6,2018-10-20T18:55:07Z,2015-06-08T11:12:48Z,"High-Dimensional Continuous Control Using Generalized Advantage
  Estimation","Policy gradient methods are an appealing approach in reinforcement learning
because they directly optimize the cumulative reward and can straightforwardly
be used with nonlinear function approximators such as neural networks. The two
main challenges are the large number of samples typically required, and the
difficulty of obtaining stable and steady improvement despite the
nonstationarity of the incoming data. We address the first challenge by using
value functions to substantially reduce the variance of policy gradient
estimates at the cost of some bias, with an exponentially-weighted estimator of
the advantage function that is analogous to TD(lambda). We address the second
challenge by using trust region optimization procedure for both the policy and
the value function, which are represented by neural networks.
  Our approach yields strong empirical results on highly challenging 3D
locomotion tasks, learning running gaits for bipedal and quadrupedal simulated
robots, and learning a policy for getting the biped to stand up from starting
out lying on the ground. In contrast to a body of prior work that uses
hand-crafted policy representations, our neural network policies map directly
from raw kinematics to joint torques. Our algorithm is fully model-free, and
the amount of simulated experience required for the learning tasks on 3D bipeds
corresponds to 1-2 weeks of real time.",arxiv
http://arxiv.org/abs/1711.07480v1,2017-11-20T17:58:10Z,2017-11-20T17:58:10Z,E-PUR: An Energy-Efficient Processing Unit for Recurrent Neural Networks,"Recurrent Neural Networks (RNNs) are a key technology for emerging
applications such as automatic speech recognition, machine translation or image
description. Long Short Term Memory (LSTM) networks are the most successful RNN
implementation, as they can learn long term dependencies to achieve high
accuracy. Unfortunately, the recurrent nature of LSTM networks significantly
constrains the amount of parallelism and, hence, multicore CPUs and many-core
GPUs exhibit poor efficiency for RNN inference. In this paper, we present
E-PUR, an energy-efficient processing unit tailored to the requirements of LSTM
computation. The main goal of E-PUR is to support large recurrent neural
networks for low-power mobile devices. E-PUR provides an efficient hardware
implementation of LSTM networks that is flexible to support diverse
applications. One of its main novelties is a technique that we call Maximizing
Weight Locality (MWL), which improves the temporal locality of the memory
accesses for fetching the synaptic weights, reducing the memory requirements by
a large extent. Our experimental results show that E-PUR achieves real-time
performance for different LSTM networks, while reducing energy consumption by
orders of magnitude with respect to general-purpose processors and GPUs, and it
requires a very small chip area. Compared to a modern mobile SoC, an NVIDIA
Tegra X1, E-PUR provides an average energy reduction of 92x.",arxiv
http://arxiv.org/abs/1706.09556v1,2017-06-29T02:43:37Z,2017-06-29T02:43:37Z,"Vision-based Detection of Acoustic Timed Events: a Case Study on
  Clarinet Note Onsets","Acoustic events often have a visual counterpart. Knowledge of visual
information can aid the understanding of complex auditory scenes, even when
only a stereo mixdown is available in the audio domain, \eg identifying which
musicians are playing in large musical ensembles. In this paper, we consider a
vision-based approach to note onset detection. As a case study we focus on
challenging, real-world clarinetist videos and carry out preliminary
experiments on a 3D convolutional neural network based on multiple streams and
purposely avoiding temporal pooling. We release an audiovisual dataset with 4.5
hours of clarinetist videos together with cleaned annotations which include
about 36,000 onsets and the coordinates for a number of salient points and
regions of interest. By performing several training trials on our dataset, we
learned that the problem is challenging. We found that the CNN model is highly
sensitive to the optimization algorithm and hyper-parameters, and that treating
the problem as binary classification may prevent the joint optimization of
precision and recall. To encourage further research, we publicly share our
dataset, annotations and all models and detail which issues we came across
during our preliminary experiments.",arxiv
http://arxiv.org/abs/1802.10271v1,2018-02-28T06:02:55Z,2018-02-28T06:02:55Z,"Multimodal Sensor-Based Semantic 3D Mapping for a Large-Scale
  Environment","Semantic 3D mapping is one of the most important fields in robotics, and has
been used in many applications, such as robot navigation, surveillance, and
virtual reality. In general, semantic 3D mapping is mainly composed of 3D
reconstruction and semantic segmentation. As these technologies evolve, there
has been great progress in semantic 3D mapping in recent years. Furthermore,
the number of robotic applications requiring semantic information in 3D mapping
to perform high-level tasks has increased, and many studies on semantic 3D
mapping have been published. Existing methods use a camera for both 3D
reconstruction and semantic segmentation. However, this is not suitable for
large-scale environments and has the disadvantage of high computational
complexity. To address this problem, we propose a multimodal sensor-based
semantic 3D mapping system using a 3D Lidar combined with a camera. In this
study, we build a 3D map by estimating odometry based on a global positioning
system (GPS) and an inertial measurement unit (IMU), and use the latest 2D
convolutional neural network (CNN) for semantic segmentation. To build a
semantic 3D map, we integrate the 3D map with semantic information by using
coordinate transformation and Bayes' update scheme. In order to improve the
semantic 3D map, we propose a 3D refinement process to correct wrongly
segmented voxels and remove traces of moving vehicles in the 3D map. Through
experiments on challenging sequences, we demonstrate that our method
outperforms state-of-the-art methods in terms of accuracy and intersection over
union (IoU). Thus, our method can be used for various applications that require
semantic information in 3D map.",arxiv
http://arxiv.org/abs/2009.06613v2,2021-01-15T08:14:55Z,2020-09-14T17:53:15Z,High-Resolution Deep Image Matting,"Image matting is a key technique for image and video editing and composition.
Conventionally, deep learning approaches take the whole input image and an
associated trimap to infer the alpha matte using convolutional neural networks.
Such approaches set state-of-the-arts in image matting; however, they may fail
in real-world matting applications due to hardware limitations, since
real-world input images for matting are mostly of very high resolution. In this
paper, we propose HDMatt, a first deep learning based image matting approach
for high-resolution inputs. More concretely, HDMatt runs matting in a
patch-based crop-and-stitch manner for high-resolution inputs with a novel
module design to address the contextual dependency and consistency issues
between different patches. Compared with vanilla patch-based inference which
computes each patch independently, we explicitly model the cross-patch
contextual dependency with a newly-proposed Cross-Patch Contextual module (CPC)
guided by the given trimap. Extensive experiments demonstrate the effectiveness
of the proposed method and its necessity for high-resolution inputs. Our HDMatt
approach also sets new state-of-the-art performance on Adobe Image Matting and
AlphaMatting benchmarks and produce impressive visual results on more
real-world high-resolution images.",arxiv
http://arxiv.org/abs/2001.02366v2,2020-06-12T01:04:07Z,2020-01-08T04:32:10Z,What can robotics research learn from computer vision research?,"The computer vision and robotics research communities are each strong.
However progress in computer vision has become turbo-charged in recent years
due to big data, GPU computing, novel learning algorithms and a very effective
research methodology. By comparison, progress in robotics seems slower. It is
true that robotics came later to exploring the potential of learning -- the
advantages over the well-established body of knowledge in dynamics, kinematics,
planning and control is still being debated, although reinforcement learning
seems to offer real potential. However, the rapid development of computer
vision compared to robotics cannot be only attributed to the former's adoption
of deep learning. In this paper, we argue that the gains in computer vision are
due to research methodology -- evaluation under strict constraints versus
experiments; bold numbers versus videos.",arxiv
http://arxiv.org/abs/2005.13770v3,2020-08-15T13:37:57Z,2020-05-28T04:02:52Z,"DeepSonar: Towards Effective and Robust Detection of AI-Synthesized Fake
  Voices","With the recent advances in voice synthesis, AI-synthesized fake voices are
indistinguishable to human ears and widely are applied to produce realistic and
natural DeepFakes, exhibiting real threats to our society. However, effective
and robust detectors for synthesized fake voices are still in their infancy and
are not ready to fully tackle this emerging threat. In this paper, we devise a
novel approach, named \emph{DeepSonar}, based on monitoring neuron behaviors of
speaker recognition (SR) system, \ie, a deep neural network (DNN), to discern
AI-synthesized fake voices. Layer-wise neuron behaviors provide an important
insight to meticulously catch the differences among inputs, which are widely
employed for building safety, robust, and interpretable DNNs. In this work, we
leverage the power of layer-wise neuron activation patterns with a conjecture
that they can capture the subtle differences between real and AI-synthesized
fake voices, in providing a cleaner signal to classifiers than raw inputs.
Experiments are conducted on three datasets (including commercial products from
Google, Baidu, \etc) containing both English and Chinese languages to
corroborate the high detection rates (98.1\% average accuracy) and low false
alarm rates (about 2\% error rate) of DeepSonar in discerning fake voices.
Furthermore, extensive experimental results also demonstrate its robustness
against manipulation attacks (\eg, voice conversion and additive real-world
noises). Our work further poses a new insight into adopting neuron behaviors
for effective and robust AI aided multimedia fakes forensics as an inside-out
approach instead of being motivated and swayed by various artifacts introduced
in synthesizing fakes.",arxiv
http://arxiv.org/abs/2009.07632v1,2020-08-26T08:58:29Z,2020-08-26T08:58:29Z,"Helping Users Tackle Algorithmic Threats on Social Media: A Multimedia
  Research Agenda","Participation on social media platforms has many benefits but also poses
substantial threats. Users often face an unintended loss of privacy, are
bombarded with mis-/disinformation, or are trapped in filter bubbles due to
over-personalized content. These threats are further exacerbated by the rise of
hidden AI-driven algorithms working behind the scenes to shape users' thoughts,
attitudes, and behavior. We investigate how multimedia researchers can help
tackle these problems to level the playing field for social media users. We
perform a comprehensive survey of algorithmic threats on social media and use
it as a lens to set a challenging but important research agenda for effective
and real-time user nudging. We further implement a conceptual prototype and
evaluate it with experts to supplement our research agenda. This paper calls
for solutions that combat the algorithmic threats on social media by utilizing
machine learning and multimedia content analysis techniques but in a
transparent manner and for the benefit of the users.",arxiv
http://arxiv.org/abs/1906.04161v1,2019-06-10T17:58:32Z,2019-06-10T17:58:32Z,Self-Supervised Exploration via Disagreement,"Efficient exploration is a long-standing problem in sensorimotor learning.
Major advances have been demonstrated in noise-free, non-stochastic domains
such as video games and simulation. However, most of these formulations either
get stuck in environments with stochastic dynamics or are too inefficient to be
scalable to real robotics setups. In this paper, we propose a formulation for
exploration inspired by the work in active learning literature. Specifically,
we train an ensemble of dynamics models and incentivize the agent to explore
such that the disagreement of those ensembles is maximized. This allows the
agent to learn skills by exploring in a self-supervised manner without any
external reward. Notably, we further leverage the disagreement objective to
optimize the agent's policy in a differentiable manner, without using
reinforcement learning, which results in a sample-efficient exploration. We
demonstrate the efficacy of this formulation across a variety of benchmark
environments including stochastic-Atari, Mujoco and Unity. Finally, we
implement our differentiable exploration on a real robot which learns to
interact with objects completely from scratch. Project videos and code are at
https://pathak22.github.io/exploration-by-disagreement/",arxiv
http://arxiv.org/abs/2110.04331v1,2021-10-08T18:42:58Z,2021-10-08T18:42:58Z,"MusicNet: Compact Convolutional Neural Network for Real-time Background
  Music Detection","With the recent growth of remote and hybrid work, online meetings often
encounter challenging audio contexts such as background noise, music, and echo.
Accurate real-time detection of music events can help to improve the user
experience in such scenarios, e.g., by switching to high-fidelity
music-specific codec or selecting the optimal noise suppression model.
  In this paper, we present MusicNet -- a compact high-performance model for
detecting background music in the real-time communications pipeline. In online
video meetings, which is our main use case, music almost always co-occurs with
speech and background noises, making the accurate classification quite
challenging.
  The proposed model is a binary classifier that consists of a compact
convolutional neural network core preceded by an in-model featurization layer.
It takes 9 seconds of raw audio as input and does not require any
model-specific featurization on the client.
  We train our model on a balanced subset of the AudioSet data and use 1000
crowd-sourced real test clips to validate the model. Finally, we compare
MusicNet performance to 20 other state-of-the-art models.
  Our classifier gives a true positive rate of 81.3% at a 0.1% false positive
rate, which is significantly better than any other model in the study. Our
model is also 10x smaller and has 4x faster inference than the comparable
baseline.",arxiv
http://arxiv.org/abs/1910.01568v2,2019-10-06T18:47:26Z,2019-10-03T16:14:57Z,"Incremental learning for the detection and classification of
  GAN-generated images","Current developments in computer vision and deep learning allow to
automatically generate hyper-realistic images, hardly distinguishable from real
ones. In particular, human face generation achieved a stunning level of
realism, opening new opportunities for the creative industry but, at the same
time, new scary scenarios where such content can be maliciously misused.
Therefore, it is essential to develop innovative methodologies to automatically
tell apart real from computer generated multimedia, possibly able to follow the
evolution and continuous improvement of data in terms of quality and realism.
In the last few years, several deep learning-based solutions have been proposed
for this problem, mostly based on Convolutional Neural Networks (CNNs).
Although results are good in controlled conditions, it is not clear how such
proposals can adapt to real-world scenarios, where learning needs to
continuously evolve as new types of generated data appear. In this work, we
tackle this problem by proposing an approach based on incremental learning for
the detection and classification of GAN-generated images. Experiments on a
dataset comprising images generated by several GAN-based architectures show
that the proposed method is able to correctly perform discrimination when new
GANs are presented to the network",arxiv
http://arxiv.org/abs/1711.02666v1,2017-11-07T17:10:28Z,2017-11-07T17:10:28Z,"Tensor-Generative Adversarial Network with Two-dimensional Sparse
  Coding: Application to Real-time Indoor Localization","Localization technology is important for the development of indoor
location-based services (LBS). Global Positioning System (GPS) becomes invalid
in indoor environments due to the non-line-of-sight issue, so it is urgent to
develop a real-time high-accuracy localization approach for smartphones.
However, accurate localization is challenging due to issues such as real-time
response requirements, limited fingerprint samples and mobile device storage.
To address these problems, we propose a novel deep learning architecture:
Tensor-Generative Adversarial Network (TGAN).
  We first introduce a transform-based 3D tensor to model fingerprint samples.
Instead of those passive methods that construct a fingerprint database as a
prior, our model applies artificial neural network with deep learning to train
network classifiers and then gives out estimations. Then we propose a novel
tensor-based super-resolution scheme using the generative adversarial network
(GAN) that adopts sparse coding as the generator network and a residual
learning network as the discriminator. Further, we analyze the performance of
tensor-GAN and implement a trace-based localization experiment, which achieves
better performance. Compared to existing methods for smartphones indoor
positioning, that are energy-consuming and high demands on devices, TGAN can
give out an improved solution in localization accuracy, response time and
implementation complexity.",arxiv
http://arxiv.org/abs/1703.04496v1,2017-03-13T17:25:52Z,2017-03-13T17:25:52Z,"Comparison of echo state network output layer classification methods on
  noisy data","Echo state networks are a recently developed type of recurrent neural network
where the internal layer is fixed with random weights, and only the output
layer is trained on specific data. Echo state networks are increasingly being
used to process spatiotemporal data in real-world settings, including speech
recognition, event detection, and robot control. A strength of echo state
networks is the simple method used to train the output layer - typically a
collection of linear readout weights found using a least squares approach.
Although straightforward to train and having a low computational cost to use,
this method may not yield acceptable accuracy performance on noisy data.
  This study compares the performance of three echo state network output layer
methods to perform classification on noisy data: using trained linear weights,
using sparse trained linear weights, and using trained low-rank approximations
of reservoir states. The methods are investigated experimentally on both
synthetic and natural datasets. The experiments suggest that using regularized
least squares to train linear output weights is superior on data with low
noise, but using the low-rank approximations may significantly improve accuracy
on datasets contaminated with higher noise levels.",arxiv
http://arxiv.org/abs/1906.01308v1,2019-06-04T10:03:08Z,2019-06-04T10:03:08Z,"Towards better Validity: Dispersion based Clustering for Unsupervised
  Person Re-identification","Person re-identification aims to establish the correct identity
correspondences of a person moving through a non-overlapping multi-camera
installation. Recent advances based on deep learning models for this task
mainly focus on supervised learning scenarios where accurate annotations are
assumed to be available for each setup. Annotating large scale datasets for
person re-identification is demanding and burdensome, which renders the
deployment of such supervised approaches to real-world applications infeasible.
Therefore, it is necessary to train models without explicit supervision in an
autonomous manner. In this paper, we propose an elegant and practical
clustering approach for unsupervised person re-identification based on the
cluster validity consideration. Concretely, we explore a fundamental concept in
statistics, namely \emph{dispersion}, to achieve a robust clustering criterion.
Dispersion reflects the compactness of a cluster when employed at the
intra-cluster level and reveals the separation when measured at the
inter-cluster level. With this insight, we design a novel Dispersion-based
Clustering (DBC) approach which can discover the underlying patterns in data.
This approach considers a wider context of sample-level pairwise relationships
to achieve a robust cluster affinity assessment which handles the complications
may arise due to prevalent imbalanced data distributions. Additionally, our
solution can automatically prioritize standalone data points and prevents
inferior clustering. Our extensive experimental analysis on image and video
re-identification benchmarks demonstrate that our method outperforms the
state-of-the-art unsupervised methods by a significant margin. Code is
available at https://github.com/gddingcs/Dispersion-based-Clustering.git.",arxiv
http://arxiv.org/abs/2007.13960v1,2020-07-28T02:53:28Z,2020-07-28T02:53:28Z,"KOVIS: Keypoint-based Visual Servoing with Zero-Shot Sim-to-Real
  Transfer for Robotics Manipulation","We present KOVIS, a novel learning-based, calibration-free visual servoing
method for fine robotic manipulation tasks with eye-in-hand stereo camera
system. We train the deep neural network only in the simulated environment; and
the trained model could be directly used for real-world visual servoing tasks.
KOVIS consists of two networks. The first keypoint network learns the keypoint
representation from the image using with an autoencoder. Then the visual
servoing network learns the motion based on keypoints extracted from the camera
image. The two networks are trained end-to-end in the simulated environment by
self-supervised learning without manual data labeling. After training with data
augmentation, domain randomization, and adversarial examples, we are able to
achieve zero-shot sim-to-real transfer to real-world robotic manipulation
tasks. We demonstrate the effectiveness of the proposed method in both
simulated environment and real-world experiment with different robotic
manipulation tasks, including grasping, peg-in-hole insertion with 4mm
clearance, and M13 screw insertion. The demo video is available at
http://youtu.be/gfBJBR2tDzA",arxiv
http://arxiv.org/abs/2004.14619v1,2020-04-30T07:47:14Z,2020-04-30T07:47:14Z,The 4th AI City Challenge,"The AI City Challenge was created to accelerate intelligent video analysis
that helps make cities smarter and safer. Transportation is one of the largest
segments that can benefit from actionable insights derived from data captured
by sensors, where computer vision and deep learning have shown promise in
achieving large-scale practical deployment. The 4th annual edition of the AI
City Challenge has attracted 315 participating teams across 37 countries, who
leveraged city-scale real traffic data and high-quality synthetic data to
compete in four challenge tracks. Track 1 addressed video-based automatic
vehicle counting, where the evaluation is conducted on both algorithmic
effectiveness and computational efficiency. Track 2 addressed city-scale
vehicle re-identification with augmented synthetic data to substantially
increase the training set for the task. Track 3 addressed city-scale
multi-target multi-camera vehicle tracking. Track 4 addressed traffic anomaly
detection. The evaluation system shows two leader boards, in which a general
leader board shows all submitted results, and a public leader board shows
results limited to our contest participation rules, that teams are not allowed
to use external data in their work. The public leader board shows results more
close to real-world situations where annotated data are limited. Our results
show promise that AI technology can enable smarter and safer transportation
systems.",arxiv
http://arxiv.org/abs/1904.08500v1,2019-04-01T05:38:59Z,2019-04-01T05:38:59Z,"Machine Vision for Natural Gas Methane Emissions Detection Using an
  Infrared Camera","It is crucial to reduce natural gas methane emissions, which can potentially
offset the climate benefits of replacing coal with gas. Optical gas imaging
(OGI) is a widely-used method to detect methane leaks, but is labor-intensive
and cannot provide leak detection results without operators' judgment. In this
paper, we develop a computer vision approach to OGI-based leak detection using
convolutional neural networks (CNN) trained on methane leak images to enable
automatic detection. First, we collect ~1 M frames of labeled video of methane
leaks from different leaking equipment for building CNN model, covering a wide
range of leak sizes (5.3-2051.6 gCH4/h) and imaging distances (4.6-15.6 m).
Second, we examine different background subtraction methods to extract the
methane plume in the foreground. Third, we then test three CNN model variants,
collectively called GasNet, to detect plumes in videos taken at other pieces of
leaking equipment. We assess the ability of GasNet to perform leak detection by
comparing it to a baseline method that uses optical-flow based change detection
algorithm. We explore the sensitivity of results to the CNN structure, with a
moderate-complexity variant performing best across distances. We find that the
detection accuracy can reach as high as 99%, the overall detection accuracy can
exceed 95% for a case across all leak sizes and imaging distances. Binary
detection accuracy exceeds 97% for large leaks (~710 gCH4/h) imaged closely
(~5-7 m). At closer imaging distances (~5-10 m), CNN-based models have greater
than 94% accuracy across all leak sizes. At farthest distances (~13-16 m),
performance degrades rapidly, but it can achieve above 95% accuracy to detect
large leaks (>950 gCH4/h). The GasNet-based computer vision approach could be
deployed in OGI surveys to allow automatic vigilance of methane leak detection
with high detection accuracy in the real world.",arxiv
http://arxiv.org/abs/2108.05713v1,2021-08-08T11:29:16Z,2021-08-08T11:29:16Z,Towards real-world navigation with deep differentiable planners,"We train embodied neural networks to plan and navigate unseen complex 3D
environments, emphasising real-world deployment. Rather than requiring prior
knowledge of the agent or environment, the planner learns to model the state
transitions and rewards. To avoid the potentially hazardous trial-and-error of
reinforcement learning, we focus on differentiable planners such as Value
Iteration Networks (VIN), which are trained offline from safe expert
demonstrations. Although they work well in small simulations, we address two
major limitations that hinder their deployment. First, we observed that current
differentiable planners struggle to plan long-term in environments with a high
branching complexity. While they should ideally learn to assign low rewards to
obstacles to avoid collisions, we posit that the constraints imposed on the
network are not strong enough to guarantee the network to learn sufficiently
large penalties for every possible collision. We thus impose a structural
constraint on the value iteration, which explicitly learns to model any
impossible actions. Secondly, we extend the model to work with a limited
perspective camera under translation and rotation, which is crucial for real
robot deployment. Many VIN-like planners assume a 360 degrees or overhead view
without rotation. In contrast, our method uses a memory-efficient lattice map
to aggregate CNN embeddings of partial observations, and models the rotational
dynamics explicitly using a 3D state-space grid (translation and rotation). Our
proposals significantly improve semantic navigation and exploration on several
2D and 3D environments, succeeding in settings that are otherwise challenging
for this class of methods. As far as we know, we are the first to successfully
perform differentiable planning on the difficult Active Vision Dataset,
consisting of real images captured from a robot.",arxiv
http://arxiv.org/abs/1904.01576v2,2019-04-11T16:00:14Z,2019-04-02T01:46:38Z,"BARISTA: Efficient and Scalable Serverless Serving System for Deep
  Learning Prediction Services","Pre-trained deep learning models are increasingly being used to offer a
variety of compute-intensive predictive analytics services such as fitness
tracking, speech and image recognition. The stateless and highly parallelizable
nature of deep learning models makes them well-suited for serverless computing
paradigm. However, making effective resource management decisions for these
services is a hard problem due to the dynamic workloads and diverse set of
available resource configurations that have their deployment and management
costs. To address these challenges, we present a distributed and scalable
deep-learning prediction serving system called Barista and make the following
contributions. First, we present a fast and effective methodology for
forecasting workloads by identifying various trends. Second, we formulate an
optimization problem to minimize the total cost incurred while ensuring bounded
prediction latency with reasonable accuracy. Third, we propose an efficient
heuristic to identify suitable compute resource configurations. Fourth, we
propose an intelligent agent to allocate and manage the compute resources by
horizontal and vertical scaling to maintain the required prediction latency.
Finally, using representative real-world workloads for urban transportation
service, we demonstrate and validate the capabilities of Barista.",arxiv
http://arxiv.org/abs/1806.07789v1,2018-06-20T15:16:43Z,2018-06-20T15:16:43Z,"Quaternion Convolutional Neural Networks for End-to-End Automatic Speech
  Recognition","Recently, the connectionist temporal classification (CTC) model coupled with
recurrent (RNN) or convolutional neural networks (CNN), made it easier to train
speech recognition systems in an end-to-end fashion. However in real-valued
models, time frame components such as mel-filter-bank energies and the cepstral
coefficients obtained from them, together with their first and second order
derivatives, are processed as individual elements, while a natural alternative
is to process such components as composed entities. We propose to group such
elements in the form of quaternions and to process these quaternions using the
established quaternion algebra. Quaternion numbers and quaternion neural
networks have shown their efficiency to process multidimensional inputs as
entities, to encode internal dependencies, and to solve many tasks with less
learning parameters than real-valued models. This paper proposes to integrate
multiple feature views in quaternion-valued convolutional neural network
(QCNN), to be used for sequence-to-sequence mapping with the CTC model.
Promising results are reported using simple QCNNs in phoneme recognition
experiments with the TIMIT corpus. More precisely, QCNNs obtain a lower phoneme
error rate (PER) with less learning parameters than a competing model based on
real-valued CNNs.",arxiv
http://arxiv.org/abs/2106.15202v1,2021-06-29T09:39:34Z,2021-06-29T09:39:34Z,"Inconspicuous Adversarial Patches for Fooling Image Recognition Systems
  on Mobile Devices","Deep learning based image recognition systems have been widely deployed on
mobile devices in today's world. In recent studies, however, deep learning
models are shown vulnerable to adversarial examples. One variant of adversarial
examples, called adversarial patch, draws researchers' attention due to its
strong attack abilities. Though adversarial patches achieve high attack success
rates, they are easily being detected because of the visual inconsistency
between the patches and the original images. Besides, it usually requires a
large amount of data for adversarial patch generation in the literature, which
is computationally expensive and time-consuming. To tackle these challenges, we
propose an approach to generate inconspicuous adversarial patches with one
single image. In our approach, we first decide the patch locations basing on
the perceptual sensitivity of victim models, then produce adversarial patches
in a coarse-to-fine way by utilizing multiple-scale generators and
discriminators. The patches are encouraged to be consistent with the background
images with adversarial training while preserving strong attack abilities. Our
approach shows the strong attack abilities in white-box settings and the
excellent transferability in black-box settings through extensive experiments
on various models with different architectures and training methods. Compared
to other adversarial patches, our adversarial patches hold the most negligible
risks to be detected and can evade human observations, which is supported by
the illustrations of saliency maps and results of user evaluations. Lastly, we
show that our adversarial patches can be applied in the physical world.",arxiv
http://arxiv.org/abs/2111.02041v1,2021-11-03T07:00:20Z,2021-11-03T07:00:20Z,"A Comparative Study of Speaker Role Identification in Air Traffic
  Communication Using Deep Learning Approaches","Automatic spoken instruction understanding (SIU) of the controller-pilot
conversations in the air traffic control (ATC) requires not only recognizing
the words and semantics of the speech but also determining the role of the
speaker. However, few of the published works on the automatic understanding
systems in air traffic communication focus on speaker role identification
(SRI). In this paper, we formulate the SRI task of controller-pilot
communication as a binary classification problem. Furthermore, the text-based,
speech-based, and speech and text based multi-modal methods are proposed to
achieve a comprehensive comparison of the SRI task. To ablate the impacts of
the comparative approaches, various advanced neural network architectures are
applied to optimize the implementation of text-based and speech-based methods.
Most importantly, a multi-modal speaker role identification network (MMSRINet)
is designed to achieve the SRI task by considering both the speech and textual
modality features. To aggregate modality features, the modal fusion module is
proposed to fuse and squeeze acoustic and textual representations by modal
attention mechanism and self-attention pooling layer, respectively. Finally,
the comparative approaches are validated on the ATCSpeech corpus collected from
a real-world ATC environment. The experimental results demonstrate that all the
comparative approaches are worked for the SRI task, and the proposed MMSRINet
shows the competitive performance and robustness than the other methods on both
seen and unseen data, achieving 98.56%, and 98.08% accuracy, respectively.",arxiv
http://arxiv.org/abs/2004.02545v1,2020-04-06T10:39:10Z,2020-04-06T10:39:10Z,"Human action recognition with a large-scale brain-inspired photonic
  computer","The recognition of human actions in video streams is a challenging task in
computer vision, with cardinal applications in e.g. brain-computer interface
and surveillance. Deep learning has shown remarkable results recently, but can
be found hard to use in practice, as its training requires large datasets and
special purpose, energy-consuming hardware. In this work, we propose a scalable
photonic neuro-inspired architecture based on the reservoir computing paradigm,
capable of recognising video-based human actions with state-of-the-art
accuracy. Our experimental optical setup comprises off-the-shelf components,
and implements a large parallel recurrent neural network that is easy to train
and can be scaled up to hundreds of thousands of nodes. This work paves the way
towards simply reconfigurable and energy-efficient photonic information
processing systems for real-time video processing.",arxiv
http://arxiv.org/abs/1901.11291v2,2019-02-17T14:34:51Z,2019-01-31T09:49:54Z,Discriminate natural versus loudspeaker emitted speech,"In this work, we address a novel, but potentially emerging, problem of
discriminating the natural human voices and those played back by any kind of
audio devices in the context of interactions with in-house voice user
interface. The tackled problem may find relevant applications in (1) the
far-field voice interactions of vocal interfaces such as Amazon Echo, Google
Home, Facebook Portal, etc, and (2) the replay spoofing attack detection. The
detection of loudspeaker emitted speech will help avoid false wake-ups or
unintended interactions with the devices in the first application, while
eliminating attacks involve the replay of recordings collected from enrolled
speakers in the second one. At first we collect a real-world dataset under
well-controlled conditions containing two classes: recorded speeches directly
spoken by numerous people (considered as the natural speech), and recorded
speeches played back from various loudspeakers (considered as the loudspeaker
emitted speech). Then from this dataset, we build prediction models based on
Deep Neural Network (DNN) for which different combination of audio features
have been considered. Experiment results confirm the feasibility of the task
where the combination of audio embeddings extracted from SoundNet and VGGish
network yields the classification accuracy up to about 90%.",arxiv
http://arxiv.org/abs/2011.05671v1,2020-11-11T10:00:12Z,2020-11-11T10:00:12Z,"VStreamDRLS: Dynamic Graph Representation Learning with Self-Attention
  for Enterprise Distributed Video Streaming Solutions","Live video streaming has become a mainstay as a standard communication
solution for several enterprises worldwide. To efficiently stream high-quality
live video content to a large amount of offices, companies employ distributed
video streaming solutions which rely on prior knowledge of the underlying
evolving enterprise network. However, such networks are highly complex and
dynamic. Hence, to optimally coordinate the live video distribution, the
available network capacity between viewers has to be accurately predicted. In
this paper we propose a graph representation learning technique on weighted and
dynamic graphs to predict the network capacity, that is the weights of
connections/links between viewers/nodes. We propose VStreamDRLS, a graph neural
network architecture with a self-attention mechanism to capture the evolution
of the graph structure of live video streaming events. VStreamDRLS employs the
graph convolutional network (GCN) model over the duration of a live video
streaming event and introduces a self-attention mechanism to evolve the GCN
parameters. In doing so, our model focuses on the GCN weights that are relevant
to the evolution of the graph and generate the node representation,
accordingly. We evaluate our proposed approach on the link prediction task on
two real-world datasets, generated by enterprise live video streaming events.
The duration of each event lasted an hour. The experimental results demonstrate
the effectiveness of VStreamDRLS when compared with state-of-the-art
strategies. Our evaluation datasets and implementation are publicly available
at https://github.com/stefanosantaris/vstreamdrls",arxiv
http://arxiv.org/abs/2107.04284v2,2021-08-14T07:27:49Z,2021-07-09T07:55:21Z,"Universal 3-Dimensional Perturbations for Black-Box Attacks on Video
  Recognition Systems","Widely deployed deep neural network (DNN) models have been proven to be
vulnerable to adversarial perturbations in many applications (e.g., image,
audio and text classifications). To date, there are only a few adversarial
perturbations proposed to deviate the DNN models in video recognition systems
by simply injecting 2D perturbations into video frames. However, such attacks
may overly perturb the videos without learning the spatio-temporal features
(across temporal frames), which are commonly extracted by DNN models for video
recognition. To our best knowledge, we propose the first black-box attack
framework that generates universal 3-dimensional (U3D) perturbations to subvert
a variety of video recognition systems. U3D has many advantages, such as (1) as
the transfer-based attack, U3D can universally attack multiple DNN models for
video recognition without accessing to the target DNN model; (2) the high
transferability of U3D makes such universal black-box attack easy-to-launch,
which can be further enhanced by integrating queries over the target model when
necessary; (3) U3D ensures human-imperceptibility; (4) U3D can bypass the
existing state-of-the-art defense schemes; (5) U3D can be efficiently generated
with a few pre-learned parameters, and then immediately injected to attack
real-time DNN-based video recognition systems. We have conducted extensive
experiments to evaluate U3D on multiple DNN models and three large-scale video
datasets. The experimental results demonstrate its superiority and
practicality.",arxiv
http://arxiv.org/abs/2010.11700v1,2020-10-20T17:05:11Z,2020-10-20T17:05:11Z,"On Benchmarking Iris Recognition within a Head-mounted Display for AR/VR
  Application","Augmented and virtual reality is being deployed in different fields of
applications. Such applications might involve accessing or processing critical
and sensitive information, which requires strict and continuous access control.
Given that Head-Mounted Displays (HMD) developed for such applications commonly
contains internal cameras for gaze tracking purposes, we evaluate the
suitability of such setup for verifying the users through iris recognition. In
this work, we first evaluate a set of iris recognition algorithms suitable for
HMD devices by investigating three well-established handcrafted feature
extraction approaches, and to complement it, we also present the analysis using
four deep learning models. While taking into consideration the minimalistic
hardware requirements of stand-alone HMD, we employ and adapt a recently
developed miniature segmentation model (EyeMMS) for segmenting the iris.
Further, to account for non-ideal and non-collaborative capture of iris, we
define a new iris quality metric that we termed as Iris Mask Ratio (IMR) to
quantify the iris recognition performance. Motivated by the performance of iris
recognition, we also propose the continuous authentication of users in a
non-collaborative capture setting in HMD. Through the experiments on a publicly
available OpenEDS dataset, we show that performance with EER = 5% can be
achieved using deep learning methods in a general setting, along with high
accuracy for continuous user authentication.",arxiv
http://arxiv.org/abs/1804.10334v3,2019-02-22T08:15:42Z,2018-04-27T04:07:49Z,"Deep Learning Coordinated Beamforming for Highly-Mobile Millimeter Wave
  Systems","Supporting high mobility in millimeter wave (mmWave) systems enables a wide
range of important applications such as vehicular communications and wireless
virtual/augmented reality. Realizing this in practice, though, requires
overcoming several challenges. First, the use of narrow beams and the
sensitivity of mmWave signals to blockage greatly impact the coverage and
reliability of highly-mobile links. Second, highly-mobile users in dense mmWave
deployments need to frequently hand-off between base stations (BSs), which is
associated with critical control and latency overhead. Further, identifying the
optimal beamforming vectors in large antenna array mmWave systems requires
considerable training overhead, which significantly affects the efficiency of
these mobile systems. In this paper, a novel integrated machine learning and
coordinated beamforming solution is developed to overcome these challenges and
enable highly-mobile mmWave applications. In the proposed solution, a number of
distributed yet coordinating BSs simultaneously serve a mobile user. This user
ideally needs to transmit only one uplink training pilot sequence that will be
jointly received at the coordinating BSs using omni or quasi-omni beam
patterns. These received signals draw a defining signature not only for the
user location, but also for its interaction with the surrounding environment.
The developed solution then leverages a deep learning model that learns how to
use these signatures to predict the beamforming vectors at the BSs. This
renders a comprehensive solution that supports highly-mobile mmWave
applications with reliable coverage, low latency, and negligible training
overhead. Simulation results show that the proposed deep-learning coordinated
beamforming strategy approaches the achievable rate of the genie-aided solution
that knows the optimal beamforming vectors with no training overhead.",arxiv
http://arxiv.org/abs/1905.03702v2,2019-05-17T16:18:02Z,2019-04-30T17:47:53Z,OpenEDS: Open Eye Dataset,"We present a large scale data set, OpenEDS: Open Eye Dataset, of eye-images
captured using a virtual-reality (VR) head mounted display mounted with two
synchronized eyefacing cameras at a frame rate of 200 Hz under controlled
illumination. This dataset is compiled from video capture of the eye-region
collected from 152 individual participants and is divided into four subsets:
(i) 12,759 images with pixel-level annotations for key eye-regions: iris, pupil
and sclera (ii) 252,690 unlabelled eye-images, (iii) 91,200 frames from
randomly selected video sequence of 1.5 seconds in duration and (iv) 143 pairs
of left and right point cloud data compiled from corneal topography of eye
regions collected from a subset, 143 out of 152, participants in the study. A
baseline experiment has been evaluated on OpenEDS for the task of semantic
segmentation of pupil, iris, sclera and background, with the mean
intersectionover-union (mIoU) of 98.3 %. We anticipate that OpenEDS will create
opportunities to researchers in the eye tracking community and the broader
machine learning and computer vision community to advance the state of
eye-tracking for VR applications. The dataset is available for download upon
request at https://research.fb.com/programs/openeds-challenge",arxiv
http://arxiv.org/abs/1706.01159v2,2017-06-15T09:08:58Z,2017-06-04T23:22:30Z,Deep Frame Interpolation,"This work presents a supervised learning based approach to the computer
vision problem of frame interpolation. The presented technique could also be
used in the cartoon animations since drawing each individual frame consumes a
noticeable amount of time. The most existing solutions to this problem use
unsupervised methods and focus only on real life videos with already high frame
rate. However, the experiments show that such methods do not work as well when
the frame rate becomes low and object displacements between frames becomes
large. This is due to the fact that interpolation of the large displacement
motion requires knowledge of the motion structure thus the simple techniques
such as frame averaging start to fail. In this work the deep convolutional
neural network is used to solve the frame interpolation problem. In addition,
it is shown that incorporating the prior information such as optical flow
improves the interpolation quality significantly.",arxiv
http://arxiv.org/abs/1703.03098v2,2017-05-30T20:12:35Z,2017-03-09T01:29:23Z,DA-RNN: Semantic Mapping with Data Associated Recurrent Neural Networks,"3D scene understanding is important for robots to interact with the 3D world
in a meaningful way. Most previous works on 3D scene understanding focus on
recognizing geometrical or semantic properties of the scene independently. In
this work, we introduce Data Associated Recurrent Neural Networks (DA-RNNs), a
novel framework for joint 3D scene mapping and semantic labeling. DA-RNNs use a
new recurrent neural network architecture for semantic labeling on RGB-D
videos. The output of the network is integrated with mapping techniques such as
KinectFusion in order to inject semantic information into the reconstructed 3D
scene. Experiments conducted on a real world dataset and a synthetic dataset
with RGB-D videos demonstrate the ability of our method in semantic 3D scene
mapping.",arxiv
http://arxiv.org/abs/2006.02266v2,2020-10-19T22:21:19Z,2020-06-03T13:32:02Z,"milliEgo: Single-chip mmWave Radar Aided Egomotion Estimation via Deep
  Sensor Fusion","Robust and accurate trajectory estimation of mobile agents such as people and
robots is a key requirement for providing spatial awareness for emerging
capabilities such as augmented reality or autonomous interaction. Although
currently dominated by optical techniques e.g., visual-inertial odometry, these
suffer from challenges with scene illumination or featureless surfaces. As an
alternative, we propose milliEgo, a novel deep-learning approach to robust
egomotion estimation which exploits the capabilities of low-cost mmWave radar.
Although mmWave radar has a fundamental advantage over monocular cameras of
being metric i.e., providing absolute scale or depth, current single chip
solutions have limited and sparse imaging resolution, making existing
point-cloud registration techniques brittle. We propose a new architecture that
is optimized for solving this challenging pose transformation problem.
Secondly, to robustly fuse mmWave pose estimates with additional sensors, e.g.
inertial or visual sensors we introduce a mixed attention approach to deep
fusion. Through extensive experiments, we demonstrate our proposed system is
able to achieve 1.3% 3D error drift and generalizes well to unseen
environments. We also show that the neural architecture can be made highly
efficient and suitable for real-time embedded applications.",arxiv
http://arxiv.org/abs/1805.00330v1,2018-04-24T22:02:10Z,2018-04-24T22:02:10Z,"Real-Time Human Detection as an Edge Service Enabled by a Lightweight
  CNN","Edge computing allows more computing tasks to take place on the decentralized
nodes at the edge of networks. Today many delay sensitive, mission-critical
applications can leverage these edge devices to reduce the time delay or even
to enable real time, online decision making thanks to their onsite presence.
Human objects detection, behavior recognition and prediction in smart
surveillance fall into that category, where a transition of a huge volume of
video streaming data can take valuable time and place heavy pressure on
communication networks. It is widely recognized that video processing and
object detection are computing intensive and too expensive to be handled by
resource limited edge devices. Inspired by the depthwise separable convolution
and Single Shot Multi-Box Detector (SSD), a lightweight Convolutional Neural
Network (LCNN) is introduced in this paper. By narrowing down the classifier's
searching space to focus on human objects in surveillance video frames, the
proposed LCNN algorithm is able to detect pedestrians with an affordable
computation workload to an edge device. A prototype has been implemented on an
edge node (Raspberry PI 3) using openCV libraries, and satisfactory performance
is achieved using real world surveillance video streams. The experimental study
has validated the design of LCNN and shown it is a promising approach to
computing intensive applications at the edge.",arxiv
http://arxiv.org/abs/2008.06655v1,2020-08-15T05:15:00Z,2020-08-15T05:15:00Z,Object Detection in the Context of Mobile Augmented Reality,"In the past few years, numerous Deep Neural Network (DNN) models and
frameworks have been developed to tackle the problem of real-time object
detection from RGB images. Ordinary object detection approaches process
information from the images only, and they are oblivious to the camera pose
with regard to the environment and the scale of the environment. On the other
hand, mobile Augmented Reality (AR) frameworks can continuously track a
camera's pose within the scene and can estimate the correct scale of the
environment by using Visual-Inertial Odometry (VIO). In this paper, we propose
a novel approach that combines the geometric information from VIO with semantic
information from object detectors to improve the performance of object
detection on mobile devices. Our approach includes three components: (1) an
image orientation correction method, (2) a scale-based filtering approach, and
(3) an online semantic map. Each component takes advantage of the different
characteristics of the VIO-based AR framework. We implemented the AR-enhanced
features using ARCore and the SSD Mobilenet model on Android phones. To
validate our approach, we manually labeled objects in image sequences taken
from 12 room-scale AR sessions. The results show that our approach can improve
on the accuracy of generic object detectors by 12% on our dataset.",arxiv
http://arxiv.org/abs/1909.06953v2,2020-05-09T08:50:33Z,2019-09-16T02:46:02Z,"Off-road Autonomous Vehicles Traversability Analysis and Trajectory
  Planning Based on Deep Inverse Reinforcement Learning","Terrain traversability analysis is a fundamental issue to achieve the
autonomy of a robot at off-road environments. Geometry-based and
appearance-based methods have been studied in decades, while behavior-based
methods exploiting learning from demonstration (LfD) are new trends.
Behavior-based methods learn cost functions that guide trajectory planning in
compliance with experts' demonstrations, which can be more scalable to various
scenes and driving behaviors. This research proposes a method of off-road
traversability analysis and trajectory planning using Deep Maximum Entropy
Inverse Reinforcement Learning. To incorporate vehicle's kinematics while
solving the problem of exponential increase of state-space complexity, two
convolutional neural networks, i.e., RL ConvNet and Svf ConvNet, are developed
to encode kinematics into convolution kernels and achieve efficient forward
reinforcement learning. We conduct experiments in off-road environments. Scene
maps are generated using 3D LiDAR data, and expert demonstrations are either
the vehicle's real driving trajectories at the scene or synthesized ones to
represent specific behaviors such as crossing negative obstacles. Different
cost functions of traversability analysis are learned and tested at various
scenes of capability in guiding the trajectory planning of different behaviors.
We also demonstrate the performance and computation efficiency of the proposed
method.",arxiv
http://arxiv.org/abs/2108.06179v1,2021-08-13T11:49:09Z,2021-08-13T11:49:09Z,"Evaluating the Robustness of Semantic Segmentation for Autonomous
  Driving against Real-World Adversarial Patch Attacks","Deep learning and convolutional neural networks allow achieving impressive
performance in computer vision tasks, such as object detection and semantic
segmentation (SS). However, recent studies have shown evident weaknesses of
such models against adversarial perturbations. In a real-world scenario
instead, like autonomous driving, more attention should be devoted to
real-world adversarial examples (RWAEs), which are physical objects (e.g.,
billboards and printable patches) optimized to be adversarial to the entire
perception pipeline. This paper presents an in-depth evaluation of the
robustness of popular SS models by testing the effects of both digital and
real-world adversarial patches. These patches are crafted with powerful attacks
enriched with a novel loss function. Firstly, an investigation on the
Cityscapes dataset is conducted by extending the Expectation Over
Transformation (EOT) paradigm to cope with SS. Then, a novel attack
optimization, called scene-specific attack, is proposed. Such an attack
leverages the CARLA driving simulator to improve the transferability of the
proposed EOT-based attack to a real 3D environment. Finally, a printed physical
billboard containing an adversarial patch was tested in an outdoor driving
scenario to assess the feasibility of the studied attacks in the real world.
Exhaustive experiments revealed that the proposed attack formulations
outperform previous work to craft both digital and real-world adversarial
patches for SS. At the same time, the experimental results showed how these
attacks are notably less effective in the real world, hence questioning the
practical relevance of adversarial attacks to SS models for autonomous/assisted
driving.",arxiv
http://arxiv.org/abs/1901.09963v5,2019-11-20T21:22:50Z,2019-01-28T19:43:27Z,"Defense Methods Against Adversarial Examples for Recurrent Neural
  Networks","Adversarial examples are known to mislead deep learning models to incorrectly
classify them, even in domains where such models achieve state-of-the-art
performance. Until recently, research on both attack and defense methods
focused on image recognition, primarily using convolutional neural networks
(CNNs). In recent years, adversarial example generation methods for recurrent
neural networks (RNNs) have been published, demonstrating that RNN classifiers
are also vulnerable to such attacks. In this paper, we present a novel defense
method, termed sequence squeezing, to make RNN classifiers more robust against
such attacks. Our method differs from previous defense methods which were
designed only for non-sequence based models. We also implement four additional
RNN defense methods inspired by recently published CNN defense methods. We
evaluate our methods against state-of-the-art attacks in the cyber security
domain where real adversaries (malware developers) exist, but our methods can
be applied against other discrete sequence based adversarial attacks, e.g., in
the NLP domain. Using our methods we were able to decrease the effectiveness of
such attack from 99.9% to 15%.",arxiv
http://arxiv.org/abs/1802.05874v1,2018-02-16T09:23:00Z,2018-02-16T09:23:00Z,"Constrained Convolutional-Recurrent Networks to Improve Speech Quality
  with Low Impact on Recognition Accuracy","For a speech-enhancement algorithm, it is highly desirable to simultaneously
improve perceptual quality and recognition rate. Thanks to computational costs
and model complexities, it is challenging to train a model that effectively
optimizes both metrics at the same time. In this paper, we propose a method for
speech enhancement that combines local and global contextual structures
information through convolutional-recurrent neural networks that improves
perceptual quality. At the same time, we introduce a new constraint on the
objective function using a language model/decoder that limits the impact on
recognition rate. Based on experiments conducted with real user data, we
demonstrate that our new context-augmented machine-learning approach for speech
enhancement improves PESQ and WER by an additional 24.5% and 51.3%,
respectively, when compared to the best-performing methods in the literature.",arxiv
http://arxiv.org/abs/1710.03877v1,2017-10-11T01:47:21Z,2017-10-11T01:47:21Z,"Fine-Grained Prediction of Syntactic Typology: Discovering Latent
  Structure with Supervised Learning","We show how to predict the basic word-order facts of a novel language given
only a corpus of part-of-speech (POS) sequences. We predict how often direct
objects follow their verbs, how often adjectives follow their nouns, and in
general the directionalities of all dependency relations. Such typological
properties could be helpful in grammar induction. While such a problem is
usually regarded as unsupervised learning, our innovation is to treat it as
supervised learning, using a large collection of realistic synthetic languages
as training data. The supervised learner must identify surface features of a
language's POS sequence (hand-engineered or neural features) that correlate
with the language's deeper structure (latent trees). In the experiment, we
show: 1) Given a small set of real languages, it helps to add many synthetic
languages to the training data. 2) Our system is robust even when the POS
sequences include noise. 3) Our system on this task outperforms a grammar
induction baseline by a large margin.",arxiv
http://arxiv.org/abs/2012.00855v1,2020-12-01T21:50:53Z,2020-12-01T21:50:53Z,"A Review of Deep Learning Approaches to EEG-Based Classification of
  Cybersickness in Virtual Reality","Cybersickness is an unpleasant side effect of exposure to a virtual reality
(VR) experience and refers to such physiological repercussions as nausea and
dizziness triggered in response to VR exposure. Given the debilitating effect
of cybersickness on the user experience in VR, academic interest in the
automatic detection of cybersickness from physiological measurements has
crested in recent years. Electroencephalography (EEG) has been extensively used
to capture changes in electrical activity in the brain and to automatically
classify cybersickness from brainwaves using a variety of machine learning
algorithms. Recent advances in deep learning (DL) algorithms and increasing
availability of computational resources for DL have paved the way for a new
area of research into the application of DL frameworks to EEG-based detection
of cybersickness. Accordingly, this review involved a systematic review of the
peer-reviewed papers concerned with the application of DL frameworks to the
classification of cybersickness from EEG signals. The relevant literature was
identified through exhaustive database searches, and the papers were
scrutinized with respect to experimental protocols for data collection, data
preprocessing, and DL architectures. The review revealed a limited number of
studies in this nascent area of research and showed that the DL frameworks
reported in these studies (i.e., DNN, CNN, and RNN) could classify
cybersickness with an average accuracy rate of 93%. This review provides a
summary of the trends and issues in the application of DL frameworks to the
EEG-based detection of cybersickness, with some guidelines for future research.",arxiv
http://arxiv.org/abs/1610.02132v4,2017-12-06T18:28:32Z,2016-10-07T03:44:34Z,QSGD: Communication-Efficient SGD via Gradient Quantization and Encoding,"Parallel implementations of stochastic gradient descent (SGD) have received
significant research attention, thanks to excellent scalability properties of
this algorithm, and to its efficiency in the context of training deep neural
networks. A fundamental barrier for parallelizing large-scale SGD is the fact
that the cost of communicating the gradient updates between nodes can be very
large. Consequently, lossy compression heuristics have been proposed, by which
nodes only communicate quantized gradients. Although effective in practice,
these heuristics do not always provably converge, and it is not clear whether
they are optimal.
  In this paper, we propose Quantized SGD (QSGD), a family of compression
schemes which allow the compression of gradient updates at each node, while
guaranteeing convergence under standard assumptions. QSGD allows the user to
trade off compression and convergence time: it can communicate a sublinear
number of bits per iteration in the model dimension, and can achieve
asymptotically optimal communication cost. We complement our theoretical
results with empirical data, showing that QSGD can significantly reduce
communication cost, while being competitive with standard uncompressed
techniques on a variety of real tasks.
  In particular, experiments show that gradient quantization applied to
training of deep neural networks for image classification and automated speech
recognition can lead to significant reductions in communication cost, and
end-to-end training time. For instance, on 16 GPUs, we are able to train a
ResNet-152 network on ImageNet 1.8x faster to full accuracy. Of note, we show
that there exist generic parameter settings under which all known network
architectures preserve or slightly improve their full accuracy when using
quantization.",arxiv
http://arxiv.org/abs/2011.03206v1,2020-11-06T06:23:47Z,2020-11-06T06:23:47Z,"Resource-Constrained Federated Learning with Heterogeneous Labels and
  Models","Various IoT applications demand resource-constrained machine learning
mechanisms for different applications such as pervasive healthcare, activity
monitoring, speech recognition, real-time computer vision, etc. This
necessitates us to leverage information from multiple devices with few
communication overheads. Federated Learning proves to be an extremely viable
option for distributed and collaborative machine learning. Particularly,
on-device federated learning is an active area of research, however, there are
a variety of challenges in addressing statistical (non-IID data) and model
heterogeneities. In addition, in this paper we explore a new challenge of
interest -- to handle label heterogeneities in federated learning. To this end,
we propose a framework with simple $\alpha$-weighted federated aggregation of
scores which leverages overlapping information gain across labels, while saving
bandwidth costs in the process. Empirical evaluation on Animals-10 dataset
(with 4 labels for effective elucidation of results) indicates an average
deterministic accuracy increase of at least ~16.7%. We also demonstrate the
on-device capabilities of our proposed framework by experimenting with
federated learning and inference across different iterations on a Raspberry Pi
2, a single-board computing platform.",arxiv
http://arxiv.org/abs/2006.05694v2,2020-09-21T20:37:02Z,2020-06-10T07:24:39Z,"HiFi-GAN: High-Fidelity Denoising and Dereverberation Based on Speech
  Deep Features in Adversarial Networks","Real-world audio recordings are often degraded by factors such as noise,
reverberation, and equalization distortion. This paper introduces HiFi-GAN, a
deep learning method to transform recorded speech to sound as though it had
been recorded in a studio. We use an end-to-end feed-forward WaveNet
architecture, trained with multi-scale adversarial discriminators in both the
time domain and the time-frequency domain. It relies on the deep feature
matching losses of the discriminators to improve the perceptual quality of
enhanced speech. The proposed model generalizes well to new speakers, new
speech content, and new environments. It significantly outperforms
state-of-the-art baseline methods in both objective and subjective experiments.",arxiv
http://arxiv.org/abs/1912.02222v1,2019-12-04T19:19:14Z,2019-12-04T19:19:14Z,"Reinforcement learning for bandwidth estimation and congestion control
  in real-time communications","Bandwidth estimation and congestion control for real-time communications
(i.e., audio and video conferencing) remains a difficult problem, despite many
years of research. Achieving high quality of experience (QoE) for end users
requires continual updates due to changing network architectures and
technologies. In this paper, we apply reinforcement learning for the first time
to the problem of real-time communications (RTC), where we seek to optimize
user-perceived quality. We present initial proof-of-concept results, where we
learn an agent to control sending rate in an RTC system, evaluating using both
network simulation and real Internet video calls. We discuss the challenges we
observed, particularly in designing realistic reward functions that reflect
QoE, and in bridging the gap between the training environment and real-world
networks.",arxiv
http://arxiv.org/abs/2003.10350v2,2020-08-22T14:46:19Z,2020-03-23T16:11:51Z,"Weakly Supervised 3D Human Pose and Shape Reconstruction with
  Normalizing Flows","Monocular 3D human pose and shape estimation is challenging due to the many
degrees of freedom of the human body and thedifficulty to acquire training data
for large-scale supervised learning in complex visual scenes. In this paper we
present practical semi-supervised and self-supervised models that support
training and good generalization in real-world images and video. Our
formulation is based on kinematic latent normalizing flow representations and
dynamics, as well as differentiable, semantic body part alignment loss
functions that support self-supervised learning. In extensive experiments using
3D motion capture datasets like CMU, Human3.6M, 3DPW, or AMASS, as well as
image repositories like COCO, we show that the proposed methods outperform the
state of the art, supporting the practical construction of an accurate family
of models based on large-scale training with diverse and incompletely labeled
image and video data.",arxiv
http://arxiv.org/abs/2005.11724v1,2020-05-24T11:37:48Z,2020-05-24T11:37:48Z,"Learning to Transfer Graph Embeddings for Inductive Graph based
  Recommendation","With the increasing availability of videos, how to edit them and present the
most interesting parts to users, i.e., video highlight, has become an urgent
need with many broad applications. As users'visual preferences are subjective
and vary from person to person, previous generalized video highlight extraction
models fail to tailor to users' unique preferences. In this paper, we study the
problem of personalized video highlight recommendation with rich visual
content. By dividing each video into non-overlapping segments, we formulate the
problem as a personalized segment recommendation task with many new segments in
the test stage. The key challenges of this problem lie in: the cold-start users
with limited video highlight records in the training data and new segments
without any user ratings at the test stage. In this paper, we propose an
inductive Graph based Transfer learning framework for personalized video
highlight Recommendation (TransGRec). TransGRec is composed of two parts: a
graph neural network followed by an item embedding transfer network.
Specifically, the graph neural network part exploits the higher-order proximity
between users and segments to alleviate the user cold-start problem. The
transfer network is designed to approximate the learned item embeddings from
graph neural networks by taking each item's visual content as input, in order
to tackle the new segment problem in the test phase. We design two detailed
implementations of the transfer learning optimization function, and we show how
the two parts of TransGRec can be efficiently optimized with different transfer
learning optimization functions. Extensive experimental results on a real-world
dataset clearly show the effectiveness of our proposed model.",arxiv
http://arxiv.org/abs/1812.03622v2,2019-03-01T03:29:53Z,2018-12-10T04:53:14Z,3D Scene Parsing via Class-Wise Adaptation,"We propose the method that uses only computer graphics datasets to parse the
real world 3D scenes. 3D scene parsing based on semantic segmentation is
required to implement the categorical interaction in the virtual world.
Convolutional Neural Networks (CNNs) have recently shown state-of-theart
performance on computer vision tasks including semantic segmentation. However,
collecting and annotating a huge amount of data are needed to train CNNs.
Especially in the case of semantic segmentation, annotating pixel by pixel
takes a significant amount of time and often makes mistakes. In contrast,
computer graphics can generate a lot of accurate annotated data and easily
scale up by changing camera positions, textures and lights. Despite these
advantages, models trained on computer graphics datasets cannot perform well on
real data, which is known as the domain shift. To address this issue, we first
present that depth modal and synthetic noise are effective to reduce the domain
shift. Then, we develop the class-wise adaptation which obtains domain
invariant features of CNNs. To reduce the domain shift, we create computer
graphics rooms with a lot of props, and provide photo-realistic rendered
images.We also demonstrate the application which is combined semantic
segmentation with Simultaneous Localization and Mapping (SLAM). Our application
performs accurate 3D scene parsing in real-time on an actual room.",arxiv
http://arxiv.org/abs/1606.05675v1,2016-06-17T21:03:19Z,2016-06-17T21:03:19Z,"DeepFood: Deep Learning-Based Food Image Recognition for Computer-Aided
  Dietary Assessment","Worldwide, in 2014, more than 1.9 billion adults, 18 years and older, were
overweight. Of these, over 600 million were obese. Accurately documenting
dietary caloric intake is crucial to manage weight loss, but also presents
challenges because most of the current methods for dietary assessment must rely
on memory to recall foods eaten. The ultimate goal of our research is to
develop computer-aided technical solutions to enhance and improve the accuracy
of current measurements of dietary intake. Our proposed system in this paper
aims to improve the accuracy of dietary assessment by analyzing the food images
captured by mobile devices (e.g., smartphone). The key technique innovation in
this paper is the deep learning-based food image recognition algorithms.
Substantial research has demonstrated that digital imaging accurately estimates
dietary intake in many environments and it has many advantages over other
methods. However, how to derive the food information (e.g., food type and
portion size) from food image effectively and efficiently remains a challenging
and open research problem. We propose a new Convolutional Neural Network
(CNN)-based food image recognition algorithm to address this problem. We
applied our proposed approach to two real-world food image data sets (UEC-256
and Food-101) and achieved impressive results. To the best of our knowledge,
these results outperformed all other reported work using these two data sets.
Our experiments have demonstrated that the proposed approach is a promising
solution for addressing the food image recognition problem. Our future work
includes further improving the performance of the algorithms and integrating
our system into a real-world mobile and cloud computing-based system to enhance
the accuracy of current measurements of dietary intake.",arxiv
http://arxiv.org/abs/2108.13680v2,2021-09-07T01:18:16Z,2021-08-31T08:37:58Z,Learning Practically Feasible Policies for Online 3D Bin Packing,"We tackle the Online 3D Bin Packing Problem, a challenging yet practically
useful variant of the classical Bin Packing Problem. In this problem, the items
are delivered to the agent without informing the full sequence information.
Agent must directly pack these items into the target bin stably without
changing their arrival order, and no further adjustment is permitted. Online
3D-BPP can be naturally formulated as Markov Decision Process (MDP). We adopt
deep reinforcement learning, in particular, the on-policy actor-critic
framework, to solve this MDP with constrained action space. To learn a
practically feasible packing policy, we propose three critical designs. First,
we propose an online analysis of packing stability based on a novel stacking
tree. It attains a high analysis accuracy while reducing the computational
complexity from $O(N^2)$ to $O(N \log N)$, making it especially suited for RL
training. Second, we propose a decoupled packing policy learning for different
dimensions of placement which enables high-resolution spatial discretization
and hence high packing precision. Third, we introduce a reward function that
dictates the robot to place items in a far-to-near order and therefore
simplifies the collision avoidance in movement planning of the robotic arm.
Furthermore, we provide a comprehensive discussion on several key implemental
issues. The extensive evaluation demonstrates that our learned policy
outperforms the state-of-the-art methods significantly and is practically
usable for real-world applications.",arxiv
http://arxiv.org/abs/1911.09960v1,2019-11-22T10:32:52Z,2019-11-22T10:32:52Z,Computational Ceramicology,"Field archeologists are called upon to identify potsherds, for which purpose
they rely on their experience and on reference works. We have developed two
complementary machine-learning tools to propose identifications based on images
captured on site. One method relies on the shape of the fracture outline of a
sherd; the other is based on decorative features. For the
outline-identification tool, a novel deep-learning architecture was employed,
one that integrates shape information from points along the inner and outer
surfaces. The decoration classifier is based on relatively standard
architectures used in image recognition. In both cases, training the
classifiers required tackling challenges that arise when working with
real-world archeological data: paucity of labeled data; extreme imbalance
between instances of the different categories; and the need to avoid neglecting
rare classes and to take note of minute distinguishing features of some
classes. The scarcity of training data was overcome by using
synthetically-produced virtual potsherds and by employing multiple
data-augmentation techniques. A novel form of training loss allowed us to
overcome the problems caused by under-populated classes and non-homogeneous
distribution of discriminative features.",arxiv
http://arxiv.org/abs/2106.14790v1,2021-06-28T15:13:16Z,2021-06-28T15:13:16Z,"PhysiNet: A Combination of Physics-based Model and Neural Network Model
  for Digital Twins","As the real-time digital counterpart of a physical system or process, digital
twins are utilized for system simulation and optimization. Neural networks are
one way to build a digital twins model by using data especially when a
physics-based model is not accurate or even not available. However, for a newly
designed system, it takes time to accumulate enough data for neural network
moded and only an approximate physics-based model is available. To take
advantage of both models, this paper proposed a model that combines the
physics-based model and the neural network model to improve the prediction
accuracy for the whole life cycle of a system. The proposed model was able to
automatically combine the models and boost their prediction performance.
Experiments showed that the proposed hybrid model outperformed both the
physics-based model and the neural network model.",arxiv
http://arxiv.org/abs/2009.05553v1,2020-09-11T17:36:13Z,2020-09-11T17:36:13Z,Deep Analog-to-Digital Converter for Wireless Communication,"With the advent of the 5G wireless networks, achieving tens of gigabits per
second throughputs and low, milliseconds, latency has become a reality. This
level of performance will fuel numerous real-time applications, such as
autonomy and augmented reality, where the computationally heavy tasks can be
performed in the cloud. The increase in the bandwidth along with the use of
dense constellations places a significant burden on the speed and accuracy of
analog-to-digital converters (ADC). A popular approach to create wideband ADCs
is utilizing multiple channels each operating at a lower speed in the
time-interleaved fashion. However, an interleaved ADC comes with its own set of
challenges. The parallel architecture is very sensitive to the inter-channel
mismatch, timing jitter, clock skew between different ADC channels as well as
the nonlinearity within individual channels. Consequently, complex
post-calibration is required using digital signal processing (DSP) after the
ADC. The traditional DSP calibration consumes a significant amount of power and
its design requires knowledge of the source and type of errors which are
becoming increasingly difficult to predict in nanometer CMOS processes. In this
paper, instead of individually targeting each source of error, we utilize a
deep learning algorithm to learn the complete and complex ADC behavior and to
compensate for it in realtime. We demonstrate this ""Deep ADC"" technique on an
8G Sample/s 8-channel time-interleaved ADC with the QAM-OFDM modulated data.
Simulation results for different QAM symbol constellations and OFDM subcarriers
show dramatic improvements of approximately 5 bits in the dynamic range with a
concomitant drastic reduction in symbol error rate. We further discuss the
hardware implementation including latency, power consumption, memory
requirements, and chip area.",arxiv
http://arxiv.org/abs/1811.10869v1,2018-11-27T08:28:52Z,2018-11-27T08:28:52Z,"Efficient non-uniform quantizer for quantized neural network targeting
  reconfigurable hardware","Convolutional Neural Networks (CNN) has become more popular choice for
various tasks such as computer vision, speech recognition and natural language
processing. Thanks to their large computational capability and throughput, GPUs
,which are not power efficient and therefore does not suit low power systems
such as mobile devices, are the most common platform for both training and
inferencing tasks. Recent studies has shown that FPGAs can provide a good
alternative to GPUs as a CNN accelerator, due to their re-configurable nature,
low power and small latency. In order for FPGA-based accelerators outperform
GPUs in inference task, both the parameters of the network and the activations
must be quantized. While most works use uniform quantizers for both parameters
and activations, it is not always the optimal one, and a non-uniform quantizer
need to be considered. In this work we introduce a custom hardware-friendly
approach to implement non-uniform quantizers. In addition, we use a single
scale integer representation of both parameters and activations, for both
training and inference. The combined method yields a hardware efficient
non-uniform quantizer, fit for real-time applications. We have tested our
method on CIFAR-10 and CIFAR-100 image classification datasets with ResNet-18
and VGG-like architectures, and saw little degradation in accuracy.",arxiv
http://arxiv.org/abs/1702.04333v1,2017-02-14T18:46:47Z,2017-02-14T18:46:47Z,"On the Relevance of Auditory-Based Gabor Features for Deep Learning in
  Automatic Speech Recognition","Previous studies support the idea of merging auditory-based Gabor features
with deep learning architectures to achieve robust automatic speech
recognition, however, the cause behind the gain of such combination is still
unknown. We believe these representations provide the deep learning decoder
with more discriminable cues. Our aim with this paper is to validate this
hypothesis by performing experiments with three different recognition tasks
(Aurora 4, CHiME 2 and CHiME 3) and assess the discriminability of the
information encoded by Gabor filterbank features. Additionally, to identify the
contribution of low, medium and high temporal modulation frequencies subsets of
the Gabor filterbank were used as features (dubbed LTM, MTM and HTM
respectively). With temporal modulation frequencies between 16 and 25 Hz, HTM
consistently outperformed the remaining ones in every condition, highlighting
the robustness of these representations against channel distortions, low
signal-to-noise ratios and acoustically challenging real-life scenarios with
relative improvements from 11 to 56% against a Mel-filterbank-DNN baseline. To
explain the results, a measure of similarity between phoneme classes from DNN
activations is proposed and linked to their acoustic properties. We find this
measure to be consistent with the observed error rates and highlight specific
differences on phoneme level to pinpoint the benefit of the proposed features.",arxiv
http://arxiv.org/abs/2005.06065v2,2020-06-28T21:37:58Z,2020-05-12T21:45:20Z,Automatic Estimation of Intelligibility Measure for Consonants in Speech,"In this article, we provide a model to estimate a real-valued measure of the
intelligibility of individual speech segments. We trained regression models
based on Convolutional Neural Networks (CNN) for stop consonants
\textipa{/p,t,k,b,d,g/} associated with vowel \textipa{/A/}, to estimate the
corresponding Signal to Noise Ratio (SNR) at which the Consonant-Vowel (CV)
sound becomes intelligible for Normal Hearing (NH) ears. The intelligibility
measure for each sound is called SNR$_{90}$, and is defined to be the SNR level
at which human participants are able to recognize the consonant at least 90\%
correctly, on average, as determined in prior experiments with NH subjects.
Performance of the CNN is compared to a baseline prediction based on automatic
speech recognition (ASR), specifically, a constant offset subtracted from the
SNR at which the ASR becomes capable of correctly labeling the consonant.
Compared to baseline, our models were able to accurately estimate the
SNR$_{90}$~intelligibility measure with less than 2 [dB$^2$] Mean Squared Error
(MSE) on average, while the baseline ASR-defined measure computes
SNR$_{90}$~with a variance of 5.2 to 26.6 [dB$^2$], depending on the consonant.",arxiv
http://arxiv.org/abs/1901.10031v2,2019-02-11T20:52:42Z,2019-01-28T23:14:58Z,Lyapunov-based Safe Policy Optimization for Continuous Control,"We study continuous action reinforcement learning problems in which it is
crucial that the agent interacts with the environment only through safe
policies, i.e.,~policies that do not take the agent to undesirable situations.
We formulate these problems as constrained Markov decision processes (CMDPs)
and present safe policy optimization algorithms that are based on a Lyapunov
approach to solve them. Our algorithms can use any standard policy gradient
(PG) method, such as deep deterministic policy gradient (DDPG) or proximal
policy optimization (PPO), to train a neural network policy, while guaranteeing
near-constraint satisfaction for every policy update by projecting either the
policy parameter or the action onto the set of feasible solutions induced by
the state-dependent linearized Lyapunov constraints. Compared to the existing
constrained PG algorithms, ours are more data efficient as they are able to
utilize both on-policy and off-policy data. Moreover, our action-projection
algorithm often leads to less conservative policy updates and allows for
natural integration into an end-to-end PG training pipeline. We evaluate our
algorithms and compare them with the state-of-the-art baselines on several
simulated (MuJoCo) tasks, as well as a real-world indoor robot navigation
problem, demonstrating their effectiveness in terms of balancing performance
and constraint satisfaction. Videos of the experiments can be found in the
following link:
https://drive.google.com/file/d/1pzuzFqWIE710bE2U6DmS59AfRzqK2Kek/view?usp=sharing.",arxiv
http://arxiv.org/abs/2008.09622v3,2021-02-11T18:36:45Z,2020-08-21T18:00:33Z,Learning to Set Waypoints for Audio-Visual Navigation,"In audio-visual navigation, an agent intelligently travels through a complex,
unmapped 3D environment using both sights and sounds to find a sound source
(e.g., a phone ringing in another room). Existing models learn to act at a
fixed granularity of agent motion and rely on simple recurrent aggregations of
the audio observations. We introduce a reinforcement learning approach to
audio-visual navigation with two key novel elements: 1) waypoints that are
dynamically set and learned end-to-end within the navigation policy, and 2) an
acoustic memory that provides a structured, spatially grounded record of what
the agent has heard as it moves. Both new ideas capitalize on the synergy of
audio and visual data for revealing the geometry of an unmapped space. We
demonstrate our approach on two challenging datasets of real-world 3D scenes,
Replica and Matterport3D. Our model improves the state of the art by a
substantial margin, and our experiments reveal that learning the links between
sights, sounds, and space is essential for audio-visual navigation. Project:
http://vision.cs.utexas.edu/projects/audio_visual_waypoints.",arxiv
http://arxiv.org/abs/2006.15190v3,2020-07-09T11:33:27Z,2020-06-26T19:42:20Z,Making DensePose fast and light,"DensePose estimation task is a significant step forward for enhancing user
experience computer vision applications ranging from augmented reality to cloth
fitting. Existing neural network models capable of solving this task are
heavily parameterized and a long way from being transferred to an embedded or
mobile device. To enable Dense Pose inference on the end device with current
models, one needs to support an expensive server-side infrastructure and have a
stable internet connection. To make things worse, mobile and embedded devices
do not always have a powerful GPU inside. In this work, we target the problem
of redesigning the DensePose R-CNN model's architecture so that the final
network retains most of its accuracy but becomes more light-weight and fast. To
achieve that, we tested and incorporated many deep learning innovations from
recent years, specifically performing an ablation study on 23 efficient
backbone architectures, multiple two-stage detection pipeline modifications,
and custom model quantization methods. As a result, we achieved $17\times$
model size reduction and $2\times$ latency improvement compared to the baseline
model.",arxiv
http://arxiv.org/abs/2001.03032v3,2020-09-19T14:14:19Z,2020-01-09T14:50:11Z,Virtual to Real adaptation of Pedestrian Detectors,"Pedestrian detection through Computer Vision is a building block for a
multitude of applications. Recently, there was an increasing interest in
Convolutional Neural Network-based architectures for the execution of such a
task. One of these supervised networks' critical goals is to generalize the
knowledge learned during the training phase to new scenarios with different
characteristics. A suitably labeled dataset is essential to achieve this
purpose. The main problem is that manually annotating a dataset usually
requires a lot of human effort, and it is costly. To this end, we introduce
ViPeD (Virtual Pedestrian Dataset), a new synthetically generated set of images
collected with the highly photo-realistic graphical engine of the video game
GTA V - Grand Theft Auto V, where annotations are automatically acquired.
However, when training solely on the synthetic dataset, the model experiences a
Synthetic2Real Domain Shift leading to a performance drop when applied to
real-world images. To mitigate this gap, we propose two different Domain
Adaptation techniques suitable for the pedestrian detection task, but possibly
applicable to general object detection. Experiments show that the network
trained with ViPeD can generalize over unseen real-world scenarios better than
the detector trained over real-world data, exploiting the variety of our
synthetic dataset. Furthermore, we demonstrate that with our Domain Adaptation
techniques, we can reduce the Synthetic2Real Domain Shift, making closer the
two domains and obtaining a performance improvement when testing the network
over the real-world images. The code, the models, and the dataset are made
freely available at https://ciampluca.github.io/viped/",arxiv
http://arxiv.org/abs/2004.09672v2,2020-05-12T16:21:12Z,2020-04-20T23:25:37Z,"LRCN-RetailNet: A recurrent neural network architecture for accurate
  people counting","Measuring and analyzing the flow of customers in retail stores is essential
for a retailer to better comprehend customers' behavior and support
decision-making. Nevertheless, not much attention has been given to the
development of novel technologies for automatic people counting. We introduce
LRCN-RetailNet: a recurrent neural network architecture capable of learning a
non-linear regression model and accurately predicting the people count from
videos captured by low-cost surveillance cameras. The input video format
follows the recently proposed RGBP image format, which is comprised of color
and people (foreground) information. Our architecture is capable of considering
two relevant aspects: spatial features extracted through convolutional layers
from the RGBP images; and the temporal coherence of the problem, which is
exploited by recurrent layers. We show that, through a supervised learning
approach, the trained models are capable of predicting the people count with
high accuracy. Additionally, we present and demonstrate that a straightforward
modification of the methodology is effective to exclude salespeople from the
people count. Comprehensive experiments were conducted to validate, evaluate
and compare the proposed architecture. Results corroborated that LRCN-RetailNet
remarkably outperforms both the previous RetailNet architecture, which was
limited to evaluating a single image per iteration; and a state-of-the-art
neural network for object detection. Finally, computational performance
experiments confirmed that the entire methodology is effective to estimate
people count in real-time.",arxiv
http://arxiv.org/abs/2008.12487v1,2020-08-28T05:49:13Z,2020-08-28T05:49:13Z,Classification of Imagined Speech Using Siamese Neural Network,"Imagined speech is spotlighted as a new trend in the brain-machine interface
due to its application as an intuitive communication tool. However, previous
studies have shown low classification performance, therefore its use in
real-life is not feasible. In addition, no suitable method to analyze it has
been found. Recently, deep learning algorithms have been applied to this
paradigm. However, due to the small amount of data, the increase in
classification performance is limited. To tackle these issues, in this study,
we proposed an end-to-end framework using Siamese neural network encoder, which
learns the discriminant features by considering the distance between classes.
The imagined words (e.g., arriba (up), abajo (down), derecha (right), izquierda
(left), adelante (forward), and atr\'as (backward)) were classified using the
raw electroencephalography (EEG) signals. We obtained a 6-class classification
accuracy of 31.40% for imagined speech, which significantly outperformed other
methods. This was possible because the Siamese neural network, which increases
the distance between dissimilar samples while decreasing the distance between
similar samples, was used. In this regard, our method can learn discriminant
features from a small dataset. The proposed framework would help to increase
the classification performance of imagined speech for a small amount of data
and implement an intuitive communication system.",arxiv
http://arxiv.org/abs/1909.12217v4,2021-01-25T21:19:51Z,2019-09-26T16:15:37Z,"Visual Exploration and Energy-aware Path Planning via Reinforcement
  Learning","Visual exploration and smart data collection via autonomous vehicles is an
attractive topic in various disciplines. Disturbances like wind significantly
influence both the power consumption of the flying robots and the performance
of the camera. We propose a reinforcement learning approach which combines the
effects of the power consumption and the object detection modules to develop a
policy for object detection in large areas with limited battery life. The
learning model enables dynamic learning of the negative rewards of each action
based on the drag forces that is resulted by the motion of the flying robot
with respect to the wind field. The algorithm is implemented in a near-real
world simulation environment both for the planar motion and flight in different
altitudes. The trained agent often performed a trade-off between detecting the
objects with high accuracy and increasing the area coverage within its battery
life. The developed exploration policy outperformed the complete coverage
algorithm by minimizing the traveled path while finding the target objects. The
performance of the algorithms under various wind fields was evaluated in planar
and 3D motion. During an exploration task with sparsely distributed goals and
within a UAV's battery life, the proposed architecture could detect more than
twice the amount of goal objects compared to the coverage path planning
algorithm in moderate wind field. In high wind intensities, the energy-aware
algorithm could detect 4 times the amount of goal objects when compared to its
complete coverage counterpart.",arxiv
http://arxiv.org/abs/1902.05703v1,2019-02-15T06:34:31Z,2019-02-15T06:34:31Z,"Network Offloading Policies for Cloud Robotics: a Learning-based
  Approach","Today's robotic systems are increasingly turning to computationally expensive
models such as deep neural networks (DNNs) for tasks like localization,
perception, planning, and object detection. However, resource-constrained
robots, like low-power drones, often have insufficient on-board compute
resources or power reserves to scalably run the most accurate, state-of-the art
neural network compute models. Cloud robotics allows mobile robots the benefit
of offloading compute to centralized servers if they are uncertain locally or
want to run more accurate, compute-intensive models. However, cloud robotics
comes with a key, often understated cost: communicating with the cloud over
congested wireless networks may result in latency or loss of data. In fact,
sending high data-rate video or LIDAR from multiple robots over congested
networks can lead to prohibitive delay for real-time applications, which we
measure experimentally. In this paper, we formulate a novel Robot Offloading
Problem --- how and when should robots offload sensing tasks, especially if
they are uncertain, to improve accuracy while minimizing the cost of cloud
communication? We formulate offloading as a sequential decision making problem
for robots, and propose a solution using deep reinforcement learning. In both
simulations and hardware experiments using state-of-the art vision DNNs, our
offloading strategy improves vision task performance by between 1.3-2.6x of
benchmark offloading strategies, allowing robots the potential to significantly
transcend their on-board sensing accuracy but with limited cost of cloud
communication.",arxiv
http://arxiv.org/abs/2006.16806v1,2020-06-28T22:04:54Z,2020-06-28T22:04:54Z,"Uncertainty-aware multi-view co-training for semi-supervised medical
  image segmentation and domain adaptation","Although having achieved great success in medical image segmentation, deep
learning-based approaches usually require large amounts of well-annotated data,
which can be extremely expensive in the field of medical image analysis.
Unlabeled data, on the other hand, is much easier to acquire. Semi-supervised
learning and unsupervised domain adaptation both take the advantage of
unlabeled data, and they are closely related to each other. In this paper, we
propose uncertainty-aware multi-view co-training (UMCT), a unified framework
that addresses these two tasks for volumetric medical image segmentation. Our
framework is capable of efficiently utilizing unlabeled data for better
performance. We firstly rotate and permute the 3D volumes into multiple views
and train a 3D deep network on each view. We then apply co-training by
enforcing multi-view consistency on unlabeled data, where an uncertainty
estimation of each view is utilized to achieve accurate labeling. Experiments
on the NIH pancreas segmentation dataset and a multi-organ segmentation dataset
show state-of-the-art performance of the proposed framework on semi-supervised
medical image segmentation. Under unsupervised domain adaptation settings, we
validate the effectiveness of this work by adapting our multi-organ
segmentation model to two pathological organs from the Medical Segmentation
Decathlon Datasets. Additionally, we show that our UMCT-DA model can even
effectively handle the challenging situation where labeled source data is
inaccessible, demonstrating strong potentials for real-world applications.",arxiv
http://arxiv.org/abs/1904.03987v1,2019-04-08T12:16:06Z,2019-04-08T12:16:06Z,"Early warning in egg production curves from commercial hens: A SVM
  approach","Artificial Intelligence allows the improvement of our daily life, for
instance, speech and handwritten text recognition, real time translation and
weather forecasting are common used applications. In the livestock sector,
machine learning algorithms have the potential for early detection and warning
of problems, which represents a significant milestone in the poultry industry.
Production problems generate economic loss that could be avoided by acting in a
timely manner. In the current study, training and testing of support vector
machines are addressed, for an early detection of problems in the production
curve of commercial eggs, using farm's egg production data of 478,919 laying
hens grouped in 24 flocks. Experiments using support vector machines with a 5
k-fold cross-validation were performed at different previous time intervals, to
alert with up to 5 days of forecasting interval, whether a flock will
experience a problem in production curve. Performance metrics such as accuracy,
specificity, sensitivity, and positive predictive value were evaluated,
reaching 0-day values of 0.9874, 0.9876, 0.9783 and 0.6518 respectively on
unseen data (test-set). The optimal forecasting interval was from zero to three
days, performance metrics decreases as the forecasting interval is increased.
It should be emphasized that this technique was able to issue an alert a day in
advance, achieving an accuracy of 0.9854, a specificity of 0.9865, a
sensitivity of 0.9333 and a positive predictive value of 0.6135. This novel
application embedded in a computer system of poultry management is able to
provide significant improvements in early detection and warning of problems
related to the production curve.",arxiv
http://arxiv.org/abs/1806.05154v1,2018-06-13T17:29:29Z,2018-06-13T17:29:29Z,"Automated Performance Assessment in Transoesophageal Echocardiography
  with Convolutional Neural Networks","Transoesophageal echocardiography (TEE) is a valuable diagnostic and
monitoring imaging modality. Proper image acquisition is essential for
diagnosis, yet current assessment techniques are solely based on manual expert
review. This paper presents a supervised deep learn ing framework for
automatically evaluating and grading the quality of TEE images. To obtain the
necessary dataset, 38 participants of varied experience performed TEE exams
with a high-fidelity virtual reality (VR) platform. Two Convolutional Neural
Network (CNN) architectures, AlexNet and VGG, structured to perform regression,
were finetuned and validated on manually graded images from three evaluators.
Two different scoring strategies, a criteria-based percentage and an overall
general impression, were used. The developed CNN models estimate the average
score with a root mean square accuracy ranging between 84%-93%, indicating the
ability to replicate expert valuation. Proposed strategies for automated TEE
assessment can have a significant impact on the training process of new TEE
operators, providing direct feedback and facilitating the development of the
necessary dexterous skills.",arxiv
http://arxiv.org/abs/2106.15409v2,2021-06-30T20:05:23Z,2021-06-28T08:07:31Z,"Efficient Realistic Data Generation Framework leveraging Deep
  Learning-based Human Digitization","The performance of supervised deep learning algorithms depends significantly
on the scale, quality and diversity of the data used for their training.
Collecting and manually annotating large amount of data can be both
time-consuming and costly tasks to perform. In the case of tasks related to
visual human-centric perception, the collection and distribution of such data
may also face restrictions due to legislation regarding privacy. In addition,
the design and testing of complex systems, e.g., robots, which often employ
deep learning-based perception models, may face severe difficulties as even
state-of-the-art methods trained on real and large-scale datasets cannot always
perform adequately due to not having been adapted to the visual differences
between the virtual and the real world data. As an attempt to tackle and
mitigate the effect of these issues, we present a method that automatically
generates realistic synthetic data with annotations for a) person detection, b)
face recognition, and c) human pose estimation. The proposed method takes as
input real background images and populates them with human figures in various
poses. Instead of using hand-made 3D human models, we propose the use of models
generated through deep learning methods, further reducing the dataset creation
costs, while maintaining a high level of realism. In addition, we provide
open-source and easy to use tools that implement the proposed pipeline,
allowing for generating highly-realistic synthetic datasets for a variety of
tasks. A benchmarking and evaluation in the corresponding tasks shows that
synthetic data can be effectively used as a supplement to real data.",arxiv
http://arxiv.org/abs/2101.04315v1,2021-01-12T06:30:24Z,2021-01-12T06:30:24Z,Neural Network-based Virtual Microphone Estimator,"Developing microphone array technologies for a small number of microphones is
important due to the constraints of many devices. One direction to address this
situation consists of virtually augmenting the number of microphone signals,
e.g., based on several physical model assumptions. However, such assumptions
are not necessarily met in realistic conditions. In this paper, as an
alternative approach, we propose a neural network-based virtual microphone
estimator (NN-VME). The NN-VME estimates virtual microphone signals directly in
the time domain, by utilizing the precise estimation capability of the recent
time-domain neural networks. We adopt a fully supervised learning framework
that uses actual observations at the locations of the virtual microphones at
training time. Consequently, the NN-VME can be trained using only multi-channel
observations and thus directly on real recordings, avoiding the need for
unrealistic physical model-based assumptions. Experiments on the CHiME-4 corpus
show that the proposed NN-VME achieves high virtual microphone estimation
performance even for real recordings and that a beamformer augmented with the
NN-VME improves both the speech enhancement and recognition performance.",arxiv
http://arxiv.org/abs/1906.03173v2,2019-08-04T21:21:33Z,2019-06-07T15:39:21Z,Ego-Pose Estimation and Forecasting as Real-Time PD Control,"We propose the use of a proportional-derivative (PD) control based policy
learned via reinforcement learning (RL) to estimate and forecast 3D human pose
from egocentric videos. The method learns directly from unsegmented egocentric
videos and motion capture data consisting of various complex human motions
(e.g., crouching, hopping, bending, and motion transitions). We propose a
video-conditioned recurrent control technique to forecast physically-valid and
stable future motions of arbitrary length. We also introduce a value function
based fail-safe mechanism which enables our method to run as a single pass
algorithm over the video data. Experiments with both controlled and in-the-wild
data show that our approach outperforms previous art in both quantitative
metrics and visual quality of the motions, and is also robust enough to
transfer directly to real-world scenarios. Additionally, our time analysis
shows that the combined use of our pose estimation and forecasting can run at
30 FPS, making it suitable for real-time applications.",arxiv
http://arxiv.org/abs/2012.09742v3,2021-11-18T03:23:27Z,2020-12-16T18:15:27Z,AutoCaption: Image Captioning with Neural Architecture Search,"Image captioning transforms complex visual information into abstract natural
language for representation, which can help computers understanding the world
quickly. However, due to the complexity of the real environment, it needs to
identify key objects and realize their connections, and further generate
natural language. The whole process involves a visual understanding module and
a language generation module, which brings more challenges to the design of
deep neural networks than other tasks. Neural Architecture Search (NAS) has
shown its important role in a variety of image recognition tasks. Besides, RNN
plays an essential role in the image captioning task. We introduce a
AutoCaption method to better design the decoder module of the image captioning
where we use the NAS to design the decoder module called AutoRNN automatically.
We use the reinforcement learning method based on shared parameters for
automatic design the AutoRNN efficiently. The search space of the AutoCaption
includes connections between the layers and the operations in layers both, and
it can make AutoRNN express more architectures. In particular, RNN is
equivalent to a subset of our search space. Experiments on the MSCOCO datasets
show that our AutoCaption model can achieve better performance than traditional
hand-design methods.",arxiv
http://arxiv.org/abs/0906.5325v1,2009-06-29T17:48:40Z,2009-06-29T17:48:40Z,Online Reinforcement Learning for Dynamic Multimedia Systems,"In our previous work, we proposed a systematic cross-layer framework for
dynamic multimedia systems, which allows each layer to make autonomous and
foresighted decisions that maximize the system's long-term performance, while
meeting the application's real-time delay constraints. The proposed solution
solved the cross-layer optimization offline, under the assumption that the
multimedia system's probabilistic dynamics were known a priori. In practice,
however, these dynamics are unknown a priori and therefore must be learned
online. In this paper, we address this problem by allowing the multimedia
system layers to learn, through repeated interactions with each other, to
autonomously optimize the system's long-term performance at run-time. We
propose two reinforcement learning algorithms for optimizing the system under
different design constraints: the first algorithm solves the cross-layer
optimization in a centralized manner, and the second solves it in a
decentralized manner. We analyze both algorithms in terms of their required
computation, memory, and inter-layer communication overheads. After noting that
the proposed reinforcement learning algorithms learn too slowly, we introduce a
complementary accelerated learning algorithm that exploits partial knowledge
about the system's dynamics in order to dramatically improve the system's
performance. In our experiments, we demonstrate that decentralized learning can
perform as well as centralized learning, while enabling the layers to act
autonomously. Additionally, we show that existing application-independent
reinforcement learning algorithms, and existing myopic learning algorithms
deployed in multimedia systems, perform significantly worse than our proposed
application-aware and foresighted learning methods.",arxiv
http://arxiv.org/abs/2101.05885v1,2021-01-14T21:59:04Z,2021-01-14T21:59:04Z,"Cocktail Edge Caching: Ride Dynamic Trends of Content Popularity with
  Ensemble Learning","Edge caching will play a critical role in facilitating the emerging
content-rich applications. However, it faces many new challenges, in
particular, the highly dynamic content popularity and the heterogeneous caching
configurations. In this paper, we propose Cocktail Edge Caching, that tackles
the dynamic popularity and heterogeneity through ensemble learning. Instead of
trying to find a single dominating caching policy for all the caching
scenarios, we employ an ensemble of constituent caching policies and adaptively
select the best-performing policy to control the cache. Towards this goal, we
first show through formal analysis and experiments that different variations of
the LFU and LRU policies have complementary performance in different caching
scenarios. We further develop a novel caching algorithm that enhances LFU/LRU
with deep recurrent neural network (LSTM) based time-series analysis. Finally,
we develop a deep reinforcement learning agent that adaptively combines base
caching policies according to their virtual hit ratios on parallel virtual
caches. Through extensive experiments driven by real content requests from two
large video streaming platforms, we demonstrate that CEC not only consistently
outperforms all single policies, but also improves the robustness of them. CEC
can be well generalized to different caching scenarios with low computation
overheads for deployment.",arxiv
http://arxiv.org/abs/1506.02328v2,2015-08-17T17:13:23Z,2015-06-08T00:34:51Z,"EventNet: A Large Scale Structured Concept Library for Complex Event
  Detection in Video","Event-specific concepts are the semantic concepts designed for the events of
interest, which can be used as a mid-level representation of complex events in
videos. Existing methods only focus on defining event-specific concepts for a
small number of predefined events, but cannot handle novel unseen events. This
motivates us to build a large scale event-specific concept library that covers
as many real-world events and their concepts as possible. Specifically, we
choose WikiHow, an online forum containing a large number of how-to articles on
human daily life events. We perform a coarse-to-fine event discovery process
and discover 500 events from WikiHow articles. Then we use each event name as
query to search YouTube and discover event-specific concepts from the tags of
returned videos. After an automatic filter process, we end up with 95,321
videos and 4,490 concepts. We train a Convolutional Neural Network (CNN) model
on the 95,321 videos over the 500 events, and use the model to extract deep
learning feature from video content. With the learned deep learning feature, we
train 4,490 binary SVM classifiers as the event-specific concept library. The
concepts and events are further organized in a hierarchical structure defined
by WikiHow, and the resultant concept library is called EventNet. Finally, the
EventNet concept library is used to generate concept based representation of
event videos. To the best of our knowledge, EventNet represents the first video
event ontology that organizes events and their concepts into a semantic
structure. It offers great potential for event retrieval and browsing.
Extensive experiments over the zero-shot event retrieval task when no training
samples are available show that the EventNet concept library consistently and
significantly outperforms the state-of-the-art (such as the 20K ImageNet
concepts trained with CNN) by a large margin up to 207%.",arxiv
http://arxiv.org/abs/2104.08002v1,2021-04-16T09:54:30Z,2021-04-16T09:54:30Z,Efficient and Generic 1D Dilated Convolution Layer for Deep Learning,"Convolutional neural networks (CNNs) have found many applications in tasks
involving two-dimensional (2D) data, such as image classification and image
processing. Therefore, 2D convolution layers have been heavily optimized on
CPUs and GPUs. However, in many applications - for example genomics and speech
recognition, the data can be one-dimensional (1D). Such applications can
benefit from optimized 1D convolution layers. In this work, we introduce our
efficient implementation of a generic 1D convolution layer covering a wide
range of parameters. It is optimized for x86 CPU architectures, in particular,
for architectures containing Intel AVX-512 and AVX-512 BFloat16 instructions.
We use the LIBXSMM library's batch-reduce General Matrix Multiplication
(BRGEMM) kernel for FP32 and BFloat16 precision. We demonstrate that our
implementation can achieve up to 80% efficiency on Intel Xeon Cascade Lake and
Cooper Lake CPUs. Additionally, we show the generalization capability of our
BRGEMM based approach by achieving high efficiency across a range of
parameters. We consistently achieve higher efficiency than the 1D convolution
layer with Intel oneDNN library backend for varying input tensor widths, filter
widths, number of channels, filters, and dilation parameters. Finally, we
demonstrate the performance of our optimized 1D convolution layer by utilizing
it in the end-to-end neural network training with real genomics datasets and
achieve up to 6.86x speedup over the oneDNN library-based implementation on
Cascade Lake CPUs. We also demonstrate the scaling with 16 sockets of
Cascade/Cooper Lake CPUs and achieve significant speedup over eight V100 GPUs
using a similar power envelop. In the end-to-end training, we get a speedup of
1.41x on Cascade Lake with FP32, 1.57x on Cooper Lake with FP32, and 2.27x on
Cooper Lake with BFloat16 over eight V100 GPUs with FP32.",arxiv
http://arxiv.org/abs/1909.07437v4,2020-12-17T02:27:29Z,2019-09-13T17:46:13Z,Heterogeneous Dataflow Accelerators for Multi-DNN Workloads,"Emerging AI-enabled applications such as augmented/virtual reality (AR/VR)
leverage multiple deep neural network (DNN) models for sub-tasks such as object
detection, hand tracking, and so on. Because of the diversity of the sub-tasks,
the layers within and across the DNN models are highly heterogeneous in
operation and shape. Such layer heterogeneity is a challenge for a fixed
dataflow accelerator (FDA) that employs a fixed dataflow on a single
accelerator substrate since each layer prefers different dataflows (computation
order and parallelization) and tile sizes. Reconfigurable DNN accelerators
(RDAs) have been proposed to adapt their dataflows to diverse layers to address
the challenge. However, the dataflow flexibility in RDAs is enabled at the area
and energy costs of expensive hardware structures (switches, controller, etc.)
and per-layer reconfiguration.
  Alternatively, this work proposes a new class of accelerators, heterogeneous
dataflow accelerators (HDAs), which deploys multiple sub-accelerators each
supporting a different dataflow. HDAs enable coarser-grained dataflow
flexibility than RDAs with higher energy efficiency and lower area cost
comparable to FDAs. To exploit such benefits, hardware resource partitioning
across sub-accelerators and layer execution schedule need to be carefully
optimized. Therefore, we also present Herald, which co-optimizes hardware
partitioning and layer execution schedule. Using Herald on a suite of AR/VR and
MLPerf workloads, we identify a promising HDA architecture, Maelstrom, which
demonstrates 65.3% lower latency and 5.0% lower energy than the best FDAs and
22.0% lower energy at the cost of 20.7% higher latency than a state-of-the-art
RDA. The results suggest that HDA is an alternative class of Pareto-optimal
accelerators to RDA with strength in energy, which can be a better choice than
RDAs depending on the use cases.",arxiv
http://arxiv.org/abs/1606.01393v1,2016-06-04T16:51:37Z,2016-06-04T16:51:37Z,"Automated Image Captioning for Rapid Prototyping and Resource
  Constrained Environments","Significant performance gains in deep learning coupled with the exponential
growth of image and video data on the Internet have resulted in the recent
emergence of automated image captioning systems. Ensuring scalability of
automated image captioning systems with respect to the ever increasing volume
of image and video data is a significant challenge. This paper provides a
valuable insight in that the detection of a few significant (top) objects in an
image allows one to extract other relevant information such as actions (verbs)
in the image. We expect this insight to be useful in the design of scalable
image captioning systems. We address two parameters by which the scalability of
image captioning systems could be quantified, i.e., the traditional algorithmic
time complexity which is important given the resource limitations of the user
device and the system development time since the programmers' time is a
critical resource constraint in many real-world scenarios. Additionally, we
address the issue of how word embeddings could be used to infer the verb
(action) from the nouns (objects) in a given image in a zero-shot manner. Our
results show that it is possible to attain reasonably good performance on
predicting actions and captioning images using our approaches with the added
advantage of simplicity of implementation.",arxiv
http://arxiv.org/abs/2006.01804v2,2020-07-05T19:17:05Z,2020-06-02T17:39:32Z,"Practical sensorless aberration estimation for 3D microscopy with deep
  learning","Estimation of optical aberrations from volumetric intensity images is a key
step in sensorless adaptive optics for 3D microscopy. Recent approaches based
on deep learning promise accurate results at fast processing speeds. However,
collecting ground truth microscopy data for training the network is typically
very difficult or even impossible thereby limiting this approach in practice.
Here, we demonstrate that neural networks trained only on simulated data yield
accurate predictions for real experimental images. We validate our approach on
simulated and experimental datasets acquired with two different microscopy
modalities, and also compare the results to non-learned methods. Additionally,
we study the predictability of individual aberrations with respect to their
data requirements and find that the symmetry of the wavefront plays a crucial
role. Finally, we make our implementation freely available as open source
software in Python.",arxiv
http://arxiv.org/abs/2010.14605v3,2021-06-07T10:06:48Z,2020-10-27T20:56:49Z,"Traffic Refinery: Cost-Aware Data Representation for Machine Learning on
  Network Traffic","Network management often relies on machine learning to make predictions about
performance and security from network traffic. Often, the representation of the
traffic is as important as the choice of the model. The features that the model
relies on, and the representation of those features, ultimately determine model
accuracy, as well as where and whether the model can be deployed in practice.
Thus, the design and evaluation of these models ultimately requires
understanding not only model accuracy but also the systems costs associated
with deploying the model in an operational network. Towards this goal, this
paper develops a new framework and system that enables a joint evaluation of
both the conventional notions of machine learning performance (e.g., model
accuracy) and the systems-level costs of different representations of network
traffic. We highlight these two dimensions for two practical network management
tasks, video streaming quality inference and malware detection, to demonstrate
the importance of exploring different representations to find the appropriate
operating point. We demonstrate the benefit of exploring a range of
representations of network traffic and present Traffic Refinery, a
proof-of-concept implementation that both monitors network traffic at 10 Gbps
and transforms traffic in real time to produce a variety of feature
representations for machine learning. Traffic Refinery both highlights this
design space and makes it possible to explore different representations for
learning, balancing systems costs related to feature extraction and model
training against model accuracy.",arxiv
http://arxiv.org/abs/2011.11557v1,2020-11-23T17:11:50Z,2020-11-23T17:11:50Z,"Planar 3D Transfer Learning for End to End Unimodal MRI Unbalanced Data
  Segmentation","We present a novel approach of 2D to 3D transfer learning based on mapping
pre-trained 2D convolutional neural network weights into planar 3D kernels. The
method is validated by the proposed planar 3D res-u-net network with encoder
transferred from the 2D VGG-16, which is applied for a single-stage unbalanced
3D image data segmentation. In particular, we evaluate the method on the MICCAI
2016 MS lesion segmentation challenge dataset utilizing solely fluid-attenuated
inversion recovery (FLAIR) sequence without brain extraction for training and
inference to simulate real medical praxis. The planar 3D res-u-net network
performed the best both in sensitivity and Dice score amongst end to end
methods processing raw MRI scans and achieved comparable Dice score to a
state-of-the-art unimodal not end to end approach. Complete source code was
released under the open-source license, and this paper complies with the
Machine learning reproducibility checklist. By implementing practical transfer
learning for 3D data representation, we could segment heavily unbalanced data
without selective sampling and achieved more reliable results using less
training data in a single modality. From a medical perspective, the unimodal
approach gives an advantage in real praxis as it does not require
co-registration nor additional scanning time during an examination. Although
modern medical imaging methods capture high-resolution 3D anatomy scans
suitable for computer-aided detection system processing, deployment of
automatic systems for interpretation of radiology imaging is still rather
theoretical in many medical areas. Our work aims to bridge the gap by offering
a solution for partial research questions.",arxiv
http://arxiv.org/abs/1702.07825v2,2017-03-07T23:09:23Z,2017-02-25T03:11:04Z,Deep Voice: Real-time Neural Text-to-Speech,"We present Deep Voice, a production-quality text-to-speech system constructed
entirely from deep neural networks. Deep Voice lays the groundwork for truly
end-to-end neural speech synthesis. The system comprises five major building
blocks: a segmentation model for locating phoneme boundaries, a
grapheme-to-phoneme conversion model, a phoneme duration prediction model, a
fundamental frequency prediction model, and an audio synthesis model. For the
segmentation model, we propose a novel way of performing phoneme boundary
detection with deep neural networks using connectionist temporal classification
(CTC) loss. For the audio synthesis model, we implement a variant of WaveNet
that requires fewer parameters and trains faster than the original. By using a
neural network for each component, our system is simpler and more flexible than
traditional text-to-speech systems, where each component requires laborious
feature engineering and extensive domain expertise. Finally, we show that
inference with our system can be performed faster than real time and describe
optimized WaveNet inference kernels on both CPU and GPU that achieve up to 400x
speedups over existing implementations.",arxiv
http://arxiv.org/abs/1911.06091v1,2019-11-14T13:49:23Z,2019-11-14T13:49:23Z,"EdgeNet: Balancing Accuracy and Performance for Edge-based Convolutional
  Neural Network Object Detectors","Visual intelligence at the edge is becoming a growing necessity for low
latency applications and situations where real-time decision is vital. Object
detection, the first step in visual data analytics, has enjoyed significant
improvements in terms of state-of-the-art accuracy due to the emergence of
Convolutional Neural Networks (CNNs) and Deep Learning. However, such complex
paradigms intrude increasing computational demands and hence prevent their
deployment on resource-constrained devices. In this work, we propose a
hierarchical framework that enables to detect objects in high-resolution video
frames, and maintain the accuracy of state-of-the-art CNN-based object
detectors while outperforming existing works in terms of processing speed when
targeting a low-power embedded processor using an intelligent data reduction
mechanism. Moreover, a use-case for pedestrian detection from
Unmanned-Areal-Vehicle (UAV) is presented showing the impact that the proposed
approach has on sensitivity, average processing time and power consumption when
is implemented on different platforms. Using the proposed selection process our
framework manages to reduce the processed data by 100x leading to under 4W
power consumption on different edge devices.",arxiv
http://arxiv.org/abs/2103.04958v1,2021-03-08T18:28:53Z,2021-03-08T18:28:53Z,"F-CAD: A Framework to Explore Hardware Accelerators for Codec Avatar
  Decoding","Creating virtual avatars with realistic rendering is one of the most
essential and challenging tasks to provide highly immersive virtual reality
(VR) experiences. It requires not only sophisticated deep neural network (DNN)
based codec avatar decoders to ensure high visual quality and precise motion
expression, but also efficient hardware accelerators to guarantee smooth
real-time rendering using lightweight edge devices, like untethered VR
headsets. Existing hardware accelerators, however, fail to deliver sufficient
performance and efficiency targeting such decoders which consist of
multi-branch DNNs and require demanding compute and memory resources. To
address these problems, we propose an automation framework, called F-CAD
(Facebook Codec avatar Accelerator Design), to explore and deliver optimized
hardware accelerators for codec avatar decoding. Novel technologies include 1)
a new accelerator architecture to efficiently handle multi-branch DNNs; 2) a
multi-branch dynamic design space to enable fine-grained architecture
configurations; and 3) an efficient architecture search for picking the
optimized hardware design based on both application-specific demands and
hardware resource constraints. To the best of our knowledge, F-CAD is the first
automation tool that supports the whole design flow of hardware acceleration of
codec avatar decoders, allowing joint optimization on decoder designs in
popular machine learning frameworks and corresponding customized accelerator
design with cycle-accurate evaluation. Results show that the accelerators
generated by F-CAD can deliver up to 122.1 frames per second (FPS) and 91.6%
hardware efficiency when running the latest codec avatar decoder. Compared to
the state-of-the-art designs, F-CAD achieves 4.0X and 2.8X higher throughput,
62.5% and 21.2% higher efficiency than DNNBuilder and HybridDNN by targeting
the same hardware device.",arxiv
http://arxiv.org/abs/1908.11312v1,2019-08-29T15:53:40Z,2019-08-29T15:53:40Z,"Flexible Conditional Image Generation of Missing Data with Learned
  Mental Maps","Real-world settings often do not allow acquisition of high-resolution
volumetric images for accurate morphological assessment and diagnostic. In
clinical practice it is frequently common to acquire only sparse data (e.g.
individual slices) for initial diagnostic decision making. Thereby, physicians
rely on their prior knowledge (or mental maps) of the human anatomy to
extrapolate the underlying 3D information. Accurate mental maps require years
of anatomy training, which in the first instance relies on normative learning,
i.e. excluding pathology. In this paper, we leverage Bayesian Deep Learning and
environment mapping to generate full volumetric anatomy representations from
none to a small, sparse set of slices. We evaluate proof of concept
implementations based on Generative Query Networks (GQN) and Conditional BRUNO
using abdominal CT and brain MRI as well as in a clinical application involving
sparse, motion-corrupted MR acquisition for fetal imaging. Our approach allows
to reconstruct 3D volumes from 1 to 4 tomographic slices, with a SSIM of 0.7+
and cross-correlation of 0.8+ compared to the 3D ground truth.",arxiv
http://arxiv.org/abs/1803.10193v2,2019-08-05T18:56:15Z,2018-03-27T17:31:47Z,"HDM-Net: Monocular Non-Rigid 3D Reconstruction with Learned Deformation
  Model","Monocular dense 3D reconstruction of deformable objects is a hard ill-posed
problem in computer vision. Current techniques either require dense
correspondences and rely on motion and deformation cues, or assume a highly
accurate reconstruction (referred to as a template) of at least a single frame
given in advance and operate in the manner of non-rigid tracking. Accurate
computation of dense point tracks often requires multiple frames and might be
computationally expensive. Availability of a template is a very strong prior
which restricts system operation to a pre-defined environment and scenarios. In
this work, we propose a new hybrid approach for monocular non-rigid
reconstruction which we call Hybrid Deformation Model Network (HDM-Net). In our
approach, deformation model is learned by a deep neural network, with a
combination of domain-specific loss functions. We train the network with
multiple states of a non-rigidly deforming structure with a known shape at
rest. HDM-Net learns different reconstruction cues including texture-dependent
surface deformations, shading and contours. We show generalisability of HDM-Net
to states not presented in the training dataset, with unseen textures and under
new illumination conditions. Experiments with noisy data and a comparison with
other methods demonstrate robustness and accuracy of the proposed approach and
suggest possible application scenarios of the new technique in interventional
diagnostics and augmented reality.",arxiv
http://arxiv.org/abs/2004.07945v1,2020-04-15T13:40:03Z,2020-04-15T13:40:03Z,ALCN: Adaptive Local Contrast Normalization,"To make Robotics and Augmented Reality applications robust to illumination
changes, the current trend is to train a Deep Network with training images
captured under many different lighting conditions. Unfortunately, creating such
a training set is a very unwieldy and complex task. We therefore propose a
novel illumination normalization method that can easily be used for different
problems with challenging illumination conditions. Our preliminary experiments
show that among current normalization methods, the Difference-of Gaussians
method remains a very good baseline, and we introduce a novel illumination
normalization model that generalizes it. Our key insight is then that the
normalization parameters should depend on the input image, and we aim to train
a Convolutional Neural Network to predict these parameters from the input
image. This, however, cannot be done in a supervised manner, as the optimal
parameters are not known a priori. We thus designed a method to train this
network jointly with another network that aims to recognize objects under
different illuminations: The latter network performs well when the former
network predicts good values for the normalization parameters. We show that our
method significantly outperforms standard normalization methods and would also
be appear to be universal since it does not have to be re-trained for each new
application. Our method improves the robustness to light changes of
state-of-the-art 3D object detection and face recognition methods.",arxiv
http://arxiv.org/abs/2107.01002v2,2021-09-27T13:42:44Z,2021-05-31T12:09:46Z,"WiCluster: Passive Indoor 2D/3D Positioning using WiFi without Precise
  Labels","We introduce WiCluster, a new machine learning (ML) approach for passive
indoor positioning using radio frequency (RF) channel state information (CSI).
WiCluster can predict both a zone-level position and a precise 2D or 3D
position, without using any precise position labels during training. Prior
CSI-based indoor positioning work has relied on non-parametric approaches using
digital signal-processing (DSP) and, more recently, parametric approaches
(e.g., fully supervised ML methods). However these do not handle the complexity
of real-world environments well and do not meet requirements for large-scale
commercial deployments: the accuracy of DSP-based method deteriorates
significantly in non-line-of-sight conditions, while supervised ML methods need
large amounts of hard-to-acquire centimeter accuracy position labels. In
contrast, WiCluster is precise, requires weaker label-information that can be
easily collected, and works well in non-line-of-sight conditions. Our first
contribution is a novel dimensionality reduction method for charting. It
combines a triplet-loss with a multi-scale clustering-loss to map the
high-dimensional CSI representation to a 2D/3D latent space. Our second
contribution is two weakly supervised losses that map this latent space into a
Cartesian map, resulting in meter-accuracy position results. These losses only
require simple to acquire priors: a sketch of the floorplan, approximate
access-point locations and a few CSI packets that are labelled with the
corresponding zone in the floorplan. Thirdly, we report results and a
robustness study for 2D positioning in two single-floor office buildings and 3D
positioning in a two-story home.",arxiv
http://arxiv.org/abs/2008.13419v1,2020-08-31T08:24:06Z,2020-08-31T08:24:06Z,"Integrative Object and Pose to Task Detection for an
  Augmented-Reality-based Human Assistance System using Neural Networks","As a result of an increasingly automatized and digitized industry, processes
are becoming more complex. Augmented Reality has shown considerable potential
in assisting workers with complex tasks by enhancing user understanding and
experience with spatial information. However, the acceptance and integration of
AR into industrial processes is still limited due to the lack of established
methods and tedious integration efforts. Meanwhile, deep neural networks have
achieved remarkable results in computer vision tasks and bear great prospects
to enrich Augmented Reality applications . In this paper, we propose an
Augmented-Reality-based human assistance system to assist workers in complex
manual tasks where we incorporate deep neural networks for computer vision
tasks. More specifically, we combine Augmented Reality with object and action
detectors to make workflows more intuitive and flexible. To evaluate our system
in terms of user acceptance and efficiency, we conducted several user studies.
We found a significant reduction in time to task completion in untrained
workers and a decrease in error rate. Furthermore, we investigated the users
learning curve with our assistance system.",arxiv
http://arxiv.org/abs/2003.11100v1,2020-03-24T20:15:12Z,2020-03-24T20:15:12Z,"How deep is your encoder: an analysis of features descriptors for an
  autoencoder-based audio-visual quality metric","The development of audio-visual quality assessment models poses a number of
challenges in order to obtain accurate predictions. One of these challenges is
the modelling of the complex interaction that audio and visual stimuli have and
how this interaction is interpreted by human users. The No-Reference
Audio-Visual Quality Metric Based on a Deep Autoencoder (NAViDAd) deals with
this problem from a machine learning perspective. The metric receives two sets
of audio and video features descriptors and produces a low-dimensional set of
features used to predict the audio-visual quality. A basic implementation of
NAViDAd was able to produce accurate predictions tested with a range of
different audio-visual databases. The current work performs an ablation study
on the base architecture of the metric. Several modules are removed or
re-trained using different configurations to have a better understanding of the
metric functionality. The results presented in this study provided important
feedback that allows us to understand the real capacity of the metric's
architecture and eventually develop a much better audio-visual quality metric.",arxiv
http://arxiv.org/abs/2104.02306v1,2021-04-06T06:04:57Z,2021-04-06T06:04:57Z,Binary Neural Network for Speaker Verification,"Although deep neural networks are successful for many tasks in the speech
domain, the high computational and memory costs of deep neural networks make it
difficult to directly deploy highperformance Neural Network systems on
low-resource embedded devices. There are several mechanisms to reduce the size
of the neural networks i.e. parameter pruning, parameter quantization, etc.
This paper focuses on how to apply binary neural networks to the task of
speaker verification. The proposed binarization of training parameters can
largely maintain the performance while significantly reducing storage space
requirements and computational costs. Experiment results show that, after
binarizing the Convolutional Neural Network, the ResNet34-based network
achieves an EER of around 5% on the Voxceleb1 testing dataset and even
outperforms the traditional real number network on the text-dependent dataset:
Xiaole while having a 32x memory saving.",arxiv
http://arxiv.org/abs/1901.05147v1,2019-01-16T06:10:45Z,2019-01-16T06:10:45Z,The Winning Solution to the IEEE CIG 2017 Game Data Mining Competition,"Machine learning competitions such as those organized by Kaggle or KDD
represent a useful benchmark for data science research. In this work, we
present our winning solution to the Game Data Mining competition hosted at the
2017 IEEE Conference on Computational Intelligence and Games (CIG 2017). The
contest consisted of two tracks, and participants (more than 250, belonging to
both industry and academia) were to predict which players would stop playing
the game, as well as their remaining lifetime. The data were provided by a
major worldwide video game company, NCSoft, and came from their successful
massively multiplayer online game Blade and Soul. Here, we describe the long
short-term memory approach and conditional inference survival ensemble model
that made us win both tracks of the contest, as well as the validation
procedure that we followed in order to prevent overfitting. In particular,
choosing a survival method able to deal with censored data was crucial to
accurately predict the moment in which each player would leave the game, as
censoring is inherent in churn. The selected models proved to be robust against
evolving conditions---since there was a change in the business model of the
game (from subscription-based to free-to-play) between the two sample datasets
provided---and efficient in terms of time cost. Thanks to these features and
also to their a ability to scale to large datasets, our models could be readily
implemented in real business settings.",arxiv
http://arxiv.org/abs/1610.07104v1,2016-10-22T23:08:01Z,2016-10-22T23:08:01Z,Independent Component Analysis by Entropy Maximization with Kernels,"Independent component analysis (ICA) is the most popular method for blind
source separation (BSS) with a diverse set of applications, such as biomedical
signal processing, video and image analysis, and communications. Maximum
likelihood (ML), an optimal theoretical framework for ICA, requires knowledge
of the true underlying probability density function (PDF) of the latent
sources, which, in many applications, is unknown. ICA algorithms cast in the ML
framework often deviate from its theoretical optimality properties due to poor
estimation of the source PDF. Therefore, accurate estimation of source PDFs is
critical in order to avoid model mismatch and poor ICA performance. In this
paper, we propose a new and efficient ICA algorithm based on entropy
maximization with kernels, (ICA-EMK), which uses both global and local
measuring functions as constraints to dynamically estimate the PDF of the
sources with reasonable complexity. In addition, the new algorithm performs
optimization with respect to each of the cost function gradient directions
separately, enabling parallel implementations on multi-core computers. We
demonstrate the superior performance of ICA-EMK over competing ICA algorithms
using simulated as well as real-world data.",arxiv
http://arxiv.org/abs/1801.03604v1,2018-01-11T01:23:50Z,2018-01-11T01:23:50Z,Conversational AI: The Science Behind the Alexa Prize,"Conversational agents are exploding in popularity. However, much work remains
in the area of social conversation as well as free-form conversation over a
broad range of domains and topics. To advance the state of the art in
conversational AI, Amazon launched the Alexa Prize, a 2.5-million-dollar
university competition where sixteen selected university teams were challenged
to build conversational agents, known as socialbots, to converse coherently and
engagingly with humans on popular topics such as Sports, Politics,
Entertainment, Fashion and Technology for 20 minutes. The Alexa Prize offers
the academic community a unique opportunity to perform research with a live
system used by millions of users. The competition provided university teams
with real user conversational data at scale, along with the user-provided
ratings and feedback augmented with annotations by the Alexa team. This enabled
teams to effectively iterate and make improvements throughout the competition
while being evaluated in real-time through live user interactions. To build
their socialbots, university teams combined state-of-the-art techniques with
novel strategies in the areas of Natural Language Understanding, Context
Modeling, Dialog Management, Response Generation, and Knowledge Acquisition. To
support the efforts of participating teams, the Alexa Prize team made
significant scientific and engineering investments to build and improve
Conversational Speech Recognition, Topic Tracking, Dialog Evaluation, Voice
User Experience, and tools for traffic management and scalability. This paper
outlines the advances created by the university teams as well as the Alexa
Prize team to achieve the common goal of solving the problem of Conversational
AI.",arxiv
http://arxiv.org/abs/1801.05889v1,2017-12-06T02:49:27Z,2017-12-06T02:49:27Z,"Perceived Audiovisual Quality Modelling based on Decison Trees, Genetic
  Programming and Neural Networks","Our objective is to build machine learning based models that predict
audiovisual quality directly from a set of correlated parameters that are
extracted from a target quality dataset. We have used the bitstream version of
the INRS audiovisual quality dataset that reflects contemporary real-time
configurations for video frame rate, video quantization, noise reduction
parameters and network packet loss rate. We have utilized this dataset to build
bitstream perceived quality estimation models based on the Random Forests,
Bagging, Deep Learning and Genetic Programming methods.
  We have taken an empirical approach and have generated models varying from
very simple to the most complex depending on the number of features used from
the quality dataset. Random Forests and Bagging models have overall generated
the most accurate results in terms of RMSE and Pearson correlation coefficient
values. Deep Learning and Genetic Programming based bitstream models have also
achieved good results but that high performance was observed only with a
limited range of features. We have also obtained the epsilon-insensitive RMSE
values for each model and have computed the significance of the difference
between the correlation coefficients.
  Overall we conclude that computing the bitstream information is worth the
effort it takes to generate and helps to build more accurate models for
real-time communications. However, it is useful only for the deployment of the
right algorithms with the carefully selected subset of the features. The
dataset and tools that have been developed during this research are publicly
available for research and development purposes.",arxiv
http://arxiv.org/abs/1910.08811v1,2019-10-19T17:56:43Z,2019-10-19T17:56:43Z,"Active 6D Multi-Object Pose Estimation in Cluttered Scenarios with Deep
  Reinforcement Learning","In this work, we explore how a strategic selection of camera movements can
facilitate the task of 6D multi-object pose estimation in cluttered scenarios
while respecting real-world constraints important in robotics and augmented
reality applications, such as time and distance traveled. In the proposed
framework, a set of multiple object hypotheses is given to an agent, which is
inferred by an object pose estimator and subsequently spatio-temporally
selected by a fusion function that makes use of a verification score that
circumvents the need of ground-truth annotations. The agent reasons about these
hypotheses, directing its attention to the object which it is most uncertain
about, moving the camera towards such an object. Unlike previous works that
propose short-sighted policies, our agent is trained in simulated scenarios
using reinforcement learning, attempting to learn the camera moves that produce
the most accurate object poses hypotheses for a given temporal and spatial
budget, without the need of viewpoints rendering during inference. Our
experiments show that the proposed approach successfully estimates the 6D
object pose of a stack of objects in both challenging cluttered synthetic and
real scenarios, showing superior performance compared to strong baselines.",arxiv
http://arxiv.org/abs/1801.01949v2,2018-08-22T09:28:14Z,2018-01-06T00:41:10Z,"Face Flashing: a Secure Liveness Detection Protocol based on Light
  Reflections","Face authentication systems are becoming increasingly prevalent, especially
with the rapid development of Deep Learning technologies. However, human facial
information is easy to be captured and reproduced, which makes face
authentication systems vulnerable to various attacks. Liveness detection is an
important defense technique to prevent such attacks, but existing solutions did
not provide clear and strong security guarantees, especially in terms of time.
  To overcome these limitations, we propose a new liveness detection protocol
called Face Flashing that significantly increases the bar for launching
successful attacks on face authentication systems. By randomly flashing
well-designed pictures on a screen and analyzing the reflected light, our
protocol has leveraged physical characteristics of human faces: reflection
processing at the speed of light, unique textual features, and uneven 3D
shapes. Cooperating with working mechanism of the screen and digital cameras,
our protocol is able to detect subtle traces left by an attacking process.
  To demonstrate the effectiveness of Face Flashing, we implemented a prototype
and performed thorough evaluations with large data set collected from
real-world scenarios. The results show that our Timing Verification can
effectively detect the time gap between legitimate authentications and
malicious cases. Our Face Verification can also differentiate 2D plane from 3D
objects accurately. The overall accuracy of our liveness detection system is
98.8\%, and its robustness was evaluated in different scenarios. In the worst
case, our system's accuracy decreased to a still-high 97.3\%.",arxiv
http://arxiv.org/abs/1712.01340v1,2017-12-04T20:39:05Z,2017-12-04T20:39:05Z,Precision Scaling of Neural Networks for Efficient Audio Processing,"While deep neural networks have shown powerful performance in many audio
applications, their large computation and memory demand has been a challenge
for real-time processing. In this paper, we study the impact of scaling the
precision of neural networks on the performance of two common audio processing
tasks, namely, voice-activity detection and single-channel speech enhancement.
We determine the optimal pair of weight/neuron bit precision by exploring its
impact on both the performance and processing time. Through experiments
conducted with real user data, we demonstrate that deep neural networks that
use lower bit precision significantly reduce the processing time (up to 30x).
However, their performance impact is low (< 3.14%) only in the case of
classification tasks such as those present in voice activity detection.",arxiv
http://arxiv.org/abs/1908.11140v3,2020-06-15T12:10:43Z,2019-08-29T10:24:10Z,"Estimation of a function of low local dimensionality by deep neural
  networks","Deep neural networks (DNNs) achieve impressive results for complicated tasks
like object detection on images and speech recognition. Motivated by this
practical success, there is now a strong interest in showing good theoretical
properties of DNNs. To describe for which tasks DNNs perform well and when they
fail, it is a key challenge to understand their performance. The aim of this
paper is to contribute to the current statistical theory of DNNs. We apply DNNs
on high dimensional data and we show that the least squares regression
estimates using DNNs are able to achieve dimensionality reduction in case that
the regression function has locally low dimensionality. Consequently, the rate
of convergence of the estimate does not depend on its input dimension $d$, but
on its local dimension $d^*$ and the DNNs are able to circumvent the curse of
dimensionality in case that $d^*$ is much smaller than $d$. In our simulation
study we provide numerical experiments to support our theoretical result and we
compare our estimate with other conventional nonparametric regression
estimates. The performance of our estimates is also validated in experiments
with real data.",arxiv
http://arxiv.org/abs/1910.10105v2,2020-04-17T13:12:46Z,2019-10-22T16:45:38Z,"Modeling plate and spring reverberation using a DSP-informed deep neural
  network","Plate and spring reverberators are electromechanical systems first used and
researched as means to substitute real room reverberation. Nowadays they are
often used in music production for aesthetic reasons due to their particular
sonic characteristics. The modeling of these audio processors and their
perceptual qualities is difficult since they use mechanical elements together
with analog electronics resulting in an extremely complex response. Based on
digital reverberators that use sparse FIR filters, we propose a signal
processing-informed deep learning architecture for the modeling of artificial
reverberators. We explore the capabilities of deep neural networks to learn
such highly nonlinear electromechanical responses and we perform modeling of
plate and spring reverberators. In order to measure the performance of the
model, we conduct a perceptual evaluation experiment and we also analyze how
the given task is accomplished and what the model is actually learning.",arxiv
http://arxiv.org/abs/2007.15152v2,2020-09-18T01:32:28Z,2020-07-29T23:41:33Z,"Accelerating Multi-attribute Unsupervised Seismic Facies Analysis With
  RAPIDS","Classification of seismic facies is done by clustering seismic data samples
based on their attributes. Year after year, 3D datasets used by exploration
geophysics increase in size, complexity, and number of attributes, requiring a
continuous rise in the classification performance. In this work, we explore the
use of Graphics Processing Units (GPUs) to perform the classification of
seismic surveys using the well-established Machine Learning (ML) method
k-means. We show that the high-performance distributed implementation of the
k-means algorithm available at the RAPIDS library can be used to classify
facies in large seismic datasets much faster than a classical parallel CPU
implementation (up to 258-fold faster in NVIDIA V100 GPUs), especially for
large seismic blocks. We tested the algorithm with different real seismic
volumes, including Netherlands, Parihaka, and Kahu (from 12GB to 66GB).",arxiv
http://arxiv.org/abs/1909.10964v2,2019-10-19T13:57:12Z,2019-09-24T14:45:43Z,A System-Level Solution for Low-Power Object Detection,"Object detection has made impressive progress in recent years with the help
of deep learning. However, state-of-the-art algorithms are both computation and
memory intensive. Though many lightweight networks are developed for a
trade-off between accuracy and efficiency, it is still a challenge to make it
practical on an embedded device. In this paper, we present a system-level
solution for efficient object detection on a heterogeneous embedded device. The
detection network is quantized to low bits and allows efficient implementation
with shift operators. In order to make the most of the benefits of low-bit
quantization, we design a dedicated accelerator with programmable logic. Inside
the accelerator, a hybrid dataflow is exploited according to the heterogeneous
property of different convolutional layers. We adopt a straightforward but
resource-friendly column-prior tiling strategy to map the computation-intensive
convolutional layers to the accelerator that can support arbitrary feature
size. Other operations can be performed on the low-power CPU cores, and the
entire system is executed in a pipelined manner. As a case study, we evaluate
our object detection system on a real-world surveillance video with input size
of 512x512, and it turns out that the system can achieve an inference speed of
18 fps at the cost of 6.9W (with display) with an mAP of 66.4 verified on the
PASCAL VOC 2012 dataset.",arxiv
http://arxiv.org/abs/1907.07500v2,2020-07-14T16:03:07Z,2019-07-17T13:20:15Z,Learning Variable Impedance Control for Contact Sensitive Tasks,"Reinforcement learning algorithms have shown great success in solving
different problems ranging from playing video games to robotics. However, they
struggle to solve delicate robotic problems, especially those involving contact
interactions. Though in principle a policy directly outputting joint torques
should be able to learn to perform these tasks, in practice we see that it has
difficulty to robustly solve the problem without any given structure in the
action space. In this paper, we investigate how the choice of action space can
give robust performance in presence of contact uncertainties. We propose
learning a policy giving as output impedance and desired position in joint
space and compare the performance of that approach to torque and position
control under different contact uncertainties. Furthermore, we propose an
additional reward term designed to regularize these variable impedance control
policies, giving them interpretability and facilitating their transfer to real
systems. We present extensive experiments in simulation of both floating and
fixed-base systems in tasks involving contact uncertainties, as well as results
for running the learned policies on a real system.",arxiv
http://arxiv.org/abs/2003.10315v1,2020-03-23T15:04:30Z,2020-03-23T15:04:30Z,Adversarial Attacks on Monocular Depth Estimation,"Recent advances of deep learning have brought exceptional performance on many
computer vision tasks such as semantic segmentation and depth estimation.
However, the vulnerability of deep neural networks towards adversarial examples
have caused grave concerns for real-world deployment. In this paper, we present
to the best of our knowledge the first systematic study of adversarial attacks
on monocular depth estimation, an important task of 3D scene understanding in
scenarios such as autonomous driving and robot navigation. In order to
understand the impact of adversarial attacks on depth estimation, we first
define a taxonomy of different attack scenarios for depth estimation, including
non-targeted attacks, targeted attacks and universal attacks. We then adapt
several state-of-the-art attack methods for classification on the field of
depth estimation. Besides, multi-task attacks are introduced to further improve
the attack performance for universal attacks. Experimental results show that it
is possible to generate significant errors on depth estimation. In particular,
we demonstrate that our methods can conduct targeted attacks on given objects
(such as a car), resulting in depth estimation 3-4x away from the ground truth
(e.g., from 20m to 80m).",arxiv
http://arxiv.org/abs/1804.05374v2,2018-06-12T01:00:03Z,2018-04-15T15:52:16Z,Twin Regularization for online speech recognition,"Online speech recognition is crucial for developing natural human-machine
interfaces. This modality, however, is significantly more challenging than
off-line ASR, since real-time/low-latency constraints inevitably hinder the use
of future information, that is known to be very helpful to perform robust
predictions. A popular solution to mitigate this issue consists of feeding
neural acoustic models with context windows that gather some future frames.
This introduces a latency which depends on the number of employed look-ahead
features. This paper explores a different approach, based on estimating the
future rather than waiting for it. Our technique encourages the hidden
representations of a unidirectional recurrent network to embed some useful
information about the future. Inspired by a recently proposed technique called
Twin Networks, we add a regularization term that forces forward hidden states
to be as close as possible to cotemporal backward ones, computed by a ""twin""
neural network running backwards in time. The experiments, conducted on a
number of datasets, recurrent architectures, input features, and acoustic
conditions, have shown the effectiveness of this approach. One important
advantage is that our method does not introduce any additional computation at
test time if compared to standard unidirectional recurrent networks.",arxiv
http://arxiv.org/abs/2011.00975v1,2020-11-02T13:50:59Z,2020-11-02T13:50:59Z,DNN-Based Semantic Model for Rescoring N-best Speech Recognition List,"The word error rate (WER) of an automatic speech recognition (ASR) system
increases when a mismatch occurs between the training and the testing
conditions due to the noise, etc. In this case, the acoustic information can be
less reliable. This work aims to improve ASR by modeling long-term semantic
relations to compensate for distorted acoustic features. We propose to perform
this through rescoring of the ASR N-best hypotheses list. To achieve this, we
train a deep neural network (DNN). Our DNN rescoring model is aimed at
selecting hypotheses that have better semantic consistency and therefore lower
WER. We investigate two types of representations as part of input features to
our DNN model: static word embeddings (from word2vec) and dynamic contextual
embeddings (from BERT). Acoustic and linguistic features are also included. We
perform experiments on the publicly available dataset TED-LIUM mixed with real
noise. The proposed rescoring approaches give significant improvement of the
WER over the ASR system without rescoring models in two noisy conditions and
with n-gram and RNNLM.",arxiv
http://arxiv.org/abs/2101.06702v3,2021-07-31T19:44:16Z,2021-01-17T16:19:47Z,"Deep Learning based Virtual Point Tracking for Real-Time Target-less
  Dynamic Displacement Measurement in Railway Applications","In the application of computer-vision based displacement measurement, an
optical target is usually required to prove the reference. In the case that the
optical target cannot be attached to the measuring objective, edge detection,
feature matching and template matching are the most common approaches in
target-less photogrammetry. However, their performance significantly relies on
parameter settings. This becomes problematic in dynamic scenes where
complicated background texture exists and varies over time. To tackle this
issue, we propose virtual point tracking for real-time target-less dynamic
displacement measurement, incorporating deep learning techniques and domain
knowledge. Our approach consists of three steps: 1) automatic calibration for
detection of region of interest; 2) virtual point detection for each video
frame using deep convolutional neural network; 3) domain-knowledge based rule
engine for point tracking in adjacent frames. The proposed approach can be
executed on an edge computer in a real-time manner (i.e. over 30 frames per
second). We demonstrate our approach for a railway application, where the
lateral displacement of the wheel on the rail is measured during operation. We
also implement an algorithm using template matching and line detection as the
baseline for comparison. The numerical experiments have been performed to
evaluate the performance and the latency of our approach in the harsh railway
environment with noisy and varying backgrounds.",arxiv
http://arxiv.org/abs/1912.03884v1,2019-12-09T07:44:32Z,2019-12-09T07:44:32Z,"MITAS: A Compressed Time-Domain Audio Separation Network with Parameter
  Sharing","Deep learning methods have brought substantial advancements in speech
separation (SS). Nevertheless, it remains challenging to deploy
deep-learning-based models on edge devices. Thus, identifying an effective way
to compress these large models without hurting SS performance has become an
important research topic. Recently, TasNet and Conv-TasNet have been proposed.
They achieved state-of-the-art results on several standardized SS tasks.
Moreover, their low latency natures make them definitely suitable for real-time
on-device applications. In this study, we propose two parameter-sharing schemes
to lower the memory consumption on TasNet and Conv-TasNet. Accordingly, we
derive a novel so-called MiTAS (Mini TasNet). Our experimental results first
confirmed the robustness of our MiTAS on two types of perturbations in mixed
audio. We also designed a series of ablation experiments to analyze the
relation between SS performance and the amount of parameters in the model. The
results show that MiTAS is able to reduce the model size by a factor of four
while maintaining comparable SS performance with improved stability as compared
to TasNet and Conv-TasNet. This suggests that MiTAS is more suitable for
real-time low latency applications.",arxiv
http://arxiv.org/abs/2007.13866v1,2020-07-27T21:09:36Z,2020-07-27T21:09:36Z,"se(3)-TrackNet: Data-driven 6D Pose Tracking by Calibrating Image
  Residuals in Synthetic Domains","Tracking the 6D pose of objects in video sequences is important for robot
manipulation. This task, however, introduces multiple challenges: (i) robot
manipulation involves significant occlusions; (ii) data and annotations are
troublesome and difficult to collect for 6D poses, which complicates machine
learning solutions, and (iii) incremental error drift often accumulates in long
term tracking to necessitate re-initialization of the object's pose. This work
proposes a data-driven optimization approach for long-term, 6D pose tracking.
It aims to identify the optimal relative pose given the current RGB-D
observation and a synthetic image conditioned on the previous best estimate and
the object's model. The key contribution in this context is a novel neural
network architecture, which appropriately disentangles the feature encoding to
help reduce domain shift, and an effective 3D orientation representation via
Lie Algebra. Consequently, even when the network is trained only with synthetic
data can work effectively over real images. Comprehensive experiments over
benchmarks - existing ones as well as a new dataset with significant occlusions
related to object manipulation - show that the proposed approach achieves
consistently robust estimates and outperforms alternatives, even though they
have been trained with real images. The approach is also the most
computationally efficient among the alternatives and achieves a tracking
frequency of 90.9Hz.",arxiv
http://arxiv.org/abs/1809.03137v3,2019-04-09T02:02:16Z,2018-09-10T04:59:25Z,"Tracking by Animation: Unsupervised Learning of Multi-Object Attentive
  Trackers","Online Multi-Object Tracking (MOT) from videos is a challenging computer
vision task which has been extensively studied for decades. Most of the
existing MOT algorithms are based on the Tracking-by-Detection (TBD) paradigm
combined with popular machine learning approaches which largely reduce the
human effort to tune algorithm parameters. However, the commonly used
supervised learning approaches require the labeled data (e.g., bounding boxes),
which is expensive for videos. Also, the TBD framework is usually suboptimal
since it is not end-to-end, i.e., it considers the task as detection and
tracking, but not jointly. To achieve both label-free and end-to-end learning
of MOT, we propose a Tracking-by-Animation framework, where a differentiable
neural model first tracks objects from input frames and then animates these
objects into reconstructed frames. Learning is then driven by the
reconstruction error through backpropagation. We further propose a
Reprioritized Attentive Tracking to improve the robustness of data association.
Experiments conducted on both synthetic and real video datasets show the
potential of the proposed model. Our project page is publicly available at:
https://github.com/zhen-he/tracking-by-animation",arxiv
http://arxiv.org/abs/2108.13344v1,2021-08-30T16:11:59Z,2021-08-30T16:11:59Z,"Enlisting 3D Crop Models and GANs for More Data Efficient and
  Generalizable Fruit Detection","Training real-world neural network models to achieve high performance and
generalizability typically requires a substantial amount of labeled data,
spanning a broad range of variation. This data-labeling process can be both
labor and cost intensive. To achieve desirable predictive performance, a
trained model is typically applied into a domain where the data distribution is
similar to the training dataset. However, for many agricultural machine
learning problems, training datasets are collected at a specific location,
during a specific period in time of the growing season. Since agricultural
systems exhibit substantial variability in terms of crop type, cultivar,
management, seasonal growth dynamics, lighting condition, sensor type, etc, a
model trained from one dataset often does not generalize well across domains.
To enable more data efficient and generalizable neural network models in
agriculture, we propose a method that generates photorealistic agricultural
images from a synthetic 3D crop model domain into real world crop domains. The
method uses a semantically constrained GAN (generative adversarial network) to
preserve the fruit position and geometry. We observe that a baseline CycleGAN
method generates visually realistic target domain images but does not preserve
fruit position information while our method maintains fruit positions well.
Image generation results in vineyard grape day and night images show the visual
outputs of our network are much better compared to a baseline network.
Incremental training experiments in vineyard grape detection tasks show that
the images generated from our method can significantly speed the domain
adaption process, increase performance for a given number of labeled images
(i.e. data efficiency), and decrease labeling requirements.",arxiv
http://arxiv.org/abs/2103.15076v1,2021-03-28T08:04:50Z,2021-03-28T08:04:50Z,Picasso: A CUDA-based Library for Deep Learning over 3D Meshes,"We present Picasso, a CUDA-based library comprising novel modules for deep
learning over complex real-world 3D meshes. Hierarchical neural architectures
have proved effective in multi-scale feature extraction which signifies the
need for fast mesh decimation. However, existing methods rely on CPU-based
implementations to obtain multi-resolution meshes. We design GPU-accelerated
mesh decimation to facilitate network resolution reduction efficiently
on-the-fly. Pooling and unpooling modules are defined on the vertex clusters
gathered during decimation. For feature learning over meshes, Picasso contains
three types of novel convolutions namely, facet2vertex, vertex2facet, and
facet2facet convolution. Hence, it treats a mesh as a geometric structure
comprising vertices and facets, rather than a spatial graph with edges as
previous methods do. Picasso also incorporates a fuzzy mechanism in its filters
for robustness to mesh sampling (vertex density). It exploits Gaussian mixtures
to define fuzzy coefficients for the facet2vertex convolution, and barycentric
interpolation to define the coefficients for the remaining two convolutions. In
this release, we demonstrate the effectiveness of the proposed modules with
competitive segmentation results on S3DIS. The library will be made public
through https://github.com/hlei-ziyan/Picasso.",arxiv
http://arxiv.org/abs/1905.03854v2,2020-09-07T15:55:40Z,2019-05-05T01:06:48Z,"Zygarde: Time-Sensitive On-Device Deep Inference and Adaptation on
  Intermittently-Powered Systems","We propose Zygarde -- which is an energy -- and accuracy-aware soft real-time
task scheduling framework for batteryless systems that flexibly execute deep
learning tasks1 that are suitable for running on microcontrollers. The sporadic
nature of harvested energy, resource constraints of the embedded platform, and
the computational demand of deep neural networks (DNNs) pose a unique and
challenging real-time scheduling problem for which no solutions have been
proposed in the literature. We empirically study the problem and model the
energy harvesting pattern as well as the trade-off between the accuracy and
execution of a DNN. We develop an imprecise computing-based scheduling
algorithm that improves the timeliness of DNN tasks on intermittently powered
systems. We evaluate Zygarde using four standard datasets as well as by
deploying it in six real-life applications involving audio and camera sensor
systems. Results show that Zygarde decreases the execution time by up to 26%
and schedules 9%-34% more tasks with up to 21% higher inference accuracy,
compared to traditional schedulers such as the earliest deadline first (EDF).",arxiv
http://arxiv.org/abs/2004.00797v2,2020-04-03T02:45:33Z,2020-04-02T03:41:39Z,SSHFD: Single Shot Human Fall Detection with Occluded Joints Resilience,"Falling can have fatal consequences for elderly people especially if the
fallen person is unable to call for help due to loss of consciousness or any
injury. Automatic fall detection systems can assist through prompt fall alarms
and by minimizing the fear of falling when living independently at home.
Existing vision-based fall detection systems lack generalization to unseen
environments due to challenges such as variations in physical appearances,
different camera viewpoints, occlusions, and background clutter. In this paper,
we explore ways to overcome the above challenges and present Single Shot Human
Fall Detector (SSHFD), a deep learning based framework for automatic fall
detection from a single image. This is achieved through two key innovations.
First, we present a human pose based fall representation which is invariant to
appearance characteristics. Second, we present neural network models for 3d
pose estimation and fall recognition which are resilient to missing joints due
to occluded body parts. Experiments on public fall datasets show that our
framework successfully transfers knowledge of 3d pose estimation and fall
recognition learnt purely from synthetic data to unseen real-world data,
showcasing its generalization capability for accurate fall detection in
real-world scenarios.",arxiv
http://arxiv.org/abs/1908.04472v1,2019-08-13T03:19:46Z,2019-08-13T03:19:46Z,Exploiting Multi-domain Visual Information for Fake News Detection,"The increasing popularity of social media promotes the proliferation of fake
news. With the development of multimedia technology, fake news attempts to
utilize multimedia contents with images or videos to attract and mislead
readers for rapid dissemination, which makes visual contents an important part
of fake news. Fake-news images, images attached in fake news posts,include not
only fake images which are maliciously tampered but also real images which are
wrongly used to represent irrelevant events. Hence, how to fully exploit the
inherent characteristics of fake-news images is an important but challenging
problem for fake news detection. In the real world, fake-news images may have
significantly different characteristics from real-news images at both physical
and semantic levels, which can be clearly reflected in the frequency and pixel
domain, respectively. Therefore, we propose a novel framework Multi-domain
Visual Neural Network (MVNN) to fuse the visual information of frequency and
pixel domains for detecting fake news. Specifically, we design a CNN-based
network to automatically capture the complex patterns of fake-news images in
the frequency domain; and utilize a multi-branch CNN-RNN model to extract
visual features from different semantic levels in the pixel domain. An
attention mechanism is utilized to fuse the feature representations of
frequency and pixel domains dynamically. Extensive experiments conducted on a
real-world dataset demonstrate that MVNN outperforms existing methods with at
least 9.2% in accuracy, and can help improve the performance of multimodal fake
news detection by over 5.2%.",arxiv
http://arxiv.org/abs/1904.11451v3,2020-12-15T06:53:39Z,2019-04-25T16:49:13Z,Large Scale Holistic Video Understanding,"Video recognition has been advanced in recent years by benchmarks with rich
annotations. However, research is still mainly limited to human action or
sports recognition - focusing on a highly specific video understanding task and
thus leaving a significant gap towards describing the overall content of a
video. We fill this gap by presenting a large-scale ""Holistic Video
Understanding Dataset""~(HVU). HVU is organized hierarchically in a semantic
taxonomy that focuses on multi-label and multi-task video understanding as a
comprehensive problem that encompasses the recognition of multiple semantic
aspects in the dynamic scene. HVU contains approx.~572k videos in total with 9
million annotations for training, validation, and test set spanning over 3142
labels. HVU encompasses semantic aspects defined on categories of scenes,
objects, actions, events, attributes, and concepts which naturally captures the
real-world scenarios.
  We demonstrate the generalization capability of HVU on three challenging
tasks: 1.) Video classification, 2.) Video captioning and 3.) Video clustering
tasks. In particular for video classification, we introduce a new
spatio-temporal deep neural network architecture called ""Holistic Appearance
and Temporal Network""~(HATNet) that builds on fusing 2D and 3D architectures
into one by combining intermediate representations of appearance and temporal
cues. HATNet focuses on the multi-label and multi-task learning problem and is
trained in an end-to-end manner. Via our experiments, we validate the idea that
holistic representation learning is complementary, and can play a key role in
enabling many real-world applications.",arxiv
http://arxiv.org/abs/1809.00510v2,2018-09-10T08:29:30Z,2018-09-03T09:07:30Z,"Flatland: a Lightweight First-Person 2-D Environment for Reinforcement
  Learning","Flatland is a simple, lightweight environment for fast prototyping and
testing of reinforcement learning agents. It is of lower complexity compared to
similar 3D platforms (e.g. DeepMind Lab or VizDoom), but emulates physical
properties of the real world, such as continuity, multi-modal
partially-observable states with first-person view and coherent physics. We
propose to use it as an intermediary benchmark for problems related to Lifelong
Learning. Flatland is highly customizable and offers a wide range of task
difficulty to extensively evaluate the properties of artificial agents. We
experiment with three reinforcement learning baseline agents and show that they
can rapidly solve a navigation task in Flatland. A video of an agent acting in
Flatland is available here: https://youtu.be/I5y6Y2ZypdA.",arxiv
http://arxiv.org/abs/2012.07086v1,2020-12-13T15:38:38Z,2020-12-13T15:38:38Z,"EfficientPose: Efficient Human Pose Estimation with Neural Architecture
  Search","Human pose estimation from image and video is a vital task in many multimedia
applications. Previous methods achieve great performance but rarely take
efficiency into consideration, which makes it difficult to implement the
networks on resource-constrained devices. Nowadays real-time multimedia
applications call for more efficient models for better interactions. Moreover,
most deep neural networks for pose estimation directly reuse the networks
designed for image classification as the backbone, which are not yet optimized
for the pose estimation task. In this paper, we propose an efficient framework
targeted at human pose estimation including two parts, the efficient backbone
and the efficient head. By implementing the differentiable neural architecture
search method, we customize the backbone network design for pose estimation and
reduce the computation cost with negligible accuracy degradation. For the
efficient head, we slim the transposed convolutions and propose a spatial
information correction module to promote the performance of the final
prediction. In experiments, we evaluate our networks on the MPII and COCO
datasets. Our smallest model has only 0.65 GFLOPs with 88.1% PCKh@0.5 on MPII
and our large model has only 2 GFLOPs while its accuracy is competitive with
the state-of-the-art large model, i.e., HRNet with 9.5 GFLOPs.",arxiv
http://arxiv.org/abs/2103.10846v1,2021-03-19T14:58:11Z,2021-03-19T14:58:11Z,A universal neural network for learning phases and criticalities,"A universal supervised neural network (NN) relevant to compute the associated
criticalities of real experiments studying phase transitions is constructed.
The validity of the built NN is examined by applying it to calculate the
criticalities of several three-dimensional (3D) models on the cubic lattice,
including the classical $O(3)$ model, the 5-state ferromagnetic Potts model,
and a dimerized quantum antiferromagnetic Heisenberg model. Particularly,
although the considered NN is only trained one time on a one-dimensional (1D)
lattice with 120 sites, yet it has successfully determined the related critical
points of the studied 3D systems. Moreover, real configurations of states are
not used in the testing stage. Instead, the employed configurations for the
prediction are constructed on a 1D lattice of 120 sites and are based on the
bulk quantities or the microscopic states of the considered models. As a
result, our calculations are ultimately efficient in computation and the
applications of the built NN is extremely broaden. Considering the fact that
the investigated systems vary dramatically from each other, it is amazing that
the combination of these two strategies in the training and the testing stages
lead to a highly universal supervised neural network for learning phases and
criticalities of 3D models. Based on the outcomes presented in this study, it
is favorably probable that much simpler but yet elegant machine learning
techniques can be constructed for fields of many-body systems other than the
critical phenomena.",arxiv
http://arxiv.org/abs/2107.12601v1,2021-07-27T05:03:18Z,2021-07-27T05:03:18Z,"Microphone Array Generalization for Multichannel Narrowband Deep Speech
  Enhancement","This paper addresses the problem of microphone array generalization for
deep-learning-based end-to-end multichannel speech enhancement. We aim to train
a unique deep neural network (DNN) potentially performing well on unseen
microphone arrays. The microphone array geometry shapes the network's
parameters when training on a fixed microphone array, and thus restricts the
generalization of the trained network to another microphone array. To resolve
this problem, a single network is trained using data recorded by various
microphone arrays of different geometries. We design three variants of our
recently proposed narrowband network to cope with the agnostic number of
microphones. Overall, the goal is to make the network learn the universal
information for speech enhancement that is available for any array geometry,
rather than learn the one-array-dedicated characteristics. The experiments on
both simulated and real room impulse responses (RIR) demonstrate the excellent
across-array generalization capability of the proposed networks, in the sense
that their performance measures are very close to, or even exceed the network
trained with test arrays. Moreover, they notably outperform various beamforming
methods and other advanced deep-learning-based methods.",arxiv
http://arxiv.org/abs/1808.00362v1,2018-08-01T15:13:48Z,2018-08-01T15:13:48Z,Deep Appearance Models for Face Rendering,"We introduce a deep appearance model for rendering the human face. Inspired
by Active Appearance Models, we develop a data-driven rendering pipeline that
learns a joint representation of facial geometry and appearance from a
multiview capture setup. Vertex positions and view-specific textures are
modeled using a deep variational autoencoder that captures complex nonlinear
effects while producing a smooth and compact latent representation.
View-specific texture enables the modeling of view-dependent effects such as
specularity. In addition, it can also correct for imperfect geometry stemming
from biased or low resolution estimates. This is a significant departure from
the traditional graphics pipeline, which requires highly accurate geometry as
well as all elements of the shading model to achieve realism through
physically-inspired light transport. Acquiring such a high level of accuracy is
difficult in practice, especially for complex and intricate parts of the face,
such as eyelashes and the oral cavity. These are handled naturally by our
approach, which does not rely on precise estimates of geometry. Instead, the
shading model accommodates deficiencies in geometry though the flexibility
afforded by the neural network employed. At inference time, we condition the
decoding network on the viewpoint of the camera in order to generate the
appropriate texture for rendering. The resulting system can be implemented
simply using existing rendering engines through dynamic textures with flat
lighting. This representation, together with a novel unsupervised technique for
mapping images to facial states, results in a system that is naturally suited
to real-time interactive settings such as Virtual Reality (VR).",arxiv
http://arxiv.org/abs/1708.05208v1,2017-08-17T11:31:22Z,2017-08-17T11:31:22Z,"Automatic HVAC Control with Real-time Occupancy Recognition and
  Simulation-guided Model Predictive Control in Low-cost Embedded System","Intelligent building automation systems can reduce the energy consumption of
heating, ventilation and air-conditioning (HVAC) units by sensing the comfort
requirements automatically and scheduling the HVAC operations dynamically.
Traditional building automation systems rely on fairly inaccurate occupancy
sensors and basic predictive control using oversimplified building thermal
response models, all of which prevent such systems from reaching their full
potential. Such limitations can now be avoided due to the recent developments
in embedded system technologies, which provide viable low-cost computing
platforms with powerful processors and sizeable memory storage in a small
footprint. As a result, building automation systems can now efficiently execute
highly-sophisticated computational tasks, such as real-time video processing
and accurate thermal-response simulations. With this in mind, we designed and
implemented an occupancy-predictive HVAC control system in a low-cost yet
powerful embedded system (using Raspberry Pi 3) to demonstrate the following
key features for building automation: (1) real-time occupancy recognition using
video-processing and machine-learning techniques, (2) dynamic analysis and
prediction of occupancy patterns, and (3) model predictive control for HVAC
operations guided by real-time building thermal response simulations (using an
on-board EnergyPlus simulator). We deployed and evaluated our system for
providing automatic HVAC control in the large public indoor space of a mosque,
thereby achieving significant energy savings.",arxiv
http://arxiv.org/abs/2102.05894v1,2021-02-11T08:56:12Z,2021-02-11T08:56:12Z,"CASA-Based Speaker Identification Using Cascaded GMM-CNN Classifier in
  Noisy and Emotional Talking Conditions","This work aims at intensifying text-independent speaker identification
performance in real application situations such as noisy and emotional talking
conditions. This is achieved by incorporating two different modules: a
Computational Auditory Scene Analysis CASA based pre-processing module for
noise reduction and cascaded Gaussian Mixture Model Convolutional Neural
Network GMM-CNN classifier for speaker identification followed by emotion
recognition. This research proposes and evaluates a novel algorithm to improve
the accuracy of speaker identification in emotional and highly-noise
susceptible conditions. Experiments demonstrate that the proposed model yields
promising results in comparison with other classifiers when Speech Under
Simulated and Actual Stress SUSAS database, Emirati Speech Database ESD, the
Ryerson Audio-Visual Database of Emotional Speech and Song RAVDESS database and
the Fluent Speech Commands database are used in a noisy environment.",arxiv
http://arxiv.org/abs/1811.07485v1,2018-11-19T03:51:19Z,2018-11-19T03:51:19Z,"Visual-Texual Emotion Analysis with Deep Coupled Video and Danmu Neural
  Networks","User emotion analysis toward videos is to automatically recognize the general
emotional status of viewers from the multimedia content embedded in the online
video stream. Existing works fall in two categories: 1) visual-based methods,
which focus on visual content and extract a specific set of features of videos.
However, it is generally hard to learn a mapping function from low-level video
pixels to high-level emotion space due to great intra-class variance. 2)
textual-based methods, which focus on the investigation of user-generated
comments associated with videos. The learned word representations by
traditional linguistic approaches typically lack emotion information and the
global comments usually reflect viewers' high-level understandings rather than
instantaneous emotions. To address these limitations, in this paper, we propose
to jointly utilize video content and user-generated texts simultaneously for
emotion analysis. In particular, we introduce exploiting a new type of
user-generated texts, i.e., ""danmu"", which are real-time comments floating on
the video and contain rich information to convey viewers' emotional opinions.
To enhance the emotion discriminativeness of words in textual feature
extraction, we propose Emotional Word Embedding (EWE) to learn text
representations by jointly considering their semantics and emotions.
Afterwards, we propose a novel visual-textual emotion analysis model with Deep
Coupled Video and Danmu Neural networks (DCVDN), in which visual and textual
features are synchronously extracted and fused to form a comprehensive
representation by deep-canonically-correlated-autoencoder-based multi-view
learning. Through extensive experiments on a self-crawled real-world
video-danmu dataset, we prove that DCVDN significantly outperforms the
state-of-the-art baselines.",arxiv
http://arxiv.org/abs/2003.00875v2,2020-04-28T11:34:58Z,2020-02-23T05:27:37Z,"Predicting TUG score from gait characteristics with video analysis and
  machine learning","Fall is a leading cause of death which suffers the elderly and society. Timed
Up and Go (TUG) test is a common tool for fall risk assessment. In this paper,
we propose a method for predicting TUG score from gait characteristics
extracted from video with computer vision and machine learning technologies.
First, 3D pose is estimated from video captured with 2D and 3D cameras during
human motion and then a group of gait characteristics are computed from 3D pose
series. After that, copula entropy is used to select those characteristics
which are mostly associated with TUG score. Finally, the selected
characteristics are fed into the predictive models to predict TUG score.
Experiments on real world data demonstrated the effectiveness of the proposed
method. As a byproduct, the associations between TUG score and several gait
characteristics are discovered, which laid the scientific foundation of the
proposed method and make the predictive models such built interpretable to
clinical users.",arxiv
http://arxiv.org/abs/2002.07325v2,2021-01-05T20:51:44Z,2020-02-18T01:30:29Z,"Decoding pedestrian and automated vehicle interactions using immersive
  virtual reality and interpretable deep learning","To ensure pedestrian friendly streets in the era of automated vehicles,
reassessment of current policies, practices, design, rules and regulations of
urban areas is of importance. This study investigates pedestrian crossing
behaviour, as an important element of urban dynamics that is expected to be
affected by the presence of automated vehicles. For this purpose, an
interpretable machine learning framework is proposed to explore factors
affecting pedestrians' wait time before crossing mid-block crosswalks in the
presence of automated vehicles. To collect rich behavioural data, we developed
a dynamic and immersive virtual reality experiment, with 180 participants from
a heterogeneous population in 4 different locations in the Greater Toronto Area
(GTA). Pedestrian wait time behaviour is then analyzed using a data-driven Cox
Proportional Hazards (CPH) model, in which the linear combination of the
covariates is replaced by a flexible non-linear deep neural network. The
proposed model achieved a 5% improvement in goodness of fit, but more
importantly, enabled us to incorporate a richer set of covariates. A game
theoretic based interpretability method is used to understand the contribution
of different covariates to the time pedestrians wait before crossing. Results
show that the presence of automated vehicles on roads, wider lane widths, high
density on roads, limited sight distance, and lack of walking habits are the
main contributing factors to longer wait times. Our study suggested that, to
move towards pedestrian-friendly urban areas, national level educational
programs for children, enhanced safety measures for seniors, promotion of
active modes of transportation, and revised traffic rules and regulations
should be considered.",arxiv
http://arxiv.org/abs/1603.07846v1,2016-03-25T08:46:02Z,2016-03-25T08:46:02Z,Deep Learning At Scale and At Ease,"Recently, deep learning techniques have enjoyed success in various multimedia
applications, such as image classification and multi-modal data analysis. Large
deep learning models are developed for learning rich representations of complex
data. There are two challenges to overcome before deep learning can be widely
adopted in multimedia and other applications. One is usability, namely the
implementation of different models and training algorithms must be done by
non-experts without much effort especially when the model is large and complex.
The other is scalability, that is the deep learning system must be able to
provision for a huge demand of computing resources for training large models
with massive datasets. To address these two challenges, in this paper, we
design a distributed deep learning platform called SINGA which has an intuitive
programming model based on the common layer abstraction of deep learning
models. Good scalability is achieved through flexible distributed training
architecture and specific optimization techniques. SINGA runs on GPUs as well
as on CPUs, and we show that it outperforms many other state-of-the-art deep
learning systems. Our experience with developing and training deep learning
models for real-life multimedia applications in SINGA shows that the platform
is both usable and scalable.",arxiv
http://arxiv.org/abs/2108.03332v1,2021-08-06T23:36:23Z,2021-08-06T23:36:23Z,"BEHAVIOR: Benchmark for Everyday Household Activities in Virtual,
  Interactive, and Ecological Environments","We introduce BEHAVIOR, a benchmark for embodied AI with 100 activities in
simulation, spanning a range of everyday household chores such as cleaning,
maintenance, and food preparation. These activities are designed to be
realistic, diverse, and complex, aiming to reproduce the challenges that agents
must face in the real world. Building such a benchmark poses three fundamental
difficulties for each activity: definition (it can differ by time, place, or
person), instantiation in a simulator, and evaluation. BEHAVIOR addresses these
with three innovations. First, we propose an object-centric, predicate
logic-based description language for expressing an activity's initial and goal
conditions, enabling generation of diverse instances for any activity. Second,
we identify the simulator-agnostic features required by an underlying
environment to support BEHAVIOR, and demonstrate its realization in one such
simulator. Third, we introduce a set of metrics to measure task progress and
efficiency, absolute and relative to human demonstrators. We include 500 human
demonstrations in virtual reality (VR) to serve as the human ground truth. Our
experiments demonstrate that even state of the art embodied AI solutions
struggle with the level of realism, diversity, and complexity imposed by the
activities in our benchmark. We make BEHAVIOR publicly available at
behavior.stanford.edu to facilitate and calibrate the development of new
embodied AI solutions.",arxiv
http://arxiv.org/abs/2110.04710v1,2021-10-10T05:40:11Z,2021-10-10T05:40:11Z,Sketch Me A Video,"Video creation has been an attractive yet challenging task for artists to
explore. With the advancement of deep learning, recent works try to utilize
deep convolutional neural networks to synthesize a video with the aid of a
guiding video, and have achieved promising results. However, the acquisition of
guiding videos, or other forms of guiding temporal information is costly
expensive and difficult in reality. Therefore, in this work we introduce a new
video synthesis task by employing two rough bad-drwan sketches only as input to
create a realistic portrait video. A two-stage Sketch-to-Video model is
proposed, which consists of two key novelties: 1) a feature retrieve and
projection (FRP) module, which parititions the input sketch into different
parts and utilizes these parts for synthesizing a realistic start or end frame
and meanwhile generating rich semantic features, is designed to alleviate the
sketch out-of-domain problem due to arbitrarily drawn free-form sketch styles
by different users. 2) A motion projection followed by feature blending module,
which projects a video (used only in training phase) into a motion space
modeled by normal distribution and blends the motion variables with semantic
features extracted above, is proposed to alleviate the guiding temporal
information missing problem in the test phase. Experiments conducted on a
combination of CelebAMask-HQ and VoxCeleb2 dataset well validate that, our
method can acheive both good quantitative and qualitative results in
synthesizing high-quality videos from two rough bad-drawn sketches.",arxiv
http://arxiv.org/abs/1806.01759v2,2018-09-25T08:29:12Z,2018-06-05T15:56:24Z,"Monte Carlo Convolution for Learning on Non-Uniformly Sampled Point
  Clouds","Deep learning systems extensively use convolution operations to process input
data. Though convolution is clearly defined for structured data such as 2D
images or 3D volumes, this is not true for other data types such as sparse
point clouds. Previous techniques have developed approximations to convolutions
for restricted conditions. Unfortunately, their applicability is limited and
cannot be used for general point clouds. We propose an efficient and effective
method to learn convolutions for non-uniformly sampled point clouds, as they
are obtained with modern acquisition techniques. Learning is enabled by four
key novelties: first, representing the convolution kernel itself as a
multilayer perceptron; second, phrasing convolution as a Monte Carlo
integration problem, third, using this notion to combine information from
multiple samplings at different levels; and fourth using Poisson disk sampling
as a scalable means of hierarchical point cloud learning. The key idea across
all these contributions is to guarantee adequate consideration of the
underlying non-uniform sample distribution function from a Monte Carlo
perspective. To make the proposed concepts applicable to real-world tasks, we
furthermore propose an efficient implementation which significantly reduces the
GPU memory required during the training process. By employing our method in
hierarchical network architectures we can outperform most of the
state-of-the-art networks on established point cloud segmentation,
classification and normal estimation benchmarks. Furthermore, in contrast to
most existing approaches, we also demonstrate the robustness of our method with
respect to sampling variations, even when training with uniformly sampled data
only. To support the direct application of these concepts, we provide a
ready-to-use TensorFlow implementation of these layers at
https://github.com/viscom-ulm/MCCNN",arxiv
http://arxiv.org/abs/1611.05356v2,2017-03-24T22:15:17Z,2016-11-16T16:50:57Z,"Towards Interconnected Virtual Reality: Opportunities, Challenges and
  Enablers","Just recently, the concept of augmented and virtual reality (AR/VR) over
wireless has taken the entire 5G ecosystem by storm spurring an unprecedented
interest from both academia, industry and others. Yet, the success of an
immersive VR experience hinges on solving a plethora of grand challenges
cutting across multiple disciplines. This article underscores the importance of
VR technology as a disruptive use case of 5G (and beyond) harnessing the latest
development of storage/memory, fog/edge computing, computer vision, artificial
intelligence and others. In particular, the main requirements of wireless
interconnected VR are described followed by a selection of key enablers, then,
research avenues and their underlying grand challenges are presented.
Furthermore, we examine three VR case studies and provide numerical results
under various storage, computing and network configurations. Finally, this
article exposes the limitations of current networks and makes the case for more
theory, and innovations to spearhead VR for the masses.",arxiv
http://arxiv.org/abs/1806.08621v1,2018-06-22T12:15:35Z,2018-06-22T12:15:35Z,Weakly Supervised Training of Speaker Identification Models,"We propose an approach for training speaker identification models in a weakly
supervised manner. We concentrate on the setting where the training data
consists of a set of audio recordings and the speaker annotation is provided
only at the recording level. The method uses speaker diarization to find unique
speakers in each recording, and i-vectors to project the speech of each speaker
to a fixed-dimensional vector. A neural network is then trained to map
i-vectors to speakers, using a special objective function that allows to
optimize the model using recording-level speaker labels. We report experiments
on two different real-world datasets. On the VoxCeleb dataset, the method
provides 94.6% accuracy on a closed set speaker identification task, surpassing
the baseline performance by a large margin. On an Estonian broadcast news
dataset, the method provides 66% time-weighted speaker identification recall at
93% precision.",arxiv
http://arxiv.org/abs/1812.06186v1,2018-12-13T18:57:53Z,2018-12-13T18:57:53Z,Towards Fast Biomechanical Modeling of Soft Tissue Using Neural Networks,"To date, the simulation of organ deformations for applications like therapy
planning or image-guided interventions is calculated by solving the
elastodynamics equations. While efficient solvers have been proposed for fast
simulations, methods that are both real-time and accurate are still an open
challenge. An ideal, interactive solver would be able to provide physically and
numerically accurate results at high frame rate, which requires efficient force
computation and time integration. Towards this goal, we explore in this paper
for the first time the use of neural networks to directly learn the underlying
biomechanics. Given a 3D mesh of a soft tissue segmented from medical images,
we train a neural network to predict vertex-wise accelerations for a large time
step based on the current state of the system. The model is trained using the
deformation of a bar under torsion, and evaluated on different motions,
geometries, and hyperelastic material models. For predictions of ten times the
original time step we observed a mean error of 0.017mm $\pm$ 0.014 (0.032) at a
mesh size of 50mm x 50mm x 100mm. Predictions at 20dt yield an error of 2.10mm
$\pm$ 1.73 (4.37) and by further increasing the prediction time step the
maximum error rises to 38.3mm due to an artificial stiffening. In all
experiments our proposed method stayed stable, while the reference solver fails
to converge. Our experiments suggest that it is possible to directly learn the
mechanical simulation and open further investigations for the direct
application of machine learning to speed-up biophysics solvers.",arxiv
http://arxiv.org/abs/1911.01562v1,2019-11-05T01:40:42Z,2019-11-05T01:40:42Z,"DeepRacer: Educational Autonomous Racing Platform for Experimentation
  with Sim2Real Reinforcement Learning","DeepRacer is a platform for end-to-end experimentation with RL and can be
used to systematically investigate the key challenges in developing intelligent
control systems. Using the platform, we demonstrate how a 1/18th scale car can
learn to drive autonomously using RL with a monocular camera. It is trained in
simulation with no additional tuning in physical world and demonstrates: 1)
formulation and solution of a robust reinforcement learning algorithm, 2)
narrowing the reality gap through joint perception and dynamics, 3) distributed
on-demand compute architecture for training optimal policies, and 4) a robust
evaluation method to identify when to stop training. It is the first successful
large-scale deployment of deep reinforcement learning on a robotic control
agent that uses only raw camera images as observations and a model-free
learning method to perform robust path planning. We open source our code and
video demo on GitHub: https://git.io/fjxoJ.",arxiv
http://arxiv.org/abs/1803.05788v1,2018-03-14T02:18:55Z,2018-03-14T02:18:55Z,"DeepN-JPEG: A Deep Neural Network Favorable JPEG-based Image Compression
  Framework","As one of most fascinating machine learning techniques, deep neural network
(DNN) has demonstrated excellent performance in various intelligent tasks such
as image classification. DNN achieves such performance, to a large extent, by
performing expensive training over huge volumes of training data. To reduce the
data storage and transfer overhead in smart resource-limited Internet-of-Thing
(IoT) systems, effective data compression is a ""must-have"" feature before
transferring real-time produced dataset for training or classification. While
there have been many well-known image compression approaches (such as JPEG), we
for the first time find that a human-visual based image compression approach
such as JPEG compression is not an optimized solution for DNN systems,
especially with high compression ratios. To this end, we develop an image
compression framework tailored for DNN applications, named ""DeepN-JPEG"", to
embrace the nature of deep cascaded information process mechanism of DNN
architecture. Extensive experiments, based on ""ImageNet"" dataset with various
state-of-the-art DNNs, show that ""DeepN-JPEG"" can achieve ~3.5x higher
compression rate over the popular JPEG solution while maintaining the same
accuracy level for image recognition, demonstrating its great potential of
storage and power efficiency in DNN-based smart IoT system design.",arxiv
http://arxiv.org/abs/2006.11321v1,2020-06-19T18:57:51Z,2020-06-19T18:57:51Z,"AutoOD: Automated Outlier Detection via Curiosity-guided Search and
  Self-imitation Learning","Outlier detection is an important data mining task with numerous practical
applications such as intrusion detection, credit card fraud detection, and
video surveillance. However, given a specific complicated task with big data,
the process of building a powerful deep learning based system for outlier
detection still highly relies on human expertise and laboring trials. Although
Neural Architecture Search (NAS) has shown its promise in discovering effective
deep architectures in various domains, such as image classification, object
detection, and semantic segmentation, contemporary NAS methods are not suitable
for outlier detection due to the lack of intrinsic search space, unstable
search process, and low sample efficiency. To bridge the gap, in this paper, we
propose AutoOD, an automated outlier detection framework, which aims to search
for an optimal neural network model within a predefined search space.
Specifically, we firstly design a curiosity-guided search strategy to overcome
the curse of local optimality. A controller, which acts as a search agent, is
encouraged to take actions to maximize the information gain about the
controller's internal belief. We further introduce an experience replay
mechanism based on self-imitation learning to improve the sample efficiency.
Experimental results on various real-world benchmark datasets demonstrate that
the deep model identified by AutoOD achieves the best performance, comparing
with existing handcrafted models and traditional search methods.",arxiv
http://arxiv.org/abs/2105.13598v3,2021-08-17T06:31:58Z,2021-05-28T05:54:59Z,End-to-End Deep Fault Tolerant Control,"Ideally, accurate sensor measurements are needed to achieve a good
performance in the closed-loop control of mechatronic systems. As a
consequence, sensor faults will prevent the system from working correctly,
unless a fault-tolerant control (FTC) architecture is adopted. As model-based
FTC algorithms for nonlinear systems are often challenging to design, this
paper focuses on a new method for FTC in the presence of sensor faults, based
on deep learning. The considered approach replaces the phases of fault
detection and isolation and controller design with a single recurrent neural
network, which has the value of past sensor measurements in a given time window
as input, and the current values of the control variables as output. This
end-to-end deep FTC method is applied to a mechatronic system composed of a
spherical inverted pendulum, whose configuration is changed via reaction
wheels, in turn actuated by electric motors. The simulation and experimental
results show that the proposed method can handle abrupt faults occurring in
link position/velocity sensors. The provided supplementary material includes a
video of real-world experiments and the software source code.",arxiv
http://arxiv.org/abs/2004.14404v2,2020-05-23T01:42:24Z,2020-04-29T18:00:22Z,Meta-Reinforcement Learning for Robotic Industrial Insertion Tasks,"Robotic insertion tasks are characterized by contact and friction mechanics,
making them challenging for conventional feedback control methods due to
unmodeled physical effects. Reinforcement learning (RL) is a promising approach
for learning control policies in such settings. However, RL can be unsafe
during exploration and might require a large amount of real-world training
data, which is expensive to collect. In this paper, we study how to use
meta-reinforcement learning to solve the bulk of the problem in simulation by
solving a family of simulated industrial insertion tasks and then adapt
policies quickly in the real world. We demonstrate our approach by training an
agent to successfully perform challenging real-world insertion tasks using less
than 20 trials of real-world experience. Videos and other material are
available at https://pearl-insertion.github.io/",arxiv
http://arxiv.org/abs/1809.01567v2,2018-09-06T10:02:02Z,2018-09-05T15:09:20Z,"Deep Depth from Defocus: how can defocus blur improve 3D estimation
  using dense neural networks?","Depth estimation is of critical interest for scene understanding and accurate
3D reconstruction. Most recent approaches in depth estimation with deep
learning exploit geometrical structures of standard sharp images to predict
corresponding depth maps. However, cameras can also produce images with defocus
blur depending on the depth of the objects and camera settings. Hence, these
features may represent an important hint for learning to predict depth. In this
paper, we propose a full system for single-image depth prediction in the wild
using depth-from-defocus and neural networks. We carry out thorough experiments
to test deep convolutional networks on real and simulated defocused images
using a realistic model of blur variation with respect to depth. We also
investigate the influence of blur on depth prediction observing model
uncertainty with a Bayesian neural network approach. From these studies, we
show that out-of-focus blur greatly improves the depth-prediction network
performances. Furthermore, we transfer the ability learned on a synthetic,
indoor dataset to real, indoor and outdoor images. For this purpose, we present
a new dataset containing real all-focus and defocused images from a Digital
Single-Lens Reflex (DSLR) camera, paired with ground truth depth maps obtained
with an active 3D sensor for indoor scenes. The proposed approach is
successfully validated on both this new dataset and standard ones as NYUv2 or
Depth-in-the-Wild. Code and new datasets are available at
https://github.com/marcelampc/d3net_depth_estimation",arxiv
http://arxiv.org/abs/1903.01712v1,2019-03-05T07:46:47Z,2019-03-05T07:46:47Z,"Deep Learning Based Motion Planning For Autonomous Vehicle Using
  Spatiotemporal LSTM Network","Motion Planning, as a fundamental technology of automatic navigation for the
autonomous vehicle, is still an open challenging issue in the real-life traffic
situation and is mostly applied by the model-based approaches. However, due to
the complexity of the traffic situations and the uncertainty of the edge cases,
it is hard to devise a general motion planning system for the autonomous
vehicle. In this paper, we proposed a motion planning model based on deep
learning (named as spatiotemporal LSTM network), which is able to generate a
real-time reflection based on spatiotemporal information extraction. To be
specific, the model based on spatiotemporal LSTM network has three main
structure. Firstly, the Convolutional Long-short Term Memory (Conv-LSTM) is
used to extract hidden features through sequential image data. Then, the 3D
Convolutional Neural Network(3D-CNN) is applied to extract the spatiotemporal
information from the multi-frame feature information. Finally, the fully
connected neural networks are used to construct a control model for autonomous
vehicle steering angle. The experiments demonstrated that the proposed method
can generate a robust and accurate visual motion planning results for the
autonomous vehicle.",arxiv
http://arxiv.org/abs/1711.00541v2,2018-04-18T02:25:29Z,2017-11-01T21:19:22Z,"TasNet: time-domain audio separation network for real-time,
  single-channel speech separation","Robust speech processing in multi-talker environments requires effective
speech separation. Recent deep learning systems have made significant progress
toward solving this problem, yet it remains challenging particularly in
real-time, short latency applications. Most methods attempt to construct a mask
for each source in time-frequency representation of the mixture signal which is
not necessarily an optimal representation for speech separation. In addition,
time-frequency decomposition results in inherent problems such as
phase/magnitude decoupling and long time window which is required to achieve
sufficient frequency resolution. We propose Time-domain Audio Separation
Network (TasNet) to overcome these limitations. We directly model the signal in
the time-domain using an encoder-decoder framework and perform the source
separation on nonnegative encoder outputs. This method removes the frequency
decomposition step and reduces the separation problem to estimation of source
masks on encoder outputs which is then synthesized by the decoder. Our system
outperforms the current state-of-the-art causal and noncausal speech separation
algorithms, reduces the computational cost of speech separation, and
significantly reduces the minimum required latency of the output. This makes
TasNet suitable for applications where low-power, real-time implementation is
desirable such as in hearable and telecommunication devices.",arxiv
http://arxiv.org/abs/2105.03668v2,2021-09-05T16:41:50Z,2021-05-08T10:42:28Z,"Real-time prediction of probabilistic crack growth with a reduced-order
  digital twin of a helicopter component","To deploy the airframe Digital Twin or to conduct probabilistic evaluations
of the remaining life of a structural component, a (near) real-time crack
growth simulation method is critical. In this paper, a reduced-order simulation
approach is developed to achieve this goal by leveraging two methods. On one
hand, the SGBEM super element - FEM coupling method is combined with parametric
modeling to generate the database of computed Stress Intensity Factors for
cracks with various sizes/shapes in a complex structural component, by which
hundreds of samples are automatically simulated within a day. On the other
hand, machine learning methods are applied to establish the relation between
crack sizes/shapes and crack front SIFs. By combining the reduced-order
computational model with load inputs and fatigue growth laws, a real time
prediction of probabilistic crack growth in complex structures with minimum
computational burden is realized. In an example of a round-robin helicopter
component, even though the fatigue crack growth is simulated cycle by cycle,
the simulation is faster than real-time (as compared to the physical test). The
proposed approach is a key simulation technology towards realizing the Digital
Twin of complex structures, which further requires fusion of model predictions
with flight/inspection/monitoring data.",arxiv
http://arxiv.org/abs/2005.09382v1,2020-05-19T12:16:58Z,2020-05-19T12:16:58Z,"Human Instruction-Following with Deep Reinforcement Learning via
  Transfer-Learning from Text","Recent work has described neural-network-based agents that are trained with
reinforcement learning (RL) to execute language-like commands in simulated
worlds, as a step towards an intelligent agent or robot that can be instructed
by human users. However, the optimisation of multi-goal motor policies via deep
RL from scratch requires many episodes of experience. Consequently,
instruction-following with deep RL typically involves language generated from
templates (by an environment simulator), which does not reflect the varied or
ambiguous expressions of real users. Here, we propose a conceptually simple
method for training instruction-following agents with deep RL that are robust
to natural human instructions. By applying our method with a state-of-the-art
pre-trained text-based language model (BERT), on tasks requiring agents to
identify and position everyday objects relative to other objects in a
naturalistic 3D simulated room, we demonstrate substantially-above-chance
zero-shot transfer from synthetic template commands to natural instructions
given by humans. Our approach is a general recipe for training any deep
RL-based system to interface with human users, and bridges the gap between two
research directions of notable recent success: agent-centric motor behavior and
text-based representation learning.",arxiv
http://arxiv.org/abs/1904.03814v2,2019-11-18T06:16:42Z,2019-04-08T03:21:11Z,Temporal Convolution for Real-time Keyword Spotting on Mobile Devices,"Keyword spotting (KWS) plays a critical role in enabling speech-based user
interactions on smart devices. Recent developments in the field of deep
learning have led to wide adoption of convolutional neural networks (CNNs) in
KWS systems due to their exceptional accuracy and robustness. The main
challenge faced by KWS systems is the trade-off between high accuracy and low
latency. Unfortunately, there has been little quantitative analysis of the
actual latency of KWS models on mobile devices. This is especially concerning
since conventional convolution-based KWS approaches are known to require a
large number of operations to attain an adequate level of performance. In this
paper, we propose a temporal convolution for real-time KWS on mobile devices.
Unlike most of the 2D convolution-based KWS approaches that require a deep
architecture to fully capture both low- and high-frequency domains, we exploit
temporal convolutions with a compact ResNet architecture. In Google Speech
Command Dataset, we achieve more than \textbf{385x} speedup on Google Pixel 1
and surpass the accuracy compared to the state-of-the-art model. In addition,
we release the implementation of the proposed and the baseline models including
an end-to-end pipeline for training models and evaluating them on mobile
devices.",arxiv
http://arxiv.org/abs/1807.03361v1,2018-07-09T19:53:16Z,2018-07-09T19:53:16Z,"Weakly-Supervised Convolutional Neural Networks for Multimodal Image
  Registration","One of the fundamental challenges in supervised learning for multimodal image
registration is the lack of ground-truth for voxel-level spatial
correspondence. This work describes a method to infer voxel-level
transformation from higher-level correspondence information contained in
anatomical labels. We argue that such labels are more reliable and practical to
obtain for reference sets of image pairs than voxel-level correspondence.
Typical anatomical labels of interest may include solid organs, vessels, ducts,
structure boundaries and other subject-specific ad hoc landmarks. The proposed
end-to-end convolutional neural network approach aims to predict displacement
fields to align multiple labelled corresponding structures for individual image
pairs during the training, while only unlabelled image pairs are used as the
network input for inference. We highlight the versatility of the proposed
strategy, for training, utilising diverse types of anatomical labels, which
need not to be identifiable over all training image pairs. At inference, the
resulting 3D deformable image registration algorithm runs in real-time and is
fully-automated without requiring any anatomical labels or initialisation.
Several network architecture variants are compared for registering T2-weighted
magnetic resonance images and 3D transrectal ultrasound images from prostate
cancer patients. A median target registration error of 3.6 mm on landmark
centroids and a median Dice of 0.87 on prostate glands are achieved from
cross-validation experiments, in which 108 pairs of multimodal images from 76
patients were tested with high-quality anatomical labels.",arxiv
http://arxiv.org/abs/1904.13030v2,2019-08-19T08:31:40Z,2019-04-30T03:09:01Z,"SeqLPD: Sequence Matching Enhanced Loop-Closure Detection Based on
  Large-Scale Point Cloud Description for Self-Driving Vehicles","Place recognition and loop-closure detection are main challenges in the
localization, mapping and navigation tasks of self-driving vehicles. In this
paper, we solve the loop-closure detection problem by incorporating the
deep-learning based point cloud description method and the coarse-to-fine
sequence matching strategy. More specifically, we propose a deep neural network
to extract a global descriptor from the original large-scale 3D point cloud,
then based on which, a typical place analysis approach is presented to
investigate the feature space distribution of the global descriptors and select
several super keyframes. Finally, a coarse-to-fine strategy, which includes a
super keyframe based coarse matching stage and a local sequence matching stage,
is presented to ensure the loop-closure detection accuracy and real-time
performance simultaneously. Thanks to the sequence matching operation, the
proposed approach obtains an improvement against the existing deep-learning
based methods. Experiment results on a self-driving vehicle validate the
effectiveness of the proposed loop-closure detection algorithm.",arxiv
http://arxiv.org/abs/1905.03554v1,2019-05-09T11:52:10Z,2019-05-09T11:52:10Z,1D Convolutional Neural Networks and Applications: A Survey,"During the last decade, Convolutional Neural Networks (CNNs) have become the
de facto standard for various Computer Vision and Machine Learning operations.
CNNs are feed-forward Artificial Neural Networks (ANNs) with alternating
convolutional and subsampling layers. Deep 2D CNNs with many hidden layers and
millions of parameters have the ability to learn complex objects and patterns
providing that they can be trained on a massive size visual database with
ground-truth labels. With a proper training, this unique ability makes them the
primary tool for various engineering applications for 2D signals such as images
and video frames. Yet, this may not be a viable option in numerous applications
over 1D signals especially when the training data is scarce or
application-specific. To address this issue, 1D CNNs have recently been
proposed and immediately achieved the state-of-the-art performance levels in
several applications such as personalized biomedical data classification and
early diagnosis, structural health monitoring, anomaly detection and
identification in power electronics and motor-fault detection. Another major
advantage is that a real-time and low-cost hardware implementation is feasible
due to the simple and compact configuration of 1D CNNs that perform only 1D
convolutions (scalar multiplications and additions). This paper presents a
comprehensive review of the general architecture and principals of 1D CNNs
along with their major engineering applications, especially focused on the
recent progress in this field. Their state-of-the-art performance is
highlighted concluding with their unique properties. The benchmark datasets and
the principal 1D CNN software used in those applications are also publically
shared in a dedicated website.",arxiv
http://arxiv.org/abs/1807.10603v2,2018-09-21T15:32:01Z,2018-07-23T10:40:22Z,A Capsule Network for Traffic Speed Prediction in Complex Road Networks,"This paper proposes a deep learning approach for traffic flow prediction in
complex road networks. Traffic flow data from induction loop sensors are
essentially a time series, which is also spatially related to traffic in
different road segments. The spatio-temporal traffic data can be converted into
an image where the traffic data are expressed in a 3D space with respect to
space and time axes. Although convolutional neural networks (CNNs) have been
showing surprising performance in understanding images, they have a major
drawback. In the max pooling operation, CNNs are losing important information
by locally taking the highest activation values. The inter-relationship in
traffic data measured by sparsely located sensors in different time intervals
should not be neglected in order to obtain accurate predictions. Thus, we
propose a neural network with capsules that replaces max pooling by dynamic
routing. This is the first approach that employs the capsule network on a time
series forecasting problem, to our best knowledge. Moreover, an experiment on
real traffic speed data measured in the Santander city of Spain demonstrates
the proposed method outperforms the state-of-the-art method based on a CNN by
13.1% in terms of root mean squared error.",arxiv
http://arxiv.org/abs/1807.11929v1,2018-07-31T17:27:19Z,2018-07-31T17:27:19Z,Egocentric Spatial Memory,"Egocentric spatial memory (ESM) defines a memory system with encoding,
storing, recognizing and recalling the spatial information about the
environment from an egocentric perspective. We introduce an integrated deep
neural network architecture for modeling ESM. It learns to estimate the
occupancy state of the world and progressively construct top-down 2D global
maps from egocentric views in a spatially extended environment. During the
exploration, our proposed ESM model updates belief of the global map based on
local observations using a recurrent neural network. It also augments the local
mapping with a novel external memory to encode and store latent representations
of the visited places over long-term exploration in large environments which
enables agents to perform place recognition and hence, loop closure. Our
proposed ESM network contributes in the following aspects: (1) without feature
engineering, our model predicts free space based on egocentric views
efficiently in an end-to-end manner; (2) different from other deep
learning-based mapping system, ESMN deals with continuous actions and states
which is vitally important for robotic control in real applications. In the
experiments, we demonstrate its accurate and robust global mapping capacities
in 3D virtual mazes and realistic indoor environments by comparing with several
competitive baselines.",arxiv
http://arxiv.org/abs/2006.13848v1,2020-06-24T16:20:48Z,2020-06-24T16:20:48Z,"DeepTracking-Net: 3D Tracking with Unsupervised Learning of Continuous
  Flow","This paper deals with the problem of 3D tracking, i.e., to find dense
correspondences in a sequence of time-varying 3D shapes. Despite deep learning
approaches have achieved promising performance for pairwise dense 3D shapes
matching, it is a great challenge to generalize those approaches for the
tracking of 3D time-varying geometries. In this paper, we aim at handling the
problem of 3D tracking, which provides the tracking of the consecutive frames
of 3D shapes. We propose a novel unsupervised 3D shape registration framework
named DeepTracking-Net, which uses the deep neural networks (DNNs) as auxiliary
functions to produce spatially and temporally continuous displacement fields
for 3D tracking of objects in a temporal order. Our key novelty is that we
present a novel temporal-aware correspondence descriptor (TCD) that captures
spatio-temporal essence from consecutive 3D point cloud frames. Specifically,
our DeepTracking-Net starts with optimizing a randomly initialized latent TCD.
The TCD is then decoded to regress a continuous flow (i.e. a displacement
vector field) which assigns a motion vector to every point of time-varying 3D
shapes. Our DeepTracking-Net jointly optimizes TCDs and DNNs' weights towards
the minimization of an unsupervised alignment loss. Experiments on both
simulated and real data sets demonstrate that our unsupervised DeepTracking-Net
outperforms the current supervised state-of-the-art method. In addition, we
prepare a new synthetic 3D data, named SynMotions, to the 3D tracking and
recognition community.",arxiv
http://arxiv.org/abs/1912.11659v1,2019-12-25T12:48:53Z,2019-12-25T12:48:53Z,Improving Visual Recognition using Ambient Sound for Supervision,"Our brains combine vision and hearing to create a more elaborate
interpretation of the world. When the visual input is insufficient, a rich
panoply of sounds can be used to describe our surroundings. Since more than
1,000 hours of videos are uploaded to the internet everyday, it is arduous, if
not impossible, to manually annotate these videos. Therefore, incorporating
audio along with visual data without annotations is crucial for leveraging this
explosion of data for recognizing and understanding objects and scenes.
Owens,et.al suggest that a rich representation of the physical world can be
learned by using a convolutional neural network to predict sound textures
associated with a given video frame. We attempt to reproduce the claims from
their experiments, of which the code is not publicly available. In addition, we
propose improvements in the pretext task that result in better performance in
other downstream computer vision tasks.",arxiv
http://arxiv.org/abs/1809.04471v2,2018-10-19T16:09:37Z,2018-09-12T14:10:35Z,Learning structure-from-motion from motion,"This work is based on a questioning of the quality metrics used by deep
neural networks performing depth prediction from a single image, and then of
the usability of recently published works on unsupervised learning of depth
from videos. To overcome their limitations, we propose to learn in the same
unsupervised manner a depth map inference system from monocular videos that
takes a pair of images as input. This algorithm actually learns
structure-from-motion from motion, and not only structure from context
appearance. The scale factor issue is explicitly treated, and the absolute
depth map can be estimated from camera displacement magnitude, which can be
easily measured from cheap external sensors. Our solution is also much more
robust with respect to domain variation and adaptation via fine tuning, because
it does not rely entirely in depth from context. Two use cases are considered,
unstabilized moving camera videos, and stabilized ones. This choice is
motivated by the UAV (for Unmanned Aerial Vehicle) use case that generally
provides reliable orientation measurement. We provide a set of experiments
showing that, used in real conditions where only speed can be known, our
network outperforms competitors for most depth quality measures. Results are
given on the well known KITTI dataset, which provides robust stabilization for
our second use case, but also contains moving scenes which are very typical of
the in-car road context. We then present results on a synthetic dataset that we
believe to be more representative of typical UAV scenes. Lastly, we present two
domain adaptation use cases showing superior robustness of our method compared
to single view depth algorithms, which indicates that it is better suited for
highly variable visual contexts.",arxiv
http://arxiv.org/abs/2104.14236v1,2021-04-29T09:57:47Z,2021-04-29T09:57:47Z,Learning Multi-Attention Context Graph for Group-Based Re-Identification,"Learning to re-identify or retrieve a group of people across non-overlapped
camera systems has important applications in video surveillance. However, most
existing methods focus on (single) person re-identification (re-id), ignoring
the fact that people often walk in groups in real scenarios. In this work, we
take a step further and consider employing context information for identifying
groups of people, i.e., group re-id. We propose a novel unified framework based
on graph neural networks to simultaneously address the group-based re-id tasks,
i.e., group re-id and group-aware person re-id. Specifically, we construct a
context graph with group members as its nodes to exploit dependencies among
different people. A multi-level attention mechanism is developed to formulate
both intra-group and inter-group context, with an additional self-attention
module for robust graph-level representations by attentively aggregating
node-level features. The proposed model can be directly generalized to tackle
group-aware person re-id using node-level representations. Meanwhile, to
facilitate the deployment of deep learning models on these tasks, we build a
new group re-id dataset that contains more than 3.8K images with 1.5K annotated
groups, an order of magnitude larger than existing group re-id datasets.
Extensive experiments on the novel dataset as well as three existing datasets
clearly demonstrate the effectiveness of the proposed framework for both
group-based re-id tasks. The code is available at
https://github.com/daodaofr/group_reid.",arxiv
http://arxiv.org/abs/1712.06107v1,2017-12-17T13:00:25Z,2017-12-17T13:00:25Z,Railway Track Specific Traffic Signal Selection Using Deep Learning,"With the railway transportation Industry moving actively towards automation,
accurate location and inventory of wayside track assets like traffic signals,
crossings, switches, mileposts, etc. is of extreme importance. With the new
Positive Train Control (PTC) regulation coming into effect, many railway safety
rules will be tied directly to location of assets like mileposts and signals.
Newer speed regulations will be enforced based on location of the Train with
respect to a wayside asset. Hence it is essential for the railroads to have an
accurate database of the types and locations of these assets. This paper talks
about a real-world use-case of detecting railway signals from a camera mounted
on a moving locomotive and tracking their locations. The camera is engineered
to withstand the environment factors on a moving train and provide a consistent
steady image at around 30 frames per second. Using advanced image analysis and
deep learning techniques, signals are detected in these camera images and a
database of their locations is created. Railway signals differ a lot from road
signals in terms of shapes and rules for placement with respect to track. Due
to space constraint and traffic densities in urban areas signals are not placed
on the same side of the track and multiple lines can run in parallel. Hence
there is need to associate signal detected with the track on which the train
runs. We present a method to associate the signals to the specific track they
belong to using a video feed from the front facing camera mounted on the lead
locomotive. A pipeline of track detection, region of interest selection, signal
detection has been implemented which gives an overall accuracy of 94.7% on a
route covering 150km with 247 signals.",arxiv
http://arxiv.org/abs/2004.06353v1,2020-04-14T08:33:42Z,2020-04-14T08:33:42Z,"Knowledge Elicitation using Deep Metric Learning and Psychometric
  Testing","Knowledge present in a domain is well expressed as relationships between
corresponding concepts. For example, in zoology, animal species form complex
hierarchies; in genomics, the different (parts of) molecules are organized in
groups and subgroups based on their functions; plants, molecules, and
astronomical objects all form complex taxonomies. Nevertheless, when applying
supervised machine learning (ML) in such domains, we commonly reduce the
complex and rich knowledge to a fixed set of labels, and induce a model shows
good generalization performance with respect to these labels. The main reason
for such a reductionist approach is the difficulty in eliciting the domain
knowledge from the experts. Developing a label structure with sufficient
fidelity and providing comprehensive multi-label annotation can be exceedingly
labor-intensive in many real-world applications. In this paper, we provide a
method for efficient hierarchical knowledge elicitation (HKE) from experts
working with high-dimensional data such as images or videos. Our method is
based on psychometric testing and active deep metric learning. The developed
models embed the high-dimensional data in a metric space where distances are
semantically meaningful, and the data can be organized in a hierarchical
structure. We provide empirical evidence with a series of experiments on a
synthetically generated dataset of simple shapes, and Cifar 10 and
Fashion-MNIST benchmarks that our method is indeed successful in uncovering
hierarchical structures.",arxiv
http://arxiv.org/abs/2001.05982v2,2020-06-04T15:13:47Z,2020-01-16T18:32:19Z,"A Common Operating Picture Framework Leveraging Data Fusion and Deep
  Learning","Organizations are starting to realize of the combined power of data and
data-driven algorithmic models to gain insights, situational awareness, and
advance their mission. A common challenge to gaining insights is connecting
inherently different datasets. These datasets (e.g. geocoded features, video
streams, raw text, social network data, etc.) per separate they provide very
narrow answers; however collectively they can provide new capabilities. In this
work, we present a data fusion framework for accelerating solutions for
Processing, Exploitation, and Dissemination (PED). Our platform is a collection
of services that extract information from several data sources (per separate)
by leveraging deep learning and other means of processing. This information is
fused by a set of analytical engines that perform data correlations, searches,
and other modeling operations to combine information from the disparate data
sources. As a result, events of interest are detected, geolocated, logged, and
presented into a common operating picture. This common operating picture allows
the user to visualize in real time all the data sources, per separate and their
collective cooperation. In addition, forensic activities have been implemented
and made available through the framework. Users can review archived results and
compare them to the most recent snapshot of the operational environment. In our
first iteration we have focused on visual data (FMV, WAMI, CCTV/PTZ-Cameras,
open source video, etc.) and AIS data streams (satellite and terrestrial
sources). As a proof-of-concept, in our experiments we show how FMV detections
can be combined with vessel tracking signals from AIS sources to confirm
identity, tip-and-cue aerial reconnaissance, and monitor vessel activity in an
area.",arxiv
http://arxiv.org/abs/2011.12378v1,2020-11-24T20:51:27Z,2020-11-24T20:51:27Z,"A Non-linear Function-on-Function Model for Regression with Time Series
  Data","In the last few decades, building regression models for non-scalar variables,
including time series, text, image, and video, has attracted increasing
interests of researchers from the data analytic community. In this paper, we
focus on a multivariate time series regression problem. Specifically, we aim to
learn mathematical mappings from multiple chronologically measured numerical
variables within a certain time interval S to multiple numerical variables of
interest over time interval T. Prior arts, including the multivariate
regression model, the Seq2Seq model, and the functional linear models, suffer
from several limitations. The first two types of models can only handle
regularly observed time series. Besides, the conventional multivariate
regression models tend to be biased and inefficient, as they are incapable of
encoding the temporal dependencies among observations from the same time
series. The sequential learning models explicitly use the same set of
parameters along time, which has negative impacts on accuracy. The
function-on-function linear model in functional data analysis (a branch of
statistics) is insufficient to capture complex correlations among the
considered time series and suffer from underfitting easily. In this paper, we
propose a general functional mapping that embraces the function-on-function
linear model as a special case. We then propose a non-linear
function-on-function model using the fully connected neural network to learn
the mapping from data, which addresses the aforementioned concerns in the
existing approaches. For the proposed model, we describe in detail the
corresponding numerical implementation procedures. The effectiveness of the
proposed model is demonstrated through the application to two real-world
problems.",arxiv
http://arxiv.org/abs/2106.00988v1,2021-06-02T07:10:54Z,2021-06-02T07:10:54Z,"OctoPath: An OcTree Based Self-Supervised Learning Approach to Local
  Trajectory Planning for Mobile Robots","Autonomous mobile robots are usually faced with challenging situations when
driving in complex environments. Namely, they have to recognize the static and
dynamic obstacles, plan the driving path and execute their motion. For
addressing the issue of perception and path planning, in this paper, we
introduce OctoPath , which is an encoder-decoder deep neural network, trained
in a self-supervised manner to predict the local optimal trajectory for the
ego-vehicle. Using the discretization provided by a 3D octree environment
model, our approach reformulates trajectory prediction as a classification
problem with a configurable resolution. During training, OctoPath minimizes the
error between the predicted and the manually driven trajectories in a given
training dataset. This allows us to avoid the pitfall of regression-based
trajectory estimation, in which there is an infinite state space for the output
trajectory points. Environment sensing is performed using a 40-channel
mechanical LiDAR sensor, fused with an inertial measurement unit and wheels
odometry for state estimation. The experiments are performed both in simulation
and real-life, using our own developed GridSim simulator and RovisLab's
Autonomous Mobile Test Unit platform. We evaluate the predictions of OctoPath
in different driving scenarios, both indoor and outdoor, while benchmarking our
system against a baseline hybrid A-Star algorithm and a regression-based
supervised learning method, as well as against a CNN learning-based optimal
path planning method.",arxiv
http://arxiv.org/abs/2109.10664v1,2021-09-22T11:47:24Z,2021-09-22T11:47:24Z,"A deep neural network for multi-species fish detection using multiple
  acoustic cameras","Underwater acoustic cameras are high potential devices for many applications
in ecology, notably for fisheries management and monitoring. However how to
extract such data into high value information without a time-consuming entire
dataset reading by an operator is still a challenge. Moreover the analysis of
acoustic imaging, due to its low signal-to-noise ratio, is a perfect training
ground for experimenting with new approaches, especially concerning Deep
Learning techniques. We present hereby a novel approach that takes advantage of
both CNN (Convolutional Neural Network) and classical CV (Computer Vision)
techniques, able to detect a generic class ''fish'' in acoustic video streams.
The pipeline pre-treats the acoustic images to extract 2 features, in order to
localise the signals and improve the detection performances. To ensure the
performances from an ecological point of view, we propose also a two-step
validation, one to validate the results of the trainings and one to test the
method on a real-world scenario. The YOLOv3-based model was trained with data
of fish from multiple species recorded by the two common acoustic cameras,
DIDSON and ARIS, including species of high ecological interest, as Atlantic
salmon or European eels. The model we developed provides satisfying results
detecting almost 80% of fish and minimizing the false positive rate, however
the model is much less efficient for eel detections on ARIS videos. The first
CNN pipeline for fish monitoring exploiting video data from two models of
acoustic cameras satisfies most of the required features. Many challenges are
still present, such as the automation of fish species identification through a
multiclass model. 1 However the results point a new solution for dealing with
complex data, such as sonar data, which can also be reapplied in other cases
where the signal-to-noise ratio is a challenge.",arxiv
http://arxiv.org/abs/2007.13371v1,2020-07-27T08:42:07Z,2020-07-27T08:42:07Z,"Building Trust in Autonomous Vehicles: Role of Virtual Reality Driving
  Simulators in HMI Design","The investigation of factors contributing at making humans trust Autonomous
Vehicles (AVs) will play a fundamental role in the adoption of such technology.
The user's ability to form a mental model of the AV, which is crucial to
establish trust, depends on effective user-vehicle communication; thus, the
importance of Human-Machine Interaction (HMI) is poised to increase. In this
work, we propose a methodology to validate the user experience in AVs based on
continuous, objective information gathered from physiological signals, while
the user is immersed in a Virtual Reality-based driving simulation. We applied
this methodology to the design of a head-up display interface delivering visual
cues about the vehicle' sensory and planning systems. Through this approach, we
obtained qualitative and quantitative evidence that a complete picture of the
vehicle's surrounding, despite the higher cognitive load, is conducive to a
less stressful experience. Moreover, after having been exposed to a more
informative interface, users involved in the study were also more willing to
test a real AV. The proposed methodology could be extended by adjusting the
simulation environment, the HMI and/or the vehicle's Artificial Intelligence
modules to dig into other aspects of the user experience.",arxiv
http://arxiv.org/abs/2008.01160v2,2020-10-23T11:44:08Z,2020-08-03T19:56:04Z,A Spectral Energy Distance for Parallel Speech Synthesis,"Speech synthesis is an important practical generative modeling problem that
has seen great progress over the last few years, with likelihood-based
autoregressive neural models now outperforming traditional concatenative
systems. A downside of such autoregressive models is that they require
executing tens of thousands of sequential operations per second of generated
audio, making them ill-suited for deployment on specialized deep learning
hardware. Here, we propose a new learning method that allows us to train highly
parallel models of speech, without requiring access to an analytical likelihood
function. Our approach is based on a generalized energy distance between the
distributions of the generated and real audio. This spectral energy distance is
a proper scoring rule with respect to the distribution over
magnitude-spectrograms of the generated waveform audio and offers statistical
consistency guarantees. The distance can be calculated from minibatches without
bias, and does not involve adversarial learning, yielding a stable and
consistent method for training implicit generative models. Empirically, we
achieve state-of-the-art generation quality among implicit generative models,
as judged by the recently-proposed cFDSD metric. When combining our method with
adversarial techniques, we also improve upon the recently-proposed GAN-TTS
model in terms of Mean Opinion Score as judged by trained human evaluators.",arxiv
http://arxiv.org/abs/2106.00010v1,2021-05-31T06:30:59Z,2021-05-31T06:30:59Z,Multi-Scale Attention Neural Network for Acoustic Echo Cancellation,"Acoustic Echo Cancellation (AEC) plays a key role in speech interaction by
suppressing the echo received at microphone introduced by acoustic
reverberations from loudspeakers. Since the performance of linear adaptive
filter (AF) would degrade severely due to nonlinear distortions, background
noises, and microphone clipping in real scenarios, deep learning has been
employed for AEC for its good nonlinear modelling ability. In this paper, we
constructed an end-to-end multi-scale attention neural network for AEC.
Temporal convolution is first used to transform waveform into spectrogram. The
spectrograms of the far-end reference and the near-end mixture are
concatenated, and fed to a temporal convolution network (TCN) with stacked
dilated convolution layers. Attention mechanism is performed among these
representations from different layers to adaptively extract relevant features
by referring to the previous hidden state in the encoder long short-term memory
(LSTM) unit. The representations are weighted averaged and fed to the encoder
LSTM for the near-end speech estimation. Experiments show the superiority of
our method in terms of the echo return loss enhancement (ERLE) for single-talk
periods and the perceptual evaluation of speech quality (PESQ) score for
double-talk periods in background noise and nonlinear distortion scenarios.",arxiv
http://arxiv.org/abs/1804.04065v1,2018-04-11T16:01:26Z,2018-04-11T16:01:26Z,Learning to Extract a Video Sequence from a Single Motion-Blurred Image,"We present a method to extract a video sequence from a single motion-blurred
image. Motion-blurred images are the result of an averaging process, where
instant frames are accumulated over time during the exposure of the sensor.
Unfortunately, reversing this process is nontrivial. Firstly, averaging
destroys the temporal ordering of the frames. Secondly, the recovery of a
single frame is a blind deconvolution task, which is highly ill-posed. We
present a deep learning scheme that gradually reconstructs a temporal ordering
by sequentially extracting pairs of frames. Our main contribution is to
introduce loss functions invariant to the temporal order. This lets a neural
network choose during training what frame to output among the possible
combinations. We also address the ill-posedness of deblurring by designing a
network with a large receptive field and implemented via resampling to achieve
a higher computational efficiency. Our proposed method can successfully
retrieve sharp image sequences from a single motion blurred image and can
generalize well on synthetic and real datasets captured with different cameras.",arxiv
http://arxiv.org/abs/2005.00828v1,2020-05-02T13:16:16Z,2020-05-02T13:16:16Z,DroTrack: High-speed Drone-based Object Tracking Under Uncertainty,"We present DroTrack, a high-speed visual single-object tracking framework for
drone-captured video sequences. Most of the existing object tracking methods
are designed to tackle well-known challenges, such as occlusion and cluttered
backgrounds. The complex motion of drones, i.e., multiple degrees of freedom in
three-dimensional space, causes high uncertainty. The uncertainty problem leads
to inaccurate location predictions and fuzziness in scale estimations. DroTrack
solves such issues by discovering the dependency between object representation
and motion geometry. We implement an effective object segmentation based on
Fuzzy C Means (FCM). We incorporate the spatial information into the membership
function to cluster the most discriminative segments. We then enhance the
object segmentation by using a pre-trained Convolution Neural Network (CNN)
model. DroTrack also leverages the geometrical angular motion to estimate a
reliable object scale. We discuss the experimental results and performance
evaluation using two datasets of 51,462 drone-captured frames. The combination
of the FCM segmentation and the angular scaling increased DroTrack precision by
up to $9\%$ and decreased the centre location error by $162$ pixels on average.
DroTrack outperforms all the high-speed trackers and achieves comparable
results in comparison to deep learning trackers. DroTrack offers high frame
rates up to 1000 frame per second (fps) with the best location precision, more
than a set of state-of-the-art real-time trackers.",arxiv
http://arxiv.org/abs/1807.05838v1,2018-07-16T13:13:37Z,2018-07-16T13:13:37Z,"Assessing fish abundance from underwater video using deep neural
  networks","Uses of underwater videos to assess diversity and abundance of fish are being
rapidly adopted by marine biologists. Manual processing of videos for
quantification by human analysts is time and labour intensive. Automatic
processing of videos can be employed to achieve the objectives in a cost and
time-efficient way. The aim is to build an accurate and reliable fish detection
and recognition system, which is important for an autonomous robotic platform.
However, there are many challenges involved in this task (e.g. complex
background, deformation, low resolution and light propagation). Recent
advancement in the deep neural network has led to the development of object
detection and recognition in real time scenarios. An end-to-end deep
learning-based architecture is introduced which outperformed the state of the
art methods and first of its kind on fish assessment task. A Region Proposal
Network (RPN) introduced by an object detector termed as Faster R-CNN was
combined with three classification networks for detection and recognition of
fish species obtained from Remote Underwater Video Stations (RUVS). An accuracy
of 82.4% (mAP) obtained from the experiments are much higher than previously
proposed methods.",arxiv
http://arxiv.org/abs/1905.05925v1,2019-05-15T03:17:18Z,2019-05-15T03:17:18Z,"SmartBullets: A Cloud-Assisted Bullet Screen Filter based on Deep
  Learning","Bullet-screen is a technique that enables the website users to send real-time
comment `bullet' cross the screen. Compared with the traditional review of a
video, bullet-screen provides new features of feeling expression to video
watching and more iterations between video viewers. However, since all the
comments from the viewers are shown on the screen publicly and simultaneously,
some low-quality bullets will reduce the watching enjoyment of the users.
Although the bullet-screen video websites have provided filter functions based
on regular expression, bad bullets can still easily pass the filter through
making a small modification.
  In this paper, we present SmartBullets, a user-centered bullet-screen filter
based on deep learning techniques. A convolutional neural network is trained as
the classifier to determine whether a bullet need to be removed according to
its quality. Moreover, to increase the scalability of the filter, we employ a
cloud-assisted framework by developing a backend cloud server and a front-end
browser extension. The evaluation of 40 volunteers shows that SmartBullets can
effectively remove the low-quality bullets and improve the overall watching
experience of viewers.",arxiv
http://arxiv.org/abs/2107.06397v1,2021-07-13T21:12:34Z,2021-07-13T21:12:34Z,"SurgeonAssist-Net: Towards Context-Aware Head-Mounted Display-Based
  Augmented Reality for Surgical Guidance","We present SurgeonAssist-Net: a lightweight framework making
action-and-workflow-driven virtual assistance, for a set of predefined surgical
tasks, accessible to commercially available optical see-through head-mounted
displays (OST-HMDs). On a widely used benchmark dataset for laparoscopic
surgical workflow, our implementation competes with state-of-the-art approaches
in prediction accuracy for automated task recognition, and yet requires 7.4x
fewer parameters, 10.2x fewer floating point operations per second (FLOPS), is
7.0x faster for inference on a CPU, and is capable of near real-time
performance on the Microsoft HoloLens 2 OST-HMD. To achieve this, we make use
of an efficient convolutional neural network (CNN) backbone to extract
discriminative features from image data, and a low-parameter recurrent neural
network (RNN) architecture to learn long-term temporal dependencies. To
demonstrate the feasibility of our approach for inference on the HoloLens 2 we
created a sample dataset that included video of several surgical tasks recorded
from a user-centric point-of-view. After training, we deployed our model and
cataloged its performance in an online simulated surgical scenario for the
prediction of the current surgical task. The utility of our approach is
explored in the discussion of several relevant clinical use-cases. Our code is
publicly available at https://github.com/doughtmw/surgeon-assist-net.",arxiv
http://arxiv.org/abs/2106.14275v1,2021-06-27T16:39:39Z,2021-06-27T16:39:39Z,Learning without Forgetting for 3D Point Cloud Objects,"When we fine-tune a well-trained deep learning model for a new set of
classes, the network learns new concepts but gradually forgets the knowledge of
old training. In some real-life applications, we may be interested in learning
new classes without forgetting the capability of previous experience. Such
learning without forgetting problem is often investigated using 2D image
recognition tasks. In this paper, considering the growth of depth camera
technology, we address the same problem for the 3D point cloud object data.
This problem becomes more challenging in the 3D domain than 2D because of the
unavailability of large datasets and powerful pretrained backbone models. We
investigate knowledge distillation techniques on 3D data to reduce catastrophic
forgetting of the previous training. Moreover, we improve the distillation
process by using semantic word vectors of object classes. We observe that
exploring the interrelation of old and new knowledge during training helps to
learn new concepts without forgetting old ones. Experimenting on three 3D point
cloud recognition backbones (PointNet, DGCNN, and PointConv) and synthetic
(ModelNet40, ModelNet10) and real scanned (ScanObjectNN) datasets, we establish
new baseline results on learning without forgetting for 3D data. This research
will instigate many future works in this area.",arxiv
http://arxiv.org/abs/1904.06585v3,2020-07-28T15:22:17Z,2019-04-13T19:01:11Z,"Recovery of Superquadrics from Range Images using Deep Learning: A
  Preliminary Study","It has been a longstanding goal in computer vision to describe the 3D
physical space in terms of parameterized volumetric models that would allow
autonomous machines to understand and interact with their surroundings. Such
models are typically motivated by human visual perception and aim to represents
all elements of the physical word ranging from individual objects to complex
scenes using a small set of parameters. One of the de facto stadards to
approach this problem are superquadrics - volumetric models that define various
3D shape primitives and can be fitted to actual 3D data (either in the form of
point clouds or range images). However, existing solutions to superquadric
recovery involve costly iterative fitting procedures, which limit the
applicability of such techniques in practice. To alleviate this problem, we
explore in this paper the possibility to recover superquadrics from range
images without time consuming iterative parameter estimation techniques by
using contemporary deep-learning models, more specifically, convolutional
neural networks (CNNs). We pose the superquadric recovery problem as a
regression task and develop a CNN regressor that is able to estimate the
parameters of a superquadric model from a given range image. We train the
regressor on a large set of synthetic range images, each containing a single
(unrotated) superquadric shape and evaluate the learned model in comparaitve
experiments with the current state-of-the-art. Additionally, we also present a
qualitative analysis involving a dataset of real-world objects. The results of
our experiments show that the proposed regressor not only outperforms the
existing state-of-the-art, but also ensures a 270x faster execution time.",arxiv
http://arxiv.org/abs/2101.08846v1,2021-01-21T20:49:18Z,2021-01-21T20:49:18Z,"Soloist: Generating Mixed-Initiative Tutorials from Existing Guitar
  Instructional Videos Through Audio Processing","Learning musical instruments using online instructional videos has become
increasingly prevalent. However, pre-recorded videos lack the instantaneous
feedback and personal tailoring that human tutors provide. In addition,
existing video navigations are not optimized for instrument learning, making
the learning experience encumbered. Guided by our formative interviews with
guitar players and prior literature, we designed Soloist, a mixed-initiative
learning framework that automatically generates customizable curriculums from
off-the-shelf guitar video lessons. Soloist takes raw videos as input and
leverages deep-learning based audio processing to extract musical information.
This back-end processing is used to provide an interactive visualization to
support effective video navigation and real-time feedback on the user's
performance, creating a guided learning experience. We demonstrate the
capabilities and specific use-cases of Soloist within the domain of learning
electric guitar solos using instructional YouTube videos. A remote user study,
conducted to gather feedback from guitar players, shows encouraging results as
the users unanimously preferred learning with Soloist over unconverted
instructional videos.",arxiv
http://arxiv.org/abs/1910.08173v2,2020-03-09T17:40:42Z,2019-10-17T21:32:06Z,Deep Weakly-Supervised Domain Adaptation for Pain Localization in Videos,"Automatic pain assessment has an important potential diagnostic value for
populations that are incapable of articulating their pain experiences. As one
of the dominating nonverbal channels for eliciting pain expression events,
facial expressions has been widely investigated for estimating the pain
intensity of individual. However, using state-of-the-art deep learning (DL)
models in real-world pain estimation applications poses several challenges
related to the subjective variations of facial expressions, operational capture
conditions, and lack of representative training videos with labels. Given the
cost of annotating intensity levels for every video frame, we propose a
weakly-supervised domain adaptation (WSDA) technique that allows for training
3D CNNs for spatio-temporal pain intensity estimation using weakly labeled
videos, where labels are provided on a periodic basis. In particular, WSDA
integrates multiple instance learning into an adversarial deep domain
adaptation framework to train an Inflated 3D-CNN (I3D) model such that it can
accurately estimate pain intensities in the target operational domain. The
training process relies on weak target loss, along with domain loss and source
loss for domain adaptation of the I3D model. Experimental results obtained
using labeled source domain RECOLA videos and weakly-labeled target domain
UNBC-McMaster videos indicate that the proposed deep WSDA approach can achieve
significantly higher level of sequence (bag)-level and frame (instance)-level
pain localization accuracy than related state-of-the-art approaches.",arxiv
http://arxiv.org/abs/2103.07939v3,2021-04-14T01:11:03Z,2021-03-14T14:28:57Z,Semi-Supervised Video Deraining with Dynamical Rain Generator,"While deep learning (DL)-based video deraining methods have achieved
significant success recently, they still exist two major drawbacks. Firstly,
most of them do not sufficiently model the characteristics of rain layers of
rainy videos. In fact, the rain layers exhibit strong physical properties
(e.g., direction, scale and thickness) in spatial dimension and natural
continuities in temporal dimension, and thus can be generally modelled by the
spatial-temporal process in statistics. Secondly, current DL-based methods
seriously depend on the labeled synthetic training data, whose rain types are
always deviated from those in unlabeled real data. Such gap between synthetic
and real data sets leads to poor performance when applying them in real
scenarios. Against these issues, this paper proposes a new semi-supervised
video deraining method, in which a dynamic rain generator is employed to fit
the rain layer, expecting to better depict its insightful characteristics.
Specifically, such dynamic generator consists of one emission model and one
transition model to simultaneously encode the spatially physical structure and
temporally continuous changes of rain streaks, respectively, which both are
parameterized as deep neural networks (DNNs). Further more, different prior
formats are designed for the labeled synthetic and unlabeled real data, so as
to fully exploit the common knowledge underlying them. Last but not least, we
also design a Monte Carlo EM algorithm to solve this model. Extensive
experiments are conducted to verify the superiorities of the proposed
semi-supervised deraining model.",arxiv
http://arxiv.org/abs/1901.08373v2,2019-09-14T08:12:30Z,2019-01-24T12:11:05Z,"Three-dimensional Backbone Network for 3D Object Detection in Traffic
  Scenes","The task of detecting 3D objects in traffic scenes has a pivotal role in many
real-world applications. However, the performance of 3D object detection is
lower than that of 2D object detection due to the lack of powerful 3D feature
extraction methods. To address this issue, this study proposes a 3D backbone
network to acquire comprehensive 3D feature maps for 3D object detection. It
primarily consists of sparse 3D convolutional neural network operations in the
point cloud. The 3D backbone network can inherently learn 3D features from the
raw data without compressing the point cloud into multiple 2D images. The
sparse 3D convolutional neural network takes full advantage of the sparsity in
the 3D point cloud to accelerate computation and save memory, which makes the
3D backbone network feasible in a real-world application. Empirical experiments
were conducted on the KITTI benchmark and comparable results were obtained with
respect to the state-of-the-art performance for 3D object detection.",arxiv
http://arxiv.org/abs/2003.08808v1,2020-03-16T00:38:13Z,2020-03-16T00:38:13Z,"Deep Learning for Automatic Tracking of Tongue Surface in Real-time
  Ultrasound Videos, Landmarks instead of Contours","One usage of medical ultrasound imaging is to visualize and characterize
human tongue shape and motion during a real-time speech to study healthy or
impaired speech production. Due to the low-contrast characteristic and noisy
nature of ultrasound images, it might require expertise for non-expert users to
recognize tongue gestures in applications such as visual training of a second
language. Moreover, quantitative analysis of tongue motion needs the tongue
dorsum contour to be extracted, tracked, and visualized. Manual tongue contour
extraction is a cumbersome, subjective, and error-prone task. Furthermore, it
is not a feasible solution for real-time applications. The growth of deep
learning has been vigorously exploited in various computer vision tasks,
including ultrasound tongue contour tracking. In the current methods, the
process of tongue contour extraction comprises two steps of image segmentation
and post-processing. This paper presents a new novel approach of automatic and
real-time tongue contour tracking using deep neural networks. In the proposed
method, instead of the two-step procedure, landmarks of the tongue surface are
tracked. This novel idea enables researchers in this filed to benefits from
available previously annotated databases to achieve high accuracy results. Our
experiment disclosed the outstanding performances of the proposed technique in
terms of generalization, performance, and accuracy.",arxiv
http://arxiv.org/abs/1912.10609v1,2019-12-23T03:50:52Z,2019-12-23T03:50:52Z,One-Shot Imitation Filming of Human Motion Videos,"Imitation learning has been applied to mimic the operation of a human
cameraman in several autonomous cinematography systems. To imitate different
filming styles, existing methods train multiple models, where each model
handles a particular style and requires a significant number of training
samples. As a result, existing methods can hardly generalize to unseen styles.
In this paper, we propose a framework, which can imitate a filming style by
""seeing"" only a single demonstration video of the same style, i.e., one-shot
imitation filming. This is done by two key enabling techniques: 1) feature
extraction of the filming style from the demo video, and 2) filming style
transfer from the demo video to the new situation. We implement the approach
with deep neural network and deploy it to a 6 degrees of freedom (DOF) real
drone cinematography system by first predicting the future camera motions, and
then converting them to the drone's control commands via an odometer. Our
experimental results on extensive datasets and showcases exhibit significant
improvements in our approach over conventional baselines and our approach can
successfully mimic the footage with an unseen style.",arxiv
http://arxiv.org/abs/2011.13084v3,2021-04-21T02:11:44Z,2020-11-26T01:23:44Z,Neural Scene Flow Fields for Space-Time View Synthesis of Dynamic Scenes,"We present a method to perform novel view and time synthesis of dynamic
scenes, requiring only a monocular video with known camera poses as input. To
do this, we introduce Neural Scene Flow Fields, a new representation that
models the dynamic scene as a time-variant continuous function of appearance,
geometry, and 3D scene motion. Our representation is optimized through a neural
network to fit the observed input views. We show that our representation can be
used for complex dynamic scenes, including thin structures, view-dependent
effects, and natural degrees of motion. We conduct a number of experiments that
demonstrate our approach significantly outperforms recent monocular view
synthesis methods, and show qualitative results of space-time view synthesis on
a variety of real-world videos.",arxiv
http://arxiv.org/abs/1902.03701v1,2019-02-11T01:45:53Z,2019-02-11T01:45:53Z,"Generalization through Simulation: Integrating Simulated and Real Data
  into Deep Reinforcement Learning for Vision-Based Autonomous Flight","Deep reinforcement learning provides a promising approach for vision-based
control of real-world robots. However, the generalization of such models
depends critically on the quantity and variety of data available for training.
This data can be difficult to obtain for some types of robotic systems, such as
fragile, small-scale quadrotors. Simulated rendering and physics can provide
for much larger datasets, but such data is inherently of lower quality: many of
the phenomena that make the real-world autonomous flight problem challenging,
such as complex physics and air currents, are modeled poorly or not at all, and
the systematic differences between simulation and the real world are typically
impossible to eliminate. In this work, we investigate how data from both
simulation and the real world can be combined in a hybrid deep reinforcement
learning algorithm. Our method uses real-world data to learn about the dynamics
of the system, and simulated data to learn a generalizable perception system
that can enable the robot to avoid collisions using only a monocular camera. We
demonstrate our approach on a real-world nano aerial vehicle collision
avoidance task, showing that with only an hour of real-world data, the
quadrotor can avoid collisions in new environments with various lighting
conditions and geometry. Code, instructions for building the aerial vehicles,
and videos of the experiments can be found at github.com/gkahn13/GtS",arxiv
http://arxiv.org/abs/2005.03415v1,2020-05-07T12:30:48Z,2020-05-07T12:30:48Z,"Kunster -- AR Art Video Maker -- Real time video neural style transfer
  on mobile devices","Neural style transfer is a well-known branch of deep learning research, with
many interesting works and two major drawbacks. Most of the works in the field
are hard to use by non-expert users and substantial hardware resources are
required. In this work, we present a solution to both of these problems. We
have applied neural style transfer to real-time video (over 25 frames per
second), which is capable of running on mobile devices. We also investigate the
works on achieving temporal coherence and present the idea of fine-tuning,
already trained models, to achieve stable video. What is more, we also analyze
the impact of the common deep neural network architecture on the performance of
mobile devices with regard to number of layers and filters present. In the
experiment section we present the results of our work with respect to the iOS
devices and discuss the problems present in current Android devices as well as
future possibilities. At the end we present the qualitative results of
stylization and quantitative results of performance tested on the iPhone 11 Pro
and iPhone 6s. The presented work is incorporated in Kunster - AR Art Video
Maker application available in the Apple's App Store.",arxiv
http://arxiv.org/abs/1710.04615v2,2018-03-06T19:00:40Z,2017-10-12T17:02:31Z,"Deep Imitation Learning for Complex Manipulation Tasks from Virtual
  Reality Teleoperation","Imitation learning is a powerful paradigm for robot skill acquisition.
However, obtaining demonstrations suitable for learning a policy that maps from
raw pixels to actions can be challenging. In this paper we describe how
consumer-grade Virtual Reality headsets and hand tracking hardware can be used
to naturally teleoperate robots to perform complex tasks. We also describe how
imitation learning can learn deep neural network policies (mapping from pixels
to actions) that can acquire the demonstrated skills. Our experiments showcase
the effectiveness of our approach for learning visuomotor skills.",arxiv
http://arxiv.org/abs/1810.01074v1,2018-10-02T05:27:22Z,2018-10-02T05:27:22Z,"NU-LiteNet: Mobile Landmark Recognition using Convolutional Neural
  Networks","The growth of high-performance mobile devices has resulted in more research
into on-device image recognition. The research problems are the latency and
accuracy of automatic recognition, which remains obstacles to its real-world
usage. Although the recently developed deep neural networks can achieve
accuracy comparable to that of a human user, some of them still lack the
necessary latency. This paper describes the development of the architecture of
a new convolutional neural network model, NU-LiteNet. For this, SqueezeNet was
developed to reduce the model size to a degree suitable for smartphones. The
model size of NU-LiteNet is therefore 2.6 times smaller than that of
SqueezeNet. The recognition accuracy of NU-LiteNet also compared favorably with
other recently developed deep neural networks, when experiments were conducted
on two standard landmark databases.",arxiv
http://arxiv.org/abs/2008.04470v1,2020-08-11T01:24:45Z,2020-08-11T01:24:45Z,"PoCoNet: Better Speech Enhancement with Frequency-Positional Embeddings,
  Semi-Supervised Conversational Data, and Biased Loss","Neural network applications generally benefit from larger-sized models, but
for current speech enhancement models, larger scale networks often suffer from
decreased robustness to the variety of real-world use cases beyond what is
encountered in training data. We introduce several innovations that lead to
better large neural networks for speech enhancement. The novel PoCoNet
architecture is a convolutional neural network that, with the use of
frequency-positional embeddings, is able to more efficiently build
frequency-dependent features in the early layers. A semi-supervised method
helps increase the amount of conversational training data by pre-enhancing
noisy datasets, improving performance on real recordings. A new loss function
biased towards preserving speech quality helps the optimization better match
human perceptual opinions on speech quality. Ablation experiments and objective
and human opinion metrics show the benefits of the proposed improvements.",arxiv
http://arxiv.org/abs/2102.07657v1,2021-02-11T21:09:58Z,2021-02-11T21:09:58Z,Real-Time Topology Optimization in 3D via Deep Transfer Learning,"The published literature on topology optimization has exploded over the last
two decades to include methods that use shape and topological derivatives or
evolutionary algorithms formulated on various geometric representations and
parametrizations. One of the key challenges of all these methods is the massive
computational cost associated with 3D topology optimization problems. We
introduce a transfer learning method based on a convolutional neural network
that (1) can handle high-resolution 3D design domains of various shapes and
topologies; (2) supports real-time design space explorations as the domain and
boundary conditions change; (3) requires a much smaller set of high-resolution
examples for the improvement of learning in a new task compared to traditional
deep learning networks; (4) is multiple orders of magnitude more efficient than
the established gradient-based methods, such as SIMP. We provide numerous 2D
and 3D examples to showcase the effectiveness and accuracy of our proposed
approach, including for design domains that are unseen to our source network,
as well as the generalization capabilities of the transfer learning-based
approach. Our experiments achieved an average binary accuracy of around 95% at
real-time prediction rates. These properties, in turn, suggest that the
proposed transfer-learning method may serve as the first practical underlying
framework for real-time 3D design exploration based on topology optimization",arxiv
http://arxiv.org/abs/2007.00114v3,2020-07-10T04:14:21Z,2020-06-30T21:23:06Z,"FathomNet: An underwater image training database for ocean exploration
  and discovery","Thousands of hours of marine video data are collected annually from remotely
operated vehicles (ROVs) and other underwater assets. However, current manual
methods of analysis impede the full utilization of collected data for real time
algorithms for ROV and large biodiversity analyses. FathomNet is a novel
baseline image training set, optimized to accelerate development of modern,
intelligent, and automated analysis of underwater imagery. Our seed data set
consists of an expertly annotated and continuously maintained database with
more than 26,000 hours of videotape, 6.8 million annotations, and 4,349 terms
in the knowledge base. FathomNet leverages this data set by providing imagery,
localizations, and class labels of underwater concepts in order to enable
machine learning algorithm development. To date, there are more than 80,000
images and 106,000 localizations for 233 different classes, including midwater
and benthic organisms. Our experiments consisted of training various deep
learning algorithms with approaches to address weakly supervised localization,
image labeling, object detection and classification which prove to be
promising. While we find quality results on prediction for this new dataset,
our results indicate that we are ultimately in need of a larger data set for
ocean exploration.",arxiv
http://arxiv.org/abs/2102.09137v1,2021-02-18T03:20:35Z,2021-02-18T03:20:35Z,"Multi-Agent Reinforcement Learning of 3D Furniture Layout Simulation in
  Indoor Graphics Scenes","In the industrial interior design process, professional designers plan the
furniture layout to achieve a satisfactory 3D design for selling. In this
paper, we explore the interior graphics scenes design task as a Markov decision
process (MDP) in 3D simulation, which is solved by multi-agent reinforcement
learning. The goal is to produce furniture layout in the 3D simulation of the
indoor graphics scenes. In particular, we firstly transform the 3D interior
graphic scenes into two 2D simulated scenes. We then design the simulated
environment and apply two reinforcement learning agents to learn the optimal 3D
layout for the MDP formulation in a cooperative way. We conduct our experiments
on a large-scale real-world interior layout dataset that contains industrial
designs from professional designers. Our numerical results demonstrate that the
proposed model yields higher-quality layouts as compared with the state-of-art
model. The developed simulator and codes are available at
\url{https://github.com/CODE-SUBMIT/simulator2}.",arxiv
http://arxiv.org/abs/2008.05650v1,2020-08-13T02:24:28Z,2020-08-13T02:24:28Z,"MLNET: An Adaptive Multiple Receptive-field Attention Neural Network for
  Voice Activity Detection","Voice activity detection (VAD) makes a distinction between speech and
non-speech and its performance is of crucial importance for speech based
services. Recently, deep neural network (DNN)-based VADs have achieved better
performance than conventional signal processing methods. The existed DNNbased
models always handcrafted a fixed window to make use of the contextual speech
information to improve the performance of VAD. However, the fixed window of
contextual speech information can't handle various unpredicatable noise
environments and highlight the critical speech information to VAD task. In
order to solve this problem, this paper proposed an adaptive multiple
receptive-field attention neural network, called MLNET, to finish VAD task. The
MLNET leveraged multi-branches to extract multiple contextual speech
information and investigated an effective attention block to weight the most
crucial parts of the context for final classification. Experiments in
real-world scenarios demonstrated that the proposed MLNET-based model
outperformed other baselines.",arxiv
http://arxiv.org/abs/2004.06912v1,2020-04-15T07:22:02Z,2020-04-15T07:22:02Z,"Combining Visible Light and Infrared Imaging for Efficient Detection of
  Respiratory Infections such as COVID-19 on Portable Device","Coronavirus Disease 2019 (COVID-19) has become a serious global epidemic in
the past few months and caused huge loss to human society worldwide. For such a
large-scale epidemic, early detection and isolation of potential virus carriers
is essential to curb the spread of the epidemic. Recent studies have shown that
one important feature of COVID-19 is the abnormal respiratory status caused by
viral infections. During the epidemic, many people tend to wear masks to reduce
the risk of getting sick. Therefore, in this paper, we propose a portable
non-contact method to screen the health condition of people wearing masks
through analysis of the respiratory characteristics. The device mainly consists
of a FLIR one thermal camera and an Android phone. This may help identify those
potential patients of COVID-19 under practical scenarios such as pre-inspection
in schools and hospitals. In this work, we perform the health screening through
the combination of the RGB and thermal videos obtained from the dual-mode
camera and deep learning architecture.We first accomplish a respiratory data
capture technique for people wearing masks by using face recognition. Then, a
bidirectional GRU neural network with attention mechanism is applied to the
respiratory data to obtain the health screening result. The results of
validation experiments show that our model can identify the health status on
respiratory with the accuracy of 83.7\% on the real-world dataset. The abnormal
respiratory data and part of normal respiratory data are collected from Ruijin
Hospital Affiliated to The Shanghai Jiao Tong University Medical School. Other
normal respiratory data are obtained from healthy people around our
researchers. This work demonstrates that the proposed portable and intelligent
health screening device can be used as a pre-scan method for respiratory
infections, which may help fight the current COVID-19 epidemic.",arxiv
http://arxiv.org/abs/2109.12400v1,2021-09-25T16:43:10Z,2021-09-25T16:43:10Z,"Communication-Efficient Distributed Linear and Deep Generalized
  Canonical Correlation Analysis","Classic and deep learning-based generalized canonical correlation analysis
(GCCA) algorithms seek low-dimensional common representations of data entities
from multiple ``views'' (e.g., audio and image) using linear transformations
and neural networks, respectively. When the views are acquired and stored at
different locations, organizations and edge devices, computing GCCA in a
distributed, parallel and efficient manner is well-motivated. However, existing
distributed GCCA algorithms may incur prohitively high communication overhead.
This work puts forth a communication-efficient distributed framework for both
linear and deep GCCA under the maximum variance (MAX-VAR) paradigm. The
overhead issue is addressed by aggressively compressing (via quantization) the
exchanging information between the distributed computing agents and a central
controller. Compared to the unquantized version, the proposed algorithm
consistently reduces the communication overhead by about $90\%$ with virtually
no loss in accuracy and convergence speed. Rigorous convergence analyses are
also presented -- which is a nontrivial effort since no existing generic result
from quantized distributed optimization covers the special problem structure of
GCCA. Our result shows that the proposed algorithms for both linear and deep
GCCA converge to critical points in a sublinear rate, even under heavy
quantization and stochastic approximations. In addition, it is shown that in
the linear MAX-VAR case, the quantized algorithm approaches a {\it global
optimum} in a {\it geometric} rate -- if the computing agents' updates meet a
certain accuracy level. Synthetic and real data experiments are used to
showcase the effectiveness of the proposed approach.",arxiv
http://arxiv.org/abs/1802.01267v1,2018-02-05T05:00:35Z,2018-02-05T05:00:35Z,"ClassSim: Similarity between Classes Defined by Misclassification Ratios
  of Trained Classifiers","Deep neural networks (DNNs) have achieved exceptional performances in many
tasks, particularly, in supervised classification tasks. However, achievements
with supervised classification tasks are based on large datasets with
well-separated classes. Typically, real-world applications involve wild
datasets that include similar classes; thus, evaluating similarities between
classes and understanding relations among classes are important. To address
this issue, a similarity metric, ClassSim, based on the misclassification
ratios of trained DNNs is proposed herein. We conducted image recognition
experiments to demonstrate that the proposed method provides better
similarities compared with existing methods and is useful for classification
problems. Source code including all experimental results is available at
https://github.com/karino2/ClassSim/.",arxiv
http://arxiv.org/abs/2009.08614v1,2020-09-18T03:32:47Z,2020-09-18T03:32:47Z,"Reinforcement Learning for Weakly Supervised Temporal Grounding of
  Natural Language in Untrimmed Videos","Temporal grounding of natural language in untrimmed videos is a fundamental
yet challenging multimedia task facilitating cross-media visual content
retrieval. We focus on the weakly supervised setting of this task that merely
accesses to coarse video-level language description annotation without temporal
boundary, which is more consistent with reality as such weak labels are more
readily available in practice. In this paper, we propose a \emph{Boundary
Adaptive Refinement} (BAR) framework that resorts to reinforcement learning
(RL) to guide the process of progressively refining the temporal boundary. To
the best of our knowledge, we offer the first attempt to extend RL to temporal
localization task with weak supervision. As it is non-trivial to obtain a
straightforward reward function in the absence of pairwise granular
boundary-query annotations, a cross-modal alignment evaluator is crafted to
measure the alignment degree of segment-query pair to provide tailor-designed
rewards. This refinement scheme completely abandons traditional sliding window
based solution pattern and contributes to acquiring more efficient,
boundary-flexible and content-aware grounding results. Extensive experiments on
two public benchmarks Charades-STA and ActivityNet demonstrate that BAR
outperforms the state-of-the-art weakly-supervised method and even beats some
competitive fully-supervised ones.",arxiv
http://arxiv.org/abs/2006.12366v1,2020-06-11T15:46:25Z,2020-06-11T15:46:25Z,"Scoring and Assessment in Medical VR Training Simulators with Dynamic
  Time Series Classification","This research proposes and evaluates scoring and assessment methods for
Virtual Reality (VR) training simulators. VR simulators capture detailed
n-dimensional human motion data which is useful for performance analysis.
Custom made medical haptic VR training simulators were developed and used to
record data from 271 trainees of multiple clinical experience levels. DTW
Multivariate Prototyping (DTW-MP) is proposed. VR data was classified as
Novice, Intermediate or Expert. Accuracy of algorithms applied for time-series
classification were: dynamic time warping 1-nearest neighbor (DTW-1NN) 60%,
nearest centroid SoftDTW classification 77.5%, Deep Learning: ResNet 85%, FCN
75%, CNN 72.5% and MCDCNN 28.5%. Expert VR data recordings can be used for
guidance of novices. Assessment feedback can help trainees to improve skills
and consistency. Motion analysis can identify different techniques used by
individuals. Mistakes can be detected dynamically in real-time, raising alarms
to prevent injuries.",arxiv
http://arxiv.org/abs/2103.13477v2,2021-03-26T02:49:43Z,2021-03-24T20:52:23Z,A Survey of Multimedia Technologies and Robust Algorithms,"Multimedia technologies are now more practical and deployable in real life,
and the algorithms are widely used in various researching areas such as deep
learning, signal processing, haptics, computer vision, robotics, and medical
multimedia processing. This survey provides an overview of multimedia
technologies and robust algorithms in multimedia data processing, medical
multimedia processing, human facial expression tracking and pose recognition,
and multimedia in education and training. This survey will also analyze and
propose a future research direction based on the overview of current robust
algorithms and multimedia technologies. We want to thank the research and
previous work done by the Multimedia Research Centre (MRC), the University of
Alberta, which is the inspiration and starting point for future research.",arxiv
http://arxiv.org/abs/1711.00199v3,2018-05-26T07:34:09Z,2017-11-01T04:10:58Z,"PoseCNN: A Convolutional Neural Network for 6D Object Pose Estimation in
  Cluttered Scenes","Estimating the 6D pose of known objects is important for robots to interact
with the real world. The problem is challenging due to the variety of objects
as well as the complexity of a scene caused by clutter and occlusions between
objects. In this work, we introduce PoseCNN, a new Convolutional Neural Network
for 6D object pose estimation. PoseCNN estimates the 3D translation of an
object by localizing its center in the image and predicting its distance from
the camera. The 3D rotation of the object is estimated by regressing to a
quaternion representation. We also introduce a novel loss function that enables
PoseCNN to handle symmetric objects. In addition, we contribute a large scale
video dataset for 6D object pose estimation named the YCB-Video dataset. Our
dataset provides accurate 6D poses of 21 objects from the YCB dataset observed
in 92 videos with 133,827 frames. We conduct extensive experiments on our
YCB-Video dataset and the OccludedLINEMOD dataset to show that PoseCNN is
highly robust to occlusions, can handle symmetric objects, and provide accurate
pose estimation using only color images as input. When using depth data to
further refine the poses, our approach achieves state-of-the-art results on the
challenging OccludedLINEMOD dataset. Our code and dataset are available at
https://rse-lab.cs.washington.edu/projects/posecnn/.",arxiv
http://arxiv.org/abs/1711.06976v4,2019-08-14T11:17:00Z,2017-11-19T06:46:21Z,"MIT Advanced Vehicle Technology Study: Large-Scale Naturalistic Driving
  Study of Driver Behavior and Interaction with Automation","For the foreseeble future, human beings will likely remain an integral part
of the driving task, monitoring the AI system as it performs anywhere from just
over 0% to just under 100% of the driving. The governing objectives of the MIT
Autonomous Vehicle Technology (MIT-AVT) study are to (1) undertake large-scale
real-world driving data collection that includes high-definition video to fuel
the development of deep learning based internal and external perception
systems, (2) gain a holistic understanding of how human beings interact with
vehicle automation technology by integrating video data with vehicle state
data, driver characteristics, mental models, and self-reported experiences with
technology, and (3) identify how technology and other factors related to
automation adoption and use can be improved in ways that save lives. In
pursuing these objectives, we have instrumented 23 Tesla Model S and Model X
vehicles, 2 Volvo S90 vehicles, 2 Range Rover Evoque, and 2 Cadillac CT6
vehicles for both long-term (over a year per driver) and medium term (one month
per driver) naturalistic driving data collection. Furthermore, we are
continually developing new methods for analysis of the massive-scale dataset
collected from the instrumented vehicle fleet. The recorded data streams
include IMU, GPS, CAN messages, and high-definition video streams of the driver
face, the driver cabin, the forward roadway, and the instrument cluster (on
select vehicles). The study is on-going and growing. To date, we have 122
participants, 15,610 days of participation, 511,638 miles, and 7.1 billion
video frames. This paper presents the design of the study, the data collection
hardware, the processing of the data, and the computer vision algorithms
currently being used to extract actionable knowledge from the data.",arxiv
http://arxiv.org/abs/1811.00454v1,2018-11-01T15:50:42Z,2018-11-01T15:50:42Z,"Referenceless Performance Evaluation of Audio Source Separation using
  Deep Neural Networks","Current performance evaluation for audio source separation depends on
comparing the processed or separated signals with reference signals. Therefore,
common performance evaluation toolkits are not applicable to real-world
situations where the ground truth audio is unavailable. In this paper, we
propose a performance evaluation technique that does not require reference
signals in order to assess separation quality. The proposed technique uses a
deep neural network (DNN) to map the processed audio into its quality score.
Our experiment results show that the DNN is capable of predicting the
sources-to-artifacts ratio from the blind source separation evaluation toolkit
without the need for reference signals.",arxiv
http://arxiv.org/abs/2109.05821v1,2021-09-13T09:46:50Z,2021-09-13T09:46:50Z,Cyber-Security in the Emerging World of Smart Everything,"The fourth industrial revolution (4IR) is a revolution many authors believe
have come to stay. It is a revolution that has been fast blurring the line
between physical, digital and biological technologies. These disruptive
technologies largely rely on high-speed internet connectivity, Cloud
technologies, Augmented Reality, Additive Manufacturing, Data science and
Artificial Intelligence. Most developed economies have embraced the it while
the developing economies are struggling to adopt 4IR because they lack the
requisite skills, knowledge and technology. Thus, this study investigates
Nigeria as one of the developing economies to understand her readiness for 4IR
and the level of preparedness to mitigate the sophisticated cyber-attacks that
comes with it. The investigation adopted quantitative research approach and
developed an online questionnaire that was shared amongst the population of
interest that includes academic, industry experts and relevant stakeholders.
The questionnaire returned 116 valid responses which were analysed with
descriptive statistical tools in SPSS. Results suggest that 60 of the
respondents opined that Nigerian government at are not showing enough evidence
to demonstrate her preparedness to leverage these promised potentials by
developing 4IR relevant laws, strong institutional frameworks and policies.
They lack significant development capacity to mitigate risks associated with
digital ecosystem and cyber ecosystem that are ushered in by the 4IR. In the
universities, 52 of the courses offered at the undergraduate and 42 at the
post-graduate levels are relevant in the development of skills required in the
revolution. The study recommends that the government at all levels make
adequate efforts in developing the countrys intangible assets. In all, this
paper posits that successful implementation of these could equip Nigeria to
embrace the 4IR in all its aspects.",arxiv
http://arxiv.org/abs/2008.02646v1,2020-08-06T13:29:05Z,2020-08-06T13:29:05Z,"Deep Reinforcement Learning for Tactile Robotics: Learning to Type on a
  Braille Keyboard","Artificial touch would seem well-suited for Reinforcement Learning (RL),
since both paradigms rely on interaction with an environment. Here we propose a
new environment and set of tasks to encourage development of tactile
reinforcement learning: learning to type on a braille keyboard. Four tasks are
proposed, progressing in difficulty from arrow to alphabet keys and from
discrete to continuous actions. A simulated counterpart is also constructed by
sampling tactile data from the physical environment. Using state-of-the-art
deep RL algorithms, we show that all of these tasks can be successfully learnt
in simulation, and 3 out of 4 tasks can be learned on the real robot. A lack of
sample efficiency currently makes the continuous alphabet task impractical on
the robot. To the best of our knowledge, this work presents the first
demonstration of successfully training deep RL agents in the real world using
observations that exclusively consist of tactile images. To aid future research
utilising this environment, the code for this project has been released along
with designs of the braille keycaps for 3D printing and a guide for recreating
the experiments. A brief video summary is also available at
https://youtu.be/eNylCA2uE_E.",arxiv
http://arxiv.org/abs/1909.10159v2,2020-03-07T01:39:00Z,2019-09-23T05:10:26Z,Self-supervised 6D Object Pose Estimation for Robot Manipulation,"To teach robots skills, it is crucial to obtain data with supervision. Since
annotating real world data is time-consuming and expensive, enabling robots to
learn in a self-supervised way is important. In this work, we introduce a robot
system for self-supervised 6D object pose estimation. Starting from modules
trained in simulation, our system is able to label real world images with
accurate 6D object poses for self-supervised learning. In addition, the robot
interacts with objects in the environment to change the object configuration by
grasping or pushing objects. In this way, our system is able to continuously
collect data and improve its pose estimation modules. We show that the
self-supervised learning improves object segmentation and 6D pose estimation
performance, and consequently enables the system to grasp objects more
reliably. A video showing the experiments can be found at
https://youtu.be/W1Y0Mmh1Gd8.",arxiv
http://arxiv.org/abs/2103.11052v1,2021-03-19T22:48:03Z,2021-03-19T22:48:03Z,"A first step towards automated species recognition from camera trap
  images of mammals using AI in a European temperate forest","Camera traps are used worldwide to monitor wildlife. Despite the increasing
availability of Deep Learning (DL) models, the effective usage of this
technology to support wildlife monitoring is limited. This is mainly due to the
complexity of DL technology and high computing requirements. This paper
presents the implementation of the light-weight and state-of-the-art YOLOv5
architecture for automated labeling of camera trap images of mammals in the
Bialowieza Forest (BF), Poland. The camera trapping data were organized and
harmonized using TRAPPER software, an open source application for managing
large-scale wildlife monitoring projects. The proposed image recognition
pipeline achieved an average accuracy of 85% F1-score in the identification of
the 12 most commonly occurring medium-size and large mammal species in BF using
a limited set of training and testing data (a total 2659 images with animals).
  Based on the preliminary results, we concluded that the YOLOv5 object
detection and classification model is a promising light-weight DL solution
after the adoption of transfer learning technique. It can be efficiently
plugged in via an API into existing web-based camera trapping data processing
platforms such as e.g. TRAPPER system. Since TRAPPER is already used to manage
and classify (manually) camera trapping datasets by many research groups in
Europe, the implementation of AI-based automated species classification may
significantly speed up the data processing workflow and thus better support
data-driven wildlife monitoring and conservation. Moreover, YOLOv5 developers
perform better performance on edge devices which may open a new chapter in
animal population monitoring in real time directly from camera trap devices.",arxiv
http://arxiv.org/abs/1911.10676v3,2020-12-12T07:50:28Z,2019-11-25T03:06:43Z,Attribute Restoration Framework for Anomaly Detection,"With the recent advances in deep neural networks, anomaly detection in
multimedia has received much attention in the computer vision community. While
reconstruction-based methods have recently shown great promise for anomaly
detection, the information equivalence among input and supervision for
reconstruction tasks can not effectively force the network to learn semantic
feature embeddings. We here propose to break this equivalence by erasing
selected attributes from the original data and reformulate it as a restoration
task, where the normal and the anomalous data are expected to be
distinguishable based on restoration errors. Through forcing the network to
restore the original image, the semantic feature embeddings related to the
erased attributes are learned by the network. During testing phases, because
anomalous data are restored with the attribute learned from the normal data,
the restoration error is expected to be large. Extensive experiments have
demonstrated that the proposed method significantly outperforms several
state-of-the-arts on multiple benchmark datasets, especially on ImageNet,
increasing the AUROC of the top-performing baseline by 10.1%. We also evaluate
our method on a real-world anomaly detection dataset MVTec AD and a video
anomaly detection dataset ShanghaiTech.",arxiv
http://arxiv.org/abs/1910.07360v1,2019-10-16T14:11:24Z,2019-10-16T14:11:24Z,"Conservation AI: Live Stream Analysis for the Detection of Endangered
  Species Using Convolutional Neural Networks and Drone Technology","Many different species are adversely affected by poaching. In response to
this escalating crisis, efforts to stop poaching using hidden cameras, drones
and DNA tracking have been implemented with varying degrees of success. Limited
resources, costs and logistical limitations are often the cause of most
unsuccessful poaching interventions. The study presented in this paper outlines
a flexible and interoperable framework for the automatic detection of animals
and poaching activity to facilitate early intervention practices. Using a
robust deep learning pipeline, a convolutional neural network is trained and
implemented to detect rhinos and cars (considered an important tool in poaching
for fast access and artefact transportation in natural habitats) in the study,
that are found within live video streamed from drones Transfer learning with
the Faster RCNN Resnet 101 is performed to train a custom model with 350 images
of rhinos and 350 images of cars. Inference is performed using a frame sampling
technique to address the required trade-off control precision and processing
speed and maintain synchronisation with the live feed. Inference models are
hosted on a web platform using flask web serving, OpenCV and TensorFlow 1.13.
Video streams are transmitted from a DJI Mavic Pro 2 drone using the Real-Time
Messaging Protocol (RMTP). The best trained Faster RCNN model achieved a mAP of
0.83 @IOU 0.50 and 0.69 @IOU 0.75 respectively. In comparison an
SSD-mobilenetmodel trained under the same experimental conditions achieved a
mAP of 0.55 @IOU .50 and 0.27 @IOU 0.75.The results demonstrate that using a
FRCNN and off-the-shelf drones is a promising and scalable option for a range
of conservation projects.",arxiv
http://arxiv.org/abs/1911.07919v1,2019-11-15T18:44:25Z,2019-11-15T18:44:25Z,ASV: Accelerated Stereo Vision System,"Estimating depth from stereo vision cameras, i.e., ""depth from stereo"", is
critical to emerging intelligent applications deployed in energy- and
performance-constrained devices, such as augmented reality headsets and mobile
autonomous robots. While existing stereo vision systems make trade-offs between
accuracy, performance and energy-efficiency, we describe ASV, an accelerated
stereo vision system that simultaneously improves both performance and
energy-efficiency while achieving high accuracy. The key to ASV is to exploit
unique characteristics inherent to stereo vision, and apply stereo-specific
optimizations, both algorithmically and computationally. We make two
contributions. Firstly, we propose a new stereo algorithm, invariant-based
stereo matching (ISM), that achieves significant speedup while retaining high
accuracy. The algorithm combines classic ""hand-crafted"" stereo algorithms with
recent developments in Deep Neural Networks (DNNs), by leveraging the
correspondence invariant unique to stereo vision systems. Secondly, we observe
that the bottleneck of the ISM algorithm is the DNN inference, and in
particular the deconvolution operations that introduce massive
compute-inefficiencies. We propose a set of software optimizations that
mitigate these inefficiencies. We show that with less than 0.5% hardware area
overhead, these algorithmic and computational optimizations can be effectively
integrated within a conventional DNN accelerator. Overall, ASV achieves 5x
speedup and 85% energy saving with 0.02% accuracy loss compared to today
DNN-based stereo vision systems.",arxiv
http://arxiv.org/abs/2104.11598v1,2021-04-23T13:46:51Z,2021-04-23T13:46:51Z,"Reconstructing Speech from Real-Time Articulatory MRI Using Neural
  Vocoders","Several approaches exist for the recording of articulatory movements, such as
eletromagnetic and permanent magnetic articulagraphy, ultrasound tongue imaging
and surface electromyography. Although magnetic resonance imaging (MRI) is more
costly than the above approaches, the recent developments in this area now
allow the recording of real-time MRI videos of the articulators with an
acceptable resolution. Here, we experiment with the reconstruction of the
speech signal from a real-time MRI recording using deep neural networks.
Instead of estimating speech directly, our networks are trained to output a
spectral vector, from which we reconstruct the speech signal using the WaveGlow
neural vocoder. We compare the performance of three deep neural architectures
for the estimation task, combining convolutional (CNN) and recurrence-based
(LSTM) neural layers. Besides the mean absolute error (MAE) of our networks, we
also evaluate our models by comparing the speech signals obtained using several
objective speech quality metrics like the mean cepstral distortion (MCD),
Short-Time Objective Intelligibility (STOI), Perceptual Evaluation of Speech
Quality (PESQ) and Signal-to-Distortion Ratio (SDR). The results indicate that
our approach can successfully reconstruct the gross spectral shape, but more
improvements are needed to reproduce the fine spectral details.",arxiv
http://arxiv.org/abs/2008.03404v2,2021-02-01T16:32:40Z,2020-08-08T00:22:43Z,VPC-Net: Completion of 3D Vehicles from MLS Point Clouds,"As a dynamic and essential component in the road environment of urban
scenarios, vehicles are the most popular investigation targets. To monitor
their behavior and extract their geometric characteristics, an accurate and
instant measurement of vehicles plays a vital role in traffic and
transportation fields. Point clouds acquired from the mobile laser scanning
(MLS) system deliver 3D information of road scenes with unprecedented detail.
They have proven to be an adequate data source in the fields of intelligent
transportation and autonomous driving, especially for extracting vehicles.
However, acquired 3D point clouds of vehicles from MLS systems are inevitably
incomplete due to object occlusion or self-occlusion. To tackle this problem,
we proposed a neural network to synthesize complete, dense, and uniform point
clouds for vehicles from MLS data, named Vehicle Points Completion-Net
(VPC-Net). In this network, we introduce a new encoder module to extract global
features from the input instance, consisting of a spatial transformer network
and point feature enhancement layer. Moreover, a new refiner module is also
presented to preserve the vehicle details from inputs and refine the complete
outputs with fine-grained information. Given sparse and partial point clouds as
inputs, the network can generate complete and realistic vehicle structures and
keep the fine-grained details from the partial inputs. We evaluated the
proposed VPC-Net in different experiments using synthetic and real-scan
datasets and applied the results to 3D vehicle monitoring tasks. Quantitative
and qualitative experiments demonstrate the promising performance of the
proposed VPC-Net and show state-of-the-art results.",arxiv
http://arxiv.org/abs/1901.10584v1,2019-01-29T22:07:41Z,2019-01-29T22:07:41Z,"Trading-off Accuracy and Energy of Deep Inference on Embedded Systems: A
  Co-Design Approach","Deep neural networks have seen tremendous success for different modalities of
data including images, videos, and speech. This success has led to their
deployment in mobile and embedded systems for real-time applications. However,
making repeated inferences using deep networks on embedded systems poses
significant challenges due to constrained resources (e.g., energy and computing
power). To address these challenges, we develop a principled co-design
approach. Building on prior work, we develop a formalism referred to as
Coarse-to-Fine Networks (C2F Nets) that allow us to employ classifiers of
varying complexity to make predictions. We propose a principled optimization
algorithm to automatically configure C2F Nets for a specified trade-off between
accuracy and energy consumption for inference. The key idea is to select a
classifier on-the-fly whose complexity is proportional to the hardness of the
input example: simple classifiers for easy inputs and complex classifiers for
hard inputs. We perform comprehensive experimental evaluation using four
different C2F Net architectures on multiple real-world image classification
tasks. Our results show that optimized C2F Net can reduce the Energy Delay
Product (EDP) by 27 to 60 percent with no loss in accuracy when compared to the
baseline solution, where all predictions are made using the most complex
classifier in C2F Net.",arxiv
http://arxiv.org/abs/2110.04378v1,2021-10-08T21:00:01Z,2021-10-08T21:00:01Z,Performance optimizations on deep noise suppression models,"We study the role of magnitude structured pruning as an architecture search
to speed up the inference time of a deep noise suppression (DNS) model. While
deep learning approaches have been remarkably successful in enhancing audio
quality, their increased complexity inhibits their deployment in real-time
applications. We achieve up to a 7.25X inference speedup over the baseline,
with a smooth model performance degradation. Ablation studies indicate that our
proposed network re-parameterization (i.e., size per layer) is the major driver
of the speedup, and that magnitude structured pruning does comparably to
directly training a model in the smaller size. We report inference speed
because a parameter reduction does not necessitate speedup, and we measure
model quality using an accurate non-intrusive objective speech quality metric.",arxiv
http://arxiv.org/abs/1912.11584v1,2019-12-25T03:31:56Z,2019-12-25T03:31:56Z,"Exploration of the Applicability of Probabilistic Inference for Learning
  Control in Underactuated Autonomous Underwater Vehicles","Underwater vehicles are employed in the exploration of dynamic environments
where tuning of a specific controller for each task would be time-consuming and
unreliable as the controller depends on calculated mathematical coefficients in
idealised conditions. For such a case, learning task from experience can be a
useful alternative. This paper explores the capability of probabilistic
inference learning to control autonomous underwater vehicles that can be used
for different tasks without re-programming the controller. Probabilistic
inference learning uses a Gaussian process model of the real vehicle to learn
the correct policy with a small number of real field experiments. The use of
probabilistic reinforced learning looks for a simple implementation of
controllers without the burden of coefficients calculation, controller tuning
or system identification. A series of computational simulations were employed
to test the applicability of model-based reinforced learning in underwater
vehicles. Three simulation scenarios were evaluated: waypoint tracking, depth
control and 3D path tracking control. The 3D path tracking is done by coupling
together a line-of-sight law with probabilistic inference for learning control.
As a comparison study LOS-PILCO algorithm can perform better than a robust
LOS-PID. The results shows that probabilistic model based reinforced learning
is a possible solution to motion control of underactuated AUVs as can generate
capable policies with minimum quantity of episodes.",arxiv
http://arxiv.org/abs/2001.03025v1,2020-01-08T10:33:23Z,2020-01-08T10:33:23Z,"Deep Time-Stream Framework for Click-Through Rate Prediction by Tracking
  Interest Evolution","Click-through rate (CTR) prediction is an essential task in industrial
applications such as video recommendation. Recently, deep learning models have
been proposed to learn the representation of users' overall interests, while
ignoring the fact that interests may dynamically change over time. We argue
that it is necessary to consider the continuous-time information in CTR models
to track user interest trend from rich historical behaviors. In this paper, we
propose a novel Deep Time-Stream framework (DTS) which introduces the time
information by an ordinary differential equations (ODE). DTS continuously
models the evolution of interests using a neural network, and thus is able to
tackle the challenge of dynamically representing users' interests based on
their historical behaviors. In addition, our framework can be seamlessly
applied to any existing deep CTR models by leveraging the additional
Time-Stream Module, while no changes are made to the original CTR models.
Experiments on public dataset as well as real industry dataset with billions of
samples demonstrate the effectiveness of proposed approaches, which achieve
superior performance compared with existing methods.",arxiv
http://arxiv.org/abs/2001.06103v1,2020-01-16T22:45:52Z,2020-01-16T22:45:52Z,"An adversarial learning framework for preserving users' anonymity in
  face-based emotion recognition","Image and video-capturing technologies have permeated our every-day life.
Such technologies can continuously monitor individuals' expressions in
real-life settings, affording us new insights into their emotional states and
transitions, thus paving the way to novel well-being and healthcare
applications. Yet, due to the strong privacy concerns, the use of such
technologies is met with strong skepticism, since current face-based emotion
recognition systems relying on deep learning techniques tend to preserve
substantial information related to the identity of the user, apart from the
emotion-specific information. This paper proposes an adversarial learning
framework which relies on a convolutional neural network (CNN) architecture
trained through an iterative procedure for minimizing identity-specific
information and maximizing emotion-dependent information. The proposed approach
is evaluated through emotion classification and face identification metrics,
and is compared against two CNNs, one trained solely for emotion recognition
and the other trained solely for face identification. Experiments are performed
using the Yale Face Dataset and Japanese Female Facial Expression Database.
Results indicate that the proposed approach can learn a convolutional
transformation for preserving emotion recognition accuracy and degrading face
identity recognition, providing a foundation toward privacy-aware emotion
recognition technologies.",arxiv
http://arxiv.org/abs/2002.02598v1,2020-02-07T03:06:07Z,2020-02-07T03:06:07Z,"Object-Adaptive LSTM Network for Real-time Visual Tracking with
  Adversarial Data Augmentation","In recent years, deep learning based visual tracking methods have obtained
great success owing to the powerful feature representation ability of
Convolutional Neural Networks (CNNs). Among these methods, classification-based
tracking methods exhibit excellent performance while their speeds are heavily
limited by the expensive computation for massive proposal feature extraction.
In contrast, matching-based tracking methods (such as Siamese networks) possess
remarkable speed superiority. However, the absence of online updating renders
these methods unadaptable to significant object appearance variations. In this
paper, we propose a novel real-time visual tracking method, which adopts an
object-adaptive LSTM network to effectively capture the video sequential
dependencies and adaptively learn the object appearance variations. For high
computational efficiency, we also present a fast proposal selection strategy,
which utilizes the matching-based tracking method to pre-estimate dense
proposals and selects high-quality ones to feed to the LSTM network for
classification. This strategy efficiently filters out some irrelevant proposals
and avoids the redundant computation for feature extraction, which enables our
method to operate faster than conventional classification-based tracking
methods. In addition, to handle the problems of sample inadequacy and class
imbalance during online tracking, we adopt a data augmentation technique based
on the Generative Adversarial Network (GAN) to facilitate the training of the
LSTM network. Extensive experiments on four visual tracking benchmarks
demonstrate the state-of-the-art performance of our method in terms of both
tracking accuracy and speed, which exhibits great potentials of recurrent
structures for visual tracking.",arxiv
http://arxiv.org/abs/2011.03462v1,2020-11-06T16:28:17Z,2020-11-06T16:28:17Z,A Comprehensive Comparison of Multi-Dimensional Image Denoising Methods,"Filtering multi-dimensional images such as color images, color videos,
multispectral images and magnetic resonance images is challenging in terms of
both effectiveness and efficiency. Leveraging the nonlocal self-similarity
(NLSS) characteristic of images and sparse representation in the transform
domain, the block-matching and 3D filtering (BM3D) based methods show powerful
denoising performance. Recently, numerous new approaches with different
regularization terms, transforms and advanced deep neural network (DNN)
architectures are proposed to improve denoising quality. In this paper, we
extensively compare over 60 methods on both synthetic and real-world datasets.
We also introduce a new color image and video dataset for benchmarking, and our
evaluations are performed from four different perspectives including
quantitative metrics, visual effects, human ratings and computational cost.
Comprehensive experiments demonstrate: (i) the effectiveness and efficiency of
the BM3D family for various denoising tasks, (ii) a simple matrix-based
algorithm could produce similar results compared with its tensor counterparts,
and (iii) several DNN models trained with synthetic Gaussian noise show
state-of-the-art performance on real-world color image and video datasets.
Despite the progress in recent years, we discuss shortcomings and possible
extensions of existing techniques. Datasets and codes for evaluation are made
publicly available at https://github.com/ZhaomingKong/Denoising-Comparison.",arxiv
http://arxiv.org/abs/2106.09093v2,2021-06-22T08:44:04Z,2021-06-16T19:35:34Z,"A Hands-on Comparison of DNNs for Dialog Separation Using Transfer
  Learning from Music Source Separation","This paper describes a hands-on comparison on using state-of-the-art music
source separation deep neural networks (DNNs) before and after task-specific
fine-tuning for separating speech content from non-speech content in broadcast
audio (i.e., dialog separation). The music separation models are selected as
they share the number of channels (2) and sampling rate (44.1 kHz or higher)
with the considered broadcast content, and vocals separation in music is
considered as a parallel for dialog separation in the target application
domain. These similarities are assumed to enable transfer learning between the
tasks. Three models pre-trained on music (Open-Unmix, Spleeter, and
Conv-TasNet) are considered in the experiments, and fine-tuned with real
broadcast data. The performance of the models is evaluated before and after
fine-tuning with computational evaluation metrics (SI-SIRi, SI-SDRi, 2f-model),
as well as with a listening test simulating an application where the non-speech
signal is partially attenuated, e.g., for better speech intelligibility. The
evaluations include two reference systems specifically developed for dialog
separation. The results indicate that pre-trained music source separation
models can be used for dialog separation to some degree, and that they benefit
from the fine-tuning, reaching a performance close to task-specific solutions.",arxiv
http://arxiv.org/abs/1910.09667v1,2019-10-21T21:44:15Z,2019-10-21T21:44:15Z,"Combining Benefits from Trajectory Optimization and Deep Reinforcement
  Learning","Recent breakthroughs both in reinforcement learning and trajectory
optimization have made significant advances towards real world robotic system
deployment. Reinforcement learning (RL) can be applied to many problems without
needing any modeling or intuition about the system, at the cost of high sample
complexity and the inability to prove any metrics about the learned policies.
Trajectory optimization (TO) on the other hand allows for stability and
robustness analyses on generated motions and trajectories, but is only as good
as the often over-simplified derived model, and may have prohibitively
expensive computation times for real-time control. This paper seeks to combine
the benefits from these two areas while mitigating their drawbacks by (1)
decreasing RL sample complexity by using existing knowledge of the problem with
optimal control, and (2) providing an upper bound estimate on the
time-to-arrival of the combined learned-optimized policy, allowing online
policy deployment at any point in the training process by using the TO as a
worst-case scenario action. This method is evaluated for a car model, with
applicability to any mobile robotic system. A video showing policy execution
comparisons can be found at https://youtu.be/mv2xw83NyWU .",arxiv
http://arxiv.org/abs/1911.00889v1,2019-11-03T14:02:58Z,2019-11-03T14:02:58Z,"eBrainII: A 3 kW Realtime Custom 3D DRAM integrated ASIC implementation
  of a Biologically Plausible Model of a Human Scale Cortex","The Artificial Neural Networks (ANNs) like CNN/DNN and LSTM are not
biologically plausible and in spite of their initial success, they cannot
attain the cognitive capabilities enabled by the dynamic hierarchical
associative memory systems of biological brains. The biologically plausible
spiking brain models, for e.g. cortex, basal ganglia and amygdala have a
greater potential to achieve biological brain like cognitive capabilities.
Bayesian Confidence Propagation Neural Network (BCPNN) is a biologically
plausible spiking model of cortex. A human scale model of BCPNN in real time
requires 162 TFlops/s, 50 TBs of synaptic weight storage to be accessed with a
bandwidth of 200 TBs. The spiking bandwidth is relatively modest at 250 GBs/s.
A hand optimized implementation of rodent scale BCPNN has been implemented on
Tesla K80 GPUs require 3 kW, we extrapolate from that a human scale network
will require 3 MW. These power numbers rule out such implementations for field
deployment as advanced cognition engines in embedded systems. The key
innovation that this paper reports is that it is feasible and affordable to
implement real time BCPNN as a custom tiled ASIC in 28 nm technology with
custom 3D DRAM - eBrain II - that consumes 3 kWs for human scale and 12 W for
rodent scale cortex model. Such implementations eminently fulfill the demands
for field deployment.",arxiv
http://arxiv.org/abs/2106.08710v1,2021-06-16T11:26:37Z,2021-06-16T11:26:37Z,"Mobile Augmented Reality: User Interfaces, Frameworks, and Intelligence","Mobile Augmented Reality (MAR) integrates computer-generated virtual objects
with physical environments for mobile devices. MAR systems enable users to
interact with MAR devices, such as smartphones and head-worn wearables, and
performs seamless transitions from the physical world to a mixed world with
digital entities. These MAR systems support user experiences by using MAR
devices to provide universal accessibility to digital contents. Over the past
20 years, a number of MAR systems have been developed, however, the studies and
design of MAR frameworks have not yet been systematically reviewed from the
perspective of user-centric design. This article presents the first effort of
surveying existing MAR frameworks (count: 37) and further discusses the latest
studies on MAR through a top-down approach: 1) MAR applications; 2) MAR
visualisation techniques adaptive to user mobility and contexts; 3) systematic
evaluation of MAR frameworks including supported platforms and corresponding
features such as tracking, feature extraction plus sensing capabilities; and 4)
underlying machine learning approaches supporting intelligent operations within
MAR systems. Finally, we summarise the development of emerging research fields,
current state-of-the-art, and discuss the important open challenges and
possible theoretical and technical directions. This survey aims to benefit both
researchers and MAR system developers alike.",arxiv
http://arxiv.org/abs/1808.09945v2,2020-12-03T15:51:08Z,2018-08-29T17:51:39Z,"Fixed-Point Convolutional Neural Network for Real-Time Video Processing
  in FPGA","Modern mobile neural networks with a reduced number of weights and parameters
do a good job with image classification tasks, but even they may be too complex
to be implemented in an FPGA for video processing tasks. The article proposes
neural network architecture for the practical task of recognizing images from a
camera, which has several advantages in terms of speed. This is achieved by
reducing the number of weights, moving from a floating-point to a fixed-point
arithmetic, and due to a number of hardware-level optimizations associated with
storing weights in blocks, a shift register, and an adjustable number of
convolutional blocks that work in parallel. The article also proposed methods
for adapting the existing data set for solving a different task. As the
experiments showed, the proposed neural network copes well with real-time video
processing even on the cheap FPGAs.",arxiv
http://arxiv.org/abs/1909.07208v2,2020-03-12T13:09:24Z,2019-09-16T14:03:01Z,"MFCC-based Recurrent Neural Network for Automatic Clinical Depression
  Recognition and Assessment from Speech","Clinical depression or Major Depressive Disorder (MDD) is a common and
serious medical illness. In this paper, a deep recurrent neural network-based
framework is presented to detect depression and to predict its severity level
from speech. Low-level and high-level audio features are extracted from audio
recordings to predict the 24 scores of the Patient Health Questionnaire and the
binary class of depression diagnosis. To overcome the problem of the small size
of Speech Depression Recognition (SDR) datasets, expanding training labels and
transferred features are considered. The proposed approach outperforms the
state-of-art approaches on the DAIC-WOZ database with an overall accuracy of
76.27% and a root mean square error of 0.4 in assessing depression, while a
root mean square error of 0.168 is achieved in predicting the depression
severity levels. The proposed framework has several advantages (fastness,
non-invasiveness, and non-intrusion), which makes it convenient for real-time
applications. The performances of the proposed approach are evaluated under a
multi-modal and a multi-features experiments. MFCC based high-level features
hold relevant information related to depression. Yet, adding visual action
units and different other acoustic features further boosts the classification
results by 20% and 10% to reach an accuracy of 95.6% and 86%, respectively.
Considering visual-facial modality needs to be carefully studied as it sparks
patient privacy concerns while adding more acoustic features increases the
computation time.",arxiv
http://arxiv.org/abs/2104.00681v1,2021-04-01T17:59:46Z,2021-04-01T17:59:46Z,NeuralRecon: Real-Time Coherent 3D Reconstruction from Monocular Video,"We present a novel framework named NeuralRecon for real-time 3D scene
reconstruction from a monocular video. Unlike previous methods that estimate
single-view depth maps separately on each key-frame and fuse them later, we
propose to directly reconstruct local surfaces represented as sparse TSDF
volumes for each video fragment sequentially by a neural network. A
learning-based TSDF fusion module based on gated recurrent units is used to
guide the network to fuse features from previous fragments. This design allows
the network to capture local smoothness prior and global shape prior of 3D
surfaces when sequentially reconstructing the surfaces, resulting in accurate,
coherent, and real-time surface reconstruction. The experiments on ScanNet and
7-Scenes datasets show that our system outperforms state-of-the-art methods in
terms of both accuracy and speed. To the best of our knowledge, this is the
first learning-based system that is able to reconstruct dense coherent 3D
geometry in real-time.",arxiv
http://arxiv.org/abs/1911.06269v1,2019-11-12T02:51:00Z,2019-11-12T02:51:00Z,"Few-Features Attack to Fool Machine Learning Models through Mask-Based
  GAN","GAN is a deep-learning based generative approach to generate contents such as
images, languages and speeches. Recently, studies have shown that GAN can also
be applied to generative adversarial attack examples to fool the
machine-learning models. In comparison with the previous non-learning
adversarial example attack approaches, the GAN-based adversarial attack example
approach can generate the adversarial samples quickly using the GAN
architecture every time facing a new sample after training, but meanwhile needs
to perturb the attack samples in great quantities, which results in the
unpractical application in reality. To address this issue, we propose a new
approach, named Few-Feature-Attack-GAN (FFA-GAN). FFA-GAN has a significant
time-consuming advantage than the non-learning adversarial samples approaches
and a better non-zero-features performance than the GANbased adversarial sample
approaches. FFA-GAN can automatically generate the attack samples in the
black-box attack through the GAN architecture instead of the evolutional
algorithms or the other non-learning approaches. Besides, we introduce the mask
mechanism into the generator network of the GAN architecture to optimize the
constraint issue, which can also be regarded as the sparsity problem of the
important features. During the training, the different weights of losses of the
generator are set in the different training phases to ensure the divergence of
the two above mentioned parallel networks of the generator. Experiments are
made respectively on the structured data sets KDD-Cup 1999 and CIC-IDS 2017, in
which the dimensions of the data are relatively low, and also on the
unstructured data sets MNIST and CIFAR-10 with the data of the relatively high
dimensions. The results of the experiments demonstrate the effectiveness and
the robustness of our proposed approach.",arxiv
http://arxiv.org/abs/1904.02422v5,2021-10-18T09:43:53Z,2019-04-04T09:19:19Z,Resource Efficient 3D Convolutional Neural Networks,"Recently, convolutional neural networks with 3D kernels (3D CNNs) have been
very popular in computer vision community as a result of their superior ability
of extracting spatio-temporal features within video frames compared to 2D CNNs.
Although there has been great advances recently to build resource efficient 2D
CNN architectures considering memory and power budget, there is hardly any
similar resource efficient architectures for 3D CNNs. In this paper, we have
converted various well-known resource efficient 2D CNNs to 3D CNNs and
evaluated their performance on three major benchmarks in terms of
classification accuracy for different complexity levels. We have experimented
on (1) Kinetics-600 dataset to inspect their capacity to learn, (2) Jester
dataset to inspect their ability to capture motion patterns, and (3) UCF-101 to
inspect the applicability of transfer learning. We have evaluated the run-time
performance of each model on a single Titan XP GPU and a Jetson TX2 embedded
system. The results of this study show that these models can be utilized for
different types of real-world applications since they provide real-time
performance with considerable accuracies and memory usage. Our analysis on
different complexity levels shows that the resource efficient 3D CNNs should
not be designed too shallow or narrow in order to save complexity. The codes
and pretrained models used in this work are publicly available.",arxiv
http://arxiv.org/abs/1705.01123v1,2017-05-02T18:17:50Z,2017-05-02T18:17:50Z,"Towards Predictions of the Image Quality of Experience for Augmented
  Reality Scenarios","Augmented Reality (AR) devices are commonly head-worn to overlay
context-dependent information into the field of view of the device operators.
One particular scenario is the overlay of still images, either in a traditional
fashion, or as spherical, i.e., immersive, content. For both media types, we
evaluate the interplay of user ratings as Quality of Experience (QoE) with (i)
the non-referential BRISQUE objective image quality metric and (ii) human
subject dry electrode EEG signals gathered with a commercial device.
Additionally, we employ basic machine learning approaches to assess the
possibility of QoE predictions based on rudimentary subject data. Corroborating
prior research for the overall scenario, we find strong correlations for both
approaches with user ratings as Mean Opinion Scores, which we consider as QoE
metric. In prediction scenarios based on data subsets, we find good performance
for the objective metric as well as the EEG-based approach. While the objective
metric can yield high QoE prediction accuracies overall, it is limited i its
application for individual subjects. The subject-based EEG approach, on the
other hand, enables good predictability of the QoE for both media types, but
with better performance for regular content. Our results can be employed in
practical scenarios by content and network service providers to optimize the
user experience in augmented reality scenarios.",arxiv
http://arxiv.org/abs/2009.05429v2,2021-01-06T18:29:42Z,2020-09-11T13:28:26Z,"Embodied Visual Navigation with Automatic Curriculum Learning in Real
  Environments","We present NavACL, a method of automatic curriculum learning tailored to the
navigation task. NavACL is simple to train and efficiently selects relevant
tasks using geometric features. In our experiments, deep reinforcement learning
agents trained using NavACL significantly outperform state-of-the-art agents
trained with uniform sampling -- the current standard. Furthermore, our agents
can navigate through unknown cluttered indoor environments to
semantically-specified targets using only RGB images. Obstacle-avoiding
policies and frozen feature networks support transfer to unseen real-world
environments, without any modification or retraining requirements. We evaluate
our policies in simulation, and in the real world on a ground robot and a
quadrotor drone. Videos of real-world results are available in the
supplementary material.",arxiv
http://arxiv.org/abs/2101.01652v1,2021-01-05T17:06:03Z,2021-01-05T17:06:03Z,"Interpersonal distance in VR: reactions of older adults to the presence
  of a virtual agent","The rapid development of virtual reality technology has increased its
availability and, consequently, increased the number of its possible
applications. The interest in the new medium has grown due to the entertainment
industry (games, VR experiences and movies). The number of freely available
training and therapeutic applications is also increasing. Contrary to popular
opinion, new technologies are also adopted by older adults. Creating virtual
environments tailored to the needs and capabilities of older adults requires
intense research on the behaviour of these participants in the most common
situations, towards commonly used elements of the virtual environment, in
typical sceneries. Comfortable immersion in a virtual environment is key to
achieving the impression of presence. Presence is, in turn, necessary to obtain
appropriate training, persuasive and therapeutic effects. A virtual agent (a
humanoid representation of an algorithm or artificial intelligence) is often an
element of the virtual environment interface. Maintaining an appropriate
distance to the agent is, therefore, a key parameter for the creator of the VR
experience. Older (65+) participants maintain greater distance towards an agent
(a young white male) than younger ones (25-35). It may be caused by differences
in the level of arousal, but also cultural norms. As a consequence, VR
developers are advised to use algorithms that maintain the agent at the
appropriate distance, depending on the user's age.",arxiv
http://arxiv.org/abs/1810.12541v1,2018-10-30T06:19:58Z,2018-10-30T06:19:58Z,"Robots Learn Social Skills: End-to-End Learning of Co-Speech Gesture
  Generation for Humanoid Robots","Co-speech gestures enhance interaction experiences between humans as well as
between humans and robots. Existing robots use rule-based speech-gesture
association, but this requires human labor and prior knowledge of experts to be
implemented. We present a learning-based co-speech gesture generation that is
learned from 52 h of TED talks. The proposed end-to-end neural network model
consists of an encoder for speech text understanding and a decoder to generate
a sequence of gestures. The model successfully produces various gestures
including iconic, metaphoric, deictic, and beat gestures. In a subjective
evaluation, participants reported that the gestures were human-like and matched
the speech content. We also demonstrate a co-speech gesture with a NAO robot
working in real time.",arxiv
http://arxiv.org/abs/1607.00325v2,2017-01-03T19:57:37Z,2016-07-01T17:34:16Z,"Permutation Invariant Training of Deep Models for Speaker-Independent
  Multi-talker Speech Separation","We propose a novel deep learning model, which supports permutation invariant
training (PIT), for speaker independent multi-talker speech separation,
commonly known as the cocktail-party problem. Different from most of the prior
arts that treat speech separation as a multi-class regression problem and the
deep clustering technique that considers it a segmentation (or clustering)
problem, our model optimizes for the separation regression error, ignoring the
order of mixing sources. This strategy cleverly solves the long-lasting label
permutation problem that has prevented progress on deep learning based
techniques for speech separation. Experiments on the equal-energy mixing setup
of a Danish corpus confirms the effectiveness of PIT. We believe improvements
built upon PIT can eventually solve the cocktail-party problem and enable
real-world adoption of, e.g., automatic meeting transcription and multi-party
human-computer interaction, where overlapping speech is common.",arxiv
http://arxiv.org/abs/2103.12516v1,2021-01-14T13:34:34Z,2021-01-14T13:34:34Z,"Edge-Cloud Collaboration Enabled Video Service Enhancement: A Hybrid
  Human-Artificial Intelligence Scheme","In this paper, a video service enhancement strategy is investigated under an
edge-cloud collaboration framework, where video caching and delivery decisions
are made in the cloud and edge respectively. We aim to guarantee the user
fairness in terms of video coding rate under statistical delay constraint and
edge caching capacity constraint. A hybrid human-artificial intelligence
approach is developed to improve the user hit rate for video caching.
Specifically, individual user interest is first characterized by merging
factorization machine (FM) model and multi-layer perceptron (MLP) model, where
both low-order and high-order features can be well learned simultaneously.
Thereafter, a social aware similarity model is constructed to transferred
individual user interest to group interest, based on which, videos can be
selected to cache. Furthermore, a double bisection exploration scheme is
proposed to optimize wireless resource allocation and video coding rate. The
effectiveness of the proposed video caching scheme and video delivery scheme is
finally validated by extensive experiments with a real-world data set.",arxiv
http://arxiv.org/abs/2104.02017v1,2021-04-05T17:12:51Z,2021-04-05T17:12:51Z,Self-Supervised Learning for Personalized Speech Enhancement,"Speech enhancement systems can show improved performance by adapting the
model towards a single test-time speaker. In this personalization context, the
test-time user might only provide a small amount of noise-free speech data,
likely insufficient for traditional fully-supervised learning. One way to
overcome the lack of personal data is to transfer the model parameters from a
speaker-agnostic model to initialize the personalized model, and then to
finetune the model using the small amount of personal speech data. This
baseline marginally adapts over the scarce clean speech data. Alternatively, we
propose self-supervised methods that are designed specifically to learn
personalized and discriminative features from abundant in-the-wild noisy, but
still personal speech recordings. Our experiment shows that the proposed
self-supervised learning methods initialize personalized speech enhancement
models better than the baseline fully-supervised methods, yielding superior
speech enhancement performance. The proposed methods also result in a more
robust feature set under the real-world conditions: compressed model sizes and
fewness of the labeled data.",arxiv
http://arxiv.org/abs/1706.05781v1,2017-06-19T04:42:14Z,2017-06-19T04:42:14Z,"Kapre: On-GPU Audio Preprocessing Layers for a Quick Implementation of
  Deep Neural Network Models with Keras","We introduce Kapre, Keras layers for audio and music signal preprocessing.
Music research using deep neural networks requires a heavy and tedious
preprocessing stage, for which audio processing parameters are often ignored in
parameter optimisation. To solve this problem, Kapre implements time-frequency
conversions, normalisation, and data augmentation as Keras layers. We report
simple benchmark results, showing real-time on-GPU preprocessing adds a
reasonable amount of computation.",arxiv
http://arxiv.org/abs/1811.12139v1,2018-11-29T13:47:01Z,2018-11-29T13:47:01Z,"Two-level Attention with Two-stage Multi-task Learning for Facial
  Emotion Recognition","Compared with facial emotion recognition on categorical model, the
dimensional emotion recognition can describe numerous emotions of the real
world more accurately. Most prior works of dimensional emotion estimation only
considered laboratory data and used video, speech or other multi-modal
features. The effect of these methods applied on static images in the real
world is unknown. In this paper, a two-level attention with two-stage
multi-task learning (2Att-2Mt) framework is proposed for facial emotion
estimation on only static images. Firstly, the features of corresponding
region(position-level features) are extracted and enhanced automatically by
first-level attention mechanism. In the following, we utilize Bi-directional
Recurrent Neural Network(Bi-RNN) with self-attention(second-level attention) to
make full use of the relationship features of different layers(layer-level
features) adaptively. Owing to the inherent complexity of dimensional emotion
recognition, we propose a two-stage multi-task learning structure to exploited
categorical representations to ameliorate the dimensional representations and
estimate valence and arousal simultaneously in view of the correlation of the
two targets. The quantitative results conducted on AffectNet dataset show
significant advancement on Concordance Correlation Coefficient(CCC) and Root
Mean Square Error(RMSE), illustrating the superiority of the proposed
framework. Besides, extensive comparative experiments have also fully
demonstrated the effectiveness of different components.",arxiv
http://arxiv.org/abs/2008.03546v1,2020-08-08T15:48:32Z,2020-08-08T15:48:32Z,Online Multi-modal Person Search in Videos,"The task of searching certain people in videos has seen increasing potential
in real-world applications, such as video organization and editing. Most
existing approaches are devised to work in an offline manner, where identities
can only be inferred after an entire video is examined. This working manner
precludes such methods from being applied to online services or those
applications that require real-time responses. In this paper, we propose an
online person search framework, which can recognize people in a video on the
fly. This framework maintains a multimodal memory bank at its heart as the
basis for person recognition, and updates it dynamically with a policy obtained
by reinforcement learning. Our experiments on a large movie dataset show that
the proposed method is effective, not only achieving remarkable improvements
over online schemes but also outperforming offline methods.",arxiv
http://arxiv.org/abs/2005.03876v1,2020-05-08T06:53:05Z,2020-05-08T06:53:05Z,OpenEDS2020: Open Eyes Dataset,"We present the second edition of OpenEDS dataset, OpenEDS2020, a novel
dataset of eye-image sequences captured at a frame rate of 100 Hz under
controlled illumination, using a virtual-reality head-mounted display mounted
with two synchronized eye-facing cameras. The dataset, which is anonymized to
remove any personally identifiable information on participants, consists of 80
participants of varied appearance performing several gaze-elicited tasks, and
is divided in two subsets: 1) Gaze Prediction Dataset, with up to 66,560
sequences containing 550,400 eye-images and respective gaze vectors, created to
foster research in spatio-temporal gaze estimation and prediction approaches;
and 2) Eye Segmentation Dataset, consisting of 200 sequences sampled at 5 Hz,
with up to 29,500 images, of which 5% contain a semantic segmentation label,
devised to encourage the use of temporal information to propagate labels to
contiguous frames. Baseline experiments have been evaluated on OpenEDS2020, one
for each task, with average angular error of 5.37 degrees when performing gaze
prediction on 1 to 5 frames into the future, and a mean intersection over union
score of 84.1% for semantic segmentation. As its predecessor, OpenEDS dataset,
we anticipate that this new dataset will continue creating opportunities to
researchers in eye tracking, machine learning and computer vision communities,
to advance the state of the art for virtual reality applications. The dataset
is available for download upon request at
http://research.fb.com/programs/openeds-2020-challenge/.",arxiv
http://arxiv.org/abs/2004.00006v4,2020-07-17T20:13:40Z,2020-03-30T19:13:26Z,PointAR: Efficient Lighting Estimation for Mobile Augmented Reality,"We propose an efficient lighting estimation pipeline that is suitable to run
on modern mobile devices, with comparable resource complexities to
state-of-the-art mobile deep learning models. Our pipeline, PointAR, takes a
single RGB-D image captured from the mobile camera and a 2D location in that
image, and estimates 2nd order spherical harmonics coefficients. This estimated
spherical harmonics coefficients can be directly utilized by rendering engines
for supporting spatially variant indoor lighting, in the context of augmented
reality. Our key insight is to formulate the lighting estimation as a point
cloud-based learning problem directly from point clouds, which is in part
inspired by the Monte Carlo integration leveraged by real-time spherical
harmonics lighting. While existing approaches estimate lighting information
with complex deep learning pipelines, our method focuses on reducing the
computational complexity. Through both quantitative and qualitative
experiments, we demonstrate that PointAR achieves lower lighting estimation
errors compared to state-of-the-art methods. Further, our method requires an
order of magnitude lower resource, comparable to that of mobile-specific DNNs.",arxiv
http://arxiv.org/abs/1809.03451v1,2018-09-10T16:49:36Z,2018-09-10T16:49:36Z,Deep Single-View 3D Object Reconstruction with Visual Hull Embedding,"3D object reconstruction is a fundamental task of many robotics and AI
problems. With the aid of deep convolutional neural networks (CNNs), 3D object
reconstruction has witnessed a significant progress in recent years. However,
possibly due to the prohibitively high dimension of the 3D object space, the
results from deep CNNs are often prone to missing some shape details. In this
paper, we present an approach which aims to preserve more shape details and
improve the reconstruction quality. The key idea of our method is to leverage
object mask and pose estimation from CNNs to assist the 3D shape learning by
constructing a probabilistic single-view visual hull inside of the network. Our
method works by first predicting a coarse shape as well as the object pose and
silhouette using CNNs, followed by a novel 3D refinement CNN which refines the
coarse shapes using the constructed probabilistic visual hulls. Experiment on
both synthetic data and real images show that embedding a single-view visual
hull for shape refinement can significantly improve the reconstruction quality
by recovering more shapes details and improving shape consistency with the
input image.",arxiv
http://arxiv.org/abs/1912.10573v1,2019-12-23T01:11:33Z,2019-12-23T01:11:33Z,"Overcoming the Channel Estimation Barrier in Massive MIMO Communication
  Systems","A new wave of wireless services, including virtual reality, autonomous
driving and internet of things, is driving the design of new generations of
wireless systems to deliver ultra-high data rates, massive number of connected
devices and ultra low latency. Massive multiple-input multiple-output (MIMO) is
one of the critical underlying technologies that allow future wireless networks
to meet these service needs. This article discusses the application of deep
learning (DL) for massive MIMO channel estimation in wireless networks by
integrating the underlying characteristics of channels in future high-speed
cellular deployment. We develop important insights derived from the physical
radio frequency (RF) channel properties and present a comprehensive overview on
the application of DL for accurately estimating channel state information (CSI)
with low overhead. We provide examples of successful DL application in CSI
estimation for massive MIMO wireless systems and highlight several promising
directions for future research.",arxiv
http://arxiv.org/abs/1908.06619v1,2019-08-19T07:21:37Z,2019-08-19T07:21:37Z,"Demonstration of 3D ISAR Security Imaging at 24GHz with a Sparse MIMO
  Array","A 3D ISAR security imaging experiment at 24GHz is demonstrated with a sparse
MIMO array. The MIMO array is an 8Tx/16Rx linear array to achieve real-aperture
imaging along the vertical dimension. It is time-switching multiplexed with a
low-cost FMCW transceiver working at 22GHz-26GHz. A calibration procedure is
proposed to calibrate the channel imbalance across the MIMO array. The
experiment is conducted on human moving on a cart, where we take advantage of
the linear motion of human to form inverse synthetic aperture along the
horizontal dimension. To track the motion of human, a 3D depth camera is used
as an auxiliary sensor to capture the rough position of target to aid ISAR
imaging. The back projection imaging algorithm is implemented on GPU for
quasi-real-time operation. Finally, experiments are conducted with real human
with concealed objects and a preliminary automatic object recognition algorithm
based on convolutional neural networks are developed and evaluated on real
data.",arxiv
http://arxiv.org/abs/1912.01177v1,2019-12-03T03:23:25Z,2019-12-03T03:23:25Z,"A New Terrain in HCI: Emotion Recognition Interface using Biometric Data
  for an Immersive VR Experience","Emotion recognition technology is crucial in providing a personalized user
experience. It is especially important in virtual reality(VR) to assess the
user's emotions to enhance their sense of immersion. We propose an emotion
recognition interface that incorporates the user's biometric data with machine
learning technology for increasing user engagement in VR. Our key technologies
include brainwave sensors and eye-tracking cameras embedded in a VR headset,
which seamlessly acquire physiological signals, and secondly, an attractiveness
recognition algorithm that uses bio-signals to predict the user's attraction on
visual stimuli. We conducted experiments to test the performance of the system,
and also interviewed experts and participants to acquire opinions on the
system. This study demonstrated the technical feasibility of our system with
high accuracy and usability. Interviewees expected that the interface will be
actively used in the context of various applications. Our proposed interface
could contribute to an immersive VR experience design.",arxiv
http://arxiv.org/abs/2005.00825v1,2020-05-02T13:02:54Z,2020-05-02T13:02:54Z,"SIGVerse: A cloud-based VR platform for research on social and embodied
  human-robot interaction","Common sense and social interaction related to daily-life environments are
considerably important for autonomous robots, which support human activities.
One of the practical approaches for acquiring such social interaction skills
and semantic information as common sense in human activity is the application
of recent machine learning techniques. Although recent machine learning
techniques have been successful in realizing automatic manipulation and driving
tasks, it is difficult to use these techniques in applications that require
human-robot interaction experience. Humans have to perform several times over a
long term to show embodied and social interaction behaviors to robots or
learning systems. To address this problem, we propose a cloud-based immersive
virtual reality (VR) platform which enables virtual human-robot interaction to
collect the social and embodied knowledge of human activities in a variety of
situations. To realize the flexible and reusable system, we develop a real-time
bridging mechanism between ROS and Unity, which is one of the standard
platforms for developing VR applications. We apply the proposed system to a
robot competition field named RoboCup@Home to confirm the feasibility of the
system in a realistic human-robot interaction scenario. Through demonstration
experiments at the competition, we show the usefulness and potential of the
system for the development and evaluation of social intelligence through
human-robot interaction. The proposed VR platform enables robot systems to
collect social experiences with several users in a short time. The platform
also contributes in providing a dataset of social behaviors, which would be a
key aspect for intelligent service robots to acquire social interaction skills
based on machine learning techniques.",arxiv
http://arxiv.org/abs/1710.08637v1,2017-10-24T07:46:57Z,2017-10-24T07:46:57Z,"Improving Accuracy of Nonparametric Transfer Learning via Vector
  Segmentation","Transfer learning using deep neural networks as feature extractors has become
increasingly popular over the past few years. It allows to obtain
state-of-the-art accuracy on datasets too small to train a deep neural network
on its own, and it provides cutting edge descriptors that, combined with
nonparametric learning methods, allow rapid and flexible deployment of
performing solutions in computationally restricted settings. In this paper, we
are interested in showing that the features extracted using deep neural
networks have specific properties which can be used to improve accuracy of
downstream nonparametric learning methods. Namely, we demonstrate that for some
distributions where information is embedded in a few coordinates, segmenting
feature vectors can lead to better accuracy. We show how this model can be
applied to real datasets by performing experiments using three mainstream deep
neural network feature extractors and four databases, in vision and audio.",arxiv
http://arxiv.org/abs/2108.13062v1,2021-08-30T08:45:02Z,2021-08-30T08:45:02Z,Unsupervised Monocular Depth Perception: Focusing on Moving Objects,"As a flexible passive 3D sensing means, unsupervised learning of depth from
monocular videos is becoming an important research topic. It utilizes the
photometric errors between the target view and the synthesized views from its
adjacent source views as the loss instead of the difference from the ground
truth. Occlusion and scene dynamics in real-world scenes still adversely affect
the learning, despite significant progress made recently. In this paper, we
show that deliberately manipulating photometric errors can efficiently deal
with these difficulties better. We first propose an outlier masking technique
that considers the occluded or dynamic pixels as statistical outliers in the
photometric error map. With the outlier masking, the network learns the depth
of objects that move in the opposite direction to the camera more accurately.
To the best of our knowledge, such cases have not been seriously considered in
the previous works, even though they pose a high risk in applications like
autonomous driving. We also propose an efficient weighted multi-scale scheme to
reduce the artifacts in the predicted depth maps. Extensive experiments on the
KITTI dataset and additional experiments on the Cityscapes dataset have
verified the proposed approach's effectiveness on depth or ego-motion
estimation. Furthermore, for the first time, we evaluate the predicted depth on
the regions of dynamic objects and static background separately for both
supervised and unsupervised methods. The evaluation further verifies the
effectiveness of our proposed technical approach and provides some interesting
observations that might inspire future research in this direction.",arxiv
http://arxiv.org/abs/2108.03272v4,2021-11-03T18:51:07Z,2021-08-06T18:41:39Z,"iGibson 2.0: Object-Centric Simulation for Robot Learning of Everyday
  Household Tasks","Recent research in embodied AI has been boosted by the use of simulation
environments to develop and train robot learning approaches. However, the use
of simulation has skewed the attention to tasks that only require what robotics
simulators can simulate: motion and physical contact. We present iGibson 2.0,
an open-source simulation environment that supports the simulation of a more
diverse set of household tasks through three key innovations. First, iGibson
2.0 supports object states, including temperature, wetness level, cleanliness
level, and toggled and sliced states, necessary to cover a wider range of
tasks. Second, iGibson 2.0 implements a set of predicate logic functions that
map the simulator states to logic states like Cooked or Soaked. Additionally,
given a logic state, iGibson 2.0 can sample valid physical states that satisfy
it. This functionality can generate potentially infinite instances of tasks
with minimal effort from the users. The sampling mechanism allows our scenes to
be more densely populated with small objects in semantically meaningful
locations. Third, iGibson 2.0 includes a virtual reality (VR) interface to
immerse humans in its scenes to collect demonstrations. As a result, we can
collect demonstrations from humans on these new types of tasks, and use them
for imitation learning. We evaluate the new capabilities of iGibson 2.0 to
enable robot learning of novel tasks, in the hope of demonstrating the
potential of this new simulator to support new research in embodied AI. iGibson
2.0 and its new dataset are publicly available at
http://svl.stanford.edu/igibson/.",arxiv
http://arxiv.org/abs/1802.09971v1,2018-02-27T15:42:34Z,2018-02-27T15:42:34Z,"Real-World Repetition Estimation by Div, Grad and Curl","We consider the problem of estimating repetition in video, such as performing
push-ups, cutting a melon or playing violin. Existing work shows good results
under the assumption of static and stationary periodicity. As realistic video
is rarely perfectly static and stationary, the often preferred Fourier-based
measurements is inapt. Instead, we adopt the wavelet transform to better handle
non-static and non-stationary video dynamics. From the flow field and its
differentials, we derive three fundamental motion types and three motion
continuities of intrinsic periodicity in 3D. On top of this, the 2D perception
of 3D periodicity considers two extreme viewpoints. What follows are 18
fundamental cases of recurrent perception in 2D. In practice, to deal with the
variety of repetitive appearance, our theory implies measuring time-varying
flow and its differentials (gradient, divergence and curl) over segmented
foreground motion. For experiments, we introduce the new QUVA Repetition
dataset, reflecting reality by including non-static and non-stationary videos.
On the task of counting repetitions in video, we obtain favorable results
compared to a deep learning alternative.",arxiv
http://arxiv.org/abs/1805.00322v1,2018-04-20T23:56:10Z,2018-04-20T23:56:10Z,"Occluded object reconstruction for first responders with augmented
  reality glasses using conditional generative adversarial networks","Firefighters suffer a variety of life-threatening risks, including
line-of-duty deaths, injuries, and exposures to hazardous substances. Support
for reducing these risks is important. We built a partially occluded object
reconstruction method on augmented reality glasses for first responders. We
used a deep learning based on conditional generative adversarial networks to
train associations between the various images of flammable and hazardous
objects and their partially occluded counterparts. Our system then
reconstructed an image of a new flammable object. Finally, the reconstructed
image was superimposed on the input image to provide ""transparency"". The system
imitates human learning about the laws of physics through experience by
learning the shape of flammable objects and the flame characteristics.",arxiv
http://arxiv.org/abs/1803.06077v2,2018-08-31T09:16:33Z,2018-03-16T05:29:12Z,"Real-time Detection, Tracking, and Classification of Moving and
  Stationary Objects using Multiple Fisheye Images","The ability to detect pedestrians and other moving objects is crucial for an
autonomous vehicle. This must be done in real-time with minimum system
overhead. This paper discusses the implementation of a surround view system to
identify moving as well as static objects that are close to the ego vehicle.
The algorithm works on 4 views captured by fisheye cameras which are merged
into a single frame. The moving object detection and tracking solution uses
minimal system overhead to isolate regions of interest (ROIs) containing moving
objects. These ROIs are then analyzed using a deep neural network (DNN) to
categorize the moving object. With deployment and testing on a real car in
urban environments, we have demonstrated the practical feasibility of the
solution. The video demos of our algorithm have been uploaded to Youtube:
https://youtu.be/vpoCfC724iA, https://youtu.be/2X4aqH2bMBs",arxiv
http://arxiv.org/abs/1910.09430v2,2020-02-06T16:28:34Z,2019-10-21T15:06:03Z,Adversarial Skill Networks: Unsupervised Robot Skill Learning from Video,"Key challenges for the deployment of reinforcement learning (RL) agents in
the real world are the discovery, representation and reuse of skills in the
absence of a reward function. To this end, we propose a novel approach to learn
a task-agnostic skill embedding space from unlabeled multi-view videos. Our
method learns a general skill embedding independently from the task context by
using an adversarial loss. We combine a metric learning loss, which utilizes
temporal video coherence to learn a state representation, with an entropy
regularized adversarial skill-transfer loss. The metric learning loss learns a
disentangled representation by attracting simultaneous viewpoints of the same
observations and repelling visually similar frames from temporal neighbors. The
adversarial skill-transfer loss enhances re-usability of learned skill
embeddings over multiple task domains. We show that the learned embedding
enables training of continuous control policies to solve novel tasks that
require the interpolation of previously seen skills. Our extensive evaluation
with both simulation and real world data demonstrates the effectiveness of our
method in learning transferable skills from unlabeled interaction videos and
composing them for new tasks. Code, pretrained models and dataset are available
at http://robotskills.cs.uni-freiburg.de",arxiv
http://arxiv.org/abs/2107.10562v1,2021-07-22T10:30:39Z,2021-07-22T10:30:39Z,"Controlling the Perceived Sound Quality for Dialogue Enhancement with
  Deep Learning","Speech enhancement attenuates interfering sounds in speech signals but may
introduce artifacts that perceivably deteriorate the output signal. We propose
a method for controlling the trade-off between the attenuation of the
interfering background signal and the loss of sound quality. A deep neural
network estimates the attenuation of the separated background signal such that
the sound quality, quantified using the Artifact-related Perceptual Score,
meets an adjustable target. Subjective evaluations indicate that consistent
sound quality is obtained across various input signals. Our experiments show
that the proposed method is able to control the trade-off with an accuracy that
is adequate for real-world dialogue enhancement applications.",arxiv
http://arxiv.org/abs/2110.06986v1,2021-10-13T18:56:59Z,2021-10-13T18:56:59Z,ADMM-DAD net: a deep unfolding network for analysis compressed sensing,"In this paper, we propose a new deep unfolding neural network based on the
ADMM algorithm for analysis Compressed Sensing. The proposed network jointly
learns a redundant analysis operator for sparsification and reconstructs the
signal of interest. We compare our proposed network with a state-of-the-art
unfolded ISTA decoder, that also learns an orthogonal sparsifier. Moreover, we
consider not only image, but also speech datasets as test examples.
Computational experiments demonstrate that our proposed network outperforms the
state-of-the-art deep unfolding networks, consistently for both real-world
image and speech datasets.",arxiv
http://arxiv.org/abs/2005.10161v1,2020-05-20T16:09:57Z,2020-05-20T16:09:57Z,User Attention and Behaviour in Virtual Reality Art Encounter,"With the proliferation of consumer virtual reality (VR) headsets and creative
tools, content creators have started to experiment with new forms of
interactive audience experience using immersive media. Understanding user
attention and behaviours in virtual environment can greatly inform creative
processes in VR. We developed an abstract VR painting and an experimentation
system to study audience encounters through eye gaze and movement tracking. The
data from a user experiment with 35 participants reveal a range of user
activity patterns in art exploration. Deep learning models are used to study
the connections between behavioural data and audience background. New
integrated methods to visualise user attention as part of the artwork are also
developed as a feedback loop to the content creator.",arxiv
http://arxiv.org/abs/2104.10505v1,2021-04-21T12:51:12Z,2021-04-21T12:51:12Z,Interpretation of multi-label classification models using shapley values,"Multi-label classification is a type of classification task, it is used when
there are two or more classes, and the data point we want to predict may belong
to none of the classes or all of them at the same time. In the real world, many
applications are actually multi-label involved, including information
retrieval, multimedia content annotation, web mining, and so on. A game
theory-based framework known as SHapley Additive exPlanations (SHAP) has been
applied to explain various supervised learning models without being aware of
the exact model. Herein, this work further extends the explanation of
multi-label classification task by using the SHAP methodology. The experiment
demonstrates a comprehensive comparision of different algorithms on well known
multi-label datasets and shows the usefulness of the interpretation.",arxiv
http://arxiv.org/abs/1912.10557v3,2020-08-07T05:37:40Z,2019-12-22T23:02:18Z,"Algorithm Unrolling: Interpretable, Efficient Deep Learning for Signal
  and Image Processing","Deep neural networks provide unprecedented performance gains in many real
world problems in signal and image processing. Despite these gains, future
development and practical deployment of deep networks is hindered by their
blackbox nature, i.e., lack of interpretability, and by the need for very large
training sets. An emerging technique called algorithm unrolling or unfolding
offers promise in eliminating these issues by providing a concrete and
systematic connection between iterative algorithms that are used widely in
signal processing and deep neural networks. Unrolling methods were first
proposed to develop fast neural network approximations for sparse coding. More
recently, this direction has attracted enormous attention and is rapidly
growing both in theoretic investigations and practical applications. The
growing popularity of unrolled deep networks is due in part to their potential
in developing efficient, high-performance and yet interpretable network
architectures from reasonable size training sets. In this article, we review
algorithm unrolling for signal and image processing. We extensively cover
popular techniques for algorithm unrolling in various domains of signal and
image processing including imaging, vision and recognition, and speech
processing. By reviewing previous works, we reveal the connections between
iterative algorithms and neural networks and present recent theoretical
results. Finally, we provide a discussion on current limitations of unrolling
and suggest possible future research directions.",arxiv
http://arxiv.org/abs/1709.01683v1,2017-09-06T06:16:52Z,2017-09-06T06:16:52Z,Affect Recognition in Ads with Application to Computational Advertising,"Advertisements (ads) often include strongly emotional content to leave a
lasting impression on the viewer. This work (i) compiles an affective ad
dataset capable of evoking coherent emotions across users, as determined from
the affective opinions of five experts and 14 annotators; (ii) explores the
efficacy of convolutional neural network (CNN) features for encoding emotions,
and observes that CNN features outperform low-level audio-visual emotion
descriptors upon extensive experimentation; and (iii) demonstrates how enhanced
affect prediction facilitates computational advertising, and leads to better
viewing experience while watching an online video stream embedded with ads
based on a study involving 17 users. We model ad emotions based on subjective
human opinions as well as objective multimodal features, and show how
effectively modeling ad emotions can positively impact a real-life application.",arxiv
http://arxiv.org/abs/1910.11306v1,2019-10-24T17:36:40Z,2019-10-24T17:36:40Z,Controllable Attention for Structured Layered Video Decomposition,"The objective of this paper is to be able to separate a video into its
natural layers, and to control which of the separated layers to attend to. For
example, to be able to separate reflections, transparency or object motion. We
make the following three contributions: (i) we introduce a new structured
neural network architecture that explicitly incorporates layers (as spatial
masks) into its design. This improves separation performance over previous
general purpose networks for this task; (ii) we demonstrate that we can augment
the architecture to leverage external cues such as audio for controllability
and to help disambiguation; and (iii) we experimentally demonstrate the
effectiveness of our approach and training procedure with controlled
experiments while also showing that the proposed model can be successfully
applied to real-word applications such as reflection removal and action
recognition in cluttered scenes.",arxiv
http://arxiv.org/abs/2005.05441v2,2020-08-29T01:27:43Z,2020-05-11T21:21:50Z,"Delay-Aware Multi-Agent Reinforcement Learning for Cooperative and
  Competitive Environments","Action and observation delays exist prevalently in the real-world
cyber-physical systems which may pose challenges in reinforcement learning
design. It is particularly an arduous task when handling multi-agent systems
where the delay of one agent could spread to other agents. To resolve this
problem, this paper proposes a novel framework to deal with delays as well as
the non-stationary training issue of multi-agent tasks with model-free deep
reinforcement learning. We formally define the Delay-Aware Markov Game that
incorporates the delays of all agents in the environment. To solve Delay-Aware
Markov Games, we apply centralized training and decentralized execution that
allows agents to use extra information to ease the non-stationarity issue of
the multi-agent systems during training, without the need of a centralized
controller during execution. Experiments are conducted in multi-agent particle
environments including cooperative communication, cooperative navigation, and
competitive experiments. We also test the proposed algorithm in traffic
scenarios that require coordination of all autonomous vehicles to show the
practical value of delay-awareness. Results show that the proposed delay-aware
multi-agent reinforcement learning algorithm greatly alleviates the performance
degradation introduced by delay. Codes and demo videos are available at:
https://github.com/baimingc/delay-aware-MARL.",arxiv
http://arxiv.org/abs/1707.08301v1,2017-07-26T06:39:45Z,2017-07-26T06:39:45Z,Graph-Based Classification of Omnidirectional Images,"Omnidirectional cameras are widely used in such areas as robotics and virtual
reality as they provide a wide field of view. Their images are often processed
with classical methods, which might unfortunately lead to non-optimal solutions
as these methods are designed for planar images that have different geometrical
properties than omnidirectional ones. In this paper we study image
classification task by taking into account the specific geometry of
omnidirectional cameras with graph-based representations. In particular, we
extend deep learning architectures to data on graphs; we propose a principled
way of graph construction such that convolutional filters respond similarly for
the same pattern on different positions of the image regardless of lens
distortions. Our experiments show that the proposed method outperforms current
techniques for the omnidirectional image classification problem.",arxiv
http://arxiv.org/abs/2010.16085v1,2020-10-30T05:31:50Z,2020-10-30T05:31:50Z,Correspondence Matrices are Underrated,"Point-cloud registration (PCR) is an important task in various applications
such as robotic manipulation, augmented and virtual reality, SLAM, etc. PCR is
an optimization problem involving minimization over two different types of
interdependent variables: transformation parameters and point-to-point
correspondences. Recent developments in deep-learning have produced
computationally fast approaches for PCR. The loss functions that are optimized
in these networks are based on the error in the transformation parameters. We
hypothesize that these methods would perform significantly better if they
calculated their loss function using correspondence error instead of only using
error in transformation parameters. We define correspondence error as a metric
based on incorrectly matched point pairs. We provide a fundamental explanation
for why this is the case and test our hypothesis by modifying existing methods
to use correspondence-based loss instead of transformation-based loss. These
experiments show that the modified networks converge faster and register more
accurately even at larger misalignment when compared to the original networks.",arxiv
http://arxiv.org/abs/1904.02223v2,2019-07-26T17:03:10Z,2019-04-03T20:07:53Z,"Learning Physics-Based Manipulation in Clutter: Combining Image-Based
  Generalization and Look-Ahead Planning","Physics-based manipulation in clutter involves complex interaction between
multiple objects. In this paper, we consider the problem of learning, from
interaction in a physics simulator, manipulation skills to solve this
multi-step sequential decision making problem in the real world. Our approach
has two key properties: (i) the ability to generalize and transfer manipulation
skills (over the type, shape, and number of objects in the scene) using an
abstract image-based representation that enables a neural network to learn
useful features; and (ii) the ability to perform look-ahead planning in the
image space using a physics simulator, which is essential for such multi-step
problems. We show, in sets of simulated and real-world experiments (video
available on https://youtu.be/EmkUQfyvwkY), that by learning to evaluate
actions in an abstract image-based representation of the real world, the robot
can generalize and adapt to the object shapes in challenging real-world
environments.",arxiv
http://arxiv.org/abs/2102.13493v1,2021-02-26T14:06:31Z,2021-02-26T14:06:31Z,"ACDnet: An action detection network for real-time edge computing based
  on flow-guided feature approximation and memory aggregation","Interpreting human actions requires understanding the spatial and temporal
context of the scenes. State-of-the-art action detectors based on Convolutional
Neural Network (CNN) have demonstrated remarkable results by adopting
two-stream or 3D CNN architectures. However, these methods typically operate in
a non-real-time, ofline fashion due to system complexity to reason
spatio-temporal information. Consequently, their high computational cost is not
compliant with emerging real-world scenarios such as service robots or public
surveillance where detection needs to take place at resource-limited edge
devices. In this paper, we propose ACDnet, a compact action detection network
targeting real-time edge computing which addresses both efficiency and
accuracy. It intelligently exploits the temporal coherence between successive
video frames to approximate their CNN features rather than naively extracting
them. It also integrates memory feature aggregation from past video frames to
enhance current detection stability, implicitly modeling long temporal cues
over time. Experiments conducted on the public benchmark datasets UCF-24 and
JHMDB-21 demonstrate that ACDnet, when integrated with the SSD detector, can
robustly achieve detection well above real-time (75 FPS). At the same time, it
retains reasonable accuracy (70.92 and 49.53 frame mAP) compared to other
top-performing methods using far heavier configurations. Codes will be
available at https://github.com/dginhac/ACDnet.",arxiv
http://arxiv.org/abs/2108.08305v1,2021-08-18T13:09:38Z,2021-08-18T13:09:38Z,Temporal Kernel Consistency for Blind Video Super-Resolution,"Deep learning-based blind super-resolution (SR) methods have recently
achieved unprecedented performance in upscaling frames with unknown
degradation. These models are able to accurately estimate the unknown
downscaling kernel from a given low-resolution (LR) image in order to leverage
the kernel during restoration. Although these approaches have largely been
successful, they are predominantly image-based and therefore do not exploit the
temporal properties of the kernels across multiple video frames. In this paper,
we investigated the temporal properties of the kernels and highlighted its
importance in the task of blind video super-resolution. Specifically, we
measured the kernel temporal consistency of real-world videos and illustrated
how the estimated kernels might change per frame in videos of varying
dynamicity of the scene and its objects. With this new insight, we revisited
previous popular video SR approaches, and showed that previous assumptions of
using a fixed kernel throughout the restoration process can lead to visual
artifacts when upscaling real-world videos. In order to counteract this, we
tailored existing single-image and video SR techniques to leverage kernel
consistency during both kernel estimation and video upscaling processes.
Extensive experiments on synthetic and real-world videos show substantial
restoration gains quantitatively and qualitatively, achieving the new
state-of-the-art in blind video SR and underlining the potential of exploiting
kernel temporal consistency.",arxiv
http://arxiv.org/abs/1605.09507v3,2016-12-26T12:29:26Z,2016-05-31T07:11:18Z,"Deep convolutional neural networks for predominant instrument
  recognition in polyphonic music","Identifying musical instruments in polyphonic music recordings is a
challenging but important problem in the field of music information retrieval.
It enables music search by instrument, helps recognize musical genres, or can
make music transcription easier and more accurate. In this paper, we present a
convolutional neural network framework for predominant instrument recognition
in real-world polyphonic music. We train our network from fixed-length music
excerpts with a single-labeled predominant instrument and estimate an arbitrary
number of predominant instruments from an audio signal with a variable length.
To obtain the audio-excerpt-wise result, we aggregate multiple outputs from
sliding windows over the test audio. In doing so, we investigated two different
aggregation methods: one takes the average for each instrument and the other
takes the instrument-wise sum followed by normalization. In addition, we
conducted extensive experiments on several important factors that affect the
performance, including analysis window size, identification threshold, and
activation functions for neural networks to find the optimal set of parameters.
Using a dataset of 10k audio excerpts from 11 instruments for evaluation, we
found that convolutional neural networks are more robust than conventional
methods that exploit spectral features and source separation with support
vector machines. Experimental results showed that the proposed convolutional
network architecture obtained an F1 measure of 0.602 for micro and 0.503 for
macro, respectively, achieving 19.6% and 16.4% in performance improvement
compared with other state-of-the-art algorithms.",arxiv
http://arxiv.org/abs/2103.04263v2,2021-03-11T01:08:38Z,2021-03-07T04:40:15Z,Deepfake Videos in the Wild: Analysis and Detection,"AI-manipulated videos, commonly known as deepfakes, are an emerging problem.
Recently, researchers in academia and industry have contributed several
(self-created) benchmark deepfake datasets, and deepfake detection algorithms.
However, little effort has gone towards understanding deepfake videos in the
wild, leading to a limited understanding of the real-world applicability of
research contributions in this space. Even if detection schemes are shown to
perform well on existing datasets, it is unclear how well the methods
generalize to real-world deepfakes. To bridge this gap in knowledge, we make
the following contributions: First, we collect and present the largest dataset
of deepfake videos in the wild, containing 1,869 videos from YouTube and
Bilibili, and extract over 4.8M frames of content. Second, we present a
comprehensive analysis of the growth patterns, popularity, creators,
manipulation strategies, and production methods of deepfake content in the
real-world. Third, we systematically evaluate existing defenses using our new
dataset, and observe that they are not ready for deployment in the real-world.
Fourth, we explore the potential for transfer learning schemes and
competition-winning techniques to improve defenses.",arxiv
http://arxiv.org/abs/2103.14734v1,2021-03-26T21:03:33Z,2021-03-26T21:03:33Z,"Fully Automated 2D and 3D Convolutional Neural Networks Pipeline for
  Video Segmentation and Myocardial Infarction Detection in Echocardiography","Cardiac imaging known as echocardiography is a non-invasive tool utilized to
produce data including images and videos, which cardiologists use to diagnose
cardiac abnormalities in general and myocardial infarction (MI) in particular.
Echocardiography machines can deliver abundant amounts of data that need to be
quickly analyzed by cardiologists to help them make a diagnosis and treat
cardiac conditions. However, the acquired data quality varies depending on the
acquisition conditions and the patient's responsiveness to the setup
instructions. These constraints are challenging to doctors especially when
patients are facing MI and their lives are at stake. In this paper, we propose
an innovative real-time end-to-end fully automated model based on convolutional
neural networks (CNN) to detect MI depending on regional wall motion
abnormalities (RWMA) of the left ventricle (LV) from videos produced by
echocardiography. Our model is implemented as a pipeline consisting of a 2D CNN
that performs data preprocessing by segmenting the LV chamber from the apical
four-chamber (A4C) view, followed by a 3D CNN that performs a binary
classification to detect if the segmented echocardiography shows signs of MI.
We trained both CNNs on a dataset composed of 165 echocardiography videos each
acquired from a distinct patient. The 2D CNN achieved an accuracy of 97.18% on
data segmentation while the 3D CNN achieved 90.9% of accuracy, 100% of
precision and 95% of recall on MI detection. Our results demonstrate that
creating a fully automated system for MI detection is feasible and propitious.",arxiv
http://arxiv.org/abs/1810.02648v3,2019-01-25T15:37:49Z,2018-10-05T12:39:37Z,LiveCap: Real-time Human Performance Capture from Monocular Video,"We present the first real-time human performance capture approach that
reconstructs dense, space-time coherent deforming geometry of entire humans in
general everyday clothing from just a single RGB video. We propose a novel
two-stage analysis-by-synthesis optimization whose formulation and
implementation are designed for high performance. In the first stage, a skinned
template model is jointly fitted to background subtracted input video, 2D and
3D skeleton joint positions found using a deep neural network, and a set of
sparse facial landmark detections. In the second stage, dense non-rigid 3D
deformations of skin and even loose apparel are captured based on a novel
real-time capable algorithm for non-rigid tracking using dense photometric and
silhouette constraints. Our novel energy formulation leverages automatically
identified material regions on the template to model the differing non-rigid
deformation behavior of skin and apparel. The two resulting non-linear
optimization problems per-frame are solved with specially-tailored
data-parallel Gauss-Newton solvers. In order to achieve real-time performance
of over 25Hz, we design a pipelined parallel architecture using the CPU and two
commodity GPUs. Our method is the first real-time monocular approach for
full-body performance capture. Our method yields comparable accuracy with
off-line performance capture techniques, while being orders of magnitude
faster.",arxiv
http://arxiv.org/abs/1807.11094v1,2018-07-29T18:22:38Z,2018-07-29T18:22:38Z,"Towards End-to-End Acoustic Localization using Deep Learning: from Audio
  Signal to Source Position Coordinates","This paper presents a novel approach for indoor acoustic source localization
using microphone arrays and based on a Convolutional Neural Network (CNN). The
proposed solution is, to the best of our knowledge, the first published work in
which the CNN is designed to directly estimate the three dimensional position
of an acoustic source, using the raw audio signal as the input information
avoiding the use of hand crafted audio features. Given the limited amount of
available localization data, we propose in this paper a training strategy based
on two steps. We first train our network using semi-synthetic data, generated
from close talk speech recordings, and where we simulate the time delays and
distortion suffered in the signal that propagates from the source to the array
of microphones. We then fine tune this network using a small amount of real
data. Our experimental results show that this strategy is able to produce
networks that significantly improve existing localization methods based on
\textit{SRP-PHAT} strategies. In addition, our experiments show that our CNN
method exhibits better resistance against varying gender of the speaker and
different window sizes compared with the other methods.",arxiv
http://arxiv.org/abs/2102.02995v1,2021-02-05T04:33:58Z,2021-02-05T04:33:58Z,"Application of Deep Learning in Recognizing Bates Numbers and
  Confidentiality Stamping from Images","In eDiscovery, it is critical to ensure that each page produced in legal
proceedings conforms with the requirements of court or government agency
production requests. Errors in productions could have severe consequences in a
case, putting a party in an adverse position. The volume of pages produced
continues to increase, and tremendous time and effort has been taken to ensure
quality control of document productions. This has historically been a manual
and laborious process. This paper demonstrates a novel automated production
quality control application which leverages deep learning-based image
recognition technology to extract Bates Number and Confidentiality Stamping
from legal case production images and validate their correctness. Effectiveness
of the method is verified with an experiment using a real-world production
data.",arxiv
http://arxiv.org/abs/1910.01990v1,2019-10-04T15:28:01Z,2019-10-04T15:28:01Z,"Detecting Deception in Political Debates Using Acoustic and Textual
  Features","We present work on deception detection, where, given a spoken claim, we aim
to predict its factuality. While previous work in the speech community has
relied on recordings from staged setups where people were asked to tell the
truth or to lie and their statements were recorded, here we use real-world
political debates. Thanks to the efforts of fact-checking organizations, it is
possible to obtain annotations for statements in the context of a political
discourse as true, half-true, or false. Starting with such data from the
CLEF-2018 CheckThat! Lab, which was limited to text, we performed alignment to
the corresponding videos, thus producing a multimodal dataset. We further
developed a multimodal deep-learning architecture for the task of deception
detection, which yielded sizable improvements over the state of the art for the
CLEF-2018 Lab task 2. Our experiments show that the use of the acoustic signal
consistently helped to improve the performance compared to using textual and
metadata features only, based on several different evaluation measures. We
release the new dataset to the research community, hoping to help advance the
overall field of multimodal deception detection.",arxiv
http://arxiv.org/abs/1209.6393v1,2012-09-27T23:03:53Z,2012-09-27T23:03:53Z,Learning Robust Low-Rank Representations,"In this paper we present a comprehensive framework for learning robust
low-rank representations by combining and extending recent ideas for learning
fast sparse coding regressors with structured non-convex optimization
techniques. This approach connects robust principal component analysis (RPCA)
with dictionary learning techniques and allows its approximation via trainable
encoders. We propose an efficient feed-forward architecture derived from an
optimization algorithm designed to exactly solve robust low dimensional
projections. This architecture, in combination with different training
objective functions, allows the regressors to be used as online approximants of
the exact offline RPCA problem or as RPCA-based neural networks. Simple
modifications of these encoders can handle challenging extensions, such as the
inclusion of geometric data transformations. We present several examples with
real data from image, audio, and video processing. When used to approximate
RPCA, our basic implementation shows several orders of magnitude speedup
compared to the exact solvers with almost no performance degradation. We show
the strength of the inclusion of learning to the RPCA approach on a music
source separation application, where the encoders outperform the exact RPCA
algorithms, which are already reported to produce state-of-the-art results on a
benchmark database. Our preliminary implementation on an iPad shows
faster-than-real-time performance with minimal latency.",arxiv
http://arxiv.org/abs/2005.01351v2,2020-05-14T06:57:58Z,2020-05-04T09:45:56Z,"Anchors Based Method for Fingertips Position Estimation from a Monocular
  RGB Image using Deep Neural Network","In Virtual, augmented, and mixed reality, the use of hand gestures is
increasingly becoming popular to reduce the difference between the virtual and
real world. The precise location of the fingertip is essential/crucial for a
seamless experience. Much of the research work is based on using depth
information for the estimation of the fingertips position. However, most of the
work using RGB images for fingertips detection is limited to a single finger.
The detection of multiple fingertips from a single RGB image is very
challenging due to various factors. In this paper, we propose a deep neural
network (DNN) based methodology to estimate the fingertips position. We
christened this methodology as an Anchor based Fingertips Position Estimation
(ABFPE), and it is a two-step process. The fingertips location is estimated
using regression by computing the difference in the location of a fingertip
from the nearest anchor point. The proposed framework performs the best with
limited dependence on hand detection results. In our experiments on the
SCUT-Ego-Gesture dataset, we achieved the fingertips detection error of 2.3552
pixels on a video frame with a resolution of $640 \times 480$ and about
$92.98\%$ of test images have average pixel errors of five pixels.",arxiv
http://arxiv.org/abs/1612.06699v3,2017-06-12T21:38:17Z,2016-12-20T15:04:38Z,Unsupervised Perceptual Rewards for Imitation Learning,"Reward function design and exploration time are arguably the biggest
obstacles to the deployment of reinforcement learning (RL) agents in the real
world. In many real-world tasks, designing a reward function takes considerable
hand engineering and often requires additional sensors to be installed just to
measure whether the task has been executed successfully. Furthermore, many
interesting tasks consist of multiple implicit intermediate steps that must be
executed in sequence. Even when the final outcome can be measured, it does not
necessarily provide feedback on these intermediate steps. To address these
issues, we propose leveraging the abstraction power of intermediate visual
representations learned by deep models to quickly infer perceptual reward
functions from small numbers of demonstrations. We present a method that is
able to identify key intermediate steps of a task from only a handful of
demonstration sequences, and automatically identify the most discriminative
features for identifying these steps. This method makes use of the features in
a pre-trained deep model, but does not require any explicit specification of
sub-goals. The resulting reward functions can then be used by an RL agent to
learn to perform the task in real-world settings. To evaluate the learned
reward, we present qualitative results on two real-world tasks and a
quantitative evaluation against a human-designed reward function. We also show
that our method can be used to learn a real-world door opening skill using a
real robot, even when the demonstration used for reward learning is provided by
a human using their own hand. To our knowledge, these are the first results
showing that complex robotic manipulation skills can be learned directly and
without supervised labels from a video of a human performing the task.
Supplementary material and data are available at
https://sermanet.github.io/rewards",arxiv
http://arxiv.org/abs/1810.11846v2,2019-02-19T05:08:46Z,2018-10-28T17:59:57Z,LPCNet: Improving Neural Speech Synthesis Through Linear Prediction,"Neural speech synthesis models have recently demonstrated the ability to
synthesize high quality speech for text-to-speech and compression applications.
These new models often require powerful GPUs to achieve real-time operation, so
being able to reduce their complexity would open the way for many new
applications. We propose LPCNet, a WaveRNN variant that combines linear
prediction with recurrent neural networks to significantly improve the
efficiency of speech synthesis. We demonstrate that LPCNet can achieve
significantly higher quality than WaveRNN for the same network size and that
high quality LPCNet speech synthesis is achievable with a complexity under 3
GFLOPS. This makes it easier to deploy neural synthesis applications on
lower-power devices, such as embedded systems and mobile phones.",arxiv
http://arxiv.org/abs/2003.02372v1,2020-03-04T23:46:45Z,2020-03-04T23:46:45Z,Dynamic Experience Replay,"We present a novel technique called Dynamic Experience Replay (DER) that
allows Reinforcement Learning (RL) algorithms to use experience replay samples
not only from human demonstrations but also successful transitions generated by
RL agents during training and therefore improve training efficiency. It can be
combined with an arbitrary off-policy RL algorithm, such as DDPG or DQN, and
their distributed versions. We build upon Ape-X DDPG and demonstrate our
approach on robotic tight-fitting joint assembly tasks, based on force/torque
and Cartesian pose observations. In particular, we run experiments on two
different tasks: peg-in-hole and lap-joint. In each case, we compare different
replay buffer structures and how DER affects them. Our ablation studies show
that Dynamic Experience Replay is a crucial ingredient that either largely
shortens the training time in these challenging environments or solves the
tasks that the vanilla Ape-X DDPG cannot solve. We also show that our policies
learned purely in simulation can be deployed successfully on the real robot.
The video presenting our experiments is available at
https://sites.google.com/site/dynamicexperiencereplay",arxiv
http://arxiv.org/abs/2004.01030v1,2020-04-01T14:50:43Z,2020-04-01T14:50:43Z,"Objects of violence: synthetic data for practical ML in human rights
  investigations","We introduce a machine learning workflow to search for, identify, and
meaningfully triage videos and images of munitions, weapons, and military
equipment, even when limited training data exists for the object of interest.
This workflow is designed to expedite the work of OSINT (""open source
intelligence"") researchers in human rights investigations. It consists of three
components: automatic rendering and annotating of synthetic datasets that make
up for a lack of training data; training image classifiers from combined sets
of photographic and synthetic data; and mtriage, an open source software that
orchestrates these classifiers' deployment to triage public domain media, and
visualise predictions in a web interface. We show that synthetic data helps to
train classifiers more effectively, and that certain approaches yield better
results for different architectures. We then demonstrate our workflow in two
real-world human rights investigations: the use of the Triple-Chaser tear gas
grenade against civilians, and the verification of allegations of military
presence in Ukraine in 2014.",arxiv
http://arxiv.org/abs/1912.01180v1,2019-12-03T03:45:45Z,2019-12-03T03:45:45Z,"RSA: Randomized Simulation as Augmentation for Robust Human Action
  Recognition","Despite the rapid growth in datasets for video activity, stable robust
activity recognition with neural networks remains challenging. This is in large
part due to the explosion of possible variation in video -- including lighting
changes, object variation, movement variation, and changes in surrounding
context. An alternative is to make use of simulation data, where all of these
factors can be artificially controlled. In this paper, we propose the
Randomized Simulation as Augmentation (RSA) framework which augments real-world
training data with synthetic data to improve the robustness of action
recognition networks. We generate large-scale synthetic datasets with
randomized nuisance factors. We show that training with such extra data, when
appropriately constrained, can significantly improve the performance of the
state-of-the-art I3D networks or, conversely, reduce the number of labeled real
videos needed to achieve good performance. Experiments on two real-world
datasets NTU RGB+D and VIRAT demonstrate the effectiveness of our method.",arxiv
http://arxiv.org/abs/1904.12356v1,2019-04-28T18:00:08Z,2019-04-28T18:00:08Z,Deferred Neural Rendering: Image Synthesis using Neural Textures,"The modern computer graphics pipeline can synthesize images at remarkable
visual quality; however, it requires well-defined, high-quality 3D content as
input. In this work, we explore the use of imperfect 3D content, for instance,
obtained from photo-metric reconstructions with noisy and incomplete surface
geometry, while still aiming to produce photo-realistic (re-)renderings. To
address this challenging problem, we introduce Deferred Neural Rendering, a new
paradigm for image synthesis that combines the traditional graphics pipeline
with learnable components. Specifically, we propose Neural Textures, which are
learned feature maps that are trained as part of the scene capture process.
Similar to traditional textures, neural textures are stored as maps on top of
3D mesh proxies; however, the high-dimensional feature maps contain
significantly more information, which can be interpreted by our new deferred
neural rendering pipeline. Both neural textures and deferred neural renderer
are trained end-to-end, enabling us to synthesize photo-realistic images even
when the original 3D content was imperfect. In contrast to traditional,
black-box 2D generative neural networks, our 3D representation gives us
explicit control over the generated output, and allows for a wide range of
application domains. For instance, we can synthesize temporally-consistent
video re-renderings of recorded 3D scenes as our representation is inherently
embedded in 3D space. This way, neural textures can be utilized to coherently
re-render or manipulate existing video content in both static and dynamic
environments at real-time rates. We show the effectiveness of our approach in
several experiments on novel view synthesis, scene editing, and facial
reenactment, and compare to state-of-the-art approaches that leverage the
standard graphics pipeline as well as conventional generative neural networks.",arxiv
http://arxiv.org/abs/1805.02792v1,2018-05-08T01:04:19Z,2018-05-08T01:04:19Z,FFNet: Video Fast-Forwarding via Reinforcement Learning,"For many applications with limited computation, communication, storage and
energy resources, there is an imperative need of computer vision methods that
could select an informative subset of the input video for efficient processing
at or near real time. In the literature, there are two relevant groups of
approaches: generating a trailer for a video or fast-forwarding while
watching/processing the video. The first group is supported by video
summarization techniques, which require processing of the entire video to
select an important subset for showing to users. In the second group, current
fast-forwarding methods depend on either manual control or automatic adaptation
of playback speed, which often do not present an accurate representation and
may still require processing of every frame. In this paper, we introduce
FastForwardNet (FFNet), a reinforcement learning agent that gets inspiration
from video summarization and does fast-forwarding differently. It is an online
framework that automatically fast-forwards a video and presents a
representative subset of frames to users on the fly. It does not require
processing the entire video, but just the portion that is selected by the
fast-forward agent, which makes the process very computationally efficient. The
online nature of our proposed method also enables the users to begin
fast-forwarding at any point of the video. Experiments on two real-world
datasets demonstrate that our method can provide better representation of the
input video with much less processing requirement.",arxiv
http://arxiv.org/abs/2003.00951v2,2021-10-19T13:38:48Z,2020-03-02T14:54:19Z,"DriverMHG: A Multi-Modal Dataset for Dynamic Recognition of Driver Micro
  Hand Gestures and a Real-Time Recognition Framework","The use of hand gestures provides a natural alternative to cumbersome
interface devices for Human-Computer Interaction (HCI) systems. However,
real-time recognition of dynamic micro hand gestures from video streams is
challenging for in-vehicle scenarios since (i) the gestures should be performed
naturally without distracting the driver, (ii) micro hand gestures occur within
very short time intervals at spatially constrained areas, (iii) the performed
gesture should be recognized only once, and (iv) the entire architecture should
be designed lightweight as it will be deployed to an embedded system. In this
work, we propose an HCI system for dynamic recognition of driver micro hand
gestures, which can have a crucial impact in automotive sector especially for
safety related issues. For this purpose, we initially collected a dataset named
Driver Micro Hand Gestures (DriverMHG), which consists of RGB, depth and
infrared modalities. The challenges for dynamic recognition of micro hand
gestures have been addressed by proposing a lightweight convolutional neural
network (CNN) based architecture which operates online efficiently with a
sliding window approach. For the CNN model, several 3-dimensional resource
efficient networks are applied and their performances are analyzed. Online
recognition of gestures has been performed with 3D-MobileNetV2, which provided
the best offline accuracy among the applied networks with similar computational
complexities. The final architecture is deployed on a driver simulator
operating in real-time. We make DriverMHG dataset and our source code publicly
available.",arxiv
http://arxiv.org/abs/1812.01029v1,2018-12-03T19:05:25Z,2018-12-03T19:05:25Z,Sensitivity based Neural Networks Explanations,"Although neural networks can achieve very high predictive performance on
various different tasks such as image recognition or natural language
processing, they are often considered as opaque ""black boxes"". The difficulty
of interpreting the predictions of a neural network often prevents its use in
fields where explainability is important, such as the financial industry where
regulators and auditors often insist on this aspect. In this paper, we present
a way to assess the relative input features importance of a neural network
based on the sensitivity of the model output with respect to its input. This
method has the advantage of being fast to compute, it can provide both global
and local levels of explanations and is applicable for many types of neural
network architectures. We illustrate the performance of this method on both
synthetic and real data and compare it with other interpretation techniques.
This method is implemented into an open-source Python package that allows its
users to easily generate and visualize explanations for their neural networks.",arxiv
http://arxiv.org/abs/1802.02961v1,2018-02-08T16:49:00Z,2018-02-08T16:49:00Z,Learning Sparse Wavelet Representations,"In this work we propose a method for learning wavelet filters directly from
data. We accomplish this by framing the discrete wavelet transform as a
modified convolutional neural network. We introduce an autoencoder wavelet
transform network that is trained using gradient descent. We show that the
model is capable of learning structured wavelet filters from synthetic and real
data. The learned wavelets are shown to be similar to traditional wavelets that
are derived using Fourier methods. Our method is simple to implement and easily
incorporated into neural network architectures. A major advantage to our model
is that we can learn from raw audio data.",arxiv
http://arxiv.org/abs/1808.06277v1,2018-08-20T00:54:29Z,2018-08-20T00:54:29Z,An Efficient Approach for Geo-Multimedia Cross-Modal Retrieval,"Due to the rapid development of mobile Internet techniques, cloud computation
and popularity of online social networking and location-based services, massive
amount of multimedia data with geographical information is generated and
uploaded to the Internet. In this paper, we propose a novel type of cross-modal
multimedia retrieval called geo-multimedia cross-modal retrieval which aims to
search out a set of geo-multimedia objects based on geographical distance
proximity and semantic similarity between different modalities. Previous
studies for cross-modal retrieval and spatial keyword search cannot address
this problem effectively because they do not consider multimedia data with
geo-tags and do not focus on this type of query. In order to address this
problem efficiently, we present the definition of $k$NN geo-multimedia
cross-modal query at the first time and introduce relevant conceptions such as
cross-modal semantic representation space. To bridge the semantic gap between
different modalities, we propose a method named cross-modal semantic matching
which contains two important component, i.e., CorrProj and LogsTran, which aims
to construct a common semantic representation space for cross-modal semantic
similarity measurement. Besides, we designed a framework based on deep learning
techniques to implement common semantic representation space construction. In
addition, a novel hybrid indexing structure named GMR-Tree combining
geo-multimedia data and R-Tree is presented and a efficient $k$NN search
algorithm called $k$GMCMS is designed. Comprehensive experimental evaluation on
real and synthetic dataset clearly demonstrates that our solution outperforms
the-state-of-the-art methods.",arxiv
http://arxiv.org/abs/1911.08563v1,2019-11-17T12:50:05Z,2019-11-17T12:50:05Z,"Robust Sub-Meter Level Indoor Localization With a Single WiFi Access
  Point-Regression Versus Classification","Precise indoor localization is an increasingly demanding requirement for
various emerging applications, like Virtual/Augmented reality and personalized
advertising. Current indoor environments are equipped with pluralities of WiFi
access points (APs), whose deployment is expected to be massive in the future
enabling highly precise localization approaches. Though the conventional
model-based localization schemes have achieved sub-meter level accuracy by
fusing multiple channel state information (CSI) observations, the corresponding
computational overhead is usually significant, especially in the current
multiple-input multiple-output orthogonal frequency division multiplexing
(MIMO-OFDM) systems. In order to address this issue, model-free localization
techniques using deep learning frameworks have been lately proposed, where
mainly classification methods were applied. In this paper, instead of
classification based mechanism, we propose a logistic regression based scheme
with the deep learning framework, combined with Cram\'er-Rao lower bound (CRLB)
assisted robust training, which achieves more robust sub-meter level accuracy
(0.97m median distance error) in the standard laboratory environment and
maintains reasonable online prediction overhead under the single WiFi AP
settings.",arxiv
http://arxiv.org/abs/2003.12425v1,2020-03-27T14:06:36Z,2020-03-27T14:06:36Z,"Mic2Mic: Using Cycle-Consistent Generative Adversarial Networks to
  Overcome Microphone Variability in Speech Systems","Mobile and embedded devices are increasingly using microphones and
audio-based computational models to infer user context. A major challenge in
building systems that combine audio models with commodity microphones is to
guarantee their accuracy and robustness in the real-world. Besides many
environmental dynamics, a primary factor that impacts the robustness of audio
models is microphone variability. In this work, we propose Mic2Mic -- a
machine-learned system component -- which resides in the inference pipeline of
audio models and at real-time reduces the variability in audio data caused by
microphone-specific factors. Two key considerations for the design of Mic2Mic
were: a) to decouple the problem of microphone variability from the audio task,
and b) put a minimal burden on end-users to provide training data. With these
in mind, we apply the principles of cycle-consistent generative adversarial
networks (CycleGANs) to learn Mic2Mic using unlabeled and unpaired data
collected from different microphones. Our experiments show that Mic2Mic can
recover between 66% to 89% of the accuracy lost due to microphone variability
for two common audio tasks.",arxiv
http://arxiv.org/abs/1910.13348v1,2019-10-29T16:07:02Z,2019-10-29T16:07:02Z,"Sequential image processing methods for improving semantic video
  segmentation algorithms","Recently, semantic video segmentation gained high attention especially for
supporting autonomous driving systems. Deep learning methods made it possible
to implement real time segmentation and object identification algorithms on
videos. However, most of the available approaches process each video frame
independently disregarding their sequential relation in time. Therefore their
results suddenly miss some of the object segments in some of the frames even if
they were detected properly in the earlier frames. Herein we propose two
sequential probabilistic video frame analysis approaches to improve the
segmentation performance of the existing algorithms. Our experiments show that
using the information of the past frames we increase the performance and
consistency of the state of the art algorithms.",arxiv
http://arxiv.org/abs/2110.03528v1,2021-10-05T15:41:04Z,2021-10-05T15:41:04Z,Decoding ECoG signal into 3D hand translation using deep learning,"Motor brain-computer interfaces (BCIs) are a promising technology that may
enable motor-impaired people to interact with their environment. Designing
real-time and accurate BCI is crucial to make such devices useful, safe, and
easy to use by patients in a real-life environment. Electrocorticography
(ECoG)-based BCIs emerge as a good compromise between invasiveness of the
recording device and good spatial and temporal resolution of the recorded
signal. However, most ECoG signal decoders used to predict continuous hand
movements are linear models. These models have a limited representational
capacity and may fail to capture the relationship between ECoG signal and
continuous hand movements. Deep learning (DL) models, which are
state-of-the-art in many problems, could be a solution to better capture this
relationship. In this study, we tested several DL-based architectures to
predict imagined 3D continuous hand translation using time-frequency features
extracted from ECoG signals. The dataset used in the analysis is a part of a
long-term clinical trial (ClinicalTrials.gov identifier: NCT02550522) and was
acquired during a closed-loop experiment with a tetraplegic subject. The
proposed architectures include multilayer perceptron (MLP), convolutional
neural networks (CNN), and long short-term memory networks (LSTM). The accuracy
of the DL-based and multilinear models was compared offline using cosine
similarity. Our results show that CNN-based architectures outperform the
current state-of-the-art multilinear model. The best architecture exploited the
spatial correlation between neighboring electrodes with CNN and benefited from
the sequential character of the desired hand trajectory by using LSTMs.
Overall, DL increased the average cosine similarity, compared to the
multilinear model, by up to 60%, from 0.189 to 0.302 and from 0.157 to 0.249
for the left and right hand, respectively.",arxiv
http://arxiv.org/abs/1806.11430v3,2018-07-31T10:31:36Z,2018-06-29T14:18:24Z,Towards real-time unsupervised monocular depth estimation on CPU,"Unsupervised depth estimation from a single image is a very attractive
technique with several implications in robotic, autonomous navigation,
augmented reality and so on. This topic represents a very challenging task and
the advent of deep learning enabled to tackle this problem with excellent
results. However, these architectures are extremely deep and complex. Thus,
real-time performance can be achieved only by leveraging power-hungry GPUs that
do not allow to infer depth maps in application fields characterized by
low-power constraints. To tackle this issue, in this paper we propose a novel
architecture capable to quickly infer an accurate depth map on a CPU, even of
an embedded system, using a pyramid of features extracted from a single input
image. Similarly to state-of-the-art, we train our network in an unsupervised
manner casting depth estimation as an image reconstruction problem. Extensive
experimental results on the KITTI dataset show that compared to the top
performing approach our network has similar accuracy but a much lower
complexity (about 6% of parameters) enabling to infer a depth map for a KITTI
image in about 1.7 s on the Raspberry Pi 3 and at more than 8 Hz on a standard
CPU. Moreover, by trading accuracy for efficiency, our network allows to infer
maps at about 2 Hz and 40 Hz respectively, still being more accurate than most
state-of-the-art slower methods. To the best of our knowledge, it is the first
method enabling such performance on CPUs paving the way for effective
deployment of unsupervised monocular depth estimation even on embedded systems.",arxiv
http://arxiv.org/abs/1811.06753v1,2018-11-16T11:08:26Z,2018-11-16T11:08:26Z,Stochastic Adaptive Neural Architecture Search for Keyword Spotting,"The problem of keyword spotting i.e. identifying keywords in a real-time
audio stream is mainly solved by applying a neural network over successive
sliding windows. Due to the difficulty of the task, baseline models are usually
large, resulting in a high computational cost and energy consumption level. We
propose a new method called SANAS (Stochastic Adaptive Neural Architecture
Search) which is able to adapt the architecture of the neural network
on-the-fly at inference time such that small architectures will be used when
the stream is easy to process (silence, low noise, ...) and bigger networks
will be used when the task becomes more difficult. We show that this adaptive
model can be learned end-to-end by optimizing a trade-off between the
prediction performance and the average computational cost per unit of time.
Experiments on the Speech Commands dataset show that this approach leads to a
high recognition level while being much faster (and/or energy saving) than
classical approaches where the network architecture is static.",arxiv
http://arxiv.org/abs/2110.06674v1,2021-10-13T12:18:09Z,2021-10-13T12:18:09Z,Truthful AI: Developing and governing AI that does not lie,"In many contexts, lying -- the use of verbal falsehoods to deceive -- is
harmful. While lying has traditionally been a human affair, AI systems that
make sophisticated verbal statements are becoming increasingly prevalent. This
raises the question of how we should limit the harm caused by AI ""lies"" (i.e.
falsehoods that are actively selected for). Human truthfulness is governed by
social norms and by laws (against defamation, perjury, and fraud). Differences
between AI and humans present an opportunity to have more precise standards of
truthfulness for AI, and to have these standards rise over time. This could
provide significant benefits to public epistemics and the economy, and mitigate
risks of worst-case AI futures.
  Establishing norms or laws of AI truthfulness will require significant work
to: (1) identify clear truthfulness standards; (2) create institutions that can
judge adherence to those standards; and (3) develop AI systems that are
robustly truthful.
  Our initial proposals for these areas include: (1) a standard of avoiding
""negligent falsehoods"" (a generalisation of lies that is easier to assess); (2)
institutions to evaluate AI systems before and after real-world deployment; and
(3) explicitly training AI systems to be truthful via curated datasets and
human interaction.
  A concerning possibility is that evaluation mechanisms for eventual
truthfulness standards could be captured by political interests, leading to
harmful censorship and propaganda. Avoiding this might take careful attention.
And since the scale of AI speech acts might grow dramatically over the coming
decades, early truthfulness standards might be particularly important because
of the precedents they set.",arxiv
http://arxiv.org/abs/2010.06659v1,2020-10-13T19:50:26Z,2020-10-13T19:50:26Z,Towards Data-efficient Modeling for Wake Word Spotting,"Wake word (WW) spotting is challenging in far-field not only because of the
interference in signal transmission but also the complexity in acoustic
environments. Traditional WW model training requires large amount of in-domain
WW-specific data with substantial human annotations therefore it is hard to
build WW models without such data. In this paper we present data-efficient
solutions to address the challenges in WW modeling, such as domain-mismatch,
noisy conditions, limited annotation, etc. Our proposed system is composed of a
multi-condition training pipeline with a stratified data augmentation, which
improves the model robustness to a variety of predefined acoustic conditions,
together with a semi-supervised learning pipeline to accurately extract the WW
and confusable examples from untranscribed speech corpus. Starting from only 10
hours of domain-mismatched WW audio, we are able to enlarge and enrich the
training dataset by 20-100 times to capture the acoustic complexity. Our
experiments on real user data show that the proposed solutions can achieve
comparable performance of a production-grade model by saving 97\% of the amount
of WW-specific data collection and 86\% of the bandwidth for annotation.",arxiv
http://arxiv.org/abs/1812.09953v3,2019-01-10T01:11:19Z,2018-12-24T16:50:49Z,"A Curriculum Domain Adaptation Approach to the Semantic Segmentation of
  Urban Scenes","During the last half decade, convolutional neural networks (CNNs) have
triumphed over semantic segmentation, which is one of the core tasks in many
applications such as autonomous driving and augmented reality. However, to
train CNNs requires a considerable amount of data, which is difficult to
collect and laborious to annotate. Recent advances in computer graphics make it
possible to train CNNs on photo-realistic synthetic imagery with
computer-generated annotations. Despite this, the domain mismatch between the
real images and the synthetic data hinders the models' performance. Hence, we
propose a curriculum-style learning approach to minimizing the domain gap in
urban scene semantic segmentation. The curriculum domain adaptation solves easy
tasks first to infer necessary properties about the target domain; in
particular, the first task is to learn global label distributions over images
and local distributions over landmark superpixels. These are easy to estimate
because images of urban scenes have strong idiosyncrasies (e.g., the size and
spatial relations of buildings, streets, cars, etc.). We then train a
segmentation network, while regularizing its predictions in the target domain
to follow those inferred properties. In experiments, our method outperforms the
baselines on two datasets and two backbone networks. We also report extensive
ablation studies about our approach.",arxiv
http://arxiv.org/abs/1802.08960v2,2019-02-01T17:08:34Z,2018-02-25T06:47:30Z,"Bonnet: An Open-Source Training and Deployment Framework for Semantic
  Segmentation in Robotics using CNNs","The ability to interpret a scene is an important capability for a robot that
is supposed to interact with its environment. The knowledge of what is in front
of the robot is, for example, relevant for navigation, manipulation, or
planning. Semantic segmentation labels each pixel of an image with a class
label and thus provides a detailed semantic annotation of the surroundings to
the robot. Convolutional neural networks (CNNs) are popular methods for
addressing this type of problem. The available software for training and the
integration of CNNs for real robots, however, is quite fragmented and often
difficult to use for non-experts, despite the availability of several
high-quality open-source frameworks for neural network implementation and
training. In this paper, we propose a tool called Bonnet, which addresses this
fragmentation problem by building a higher abstraction that is specific for the
semantic segmentation task. It provides a modular approach to simplify the
training of a semantic segmentation CNN independently of the used dataset and
the intended task. Furthermore, we also address the deployment on a real
robotic platform. Thus, we do not propose a new CNN approach in this paper.
Instead, we provide a stable and easy-to-use tool to make this technology more
approachable in the context of autonomous systems. In this sense, we aim at
closing a gap between computer vision research and its use in robotics
research. We provide an open-source codebase for training and deployment. The
training interface is implemented in Python using TensorFlow and the deployment
interface provides a C++ library that can be easily integrated in an existing
robotics codebase, a ROS node, and two standalone applications for label
prediction in images and videos.",arxiv
http://arxiv.org/abs/2110.07728v1,2021-10-07T17:48:57Z,2021-10-07T17:48:57Z,Pre-training Molecular Graph Representation with 3D Geometry,"Molecular graph representation learning is a fundamental problem in modern
drug and material discovery. Molecular graphs are typically modeled by their 2D
topological structures, but it has been recently discovered that 3D geometric
information plays a more vital role in predicting molecular functionalities.
However, the lack of 3D information in real-world scenarios has significantly
impeded the learning of geometric graph representation. To cope with this
challenge, we propose the Graph Multi-View Pre-training (GraphMVP) framework
where self-supervised learning (SSL) is performed by leveraging the
correspondence and consistency between 2D topological structures and 3D
geometric views. GraphMVP effectively learns a 2D molecular graph encoder that
is enhanced by richer and more discriminative 3D geometry. We further provide
theoretical insights to justify the effectiveness of GraphMVP. Finally,
comprehensive experiments show that GraphMVP can consistently outperform
existing graph SSL methods.",arxiv
http://arxiv.org/abs/2009.05076v2,2021-09-21T00:27:18Z,2020-09-10T18:25:33Z,Utterance Clustering Using Stereo Audio Channels,"Utterance clustering is one of the actively researched topics in audio signal
processing and machine learning. This study aims to improve the performance of
utterance clustering by processing multichannel (stereo) audio signals.
Processed audio signals were generated by combining left- and right-channel
audio signals in a few different ways and then extracted embedded features
(also called d-vectors) from those processed audio signals. This study applied
the Gaussian mixture model for supervised utterance clustering. In the training
phase, a parameter sharing Gaussian mixture model was conducted to train the
model for each speaker. In the testing phase, the speaker with the maximum
likelihood was selected as the detected speaker. Results of experiments with
real audio recordings of multi-person discussion sessions showed that the
proposed method that used multichannel audio signals achieved significantly
better performance than a conventional method with mono audio signals in more
complicated conditions.",arxiv
http://arxiv.org/abs/2105.14391v1,2021-05-29T23:56:05Z,2021-05-29T23:56:05Z,"Data-driven 6D Pose Tracking by Calibrating Image Residuals in Synthetic
  Domains","Tracking the 6D pose of objects in video sequences is important for robot
manipulation. This work presents se(3)-TrackNet, a data-driven optimization
approach for long term, 6D pose tracking. It aims to identify the optimal
relative pose given the current RGB-D observation and a synthetic image
conditioned on the previous best estimate and the object's model. The key
contribution in this context is a novel neural network architecture, which
appropriately disentangles the feature encoding to help reduce domain shift,
and an effective 3D orientation representation via Lie Algebra. Consequently,
even when the network is trained solely with synthetic data can work
effectively over real images. Comprehensive experiments over multiple
benchmarks show se(3)-TrackNet achieves consistently robust estimates and
outperforms alternatives, even though they have been trained with real images.
The approach runs in real time at 90.9Hz. Code, data and supplementary video
for this project are available at
https://github.com/wenbowen123/iros20-6d-pose-tracking",arxiv
http://arxiv.org/abs/1910.06407v1,2019-10-14T20:19:15Z,2019-10-14T20:19:15Z,FireNet: Real-time Segmentation of Fire Perimeter from Aerial Video,"In this paper, we share our approach to real-time segmentation of fire
perimeter from aerial full-motion infrared video. We start by describing the
problem from a humanitarian aid and disaster response perspective.
Specifically, we explain the importance of the problem, how it is currently
resolved, and how our machine learning approach improves it. To test our models
we annotate a large-scale dataset of 400,000 frames with guidance from domain
experts. Finally, we share our approach currently deployed in production with
inference speed of 20 frames per second and an accuracy of 92 (F1 Score).",arxiv
http://arxiv.org/abs/2108.04561v1,2021-08-10T10:15:54Z,2021-08-10T10:15:54Z,Evolution of NOMA Toward Next Generation Multiple Access (NGMA),"Due to the explosive growth in the number of wireless devices and diverse
wireless services, such as virtual/augmented reality and
Internet-of-Everything, next generation wireless networks face unprecedented
challenges caused by heterogeneous data traffic, massive connectivity, and
ultra-high bandwidth efficiency and ultra-low latency requirements. To address
these challenges, advanced multiple access schemes are expected to be
developed, namely next generation multiple access (NGMA), which are capable of
supporting massive numbers of users in a more resource- and
complexity-efficient manner than existing multiple access schemes. As the
research on NGMA is in a very early stage, in this paper, we explore the
evolution of NGMA with a particular focus on non-orthogonal multiple access
(NOMA), i.e., the transition from NOMA to NGMA. In particular, we first review
the fundamental capacity limits of NOMA, elaborate the new requirements for
NGMA, and discuss several possible candidate techniques. Moreover, given the
high compatibility and flexibility of NOMA, we provide an overview of current
research efforts on multi-antenna techniques for NOMA, promising future
application scenarios of NOMA, and the interplay between NOMA and other
emerging physical layer techniques. Furthermore, we discuss advanced
mathematical tools for facilitating the design of NOMA communication systems,
including conventional optimization approaches and new machine learning
techniques. Next, we propose a unified framework for NGMA based on multiple
antennas and NOMA, where both downlink and uplink transmission are considered,
thus setting the foundation for this emerging research area. Finally, several
practical implementation challenges for NGMA are highlighted as motivation for
future work.",arxiv
http://arxiv.org/abs/1811.12238v1,2018-11-29T15:13:26Z,2018-11-29T15:13:26Z,Perceiving Physical Equation by Observing Visual Scenarios,"Inferring universal laws of the environment is an important ability of human
intelligence as well as a symbol of general AI. In this paper, we take a step
toward this goal such that we introduce a new challenging problem of inferring
invariant physical equation from visual scenarios. For instance, teaching a
machine to automatically derive the gravitational acceleration formula by
watching a free-falling object. To tackle this challenge, we present a novel
pipeline comprised of an Observer Engine and a Physicist Engine by respectively
imitating the actions of an observer and a physicist in the real world.
Generally, the Observer Engine watches the visual scenarios and then extracting
the physical properties of objects. The Physicist Engine analyses these data
and then summarizing the inherent laws of object dynamics. Specifically, the
learned laws are expressed by mathematical equations such that they are more
interpretable than the results given by common probabilistic models.
Experiments on synthetic videos have shown that our pipeline is able to
discover physical equations on various physical worlds with different visual
appearances.",arxiv
http://arxiv.org/abs/1811.10399v1,2018-11-26T14:38:25Z,2018-11-26T14:38:25Z,"A Convolutional Neural Network based Live Object Recognition System as
  Blind Aid","This paper introduces a live object recognition system that serves as a blind
aid. Visually impaired people heavily rely on their other senses such as touch
and auditory signals for understanding the environment around them. The act of
knowing what object is in front of the blind person without touching it (by
hand or some other tool) is very difficult. In some cases, the physical contact
between the person and object can be dangerous, and even lethal.
  This project employs a Convolutional Neural Network for recognition of
pre-trained objects on the ImageNet dataset. A camera, aligned with the
system's predetermined orientation serves as input to the computer system,
which has the object recognition Neural Network deployed to carry out real-time
object detection. Output from the network can then be parsed to present to the
visually impaired person either in the form of audio or Braille text.",arxiv
http://arxiv.org/abs/2103.17088v1,2021-03-31T13:58:39Z,2021-03-31T13:58:39Z,"Deep Noise Suppression With Non-Intrusive PESQNet Supervision Enabling
  the Use of Real Training Data","Data-driven speech enhancement employing deep neural networks (DNNs) can
provide state-of-the-art performance even in the presence of non-stationary
noise. During the training process, most of the speech enhancement neural
networks are trained in a fully supervised way with losses requiring noisy
speech to be synthesized by clean speech and additive noise. However, in a real
implementation, only the noisy speech mixture is available, which leads to the
question, how such data could be advantageously employed in training. In this
work, we propose an end-to-end non-intrusive PESQNet DNN which estimates
perceptual evaluation of speech quality (PESQ) scores, allowing a
reference-free loss for real data. As a further novelty, we combine the PESQNet
loss with denoising and dereverberation loss terms, and train a complex
mask-based fully convolutional recurrent neural network (FCRN) in a ""weakly""
supervised way, each training cycle employing some synthetic data, some real
data, and again synthetic data to keep the PESQNet up-to-date. In a subjective
listening test, our proposed framework outperforms the Interspeech 2021 Deep
Noise Suppression (DNS) Challenge baseline overall by 0.09 MOS points and in
particular by 0.45 background noise MOS points.",arxiv
http://arxiv.org/abs/2108.00516v1,2021-08-01T18:14:46Z,2021-08-01T18:14:46Z,"BundleTrack: 6D Pose Tracking for Novel Objects without Instance or
  Category-Level 3D Models","Tracking the 6D pose of objects in video sequences is important for robot
manipulation. Most prior efforts, however, often assume that the target
object's CAD model, at least at a category-level, is available for offline
training or during online template matching. This work proposes BundleTrack, a
general framework for 6D pose tracking of novel objects, which does not depend
upon 3D models, either at the instance or category-level. It leverages the
complementary attributes of recent advances in deep learning for segmentation
and robust feature extraction, as well as memory-augmented pose graph
optimization for spatiotemporal consistency. This enables long-term, low-drift
tracking under various challenging scenarios, including significant occlusions
and object motions. Comprehensive experiments given two public benchmarks
demonstrate that the proposed approach significantly outperforms state-of-art,
category-level 6D tracking or dynamic SLAM methods. When compared against
state-of-art methods that rely on an object instance CAD model, comparable
performance is achieved, despite the proposed method's reduced information
requirements. An efficient implementation in CUDA provides a real-time
performance of 10Hz for the entire framework. Code is available at:
https://github.com/wenbowen123/BundleTrack",arxiv
http://arxiv.org/abs/2004.00137v1,2020-03-31T22:02:38Z,2020-03-31T22:02:38Z,Revisiting Few-shot Activity Detection with Class Similarity Control,"Many interesting events in the real world are rare making preannotated
machine learning ready videos a rarity in consequence. Thus, temporal activity
detection models that are able to learn from a few examples are desirable. In
this paper, we present a conceptually simple and general yet novel framework
for few-shot temporal activity detection based on proposal regression which
detects the start and end time of the activities in untrimmed videos. Our model
is end-to-end trainable, takes into account the frame rate differences between
few-shot activities and untrimmed test videos, and can benefit from additional
few-shot examples. We experiment on three large scale benchmarks for temporal
activity detection (ActivityNet1.2, ActivityNet1.3 and THUMOS14 datasets) in a
few-shot setting. We also study the effect on performance of different amount
of overlap with activities used to pretrain the video classification backbone
and propose corrective measures for future works in this domain. Our code will
be made available.",arxiv
http://arxiv.org/abs/1912.07906v4,2020-04-28T23:24:12Z,2019-12-17T09:56:15Z,"Deep SCNN-based Real-time Object Detection for Self-driving Vehicles
  Using LiDAR Temporal Data","Real-time accurate detection of three-dimensional (3D) objects is a
fundamental necessity for self-driving vehicles. Most existing computer vision
approaches are based on convolutional neural networks (CNNs). Although the
CNN-based approaches can achieve high detection accuracy, their high energy
consumption is a severe drawback. To resolve this problem, novel energy
efficient approaches should be explored. Spiking neural network (SNN) is a
promising candidate because it has orders-of-magnitude lower energy consumption
than CNN. Unfortunately, the studying of SNN has been limited in small networks
only. The application of SNN for large 3D object detection networks has remain
largely open. In this paper, we integrate spiking convolutional neural network
(SCNN) with temporal coding into the YOLOv2 architecture for real-time object
detection. To take the advantage of spiking signals, we develop a novel data
preprocessing layer that translates 3D point-cloud data into spike time data.
We propose an analog circuit to implement the non-leaky integrate and fire
neuron used in our SCNN, from which the energy consumption of each spike is
estimated. Moreover, we present a method to calculate the network sparsity and
the energy consumption of the overall network. Extensive experiments have been
conducted based on the KITTI dataset, which show that the proposed network can
reach competitive detection accuracy as existing approaches, yet with much
lower average energy consumption. If implemented in dedicated hardware, our
network could have a mean sparsity of 56.24% and extremely low total energy
consumption of 0.247mJ only. Implemented in NVIDIA GTX 1080i GPU, we can
achieve 35.7 fps frame rate, high enough for real-time object detection.",arxiv
http://arxiv.org/abs/1804.09914v1,2018-04-26T07:02:07Z,2018-04-26T07:02:07Z,"iTeleScope: Intelligent Video Telemetry and Classification in Real-Time
  using Software Defined Networking","Video continues to dominate network traffic, yet operators today have poor
visibility into the number, duration, and resolutions of the video streams
traversing their domain. Current approaches are inaccurate, expensive, or
unscalable, as they rely on statistical sampling, middle-box hardware, or
packet inspection software. We present {\em iTelescope}, the first intelligent,
inexpensive, and scalable SDN-based solution for identifying and classifying
video flows in real-time. Our solution is novel in combining dynamic flow rules
with telemetry and machine learning, and is built on commodity OpenFlow
switches and open-source software. We develop a fully functional system, train
it in the lab using multiple machine learning algorithms, and validate its
performance to show over 95\% accuracy in identifying and classifying video
streams from many providers including Youtube and Netflix. Lastly, we conduct
tests to demonstrate its scalability to tens of thousands of concurrent
streams, and deploy it live on a campus network serving several hundred real
users. Our system gives unprecedented fine-grained real-time visibility of
video streaming performance to operators of enterprise and carrier networks at
very low cost.",arxiv
http://arxiv.org/abs/1208.6057v1,2012-08-30T00:58:22Z,2012-08-30T00:58:22Z,"Self-paced brain-computer interface control of ambulation in a virtual
  reality environment","Objective: Spinal cord injury (SCI) often leaves affected individuals unable
to ambulate. Electroencephalogramme (EEG) based brain-computer interface (BCI)
controlled lower extremity prostheses may restore intuitive and able-body-like
ambulation after SCI. To test its feasibility, the authors developed and tested
a novel EEG-based, data-driven BCI system for intuitive and self-paced control
of the ambulation of an avatar within a virtual reality environment (VRE).
  Approach: Eight able-bodied subjects and one with SCI underwent the following
10-min training session: subjects alternated between idling and walking
kinaesthetic motor imageries (KMI) while their EEG were recorded and analysed
to generate subject-specific decoding models. Subjects then performed a
goal-oriented online task, repeated over 5 sessions, in which they utilised the
KMI to control the linear ambulation of an avatar and make 10 sequential stops
at designated points within the VRE.
  Main results: The average offline training performance across subjects was
77.2 +/- 9.5%, ranging from 64.3% (p = 0.00176) to 94.5% (p = 6.26*10^-23),
with chance performance being 50%. The average online performance was 8.4 +/-
1.0 (out of 10) successful stops and 303 +/- 53 sec completion time (perfect =
211 sec). All subjects achieved performances significantly different than those
of random walk (p < 0.05) in 44 of the 45 online sessions.
  Significance: By using a data-driven machine learning approach to decode
users' KMI, this BCIVRE system enabled intuitive and purposeful self-paced
control of ambulation after only a 10-minute training. The ability to achieve
such BCI control with minimal training indicates that the implementation of
future BCI-lower extremity prosthesis systems may be feasible.",arxiv
http://arxiv.org/abs/2111.03776v1,2021-11-07T13:02:52Z,2021-11-07T13:02:52Z,"Wireless Edge-Empowered Metaverse: A Learning-Based Incentive Mechanism
  for Virtual Reality","The Metaverse is regarded as the next-generation Internet paradigm that
allows humans to play, work, and socialize in an alternative virtual world with
immersive experience, for instance, via head-mounted display for Virtual
Reality (VR) rendering. With the help of ubiquitous wireless connections and
powerful edge computing technologies, VR users in wireless edge-empowered
Metaverse can immerse in the virtual through the access of VR services offered
by different providers. However, VR applications are computation- and
communication-intensive. The VR service providers (SPs) have to optimize the VR
service delivery efficiently and economically given their limited communication
and computation resources. An incentive mechanism can be thus applied as an
effective tool for managing VR services between providers and users. Therefore,
in this paper, we propose a learning-based Incentive Mechanism framework for VR
services in the Metaverse. First, we propose the quality of perception as the
metric for VR users immersing in the virtual world. Second, for quick trading
of VR services between VR users (i.e., buyers) and VR SPs (i.e., sellers), we
design a double Dutch auction mechanism to determine optimal pricing and
allocation rules in this market. Third, for auction communication reduction, we
design a deep reinforcement learning-based auctioneer to accelerate this
auction process. Experimental results demonstrate that the proposed framework
can achieve near-optimal social welfare while reducing at least half of the
auction information exchange cost than baseline methods.",arxiv
http://arxiv.org/abs/1908.09464v1,2019-08-26T04:35:15Z,2019-08-26T04:35:15Z,Shape-Aware Human Pose and Shape Reconstruction Using Multi-View Images,"We propose a scalable neural network framework to reconstruct the 3D mesh of
a human body from multi-view images, in the subspace of the SMPL model. Use of
multi-view images can significantly reduce the projection ambiguity of the
problem, increasing the reconstruction accuracy of the 3D human body under
clothing. Our experiments show that this method benefits from the synthetic
dataset generated from our pipeline since it has good flexibility of variable
control and can provide ground-truth for validation. Our method outperforms
existing methods on real-world images, especially on shape estimations.",arxiv
http://arxiv.org/abs/1805.01956v1,2018-05-04T22:45:08Z,2018-05-04T22:45:08Z,"Motion Planning Among Dynamic, Decision-Making Agents with Deep
  Reinforcement Learning","Robots that navigate among pedestrians use collision avoidance algorithms to
enable safe and efficient operation. Recent works present deep reinforcement
learning as a framework to model the complex interactions and cooperation.
However, they are implemented using key assumptions about other agents'
behavior that deviate from reality as the number of agents in the environment
increases. This work extends our previous approach to develop an algorithm that
learns collision avoidance among a variety of types of dynamic agents without
assuming they follow any particular behavior rules. This work also introduces a
strategy using LSTM that enables the algorithm to use observations of an
arbitrary number of other agents, instead of previous methods that have a fixed
observation size. The proposed algorithm outperforms our previous approach in
simulation as the number of agents increases, and the algorithm is demonstrated
on a fully autonomous robotic vehicle traveling at human walking speed, without
the use of a 3D Lidar.",arxiv
http://arxiv.org/abs/1910.06404v3,2020-09-30T21:24:00Z,2019-10-14T20:13:36Z,Learning to Generate 6-DoF Grasp Poses with Reachability Awareness,"Motivated by the stringent requirements of unstructured real-world where a
plethora of unknown objects reside in arbitrary locations of the surface, we
propose a voxel-based deep 3D Convolutional Neural Network (3D CNN) that
generates feasible 6-DoF grasp poses in unrestricted workspace with
reachability awareness. Unlike the majority of works that predict if a proposed
grasp pose within the restricted workspace will be successful solely based on
grasp pose stability, our approach further learns a reachability predictor that
evaluates if the grasp pose is reachable or not from robot's own experience. To
avoid the laborious real training data collection, we exploit the power of
simulation to train our networks on a large-scale synthetic dataset. This work
is an early attempt that simultaneously evaluates grasping reachability from
learned knowledge while proposing feasible grasp poses with 3D CNN.
Experimental results in both simulation and real-world demonstrate that our
approach outperforms several other methods and achieves 82.5% grasping success
rate on unknown objects.",arxiv
http://arxiv.org/abs/2004.13358v1,2020-04-28T08:15:35Z,2020-04-28T08:15:35Z,Transferable Active Grasping and Real Embodied Dataset,"Grasping in cluttered scenes is challenging for robot vision systems, as
detection accuracy can be hindered by partial occlusion of objects. We adopt a
reinforcement learning (RL) framework and 3D vision architectures to search for
feasible viewpoints for grasping by the use of hand-mounted RGB-D cameras. To
overcome the disadvantages of photo-realistic environment simulation, we
propose a large-scale dataset called Real Embodied Dataset (RED), which
includes full-viewpoint real samples on the upper hemisphere with amodal
annotation and enables a simulator that has real visual feedback. Based on this
dataset, a practical 3-stage transferable active grasping pipeline is
developed, that is adaptive to unseen clutter scenes. In our pipeline, we
propose a novel mask-guided reward to overcome the sparse reward issue in
grasping and ensure category-irrelevant behavior. The grasping pipeline and its
possible variants are evaluated with extensive experiments both in simulation
and on a real-world UR-5 robotic arm.",arxiv
http://arxiv.org/abs/2007.05616v1,2020-07-10T21:29:23Z,2020-07-10T21:29:23Z,NaviGAN: A Generative Approach for Socially Compliant Navigation,"Robots navigating in human crowds need to optimize their paths not only for
their task performance but also for their compliance to social norms. One of
the key challenges in this context is the lack of standard metrics for
evaluating and optimizing a socially compliant behavior. Existing works in
social navigation can be grouped according to the differences in their
optimization objectives. For instance, the reinforcement learning approaches
tend to optimize on the \textit{comfort} aspect of the socially compliant
navigation, whereas the inverse reinforcement learning approaches are designed
to achieve \textit{natural} behavior. In this paper, we propose NaviGAN, a
generative navigation algorithm that jointly optimizes both of the
\textit{comfort} and \textit{naturalness} aspects. Our approach is designed as
an adversarial training framework that can learn to generate a navigation path
that is both optimized for achieving a goal and for complying with latent
social rules. A set of experiments has been carried out on multiple datasets to
demonstrate the strengths of the proposed approach quantitatively. We also
perform extensive experiments using a physical robot in a real-world
environment to qualitatively evaluate the trained social navigation behavior.
The video recordings of the robot experiments can be found in the link:
https://youtu.be/61blDymjCpw.",arxiv
http://arxiv.org/abs/2104.08212v2,2021-04-27T20:06:33Z,2021-04-16T16:38:02Z,MT-Opt: Continuous Multi-Task Robotic Reinforcement Learning at Scale,"General-purpose robotic systems must master a large repertoire of diverse
skills to be useful in a range of daily tasks. While reinforcement learning
provides a powerful framework for acquiring individual behaviors, the time
needed to acquire each skill makes the prospect of a generalist robot trained
with RL daunting. In this paper, we study how a large-scale collective robotic
learning system can acquire a repertoire of behaviors simultaneously, sharing
exploration, experience, and representations across tasks. In this framework
new tasks can be continuously instantiated from previously learned tasks
improving overall performance and capabilities of the system. To instantiate
this system, we develop a scalable and intuitive framework for specifying new
tasks through user-provided examples of desired outcomes, devise a multi-robot
collective learning system for data collection that simultaneously collects
experience for multiple tasks, and develop a scalable and generalizable
multi-task deep reinforcement learning method, which we call MT-Opt. We
demonstrate how MT-Opt can learn a wide range of skills, including semantic
picking (i.e., picking an object from a particular category), placing into
various fixtures (e.g., placing a food item onto a plate), covering, aligning,
and rearranging. We train and evaluate our system on a set of 12 real-world
tasks with data collected from 7 robots, and demonstrate the performance of our
system both in terms of its ability to generalize to structurally similar new
tasks, and acquire distinct new tasks more quickly by leveraging past
experience. We recommend viewing the videos at
https://karolhausman.github.io/mt-opt/",arxiv
http://arxiv.org/abs/1910.08406v2,2020-01-20T09:11:34Z,2019-10-18T13:16:57Z,Fully Parallel Hyperparameter Search: Reshaped Space-Filling,"Space-filling designs such as scrambled-Hammersley, Latin Hypercube Sampling
and Jittered Sampling have been proposed for fully parallel hyperparameter
search, and were shown to be more effective than random or grid search. In this
paper, we show that these designs only improve over random search by a constant
factor. In contrast, we introduce a new approach based on reshaping the search
distribution, which leads to substantial gains over random search, both
theoretically and empirically. We propose two flavors of reshaping. First, when
the distribution of the optimum is some known $P_0$, we propose Recentering,
which uses as search distribution a modified version of $P_0$ tightened closer
to the center of the domain, in a dimension-dependent and budget-dependent
manner. Second, we show that in a wide range of experiments with $P_0$ unknown,
using a proposed Cauchy transformation, which simultaneously has a heavier tail
(for unbounded hyperparameters) and is closer to the boundaries (for bounded
hyperparameters), leads to improved performances. Besides artificial
experiments and simple real world tests on clustering or Salmon mappings, we
check our proposed methods on expensive artificial intelligence tasks such as
attend/infer/repeat, video next frame segmentation forecasting and progressive
generative adversarial networks.",arxiv
http://arxiv.org/abs/1910.06017v1,2019-10-14T09:52:40Z,2019-10-14T09:52:40Z,"OmniTrack: Real-time detection and tracking of objects, text and logos
  in video","The automatic detection and tracking of general objects (like persons,
animals or cars), text and logos in a video is crucial for many video
understanding tasks, and usually real-time processing as required. We propose
OmniTrack, an efficient and robust algorithm which is able to automatically
detect and track objects, text as well as brand logos in real-time. It combines
a powerful deep learning based object detector (YoloV3) with high-quality
optical flow methods. Based on the reference YoloV3 C++ implementation, we did
some important performance optimizations which will be described. The major
steps in the training procedure for the combined detector for text and logo
will be presented. We will describe then the OmniTrack algorithm, consisting of
the phases preprocessing, feature calculation, prediction, matching and update.
Several performance optimizations have been implemented there as well, like
doing the object detection and optical flow calculation asynchronously.
Experiments show that the proposed algorithm runs in real-time for standard
definition ($720x576$) video on a PC with a Quadro RTX 5000 GPU.",arxiv
http://arxiv.org/abs/2001.08481v2,2020-02-21T18:14:11Z,2020-01-23T12:58:50Z,"Learning Object Placements For Relational Instructions by Hallucinating
  Scene Representations","Robots coexisting with humans in their environment and performing services
for them need the ability to interact with them. One particular requirement for
such robots is that they are able to understand spatial relations and can place
objects in accordance with the spatial relations expressed by their user. In
this work, we present a convolutional neural network for estimating pixelwise
object placement probabilities for a set of spatial relations from a single
input image. During training, our network receives the learning signal by
classifying hallucinated high-level scene representations as an auxiliary task.
Unlike previous approaches, our method does not require ground truth data for
the pixelwise relational probabilities or 3D models of the objects, which
significantly expands the applicability in practical applications. Our results
obtained using real-world data and human-robot experiments demonstrate the
effectiveness of our method in reasoning about the best way to place objects to
reproduce a spatial relation. Videos of our experiments can be found at
https://youtu.be/zaZkHTWFMKM",arxiv
http://arxiv.org/abs/2003.11117v1,2020-03-24T21:17:44Z,2020-03-24T21:17:44Z,"COVID-19 and Computer Audition: An Overview on What Speech & Sound
  Analysis Could Contribute in the SARS-CoV-2 Corona Crisis","At the time of writing, the world population is suffering from more than
10,000 registered COVID-19 disease epidemic induced deaths since the outbreak
of the Corona virus more than three months ago now officially known as
SARS-CoV-2. Since, tremendous efforts have been made worldwide to counter-steer
and control the epidemic by now labelled as pandemic. In this contribution, we
provide an overview on the potential for computer audition (CA), i.e., the
usage of speech and sound analysis by artificial intelligence to help in this
scenario. We first survey which types of related or contextually significant
phenomena can be automatically assessed from speech or sound. These include the
automatic recognition and monitoring of breathing, dry and wet coughing or
sneezing sounds, speech under cold, eating behaviour, sleepiness, or pain to
name but a few. Then, we consider potential use-cases for exploitation. These
include risk assessment and diagnosis based on symptom histograms and their
development over time, as well as monitoring of spread, social distancing and
its effects, treatment and recovery, and patient wellbeing. We quickly guide
further through challenges that need to be faced for real-life usage. We come
to the conclusion that CA appears ready for implementation of (pre-)diagnosis
and monitoring tools, and more generally provides rich and significant, yet so
far untapped potential in the fight against COVID-19 spread.",arxiv
http://arxiv.org/abs/2010.15824v2,2020-11-03T16:19:59Z,2020-10-29T17:57:12Z,Passport-aware Normalization for Deep Model Protection,"Despite tremendous success in many application scenarios, deep learning faces
serious intellectual property (IP) infringement threats. Considering the cost
of designing and training a good model, infringements will significantly
infringe the interests of the original model owner. Recently, many impressive
works have emerged for deep model IP protection. However, they either are
vulnerable to ambiguity attacks, or require changes in the target network
structure by replacing its original normalization layers and hence cause
significant performance drops. To this end, we propose a new passport-aware
normalization formulation, which is generally applicable to most existing
normalization layers and only needs to add another passport-aware branch for IP
protection. This new branch is jointly trained with the target model but
discarded in the inference stage. Therefore it causes no structure change in
the target model. Only when the model IP is suspected to be stolen by someone,
the private passport-aware branch is added back for ownership verification.
Through extensive experiments, we verify its effectiveness in both image and 3D
point recognition models. It is demonstrated to be robust not only to common
attack techniques like fine-tuning and model compression, but also to ambiguity
attacks. By further combining it with trigger-set based methods, both black-box
and white-box verification can be achieved for enhanced security of deep
learning models deployed in real systems. Code can be found at
https://github.com/ZJZAC/Passport-aware-Normalization.",arxiv
http://arxiv.org/abs/1904.09412v1,2019-04-20T07:45:08Z,2019-04-20T07:45:08Z,Cubic LSTMs for Video Prediction,"Predicting future frames in videos has become a promising direction of
research for both computer vision and robot learning communities. The core of
this problem involves moving object capture and future motion prediction. While
object capture specifies which objects are moving in videos, motion prediction
describes their future dynamics. Motivated by this analysis, we propose a Cubic
Long Short-Term Memory (CubicLSTM) unit for video prediction. CubicLSTM
consists of three branches, i.e., a spatial branch for capturing moving
objects, a temporal branch for processing motions, and an output branch for
combining the first two branches to generate predicted frames. Stacking
multiple CubicLSTM units along the spatial branch and output branch, and then
evolving along the temporal branch can form a cubic recurrent neural network
(CubicRNN). Experiment shows that CubicRNN produces more accurate video
predictions than prior methods on both synthetic and real-world datasets.",arxiv
http://arxiv.org/abs/2008.00376v1,2020-08-02T01:18:18Z,2020-08-02T01:18:18Z,"Velocity Regulation of 3D Bipedal Walking Robots with Uncertain Dynamics
  Through Adaptive Neural Network Controller","This paper presents a neural-network based adaptive feedback control
structure to regulate the velocity of 3D bipedal robots under dynamics
uncertainties. Existing Hybrid Zero Dynamics (HZD)-based controllers regulate
velocity through the implementation of heuristic regulators that do not
consider model and environmental uncertainties, which may significantly affect
the tracking performance of the controllers. In this paper, we address the
uncertainties in the robot dynamics from the perspective of the reduced
dimensional representation of virtual constraints and propose the integration
of an adaptive neural network-based controller to regulate the robot velocity
in the presence of model parameter uncertainties. The proposed approach yields
improved tracking performance under dynamics uncertainties. The shallow
adaptive neural network used in this paper does not require training a priori
and has the potential to be implemented on the real-time robotic controller. A
comparative simulation study of a 3D Cassie robot is presented to illustrate
the performance of the proposed approach under various scenarios.",arxiv
http://arxiv.org/abs/1811.00661v2,2018-11-13T21:47:51Z,2018-11-01T22:27:20Z,Exposing Deep Fakes Using Inconsistent Head Poses,"In this paper, we propose a new method to expose AI-generated fake face
images or videos (commonly known as the Deep Fakes). Our method is based on the
observations that Deep Fakes are created by splicing synthesized face region
into the original image, and in doing so, introducing errors that can be
revealed when 3D head poses are estimated from the face images. We perform
experiments to demonstrate this phenomenon and further develop a classification
method based on this cue. Using features based on this cue, an SVM classifier
is evaluated using a set of real face images and Deep Fakes.",arxiv
http://arxiv.org/abs/1801.03002v2,2019-02-20T12:44:10Z,2018-01-08T14:08:55Z,DeepStyle: Multimodal Search Engine for Fashion and Interior Design,"In this paper, we propose a multimodal search engine that combines visual and
textual cues to retrieve items from a multimedia database aesthetically similar
to the query. The goal of our engine is to enable intuitive retrieval of
fashion merchandise such as clothes or furniture. Existing search engines treat
textual input only as an additional source of information about the query image
and do not correspond to the real-life scenario where the user looks for 'the
same shirt but of denim'. Our novel method, dubbed DeepStyle, mitigates those
shortcomings by using a joint neural network architecture to model contextual
dependencies between features of different modalities. We prove the robustness
of this approach on two different challenging datasets of fashion items and
furniture where our DeepStyle engine outperforms baseline methods by 18-21% on
the tested datasets. Our search engine is commercially deployed and available
through a Web-based application.",arxiv
http://arxiv.org/abs/1903.06571v1,2019-03-15T14:30:22Z,2019-03-15T14:30:22Z,Inserting Videos into Videos,"In this paper, we introduce a new problem of manipulating a given video by
inserting other videos into it. Our main task is, given an object video and a
scene video, to insert the object video at a user-specified location in the
scene video so that the resulting video looks realistic. We aim to handle
different object motions and complex backgrounds without expensive segmentation
annotations. As it is difficult to collect training pairs for this problem, we
synthesize fake training pairs that can provide helpful supervisory signals
when training a neural network with unpaired real data. The proposed network
architecture can take both real and fake pairs as input and perform both
supervised and unsupervised training in an adversarial learning scheme. To
synthesize a realistic video, the network renders each frame based on the
current input and previous frames. Within this framework, we observe that
injecting noise into previous frames while generating the current frame
stabilizes training. We conduct experiments on real-world videos in object
tracking and person re-identification benchmark datasets. Experimental results
demonstrate that the proposed algorithm is able to synthesize long sequences of
realistic videos with a given object video inserted.",arxiv
http://arxiv.org/abs/2105.14428v1,2021-05-30T04:55:04Z,2021-05-30T04:55:04Z,"DAGNN: Demand-aware Graph Neural Networks for Session-based
  Recommendation","Session-based recommendations have been widely adopted for various online
video and E-commerce Websites. Most existing approaches are intuitively
proposed to discover underlying interests or preferences out of the anonymous
session data. This apparently ignores the fact these sequential behaviors
usually reflect session user's potential demand, i.e., a semantic level factor,
and therefore how to estimate underlying demands from a session is challenging.
To address aforementioned issue, this paper proposes a demand-aware graph
neural networks (DAGNN). Particularly, a demand modeling component is designed
to first extract session demand and the underlying multiple demands of each
session is estimated using the global demand matrix. Then, the demand-aware
graph neural network is designed to extract session demand graph to learn the
demand-aware item embedddings for the later recommendations. The mutual
information loss is further designed to enhance the quality of the learnt
embeddings. Extensive experiments are evaluated on several real-world datasets
and the proposed model achieves the SOTA model performance.",arxiv
http://arxiv.org/abs/2107.12744v2,2021-08-04T14:59:30Z,2021-07-27T11:38:44Z,"Real-Time Activity Recognition and Intention Recognition Using a
  Vision-based Embedded System","With the rapid increase in digital technologies, most fields of study include
recognition of human activity and intention recognition, which are essential in
smart environments. In this study, we equipped the activity recognition system
with the ability to recognize intentions by affecting the pace of movement of
individuals in the representation of images. Using this technology in various
environments such as elevators and automatic doors will lead to identifying
those who intend to pass the automatic door from those who are passing by. This
system, if applied in elevators and automatic doors, will save energy and
increase efficiency. For this study, data preparation is applied to combine the
spatial and temporal features with the help of digital image processing
principles. Nevertheless, unlike previous studies, only one AlexNet neural
network is used instead of two-stream convolutional neural networks. Our
embedded system was implemented with an accuracy of 98.78% on our intention
recognition dataset. We also examined our data representation approach on other
datasets, including HMDB-51, KTH, and Weizmann, and obtained accuracy of
78.48%, 97.95%, and 100%, respectively. The image recognition and neural
network models were simulated and implemented using Xilinx simulators for the
Xilinx ZCU102 board. The operating frequency of this embedded system is 333
MHz, and it works in real-time with 120 frames per second (fps).",arxiv
http://arxiv.org/abs/2110.09075v1,2021-10-18T07:52:17Z,2021-10-18T07:52:17Z,"Boosting the Transferability of Video Adversarial Examples via Temporal
  Translation","Although deep-learning based video recognition models have achieved
remarkable success, they are vulnerable to adversarial examples that are
generated by adding human-imperceptible perturbations on clean video samples.
As indicated in recent studies, adversarial examples are transferable, which
makes it feasible for black-box attacks in real-world applications.
Nevertheless, most existing adversarial attack methods have poor
transferability when attacking other video models and transfer-based attacks on
video models are still unexplored. To this end, we propose to boost the
transferability of video adversarial examples for black-box attacks on video
recognition models. Through extensive analysis, we discover that different
video recognition models rely on different discriminative temporal patterns,
leading to the poor transferability of video adversarial examples. This
motivates us to introduce a temporal translation attack method, which optimizes
the adversarial perturbations over a set of temporal translated video clips. By
generating adversarial examples over translated videos, the resulting
adversarial examples are less sensitive to temporal patterns existed in the
white-box model being attacked and thus can be better transferred. Extensive
experiments on the Kinetics-400 dataset and the UCF-101 dataset demonstrate
that our method can significantly boost the transferability of video
adversarial examples. For transfer-based attack against video recognition
models, it achieves a 61.56% average attack success rate on the Kinetics-400
and 48.60% on the UCF-101.",arxiv
http://arxiv.org/abs/2003.14414v1,2020-03-31T17:57:16Z,2020-03-31T17:57:16Z,Optical Non-Line-of-Sight Physics-based 3D Human Pose Estimation,"We describe a method for 3D human pose estimation from transient images
(i.e., a 3D spatio-temporal histogram of photons) acquired by an optical
non-line-of-sight (NLOS) imaging system. Our method can perceive 3D human pose
by `looking around corners' through the use of light indirectly reflected by
the environment. We bring together a diverse set of technologies from NLOS
imaging, human pose estimation and deep reinforcement learning to construct an
end-to-end data processing pipeline that converts a raw stream of photon
measurements into a full 3D human pose sequence estimate. Our contributions are
the design of data representation process which includes (1) a learnable
inverse point spread function (PSF) to convert raw transient images into a deep
feature vector; (2) a neural humanoid control policy conditioned on the
transient image feature and learned from interactions with a physics simulator;
and (3) a data synthesis and augmentation strategy based on depth data that can
be transferred to a real-world NLOS imaging system. Our preliminary experiments
suggest that our method is able to generalize to real-world NLOS measurement to
estimate physically-valid 3D human poses.",arxiv
http://arxiv.org/abs/1602.01208v3,2016-05-07T11:59:51Z,2016-02-03T06:56:51Z,"Spatial Concept Acquisition for a Mobile Robot that Integrates
  Self-Localization and Unsupervised Word Discovery from Spoken Sentences","In this paper, we propose a novel unsupervised learning method for the
lexical acquisition of words related to places visited by robots, from human
continuous speech signals. We address the problem of learning novel words by a
robot that has no prior knowledge of these words except for a primitive
acoustic model. Further, we propose a method that allows a robot to effectively
use the learned words and their meanings for self-localization tasks. The
proposed method is nonparametric Bayesian spatial concept acquisition method
(SpCoA) that integrates the generative model for self-localization and the
unsupervised word segmentation in uttered sentences via latent variables
related to the spatial concept. We implemented the proposed method SpCoA on
SIGVerse, which is a simulation environment, and TurtleBot2, which is a mobile
robot in a real environment. Further, we conducted experiments for evaluating
the performance of SpCoA. The experimental results showed that SpCoA enabled
the robot to acquire the names of places from speech sentences. They also
revealed that the robot could effectively utilize the acquired spatial concepts
and reduce the uncertainty in self-localization.",arxiv
http://arxiv.org/abs/2012.15373v1,2020-12-30T23:59:09Z,2020-12-30T23:59:09Z,Model-Based Visual Planning with Self-Supervised Functional Distances,"A generalist robot must be able to complete a variety of tasks in its
environment. One appealing way to specify each task is in terms of a goal
observation. However, learning goal-reaching policies with reinforcement
learning remains a challenging problem, particularly when hand-engineered
reward functions are not available. Learned dynamics models are a promising
approach for learning about the environment without rewards or task-directed
data, but planning to reach goals with such a model requires a notion of
functional similarity between observations and goal states. We present a
self-supervised method for model-based visual goal reaching, which uses both a
visual dynamics model as well as a dynamical distance function learned using
model-free reinforcement learning. Our approach learns entirely using offline,
unlabeled data, making it practical to scale to large and diverse datasets. In
our experiments, we find that our method can successfully learn models that
perform a variety of tasks at test-time, moving objects amid distractors with a
simulated robotic arm and even learning to open and close a drawer using a
real-world robot. In comparisons, we find that this approach substantially
outperforms both model-free and model-based prior methods. Videos and
visualizations are available here: http://sites.google.com/berkeley.edu/mbold.",arxiv
http://arxiv.org/abs/2107.05627v1,2021-07-12T17:59:58Z,2021-07-12T17:59:58Z,Hierarchical Neural Dynamic Policies,"We tackle the problem of generalization to unseen configurations for dynamic
tasks in the real world while learning from high-dimensional image input. The
family of nonlinear dynamical system-based methods have successfully
demonstrated dynamic robot behaviors but have difficulty in generalizing to
unseen configurations as well as learning from image inputs. Recent works
approach this issue by using deep network policies and reparameterize actions
to embed the structure of dynamical systems but still struggle in domains with
diverse configurations of image goals, and hence, find it difficult to
generalize. In this paper, we address this dichotomy by leveraging embedding
the structure of dynamical systems in a hierarchical deep policy learning
framework, called Hierarchical Neural Dynamical Policies (H-NDPs). Instead of
fitting deep dynamical systems to diverse data directly, H-NDPs form a
curriculum by learning local dynamical system-based policies on small regions
in state-space and then distill them into a global dynamical system-based
policy that operates only from high-dimensional images. H-NDPs additionally
provide smooth trajectories, a strong safety benefit in the real world. We
perform extensive experiments on dynamic tasks both in the real world (digit
writing, scooping, and pouring) and simulation (catching, throwing, picking).
We show that H-NDPs are easily integrated with both imitation as well as
reinforcement learning setups and achieve state-of-the-art results. Video
results are at https://shikharbahl.github.io/hierarchical-ndps/",arxiv
http://arxiv.org/abs/1808.06099v1,2018-08-18T16:27:10Z,2018-08-18T16:27:10Z,Multi-dimensional Graph Convolutional Networks,"Convolutional neural networks (CNNs) leverage the great power in
representation learning on regular grid data such as image and video. Recently,
increasing attention has been paid on generalizing CNNs to graph or network
data which is highly irregular. Some focus on graph-level representation
learning while others aim to learn node-level representations. These methods
have been shown to boost the performance of many graph-level tasks such as
graph classification and node-level tasks such as node classification. Most of
these methods have been designed for single-dimensional graphs where a pair of
nodes can only be connected by one type of relation. However, many real-world
graphs have multiple types of relations and they can be naturally modeled as
multi-dimensional graphs with each type of relation as a dimension.
Multi-dimensional graphs bring about richer interactions between dimensions,
which poses tremendous challenges to the graph convolutional neural networks
designed for single-dimensional graphs. In this paper, we study the problem of
graph convolutional networks for multi-dimensional graphs and propose a
multi-dimensional convolutional neural network model mGCN aiming to capture
rich information in learning node-level representations for multi-dimensional
graphs. Comprehensive experiments on real-world multi-dimensional graphs
demonstrate the effectiveness of the proposed framework.",arxiv
http://arxiv.org/abs/2110.04176v1,2021-10-08T14:57:19Z,2021-10-08T14:57:19Z,"Lightweight Convolutional Neural Networks By Hypercomplex
  Parameterization","Hypercomplex neural networks have proved to reduce the overall number of
parameters while ensuring valuable performances by leveraging the properties of
Clifford algebras. Recently, hypercomplex linear layers have been further
improved by involving efficient parameterized Kronecker products. In this
paper, we define the parameterization of hypercomplex convolutional layers to
develop lightweight and efficient large-scale convolutional models. Our method
grasps the convolution rules and the filters organization directly from data
without requiring a rigidly predefined domain structure to follow. The proposed
approach is flexible to operate in any user-defined or tuned domain, from 1D to
$n$D regardless of whether the algebra rules are preset. Such a malleability
allows processing multidimensional inputs in their natural domain without
annexing further dimensions, as done, instead, in quaternion neural networks
for 3D inputs like color images. As a result, the proposed method operates with
$1/n$ free parameters as regards its analog in the real domain. We demonstrate
the versatility of this approach to multiple domains of application by
performing experiments on various image datasets as well as audio datasets in
which our method outperforms real and quaternion-valued counterparts.",arxiv
http://arxiv.org/abs/1710.00082v1,2017-09-29T20:33:38Z,2017-09-29T20:33:38Z,"Real-Time Wind Noise Detection and Suppression with Neural-Based Signal
  Reconstruction for Mult-Channel, Low-Power Devices","Active wind noise detection and suppression techniques are a new and
essential paradigm for enhancing ASR-based functionality with smart glasses, in
addition to other wearable and smart devices in the broader IoT (Internet of
things). In this paper, we develop two separate algorithms for wind noise
detection and suppression, respectively, operational in a challenging,
low-energy regime. Together, these algorithms comprise a robust wind noise
suppression system. In the first case, we advance a real-time wind detection
algorithm (RTWD) that uses two distinct sets of low-dimensional signal features
to discriminate the presence of wind noise with high accuracy. For wind noise
suppression, we employ an additional algorithm - attentive neural wind
suppression (ANWS) - that utilizes a neural network to reconstruct the wearer
speech signal from wind-corrupted audio in the spectral regions that are most
adversely affected by wind noise. Finally, we test our algorithms through
real-time experiments using low-power, multi-microphone devices with a wind
simulator under challenging detection criteria and a variety of wind
intensities.",arxiv
http://arxiv.org/abs/2008.00889v1,2020-08-03T14:09:28Z,2020-08-03T14:09:28Z,"Speaker dependent articulatory-to-acoustic mapping using real-time MRI
  of the vocal tract","Articulatory-to-acoustic (forward) mapping is a technique to predict speech
using various articulatory acquisition techniques (e.g. ultrasound tongue
imaging, lip video). Real-time MRI (rtMRI) of the vocal tract has not been used
before for this purpose. The advantage of MRI is that it has a high `relative'
spatial resolution: it can capture not only lingual, labial and jaw motion, but
also the velum and the pharyngeal region, which is typically not possible with
other techniques. In the current paper, we train various DNNs (fully connected,
convolutional and recurrent neural networks) for articulatory-to-speech
conversion, using rtMRI as input, in a speaker-specific way. We use two male
and two female speakers of the USC-TIMIT articulatory database, each of them
uttering 460 sentences. We evaluate the results with objective (Normalized MSE
and MCD) and subjective measures (perceptual test) and show that CNN-LSTM
networks are preferred which take multiple images as input, and achieve MCD
scores between 2.8-4.5 dB. In the experiments, we find that the predictions of
speaker `m1' are significantly weaker than other speakers. We show that this is
caused by the fact that 74% of the recordings of speaker `m1' are out of sync.",arxiv
http://arxiv.org/abs/2104.03657v1,2021-04-08T10:18:52Z,2021-04-08T10:18:52Z,"Dynamic Object Aware LiDAR SLAM based on Automatic Generation of
  Training Data","Highly dynamic environments, with moving objects such as cars or humans, can
pose a performance challenge for LiDAR SLAM systems that assume largely static
scenes. To overcome this challenge and support the deployment of robots in real
world scenarios, we propose a complete solution for a dynamic object aware
LiDAR SLAM algorithm. This is achieved by leveraging a real-time capable neural
network that can detect dynamic objects, thus allowing our system to deal with
them explicitly. To efficiently generate the necessary training data which is
key to our approach, we present a novel end-to-end occupancy grid based
pipeline that can automatically label a wide variety of arbitrary dynamic
objects. Our solution can thus generalize to different environments without the
need for expensive manual labeling and at the same time avoids assumptions
about the presence of a predefined set of known objects in the scene. Using
this technique, we automatically label over 12000 LiDAR scans collected in an
urban environment with a large amount of pedestrians and use this data to train
a neural network, achieving an average segmentation IoU of 0.82. We show that
explicitly dealing with dynamic objects can improve the LiDAR SLAM odometry
performance by 39.6% while yielding maps which better represent the
environments. A supplementary video as well as our test data are available
online.",arxiv
http://arxiv.org/abs/1808.06940v1,2018-08-20T09:25:30Z,2018-08-20T09:25:30Z,End to End Vehicle Lateral Control Using a Single Fisheye Camera,"Convolutional neural networks are commonly used to control the steering angle
for autonomous cars. Most of the time, multiple long range cameras are used to
generate lateral failure cases. In this paper we present a novel model to
generate this data and label augmentation using only one short range fisheye
camera. We present our simulator and how it can be used as a consistent metric
for lateral end-to-end control evaluation. Experiments are conducted on a
custom dataset corresponding to more than 10000 km and 200 hours of open road
driving. Finally we evaluate this model on real world driving scenarios, open
road and a custom test track with challenging obstacle avoidance and sharp
turns. In our simulator based on real-world videos, the final model was capable
of more than 99% autonomy on urban road",arxiv
http://arxiv.org/abs/2101.10278v1,2021-01-25T17:58:47Z,2021-01-25T17:58:47Z,"High-Quality Vocoding Design with Signal Processing for Speech Synthesis
  and Voice Conversion","This Ph.D. thesis focuses on developing a system for high-quality speech
synthesis and voice conversion. Vocoder-based speech analysis, manipulation,
and synthesis plays a crucial role in various kinds of statistical parametric
speech research. Although there are vocoding methods which yield close to
natural synthesized speech, they are typically computationally expensive, and
are thus not suitable for real-time implementation, especially in embedded
environments. Therefore, there is a need for simple and computationally
feasible digital signal processing algorithms for generating high-quality and
natural-sounding synthesized speech. In this dissertation, I propose a solution
to extract optimal acoustic features and a new waveform generator to achieve
higher sound quality and conversion accuracy by applying advances in deep
learning. The approach remains computationally efficient. This challenge
resulted in five thesis groups, which are briefly summarized below.",arxiv
http://arxiv.org/abs/1711.07128v3,2018-02-14T19:24:55Z,2017-11-20T03:19:03Z,Hello Edge: Keyword Spotting on Microcontrollers,"Keyword spotting (KWS) is a critical component for enabling speech based user
interactions on smart devices. It requires real-time response and high accuracy
for good user experience. Recently, neural networks have become an attractive
choice for KWS architecture because of their superior accuracy compared to
traditional speech processing algorithms. Due to its always-on nature, KWS
application has highly constrained power budget and typically runs on tiny
microcontrollers with limited memory and compute capability. The design of
neural network architecture for KWS must consider these constraints. In this
work, we perform neural network architecture evaluation and exploration for
running KWS on resource-constrained microcontrollers. We train various neural
network architectures for keyword spotting published in literature to compare
their accuracy and memory/compute requirements. We show that it is possible to
optimize these neural network architectures to fit within the memory and
compute constraints of microcontrollers without sacrificing accuracy. We
further explore the depthwise separable convolutional neural network (DS-CNN)
and compare it against other neural network architectures. DS-CNN achieves an
accuracy of 95.4%, which is ~10% higher than the DNN model with similar number
of parameters.",arxiv
http://arxiv.org/abs/1907.01172v3,2020-04-13T05:49:55Z,2019-07-02T05:17:44Z,Procedure Planning in Instructional Videos,"In this paper, we study the problem of procedure planning in instructional
videos, which can be seen as a step towards enabling autonomous agents to plan
for complex tasks in everyday settings such as cooking. Given the current
visual observation of the world and a visual goal, we ask the question ""What
actions need to be taken in order to achieve the goal?"". The key technical
challenge is to learn structured and plannable state and action spaces directly
from unstructured videos. We address this challenge by proposing Dual Dynamics
Networks (DDN), a framework that explicitly leverages the structured priors
imposed by the conjugate relationships between states and actions in a learned
plannable latent space. We evaluate our method on real-world instructional
videos. Our experiments show that DDN learns plannable representations that
lead to better planning performance compared to existing planning approaches
and neural network policies.",arxiv
http://arxiv.org/abs/2104.04687v2,2021-07-26T08:43:15Z,2021-04-10T05:40:42Z,"Learning from 2D: Contrastive Pixel-to-Point Knowledge Transfer for 3D
  Pretraining","Most 3D neural networks are trained from scratch owing to the lack of
large-scale labeled datasets. In this paper, we present a novel 3D pretraining
method by leveraging 2D networks learned from rich 2D datasets. We propose the
contrastive pixel-to-point knowledge transfer to effectively utilize the 2D
information by mapping the pixel-level and point-level features into the same
embedding space. Due to the heterogeneous nature between 2D and 3D networks, we
introduce the back-projection function to align the features between 2D and 3D
to make the transfer possible. Additionally, we devise an upsampling feature
projection layer to increase the spatial resolution of high-level 2D feature
maps, which helps learning fine-grained 3D representations. With a pretrained
2D network, the proposed pretraining process requires no additional 2D or 3D
labeled data, further alleviating the expansive 3D data annotation cost. To the
best of our knowledge, we are the first to exploit existing 2D trained weights
to pretrain 3D deep neural networks. Our intensive experiments show that the 3D
models pretrained with 2D knowledge boost the performances across various
real-world 3D downstream tasks.",arxiv
http://arxiv.org/abs/2111.06953v1,2021-11-12T21:38:25Z,2021-11-12T21:38:25Z,"Distributed on-line reinforcement learning in a swarm of sterically
  interacting robots","While naturally occurring swarms thrive when crowded, physical interactions
in robotic swarms are either avoided or carefully controlled, thus limiting
their operational density. Designing behavioral strategies under such
circumstances remains a challenge, even though it may offer an opportunity for
exploring morpho-functional self-organized behaviors. In this paper, we
explicitly consider dense swarms of robots where physical interactions are
inevitable. We demonstrate experimentally that an a priori minor difference in
the mechanical design of the robots leads to important differences in their
dynamical behaviors when they evolve in crowded environments. We design
Morphobots, which are Kilobots augmented with a 3D-printed exoskeleton. The
exoskeleton not only significantly improves the motility and stability of the
Kilobots, it also allows to encode physically two contrasting dynamical
behaviors in response to an external force or a collision. This difference
translates into distinct performances during self-organized aggregation when
addressing a phototactic task. Having characterized the dynamical mechanism at
the root of these differences, we implement a decentralized on-line
evolutionary reinforcement learning algorithm in a swarm of Morphobots. We
demonstrate the learning efficiency and show that the learning reduces the
dependency on the morphology. We present a kinetic model that links the reward
function to an effective phototactic policy. Our results are of relevance for
the deployment of robust swarms of robots in a real environment, where robots
are deemed to collide, and to be exposed to external forces.",arxiv
http://arxiv.org/abs/2003.03118v3,2020-07-23T17:13:23Z,2020-03-06T10:19:02Z,"Evolved Neuromorphic Control for High Speed Divergence-based Landings of
  MAVs","Flying insects are capable of vision-based navigation in cluttered
environments, reliably avoiding obstacles through fast and agile maneuvers,
while being very efficient in the processing of visual stimuli. Meanwhile,
autonomous micro air vehicles still lag far behind their biological
counterparts, displaying inferior performance at a much higher energy
consumption. In light of this, we want to mimic flying insects in terms of
their processing capabilities, and consequently show the efficiency of this
approach in the real world. This letter does so through evolving spiking neural
networks for controlling landings of micro air vehicles using optical flow
divergence from a downward-looking camera. We demonstrate that the resulting
neuromorphic controllers transfer robustly from a highly abstracted simulation
to the real world, performing fast and safe landings while keeping network
spike rate minimal. Furthermore, we provide insight into the resources required
for successfully solving the problem of divergence-based landing, showing that
high-resolution control can be learned with only a single spiking neuron. To
the best of our knowledge, this work is the first to integrate spiking neural
networks in the control loop of a real-world flying robot. Videos of the
experiments can be found at https://bit.ly/neuro-controller .",arxiv
http://arxiv.org/abs/2011.02265v1,2020-11-04T13:09:26Z,2020-11-04T13:09:26Z,"S3-Net: A Fast and Lightweight Video Scene Understanding Network by
  Single-shot Segmentation","Real-time understanding in video is crucial in various AI applications such
as autonomous driving. This work presents a fast single-shot segmentation
strategy for video scene understanding. The proposed net, called S3-Net,
quickly locates and segments target sub-scenes, meanwhile extracts structured
time-series semantic features as inputs to an LSTM-based spatio-temporal model.
Utilizing tensorization and quantization techniques, S3-Net is intended to be
lightweight for edge computing. Experiments using CityScapes, UCF11, HMDB51 and
MOMENTS datasets demonstrate that the proposed S3-Net achieves an accuracy
improvement of 8.1% versus the 3D-CNN based approach on UCF11, a storage
reduction of 6.9x and an inference speed of 22.8 FPS on CityScapes with a
GTX1080Ti GPU.",arxiv
http://arxiv.org/abs/1912.00439v3,2020-08-13T14:47:16Z,2019-12-01T16:50:07Z,"DeepC-MVS: Deep Confidence Prediction for Multi-View Stereo
  Reconstruction","Deep Neural Networks (DNNs) have the potential to improve the quality of
image-based 3D reconstructions. However, the use of DNNs in the context of 3D
reconstruction from large and high-resolution image datasets is still an open
challenge, due to memory and computational constraints. We propose a pipeline
which takes advantage of DNNs to improve the quality of 3D reconstructions
while being able to handle large and high-resolution datasets. In particular,
we propose a confidence prediction network explicitly tailored for Multi-View
Stereo (MVS) and we use it for both depth map outlier filtering and depth map
refinement within our pipeline, in order to improve the quality of the final 3D
reconstructions. We train our confidence prediction network on (semi-)dense
ground truth depth maps from publicly available real world MVS datasets. With
extensive experiments on popular benchmarks, we show that our overall pipeline
can produce state-of-the-art 3D reconstructions, both qualitatively and
quantitatively.",arxiv
http://arxiv.org/abs/1901.02495v1,2019-01-08T20:08:42Z,2019-01-08T20:08:42Z,"Presence-absence estimation in audio recordings of tropical frog
  communities","One non-invasive way to study frog communities is by analyzing long-term
samples of acoustic material containing calls. This immense task has been
optimized by the development of Machine Learning tools to extract ecological
information. We explored a likelihood-ratio audio detector based on Gaussian
mixture model classification of 10 frog species, and applied it to estimate
presence-absence in audio recordings from an actual amphibian monitoring
performed at Yasun\'i National Park in the Ecuadorian Amazonia. A modified
filter-bank was used to extract 20 cepstral features that model the spectral
content of frog calls. Experiments were carried out to investigate the
hyperparameters and the minimum frog-call time needed to train an accurate GMM
classifier. With 64 Gaussians and 12 seconds of training time, the classifier
achieved an average weighted error rate of 0.9% on the 10-fold cross-validation
for nine species classification, as compared to 3% with MFCC and 1.8% with PLP
features. For testing, 10 GMMs were trained using all the available
training-validation dataset to study 23.5 hours in 141, 10-minute long samples
of unidentified real-world audio recorded at two frog communities in 2001 with
analog equipment. To evaluate automatic presence-absence estimation, we
characterized the audio samples with 10 binary variables each corresponding to
a frog species, and manually labeled a sub-set of 18 samples using headphones.
A recall of 87.5% and precision of 100% with average accuracy of 96.66%
suggests good generalization ability of the algorithm, and provides evidence of
the validity of this approach to study real-world audio recorded in a tropical
acoustic environment. Finally, we applied the algorithm to the available
corpus, and show its potentiality to gain insights into the temporal
reproductive behavior of frogs.",arxiv
http://arxiv.org/abs/2001.01043v2,2020-04-06T03:26:24Z,2020-01-04T06:05:23Z,"SurveilEdge: Real-time Video Query based on Collaborative Cloud-Edge
  Deep Learning","The real-time query of massive surveillance video data plays a fundamental
role in various smart urban applications such as public safety and intelligent
transportation. Traditional cloud-based approaches are not applicable because
of high transmission latency and prohibitive bandwidth cost, while edge devices
are often incapable of executing complex vision algorithms with low latency and
high accuracy due to restricted resources. Given the infeasibility of both
cloud-only and edge-only solutions, we present SurveilEdge, a collaborative
cloud-edge system for real-time queries of large-scale surveillance video
streams. Specifically, we design a convolutional neural network (CNN) training
scheme to reduce the training time with high accuracy, and an intelligent task
allocator to balance the load among different computing nodes and to achieve
the latency-accuracy tradeoff for real-time queries. We implement SurveilEdge
on a prototype with multiple edge devices and a public Cloud, and conduct
extensive experiments using realworld surveillance video datasets. Evaluation
results demonstrate that SurveilEdge manages to achieve up to 7x less bandwidth
cost and 5.4x faster query response time than the cloud-only solution; and can
improve query accuracy by up to 43.9% and achieve 15.8x speedup respectively,
in comparison with edge-only approaches.",arxiv
http://arxiv.org/abs/1612.00881v2,2017-07-19T10:34:36Z,2016-12-02T22:24:24Z,"Procedural Generation of Videos to Train Deep Action Recognition
  Networks","Deep learning for human action recognition in videos is making significant
progress, but is slowed down by its dependency on expensive manual labeling of
large video collections. In this work, we investigate the generation of
synthetic training data for action recognition, as it has recently shown
promising results for a variety of other computer vision tasks. We propose an
interpretable parametric generative model of human action videos that relies on
procedural generation and other computer graphics techniques of modern game
engines. We generate a diverse, realistic, and physically plausible dataset of
human action videos, called PHAV for ""Procedural Human Action Videos"". It
contains a total of 39,982 videos, with more than 1,000 examples for each
action of 35 categories. Our approach is not limited to existing motion capture
sequences, and we procedurally define 14 synthetic actions. We introduce a deep
multi-task representation learning architecture to mix synthetic and real
videos, even if the action categories differ. Our experiments on the UCF101 and
HMDB51 benchmarks suggest that combining our large set of synthetic videos with
small real-world datasets can boost recognition performance, significantly
outperforming fine-tuning state-of-the-art unsupervised generative models of
videos.",arxiv
http://arxiv.org/abs/1907.02526v1,2019-07-03T21:25:21Z,2019-07-03T21:25:21Z,"Convolutional Neural Network-based Speech Enhancement for Cochlear
  Implant Recipients","Attempts to develop speech enhancement algorithms with improved speech
intelligibility for cochlear implant (CI) users have met with limited success.
To improve speech enhancement methods for CI users, we propose to perform
speech enhancement in a cochlear filter-bank feature space, a feature-set
specifically designed for CI users based on CI auditory stimuli. We leverage a
convolutional neural network (CNN) to extract both stationary and
non-stationary components of environmental acoustics and speech. We propose
three CNN architectures: (1) vanilla CNN that directly generates the enhanced
signal; (2) spectral-subtraction-style CNN (SS-CNN) that first predicts noise
and then generates the enhanced signal by subtracting noise from the noisy
signal; (3) Wiener-style CNN (Wiener-CNN) that generates an optimal mask for
suppressing noise. An important problem of the proposed networks is that they
introduce considerable delays, which limits their real-time application for CI
users. To address this, this study also considers causal variations of these
networks. Our experiments show that the proposed networks (both causal and
non-causal forms) achieve significant improvement over existing baseline
systems. We also found that causal Wiener-CNN outperforms other networks, and
leads to the best overall envelope coefficient measure (ECM). The proposed
algorithms represent a viable option for implementation on the CCi-MOBILE
research platform as a pre-processor for CI users in naturalistic environments.",arxiv
http://arxiv.org/abs/2012.05228v2,2021-03-06T07:22:22Z,2020-12-09T18:49:24Z,Video Deblurring by Fitting to Test Data,"Motion blur in videos captured by autonomous vehicles and robots can degrade
their perception capability. In this work, we present a novel approach to video
deblurring by fitting a deep network to the test video. Our key observation is
that some frames in a video with motion blur are much sharper than others, and
thus we can transfer the texture information in those sharp frames to blurry
frames. Our approach heuristically selects sharp frames from a video and then
trains a convolutional neural network on these sharp frames. The trained
network often absorbs enough details in the scene to perform deblurring on all
the video frames. As an internal learning method, our approach has no domain
gap between training and test data, which is a problematic issue for existing
video deblurring approaches. The conducted experiments on real-world video data
show that our model can reconstruct clearer and sharper videos than
state-of-the-art video deblurring approaches. Code and data are available at
https://github.com/xrenaa/Deblur-by-Fitting.",arxiv
http://arxiv.org/abs/1310.3322v1,2013-10-12T01:16:32Z,2013-10-12T01:16:32Z,GPU-Framework for Teamwork Action Recognition,"Real time processing for teamwork action recognition is a challenge, due to
complex computational models to achieve high system performance. Hence, this
paper proposes a framework based on Graphical Processing Units (GPUs) to
achieve a significant speed up in the performance of role based activity
recognition of teamwork. The framework can be applied in various fields,
especially athletic and military applications. Furthermore, the framework can
be customized for many action recognition applications. The paper presents the
stages of the framework where GPUs are the main tool for performance
improvement. The speedup is achieved by performing video processing and Machine
learning algorithms on GPU. Video processing and machine learning algorithms
covers all computations involved in our framework. Video processing tasks on
involves GPU implementation of Motion detection, segmentation and object
tracking algorithms. In addition, our framework is integrated with GPUCV, a GPU
version of OpenCV functions. Machine learning tasks are supported under our
framework with GPU implementations of Support Vector Machine (SVM) for object
classification and feature discretization, Hidden Marcov Model (HMM) for
activity recognition phase, and ID3 algorithm for role recognition of team
members. The system was tested against UC-Teamwork dataset and speedup of 20X
has been achieved on NVidia 9500GT graphics card (32 500MHZ processors).",arxiv
http://arxiv.org/abs/2005.07872v2,2020-08-16T08:07:49Z,2020-05-16T04:54:37Z,"Gentlemen on the Road: Understanding How Pedestrians Interpret Yielding
  Behavior of Autonomous Vehicles using Machine Learning","Autonomous vehicles (AVs) can prevent collisions by understanding pedestrian
intention. We conducted a virtual reality experiment with 39 participants and
measured crossing times (seconds) and head orientation (yaw degrees). We
manipulated AV yielding behavior (no-yield, slow-yield, and fast-yield) and the
AV size (small, medium, and large). Using machine learning approach, we
classified head orientation change of pedestrians by time into 6 clusters of
patterns. Results indicate that pedestrian head orientation change was
influenced by AV yielding behavior as well as the size of the AV. Participants
fixated on the front most of the time even when the car approached near.
Participants changed head orientation most frequently when a large size AV did
not yield (no-yield). In post-experiment interviews, participants reported that
yielding behavior and size affected their decision to cross and perceived
safety. For autonomous vehicles to be perceived more safe and trustful,
vehicle-specific factors such as size and yielding behavior should be
considered in the designing process.",arxiv
http://arxiv.org/abs/1905.05212v1,2019-05-13T18:01:01Z,2019-05-13T18:01:01Z,"Lightweight Monocular Depth Estimation Model by Joint End-to-End Filter
  pruning","Convolutional neural networks (CNNs) have emerged as the state-of-the-art in
multiple vision tasks including depth estimation. However, memory and computing
power requirements remain as challenges to be tackled in these models.
Monocular depth estimation has significant use in robotics and virtual reality
that requires deployment on low-end devices. Training a small model from
scratch results in a significant drop in accuracy and it does not benefit from
pre-trained large models. Motivated by the literature of model pruning, we
propose a lightweight monocular depth model obtained from a large trained
model. This is achieved by removing the least important features with a novel
joint end-to-end filter pruning. We propose to learn a binary mask for each
filter to decide whether to drop the filter or not. These masks are trained
jointly to exploit relations between filters at different layers as well as
redundancy within the same layer. We show that we can achieve around 5x
compression rate with small drop in accuracy on the KITTI driving dataset. We
also show that masking can improve accuracy over the baseline with fewer
parameters, even without enforcing compression loss.",arxiv
http://arxiv.org/abs/2109.07165v1,2021-09-15T09:00:56Z,2021-09-15T09:00:56Z,3D Annotation Of Arbitrary Objects In The Wild,"Recent years have produced a variety of learning based methods in the context
of computer vision and robotics. Most of the recently proposed methods are
based on deep learning, which require very large amounts of data compared to
traditional methods. The performance of the deep learning methods are largely
dependent on the data distribution they were trained on, and it is important to
use data from the robot's actual operating domain during training. Therefore,
it is not possible to rely on pre-built, generic datasets when deploying robots
in real environments, creating a need for efficient data collection and
annotation in the specific operating conditions the robots will operate in. The
challenge is then: how do we reduce the cost of obtaining such datasets to a
point where we can easily deploy our robots in new conditions, environments and
to support new sensors? As an answer to this question, we propose a data
annotation pipeline based on SLAM, 3D reconstruction, and 3D-to-2D geometry.
The pipeline allows creating 3D and 2D bounding boxes, along with per-pixel
annotations of arbitrary objects without needing accurate 3D models of the
objects prior to data collection and annotation. Our results showcase almost
90% Intersection-over-Union (IoU) agreement on both semantic segmentation and
2D bounding box detection across a variety of objects and scenes, while
speeding up the annotation process by several orders of magnitude compared to
traditional manual annotation.",arxiv
http://arxiv.org/abs/1912.04443v3,2020-06-21T20:16:28Z,2019-12-10T01:36:18Z,"AVID: Learning Multi-Stage Tasks via Pixel-Level Translation of Human
  Videos","Robotic reinforcement learning (RL) holds the promise of enabling robots to
learn complex behaviors through experience. However, realizing this promise for
long-horizon tasks in the real world requires mechanisms to reduce human burden
in terms of defining the task and scaffolding the learning process. In this
paper, we study how these challenges can be alleviated with an automated
robotic learning framework, in which multi-stage tasks are defined simply by
providing videos of a human demonstrator and then learned autonomously by the
robot from raw image observations. A central challenge in imitating human
videos is the difference in appearance between the human and robot, which
typically requires manual correspondence. We instead take an automated approach
and perform pixel-level image translation via CycleGAN to convert the human
demonstration into a video of a robot, which can then be used to construct a
reward function for a model-based RL algorithm. The robot then learns the task
one stage at a time, automatically learning how to reset each stage to retry it
multiple times without human-provided resets. This makes the learning process
largely automatic, from intuitive task specification via a video to automated
training with minimal human intervention. We demonstrate that our approach is
capable of learning complex tasks, such as operating a coffee machine, directly
from raw image observations, requiring only 20 minutes to provide human
demonstrations and about 180 minutes of robot interaction.",arxiv
http://arxiv.org/abs/2006.12349v1,2020-06-22T15:47:01Z,2020-06-22T15:47:01Z,"Tactile Perception of Objects by the User's Palm for the Development of
  Multi-contact Wearable Tactile Displays","The user's palm plays an important role in object detection and manipulation.
The design of a robust multi-contact tactile display must consider the
sensation and perception of of the stimulated area aiming to deliver the right
stimuli at the correct location. To the best of our knowledge, there is no
study to obtain the human palm data for this purpose. The objective of this
work is to introduce the method to investigate the user's palm sensations
during the interaction with objects. An array of fifteen Force Sensitive
Resistors (FSRs) was located at the user's palm to get the area of interaction,
and the normal force delivered to four different convex surfaces. Experimental
results showed the active areas at the palm during the interaction with each of
the surfaces at different forces. The obtained results can be applied in the
development of multi-contact wearable tactile and haptic displays for the palm,
and in training a machine-learning algorithm to predict stimuli aiming to
achieve a highly immersive experience in Virtual Reality.",arxiv
http://arxiv.org/abs/2108.09538v1,2021-08-21T16:26:04Z,2021-08-21T16:26:04Z,"Using Trajectory Compression Rate to Predict Changes in Cybersickness in
  Virtual Reality Games","Identifying cybersickness in virtual reality (VR) applications such as games
in a fast, precise, non-intrusive, and non-disruptive way remains challenging.
Several factors can cause cybersickness, and their identification will help
find its origins and prevent or minimize it. One such factor is virtual
movement. Movement, whether physical or virtual, can be represented in
different forms. One way to represent and store it is with a temporally
annotated point sequence. Because a sequence is memory-consuming, it is often
preferable to save it in a compressed form. Compression allows redundant data
to be eliminated while still preserving changes in speed and direction. Since
changes in direction and velocity in VR can be associated with cybersickness,
changes in compression rate can likely indicate changes in cybersickness
levels. In this research, we explore whether quantifying changes in virtual
movement can be used to estimate variation in cybersickness levels of VR users.
We investigate the correlation between changes in the compression rate of
movement data in two VR games with changes in players' cybersickness levels
captured during gameplay. Our results show (1) a clear correlation between
changes in compression rate and cybersickness, and(2) that a machine learning
approach can be used to identify these changes. Finally, results from a second
experiment show that our approach is feasible for cybersickness inference in
games and other VR applications that involve movement.",arxiv
http://arxiv.org/abs/1909.02168v2,2019-10-18T21:48:03Z,2019-09-05T00:34:33Z,Future Frame Prediction Using Convolutional VRNN for Anomaly Detection,"Anomaly detection in videos aims at reporting anything that does not conform
the normal behaviour or distribution. However, due to the sparsity of abnormal
video clips in real life, collecting annotated data for supervised learning is
exceptionally cumbersome. Inspired by the practicability of generative models
for semi-supervised learning, we propose a novel sequential generative model
based on variational autoencoder (VAE) for future frame prediction with
convolutional LSTM (ConvLSTM). To the best of our knowledge, this is the first
work that considers temporal information in future frame prediction based
anomaly detection framework from the model perspective. Our experiments
demonstrate that our approach is superior to the state-of-the-art methods on
three benchmark datasets.",arxiv
http://arxiv.org/abs/2109.14797v2,2021-10-02T00:05:08Z,2021-09-30T01:54:44Z,"Emergency Vehicles Audio Detection and Localization in Autonomous
  Driving","Emergency vehicles in service have right-of-way over all other vehicles.
Hence, all other vehicles are supposed to take proper actions to yield
emergency vehicles with active sirens. As this task requires the cooperation
between ears and eyes for human drivers, it also needs audio detection as a
supplement to vision-based algorithms for fully autonomous driving vehicles. In
urban driving scenarios, we need to know both the existence of emergency
vehicles and their relative positions to us to decide the proper actions. We
present a novel system from collecting the real-world siren data to the
deployment of models using only two cost-efficient microphones. We are able to
achieve promising performance for each task separately, especially within the
crucial 10m to 50m distance range to react (the size of our ego vehicle is
around 5m in length and 2m in width). The recall rate to determine the
existence of sirens is 99.16% , the median and mean angle absolute error is
9.64{\deg} and 19.18{\deg} respectively, and the median and mean distance
absolute error of 9.30m and 10.58m respectively within that range. We also
benchmark various machine learning approaches that can determine the siren
existence and sound source localization which includes direction and distance
simultaneously within 50ms of latency.",arxiv
http://arxiv.org/abs/1804.04118v2,2018-10-08T12:20:33Z,2018-04-11T17:55:00Z,Personalized Dynamics Models for Adaptive Assistive Navigation Systems,"Consider an assistive system that guides visually impaired users through
speech and haptic feedback to their destination. Existing robotic and
ubiquitous navigation technologies (e.g., portable, ground, or wearable
systems) often operate in a generic, user-agnostic manner. However, to minimize
confusion and navigation errors, our real-world analysis reveals a crucial need
to adapt the instructional guidance across different end-users with diverse
mobility skills. To address this practical issue in scalable system design, we
propose a novel model-based reinforcement learning framework for personalizing
the system-user interaction experience. When incrementally adapting the system
to new users, we propose to use a weighted experts model for addressing
data-efficiency limitations in transfer learning with deep models. A real-world
dataset of navigation by blind users is used to show that the proposed approach
allows for (1) more accurate long-term human behavior prediction (up to 20
seconds into the future) through improved reasoning over personal mobility
characteristics, interaction with surrounding obstacles, and the current
navigation goal, and (2) quick adaptation at the onset of learning, when data
is limited.",arxiv
http://arxiv.org/abs/2001.03855v1,2020-01-12T05:25:02Z,2020-01-12T05:25:02Z,"Hyperparameters optimization for Deep Learning based emotion prediction
  for Human Robot Interaction","To enable humanoid robots to share our social space we need to develop
technology for easy interaction with the robots using multiple modes such as
speech, gestures and share our emotions with them. We have targeted this
research towards addressing the core issue of emotion recognition problem which
would require less computation resources and much lesser number of network
hyperparameters which will be more adaptive to be computed on low resourced
social robots for real time communication. More specifically, here we have
proposed an Inception module based Convolutional Neural Network Architecture
which has achieved improved accuracy of upto 6% improvement over the existing
network architecture for emotion classification when combinedly tested over
multiple datasets when tried over humanoid robots in real - time. Our proposed
model is reducing the trainable Hyperparameters to an extent of 94% as compared
to vanilla CNN model which clearly indicates that it can be used in real time
based application such as human robot interaction. Rigorous experiments have
been performed to validate our methodology which is sufficiently robust and
could achieve high level of accuracy. Finally, the model is implemented in a
humanoid robot, NAO in real time and robustness of the model is evaluated.",arxiv
http://arxiv.org/abs/1907.11394v1,2019-07-26T06:36:18Z,2019-07-26T06:36:18Z,"A Comparative Study of High-Recall Real-Time Semantic Segmentation Based
  on Swift Factorized Network","Semantic Segmentation (SS) is the task to assign a semantic label to each
pixel of the observed images, which is of crucial significance for autonomous
vehicles, navigation assistance systems for the visually impaired, and
augmented reality devices. However, there is still a long way for SS to be put
into practice as there are two essential challenges that need to be addressed:
efficiency and evaluation criterions for practical application. For specific
application scenarios, different criterions need to be adopted. Recall rate is
an important criterion for many tasks like autonomous vehicles. For autonomous
vehicles, we need to focus on the detection of the traffic objects like cars,
buses, and pedestrians, which should be detected with high recall rates. In
other words, it is preferable to detect it wrongly than miss it, because the
other traffic objects will be dangerous if the algorithm miss them and segment
them as safe roadways. In this paper, our main goal is to explore possible
methods to attain high recall rate. Firstly, we propose a real-time SS network
named Swift Factorized Network (SFN). The proposed network is adapted from
SwiftNet, whose structure is a typical U-shape structure with lateral
connections. Inspired by ERFNet and Global convolution Networks (GCNet), we
propose two different blocks to enlarge valid receptive field. They do not take
up too much calculation resources, but significantly enhance the performance
compared with the baseline network. Secondly, we explore three ways to achieve
higher recall rate, i.e. loss function, classifier and decision rules. We
perform a comprehensive set of experiments on state-of-the-art datasets
including CamVid and Cityscapes. We demonstrate that our SS convolutional
neural networks reach excellent performance. Furthermore, we make a detailed
analysis and comparison of the three proposed methods on the promotion of
recall rate.",arxiv
http://arxiv.org/abs/2007.02351v1,2020-07-05T14:24:24Z,2020-07-05T14:24:24Z,Offline Model Guard: Secure and Private ML on Mobile Devices,"Performing machine learning tasks in mobile applications yields a challenging
conflict of interest: highly sensitive client information (e.g., speech data)
should remain private while also the intellectual property of service providers
(e.g., model parameters) must be protected. Cryptographic techniques offer
secure solutions for this, but have an unacceptable overhead and moreover
require frequent network interaction. In this work, we design a practically
efficient hardware-based solution. Specifically, we build Offline Model Guard
(OMG) to enable privacy-preserving machine learning on the predominant mobile
computing platform ARM - even in offline scenarios. By leveraging a trusted
execution environment for strict hardware-enforced isolation from other system
components, OMG guarantees privacy of client data, secrecy of provided models,
and integrity of processing algorithms. Our prototype implementation on an ARM
HiKey 960 development board performs privacy-preserving keyword recognition
using TensorFlow Lite for Microcontrollers in real time.",arxiv
http://arxiv.org/abs/2101.11451v1,2021-01-27T14:35:52Z,2021-01-27T14:35:52Z,"Controlling by Showing: i-Mimic: A Video-based Method to Control Robotic
  Arms","A novel concept of vision-based intelligent control of robotic arms is
developed here in this work. This work enables the controlling of robotic arms
motion only with visual inputs, that is, controlling by showing the videos of
correct movements. This work can broadly be sub-divided into two segments. The
first part of this work is to develop an unsupervised vision-based method to
control robotic arm in 2-D plane, and the second one is with deep CNN in the
same task in 3-D plane. The first method is unsupervised, where our aim is to
perform mimicking of human arm motion in real-time by a manipulator. We
developed a network, namely the vision-to-motion optical network (DON), where
the input should be a video stream containing hand movements of human, the the
output would be out the velocity and torque information of the hand movements
shown in the videos. The output information of the DON is then fed to the
robotic arm by enabling it to generate motion according to the real hand
videos. The method has been tested with both live-stream video feed as well as
on recorded video obtained from a monocular camera even by intelligently
predicting the trajectory of human hand hand when it gets occluded. This is why
the mimicry of the arm incorporates some intelligence to it and becomes
intelligent mimic (i-mimic). Alongside the unsupervised method another method
has also been developed deploying the deep neural network technique with CNN
(Convolutional Neural Network) to perform the mimicking, where labelled
datasets are used for training. The same dataset, as used in the unsupervised
DON-based method, is used in the deep CNN method, after manual annotations.
Both the proposed methods are validated with off-line as well as with on-line
video datasets in real-time. The entire methodology is validated with real-time
1-link and simulated n-link manipulators alongwith suitable comparisons.",arxiv
http://arxiv.org/abs/1812.01803v3,2019-04-06T04:46:52Z,2018-12-05T03:31:02Z,"ECC: Platform-Independent Energy-Constrained Deep Neural Network
  Compression via a Bilinear Regression Model","Many DNN-enabled vision applications constantly operate under severe energy
constraints such as unmanned aerial vehicles, Augmented Reality headsets, and
smartphones. Designing DNNs that can meet a stringent energy budget is becoming
increasingly important. This paper proposes ECC, a framework that compresses
DNNs to meet a given energy constraint while minimizing accuracy loss. The key
idea of ECC is to model the DNN energy consumption via a novel bilinear
regression function. The energy estimate model allows us to formulate DNN
compression as a constrained optimization that minimizes the DNN loss function
over the energy constraint. The optimization problem, however, has nontrivial
constraints. Therefore, existing deep learning solvers do not apply directly.
We propose an optimization algorithm that combines the essence of the
Alternating Direction Method of Multipliers (ADMM) framework with
gradient-based learning algorithms. The algorithm decomposes the original
constrained optimization into several subproblems that are solved iteratively
and efficiently. ECC is also portable across different hardware platforms
without requiring hardware knowledge. Experiments show that ECC achieves higher
accuracy under the same or lower energy budget compared to state-of-the-art
resource-constrained DNN compression techniques.",arxiv
http://arxiv.org/abs/1809.10239v2,2019-02-15T09:36:18Z,2018-09-20T08:13:52Z,Empty Cities: Image Inpainting for a Dynamic-Object-Invariant Space,"In this paper we present an end-to-end deep learning framework to turn images
that show dynamic content, such as vehicles or pedestrians, into realistic
static frames. This objective encounters two main challenges: detecting all the
dynamic objects, and inpainting the static occluded background with plausible
imagery. The second problem is approached with a conditional generative
adversarial model that, taking as input the original dynamic image and its
dynamic/static binary mask, is capable of generating the final static image.
The former challenge is addressed by the use of a convolutional network that
learns a multi-class semantic segmentation of the image.
  These generated images can be used for applications such as augmented reality
or vision-based robot localization purposes. To validate our approach, we show
both qualitative and quantitative comparisons against other state-of-the-art
inpainting methods by removing the dynamic objects and hallucinating the static
structure behind them. Furthermore, to demonstrate the potential of our
results, we carry out pilot experiments that show the benefits of our proposal
for visual place recognition.",arxiv
http://arxiv.org/abs/2110.10571v1,2021-10-20T14:00:20Z,2021-10-20T14:00:20Z,"CobotAR: Interaction with Robots using Omnidirectionally Projected Image
  and DNN-based Gesture Recognition","Several technological solutions supported the creation of interfaces for
Augmented Reality (AR) multi-user collaboration in the last years. However,
these technologies require the use of wearable devices. We present CobotAR - a
new AR technology to achieve the Human-Robot Interaction (HRI) by gesture
recognition based on Deep Neural Network (DNN) - without an extra wearable
device for the user. The system allows users to have a more intuitive
experience with robotic applications using just their hands. The CobotAR system
assumes the AR spatial display created by a mobile projector mounted on a 6 DoF
robot. The proposed technology suggests a novel way of interaction with
machines to achieve safe, intuitive, and immersive control mediated by a
robotic projection system and DNN-based algorithm. We conducted the experiment
with several parameters assessment during this research, which allows the users
to define the positives and negatives of the new approach. The mental demand of
CobotAR system is twice less than Wireless Gamepad and by 16\% less than Teach
Pendant.",arxiv
http://arxiv.org/abs/1910.10750v1,2019-10-23T18:16:53Z,2019-10-23T18:16:53Z,6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints,"We present 6-PACK, a deep learning approach to category-level 6D object pose
tracking on RGB-D data. Our method tracks in real-time novel object instances
of known object categories such as bowls, laptops, and mugs. 6-PACK learns to
compactly represent an object by a handful of 3D keypoints, based on which the
interframe motion of an object instance can be estimated through keypoint
matching. These keypoints are learned end-to-end without manual supervision in
order to be most effective for tracking. Our experiments show that our method
substantially outperforms existing methods on the NOCS category-level 6D pose
estimation benchmark and supports a physical robot to perform simple
vision-based closed-loop manipulation tasks. Our code and video are available
at https://sites.google.com/view/6packtracking.",arxiv
http://arxiv.org/abs/1712.04569v1,2017-12-12T23:47:53Z,2017-12-12T23:47:53Z,"Im2Pano3D: Extrapolating 360 Structure and Semantics Beyond the Field of
  View","We present Im2Pano3D, a convolutional neural network that generates a dense
prediction of 3D structure and a probability distribution of semantic labels
for a full 360 panoramic view of an indoor scene when given only a partial
observation (<= 50%) in the form of an RGB-D image. To make this possible,
Im2Pano3D leverages strong contextual priors learned from large-scale synthetic
and real-world indoor scenes. To ease the prediction of 3D structure, we
propose to parameterize 3D surfaces with their plane equations and train the
model to predict these parameters directly. To provide meaningful training
supervision, we use multiple loss functions that consider both pixel level
accuracy and global context consistency. Experiments demon- strate that
Im2Pano3D is able to predict the semantics and 3D structure of the unobserved
scene with more than 56% pixel accuracy and less than 0.52m average distance
error, which is significantly better than alternative approaches.",arxiv
http://arxiv.org/abs/2010.04595v3,2021-08-11T07:09:11Z,2020-10-09T14:21:43Z,"GRF: Learning a General Radiance Field for 3D Representation and
  Rendering","We present a simple yet powerful neural network that implicitly represents
and renders 3D objects and scenes only from 2D observations. The network models
3D geometries as a general radiance field, which takes a set of 2D images with
camera poses and intrinsics as input, constructs an internal representation for
each point of the 3D space, and then renders the corresponding appearance and
geometry of that point viewed from an arbitrary position. The key to our
approach is to learn local features for each pixel in 2D images and to then
project these features to 3D points, thus yielding general and rich point
representations. We additionally integrate an attention mechanism to aggregate
pixel features from multiple 2D views, such that visual occlusions are
implicitly taken into account. Extensive experiments demonstrate that our
method can generate high-quality and realistic novel views for novel objects,
unseen categories and challenging real-world scenes.",arxiv
http://arxiv.org/abs/1811.03711v1,2018-11-08T23:11:38Z,2018-11-08T23:11:38Z,"Benchmarking Deep Sequential Models on Volatility Predictions for
  Financial Time Series","Volatility is a quantity of measurement for the price movements of stocks or
options which indicates the uncertainty within financial markets. As an
indicator of the level of risk or the degree of variation, volatility is
important to analyse the financial market, and it is taken into consideration
in various decision-making processes in financial activities. On the other
hand, recent advancement in deep learning techniques has shown strong
capabilities in modelling sequential data, such as speech and natural language.
In this paper, we empirically study the applicability of the latest deep
structures with respect to the volatility modelling problem, through which we
aim to provide an empirical guidance for the theoretical analysis of the
marriage between deep learning techniques and financial applications in the
future. We examine both the traditional approaches and the deep sequential
models on the task of volatility prediction, including the most recent variants
of convolutional and recurrent networks, such as the dilated architecture.
Accordingly, experiments with real-world stock price datasets are performed on
a set of 1314 daily stock series for 2018 days of transaction. The evaluation
and comparison are based on the negative log likelihood (NLL) of real-world
stock price time series. The result shows that the dilated neural models,
including dilated CNN and Dilated RNN, produce most accurate estimation and
prediction, outperforming various widely-used deterministic models in the GARCH
family and several recently proposed stochastic models. In addition, the high
flexibility and rich expressive power are validated in this study.",arxiv
http://arxiv.org/abs/2105.04309v1,2021-05-10T12:43:35Z,2021-05-10T12:43:35Z,"Multi-modal Conditional Bounding Box Regression for Music Score
  Following","This paper addresses the problem of sheet-image-based on-line audio-to-score
alignment also known as score following. Drawing inspiration from object
detection, a conditional neural network architecture is proposed that directly
predicts x,y coordinates of the matching positions in a complete score sheet
image at each point in time for a given musical performance. Experiments are
conducted on a synthetic polyphonic piano benchmark dataset and the new method
is compared to several existing approaches from the literature for
sheet-image-based score following as well as an Optical Music Recognition
baseline. The proposed approach achieves new state-of-the-art results and
furthermore significantly improves the alignment performance on a set of
real-world piano recordings by applying Impulse Responses as a data
augmentation technique.",arxiv
http://arxiv.org/abs/2005.04292v2,2020-06-26T17:47:47Z,2020-05-08T21:08:58Z,"Deep Residual Network based food recognition for enhanced Augmented
  Reality application","Deep neural network based learning approaches is widely utilized for image
classification or object detection based problems with remarkable outcomes.
Realtime Object state estimation of objects can be used to track and estimate
the features that the object of the current frame possesses without causing any
significant delay and misclassification. A system that can detect the features
of such objects in the present state from camera images can be used to enhance
the application of Augmented Reality for improving user experience and
delivering information in a much perceptual way. The focus behind this paper is
to determine the most suitable model to create a low-latency assistance AR to
aid users by providing them nutritional information about the food that they
consume in order to promote healthier life choices. Hence the dataset has been
collected and acquired in such a manner, and we conduct various tests in order
to identify the most suitable DNN in terms of performance and complexity and
establish a system that renders such information realtime to the user.",arxiv
http://arxiv.org/abs/1909.10914v1,2019-09-23T09:20:44Z,2019-09-23T09:20:44Z,Sensor-Augmented Neural Adaptive Bitrate Video Streaming on UAVs,"Recent advances in unmanned aerial vehicle (UAV) technology have
revolutionized a broad class of civil and military applications. However, the
designs of wireless technologies that enable real-time streaming of
high-definition video between UAVs and ground clients present a conundrum. Most
existing adaptive bitrate (ABR) algorithms are not optimized for the
air-to-ground links, which usually fluctuate dramatically due to the dynamic
flight states of the UAV. In this paper, we present SA-ABR, a new
sensor-augmented system that generates ABR video streaming algorithms with the
assistance of various kinds of inherent sensor data that are used to pilot
UAVs. By incorporating the inherent sensor data with network observations,
SA-ABR trains a deep reinforcement learning (DRL) model to extract salient
features from the flight state information and automatically learn an ABR
algorithm to adapt to the varying UAV channel capacity through the training
process. SA-ABR does not rely on any assumptions or models about UAV's flight
states or the environment, but instead, it makes decisions by exploiting
temporal properties of past throughput through the long short-term memory
(LSTM) to adapt itself to a wide range of highly dynamic environments. We have
implemented SA-ABR in a commercial UAV and evaluated it in the wild. We compare
SA-ABR with a variety of existing state-of-the-art ABR algorithms, and the
results show that our system outperforms the best known existing ABR algorithm
by 21.4% in terms of the average quality of experience (QoE) reward.",arxiv
http://arxiv.org/abs/1804.10147v3,2019-07-09T20:20:04Z,2018-04-26T16:21:34Z,"Detection of Glottal Closure Instants from Raw Speech using
  Convolutional Neural Networks","Glottal Closure Instants (GCIs) correspond to the temporal locations of
significant excitation to the vocal tract occurring during the production of
voiced speech. GCI detection from speech signals is a well-studied problem
given its importance in speech processing. Most of the existing approaches for
GCI detection adopt a two-stage approach (i) Transformation of speech signal
into a representative signal where GCIs are localized better, (ii) extraction
of GCIs using the representative signal obtained in first stage. The former
stage is accomplished using signal processing techniques based on the
principles of speech production and the latter with heuristic-algorithms such
as dynamic-programming and peak-picking. These methods are thus task-specific
and rely on the methods used for representative signal extraction. However, in
this paper, we formulate the GCI detection problem from a representation
learning perspective where appropriate representation is implicitly learned
from the raw-speech data samples. Specifically, GCI detection is cast as a
supervised multi-task learning problem solved using a deep convolutional neural
network jointly optimizing a classification and regression cost. The learning
capability is demonstrated with several experiments on standard datasets. The
results compare well with the state-of-the-art algorithms while performing
better in the case of presence of real-world non-stationary noise.",arxiv
http://arxiv.org/abs/1909.11886v1,2019-09-26T04:38:01Z,2019-09-26T04:38:01Z,"Self-Adaptive Soft Voice Activity Detection using Deep Neural Networks
  for Robust Speaker Verification","Voice activity detection (VAD), which classifies frames as speech or
non-speech, is an important module in many speech applications including
speaker verification. In this paper, we propose a novel method, called
self-adaptive soft VAD, to incorporate a deep neural network (DNN)-based VAD
into a deep speaker embedding system. The proposed method is a combination of
the following two approaches. The first approach is soft VAD, which performs a
soft selection of frame-level features extracted from a speaker feature
extractor. The frame-level features are weighted by their corresponding speech
posteriors estimated from the DNN-based VAD, and then aggregated to generate a
speaker embedding. The second approach is self-adaptive VAD, which fine-tunes
the pre-trained VAD on the speaker verification data to reduce the domain
mismatch. Here, we introduce two unsupervised domain adaptation (DA) schemes,
namely speech posterior-based DA (SP-DA) and joint learning-based DA (JL-DA).
Experiments on a Korean speech database demonstrate that the verification
performance is improved significantly in real-world environments by using
self-adaptive soft VAD.",arxiv
http://arxiv.org/abs/1910.08685v1,2019-10-19T03:12:26Z,2019-10-19T03:12:26Z,Real-Time Lip Sync for Live 2D Animation,"The emergence of commercial tools for real-time performance-based 2D
animation has enabled 2D characters to appear on live broadcasts and streaming
platforms. A key requirement for live animation is fast and accurate lip sync
that allows characters to respond naturally to other actors or the audience
through the voice of a human performer. In this work, we present a deep
learning based interactive system that automatically generates live lip sync
for layered 2D characters using a Long Short Term Memory (LSTM) model. Our
system takes streaming audio as input and produces viseme sequences with less
than 200ms of latency (including processing time). Our contributions include
specific design decisions for our feature definition and LSTM configuration
that provide a small but useful amount of lookahead to produce accurate lip
sync. We also describe a data augmentation procedure that allows us to achieve
good results with a very small amount of hand-animated training data (13-20
minutes). Extensive human judgement experiments show that our results are
preferred over several competing methods, including those that only support
offline (non-live) processing. Video summary and supplementary results at
GitHub link: https://github.com/deepalianeja/CharacterLipSync2D",arxiv
http://arxiv.org/abs/2008.08198v2,2021-06-02T20:10:12Z,2020-08-18T23:57:07Z,BraggNN: Fast X-ray Bragg Peak Analysis Using Deep Learning,"X-ray diffraction based microscopy techniques such as High Energy Diffraction
Microscopy rely on knowledge of the position of diffraction peaks with high
precision. These positions are typically computed by fitting the observed
intensities in area detector data to a theoretical peak shape such as
pseudo-Voigt. As experiments become more complex and detector technologies
evolve, the computational cost of such peak detection and shape fitting becomes
the biggest hurdle to the rapid analysis required for real-time feedback during
in-situ experiments. To this end, we propose BraggNN, a deep learning-based
method that can determine peak positions much more rapidly than conventional
pseudo-Voigt peak fitting. When applied to a test dataset, BraggNN gives errors
of less than 0.29 and 0.57 pixels, relative to the conventional method, for 75%
and 95% of the peaks, respectively. When applied to a real experimental
dataset, a 3D reconstruction that used peak positions computed by BraggNN
yields 15% better results on average as compared to a reconstruction obtained
using peak positions determined using conventional 2D pseudo-Voigt fitting.
Recent advances in deep learning method implementations and special-purpose
model inference accelerators allow BraggNN to deliver enormous performance
improvements relative to the conventional method, running, for example, more
than 200 times faster than a conventional method on a consumer-class GPU card
with out-of-the-box software.",arxiv
http://arxiv.org/abs/2102.07330v1,2021-02-15T04:03:53Z,2021-02-15T04:03:53Z,"A Modulation-Domain Loss for Neural-Network-based Real-time Speech
  Enhancement","We describe a modulation-domain loss function for deep-learning-based speech
enhancement systems. Learnable spectro-temporal receptive fields (STRFs) were
adapted to optimize for a speaker identification task. The learned STRFs were
then used to calculate a weighted mean-squared error (MSE) in the modulation
domain for training a speech enhancement system. Experiments showed that adding
the modulation-domain MSE to the MSE in the spectro-temporal domain
substantially improved the objective prediction of speech quality and
intelligibility for real-time speech enhancement systems without incurring
additional computation during inference.",arxiv
http://arxiv.org/abs/2007.07577v1,2020-07-15T09:52:35Z,2020-07-15T09:52:35Z,"CycAs: Self-supervised Cycle Association for Learning Re-identifiable
  Descriptions","This paper proposes a self-supervised learning method for the person
re-identification (re-ID) problem, where existing unsupervised methods usually
rely on pseudo labels, such as those from video tracklets or clustering. A
potential drawback of using pseudo labels is that errors may accumulate and it
is challenging to estimate the number of pseudo IDs. We introduce a different
unsupervised method that allows us to learn pedestrian embeddings from raw
videos, without resorting to pseudo labels. The goal is to construct a
self-supervised pretext task that matches the person re-ID objective. Inspired
by the \emph{data association} concept in multi-object tracking, we propose the
\textbf{Cyc}le \textbf{As}sociation (\textbf{CycAs}) task: after performing
data association between a pair of video frames forward and then backward, a
pedestrian instance is supposed to be associated to itself. To fulfill this
goal, the model must learn a meaningful representation that can well describe
correspondences between instances in frame pairs. We adapt the discrete
association process to a differentiable form, such that end-to-end training
becomes feasible. Experiments are conducted in two aspects: We first compare
our method with existing unsupervised re-ID methods on seven benchmarks and
demonstrate CycAs' superiority. Then, to further validate the practical value
of CycAs in real-world applications, we perform training on self-collected
videos and report promising performance on standard test sets.",arxiv
http://arxiv.org/abs/1906.07372v4,2020-07-21T18:35:49Z,2019-06-18T04:33:51Z,"RIDM: Reinforced Inverse Dynamics Modeling for Learning from a Single
  Observed Demonstration","Augmenting reinforcement learning with imitation learning is often hailed as
a method by which to improve upon learning from scratch. However, most existing
methods for integrating these two techniques are subject to several strong
assumptions---chief among them that information about demonstrator actions is
available. In this paper, we investigate the extent to which this assumption is
necessary by introducing and evaluating reinforced inverse dynamics modeling
(RIDM), a novel paradigm for combining imitation from observation (IfO) and
reinforcement learning with no dependence on demonstrator action information.
Moreover, RIDM requires only a single demonstration trajectory and is able to
operate directly on raw (unaugmented) state features. We find experimentally
that RIDM performs favorably compared to a baseline approach for several tasks
in simulation as well as for tasks on a real UR5 robot arm. Experiment videos
can be found at https://sites.google.com/view/ridm-reinforced-inverse-dynami.",arxiv
http://arxiv.org/abs/2106.01674v1,2021-06-03T08:23:24Z,2021-06-03T08:23:24Z,"JIZHI: A Fast and Cost-Effective Model-As-A-Service System for Web-Scale
  Online Inference at Baidu","In modern internet industries, deep learning based recommender systems have
became an indispensable building block for a wide spectrum of applications,
such as search engine, news feed, and short video clips. However, it remains
challenging to carry the well-trained deep models for online real-time
inference serving, with respect to the time-varying web-scale traffics from
billions of users, in a cost-effective manner. In this work, we present JIZHI -
a Model-as-a-Service system - that per second handles hundreds of millions of
online inference requests to huge deep models with more than trillions of
sparse parameters, for over twenty real-time recommendation services at Baidu,
Inc. In JIZHI, the inference workflow of every recommendation request is
transformed to a Staged Event-Driven Pipeline (SEDP), where each node in the
pipeline refers to a staged computation or I/O intensive task processor. With
traffics of real-time inference requests arrived, each modularized processor
can be run in a fully asynchronized way and managed separately. Besides, JIZHI
introduces heterogeneous and hierarchical storage to further accelerate the
online inference process by reducing unnecessary computations and potential
data access latency induced by ultra-sparse model parameters. Moreover, an
intelligent resource manager has been deployed to maximize the throughput of
JIZHI over the shared infrastructure by searching the optimal resource
allocation plan from historical logs and fine-tuning the load shedding policies
over intermediate system feedback. Extensive experiments have been done to
demonstrate the advantages of JIZHI from the perspectives of end-to-end service
latency, system-wide throughput, and resource consumption. JIZHI has helped
Baidu saved more than ten million US dollars in hardware and utility costs
while handling 200% more traffics without sacrificing inference efficiency.",arxiv
http://arxiv.org/abs/1801.10281v1,2018-01-31T02:35:30Z,2018-01-31T02:35:30Z,Learning Video-Story Composition via Recurrent Neural Network,"In this paper, we propose a learning-based method to compose a video-story
from a group of video clips that describe an activity or experience. We learn
the coherence between video clips from real videos via the Recurrent Neural
Network (RNN) that jointly incorporates the spatial-temporal semantics and
motion dynamics to generate smooth and relevant compositions. We further
rearrange the results generated by the RNN to make the overall video-story
compatible with the storyline structure via a submodular ranking optimization
process. Experimental results on the video-story dataset show that the proposed
algorithm outperforms the state-of-the-art approach.",arxiv
http://arxiv.org/abs/2102.07764v2,2021-02-17T18:56:39Z,2021-02-15T18:59:07Z,End-to-End Egospheric Spatial Memory,"Spatial memory, or the ability to remember and recall specific locations and
objects, is central to autonomous agents' ability to carry out tasks in real
environments. However, most existing artificial memory modules are not very
adept at storing spatial information. We propose a parameter-free module,
Egospheric Spatial Memory (ESM), which encodes the memory in an ego-sphere
around the agent, enabling expressive 3D representations. ESM can be trained
end-to-end via either imitation or reinforcement learning, and improves both
training efficiency and final performance against other memory baselines on
both drone and manipulator visuomotor control tasks. The explicit egocentric
geometry also enables us to seamlessly combine the learned controller with
other non-learned modalities, such as local obstacle avoidance. We further show
applications to semantic segmentation on the ScanNet dataset, where ESM
naturally combines image-level and map-level inference modalities. Through our
broad set of experiments, we show that ESM provides a general computation graph
for embodied spatial reasoning, and the module forms a bridge between real-time
mapping systems and differentiable memory architectures. Implementation at:
https://github.com/ivy-dl/memory.",arxiv
http://arxiv.org/abs/1902.03722v1,2019-02-11T04:20:20Z,2019-02-11T04:20:20Z,"A Minimal Template for Interactive Web-based Demonstrations of Musical
  Machine Learning","New machine learning algorithms are being developed to solve problems in
different areas, including music. Intuitive, accessible, and understandable
demonstrations of the newly built models could help attract the attention of
people from different disciplines and evoke discussions. However, we notice
that it has not been a common practice for researchers working on musical
machine learning to demonstrate their models in an interactive way. To address
this issue, we present in this paper an template that is specifically designed
to demonstrate symbolic musical machine learning models on the web. The
template comes with a small codebase, is open source, and is meant to be easy
to use by any practitioners to implement their own demonstrations. Moreover,
its modular design facilitates the reuse of the musical components and
accelerates the implementation. We use the template to build interactive
demonstrations of four exemplary music generation models. We show that the
built-in interactivity and real-time audio rendering of the browser make the
demonstration easier to understand and to play with. It also helps researchers
to gain insights into different models and to A/B test them.",arxiv
http://arxiv.org/abs/1705.02514v2,2017-10-31T01:58:50Z,2017-05-06T18:27:09Z,End-to-end Source Separation with Adaptive Front-Ends,"Source separation and other audio applications have traditionally relied on
the use of short-time Fourier transforms as a front-end frequency domain
representation step. The unavailability of a neural network equivalent to
forward and inverse transforms hinders the implementation of end-to-end
learning systems for these applications. We present an auto-encoder neural
network that can act as an equivalent to short-time front-end transforms. We
demonstrate the ability of the network to learn optimal, real-valued basis
functions directly from the raw waveform of a signal and further show how it
can be used as an adaptive front-end for supervised source separation. In terms
of separation performance, these transforms significantly outperform their
Fourier counterparts. Finally, we also propose a novel source to distortion
ratio based cost function for end-to-end source separation.",arxiv
http://arxiv.org/abs/2007.04480v1,2020-07-08T23:58:41Z,2020-07-08T23:58:41Z,Automatic Probe Movement Guidance for Freehand Obstetric Ultrasound,"We present the first system that provides real-time probe movement guidance
for acquiring standard planes in routine freehand obstetric ultrasound
scanning. Such a system can contribute to the worldwide deployment of obstetric
ultrasound scanning by lowering the required level of operator expertise. The
system employs an artificial neural network that receives the ultrasound video
signal and the motion signal of an inertial measurement unit (IMU) that is
attached to the probe, and predicts a guidance signal. The network termed
US-GuideNet predicts either the movement towards the standard plane position
(goal prediction), or the next movement that an expert sonographer would
perform (action prediction). While existing models for other ultrasound
applications are trained with simulations or phantoms, we train our model with
real-world ultrasound video and probe motion data from 464 routine clinical
scans by 17 accredited sonographers. Evaluations for 3 standard plane types
show that the model provides a useful guidance signal with an accuracy of 88.8%
for goal prediction and 90.9% for action prediction.",arxiv
http://arxiv.org/abs/1705.03428v1,2017-05-09T16:59:41Z,2017-05-09T16:59:41Z,Deep Projective 3D Semantic Segmentation,"Semantic segmentation of 3D point clouds is a challenging problem with
numerous real-world applications. While deep learning has revolutionized the
field of image semantic segmentation, its impact on point cloud data has been
limited so far. Recent attempts, based on 3D deep learning approaches
(3D-CNNs), have achieved below-expected results. Such methods require
voxelizations of the underlying point cloud data, leading to decreased spatial
resolution and increased memory consumption. Additionally, 3D-CNNs greatly
suffer from the limited availability of annotated datasets.
  In this paper, we propose an alternative framework that avoids the
limitations of 3D-CNNs. Instead of directly solving the problem in 3D, we first
project the point cloud onto a set of synthetic 2D-images. These images are
then used as input to a 2D-CNN, designed for semantic segmentation. Finally,
the obtained prediction scores are re-projected to the point cloud to obtain
the segmentation results. We further investigate the impact of multiple
modalities, such as color, depth and surface normals, in a multi-stream network
architecture. Experiments are performed on the recent Semantic3D dataset. Our
approach sets a new state-of-the-art by achieving a relative gain of 7.9 %,
compared to the previous best approach.",arxiv
http://arxiv.org/abs/1805.03994v2,2018-05-29T16:28:18Z,2018-05-10T14:19:50Z,"Multi-View Semantic Labeling of 3D Point Clouds for Automated Plant
  Phenotyping","Semantic labeling of 3D point clouds is important for the derivation of 3D
models from real world scenarios in several economic fields such as building
industry, facility management, town planning or heritage conservation. In
contrast to these most common applications, we describe in this study the
semantic labeling of 3D point clouds derived from plant organs by
high-precision scanning. Our approach is optimized for the task of plant
phenotyping with its very specific challenges and is employing a deep learning
framework. Thereby, we report important experiences concerning detailed
parameter initialization and optimization techniques. By evaluating our
approach with challenging datasets we achieve state-of-the-art results without
difficult and time consuming feature engineering as being necessary in
traditional approaches to semantic labeling.",arxiv
http://arxiv.org/abs/2111.00316v1,2021-10-30T19:24:57Z,2021-10-30T19:24:57Z,"Real-time Speaker counting in a cocktail party scenario using
  Attention-guided Convolutional Neural Network","Most current speech technology systems are designed to operate well even in
the presence of multiple active speakers. However, most solutions assume that
the number of co-current speakers is known. Unfortunately, this information
might not always be available in real-world applications. In this study, we
propose a real-time, single-channel attention-guided Convolutional Neural
Network (CNN) to estimate the number of active speakers in overlapping speech.
The proposed system extracts higher-level information from the speech spectral
content using a CNN model. Next, the attention mechanism summarizes the
extracted information into a compact feature vector without losing critical
information. Finally, the active speakers are classified using a fully
connected network. Experiments on simulated overlapping speech using WSJ corpus
show that the attention solution is shown to improve the performance by almost
3% absolute over conventional temporal average pooling. The proposed
Attention-guided CNN achieves 76.15% for both Weighted Accuracy and average
Recall, and 75.80% Precision on speech segments as short as 20 frames (i.e.,
200 ms). All the classification metrics exceed 92% for the attention-guided
model in offline scenarios where the input signal is more than 100 frames long
(i.e., 1s).",arxiv
http://arxiv.org/abs/2008.11887v1,2020-08-27T02:14:15Z,2020-08-27T02:14:15Z,"A Self-Reasoning Framework for Anomaly Detection Using Video-Level
  Labels","Anomalous event detection in surveillance videos is a challenging and
practical research problem among image and video processing community. Compared
to the frame-level annotations of anomalous events, obtaining video-level
annotations is quite fast and cheap though such high-level labels may contain
significant noise. More specifically, an anomalous labeled video may actually
contain anomaly only in a short duration while the rest of the video frames may
be normal. In the current work, we propose a weakly supervised anomaly
detection framework based on deep neural networks which is trained in a
self-reasoning fashion using only video-level labels. To carry out the
self-reasoning based training, we generate pseudo labels by using binary
clustering of spatio-temporal video features which helps in mitigating the
noise present in the labels of anomalous videos. Our proposed formulation
encourages both the main network and the clustering to complement each other in
achieving the goal of more accurate anomaly detection. The proposed framework
has been evaluated on publicly available real-world anomaly detection datasets
including UCF-crime, ShanghaiTech and UCSD Ped2. The experiments demonstrate
superiority of our proposed framework over the current state-of-the-art
methods.",arxiv
http://arxiv.org/abs/1604.07322v2,2016-04-27T06:16:40Z,2016-04-25T16:34:17Z,Predictive No-Reference Assessment of Video Quality,"Among the various means to evaluate the quality of video streams,
No-Reference (NR) methods have low computation and may be executed on thin
clients. Thus, NR algorithms would be perfect candidates in cases of real-time
quality assessment, automated quality control and, particularly, in adaptive
mobile streaming. Yet, existing NR approaches are often inaccurate, in
comparison to Full-Reference (FR) algorithms, especially under lossy network
conditions. In this work, we present an NR method that combines machine
learning with simple NR metrics to achieve a quality index comparably as
accurate as the Video Quality Metric (VQM) Full-Reference algorithm. Our method
is tested in an extensive dataset (960 videos), under lossy network conditions
and considering nine different machine learning algorithms. Overall, we achieve
an over 97% correlation with VQM, while allowing real-time assessment of video
quality of experience in realistic streaming scenarios.",arxiv
http://arxiv.org/abs/2107.12563v1,2021-07-27T02:50:46Z,2021-07-27T02:50:46Z,Parallel Detection for Efficient Video Analytics at the Edge,"Deep Neural Network (DNN) trained object detectors are widely deployed in
many mission-critical systems for real time video analytics at the edge, such
as autonomous driving and video surveillance. A common performance requirement
in these mission-critical edge services is the near real-time latency of online
object detection on edge devices. However, even with well-trained DNN object
detectors, the online detection quality at edge may deteriorate for a number of
reasons, such as limited capacity to run DNN object detection models on
heterogeneous edge devices, and detection quality degradation due to random
frame dropping when the detection processing rate is significantly slower than
the incoming video frame rate. This paper addresses these problems by
exploiting multi-model multi-device detection parallelism for fast object
detection in edge systems with heterogeneous edge devices. First, we analyze
the performance bottleneck of running a well-trained DNN model at edge for real
time online object detection. We use the offline detection as a reference
model, and examine the root cause by analyzing the mismatch among the incoming
video streaming rate, video processing rate for object detection, and output
rate for real time detection visualization of video streaming. Second, we study
performance optimizations by exploiting multi-model detection parallelism. We
show that the model-parallel detection approach can effectively speed up the
FPS detection processing rate, minimizing the FPS disparity with the incoming
video frame rate on heterogeneous edge devices. We evaluate the proposed
approach using SSD300 and YOLOv3 on benchmark videos of different video stream
rates. The results show that exploiting multi-model detection parallelism can
speed up the online object detection processing rate and deliver near real-time
object detection performance for efficient video analytics at edge.",arxiv
http://arxiv.org/abs/2008.04594v1,2020-08-11T09:13:54Z,2020-08-11T09:13:54Z,Multi-modal segmentation of 3D brain scans using neural networks,"Purpose: To implement a brain segmentation pipeline based on convolutional
neural networks, which rapidly segments 3D volumes into 27 anatomical
structures. To provide an extensive, comparative study of segmentation
performance on various contrasts of magnetic resonance imaging (MRI) and
computed tomography (CT) scans. Methods: Deep convolutional neural networks are
trained to segment 3D MRI (MPRAGE, DWI, FLAIR) and CT scans. A large database
of in total 851 MRI/CT scans is used for neural network training. Training
labels are obtained on the MPRAGE contrast and coregistered to the other
imaging modalities. The segmentation quality is quantified using the Dice
metric for a total of 27 anatomical structures. Dropout sampling is implemented
to identify corrupted input scans or low-quality segmentations. Full
segmentation of 3D volumes with more than 2 million voxels is obtained in less
than 1s of processing time on a graphical processing unit. Results: The best
average Dice score is found on $T_1$-weighted MPRAGE ($85.3\pm4.6\,\%$).
However, for FLAIR ($80.0\pm7.1\,\%$), DWI ($78.2\pm7.9\,\%$) and CT ($79.1\pm
7.9\,\%$), good-quality segmentation is feasible for most anatomical
structures. Corrupted input volumes or low-quality segmentations can be
detected using dropout sampling. Conclusion: The flexibility and performance of
deep convolutional neural networks enables the direct, real-time segmentation
of FLAIR, DWI and CT scans without requiring $T_1$-weighted scans.",arxiv
http://arxiv.org/abs/1807.01866v3,2021-08-26T09:51:33Z,2018-07-05T06:58:13Z,Multi-Task Trust Transfer for Human-Robot Interaction,"Trust is essential in shaping human interactions with one another and with
robots. This paper discusses how human trust in robot capabilities transfers
across multiple tasks. We first present a human-subject study of two distinct
task domains: a Fetch robot performing household tasks and a virtual reality
simulation of an autonomous vehicle performing driving and parking maneuvers.
The findings expand our understanding of trust and inspire new differentiable
models of trust evolution and transfer via latent task representations: (i) a
rational Bayes model, (ii) a data-driven neural network model, and (iii) a
hybrid model that combines the two. Experiments show that the proposed models
outperform prevailing models when predicting trust over unseen tasks and users.
These results suggest that (i) task-dependent functional trust models capture
human trust in robot capabilities more accurately, and (ii) trust transfer
across tasks can be inferred to a good degree. The latter enables
trust-mediated robot decision-making for fluent human-robot interaction in
multi-task settings.",arxiv
http://arxiv.org/abs/1812.05785v1,2018-12-14T05:16:03Z,2018-12-14T05:16:03Z,Deep Active Learning for Video-based Person Re-identification,"It is prohibitively expensive to annotate a large-scale video-based person
re-identification (re-ID) dataset, which makes fully supervised methods
inapplicable to real-world deployment. How to maximally reduce the annotation
cost while retaining the re-ID performance becomes an interesting problem. In
this paper, we address this problem by integrating an active learning scheme
into a deep learning framework. Noticing that the truly matched tracklet-pairs,
also denoted as true positives (TP), are the most informative samples for our
re-ID model, we propose a sampling criterion to choose the most TP-likely
tracklet-pairs for annotation. A view-aware sampling strategy considering
view-specific biases is designed to facilitate candidate selection, followed by
an adaptive resampling step to leave out the selected candidates that are
unnecessary to annotate. Our method learns the re-ID model and updates the
annotation set iteratively. The re-ID model is supervised by the tracklets'
pesudo labels that are initialized by treating each tracklet as a distinct
class. With the gained annotations of the actively selected candidates, the
tracklets' pesudo labels are updated by label merging and further used to
re-train our re-ID model. While being simple, the proposed method demonstrates
its effectiveness on three video-based person re-ID datasets. Experimental
results show that less than 3\% pairwise annotations are needed for our method
to reach comparable performance with the fully-supervised setting.",arxiv
http://arxiv.org/abs/2001.06291v1,2020-01-16T06:27:59Z,2020-01-16T06:27:59Z,Predicting the Physical Dynamics of Unseen 3D Objects,"Machines that can predict the effect of physical interactions on the dynamics
of previously unseen object instances are important for creating better robots
and interactive virtual worlds. In this work, we focus on predicting the
dynamics of 3D objects on a plane that have just been subjected to an impulsive
force. In particular, we predict the changes in state - 3D position, rotation,
velocities, and stability. Different from previous work, our approach can
generalize dynamics predictions to object shapes and initial conditions that
were unseen during training. Our method takes the 3D object's shape as a point
cloud and its initial linear and angular velocities as input. We extract shape
features and use a recurrent neural network to predict the full change in state
at each time step. Our model can support training with data from both a physics
engine or the real world. Experiments show that we can accurately predict the
changes in state for unseen object geometries and initial conditions.",arxiv
http://arxiv.org/abs/1612.07857v2,2020-07-05T01:19:57Z,2016-12-23T02:28:04Z,"Human Action Attribute Learning From Video Data Using Low-Rank
  Representations","Representation of human actions as a sequence of human body movements or
action attributes enables the development of models for human activity
recognition and summarization. We present an extension of the low-rank
representation (LRR) model, termed the clustering-aware structure-constrained
low-rank representation (CS-LRR) model, for unsupervised learning of human
action attributes from video data. Our model is based on the union-of-subspaces
(UoS) framework, and integrates spectral clustering into the LRR optimization
problem for better subspace clustering results. We lay out an efficient linear
alternating direction method to solve the CS-LRR optimization problem. We also
introduce a hierarchical subspace clustering approach, termed hierarchical
CS-LRR, to learn the attributes without the need for a priori specification of
their number. By visualizing and labeling these action attributes, the
hierarchical model can be used to semantically summarize long video sequences
of human actions at multiple resolutions. A human action or activity can also
be uniquely represented as a sequence of transitions from one action attribute
to another, which can then be used for human action recognition. We demonstrate
the effectiveness of the proposed model for semantic summarization and action
recognition through comprehensive experiments on five real-world human action
datasets.",arxiv
http://arxiv.org/abs/2008.04848v1,2020-08-11T16:47:02Z,2020-08-11T16:47:02Z,Exposing Deep-faked Videos by Anomalous Co-motion Pattern Detection,"Recent deep learning based video synthesis approaches, in particular with
applications that can forge identities such as ""DeepFake"", have raised great
security concerns. Therefore, corresponding deep forensic methods are proposed
to tackle this problem. However, existing methods are either based on
unexplainable deep networks which greatly degrades the principal
interpretability factor to media forensic, or rely on fragile image statistics
such as noise pattern, which in real-world scenarios can be easily deteriorated
by data compression. In this paper, we propose an fully-interpretable video
forensic method that is designed specifically to expose deep-faked videos. To
enhance generalizability on videos with various content, we model the temporal
motion of multiple specific spatial locations in the videos to extract a robust
and reliable representation, called Co-Motion Pattern. Such kind of conjoint
pattern is mined across local motion features which is independent of the video
contents so that the instance-wise variation can also be largely alleviated.
More importantly, our proposed co-motion pattern possesses both superior
interpretability and sufficient robustness against data compression for
deep-faked videos. We conduct extensive experiments to empirically demonstrate
the superiority and effectiveness of our approach under both classification and
anomaly detection evaluation settings against the state-of-the-art deep
forensic methods.",arxiv
http://arxiv.org/abs/1907.10493v1,2019-07-24T14:58:24Z,2019-07-24T14:58:24Z,"Learning Wi-Fi Connection Loss Predictions for Seamless Vertical
  Handovers Using Multipath TCP","We present a novel data-driven approach to perform smooth Wi-Fi/cellular
handovers on smartphones. Our approach relies on data provided by multiple
smartphone sensors (e.g., Wi-Fi RSSI, acceleration, compass, step counter, air
pressure) to predict Wi-Fi connection loss and uses Multipath TCP to
dynamically switch between different connectivity modes. We train a random
forest classifier and an artificial neural network on real-world sensor data
collected by five smartphone users over a period of three months. The trained
models are executed on smartphones to reliably predict Wi-Fi connection loss 15
seconds ahead of time, with a precision of up to 0.97 and a recall of up to
0.98. Furthermore, we present results for four DASH video streaming experiments
that run on a Nexus 5 smartphone using available Wi-Fi/cellular networks. The
neural network predictions for Wi-Fi connection loss are used to establish
MPTCP subflows on the cellular link. The experiments show that our approach
provides seamless wireless connectivity, improves quality of experience of DASH
video streaming, and requires less cellular data compared to handover
approaches without Wi-Fi connection loss predictions.",arxiv
http://arxiv.org/abs/2004.03706v2,2020-10-19T11:22:02Z,2020-04-07T20:57:44Z,Long-Tailed Recognition Using Class-Balanced Experts,"Deep learning enables impressive performance in image recognition using
large-scale artificially-balanced datasets. However, real-world datasets
exhibit highly class-imbalanced distributions, yielding two main challenges:
relative imbalance amongst the classes and data scarcity for mediumshot or
fewshot classes. In this work, we address the problem of long-tailed
recognition wherein the training set is highly imbalanced and the test set is
kept balanced. Differently from existing paradigms relying on data-resampling,
cost-sensitive learning, online hard example mining, loss objective reshaping,
and/or memory-based modeling, we propose an ensemble of class-balanced experts
that combines the strength of diverse classifiers. Our ensemble of
class-balanced experts reaches results close to state-of-the-art and an
extended ensemble establishes a new state-of-the-art on two benchmarks for
long-tailed recognition. We conduct extensive experiments to analyse the
performance of the ensembles, and discover that in modern large-scale datasets,
relative imbalance is a harder problem than data scarcity. The training and
evaluation code is available at
https://github.com/ssfootball04/class-balanced-experts.",arxiv
http://arxiv.org/abs/2007.00493v1,2020-07-01T13:50:42Z,2020-07-01T13:50:42Z,"Optimisation of the PointPillars network for 3D object detection in
  point clouds","In this paper we present our research on the optimisation of a deep neural
network for 3D object detection in a point cloud. Techniques like quantisation
and pruning available in the Brevitas and PyTorch tools were used. We performed
the experiments for the PointPillars network, which offers a reasonable
compromise between detection accuracy and calculation complexity. The aim of
this work was to propose a variant of the network which we will ultimately
implement in an FPGA device. This will allow for real-time LiDAR data
processing with low energy consumption. The obtained results indicate that even
a significant quantisation from 32-bit floating point to 2-bit integer in the
main part of the algorithm, results in 5%-9% decrease of the detection
accuracy, while allowing for almost a 16-fold reduction in size of the model.",arxiv
http://arxiv.org/abs/1908.02270v2,2019-12-23T07:54:23Z,2019-08-06T17:48:46Z,Comyco: Quality-Aware Adaptive Video Streaming via Imitation Learning,"Learning-based Adaptive Bit Rate~(ABR) method, aiming to learn outstanding
strategies without any presumptions, has become one of the research hotspots
for adaptive streaming. However, it typically suffers from several issues,
i.e., low sample efficiency and lack of awareness of the video quality
information. In this paper, we propose Comyco, a video quality-aware ABR
approach that enormously improves the learning-based methods by tackling the
above issues. Comyco trains the policy via imitating expert trajectories given
by the instant solver, which can not only avoid redundant exploration but also
make better use of the collected samples. Meanwhile, Comyco attempts to pick
the chunk with higher perceptual video qualities rather than video bitrates. To
achieve this, we construct Comyco's neural network architecture, video datasets
and QoE metrics with video quality features. Using trace-driven and real-world
experiments, we demonstrate significant improvements of Comyco's sample
efficiency in comparison to prior work, with 1700x improvements in terms of the
number of samples required and 16x improvements on training time required.
Moreover, results illustrate that Comyco outperforms previously proposed
methods, with the improvements on average QoE of 7.5% - 16.79%. Especially,
Comyco also surpasses state-of-the-art approach Pensieve by 7.37% on average
video quality under the same rebuffering time.",arxiv
http://arxiv.org/abs/1707.08101v2,2018-02-05T18:42:35Z,2017-07-25T17:36:36Z,Learning to Singulate Objects using a Push Proposal Network,"Learning to act in unstructured environments, such as cluttered piles of
objects, poses a substantial challenge for manipulation robots. We present a
novel neural network-based approach that separates unknown objects in clutter
by selecting favourable push actions. Our network is trained from data
collected through autonomous interaction of a PR2 robot with randomly organized
tabletop scenes. The model is designed to propose meaningful push actions based
on over-segmented RGB-D images. We evaluate our approach by singulating up to 8
unknown objects in clutter. We demonstrate that our method enables the robot to
perform the task with a high success rate and a low number of required push
actions. Our results based on real-world experiments show that our network is
able to generalize to novel objects of various sizes and shapes, as well as to
arbitrary object configurations. Videos of our experiments can be viewed at
http://robotpush.cs.uni-freiburg.de",arxiv
http://arxiv.org/abs/1904.12618v1,2019-04-19T10:48:18Z,2019-04-19T10:48:18Z,Deep Learning Based Automatic Video Annotation Tool for Self-Driving Car,"In a self-driving car, objection detection, object classification, lane
detection and object tracking are considered to be the crucial modules. In
recent times, using the real time video one wants to narrate the scene captured
by the camera fitted in our vehicle. To effectively implement this task, deep
learning techniques and automatic video annotation tools are widely used. In
the present paper, we compare the various techniques that are available for
each module and choose the best algorithm among them by using appropriate
metrics. For object detection, YOLO and Retinanet-50 are considered and the
best one is chosen based on mean Average Precision (mAP). For object
classification, we consider VGG-19 and Resnet-50 and select the best algorithm
based on low error rate and good accuracy. For lane detection, Udacity's
'Finding Lane Line' and deep learning based LaneNet algorithms are compared and
the best one that can accurately identify the given lane is chosen for
implementation. As far as object tracking is concerned, we compare Udacity's
'Object Detection and Tracking' algorithm and deep learning based Deep Sort
algorithm. Based on the accuracy of tracking the same object in many frames and
predicting the movement of objects, the best algorithm is chosen. Our automatic
video annotation tool is found to be 83% accurate when compared with a human
annotator. We considered a video with 530 frames each of resolution 1035 x 1800
pixels. At an average each frame had about 15 objects. Our annotation tool
consumed 43 minutes in a CPU based system and 2.58 minutes in a mid-level GPU
based system to process all four modules. But the same video took nearly 3060
minutes for one human annotator to narrate the scene in the given video. Thus
we claim that our proposed automatic video annotation tool is reasonably fast
(about 1200 times in a GPU system) and accurate.",arxiv
http://arxiv.org/abs/1812.00879v2,2020-03-12T08:58:22Z,2018-11-30T17:27:53Z,"Image-based model parameter optimization using Model-Assisted Generative
  Adversarial Networks","We propose and demonstrate the use of a model-assisted generative adversarial
network (GAN) to produce fake images that accurately match true images through
the variation of the parameters of the model that describes the features of the
images. The generator learns the model parameter values that produce fake
images that best match the true images. Two case studies show excellent
agreement between the generated best match parameters and the true parameters.
The best match model parameter values can be used to retune the default
simulation to minimize any bias when applying image recognition techniques to
fake and true images. In the case of a real-world experiment, the true images
are experimental data with unknown true model parameter values, and the fake
images are produced by a simulation that takes the model parameters as input.
The model-assisted GAN uses a convolutional neural network to emulate the
simulation for all parameter values that, when trained, can be used as a
conditional generator for fast fake-image production.",arxiv
http://arxiv.org/abs/2004.09048v3,2020-10-26T05:01:44Z,2020-04-20T04:28:45Z,"Extending DeepSDF for automatic 3D shape retrieval and similarity
  transform estimation","Recent advances in computer graphics and computer vision have found
successful application of deep neural network models for 3D shapes based on
signed distance functions (SDFs) that are useful for shape representation,
retrieval, and completion. However, this approach has been limited by the need
to have query shapes in the same canonical scale and pose as those observed
during training, restricting its effectiveness on real world scenes. We present
a formulation to overcome this issue by jointly estimating shape and similarity
transform parameters. We conduct experiments to demonstrate the effectiveness
of this formulation on synthetic and real datasets and report favorable
comparisons to the state of the art. Finally, we also emphasize the viability
of this approach as a form of 3D model compression.",arxiv
http://arxiv.org/abs/2005.09199v1,2020-05-19T03:52:21Z,2020-05-19T03:52:21Z,FrameProv: Towards End-To-End Video Provenance,"Video feeds are often deliberately used as evidence, as in the case of CCTV
footage; but more often than not, the existence of footage of a supposed event
is perceived as proof of fact in the eyes of the public at large. This reliance
represents a societal vulnerability given the existence of easy-to-use editing
tools and means to fabricate entire video feeds using machine learning. And, as
the recent barrage of fake news and fake porn videos have shown, this isn't
merely an academic concern, it is actively been exploited. I posit that this
exploitation is only going to get more insidious. In this position paper, I
introduce a long term project that aims to mitigate some of the most egregious
forms of manipulation by embedding trustworthy components in the video
transmission chain. Unlike earlier works, I am not aiming to do tamper
detection or other forms of forensics -- approaches I think are bound to fail
in the face of the reality of necessary editing and compression -- instead, the
aim here is to provide a way for the video publisher to prove the integrity of
the video feed as well as make explicit any edits they may have performed. To
do this, I present a novel data structure, a video-edit specification language
and supporting infrastructure that provides end-to-end video provenance, from
the camera sensor to the viewer. I have implemented a prototype of this system
and am in talks with journalists and video editors to discuss the best ways
forward with introducing this idea to the mainstream.",arxiv
http://arxiv.org/abs/1806.01248v2,2018-06-08T22:01:12Z,2018-06-04T17:39:58Z,"Dynamically Hierarchy Revolution: DirNet for Compressing Recurrent
  Neural Network on Mobile Devices","Recurrent neural networks (RNNs) achieve cutting-edge performance on a
variety of problems. However, due to their high computational and memory
demands, deploying RNNs on resource constrained mobile devices is a challenging
task. To guarantee minimum accuracy loss with higher compression rate and
driven by the mobile resource requirement, we introduce a novel model
compression approach DirNet based on an optimized fast dictionary learning
algorithm, which 1) dynamically mines the dictionary atoms of the projection
dictionary matrix within layer to adjust the compression rate 2) adaptively
changes the sparsity of sparse codes cross the hierarchical layers.
Experimental results on language model and an ASR model trained with a 1000h
speech dataset demonstrate that our method significantly outperforms prior
approaches. Evaluated on off-the-shelf mobile devices, we are able to reduce
the size of original model by eight times with real-time model inference and
negligible accuracy loss.",arxiv
http://arxiv.org/abs/2012.06380v1,2020-12-11T14:28:30Z,2020-12-11T14:28:30Z,Parallelized Rate-Distortion Optimized Quantization Using Deep Learning,"Rate-Distortion Optimized Quantization (RDOQ) has played an important role in
the coding performance of recent video compression standards such as H.264/AVC,
H.265/HEVC, VP9 and AV1. This scheme yields significant reductions in bit-rate
at the expense of relatively small increases in distortion. Typically, RDOQ
algorithms are prohibitively expensive to implement on real-time hardware
encoders due to their sequential nature and their need to frequently obtain
entropy coding costs. This work addresses this limitation using a neural
network-based approach, which learns to trade-off rate and distortion during
offline supervised training. As these networks are based solely on standard
arithmetic operations that can be executed on existing neural network hardware,
no additional area-on-chip needs to be reserved for dedicated RDOQ circuitry.
We train two classes of neural networks, a fully-convolutional network and an
auto-regressive network, and evaluate each as a post-quantization step designed
to refine cheap quantization schemes such as scalar quantization (SQ). Both
network architectures are designed to have a low computational overhead. After
training they are integrated into the HM 16.20 implementation of HEVC, and
their video coding performance is evaluated on a subset of the H.266/VVC SDR
common test sequences. Comparisons are made to RDOQ and SQ implementations in
HM 16.20. Our method achieves 1.64% BD-rate savings on luminosity compared to
the HM SQ anchor, and on average reaches 45% of the performance of the
iterative HM RDOQ algorithm.",arxiv
http://arxiv.org/abs/2111.08897v1,2021-11-17T04:27:01Z,2021-11-17T04:27:01Z,"ARKitScenes -- A Diverse Real-World Dataset For 3D Indoor Scene
  Understanding Using Mobile RGB-D Data","Scene understanding is an active research area. Commercial depth sensors,
such as Kinect, have enabled the release of several RGB-D datasets over the
past few years which spawned novel methods in 3D scene understanding. More
recently with the launch of the LiDAR sensor in Apple's iPads and iPhones, high
quality RGB-D data is accessible to millions of people on a device they
commonly use. This opens a whole new era in scene understanding for the
Computer Vision community as well as app developers. The fundamental research
in scene understanding together with the advances in machine learning can now
impact people's everyday experiences. However, transforming these scene
understanding methods to real-world experiences requires additional innovation
and development. In this paper we introduce ARKitScenes. It is not only the
first RGB-D dataset that is captured with a now widely available depth sensor,
but to our best knowledge, it also is the largest indoor scene understanding
data released. In addition to the raw and processed data from the mobile
device, ARKitScenes includes high resolution depth maps captured using a
stationary laser scanner, as well as manually labeled 3D oriented bounding
boxes for a large taxonomy of furniture. We further analyze the usefulness of
the data for two downstream tasks: 3D object detection and color-guided depth
upsampling. We demonstrate that our dataset can help push the boundaries of
existing state-of-the-art methods and it introduces new challenges that better
represent real-world scenarios.",arxiv
http://arxiv.org/abs/2010.04950v1,2020-10-10T08:53:25Z,2020-10-10T08:53:25Z,"A Model Compression Method with Matrix Product Operators for Speech
  Enhancement","The deep neural network (DNN) based speech enhancement approaches have
achieved promising performance. However, the number of parameters involved in
these methods is usually enormous for the real applications of speech
enhancement on the device with the limited resources. This seriously restricts
the applications. To deal with this issue, model compression techniques are
being widely studied. In this paper, we propose a model compression method
based on matrix product operators (MPO) to substantially reduce the number of
parameters in DNN models for speech enhancement. In this method, the weight
matrices in the linear transformations of neural network model are replaced by
the MPO decomposition format before training. In experiment, this process is
applied to the causal neural network models, such as the feedforward multilayer
perceptron (MLP) and long short-term memory (LSTM) models. Both MLP and LSTM
models with/without compression are then utilized to estimate the ideal ratio
mask for monaural speech enhancement. The experimental results show that our
proposed MPO-based method outperforms the widely-used pruning method for speech
enhancement under various compression rates, and further improvement can be
achieved with respect to low compression rates. Our proposal provides an
effective model compression method for speech enhancement, especially in
cloud-free application.",arxiv
http://arxiv.org/abs/1910.10653v1,2019-10-23T16:23:12Z,2019-10-23T16:23:12Z,"Accurate 6D Object Pose Estimation by Pose Conditioned Mesh
  Reconstruction","Current 6D object pose methods consist of deep CNN models fully optimized for
a single object but with its architecture standardized among objects with
different shapes. In contrast to previous works, we explicitly exploit each
object's distinct topological information i.e. 3D dense meshes in the pose
estimation model, with an automated process and prior to any post-processing
refinement stage. In order to achieve this, we propose a learning framework in
which a Graph Convolutional Neural Network reconstructs a pose conditioned 3D
mesh of the object. A robust estimation of the allocentric orientation is
recovered by computing, in a differentiable manner, the Procrustes' alignment
between the canonical and reconstructed dense 3D meshes. 6D egocentric pose is
then lifted using additional mask and 2D centroid projection estimations. Our
method is capable of self validating its pose estimation by measuring the
quality of the reconstructed mesh, which is invaluable in real life
applications. In our experiments on the LINEMOD, OCCLUSION and YCB-Video
benchmarks, the proposed method outperforms state-of-the-arts.",arxiv
http://arxiv.org/abs/2109.13630v1,2021-09-28T11:47:12Z,2021-09-28T11:47:12Z,Unsupervised Diffeomorphic Surface Registration and Non-Linear Modelling,"Registration is an essential tool in image analysis. Deep learning based
alternatives have recently become popular, achieving competitive performance at
a faster speed. However, many contemporary techniques are limited to volumetric
representations, despite increased popularity of 3D surface and shape data in
medical image analysis. We propose a one-step registration model for 3D
surfaces that internalises a lower dimensional probabilistic deformation model
(PDM) using conditional variational autoencoders (CVAE). The deformations are
constrained to be diffeomorphic using an exponentiation layer. The one-step
registration model is benchmarked against iterative techniques, trading in a
slightly lower performance in terms of shape fit for a higher compactness. We
experiment with two distance metrics, Chamfer distance (CD) and Sinkhorn
divergence (SD), as specific distance functions for surface data in real-world
registration scenarios. The internalised deformation model is benchmarked
against linear principal component analysis (PCA) achieving competitive results
and improved generalisability from lower dimensions.",arxiv
http://arxiv.org/abs/2012.07138v1,2020-12-13T20:24:48Z,2020-12-13T20:24:48Z,Learning Contextual Causality from Time-consecutive Images,"Causality knowledge is crucial for many artificial intelligence systems.
Conventional textual-based causality knowledge acquisition methods typically
require laborious and expensive human annotations. As a result, their scale is
often limited. Moreover, as no context is provided during the annotation, the
resulting causality knowledge records (e.g., ConceptNet) typically do not take
the context into consideration. To explore a more scalable way of acquiring
causality knowledge, in this paper, we jump out of the textual domain and
investigate the possibility of learning contextual causality from the visual
signal. Compared with pure text-based approaches, learning causality from the
visual signal has the following advantages: (1) Causality knowledge belongs to
the commonsense knowledge, which is rarely expressed in the text but rich in
videos; (2) Most events in the video are naturally time-ordered, which provides
a rich resource for us to mine causality knowledge from; (3) All the objects in
the video can be used as context to study the contextual property of causal
relations. In detail, we first propose a high-quality dataset Vis-Causal and
then conduct experiments to demonstrate that with good language and visual
representation models as well as enough training signals, it is possible to
automatically discover meaningful causal knowledge from the videos. Further
analysis also shows that the contextual property of causal relations indeed
exists, taking which into consideration might be crucial if we want to use the
causality knowledge in real applications, and the visual signal could serve as
a good resource for learning such contextual causality.",arxiv
http://arxiv.org/abs/1810.07483v3,2020-12-21T13:43:15Z,2018-10-17T11:33:11Z,O2A: One-shot Observational learning with Action vectors,"We present O2A, a novel method for learning to perform robotic manipulation
tasks from a single (one-shot) third-person demonstration video. To our
knowledge, it is the first time this has been done for a single demonstration.
The key novelty lies in pre-training a feature extractor for creating a
perceptual representation for actions that we call 'action vectors'. The action
vectors are extracted using a 3D-CNN model pre-trained as an action classifier
on a generic action dataset. The distance between the action vectors from the
observed third-person demonstration and trial robot executions is used as a
reward for reinforcement learning of the demonstrated task. We report on
experiments in simulation and on a real robot, with changes in viewpoint of
observation, properties of the objects involved, scene background and
morphology of the manipulator between the demonstration and the learning
domains. O2A outperforms baseline approaches under different domain shifts and
has comparable performance with an oracle (that uses an ideal reward function).",arxiv
http://arxiv.org/abs/2102.07887v2,2021-10-05T01:43:41Z,2021-02-15T22:57:52Z,VA-RED$^2$: Video Adaptive Redundancy Reduction,"Performing inference on deep learning models for videos remains a challenge
due to the large amount of computational resources required to achieve robust
recognition. An inherent property of real-world videos is the high correlation
of information across frames which can translate into redundancy in either
temporal or spatial feature maps of the models, or both. The type of redundant
features depends on the dynamics and type of events in the video: static videos
have more temporal redundancy while videos focusing on objects tend to have
more channel redundancy. Here we present a redundancy reduction framework,
termed VA-RED$^2$, which is input-dependent. Specifically, our VA-RED$^2$
framework uses an input-dependent policy to decide how many features need to be
computed for temporal and channel dimensions. To keep the capacity of the
original model, after fully computing the necessary features, we reconstruct
the remaining redundant features from those using cheap linear operations. We
learn the adaptive policy jointly with the network weights in a differentiable
way with a shared-weight mechanism, making it highly efficient. Extensive
experiments on multiple video datasets and different visual tasks show that our
framework achieves $20\% - 40\%$ reduction in computation (FLOPs) when compared
to state-of-the-art methods without any performance loss. Project page:
http://people.csail.mit.edu/bpan/va-red/.",arxiv
http://arxiv.org/abs/2111.07950v1,2021-11-15T17:59:03Z,2021-11-15T17:59:03Z,Occluded Video Instance Segmentation: Dataset and ICCV 2021 Challenge,"Although deep learning methods have achieved advanced video object
recognition performance in recent years, perceiving heavily occluded objects in
a video is still a very challenging task. To promote the development of
occlusion understanding, we collect a large-scale dataset called OVIS for video
instance segmentation in the occluded scenario. OVIS consists of 296k
high-quality instance masks and 901 occluded scenes. While our human vision
systems can perceive those occluded objects by contextual reasoning and
association, our experiments suggest that current video understanding systems
cannot. On the OVIS dataset, all baseline methods encounter a significant
performance degradation of about 80% in the heavily occluded object group,
which demonstrates that there is still a long way to go in understanding
obscured objects and videos in a complex real-world scenario. To facilitate the
research on new paradigms for video understanding systems, we launched a
challenge based on the OVIS dataset. The submitted top-performing algorithms
have achieved much higher performance than our baselines. In this paper, we
will introduce the OVIS dataset and further dissect it by analyzing the results
of baselines and submitted methods. The OVIS dataset and challenge information
can be found at http://songbai.site/ovis .",arxiv
http://arxiv.org/abs/2010.08252v4,2021-03-25T03:30:59Z,2020-10-16T08:58:24Z,Hyperparameter Auto-tuning in Self-Supervised Robotic Learning,"Policy optimization in reinforcement learning requires the selection of
numerous hyperparameters across different environments. Fixing them incorrectly
may negatively impact optimization performance leading notably to insufficient
or redundant learning. Insufficient learning (due to convergence to local
optima) results in under-performing policies whilst redundant learning wastes
time and resources. The effects are further exacerbated when using single
policies to solve multi-task learning problems. Observing that the Evidence
Lower Bound (ELBO) used in Variational Auto-Encoders correlates with the
diversity of image samples, we propose an auto-tuning technique based on the
ELBO for self-supervised reinforcement learning. Our approach can auto-tune
three hyperparameters: the replay buffer size, the number of policy gradient
updates during each epoch, and the number of exploration steps during each
epoch. We use a state-of-the-art self-supervised robot learning framework
(Reinforcement Learning with Imagined Goals (RIG) using Soft Actor-Critic) as
baseline for experimental verification. Experiments show that our method can
auto-tune online and yields the best performance at a fraction of the time and
computational resources. Code, video, and appendix for simulated and real-robot
experiments can be found at the project page \url{www.JuanRojas.net/autotune}.",arxiv
http://arxiv.org/abs/2105.01058v2,2021-08-16T15:16:34Z,2021-05-03T17:58:45Z,"A Dataset and System for Real-Time Gun Detection in Surveillance Video
  Using Deep Learning","Gun violence is a severe problem in the world, particularly in the United
States. Deep learning methods have been studied to detect guns in surveillance
video cameras or smart IP cameras and to send a real-time alert to security
personals. One problem for the development of gun detection algorithms is the
lack of large public datasets. In this work, we first publish a dataset with
51K annotated gun images for gun detection and other 51K cropped gun chip
images for gun classification we collect from a few different sources. To our
knowledge, this is the largest dataset for the study of gun detection. This
dataset can be downloaded at www.linksprite.com/gun-detection-datasets. We
present a gun detection system using a smart IP camera as an embedded edge
device, and a cloud server as a manager for device, data, alert, and to further
reduce the false positive rate. We study to find solutions for gun detection in
an embedded device, and for gun classification on the edge device and the cloud
server. This edge/cloud framework makes the deployment of gun detection in the
real world possible.",arxiv
http://arxiv.org/abs/2106.03167v2,2021-06-16T00:51:02Z,2021-06-06T16:16:08Z,"Mathematical Vocoder Algorithm : Modified Spectral Inversion for
  Efficient Neural Speech Synthesis","In this work, we propose a new mathematical vocoder algorithm(modified
spectral inversion) that generates a waveform from acoustic features without
phase estimation. The main benefit of using our proposed method is that it
excludes the training stage of the neural vocoder from the end-to-end speech
synthesis model. Our implementation can synthesize high fidelity speech at
approximately 20 Mhz on CPU and 59.6MHz on GPU. This is 909 and 2,702 times
faster compared to real-time. Since the proposed methodology is not a
data-driven method, it is applicable to unseen voices and multiple languages
without any additional work. The proposed method is expected to adapt for
researching on neural network models capable of synthesizing speech at the
studio recording level.",arxiv
http://arxiv.org/abs/1411.2861v3,2015-05-04T02:33:26Z,2014-11-11T16:00:59Z,Computational Baby Learning,"Intuitive observations show that a baby may inherently possess the capability
of recognizing a new visual concept (e.g., chair, dog) by learning from only
very few positive instances taught by parent(s) or others, and this recognition
capability can be gradually further improved by exploring and/or interacting
with the real instances in the physical world. Inspired by these observations,
we propose a computational model for slightly-supervised object detection,
based on prior knowledge modelling, exemplar learning and learning with video
contexts. The prior knowledge is modeled with a pre-trained Convolutional
Neural Network (CNN). When very few instances of a new concept are given, an
initial concept detector is built by exemplar learning over the deep features
from the pre-trained CNN. Simulating the baby's interaction with physical
world, the well-designed tracking solution is then used to discover more
diverse instances from the massive online unlabeled videos. Once a positive
instance is detected/identified with high score in each video, more variable
instances possibly from different view-angles and/or different distances are
tracked and accumulated. Then the concept detector can be fine-tuned based on
these new instances. This process can be repeated again and again till we
obtain a very mature concept detector. Extensive experiments on Pascal
VOC-07/10/12 object detection datasets well demonstrate the effectiveness of
our framework. It can beat the state-of-the-art full-training based
performances by learning from very few samples for each object category, along
with about 20,000 unlabeled videos.",arxiv
http://arxiv.org/abs/2010.06676v1,2020-10-13T20:23:47Z,2020-10-13T20:23:47Z,On Front-end Gain Invariant Modeling for Wake Word Spotting,"Wake word (WW) spotting is challenging in far-field due to the complexities
and variations in acoustic conditions and the environmental interference in
signal transmission. A suite of carefully designed and optimized audio
front-end (AFE) algorithms help mitigate these challenges and provide better
quality audio signals to the downstream modules such as WW spotter. Since the
WW model is trained with the AFE-processed audio data, its performance is
sensitive to AFE variations, such as gain changes. In addition, when deploying
to new devices, the WW performance is not guaranteed because the AFE is unknown
to the WW model. To address these issues, we propose a novel approach to use a
new feature called $\Delta$LFBE to decouple the AFE gain variations from the WW
model. We modified the neural network architectures to accommodate the delta
computation, with the feature extraction module unchanged. We evaluate our WW
models using data collected from real household settings and showed the models
with the $\Delta$LFBE is robust to AFE gain changes. Specifically, when AFE
gain changes up to $\pm$12dB, the baseline CNN model lost up to relative 19.0%
in false alarm rate or 34.3% in false reject rate, while the model with
$\Delta$LFBE demonstrates no performance loss.",arxiv
http://arxiv.org/abs/1901.00466v1,2019-01-02T17:57:50Z,2019-01-02T17:57:50Z,Learning Generalizable Physical Dynamics of 3D Rigid Objects,"Humans have a remarkable ability to predict the effect of physical
interactions on the dynamics of objects. Endowing machines with this ability
would allow important applications in areas like robotics and autonomous
vehicles. In this work, we focus on predicting the dynamics of 3D rigid
objects, in particular an object's final resting position and total rotation
when subjected to an impulsive force. Different from previous work, our
approach is capable of generalizing to unseen object shapes - an important
requirement for real-world applications. To achieve this, we represent object
shape as a 3D point cloud that is used as input to a neural network, making our
approach agnostic to appearance variation. The design of our network is
informed by an understanding of physical laws. We train our model with data
from a physics engine that simulates the dynamics of a large number of shapes.
Experiments show that we can accurately predict the resting position and total
rotation for unseen object geometries.",arxiv
http://arxiv.org/abs/2009.03705v1,2020-09-08T12:48:49Z,2020-09-08T12:48:49Z,"Comparison of camera-based and 3D LiDAR-based loop closures across
  weather conditions","Loop closure based on camera images provides excellent results on
benchmarking datasets, but might struggle in real-world adverse weather
conditions like direct sun, rain, fog, or just darkness at night. In automotive
applications, the sensory setups include 3D LiDARs that provide information
complementary to cameras. The presented article focuses on the evaluation of
camera-based, LiDAR-based, and joint camera-LiDAR-based loop closures applying
a similar processing pipeline consisting of a neural network under varying
weather conditions using the newly available USyd dataset. The experiments
performed on the same trajectories in diverse weather conditions over 50 weeks
prove that a 16-line 3D LiDAR can be used to supplement image-based loop
closure to increase loop closure performance. This proves that there is a need
for more research into loop closures performed with multi-sensory setups.",arxiv
http://arxiv.org/abs/1709.09312v1,2017-09-27T02:42:29Z,2017-09-27T02:42:29Z,"A Simple Reinforcement Learning Mechanism for Resource Allocation in
  LTE-A Networks with Markov Decision Process and Q-Learning","Resource allocation is still a difficult issue to deal with in wireless
networks. The unstable channel condition and traffic demand for Quality of
Service (QoS) raise some barriers that interfere with the process. It is
significant that an optimal policy takes into account some resources available
to each traffic class while considering the spectral efficiency and other
related channel issues. Reinforcement learning is a dynamic and effective
method to support the accomplishment of resource allocation properly
maintaining QoS levels for applications. The technique can track the system
state as feedback to enhance the performance of a given task. Herein, it is
proposed a simple reinforcement learning mechanism introduced in LTE-A networks
and aimed to choose and limit the number of resources allocated for each
traffic class, regarding the QoS Class Identifier (QCI), at each Transmission
Time Interval (TTI) along the scheduling procedure. The proposed mechanism
implements a Markov Decision Process (MDP) solved by the Q-Learning algorithm
to find an optimal action-state decision policy. The results obtained from
simulation exhibit good performance, especially for the real-time Video
application.",arxiv
http://arxiv.org/abs/2111.03821v1,2021-11-06T07:30:00Z,2021-11-06T07:30:00Z,ROFT: Real-Time Optical Flow-Aided 6D Object Pose and Velocity Tracking,"6D object pose tracking has been extensively studied in the robotics and
computer vision communities. The most promising solutions, leveraging on deep
neural networks and/or filtering and optimization, exhibit notable performance
on standard benchmarks. However, to our best knowledge, these have not been
tested thoroughly against fast object motions. Tracking performance in this
scenario degrades significantly, especially for methods that do not achieve
real-time performance and introduce non negligible delays. In this work, we
introduce ROFT, a Kalman filtering approach for 6D object pose and velocity
tracking from a stream of RGB-D images. By leveraging real-time optical flow,
ROFT synchronizes delayed outputs of low frame rate Convolutional Neural
Networks for instance segmentation and 6D object pose estimation with the RGB-D
input stream to achieve fast and precise 6D object pose and velocity tracking.
We test our method on a newly introduced photorealistic dataset, Fast-YCB,
which comprises fast moving objects from the YCB model set, and on the dataset
for object and hand pose estimation HO-3D. Results demonstrate that our
approach outperforms state-of-the-art methods for 6D object pose tracking,
while also providing 6D object velocity tracking. A video showing the
experiments is provided as supplementary material.",arxiv
http://arxiv.org/abs/2005.07302v2,2020-09-09T02:00:26Z,2020-05-15T00:14:39Z,"Investigating Bias in Deep Face Analysis: The KANFace Dataset and
  Empirical Study","Deep learning-based methods have pushed the limits of the state-of-the-art in
face analysis. However, despite their success, these models have raised
concerns regarding their bias towards certain demographics. This bias is
inflicted both by limited diversity across demographics in the training set, as
well as the design of the algorithms. In this work, we investigate the
demographic bias of deep learning models in face recognition, age estimation,
gender recognition and kinship verification. To this end, we introduce the most
comprehensive, large-scale dataset of facial images and videos to date. It
consists of 40K still images and 44K sequences (14.5M video frames in total)
captured in unconstrained, real-world conditions from 1,045 subjects. The data
are manually annotated in terms of identity, exact age, gender and kinship. The
performance of state-of-the-art models is scrutinized and demographic bias is
exposed by conducting a series of experiments. Lastly, a method to debias
network embeddings is introduced and tested on the proposed benchmarks.",arxiv
http://arxiv.org/abs/1609.05281v1,2016-09-17T04:18:02Z,2016-09-17T04:18:02Z,"GeThR-Net: A Generalized Temporally Hybrid Recurrent Neural Network for
  Multimodal Information Fusion","Data generated from real world events are usually temporal and contain
multimodal information such as audio, visual, depth, sensor etc. which are
required to be intelligently combined for classification tasks. In this paper,
we propose a novel generalized deep neural network architecture where temporal
streams from multiple modalities are combined. There are total M+1 (M is the
number of modalities) components in the proposed network. The first component
is a novel temporally hybrid Recurrent Neural Network (RNN) that exploits the
complimentary nature of the multimodal temporal information by allowing the
network to learn both modality specific temporal dynamics as well as the
dynamics in a multimodal feature space. M additional components are added to
the network which extract discriminative but non-temporal cues from each
modality. Finally, the predictions from all of these components are linearly
combined using a set of automatically learned weights. We perform exhaustive
experiments on three different datasets spanning four modalities. The proposed
network is relatively 3.5%, 5.7% and 2% better than the best performing
temporal multimodal baseline for UCF-101, CCV and Multimodal Gesture datasets
respectively.",arxiv
http://arxiv.org/abs/2108.09491v1,2021-08-21T11:16:49Z,2021-08-21T11:16:49Z,"Flikcer -- A Chrome Extension to Resolve Online Epileptogenic Visual
  Content with Real-Time Luminance Frequency Analysis","Video content with fast luminance variations, or with spatial patterns of
high contrast - referred to as epileptogenic visual content - may induce
seizures on viewers with photosensitive epilepsy, and even cause discomfort in
users not affected by this disease. Flikcer is a web app in the form of a
website and chrome extension which aims to resolve epileptic content in videos.
It provides the number of possible triggers for a seizure. It also provides the
timestamps for these triggers along with a safer version of the video, free to
download. The algorithm is written in Python and uses machine learning and
computer vision. A key aspect of the algorithm is its computational efficiency,
allowing real time implementation for public users.",arxiv
http://arxiv.org/abs/1602.00828v1,2016-02-02T08:42:44Z,2016-02-02T08:42:44Z,Learning a Deep Model for Human Action Recognition from Novel Viewpoints,"Recognizing human actions from unknown and unseen (novel) views is a
challenging problem. We propose a Robust Non-Linear Knowledge Transfer Model
(R-NKTM) for human action recognition from novel views. The proposed R-NKTM is
a deep fully-connected neural network that transfers knowledge of human actions
from any unknown view to a shared high-level virtual view by finding a
non-linear virtual path that connects the views. The R-NKTM is learned from
dense trajectories of synthetic 3D human models fitted to real motion capture
data and generalizes to real videos of human actions. The strength of our
technique is that we learn a single R-NKTM for all actions and all viewpoints
for knowledge transfer of any real human action video without the need for
re-training or fine-tuning the model. Thus, R-NKTM can efficiently scale to
incorporate new action classes. R-NKTM is learned with dummy labels and does
not require knowledge of the camera viewpoint at any stage. Experiments on
three benchmark cross-view human action datasets show that our method
outperforms existing state-of-the-art.",arxiv
http://arxiv.org/abs/2106.14440v1,2021-06-28T07:47:31Z,2021-06-28T07:47:31Z,"VAT-Mart: Learning Visual Action Trajectory Proposals for Manipulating
  3D ARTiculated Objects","Perceiving and manipulating 3D articulated objects (e.g., cabinets, doors) in
human environments is an important yet challenging task for future
home-assistant robots. The space of 3D articulated objects is exceptionally
rich in their myriad semantic categories, diverse shape geometry, and
complicated part functionality. Previous works mostly abstract kinematic
structure with estimated joint parameters and part poses as the visual
representations for manipulating 3D articulated objects. In this paper, we
propose object-centric actionable visual priors as a novel
perception-interaction handshaking point that the perception system outputs
more actionable guidance than kinematic structure estimation, by predicting
dense geometry-aware, interaction-aware, and task-aware visual action
affordance and trajectory proposals. We design an interaction-for-perception
framework VAT-Mart to learn such actionable visual representations by
simultaneously training a curiosity-driven reinforcement learning policy
exploring diverse interaction trajectories and a perception module summarizing
and generalizing the explored knowledge for pointwise predictions among diverse
shapes. Experiments prove the effectiveness of the proposed approach using the
large-scale PartNet-Mobility dataset in SAPIEN environment and show promising
generalization capabilities to novel test shapes, unseen object categories, and
real-world data. Project page: https://hyperplane-lab.github.io/vat-mart",arxiv
http://arxiv.org/abs/2110.04361v2,2021-10-27T10:49:50Z,2021-10-08T20:11:09Z,"SubTab: Subsetting Features of Tabular Data for Self-Supervised
  Representation Learning","Self-supervised learning has been shown to be very effective in learning
useful representations, and yet much of the success is achieved in data types
such as images, audio, and text. The success is mainly enabled by taking
advantage of spatial, temporal, or semantic structure in the data through
augmentation. However, such structure may not exist in tabular datasets
commonly used in fields such as healthcare, making it difficult to design an
effective augmentation method, and hindering a similar progress in tabular data
setting. In this paper, we introduce a new framework, Subsetting features of
Tabular data (SubTab), that turns the task of learning from tabular data into a
multi-view representation learning problem by dividing the input features to
multiple subsets. We argue that reconstructing the data from the subset of its
features rather than its corrupted version in an autoencoder setting can better
capture its underlying latent representation. In this framework, the joint
representation can be expressed as the aggregate of latent variables of the
subsets at test time, which we refer to as collaborative inference. Our
experiments show that the SubTab achieves the state of the art (SOTA)
performance of 98.31% on MNIST in tabular setting, on par with CNN-based SOTA
models, and surpasses existing baselines on three other real-world datasets by
a significant margin.",arxiv
http://arxiv.org/abs/2107.14230v2,2021-08-05T17:59:08Z,2021-07-29T17:59:54Z,Learning with Noisy Labels for Robust Point Cloud Segmentation,"Point cloud segmentation is a fundamental task in 3D. Despite recent progress
on point cloud segmentation with the power of deep networks, current deep
learning methods based on the clean label assumptions may fail with noisy
labels. Yet, object class labels are often mislabeled in real-world point cloud
datasets. In this work, we take the lead in solving this issue by proposing a
novel Point Noise-Adaptive Learning (PNAL) framework. Compared to existing
noise-robust methods on image tasks, our PNAL is noise-rate blind, to cope with
the spatially variant noise rate problem specific to point clouds.
Specifically, we propose a novel point-wise confidence selection to obtain
reliable labels based on the historical predictions of each point. A novel
cluster-wise label correction is proposed with a voting strategy to generate
the best possible label taking the neighbor point correlations into
consideration. We conduct extensive experiments to demonstrate the
effectiveness of PNAL on both synthetic and real-world noisy datasets. In
particular, even with $60\%$ symmetric noisy labels, our proposed method
produces much better results than its baseline counterpart without PNAL and is
comparable to the ideal upper bound trained on a completely clean dataset.
Moreover, we fully re-labeled the validation set of a popular but noisy
real-world scene dataset ScanNetV2 to make it clean, for rigorous experiment
and future research. Our code and data are available at
\url{https://shuquanye.com/PNAL_website/}.",arxiv
http://arxiv.org/abs/2003.07018v4,2020-05-22T15:57:57Z,2020-03-16T04:23:42Z,"Closed-loop Matters: Dual Regression Networks for Single Image
  Super-Resolution","Deep neural networks have exhibited promising performance in image
super-resolution (SR) by learning a nonlinear mapping function from
low-resolution (LR) images to high-resolution (HR) images. However, there are
two underlying limitations to existing SR methods. First, learning the mapping
function from LR to HR images is typically an ill-posed problem, because there
exist infinite HR images that can be downsampled to the same LR image. As a
result, the space of the possible functions can be extremely large, which makes
it hard to find a good solution. Second, the paired LR-HR data may be
unavailable in real-world applications and the underlying degradation method is
often unknown. For such a more general case, existing SR models often incur the
adaptation problem and yield poor performance. To address the above issues, we
propose a dual regression scheme by introducing an additional constraint on LR
data to reduce the space of the possible functions. Specifically, besides the
mapping from LR to HR images, we learn an additional dual regression mapping
estimates the down-sampling kernel and reconstruct LR images, which forms a
closed-loop to provide additional supervision. More critically, since the dual
regression process does not depend on HR images, we can directly learn from LR
images. In this sense, we can easily adapt SR models to real-world data, e.g.,
raw video frames from YouTube. Extensive experiments with paired training data
and unpaired real-world data demonstrate our superiority over existing methods.",arxiv
http://arxiv.org/abs/2109.03484v2,2021-09-09T12:55:06Z,2021-09-08T08:14:13Z,Shuffled Patch-Wise Supervision for Presentation Attack Detection,"Face anti-spoofing is essential to prevent false facial verification by using
a photo, video, mask, or a different substitute for an authorized person's
face. Most of the state-of-the-art presentation attack detection (PAD) systems
suffer from overfitting, where they achieve near-perfect scores on a single
dataset but fail on a different dataset with more realistic data. This problem
drives researchers to develop models that perform well under real-world
conditions. This is an especially challenging problem for frame-based
presentation attack detection systems that use convolutional neural networks
(CNN). To this end, we propose a new PAD approach, which combines pixel-wise
binary supervision with patch-based CNN. We believe that training a CNN with
face patches allows the model to distinguish spoofs without learning background
or dataset-specific traces. We tested the proposed method both on the standard
benchmark datasets -- Replay-Mobile, OULU-NPU -- and on a real-world dataset.
The proposed approach shows its superiority on challenging experimental setups.
Namely, it achieves higher performance on OULU-NPU protocol 3, 4 and on
inter-dataset real-world experiments.",arxiv
http://arxiv.org/abs/2005.12074v2,2020-06-08T14:58:07Z,2020-05-25T12:34:47Z,Egocentric Human Segmentation for Mixed Reality,"The objective of this work is to segment human body parts from egocentric
video using semantic segmentation networks. Our contribution is two-fold: i) we
create a semi-synthetic dataset composed of more than 15, 000 realistic images
and associated pixel-wise labels of egocentric human body parts, such as arms
or legs including different demographic factors; ii) building upon the
ThunderNet architecture, we implement a deep learning semantic segmentation
algorithm that is able to perform beyond real-time requirements (16 ms for 720
x 720 images). It is believed that this method will enhance sense of presence
of Virtual Environments and will constitute a more realistic solution to the
standard virtual avatars.",arxiv
http://arxiv.org/abs/1703.04699v1,2017-03-14T20:23:48Z,2017-03-14T20:23:48Z,"A fully end-to-end deep learning approach for real-time simultaneous 3D
  reconstruction and material recognition","This paper addresses the problem of simultaneous 3D reconstruction and
material recognition and segmentation. Enabling robots to recognise different
materials (concrete, metal etc.) in a scene is important for many tasks, e.g.
robotic interventions in nuclear decommissioning. Previous work on 3D semantic
reconstruction has predominantly focused on recognition of everyday domestic
objects (tables, chairs etc.), whereas previous work on material recognition
has largely been confined to single 2D images without any 3D reconstruction.
Meanwhile, most 3D semantic reconstruction methods rely on computationally
expensive post-processing, using Fully-Connected Conditional Random Fields
(CRFs), to achieve consistent segmentations. In contrast, we propose a deep
learning method which performs 3D reconstruction while simultaneously
recognising different types of materials and labelling them at the pixel level.
Unlike previous methods, we propose a fully end-to-end approach, which does not
require hand-crafted features or CRF post-processing. Instead, we use only
learned features, and the CRF segmentation constraints are incorporated inside
the fully end-to-end learned system. We present the results of experiments, in
which we trained our system to perform real-time 3D semantic reconstruction for
23 different materials in a real-world application. The run-time performance of
the system can be boosted to around 10Hz, using a conventional GPU, which is
enough to achieve real-time semantic reconstruction using a 30fps RGB-D camera.
To the best of our knowledge, this work is the first real-time end-to-end
system for simultaneous 3D reconstruction and material recognition.",arxiv
http://arxiv.org/abs/1909.03656v1,2019-09-09T06:50:33Z,2019-09-09T06:50:33Z,Saliency based Semi-supervised Learning for Orbiting Satellite Tracking,"The trajectory and boundary of an orbiting satellite are fundamental
information for on-orbit repairing and manipulation by space robots. This task,
however, is challenging owing to the freely and rapidly motion of on-orbiting
satellites, the quickly varying background and the sudden change in
illumination conditions. Traditional tracking usually relies on a single
bounding box of the target object, however, more detailed information should be
provided by visual tracking such as binary mask. In this paper, we proposed a
SSLT (Saliency-based Semi-supervised Learning for Tracking) algorithm that
provides both the bounding box and segmentation binary mask of target
satellites at 12 frame per second without requirement of annotated data. Our
method, SSLT, improves the segmentation performance by generating a saliency
map based semi-supervised on-line learning approach within the initial bounding
box estimated by tracking. Once a customized segmentation model has been
trained, the bounding box and satellite trajectory will be refined using the
binary segmentation result. Experiment using real on-orbit rendezvous and
docking video from NASA (Nation Aeronautics and Space Administration),
simulated satellite animation sequence from ESA (European Space Agency) and
image sequences of 3D printed satellite model took in our laboratory
demonstrate the robustness, versatility and fast speed of our method compared
to state-of-the-art tracking and segmentation methods. Our dataset will be
released for academic use in future.",arxiv
http://arxiv.org/abs/1908.09089v1,2019-08-24T04:06:01Z,2019-08-24T04:06:01Z,"Web-enabled Intelligent System for Continuous Sensor Data Processing and
  Visualization","A large number of sensors deployed in recent years in various setups and
their data is readily available in dedicated databases or in the cloud. Of
particular interest is real-time data processing and 3D visualization in
web-based user interfaces that facilitate spatial information understanding and
sharing, hence helping the decision making process for all the parties
involved. In this research, we provide a prototype system for near real-time,
continuous X3D-based visualization of processed sensor data for two significant
applications: thermal monitoring for residential/commercial buildings and
nitrogen cycle monitoring in water beds for aquaponics systems. As sensors are
sparsely placed, in each application, where they collect data for large periods
(of up to one year), we employ a Finite Differences Method and a Neural
Networks model to approximate data distribution in the entire volume.",arxiv
http://arxiv.org/abs/2101.09500v1,2021-01-23T13:39:34Z,2021-01-23T13:39:34Z,Disentangled Sequence Clustering for Human Intention Inference,"Equipping robots with the ability to infer human intent is a vital
precondition for effective collaboration. Most computational approaches towards
this objective employ probabilistic reasoning to recover a distribution of
""intent"" conditioned on the robot's perceived sensory state. However, these
approaches typically assume task-specific notions of human intent (e.g.
labelled goals) are known a priori. To overcome this constraint, we propose the
Disentangled Sequence Clustering Variational Autoencoder (DiSCVAE), a
clustering framework that can be used to learn such a distribution of intent in
an unsupervised manner. The DiSCVAE leverages recent advances in unsupervised
learning to derive a disentangled latent representation of sequential data,
separating time-varying local features from time-invariant global aspects.
Though unlike previous frameworks for disentanglement, the proposed variant
also infers a discrete variable to form a latent mixture model and enable
clustering of global sequence concepts, e.g. intentions from observed human
behaviour. To evaluate the DiSCVAE, we first validate its capacity to discover
classes from unlabelled sequences using video datasets of bouncing digits and
2D animations. We then report results from a real-world human-robot interaction
experiment conducted on a robotic wheelchair. Our findings glean insights into
how the inferred discrete variable coincides with human intent and thus serves
to improve assistance in collaborative settings, such as shared control.",arxiv
http://arxiv.org/abs/1909.03602v3,2021-05-05T01:39:14Z,2019-09-09T02:56:03Z,"DEAR: Deep Reinforcement Learning for Online Advertising Impression in
  Recommender Systems","With the recent prevalence of Reinforcement Learning (RL), there have been
tremendous interests in utilizing RL for online advertising in recommendation
platforms (e.g., e-commerce and news feed sites). However, most RL-based
advertising algorithms focus on optimizing ads' revenue while ignoring the
possible negative influence of ads on user experience of recommended items
(products, articles and videos). Developing an optimal advertising algorithm in
recommendations faces immense challenges because interpolating ads improperly
or too frequently may decrease user experience, while interpolating fewer ads
will reduce the advertising revenue. Thus, in this paper, we propose a novel
advertising strategy for the rec/ads trade-off. To be specific, we develop an
RL-based framework that can continuously update its advertising strategies and
maximize reward in the long run. Given a recommendation list, we design a novel
Deep Q-network architecture that can determine three internally related tasks
jointly, i.e., (i) whether to interpolate an ad or not in the recommendation
list, and if yes, (ii) the optimal ad and (iii) the optimal location to
interpolate. The experimental results based on real-world data demonstrate the
effectiveness of the proposed framework.",arxiv
http://arxiv.org/abs/2011.05632v2,2020-11-12T01:21:35Z,2020-11-11T08:42:30Z,"Exploratory Grasping: Asymptotically Optimal Algorithms for Grasping
  Challenging Polyhedral Objects","There has been significant recent work on data-driven algorithms for learning
general-purpose grasping policies. However, these policies can consistently
fail to grasp challenging objects which are significantly out of the
distribution of objects in the training data or which have very few high
quality grasps. Motivated by such objects, we propose a novel problem setting,
Exploratory Grasping, for efficiently discovering reliable grasps on an unknown
polyhedral object via sequential grasping, releasing, and toppling. We
formalize Exploratory Grasping as a Markov Decision Process, study the
theoretical complexity of Exploratory Grasping in the context of reinforcement
learning and present an efficient bandit-style algorithm, Bandits for Online
Rapid Grasp Exploration Strategy (BORGES), which leverages the structure of the
problem to efficiently discover high performing grasps for each object stable
pose. BORGES can be used to complement any general-purpose grasping algorithm
with any grasp modality (parallel-jaw, suction, multi-fingered, etc) to learn
policies for objects in which they exhibit persistent failures. Simulation
experiments suggest that BORGES can significantly outperform both
general-purpose grasping pipelines and two other online learning algorithms and
achieves performance within 5% of the optimal policy within 1000 and 8000
timesteps on average across 46 challenging objects from the Dex-Net adversarial
and EGAD! object datasets, respectively. Initial physical experiments suggest
that BORGES can improve grasp success rate by 45% over a Dex-Net baseline with
just 200 grasp attempts in the real world. See https://tinyurl.com/exp-grasping
for supplementary material and videos.",arxiv
http://arxiv.org/abs/2110.01954v1,2021-10-05T11:33:37Z,2021-10-05T11:33:37Z,Continuous-Time Fitted Value Iteration for Robust Policies,"Solving the Hamilton-Jacobi-Bellman equation is important in many domains
including control, robotics and economics. Especially for continuous control,
solving this differential equation and its extension the Hamilton-Jacobi-Isaacs
equation, is important as it yields the optimal policy that achieves the
maximum reward on a give task. In the case of the Hamilton-Jacobi-Isaacs
equation, which includes an adversary controlling the environment and
minimizing the reward, the obtained policy is also robust to perturbations of
the dynamics. In this paper we propose continuous fitted value iteration (cFVI)
and robust fitted value iteration (rFVI). These algorithms leverage the
non-linear control-affine dynamics and separable state and action reward of
many continuous control problems to derive the optimal policy and optimal
adversary in closed form. This analytic expression simplifies the differential
equations and enables us to solve for the optimal value function using value
iteration for continuous actions and states as well as the adversarial case.
Notably, the resulting algorithms do not require discretization of states or
actions. We apply the resulting algorithms to the Furuta pendulum and cartpole.
We show that both algorithms obtain the optimal policy. The robustness Sim2Real
experiments on the physical systems show that the policies successfully achieve
the task in the real-world. When changing the masses of the pendulum, we
observe that robust value iteration is more robust compared to deep
reinforcement learning algorithm and the non-robust version of the algorithm.
Videos of the experiments are shown at https://sites.google.com/view/rfvi",arxiv
http://arxiv.org/abs/1505.02056v1,2015-05-08T14:51:12Z,2015-05-08T14:51:12Z,"DDA: Cross-Session Throughput Prediction with Applications to Video
  Bitrate Selection","User experience of video streaming could be greatly improved by selecting a
high-yet-sustainable initial video bitrate, and it is therefore critical to
accurately predict throughput before a video session starts. Inspired by
previous studies that show similarity among throughput of similar sessions
(e.g., those sharing same bottleneck link), we argue for a cross-session
prediction approach, where throughput measured on other sessions is used to
predict the throughput of a new session. In this paper, we study the challenges
of cross-session throughput prediction, develop an accurate throughput
predictor called DDA, and evaluate the performance of the predictor with
real-world datasets. We show that DDA can predict throughput more accurately
than simple predictors and conventional machine learning algorithms; e.g.,
DDA's 80%ile prediction error of DDA is > 50% lower than other algorithms. We
also show that this improved accuracy enables video players to select a higher
sustainable initial bitrate; e.g., compared to initial bitrate without
prediction, DDA leads to 4x higher average bitrate.",arxiv
http://arxiv.org/abs/1605.05860v3,2016-06-16T15:30:43Z,2016-05-19T09:14:39Z,"False Discovery Rate Control and Statistical Quality Assessment of
  Annotators in Crowdsourced Ranking","With the rapid growth of crowdsourcing platforms it has become easy and
relatively inexpensive to collect a dataset labeled by multiple annotators in a
short time. However due to the lack of control over the quality of the
annotators, some abnormal annotators may be affected by position bias which can
potentially degrade the quality of the final consensus labels. In this paper we
introduce a statistical framework to model and detect annotator's position bias
in order to control the false discovery rate (FDR) without a prior knowledge on
the amount of biased annotators - the expected fraction of false discoveries
among all discoveries being not too high, in order to assure that most of the
discoveries are indeed true and replicable. The key technical development
relies on some new knockoff filters adapted to our problem and new algorithms
based on the Inverse Scale Space dynamics whose discretization is potentially
suitable for large scale crowdsourcing data analysis. Our studies are supported
by experiments with both simulated examples and real-world data. The proposed
framework provides us a useful tool for quantitatively studying annotator's
abnormal behavior in crowdsourcing data arising from machine learning,
sociology, computer vision, multimedia, etc.",arxiv
http://arxiv.org/abs/2001.05097v1,2020-01-15T01:31:01Z,2020-01-15T01:31:01Z,"Lightweight 3D Human Pose Estimation Network Training Using
  Teacher-Student Learning","We present MoVNect, a lightweight deep neural network to capture 3D human
pose using a single RGB camera. To improve the overall performance of the
model, we apply the teacher-student learning method based knowledge
distillation to 3D human pose estimation. Real-time post-processing makes the
CNN output yield temporally stable 3D skeletal information, which can be used
in applications directly. We implement a 3D avatar application running on
mobile in real-time to demonstrate that our network achieves both high accuracy
and fast inference time. Extensive evaluations show the advantages of our
lightweight model with the proposed training method over previous 3D pose
estimation methods on the Human3.6M dataset and mobile devices.",arxiv
http://arxiv.org/abs/1905.06809v1,2019-05-16T14:50:28Z,2019-05-16T14:50:28Z,Occupancy Estimation Using Low-Cost Wi-Fi Sniffers,"Real-time measurements on the occupancy status of indoor and outdoor spaces
can be exploited in many scenarios (HVAC and lighting system control, building
energy optimization, allocation and reservation of spaces, etc.). Traditional
systems for occupancy estimation rely on environmental sensors (CO2,
temperature, humidity) or video cameras. In this paper, we depart from such
traditional approaches and propose a novel occupancy estimation system which is
based on the capture of Wi-Fi management packets from users' devices. The
system, implemented on a low-cost ESP8266 microcontroller, leverages a
supervised learning model to adapt to different spaces and transmits occupancy
information through the MQTT protocol to a web-based dashboard. Experimental
results demonstrate the validity of the proposed solution in four different
indoor university spaces.",arxiv
http://arxiv.org/abs/1607.05427v2,2017-05-17T09:12:19Z,2016-07-19T07:14:28Z,"Trunk-Branch Ensemble Convolutional Neural Networks for Video-based Face
  Recognition","Human faces in surveillance videos often suffer from severe image blur,
dramatic pose variations, and occlusion. In this paper, we propose a
comprehensive framework based on Convolutional Neural Networks (CNN) to
overcome challenges in video-based face recognition (VFR). First, to learn
blur-robust face representations, we artificially blur training data composed
of clear still images to account for a shortfall in real-world video training
data. Using training data composed of both still images and artificially
blurred data, CNN is encouraged to learn blur-insensitive features
automatically. Second, to enhance robustness of CNN features to pose variations
and occlusion, we propose a Trunk-Branch Ensemble CNN model (TBE-CNN), which
extracts complementary information from holistic face images and patches
cropped around facial components. TBE-CNN is an end-to-end model that extracts
features efficiently by sharing the low- and middle-level convolutional layers
between the trunk and branch networks. Third, to further promote the
discriminative power of the representations learnt by TBE-CNN, we propose an
improved triplet loss function. Systematic experiments justify the
effectiveness of the proposed techniques. Most impressively, TBE-CNN achieves
state-of-the-art performance on three popular video face databases: PaSC, COX
Face, and YouTube Faces. With the proposed techniques, we also obtain the first
place in the BTAS 2016 Video Person Recognition Evaluation.",arxiv
http://arxiv.org/abs/2105.00114v1,2021-04-30T22:34:45Z,2021-04-30T22:34:45Z,"Improved Real-Time Monocular SLAM Using Semantic Segmentation on
  Selective Frames","Monocular simultaneous localization and mapping (SLAM) is emerging in
advanced driver assistance systems and autonomous driving, because a single
camera is cheap and easy to install. Conventional monocular SLAM has two major
challenges leading inaccurate localization and mapping. First, it is
challenging to estimate scales in localization and mapping. Second,
conventional monocular SLAM uses inappropriate mapping factors such as dynamic
objects and low-parallax ares in mapping. This paper proposes an improved
real-time monocular SLAM that resolves the aforementioned challenges by
efficiently using deep learning-based semantic segmentation. To achieve the
real-time execution of the proposed method, we apply semantic segmentation only
to downsampled keyframes in parallel with mapping processes. In addition, the
proposed method corrects scales of camera poses and three-dimensional (3D)
points, using estimated ground plane from road-labeled 3D points and the real
camera height. The proposed method also removes inappropriate corner features
labeled as moving objects and low parallax areas. Experiments with six video
sequences demonstrate that the proposed monocular SLAM system achieves
significantly more accurate trajectory tracking accuracy compared to
state-of-the-art monocular SLAM and comparable trajectory tracking accuracy
compared to state-of-the-art stereo SLAM.",arxiv
http://arxiv.org/abs/2101.10955v2,2021-11-14T17:09:25Z,2021-01-26T17:23:46Z,"RAPIQUE: Rapid and Accurate Video Quality Prediction of User Generated
  Content","Blind or no-reference video quality assessment of user-generated content
(UGC) has become a trending, challenging, heretofore unsolved problem. Accurate
and efficient video quality predictors suitable for this content are thus in
great demand to achieve more intelligent analysis and processing of UGC videos.
Previous studies have shown that natural scene statistics and deep learning
features are both sufficient to capture spatial distortions, which contribute
to a significant aspect of UGC video quality issues. However, these models are
either incapable or inefficient for predicting the quality of complex and
diverse UGC videos in practical applications. Here we introduce an effective
and efficient video quality model for UGC content, which we dub the Rapid and
Accurate Video Quality Evaluator (RAPIQUE), which we show performs comparably
to state-of-the-art (SOTA) models but with orders-of-magnitude faster runtime.
RAPIQUE combines and leverages the advantages of both quality-aware scene
statistics features and semantics-aware deep convolutional features, allowing
us to design the first general and efficient spatial and temporal (space-time)
bandpass statistics model for video quality modeling. Our experimental results
on recent large-scale UGC video quality databases show that RAPIQUE delivers
top performances on all the datasets at a considerably lower computational
expense. We hope this work promotes and inspires further efforts towards
practical modeling of video quality problems for potential real-time and
low-latency applications. To promote public usage, an implementation of RAPIQUE
has been made freely available online: \url{https://github.com/vztu/RAPIQUE}.",arxiv
http://arxiv.org/abs/1802.03257v1,2018-02-09T13:46:41Z,2018-02-09T13:46:41Z,"Video Event Recognition and Anomaly Detection by Combining Gaussian
  Process and Hierarchical Dirichlet Process Models","In this paper, we present an unsupervised learning framework for analyzing
activities and interactions in surveillance videos. In our framework, three
levels of video events are connected by Hierarchical Dirichlet Process (HDP)
model: low-level visual features, simple atomic activities, and multi-agent
interactions. Atomic activities are represented as distribution of low-level
features, while complicated interactions are represented as distribution of
atomic activities. This learning process is unsupervised. Given a training
video sequence, low-level visual features are extracted based on optic flow and
then clustered into different atomic activities and video clips are clustered
into different interactions. The HDP model automatically decide the number of
clusters, i.e. the categories of atomic activities and interactions. Based on
the learned atomic activities and interactions, a training dataset is generated
to train the Gaussian Process (GP) classifier. Then the trained GP models work
in newly captured video to classify interactions and detect abnormal events in
real time. Furthermore, the temporal dependencies between video events learned
by HDP-Hidden Markov Models (HMM) are effectively integrated into GP classifier
to enhance the accuracy of the classification in newly captured videos. Our
framework couples the benefits of the generative model (HDP) with the
discriminant model (GP). We provide detailed experiments showing that our
framework enjoys favorable performance in video event classification in
real-time in a crowded traffic scene.",arxiv
http://arxiv.org/abs/1908.03440v1,2019-08-08T07:53:24Z,2019-08-08T07:53:24Z,"Learning to Grasp from 2.5D images: a Deep Reinforcement Learning
  Approach","In this paper, we propose a deep reinforcement learning (DRL) solution to the
grasping problem using 2.5D images as the only source of information. In
particular, we developed a simulated environment where a robot equipped with a
vacuum gripper has the aim of reaching blocks with planar surfaces. These
blocks can have different dimensions, shapes, position and orientation. Unity
3D allowed us to simulate a real-world setup, where a depth camera is placed in
a fixed position and the stream of images is used by our policy network to
learn how to solve the task. We explored different DRL algorithms and problem
configurations. The experiments demonstrated the effectiveness of the proposed
DRL algorithm applied to grasp tasks guided by visual depth camera inputs. When
using the proper policy, the proposed method estimates a robot tool
configuration that reaches the object surface with negligible position and
orientation errors. This is, to the best of our knowledge, the first successful
attempt of using 2.5D images only as of the input of a DRL algorithm, to solve
the grasping problem regressing 3D world coordinates.",arxiv
http://arxiv.org/abs/1902.09868v2,2019-03-12T14:20:04Z,2019-02-26T11:23:54Z,"RepNet: Weakly Supervised Training of an Adversarial Reprojection
  Network for 3D Human Pose Estimation","This paper addresses the problem of 3D human pose estimation from single
images. While for a long time human skeletons were parameterized and fitted to
the observation by satisfying a reprojection error, nowadays researchers
directly use neural networks to infer the 3D pose from the observations.
However, most of these approaches ignore the fact that a reprojection
constraint has to be satisfied and are sensitive to overfitting. We tackle the
overfitting problem by ignoring 2D to 3D correspondences. This efficiently
avoids a simple memorization of the training data and allows for a weakly
supervised training. One part of the proposed reprojection network (RepNet)
learns a mapping from a distribution of 2D poses to a distribution of 3D poses
using an adversarial training approach. Another part of the network estimates
the camera. This allows for the definition of a network layer that performs the
reprojection of the estimated 3D pose back to 2D which results in a
reprojection loss function. Our experiments show that RepNet generalizes well
to unknown data and outperforms state-of-the-art methods when applied to unseen
data. Moreover, our implementation runs in real-time on a standard desktop PC.",arxiv
http://arxiv.org/abs/1701.00561v1,2017-01-03T01:10:51Z,2017-01-03T01:10:51Z,Robust and Real-time Deep Tracking Via Multi-Scale Domain Adaptation,"Visual tracking is a fundamental problem in computer vision. Recently, some
deep-learning-based tracking algorithms have been achieving record-breaking
performances. However, due to the high complexity of deep learning, most deep
trackers suffer from low tracking speed, and thus are impractical in many
real-world applications. Some new deep trackers with smaller network structure
achieve high efficiency while at the cost of significant decrease on precision.
In this paper, we propose to transfer the feature for image classification to
the visual tracking domain via convolutional channel reductions. The channel
reduction could be simply viewed as an additional convolutional layer with the
specific task. It not only extracts useful information for object tracking but
also significantly increases the tracking speed. To better accommodate the
useful feature of the target in different scales, the adaptation filters are
designed with different sizes. The yielded visual tracker is real-time and also
illustrates the state-of-the-art accuracies in the experiment involving two
well-adopted benchmarks with more than 100 test videos.",arxiv
http://arxiv.org/abs/1810.03043v1,2018-10-06T19:51:46Z,2018-10-06T19:51:46Z,"Robustness via Retrying: Closed-Loop Robotic Manipulation with
  Self-Supervised Learning","Prediction is an appealing objective for self-supervised learning of
behavioral skills, particularly for autonomous robots. However, effectively
utilizing predictive models for control, especially with raw image inputs,
poses a number of major challenges. How should the predictions be used? What
happens when they are inaccurate? In this paper, we tackle these questions by
proposing a method for learning robotic skills from raw image observations,
using only autonomously collected experience. We show that even an imperfect
model can complete complex tasks if it can continuously retry, but this
requires the model to not lose track of the objective (e.g., the object of
interest). To enable a robot to continuously retry a task, we devise a
self-supervised algorithm for learning image registration, which can keep track
of objects of interest for the duration of the trial. We demonstrate that this
idea can be combined with a video-prediction based controller to enable complex
behaviors to be learned from scratch using only raw visual inputs, including
grasping, repositioning objects, and non-prehensile manipulation. Our
real-world experiments demonstrate that a model trained with 160 robot hours of
autonomously collected, unlabeled data is able to successfully perform complex
manipulation tasks with a wide range of objects not seen during training.",arxiv
http://arxiv.org/abs/1803.09956v3,2018-09-30T20:34:49Z,2018-03-27T08:31:28Z,"Learning Synergies between Pushing and Grasping with Self-supervised
  Deep Reinforcement Learning","Skilled robotic manipulation benefits from complex synergies between
non-prehensile (e.g. pushing) and prehensile (e.g. grasping) actions: pushing
can help rearrange cluttered objects to make space for arms and fingers;
likewise, grasping can help displace objects to make pushing movements more
precise and collision-free. In this work, we demonstrate that it is possible to
discover and learn these synergies from scratch through model-free deep
reinforcement learning. Our method involves training two fully convolutional
networks that map from visual observations to actions: one infers the utility
of pushes for a dense pixel-wise sampling of end effector orientations and
locations, while the other does the same for grasping. Both networks are
trained jointly in a Q-learning framework and are entirely self-supervised by
trial and error, where rewards are provided from successful grasps. In this
way, our policy learns pushing motions that enable future grasps, while
learning grasps that can leverage past pushes. During picking experiments in
both simulation and real-world scenarios, we find that our system quickly
learns complex behaviors amid challenging cases of clutter, and achieves better
grasping success rates and picking efficiencies than baseline alternatives
after only a few hours of training. We further demonstrate that our method is
capable of generalizing to novel objects. Qualitative results (videos), code,
pre-trained models, and simulation environments are available at
http://vpg.cs.princeton.edu",arxiv
http://arxiv.org/abs/2106.07156v1,2021-06-14T04:31:15Z,2021-06-14T04:31:15Z,Temporal Predictive Coding For Model-Based Planning In Latent Space,"High-dimensional observations are a major challenge in the application of
model-based reinforcement learning (MBRL) to real-world environments. To handle
high-dimensional sensory inputs, existing approaches use representation
learning to map high-dimensional observations into a lower-dimensional latent
space that is more amenable to dynamics estimation and planning. In this work,
we present an information-theoretic approach that employs temporal predictive
coding to encode elements in the environment that can be predicted across time.
Since this approach focuses on encoding temporally-predictable information, we
implicitly prioritize the encoding of task-relevant components over nuisance
information within the environment that are provably task-irrelevant. By
learning this representation in conjunction with a recurrent state space model,
we can then perform planning in latent space. We evaluate our model on a
challenging modification of standard DMControl tasks where the background is
replaced with natural videos that contain complex but irrelevant information to
the planning task. Our experiments show that our model is superior to existing
methods in the challenging complex-background setting while remaining
competitive with current state-of-the-art models in the standard setting.",arxiv
http://arxiv.org/abs/1803.03487v2,2018-10-09T13:05:43Z,2018-03-09T12:27:14Z,"Cooperative Starting Movement Detection of Cyclists Using Convolutional
  Neural Networks and a Boosted Stacking Ensemble","In future, vehicles and other traffic participants will be interconnected and
equipped with various types of sensors, allowing for cooperation on different
levels, such as situation prediction or intention detection. In this article we
present a cooperative approach for starting movement detection of cyclists
using a boosted stacking ensemble approach realizing feature- and decision
level cooperation. We introduce a novel method based on a 3D Convolutional
Neural Network (CNN) to detect starting motions on image sequences by learning
spatio-temporal features. The CNN is complemented by a smart device based
starting movement detection originating from smart devices carried by the
cyclist. Both model outputs are combined in a stacking ensemble approach using
an extreme gradient boosting classifier resulting in a fast and yet robust
cooperative starting movement detector. We evaluate our cooperative approach on
real-world data originating from experiments with 49 test subjects consisting
of 84 starting motions.",arxiv
http://arxiv.org/abs/1707.00893v4,2019-07-01T13:44:40Z,2017-07-04T10:19:34Z,"Optimization Beyond the Convolution: Generalizing Spatial Relations with
  End-to-End Metric Learning","To operate intelligently in domestic environments, robots require the ability
to understand arbitrary spatial relations between objects and to generalize
them to objects of varying sizes and shapes. In this work, we present a novel
end-to-end approach to generalize spatial relations based on distance metric
learning. We train a neural network to transform 3D point clouds of objects to
a metric space that captures the similarity of the depicted spatial relations,
using only geometric models of the objects. Our approach employs gradient-based
optimization to compute object poses in order to imitate an arbitrary target
relation by reducing the distance to it under the learned metric. Our results
based on simulated and real-world experiments show that the proposed method
enables robots to generalize spatial relations to unknown objects over a
continuous spectrum.",arxiv
http://arxiv.org/abs/1911.04460v2,2020-03-26T15:51:54Z,2019-11-11T18:56:49Z,360SD-Net: 360° Stereo Depth Estimation with Learnable Cost Volume,"Recently, end-to-end trainable deep neural networks have significantly
improved stereo depth estimation for perspective images. However, 360{\deg}
images captured under equirectangular projection cannot benefit from directly
adopting existing methods due to distortion introduced (i.e., lines in 3D are
not projected onto lines in 2D). To tackle this issue, we present a novel
architecture specifically designed for spherical disparity using the setting of
top-bottom 360{\deg} camera pairs. Moreover, we propose to mitigate the
distortion issue by (1) an additional input branch capturing the position and
relation of each pixel in the spherical coordinate, and (2) a cost volume built
upon a learnable shifting filter. Due to the lack of 360{\deg} stereo data, we
collect two 360{\deg} stereo datasets from Matterport3D and Stanford3D for
training and evaluation. Extensive experiments and ablation study are provided
to validate our method against existing algorithms. Finally, we show promising
results on real-world environments capturing images with two consumer-level
cameras.",arxiv
http://arxiv.org/abs/2110.05379v1,2021-10-11T16:11:26Z,2021-10-11T16:11:26Z,Point Cloud Augmentation with Weighted Local Transformations,"Despite the extensive usage of point clouds in 3D vision, relatively limited
data are available for training deep neural networks. Although data
augmentation is a standard approach to compensate for the scarcity of data, it
has been less explored in the point cloud literature. In this paper, we propose
a simple and effective augmentation method called PointWOLF for point cloud
augmentation. The proposed method produces smoothly varying non-rigid
deformations by locally weighted transformations centered at multiple anchor
points. The smooth deformations allow diverse and realistic augmentations.
Furthermore, in order to minimize the manual efforts to search the optimal
hyperparameters for augmentation, we present AugTune, which generates augmented
samples of desired difficulties producing targeted confidence scores. Our
experiments show our framework consistently improves the performance for both
shape classification and part segmentation tasks. Particularly, with
PointNet++, PointWOLF achieves the state-of-the-art 89.7 accuracy on shape
classification with the real-world ScanObjectNN dataset.",arxiv
http://arxiv.org/abs/2106.13711v1,2021-06-22T21:21:29Z,2021-06-22T21:21:29Z,Multimodal Emergent Fake News Detection via Meta Neural Process Networks,"Fake news travels at unprecedented speeds, reaches global audiences and puts
users and communities at great risk via social media platforms. Deep learning
based models show good performance when trained on large amounts of labeled
data on events of interest, whereas the performance of models tends to degrade
on other events due to domain shift. Therefore, significant challenges are
posed for existing detection approaches to detect fake news on emergent events,
where large-scale labeled datasets are difficult to obtain. Moreover, adding
the knowledge from newly emergent events requires to build a new model from
scratch or continue to fine-tune the model, which can be challenging,
expensive, and unrealistic for real-world settings. In order to address those
challenges, we propose an end-to-end fake news detection framework named
MetaFEND, which is able to learn quickly to detect fake news on emergent events
with a few verified posts. Specifically, the proposed model integrates
meta-learning and neural process methods together to enjoy the benefits of
these approaches. In particular, a label embedding module and a hard attention
mechanism are proposed to enhance the effectiveness by handling categorical
information and trimming irrelevant posts. Extensive experiments are conducted
on multimedia datasets collected from Twitter and Weibo. The experimental
results show our proposed MetaFEND model can detect fake news on never-seen
events effectively and outperform the state-of-the-art methods.",arxiv
http://arxiv.org/abs/1609.03759v2,2016-12-13T16:09:17Z,2016-09-13T10:40:24Z,3D Simulation for Robot Arm Control with Deep Q-Learning,"Recent trends in robot arm control have seen a shift towards end-to-end
solutions, using deep reinforcement learning to learn a controller directly
from raw sensor data, rather than relying on a hand-crafted, modular pipeline.
However, the high dimensionality of the state space often means that it is
impractical to generate sufficient training data with real-world experiments.
As an alternative solution, we propose to learn a robot controller in
simulation, with the potential of then transferring this to a real robot.
Building upon the recent success of deep Q-networks, we present an approach
which uses 3D simulations to train a 7-DOF robotic arm in a control task
without any prior knowledge. The controller accepts images of the environment
as its only input, and outputs motor actions for the task of locating and
grasping a cube, over a range of initial configurations. To encourage efficient
learning, a structured reward function is designed with intermediate rewards.
We also present preliminary results in direct transfer of policies over to a
real robot, without any further training.",arxiv
http://arxiv.org/abs/2007.13666v2,2020-08-09T17:22:43Z,2020-07-27T16:19:52Z,"3D Human Shape and Pose from a Single Low-Resolution Image with
  Self-Supervised Learning","3D human shape and pose estimation from monocular images has been an active
area of research in computer vision, having a substantial impact on the
development of new applications, from activity recognition to creating virtual
avatars. Existing deep learning methods for 3D human shape and pose estimation
rely on relatively high-resolution input images; however, high-resolution
visual content is not always available in several practical scenarios such as
video surveillance and sports broadcasting. Low-resolution images in real
scenarios can vary in a wide range of sizes, and a model trained in one
resolution does not typically degrade gracefully across resolutions. Two common
approaches to solve the problem of low-resolution input are applying
super-resolution techniques to the input images which may result in visual
artifacts, or simply training one model for each resolution, which is
impractical in many realistic applications. To address the above issues, this
paper proposes a novel algorithm called RSC-Net, which consists of a
Resolution-aware network, a Self-supervision loss, and a Contrastive learning
scheme. The proposed network is able to learn the 3D body shape and pose across
different resolutions with a single model. The self-supervision loss encourages
scale-consistency of the output, and the contrastive learning scheme enforces
scale-consistency of the deep features. We show that both these new training
losses provide robustness when learning 3D shape and pose in a
weakly-supervised manner. Extensive experiments demonstrate that the RSC-Net
can achieve consistently better results than the state-of-the-art methods for
challenging low-resolution images.",arxiv
http://arxiv.org/abs/1911.05761v1,2019-11-13T19:07:07Z,2019-11-13T19:07:07Z,Predicting Unobserved Space For Planning via Depth Map Augmentation,"Safe and efficient path planning is crucial for autonomous mobile robots. A
prerequisite for path planning is to have a comprehensive understanding of the
3D structure of the robot's environment. On MAVs this is commonly achieved
using low-cost sensors, such as stereo or RGB-D cameras. These sensors may fail
to provide depth measurements in textureless or IR-absorbing areas and have
limited effective range. In path planning, this results in inefficient
trajectories or failure to recognize a feasible path to the goal, hence
significantly impairing the robot's mobility. Recent advances in deep learning
enables us to exploit prior experience about the shape of the world and hence
to infer complete depth maps from color images and additional sparse depth
measurements. In this work, we present an augmented planning system and
investigate the effects of employing state-of-the-art depth completion
techniques, specifically trained to augment sparse depth maps originating from
RGB-D sensors, semi-dense methods and stereo matchers. We extensively evaluate
our approach in online path planning experiments based on simulated data, as
well as global path planning experiments based on real world MAV data. We show
that our augmented system, provided with only sparse depth perception, can
reach on-par performance to ground truth depth input in simulated online
planning experiments. On real world MAV data the augmented system demonstrates
superior performance compared to a planner based on very dense RGB-D depth
maps.",arxiv
http://arxiv.org/abs/2005.01939v1,2020-05-05T04:25:16Z,2020-05-05T04:25:16Z,"From Image Collections to Point Clouds with Self-supervised Shape and
  Pose Networks","Reconstructing 3D models from 2D images is one of the fundamental problems in
computer vision. In this work, we propose a deep learning technique for 3D
object reconstruction from a single image. Contrary to recent works that either
use 3D supervision or multi-view supervision, we use only single view images
with no pose information during training as well. This makes our approach more
practical requiring only an image collection of an object category and the
corresponding silhouettes. We learn both 3D point cloud reconstruction and pose
estimation networks in a self-supervised manner, making use of differentiable
point cloud renderer to train with 2D supervision. A key novelty of the
proposed technique is to impose 3D geometric reasoning into predicted 3D point
clouds by rotating them with randomly sampled poses and then enforcing cycle
consistency on both 3D reconstructions and poses. In addition, using
single-view supervision allows us to do test-time optimization on a given test
image. Experiments on the synthetic ShapeNet and real-world Pix3D datasets
demonstrate that our approach, despite using less supervision, can achieve
competitive performance compared to pose-supervised and multi-view supervised
approaches.",arxiv
http://arxiv.org/abs/1808.05143v1,2018-08-15T15:31:14Z,2018-08-15T15:31:14Z,"Collaborative Brain-Computer Interface for Human Interest Detection in
  Complex and Dynamic Settings","Humans can fluidly adapt their interest in complex environments in ways that
machines cannot. Here, we lay the groundwork for a real-world system that
passively monitors and merges neural correlates of visual interest across team
members via Collaborative Brain Computer Interface (cBCI). When group interest
is detected and co-registered in time and space, it can be used to model the
task relevance of items in a dynamic, natural environment. Previous work in
cBCIs focuses on static stimuli, stimulus- or response- locked analyses, and
often within-subject and experiment model training. The contributions of this
work are twofold. First, we test the utility of cBCI on a scenario that more
closely resembles natural conditions, where subjects visually scanned a video
for target items in a virtual environment. Second, we use an
experiment-agnostic deep learning model to account for the real-world use case
where no training set exists that exactly matches the end-users task and
circumstances. With our approach we show improved performance as the number of
subjects in the cBCI ensemble grows, and the potential to reconstruct
ground-truth target occurrence in an otherwise noisy and complex environment.",arxiv
http://arxiv.org/abs/2105.10389v3,2021-11-09T22:30:45Z,2021-05-21T15:03:29Z,Learning Visible Connectivity Dynamics for Cloth Smoothing,"Robotic manipulation of cloth remains challenging for robotics due to the
complex dynamics of the cloth, lack of a low-dimensional state representation,
and self-occlusions. In contrast to previous model-based approaches that learn
a pixel-based dynamics model or a compressed latent vector dynamics, we propose
to learn a particle-based dynamics model from a partial point cloud
observation. To overcome the challenges of partial observability, we infer
which visible points are connected on the underlying cloth mesh. We then learn
a dynamics model over this visible connectivity graph. Compared to previous
learning-based approaches, our model poses strong inductive bias with its
particle based representation for learning the underlying cloth physics; it is
invariant to visual features; and the predictions can be more easily
visualized. We show that our method greatly outperforms previous
state-of-the-art model-based and model-free reinforcement learning methods in
simulation. Furthermore, we demonstrate zero-shot sim-to-real transfer where we
deploy the model trained in simulation on a Franka arm and show that the model
can successfully smooth different types of cloth from crumpled configurations.
Videos can be found on our project website.",arxiv
http://arxiv.org/abs/1808.03941v2,2019-05-05T03:42:19Z,2018-08-12T13:30:27Z,"Denoising of 3-D Magnetic Resonance Images Using a Residual
  Encoder-Decoder Wasserstein Generative Adversarial Network","Structure-preserved denoising of 3D magnetic resonance imaging (MRI) images
is a critical step in medical image analysis. Over the past few years, many
algorithms with impressive performances have been proposed. In this paper,
inspired by the idea of deep learning, we introduce an MRI denoising method
based on the residual encoder-decoder Wasserstein generative adversarial
network (RED-WGAN). Specifically, to explore the structure similarity between
neighboring slices, a 3D configuration is utilized as the basic processing
unit. Residual autoencoders combined with deconvolution operations are
introduced into the generator network. Furthermore, to alleviate the
oversmoothing shortcoming of the traditional mean squared error (MSE) loss
function, the perceptual similarity, which is implemented by calculating the
distances in the feature space extracted by a pretrained VGG-19 network, is
incorporated with the MSE and adversarial losses to form the new loss function.
Extensive experiments are implemented to assess the performance of the proposed
method. The experimental results show that the proposed RED-WGAN achieves
performance superior to several state-of-the-art methods in both simulated and
real clinical data. In particular, our method demonstrates powerful abilities
in both noise suppression and structure preservation.",arxiv
http://arxiv.org/abs/1808.01356v1,2018-07-31T10:33:09Z,2018-07-31T10:33:09Z,"Deep Learning-Based Multiple Object Visual Tracking on Embedded System
  for IoT and Mobile Edge Computing Applications","Compute and memory demands of state-of-the-art deep learning methods are
still a shortcoming that must be addressed to make them useful at IoT
end-nodes. In particular, recent results depict a hopeful prospect for image
processing using Convolutional Neural Netwoks, CNNs, but the gap between
software and hardware implementations is already considerable for IoT and
mobile edge computing applications due to their high power consumption. This
proposal performs low-power and real time deep learning-based multiple object
visual tracking implemented on an NVIDIA Jetson TX2 development kit. It
includes a camera and wireless connection capability and it is battery powered
for mobile and outdoor applications. A collection of representative sequences
captured with the on-board camera, dETRUSC video dataset, is used to exemplify
the performance of the proposed algorithm and to facilitate benchmarking. The
results in terms of power consumption and frame rate demonstrate the
feasibility of deep learning algorithms on embedded platforms although more
effort to joint algorithm and hardware design of CNNs is needed.",arxiv
http://arxiv.org/abs/2012.11547v1,2020-12-21T18:28:17Z,2020-12-21T18:28:17Z,Offline Reinforcement Learning from Images with Latent Space Models,"Offline reinforcement learning (RL) refers to the problem of learning
policies from a static dataset of environment interactions. Offline RL enables
extensive use and re-use of historical datasets, while also alleviating safety
concerns associated with online exploration, thereby expanding the real-world
applicability of RL. Most prior work in offline RL has focused on tasks with
compact state representations. However, the ability to learn directly from rich
observation spaces like images is critical for real-world applications such as
robotics. In this work, we build on recent advances in model-based algorithms
for offline RL, and extend them to high-dimensional visual observation spaces.
Model-based offline RL algorithms have achieved state of the art results in
state based tasks and have strong theoretical guarantees. However, they rely
crucially on the ability to quantify uncertainty in the model predictions,
which is particularly challenging with image observations. To overcome this
challenge, we propose to learn a latent-state dynamics model, and represent the
uncertainty in the latent space. Our approach is both tractable in practice and
corresponds to maximizing a lower bound of the ELBO in the unknown POMDP. In
experiments on a range of challenging image-based locomotion and manipulation
tasks, we find that our algorithm significantly outperforms previous offline
model-free RL methods as well as state-of-the-art online visual model-based RL
methods. Moreover, we also find that our approach excels on an image-based
drawer closing task on a real robot using a pre-existing dataset. All results
including videos can be found online at https://sites.google.com/view/lompo/ .",arxiv
http://arxiv.org/abs/1711.08664v1,2017-11-23T12:06:20Z,2017-11-23T12:06:20Z,Self-view Grounding Given a Narrated 360° Video,"Narrated 360{\deg} videos are typically provided in many touring scenarios to
mimic real-world experience. However, previous work has shown that smart
assistance (i.e., providing visual guidance) can significantly help users to
follow the Normal Field of View (NFoV) corresponding to the narrative. In this
project, we aim at automatically grounding the NFoVs of a 360{\deg} video given
subtitles of the narrative (referred to as ""NFoV-grounding""). We propose a
novel Visual Grounding Model (VGM) to implicitly and efficiently predict the
NFoVs given the video content and subtitles. Specifically, at each frame, we
efficiently encode the panorama into feature map of candidate NFoVs using a
Convolutional Neural Network (CNN) and the subtitles to the same hidden space
using an RNN with Gated Recurrent Units (GRU). Then, we apply soft-attention on
candidate NFoVs to trigger sentence decoder aiming to minimize the reconstruct
loss between the generated and given sentence. Finally, we obtain the NFoV as
the candidate NFoV with the maximum attention without any human supervision. To
train VGM more robustly, we also generate a reverse sentence conditioning on
one minus the soft-attention such that the attention focuses on candidate NFoVs
less relevant to the given sentence. The negative log reconstruction loss of
the reverse sentence (referred to as ""irrelevant loss"") is jointly minimized to
encourage the reverse sentence to be different from the given sentence. To
evaluate our method, we collect the first narrated 360{\deg} videos dataset and
achieve state-of-the-art NFoV-grounding performance.",arxiv
http://arxiv.org/abs/2008.02763v1,2020-08-06T17:04:34Z,2020-08-06T17:04:34Z,"Joint Self-Attention and Scale-Aggregation for Self-Calibrated Deraining
  Network","In the field of multimedia, single image deraining is a basic pre-processing
work, which can greatly improve the visual effect of subsequent high-level
tasks in rainy conditions. In this paper, we propose an effective algorithm,
called JDNet, to solve the single image deraining problem and conduct the
segmentation and detection task for applications. Specifically, considering the
important information on multi-scale features, we propose a Scale-Aggregation
module to learn the features with different scales. Simultaneously,
Self-Attention module is introduced to match or outperform their convolutional
counterparts, which allows the feature aggregation to adapt to each channel.
Furthermore, to improve the basic convolutional feature transformation process
of Convolutional Neural Networks (CNNs), Self-Calibrated convolution is applied
to build long-range spatial and inter-channel dependencies around each spatial
location that explicitly expand fields-of-view of each convolutional layer
through internal communications and hence enriches the output features. By
designing the Scale-Aggregation and Self-Attention modules with Self-Calibrated
convolution skillfully, the proposed model has better deraining results both on
real-world and synthetic datasets. Extensive experiments are conducted to
demonstrate the superiority of our method compared with state-of-the-art
methods. The source code will be available at
\url{https://supercong94.wixsite.com/supercong94}.",arxiv
http://arxiv.org/abs/2108.03354v1,2021-08-07T03:03:52Z,2021-08-07T03:03:52Z,"HetEmotionNet: Two-Stream Heterogeneous Graph Recurrent Neural Network
  for Multi-modal Emotion Recognition","The research on human emotion under multimedia stimulation based on
physiological signals is an emerging field, and important progress has been
achieved for emotion recognition based on multi-modal signals. However, it is
challenging to make full use of the complementarity among
spatial-spectral-temporal domain features for emotion recognition, as well as
model the heterogeneity and correlation among multi-modal signals. In this
paper, we propose a novel two-stream heterogeneous graph recurrent neural
network, named HetEmotionNet, fusing multi-modal physiological signals for
emotion recognition. Specifically, HetEmotionNet consists of the
spatial-temporal stream and the spatial-spectral stream, which can fuse
spatial-spectral-temporal domain features in a unified framework. Each stream
is composed of the graph transformer network for modeling the heterogeneity,
the graph convolutional network for modeling the correlation, and the gated
recurrent unit for capturing the temporal domain or spectral domain dependency.
Extensive experiments on two real-world datasets demonstrate that our proposed
model achieves better performance than state-of-the-art baselines.",arxiv
http://arxiv.org/abs/1802.10408v3,2018-09-24T20:22:52Z,2018-02-28T13:49:18Z,"A Neurorobotic Experiment for Crossmodal Conflict Resolution in Complex
  Environments","Crossmodal conflict resolution is crucial for robot sensorimotor coupling
through the interaction with the environment, yielding swift and robust
behaviour also in noisy conditions. In this paper, we propose a neurorobotic
experiment in which an iCub robot exhibits human-like responses in a complex
crossmodal environment. To better understand how humans deal with multisensory
conflicts, we conducted a behavioural study exposing 33 subjects to congruent
and incongruent dynamic audio-visual cues. In contrast to previous studies
using simplified stimuli, we designed a scenario with four animated avatars and
observed that the magnitude and extension of the visual bias are related to the
semantics embedded in the scene, i.e., visual cues that are congruent with
environmental statistics (moving lips and vocalization) induce the strongest
bias. We implement a deep learning model that processes stereophonic sound,
facial features, and body motion to trigger a discrete behavioural response.
After training the model, we exposed the iCub to the same experimental
conditions as the human subjects, showing that the robot can replicate similar
responses in real time. Our interdisciplinary work provides important insights
into how crossmodal conflict resolution can be modelled in robots and
introduces future research directions for the efficient combination of sensory
observations with internally generated knowledge and expectations.",arxiv
http://arxiv.org/abs/1611.08930v2,2017-03-28T03:15:07Z,2016-11-27T22:47:23Z,Deep attractor network for single-microphone speaker separation,"Despite the overwhelming success of deep learning in various speech
processing tasks, the problem of separating simultaneous speakers in a mixture
remains challenging. Two major difficulties in such systems are the arbitrary
source permutation and unknown number of sources in the mixture. We propose a
novel deep learning framework for single channel speech separation by creating
attractor points in high dimensional embedding space of the acoustic signals
which pull together the time-frequency bins corresponding to each source.
Attractor points in this study are created by finding the centroids of the
sources in the embedding space, which are subsequently used to determine the
similarity of each bin in the mixture to each source. The network is then
trained to minimize the reconstruction error of each source by optimizing the
embeddings. The proposed model is different from prior works in that it
implements an end-to-end training, and it does not depend on the number of
sources in the mixture. Two strategies are explored in the test time, K-means
and fixed attractor points, where the latter requires no post-processing and
can be implemented in real-time. We evaluated our system on Wall Street Journal
dataset and show 5.49\% improvement over the previous state-of-the-art methods.",arxiv
http://arxiv.org/abs/1903.10883v1,2019-03-25T15:40:34Z,2019-03-25T15:40:34Z,Generalized Feedback Loop for Joint Hand-Object Pose Estimation,"We propose an approach to estimating the 3D pose of a hand, possibly handling
an object, given a depth image. We show that we can correct the mistakes made
by a Convolutional Neural Network trained to predict an estimate of the 3D pose
by using a feedback loop. The components of this feedback loop are also Deep
Networks, optimized using training data. This approach can be generalized to a
hand interacting with an object. Therefore, we jointly estimate the 3D pose of
the hand and the 3D pose of the object. Our approach performs en-par with
state-of-the-art methods for 3D hand pose estimation, and outperforms
state-of-the-art methods for joint hand-object pose estimation when using depth
images only. Also, our approach is efficient as our implementation runs in
real-time on a single GPU.",arxiv
http://arxiv.org/abs/1711.05253v3,2018-03-30T17:37:00Z,2017-11-14T18:56:12Z,"Learning Image-Conditioned Dynamics Models for Control of Under-actuated
  Legged Millirobots","Millirobots are a promising robotic platform for many applications due to
their small size and low manufacturing costs. Legged millirobots, in
particular, can provide increased mobility in complex environments and improved
scaling of obstacles. However, controlling these small, highly dynamic, and
underactuated legged systems is difficult. Hand-engineered controllers can
sometimes control these legged millirobots, but they have difficulties with
dynamic maneuvers and complex terrains. We present an approach for controlling
a real-world legged millirobot that is based on learned neural network models.
Using less than 17 minutes of data, our method can learn a predictive model of
the robot's dynamics that can enable effective gaits to be synthesized on the
fly for following user-specified waypoints on a given terrain. Furthermore, by
leveraging expressive, high-capacity neural network models, our approach allows
for these predictions to be directly conditioned on camera images, endowing the
robot with the ability to predict how different terrains might affect its
dynamics. This enables sample-efficient and effective learning for locomotion
of a dynamic legged millirobot on various terrains, including gravel, turf,
carpet, and styrofoam. Experiment videos can be found at
https://sites.google.com/view/imageconddyn",arxiv
http://arxiv.org/abs/2006.16621v1,2020-06-30T09:19:54Z,2020-06-30T09:19:54Z,A Simple Domain Shifting Networkfor Generating Low Quality Images,"Deep Learning systems have proven to be extremely successful for image
recognition tasks for which significant amounts of training data is available,
e.g., on the famous ImageNet dataset. We demonstrate that for robotics
applications with cheap camera equipment, the low image quality,
however,influences the classification accuracy, and freely available databases
cannot be exploited in a straight forward way to train classifiers to be used
on a robot. As a solution we propose to train a network on degrading the
quality images in order to mimic specific low quality imaging systems.
Numerical experiments demonstrate that classification networks trained by using
images produced by our quality degrading network along with the high quality
images outperform classification networks trained only on high quality data
when used on a real robot system, while being significantly easier to use than
competing zero-shot domain adaptation techniques.",arxiv
http://arxiv.org/abs/1708.01566v1,2017-08-04T16:03:52Z,2017-08-04T16:03:52Z,"Augmented Reality Meets Computer Vision : Efficient Data Generation for
  Urban Driving Scenes","The success of deep learning in computer vision is based on availability of
large annotated datasets. To lower the need for hand labeled images, virtually
rendered 3D worlds have recently gained popularity. Creating realistic 3D
content is challenging on its own and requires significant human effort. In
this work, we propose an alternative paradigm which combines real and synthetic
data for learning semantic instance segmentation and object detection models.
Exploiting the fact that not all aspects of the scene are equally important for
this task, we propose to augment real-world imagery with virtual objects of the
target category. Capturing real-world images at large scale is easy and cheap,
and directly provides real background appearances without the need for creating
complex 3D models of the environment. We present an efficient procedure to
augment real images with virtual objects. This allows us to create realistic
composite images which exhibit both realistic background appearance and a large
number of complex object arrangements. In contrast to modeling complete 3D
environments, our augmentation approach requires only a few user interactions
in combination with 3D shapes of the target object. Through extensive
experimentation, we conclude the right set of parameters to produce augmented
data which can maximally enhance the performance of instance segmentation
models. Further, we demonstrate the utility of our approach on training
standard deep models for semantic instance segmentation and object detection of
cars in outdoor driving scenes. We test the models trained on our augmented
data on the KITTI 2015 dataset, which we have annotated with pixel-accurate
ground truth, and on Cityscapes dataset. Our experiments demonstrate that
models trained on augmented imagery generalize better than those trained on
synthetic data or models trained on limited amount of annotated real data.",arxiv
http://arxiv.org/abs/2005.11101v1,2020-05-22T10:54:49Z,2020-05-22T10:54:49Z,"A Comparative Evaluation of Heart Rate Estimation Methods using Face
  Videos","This paper presents a comparative evaluation of methods for remote heart rate
estimation using face videos, i.e., given a video sequence of the face as
input, methods to process it to obtain a robust estimation of the subjects
heart rate at each moment. Four alternatives from the literature are tested,
three based in hand crafted approaches and one based on deep learning. The
methods are compared using RGB videos from the COHFACE database. Experiments
show that the learning-based method achieves much better accuracy than the hand
crafted ones. The low error rate achieved by the learning based model makes
possible its application in real scenarios, e.g. in medical or sports
environments.",arxiv
http://arxiv.org/abs/1910.02550v2,2019-10-14T17:29:36Z,2019-10-06T23:22:56Z,ClearGrasp: 3D Shape Estimation of Transparent Objects for Manipulation,"Transparent objects are a common part of everyday life, yet they possess
unique visual properties that make them incredibly difficult for standard 3D
sensors to produce accurate depth estimates for. In many cases, they often
appear as noisy or distorted approximations of the surfaces that lie behind
them. To address these challenges, we present ClearGrasp -- a deep learning
approach for estimating accurate 3D geometry of transparent objects from a
single RGB-D image for robotic manipulation. Given a single RGB-D image of
transparent objects, ClearGrasp uses deep convolutional networks to infer
surface normals, masks of transparent surfaces, and occlusion boundaries. It
then uses these outputs to refine the initial depth estimates for all
transparent surfaces in the scene. To train and test ClearGrasp, we construct a
large-scale synthetic dataset of over 50,000 RGB-D images, as well as a
real-world test benchmark with 286 RGB-D images of transparent objects and
their ground truth geometries. The experiments demonstrate that ClearGrasp is
substantially better than monocular depth estimation baselines and is capable
of generalizing to real-world images and novel objects. We also demonstrate
that ClearGrasp can be applied out-of-the-box to improve grasping algorithms'
performance on transparent objects. Code, data, and benchmarks will be
released. Supplementary materials available on the project website:
https://sites.google.com/view/cleargrasp",arxiv
http://arxiv.org/abs/2008.01003v2,2021-02-25T18:54:30Z,2020-08-03T16:41:19Z,"Teacher-Student Training and Triplet Loss for Facial Expression
  Recognition under Occlusion","In this paper, we study the task of facial expression recognition under
strong occlusion. We are particularly interested in cases where 50% of the face
is occluded, e.g. when the subject wears a Virtual Reality (VR) headset. While
previous studies show that pre-training convolutional neural networks (CNNs) on
fully-visible (non-occluded) faces improves the accuracy, we propose to employ
knowledge distillation to achieve further improvements. First of all, we employ
the classic teacher-student training strategy, in which the teacher is a CNN
trained on fully-visible faces and the student is a CNN trained on occluded
faces. Second of all, we propose a new approach for knowledge distillation
based on triplet loss. During training, the goal is to reduce the distance
between an anchor embedding, produced by a student CNN that takes occluded
faces as input, and a positive embedding (from the same class as the anchor),
produced by a teacher CNN trained on fully-visible faces, so that it becomes
smaller than the distance between the anchor and a negative embedding (from a
different class than the anchor), produced by the student CNN. Third of all, we
propose to combine the distilled embeddings obtained through the classic
teacher-student strategy and our novel teacher-student strategy based on
triplet loss into a single embedding vector. We conduct experiments on two
benchmarks, FER+ and AffectNet, with two CNN architectures, VGG-f and VGG-face,
showing that knowledge distillation can bring significant improvements over the
state-of-the-art methods designed for occluded faces in the VR setting.",arxiv
http://arxiv.org/abs/1906.09832v1,2019-06-24T10:14:24Z,2019-06-24T10:14:24Z,"A computational model of early language acquisition from audiovisual
  experiences of young infants","Earlier research has suggested that human infants might use statistical
dependencies between speech and non-linguistic multimodal input to bootstrap
their language learning before they know how to segment words from running
speech. However, feasibility of this hypothesis in terms of real-world infant
experiences has remained unclear. This paper presents a step towards a more
realistic test of the multimodal bootstrapping hypothesis by describing a
neural network model that can learn word segments and their meanings from
referentially ambiguous acoustic input. The model is tested on recordings of
real infant-caregiver interactions using utterance-level labels for concrete
visual objects that were attended by the infant when caregiver spoke an
utterance containing the name of the object, and using random visual labels for
utterances during absence of attention. The results show that beginnings of
lexical knowledge may indeed emerge from individually ambiguous learning
scenarios. In addition, the hidden layers of the network show gradually
increasing selectivity to phonetic categories as a function of layer depth,
resembling models trained for phone recognition in a supervised manner.",arxiv
http://arxiv.org/abs/2103.16849v2,2021-08-26T09:57:49Z,2021-03-31T07:03:26Z,"TeCANet: Temporal-Contextual Attention Network for Environment-Aware
  Speech Dereverberation","In this paper, we exploit the effective way to leverage contextual
information to improve the speech dereverberation performance in real-world
reverberant environments. We propose a temporal-contextual attention approach
on the deep neural network (DNN) for environment-aware speech dereverberation,
which can adaptively attend to the contextual information. More specifically, a
FullBand based Temporal Attention approach (FTA) is proposed, which models the
correlations between the fullband information of the context frames. In
addition, considering the difference between the attenuation of high frequency
bands and low frequency bands (high frequency bands attenuate faster than low
frequency bands) in the room impulse response (RIR), we also propose a SubBand
based Temporal Attention approach (STA). In order to guide the network to be
more aware of the reverberant environments, we jointly optimize the
dereverberation network and the reverberation time (RT60) estimator in a
multi-task manner. Our experimental results indicate that the proposed method
outperforms our previously proposed reverberation-time-aware DNN and the
learned attention weights are fully physical consistent. We also report a
preliminary yet promising dereverberation and recognition experiment on real
test data.",arxiv
http://arxiv.org/abs/2109.04153v1,2021-09-09T10:28:37Z,2021-09-09T10:28:37Z,Single Image 3D Object Estimation with Primitive Graph Networks,"Reconstructing 3D object from a single image (RGB or depth) is a fundamental
problem in visual scene understanding and yet remains challenging due to its
ill-posed nature and complexity in real-world scenes. To address those
challenges, we adopt a primitive-based representation for 3D object, and
propose a two-stage graph network for primitive-based 3D object estimation,
which consists of a sequential proposal module and a graph reasoning module.
Given a 2D image, our proposal module first generates a sequence of 3D
primitives from input image with local feature attention. Then the graph
reasoning module performs joint reasoning on a primitive graph to capture the
global shape context for each primitive. Such a framework is capable of taking
into account rich geometry and semantic constraints during 3D structure
recovery, producing 3D objects with more coherent structure even under
challenging viewing conditions. We train the entire graph neural network in a
stage-wise strategy and evaluate it on three benchmarks: Pix3D, ModelNet and
NYU Depth V2. Extensive experiments show that our approach outperforms the
previous state of the arts with a considerable margin.",arxiv
http://arxiv.org/abs/2104.11008v1,2021-04-22T12:10:38Z,2021-04-22T12:10:38Z,"Unsupervised anomaly detection for a Smart Autonomous Robotic Assistant
  Surgeon (SARAS)using a deep residual autoencoder","Anomaly detection in Minimally-Invasive Surgery (MIS) traditionally requires
a human expert monitoring the procedure from a console. Data scarcity, on the
other hand, hinders what would be a desirable migration towards autonomous
robotic-assisted surgical systems. Automated anomaly detection systems in this
area typically rely on classical supervised learning. Anomalous events in a
surgical setting, however, are rare, making it difficult to capture data to
train a detection model in a supervised fashion. In this work we thus propose
an unsupervised approach to anomaly detection for robotic-assisted surgery
based on deep residual autoencoders. The idea is to make the autoencoder learn
the 'normal' distribution of the data and detect abnormal events deviating from
this distribution by measuring the reconstruction error. The model is trained
and validated upon both the publicly available Cholec80 dataset, provided with
extra annotation, and on a set of videos captured on procedures using
artificial anatomies ('phantoms') produced as part of the Smart Autonomous
Robotic Assistant Surgeon (SARAS) project. The system achieves recall and
precision equal to 78.4%, 91.5%, respectively, on Cholec80 and of 95.6%, 88.1%
on the SARAS phantom dataset. The end-to-end system was developed and deployed
as part of the SARAS demonstration platform for real-time anomaly detection
with a processing time of about 25 ms per frame.",arxiv
http://arxiv.org/abs/2008.06181v2,2020-08-17T03:14:12Z,2020-08-14T03:49:14Z,"Apparel-invariant Feature Learning for Apparel-changed Person
  Re-identification","With the rise of deep learning methods, person Re-Identification (ReID)
performance has been improved tremendously in many public datasets. However,
most public ReID datasets are collected in a short time window in which
persons' appearance rarely changes. In real-world applications such as in a
shopping mall, the same person's clothing may change, and different persons may
wearing similar clothes. All these cases can result in an inconsistent ReID
performance, revealing a critical problem that current ReID models heavily rely
on person's apparels. Therefore, it is critical to learn an apparel-invariant
person representation under cases like cloth changing or several persons
wearing similar clothes. In this work, we tackle this problem from the
viewpoint of invariant feature representation learning. The main contributions
of this work are as follows. (1) We propose the semi-supervised
Apparel-invariant Feature Learning (AIFL) framework to learn an
apparel-invariant pedestrian representation using images of the same person
wearing different clothes. (2) To obtain images of the same person wearing
different clothes, we propose an unsupervised apparel-simulation GAN (AS-GAN)
to synthesize cloth changing images according to the target cloth embedding.
It's worth noting that the images used in ReID tasks were cropped from
real-world low-quality CCTV videos, making it more challenging to synthesize
cloth changing images. We conduct extensive experiments on several datasets
comparing with several baselines. Experimental results demonstrate that our
proposal can improve the ReID performance of the baseline models.",arxiv
http://arxiv.org/abs/1907.05418v1,2019-07-11T17:59:13Z,2019-07-11T17:59:13Z,Adversarial Objects Against LiDAR-Based Autonomous Driving Systems,"Deep neural networks (DNNs) are found to be vulnerable against adversarial
examples, which are carefully crafted inputs with a small magnitude of
perturbation aiming to induce arbitrarily incorrect predictions. Recent studies
show that adversarial examples can pose a threat to real-world
security-critical applications: a ""physical adversarial Stop Sign"" can be
synthesized such that the autonomous driving cars will misrecognize it as
others (e.g., a speed limit sign). However, these image-space adversarial
examples cannot easily alter 3D scans of widely equipped LiDAR or radar on
autonomous vehicles. In this paper, we reveal the potential vulnerabilities of
LiDAR-based autonomous driving detection systems, by proposing an optimization
based approach LiDAR-Adv to generate adversarial objects that can evade the
LiDAR-based detection system under various conditions. We first show the
vulnerabilities using a blackbox evolution-based algorithm, and then explore
how much a strong adversary can do, using our gradient-based approach
LiDAR-Adv. We test the generated adversarial objects on the Baidu Apollo
autonomous driving platform and show that such physical systems are indeed
vulnerable to the proposed attacks. We also 3D-print our adversarial objects
and perform physical experiments to illustrate that such vulnerability exists
in the real world. Please find more visualizations and results on the anonymous
website: https://sites.google.com/view/lidar-adv.",arxiv
http://arxiv.org/abs/1507.05695v1,2015-07-21T03:48:04Z,2015-07-21T03:48:04Z,"A neuromorphic hardware architecture using the Neural Engineering
  Framework for pattern recognition","We present a hardware architecture that uses the Neural Engineering Framework
(NEF) to implement large-scale neural networks on Field Programmable Gate
Arrays (FPGAs) for performing pattern recognition in real time. NEF is a
framework that is capable of synthesising large-scale cognitive systems from
subnetworks. We will first present the architecture of the proposed neural
network implemented using fixed-point numbers and demonstrate a routine that
computes the decoding weights by using the online pseudoinverse update method
(OPIUM) in a parallel and distributed manner. The proposed system is
efficiently implemented on a compact digital neural core. This neural core
consists of 64 neurons that are instantiated by a single physical neuron using
a time-multiplexing approach. As a proof of concept, we combined 128 identical
neural cores together to build a handwritten digit recognition system using the
MNIST database and achieved a recognition rate of 96.55%. The system is
implemented on a state-of-the-art FPGA and can process 5.12 million digits per
second. The architecture is not limited to handwriting recognition, but is
generally applicable as an extremely fast pattern recognition processor for
various kinds of patterns such as speech and images.",arxiv
http://arxiv.org/abs/2001.01026v2,2020-04-25T22:20:46Z,2020-01-04T03:12:38Z,Painting Many Pasts: Synthesizing Time Lapse Videos of Paintings,"We introduce a new video synthesis task: synthesizing time lapse videos
depicting how a given painting might have been created. Artists paint using
unique combinations of brushes, strokes, and colors. There are often many
possible ways to create a given painting. Our goal is to learn to capture this
rich range of possibilities.
  Creating distributions of long-term videos is a challenge for learning-based
video synthesis methods. We present a probabilistic model that, given a single
image of a completed painting, recurrently synthesizes steps of the painting
process. We implement this model as a convolutional neural network, and
introduce a novel training scheme to enable learning from a limited dataset of
painting time lapses. We demonstrate that this model can be used to sample many
time steps, enabling long-term stochastic video synthesis. We evaluate our
method on digital and watercolor paintings collected from video websites, and
show that human raters find our synthetic videos to be similar to time lapse
videos produced by real artists. Our code is available at
https://xamyzhao.github.io/timecraft.",arxiv
http://arxiv.org/abs/2110.03446v1,2021-10-06T00:25:22Z,2021-10-06T00:25:22Z,"A Hierarchical Variational Neural Uncertainty Model for Stochastic Video
  Prediction","Predicting the future frames of a video is a challenging task, in part due to
the underlying stochastic real-world phenomena. Prior approaches to solve this
task typically estimate a latent prior characterizing this stochasticity,
however do not account for the predictive uncertainty of the (deep learning)
model. Such approaches often derive the training signal from the mean-squared
error (MSE) between the generated frame and the ground truth, which can lead to
sub-optimal training, especially when the predictive uncertainty is high.
Towards this end, we introduce Neural Uncertainty Quantifier (NUQ) - a
stochastic quantification of the model's predictive uncertainty, and use it to
weigh the MSE loss. We propose a hierarchical, variational framework to derive
NUQ in a principled manner using a deep, Bayesian graphical model. Our
experiments on four benchmark stochastic video prediction datasets show that
our proposed framework trains more effectively compared to the state-of-the-art
models (especially when the training sets are small), while demonstrating
better video generation quality and diversity against several evaluation
metrics.",arxiv
http://arxiv.org/abs/2008.00706v1,2020-08-03T08:19:57Z,2020-08-03T08:19:57Z,LiDAR point-cloud processing based on projection methods: a comparison,"An accurate and rapid-response perception system is fundamental for
autonomous vehicles to operate safely. 3D object detection methods handle point
clouds given by LiDAR sensors to provide accurate depth and position
information for each detection, together with its dimensions and
classification. The information is then used to track vehicles and other
obstacles in the surroundings of the autonomous vehicle, and also to feed
control units that guarantee collision avoidance and motion planning. Nowadays,
object detection systems can be divided into two main categories. The first
ones are the geometric based, which retrieve the obstacles using geometric and
morphological operations on the 3D points. The seconds are the deep
learning-based, which process the 3D points, or an elaboration of the 3D
point-cloud, with deep learning techniques to retrieve a set of obstacles. This
paper presents a comparison between those two approaches, presenting one
implementation of each class on a real autonomous vehicle. Accuracy of the
estimates of the algorithms has been evaluated with experimental tests carried
in the Monza ENI circuit. The position of the ego vehicle and the obstacle is
given by GPS sensors with RTK correction, which guarantees an accurate ground
truth for the comparison. Both algorithms have been implemented on ROS and run
on a consumer laptop.",arxiv
http://arxiv.org/abs/2003.09670v2,2020-05-30T14:04:45Z,2020-03-21T14:34:36Z,"Identifying At-Risk K-12 Students in Multimodal Online Environments: A
  Machine Learning Approach","With the rapid emergence of K-12 online learning platforms, a new era of
education has been opened up. It is crucial to have a dropout warning framework
to preemptively identify K-12 students who are at risk of dropping out of the
online courses. Prior researchers have focused on predicting dropout in Massive
Open Online Courses (MOOCs), which often deliver higher education, i.e.,
graduate level courses at top institutions. However, few studies have focused
on developing a machine learning approach for students in K-12 online courses.
In this paper, we develop a machine learning framework to conduct accurate
at-risk student identification specialized in K-12 multimodal online
environments. Our approach considers both online and offline factors around
K-12 students and aims at solving the challenges of (1) multiple modalities,
i.e., K-12 online environments involve interactions from different modalities
such as video, voice, etc; (2) length variability, i.e., students with
different lengths of learning history; (3) time sensitivity, i.e., the dropout
likelihood is changing with time; and (4) data imbalance, i.e., only less than
20\% of K-12 students will choose to drop out the class. We conduct a wide
range of offline and online experiments to demonstrate the effectiveness of our
approach. In our offline experiments, we show that our method improves the
dropout prediction performance when compared to state-of-the-art baselines on a
real-world educational dataset. In our online experiments, we test our approach
on a third-party K-12 online tutoring platform for two months and the results
show that more than 70\% of dropout students are detected by the system.",arxiv
http://arxiv.org/abs/2002.03977v2,2020-02-12T06:09:28Z,2020-02-10T17:41:51Z,"Multimodal active speaker detection and virtual cinematography for video
  conferencing","Active speaker detection (ASD) and virtual cinematography (VC) can
significantly improve the remote user experience of a video conference by
automatically panning, tilting and zooming of a video conferencing camera:
users subjectively rate an expert video cinematographer's video significantly
higher than unedited video. We describe a new automated ASD and VC that
performs within 0.3 MOS of an expert cinematographer based on subjective
ratings with a 1-5 scale. This system uses a 4K wide-FOV camera, a depth
camera, and a microphone array; it extracts features from each modality and
trains an ASD using an AdaBoost machine learning system that is very efficient
and runs in real-time. A VC is similarly trained using machine learning to
optimize the subjective quality of the overall experience. To avoid distracting
the room participants and reduce switching latency the system has no moving
parts -- the VC works by cropping and zooming the 4K wide-FOV video stream. The
system was tuned and evaluated using extensive crowdsourcing techniques and
evaluated on a dataset with N=100 meetings, each 2-5 minutes in length.",arxiv
http://arxiv.org/abs/1807.07501v3,2019-07-01T09:02:16Z,2018-07-19T15:42:26Z,Noise Adaptive Speech Enhancement using Domain Adversarial Training,"In this study, we propose a novel noise adaptive speech enhancement (SE)
system, which employs a domain adversarial training (DAT) approach to tackle
the issue of a noise type mismatch between the training and testing conditions.
Such a mismatch is a critical problem in deep-learning-based SE systems. A
large mismatch may cause a serious performance degradation to the SE
performance. Because we generally use a well-trained SE system to handle
various unseen noise types, a noise type mismatch commonly occurs in real-world
scenarios. The proposed noise adaptive SE system contains an
encoder-decoder-based enhancement model and a domain discriminator model.
During adaptation, the DAT approach encourages the encoder to produce
noise-invariant features based on the information from the discriminator model
and consequentially increases the robustness of the enhancement model to unseen
noise types. Herein, we regard stationary noises as the source domain (with the
ground truth of clean speech) and non-stationary noises as the target domain
(without the ground truth). We evaluated the proposed system on TIMIT
sentences. The experiment results show that the proposed noise adaptive SE
system successfully provides significant improvements in PESQ (19.0%), SSNR
(39.3%), and STOI (27.0%) over the SE system without an adaptation.",arxiv
http://arxiv.org/abs/1802.05891v1,2018-02-16T11:05:18Z,2018-02-16T11:05:18Z,Training Deep Face Recognition Systems with Synthetic Data,"Recent advances in deep learning have significantly increased the performance
of face recognition systems. The performance and reliability of these models
depend heavily on the amount and quality of the training data. However, the
collection of annotated large datasets does not scale well and the control over
the quality of the data decreases with the size of the dataset. In this work,
we explore how synthetically generated data can be used to decrease the number
of real-world images needed for training deep face recognition systems. In
particular, we make use of a 3D morphable face model for the generation of
images with arbitrary amounts of facial identities and with full control over
image variations, such as pose, illumination, and background. In our
experiments with an off-the-shelf face recognition software we observe the
following phenomena: 1) The amount of real training data needed to train
competitive deep face recognition systems can be reduced significantly. 2)
Combining large-scale real-world data with synthetic data leads to an increased
performance. 3) Models trained only on synthetic data with strong variations in
pose, illumination, and background perform very well across different datasets
even without dataset adaptation. 4) The real-to-virtual performance gap can be
closed when using synthetic data for pre-training, followed by fine-tuning with
real-world images. 5) There are no observable negative effects of pre-training
with synthetic data. Thus, any face recognition system in our experiments
benefits from using synthetic face images. The synthetic data generator, as
well as all experiments, are publicly available.",arxiv
http://arxiv.org/abs/2102.00382v2,2021-02-14T04:52:40Z,2021-01-31T05:14:58Z,"Structure-Aware Audio-to-Score Alignment using Progressively Dilated
  Convolutional Neural Networks","The identification of structural differences between a music performance and
the score is a challenging yet integral step of audio-to-score alignment, an
important subtask of music information retrieval. We present a novel method to
detect such differences between the score and performance for a given piece of
music using progressively dilated convolutional neural networks. Our method
incorporates varying dilation rates at different layers to capture both
short-term and long-term context, and can be employed successfully in the
presence of limited annotated data. We conduct experiments on audio recordings
of real performances that differ structurally from the score, and our results
demonstrate that our models outperform standard methods for structure-aware
audio-to-score alignment.",arxiv
http://arxiv.org/abs/2012.14618v4,2021-08-06T11:04:09Z,2020-12-29T05:58:35Z,FPCC: Fast Point Cloud Clustering for Instance Segmentation,"Instance segmentation is an important pre-processing task in numerous
real-world applications, such as robotics, autonomous vehicles, and
human-computer interaction. Compared with the rapid development of deep
learning for two-dimensional (2D) image tasks, deep learning-based instance
segmentation of 3D point cloud still has a lot of room for development. In
particular, distinguishing a large number of occluded objects of the same class
is a highly challenging problem, which is seen in a robotic bin-picking. In a
usual bin-picking scene, many indentical objects are stacked together and the
model of the objects is known. Thus, the semantic information can be ignored;
instead, the focus in the bin-picking is put on the segmentation of instances.
Based on this task requirement, we propose a Fast Point Cloud Clustering (FPCC)
for instance segmentation of bin-picking scene. FPCC includes a network named
FPCC-Net and a fast clustering algorithm. FPCC-net has two subnets, one for
inferring the geometric centers for clustering and the other for describing
features of each point. FPCC-Net extracts features of each point and infers
geometric center points of each instance simultaneously. After that, the
proposed clustering algorithm clusters the remaining points to the closest
geometric center in feature embedding space. Experiments show that FPCC also
surpasses the existing works in bin-picking scenes and is more computationally
efficient. Our code and data are available at https://github.com/xyjbaal/FPCC.",arxiv
http://arxiv.org/abs/1610.06447v4,2018-07-14T20:01:46Z,2016-10-20T14:49:36Z,Regularized Optimal Transport and the Rot Mover's Distance,"This paper presents a unified framework for smooth convex regularization of
discrete optimal transport problems. In this context, the regularized optimal
transport turns out to be equivalent to a matrix nearness problem with respect
to Bregman divergences. Our framework thus naturally generalizes a previously
proposed regularization based on the Boltzmann-Shannon entropy related to the
Kullback-Leibler divergence, and solved with the Sinkhorn-Knopp algorithm. We
call the regularized optimal transport distance the rot mover's distance in
reference to the classical earth mover's distance. We develop two generic
schemes that we respectively call the alternate scaling algorithm and the
non-negative alternate scaling algorithm, to compute efficiently the
regularized optimal plans depending on whether the domain of the regularizer
lies within the non-negative orthant or not. These schemes are based on
Dykstra's algorithm with alternate Bregman projections, and further exploit the
Newton-Raphson method when applied to separable divergences. We enhance the
separable case with a sparse extension to deal with high data dimensions. We
also instantiate our proposed framework and discuss the inherent specificities
for well-known regularizers and statistical divergences in the machine learning
and information geometry communities. Finally, we demonstrate the merits of our
methods with experiments using synthetic data to illustrate the effect of
different regularizers and penalties on the solutions, as well as real-world
data for a pattern recognition application to audio scene classification.",arxiv
http://arxiv.org/abs/2111.06276v1,2021-11-11T15:36:55Z,2021-11-11T15:36:55Z,"6D Pose Estimation with Combined Deep Learning and 3D Vision Techniques
  for a Fast and Accurate Object Grasping","Real-time robotic grasping, supporting a subsequent precise object-in-hand
operation task, is a priority target towards highly advanced autonomous
systems. However, such an algorithm which can perform sufficiently-accurate
grasping with time efficiency is yet to be found. This paper proposes a novel
method with a 2-stage approach that combines a fast 2D object recognition using
a deep neural network and a subsequent accurate and fast 6D pose estimation
based on Point Pair Feature framework to form a real-time 3D object recognition
and grasping solution capable of multi-object class scenes. The proposed
solution has a potential to perform robustly on real-time applications,
requiring both efficiency and accuracy. In order to validate our method, we
conducted extensive and thorough experiments involving laborious preparation of
our own dataset. The experiment results show that the proposed method scores
97.37% accuracy in 5cm5deg metric and 99.37% in Average Distance metric.
Experiment results have shown an overall 62% relative improvement (5cm5deg
metric) and 52.48% (Average Distance metric) by using the proposed method.
Moreover, the pose estimation execution also showed an average improvement of
47.6% in running time. Finally, to illustrate the overall efficiency of the
system in real-time operations, a pick-and-place robotic experiment is
conducted and has shown a convincing success rate with 90% of accuracy. This
experiment video is available at https://sites.google.com/view/dl-ppf6dpose/.",arxiv
http://arxiv.org/abs/2011.02838v1,2020-10-11T15:04:34Z,2020-10-11T15:04:34Z,"Real-time parameter inference in reduced-order flame models with
  heteroscedastic Bayesian neural network ensembles","The estimation of model parameters with uncertainties from observed data is a
ubiquitous inverse problem in science and engineering. In this paper, we
suggest an inexpensive and easy to implement parameter estimation technique
that uses a heteroscedastic Bayesian Neural Network trained using anchored
ensembling. The heteroscedastic aleatoric error of the network models the
irreducible uncertainty due to parameter degeneracies in our inverse problem,
while the epistemic uncertainty of the Bayesian model captures uncertainties
which may arise from an input observation's out-of-distribution nature. We use
this tool to perform real-time parameter inference in a 6 parameter G-equation
model of a ducted, premixed flame from observations of acoustically excited
flames. We train our networks on a library of 2.1 million simulated flame
videos. Results on the test dataset of simulated flames show that the network
recovers flame model parameters, with the correlation coefficient between
predicted and true parameters ranging from 0.97 to 0.99, and well-calibrated
uncertainty estimates. The trained neural networks are then used to infer model
parameters from real videos of a premixed Bunsen flame captured using a
high-speed camera in our lab. Re-simulation using inferred parameters shows
excellent agreement between the real and simulated flames. Compared to Ensemble
Kalman Filter-based tools that have been proposed for this problem in the
combustion literature, our neural network ensemble achieves better
data-efficiency and our sub-millisecond inference times represent a savings on
computational costs by several orders of magnitude. This allows us to calibrate
our reduced-order flame model in real-time and predict the thermoacoustic
instability behaviour of the flame more accurately.",arxiv
http://arxiv.org/abs/1903.02550v1,2019-03-06T00:27:08Z,2019-03-06T00:27:08Z,"Towards a Uniform Architecture for the Efficient Implementation of 2D
  and 3D Deconvolutional Neural Networks on FPGAs","Three-dimensional deconvolution is widely used in many computer vision
applications. However, most previous works have only focused on accelerating 2D
deconvolutional neural networks (DCNNs) on FPGAs, while the acceleration of 3D
DCNNs has not been studied in depth as they have higher computational
complexity and sparsity than 2D DCNNs. In this paper, we focus on the
acceleration of both 2D and 3D DCNNs on FPGAs by proposing efficient schemes
for mapping 2D and 3D DCNNs on a uniform architecture. By implementing our
design on the Xilinx VC709 platform for four real-life 2D and 3D DCNNs, we can
achieve up to 3.0 TOPS with high hardware efficiency. Comparisons with CPU and
GPU solutions demonstrate that we can achieve an improvement of up to 63.3X in
throughput relative to a CPU solution and an improvement of up to 8.3X in
energy efficiency compared to a GPU solution.",arxiv
http://arxiv.org/abs/1809.01372v1,2018-09-05T08:01:15Z,2018-09-05T08:01:15Z,Temporally Coherent Video Harmonization Using Adversarial Networks,"Compositing is one of the most important editing operations for images and
videos. The process of improving the realism of composite results is often
called harmonization. Previous approaches for harmonization mainly focus on
images. In this work, we take one step further to attack the problem of video
harmonization. Specifically, we train a convolutional neural network in an
adversarial way, exploiting a pixel-wise disharmony discriminator to achieve
more realistic harmonized results and introducing a temporal loss to increase
temporal consistency between consecutive harmonized frames. Thanks to the
pixel-wise disharmony discriminator, we are also able to relieve the need of
input foreground masks. Since existing video datasets which have ground-truth
foreground masks and optical flows are not sufficiently large, we propose a
simple yet efficient method to build up a synthetic dataset supporting
supervised training of the proposed adversarial network. Experiments show that
training on our synthetic dataset generalizes well to the real-world composite
dataset. Also, our method successfully incorporates temporal consistency during
training and achieves more harmonious results than previous methods.",arxiv
http://arxiv.org/abs/1906.04232v1,2019-06-10T19:04:09Z,2019-06-10T19:04:09Z,"BowNet: Dilated Convolution Neural Network for Ultrasound Tongue Contour
  Extraction","Ultrasound imaging is safe, relatively affordable, and capable of real-time
performance. One application of this technology is to visualize and to
characterize human tongue shape and motion during a real-time speech to study
healthy or impaired speech production. Due to the noisy nature of ultrasound
images with low-contrast characteristic, it might require expertise for
non-expert users to recognize organ shape such as tongue surface (dorsum). To
alleviate this difficulty for quantitative analysis of tongue shape and motion,
tongue surface can be extracted, tracked, and visualized instead of the whole
tongue region. Delineating the tongue surface from each frame is a cumbersome,
subjective, and error-prone task. Furthermore, the rapidity and complexity of
tongue gestures have made it a challenging task, and manual segmentation is not
a feasible solution for real-time applications. Employing the power of
state-of-the-art deep neural network models and training techniques, it is
feasible to implement new fully-automatic, accurate, and robust segmentation
methods with the capability of real-time performance, applicable for tracking
of the tongue contours during the speech. This paper presents two novel deep
neural network models named BowNet and wBowNet benefits from the ability of
global prediction of decoding-encoding models, with integrated multi-scale
contextual information, and capability of full-resolution (local) extraction of
dilated convolutions. Experimental results using several ultrasound tongue
image datasets revealed that the combination of both localization and
globalization searching could improve prediction result significantly.
Assessment of BowNet models using both qualitatively and quantitatively studies
showed them outstanding achievements in terms of accuracy and robustness in
comparison with similar techniques.",arxiv
http://arxiv.org/abs/1706.07911v3,2017-11-28T19:05:37Z,2017-06-24T05:59:37Z,Large-Scale Mapping of Human Activity using Geo-Tagged Videos,"This paper is the first work to perform spatio-temporal mapping of human
activity using the visual content of geo-tagged videos. We utilize a recent
deep-learning based video analysis framework, termed hidden two-stream
networks, to recognize a range of activities in YouTube videos. This framework
is efficient and can run in real time or faster which is important for
recognizing events as they occur in streaming video or for reducing latency in
analyzing already captured video. This is, in turn, important for using video
in smart-city applications. We perform a series of experiments to show our
approach is able to accurately map activities both spatially and temporally. We
also demonstrate the advantages of using the visual content over the
tags/titles.",arxiv
http://arxiv.org/abs/2004.05973v4,2021-10-18T04:37:58Z,2020-04-13T14:47:34Z,"Speak2Label: Using Domain Knowledge for Creating a Large Scale Driver
  Gaze Zone Estimation Dataset","Labelling of human behavior analysis data is a complex and time consuming
task. In this paper, a fully automatic technique for labelling an image based
gaze behavior dataset for driver gaze zone estimation is proposed. Domain
knowledge is added to the data recording paradigm and later labels are
generated in an automatic manner using Speech To Text conversion (STT). In
order to remove the noise in the STT process due to different illumination and
ethnicity of subjects in our data, the speech frequency and energy are
analysed. The resultant Driver Gaze in the Wild (DGW) dataset contains 586
recordings, captured during different times of the day including evenings. The
large scale dataset contains 338 subjects with an age range of 18-63 years. As
the data is recorded in different lighting conditions, an illumination robust
layer is proposed in the Convolutional Neural Network (CNN). The extensive
experiments show the variance in the dataset resembling real-world conditions
and the effectiveness of the proposed CNN pipeline. The proposed network is
also fine-tuned for the eye gaze prediction task, which shows the
discriminativeness of the representation learnt by our network on the proposed
DGW dataset. Project Page:
https://sites.google.com/view/drivergazeprediction/home",arxiv
http://arxiv.org/abs/2109.11844v1,2021-09-24T09:44:22Z,2021-09-24T09:44:22Z,"Learnable Triangulation for Deep Learning-based 3D Reconstruction of
  Objects of Arbitrary Topology from Single RGB Images","We propose a novel deep reinforcement learning-based approach for 3D object
reconstruction from monocular images. Prior works that use mesh representations
are template based. Thus, they are limited to the reconstruction of objects
that have the same topology as the template. Methods that use volumetric grids
as intermediate representations are computationally expensive, which limits
their application in real-time scenarios. In this paper, we propose a novel
end-to-end method that reconstructs 3D objects of arbitrary topology from a
monocular image. It is composed of of (1) a Vertex Generation Network (VGN),
which predicts the initial 3D locations of the object's vertices from an input
RGB image, (2) a differentiable triangulation layer, which learns in a
non-supervised manner, using a novel reinforcement learning algorithm, the best
triangulation of the object's vertices, and finally, (3) a hierarchical mesh
refinement network that uses graph convolutions to refine the initial mesh. Our
key contribution is the learnable triangulation process, which recovers in an
unsupervised manner the topology of the input shape. Our experiments on
ShapeNet and Pix3D benchmarks show that the proposed method outperforms the
state-of-the-art in terms of visual quality, reconstruction accuracy, and
computational time.",arxiv
http://arxiv.org/abs/2010.12025v3,2021-05-07T08:59:17Z,2020-10-22T20:16:36Z,Combination of Deep Speaker Embeddings for Diarisation,"Significant progress has recently been made in speaker diarisation after the
introduction of d-vectors as speaker embeddings extracted from neural network
(NN) speaker classifiers for clustering speech segments. To extract
better-performing and more robust speaker embeddings, this paper proposes a
c-vector method by combining multiple sets of complementary d-vectors derived
from systems with different NN components. Three structures are used to
implement the c-vectors, namely 2D self-attentive, gated additive, and bilinear
pooling structures, relying on attention mechanisms, a gating mechanism, and a
low-rank bilinear pooling mechanism respectively. Furthermore, a neural-based
single-pass speaker diarisation pipeline is also proposed in this paper, which
uses NNs to achieve voice activity detection, speaker change point detection,
and speaker embedding extraction. Experiments and detailed analyses are
conducted on the challenging AMI and NIST RT05 datasets which consist of real
meetings with 4--10 speakers and a wide range of acoustic conditions. For
systems trained on the AMI training set, relative speaker error rate (SER)
reductions of 13% and 29% are obtained by using c-vectors instead of d-vectors
on the AMI dev and eval sets respectively, and a relative reduction of 15% in
SER is observed on RT05, which shows the robustness of the proposed methods. By
incorporating VoxCeleb data into the training set, the best c-vector system
achieved 7%, 17% and16% relative SER reduction compared to the d-vector on the
AMI dev, eval, and RT05 sets respectively",arxiv
http://arxiv.org/abs/2010.10867v1,2020-10-21T09:52:47Z,2020-10-21T09:52:47Z,LCD -- Line Clustering and Description for Place Recognition,"Current research on visual place recognition mostly focuses on aggregating
local visual features of an image into a single vector representation.
Therefore, high-level information such as the geometric arrangement of the
features is typically lost. In this paper, we introduce a novel learning-based
approach to place recognition, using RGB-D cameras and line clusters as visual
and geometric features. We state the place recognition problem as a problem of
recognizing clusters of lines instead of individual patches, thus maintaining
structural information. In our work, line clusters are defined as lines that
make up individual objects, hence our place recognition approach can be
understood as object recognition. 3D line segments are detected in RGB-D images
using state-of-the-art techniques. We present a neural network architecture
based on the attention mechanism for frame-wise line clustering. A similar
neural network is used for the description of these clusters with a compact
embedding of 128 floating point numbers, trained with triplet loss on training
data obtained from the InteriorNet dataset. We show experiments on a large
number of indoor scenes and compare our method with the bag-of-words
image-retrieval approach using SIFT and SuperPoint features and the global
descriptor NetVLAD. Trained only on synthetic data, our approach generalizes
well to real-world data captured with Kinect sensors, while also providing
information about the geometric arrangement of instances.",arxiv
http://arxiv.org/abs/1806.08085v1,2018-06-21T07:03:36Z,2018-06-21T07:03:36Z,"Inference of Quantized Neural Networks on Heterogeneous All-Programmable
  Devices","Neural networks have established as a generic and powerful means to approach
challenging problems such as image classification, object detection or decision
making. Their successful employment foots on an enormous demand of compute. The
quantization of network parameters and the processed data has proven a valuable
measure to reduce the challenges of network inference so effectively that the
feasible scope of applications is expanded even into the embedded domain. This
paper describes the making of a real-time object detection in a live video
stream processed on an embedded all-programmable device. The presented case
illustrates how the required processing is tamed and parallelized across both
the CPU cores and the programmable logic and how the most suitable resources
and powerful extensions, such as NEON vectorization, are leveraged for the
individual processing steps. The crafted result is an extended Darknet
framework implementing a fully integrated, end-to-end solution from video
capture over object annotation to video output applying neural network
inference at different quantization levels running at 16~frames per second on
an embedded Zynq UltraScale+ (XCZU3EG) platform.",arxiv
http://arxiv.org/abs/1803.06312v2,2018-04-17T02:26:35Z,2018-03-16T16:59:47Z,EVA$^2$: Exploiting Temporal Redundancy in Live Computer Vision,"Hardware support for deep convolutional neural networks (CNNs) is critical to
advanced computer vision in mobile and embedded devices. Current designs,
however, accelerate generic CNNs; they do not exploit the unique
characteristics of real-time vision. We propose to use the temporal redundancy
in natural video to avoid unnecessary computation on most frames. A new
algorithm, activation motion compensation, detects changes in the visual input
and incrementally updates a previously-computed output. The technique takes
inspiration from video compression and applies well-known motion estimation
techniques to adapt to visual changes. We use an adaptive key frame rate to
control the trade-off between efficiency and vision quality as the input
changes. We implement the technique in hardware as an extension to existing
state-of-the-art CNN accelerator designs. The new unit reduces the average
energy per frame by 54.2%, 61.7%, and 87.6% for three CNNs with less than 1%
loss in vision accuracy.",arxiv
http://arxiv.org/abs/1710.08135v1,2017-10-23T08:12:45Z,2017-10-23T08:12:45Z,"An iterative closest point method for measuring the level of similarity
  of 3d log scans in wood industry","In the Canadian's lumber industry, simulators are used to predict the lumbers
resulting from the sawing of a log at a given sawmill. Giving a log or several
logs' 3D scans as input, simulators perform a real-time job to predict the
lumbers. These simulators, however, tend to be slow at processing large volume
of wood. We thus explore an alternative approximation techniques based on the
Iterative Closest Point (ICP) algorithm to identify the already processed log
to which an unseen log resembles the most. The main benefit of the ICP approach
is that it can easily handle 3D scans with a variable number of points. We
compare this ICP-based nearest neighbor predictor, to predictors built using
machine learning algorithms such as the K-nearest-neighbor (kNN) and Random
Forest (RF). The implemented ICP-based predictor enabled us to identify key
points in using the 3D scans directly for distance calculation. The long-term
goal of this ongoing research is to integrated ICP distance calculations and
machine learning.",arxiv
http://arxiv.org/abs/2108.11774v1,2021-08-26T13:14:24Z,2021-08-26T13:14:24Z,Quadratic mutual information regularization in real-time deep CNN models,"In this paper, regularized lightweight deep convolutional neural network
models, capable of effectively operating in real-time on devices with
restricted computational power for high-resolution video input are proposed.
Furthermore, a novel regularization method motivated by the Quadratic Mutual
Information, in order to improve the generalization ability of the utilized
models is proposed. Extensive experiments on various binary classification
problems involved in autonomous systems are performed, indicating the
effectiveness of the proposed models as well as of the proposed regularizer.",arxiv
http://arxiv.org/abs/2108.03799v1,2021-08-09T04:19:25Z,2021-08-09T04:19:25Z,COVID-view: Diagnosis of COVID-19 using Chest CT,"Significant work has been done towards deep learning (DL) models for
automatic lung and lesion segmentation and classification of COVID-19 on chest
CT data. However, comprehensive visualization systems focused on supporting the
dual visual+DL diagnosis of COVID-19 are non-existent. We present COVID-view, a
visualization application specially tailored for radiologists to diagnose
COVID-19 from chest CT data. The system incorporates a complete pipeline of
automatic lungs segmentation, localization/ isolation of lung abnormalities,
followed by visualization, visual and DL analysis, and
measurement/quantification tools. Our system combines the traditional 2D
workflow of radiologists with newer 2D and 3D visualization techniques with DL
support for a more comprehensive diagnosis. COVID-view incorporates a novel DL
model for classifying the patients into positive/negative COVID-19 cases, which
acts as a reading aid for the radiologist using COVID-view and provides the
attention heatmap as an explainable DL for the model output. We designed and
evaluated COVID-view through suggestions, close feedback and conducting case
studies of real-world patient data by expert radiologists who have substantial
experience diagnosing chest CT scans for COVID-19, pulmonary embolism, and
other forms of lung infections. We present requirements and task analysis for
the diagnosis of COVID-19 that motivate our design choices and results in a
practical system which is capable of handling real-world patient cases.",arxiv
http://arxiv.org/abs/2010.11162v1,2020-10-21T17:28:56Z,2020-10-21T17:28:56Z,In-the-wild Drowsiness Detection from Facial Expressions,"Driving in a state of drowsiness is a major cause of road accidents,
resulting in tremendous damage to life and property. Developing robust,
automatic, real-time systems that can infer drowsiness states of drivers has
the potential of making life-saving impact. However, developing drowsiness
detection systems that work well in real-world scenarios is challenging because
of the difficulties associated with collecting high-volume realistic drowsy
data and modeling the complex temporal dynamics of evolving drowsy states. In
this paper, we propose a data collection protocol that involves outfitting
vehicles of overnight shift workers with camera kits that record their faces
while driving. We develop a drowsiness annotation guideline to enable humans to
label the collected videos into 4 levels of drowsiness: `alert', `slightly
drowsy', `moderately drowsy' and `extremely drowsy'. We experiment with
different convolutional and temporal neural network architectures to predict
drowsiness states from pose, expression and emotion-based representation of the
input video of the driver's face. Our best performing model achieves a macro
ROC-AUC of 0.78, compared to 0.72 for a baseline model.",arxiv
http://arxiv.org/abs/1910.11818v2,2019-11-01T15:31:34Z,2019-10-25T16:00:05Z,"Real-time Memory Efficient Large-pose Face Alignment via Deep
  Evolutionary Network","There is an urgent need to apply face alignment in a memory-efficient and
real-time manner due to the recent explosion of face recognition applications.
However, impact factors such as large pose variation and computational
inefficiency, still hinder its broad implementation. To this end, we propose a
computationally efficient deep evolutionary model integrated with 3D Diffusion
Heap Maps (DHM). First, we introduce a sparse 3D DHM to assist the initial
modeling process under extreme pose conditions. Afterward, a simple and
effective CNN feature is extracted and fed to Recurrent Neural Network (RNN)
for evolutionary learning. To accelerate the model, we propose an efficient
network structure to accelerate the evolutionary learning process through a
factorization strategy. Extensive experiments on three popular alignment
databases demonstrate the advantage of the proposed models over the
state-of-the-art, especially under large-pose conditions. Notably, the
computational speed of our model is 6 times faster than the state-of-the-art on
CPU and 14 times on GPU. We also discuss and analyze the limitations of our
models and future research work.",arxiv
http://arxiv.org/abs/2111.08468v1,2021-11-16T13:45:23Z,2021-11-16T13:45:23Z,"Point detection through multi-instance deep heatmap regression for
  sutures in endoscopy","Purpose: Mitral valve repair is a complex minimally invasive surgery of the
heart valve. In this context, suture detection from endoscopic images is a
highly relevant task that provides quantitative information to analyse suturing
patterns, assess prosthetic configurations and produce augmented reality
visualisations. Facial or anatomical landmark detection tasks typically contain
a fixed number of landmarks, and use regression or fixed heatmap-based
approaches to localize the landmarks. However in endoscopy, there are a varying
number of sutures in every image, and the sutures may occur at any location in
the annulus, as they are not semantically unique. Method: In this work, we
formulate the suture detection task as a multi-instance deep heatmap regression
problem, to identify entry and exit points of sutures. We extend our previous
work, and introduce the novel use of a 2D Gaussian layer followed by a
differentiable 2D spatial Soft-Argmax layer to function as a local non-maximum
suppression. Results: We present extensive experiments with multiple heatmap
distribution functions and two variants of the proposed model. In the
intra-operative domain, Variant 1 showed a mean F1 of +0.0422 over the
baseline. Similarly, in the simulator domain, Variant 1 showed a mean F1 of
+0.0865 over the baseline. Conclusion: The proposed model shows an improvement
over the baseline in the intra-operative and the simulator domains. The data is
made publicly available within the scope of the MICCAI AdaptOR2021 Challenge
https://adaptor2021.github.io/, and the code at
https://github.com/Cardio-AI/suture-detection-pytorch/.
DOI:10.1007/s11548-021-02523-w. The link to the open access article can be
found here: https://link.springer.com/article/10.1007%2Fs11548-021-02523-w",arxiv
http://arxiv.org/abs/1807.02996v1,2018-07-09T09:17:59Z,2018-07-09T09:17:59Z,"Dynamic Objects Segmentation for Visual Localization in Urban
  Environments","Visual localization and mapping is a crucial capability to address many
challenges in mobile robotics. It constitutes a robust, accurate and
cost-effective approach for local and global pose estimation within prior maps.
Yet, in highly dynamic environments, like crowded city streets, problems arise
as major parts of the image can be covered by dynamic objects. Consequently,
visual odometry pipelines often diverge and the localization systems
malfunction as detected features are not consistent with the precomputed 3D
model. In this work, we present an approach to automatically detect dynamic
object instances to improve the robustness of vision-based localization and
mapping in crowded environments. By training a convolutional neural network
model with a combination of synthetic and real-world data, dynamic object
instance masks are learned in a semi-supervised way. The real-world data can be
collected with a standard camera and requires minimal further post-processing.
Our experiments show that a wide range of dynamic objects can be reliably
detected using the presented method. Promising performance is demonstrated on
our own and also publicly available datasets, which also shows the
generalization capabilities of this approach.",arxiv
http://arxiv.org/abs/2003.08056v1,2020-03-18T05:52:10Z,2020-03-18T05:52:10Z,"OmniSLAM: Omnidirectional Localization and Dense Mapping for
  Wide-baseline Multi-camera Systems","In this paper, we present an omnidirectional localization and dense mapping
system for a wide-baseline multiview stereo setup with ultra-wide field-of-view
(FOV) fisheye cameras, which has a 360 degrees coverage of stereo observations
of the environment. For more practical and accurate reconstruction, we first
introduce improved and light-weighted deep neural networks for the
omnidirectional depth estimation, which are faster and more accurate than the
existing networks. Second, we integrate our omnidirectional depth estimates
into the visual odometry (VO) and add a loop closing module for global
consistency. Using the estimated depth map, we reproject keypoints onto each
other view, which leads to a better and more efficient feature matching
process. Finally, we fuse the omnidirectional depth maps and the estimated rig
poses into the truncated signed distance function (TSDF) volume to acquire a 3D
map. We evaluate our method on synthetic datasets with ground-truth and
real-world sequences of challenging environments, and the extensive experiments
show that the proposed system generates excellent reconstruction results in
both synthetic and real-world environments.",arxiv
http://arxiv.org/abs/1901.09482v3,2020-02-19T19:05:12Z,2019-01-28T01:34:32Z,"Bridging the Gap Between Computational Photography and Visual
  Recognition","What is the current state-of-the-art for image restoration and enhancement
applied to degraded images acquired under less than ideal circumstances? Can
the application of such algorithms as a pre-processing step to improve image
interpretability for manual analysis or automatic visual recognition to
classify scene content? While there have been important advances in the area of
computational photography to restore or enhance the visual quality of an image,
the capabilities of such techniques have not always translated in a useful way
to visual recognition tasks. Consequently, there is a pressing need for the
development of algorithms that are designed for the joint problem of improving
visual appearance and recognition, which will be an enabling factor for the
deployment of visual recognition tools in many real-world scenarios. To address
this, we introduce the UG^2 dataset as a large-scale benchmark composed of
video imagery captured under challenging conditions, and two enhancement tasks
designed to test algorithmic impact on visual quality and automatic object
recognition. Furthermore, we propose a set of metrics to evaluate the joint
improvement of such tasks as well as individual algorithmic advances, including
a novel psychophysics-based evaluation regime for human assessment and a
realistic set of quantitative measures for object recognition performance. We
introduce six new algorithms for image restoration or enhancement, which were
created as part of the IARPA sponsored UG^2 Challenge workshop held at CVPR
2018. Under the proposed evaluation regime, we present an in-depth analysis of
these algorithms and a host of deep learning-based and classic baseline
approaches. From the observed results, it is evident that we are in the early
days of building a bridge between computational photography and visual
recognition, leaving many opportunities for innovation in this area.",arxiv
http://arxiv.org/abs/1207.1019v1,2012-07-04T15:09:05Z,2012-07-04T15:09:05Z,PAC-Bayesian Majority Vote for Late Classifier Fusion,"A lot of attention has been devoted to multimedia indexing over the past few
years. In the literature, we often consider two kinds of fusion schemes: The
early fusion and the late fusion. In this paper we focus on late classifier
fusion, where one combines the scores of each modality at the decision level.
To tackle this problem, we investigate a recent and elegant well-founded
quadratic program named MinCq coming from the Machine Learning PAC-Bayes
theory. MinCq looks for the weighted combination, over a set of real-valued
functions seen as voters, leading to the lowest misclassification rate, while
making use of the voters' diversity. We provide evidence that this method is
naturally adapted to late fusion procedure. We propose an extension of MinCq by
adding an order- preserving pairwise loss for ranking, helping to improve Mean
Averaged Precision measure. We confirm the good behavior of the MinCq-based
fusion approaches with experiments on a real image benchmark.",arxiv
http://arxiv.org/abs/1710.02909v2,2018-02-07T02:12:50Z,2017-10-09T02:01:58Z,"UG^2: a Video Benchmark for Assessing the Impact of Image Restoration
  and Enhancement on Automatic Visual Recognition","Advances in image restoration and enhancement techniques have led to
discussion about how such algorithmscan be applied as a pre-processing step to
improve automatic visual recognition. In principle, techniques like deblurring
and super-resolution should yield improvements by de-emphasizing noise and
increasing signal in an input image. But the historically divergent goals of
the computational photography and visual recognition communities have created a
significant need for more work in this direction. To facilitate new research,
we introduce a new benchmark dataset called UG^2, which contains three
difficult real-world scenarios: uncontrolled videos taken by UAVs and manned
gliders, as well as controlled videos taken on the ground. Over 160,000
annotated frames forhundreds of ImageNet classes are available, which are used
for baseline experiments that assess the impact of known and unknown image
artifacts and other conditions on common deep learning-based object
classification approaches. Further, current image restoration and enhancement
techniques are evaluated by determining whether or not theyimprove baseline
classification performance. Results showthat there is plenty of room for
algorithmic innovation, making this dataset a useful tool going forward.",arxiv
http://arxiv.org/abs/2007.16170v1,2020-07-31T16:43:10Z,2020-07-31T16:43:10Z,Diet deep generative audio models with structured lottery,"Deep learning models have provided extremely successful solutions in most
audio application fields. However, the high accuracy of these models comes at
the expense of a tremendous computation cost. This aspect is almost always
overlooked in evaluating the quality of proposed models. However, models should
not be evaluated without taking into account their complexity. This aspect is
especially critical in audio applications, which heavily relies on specialized
embedded hardware with real-time constraints. In this paper, we build on recent
observations that deep models are highly overparameterized, by studying the
lottery ticket hypothesis on deep generative audio models. This hypothesis
states that extremely efficient small sub-networks exist in deep models and
would provide higher accuracy than larger models if trained in isolation.
However, lottery tickets are found by relying on unstructured masking, which
means that resulting models do not provide any gain in either disk size or
inference time. Instead, we develop here a method aimed at performing
structured trimming. We show that this requires to rely on global selection and
introduce a specific criterion based on mutual information. First, we confirm
the surprising result that smaller models provide higher accuracy than their
large counterparts. We further show that we can remove up to 95% of the model
weights without significant degradation in accuracy. Hence, we can obtain very
light models for generative audio across popular methods such as Wavenet, SING
or DDSP, that are up to 100 times smaller with commensurate accuracy. We study
the theoretical bounds for embedding these models on Raspberry Pi and Arduino,
and show that we can obtain generative models on CPU with equivalent quality as
large GPU models. Finally, we discuss the possibility of implementing deep
generative audio models on embedded platforms.",arxiv
http://arxiv.org/abs/1601.02376v1,2016-01-11T10:04:40Z,2016-01-11T10:04:40Z,"Deep Learning over Multi-field Categorical Data: A Case Study on User
  Response Prediction","Predicting user responses, such as click-through rate and conversion rate,
are critical in many web applications including web search, personalised
recommendation, and online advertising. Different from continuous raw features
that we usually found in the image and audio domains, the input features in web
space are always of multi-field and are mostly discrete and categorical while
their dependencies are little known. Major user response prediction models have
to either limit themselves to linear models or require manually building up
high-order combination features. The former loses the ability of exploring
feature interactions, while the latter results in a heavy computation in the
large feature space. To tackle the issue, we propose two novel models using
deep neural networks (DNNs) to automatically learn effective patterns from
categorical feature interactions and make predictions of users' ad clicks. To
get our DNNs efficiently work, we propose to leverage three feature
transformation methods, i.e., factorisation machines (FMs), restricted
Boltzmann machines (RBMs) and denoising auto-encoders (DAEs). This paper
presents the structure of our models and their efficient training algorithms.
The large-scale experiments with real-world data demonstrate that our methods
work better than major state-of-the-art models.",arxiv
http://arxiv.org/abs/2102.06838v1,2021-02-13T01:07:26Z,2021-02-13T01:07:26Z,"Learning Variable Impedance Control via Inverse Reinforcement Learning
  for Force-Related Tasks","Many manipulation tasks require robots to interact with unknown environments.
In such applications, the ability to adapt the impedance according to different
task phases and environment constraints is crucial for safety and performance.
Although many approaches based on deep reinforcement learning (RL) and learning
from demonstration (LfD) have been proposed to obtain variable impedance skills
on contact-rich manipulation tasks, these skills are typically task-specific
and could be sensitive to changes in task settings. This paper proposes an
inverse reinforcement learning (IRL) based approach to recover both the
variable impedance policy and reward function from expert demonstrations. We
explore different action space of the reward functions to achieve a more
general representation of expert variable impedance skills. Experiments on two
variable impedance tasks (Peg-in-Hole and Cup-on-Plate) were conducted in both
simulations and on a real FANUC LR Mate 200iD/7L industrial robot. The
comparison results with behavior cloning and force-based IRL proved that the
learned reward function in the gain action space has better transferability
than in the force space. Experiment videos are available at
https://msc.berkeley.edu/research/impedance-irl.html.",arxiv
http://arxiv.org/abs/2002.07381v2,2020-08-26T09:23:29Z,2020-02-18T05:35:29Z,"Spatial Concept-Based Navigation with Human Speech Instructions via
  Probabilistic Inference on Bayesian Generative Model","Robots are required to not only learn spatial concepts autonomously but also
utilize such knowledge for various tasks in a domestic environment. Spatial
concept represents a multimodal place category acquired from the robot's
spatial experience including vision, speech-language, and self-position. The
aim of this study is to enable a mobile robot to perform navigational tasks
with human speech instructions, such as `Go to the kitchen', via probabilistic
inference on a Bayesian generative model using spatial concepts. Specifically,
path planning was formalized as the maximization of probabilistic distribution
on the path-trajectory under speech instruction, based on a
control-as-inference framework. Furthermore, we described the relationship
between probabilistic inference based on the Bayesian generative model and
control problem including reinforcement learning. We demonstrated path planning
based on human instruction using acquired spatial concepts to verify the
usefulness of the proposed approach in the simulator and in real environments.
Experimentally, places instructed by the user's speech commands showed high
probability values, and the trajectory toward the target place was correctly
estimated. Our approach, based on probabilistic inference concerning
decision-making, can lead to further improvement in robot autonomy.",arxiv
http://arxiv.org/abs/1809.06267v4,2019-02-18T14:59:48Z,2018-09-17T15:02:52Z,PointNetGPD: Detecting Grasp Configurations from Point Sets,"In this paper, we propose an end-to-end grasp evaluation model to address the
challenging problem of localizing robot grasp configurations directly from the
point cloud. Compared to recent grasp evaluation metrics that are based on
handcrafted depth features and a convolutional neural network (CNN), our
proposed PointNetGPD is lightweight and can directly process the 3D point cloud
that locates within the gripper for grasp evaluation. Taking the raw point
cloud as input, our proposed grasp evaluation network can capture the complex
geometric structure of the contact area between the gripper and the object even
if the point cloud is very sparse. To further improve our proposed model, we
generate a larger-scale grasp dataset with 350k real point cloud and grasps
with the YCB object set for training. The performance of the proposed model is
quantitatively measured both in simulation and on robotic hardware. Experiments
on object grasping and clutter removal show that our proposed model generalizes
well to novel objects and outperforms state-of-the-art methods. Code and video
are available at
\href{https://lianghongzhuo.github.io/PointNetGPD}{https://lianghongzhuo.github.io/PointNetGPD}",arxiv
http://arxiv.org/abs/1904.05343v2,2020-03-26T02:51:10Z,2019-04-10T17:53:38Z,StegaStamp: Invisible Hyperlinks in Physical Photographs,"Printed and digitally displayed photos have the ability to hide imperceptible
digital data that can be accessed through internet-connected imaging systems.
Another way to think about this is physical photographs that have unique QR
codes invisibly embedded within them. This paper presents an architecture,
algorithms, and a prototype implementation addressing this vision. Our key
technical contribution is StegaStamp, a learned steganographic algorithm to
enable robust encoding and decoding of arbitrary hyperlink bitstrings into
photos in a manner that approaches perceptual invisibility. StegaStamp
comprises a deep neural network that learns an encoding/decoding algorithm
robust to image perturbations approximating the space of distortions resulting
from real printing and photography. We demonstrates real-time decoding of
hyperlinks in photos from in-the-wild videos that contain variation in
lighting, shadows, perspective, occlusion and viewing distance. Our prototype
system robustly retrieves 56 bit hyperlinks after error correction - sufficient
to embed a unique code within every photo on the internet.",arxiv
http://arxiv.org/abs/1810.11348v1,2018-10-26T14:37:45Z,2018-10-26T14:37:45Z,Security Event Recognition for Visual Surveillance,"With rapidly increasing deployment of surveillance cameras, the reliable
methods for automatically analyzing the surveillance video and recognizing
special events are demanded by different practical applications. This paper
proposes a novel effective framework for security event analysis in
surveillance videos. First, convolutional neural network (CNN) framework is
used to detect objects of interest in the given videos. Second, the owners of
the objects are recognized and monitored in real-time as well. If anyone moves
any object, this person will be verified whether he/she is its owner. If not,
this event will be further analyzed and distinguished between two different
scenes: moving the object away or stealing it. To validate the proposed
approach, a new video dataset consisting of various scenarios is constructed
for more complex tasks. For comparison purpose, the experiments are also
carried out on the benchmark databases related to the task on abandoned luggage
detection. The experimental results show that the proposed approach outperforms
the state-of-the-art methods and effective in recognizing complex security
events.",arxiv
http://arxiv.org/abs/2111.08462v1,2021-11-14T13:36:18Z,2021-11-14T13:36:18Z,"Towards Lightweight Controllable Audio Synthesis with Conditional
  Implicit Neural Representations","The high temporal resolution of audio and our perceptual sensitivity to small
irregularities in waveforms make synthesizing at high sampling rates a complex
and computationally intensive task, prohibiting real-time, controllable
synthesis within many approaches. In this work we aim to shed light on the
potential of Conditional Implicit Neural Representations (CINRs) as lightweight
backbones in generative frameworks for audio synthesis.
  Implicit neural representations (INRs) are neural networks used to
approximate low-dimensional functions, trained to represent a single geometric
object by mapping input coordinates to structural information at input
locations. In contrast with other neural methods for representing geometric
objects, the memory required to parameterize the object is independent of
resolution, and only scales with its complexity. A corollary of this is that
INRs have infinite resolution, as they can be sampled at arbitrary resolutions.
To apply the concept of INRs in the generative domain we frame generative
modelling as learning a distribution of continuous functions. This can be
achieved by introducing conditioning methods to INRs.
  Our experiments show that Periodic Conditional INRs (PCINRs) learn faster and
generally produce quantitatively better audio reconstructions than Transposed
Convolutional Neural Networks with equal parameter counts. However, their
performance is very sensitive to activation scaling hyperparameters. When
learning to represent more uniform sets, PCINRs tend to introduce artificial
high-frequency components in reconstructions. We validate this noise can be
minimized by applying standard weight regularization during training or
decreasing the compositional depth of PCINRs, and suggest directions for future
research.",arxiv
http://arxiv.org/abs/1612.06615v1,2016-12-20T11:33:31Z,2016-12-20T11:33:31Z,Deep Motion Features for Visual Tracking,"Robust visual tracking is a challenging computer vision problem, with many
real-world applications. Most existing approaches employ hand-crafted
appearance features, such as HOG or Color Names. Recently, deep RGB features
extracted from convolutional neural networks have been successfully applied for
tracking. Despite their success, these features only capture appearance
information. On the other hand, motion cues provide discriminative and
complementary information that can improve tracking performance. Contrary to
visual tracking, deep motion features have been successfully applied for action
recognition and video classification tasks. Typically, the motion features are
learned by training a CNN on optical flow images extracted from large amounts
of labeled videos.
  This paper presents an investigation of the impact of deep motion features in
a tracking-by-detection framework. We further show that hand-crafted, deep RGB,
and deep motion features contain complementary information. To the best of our
knowledge, we are the first to propose fusing appearance information with deep
motion features for visual tracking. Comprehensive experiments clearly suggest
that our fusion approach with deep motion features outperforms standard methods
relying on appearance information alone.",arxiv
http://arxiv.org/abs/2010.01824v1,2020-10-05T07:19:19Z,2020-10-05T07:19:19Z,Class-Wise Difficulty-Balanced Loss for Solving Class-Imbalance,"Class-imbalance is one of the major challenges in real world datasets, where
a few classes (called majority classes) constitute much more data samples than
the rest (called minority classes). Learning deep neural networks using such
datasets leads to performances that are typically biased towards the majority
classes. Most of the prior works try to solve class-imbalance by assigning more
weights to the minority classes in various manners (e.g., data re-sampling,
cost-sensitive learning). However, we argue that the number of available
training data may not be always a good clue to determine the weighting strategy
because some of the minority classes might be sufficiently represented even by
a small number of training data. Overweighting samples of such classes can lead
to drop in the model's overall performance. We claim that the 'difficulty' of a
class as perceived by the model is more important to determine the weighting.
In this light, we propose a novel loss function named Class-wise
Difficulty-Balanced loss, or CDB loss, which dynamically distributes weights to
each sample according to the difficulty of the class that the sample belongs
to. Note that the assigned weights dynamically change as the 'difficulty' for
the model may change with the learning progress. Extensive experiments are
conducted on both image (artificially induced class-imbalanced MNIST,
long-tailed CIFAR and ImageNet-LT) and video (EGTEA) datasets. The results show
that CDB loss consistently outperforms the recently proposed loss functions on
class-imbalanced datasets irrespective of the data type (i.e., video or image).",arxiv
http://arxiv.org/abs/1603.04922v4,2017-08-16T04:12:50Z,2016-03-16T00:09:41Z,"DeepContext: Context-Encoding Neural Pathways for 3D Holistic Scene
  Understanding","While deep neural networks have led to human-level performance on computer
vision tasks, they have yet to demonstrate similar gains for holistic scene
understanding. In particular, 3D context has been shown to be an extremely
important cue for scene understanding - yet very little research has been done
on integrating context information with deep models. This paper presents an
approach to embed 3D context into the topology of a neural network trained to
perform holistic scene understanding. Given a depth image depicting a 3D scene,
our network aligns the observed scene with a predefined 3D scene template, and
then reasons about the existence and location of each object within the scene
template. In doing so, our model recognizes multiple objects in a single
forward pass of a 3D convolutional neural network, capturing both global scene
and local object information simultaneously. To create training data for this
3D network, we generate partly hallucinated depth images which are rendered by
replacing real objects with a repository of CAD models of the same object
category. Extensive experiments demonstrate the effectiveness of our algorithm
compared to the state-of-the-arts. Source code and data are available at
http://deepcontext.cs.princeton.edu.",arxiv
http://arxiv.org/abs/2005.08465v1,2020-05-18T05:49:48Z,2020-05-18T05:49:48Z,Context-aware and Scale-insensitive Temporal Repetition Counting,"Temporal repetition counting aims to estimate the number of cycles of a given
repetitive action. Existing deep learning methods assume repetitive actions are
performed in a fixed time-scale, which is invalid for the complex repetitive
actions in real life. In this paper, we tailor a context-aware and
scale-insensitive framework, to tackle the challenges in repetition counting
caused by the unknown and diverse cycle-lengths. Our approach combines two key
insights: (1) Cycle lengths from different actions are unpredictable that
require large-scale searching, but, once a coarse cycle length is determined,
the variety between repetitions can be overcome by regression. (2) Determining
the cycle length cannot only rely on a short fragment of video but a contextual
understanding. The first point is implemented by a coarse-to-fine cycle
refinement method. It avoids the heavy computation of exhaustively searching
all the cycle lengths in the video, and, instead, it propagates the coarse
prediction for further refinement in a hierarchical manner. We secondly propose
a bidirectional cycle length estimation method for a context-aware prediction.
It is a regression network that takes two consecutive coarse cycles as input,
and predicts the locations of the previous and next repetitive cycles. To
benefit the training and evaluation of temporal repetition counting area, we
construct a new and largest benchmark, which contains 526 videos with diverse
repetitive actions. Extensive experiments show that the proposed network
trained on a single dataset outperforms state-of-the-art methods on several
benchmarks, indicating that the proposed framework is general enough to capture
repetition patterns across domains.",arxiv
http://arxiv.org/abs/2005.07976v2,2020-10-30T13:38:45Z,2020-05-16T12:58:31Z,"Target Speech Extraction Based on Blind Source Separation and
  X-vector-based Speaker Selection Trained with Data Augmentation","Extracting the desired speech from a mixture is a meaningful and challenging
task. The end-to-end DNN-based methods, though attractive, face the problem of
generalization. In this paper, we explore a sequential approach for target
speech extraction by combining blind source separation (BSS) with the x-vector
based speaker recognition (SR) module. Two promising BSS methods based on
source independence assumption, independent low-rank matrix analysis (ILRMA)
and multi-channel variational autoencoder (MVAE), are utilized and compared.
ILRMA employs nonnegative matrix factorization (NMF) to capture spectral
structures of source signals and MVAE utilizes the strong modeling power of
deep neural networks (DNN). However, the investigation of MVAE has been limited
to the training with very few speakers and the speech signals of test speakers
are usually included. We extend the training of MVAE using clean speech signals
of 500 speakers to evaluate its generalization to unseen speakers. To improve
the correct extraction rate, two data augmentation strategies are implemented
to train the SR module. The performance of the proposed cascaded approach is
investigated with test data constructed with real room impulse responses under
varied environments.",arxiv
http://arxiv.org/abs/1807.00139v1,2018-06-30T08:31:54Z,2018-06-30T08:31:54Z,Harnessing constrained resources in service industry via video analytics,"Service industries contribute significantly to many developed and developing
- economies. As their business activities expand rapidly, many service
companies struggle to maintain customer's satisfaction due to sluggish service
response caused by resource shortages. Anticipating resource shortages and
proffering solutions before they happen is an effective way of reducing the
adverse effect on operations. However, this proactive approach is very
expensive in terms of capacity and labor costs. Many companies fall into
productivity conundrum as they fail to find sufficient strong arguments to
justify the cost of a new technology yet cannot afford not to invest in new
technologies to match up with competitors. The question is whether there is an
innovative solution to maximally utilize available resources and drastically
reduce the effect that the shortages of resources may cause yet achieving high
level of service quality at a low cost. This work demonstrates with a practical
analysis of a trolley tracking system we designed and deployed at Hong Kong
International Airport (HKIA) on how video analytics helps achieve management's
goal of satisfying customer's needs via real-time detection and prevention of
problems they may encounter during the service consumption process using
existing video technology rather than adopting new technologies. This paper
presents the integration of commercial video surveillance system with deep
learning algorithms for video analytics. We show that our system can provide
accurate decision when faced with total or partial occlusion with high accuracy
and it significantly improves daily operation. It is envisioned that this work
will heighten the appreciation of integrative technologies for resource
management within the service industries and as a measure for real-time
customer assistance.",arxiv
http://arxiv.org/abs/1902.09074v1,2019-02-25T03:32:58Z,2019-02-25T03:32:58Z,"Channel adversarial training for cross-channel text-independent speaker
  recognition","The conventional speaker recognition frameworks (e.g., the i-vector and
CNN-based approach) have been successfully applied to various tasks when the
channel of the enrolment dataset is similar to that of the test dataset.
However, in real-world applications, mismatch always exists between these two
datasets, which may severely deteriorate the recognition performance.
Previously, a few channel compensation algorithms have been proposed, such as
Linear Discriminant Analysis (LDA) and Probabilistic LDA. However, these
methods always require the collections of different channels from a specific
speaker, which is unrealistic to be satisfied in real scenarios. Inspired by
domain adaptation, we propose a novel deep-learning based speaker recognition
framework to learn the channel-invariant and speaker-discriminative speech
representations via channel adversarial training. Specifically, we first employ
a gradient reversal layer to remove variations across different channels. Then,
the compressed information is projected into the same subspace by adversarial
training. Experiments on test datasets with 54,133 speakers demonstrate that
the proposed method is not only effective at alleviating the channel mismatch
problem, but also outperforms state-of-the-art speaker recognition methods.
Compared with the i-vector-based method and the CNN-based method, our proposed
method achieves significant relative improvement of 44.7% and 22.6%
respectively in terms of the Top1 recall.",arxiv
http://arxiv.org/abs/2010.13938v1,2020-10-26T22:49:45Z,2020-10-26T22:49:45Z,Neural Unsigned Distance Fields for Implicit Function Learning,"In this work we target a learnable output representation that allows
continuous, high resolution outputs of arbitrary shape. Recent works represent
3D surfaces implicitly with a Neural Network, thereby breaking previous
barriers in resolution, and ability to represent diverse topologies. However,
neural implicit representations are limited to closed surfaces, which divide
the space into inside and outside. Many real world objects such as walls of a
scene scanned by a sensor, clothing, or a car with inner structures are not
closed. This constitutes a significant barrier, in terms of data pre-processing
(objects need to be artificially closed creating artifacts), and the ability to
output open surfaces. In this work, we propose Neural Distance Fields (NDF), a
neural network based model which predicts the unsigned distance field for
arbitrary 3D shapes given sparse point clouds. NDF represent surfaces at high
resolutions as prior implicit models, but do not require closed surface data,
and significantly broaden the class of representable shapes in the output. NDF
allow to extract the surface as very dense point clouds and as meshes. We also
show that NDF allow for surface normal calculation and can be rendered using a
slight modification of sphere tracing. We find NDF can be used for multi-target
regression (multiple outputs for one input) with techniques that have been
exclusively used for rendering in graphics. Experiments on ShapeNet show that
NDF, while simple, is the state-of-the art, and allows to reconstruct shapes
with inner structures, such as the chairs inside a bus. Notably, we show that
NDF are not restricted to 3D shapes, and can approximate more general open
surfaces such as curves, manifolds, and functions. Code is available for
research at https://virtualhumans.mpi-inf.mpg.de/ndf/.",arxiv
http://arxiv.org/abs/1906.03037v1,2019-04-29T02:16:32Z,2019-04-29T02:16:32Z,"Argus: Smartphone-enabled Human Cooperation via Multi-Agent
  Reinforcement Learning for Disaster Situational Awareness","Argus exploits a Multi-Agent Reinforcement Learning (MARL) framework to
create a 3D mapping of the disaster scene using agents present around the
incident zone to facilitate the rescue operations. The agents can be both human
bystanders at the disaster scene as well as drones or robots that can assist
the humans. The agents are involved in capturing the images of the scene using
their smartphones (or on-board cameras in case of drones) as directed by the
MARL algorithm. These images are used to build real time a 3D map of the
disaster scene. Via both simulations and real experiments, an evaluation of the
framework in terms of effectiveness in tracking random dynamicity of the
environment is presented.",arxiv
http://arxiv.org/abs/1912.05958v2,2020-06-19T14:29:57Z,2019-12-12T13:54:24Z,Parareal with a Learned Coarse Model for Robotic Manipulation,"A key component of many robotics model-based planning and control algorithms
is physics predictions, that is, forecasting a sequence of states given an
initial state and a sequence of controls. This process is slow and a major
computational bottleneck for robotics planning algorithms. Parallel-in-time
integration methods can help to leverage parallel computing to accelerate
physics predictions and thus planning. The Parareal algorithm iterates between
a coarse serial integrator and a fine parallel integrator. A key challenge is
to devise a coarse model that is computationally cheap but accurate enough for
Parareal to converge quickly. Here, we investigate the use of a deep neural
network physics model as a coarse model for Parareal in the context of robotic
manipulation. In simulated experiments using the physics engine Mujoco as fine
propagator we show that the learned coarse model leads to faster Parareal
convergence than a coarse physics-based model. We further show that the learned
coarse model allows to apply Parareal to scenarios with multiple objects, where
the physics-based coarse model is not applicable. Finally, we conduct
experiments on a real robot and show that Parareal predictions are close to
real-world physics predictions for robotic pushing of multiple objects. Videos
are at https://youtu.be/wCh2o1rf-gA.",arxiv
http://arxiv.org/abs/1902.00637v1,2019-02-02T03:45:43Z,2019-02-02T03:45:43Z,"Multiuser Video Streaming Rate Adaptation: A Physical Layer
  Resource-Aware Deep Reinforcement Learning Approach","We consider a multi-user video streaming service optimization problem over a
time-varying and mutually interfering multi-cell wireless network. The key
research challenge is to appropriately adapt each user's video streaming rate
according to the radio frequency environment (e.g., channel fading and
interference level) and service demands (e.g., play request), so that the
users' long-term experience for watching videos can be optimized. To address
the above challenge, we propose a novel two-level cross-layer optimization
framework for multiuser adaptive video streaming over wireless networks. The
key idea is to jointly design the physical layer optimization-based beamforming
scheme (performed at the base stations) and the application layer Deep
Reinforcement Learning (DRL)-based scheme (performed at the user terminals), so
that a highly complex multi-user, cross-layer, time-varying video streaming
problem can be decomposed into relatively simple problems and solved
effectively. Our strategy represents a significant departure for the existing
schemes where either short-term user experience optimization is considered, or
only single-user point-to-point long-term optimization is considered. Extensive
simulations based on real-data sets show that the proposed cross-layer design
is effective and promising.",arxiv
http://arxiv.org/abs/2010.08844v2,2021-06-11T00:42:30Z,2020-10-17T18:35:32Z,"Finding Physical Adversarial Examples for Autonomous Driving with Fast
  and Differentiable Image Compositing","There is considerable evidence that deep neural networks are vulnerable to
adversarial perturbations applied directly to their digital inputs. However, it
remains an open question whether this translates to vulnerabilities in real
systems. For example, an attack on self-driving cars would in practice entail
modifying the driving environment, which then impacts the video inputs to the
car's controller, thereby indirectly leading to incorrect driving decisions.
Such attacks require accounting for system dynamics and tracking viewpoint
changes. We propose a scalable approach for finding adversarial modifications
of a simulated autonomous driving environment using a differentiable
approximation for the mapping from environmental modifications (rectangles on
the road) to the corresponding video inputs to the controller neural network.
Given the parameters of the rectangles, our proposed differentiable mapping
composites them onto pre-recorded video streams of the original environment,
accounting for geometric and color variations. Moreover, we propose a multiple
trajectory sampling approach that enables our attacks to be robust to a car's
self-correcting behavior. When combined with a neural network-based controller,
our approach allows the design of adversarial modifications through end-to-end
gradient-based optimization. Using the Carla autonomous driving simulator, we
show that our approach is significantly more scalable and far more effective at
identifying autonomous vehicle vulnerabilities in simulation experiments than a
state-of-the-art approach based on Bayesian Optimization.",arxiv
http://arxiv.org/abs/2110.09955v2,2021-11-08T02:43:36Z,2021-10-13T12:03:36Z,"Positional-Spectral-Temporal Attention in 3D Convolutional Neural
  Networks for EEG Emotion Recognition","Recognizing the feelings of human beings plays a critical role in our daily
communication. Neuroscience has demonstrated that different emotion states
present different degrees of activation in different brain regions, EEG
frequency bands and temporal stamps. In this paper, we propose a novel
structure to explore the informative EEG features for emotion recognition. The
proposed module, denoted by PST-Attention, consists of Positional, Spectral and
Temporal Attention modules to explore more discriminative EEG features.
Specifically, the Positional Attention module is to capture the activate
regions stimulated by different emotions in the spatial dimension. The Spectral
and Temporal Attention modules assign the weights of different frequency bands
and temporal slices respectively. Our method is adaptive as well as efficient
which can be fit into 3D Convolutional Neural Networks (3D-CNN) as a plug-in
module. We conduct experiments on two real-world datasets. 3D-CNN combined with
our module achieves promising results and demonstrate that the PST-Attention is
able to capture stable patterns for emotion recognition from EEG.",arxiv
http://arxiv.org/abs/2009.02617v1,2020-09-05T23:55:24Z,2020-09-05T23:55:24Z,"Artefact removal in ground truth and noise model deficient sub-cellular
  nanoscopy images using auto-encoder deep learning","Image denoising or artefact removal using deep learning is possible in the
availability of supervised training dataset acquired in real experiments or
synthesized using known noise models. Neither of the conditions can be
fulfilled for nanoscopy (super-resolution optical microscopy) images that are
generated from microscopy videos through statistical analysis techniques. Due
to several physical constraints, supervised dataset cannot be measured. Due to
non-linear spatio-temporal mixing of data and valuable statistics of
fluctuations from fluorescent molecules which compete with noise statistics,
noise or artefact models in nanoscopy images cannot be explicitly learnt.
Therefore, such problem poses unprecedented challenges to deep learning. Here,
we propose a robust and versatile simulation-supervised training approach of
deep learning auto-encoder architectures for the highly challenging nanoscopy
images of sub-cellular structures inside biological samples. We show the proof
of concept for one nanoscopy method and investigate the scope of
generalizability across structures, noise models, and nanoscopy algorithms not
included during simulation-supervised training. We also investigate a variety
of loss functions and learning models and discuss the limitation of existing
performance metrics for nanoscopy images. We generate valuable insights for
this highly challenging and unsolved problem in nanoscopy, and set the
foundation for application of deep learning problems in nanoscopy for life
sciences.",arxiv
http://arxiv.org/abs/2005.12762v2,2020-05-27T13:32:15Z,2020-05-26T14:34:07Z,"Exploring aspects of similarity between spoken personal narratives by
  disentangling them into narrative clause types","Sharing personal narratives is a fundamental aspect of human social behavior
as it helps share our life experiences. We can tell stories and rely on our
background to understand their context, similarities, and differences. A
substantial effort has been made towards developing storytelling machines or
inferring characters' features. However, we don't usually find models that
compare narratives. This task is remarkably challenging for machines since
they, as sometimes we do, lack an understanding of what similarity means. To
address this challenge, we first introduce a corpus of real-world spoken
personal narratives comprising 10,296 narrative clauses from 594 video
transcripts. Second, we ask non-narrative experts to annotate those clauses
under Labov's sociolinguistic model of personal narratives (i.e., action,
orientation, and evaluation clause types) and train a classifier that reaches
84.7% F-score for the highest-agreed clauses. Finally, we match stories and
explore whether people implicitly rely on Labov's framework to compare
narratives. We show that actions followed by the narrator's evaluation of these
are the aspects non-experts consider the most. Our approach is intended to help
inform machine learning methods aimed at studying or representing personal
narratives.",arxiv
http://arxiv.org/abs/2011.14579v2,2021-06-18T02:06:59Z,2020-11-30T06:55:05Z,"End-to-End 3D Point Cloud Learning for Registration Task Using Virtual
  Correspondences","3D Point cloud registration is still a very challenging topic due to the
difficulty in finding the rigid transformation between two point clouds with
partial correspondences, and it's even harder in the absence of any initial
estimation information. In this paper, we present an end-to-end deep-learning
based approach to resolve the point cloud registration problem. Firstly, the
revised LPD-Net is introduced to extract features and aggregate them with the
graph network. Secondly, the self-attention mechanism is utilized to enhance
the structure information in the point cloud and the cross-attention mechanism
is designed to enhance the corresponding information between the two input
point clouds. Based on which, the virtual corresponding points can be generated
by a soft pointer based method, and finally, the point cloud registration
problem can be solved by implementing the SVD method. Comparison results in
ModelNet40 dataset validate that the proposed approach reaches the
state-of-the-art in point cloud registration tasks and experiment resutls in
KITTI dataset validate the effectiveness of the proposed approach in real
applications.Our source code is available at
\url{https://github.com/qiaozhijian/VCR-Net.git}",arxiv
http://arxiv.org/abs/2110.01770v2,2021-10-08T18:27:02Z,2021-10-05T01:06:53Z,"Procedure Planning in Instructional Videos via Contextual Modeling and
  Model-based Policy Learning","Learning new skills by observing humans' behaviors is an essential capability
of AI. In this work, we leverage instructional videos to study humans'
decision-making processes, focusing on learning a model to plan goal-directed
actions in real-life videos. In contrast to conventional action recognition,
goal-directed actions are based on expectations of their outcomes requiring
causal knowledge of potential consequences of actions. Thus, integrating the
environment structure with goals is critical for solving this task. Previous
works learn a single world model will fail to distinguish various tasks,
resulting in an ambiguous latent space; planning through it will gradually
neglect the desired outcomes since the global information of the future goal
degrades quickly as the procedure evolves. We address these limitations with a
new formulation of procedure planning and propose novel algorithms to model
human behaviors through Bayesian Inference and model-based Imitation Learning.
Experiments conducted on real-world instructional videos show that our method
can achieve state-of-the-art performance in reaching the indicated goals.
Furthermore, the learned contextual information presents interesting features
for planning in a latent space.",arxiv
http://arxiv.org/abs/1801.05394v2,2018-01-26T11:51:31Z,2018-01-16T18:05:08Z,Time Series Segmentation through Automatic Feature Learning,"Internet of things (IoT) applications have become increasingly popular in
recent years, with applications ranging from building energy monitoring to
personal health tracking and activity recognition. In order to leverage these
data, automatic knowledge extraction - whereby we map from observations to
interpretable states and transitions - must be done at scale. As such, we have
seen many recent IoT data sets include annotations with a human expert
specifying states, recorded as a set of boundaries and associated labels in a
data sequence. These data can be used to build automatic labeling algorithms
that produce labels as an expert would. Here, we refer to human-specified
boundaries as breakpoints. Traditional changepoint detection methods only look
for statistically-detectable boundaries that are defined as abrupt variations
in the generative parameters of a data sequence. However, we observe that
breakpoints occur on more subtle boundaries that are non-trivial to detect with
these statistical methods. In this work, we propose a new unsupervised
approach, based on deep learning, that outperforms existing techniques and
learns the more subtle, breakpoint boundaries with a high accuracy. Through
extensive experiments on various real-world data sets - including
human-activity sensing data, speech signals, and electroencephalogram (EEG)
activity traces - we demonstrate the effectiveness of our algorithm for
practical applications. Furthermore, we show that our approach achieves
significantly better performance than previous methods.",arxiv
http://arxiv.org/abs/2011.11270v1,2020-11-23T08:20:21Z,2020-11-23T08:20:21Z,"COCOI: Contact-aware Online Context Inference for Generalizable
  Non-planar Pushing","General contact-rich manipulation problems are long-standing challenges in
robotics due to the difficulty of understanding complicated contact physics.
Deep reinforcement learning (RL) has shown great potential in solving robot
manipulation tasks. However, existing RL policies have limited adaptability to
environments with diverse dynamics properties, which is pivotal in solving many
contact-rich manipulation tasks. In this work, we propose Contact-aware Online
COntext Inference (COCOI), a deep RL method that encodes a context embedding of
dynamics properties online using contact-rich interactions. We study this
method based on a novel and challenging non-planar pushing task, where the
robot uses a monocular camera image and wrist force torque sensor reading to
push an object to a goal location while keeping it upright. We run extensive
experiments to demonstrate the capability of COCOI in a wide range of settings
and dynamics properties in simulation, and also in a sim-to-real transfer
scenario on a real robot (Video: https://youtu.be/nrmJYksh1Kc)",arxiv
http://arxiv.org/abs/1804.05195v2,2018-07-24T08:49:35Z,2018-04-14T09:33:40Z,Motion-based Object Segmentation based on Dense RGB-D Scene Flow,"Given two consecutive RGB-D images, we propose a model that estimates a dense
3D motion field, also known as scene flow. We take advantage of the fact that
in robot manipulation scenarios, scenes often consist of a set of rigidly
moving objects. Our model jointly estimates (i) the segmentation of the scene
into an unknown but finite number of objects, (ii) the motion trajectories of
these objects and (iii) the object scene flow. We employ an hourglass, deep
neural network architecture. In the encoding stage, the RGB and depth images
undergo spatial compression and correlation. In the decoding stage, the model
outputs three images containing a per-pixel estimate of the corresponding
object center as well as object translation and rotation. This forms the basis
for inferring the object segmentation and final object scene flow. To evaluate
our model, we generated a new and challenging, large-scale, synthetic dataset
that is specifically targeted at robotic manipulation: It contains a large
number of scenes with a very diverse set of simultaneously moving 3D objects
and is recorded with a simulated, static RGB-D camera. In quantitative
experiments, we show that we outperform state-of-the-art scene flow and
motion-segmentation methods on this data set. In qualitative experiments, we
show how our learned model transfers to challenging real-world scenes, visually
generating better results than existing methods.",arxiv
http://arxiv.org/abs/2103.07298v1,2021-03-12T14:14:45Z,2021-03-12T14:14:45Z,Augmented Environment Representations with Complete Object Models,"While 2D occupancy maps commonly used in mobile robotics enable safe
navigation in indoor environments, in order for robots to understand their
environment to the level required for them to perform more advanced tasks,
representing 3D geometry and semantic environment information is required. We
propose a pipeline that can generate a multi-layer representation of indoor
environments for robotic applications. The proposed representation includes 3D
metric-semantic layers, a 2D occupancy layer, and an object instance layer
where known objects are replaced with an approximate model obtained through a
novel model-matching approach. The metric-semantic layer and the object
instance layer are combined to form an augmented representation of the
environment. Experiments show that the proposed shape matching method
outperforms a state-of-the-art deep learning method when tasked to complete
unseen parts of objects in the scene. The pipeline performance translates well
from simulation to real world as shown by F1-score analysis, with semantic
segmentation accuracy using Mask R-CNN acting as the major bottleneck. Finally,
we also demonstrate on a real robotic platform how the multi-layer map can be
used to improve navigation safety.",arxiv
http://arxiv.org/abs/2106.01970v1,2021-06-03T16:18:01Z,2021-06-03T16:18:01Z,"NeRFactor: Neural Factorization of Shape and Reflectance Under an
  Unknown Illumination","We address the problem of recovering the shape and spatially-varying
reflectance of an object from posed multi-view images of the object illuminated
by one unknown lighting condition. This enables the rendering of novel views of
the object under arbitrary environment lighting and editing of the object's
material properties. The key to our approach, which we call Neural Radiance
Factorization (NeRFactor), is to distill the volumetric geometry of a Neural
Radiance Field (NeRF) [Mildenhall et al. 2020] representation of the object
into a surface representation and then jointly refine the geometry while
solving for the spatially-varying reflectance and the environment lighting.
Specifically, NeRFactor recovers 3D neural fields of surface normals, light
visibility, albedo, and Bidirectional Reflectance Distribution Functions
(BRDFs) without any supervision, using only a re-rendering loss, simple
smoothness priors, and a data-driven BRDF prior learned from real-world BRDF
measurements. By explicitly modeling light visibility, NeRFactor is able to
separate shadows from albedo and synthesize realistic soft or hard shadows
under arbitrary lighting conditions. NeRFactor is able to recover convincing 3D
models for free-viewpoint relighting in this challenging and underconstrained
capture setup for both synthetic and real scenes. Qualitative and quantitative
experiments show that NeRFactor outperforms classic and deep learning-based
state of the art across various tasks. Our code and data are available at
people.csail.mit.edu/xiuming/projects/nerfactor/.",arxiv
http://arxiv.org/abs/2104.06826v1,2021-04-14T12:57:40Z,2021-04-14T12:57:40Z,Towards Unsupervised Fine-Tuning for Edge Video Analytics,"Judging by popular and generic computer vision challenges, such as the
ImageNet or PASCAL VOC, neural networks have proven to be exceptionally
accurate in recognition tasks. However, state-of-the-art accuracy often comes
at a high computational price, requiring equally state-of-the-art and high-end
hardware acceleration to achieve anything near real-time performance. At the
same time, use cases such as smart cities or autonomous vehicles require an
automated analysis of images from fixed cameras in real-time. Due to the huge
and constant amount of network bandwidth these streams would generate, we
cannot rely on offloading compute to the omnipresent and omnipotent cloud.
Therefore, a distributed Edge Cloud must be in charge to process images
locally. However, the Edge Cloud is, by nature, resource-constrained, which
puts a limit on the computational complexity of the models executed in the
edge. Nonetheless, there is a need for a meeting point between the Edge Cloud
and accurate real-time video analytics. In this paper, we propose a method for
improving accuracy of edge models without any extra compute cost by means of
automatic model specialization. First, we show how the sole assumption of
static cameras allows us to make a series of considerations that greatly
simplify the scope of the problem. Then, we present Edge AutoTuner, a framework
that implements and brings these considerations together to automate the
end-to-end fine-tuning of models. Finally, we show that complex neural networks
- able to generalize better - can be effectively used as teachers to annotate
datasets for the fine-tuning of lightweight neural networks and tailor them to
the specific edge context, which boosts accuracy at constant computational
cost, and do so without any human interaction. Results show that our method can
automatically improve accuracy of pre-trained models by an average of 21%.",arxiv
http://arxiv.org/abs/2108.03008v1,2021-08-06T08:51:16Z,2021-08-06T08:51:16Z,"An Empirical Study on End-to-End Singing Voice Synthesis with
  Encoder-Decoder Architectures","With the rapid development of neural network architectures and speech
processing models, singing voice synthesis with neural networks is becoming the
cutting-edge technique of digital music production. In this work, in order to
explore how to improve the quality and efficiency of singing voice synthesis,
in this work, we use encoder-decoder neural models and a number of vocoders to
achieve singing voice synthesis. We conduct experiments to demonstrate that the
models can be trained using voice data with pitch information, lyrics and beat
information, and the trained models can produce smooth, clear and natural
singing voice that is close to real human voice. As the models work in the
end-to-end manner, they allow users who are not domain experts to directly
produce singing voice by arranging pitches, lyrics and beats.",arxiv
http://arxiv.org/abs/1709.06734v2,2018-07-06T13:20:51Z,2017-09-20T06:36:14Z,Enhancing Quality for HEVC Compressed Videos,"The latest High Efficiency Video Coding (HEVC) standard has been increasingly
applied to generate video streams over the Internet. However, HEVC compressed
videos may incur severe quality degradation, particularly at low bit-rates.
Thus, it is necessary to enhance the visual quality of HEVC videos at the
decoder side. To this end, this paper proposes a Quality Enhancement
Convolutional Neural Network (QE-CNN) method that does not require any
modification of the encoder to achieve quality enhancement for HEVC. In
particular, our QE-CNN method learns QE-CNN-I and QE-CNN-P models to reduce the
distortion of HEVC I and P frames, respectively. The proposed method differs
from the existing CNN-based quality enhancement approaches, which only handle
intra-coding distortion and are thus not suitable for P frames. Our
experimental results validate that our QE-CNN method is effective in enhancing
quality for both I and P frames of HEVC videos. To apply our QE-CNN method in
time-constrained scenarios, we further propose a Time-constrained Quality
Enhancement Optimization (TQEO) scheme. Our TQEO scheme controls the
computational time of QE-CNN to meet a target, meanwhile maximizing the quality
enhancement. Next, the experimental results demonstrate the effectiveness of
our TQEO scheme from the aspects of time control accuracy and quality
enhancement under different time constraints. Finally, we design a prototype to
implement our TQEO scheme in a real-time scenario.",arxiv
http://arxiv.org/abs/1804.02173v1,2018-04-06T09:03:29Z,2018-04-06T09:03:29Z,"On the Robustness of Speech Emotion Recognition for Human-Robot
  Interaction with Deep Neural Networks","Speech emotion recognition (SER) is an important aspect of effective
human-robot collaboration and received a lot of attention from the research
community. For example, many neural network-based architectures were proposed
recently and pushed the performance to a new level. However, the applicability
of such neural SER models trained only on in-domain data to noisy conditions is
currently under-researched. In this work, we evaluate the robustness of
state-of-the-art neural acoustic emotion recognition models in human-robot
interaction scenarios. We hypothesize that a robot's ego noise, room
conditions, and various acoustic events that can occur in a home environment
can significantly affect the performance of a model. We conduct several
experiments on the iCub robot platform and propose several novel ways to reduce
the gap between the model's performance during training and testing in
real-world conditions. Furthermore, we observe large improvements in the model
performance on the robot and demonstrate the necessity of introducing several
data augmentation techniques like overlaying background noise and loudness
variations to improve the robustness of the neural approaches.",arxiv
http://arxiv.org/abs/1812.01346v1,2018-12-04T11:37:25Z,2018-12-04T11:37:25Z,"LSTM based AE-DNN constraint for better late reverb suppression in
  multi-channel LP formulation","Prediction of late reverberation component using multi-channel linear
prediction (MCLP) in short-time Fourier transform (STFT) domain is an effective
means to enhance reverberant speech. Traditionally, a speech power spectral
density (PSD) weighted prediction error (WPE) minimization approach is used to
estimate the prediction filters. The method is sensitive to the estimate of the
desired signal PSD. In this paper, we propose a deep neural network (DNN) based
non-linear estimate for the desired signal PSD. An auto encoder trained on
clean speech STFT coefficients is used as the desired signal prior. We explore
two different architectures based on (i) fully-connected (FC) feed-forward, and
(ii) recurrent long short-term memory (LSTM) layers. Experiments using real
room impulse responses show that the LSTM-DNN based PSD estimate performs
better than the traditional methods for late reverb suppression.",arxiv
http://arxiv.org/abs/1811.07112v2,2019-04-10T09:59:47Z,2018-11-17T07:09:13Z,Augmented LiDAR Simulator for Autonomous Driving,"In Autonomous Driving (AD), detection and tracking of obstacles on the roads
is a critical task. Deep-learning based methods using annotated LiDAR data have
been the most widely adopted approach for this. Unfortunately, annotating 3D
point cloud is a very challenging, time- and money-consuming task. In this
paper, we propose a novel LiDAR simulator that augments real point cloud with
synthetic obstacles (e.g., cars, pedestrians, and other movable objects).
Unlike previous simulators that entirely rely on CG models and game engines,
our augmented simulator bypasses the requirement to create high-fidelity
background CAD models. Instead, we can simply deploy a vehicle with a LiDAR
scanner to sweep the street of interests to obtain the background point cloud,
based on which annotated point cloud can be automatically generated. This
unique ""scan-and-simulate"" capability makes our approach scalable and
practical, ready for large-scale industrial applications. In this paper, we
describe our simulator in detail, in particular the placement of obstacles that
is critical for performance enhancement. We show that detectors with our
simulated LiDAR point cloud alone can perform comparably (within two percentage
points) with these trained with real data. Mixing real and simulated data can
achieve over 95% accuracy.",arxiv
http://arxiv.org/abs/2012.15531v1,2020-12-31T10:33:09Z,2020-12-31T10:33:09Z,"Colonoscopy Polyp Detection: Domain Adaptation From Medical Report
  Images to Real-time Videos","Automatic colorectal polyp detection in colonoscopy video is a fundamental
task, which has received a lot of attention. Manually annotating polyp region
in a large scale video dataset is time-consuming and expensive, which limits
the development of deep learning techniques. A compromise is to train the
target model by using labeled images and infer on colonoscopy videos. However,
there are several issues between the image-based training and video-based
inference, including domain differences, lack of positive samples, and temporal
smoothness. To address these issues, we propose an Image-video-joint polyp
detection network (Ivy-Net) to address the domain gap between colonoscopy
images from historical medical reports and real-time videos. In our Ivy-Net, a
modified mixup is utilized to generate training data by combining the positive
images and negative video frames at the pixel level, which could learn the
domain adaptive representations and augment the positive samples.
Simultaneously, a temporal coherence regularization (TCR) is proposed to
introduce the smooth constraint on feature-level in adjacent frames and improve
polyp detection by unlabeled colonoscopy videos. For evaluation, a new large
colonoscopy polyp dataset is collected, which contains 3056 images from
historical medical reports of 889 positive patients and 7.5-hour videos of 69
patients (28 positive). The experiments on the collected dataset demonstrate
that our Ivy-Net achieves the state-of-the-art result on colonoscopy video.",arxiv
http://arxiv.org/abs/1610.10042v2,2016-11-21T13:50:11Z,2016-10-31T18:08:54Z,ConfocalGN : a minimalistic confocal image simulator,"SUMMARY : We developed a user-friendly software to generate synthetic
confocal microscopy images from a ground truth specified as a 3D bitmap with
pixels of arbitrary size. The software can analyze a real confocal stack to
derivate noise parameters and will use them directly to generate new images
with similar noise characteristics. Such synthetic images can then be used to
assert the quality and robustness of an image analysis pipeline, as well as be
used to train machine-learning image analysis procedures. We illustrate the
approach with closed curves corresponding to the microtubule ring present in
blood platelet. AVAILABILITY AND IMPLEMENTATION: ConfocalGN is written in
Matlab but does not require any toolbox. The source code is distributed under
the GPL 3.0 licence on https://github.com/SergeDmi/ConfocalGN.",arxiv
http://arxiv.org/abs/1806.01196v2,2021-02-10T13:09:00Z,2018-06-04T16:38:45Z,Face Synthesis for Eyeglass-Robust Face Recognition,"In the application of face recognition, eyeglasses could significantly
degrade the recognition accuracy. A feasible method is to collect large-scale
face images with eyeglasses for training deep learning methods. However, it is
difficult to collect the images with and without glasses of the same identity,
so that it is difficult to optimize the intra-variations caused by eyeglasses.
In this paper, we propose to address this problem in a virtual synthesis
manner. The high-fidelity face images with eyeglasses are synthesized based on
3D face model and 3D eyeglasses. Models based on deep learning methods are then
trained on the synthesized eyeglass face dataset, achieving better performance
than previous ones. Experiments on the real face database validate the
effectiveness of our synthesized data for improving eyeglass face recognition
performance.",arxiv
http://arxiv.org/abs/2012.03911v1,2020-12-07T18:41:35Z,2020-12-07T18:41:35Z,"Learning Video Instance Segmentation with Recurrent Graph Neural
  Networks","Most existing approaches to video instance segmentation comprise multiple
modules that are heuristically combined to produce the final output.
Formulating a purely learning-based method instead, which models both the
temporal aspect as well as a generic track management required to solve the
video instance segmentation task, is a highly challenging problem. In this
work, we propose a novel learning formulation, where the entire video instance
segmentation problem is modelled jointly. We fit a flexible model to our
formulation that, with the help of a graph neural network, processes all
available new information in each frame. Past information is considered and
processed via a recurrent connection. We demonstrate the effectiveness of the
proposed approach in comprehensive experiments. Our approach, operating at over
25 FPS, outperforms previous video real-time methods. We further conduct
detailed ablative experiments that validate the different aspects of our
approach.",arxiv
http://arxiv.org/abs/2110.15727v1,2021-10-11T07:42:41Z,2021-10-11T07:42:41Z,"Calling to CNN-LSTM for Rumor Detection: A Deep Multi-channel Model for
  Message Veracity Classification in Microblogs","Reputed by their low-cost, easy-access, real-time and valuable information,
social media also wildly spread unverified or fake news. Rumors can notably
cause severe damage on individuals and the society. Therefore, rumor detection
on social media has recently attracted tremendous attention. Most rumor
detection approaches focus on rumor feature analysis and social features, i.e.,
metadata in social media. Unfortunately, these features are data-specific and
may not always be available, e.g., when the rumor has just popped up and not
yet propagated. In contrast, post contents (including images or videos) play an
important role and can indicate the diffusion purpose of a rumor. Furthermore,
rumor classification is also closely related to opinion mining and sentiment
analysis. Yet, to the best of our knowledge, exploiting images and sentiments
is little investigated.Considering the available multimodal features from
microblogs, notably, we propose in this paper an end-to-end model called
deepMONITOR that is based on deep neural networks and allows quite accurate
automated rumor verification, by utilizing all three characteristics: post
textual and image contents, as well as sentiment. deepMONITOR concatenates
image features with the joint text and sentiment features to produce a
reliable, fused classification. We conduct extensive experiments on two
large-scale, real-world datasets. The results show that deepMONITOR achieves a
higher accuracy than state-of-the-art methods.",arxiv
http://arxiv.org/abs/1611.01235v1,2016-11-04T01:10:07Z,2016-11-04T01:10:07Z,"A Self-Driving Robot Using Deep Convolutional Neural Networks on
  Neuromorphic Hardware","Neuromorphic computing is a promising solution for reducing the size, weight
and power of mobile embedded systems. In this paper, we introduce a realization
of such a system by creating the first closed-loop battery-powered
communication system between an IBM TrueNorth NS1e and an autonomous
Android-Based Robotics platform. Using this system, we constructed a dataset of
path following behavior by manually driving the Android-Based robot along steep
mountain trails and recording video frames from the camera mounted on the robot
along with the corresponding motor commands. We used this dataset to train a
deep convolutional neural network implemented on the TrueNorth NS1e. The NS1e,
which was mounted on the robot and powered by the robot's battery, resulted in
a self-driving robot that could successfully traverse a steep mountain path in
real time. To our knowledge, this represents the first time the TrueNorth NS1e
neuromorphic chip has been embedded on a mobile platform under closed-loop
control.",arxiv
http://arxiv.org/abs/1808.03506v4,2019-03-05T23:50:01Z,2018-08-10T12:30:03Z,"ChipNet: Real-Time LiDAR Processing for Drivable Region Segmentation on
  an FPGA","This paper presents a field-programmable gate array (FPGA) design of a
segmentation algorithm based on convolutional neural network (CNN) that can
process light detection and ranging (LiDAR) data in real-time. For autonomous
vehicles, drivable region segmentation is an essential step that sets up the
static constraints for planning tasks. Traditional drivable region segmentation
algorithms are mostly developed on camera data, so their performance is
susceptible to the light conditions and the qualities of road markings. LiDAR
sensors can obtain the 3D geometry information of the vehicle surroundings with
high precision. However, it is a computational challenge to process a large
amount of LiDAR data in real-time. In this paper, a convolutional neural
network model is proposed and trained to perform semantic segmentation using
data from the LiDAR sensor. An efficient hardware architecture is proposed and
implemented on an FPGA that can process each LiDAR scan in 17.59 ms, which is
much faster than the previous works. Evaluated using Ford and KITTI road
detection benchmarks, the proposed solution achieves both high accuracy in
performance and real-time processing in speed.",arxiv
http://arxiv.org/abs/2011.00192v1,2020-10-31T05:26:49Z,2020-10-31T05:26:49Z,Personalized Multimodal Feedback Generation in Education,"The automatic evaluation for school assignments is an important application
of AI in the education field. In this work, we focus on the task of
personalized multimodal feedback generation, which aims to generate
personalized feedback for various teachers to evaluate students' assignments
involving multimodal inputs such as images, audios, and texts. This task
involves the representation and fusion of multimodal information and natural
language generation, which presents the challenges from three aspects: 1) how
to encode and integrate multimodal inputs; 2) how to generate feedback specific
to each modality; and 3) how to realize personalized feedback generation. In
this paper, we propose a novel Personalized Multimodal Feedback Generation
Network (PMFGN) armed with a modality gate mechanism and a personalized bias
mechanism to address these challenges. The extensive experiments on real-world
K-12 education data show that our model significantly outperforms several
baselines by generating more accurate and diverse feedback. In addition,
detailed ablation experiments are conducted to deepen our understanding of the
proposed framework.",arxiv
http://arxiv.org/abs/1608.02239v1,2016-08-07T16:30:42Z,2016-08-07T16:30:42Z,"Deep Learning a Grasp Function for Grasping under Gripper Pose
  Uncertainty","This paper presents a new method for parallel-jaw grasping of isolated
objects from depth images, under large gripper pose uncertainty. Whilst most
approaches aim to predict the single best grasp pose from an image, our method
first predicts a score for every possible grasp pose, which we denote the grasp
function. With this, it is possible to achieve grasping robust to the gripper's
pose uncertainty, by smoothing the grasp function with the pose uncertainty
function. Therefore, if the single best pose is adjacent to a region of poor
grasp quality, that pose will no longer be chosen, and instead a pose will be
chosen which is surrounded by a region of high grasp quality. To learn this
function, we train a Convolutional Neural Network which takes as input a single
depth image of an object, and outputs a score for each grasp pose across the
image. Training data for this is generated by use of physics simulation and
depth image simulation with 3D object meshes, to enable acquisition of
sufficient data without requiring exhaustive real-world experiments. We
evaluate with both synthetic and real experiments, and show that the learned
grasp score is more robust to gripper pose uncertainty than when this
uncertainty is not accounted for.",arxiv
http://arxiv.org/abs/1602.02255v2,2016-02-15T09:43:56Z,2016-02-06T13:43:24Z,Deep Cross-Modal Hashing,"Due to its low storage cost and fast query speed, cross-modal hashing (CMH)
has been widely used for similarity search in multimedia retrieval
applications. However, almost all existing CMH methods are based on
hand-crafted features which might not be optimally compatible with the
hash-code learning procedure. As a result, existing CMH methods with
handcrafted features may not achieve satisfactory performance. In this paper,
we propose a novel cross-modal hashing method, called deep crossmodal hashing
(DCMH), by integrating feature learning and hash-code learning into the same
framework. DCMH is an end-to-end learning framework with deep neural networks,
one for each modality, to perform feature learning from scratch. Experiments on
two real datasets with text-image modalities show that DCMH can outperform
other baselines to achieve the state-of-the-art performance in cross-modal
retrieval applications.",arxiv
http://arxiv.org/abs/1701.04925v1,2017-01-18T02:10:56Z,2017-01-18T02:10:56Z,Action Recognition: From Static Datasets to Moving Robots,"Deep learning models have achieved state-of-the- art performance in
recognizing human activities, but often rely on utilizing background cues
present in typical computer vision datasets that predominantly have a
stationary camera. If these models are to be employed by autonomous robots in
real world environments, they must be adapted to perform independently of
background cues and camera motion effects. To address these challenges, we
propose a new method that firstly generates generic action region proposals
with good potential to locate one human action in unconstrained videos
regardless of camera motion and then uses action proposals to extract and
classify effective shape and motion features by a ConvNet framework. In a range
of experiments, we demonstrate that by actively proposing action regions during
both training and testing, state-of-the-art or better performance is achieved
on benchmarks. We show the outperformance of our approach compared to the
state-of-the-art in two new datasets; one emphasizes on irrelevant background,
the other highlights the camera motion. We also validate our action recognition
method in an abnormal behavior detection scenario to improve workplace safety.
The results verify a higher success rate for our method due to the ability of
our system to recognize human actions regardless of environment and camera
motion.",arxiv
http://arxiv.org/abs/2106.07910v2,2021-08-15T08:40:46Z,2021-06-15T06:47:51Z,"Wavelength-based Attributed Deep Neural Network for Underwater Image
  Restoration","Background: Underwater images, in general, suffer from low contrast and high
color distortions due to the non-uniform attenuation of the light as it
propagates through the water. In addition, the degree of attenuation varies
with the wavelength resulting in the asymmetric traversing of colors. Despite
the prolific works for underwater image restoration (UIR) using deep learning,
the above asymmetricity has not been addressed in the respective network
engineering.
  Contributions: As the first novelty, this paper shows that attributing the
right receptive field size (context) based on the traversing range of the color
channel may lead to a substantial performance gain for the task of UIR.
Further, it is important to suppress the irrelevant multi-contextual features
and increase the representational power of the model. Therefore, as a second
novelty, we have incorporated an attentive skip mechanism to adaptively refine
the learned multi-contextual features. The proposed framework, called Deep
WaveNet, is optimized using the traditional pixel-wise and feature-based cost
functions. An extensive set of experiments have been carried out to show the
efficacy of the proposed scheme over existing best-published literature on
benchmark datasets. More importantly, we have demonstrated a comprehensive
validation of enhanced images across various high-level vision tasks, e.g.,
underwater image semantic segmentation, and diver's 2D pose estimation. A
sample video to exhibit our real-world performance is available at
\url{https://tinyurl.com/yzcrup9n}. Also, we have open-sourced our framework at
\url{https://github.com/pksvision/Deep-WaveNet-UnderwaterImage-Restoration}.",arxiv
http://arxiv.org/abs/1907.06007v2,2019-12-09T12:17:41Z,2019-07-13T04:18:04Z,SynthText3D: Synthesizing Scene Text Images from 3D Virtual Worlds,"With the development of deep neural networks, the demand for a significant
amount of annotated training data becomes the performance bottlenecks in many
fields of research and applications. Image synthesis can generate annotated
images automatically and freely, which gains increasing attention recently. In
this paper, we propose to synthesize scene text images from the 3D virtual
worlds, where the precise descriptions of scenes, editable
illumination/visibility, and realistic physics are provided. Different from the
previous methods which paste the rendered text on static 2D images, our method
can render the 3D virtual scene and text instances as an entirety. In this way,
real-world variations, including complex perspective transformations, various
illuminations, and occlusions, can be realized in our synthesized scene text
images. Moreover, the same text instances with various viewpoints can be
produced by randomly moving and rotating the virtual camera, which acts as
human eyes. The experiments on the standard scene text detection benchmarks
using the generated synthetic data demonstrate the effectiveness and
superiority of the proposed method. The code and synthetic data is available
at: https://github.com/MhLiao/SynthText3D",arxiv
http://arxiv.org/abs/2106.02285v1,2021-06-04T06:50:34Z,2021-06-04T06:50:34Z,Subdivision-Based Mesh Convolution Networks,"Convolutional neural networks (CNNs) have made great breakthroughs in 2D
computer vision. However, the irregular structure of meshes makes it hard to
exploit the power of CNNs directly. A subdivision surface provides a
hierarchical multi-resolution structure, and each face in a closed 2-manifold
triangle mesh is exactly adjacent to three faces. Motivated by these two
properties, this paper introduces a novel and flexible CNN framework, named
SubdivNet, for 3D triangle meshes with Loop subdivision sequence connectivity.
Making an analogy between mesh faces and pixels in a 2D image allows us to
present a mesh convolution operator to aggregate local features from adjacent
faces. By exploiting face neighborhoods, this convolution can support standard
2D convolutional network concepts, e.g. variable kernel size, stride, and
dilation. Based on the multi-resolution hierarchy, we propose a spatial uniform
pooling layer which merges four faces into one and an upsampling method which
splits one face into four. As a result, many popular 2D CNN architectures can
be readily adapted to processing 3D meshes. Meshes with arbitrary connectivity
can be remeshed to hold Loop subdivision sequence connectivity via
self-parameterization, making SubdivNet a general approach. Experiments on mesh
classification, segmentation, correspondence, and retrieval from the real-world
demonstrate the effectiveness and efficiency of SubdivNet.",arxiv
http://arxiv.org/abs/1906.06059v2,2019-08-20T15:43:44Z,2019-06-14T07:39:03Z,"MonoLoco: Monocular 3D Pedestrian Localization and Uncertainty
  Estimation","We tackle the fundamentally ill-posed problem of 3D human localization from
monocular RGB images. Driven by the limitation of neural networks outputting
point estimates, we address the ambiguity in the task by predicting confidence
intervals through a loss function based on the Laplace distribution. Our
architecture is a light-weight feed-forward neural network that predicts 3D
locations and corresponding confidence intervals given 2D human poses. The
design is particularly well suited for small training data, cross-dataset
generalization, and real-time applications. Our experiments show that we (i)
outperform state-of-the-art results on KITTI and nuScenes datasets, (ii) even
outperform a stereo-based method for far-away pedestrians, and (iii) estimate
meaningful confidence intervals. We further share insights on our model of
uncertainty in cases of limited observations and out-of-distribution samples.",arxiv
http://arxiv.org/abs/2011.09855v1,2020-11-19T14:36:33Z,2020-11-19T14:36:33Z,"Recursive Deep Prior Video: a Super Resolution algorithm for Time-Lapse
  Microscopy of organ-on-chip experiments","Biological experiments based on organ-on-chips (OOCs) exploit light
Time-Lapse Microscopy (TLM) for a direct observation of cell movement that is
an observable signature of underlying biological processes. A high spatial
resolution is essential to capture cell dynamics and interactions from recorded
experiments by TLM. Unfortunately, due to physical and cost limitations,
acquiring high resolution videos is not always possible. To overcome the
problem, we present here a new deep learning-based algorithm that extends the
well known Deep Image Prior (DIP) to TLM Video Super Resolution (SR) without
requiring any training. The proposed Recursive Deep Prior Video (RDPV) method
introduces some novelties. The weights of the DIP network architecture are
initialized for each of the frames according to a new recursive updating rule
combined with an efficient early stopping criterion. Moreover, the DIP loss
function is penalized by two different Total Variation (TV) based terms. The
method has been validated on synthetic, i.e., artificially generated, as well
as real videos from OOC experiments related to tumor-immune interaction.
Achieved results are compared with several state-of-the-art trained deep
learning SR algorithms showing outstanding performances.",arxiv
http://arxiv.org/abs/2006.00781v1,2020-06-01T08:14:10Z,2020-06-01T08:14:10Z,"Reducing the X-ray radiation exposure frequency in cardio-angiography
  via deep-learning based video interpolation","Cardiac coronary angiography is a major technology to assist doctors during
cardiac interventional surgeries. Under the exposure of X-ray radiation,
doctors inject contrast agents through catheters to determine the position and
status of coronary vessels in real time. To get a coronary angiography video
with a high frame rate, the doctor needs to increase the exposure frequency and
intensity of the X-ray. This will inevitably increase the X-ray harm to both
patients and surgeons. In this work, we innovatively utilize a deep-learning
based video interpolation algorithm to interpolate coronary angiography videos.
Moreover, we establish a new coronary angiography image dataset ,which contains
95,039 triplets images to retrain the video interpolation network model. Using
the retrained network we synthesize high frame rate coronary angiography video
from the low frame rate coronary angiography video. The average peak signal to
noise ratio(PSNR) of those synthesized video frames reaches 34dB. Extensive
experiment results demonstrate the feasibility of using the video frame
interpolation algorithm to synthesize continuous and clear high frame rate
coronary angiography video. With the help of this technology, doctors can
significantly reduce exposure frequency and intensity of the X-ray during
coronary angiography.",arxiv
http://arxiv.org/abs/2104.03866v1,2021-04-08T16:15:46Z,2021-04-08T16:15:46Z,SMD-Nets: Stereo Mixture Density Networks,"Despite stereo matching accuracy has greatly improved by deep learning in the
last few years, recovering sharp boundaries and high-resolution outputs
efficiently remains challenging. In this paper, we propose Stereo Mixture
Density Networks (SMD-Nets), a simple yet effective learning framework
compatible with a wide class of 2D and 3D architectures which ameliorates both
issues. Specifically, we exploit bimodal mixture densities as output
representation and show that this allows for sharp and precise disparity
estimates near discontinuities while explicitly modeling the aleatoric
uncertainty inherent in the observations. Moreover, we formulate disparity
estimation as a continuous problem in the image domain, allowing our model to
query disparities at arbitrary spatial precision. We carry out comprehensive
experiments on a new high-resolution and highly realistic synthetic stereo
dataset, consisting of stereo pairs at 8Mpx resolution, as well as on
real-world stereo datasets. Our experiments demonstrate increased depth
accuracy near object boundaries and prediction of ultra high-resolution
disparity maps on standard GPUs. We demonstrate the flexibility of our
technique by improving the performance of a variety of stereo backbones.",arxiv
http://arxiv.org/abs/1811.07503v1,2018-11-19T05:10:14Z,2018-11-19T05:10:14Z,"Compressing Recurrent Neural Networks with Tensor Ring for Action
  Recognition","Recurrent Neural Networks (RNNs) and their variants, such as Long-Short Term
Memory (LSTM) networks, and Gated Recurrent Unit (GRU) networks, have achieved
promising performance in sequential data modeling. The hidden layers in RNNs
can be regarded as the memory units, which are helpful in storing information
in sequential contexts. However, when dealing with high dimensional input data,
such as video and text, the input-to-hidden linear transformation in RNNs
brings high memory usage and huge computational cost. This makes the training
of RNNs unscalable and difficult. To address this challenge, we propose a novel
compact LSTM model, named as TR-LSTM, by utilizing the low-rank tensor ring
decomposition (TRD) to reformulate the input-to-hidden transformation. Compared
with other tensor decomposition methods, TR-LSTM is more stable. In addition,
TR-LSTM can complete an end-to-end training and also provide a fundamental
building block for RNNs in handling large input data. Experiments on real-world
action recognition datasets have demonstrated the promising performance of the
proposed TR-LSTM compared with the tensor train LSTM and other state-of-the-art
competitors.",arxiv
http://arxiv.org/abs/1905.06650v1,2019-05-16T10:45:06Z,2019-05-16T10:45:06Z,Reactive Video Caching via long-short-term fusion approach,"Video caching has been a basic network functionality in today's network
architectures. Although the abundance of caching replacement algorithms has
been proposed recently, these methods all suffer from a key limitation: due to
their immature rules, inaccurate feature engineering or unresponsive model
update, they cannot strike a balance between the long-term history and
short-term sudden events. To address this concern, we propose LA-E2, a
long-short-term fusion caching replacement approach, which is based on a
learning-aided exploration-exploitation process. Specifically, by effectively
combining the deep neural network (DNN) based prediction with the online
exploitation-exploration process through a \emph{top-k} method, LA-E2 can both
make use of the historical information and adapt to the constantly changing
popularity responsively. Through the extensive experiments in two real-world
datasets, we show that LA-E2 can achieve state-of-the-art performance and
generalize well. Especially when the cache size is small, our approach can
outperform the baselines by 17.5\%-68.7\% higher in total hit rate.",arxiv
http://arxiv.org/abs/2004.05794v1,2020-04-13T07:01:06Z,2020-04-13T07:01:06Z,Learning Event-Based Motion Deblurring,"Recovering sharp video sequence from a motion-blurred image is highly
ill-posed due to the significant loss of motion information in the blurring
process. For event-based cameras, however, fast motion can be captured as
events at high time rate, raising new opportunities to exploring effective
solutions. In this paper, we start from a sequential formulation of event-based
motion deblurring, then show how its optimization can be unfolded with a novel
end-to-end deep architecture. The proposed architecture is a convolutional
recurrent neural network that integrates visual and temporal knowledge of both
global and local scales in principled manner. To further improve the
reconstruction, we propose a differentiable directional event filtering module
to effectively extract rich boundary prior from the stream of events. We
conduct extensive experiments on the synthetic GoPro dataset and a large newly
introduced dataset captured by a DAVIS240C camera. The proposed approach
achieves state-of-the-art reconstruction quality, and generalizes better to
handling real-world motion blur.",arxiv
http://arxiv.org/abs/2006.02682v2,2021-06-18T07:42:07Z,2020-06-04T07:55:41Z,Some Theoretical Insights into Wasserstein GANs,"Generative Adversarial Networks (GANs) have been successful in producing
outstanding results in areas as diverse as image, video, and text generation.
Building on these successes, a large number of empirical studies have validated
the benefits of the cousin approach called Wasserstein GANs (WGANs), which
brings stabilization in the training process. In the present paper, we add a
new stone to the edifice by proposing some theoretical advances in the
properties of WGANs. First, we properly define the architecture of WGANs in the
context of integral probability metrics parameterized by neural networks and
highlight some of their basic mathematical features. We stress in particular
interesting optimization properties arising from the use of a parametric
1-Lipschitz discriminator. Then, in a statistically-driven approach, we study
the convergence of empirical WGANs as the sample size tends to infinity, and
clarify the adversarial effects of the generator and the discriminator by
underlining some trade-off properties. These features are finally illustrated
with experiments using both synthetic and real-world datasets.",arxiv
http://arxiv.org/abs/2009.14757v1,2020-09-30T15:45:36Z,2020-09-30T15:45:36Z,Attention-Aware Noisy Label Learning for Image Classification,"Deep convolutional neural networks (CNNs) learned on large-scale labeled
samples have achieved remarkable progress in computer vision, such as
image/video classification. The cheapest way to obtain a large body of labeled
visual data is to crawl from websites with user-supplied labels, such as
Flickr. However, these samples often tend to contain incorrect labels (i.e.
noisy labels), which will significantly degrade the network performance. In
this paper, the attention-aware noisy label learning approach ($A^2NL$) is
proposed to improve the discriminative capability of the network trained on
datasets with potential label noise. Specifically, a Noise-Attention model,
which contains multiple noise-specific units, is designed to better capture
noisy information. Each unit is expected to learn a specific noisy distribution
for a subset of images so that different disturbances are more precisely
modeled. Furthermore, a recursive learning process is introduced to strengthen
the learning ability of the attention network by taking advantage of the
learned high-level knowledge. To fully evaluate the proposed method, we conduct
experiments from two aspects: manually flipped label noise on large-scale image
classification datasets, including CIFAR-10, SVHN; and real-world label noise
on an online crawled clothing dataset with multiple attributes. The superior
results over state-of-the-art methods validate the effectiveness of our
proposed approach.",arxiv
http://arxiv.org/abs/2106.07217v1,2021-06-14T08:04:18Z,2021-06-14T08:04:18Z,Over-Fit: Noisy-Label Detection based on the Overfitted Model Property,"Due to the increasing need to handle the noisy label problem in a massive
dataset, learning with noisy labels has received much attention in recent
years. As a promising approach, there have been recent studies to select clean
training data by finding small-loss instances before a deep neural network
overfits the noisy-label data. However, it is challenging to prevent
overfitting. In this paper, we propose a novel noisy-label detection algorithm
by employing the property of overfitting on individual data points. To this
end, we present two novel criteria that statistically measure how much each
training sample abnormally affects the model and clean validation data. Using
the criteria, our iterative algorithm removes noisy-label samples and retrains
the model alternately until no further performance improvement is made. In
experiments on multiple benchmark datasets, we demonstrate the validity of our
algorithm and show that our algorithm outperforms the state-of-the-art methods
when the exact noise rates are not given. Furthermore, we show that our method
can not only be expanded to a real-world video dataset but also can be viewed
as a regularization method to solve problems caused by overfitting.",arxiv
http://arxiv.org/abs/2109.03393v2,2021-09-09T00:59:19Z,2021-09-08T01:51:51Z,"Learning to Discriminate Information for Online Action Detection:
  Analysis and Application","Online action detection, which aims to identify an ongoing action from a
streaming video, is an important subject in real-world applications. For this
task, previous methods use recurrent neural networks for modeling temporal
relations in an input sequence. However, these methods overlook the fact that
the input image sequence includes not only the action of interest but
background and irrelevant actions. This would induce recurrent units to
accumulate unnecessary information for encoding features on the action of
interest. To overcome this problem, we propose a novel recurrent unit, named
Information Discrimination Unit (IDU), which explicitly discriminates the
information relevancy between an ongoing action and others to decide whether to
accumulate the input information. This enables learning more discriminative
representations for identifying an ongoing action. In this paper, we further
present a new recurrent unit, called Information Integration Unit (IIU), for
action anticipation. Our IIU exploits the outputs from IDU as pseudo action
labels as well as RGB frames to learn enriched features of observed actions
effectively. In experiments on TVSeries and THUMOS-14, the proposed methods
outperform state-of-the-art methods by a significant margin in online action
detection and action anticipation. Moreover, we demonstrate the effectiveness
of the proposed units by conducting comprehensive ablation studies.",arxiv
http://arxiv.org/abs/2111.01203v2,2021-11-03T02:11:09Z,2021-11-01T18:56:42Z,One Proxy Device Is Enough for Hardware-Aware Neural Architecture Search,"Convolutional neural networks (CNNs) are used in numerous real-world
applications such as vision-based autonomous driving and video content
analysis. To run CNN inference on various target devices, hardware-aware neural
architecture search (NAS) is crucial. A key requirement of efficient
hardware-aware NAS is the fast evaluation of inference latencies in order to
rank different architectures. While building a latency predictor for each
target device has been commonly used in state of the art, this is a very
time-consuming process, lacking scalability in the presence of extremely
diverse devices. In this work, we address the scalability challenge by
exploiting latency monotonicity -- the architecture latency rankings on
different devices are often correlated. When strong latency monotonicity
exists, we can re-use architectures searched for one proxy device on new target
devices, without losing optimality. In the absence of strong latency
monotonicity, we propose an efficient proxy adaptation technique to
significantly boost the latency monotonicity. Finally, we validate our approach
and conduct experiments with devices of different platforms on multiple
mainstream search spaces, including MobileNet-V2, MobileNet-V3, NAS-Bench-201,
ProxylessNAS and FBNet. Our results highlight that, by using just one proxy
device, we can find almost the same Pareto-optimal architectures as the
existing per-device NAS, while avoiding the prohibitive cost of building a
latency predictor for each device. GitHub:
https://github.com/Ren-Research/OneProxy",arxiv
http://arxiv.org/abs/1910.05318v2,2019-12-13T22:42:52Z,2019-10-11T17:24:45Z,Aff-Wild Database and AffWildNet,"In the context of HCI, building an automatic system to recognize affect of
human facial expression in real-world condition is very crucial to make machine
interact naturallisticaly with a man. However, existing facial emotion
databases usually contain expression in the limited scenario under
well-controlled condition. Aff-Wild is currently the largest database
consisting of spontaneous facial expression in the wild annotated with valence
and arousal. The first contribution of this project is the completion of
extending Aff-Wild database which is fulfilled by collecting videos from
YouTube on which the videos have spontaneous facial expressions in the wild,
annotating videos with valence and arousal ranging in [-1,1], detecting faces
in frames using FFLD2 detector and partitioning the whole data set into train,
validate and test set, with 527056, 94223 and 135145 frames. The diversity is
guaranteed regarding age, ethnicity and values of valence and arousal. The
ratio of male to female is close to 1. Regarding the techniques used to build
the automatic system, deep learning is outstanding since almost all winning
methods in emotion challenges adopt DNN techniques. The second contribution of
this project is that an end-to-end DNN is constructed to have joint CNN and RNN
block and gives the estimation on valence and arousal for each frame in
sequential data. VGGFace, ResNet, DenseNet with the corresponding pre-trained
model for CNN block and LSTM, GRU, IndRNN, Attention mechanism for RNN block
are experimented aiming to find the best combination. Fine tuning and transfer
learning techniques are also tried out. By comparing the CCC evaluation value
on test data, the best model is found to be pre-trained VGGFace connected with
2 layers GRU with attention mechanism. The models test performance is 0.555 CCC
for valence with sequence length 80 and 0.499 CCC for arousal with sequence
length 70.",arxiv
http://arxiv.org/abs/2101.06409v1,2021-01-16T09:00:34Z,2021-01-16T09:00:34Z,Shape Back-Projection In 3D Scenes,"In this work, we propose a novel framework shape back-projection for
computationally efficient point cloud processing in a probabilistic manner. The
primary component of the technique is shape histogram and a back-projection
procedure. The technique measures similarity between 3D surfaces, by analyzing
their geometrical properties. It is analogous to color back-projection which
measures similarity between images, simply by looking at their color
distributions. In the overall process, first, shape histogram of a sample
surface (e.g. planar) is computed, which captures the profile of surface
normals around a point in form of a probability distribution. Later, the
histogram is back-projected onto a test surface and a likelihood score is
obtained. The score depicts that how likely a point in the test surface behaves
similar to the sample surface, geometrically. Shape back-projection finds its
application in binary surface classification, high curvature edge detection in
unorganized point cloud, automated point cloud labeling for 3D-CNNs
(convolutional neural network) etc. The algorithm can also be used for
real-time robotic operations such as autonomous object picking in warehouse
automation, ground plane extraction for autonomous vehicles and can be deployed
easily on computationally limited platforms (UAVs).",arxiv
http://arxiv.org/abs/2104.04170v2,2021-08-16T12:54:57Z,2021-04-09T02:58:59Z,Stereo Matching by Self-supervision of Multiscopic Vision,"Self-supervised learning for depth estimation possesses several advantages
over supervised learning. The benefits of no need for ground-truth depth,
online fine-tuning, and better generalization with unlimited data attract
researchers to seek self-supervised solutions. In this work, we propose a new
self-supervised framework for stereo matching utilizing multiple images
captured at aligned camera positions. A cross photometric loss, an
uncertainty-aware mutual-supervision loss, and a new smoothness loss are
introduced to optimize the network in learning disparity maps end-to-end
without ground-truth depth information. To train this framework, we build a new
multiscopic dataset consisting of synthetic images rendered by 3D engines and
real images captured by real cameras. After being trained with only the
synthetic images, our network can perform well in unseen outdoor scenes. Our
experiment shows that our model obtains better disparity maps than previous
unsupervised methods on the KITTI dataset and is comparable to supervised
methods when generalized to unseen data. Our source code and dataset are
available at https://sites.google.com/view/multiscopic.",arxiv
http://arxiv.org/abs/2005.13131v1,2020-05-27T02:17:54Z,2020-05-27T02:17:54Z,"Efficient Pig Counting in Crowds with Keypoints Tracking and
  Spatial-aware Temporal Response Filtering","Pig counting is a crucial task for large-scale pig farming, which is usually
completed by human visually. But this process is very time-consuming and
error-prone. Few studies in literature developed automated pig counting method.
Existing methods only focused on pig counting using single image, and its
accuracy is challenged by several factors, including pig movements, occlusion
and overlapping. Especially, the field of view of a single image is very
limited, and could not meet the requirements of pig counting for large pig
grouping houses. To that end, we presented a real-time automated pig counting
system in crowds using only one monocular fisheye camera with an inspection
robot. Our system showed that it produces accurate results surpassing human.
Our pipeline began with a novel bottom-up pig detection algorithm to avoid
false negatives due to overlapping, occlusion and deformation of pigs. A deep
convolution neural network (CNN) is designed to detect keypoints of pig body
part and associate the keypoints to identify individual pigs. After that, an
efficient on-line tracking method is used to associate pigs across video
frames. Finally, a novel spatial-aware temporal response filtering (STRF)
method is proposed to predict the counts of pigs, which is effective to
suppress false positives caused by pig or camera movements or tracking
failures. The whole pipeline has been deployed in an edge computing device, and
demonstrated the effectiveness.",arxiv
http://arxiv.org/abs/1502.04221v1,2015-02-14T16:14:05Z,2015-02-14T16:14:05Z,"A Row-parallel 8$\times$8 2-D DCT Architecture Using Algebraic Integer
  Based Exact Computation","An algebraic integer (AI) based time-multiplexed row-parallel architecture
and two final-reconstruction step (FRS) algorithms are proposed for the
implementation of bivariate AI-encoded 2-D discrete cosine transform (DCT). The
architecture directly realizes an error-free 2-D DCT without using FRSs between
row-column transforms, leading to an 8$\times$8 2-D DCT which is entirely free
of quantization errors in AI basis. As a result, the user-selectable accuracy
for each of the coefficients in the FRS facilitates each of the 64 coefficients
to have its precision set independently of others, avoiding the leakage of
quantization noise between channels as is the case for published DCT designs.
The proposed FRS uses two approaches based on (i) optimized Dempster-Macleod
multipliers and (ii) expansion factor scaling. This architecture enables
low-noise high-dynamic range applications in digital video processing that
requires full control of the finite-precision computation of the 2-D DCT. The
proposed architectures and FRS techniques are experimentally verified and
validated using hardware implementations that are physically realized and
verified on FPGA chip. Six designs, for 4- and 8-bit input word sizes, using
the two proposed FRS schemes, have been designed, simulated, physically
implemented and measured. The maximum clock rate and block-rate achieved among
8-bit input designs are 307.787 MHz and 38.47 MHz, respectively, implying a
pixel rate of 8$\times$307.787$\approx$2.462 GHz if eventually embedded in a
real-time video-processing system. The equivalent frame rate is about 1187.35
Hz for the image size of 1920$\times$1080. All implementations are functional
on a Xilinx Virtex-6 XC6VLX240T FPGA device.",arxiv
http://arxiv.org/abs/1711.02757v1,2017-11-07T22:42:09Z,2017-11-07T22:42:09Z,Real-Time Road Segmentation Using LiDAR Data Processing on an FPGA,"This paper presents the FPGA design of a convolutional neural network (CNN)
based road segmentation algorithm for real-time processing of LiDAR data. For
autonomous vehicles, it is important to perform road segmentation and obstacle
detection such that the drivable region can be identified for path planning.
Traditional road segmentation algorithms are mainly based on image data from
cameras, which is subjected to the light condition as well as the quality of
road markings. LiDAR sensor can obtain the 3D geometry information of the
vehicle surroundings with very high accuracy. However, it is a computational
challenge to process a large amount of LiDAR data at real-time. In this work, a
convolutional neural network model is proposed and trained to perform semantic
segmentation using the LiDAR sensor data. Furthermore, an efficient hardware
design is implemented on the FPGA that can process each LiDAR scan in 16.9ms,
which is much faster than the previous works. Evaluated using KITTI road
benchmarks, the proposed solution achieves high accuracy of road segmentation.",arxiv
http://arxiv.org/abs/2007.00491v1,2020-07-01T13:49:56Z,2020-07-01T13:49:56Z,"Optimisation of a Siamese Neural Network for Real-Time Energy Efficient
  Object Tracking","In this paper the research on optimisation of visual object tracking using a
Siamese neural network for embedded vision systems is presented. It was assumed
that the solution shall operate in real-time, preferably for a high resolution
video stream, with the lowest possible energy consumption. To meet these
requirements, techniques such as the reduction of computational precision and
pruning were considered. Brevitas, a tool dedicated for optimisation and
quantisation of neural networks for FPGA implementation, was used. A number of
training scenarios were tested with varying levels of optimisations - from
integer uniform quantisation with 16 bits to ternary and binary networks. Next,
the influence of these optimisations on the tracking performance was evaluated.
It was possible to reduce the size of the convolutional filters up to 10 times
in relation to the original network. The obtained results indicate that using
quantisation can significantly reduce the memory and computational complexity
of the proposed network while still enabling precise tracking, thus allow to
use it in embedded vision systems. Moreover, quantisation of weights positively
affects the network training by decreasing overfitting.",arxiv
http://arxiv.org/abs/1910.04433v1,2019-10-10T08:40:15Z,2019-10-10T08:40:15Z,"Hierarchical Representation Network for Steganalysis of QIM
  Steganography in Low-Bit-Rate Speech Signals","With the Volume of Voice over IP (VoIP) traffic rises shapely, more and more
VoIP-based steganography methods have emerged in recent years, which poses a
great threat to the security of cyberspace. Low bit-rate speech codecs are
widely used in the VoIP application due to its powerful compression capability.
QIM steganography makes it possible to hide secret information in VoIP streams.
Previous research mostly focus on capturing the inter-frame correlation or
inner-frame correlation features in code-words but ignore the hierarchical
structure which exists in speech frame. In this paper, motivated by the complex
multi-scale structure, we design a Hierarchical Representation Network to
tackle the steganalysis of QIM steganography in low-bit-rate speech signal. In
the proposed model, Convolution Neural Network (CNN) is used to model the
hierarchical structure in the speech frame, and three level of attention
mechanisms are applied at different convolution block, enabling it to attend
differentially to more and less important content in speech frame. Experiments
demonstrated that the steganalysis performance of the proposed method can
outperforms the state-of-the-art methods especially in detecting both short and
low embeded speech samples. Moreover, our model needs less computation and has
higher time efficiency to be applied to real online services.",arxiv
http://arxiv.org/abs/2101.04240v2,2021-01-15T22:46:36Z,2021-01-11T23:58:56Z,"Lesion2Vec: Deep Metric Learning for Few-Shot Multiple Lesions
  Recognition in Wireless Capsule Endoscopy Video","Effective and rapid detection of lesions in the Gastrointestinal tract is
critical to gastroenterologist's response to some life-threatening diseases.
Wireless Capsule Endoscopy (WCE) has revolutionized traditional endoscopy
procedure by allowing gastroenterologists visualize the entire GI tract
non-invasively. Once the tiny capsule is swallowed, it sequentially capture
images of the GI tract at about 2 to 6 frames per second (fps). A single video
can last up to 8 hours producing between 30,000 to 100,000 images. Automating
the detection of frames containing specific lesion in WCE video would relieve
gastroenterologists the arduous task of reviewing the entire video before
making diagnosis. While the WCE produces large volume of images, only about 5\%
of the frames contain lesions that aid the diagnosis process. Convolutional
Neural Network (CNN) based models have been very successful in various image
classification tasks. However, they suffer excessive parameters, are sample
inefficient and rely on very large amount of training data. Deploying a CNN
classifier for lesion detection task will require time-to-time fine-tuning to
generalize to any unforeseen category. In this paper, we propose a metric-based
learning framework followed by a few-shot lesion recognition in WCE data.
Metric-based learning is a meta-learning framework designed to establish
similarity or dissimilarity between concepts while few-shot learning (FSL) aims
to identify new concepts from only a small number of examples. We train a
feature extractor to learn a representation for different small bowel lesions
using metric-based learning. At the testing stage, the category of an unseen
sample is predicted from only a few support examples, thereby allowing the
model to generalize to a new category that has never been seen before. We
demonstrated the efficacy of this method on real patient capsule endoscopy
data.",arxiv
http://arxiv.org/abs/2107.02174v2,2021-09-10T03:04:13Z,2021-07-05T17:59:35Z,What Makes for Hierarchical Vision Transformer?,"Recent studies indicate that hierarchical Vision Transformer with a macro
architecture of interleaved non-overlapped window-based self-attention \&
shifted-window operation is able to achieve state-of-the-art performance in
various visual recognition tasks, and challenges the ubiquitous convolutional
neural networks (CNNs) using densely slid kernels. Most follow-up works attempt
to replace the shifted-window operation with other kinds of cross-window
communication paradigms, while treating self-attention as the de-facto standard
for window-based information aggregation. In this manuscript, we question
whether self-attention is the only choice for hierarchical Vision Transformer
to attain strong performance, and the effects of different kinds of
cross-window communication. To this end, we replace self-attention layers with
embarrassingly simple linear mapping layers, and the resulting proof-of-concept
architecture termed as LinMapper can achieve very strong performance in
ImageNet-1k image recognition. Moreover, we find that LinMapper is able to
better leverage the pre-trained representations from image recognition and
demonstrates excellent transfer learning properties on downstream dense
prediction tasks such as object detection and instance segmentation. We also
experiment with other alternatives to self-attention for content aggregation
inside each non-overlapped window under different cross-window communication
approaches, which all give similar competitive results. Our study reveals that
the \textbf{macro architecture} of Swin model families, other than specific
aggregation layers or specific means of cross-window communication, may be more
responsible for its strong performance and is the real challenger to the
ubiquitous CNN's dense sliding window paradigm. Code and models will be
publicly available to facilitate future research.",arxiv
http://arxiv.org/abs/2106.16118v1,2021-06-30T15:18:14Z,2021-06-30T15:18:14Z,"SimNet: Enabling Robust Unknown Object Manipulation from Pure Synthetic
  Data via Stereo","Robot manipulation of unknown objects in unstructured environments is a
challenging problem due to the variety of shapes, materials, arrangements and
lighting conditions. Even with large-scale real-world data collection, robust
perception and manipulation of transparent and reflective objects across
various lighting conditions remain challenging. To address these challenges we
propose an approach to performing sim-to-real transfer of robotic perception.
The underlying model, SimNet, is trained as a single multi-headed neural
network using simulated stereo data as input and simulated object segmentation
masks, 3D oriented bounding boxes (OBBs), object keypoints, and disparity as
output. A key component of SimNet is the incorporation of a learned stereo
sub-network that predicts disparity. SimNet is evaluated on 2D car detection,
unknown object detection, and deformable object keypoint detection and
significantly outperforms a baseline that uses a structured light RGB-D sensor.
By inferring grasp positions using the OBB and keypoint predictions, SimNet can
be used to perform end-to-end manipulation of unknown objects in both easy and
hard scenarios using our fleet of Toyota HSR robots in four home environments.
In unknown object grasping experiments, the predictions from the baseline RGB-D
network and SimNet enable successful grasps of most of the easy objects.
However, the RGB-D baseline only grasps 35% of the hard (e.g., transparent)
objects, while SimNet grasps 95%, suggesting that SimNet can enable robust
manipulation of unknown objects, including transparent objects, in unknown
environments.",arxiv
http://arxiv.org/abs/2004.00060v1,2020-03-31T19:01:42Z,2020-03-31T19:01:42Z,HOPE-Net: A Graph-based Model for Hand-Object Pose Estimation,"Hand-object pose estimation (HOPE) aims to jointly detect the poses of both a
hand and of a held object. In this paper, we propose a lightweight model called
HOPE-Net which jointly estimates hand and object pose in 2D and 3D in
real-time. Our network uses a cascade of two adaptive graph convolutional
neural networks, one to estimate 2D coordinates of the hand joints and object
corners, followed by another to convert 2D coordinates to 3D. Our experiments
show that through end-to-end training of the full network, we achieve better
accuracy for both the 2D and 3D coordinate estimation problems. The proposed 2D
to 3D graph convolution-based model could be applied to other 3D landmark
detection problems, where it is possible to first predict the 2D keypoints and
then transform them to 3D.",arxiv
http://arxiv.org/abs/1712.02294v4,2018-07-12T14:11:40Z,2017-12-06T17:20:21Z,Joint 3D Proposal Generation and Object Detection from View Aggregation,"We present AVOD, an Aggregate View Object Detection network for autonomous
driving scenarios. The proposed neural network architecture uses LIDAR point
clouds and RGB images to generate features that are shared by two subnetworks:
a region proposal network (RPN) and a second stage detector network. The
proposed RPN uses a novel architecture capable of performing multimodal feature
fusion on high resolution feature maps to generate reliable 3D object proposals
for multiple object classes in road scenes. Using these proposals, the second
stage detection network performs accurate oriented 3D bounding box regression
and category classification to predict the extents, orientation, and
classification of objects in 3D space. Our proposed architecture is shown to
produce state of the art results on the KITTI 3D object detection benchmark
while running in real time with a low memory footprint, making it a suitable
candidate for deployment on autonomous vehicles. Code is at:
https://github.com/kujason/avod",arxiv
http://arxiv.org/abs/2107.05307v2,2021-07-14T14:42:34Z,2021-07-12T10:35:05Z,Real-Time Super-Resolution System of 4K-Video Based on Deep Learning,"Video super-resolution (VSR) technology excels in reconstructing low-quality
video, avoiding unpleasant blur effect caused by interpolation-based
algorithms. However, vast computation complexity and memory occupation hampers
the edge of deplorability and the runtime inference in real-life applications,
especially for large-scale VSR task. This paper explores the possibility of
real-time VSR system and designs an efficient and generic VSR network, termed
EGVSR. The proposed EGVSR is based on spatio-temporal adversarial learning for
temporal coherence. In order to pursue faster VSR processing ability up to 4K
resolution, this paper tries to choose lightweight network structure and
efficient upsampling method to reduce the computation required by EGVSR network
under the guarantee of high visual quality. Besides, we implement the batch
normalization computation fusion, convolutional acceleration algorithm and
other neural network acceleration techniques on the actual hardware platform to
optimize the inference process of EGVSR network. Finally, our EGVSR achieves
the real-time processing capacity of 4K@29.61FPS. Compared with TecoGAN, the
most advanced VSR network at present, we achieve 85.04% reduction of
computation density and 7.92x performance speedups. In terms of visual quality,
the proposed EGVSR tops the list of most metrics (such as LPIPS, tOF, tLP,
etc.) on the public test dataset Vid4 and surpasses other state-of-the-art
methods in overall performance score. The source code of this project can be
found on https://github.com/Thmen/EGVSR.",arxiv
http://arxiv.org/abs/1811.08565v2,2019-06-23T00:26:34Z,2018-11-19T21:17:21Z,"Can Synthetic Faces Undo the Damage of Dataset Bias to Face Recognition
  and Facial Landmark Detection?","It is well known that deep learning approaches to face recognition and facial
landmark detection suffer from biases in modern training datasets. In this
work, we propose to use synthetic face images to reduce the negative effects of
dataset biases on these tasks. Using a 3D morphable face model, we generate
large amounts of synthetic face images with full control over facial shape and
color, pose, illumination, and background. With a series of experiments, we
extensively test the effects of priming deep nets by pre-training them with
synthetic faces. We observe the following positive effects for face recognition
and facial landmark detection tasks: 1) Priming with synthetic face images
improves the performance consistently across all benchmarks because it reduces
the negative effects of biases in the training data. 2) Traditional approaches
for reducing the damage of dataset bias, such as data augmentation and transfer
learning, are less effective than training with synthetic faces. 3) Using
synthetic data, we can reduce the size of real-world datasets by 75% for face
recognition and by 50% for facial landmark detection while maintaining
performance. Thus, offering a means to focus the data collection process on
less but higher quality data.",arxiv
http://arxiv.org/abs/1908.07906v2,2019-11-04T04:52:47Z,2019-08-21T15:10:24Z,PCRNet: Point Cloud Registration Network using PointNet Encoding,"PointNet has recently emerged as a popular representation for unstructured
point cloud data, allowing application of deep learning to tasks such as object
detection, segmentation and shape completion. However, recent works in
literature have shown the sensitivity of the PointNet representation to pose
misalignment. This paper presents a novel framework that uses the PointNet
representation to align point clouds and perform registration for applications
such as tracking, 3D reconstruction and pose estimation. We develop a framework
that compares PointNet features of template and source point clouds to find the
transformation that aligns them accurately. Depending on the prior information
about the shape of the object formed by the point clouds, our framework can
produce approaches that are shape specific or general to unseen shapes. The
shape specific approach uses a Siamese architecture with fully connected (FC)
layers and is robust to noise and initial misalignment in data. We perform
extensive simulation and real-world experiments to validate the efficacy of our
approach and compare the performance with state-of-art approaches.",arxiv
http://arxiv.org/abs/1912.05766v1,2019-12-12T04:16:47Z,2019-12-12T04:16:47Z,"One Framework to Register Them All: PointNet Encoding for Point Cloud
  Alignment","PointNet has recently emerged as a popular representation for unstructured
point cloud data, allowing application of deep learning to tasks such as object
detection, segmentation and shape completion. However, recent works in
literature have shown the sensitivity of the PointNet representation to pose
misalignment. This paper presents a novel framework that uses PointNet encoding
to align point clouds and perform registration for applications such as 3D
reconstruction, tracking and pose estimation. We develop a framework that
compares PointNet features of template and source point clouds to find the
transformation that aligns them accurately. In doing so, we avoid
computationally expensive correspondence finding steps, that are central to
popular registration methods such as ICP and its variants. Depending on the
prior information about the shape of the object formed by the point clouds, our
framework can produce approaches that are shape specific or general to unseen
shapes. Our framework produces approaches that are robust to noise and initial
misalignment in data and work robustly with sparse as well as partial point
clouds. We perform extensive simulation and real-world experiments to validate
the efficacy of our approach and compare the performance with state-of-art
approaches. Code is available at
https://github.com/vinits5/pointnet-registrationframework.",arxiv
http://arxiv.org/abs/2104.10674v1,2021-04-21T17:56:23Z,2021-04-21T17:56:23Z,"Hierarchical Cross-Modal Agent for Robotics Vision-and-Language
  Navigation","Deep Learning has revolutionized our ability to solve complex problems such
as Vision-and-Language Navigation (VLN). This task requires the agent to
navigate to a goal purely based on visual sensory inputs given natural language
instructions. However, prior works formulate the problem as a navigation graph
with a discrete action space. In this work, we lift the agent off the
navigation graph and propose a more complex VLN setting in continuous 3D
reconstructed environments. Our proposed setting, Robo-VLN, more closely mimics
the challenges of real world navigation. Robo-VLN tasks have longer trajectory
lengths, continuous action spaces, and challenges such as obstacles. We provide
a suite of baselines inspired by state-of-the-art works in discrete VLN and
show that they are less effective at this task. We further propose that
decomposing the task into specialized high- and low-level policies can more
effectively tackle this task. With extensive experiments, we show that by using
layered decision making, modularized training, and decoupling reasoning and
imitation, our proposed Hierarchical Cross-Modal (HCM) agent outperforms
existing baselines in all key metrics and sets a new benchmark for Robo-VLN.",arxiv
http://arxiv.org/abs/2105.08468v2,2021-05-24T06:31:00Z,2021-05-18T12:20:00Z,"Progressively Normalized Self-Attention Network for Video Polyp
  Segmentation","Existing video polyp segmentation (VPS) models typically employ convolutional
neural networks (CNNs) to extract features. However, due to their limited
receptive fields, CNNs can not fully exploit the global temporal and spatial
information in successive video frames, resulting in false-positive
segmentation results. In this paper, we propose the novel PNS-Net
(Progressively Normalized Self-attention Network), which can efficiently learn
representations from polyp videos with real-time speed (~140fps) on a single
RTX 2080 GPU and no post-processing. Our PNS-Net is based solely on a basic
normalized self-attention block, equipping with recurrence and CNNs entirely.
Experiments on challenging VPS datasets demonstrate that the proposed PNS-Net
achieves state-of-the-art performance. We also conduct extensive experiments to
study the effectiveness of the channel split, soft-attention, and progressive
learning strategy. We find that our PNS-Net works well under different
settings, making it a promising solution to the VPS task.",arxiv
http://arxiv.org/abs/2108.06174v1,2021-08-13T11:39:50Z,2021-08-13T11:39:50Z,"Feature learning for efficient ASR-free keyword spotting in low-resource
  languages","We consider feature learning for efficient keyword spotting that can be
applied in severely under-resourced settings. The objective is to support
humanitarian relief programmes by the United Nations in parts of Africa in
which almost no language resources are available. For rapid development in such
languages, we rely on a small, easily-compiled set of isolated keywords. These
keyword templates are applied to a large corpus of in-domain but untranscribed
speech using dynamic time warping (DTW). The resulting DTW alignment scores are
used to train a convolutional neural network (CNN) which is orders of magnitude
more computationally efficient and suitable for real-time application. We
optimise this neural network keyword spotter by identifying robust acoustic
features in this almost zero-resource setting. First, we incorporate
information from well-resourced but unrelated languages using a multilingual
bottleneck feature (BNF) extractor. Next, we consider features extracted from
an autoencoder (AE) trained on in-domain but untranscribed data. Finally, we
consider correspondence autoencoder (CAE) features which are fine-tuned on the
small set of in-domain labelled data. Experiments in South African English and
Luganda, a low-resource language, show that BNF and CAE features achieve a 5%
relative performance improvement over baseline MFCCs. However, using BNFs as
input to the CAE results in a more than 27% relative improvement over MFCCs in
ROC area-under-the-curve (AUC) and more than twice as many top-10 retrievals.
We show that, using these features, the CNN-DTW keyword spotter performs almost
as well as the DTW keyword spotter while outperforming a baseline CNN trained
only on the keyword templates. The CNN-DTW keyword spotter using BNF-derived
CAE features represents an efficient approach with competitive performance
suited to rapid deployment in a severely under-resourced scenario.",arxiv
http://arxiv.org/abs/1910.06540v1,2019-10-15T05:44:28Z,2019-10-15T05:44:28Z,"Real-time monitoring of driver drowsiness on mobile platforms using 3D
  neural networks","Driver drowsiness increases crash risk, leading to substantial road trauma
each year. Drowsiness detection methods have received considerable attention,
but few studies have investigated the implementation of a detection approach on
a mobile phone. Phone applications reduce the need for specialised hardware and
hence, enable a cost-effective roll-out of the technology across the driving
population. While it has been shown that three-dimensional (3D) operations are
more suitable for spatiotemporal feature learning, current methods for
drowsiness detection commonly use frame-based, multi-step approaches. However,
computationally expensive techniques that achieve superior results on action
recognition benchmarks (e.g. 3D convolutions, optical flow extraction) create
bottlenecks for real-time, safety-critical applications on mobile devices.
Here, we show how depthwise separable 3D convolutions, combined with an early
fusion of spatial and temporal information, can achieve a balance between high
prediction accuracy and real-time inference requirements. In particular,
increased accuracy is achieved when assessment requires motion information, for
example, when sunglasses conceal the eyes. Further, a custom TensorFlow-based
smartphone application shows the true impact of various approaches on inference
times and demonstrates the effectiveness of real-time monitoring based on
out-of-sample data to alert a drowsy driver. Our model is pre-trained on
ImageNet and Kinetics and fine-tuned on a publicly available Driver Drowsiness
Detection dataset. Fine-tuning on large naturalistic driving datasets could
further improve accuracy to obtain robust in-vehicle performance. Overall, our
research is a step towards practical deep learning applications, potentially
preventing micro-sleeps and reducing road trauma.",arxiv
http://arxiv.org/abs/1912.04138v2,2021-08-09T13:11:47Z,2019-12-09T15:48:34Z,"A Weak Supervision Approach to Detecting Visual Anomalies for Automated
  Testing of Graphics Units","We present a deep learning system for testing graphics units by detecting
novel visual corruptions in videos. Unlike previous work in which manual
tagging was required to collect labeled training data, our weak supervision
method is fully automatic and needs no human labelling. This is achieved by
reproducing driver bugs that increase the probability of generating
corruptions, and by making use of ideas and methods from the Multiple Instance
Learning (MIL) setting. In our experiments, we significantly outperform
unsupervised methods such as GAN-based models and discover novel corruptions
undetected by baselines, while adhering to strict requirements on accuracy and
efficiency of our real-time system.",arxiv
http://arxiv.org/abs/2011.04217v2,2021-05-19T19:14:25Z,2020-11-09T06:30:49Z,NeuralSim: Augmenting Differentiable Simulators with Neural Networks,"Differentiable simulators provide an avenue for closing the sim-to-real gap
by enabling the use of efficient, gradient-based optimization algorithms to
find the simulation parameters that best fit the observed sensor readings.
Nonetheless, these analytical models can only predict the dynamical behavior of
systems for which they have been designed. In this work, we study the
augmentation of a novel differentiable rigid-body physics engine via neural
networks that is able to learn nonlinear relationships between dynamic
quantities and can thus learn effects not accounted for in traditional
simulators.Such augmentations require less data to train and generalize better
compared to entirely data-driven models. Through extensive experiments, we
demonstrate the ability of our hybrid simulator to learn complex dynamics
involving frictional contacts from real data, as well as match known models of
viscous friction, and present an approach for automatically discovering useful
augmentations. We show that, besides benefiting dynamics modeling, inserting
neural networks can accelerate model-based control architectures. We observe a
ten-fold speed-up when replacing the QP solver inside a model-predictive gait
controller for quadruped robots with a neural network, allowing us to
significantly improve control delays as we demonstrate in real-hardware
experiments.
  We publish code, additional results and videos from our experiments on our
project webpage at https://sites.google.com/usc.edu/neuralsim.",arxiv
http://arxiv.org/abs/2005.11487v1,2020-05-23T07:36:23Z,2020-05-23T07:36:23Z,Self-Training for Domain Adaptive Scene Text Detection,"Though deep learning based scene text detection has achieved great progress,
well-trained detectors suffer from severe performance degradation for different
domains. In general, a tremendous amount of data is indispensable to train the
detector in the target domain. However, data collection and annotation are
expensive and time-consuming. To address this problem, we propose a
self-training framework to automatically mine hard examples with pseudo-labels
from unannotated videos or images. To reduce the noise of hard examples, a
novel text mining module is implemented based on the fusion of detection and
tracking results. Then, an image-to-video generation method is designed for the
tasks that videos are unavailable and only images can be used. Experimental
results on standard benchmarks, including ICDAR2015, MSRA-TD500, ICDAR2017 MLT,
demonstrate the effectiveness of our self-training method. The simple Mask
R-CNN adapted with self-training and fine-tuned on real data can achieve
comparable or even superior results with the state-of-the-art methods.",arxiv
http://arxiv.org/abs/1712.00133v1,2017-12-01T00:07:47Z,2017-12-01T00:07:47Z,Video retrieval based on deep convolutional neural network,"Recently, with the enormous growth of online videos, fast video retrieval
research has received increasing attention. As an extension of image hashing
techniques, traditional video hashing methods mainly depend on hand-crafted
features and transform the real-valued features into binary hash codes. As
videos provide far more diverse and complex visual information than images,
extracting features from videos is much more challenging than that from images.
Therefore, high-level semantic features to represent videos are needed rather
than low-level hand-crafted methods. In this paper, a deep convolutional neural
network is proposed to extract high-level semantic features and a binary hash
function is then integrated into this framework to achieve an end-to-end
optimization. Particularly, our approach also combines triplet loss function
which preserves the relative similarity and difference of videos and
classification loss function as the optimization objective. Experiments have
been performed on two public datasets and the results demonstrate the
superiority of our proposed method compared with other state-of-the-art video
retrieval methods.",arxiv
http://arxiv.org/abs/1812.01779v1,2018-12-04T00:37:33Z,2018-12-04T00:37:33Z,Voice Disorder Detection Using Long Short Term Memory (LSTM) Model,"Automated detection of voice disorders with computational methods is a recent
research area in the medical domain since it requires a rigorous endoscopy for
the accurate diagnosis. Efficient screening methods are required for the
diagnosis of voice disorders so as to provide timely medical facilities in
minimal resources. Detecting Voice disorder using computational methods is a
challenging problem since audio data is continuous due to which extracting
relevant features and applying machine learning is hard and unreliable. This
paper proposes a Long short term memory model (LSTM) to detect pathological
voice disorders and evaluates its performance in a real 400 testing samples
without any labels. Different feature extraction methods are used to provide
the best set of features before applying LSTM model for classification. The
paper describes the approach and experiments that show promising results with
22% sensitivity, 97% specificity and 56% unweighted average recall.",arxiv
http://arxiv.org/abs/1812.03170v3,2019-09-06T11:41:23Z,2018-12-08T16:53:02Z,Variational Saccading: Efficient Inference for Large Resolution Images,"Image classification with deep neural networks is typically restricted to
images of small dimensionality such as 224 x 244 in Resnet models [24]. This
limitation excludes the 4000 x 3000 dimensional images that are taken by modern
smartphone cameras and smart devices. In this work, we aim to mitigate the
prohibitive inferential and memory costs of operating in such large dimensional
spaces. To sample from the high-resolution original input distribution, we
propose using a smaller proxy distribution to learn the co-ordinates that
correspond to regions of interest in the high-dimensional space. We introduce a
new principled variational lower bound that captures the relationship of the
proxy distribution's posterior and the original image's co-ordinate space in a
way that maximizes the conditional classification likelihood. We empirically
demonstrate on one synthetic benchmark and one real world large resolution DSLR
camera image dataset that our method produces comparable results with ~10x
faster inference and lower memory consumption than a model that utilizes the
entire original input distribution. Finally, we experiment with a more complex
setting using mini-maps from Starcraft II [56] to infer the number of
characters in a complex 3d-rendered scene. Even in such complicated scenes our
model provides strong localization: a feature missing from traditional
classification models.",arxiv
http://arxiv.org/abs/1804.09270v1,2018-04-24T21:51:13Z,2018-04-24T21:51:13Z,Learning 3D Segment Descriptors for Place Recognition,"In the absence of global positioning information, place recognition is a key
capability for enabling localization, mapping and navigation in any
environment. Most place recognition methods rely on images, point clouds, or a
combination of both. In this work we leverage a segment extraction and matching
approach to achieve place recognition in Light Detection and Ranging (LiDAR)
based 3D point cloud maps. One challenge related to this approach is the
recognition of segments despite changes in point of view or occlusion. We
propose using a learning based method in order to reach a higher recall
accuracy then previously proposed methods. Using Convolutional Neural Networks
(CNNs), which are state-of-the-art classifiers, we propose a new approach to
segment recognition based on learned descriptors. In this paper we compare the
effectiveness of three different structures and training methods for CNNs. We
demonstrate through several experiments on real-world data collected in an
urban driving scenario that the proposed learning based methods outperform
hand-crafted descriptors.",arxiv
http://arxiv.org/abs/1908.01238v1,2019-08-03T22:06:34Z,2019-08-03T22:06:34Z,Learning Guided Convolutional Network for Depth Completion,"Dense depth perception is critical for autonomous driving and other robotics
applications. However, modern LiDAR sensors only provide sparse depth
measurement. It is thus necessary to complete the sparse LiDAR data, where a
synchronized guidance RGB image is often used to facilitate this completion.
Many neural networks have been designed for this task. However, they often
na\""{\i}vely fuse the LiDAR data and RGB image information by performing
feature concatenation or element-wise addition. Inspired by the guided image
filtering, we design a novel guided network to predict kernel weights from the
guidance image. These predicted kernels are then applied to extract the depth
image features. In this way, our network generates content-dependent and
spatially-variant kernels for multi-modal feature fusion. Dynamically generated
spatially-variant kernels could lead to prohibitive GPU memory consumption and
computation overhead. We further design a convolution factorization to reduce
computation and memory consumption. The GPU memory reduction makes it possible
for feature fusion to work in multi-stage scheme. We conduct comprehensive
experiments to verify our method on real-world outdoor, indoor and synthetic
datasets. Our method produces strong results. It outperforms state-of-the-art
methods on the NYUv2 dataset and ranks 1st on the KITTI depth completion
benchmark at the time of submission. It also presents strong generalization
capability under different 3D point densities, various lighting and weather
conditions as well as cross-dataset evaluations. The code will be released for
reproduction.",arxiv
http://arxiv.org/abs/2003.10138v1,2020-03-23T08:56:32Z,2020-03-23T08:56:32Z,Depth Edge Guided CNNs for Sparse Depth Upsampling,"Guided sparse depth upsampling aims to upsample an irregularly sampled sparse
depth map when an aligned high-resolution color image is given as guidance.
Many neural networks have been designed for this task. However, they often
ignore the structural difference between the depth and the color image,
resulting in obvious artifacts such as texture copy and depth blur at the
upsampling depth. Inspired by the normalized convolution operation, we propose
a guided convolutional layer to recover dense depth from sparse and irregular
depth image with an depth edge image as guidance. Our novel guided network can
prevent the depth value from crossing the depth edge to facilitate upsampling.
We further design a convolution network based on proposed convolutional layer
to combine the advantages of different algorithms and achieve better
performance. We conduct comprehensive experiments to verify our method on
real-world indoor and synthetic outdoor datasets. Our method produces strong
results. It outperforms state-of-the-art methods on the Virtual KITTI dataset
and the Middlebury dataset. It also presents strong generalization capability
under different 3D point densities, various lighting and weather conditions.",arxiv
http://arxiv.org/abs/2007.13551v2,2020-08-09T09:23:44Z,2020-07-27T13:31:41Z,Differentiable Manifold Reconstruction for Point Cloud Denoising,"3D point clouds are often perturbed by noise due to the inherent limitation
of acquisition equipments, which obstructs downstream tasks such as surface
reconstruction, rendering and so on. Previous works mostly infer the
displacement of noisy points from the underlying surface, which however are not
designated to recover the surface explicitly and may lead to sub-optimal
denoising results. To this end, we propose to learn the underlying manifold of
a noisy point cloud from differentiably subsampled points with trivial noise
perturbation and their embedded neighborhood feature, aiming to capture
intrinsic structures in point clouds. Specifically, we present an
autoencoder-like neural network. The encoder learns both local and non-local
feature representations of each point, and then samples points with low noise
via an adaptive differentiable pooling operation. Afterwards, the decoder
infers the underlying manifold by transforming each sampled point along with
the embedded feature of its neighborhood to a local surface centered around the
point. By resampling on the reconstructed manifold, we obtain a denoised point
cloud. Further, we design an unsupervised training loss, so that our network
can be trained in either an unsupervised or supervised fashion. Experiments
show that our method significantly outperforms state-of-the-art denoising
methods under both synthetic noise and real world noise. The code and data are
available at https://github.com/luost26/DMRDenoise",arxiv
http://arxiv.org/abs/2011.08177v1,2020-11-16T18:59:33Z,2020-11-16T18:59:33Z,"A Long Horizon Planning Framework for Manipulating Rigid Pointcloud
  Objects","We present a framework for solving long-horizon planning problems involving
manipulation of rigid objects that operates directly from a point-cloud
observation, i.e. without prior object models. Our method plans in the space of
object subgoals and frees the planner from reasoning about robot-object
interaction dynamics by relying on a set of generalizable manipulation
primitives. We show that for rigid bodies, this abstraction can be realized
using low-level manipulation skills that maintain sticking contact with the
object and represent subgoals as 3D transformations. To enable generalization
to unseen objects and improve planning performance, we propose a novel way of
representing subgoals for rigid-body manipulation and a graph-attention based
neural network architecture for processing point-cloud inputs. We
experimentally validate these choices using simulated and real-world
experiments on the YuMi robot. Results demonstrate that our method can
successfully manipulate new objects into target configurations requiring
long-term planning. Overall, our framework realizes the best of the worlds of
task-and-motion planning (TAMP) and learning-based approaches. Project website:
https://anthonysimeonov.github.io/rpo-planning-framework/.",arxiv
http://arxiv.org/abs/2011.12490v1,2020-11-25T02:47:16Z,2020-11-25T02:47:16Z,DeRF: Decomposed Radiance Fields,"With the advent of Neural Radiance Fields (NeRF), neural networks can now
render novel views of a 3D scene with quality that fools the human eye. Yet,
generating these images is very computationally intensive, limiting their
applicability in practical scenarios. In this paper, we propose a technique
based on spatial decomposition capable of mitigating this issue. Our key
observation is that there are diminishing returns in employing larger (deeper
and/or wider) networks. Hence, we propose to spatially decompose a scene and
dedicate smaller networks for each decomposed part. When working together,
these networks can render the whole scene. This allows us near-constant
inference time regardless of the number of decomposed parts. Moreover, we show
that a Voronoi spatial decomposition is preferable for this purpose, as it is
provably compatible with the Painter's Algorithm for efficient and GPU-friendly
rendering. Our experiments show that for real-world scenes, our method provides
up to 3x more efficient inference than NeRF (with the same rendering quality),
or an improvement of up to 1.0~dB in PSNR (for the same inference cost).",arxiv
http://arxiv.org/abs/2106.09857v1,2021-06-18T01:03:13Z,2021-06-18T01:03:13Z,Effective Model Sparsification by Scheduled Grow-and-Prune Methods,"Deep neural networks (DNNs) are effective in solving many real-world
problems. Larger DNN models usually exhibit better quality (e.g., accuracy) but
their excessive computation results in long training and inference time. Model
sparsification can reduce the computation and memory cost while maintaining
model quality. Most existing sparsification algorithms unidirectionally remove
weights, while others randomly or greedily explore a small subset of weights in
each layer. The inefficiency of the algorithms reduces the achievable sparsity
level. In addition, many algorithms still require pre-trained dense models and
thus suffer from large memory footprint and long training time. In this paper,
we propose a novel scheduled grow-and-prune (GaP) methodology without
pre-training the dense models. It addresses the shortcomings of the previous
works by repeatedly growing a subset of layers to dense and then pruning back
to sparse after some training. Experiments have shown that such models can
match or beat the quality of highly optimized dense models at 80% sparsity on a
variety of tasks, such as image classification, objective detection, 3D object
part segmentation, and translation. They also outperform other state-of-the-art
(SOTA) pruning methods, including pruning from pre-trained dense models. As an
example, a 90% sparse ResNet-50 obtained via GaP achieves 77.9% top-1 accuracy
on ImageNet, improving the SOTA results by 1.5%.",arxiv
http://arxiv.org/abs/1811.10020v2,2018-12-12T15:20:04Z,2018-11-25T14:20:44Z,Background Subtraction with Real-time Semantic Segmentation,"Accurate and fast foreground object extraction is very important for object
tracking and recognition in video surveillance. Although many background
subtraction (BGS) methods have been proposed in the recent past, it is still
regarded as a tough problem due to the variety of challenging situations that
occur in real-world scenarios. In this paper, we explore this problem from a
new perspective and propose a novel background subtraction framework with
real-time semantic segmentation (RTSS). Our proposed framework consists of two
components, a traditional BGS segmenter $\mathcal{B}$ and a real-time semantic
segmenter $\mathcal{S}$. The BGS segmenter $\mathcal{B}$ aims to construct
background models and segments foreground objects. The real-time semantic
segmenter $\mathcal{S}$ is used to refine the foreground segmentation outputs
as feedbacks for improving the model updating accuracy. $\mathcal{B}$ and
$\mathcal{S}$ work in parallel on two threads. For each input frame $I_t$, the
BGS segmenter $\mathcal{B}$ computes a preliminary foreground/background
(FG/BG) mask $B_t$. At the same time, the real-time semantic segmenter
$\mathcal{S}$ extracts the object-level semantics ${S}_t$. Then, some specific
rules are applied on ${B}_t$ and ${S}_t$ to generate the final detection
${D}_t$. Finally, the refined FG/BG mask ${D}_t$ is fed back to update the
background model. Comprehensive experiments evaluated on the CDnet 2014 dataset
demonstrate that our proposed method achieves state-of-the-art performance
among all unsupervised background subtraction methods while operating at
real-time, and even performs better than some deep learning based supervised
algorithms. In addition, our proposed framework is very flexible and has the
potential for generalization.",arxiv
http://arxiv.org/abs/2004.13217v1,2020-04-28T00:15:26Z,2020-04-28T00:15:26Z,Inferring Temporal Compositions of Actions Using Probabilistic Automata,"This paper presents a framework to recognize temporal compositions of atomic
actions in videos. Specifically, we propose to express temporal compositions of
actions as semantic regular expressions and derive an inference framework using
probabilistic automata to recognize complex actions as satisfying these
expressions on the input video features. Our approach is different from
existing works that either predict long-range complex activities as unordered
sets of atomic actions, or retrieve videos using natural language sentences.
Instead, the proposed approach allows recognizing complex fine-grained
activities using only pretrained action classifiers, without requiring any
additional data, annotations or neural network training. To evaluate the
potential of our approach, we provide experiments on synthetic datasets and
challenging real action recognition datasets, such as MultiTHUMOS and Charades.
We conclude that the proposed approach can extend state-of-the-art primitive
action classifiers to vastly more complex activities without large performance
degradation.",arxiv
http://arxiv.org/abs/1911.07937v2,2019-12-02T16:19:18Z,2019-10-31T09:14:28Z,Inverse Graphics: Unsupervised Learning of 3D Shapes from Single Images,"Using generative models for Inverse Graphics is an active area of research.
However, most works focus on developing models for supervised and
semi-supervised methods. In this paper, we study the problem of unsupervised
learning of 3D geometry from single images. Our approach is to use a generative
model that produces 2-D images as projections of a latent 3D voxel grid, which
we train either as a variational auto-encoder or using adversarial methods. Our
contributions are as follows: First, we show how to recover 3D shape and pose
from general datasets such as MNIST, and MNIST Fashion in good quality. Second,
we compare the shapes learned using adversarial and variational methods.
Adversarial approach gives denser 3D shapes. Third, we explore the idea of
modelling the pose of an object as uniform distribution to recover 3D shape
from a single image. Our experiment with the CelebA dataset
\cite{liu2015faceattributes} proves that we can recover complete 3D shape from
a single image when the object is symmetric along one, or more axis whilst
results obtained using ModelNet40 \cite{wu20153d} show the potential
side-effects, in which the model learns 3D shapes such that it can render the
same image from any viewpoint. Forth, we present a general end-to-end approach
to learning 3D shapes from single images in a completely unsupervised fashion
by modelling the factors of variation such as azimuth as independent latent
variables. Our method makes no assumptions about the dataset, and can work with
synthetic as well as real images (i.e. unsupervised in true sense). We present
our results, by training the model using the $\mu$-VAE objective
\cite{ucar2019bridging} and a dataset combining all images from MNIST, MNIST
Fashion, CelebA and six categories of ModelNet40. The model is able to learn 3D
shapes and the pose in qood quality and leverages information learned across
all datasets.",arxiv
http://arxiv.org/abs/1705.00919v2,2017-12-12T12:57:36Z,2017-05-02T11:31:43Z,"Broadband DOA estimation using Convolutional neural networks trained
  with noise signals","A convolution neural network (CNN) based classification method for broadband
DOA estimation is proposed, where the phase component of the short-time Fourier
transform coefficients of the received microphone signals are directly fed into
the CNN and the features required for DOA estimation are learnt during
training. Since only the phase component of the input is used, the CNN can be
trained with synthesized noise signals, thereby making the preparation of the
training data set easier compared to using speech signals. Through experimental
evaluation, the ability of the proposed noise trained CNN framework to
generalize to speech sources is demonstrated. In addition, the robustness of
the system to noise, small perturbations in microphone positions, as well as
its ability to adapt to different acoustic conditions is investigated using
experiments with simulated and real data.",arxiv
http://arxiv.org/abs/2004.03434v1,2020-04-07T14:29:28Z,2020-04-07T14:29:28Z,Learning to fool the speaker recognition,"Due to the widespread deployment of fingerprint/face/speaker recognition
systems, attacking deep learning based biometric systems has drawn more and
more attention. Previous research mainly studied the attack to the vision-based
system, such as fingerprint and face recognition. While the attack for speaker
recognition has not been investigated yet, although it has been widely used in
our daily life. In this paper, we attempt to fool the state-of-the-art speaker
recognition model and present \textit{speaker recognition attacker}, a
lightweight model to fool the deep speaker recognition model by adding
imperceptible perturbations onto the raw speech waveform. We find that the
speaker recognition system is also vulnerable to the attack, and we achieve a
high success rate on the non-targeted attack. Besides, we also present an
effective method to optimize the speaker recognition attacker to obtain a
trade-off between the attack success rate with the perceptual quality.
Experiments on the TIMIT dataset show that we can achieve a sentence error rate
of $99.2\%$ with an average SNR $57.2\text{dB}$ and PESQ 4.2 with speed rather
faster than real-time.",arxiv
http://arxiv.org/abs/2104.06237v1,2021-04-13T14:31:37Z,2021-04-13T14:31:37Z,"Learning to recover orientations from projections in single-particle
  cryo-EM","A major challenge in single-particle cryo-electron microscopy (cryo-EM) is
that the orientations adopted by the 3D particles prior to imaging are unknown;
yet, this knowledge is essential for high-resolution reconstruction. We present
a method to recover these orientations directly from the acquired set of 2D
projections. Our approach consists of two steps: (i) the estimation of
distances between pairs of projections, and (ii) the recovery of the
orientation of each projection from these distances. In step (i), pairwise
distances are estimated by a Siamese neural network trained on synthetic
cryo-EM projections from resolved bio-structures. In step (ii), orientations
are recovered by minimizing the difference between the distances estimated from
the projections and the distances induced by the recovered orientations. We
evaluated the method on synthetic cryo-EM datasets. Current results demonstrate
that orientations can be accurately recovered from projections that are shifted
and corrupted with a high level of noise. The accuracy of the recovery depends
on the accuracy of the distance estimator. While not yet deployed in a real
experimental setup, the proposed method offers a novel learning-based take on
orientation recovery in SPA. Our code is available at
https://github.com/JelenaBanjac/protein-reconstruction",arxiv
http://arxiv.org/abs/1710.09975v1,2017-10-27T03:32:48Z,2017-10-27T03:32:48Z,"A Single-Channel Architecture for Algebraic Integer Based 8$\times$8 2-D
  DCT Computation","An area efficient row-parallel architecture is proposed for the real-time
implementation of bivariate algebraic integer (AI) encoded 2-D discrete cosine
transform (DCT) for image and video processing. The proposed architecture
computes 8$\times$8 2-D DCT transform based on the Arai DCT algorithm. An
improved fast algorithm for AI based 1-D DCT computation is proposed along with
a single channel 2-D DCT architecture. The design improves on the 4-channel AI
DCT architecture that was published recently by reducing the number of integer
channels to one and the number of 8-point 1-D DCT cores from 5 down to 2. The
architecture offers exact computation of 8$\times$8 blocks of the 2-D DCT
coefficients up to the FRS, which converts the coefficients from the AI
representation to fixed-point format using the method of expansion factors.
Prototype circuits corresponding to FRS blocks based on two expansion factors
are realized, tested, and verified on FPGA-chip, using a Xilinx Virtex-6
XC6VLX240T device. Post place-and-route results show a 20% reduction in terms
of area compared to the 2-D DCT architecture requiring five 1-D AI cores. The
area-time and area-time${}^2$ complexity metrics are also reduced by 23% and
22% respectively for designs with 8-bit input word length. The digital
realizations are simulated up to place and route for ASICs using 45 nm CMOS
standard cells. The maximum estimated clock rate is 951 MHz for the CMOS
realizations indicating 7.608$\cdot$10$^9$ pixels/seconds and a 8$\times$8
block rate of 118.875 MHz.",arxiv
http://arxiv.org/abs/2104.08797v1,2021-04-18T10:07:52Z,2021-04-18T10:07:52Z,MonoGRNet: A General Framework for Monocular 3D Object Detection,"Detecting and localizing objects in the real 3D space, which plays a crucial
role in scene understanding, is particularly challenging given only a monocular
image due to the geometric information loss during imagery projection. We
propose MonoGRNet for the amodal 3D object detection from a monocular image via
geometric reasoning in both the observed 2D projection and the unobserved depth
dimension. MonoGRNet decomposes the monocular 3D object detection task into
four sub-tasks including 2D object detection, instance-level depth estimation,
projected 3D center estimation and local corner regression. The task
decomposition significantly facilitates the monocular 3D object detection,
allowing the target 3D bounding boxes to be efficiently predicted in a single
forward pass, without using object proposals, post-processing or the
computationally expensive pixel-level depth estimation utilized by previous
methods. In addition, MonoGRNet flexibly adapts to both fully and weakly
supervised learning, which improves the feasibility of our framework in diverse
settings. Experiments are conducted on KITTI, Cityscapes and MS COCO datasets.
Results demonstrate the promising performance of our framework in various
scenarios.",arxiv
http://arxiv.org/abs/1811.07770v2,2019-12-13T23:44:20Z,2018-11-11T01:57:15Z,Aff-Wild2: Extending the Aff-Wild Database for Affect Recognition,"Automatic understanding of human affect using visual signals is a problem
that has attracted significant interest over the past 20 years. However, human
emotional states are quite complex. To appraise such states displayed in
real-world settings, we need expressive emotional descriptors that are capable
of capturing and describing this complexity. The circumplex model of affect,
which is described in terms of valence (i.e., how positive or negative is an
emotion) and arousal (i.e., power of the activation of the emotion), can be
used for this purpose. Recent progress in the emotion recognition domain has
been achieved through the development of deep neural architectures and the
availability of very large training databases. To this end, Aff-Wild has been
the first large-scale ""in-the-wild"" database, containing around 1,200,000
frames. In this paper, we build upon this database, extending it with 260 more
subjects and 1,413,000 new video frames. We call the union of Aff-Wild with the
additional data, Aff-Wild2. The videos are downloaded from Youtube and have
large variations in pose, age, illumination conditions, ethnicity and
profession. Both database-specific as well as cross-database experiments are
performed in this paper, by utilizing the Aff-Wild2, along with the RECOLA
database. The developed deep neural architectures are based on the joint
training of state-of-the-art convolutional and recurrent neural networks with
attention mechanism; thus exploiting both the invariant properties of
convolutional features, while modeling temporal dynamics that arise in human
behaviour via the recurrent layers. The obtained results show premise for
utilization of the extended Aff-Wild, as well as of the developed deep neural
architectures for visual analysis of human behaviour in terms of continuous
emotion dimensions.",arxiv
http://arxiv.org/abs/2006.16007v1,2020-06-29T12:48:57Z,2020-06-29T12:48:57Z,MoNet3D: Towards Accurate Monocular 3D Object Localization in Real Time,"Monocular multi-object detection and localization in 3D space has been proven
to be a challenging task. The MoNet3D algorithm is a novel and effective
framework that can predict the 3D position of each object in a monocular image
and draw a 3D bounding box for each object. The MoNet3D method incorporates
prior knowledge of the spatial geometric correlation of neighbouring objects
into the deep neural network training process to improve the accuracy of 3D
object localization. Experiments on the KITTI dataset show that the accuracy
for predicting the depth and horizontal coordinates of objects in 3D space can
reach 96.25\% and 94.74\%, respectively. Moreover, the method can realize the
real-time image processing at 27.85 FPS, showing promising potential for
embedded advanced driving-assistance system applications. Our code is publicly
available at https://github.com/CQUlearningsystemgroup/YicongPeng.",arxiv
http://arxiv.org/abs/2001.09407v1,2020-01-26T06:02:16Z,2020-01-26T06:02:16Z,Fast Graph Convolutional Recurrent Neural Networks,"This paper proposes a Fast Graph Convolutional Neural Network (FGRNN)
architecture to predict sequences with an underlying graph structure. The
proposed architecture addresses the limitations of the standard recurrent
neural network (RNN), namely, vanishing and exploding gradients, causing
numerical instabilities during training. State-of-the-art architectures that
combine gated RNN architectures, such as Long Short-Term Memory (LSTM) and
Gated Recurrent Unit (GRU) with graph convolutions are known to improve the
numerical stability during the training phase, but at the expense of the model
size involving a large number of training parameters. FGRNN addresses this
problem by adding a weighted residual connection with only two extra training
parameters as compared to the standard RNN. Numerical experiments on the real
3D point cloud dataset corroborates the proposed architecture.",arxiv
http://arxiv.org/abs/2002.10718v2,2020-06-26T07:43:00Z,2020-02-25T08:04:31Z,"Denoising IMU Gyroscopes with Deep Learning for Open-Loop Attitude
  Estimation","This paper proposes a learning method for denoising gyroscopes of Inertial
Measurement Units (IMUs) using ground truth data, and estimating in real time
the orientation (attitude) of a robot in dead reckoning. The obtained algorithm
outperforms the state-of-the-art on the (unseen) test sequences. The obtained
performances are achieved thanks to a well-chosen model, a proper loss function
for orientation increments, and through the identification of key points when
training with high-frequency inertial data. Our approach builds upon a neural
network based on dilated convolutions, without requiring any recurrent neural
network. We demonstrate how efficient our strategy is for 3D attitude
estimation on the EuRoC and TUM-VI datasets. Interestingly, we observe our dead
reckoning algorithm manages to beat top-ranked visual-inertial odometry systems
in terms of attitude estimation although it does not use vision sensors. We
believe this paper offers new perspectives for visual-inertial localization and
constitutes a step toward more efficient learning methods involving IMUs. Our
open-source implementation is available at
https://github.com/mbrossar/denoise-imu-gyro.",arxiv
http://arxiv.org/abs/1508.01292v3,2015-11-23T20:01:06Z,2015-08-06T07:01:55Z,Compact Convolutional Neural Network Cascade for Face Detection,"The problem of faces detection in images or video streams is a classical
problem of computer vision. The multiple solutions of this problem have been
proposed, but the question of their optimality is still open. Many algorithms
achieve a high quality face detection, but at the cost of high computational
complexity. This restricts their application in the real-time systems. This
paper presents a new solution of the frontal face detection problem based on
compact convolutional neural networks cascade. The test results on FDDB dataset
show that it is competitive with state-of-the-art algorithms. This proposed
detector is implemented using three technologies: SSE/AVX/AVX2 instruction sets
for Intel CPUs, Nvidia CUDA, OpenCL. The detection speed of our approach
considerably exceeds all the existing CPU-based and GPU-based algorithms.
Because of high computational efficiency, our detector can processing 4K Ultra
HD video stream in real time (up to 27 fps) on mobile platforms (Intel Ivy
Bridge CPUs and Nvidia Kepler GPUs) in searching objects with the dimension
60x60 pixels or higher. At the same time its performance weakly dependent on
the background and number of objects in scene. This is achieved by the
asynchronous computation of stages in the cascade.",arxiv
http://arxiv.org/abs/1905.08790v4,2019-08-28T15:07:07Z,2019-05-21T19:53:38Z,"DoPa: A Comprehensive CNN Detection Methodology against Physical
  Adversarial Attacks","Recently, Convolutional Neural Networks (CNNs) demonstrate a considerable
vulnerability to adversarial attacks, which can be easily misled by adversarial
perturbations. With more aggressive methods proposed, adversarial attacks can
be also applied to the physical world, causing practical issues to various CNN
powered applications. To secure CNNs, adversarial attack detection is
considered as the most critical approach. However, most existing works focus on
superficial patterns and merely search a particular method to differentiate the
adversarial inputs and natural inputs, ignoring the analysis of CNN inner
vulnerability. Therefore, they can only target to specific physical adversarial
attacks, lacking expected versatility to different attacks. To address this
issue, we propose DoPa -- a comprehensive CNN detection methodology for various
physical adversarial attacks. By interpreting the CNN's vulnerability, we find
that non-semantic adversarial perturbations can activate CNN with significantly
abnormal activations and even overwhelm other semantic input patterns'
activations. Therefore, we add a self-verification stage to analyze the
semantics of distinguished activation patterns, which improves the CNN
recognition process. We apply such a detection methodology into both image and
audio CNN recognition scenarios. Experiments show that DoPa can achieve an
average rate of 90% success for image attack detection and 92% success for
audio attack detection.
  Announcement:[The original DoPa draft on arXiv was modified and submitted to
a conference already, while this short abstract was submitted only for a
presentation at the KDD 2019 AIoT Workshop.]",arxiv
http://arxiv.org/abs/1907.07263v2,2020-04-01T23:39:08Z,2019-07-16T21:18:14Z,"Caching as an Image Characterization Problem using Deep Convolutional
  Neural Networks","Caching of popular content closer to the mobile user can significantly
increase overall user experience as well as network efficiency by decongesting
backbone network segments in the case of congestion episodes. In order to find
the optimal caching locations, many conventional approaches rely on solving a
complex optimization problem that suffers from the curse of dimensionality,
which may fail to support online decision making. In this paper we propose a
framework to amalgamate model based optimization with data driven techniques by
transforming an optimization problem to a grayscale image and train a
convolutional neural network (CNN) to predict optimal caching location
policies. The rationale for the proposed modelling comes from CNN's superiority
to capture features in grayscale images reaching human level performance in
image recognition problems. The CNN is trained with optimal solutions and
numerical investigations reveal that the performance can increase by more than
400% compared to powerful randomized greedy algorithms. To this end, the
proposed technique seems as a promising way forward to the holy grail aspect in
resource orchestration which is providing high quality decision making in real
time.",arxiv
http://arxiv.org/abs/2103.05073v1,2021-03-08T21:02:37Z,2021-03-08T21:02:37Z,Offboard 3D Object Detection from Point Cloud Sequences,"While current 3D object recognition research mostly focuses on the real-time,
onboard scenario, there are many offboard use cases of perception that are
largely under-explored, such as using machines to automatically generate
high-quality 3D labels. Existing 3D object detectors fail to satisfy the
high-quality requirement for offboard uses due to the limited input and speed
constraints. In this paper, we propose a novel offboard 3D object detection
pipeline using point cloud sequence data. Observing that different frames
capture complementary views of objects, we design the offboard detector to make
use of the temporal points through both multi-frame object detection and novel
object-centric refinement models. Evaluated on the Waymo Open Dataset, our
pipeline named 3D Auto Labeling shows significant gains compared to the
state-of-the-art onboard detectors and our offboard baselines. Its performance
is even on par with human labels verified through a human label study. Further
experiments demonstrate the application of auto labels for semi-supervised
learning and provide extensive analysis to validate various design choices.",arxiv
http://arxiv.org/abs/2010.10910v1,2020-10-21T11:44:04Z,2020-10-21T11:44:04Z,Complaint Identification in Social Media with Transformer Networks,"Complaining is a speech act extensively used by humans to communicate a
negative inconsistency between reality and expectations. Previous work on
automatically identifying complaints in social media has focused on using
feature-based and task-specific neural network models. Adapting
state-of-the-art pre-trained neural language models and their combinations with
other linguistic information from topics or sentiment for complaint prediction
has yet to be explored. In this paper, we evaluate a battery of neural models
underpinned by transformer networks which we subsequently combine with
linguistic information. Experiments on a publicly available data set of
complaints demonstrate that our models outperform previous state-of-the-art
methods by a large margin achieving a macro F1 up to 87.",arxiv
http://arxiv.org/abs/1808.06722v1,2018-08-20T23:52:14Z,2018-08-20T23:52:14Z,Mechanisms for Resilient Video Transmission,"Wireless networks are envisaged to be one of the most important technologies
to provide cost-efficient content delivery, including for video applications.
They will allow thousands of thousands of fixed and mobile users to access,
produce, share, and consume video content in a ubiquitous way. Real-time video
services over these networks are becoming a part of everyday life and have been
used to spread information ranging from education to entertainment content.
However, the challenge of dealing with the fluctuating bandwidth, scarce
resources, and time-varying error rate of these networks, highlights the need
for error-resilient video transmission. In this context, the combination of
Forward Error Correction (FEC) and Unequal Error Protection (UEP) approaches is
known to provide the distribution of video applications for wireless users with
Quality of Experience (QoE) assurance. This thesis proposed a procedure to
assess the video characteristics and their related impact on the perceived
quality to end-users. This thesis proposes a series of cross-layer video-aware
and FEC-based mechanisms with UEP to enhance video transmission in several
types of wireless networks. A number of methods to set an adaptive amount of
redundancy were used in these mechanisms, such as heuristic techniques, random
neural networks, ant colony optimisation, and fuzzy logic. In the first one,
heuristic techniques, the mechanisms rely on human experience to define the
best strategy. The advantages and drawbacks of the proposed mechanisms were
demonstrated in realistic simulations using real video sequences and actual
network traces. The assessments were conducted with well-known QoE metrics. The
results show that all the proposed mechanisms were able to outperform the
competitors on both perceived video quality and network footprint.",arxiv
http://arxiv.org/abs/2007.06292v1,2020-07-13T10:20:58Z,2020-07-13T10:20:58Z,"Knowledge Graph Driven Approach to Represent Video Streams for
  Spatiotemporal Event Pattern Matching in Complex Event Processing","Complex Event Processing (CEP) is an event processing paradigm to perform
real-time analytics over streaming data and match high-level event patterns.
Presently, CEP is limited to process structured data stream. Video streams are
complicated due to their unstructured data model and limit CEP systems to
perform matching over them. This work introduces a graph-based structure for
continuous evolving video streams, which enables the CEP system to query
complex video event patterns. We propose the Video Event Knowledge Graph
(VEKG), a graph driven representation of video data. VEKG models video objects
as nodes and their relationship interaction as edges over time and space. It
creates a semantic knowledge representation of video data derived from the
detection of high-level semantic concepts from the video using an ensemble of
deep learning models. A CEP-based state optimization - VEKG-Time Aggregated
Graph (VEKG-TAG) is proposed over VEKG representation for faster event
detection. VEKG-TAG is a spatiotemporal graph aggregation method that provides
a summarized view of the VEKG graph over a given time length. We defined a set
of nine event pattern rules for two domains (Activity Recognition and Traffic
Management), which act as a query and applied over VEKG graphs to discover
complex event patterns. To show the efficacy of our approach, we performed
extensive experiments over 801 video clips across 10 datasets. The proposed
VEKG approach was compared with other state-of-the-art methods and was able to
detect complex event patterns over videos with F-Score ranging from 0.44 to
0.90. In the given experiments, the optimized VEKG-TAG was able to reduce 99%
and 93% of VEKG nodes and edges, respectively, with 5.19X faster search time,
achieving sub-second median latency of 4-20 milliseconds.",arxiv
http://arxiv.org/abs/2003.10758v1,2020-03-24T10:27:11Z,2020-03-24T10:27:11Z,FADNet: A Fast and Accurate Network for Disparity Estimation,"Deep neural networks (DNNs) have achieved great success in the area of
computer vision. The disparity estimation problem tends to be addressed by DNNs
which achieve much better prediction accuracy in stereo matching than
traditional hand-crafted feature based methods. On one hand, however, the
designed DNNs require significant memory and computation resources to
accurately predict the disparity, especially for those 3D convolution based
networks, which makes it difficult for deployment in real-time applications. On
the other hand, existing computation-efficient networks lack expression
capability in large-scale datasets so that they cannot make an accurate
prediction in many scenarios. To this end, we propose an efficient and accurate
deep network for disparity estimation named FADNet with three main features: 1)
It exploits efficient 2D based correlation layers with stacked blocks to
preserve fast computation; 2) It combines the residual structures to make the
deeper model easier to learn; 3) It contains multi-scale predictions so as to
exploit a multi-scale weight scheduling training technique to improve the
accuracy. We conduct experiments to demonstrate the effectiveness of FADNet on
two popular datasets, Scene Flow and KITTI 2015. Experimental results show that
FADNet achieves state-of-the-art prediction accuracy, and runs at a significant
order of magnitude faster speed than existing 3D models. The codes of FADNet
are available at https://github.com/HKBU-HPML/FADNet.",arxiv
http://arxiv.org/abs/2104.11138v1,2021-04-22T15:40:28Z,2021-04-22T15:40:28Z,"NanoNet: Real-Time Polyp Segmentation in Video Capsule Endoscopy and
  Colonoscopy","Deep learning in gastrointestinal endoscopy can assist to improve clinical
performance and be helpful to assess lesions more accurately. To this extent,
semantic segmentation methods that can perform automated real-time delineation
of a region-of-interest, e.g., boundary identification of cancer or
precancerous lesions, can benefit both diagnosis and interventions. However,
accurate and real-time segmentation of endoscopic images is extremely
challenging due to its high operator dependence and high-definition image
quality. To utilize automated methods in clinical settings, it is crucial to
design lightweight models with low latency such that they can be integrated
with low-end endoscope hardware devices. In this work, we propose NanoNet, a
novel architecture for the segmentation of video capsule endoscopy and
colonoscopy images. Our proposed architecture allows real-time performance and
has higher segmentation accuracy compared to other more complex ones. We use
video capsule endoscopy and standard colonoscopy datasets with polyps, and a
dataset consisting of endoscopy biopsies and surgical instruments, to evaluate
the effectiveness of our approach. Our experiments demonstrate the increased
performance of our architecture in terms of a trade-off between model
complexity, speed, model parameters, and metric performances. Moreover, the
resulting model size is relatively tiny, with only nearly 36,000 parameters
compared to traditional deep learning approaches having millions of parameters.",arxiv
http://arxiv.org/abs/1902.01286v1,2019-02-04T16:23:56Z,2019-02-04T16:23:56Z,"Real-Time Steganalysis for Stream Media Based on Multi-channel
  Convolutional Sliding Windows","Previous VoIP steganalysis methods face great challenges in detecting speech
signals at low embedding rates, and they are also generally difficult to
perform real-time detection, making them hard to truly maintain cyberspace
security. To solve these two challenges, in this paper, combined with the
sliding window detection algorithm and Convolution Neural Network we propose a
real-time VoIP steganalysis method which based on multi-channel convolution
sliding windows. In order to analyze the correlations between frames and
different neighborhood frames in a VoIP signal, we define multi channel sliding
detection windows. Within each sliding window, we design two feature extraction
channels which contain multiple convolution layers with multiple convolution
kernels each layer to extract correlation features of the input signal. Then
based on these extracted features, we use a forward fully connected network for
feature fusion. Finally, by analyzing the statistical distribution of these
features, the discriminator will determine whether the input speech signal
contains covert information or not.We designed several experiments to test the
proposed model's detection ability under various conditions, including
different embedding rates, different speech length, etc. Experimental results
showed that the proposed model outperforms all the previous methods, especially
in the case of low embedding rate, which showed state-of-the-art performance.
In addition, we also tested the detection efficiency of the proposed model, and
the results showed that it can achieve almost real-time detection of VoIP
speech signals.",arxiv
http://arxiv.org/abs/1911.01921v1,2019-11-05T16:31:23Z,2019-11-05T16:31:23Z,DLA: Dense-Layer-Analysis for Adversarial Example Detection,"In recent years Deep Neural Networks (DNNs) have achieved remarkable results
and even showed super-human capabilities in a broad range of domains. This led
people to trust in DNNs' classifications and resulting actions even in
security-sensitive environments like autonomous driving.
  Despite their impressive achievements, DNNs are known to be vulnerable to
adversarial examples. Such inputs contain small perturbations to intentionally
fool the attacked model.
  In this paper, we present a novel end-to-end framework to detect such attacks
during classification without influencing the target model's performance.
Inspired by recent research in neuron-coverage guided testing we show that
dense layers of DNNs carry security-sensitive information. With a secondary DNN
we analyze the activation patterns of the dense layers during classification
runtime, which enables effective and real-time detection of adversarial
examples.
  Our prototype implementation successfully detects adversarial examples in
image, natural language, and audio processing. Thereby, we cover a variety of
target DNNs, including Long Short Term Memory (LSTM) architectures. In
addition, to effectively defend against state-of-the-art attacks, our approach
generalizes between different sets of adversarial examples. Thus, our method
most likely enables us to detect even future, yet unknown attacks. Finally,
during white-box adaptive attacks, we show our method cannot be easily
bypassed.",arxiv
http://arxiv.org/abs/2007.08389v2,2020-08-27T00:33:27Z,2020-07-16T15:07:14Z,"Device-Robust Acoustic Scene Classification Based on Two-Stage
  Categorization and Data Augmentation","In this technical report, we present a joint effort of four groups, namely
GT, USTC, Tencent, and UKE, to tackle Task 1 - Acoustic Scene Classification
(ASC) in the DCASE 2020 Challenge. Task 1 comprises two different sub-tasks:
(i) Task 1a focuses on ASC of audio signals recorded with multiple (real and
simulated) devices into ten different fine-grained classes, and (ii) Task 1b
concerns with classification of data into three higher-level classes using
low-complexity solutions. For Task 1a, we propose a novel two-stage ASC system
leveraging upon ad-hoc score combination of two convolutional neural networks
(CNNs), classifying the acoustic input according to three classes, and then ten
classes, respectively. Four different CNN-based architectures are explored to
implement the two-stage classifiers, and several data augmentation techniques
are also investigated. For Task 1b, we leverage upon a quantization method to
reduce the complexity of two of our top-accuracy three-classes CNN-based
architectures. On Task 1a development data set, an ASC accuracy of 76.9\% is
attained using our best single classifier and data augmentation. An accuracy of
81.9\% is then attained by a final model fusion of our two-stage ASC
classifiers. On Task 1b development data set, we achieve an accuracy of 96.7\%
with a model size smaller than 500KB. Code is available:
https://github.com/MihawkHu/DCASE2020_task1.",arxiv
http://arxiv.org/abs/1905.02949v1,2019-05-08T08:04:35Z,2019-05-08T08:04:35Z,Deep Blind Video Decaptioning by Temporal Aggregation and Recurrence,"Blind video decaptioning is a problem of automatically removing text overlays
and inpainting the occluded parts in videos without any input masks. While
recent deep learning based inpainting methods deal with a single image and
mostly assume that the positions of the corrupted pixels are known, we aim at
automatic text removal in video sequences without mask information. In this
paper, we propose a simple yet effective framework for fast blind video
decaptioning. We construct an encoder-decoder model, where the encoder takes
multiple source frames that can provide visible pixels revealed from the scene
dynamics. These hints are aggregated and fed into the decoder. We apply a
residual connection from the input frame to the decoder output to enforce our
network to focus on the corrupted regions only. Our proposed model was ranked
in the first place in the ECCV Chalearn 2018 LAP Inpainting Competition Track2:
Video decaptioning. In addition, we further improve this strong model by
applying a recurrent feedback. The recurrent feedback not only enforces
temporal coherence but also provides strong clues on where the corrupted pixels
are. Both qualitative and quantitative experiments demonstrate that our full
model produces accurate and temporally consistent video results in real time
(50+ fps).",arxiv
http://arxiv.org/abs/1903.00374v4,2020-02-19T23:00:23Z,2019-03-01T15:40:19Z,Model-Based Reinforcement Learning for Atari,"Model-free reinforcement learning (RL) can be used to learn effective
policies for complex tasks, such as Atari games, even from image observations.
However, this typically requires very large amounts of interaction --
substantially more, in fact, than a human would need to learn the same games.
How can people learn so quickly? Part of the answer may be that people can
learn how the game works and predict which actions will lead to desirable
outcomes. In this paper, we explore how video prediction models can similarly
enable agents to solve Atari games with fewer interactions than model-free
methods. We describe Simulated Policy Learning (SimPLe), a complete model-based
deep RL algorithm based on video prediction models and present a comparison of
several model architectures, including a novel architecture that yields the
best results in our setting. Our experiments evaluate SimPLe on a range of
Atari games in low data regime of 100k interactions between the agent and the
environment, which corresponds to two hours of real-time play. In most games
SimPLe outperforms state-of-the-art model-free algorithms, in some games by
over an order of magnitude.",arxiv
http://arxiv.org/abs/2110.02582v1,2021-10-06T08:50:33Z,2021-10-06T08:50:33Z,"FADNet++: Real-Time and Accurate Disparity Estimation with Configurable
  Networks","Deep neural networks (DNNs) have achieved great success in the area of
computer vision. The disparity estimation problem tends to be addressed by DNNs
which achieve much better prediction accuracy than traditional hand-crafted
feature-based methods. However, the existing DNNs hardly serve both efficient
computation and rich expression capability, which makes them difficult for
deployment in real-time and high-quality applications, especially on mobile
devices. To this end, we propose an efficient, accurate, and configurable deep
network for disparity estimation named FADNet++. Leveraging several liberal
network design and training techniques, FADNet++ can boost its accuracy with a
fast model inference speed for real-time applications. Besides, it enables
users to easily configure different sizes of models for balancing accuracy and
inference efficiency. We conduct extensive experiments to demonstrate the
effectiveness of FADNet++ on both synthetic and realistic datasets among six
GPU devices varying from server to mobile platforms. Experimental results show
that FADNet++ and its variants achieve state-of-the-art prediction accuracy,
and run at a significant order of magnitude faster speed than existing 3D
models. With the constraint of running at above 15 frames per second (FPS) on a
mobile GPU, FADNet++ achieves a new state-of-the-art result for the SceneFlow
dataset.",arxiv
http://arxiv.org/abs/1912.01100v2,2020-03-04T09:50:32Z,2019-12-02T22:16:32Z,Latent Replay for Real-Time Continual Learning,"Training deep neural networks at the edge on light computational devices,
embedded systems and robotic platforms is nowadays very challenging. Continual
learning techniques, where complex models are incrementally trained on small
batches of new data, can make the learning problem tractable even for CPU-only
embedded devices enabling remarkable levels of adaptiveness and autonomy.
However, a number of practical problems need to be solved: catastrophic
forgetting before anything else. In this paper we introduce an original
technique named ""Latent Replay"" where, instead of storing a portion of past
data in the input space, we store activations volumes at some intermediate
layer. This can significantly reduce the computation and storage required by
native rehearsal. To keep the representation stable and the stored activations
valid we propose to slow-down learning at all the layers below the latent
replay one, leaving the layers above free to learn at full pace. In our
experiments we show that Latent Replay, combined with existing continual
learning techniques, achieves state-of-the-art performance on complex video
benchmarks such as CORe50 NICv2 (with nearly 400 small and highly non-i.i.d.
batches) and OpenLORIS. Finally, we demonstrate the feasibility of nearly
real-time continual learning on the edge through the deployment of the proposed
technique on a smartphone device.",arxiv
http://arxiv.org/abs/1812.09324v6,2020-11-01T22:04:47Z,2018-12-21T18:39:12Z,End-to-End Classification of Reverberant Rooms using DNNs,"Reverberation is present in our workplaces, our homes, concert halls and
theatres. This paper investigates how deep learning can use the effect of
reverberation on speech to classify a recording in terms of the room in which
it was recorded. Existing approaches in the literature rely on domain expertise
to manually select acoustic parameters as inputs to classifiers. Estimation of
these parameters from reverberant speech is adversely affected by estimation
errors, impacting the classification accuracy. In order to overcome the
limitations of previously proposed methods, this paper shows how DNNs can
perform the classification by operating directly on reverberant speech spectra
and a CRNN with an attention-mechanism is proposed for the task. The
relationship is investigated between the reverberant speech representations
learned by the DNNs and acoustic parameters. For evaluation, AIRs are used from
the ACE-challenge dataset that were measured in 7 real rooms. The
classification accuracy of the CRNN classifier in the experiments is 78% when
using 5 hours of training data and 90% when using 10 hours.",arxiv
http://arxiv.org/abs/2108.09518v1,2021-08-21T14:25:25Z,2021-08-21T14:25:25Z,MOTSynth: How Can Synthetic Data Help Pedestrian Detection and Tracking?,"Deep learning-based methods for video pedestrian detection and tracking
require large volumes of training data to achieve good performance. However,
data acquisition in crowded public environments raises data privacy concerns --
we are not allowed to simply record and store data without the explicit consent
of all participants. Furthermore, the annotation of such data for computer
vision applications usually requires a substantial amount of manual effort,
especially in the video domain. Labeling instances of pedestrians in highly
crowded scenarios can be challenging even for human annotators and may
introduce errors in the training data. In this paper, we study how we can
advance different aspects of multi-person tracking using solely synthetic data.
To this end, we generate MOTSynth, a large, highly diverse synthetic dataset
for object detection and tracking using a rendering game engine. Our
experiments show that MOTSynth can be used as a replacement for real data on
tasks such as pedestrian detection, re-identification, segmentation, and
tracking.",arxiv
http://arxiv.org/abs/2003.02301v2,2020-05-01T02:33:22Z,2020-03-04T19:30:15Z,"Real-time, Universal, and Robust Adversarial Attacks Against Speaker
  Recognition Systems","As the popularity of voice user interface (VUI) exploded in recent years,
speaker recognition system has emerged as an important medium of identifying a
speaker in many security-required applications and services. In this paper, we
propose the first real-time, universal, and robust adversarial attack against
the state-of-the-art deep neural network (DNN) based speaker recognition
system. Through adding an audio-agnostic universal perturbation on arbitrary
enrolled speaker's voice input, the DNN-based speaker recognition system would
identify the speaker as any target (i.e., adversary-desired) speaker label. In
addition, we improve the robustness of our attack by modeling the sound
distortions caused by the physical over-the-air propagation through estimating
room impulse response (RIR). Experiment using a public dataset of 109 English
speakers demonstrates the effectiveness and robustness of our proposed attack
with a high attack success rate of over 90%. The attack launching time also
achieves a 100X speedup over contemporary non-universal attacks.",arxiv
http://arxiv.org/abs/1710.01559v2,2018-05-06T08:21:51Z,2017-10-04T11:48:34Z,"Monitoring tool usage in surgery videos using boosted convolutional and
  recurrent neural networks","This paper investigates the automatic monitoring of tool usage during a
surgery, with potential applications in report generation, surgical training
and real-time decision support. Two surgeries are considered: cataract surgery,
the most common surgical procedure, and cholecystectomy, one of the most common
digestive surgeries. Tool usage is monitored in videos recorded either through
a microscope (cataract surgery) or an endoscope (cholecystectomy). Following
state-of-the-art video analysis solutions, each frame of the video is analyzed
by convolutional neural networks (CNNs) whose outputs are fed to recurrent
neural networks (RNNs) in order to take temporal relationships between events
into account. Novelty lies in the way those CNNs and RNNs are trained.
Computational complexity prevents the end-to-end training of ""CNN+RNN"" systems.
Therefore, CNNs are usually trained first, independently from the RNNs. This
approach is clearly suboptimal for surgical tool analysis: many tools are very
similar to one another, but they can generally be differentiated based on past
events. CNNs should be trained to extract the most useful visual features in
combination with the temporal context. A novel boosting strategy is proposed to
achieve this goal: the CNN and RNN parts of the system are simultaneously
enriched by progressively adding weak classifiers (either CNNs or RNNs) trained
to improve the overall classification accuracy. Experiments were performed in a
dataset of 50 cataract surgery videos and a dataset of 80 cholecystectomy
videos. Very good classification performance are achieved in both datasets:
tool usage could be labeled with an average area under the ROC curve of $A_z =
0.9961$ and $A_z = 0.9939$, respectively, in offline mode (using past, present
and future information), and $A_z = 0.9957$ and $A_z = 0.9936$, respectively,
in online mode (using past and present information only).",arxiv
http://arxiv.org/abs/1912.05622v3,2020-11-12T00:54:48Z,2019-12-11T21:00:08Z,"Efficient in-situ image and video compression through probabilistic
  image representation","Fast and effective image compression for multi-dimensional images has become
increasingly important for efficient storage and transfer of massive amounts of
high-resolution images and videos. Desirable properties in compression methods
include (1) high reconstruction quality at a wide range of compression rates
while preserving key local details, (2) computational scalability, (3)
applicability to a variety of different image/video types and of different
dimensions, (4) progressive transmission, and (5) ease of tuning. We present
such a method for multi-dimensional image compression called Compression via
Adaptive Recursive Partitioning (CARP). CARP uses an optimal permutation of the
image pixels inferred from a Bayesian probabilistic model on recursive
partitions of the image to reduce its effective dimensionality, achieving a
parsimonious representation that preserves information. CARP uses a multi-layer
Bayesian hierarchical model to achieve in-situ compression along with
self-tuning and regularization, with just one single parameter to be specified
by the user to achieve the desired compression rate. Extensive numerical
experiments using a variety of datasets including 2D still images, real-life
YouTube videos, and surveillance videos show that CARP dominates the
state-of-the-art image/video compression approaches---including JPEG, JPEG2000,
BPG, MPEG4, HEVC and a neural network-based method---for all of these different
image types and on nearly all of the individual images and videos over some
methods.",arxiv
http://arxiv.org/abs/1802.01144v2,2018-02-12T06:49:38Z,2018-02-04T15:25:52Z,"Human Action Adverb Recognition: ADHA Dataset and A Three-Stream Hybrid
  Model","We introduce the first benchmark for a new problem --- recognizing human
action adverbs (HAA): ""Adverbs Describing Human Actions"" (ADHA). This is the
first step for computer vision to change over from pattern recognition to real
AI. We demonstrate some key features of ADHA: a semantically complete set of
adverbs describing human actions, a set of common, describable human actions,
and an exhaustive labeling of simultaneously emerging actions in each video. We
commit an in-depth analysis on the implementation of current effective models
in action recognition and image captioning on adverb recognition, and the
results show that such methods are unsatisfactory. Moreover, we propose a novel
three-stream hybrid model to deal the HAA problem, which achieves a better
result.",arxiv
http://arxiv.org/abs/1909.11799v4,2021-08-07T16:30:21Z,2019-09-25T22:28:47Z,"Manifold Oblique Random Forests: Towards Closing the Gap on
  Convolutional Deep Networks","Decision forests (Forests), in particular random forests and gradient
boosting trees, have demonstrated state-of-the-art accuracy compared to other
methods in many supervised learning scenarios. In particular, Forests dominate
other methods in tabular data, that is, when the feature space is unstructured,
so that the signal is invariant to a permutation of the feature indices.
However, in structured data lying on a manifold (such as images, text, and
speech) deep networks (Networks), specifically convolutional deep networks
(ConvNets), tend to outperform Forests. We conjecture that at least part of the
reason for this is that the input to Networks is not simply the feature
magnitudes, but also their indices. In contrast, naive Forest implementations
fail to explicitly consider feature indices. A recently proposed Forest
approach demonstrates that Forests, for each node, implicitly sample a random
matrix from some specific distribution. These Forests, like some classes of
Networks, learn by partitioning the feature space into convex polytopes
corresponding to linear functions. We build on that approach and show that one
can choose distributions in a manifold-aware fashion to incorporate feature
locality. We demonstrate the empirical performance on data whose features live
on three different manifolds: a torus, images, and time-series. Moreover, we
demonstrate its strength in multivariate simulated settings and also show
superiority in predicting surgical outcome in epilepsy patients and predicting
movement direction from raw stereotactic EEG data from non-motor brain regions.
In all simulations and real data, Manifold Oblique Random Forest (MORF)
algorithm outperforms approaches that ignore feature space structure and
challenges the performance of ConvNets. Moreover, MORF runs fast and maintains
interpretability and theoretical justification.",arxiv
http://arxiv.org/abs/1910.06790v1,2019-10-14T07:47:55Z,2019-10-14T07:47:55Z,"Weakly Labeled Sound Event Detection Using Tri-training and Adversarial
  Learning","This paper considers a semi-supervised learning framework for weakly labeled
polyphonic sound event detection problems for the DCASE 2019 challenge's task4
by combining both the tri-training and adversarial learning. The goal of the
task4 is to detect onsets and offsets of multiple sound events in a single
audio clip. The entire dataset consists of the synthetic data with a strong
label (sound event labels with boundaries) and real data with weakly labeled
(sound event labels) and unlabeled dataset. Given this dataset, we apply the
tri-training where two different classifiers are used to obtain pseudo labels
on the weakly labeled and unlabeled dataset, and the final classifier is
trained using the strongly labeled dataset and weakly/unlabeled dataset with
pseudo labels. Also, we apply the adversarial learning to reduce the domain gap
between the real and synthetic dataset. We evaluated our learning framework
using the validation set of the task4 dataset, and in the experiments, our
learning framework shows a considerable performance improvement over the
baseline model.",arxiv
http://arxiv.org/abs/1512.08512v2,2016-04-30T03:03:04Z,2015-12-28T20:56:50Z,Visually Indicated Sounds,"Objects make distinctive sounds when they are hit or scratched. These sounds
reveal aspects of an object's material properties, as well as the actions that
produced them. In this paper, we propose the task of predicting what sound an
object makes when struck as a way of studying physical interactions within a
visual scene. We present an algorithm that synthesizes sound from silent videos
of people hitting and scratching objects with a drumstick. This algorithm uses
a recurrent neural network to predict sound features from videos and then
produces a waveform from these features with an example-based synthesis
procedure. We show that the sounds predicted by our model are realistic enough
to fool participants in a ""real or fake"" psychophysical experiment, and that
they convey significant information about material properties and physical
interactions.",arxiv
http://arxiv.org/abs/1909.09177v2,2020-03-30T06:25:14Z,2019-09-19T18:08:41Z,"Nonlinear Multiview Analysis: Identifiability and Neural
  Network-assisted Implementation","Multiview analysis aims at extracting shared latent components from data
samples that are acquired in different domains, e.g., image, text, and audio.
Classic multiview analysis, e.g., canonical correlation analysis (CCA), tackles
this problem via matching the linearly transformed views in a certain latent
domain. More recently, powerful nonlinear learning tools such as kernel methods
and neural networks are utilized for enhancing the classic CCA. However, unlike
linear CCA whose theoretical aspects are clearly understood, nonlinear CCA
approaches are largely intuition-driven. In particular, it is unclear under
what conditions the shared latent components across the views can be
identified---while identifiability plays an essential role in many
applications. In this work, we revisit nonlinear multiview analysis and address
both the theoretical and computational aspects. Our work leverages a useful
nonlinear model, namely, the post-nonlinear model, from the nonlinear mixture
separation literature. Combining with multiview data, we take a nonlinear
multiview mixture learning viewpoint, which is a natural extension of the
classic generative models for linear CCA. From there, we derive a learning
criterion. We show that minimizing this criterion leads to identification of
the latent shared components up to certain ambiguities, under reasonable
conditions. Our derivation and formulation also offer new insights and
interpretations to existing deep neural network-based CCA formulations. On the
computation side, we propose an effective algorithm with simple and scalable
update rules. A series of simulations and real-data experiments corroborate our
theoretical analysis.",arxiv
http://arxiv.org/abs/1905.04307v1,2019-05-10T15:42:35Z,2019-05-10T15:42:35Z,Semantic Segmentation of Seismic Images,"Almost all work to understand Earth's subsurface on a large scale relies on
the interpretation of seismic surveys by experts who segment the survey
(usually a cube) into layers; a process that is very time demanding. In this
paper, we present a new deep neural network architecture specially designed to
semantically segment seismic images with a minimal amount of training data. To
achieve this, we make use of a transposed residual unit that replaces the
traditional dilated convolution for the decode block. Also, instead of using a
predefined shape for up-scaling, our network learns all the steps to upscale
the features from the encoder. We train our neural network using the Penobscot
3D dataset; a real seismic dataset acquired offshore Nova Scotia, Canada. We
compare our approach with two well-known deep neural network topologies: Fully
Convolutional Network and U-Net. In our experiments, we show that our approach
can achieve more than 99 percent of the mean intersection over union (mIOU)
metric, outperforming the existing topologies. Moreover, our qualitative
results show that the obtained model can produce masks very close to human
interpretation with very little discontinuity.",arxiv
http://arxiv.org/abs/1612.07978v1,2016-12-23T14:17:31Z,2016-12-23T14:17:31Z,"Two-stream convolutional neural network for accurate RGB-D fingertip
  detection using depth and edge information","Accurate detection of fingertips in depth image is critical for
human-computer interaction. In this paper, we present a novel two-stream
convolutional neural network (CNN) for RGB-D fingertip detection. Firstly edge
image is extracted from raw depth image using random forest. Then the edge
information is combined with depth information in our CNN structure. We study
several fusion approaches and suggest a slow fusion strategy as a promising way
of fingertip detection. As shown in our experiments, our real-time algorithm
outperforms state-of-the-art fingertip detection methods on the public dataset
HandNet with an average 3D error of 9.9mm, and shows comparable accuracy of
fingertip estimation on NYU hand dataset.",arxiv
http://arxiv.org/abs/1312.5457v1,2013-12-19T09:40:03Z,2013-12-19T09:40:03Z,"Codebook based Audio Feature Representation for Music Information
  Retrieval","Digital music has become prolific in the web in recent decades. Automated
recommendation systems are essential for users to discover music they love and
for artists to reach appropriate audience. When manual annotations and user
preference data is lacking (e.g. for new artists) these systems must rely on
\emph{content based} methods. Besides powerful machine learning tools for
classification and retrieval, a key component for successful recommendation is
the \emph{audio content representation}.
  Good representations should capture informative musical patterns in the audio
signal of songs. These representations should be concise, to enable efficient
(low storage, easy indexing, fast search) management of huge music
repositories, and should also be easy and fast to compute, to enable real-time
interaction with a user supplying new songs to the system.
  Before designing new audio features, we explore the usage of traditional
local features, while adding a stage of encoding with a pre-computed
\emph{codebook} and a stage of pooling to get compact vectorial
representations. We experiment with different encoding methods, namely
\emph{the LASSO}, \emph{vector quantization (VQ)} and \emph{cosine similarity
(CS)}. We evaluate the representations' quality in two music information
retrieval applications: query-by-tag and query-by-example. Our results show
that concise representations can be used for successful performance in both
applications. We recommend using top-$\tau$ VQ encoding, which consistently
performs well in both applications, and requires much less computation time
than the LASSO.",arxiv
http://arxiv.org/abs/2106.13041v1,2021-06-24T14:15:50Z,2021-06-24T14:15:50Z,"Unsupervised Learning of Depth and Depth-of-Field Effect from Natural
  Images with Aperture Rendering Generative Adversarial Networks","Understanding the 3D world from 2D projected natural images is a fundamental
challenge in computer vision and graphics. Recently, an unsupervised learning
approach has garnered considerable attention owing to its advantages in data
collection. However, to mitigate training limitations, typical methods need to
impose assumptions for viewpoint distribution (e.g., a dataset containing
various viewpoint images) or object shape (e.g., symmetric objects). These
assumptions often restrict applications; for instance, the application to
non-rigid objects or images captured from similar viewpoints (e.g., flower or
bird images) remains a challenge. To complement these approaches, we propose
aperture rendering generative adversarial networks (AR-GANs), which equip
aperture rendering on top of GANs, and adopt focus cues to learn the depth and
depth-of-field (DoF) effect of unlabeled natural images. To address the
ambiguities triggered by unsupervised setting (i.e., ambiguities between smooth
texture and out-of-focus blurs, and between foreground and background blurs),
we develop DoF mixture learning, which enables the generator to learn real
image distribution while generating diverse DoF images. In addition, we devise
a center focus prior to guiding the learning direction. In the experiments, we
demonstrate the effectiveness of AR-GANs in various datasets, such as flower,
bird, and face images, demonstrate their portability by incorporating them into
other 3D representation learning GANs, and validate their applicability in
shallow DoF rendering.",arxiv
http://arxiv.org/abs/1909.05776v1,2019-09-12T16:14:37Z,2019-09-12T16:14:37Z,"I-SAFE: Instant Suspicious Activity identiFication at the Edge using
  Fuzzy Decision Making","Urban imagery usually serves as forensic analysis and by design is available
for incident mitigation. As more imagery collected, it is harder to narrow down
to certain frames among thousands of video clips to a specific incident. A
real-time, proactive surveillance system is desirable, which could instantly
detect dubious personnel, identify suspicious activities, or raise momentous
alerts. The recent proliferation of the edge computing paradigm allows more
data-intensive tasks to be accomplished by smart edge devices with lightweight
but powerful algorithms. This paper presents a forensic surveillance strategy
by introducing an Instant Suspicious Activity identiFication at the Edge
(I-SAFE) using fuzzy decision making. A fuzzy control system is proposed to
mimic the decision-making process of a security officer. Decisions are made
based on video features extracted by a lightweight Deep Machine Learning (DML)
model. Based on the requirements from the first-line law enforcement officers,
several features are selected and fuzzified to cope with the state of
uncertainty that exists in the officers' decision-making process. Using
features in the edge hierarchy minimizes the communication delay such that
instant alerting is achieved. Additionally, leveraging the Microservices
architecture, the I-SAFE scheme possesses good scalability given the increasing
complexities at the network edge. Implemented as an edge-based application and
tested using exemplary and various labeled dataset surveillance videos, the
I-SAFE scheme raises alerts by identifying the suspicious activity in an
average of 0.002 seconds. Compared to four other state-of-the-art methods over
two other data sets, the experimental study verified the superiority of the
I-SAFE decentralized method.",arxiv
http://arxiv.org/abs/1904.05191v3,2019-04-16T07:53:34Z,2019-04-10T13:55:10Z,"Weakly-Supervised White and Grey Matter Segmentation in 3D Brain
  Ultrasound","Although the segmentation of brain structures in ultrasound helps initialize
image based registration, assist brain shift compensation, and provides
interventional decision support, the task of segmenting grey and white matter
in cranial ultrasound is very challenging and has not been addressed yet. We
train a multi-scale fully convolutional neural network simultaneously for two
classes in order to segment real clinical 3D ultrasound data. Parallel pathways
working at different levels of resolution account for high frequency speckle
noise and global 3D image features. To ensure reproducibility, the publicly
available RESECT dataset is utilized for training and cross-validation. Due to
the absence of a ground truth, we train with weakly annotated label. We
implement label transfer from MRI to US, which is prone to a residual but
inevitable registration error. To further improve results, we perform transfer
learning using synthetic US data. The resulting method leads to excellent Dice
scores of 0.7080, 0.8402 and 0.9315 for grey matter, white matter and
background. Our proposed methodology sets an unparalleled standard for white
and grey matter segmentation in 3D intracranial ultrasound.",arxiv
http://arxiv.org/abs/2006.14837v1,2020-06-26T07:32:30Z,2020-06-26T07:32:30Z,Expandable YOLO: 3D Object Detection from RGB-D Images,"This paper aims at constructing a light-weight object detector that inputs a
depth and a color image from a stereo camera. Specifically, by extending the
network architecture of YOLOv3 to 3D in the middle, it is possible to output in
the depth direction. In addition, Intersection over Uninon (IoU) in 3D space is
introduced to confirm the accuracy of region extraction results. In the field
of deep learning, object detectors that use distance information as input are
actively studied for utilizing automated driving. However, the conventional
detector has a large network structure, and the real-time property is impaired.
The effectiveness of the detector constructed as described above is verified
using datasets. As a result of this experiment, the proposed model is able to
output 3D bounding boxes and detect people whose part of the body is hidden.
Further, the processing speed of the model is 44.35 fps.",arxiv
http://arxiv.org/abs/1911.00686v3,2020-03-04T13:51:41Z,2019-11-02T09:42:25Z,Unmasking DeepFakes with simple Features,"Deep generative models have recently achieved impressive results for many
real-world applications, successfully generating high-resolution and diverse
samples from complex datasets. Due to this improvement, fake digital contents
have proliferated growing concern and spreading distrust in image content,
leading to an urgent need for automated ways to detect these AI-generated fake
images.
  Despite the fact that many face editing algorithms seem to produce realistic
human faces, upon closer examination, they do exhibit artifacts in certain
domains which are often hidden to the naked eye. In this work, we present a
simple way to detect such fake face images - so-called DeepFakes. Our method is
based on a classical frequency domain analysis followed by basic classifier.
Compared to previous systems, which need to be fed with large amounts of
labeled data, our approach showed very good results using only a few annotated
training samples and even achieved good accuracies in fully unsupervised
scenarios. For the evaluation on high resolution face images, we combined
several public datasets of real and fake faces into a new benchmark: Faces-HQ.
Given such high-resolution images, our approach reaches a perfect
classification accuracy of 100% when it is trained on as little as 20 annotated
samples. In a second experiment, in the evaluation of the medium-resolution
images of the CelebA dataset, our method achieves 100% accuracy supervised and
96% in an unsupervised setting. Finally, evaluating a low-resolution video
sequences of the FaceForensics++ dataset, our method achieves 91% accuracy
detecting manipulated videos.
  Source Code: https://github.com/cc-hpc-itwm/DeepFakeDetection",arxiv
http://arxiv.org/abs/2106.12302v1,2021-06-23T10:49:34Z,2021-06-23T10:49:34Z,"3D human tongue reconstruction from single ""in-the-wild"" images","3D face reconstruction from a single image is a task that has garnered
increased interest in the Computer Vision community, especially due to its
broad use in a number of applications such as realistic 3D avatar creation,
pose invariant face recognition and face hallucination. Since the introduction
of the 3D Morphable Model in the late 90's, we witnessed an explosion of
research aiming at particularly tackling this task. Nevertheless, despite the
increasing level of detail in the 3D face reconstructions from single images
mainly attributed to deep learning advances, finer and highly deformable
components of the face such as the tongue are still absent from all 3D face
models in the literature, although being very important for the realness of the
3D avatar representations. In this work we present the first, to the best of
our knowledge, end-to-end trainable pipeline that accurately reconstructs the
3D face together with the tongue. Moreover, we make this pipeline robust in
""in-the-wild"" images by introducing a novel GAN method tailored for 3D tongue
surface generation. Finally, we make publicly available to the community the
first diverse tongue dataset, consisting of 1,800 raw scans of 700 individuals
varying in gender, age, and ethnicity backgrounds. As we demonstrate in an
extensive series of quantitative as well as qualitative experiments, our model
proves to be robust and realistically captures the 3D tongue structure, even in
adverse ""in-the-wild"" conditions.",arxiv
http://arxiv.org/abs/2007.00290v1,2020-07-01T07:29:35Z,2020-07-01T07:29:35Z,"Robust Semantic Segmentation in Adverse Weather Conditions by means of
  Fast Video-Sequence Segmentation","Computer vision tasks such as semantic segmentation perform very well in good
weather conditions, but if the weather turns bad, they have problems to achieve
this performance in these conditions. One possibility to obtain more robust and
reliable results in adverse weather conditions is to use video-segmentation
approaches instead of commonly used single-image segmentation methods.
Video-segmentation approaches capture temporal information of the previous
video-frames in addition to current image information, and hence, they are more
robust against disturbances, especially if they occur in only a few frames of
the video-sequence. However, video-segmentation approaches, which are often
based on recurrent neural networks, cannot be applied in real-time applications
anymore, since their recurrent structures in the network are computational
expensive. For instance, the inference time of the LSTM-ICNet, in which
recurrent units are placed at proper positions in the single-segmentation
approach ICNet, increases up to 61 percent compared to the basic ICNet. Hence,
in this work, the LSTM-ICNet is sped up by modifying the recurrent units of the
network so that it becomes real-time capable again. Experiments on different
datasets and various weather conditions show that the inference time can be
decreased by about 23 percent by these modifications, while they achieve
similar performance than the LSTM-ICNet and outperform the single-segmentation
approach enormously in adverse weather conditions.",arxiv
http://arxiv.org/abs/1905.02882v1,2019-05-08T03:13:54Z,2019-05-08T03:13:54Z,Frame-Recurrent Video Inpainting by Robust Optical Flow Inference,"In this paper, we present a new inpainting framework for recovering missing
regions of video frames. Compared with image inpainting, performing this task
on video presents new challenges such as how to preserving temporal consistency
and spatial details, as well as how to handle arbitrary input video size and
length fast and efficiently. Towards this end, we propose a novel deep learning
architecture which incorporates ConvLSTM and optical flow for modeling the
spatial-temporal consistency in videos. It also saves much computational
resource such that our method can handle videos with larger frame size and
arbitrary length streamingly in real-time. Furthermore, to generate an accurate
optical flow from corrupted frames, we propose a robust flow generation module,
where two sources of flows are fed and a flow blending network is trained to
fuse them. We conduct extensive experiments to evaluate our method in various
scenarios and different datasets, both qualitatively and quantitatively. The
experimental results demonstrate the superior of our method compared with the
state-of-the-art inpainting approaches.",arxiv
http://arxiv.org/abs/1907.10283v1,2019-07-24T07:54:45Z,2019-07-24T07:54:45Z,"StableNet: Semi-Online, Multi-Scale Deep Video Stabilization","Video stabilization algorithms are of greater importance nowadays with the
prevalence of hand-held devices which unavoidably produce videos with
undesirable shaky motions. In this paper we propose a data-driven online video
stabilization method along with a paired dataset for deep learning. The network
processes each unsteady frame progressively in a multi-scale manner, from low
resolution to high resolution, and then outputs an affine transformation to
stabilize the frame. Different from conventional methods which require explicit
feature tracking or optical flow estimation, the underlying stabilization
process is learned implicitly from the training data, and the stabilization
process can be done online. Since there are limited public video stabilization
datasets available, we synthesized unstable videos with different extent of
shake that simulate real-life camera movement. Experiments show that our method
is able to outperform other stabilization methods in several unstable samples
while remaining comparable in general. Also, our method is tested on complex
contents and found robust enough to dampen these samples to some extent even it
was not explicitly trained in the contents.",arxiv
http://arxiv.org/abs/2004.14674v2,2020-05-02T12:33:08Z,2020-04-30T10:28:08Z,SS3D: Single Shot 3D Object Detector,"Single stage deep learning algorithm for 2D object detection was made popular
by Single Shot MultiBox Detector (SSD) and it was heavily adopted in several
embedded applications. PointPillars is a state of the art 3D object detection
algorithm that uses a Single Shot Detector adapted for 3D object detection. The
main downside of PointPillars is that it has a two stage approach with learned
input representation based on fully connected layers followed by the Single
Shot Detector for 3D detection. In this paper we present Single Shot 3D Object
Detection (SS3D) - a single stage 3D object detection algorithm which combines
straight forward, statistically computed input representation and a Single Shot
Detector (based on PointPillars). Computing the input representation is
straight forward, does not involve learning and does not have much
computational cost. We also extend our method to stereo input and show that,
aided by additional semantic segmentation input; our method produces similar
accuracy as state of the art stereo based detectors. Achieving the accuracy of
two stage detectors using a single stage approach is important as single stage
approaches are simpler to implement in embedded, real-time applications. With
LiDAR as well as stereo input, our method outperforms PointPillars. When using
LiDAR input, our input representation is able to improve the AP3D of Cars
objects in the moderate category from 74.99 to 76.84. When using stereo input,
our input representation is able to improve the AP3D of Cars objects in the
moderate category from 38.13 to 45.13. Our results are also better than other
popular 3D object detectors such as AVOD and F-PointNet.",arxiv
http://arxiv.org/abs/1807.11147v1,2018-07-30T02:32:38Z,2018-07-30T02:32:38Z,"Occluded Joints Recovery in 3D Human Pose Estimation based on Distance
  Matrix","Albeit the recent progress in single image 3D human pose estimation due to
the convolutional neural network, it is still challenging to handle real
scenarios such as highly occluded scenes. In this paper, we propose to address
the problem of single image 3D human pose estimation with occluded measurements
by exploiting the Euclidean distance matrix (EDM). Specifically, we present two
approaches based on EDM, which could effectively handle occluded joints in 2D
images. The first approach is based on 2D-to-2D distance matrix regression
achieved by a simple CNN architecture. The second approach is based on sparse
coding along with a learned over-complete dictionary. Experiments on the
Human3.6M dataset show the excellent performance of these two approaches in
recovering occluded observations and demonstrate the improvements in accuracy
for 3D human pose estimation with occluded joints.",arxiv
http://arxiv.org/abs/2012.06178v1,2020-12-11T08:07:39Z,2020-12-11T08:07:39Z,"Detailed 3D Human Body Reconstruction from Multi-view Images Combining
  Voxel Super-Resolution and Learned Implicit Representation","The task of reconstructing detailed 3D human body models from images is
interesting but challenging in computer vision due to the high freedom of human
bodies. In order to tackle the problem, we propose a coarse-to-fine method to
reconstruct a detailed 3D human body from multi-view images combining voxel
super-resolution based on learning the implicit representation. Firstly, the
coarse 3D models are estimated by learning an implicit representation based on
multi-scale features which are extracted by multi-stage hourglass networks from
the multi-view images. Then, taking the low resolution voxel grids which are
generated by the coarse 3D models as input, the voxel super-resolution based on
an implicit representation is learned through a multi-stage 3D convolutional
neural network. Finally, the refined detailed 3D human body models can be
produced by the voxel super-resolution which can preserve the details and
reduce the false reconstruction of the coarse 3D models. Benefiting from the
implicit representation, the training process in our method is memory efficient
and the detailed 3D human body produced by our method from multi-view images is
the continuous decision boundary with high-resolution geometry. In addition,
the coarse-to-fine method based on voxel super-resolution can remove false
reconstructions and preserve the appearance details in the final
reconstruction, simultaneously. In the experiments, our method quantitatively
and qualitatively achieves the competitive 3D human body reconstructions from
images with various poses and shapes on both the real and synthetic datasets.",arxiv
http://arxiv.org/abs/2105.01948v1,2021-05-05T09:36:30Z,2021-05-05T09:36:30Z,"Similarity Measures for Location-Dependent MMIMO, 5G Base Stations
  On/Off Switching Using Radio Environment Map","The Massive Multiple-Input Multiple-Output (MMIMO) technique together with
Heterogeneous Network (Het-Net) deployment enables high throughput of 5G and
beyond networks. However, a high number of antennas and a high number of Base
Stations (BSs) can result in significant power consumption. Previous studies
have shown that the energy efficiency (EE) of such a network can be effectively
increased by turning off some BSs depending on User Equipments (UEs) positions.
Such mapping is obtained by using Reinforcement Learning. Its results are
stored in a so-called Radio Environment Map (REM). However, in a real network,
the number of UEs' positions patterns would go to infinity. This paper aims to
determine how to match the current set of UEs' positions to the most similar
pattern, i.e., providing the same optimal active BSs set, saved in REM. We
compare several state-of-the-art distance metrics using a computer simulator:
an accurate 3D-Ray-Tracing model of the radio channel and an advanced
system-level simulator of MMIMO Het-Net. The results have shown that the
so-called Sum of Minimums Distance provides the best matching between REM data
and UEs' positions, enabling up to 56% EE improvement over the scenario without
EE optimization.",arxiv
http://arxiv.org/abs/1808.00259v1,2018-08-01T10:50:47Z,2018-08-01T10:50:47Z,Drone Detection Using Depth Maps,"Obstacle avoidance is a key feature for safe Unmanned Aerial Vehicle (UAV)
navigation. While solutions have been proposed for static obstacle avoidance,
systems enabling avoidance of dynamic objects, such as drones, are hard to
implement due to the detection range and field-of-view (FOV) requirements, as
well as the constraints for integrating such systems on-board small UAVs. In
this work, a dataset of 6k synthetic depth maps of drones has been generated
and used to train a state-of-the-art deep learning-based drone detection model.
While many sensing technologies can only provide relative altitude and azimuth
of an obstacle, our depth map-based approach enables full 3D localization of
the obstacle. This is extremely useful for collision avoidance, as 3D
localization of detected drones is key to perform efficient collision-free path
planning. The proposed detection technique has been validated in several real
depth map sequences, with multiple types of drones flying at up to 2 m/s,
achieving an average precision of 98.7%, an average recall of 74.7% and a
record detection range of 9.5 meters.",arxiv
http://arxiv.org/abs/2006.07364v2,2020-10-22T17:57:12Z,2020-06-12T17:56:16Z,"Residual Force Control for Agile Human Behavior Imitation and Extended
  Motion Synthesis","Reinforcement learning has shown great promise for synthesizing realistic
human behaviors by learning humanoid control policies from motion capture data.
However, it is still very challenging to reproduce sophisticated human skills
like ballet dance, or to stably imitate long-term human behaviors with complex
transitions. The main difficulty lies in the dynamics mismatch between the
humanoid model and real humans. That is, motions of real humans may not be
physically possible for the humanoid model. To overcome the dynamics mismatch,
we propose a novel approach, residual force control (RFC), that augments a
humanoid control policy by adding external residual forces into the action
space. During training, the RFC-based policy learns to apply residual forces to
the humanoid to compensate for the dynamics mismatch and better imitate the
reference motion. Experiments on a wide range of dynamic motions demonstrate
that our approach outperforms state-of-the-art methods in terms of convergence
speed and the quality of learned motions. Notably, we showcase a physics-based
virtual character empowered by RFC that can perform highly agile ballet dance
moves such as pirouette, arabesque and jet\'e. Furthermore, we propose a
dual-policy control framework, where a kinematic policy and an RFC-based policy
work in tandem to synthesize multi-modal infinite-horizon human motions without
any task guidance or user input. Our approach is the first humanoid control
method that successfully learns from a large-scale human motion dataset
(Human3.6M) and generates diverse long-term motions. Code and videos are
available at https://www.ye-yuan.com/rfc.",arxiv
http://arxiv.org/abs/1711.10388v3,2018-07-11T17:20:08Z,2017-11-28T16:37:14Z,"Lose The Views: Limited Angle CT Reconstruction via Implicit Sinogram
  Completion","Computed Tomography (CT) reconstruction is a fundamental component to a wide
variety of applications ranging from security, to healthcare. The classical
techniques require measuring projections, called sinograms, from a full
180$^\circ$ view of the object. This is impractical in a limited angle
scenario, when the viewing angle is less than 180$^\circ$, which can occur due
to different factors including restrictions on scanning time, limited
flexibility of scanner rotation, etc. The sinograms obtained as a result, cause
existing techniques to produce highly artifact-laden reconstructions. In this
paper, we propose to address this problem through implicit sinogram completion,
on a challenging real world dataset containing scans of common checked-in
luggage. We propose a system, consisting of 1D and 2D convolutional neural
networks, that operates on a limited angle sinogram to directly produce the
best estimate of a reconstruction. Next, we use the x-ray transform on this
reconstruction to obtain a ""completed"" sinogram, as if it came from a full
180$^\circ$ measurement. We feed this to standard analytical and iterative
reconstruction techniques to obtain the final reconstruction. We show with
extensive experimentation that this combined strategy outperforms many
competitive baselines. We also propose a measure of confidence for the
reconstruction that enables a practitioner to gauge the reliability of a
prediction made by our network. We show that this measure is a strong indicator
of quality as measured by the PSNR, while not requiring ground truth at test
time. Finally, using a segmentation experiment, we show that our reconstruction
preserves the 3D structure of objects effectively.",arxiv
http://arxiv.org/abs/1808.04525v1,2018-08-14T05:13:23Z,2018-08-14T05:13:23Z,Discrete Structural Planning for Neural Machine Translation,"Structural planning is important for producing long sentences, which is a
missing part in current language generation models. In this work, we add a
planning phase in neural machine translation to control the coarse structure of
output sentences. The model first generates some planner codes, then predicts
real output words conditioned on them. The codes are learned to capture the
coarse structure of the target sentence. In order to obtain the codes, we
design an end-to-end neural network with a discretization bottleneck, which
predicts the simplified part-of-speech tags of target sentences. Experiments
show that the translation performance are generally improved by planning ahead.
We also find that translations with different structures can be obtained by
manipulating the planner codes.",arxiv
http://arxiv.org/abs/2008.02321v2,2021-02-25T03:04:37Z,2020-08-05T19:00:36Z,"Can I Pour into It? Robot Imagining Open Containability Affordance of
  Previously Unseen Objects via Physical Simulations","Open containers, i.e., containers without covers, are an important and
ubiquitous class of objects in human life. In this letter, we propose a novel
method for robots to ""imagine"" the open containability affordance of a
previously unseen object via physical simulations. We implement our imagination
method on a UR5 manipulator. The robot autonomously scans the object with an
RGB-D camera. The scanned 3D model is used for open containability imagination
which quantifies the open containability affordance by physically simulating
dropping particles onto the object and counting how many particles are retained
in it. This quantification is used for open-container vs. non-open-container
binary classification (hereafter referred to as open container classification).
If the object is classified as an open container, the robot further imagines
pouring into the object, again using physical simulations, to obtain the
pouring position and orientation for real robot autonomous pouring. We evaluate
our method on open container classification and autonomous pouring of granular
material on a dataset containing 130 previously unseen objects with 57 object
categories. Although our proposed method uses only 11 objects for simulation
calibration (training), its open container classification aligns well with
human judgements. In addition, our method endows the robot with the capability
to autonomously pour into the 55 containers in the dataset with a very high
success rate. We also compare to a deep learning method. Results show that our
method achieves the same performance as the deep learning method on open
container classification and outperforms it on autonomous pouring. Moreover,
our method is fully explainable.",arxiv
http://arxiv.org/abs/2109.11798v1,2021-09-24T08:11:34Z,2021-09-24T08:11:34Z,Adversarial Domain Feature Adaptation for Bronchoscopic Depth Estimation,"Depth estimation from monocular images is an important task in localization
and 3D reconstruction pipelines for bronchoscopic navigation. Various
supervised and self-supervised deep learning-based approaches have proven
themselves on this task for natural images. However, the lack of labeled data
and the bronchial tissue's feature-scarce texture make the utilization of these
methods ineffective on bronchoscopic scenes. In this work, we propose an
alternative domain-adaptive approach. Our novel two-step structure first trains
a depth estimation network with labeled synthetic images in a supervised
manner; then adopts an unsupervised adversarial domain feature adaptation
scheme to improve the performance on real images. The results of our
experiments show that the proposed method improves the network's performance on
real images by a considerable margin and can be employed in 3D reconstruction
pipelines.",arxiv
http://arxiv.org/abs/1708.09588v1,2017-08-31T07:01:21Z,2017-08-31T07:01:21Z,"Joint Separation and Denoising of Noisy Multi-talker Speech using
  Recurrent Neural Networks and Permutation Invariant Training","In this paper we propose to use utterance-level Permutation Invariant
Training (uPIT) for speaker independent multi-talker speech separation and
denoising, simultaneously. Specifically, we train deep bi-directional Long
Short-Term Memory (LSTM) Recurrent Neural Networks (RNNs) using uPIT, for
single-channel speaker independent multi-talker speech separation in multiple
noisy conditions, including both synthetic and real-life noise signals. We
focus our experiments on generalizability and noise robustness of models that
rely on various types of a priori knowledge e.g. in terms of noise type and
number of simultaneous speakers. We show that deep bi-directional LSTM RNNs
trained using uPIT in noisy environments can improve the Signal-to-Distortion
Ratio (SDR) as well as the Extended Short-Time Objective Intelligibility
(ESTOI) measure, on the speaker independent multi-talker speech separation and
denoising task, for various noise types and Signal-to-Noise Ratios (SNRs).
Specifically, we first show that LSTM RNNs can achieve large SDR and ESTOI
improvements, when evaluated using known noise types, and that a single model
is capable of handling multiple noise types with only a slight decrease in
performance. Furthermore, we show that a single LSTM RNN can handle both
two-speaker and three-speaker noisy mixtures, without a priori knowledge about
the exact number of speakers. Finally, we show that LSTM RNNs trained using
uPIT generalize well to noise types not seen during training.",arxiv
http://arxiv.org/abs/1911.03074v2,2020-03-05T17:02:11Z,2019-11-08T06:29:31Z,"Mapless Navigation among Dynamics with Social-safety-awareness: a
  reinforcement learning approach from 2D laser scans","We propose a method to tackle the problem of mapless collision-avoidance
navigation where humans are present using 2D laser scans. Our proposed method
uses ego-safety to measure collision from the robot's perspective while
social-safety to measure the impact of our robot's actions on surrounding
pedestrians. Specifically, the social-safety part predicts the intrusion impact
of our robot's action into the interaction area with surrounding humans. We
train the policy using reinforcement learning on a simple simulator and
directly evaluate the learned policy in Gazebo and real robot tests.
Experiments show the learned policy can be smoothly transferred without any
fine tuning. We observe that our method demonstrates time-efficient path
planning behavior with high success rate in mapless navigation tasks.
Furthermore, we test our method in a navigation among dynamic crowds task
considering both low and high volume traffic. Our learned policy demonstrates
cooperative behavior that actively drives our robot into traffic flows while
showing respect to nearby pedestrians. Evaluation videos are at
https://sites.google.com/view/ssw-batman",arxiv
http://arxiv.org/abs/2002.04109v1,2020-02-10T22:00:16Z,2020-02-10T22:00:16Z,"On Reward Shaping for Mobile Robot Navigation: A Reinforcement Learning
  and SLAM Based Approach","We present a map-less path planning algorithm based on Deep Reinforcement
Learning (DRL) for mobile robots navigating in unknown environment that only
relies on 40-dimensional raw laser data and odometry information. The planner
is trained using a reward function shaped based on the online knowledge of the
map of the training environment, obtained using grid-based Rao-Blackwellized
particle filter, in an attempt to enhance the obstacle awareness of the agent.
The agent is trained in a complex simulated environment and evaluated in two
unseen ones. We show that the policy trained using the introduced reward
function not only outperforms standard reward functions in terms of convergence
speed, by a reduction of 36.9\% of the iteration steps, and reduction of the
collision samples, but it also drastically improves the behaviour of the agent
in unseen environments, respectively by 23\% in a simpler workspace and by 45\%
in a more clustered one. Furthermore, the policy trained in the simulation
environment can be directly and successfully transferred to the real robot. A
video of our experiments can be found at: https://youtu.be/UEV7W6e6ZqI",arxiv
http://arxiv.org/abs/2011.03275v3,2021-03-24T16:02:25Z,2020-11-06T10:42:41Z,Sample-efficient Reinforcement Learning in Robotic Table Tennis,"Reinforcement learning (RL) has achieved some impressive recent successes in
various computer games and simulations. Most of these successes are based on
having large numbers of episodes from which the agent can learn. In typical
robotic applications, however, the number of feasible attempts is very limited.
In this paper we present a sample-efficient RL algorithm applied to the example
of a table tennis robot. In table tennis every stroke is different, with
varying placement, speed and spin. An accurate return therefore has to be found
depending on a high-dimensional continuous state space. To make learning in few
trials possible the method is embedded into our robot system. In this way we
can use a one-step environment. The state space depends on the ball at hitting
time (position, velocity, spin) and the action is the racket state
(orientation, velocity) at hitting. An actor-critic based deterministic policy
gradient algorithm was developed for accelerated learning. Our approach
performs competitively both in a simulation and on the real robot in a number
of challenging scenarios. Accurate results are obtained without pre-training in
under $200$ episodes of training. The video presenting our experiments is
available at https://youtu.be/uRAtdoL6Wpw.",arxiv
http://arxiv.org/abs/2008.01567v3,2021-02-20T01:06:06Z,2020-08-01T02:32:43Z,"Multi-Slice Fusion for Sparse-View and Limited-Angle 4D CT
  Reconstruction","Inverse problems spanning four or more dimensions such as space, time and
other independent parameters have become increasingly important.
State-of-the-art 4D reconstruction methods use model based iterative
reconstruction (MBIR), but depend critically on the quality of the prior
modeling. Recently, plug-and-play (PnP) methods have been shown to be an
effective way to incorporate advanced prior models using state-of-the-art
denoising algorithms. However, state-of-the-art denoisers such as BM4D and deep
convolutional neural networks (CNNs) are primarily available for 2D or 3D
images and extending them to higher dimensions is difficult due to algorithmic
complexity and the increased difficulty of effective training.
  In this paper, we present multi-slice fusion, a novel algorithm for 4D
reconstruction, based on the fusion of multiple low-dimensional denoisers. Our
approach uses multi-agent consensus equilibrium (MACE), an extension of
plug-and-play, as a framework for integrating the multiple lower-dimensional
models. We apply our method to 4D cone-beam X-ray CT reconstruction for non
destructive evaluation (NDE) of samples that are dynamically moving during
acquisition. We implement multi-slice fusion on distributed, heterogeneous
clusters in order to reconstruct large 4D volumes in reasonable time and
demonstrate the inherent parallelizable nature of the algorithm. We present
simulated and real experimental results on sparse-view and limited-angle CT
data to demonstrate that multi-slice fusion can substantially improve the
quality of reconstructions relative to traditional methods, while also being
practical to implement and train.",arxiv
http://arxiv.org/abs/2010.14742v1,2020-10-28T04:27:28Z,2020-10-28T04:27:28Z,"ElderSim: A Synthetic Data Generation Platform for Human Action
  Recognition in Eldercare Applications","To train deep learning models for vision-based action recognition of elders'
daily activities, we need large-scale activity datasets acquired under various
daily living environments and conditions. However, most public datasets used in
human action recognition either differ from or have limited coverage of elders'
activities in many aspects, making it challenging to recognize elders' daily
activities well by only utilizing existing datasets. Recently, such limitations
of available datasets have actively been compensated by generating synthetic
data from realistic simulation environments and using those data to train deep
learning models. In this paper, based on these ideas we develop ElderSim, an
action simulation platform that can generate synthetic data on elders' daily
activities. For 55 kinds of frequent daily activities of the elders, ElderSim
generates realistic motions of synthetic characters with various adjustable
data-generating options, and provides different output modalities including RGB
videos, two- and three-dimensional skeleton trajectories. We then generate KIST
SynADL, a large-scale synthetic dataset of elders' activities of daily living,
from ElderSim and use the data in addition to real datasets to train three
state-of the-art human action recognition models. From the experiments
following several newly proposed scenarios that assume different real and
synthetic dataset configurations for training, we observe a noticeable
performance improvement by augmenting our synthetic data. We also offer
guidance with insights for the effective utilization of synthetic data to help
recognize elders' daily activities.",arxiv
http://arxiv.org/abs/2003.05583v1,2020-03-12T02:40:36Z,2020-03-12T02:40:36Z,ZSTAD: Zero-Shot Temporal Activity Detection,"An integral part of video analysis and surveillance is temporal activity
detection, which means to simultaneously recognize and localize activities in
long untrimmed videos. Currently, the most effective methods of temporal
activity detection are based on deep learning, and they typically perform very
well with large scale annotated videos for training. However, these methods are
limited in real applications due to the unavailable videos about certain
activity classes and the time-consuming data annotation. To solve this
challenging problem, we propose a novel task setting called zero-shot temporal
activity detection (ZSTAD), where activities that have never been seen in
training can still be detected. We design an end-to-end deep network based on
R-C3D as the architecture for this solution. The proposed network is optimized
with an innovative loss function that considers the embeddings of activity
labels and their super-classes while learning the common semantics of seen and
unseen activities. Experiments on both the THUMOS14 and the Charades datasets
show promising performance in terms of detecting unseen activities.",arxiv
http://arxiv.org/abs/1804.03547v2,2018-04-11T15:20:31Z,2018-04-10T14:07:45Z,"A real-time and unsupervised face Re-Identification system for
  Human-Robot Interaction","In the context of Human-Robot Interaction (HRI), face Re-Identification (face
Re-ID) aims to verify if certain detected faces have already been observed by
robots. The ability of distinguishing between different users is crucial in
social robots as it will enable the robot to tailor the interaction strategy
toward the users' individual preferences. So far face recognition research has
achieved great success, however little attention has been paid to the realistic
applications of Face Re-ID in social robots. In this paper, we present an
effective and unsupervised face Re-ID system which simultaneously re-identifies
multiple faces for HRI. This Re-ID system employs Deep Convolutional Neural
Networks to extract features, and an online clustering algorithm to determine
the face's ID. Its performance is evaluated on two datasets: the TERESA video
dataset collected by the TERESA robot, and the YouTube Face Dataset (YTF
Dataset). We demonstrate that the optimised combination of techniques achieves
an overall 93.55% accuracy on TERESA dataset and an overall 90.41% accuracy on
YTF dataset. We have implemented the proposed method into a software module in
the HCI^2 Framework for it to be further integrated into the TERESA robot, and
has achieved real-time performance at 10~26 Frames per second.",arxiv
http://arxiv.org/abs/1910.08536v1,2019-10-17T21:38:11Z,2019-10-17T21:38:11Z,"LanCe: A Comprehensive and Lightweight CNN Defense Methodology against
  Physical Adversarial Attacks on Embedded Multimedia Applications","Recently, adversarial attacks can be applied to the physical world, causing
practical issues to various Convolutional Neural Networks (CNNs) powered
applications. Most existing physical adversarial attack defense works only
focus on eliminating explicit perturbation patterns from inputs, ignoring
interpretation to CNN's intrinsic vulnerability. Therefore, they lack the
expected versatility to different attacks and thereby depend on considerable
data processing costs. In this paper, we propose LanCe -- a comprehensive and
lightweight CNN defense methodology against different physical adversarial
attacks. By interpreting CNN's vulnerability, we find that non-semantic
adversarial perturbations can activate CNN with significantly abnormal
activations and even overwhelm other semantic input patterns' activations. We
improve the CNN recognition process by adding a self-verification stage to
detect the potential adversarial input with only one CNN inference cost. Based
on the detection result, we further propose a data recovery methodology to
defend the physical adversarial attacks. We apply such defense methodology into
both image and audio CNN recognition scenarios and analyze the computational
complexity for each scenario, respectively. Experiments show that our
methodology can achieve an average 91% successful rate for attack detection and
89% accuracy recovery. Moreover, it is at most 3x faster compared with the
state-of-the-art defense methods, making it feasible to resource-constrained
embedded systems, such as mobile devices.",arxiv
http://arxiv.org/abs/2107.10658v1,2021-07-21T12:03:27Z,2021-07-21T12:03:27Z,Digital Einstein Experience: Fast Text-to-Speech for Conversational AI,"We describe our approach to create and deliver a custom voice for a
conversational AI use-case. More specifically, we provide a voice for a Digital
Einstein character, to enable human-computer interaction within the digital
conversation experience. To create the voice which fits the context well, we
first design a voice character and we produce the recordings which correspond
to the desired speech attributes. We then model the voice. Our solution
utilizes Fastspeech 2 for log-scaled mel-spectrogram prediction from phonemes
and Parallel WaveGAN to generate the waveforms. The system supports a character
input and gives a speech waveform at the output. We use a custom dictionary for
selected words to ensure their proper pronunciation. Our proposed cloud
architecture enables for fast voice delivery, making it possible to talk to the
digital version of Albert Einstein in real-time.",arxiv
http://arxiv.org/abs/1905.13675v1,2019-05-31T15:31:52Z,2019-05-31T15:31:52Z,2.5D Image based Robotic Grasping,"We consider the problem of robotic grasping using depth + RGB information
sampling from a real sensor. we design an encoder-decoder neural network to
predict grasp policy in real time. This method can fuse the advantage of depth
image and RGB image at the same time and is robust for grasp and observation
height.We evaluate our method in a physical robotic system and propose an
open-loop algorithm to realize robotic grasp operation. We analyze the result
of experiment from multi-perspective and the result shows that our method is
competitive with the state-of-the-art in grasp performance, real-time and model
size. The video is available in https://youtu.be/Wxw_r5a8qV0",arxiv
http://arxiv.org/abs/1609.00086v1,2016-09-01T01:58:50Z,2016-09-01T01:58:50Z,"A novel online multi-label classifier for high-speed streaming data
  applications","In this paper, a high-speed online neural network classifier based on extreme
learning machines for multi-label classification is proposed. In multi-label
classification, each of the input data sample belongs to one or more than one
of the target labels. The traditional binary and multi-class classification
where each sample belongs to only one target class forms the subset of
multi-label classification. Multi-label classification problems are far more
complex than binary and multi-class classification problems, as both the number
of target labels and each of the target labels corresponding to each of the
input samples are to be identified. The proposed work exploits the high-speed
nature of the extreme learning machines to achieve real-time multi-label
classification of streaming data. A new threshold-based online sequential
learning algorithm is proposed for high speed and streaming data classification
of multi-label problems. The proposed method is experimented with six different
datasets from different application domains such as multimedia, text, and
biology. The hamming loss, accuracy, training time and testing time of the
proposed technique is compared with nine different state-of-the-art methods.
Experimental studies shows that the proposed technique outperforms the existing
multi-label classifiers in terms of performance and speed.",arxiv
http://arxiv.org/abs/1904.13325v2,2019-05-15T01:59:24Z,2019-04-30T15:41:05Z,Effective and Efficient Indexing in Cross-Modal Hashing-Based Datasets,"To overcome the barrier of storage and computation, the hashing technique has
been widely used for nearest neighbor search in multimedia retrieval
applications recently. Particularly, cross-modal retrieval that searches across
different modalities becomes an active but challenging problem. Although dozens
of cross-modal hashing algorithms are proposed to yield compact binary codes,
the exhaustive search is impractical for the real-time purpose, and Hamming
distance computation suffers inaccurate results. In this paper, we propose a
novel search method that utilizes a probability-based index scheme over binary
hash codes in cross-modal retrieval. The proposed hash code indexing scheme
exploits a few binary bits of the hash code as the index code. We construct an
inverted index table based on index codes and train a neural network to improve
the indexing accuracy and efficiency. Experiments are performed on two
benchmark datasets for retrieval across image and text modalities, where hash
codes are generated by three cross-modal hashing methods. Results show the
proposed method effectively boost the performance on these hash methods.",arxiv
http://arxiv.org/abs/1904.00198v1,2019-03-30T11:01:16Z,2019-03-30T11:01:16Z,Boundary Aware Multi-Focus Image Fusion Using Deep Neural Network,"Since it is usually difficult to capture an all-in-focus image of a 3D scene
directly, various multi-focus image fusion methods are employed to generate it
from several images focusing at different depths. However, the performance of
existing methods is barely satisfactory and often degrades for areas near the
focused/defocused boundary (FDB). In this paper, a boundary aware method using
deep neural network is proposed to overcome this problem. (1) Aiming to acquire
improved fusion images, a 2-channel deep network is proposed to better extract
the relative defocus information of the two source images. (2) After analyzing
the different situations for patches far away from and near the FDB, we use two
networks to handle them respectively. (3) To simulate the reality more
precisely, a new approach of dataset generation is designed. Experiments
demonstrate that the proposed method outperforms the state-of-the-art methods,
both qualitatively and quantitatively.",arxiv
http://arxiv.org/abs/2103.10380v2,2021-04-15T11:01:16Z,2021-03-18T17:09:12Z,FastNeRF: High-Fidelity Neural Rendering at 200FPS,"Recent work on Neural Radiance Fields (NeRF) showed how neural networks can
be used to encode complex 3D environments that can be rendered
photorealistically from novel viewpoints. Rendering these images is very
computationally demanding and recent improvements are still a long way from
enabling interactive rates, even on high-end hardware. Motivated by scenarios
on mobile and mixed reality devices, we propose FastNeRF, the first NeRF-based
system capable of rendering high fidelity photorealistic images at 200Hz on a
high-end consumer GPU. The core of our method is a graphics-inspired
factorization that allows for (i) compactly caching a deep radiance map at each
position in space, (ii) efficiently querying that map using ray directions to
estimate the pixel values in the rendered image. Extensive experiments show
that the proposed method is 3000 times faster than the original NeRF algorithm
and at least an order of magnitude faster than existing work on accelerating
NeRF, while maintaining visual quality and extensibility.",arxiv
http://arxiv.org/abs/1711.01666v2,2017-12-24T22:23:19Z,2017-11-05T22:01:57Z,"Label-driven weakly-supervised learning for multimodal deformable image
  registration","Spatially aligning medical images from different modalities remains a
challenging task, especially for intraoperative applications that require fast
and robust algorithms. We propose a weakly-supervised, label-driven formulation
for learning 3D voxel correspondence from higher-level label correspondence,
thereby bypassing classical intensity-based image similarity measures. During
training, a convolutional neural network is optimised by outputting a dense
displacement field (DDF) that warps a set of available anatomical labels from
the moving image to match their corresponding counterparts in the fixed image.
These label pairs, including solid organs, ducts, vessels, point landmarks and
other ad hoc structures, are only required at training time and can be
spatially aligned by minimising a cross-entropy function of the warped moving
label and the fixed label. During inference, the trained network takes a new
image pair to predict an optimal DDF, resulting in a fully-automatic,
label-free, real-time and deformable registration. For interventional
applications where large global transformation prevails, we also propose a
neural network architecture to jointly optimise the global- and local
displacements. Experiment results are presented based on cross-validating
registrations of 111 pairs of T2-weighted magnetic resonance images and 3D
transrectal ultrasound images from prostate cancer patients with a total of
over 4000 anatomical labels, yielding a median target registration error of 4.2
mm on landmark centroids and a median Dice of 0.88 on prostate glands.",arxiv
http://arxiv.org/abs/2004.01588v1,2020-04-03T14:27:16Z,2020-04-03T14:27:16Z,"HandVoxNet: Deep Voxel-Based Network for 3D Hand Shape and Pose
  Estimation from a Single Depth Map","3D hand shape and pose estimation from a single depth map is a new and
challenging computer vision problem with many applications. The
state-of-the-art methods directly regress 3D hand meshes from 2D depth images
via 2D convolutional neural networks, which leads to artefacts in the
estimations due to perspective distortions in the images. In contrast, we
propose a novel architecture with 3D convolutions trained in a
weakly-supervised manner. The input to our method is a 3D voxelized depth map,
and we rely on two hand shape representations. The first one is the 3D
voxelized grid of the shape which is accurate but does not preserve the mesh
topology and the number of mesh vertices. The second representation is the 3D
hand surface which is less accurate but does not suffer from the limitations of
the first representation. We combine the advantages of these two
representations by registering the hand surface to the voxelized hand shape. In
the extensive experiments, the proposed approach improves over the state of the
art by 47.8% on the SynHand5M dataset. Moreover, our augmentation policy for
voxelized depth maps further enhances the accuracy of 3D hand pose estimation
on real data. Our method produces visually more reasonable and realistic hand
shapes on NYU and BigHand2.2M datasets compared to the existing approaches.",arxiv
http://arxiv.org/abs/1509.02069v2,2017-02-09T14:25:13Z,2015-09-07T15:00:55Z,"EMMIXcskew: an R Package for the Fitting of a Mixture of Canonical
  Fundamental Skew t-Distributions","This paper presents an R package EMMIXcskew for the fitting of the canonical
fundamental skew t-distribution (CFUST) and finite mixtures of this
distribution (FM-CFUST) via maximum likelihood (ML). The CFUST distribution
provides a flexible family of models to handle non-normal data, with parameters
for capturing skewness and heavy-tails in the data. It formally encompasses the
normal, t, and skew-normal distributions as special and/or limiting cases. A
few other versions of the skew t-distributions are also nested within the CFUST
distribution. In this paper, an Expectation-Maximization (EM) algorithm is
described for computing the ML estimates of the parameters of the FM-CFUST
model, and different strategies for initializing the algorithm are discussed
and illustrated. The methodology is implemented in the EMMIXcskew package, and
examples are presented using two real datasets. The EMMIXcskew package contains
functions to fit the FM-CFUST model, including procedures for generating
different initial values. Additional features include random sample generation
and contour visualization in 2D and 3D.",arxiv
http://arxiv.org/abs/1805.01195v1,2018-05-03T09:59:45Z,2018-05-03T09:59:45Z,BirdNet: a 3D Object Detection Framework from LiDAR information,"Understanding driving situations regardless the conditions of the traffic
scene is a cornerstone on the path towards autonomous vehicles; however,
despite common sensor setups already include complementary devices such as
LiDAR or radar, most of the research on perception systems has traditionally
focused on computer vision. We present a LiDAR-based 3D object detection
pipeline entailing three stages. First, laser information is projected into a
novel cell encoding for bird's eye view projection. Later, both object location
on the plane and its heading are estimated through a convolutional neural
network originally designed for image processing. Finally, 3D oriented
detections are computed in a post-processing phase. Experiments on KITTI
dataset show that the proposed framework achieves state-of-the-art results
among comparable methods. Further tests with different LiDAR sensors in real
scenarios assess the multi-device capabilities of the approach.",arxiv
http://arxiv.org/abs/1807.06288v8,2018-09-25T07:41:47Z,2018-07-17T09:06:30Z,PointSeg: Real-Time Semantic Segmentation Based on 3D LiDAR Point Cloud,"In this paper, we propose PointSeg, a real-time end-to-end semantic
segmentation method for road-objects based on spherical images. We take the
spherical image, which is transformed from the 3D LiDAR point clouds, as input
of the convolutional neural networks (CNNs) to predict the point-wise semantic
map. To make PointSeg applicable on a mobile system, we build the model based
on the light-weight network, SqueezeNet, with several improvements. It
maintains a good balance between memory cost and prediction performance. Our
model is trained on spherical images and label masks projected from the KITTI
3D object detection dataset. Experiments show that PointSeg can achieve
competitive accuracy with 90fps on a single GPU 1080ti. which makes it quite
compatible for autonomous driving applications.",arxiv
http://arxiv.org/abs/2103.08737v2,2021-06-04T15:13:07Z,2021-03-15T21:51:04Z,"Growing 3D Artefacts and Functional Machines with Neural Cellular
  Automata","Neural Cellular Automata (NCAs) have been proven effective in simulating
morphogenetic processes, the continuous construction of complex structures from
very few starting cells. Recent developments in NCAs lie in the 2D domain,
namely reconstructing target images from a single pixel or infinitely growing
2D textures. In this work, we propose an extension of NCAs to 3D, utilizing 3D
convolutions in the proposed neural network architecture. Minecraft is selected
as the environment for our automaton since it allows the generation of both
static structures and moving machines. We show that despite their simplicity,
NCAs are capable of growing complex entities such as castles, apartment blocks,
and trees, some of which are composed of over 3,000 blocks. Additionally, when
trained for regeneration, the system is able to regrow parts of simple
functional machines, significantly expanding the capabilities of simulated
morphogenetic systems. The code for the experiment in this paper can be found
at: https://github.com/real-itu/3d-artefacts-nca.",arxiv
http://arxiv.org/abs/2108.04602v1,2021-08-10T11:17:05Z,2021-08-10T11:17:05Z,"Joint Multi-Object Detection and Tracking with Camera-LiDAR Fusion for
  Autonomous Driving","Multi-object tracking (MOT) with camera-LiDAR fusion demands accurate results
of object detection, affinity computation and data association in real time.
This paper presents an efficient multi-modal MOT framework with online joint
detection and tracking schemes and robust data association for autonomous
driving applications. The novelty of this work includes: (1) development of an
end-to-end deep neural network for joint object detection and correlation using
2D and 3D measurements; (2) development of a robust affinity computation module
to compute occlusion-aware appearance and motion affinities in 3D space; (3)
development of a comprehensive data association module for joint optimization
among detection confidences, affinities and start-end probabilities. The
experiment results on the KITTI tracking benchmark demonstrate the superior
performance of the proposed method in terms of both tracking accuracy and
processing speed.",arxiv
http://arxiv.org/abs/2101.07594v1,2021-01-19T12:42:58Z,2021-01-19T12:42:58Z,"Real-Time Limited-View CT Inpainting and Reconstruction with Dual Domain
  Based on Spatial Information","Low-dose Computed Tomography is a common issue in reality. Current reduction,
sparse sampling and limited-view scanning can all cause it. Between them,
limited-view CT is general in the industry due to inevitable mechanical and
physical limitation. However, limited-view CT can cause serious imaging problem
on account of its massive information loss. Thus, we should effectively utilize
the scant prior information to perform completion. It is an undeniable fact
that CT imaging slices are extremely dense, which leads to high continuity
between successive images. We realized that fully exploit the spatial
correlation between consecutive frames can significantly improve restoration
results in video inpainting. Inspired by this, we propose a deep learning-based
three-stage algorithm that hoist limited-view CT imaging quality based on
spatial information. In stage one, to better utilize prior information in the
Radon domain, we design an adversarial autoencoder to complement the Radon
data. In the second stage, a model is built to perform inpainting based on
spatial continuity in the image domain. At this point, we have roughly restored
the imaging, while its texture still needs to be finely repaired. Hence, we
propose a model to accurately restore the image in stage three, and finally
achieve an ideal inpainting result. In addition, we adopt FBP instead of
SART-TV to make our algorithm more suitable for real-time use. In the
experiment, we restore and reconstruct the Radon data that has been cut the
rear one-third part, they achieve PSNR of 40.209, SSIM of 0.943, while
precisely present the texture.",arxiv
http://arxiv.org/abs/1804.06504v2,2018-05-23T10:21:03Z,2018-04-17T23:36:12Z,Learning how to be robust: Deep polynomial regression,"Polynomial regression is a recurrent problem with a large number of
applications. In computer vision it often appears in motion analysis. Whatever
the application, standard methods for regression of polynomial models tend to
deliver biased results when the input data is heavily contaminated by outliers.
Moreover, the problem is even harder when outliers have strong structure.
Departing from problem-tailored heuristics for robust estimation of parametric
models, we explore deep convolutional neural networks. Our work aims to find a
generic approach for training deep regression models without the explicit need
of supervised annotation. We bypass the need for a tailored loss function on
the regression parameters by attaching to our model a differentiable hard-wired
decoder corresponding to the polynomial operation at hand. We demonstrate the
value of our findings by comparing with standard robust regression methods.
Furthermore, we demonstrate how to use such models for a real computer vision
problem, i.e., video stabilization. The qualitative and quantitative
experiments show that neural networks are able to learn robustness for general
polynomial regression, with results that well overpass scores of traditional
robust estimation methods.",arxiv
http://arxiv.org/abs/1910.13232v1,2019-10-29T12:50:40Z,2019-10-29T12:50:40Z,Detecting motorcycle helmet use with deep learning,"The continuous motorization of traffic has led to a sustained increase in the
global number of road related fatalities and injuries. To counter this,
governments are focusing on enforcing safe and law-abiding behavior in traffic.
However, especially in developing countries where the motorcycle is the main
form of transportation, there is a lack of comprehensive data on the
safety-critical behavioral metric of motorcycle helmet use. This lack of data
prohibits targeted enforcement and education campaigns which are crucial for
injury prevention. Hence, we have developed an algorithm for the automated
registration of motorcycle helmet usage from video data, using a deep learning
approach. Based on 91,000 annotated frames of video data, collected at multiple
observation sites in 7 cities across the country of Myanmar, we trained our
algorithm to detect active motorcycles, the number and position of riders on
the motorcycle, as well as their helmet use. An analysis of the algorithm's
accuracy on an annotated test data set, and a comparison to available
human-registered helmet use data reveals a high accuracy of our approach. Our
algorithm registers motorcycle helmet use rates with an accuracy of -4.4% and
+2.1% in comparison to a human observer, with minimal training for individual
observation sites. Without observation site specific training, the accuracy of
helmet use detection decreases slightly, depending on a number of factors. Our
approach can be implemented in existing roadside traffic surveillance
infrastructure and can facilitate targeted data-driven injury prevention
campaigns with real-time speed. Implications of the proposed method, as well as
measures that can further improve detection accuracy are discussed.",arxiv
http://arxiv.org/abs/2011.06294v11,2021-11-17T08:39:12Z,2020-11-12T10:12:06Z,"RIFE: Real-Time Intermediate Flow Estimation for Video Frame
  Interpolation","We propose RIFE, a Real-time Intermediate Flow Estimation algorithm for Video
Frame Interpolation (VFI). Many recent flow-based VFI methods first estimate
the bi-directional optical flows, then scale and reverse them to approximate
intermediate flows, leading to artifacts on motion boundaries and complex
pipelines. RIFE uses a neural network named IFNet that can directly estimate
the intermediate flows from coarse-to-fine with much better speed. We design a
privileged distillation scheme for training IFNet, resulting in a large
performance improvement. RIFE does not rely on pre-trained optical flow models
and can support arbitrary-timestep frame interpolation with the temporal
encoding input. Experiments demonstrate that RIFE achieves state-of-the-art
performance on several public benchmarks. Compared with the popular SuperSlomo
and DAIN methods, RIFE is 4--27 times faster and produces better results. The
code is available at https://github.com/hzwer/arXiv2020-RIFE.",arxiv
http://arxiv.org/abs/1812.04429v1,2018-12-11T14:32:20Z,2018-12-11T14:32:20Z,Face-Focused Cross-Stream Network for Deception Detection in Videos,"Automated deception detection (ADD) from real-life videos is a challenging
task. It specifically needs to address two problems: (1) Both face and body
contain useful cues regarding whether a subject is deceptive. How to
effectively fuse the two is thus key to the effectiveness of an ADD model. (2)
Real-life deceptive samples are hard to collect; learning with limited training
data thus challenges most deep learning based ADD models. In this work, both
problems are addressed. Specifically, for face-body multimodal learning, a
novel face-focused cross-stream network (FFCSN) is proposed. It differs
significantly from the popular two-stream networks in that: (a) face detection
is added into the spatial stream to capture the facial expressions explicitly,
and (b) correlation learning is performed across the spatial and temporal
streams for joint deep feature learning across both face and body. To address
the training data scarcity problem, our FFCSN model is trained with both meta
learning and adversarial learning. Extensive experiments show that our FFCSN
model achieves state-of-the-art results. Further, the proposed FFCSN model as
well as its robust training strategy are shown to be generally applicable to
other human-centric video analysis tasks such as emotion recognition from
user-generated videos.",arxiv
http://arxiv.org/abs/2103.05407v1,2021-03-09T13:08:19Z,2021-03-09T13:08:19Z,"Efficient Multi-Stage Video Denoising with Recurrent Spatio-Temporal
  Fusion","In recent years, methods based on deep learning have achieved unparalleled
performance at the cost of large computational complexity. In this work, we
propose an Efficient Multi-stage Video Denoising algorithm, called EMVD, to
drastically reduce the complexity while maintaining or even improving the
performance. First, a fusion stage reduces the noise through a recursive
combination of all past frames in the video. Then, a denoising stage removes
the noise in the fused frame. Finally, a refinement stage restores the missing
high frequency in the denoised frame. All stages operate on a transform-domain
representation obtained by learnable and invertible linear operators which
simultaneously increase accuracy and decrease complexity of the model. A single
loss on the final output is sufficient for successful convergence, hence making
EMVD easy to train. Experiments on real raw data demonstrate that EMVD
outperforms the state of the art when complexity is constrained, and even
remains competitive against methods whose complexities are several orders of
magnitude higher. The low complexity and memory requirements of EMVD enable
real-time video denoising on low-powered commercial SoC.",arxiv
http://arxiv.org/abs/2106.06007v1,2021-06-10T19:00:08Z,2021-06-10T19:00:08Z,"Overcoming Difficulty in Obtaining Dark-skinned Subjects for Remote-PPG
  by Synthetic Augmentation","Camera-based remote photoplethysmography (rPPG) provides a non-contact way to
measure physiological signals (e.g., heart rate) using facial videos. Recent
deep learning architectures have improved the accuracy of such physiological
measurement significantly, yet they are restricted by the diversity of the
annotated videos. The existing datasets MMSE-HR, AFRL, and UBFC-RPPG contain
roughly 10%, 0%, and 5% of dark-skinned subjects respectively. The unbalanced
training sets result in a poor generalization capability to unseen subjects and
lead to unwanted bias toward different demographic groups. In Western academia,
it is regrettably difficult in a university setting to collect data on these
dark-skinned subjects. Here we show a first attempt to overcome the lack of
dark-skinned subjects by synthetic augmentation. A joint optimization framework
is utilized to translate real videos from light-skinned subjects to dark skin
tones while retaining their pulsatile signals. In the experiment, our method
exhibits around 31% reduction in mean absolute error for the dark-skinned group
and 46% improvement on bias mitigation for all the groups, as compared with the
previous work trained with just real samples.",arxiv
http://arxiv.org/abs/2012.03325v1,2020-12-06T17:06:33Z,2020-12-06T17:06:33Z,EasyPBR: A Lightweight Physically-Based Renderer,"Modern rendering libraries provide unprecedented realism, producing real-time
photorealistic 3D graphics on commodity hardware. Visual fidelity, however,
comes at the cost of increased complexity and difficulty of usage, with many
rendering parameters requiring a deep understanding of the pipeline. We propose
EasyPBR as an alternative rendering library that strikes a balance between
ease-of-use and visual quality. EasyPBR consists of a deferred renderer that
implements recent state-of-the-art approaches in physically based rendering. It
offers an easy-to-use Python and C++ interface that allows high-quality images
to be created in only a few lines of code or directly through a graphical user
interface. The user can choose between fully controlling the rendering pipeline
or letting EasyPBR automatically infer the best parameters based on the current
scene composition. The EasyPBR library can help the community to more easily
leverage the power of current GPUs to create realistic images. These can then
be used as synthetic data for deep learning or for creating animations for
academic purposes.",arxiv
http://arxiv.org/abs/1901.01138v1,2019-01-04T15:05:32Z,2019-01-04T15:05:32Z,"Intelligent Intersection: Two-Stream Convolutional Networks for
  Real-time Near Accident Detection in Traffic Video","In Intelligent Transportation System, real-time systems that monitor and
analyze road users become increasingly critical as we march toward the smart
city era. Vision-based frameworks for Object Detection, Multiple Object
Tracking, and Traffic Near Accident Detection are important applications of
Intelligent Transportation System, particularly in video surveillance and etc.
Although deep neural networks have recently achieved great success in many
computer vision tasks, a uniformed framework for all the three tasks is still
challenging where the challenges multiply from demand for real-time
performance, complex urban setting, highly dynamic traffic event, and many
traffic movements. In this paper, we propose a two-stream Convolutional Network
architecture that performs real-time detection, tracking, and near accident
detection of road users in traffic video data. The two-stream model consists of
a spatial stream network for Object Detection and a temporal stream network to
leverage motion features for Multiple Object Tracking. We detect near accidents
by incorporating appearance features and motion features from two-stream
networks. Using aerial videos, we propose a Traffic Near Accident Dataset
(TNAD) covering various types of traffic interactions that is suitable for
vision-based traffic analysis tasks. Our experiments demonstrate the advantage
of our framework with an overall competitive qualitative and quantitative
performance at high frame rates on the TNAD dataset.",arxiv
http://arxiv.org/abs/2007.14432v1,2020-07-28T18:47:21Z,2020-07-28T18:47:21Z,"A Convolutional Neural Network for gaze preference detection: A
  potential tool for diagnostics of autism spectrum disorder in children","Early diagnosis of autism spectrum disorder (ASD) is known to improve the
quality of life of affected individuals. However, diagnosis is often delayed
even in wealthier countries including the US, largely due to the fact that gold
standard diagnostic tools such as the Autism Diagnostic Observation Schedule
(ADOS) and the Autism Diagnostic Interview-Revised (ADI-R) are time consuming
and require expertise to administer. This trend is even more pronounced lower
resources settings due to a lack of trained experts. As a result, alternative,
less technical methods that leverage the unique ways in which children with ASD
react to visual stimulation in a controlled environment have been developed to
help facilitate early diagnosis. Previous studies have shown that, when exposed
to a video that presents both social and abstract scenes side by side, a child
with ASD will focus their attention towards the abstract images on the screen
to a greater extent than a child without ASD. Such differential responses make
it possible to implement an algorithm for the rapid diagnosis of ASD based on
eye tracking against different visual stimuli. Here we propose a convolutional
neural network (CNN) algorithm for gaze prediction using images extracted from
a one-minute stimulus video. Our model achieved a high accuracy rate and
robustness for prediction of gaze direction with independent persons and
employing a different camera than the one used during testing. In addition to
this, the proposed algorithm achieves a fast response time, providing a near
real-time evaluation of ASD. Thereby, by applying the proposed method, we could
significantly reduce the diagnosis time and facilitate the diagnosis of ASD in
low resource regions.",arxiv
http://arxiv.org/abs/1806.05117v1,2018-06-13T15:41:34Z,2018-06-13T15:41:34Z,"Learning to Shoot in First Person Shooter Games by Stabilizing Actions
  and Clustering Rewards for Reinforcement Learning","While reinforcement learning (RL) has been applied to turn-based board games
for many years, more complex games involving decision-making in real-time are
beginning to receive more attention. A challenge in such environments is that
the time that elapses between deciding to take an action and receiving a reward
based on its outcome can be longer than the interval between successive
decisions. We explore this in the context of a non-player character (NPC) in a
modern first-person shooter game. Such games take place in 3D environments
where players, both human and computer-controlled, compete by engaging in
combat and completing task objectives. We investigate the use of RL to enable
NPCs to gather experience from game-play and improve their shooting skill over
time from a reward signal based on the damage caused to opponents. We propose a
new method for RL updates and reward calculations, in which the updates are
carried out periodically, after each shooting encounter has ended, and a new
weighted-reward mechanism is used which increases the reward applied to actions
that lead to damaging the opponent in successive hits in what we term ""hit
clusters"".",arxiv
http://arxiv.org/abs/2012.09242v1,2020-12-16T20:14:41Z,2020-12-16T20:14:41Z,"S3CNet: A Sparse Semantic Scene Completion Network for LiDAR Point
  Clouds","With the increasing reliance of self-driving and similar robotic systems on
robust 3D vision, the processing of LiDAR scans with deep convolutional neural
networks has become a trend in academia and industry alike. Prior attempts on
the challenging Semantic Scene Completion task - which entails the inference of
dense 3D structure and associated semantic labels from ""sparse"" representations
- have been, to a degree, successful in small indoor scenes when provided with
dense point clouds or dense depth maps often fused with semantic segmentation
maps from RGB images. However, the performance of these systems drop
drastically when applied to large outdoor scenes characterized by dynamic and
exponentially sparser conditions. Likewise, processing of the entire sparse
volume becomes infeasible due to memory limitations and workarounds introduce
computational inefficiency as practitioners are forced to divide the overall
volume into multiple equal segments and infer on each individually, rendering
real-time performance impossible. In this work, we formulate a method that
subsumes the sparsity of large-scale environments and present S3CNet, a sparse
convolution based neural network that predicts the semantically completed scene
from a single, unified LiDAR point cloud. We show that our proposed method
outperforms all counterparts on the 3D task, achieving state-of-the art results
on the SemanticKITTI benchmark. Furthermore, we propose a 2D variant of S3CNet
with a multi-view fusion strategy to complement our 3D network, providing
robustness to occlusions and extreme sparsity in distant regions. We conduct
experiments for the 2D semantic scene completion task and compare the results
of our sparse 2D network against several leading LiDAR segmentation models
adapted for bird's eye view segmentation on two open-source datasets.",arxiv
http://arxiv.org/abs/2008.10710v2,2021-01-05T04:03:55Z,2020-08-24T21:14:13Z,"Exploit Camera Raw Data for Video Super-Resolution via Hidden Markov
  Model Inference","To the best of our knowledge, the existing deep-learning-based Video
Super-Resolution (VSR) methods exclusively make use of videos produced by the
Image Signal Processor (ISP) of the camera system as inputs. Such methods are
1) inherently suboptimal due to information loss incurred by non-invertible
operations in ISP, and 2) inconsistent with the real imaging pipeline where VSR
in fact serves as a pre-processing unit of ISP. To address this issue, we
propose a new VSR method that can directly exploit camera sensor data,
accompanied by a carefully built Raw Video Dataset (RawVD) for training,
validation, and testing. This method consists of a Successive Deep Inference
(SDI) module and a reconstruction module, among others. The SDI module is
designed according to the architectural principle suggested by a canonical
decomposition result for Hidden Markov Model (HMM) inference; it estimates the
target high-resolution frame by repeatedly performing pairwise feature fusion
using deformable convolutions. The reconstruction module, built with
elaborately designed Attention-based Residual Dense Blocks (ARDBs), serves the
purpose of 1) refining the fused feature and 2) learning the color information
needed to generate a spatial-specific transformation for accurate color
correction. Extensive experiments demonstrate that owing to the informativeness
of the camera raw data, the effectiveness of the network architecture, and the
separation of super-resolution and color correction processes, the proposed
method achieves superior VSR results compared to the state-of-the-art and can
be adapted to any specific camera-ISP. Code and dataset are available at
https://github.com/proteus1991/RawVSR.",arxiv
http://arxiv.org/abs/2107.13273v1,2021-07-28T11:15:04Z,2021-07-28T11:15:04Z,Rank-based verification for long-term face tracking in crowded scenes,"Most current multi-object trackers focus on short-term tracking, and are
based on deep and complex systems that often cannot operate in real-time,
making them impractical for video-surveillance. In this paper we present a
long-term, multi-face tracking architecture conceived for working in crowded
contexts where faces are often the only visible part of a person. Our system
benefits from advances in the fields of face detection and face recognition to
achieve long-term tracking, and is particularly unconstrained to the motion and
occlusions of people. It follows a tracking-by-detection approach, combining a
fast short-term visual tracker with a novel online tracklet reconnection
strategy grounded on rank-based face verification. The proposed rank-based
constraint favours higher inter-class distance among tracklets, and reduces the
propagation of errors due to wrong reconnections. Additionally, a correction
module is included to correct past assignments with no extra computational
cost. We present a series of experiments introducing novel specialized metrics
for the evaluation of long-term tracking capabilities, and publicly release a
video dataset with 10 manually annotated videos and a total length of 8' 54"".
Our findings validate the robustness of each of the proposed modules, and
demonstrate that, in these challenging contexts, our approach yields up to 50%
longer tracks than state-of-the-art deep learning trackers.",arxiv
http://arxiv.org/abs/1801.02108v2,2018-06-07T14:44:00Z,2018-01-07T01:03:25Z,SBNet: Sparse Blocks Network for Fast Inference,"Conventional deep convolutional neural networks (CNNs) apply convolution
operators uniformly in space across all feature maps for hundreds of layers -
this incurs a high computational cost for real-time applications. For many
problems such as object detection and semantic segmentation, we are able to
obtain a low-cost computation mask, either from a priori problem knowledge, or
from a low-resolution segmentation network. We show that such computation masks
can be used to reduce computation in the high-resolution main network. Variants
of sparse activation CNNs have previously been explored on small-scale tasks
and showed no degradation in terms of object classification accuracy, but often
measured gains in terms of theoretical FLOPs without realizing a practical
speed-up when compared to highly optimized dense convolution implementations.
In this work, we leverage the sparsity structure of computation masks and
propose a novel tiling-based sparse convolution algorithm. We verified the
effectiveness of our sparse CNN on LiDAR-based 3D object detection, and we
report significant wall-clock speed-ups compared to dense convolution without
noticeable loss of accuracy.",arxiv
http://arxiv.org/abs/1905.11299v1,2019-05-27T15:32:59Z,2019-05-27T15:32:59Z,"ImgSensingNet: UAV Vision Guided Aerial-Ground Air Quality Sensing
  System","Given the increasingly serious air pollution problem, the monitoring of air
quality index (AQI) in urban areas has drawn considerable attention. This paper
presents ImgSensingNet, a vision guided aerial-ground sensing system, for
fine-grained air quality monitoring and forecasting using the fusion of haze
images taken by the unmanned-aerial-vehicle (UAV) and the AQI data collected by
an on-ground three-dimensional (3D) wireless sensor network (WSN).
Specifically, ImgSensingNet first leverages the computer vision technique to
tell the AQI scale in different regions from the taken haze images, where
haze-relevant features and a deep convolutional neural network (CNN) are
designed for direct learning between haze images and corresponding AQI scale.
Based on the learnt AQI scale, ImgSensingNet determines whether to wake up
on-ground wireless sensors for small-scale AQI monitoring and inference, which
can greatly reduce the energy consumption of the system. An entropy-based model
is employed for accurate real-time AQI inference at unmeasured locations and
future air quality distribution forecasting. We implement and evaluate
ImgSensingNet on two university campuses since Feb. 2018, and has collected
17,630 photos and 2.6 millions of AQI data samples. Experimental results
confirm that ImgSensingNet can achieve higher inference accuracy while greatly
reduce the energy consumption, compared to state-of-the-art AQI monitoring
approaches.",arxiv
http://arxiv.org/abs/1908.06571v2,2019-10-13T15:37:29Z,2019-08-19T03:14:00Z,PolyGAN: High-Order Polynomial Generators,"Generative Adversarial Networks (GANs) have become the gold standard when it
comes to learning generative models for high-dimensional distributions. Since
their advent, numerous variations of GANs have been introduced in the
literature, primarily focusing on utilization of novel loss functions,
optimization/regularization strategies and network architectures. In this
paper, we turn our attention to the generator and investigate the use of
high-order polynomials as an alternative class of universal function
approximators. Concretely, we propose PolyGAN, where we model the data
generator by means of a high-order polynomial whose unknown parameters are
naturally represented by high-order tensors. We introduce two tensor
decompositions that significantly reduce the number of parameters and show how
they can be efficiently implemented by hierarchical neural networks that only
employ linear/convolutional blocks. We exhibit for the first time that by using
our approach a GAN generator can approximate the data distribution without
using any activation functions. Thorough experimental evaluation on both
synthetic and real data (images and 3D point clouds) demonstrates the merits of
PolyGAN against the state of the art.",arxiv
http://arxiv.org/abs/2102.03326v1,2021-02-05T18:14:36Z,2021-02-05T18:14:36Z,"Fusion of neural networks, for LIDAR-based evidential road mapping","LIDAR sensors are usually used to provide autonomous vehicles with 3D
representations of their environment. In ideal conditions, geometrical models
could detect the road in LIDAR scans, at the cost of a manual tuning of
numerical constraints, and a lack of flexibility. We instead propose an
evidential pipeline, to accumulate road detection results obtained from neural
networks. First, we introduce RoadSeg, a new convolutional architecture that is
optimized for road detection in LIDAR scans. RoadSeg is used to classify
individual LIDAR points as either belonging to the road, or not. Yet, such
point-level classification results need to be converted into a dense
representation, that can be used by an autonomous vehicle. We thus secondly
present an evidential road mapping algorithm, that fuses consecutive road
detection results. We benefitted from a reinterpretation of logistic
classifiers, which can be seen as generating a collection of simple evidential
mass functions. An evidential grid map that depicts the road can then be
obtained, by projecting the classification results from RoadSeg into grid
cells, and by handling moving objects via conflict analysis. The system was
trained and evaluated on real-life data. A python implementation maintains a 10
Hz framerate. Since road labels were needed for training, a soft labelling
procedure, relying lane-level HD maps, was used to generate coarse training and
validation sets. An additional test set was manually labelled for evaluation
purposes. So as to reach satisfactory results, the system fuses road detection
results obtained from three variants of RoadSeg, processing different LIDAR
features.",arxiv
http://arxiv.org/abs/2105.00368v2,2021-05-29T22:41:12Z,2021-05-02T01:09:13Z,"MarkerPose: Robust Real-time Planar Target Tracking for Accurate Stereo
  Pose Estimation","Despite the attention marker-less pose estimation has attracted in recent
years, marker-based approaches still provide unbeatable accuracy under
controlled environmental conditions. Thus, they are used in many fields such as
robotics or biomedical applications but are primarily implemented through
classical approaches, which require lots of heuristics and parameter tuning for
reliable performance under different environments. In this work, we propose
MarkerPose, a robust, real-time pose estimation system based on a planar target
of three circles and a stereo vision system. MarkerPose is meant for
high-accuracy pose estimation applications. Our method consists of two deep
neural networks for marker point detection. A SuperPoint-like network for
pixel-level accuracy keypoint localization and classification, and we introduce
EllipSegNet, a lightweight ellipse segmentation network for sub-pixel-level
accuracy keypoint detection. The marker's pose is estimated through stereo
triangulation. The target point detection is robust to low lighting and motion
blur conditions. We compared MarkerPose with a detection method based on
classical computer vision techniques using a robotic arm for validation. The
results show our method provides better accuracy than the classical technique.
Finally, we demonstrate the suitability of MarkerPose in a 3D freehand
ultrasound system, which is an application where highly accurate pose
estimation is required. Code is available in Python and C++ at
https://github.com/jhacsonmeza/MarkerPose.",arxiv
http://arxiv.org/abs/1906.05378v2,2019-12-26T17:59:44Z,2019-06-12T21:15:12Z,Eye Contact Correction using Deep Neural Networks,"In a typical video conferencing setup, it is hard to maintain eye contact
during a call since it requires looking into the camera rather than the
display. We propose an eye contact correction model that restores the eye
contact regardless of the relative position of the camera and display. Unlike
previous solutions, our model redirects the gaze from an arbitrary direction to
the center without requiring a redirection angle or camera/display/user
geometry as inputs. We use a deep convolutional neural network that inputs a
monocular image and produces a vector field and a brightness map to correct the
gaze. We train this model in a bi-directional way on a large set of
synthetically generated photorealistic images with perfect labels. The learned
model is a robust eye contact corrector which also predicts the input gaze
implicitly at no additional cost. Our system is primarily designed to improve
the quality of video conferencing experience. Therefore, we use a set of
control mechanisms to prevent creepy results and to ensure a smooth and natural
video conferencing experience. The entire eye contact correction system runs
end-to-end in real-time on a commodity CPU and does not require any dedicated
hardware, making our solution feasible for a variety of devices.",arxiv
http://arxiv.org/abs/2104.10891v1,2021-04-22T06:43:02Z,2021-04-22T06:43:02Z,"Computer Vision-based Social Distancing Surveillance Solution with
  Optional Automated Camera Calibration for Large Scale Deployment","Social distancing has been suggested as one of the most effective measures to
break the chain of viral transmission in the current COVID-19 pandemic. We
herein describe a computer vision-based AI-assisted solution to aid compliance
with social distancing norms. The solution consists of modules to detect and
track people and to identify distance violations. It provides the flexibility
to choose between a tool-based mode or an automated mode of camera calibration,
making the latter suitable for large-scale deployments. In this paper, we
discuss different metrics to assess the risk associated with social distancing
violations and how we can differentiate between transient or persistent
violations. Our proposed solution performs satisfactorily under different test
scenarios, processes video feed at real-time speed as well as addresses data
privacy regulations by blurring faces of detected people, making it ideal for
deployments.",arxiv
http://arxiv.org/abs/2107.05916v2,2021-10-21T08:04:53Z,2021-07-13T08:34:44Z,"Towards Automatic Instrumentation by Learning to Separate Parts in
  Symbolic Multitrack Music","Modern keyboards allow a musician to play multiple instruments at the same
time by assigning zones -- fixed pitch ranges of the keyboard -- to different
instruments. In this paper, we aim to further extend this idea and examine the
feasibility of automatic instrumentation -- dynamically assigning instruments
to notes in solo music during performance. In addition to the online,
real-time-capable setting for performative use cases, automatic instrumentation
can also find applications in assistive composing tools in an offline setting.
Due to the lack of paired data of original solo music and their full
arrangements, we approach automatic instrumentation by learning to separate
parts (e.g., voices, instruments and tracks) from their mixture in symbolic
multitrack music, assuming that the mixture is to be played on a keyboard. We
frame the task of part separation as a sequential multi-class classification
problem and adopt machine learning to map sequences of notes into sequences of
part labels. To examine the effectiveness of our proposed models, we conduct a
comprehensive empirical evaluation over four diverse datasets of different
genres and ensembles -- Bach chorales, string quartets, game music and pop
music. Our experiments show that the proposed models outperform various
baselines. We also demonstrate the potential for our proposed models to produce
alternative convincing instrumentations for an existing arrangement by
separating its mixture into parts. All source code and audio samples can be
found at https://salu133445.github.io/arranger/ .",arxiv
http://arxiv.org/abs/1812.06408v1,2018-12-16T07:27:26Z,2018-12-16T07:27:26Z,"Human Pose and Path Estimation from Aerial Video using Dynamic
  Classifier Selection","We consider the problem of estimating human pose and trajectory by an aerial
robot with a monocular camera in near real time. We present a preliminary
solution whose distinguishing feature is a dynamic classifier selection
architecture. In our solution, each video frame is corrected for perspective
using projective transformation. Then, two alternative feature sets are used:
(i) Histogram of Oriented Gradients (HOG) of the silhouette, (ii) Convolutional
Neural Network (CNN) features of the RGB image. The features (HOG or CNN) are
classified using a dynamic classifier. A class is defined as a pose-viewpoint
pair, and a total of 64 classes are defined to represent a forward walking and
turning gait sequence. Our solution provides three main advantages: (i)
Classification is efficient due to dynamic selection (4-class vs. 64-class
classification). (ii) Classification errors are confined to neighbors of the
true view-points. (iii) The robust temporal relationship between poses is used
to resolve the left-right ambiguities of human silhouettes. Experiments
conducted on both fronto-parallel videos and aerial videos confirm our solution
can achieve accurate pose and trajectory estimation for both scenarios. We
found using HOG features provides higher accuracy than using CNN features. For
example, applying the HOG-based variant of our scheme to the 'walking on a
figure 8-shaped path' dataset (1652 frames) achieved estimation accuracies of
99.6% for viewpoints and 96.2% for number of poses.",arxiv
http://arxiv.org/abs/1610.00759v1,2016-10-03T21:23:13Z,2016-10-03T21:23:13Z,Prediction of Manipulation Actions,"Looking at a person's hands one often can tell what the person is going to do
next, how his/her hands are moving and where they will be, because an actor's
intentions shape his/her movement kinematics during action execution.
Similarly, active systems with real-time constraints must not simply rely on
passive video-segment classification, but they have to continuously update
their estimates and predict future actions. In this paper, we study the
prediction of dexterous actions. We recorded from subjects performing different
manipulation actions on the same object, such as ""squeezing"", ""flipping"",
""washing"", ""wiping"" and ""scratching"" with a sponge. In psychophysical
experiments, we evaluated human observers' skills in predicting actions from
video sequences of different length, depicting the hand movement in the
preparation and execution of actions before and after contact with the object.
We then developed a recurrent neural network based method for action prediction
using as input patches around the hand. We also used the same formalism to
predict the forces on the finger tips using for training synchronized video and
force data streams. Evaluations on two new datasets showed that our system
closely matches human performance in the recognition task, and demonstrate the
ability of our algorithm to predict what and how a dexterous action is
performed.",arxiv
http://arxiv.org/abs/2104.11653v2,2021-05-03T07:14:19Z,2021-04-23T15:07:58Z,"MULTICAST: MULTI Confirmation-level Alarm SysTem based on CNN and LSTM
  to mitigate false alarms for handgun detection in video-surveillance","Despite the constant advances in computer vision, integrating modern
single-image detectors in real-time handgun alarm systems in video-surveillance
is still debatable. Using such detectors still implies a high number of false
alarms and false negatives. In this context, most existent studies select one
of the latest single-image detectors and train it on a better dataset or use
some pre-processing, post-processing or data-fusion approach to further reduce
false alarms. However, none of these works tried to exploit the temporal
information present in the videos to mitigate false detections. This paper
presents a new system, called MULTI Confirmation-level Alarm SysTem based on
Convolutional Neural Networks (CNN) and Long Short Term Memory networks (LSTM)
(MULTICAST), that leverages not only the spacial information but also the
temporal information existent in the videos for a more reliable handgun
detection. MULTICAST consists of three stages, i) a handgun detection stage,
ii) a CNN-based spacial confirmation stage and iii) LSTM-based temporal
confirmation stage. The temporal confirmation stage uses the positions of the
detected handgun in previous instants to predict its trajectory in the next
frame. Our experiments show that MULTICAST reduces by 80% the number of false
alarms with respect to Faster R-CNN based-single-image detector, which makes it
more useful in providing more effective and rapid security responses.",arxiv
http://arxiv.org/abs/1908.00382v1,2019-08-01T13:27:41Z,2019-08-01T13:27:41Z,"Cascaded Context Pyramid for Full-Resolution 3D Semantic Scene
  Completion","Semantic Scene Completion (SSC) aims to simultaneously predict the volumetric
occupancy and semantic category of a 3D scene. It helps intelligent devices to
understand and interact with the surrounding scenes. Due to the high-memory
requirement, current methods only produce low-resolution completion
predictions, and generally lose the object details. Furthermore, they also
ignore the multi-scale spatial contexts, which play a vital role for the 3D
inference. To address these issues, in this work we propose a novel deep
learning framework, named Cascaded Context Pyramid Network (CCPNet), to jointly
infer the occupancy and semantic labels of a volumetric 3D scene from a single
depth image. The proposed CCPNet improves the labeling coherence with a
cascaded context pyramid. Meanwhile, based on the low-level features, it
progressively restores the fine-structures of objects with Guided Residual
Refinement (GRR) modules. Our proposed framework has three outstanding
advantages: (1) it explicitly models the 3D spatial context for performance
improvement; (2) full-resolution 3D volumes are produced with
structure-preserving details; (3) light-weight models with low-memory
requirements are captured with a good extensibility. Extensive experiments
demonstrate that in spite of taking a single-view depth map, our proposed
framework can generate high-quality SSC results, and outperforms
state-of-the-art approaches on both the synthetic SUNCG and real NYU datasets.",arxiv
http://arxiv.org/abs/2111.00190v1,2021-10-30T06:46:44Z,2021-10-30T06:46:44Z,"Leveraging SE(3) Equivariance for Self-Supervised Category-Level Object
  Pose Estimation","Category-level object pose estimation aims to find 6D object poses of
previously unseen object instances from known categories without access to
object CAD models. To reduce the huge amount of pose annotations needed for
category-level learning, we propose for the first time a self-supervised
learning framework to estimate category-level 6D object pose from single 3D
point clouds.During training, our method assumes no ground-truth pose
annotations, no CAD models, and no multi-view supervision. The key to our
method is to disentangle shape and pose through an invariant shape
reconstruction module and an equivariant pose estimation module, empowered by
SE(3) equivariant point cloud networks.The invariant shape reconstruction
module learns to perform aligned reconstructions, yielding a category-level
reference frame without using any annotations. In addition, the equivariant
pose estimation module achieves category-level pose estimation accuracy that is
comparable to some fully supervised methods. Extensive experiments demonstrate
the effectiveness of our approach on both complete and partial depth point
clouds from the ModelNet40 benchmark, and on real depth point clouds from the
NOCS-REAL 275 dataset. The project page with code and visualizations can be
found at: https://dragonlong.github.io/equi-pose.",arxiv
http://arxiv.org/abs/2003.00637v3,2020-03-16T04:27:33Z,2020-03-02T03:04:13Z,"A Novel Recurrent Encoder-Decoder Structure for Large-Scale Multi-view
  Stereo Reconstruction from An Open Aerial Dataset","A great deal of research has demonstrated recently that multi-view stereo
(MVS) matching can be solved with deep learning methods. However, these efforts
were focused on close-range objects and only a very few of the deep
learning-based methods were specifically designed for large-scale 3D urban
reconstruction due to the lack of multi-view aerial image benchmarks. In this
paper, we present a synthetic aerial dataset, called the WHU dataset, we
created for MVS tasks, which, to our knowledge, is the first large-scale
multi-view aerial dataset. It was generated from a highly accurate 3D digital
surface model produced from thousands of real aerial images with precise camera
parameters. We also introduce in this paper a novel network, called RED-Net,
for wide-range depth inference, which we developed from a recurrent
encoder-decoder structure to regularize cost maps across depths and a 2D fully
convolutional network as framework. RED-Net's low memory requirements and high
performance make it suitable for large-scale and highly accurate 3D Earth
surface reconstruction. Our experiments confirmed that not only did our method
exceed the current state-of-the-art MVS methods by more than 50% mean absolute
error (MAE) with less memory and computational cost, but its efficiency as
well. It outperformed one of the best commercial software programs based on
conventional methods, improving their efficiency 16 times over. Moreover, we
proved that our RED-Net model pre-trained on the synthetic WHU dataset can be
efficiently transferred to very different multi-view aerial image datasets
without any fine-tuning. Dataset are available at http://gpcv.whu.edu.cn/data.",arxiv
http://arxiv.org/abs/1708.09580v1,2017-08-31T06:26:42Z,2017-08-31T06:26:42Z,"Fast Landmark Localization with 3D Component Reconstruction and CNN for
  Cross-Pose Recognition","Two approaches are proposed for cross-pose face recognition, one is based on
the 3D reconstruction of facial components and the other is based on the deep
Convolutional Neural Network (CNN). Unlike most 3D approaches that consider
holistic faces, the proposed approach considers 3D facial components. It
segments a 2D gallery face into components, reconstructs the 3D surface for
each component, and recognizes a probe face by component features. The
segmentation is based on the landmarks located by a hierarchical algorithm that
combines the Faster R-CNN for face detection and the Reduced Tree Structured
Model for landmark localization. The core part of the CNN-based approach is a
revised VGG network. We study the performances with different settings on the
training set, including the synthesized data from 3D reconstruction, the
real-life data from an in-the-wild database, and both types of data combined.
We investigate the performances of the network when it is employed as a
classifier or designed as a feature extractor. The two recognition approaches
and the fast landmark localization are evaluated in extensive experiments, and
compared to stateof-the-art methods to demonstrate their efficacy.",arxiv
http://arxiv.org/abs/2007.12140v3,2021-04-08T17:51:33Z,2020-07-23T17:11:48Z,"HITNet: Hierarchical Iterative Tile Refinement Network for Real-time
  Stereo Matching","This paper presents HITNet, a novel neural network architecture for real-time
stereo matching. Contrary to many recent neural network approaches that operate
on a full cost volume and rely on 3D convolutions, our approach does not
explicitly build a volume and instead relies on a fast multi-resolution
initialization step, differentiable 2D geometric propagation and warping
mechanisms to infer disparity hypotheses. To achieve a high level of accuracy,
our network not only geometrically reasons about disparities but also infers
slanted plane hypotheses allowing to more accurately perform geometric warping
and upsampling operations. Our architecture is inherently multi-resolution
allowing the propagation of information across different levels. Multiple
experiments prove the effectiveness of the proposed approach at a fraction of
the computation required by state-of-the-art methods. At the time of writing,
HITNet ranks 1st-3rd on all the metrics published on the ETH3D website for two
view stereo, ranks 1st on most of the metrics among all the end-to-end learning
approaches on Middlebury-v3, ranks 1st on the popular KITTI 2012 and 2015
benchmarks among the published methods faster than 100ms.",arxiv
http://arxiv.org/abs/2106.14742v1,2021-06-28T14:17:22Z,2021-06-28T14:17:22Z,TENT: Tensorized Encoder Transformer for Temperature Forecasting,"Reliable weather forecasting is of great importance in science, business and
society. The best performing data-driven models for weather prediction tasks
rely on recurrent or convolutional neural networks, where some of which
incorporate attention mechanisms. In this work, we introduce a new model based
on the Transformer architecture for weather forecasting. The proposed Tensorial
Encoder Transformer (TENT) model is equipped with tensorial attention and thus
it exploits the spatiotemporal structure of weather data by processing it in
multidimensional tensorial format. We show that compared to the encoder part of
the original transformer and 3D convolutional neural networks, the proposed
TENT model can better model the underlying complex pattern of weather data for
the studied temperature prediction task. Experiments on two real-life weather
datasets are performed. The datasets consist of historical measurements from
USA, Canada and European cities. The first dataset contains hourly measurements
of weather attributes for 30 cities in USA and Canada from October 2012 to
November 2017. The second dataset contains daily measurements of weather
attributes of 18 cities across Europe from May 2005 to April 2020. We use
attention scores calculated from our attention mechanism to shed light on the
decision-making process of our model and have insight knowledge on the most
important cities for the task.",arxiv
http://arxiv.org/abs/2107.05997v2,2021-09-30T18:00:04Z,2021-07-13T11:25:54Z,"Scalable, Axiomatic Explanations of Deep Alzheimer's Diagnosis from
  Heterogeneous Data","Deep Neural Networks (DNNs) have an enormous potential to learn from complex
biomedical data. In particular, DNNs have been used to seamlessly fuse
heterogeneous information from neuroanatomy, genetics, biomarkers, and
neuropsychological tests for highly accurate Alzheimer's disease diagnosis. On
the other hand, their black-box nature is still a barrier for the adoption of
such a system in the clinic, where interpretability is absolutely essential. We
propose Shapley Value Explanation of Heterogeneous Neural Networks (SVEHNN) for
explaining the Alzheimer's diagnosis made by a DNN from the 3D point cloud of
the neuroanatomy and tabular biomarkers. Our explanations are based on the
Shapley value, which is the unique method that satisfies all fundamental axioms
for local explanations previously established in the literature. Thus, SVEHNN
has many desirable characteristics that previous work on interpretability for
medical decision making is lacking. To avoid the exponential time complexity of
the Shapley value, we propose to transform a given DNN into a Lightweight
Probabilistic Deep Network without re-training, thus achieving a complexity
only quadratic in the number of features. In our experiments on synthetic and
real data, we show that we can closely approximate the exact Shapley value with
a dramatically reduced runtime and can reveal the hidden knowledge the network
has learned from the data.",arxiv
http://arxiv.org/abs/1904.11245v2,2019-12-25T05:20:30Z,2019-04-25T10:03:44Z,Exploring Object Relation in Mean Teacher for Cross-Domain Detection,"Rendering synthetic data (e.g., 3D CAD-rendered images) to generate
annotations for learning deep models in vision tasks has attracted increasing
attention in recent years. However, simply applying the models learnt on
synthetic images may lead to high generalization error on real images due to
domain shift. To address this issue, recent progress in cross-domain
recognition has featured the Mean Teacher, which directly simulates
unsupervised domain adaptation as semi-supervised learning. The domain gap is
thus naturally bridged with consistency regularization in a teacher-student
scheme. In this work, we advance this Mean Teacher paradigm to be applicable
for cross-domain detection. Specifically, we present Mean Teacher with Object
Relations (MTOR) that novelly remolds Mean Teacher under the backbone of Faster
R-CNN by integrating the object relations into the measure of consistency cost
between teacher and student modules. Technically, MTOR firstly learns
relational graphs that capture similarities between pairs of regions for
teacher and student respectively. The whole architecture is then optimized with
three consistency regularizations: 1) region-level consistency to align the
region-level predictions between teacher and student, 2) inter-graph
consistency for matching the graph structures between teacher and student, and
3) intra-graph consistency to enhance the similarity between regions of same
class within the graph of student. Extensive experiments are conducted on the
transfers across Cityscapes, Foggy Cityscapes, and SIM10k, and superior results
are reported when comparing to state-of-the-art approaches. More remarkably, we
obtain a new record of single model: 22.8% of mAP on Syn2Real detection
dataset.",arxiv
http://arxiv.org/abs/1903.07504v1,2019-03-18T15:24:11Z,2019-03-18T15:24:11Z,"Understanding the Limitations of CNN-based Absolute Camera Pose
  Regression","Visual localization is the task of accurate camera pose estimation in a known
scene. It is a key problem in computer vision and robotics, with applications
including self-driving cars, Structure-from-Motion, SLAM, and Mixed Reality.
Traditionally, the localization problem has been tackled using 3D geometry.
Recently, end-to-end approaches based on convolutional neural networks have
become popular. These methods learn to directly regress the camera pose from an
input image. However, they do not achieve the same level of pose accuracy as 3D
structure-based methods. To understand this behavior, we develop a theoretical
model for camera pose regression. We use our model to predict failure cases for
pose regression techniques and verify our predictions through experiments. We
furthermore use our model to show that pose regression is more closely related
to pose approximation via image retrieval than to accurate pose estimation via
3D structure. A key result is that current approaches do not consistently
outperform a handcrafted image retrieval baseline. This clearly shows that
additional research is needed before pose regression algorithms are ready to
compete with structure-based methods.",arxiv
http://arxiv.org/abs/1708.02721v2,2018-03-12T12:30:36Z,2017-08-09T05:39:36Z,Deep Face Feature for Face Alignment,"In this paper, we present a deep learning based image feature extraction
method designed specifically for face images. To train the feature extraction
model, we construct a large scale photo-realistic face image dataset with
ground-truth correspondence between multi-view face images, which are
synthesized from real photographs via an inverse rendering procedure. The deep
face feature (DFF) is trained using correspondence between face images rendered
from different views. Using the trained DFF model, we can extract a feature
vector for each pixel of a face image, which distinguishes different facial
regions and is shown to be more effective than general-purpose feature
descriptors for face-related tasks such as matching and alignment. Based on the
DFF, we develop a robust face alignment method, which iteratively updates
landmarks, pose and 3D shape. Extensive experiments demonstrate that our method
can achieve state-of-the-art results for face alignment under highly
unconstrained face images.",arxiv
http://arxiv.org/abs/2111.07640v1,2021-11-15T10:00:06Z,2021-11-15T10:00:06Z,"AnimeCeleb: Large-Scale Animation CelebFaces Dataset via Controllable 3D
  Synthetic Models","Despite remarkable success in deep learning-based face-related models, these
models are still limited to the domain of real human faces. On the other hand,
the domain of animation faces has been studied less intensively due to the
absence of a well-organized dataset. In this paper, we present a large-scale
animation celebfaces dataset (AnimeCeleb) via controllable synthetic animation
models to boost research on the animation face domain. To facilitate the data
generation process, we build a semi-automatic pipeline based on an open 3D
software and a developed annotation system. This leads to constructing a
large-scale animation face dataset that includes multi-pose and multi-style
animation faces with rich annotations. Experiments suggest that our dataset is
applicable to various animation-related tasks such as head reenactment and
colorization.",arxiv
http://arxiv.org/abs/1803.00680v2,2019-03-17T01:34:35Z,2018-03-02T01:34:06Z,"A Tutorial on UAVs for Wireless Networks: Applications, Challenges, and
  Open Problems","The use of flying platforms such as unmanned aerial vehicles (UAVs),
popularly known as drones, is rapidly growing. In particular, with their
inherent attributes such as mobility, flexibility, and adaptive altitude, UAVs
admit several key potential applications in wireless systems. On the one hand,
UAVs can be used as aerial base stations to enhance coverage, capacity,
reliability, and energy efficiency of wireless networks. On the other hand,
UAVs can operate as flying mobile terminals within a cellular network. Such
cellular-connected UAVs can enable several applications ranging from real-time
video streaming to item delivery. In this paper, a comprehensive tutorial on
the potential benefits and applications of UAVs in wireless communications is
presented. Moreover, the important challenges and the fundamental tradeoffs in
UAV-enabled wireless networks are thoroughly investigated. In particular, the
key UAV challenges such as three-dimensional deployment, performance analysis,
channel modeling, and energy efficiency are explored along with representative
results. Then, open problems and potential research directions pertaining to
UAV communications are introduced. Finally, various analytical frameworks and
mathematical tools such as optimization theory, machine learning, stochastic
geometry, transport theory, and game theory are described. The use of such
tools for addressing unique UAV problems is also presented. In a nutshell, this
tutorial provides key guidelines on how to analyze, optimize, and design
UAV-based wireless communication systems.",arxiv
http://arxiv.org/abs/1906.05336v2,2020-03-02T19:59:44Z,2019-06-12T19:17:49Z,Existence of Life in 2 + 1 Dimensions,"There are anthropic reasons to suspect that life in more than three spatial
dimensions is not possible, and if the same could be said of fewer than three,
then one would have an anthropic argument for why we experience precisely three
large spatial dimensions. There are two main arguments levelled against the
possibility of life in 2 + 1 dimensions: the lack of a local gravitational
force and Newtonian limit in 3D general relativity, and the claim that the
restriction to a planar topology means that the possibilities are 'too simple'
for life to exist. I will examine these arguments and show how a purely scalar
theory of gravity may evade the first one, before considering certain families
of planar graphs which share properties which are observed in real-life
biological neural networks and are argued to be important for their
functioning.",arxiv
http://arxiv.org/abs/2003.06631v1,2020-03-14T13:51:50Z,2020-03-14T13:51:50Z,Non-Local Part-Aware Point Cloud Denoising,"This paper presents a novel non-local part-aware deep neural network to
denoise point clouds by exploring the inherent non-local self-similarity in 3D
objects and scenes. Different from existing works that explore small local
patches, we design the non-local learning unit (NLU) customized with a graph
attention module to adaptively capture non-local semantically-related features
over the entire point cloud. To enhance the denoising performance, we cascade a
series of NLUs to progressively distill the noise features from the noisy
inputs. Further, besides the conventional surface reconstruction loss, we
formulate a semantic part loss to regularize the predictions towards the
relevant parts and enable denoising in a part-aware manner. Lastly, we
performed extensive experiments to evaluate our method, both quantitatively and
qualitatively, and demonstrate its superiority over the state-of-the-arts on
both synthetic and real-scanned noisy inputs.",arxiv
http://arxiv.org/abs/2008.01815v1,2020-08-04T20:29:15Z,2020-08-04T20:29:15Z,Deep Multi Depth Panoramas for View Synthesis,"We propose a learning-based approach for novel view synthesis for
multi-camera 360$^{\circ}$ panorama capture rigs. Previous work constructs RGBD
panoramas from such data, allowing for view synthesis with small amounts of
translation, but cannot handle the disocclusions and view-dependent effects
that are caused by large translations. To address this issue, we present a
novel scene representation - Multi Depth Panorama (MDP) - that consists of
multiple RGBD$\alpha$ panoramas that represent both scene geometry and
appearance. We demonstrate a deep neural network-based method to reconstruct
MDPs from multi-camera 360$^{\circ}$ images. MDPs are more compact than
previous 3D scene representations and enable high-quality, efficient new view
rendering. We demonstrate this via experiments on both synthetic and real data
and comparisons with previous state-of-the-art methods spanning both
learning-based approaches and classical RGBD-based methods.",arxiv
http://arxiv.org/abs/1703.03530v1,2017-03-10T03:17:17Z,2017-03-10T03:17:17Z,"Towards Wi-Fi AP-Assisted Content Prefetching for On-Demand TV Series: A
  Reinforcement Learning Approach","The emergence of smart Wi-Fi APs (Access Point), which are equipped with huge
storage space, opens a new research area on how to utilize these resources at
the edge network to improve users' quality of experience (QoE) (e.g., a short
startup delay and smooth playback). One important research interest in this
area is content prefetching, which predicts and accurately fetches contents
ahead of users' requests to shift the traffic away during peak periods.
However, in practice, the different video watching patterns among users, and
the varying network connection status lead to the time-varying server load,
which eventually makes the content prefetching problem challenging. To
understand this challenge, this paper first performs a large-scale measurement
study on users' AP connection and TV series watching patterns using
real-traces. Then, based on the obtained insights, we formulate the content
prefetching problem as a Markov Decision Process (MDP). The objective is to
strike a balance between the increased prefetching&storage cost incurred by
incorrect prediction and the reduced content download delay because of
successful prediction. A learning-based approach is proposed to solve this
problem and another three algorithms are adopted as baselines. In particular,
first, we investigate the performance lower bound by using a random algorithm,
and the upper bound by using an ideal offline approach. Then, we present a
heuristic algorithm as another baseline. Finally, we design a reinforcement
learning algorithm that is more practical to work in the online manner. Through
extensive trace-based experiments, we demonstrate the performance gain of our
design. Remarkably, our learning-based algorithm achieves a better precision
and hit ratio (e.g., 80%) with about 70% (resp. 50%) cost saving compared to
the random (resp. heuristic) algorithm.",arxiv
http://arxiv.org/abs/2110.13465v1,2021-10-26T08:00:03Z,2021-10-26T08:00:03Z,"CS-Rep: Making Speaker Verification Networks Embracing
  Re-parameterization","Automatic speaker verification (ASV) systems, which determine whether two
speeches are from the same speaker, mainly focus on verification accuracy while
ignoring inference speed. However, in real applications, both inference speed
and verification accuracy are essential. This study proposes cross-sequential
re-parameterization (CS-Rep), a novel topology re-parameterization strategy for
multi-type networks, to increase the inference speed and verification accuracy
of models. CS-Rep solves the problem that existing re-parameterization methods
are unsuitable for typical ASV backbones. When a model applies CS-Rep, the
training-period network utilizes a multi-branch topology to capture speaker
information, whereas the inference-period model converts to a time-delay neural
network (TDNN)-like plain backbone with stacked TDNN layers to achieve the fast
inference speed. Based on CS-Rep, an improved TDNN with friendly test and
deployment called Rep-TDNN is proposed. Compared with the state-of-the-art
model ECAPA-TDNN, which is highly recognized in the industry, Rep-TDNN
increases the actual inference speed by about 50% and reduces the EER by 10%.
The code will be released.",arxiv
http://arxiv.org/abs/1907.02265v1,2019-07-04T08:16:20Z,2019-07-04T08:16:20Z,Supervised Symbolic Music Style Translation Using Synthetic Data,"Research on style transfer and domain translation has clearly demonstrated
the ability of deep learning-based algorithms to manipulate images in terms of
artistic style. More recently, several attempts have been made to extend such
approaches to music (both symbolic and audio) in order to enable transforming
musical style in a similar manner. In this study, we focus on symbolic music
with the goal of altering the 'style' of a piece while keeping its original
'content'. As opposed to the current methods, which are inherently restricted
to be unsupervised due to the lack of 'aligned' data (i.e. the same musical
piece played in multiple styles), we develop the first fully supervised
algorithm for this task. At the core of our approach lies a synthetic data
generation scheme which allows us to produce virtually unlimited amounts of
aligned data, and hence avoid the above issue. In view of this data generation
scheme, we propose an encoder-decoder model for translating symbolic music
accompaniments between a number of different styles. Our experiments show that
our models, although trained entirely on synthetic data, are capable of
producing musically meaningful accompaniments even for real (non-synthetic)
MIDI recordings.",arxiv
http://arxiv.org/abs/1612.05601v2,2017-07-25T16:12:50Z,2016-12-16T19:20:20Z,"SonoNet: Real-Time Detection and Localisation of Fetal Standard Scan
  Planes in Freehand Ultrasound","Identifying and interpreting fetal standard scan planes during 2D ultrasound
mid-pregnancy examinations are highly complex tasks which require years of
training. Apart from guiding the probe to the correct location, it can be
equally difficult for a non-expert to identify relevant structures within the
image. Automatic image processing can provide tools to help experienced as well
as inexperienced operators with these tasks. In this paper, we propose a novel
method based on convolutional neural networks which can automatically detect 13
fetal standard views in freehand 2D ultrasound data as well as provide a
localisation of the fetal structures via a bounding box. An important
contribution is that the network learns to localise the target anatomy using
weak supervision based on image-level labels only. The network architecture is
designed to operate in real-time while providing optimal output for the
localisation task. We present results for real-time annotation, retrospective
frame retrieval from saved videos, and localisation on a very large and
challenging dataset consisting of images and video recordings of full clinical
anomaly screenings. We found that the proposed method achieved an average
F1-score of 0.798 in a realistic classification experiment modelling real-time
detection, and obtained a 90.09% accuracy for retrospective frame retrieval.
Moreover, an accuracy of 77.8% was achieved on the localisation task.",arxiv
http://arxiv.org/abs/1808.03823v2,2018-08-20T21:22:36Z,2018-08-11T16:09:45Z,"Learning Discriminative 3D Shape Representations by View Discerning
  Networks","In view-based 3D shape recognition, extracting discriminative visual
representation of 3D shapes from projected images is considered the core
problem. Projections with low discriminative ability can adversely influence
the final 3D shape representation. Especially under the real situations with
background clutter and object occlusion, the adverse effect is even more
severe. To resolve this problem, we propose a novel deep neural network, View
Discerning Network, which learns to judge the quality of views and adjust their
contributions to the representation of shapes. In this network, a Score
Generation Unit is devised to evaluate the quality of each projected image with
score vectors. These score vectors are used to weight the image features and
the weighted features perform much better than original features in 3D shape
recognition task. In particular, we introduce two structures of Score
Generation Unit, Channel-wise Score Unit and Part-wise Score Unit, to assess
the quality of feature maps from different perspectives. Our network aggregates
features and scores in an end-to-end framework, so that final shape descriptors
are directly obtained from its output. Our experiments on ModelNet and ShapeNet
Core55 show that View Discerning Network outperforms the state-of-the-arts in
terms of the retrieval task, with excellent robustness against background
clutter and object occlusion.",arxiv
http://arxiv.org/abs/2010.08675v1,2020-10-17T00:11:13Z,2020-10-17T00:11:13Z,Long-Term Face Tracking for Crowded Video-Surveillance Scenarios,"Most current multi-object trackers focus on short-term tracking, and are
based on deep and complex systems that do not operate in real-time, often
making them impractical for video-surveillance. In this paper, we present a
long-term multi-face tracking architecture conceived for working in crowded
contexts, particularly unconstrained in terms of movement and occlusions, and
where the face is often the only visible part of the person. Our system
benefits from advances in the fields of face detection and face recognition to
achieve long-term tracking. It follows a tracking-by-detection approach,
combining a fast short-term visual tracker with a novel online tracklet
reconnection strategy grounded on face verification. Additionally, a correction
module is included to correct past track assignments with no extra
computational cost. We present a series of experiments introducing novel,
specialized metrics for the evaluation of long-term tracking capabilities and a
video dataset that we publicly release. Findings demonstrate that, in this
context, our approach allows to obtain up to 50% longer tracks than
state-of-the-art deep learning trackers.",arxiv
http://arxiv.org/abs/2107.02220v1,2021-07-05T18:40:43Z,2021-07-05T18:40:43Z,Graph Convolution for Re-ranking in Person Re-identification,"Nowadays, deep learning is widely applied to extract features for similarity
computation in person re-identification (re-ID) and have achieved great
success. However, due to the non-overlapping between training and testing IDs,
the difference between the data used for model training and the testing data
makes the performance of learned feature degraded during testing. Hence,
re-ranking is proposed to mitigate this issue and various algorithms have been
developed. However, most of existing re-ranking methods focus on replacing the
Euclidean distance with sophisticated distance metrics, which are not friendly
to downstream tasks and hard to be used for fast retrieval of massive data in
real applications. In this work, we propose a graph-based re-ranking method to
improve learned features while still keeping Euclidean distance as the
similarity metric. Inspired by graph convolution networks, we develop an
operator to propagate features over an appropriate graph. Since graph is the
essential key for the propagation, two important criteria are considered for
designing the graph, and three different graphs are explored accordingly.
Furthermore, a simple yet effective method is proposed to generate a profile
vector for each tracklet in videos, which helps extend our method to video
re-ID. Extensive experiments on three benchmark data sets, e.g., Market-1501,
Duke, and MARS, demonstrate the effectiveness of our proposed approach.",arxiv
http://arxiv.org/abs/2104.06535v1,2021-04-13T22:34:33Z,2021-04-13T22:34:33Z,NPE: An FPGA-based Overlay Processor for Natural Language Processing,"In recent years, transformer-based models have shown state-of-the-art results
for Natural Language Processing (NLP). In particular, the introduction of the
BERT language model brought with it breakthroughs in tasks such as question
answering and natural language inference, advancing applications that allow
humans to interact naturally with embedded devices. FPGA-based overlay
processors have been shown as effective solutions for edge image and video
processing applications, which mostly rely on low precision linear matrix
operations. In contrast, transformer-based NLP techniques employ a variety of
higher precision nonlinear operations with significantly higher frequency. We
present NPE, an FPGA-based overlay processor that can efficiently execute a
variety of NLP models. NPE offers software-like programmability to the end user
and, unlike FPGA designs that implement specialized accelerators for each
nonlinear function, can be upgraded for future NLP models without requiring
reconfiguration. We demonstrate that NPE can meet real-time conversational AI
latency targets for the BERT language model with $4\times$ lower power than
CPUs and $6\times$ lower power than GPUs. We also show NPE uses $3\times$ fewer
FPGA resources relative to comparable BERT network-specific accelerators in the
literature. NPE provides a cost-effective and power-efficient FPGA-based
solution for Natural Language Processing at the edge.",arxiv
http://arxiv.org/abs/2011.08517v3,2021-08-17T09:50:01Z,2020-11-17T09:12:11Z,"Bridging the Reality Gap for Pose Estimation Networks using Sensor-Based
  Domain Randomization","Since the introduction of modern deep learning methods for object pose
estimation, test accuracy and efficiency has increased significantly. For
training, however, large amounts of annotated training data are required for
good performance. While the use of synthetic training data prevents the need
for manual annotation, there is currently a large performance gap between
methods trained on real and synthetic data. This paper introduces a new method,
which bridges this gap.
  Most methods trained on synthetic data use 2D images, as domain randomization
in 2D is more developed. To obtain precise poses, many of these methods perform
a final refinement using 3D data. Our method integrates the 3D data into the
network to increase the accuracy of the pose estimation. To allow for domain
randomization in 3D, a sensor-based data augmentation has been developed.
Additionally, we introduce the SparseEdge feature, which uses a wider search
space during point cloud propagation to avoid relying on specific features
without increasing run-time.
  Experiments on three large pose estimation benchmarks show that the presented
method outperforms previous methods trained on synthetic data and achieves
comparable results to existing methods trained on real data.",arxiv
http://arxiv.org/abs/1804.08286v1,2018-04-23T08:44:52Z,2018-04-23T08:44:52Z,Fully Convolutional Adaptation Networks for Semantic Segmentation,"The recent advances in deep neural networks have convincingly demonstrated
high capability in learning vision models on large datasets. Nevertheless,
collecting expert labeled datasets especially with pixel-level annotations is
an extremely expensive process. An appealing alternative is to render synthetic
data (e.g., computer games) and generate ground truth automatically. However,
simply applying the models learnt on synthetic images may lead to high
generalization error on real images due to domain shift. In this paper, we
facilitate this issue from the perspectives of both visual appearance-level and
representation-level domain adaptation. The former adapts source-domain images
to appear as if drawn from the ""style"" in the target domain and the latter
attempts to learn domain-invariant representations. Specifically, we present
Fully Convolutional Adaptation Networks (FCAN), a novel deep architecture for
semantic segmentation which combines Appearance Adaptation Networks (AAN) and
Representation Adaptation Networks (RAN). AAN learns a transformation from one
domain to the other in the pixel space and RAN is optimized in an adversarial
learning manner to maximally fool the domain discriminator with the learnt
source and target representations. Extensive experiments are conducted on the
transfer from GTA5 (game videos) to Cityscapes (urban street scenes) on
semantic segmentation and our proposal achieves superior results when comparing
to state-of-the-art unsupervised adaptation techniques. More remarkably, we
obtain a new record: mIoU of 47.5% on BDDS (drive-cam videos) in an
unsupervised setting.",arxiv
http://arxiv.org/abs/2008.05924v1,2020-08-13T14:10:05Z,2020-08-13T14:10:05Z,"DFEW: A Large-Scale Database for Recognizing Dynamic Facial Expressions
  in the Wild","Recently, facial expression recognition (FER) in the wild has gained a lot of
researchers' attention because it is a valuable topic to enable the FER
techniques to move from the laboratory to the real applications. In this paper,
we focus on this challenging but interesting topic and make contributions from
three aspects. First, we present a new large-scale 'in-the-wild' dynamic facial
expression database, DFEW (Dynamic Facial Expression in the Wild), consisting
of over 16,000 video clips from thousands of movies. These video clips contain
various challenging interferences in practical scenarios such as extreme
illumination, occlusions, and capricious pose changes. Second, we propose a
novel method called Expression-Clustered Spatiotemporal Feature Learning
(EC-STFL) framework to deal with dynamic FER in the wild. Third, we conduct
extensive benchmark experiments on DFEW using a lot of spatiotemporal deep
feature learning methods as well as our proposed EC-STFL. Experimental results
show that DFEW is a well-designed and challenging database, and the proposed
EC-STFL can promisingly improve the performance of existing spatiotemporal deep
neural networks in coping with the problem of dynamic FER in the wild. Our DFEW
database is publicly available and can be freely downloaded from
https://dfew-dataset.github.io/.",arxiv
http://arxiv.org/abs/2102.00103v1,2021-01-29T22:52:47Z,2021-01-29T22:52:47Z,Synthetic Data and Hierarchical Object Detection in Overhead Imagery,"The performance of neural network models is often limited by the availability
of big data sets. To treat this problem, we survey and develop novel synthetic
data generation and augmentation techniques for enhancing low/zero-sample
learning in satellite imagery. In addition to extending synthetic data
generation approaches, we propose a hierarchical detection approach to improve
the utility of synthetic training samples. We consider existing techniques for
producing synthetic imagery--3D models and neural style transfer--as well as
introducing our own adversarially trained reskinning network, the
GAN-Reskinner, to blend 3D models. Additionally, we test the value of synthetic
data in a two-stage, hierarchical detection/classification model of our own
construction. To test the effectiveness of synthetic imagery, we employ it in
the training of detection models and our two stage model, and evaluate the
resulting models on real satellite images. All modalities of synthetic data are
tested extensively on practical, geospatial analysis problems. Our experiments
show that synthetic data developed using our approach can often enhance
detection performance, particularly when combined with some real training
images. When the only source of data is synthetic, our GAN-Reskinner often
boosts performance over conventionally rendered 3D models and in all cases the
hierarchical model outperforms the baseline end-to-end detection architecture.",arxiv
http://arxiv.org/abs/2006.04356v1,2020-06-08T05:15:06Z,2020-06-08T05:15:06Z,"Associate-3Ddet: Perceptual-to-Conceptual Association for 3D Point Cloud
  Object Detection","Object detection from 3D point clouds remains a challenging task, though
recent studies pushed the envelope with the deep learning techniques. Owing to
the severe spatial occlusion and inherent variance of point density with the
distance to sensors, appearance of a same object varies a lot in point cloud
data. Designing robust feature representation against such appearance changes
is hence the key issue in a 3D object detection method. In this paper, we
innovatively propose a domain adaptation like approach to enhance the
robustness of the feature representation. More specifically, we bridge the gap
between the perceptual domain where the feature comes from a real scene and the
conceptual domain where the feature is extracted from an augmented scene
consisting of non-occlusion point cloud rich of detailed information. This
domain adaptation approach mimics the functionality of the human brain when
proceeding object perception. Extensive experiments demonstrate that our simple
yet effective approach fundamentally boosts the performance of 3D point cloud
object detection and achieves the state-of-the-art results.",arxiv
http://arxiv.org/abs/1704.02581v2,2017-04-12T09:08:19Z,2017-04-09T10:09:55Z,"Modeling Temporal Dynamics and Spatial Configurations of Actions Using
  Two-Stream Recurrent Neural Networks","Recently, skeleton based action recognition gains more popularity due to
cost-effective depth sensors coupled with real-time skeleton estimation
algorithms. Traditional approaches based on handcrafted features are limited to
represent the complexity of motion patterns. Recent methods that use Recurrent
Neural Networks (RNN) to handle raw skeletons only focus on the contextual
dependency in the temporal domain and neglect the spatial configurations of
articulated skeletons. In this paper, we propose a novel two-stream RNN
architecture to model both temporal dynamics and spatial configurations for
skeleton based action recognition. We explore two different structures for the
temporal stream: stacked RNN and hierarchical RNN. Hierarchical RNN is designed
according to human body kinematics. We also propose two effective methods to
model the spatial structure by converting the spatial graph into a sequence of
joints. To improve generalization of our model, we further exploit 3D
transformation based data augmentation techniques including rotation and
scaling transformation to transform the 3D coordinates of skeletons during
training. Experiments on 3D action recognition benchmark datasets show that our
method brings a considerable improvement for a variety of actions, i.e.,
generic actions, interaction activities and gestures.",arxiv
http://arxiv.org/abs/1807.10972v1,2018-07-28T19:36:46Z,2018-07-28T19:36:46Z,"RS-Net: Regression-Segmentation 3D CNN for Synthesis of Full Resolution
  Missing Brain MRI in the Presence of Tumours","Accurate synthesis of a full 3D MR image containing tumours from available
MRI (e.g. to replace an image that is currently unavailable or corrupted) would
provide a clinician as well as downstream inference methods with important
complementary information for disease analysis. In this paper, we present an
end-to-end 3D convolution neural network that takes a set of acquired MR image
sequences (e.g. T1, T2, T1ce) as input and concurrently performs (1) regression
of the missing full resolution 3D MRI (e.g. FLAIR) and (2) segmentation of the
tumour into subtypes (e.g. enhancement, core). The hypothesis is that this
would focus the network to perform accurate synthesis in the area of the
tumour. Experiments on the BraTS 2015 and 2017 datasets [1] show that: (1) the
proposed method gives better performance than state-of-the-art methods in terms
of established global evaluation metrics (e.g. PSNR), (2) replacing real MR
volumes with the synthesized MRI does not lead to significant degradation in
tumour and sub-structure segmentation accuracy. The system further provides
uncertainty estimates based on Monte Carlo (MC) dropout [11] for the
synthesized volume at each voxel, permitting quantification of the system's
confidence in the output at each location.",arxiv
http://arxiv.org/abs/2101.05212v1,2020-12-27T19:52:58Z,2020-12-27T19:52:58Z,"Ellipse Regression with Predicted Uncertainties for Accurate Multi-View
  3D Object Estimation","Convolutional neural network (CNN) based architectures, such as Mask R-CNN,
constitute the state of the art in object detection and segmentation. Recently,
these methods have been extended for model-based segmentation where the network
outputs the parameters of a geometric model (e.g. an ellipse) directly. This
work considers objects whose three-dimensional models can be represented as
ellipsoids. We present a variant of Mask R-CNN for estimating the parameters of
ellipsoidal objects by segmenting each object and accurately regressing the
parameters of projection ellipses. We show that model regression is sensitive
to the underlying occlusion scenario and that prediction quality for each
object needs to be characterized individually for accurate 3D object
estimation. We present a novel ellipse regression loss which can learn the
offset parameters with their uncertainties and quantify the overall geometric
quality of detection for each ellipse. These values, in turn, allow us to fuse
multi-view detections to obtain 3D ellipsoid parameters in a principled
fashion. The experiments on both synthetic and real datasets quantitatively
demonstrate the high accuracy of our proposed method in estimating 3D objects
under heavy occlusions compared to previous state-of-the-art methods.",arxiv
http://arxiv.org/abs/2108.12655v1,2021-08-28T14:18:29Z,2021-08-28T14:18:29Z,"DenseLiDAR: A Real-Time Pseudo Dense Depth Guided Depth Completion
  Network","Depth Completion can produce a dense depth map from a sparse input and
provide a more complete 3D description of the environment. Despite great
progress made in depth completion, the sparsity of the input and low density of
the ground truth still make this problem challenging. In this work, we propose
DenseLiDAR, a novel real-time pseudo-depth guided depth completion neural
network. We exploit dense pseudo-depth map obtained from simple morphological
operations to guide the network in three aspects: (1) Constructing a residual
structure for the output; (2) Rectifying the sparse input data; (3) Providing
dense structural loss for training the network. Thanks to these novel designs,
higher performance of the output could be achieved. In addition, two new
metrics for better evaluating the quality of the predicted depth map are also
presented. Extensive experiments on KITTI depth completion benchmark suggest
that our model is able to achieve the state-of-the-art performance at the
highest frame rate of 50Hz. The predicted dense depth is further evaluated by
several downstream robotic perception or positioning tasks. For the task of 3D
object detection, 3~5 percent performance gains on small objects categories are
achieved on KITTI 3D object detection dataset. For RGB-D SLAM, higher accuracy
on vehicle's trajectory is also obtained in KITTI Odometry dataset. These
promising results not only verify the high quality of our depth prediction, but
also demonstrate the potential of improving the related downstream tasks by
using depth completion results.",arxiv
http://arxiv.org/abs/2111.09515v1,2021-11-18T04:20:13Z,2021-11-18T04:20:13Z,"RAANet: Range-Aware Attention Network for LiDAR-based 3D Object
  Detection with Auxiliary Density Level Estimation","3D object detection from LiDAR data for autonomous driving has been making
remarkable strides in recent years. Among the state-of-the-art methodologies,
encoding point clouds into a bird's-eye view (BEV) has been demonstrated to be
both effective and efficient. Different from perspective views, BEV preserves
rich spatial and distance information between objects; and while farther
objects of the same type do not appear smaller in the BEV, they contain sparser
point cloud features. This fact weakens BEV feature extraction using
shared-weight convolutional neural networks. In order to address this
challenge, we propose Range-Aware Attention Network (RAANet), which extracts
more powerful BEV features and generates superior 3D object detections. The
range-aware attention (RAA) convolutions significantly improve feature
extraction for near as well as far objects. Moreover, we propose a novel
auxiliary loss for density estimation to further enhance the detection accuracy
of RAANet for occluded objects. It is worth to note that our proposed RAA
convolution is lightweight and compatible to be integrated into any CNN
architecture used for the BEV detection. Extensive experiments on the nuScenes
dataset demonstrate that our proposed approach outperforms the state-of-the-art
methods for LiDAR-based 3D object detection, with real-time inference speed of
16 Hz for the full version and 22 Hz for the lite version. The code is publicly
available at an anonymous Github repository
https://github.com/anonymous0522/RAAN.",arxiv
http://arxiv.org/abs/1206.4662v1,2012-06-18T15:30:35Z,2012-06-18T15:30:35Z,Bayesian Watermark Attacks,"This paper presents an application of statistical machine learning to the
field of watermarking. We propose a new attack model on additive
spread-spectrum watermarking systems. The proposed attack is based on Bayesian
statistics. We consider the scenario in which a watermark signal is repeatedly
embedded in specific, possibly chosen based on a secret message bitstream,
segments (signals) of the host data. The host signal can represent a patch of
pixels from an image or a video frame. We propose a probabilistic model that
infers the embedded message bitstream and watermark signal, directly from the
watermarked data, without access to the decoder. We develop an efficient Markov
chain Monte Carlo sampler for updating the model parameters from their
conjugate full conditional posteriors. We also provide a variational Bayesian
solution, which further increases the convergence speed of the algorithm.
Experiments with synthetic and real image signals demonstrate that the attack
model is able to correctly infer a large part of the message bitstream and
obtain a very accurate estimate of the watermark signal.",arxiv
http://arxiv.org/abs/1803.00303v2,2018-05-29T14:33:33Z,2018-03-01T10:59:20Z,"Classifying flows and buffer state for YouTube's HTTP adaptive streaming
  service in mobile networks","Accurate cross-layer information is very useful to optimize mobile networks
for specific applications. However, providing application-layer information to
lower protocol layers has become very difficult due to the wide adoption of
end-to-end encryption and due to the absence of cross-layer signaling
standards. As an alternative, this paper presents a traffic profiling solution
to passively estimate parameters of HTTP Adaptive Streaming (HAS) applications
at the lower layers. By observing IP packet arrivals, our machine learning
system identifies video flows and detects the state of an HAS client's
play-back buffer in real time. Our experiments with YouTube's mobile client
show that Random Forests achieve very high accuracy even with a strong
variation of link quality. Since this high performance is achieved at IP level
with a small, generic feature set, our approach requires no Deep Packet
Inspection (DPI), comes at low complexity, and does not interfere with
end-to-end encryption. Traffic profiling is, thus, a powerful new tool for
monitoring and managing even encrypted HAS traffic in mobile networks.",arxiv
http://arxiv.org/abs/1901.03257v4,2020-12-04T14:28:30Z,2019-01-10T16:34:15Z,"Data Augmentation of Room Classifiers using Generative Adversarial
  Networks","The classification of acoustic environments allows for machines to better
understand the auditory world around them. The use of deep learning in order to
teach machines to discriminate between different rooms is a new area of
research. Similarly to other learning tasks, this task suffers from the
high-dimensionality and the limited availability of training data. Data
augmentation methods have proven useful in addressing this issue in the tasks
of sound event detection and scene classification. This paper proposes a method
for data augmentation for the task of room classification from reverberant
speech. Generative Adversarial Networks (GANs) are trained that generate
artificial data as if they were measured in real rooms. This provides
additional training examples to the classifiers without the need for any
additional data collection, which is time-consuming and often impractical. A
representation of acoustic environments is proposed, which is used to train the
GANs. The representation is based on a sparse model for the early reflections,
a stochastic model for the reverberant tail and a mixing mechanism between the
two. In the experiments shown, the proposed data augmentation method increases
the test accuracy of a CNN-RNN room classifier from 89.4% to 95.5%.",arxiv
http://arxiv.org/abs/1911.06713v1,2019-11-15T15:56:43Z,2019-11-15T15:56:43Z,"Sample Drop Detection for Distant-speech Recognition with Asynchronous
  Devices Distributed in Space","In many applications of multi-microphone multi-device processing, the
synchronization among different input channels can be affected by the lack of a
common clock and isolated drops of samples. In this work, we address the issue
of sample drop detection in the context of a conversational speech scenario,
recorded by a set of microphones distributed in space. The goal is to design a
neural-based model that given a short window in the time domain, detects
whether one or more devices have been subjected to a sample drop event. The
candidate time windows are selected from a set of large time intervals,
possibly including a sample drop, and by using a preprocessing step. The latter
is based on the application of normalized cross-correlation between signals
acquired by different devices. The architecture of the neural network relies on
a CNN-LSTM encoder, followed by multi-head attention. The experiments are
conducted using both artificial and real data. Our proposed approach obtained
F1 score of 88% on an evaluation set extracted from the CHiME-5 corpus. A
comparable performance was found in a larger set of experiments conducted on a
set of multi-channel artificial scenes.",arxiv
http://arxiv.org/abs/2003.01395v1,2020-03-03T09:05:05Z,2020-03-03T09:05:05Z,"DeepSperm: A robust and real-time bull sperm-cell detection in densely
  populated semen videos","Background and Objective: Object detection is a primary research interest in
computer vision. Sperm-cell detection in a densely populated bull semen
microscopic observation video presents challenges such as partial occlusion,
vast number of objects in a single video frame, tiny size of the object,
artifacts, low contrast, and blurry objects because of the rapid movement of
the sperm cells. This study proposes an architecture, called DeepSperm, that
solves the aforementioned challenges and is more accurate and faster than
state-of-the-art architectures. Methods: In the proposed architecture, we use
only one detection layer, which is specific for small object detection. For
handling overfitting and increasing accuracy, we set a higher network
resolution, use a dropout layer, and perform data augmentation on hue,
saturation, and exposure. Several hyper-parameters are tuned to achieve better
performance. We compare our proposed method with those of a conventional image
processing-based object-detection method, you only look once (YOLOv3), and mask
region-based convolutional neural network (Mask R-CNN). Results: In our
experiment, we achieve 86.91 mAP on the test dataset and a processing speed of
50.3 fps. In comparison with YOLOv3, we achieve an increase of 16.66 mAP point,
3.26 x faster on testing, and 1.4 x faster on training with a small training
dataset, which contains 40 video frames. The weights file size was also reduced
significantly, with 16.94 x smaller than that of YOLOv3. Moreover, it requires
1.3 x less graphical processing unit (GPU) memory than YOLOv3. Conclusions:
This study proposes DeepSperm, which is a simple, effective, and efficient
architecture with its hyper-parameters and configuration to detect bull sperm
cells robustly in real time. In our experiment, we surpass the state of the art
in terms of accuracy, speed, and resource needs.",arxiv
http://arxiv.org/abs/2105.10316v1,2021-05-21T12:40:59Z,2021-05-21T12:40:59Z,"Analysis of voxel-based 3D object detection methods efficiency for
  real-time embedded systems","Real-time detection of objects in the 3D scene is one of the tasks an
autonomous agent needs to perform for understanding its surroundings. While
recent Deep Learning-based solutions achieve satisfactory performance, their
high computational cost renders their application in real-life settings in
which computations need to be performed on embedded platforms intractable. In
this paper, we analyze the efficiency of two popular voxel-based 3D object
detection methods providing a good compromise between high performance and
speed based on two aspects, their ability to detect objects located at large
distances from the agent and their ability to operate in real time on embedded
platforms equipped with high-performance GPUs. Our experiments show that these
methods mostly fail to detect distant small objects due to the sparsity of the
input point clouds at large distances. Moreover, models trained on near objects
achieve similar or better performance compared to those trained on all objects
in the scene. This means that the models learn object appearance
representations mostly from near objects. Our findings suggest that a
considerable part of the computations of existing methods is focused on
locations of the scene that do not contribute with successful detection. This
means that the methods can achieve a speed-up of $40$-$60\%$ by restricting
operation to near objects while not sacrificing much in performance.",arxiv
http://arxiv.org/abs/1903.10661v1,2019-03-26T03:19:42Z,2019-03-26T03:19:42Z,"Semantic Alignment: Finding Semantically Consistent Ground-truth for
  Facial Landmark Detection","Recently, deep learning based facial landmark detection has achieved great
success. Despite this, we notice that the semantic ambiguity greatly degrades
the detection performance. Specifically, the semantic ambiguity means that some
landmarks (e.g. those evenly distributed along the face contour) do not have
clear and accurate definition, causing inconsistent annotations by annotators.
Accordingly, these inconsistent annotations, which are usually provided by
public databases, commonly work as the ground-truth to supervise network
training, leading to the degraded accuracy. To our knowledge, little research
has investigated this problem. In this paper, we propose a novel probabilistic
model which introduces a latent variable, i.e. the 'real' ground-truth which is
semantically consistent, to optimize. This framework couples two parts (1)
training landmark detection CNN and (2) searching the 'real' ground-truth.
These two parts are alternatively optimized: the searched 'real' ground-truth
supervises the CNN training; and the trained CNN assists the searching of
'real' ground-truth. In addition, to recover the unconfidently predicted
landmarks due to occlusion and low quality, we propose a global heatmap
correction unit (GHCU) to correct outliers by considering the global face shape
as a constraint. Extensive experiments on both image-based (300W and AFLW) and
video-based (300-VW) databases demonstrate that our method effectively improves
the landmark detection accuracy and achieves the state of the art performance.",arxiv
http://arxiv.org/abs/1903.02844v1,2019-03-07T11:26:10Z,2019-03-07T11:26:10Z,Voice Activity Detection: Merging Source and Filter-based Information,"Voice Activity Detection (VAD) refers to the problem of distinguishing speech
segments from background noise. Numerous approaches have been proposed for this
purpose. Some are based on features derived from the power spectral density,
others exploit the periodicity of the signal. The goal of this paper is to
investigate the joint use of source and filter-based features. Interestingly, a
mutual information-based assessment shows superior discrimination power for the
source-related features, especially the proposed ones. The features are further
the input of an artificial neural network-based classifier trained on a
multi-condition database. Two strategies are proposed to merge source and
filter information: feature and decision fusion. Our experiments indicate an
absolute reduction of 3% of the equal error rate when using decision fusion.
The final proposed system is compared to four state-of-the-art methods on 150
minutes of data recorded in real environments. Thanks to the robustness of its
source-related features, its multi-condition training and its efficient
information fusion, the proposed system yields over the best state-of-the-art
VAD a substantial increase of accuracy across all conditions (24% absolute on
average).",arxiv
http://arxiv.org/abs/2108.08083v1,2021-08-18T10:29:53Z,2021-08-18T10:29:53Z,"Promoting Mental Well-Being for Audiences in a Live-Streaming Game by
  Highlight-Based Bullet Comments","This paper proposes a method for generating bullet comments for
live-streaming games based on highlights (i.e., the exciting parts of video
clips) extracted from the game content and evaluate the effect of mental health
promotion. Game live streaming is becoming a popular theme for academic
research. Compared to traditional online video sharing platforms, such as
Youtube and Vimeo, video live streaming platform has the benefits of
communicating with other viewers in real-time. In sports broadcasting, the
commentator plays an essential role as mood maker by making matches more
exciting. The enjoyment emerged while watching game live streaming also
benefits the audience's mental health. However, many e-sports live streaming
channels do not have a commentator for entertaining viewers. Therefore, this
paper presents a design of an AI commentator that can be embedded in live
streaming games. To generate bullet comments for real-time game live streaming,
the system employs highlight evaluation to detect the highlights, and generate
the bullet comments. An experiment is conducted and the effectiveness of
generated bullet comments in a live-streaming fighting game channel is
evaluated.",arxiv
http://arxiv.org/abs/1611.03130v1,2016-11-09T23:13:18Z,2016-11-09T23:13:18Z,"Computationally Efficient Target Classification in Multispectral Image
  Data with Deep Neural Networks","Detecting and classifying targets in video streams from surveillance cameras
is a cumbersome, error-prone and expensive task. Often, the incurred costs are
prohibitive for real-time monitoring. This leads to data being stored locally
or transmitted to a central storage site for post-incident examination. The
required communication links and archiving of the video data are still
expensive and this setup excludes preemptive actions to respond to imminent
threats. An effective way to overcome these limitations is to build a smart
camera that transmits alerts when relevant video sequences are detected. Deep
neural networks (DNNs) have come to outperform humans in visual classifications
tasks. The concept of DNNs and Convolutional Networks (ConvNets) can easily be
extended to make use of higher-dimensional input data such as multispectral
data. We explore this opportunity in terms of achievable accuracy and required
computational effort. To analyze the precision of DNNs for scene labeling in an
urban surveillance scenario we have created a dataset with 8 classes obtained
in a field experiment. We combine an RGB camera with a 25-channel VIS-NIR
snapshot sensor to assess the potential of multispectral image data for target
classification. We evaluate several new DNNs, showing that the spectral
information fused together with the RGB frames can be used to improve the
accuracy of the system or to achieve similar accuracy with a 3x smaller
computation effort. We achieve a very high per-pixel accuracy of 99.1%. Even
for scarcely occurring, but particularly interesting classes, such as cars, 75%
of the pixels are labeled correctly with errors occurring only around the
border of the objects. This high accuracy was obtained with a training set of
only 30 labeled images, paving the way for fast adaptation to various
application scenarios.",arxiv
http://arxiv.org/abs/1904.08159v2,2019-05-22T22:46:23Z,2019-04-17T09:51:12Z,"3D Object Recognition with Ensemble Learning --- A Study of Point
  Cloud-Based Deep Learning Models","In this study, we present an analysis of model-based ensemble learning for 3D
point-cloud object classification and detection. An ensemble of multiple model
instances is known to outperform a single model instance, but there is little
study of the topic of ensemble learning for 3D point clouds. First, an ensemble
of multiple model instances trained on the same part of the
$\textit{ModelNet40}$ dataset was tested for seven deep learning, point
cloud-based classification algorithms: $\textit{PointNet}$,
$\textit{PointNet++}$, $\textit{SO-Net}$, $\textit{KCNet}$,
$\textit{DeepSets}$, $\textit{DGCNN}$, and $\textit{PointCNN}$. Second, the
ensemble of different architectures was tested. Results of our experiments show
that the tested ensemble learning methods improve over state-of-the-art on the
$\textit{ModelNet40}$ dataset, from $92.65\%$ to $93.64\%$ for the ensemble of
single architecture instances, $94.03\%$ for two different architectures, and
$94.15\%$ for five different architectures. We show that the ensemble of two
models with different architectures can be as effective as the ensemble of 10
models with the same architecture. Third, a study on classic bagging i.e. with
different subsets used for training multiple model instances) was tested and
sources of ensemble accuracy growth were investigated for best-performing
architecture, i.e. $\textit{SO-Net}$. We also investigate the ensemble learning
of $\textit{Frustum PointNet}$ approach in the task of 3D object detection,
increasing the average precision of 3D box detection on the $\textit{KITTI}$
dataset from $63.1\%$ to $66.5\%$ using only three model instances. We measure
the inference time of all 3D classification architectures on a $\textit{Nvidia
Jetson TX2}$, a common embedded computer for mobile robots, to allude to the
use of these models in real-life applications.",arxiv
http://arxiv.org/abs/1809.04766v2,2019-02-27T05:53:59Z,2018-09-13T04:19:26Z,"Real-Time Joint Semantic Segmentation and Depth Estimation Using
  Asymmetric Annotations","Deployment of deep learning models in robotics as sensory information
extractors can be a daunting task to handle, even using generic GPU cards.
Here, we address three of its most prominent hurdles, namely, i) the adaptation
of a single model to perform multiple tasks at once (in this work, we consider
depth estimation and semantic segmentation crucial for acquiring geometric and
semantic understanding of the scene), while ii) doing it in real-time, and iii)
using asymmetric datasets with uneven numbers of annotations per each modality.
To overcome the first two issues, we adapt a recently proposed real-time
semantic segmentation network, making changes to further reduce the number of
floating point operations. To approach the third issue, we embrace a simple
solution based on hard knowledge distillation under the assumption of having
access to a powerful `teacher' network. We showcase how our system can be
easily extended to handle more tasks, and more datasets, all at once,
performing depth estimation and segmentation both indoors and outdoors with a
single model. Quantitatively, we achieve results equivalent to (or better than)
current state-of-the-art approaches with one forward pass costing just 13ms and
6.5 GFLOPs on 640x480 inputs. This efficiency allows us to directly incorporate
the raw predictions of our network into the SemanticFusion framework for dense
3D semantic reconstruction of the scene.",arxiv
http://arxiv.org/abs/1901.08025v1,2019-01-23T17:55:39Z,2019-01-23T17:55:39Z,"Generalization of Spoofing Countermeasures: a Case Study with ASVspoof
  2015 and BTAS 2016 Corpora","Voice-based biometric systems are highly prone to spoofing attacks. Recently,
various countermeasures have been developed for detecting different kinds of
attacks such as replay, speech synthesis (SS) and voice conversion (VC). Most
of the existing studies are conducted with a specific training set defined by
the evaluation protocol. However, for realistic scenarios, selecting
appropriate training data is an open challenge for the system administrator.
Motivated by this practical concern, this work investigates the generalization
capability of spoofing countermeasures in restricted training conditions where
speech from a broad attack types are left out in the training database. We
demonstrate that different spoofing types have considerably different
generalization capabilities. For this study, we analyze the performance using
two kinds of features, mel-frequency cepstral coefficients (MFCCs) which are
considered as baseline and recently proposed constant Q cepstral coefficients
(CQCCs). The experiments are conducted with standard Gaussian mixture model -
maximum likelihood (GMM-ML) classifier on two recently released spoofing
corpora: ASVspoof 2015 and BTAS 2016 that includes cross-corpora performance
analysis. Feature-level analysis suggests that static and dynamic coefficients
of spectral features, both are important for detecting spoofing attacks in the
real-life condition.",arxiv
http://arxiv.org/abs/2011.14618v1,2020-11-30T08:42:13Z,2020-11-30T08:42:13Z,"CovidExplorer: A Multi-faceted AI-based Search and Visualization Engine
  for COVID-19 Information","The entire world is engulfed in the fight against the COVID-19 pandemic,
leading to a significant surge in research experiments, government policies,
and social media discussions. A multi-modal information access and data
visualization platform can play a critical role in supporting research aimed at
understanding and developing preventive measures for the pandemic. In this
paper, we present a multi-faceted AI-based search and visualization engine,
CovidExplorer. Our system aims to help researchers understand current
state-of-the-art COVID-19 research, identify research articles relevant to
their domain, and visualize real-time trends and statistics of COVID-19 cases.
In contrast to other existing systems, CovidExplorer also brings in
India-specific topical discussions on social media to study different aspects
of COVID-19. The system, demo video, and the datasets are available at
http://covidexplorer.in.",arxiv
http://arxiv.org/abs/2101.01444v1,2021-01-05T10:34:35Z,2021-01-05T10:34:35Z,CycleGAN for Interpretable Online EMT Compensation,"Purpose: Electromagnetic Tracking (EMT) can partially replace X-ray guidance
in minimally invasive procedures, reducing radiation in the OR. However, in
this hybrid setting, EMT is disturbed by metallic distortion caused by the
X-ray device. We plan to make hybrid navigation clinical reality to reduce
radiation exposure for patients and surgeons, by compensating EMT error.
  Methods: Our online compensation strategy exploits cycle-consistent
generative adversarial neural networks (CycleGAN). 3D positions are translated
from various bedside environments to their bench equivalents. Domain-translated
points are fine-tuned to reduce error in the bench domain. We evaluate our
compensation approach in a phantom experiment.
  Results: Since the domain-translation approach maps distorted points to their
lab equivalents, predictions are consistent among different C-arm environments.
Error is successfully reduced in all evaluation environments. Our qualitative
phantom experiment demonstrates that our approach generalizes well to an unseen
C-arm environment.
  Conclusion: Adversarial, cycle-consistent training is an explicable,
consistent and thus interpretable approach for online error compensation.
Qualitative assessment of EMT error compensation gives a glimpse to the
potential of our method for rotational error compensation.",arxiv
http://arxiv.org/abs/2103.03819v1,2021-03-05T17:34:52Z,2021-03-05T17:34:52Z,"Hybrid Point Cloud Semantic Compression for Automotive Sensors: A
  Performance Evaluation","In a fully autonomous driving framework, where vehicles operate without human
intervention, information sharing plays a fundamental role. In this context,
new network solutions have to be designed to handle the large volumes of data
generated by the rich sensor suite of the cars in a reliable and efficient way.
Among all the possible sensors, Light Detection and Ranging (LiDAR) can produce
an accurate 3D point cloud representation of the surrounding environment, which
in turn generates high data rates. For this reason, efficient point cloud
compression is paramount to alleviate the burden of data transmission over
bandwidth-constrained channels and to facilitate real-time communications. In
this paper, we propose a pipeline to efficiently compress LiDAR observations in
an automotive scenario. First, we leverage the capabilities of RangeNet++, a
Deep Neural Network (DNN) used to semantically infer point labels, to reduce
the channel load by selecting the most valuable environmental data to be
disseminated. Second, we compress the selected points using Draco, a 3D
compression algorithm which is able to obtain compression up to the
quantization error. Our experiments, validated on the Semantic KITTI dataset,
demonstrate that it is possible to compress and send the information at the
frame rate of the LiDAR, thus achieving real-time performance.",arxiv
http://arxiv.org/abs/1807.10583v1,2018-07-19T12:07:50Z,2018-07-19T12:07:50Z,"EchoFusion: Tracking and Reconstruction of Objects in 4D Freehand
  Ultrasound Imaging without External Trackers","Ultrasound (US) is the most widely used fetal imaging technique. However, US
images have limited capture range, and suffer from view dependent artefacts
such as acoustic shadows. Compounding of overlapping 3D US acquisitions into a
high-resolution volume can extend the field of view and remove image artefacts,
which is useful for retrospective analysis including population based studies.
However, such volume reconstructions require information about relative
transformations between probe positions from which the individual volumes were
acquired. In prenatal US scans, the fetus can move independently from the
mother, making external trackers such as electromagnetic or optical tracking
unable to track the motion between probe position and the moving fetus. We
provide a novel methodology for image-based tracking and volume reconstruction
by combining recent advances in deep learning and simultaneous localisation and
mapping (SLAM). Tracking semantics are established through the use of a
Residual 3D U-Net and the output is fed to the SLAM algorithm. As a proof of
concept, experiments are conducted on US volumes taken from a whole body fetal
phantom, and from the heads of real fetuses. For the fetal head segmentation,
we also introduce a novel weak annotation approach to minimise the required
manual effort for ground truth annotation. We evaluate our method
qualitatively, and quantitatively with respect to tissue discrimination
accuracy and tracking robustness.",arxiv
http://arxiv.org/abs/1708.00187v1,2017-08-01T07:23:53Z,2017-08-01T07:23:53Z,Real-time Deep Video Deinterlacing,"Interlacing is a widely used technique, for television broadcast and video
recording, to double the perceived frame rate without increasing the bandwidth.
But it presents annoying visual artifacts, such as flickering and silhouette
""serration,"" during the playback. Existing state-of-the-art deinterlacing
methods either ignore the temporal information to provide real-time performance
but lower visual quality, or estimate the motion for better deinterlacing but
with a trade-off of higher computational cost. In this paper, we present the
first and novel deep convolutional neural networks (DCNNs) based method to
deinterlace with high visual quality and real-time performance. Unlike existing
models for super-resolution problems which relies on the translation-invariant
assumption, our proposed DCNN model utilizes the temporal information from both
the odd and even half frames to reconstruct only the missing scanlines, and
retains the given odd and even scanlines for producing the full deinterlaced
frames. By further introducing a layer-sharable architecture, our system can
achieve real-time performance on a single GPU. Experiments shows that our
method outperforms all existing methods, in terms of reconstruction accuracy
and computational performance.",arxiv
http://arxiv.org/abs/1811.10280v2,2019-03-01T10:52:51Z,2018-11-26T10:47:41Z,"Using Variable Natural Environment Brain-Computer Interface Stimuli for
  Real-time Humanoid Robot Navigation","This paper addresses the challenge of humanoid robot teleoperation in a
natural indoor environment via a Brain-Computer Interface (BCI). We leverage
deep Convolutional Neural Network (CNN) based image and signal understanding to
facilitate both real-time bject detection and dry-Electroencephalography (EEG)
based human cortical brain bio-signals decoding. We employ recent advances in
dry-EEG technology to stream and collect the cortical waveforms from subjects
while they fixate on variable Steady State Visual Evoked Potential (SSVEP)
stimuli generated directly from the environment the robot is navigating. To
these ends, we propose the use of novel variable BCI stimuli by utilising the
real-time video streamed via the on-board robot camera as visual input for
SSVEP, where the CNN detected natural scene objects are altered and flickered
with differing frequencies (10Hz, 12Hz and 15Hz). These stimuli are not akin to
traditional stimuli - as both the dimensions of the flicker regions and their
on-screen position changes depending on the scene objects detected. On-screen
object selection via such a dry-EEG enabled SSVEP methodology, facilitates the
on-line decoding of human cortical brain signals, via a specialised secondary
CNN, directly into teleoperation robot commands (approach object, move in a
specific direction: right, left or back). This SSVEP decoding model is trained
via a priori offline experimental data in which very similar visual input is
present for all subjects. The resulting classification demonstrates high
performance with mean accuracy of 85% for the real-time robot navigation
experiment across multiple test subjects.",arxiv
http://arxiv.org/abs/1809.04427v1,2018-09-12T13:39:27Z,2018-09-12T13:39:27Z,"Real-time Multiple People Tracking with Deeply Learned Candidate
  Selection and Person Re-Identification","Online multi-object tracking is a fundamental problem in time-critical video
analysis applications. A major challenge in the popular tracking-by-detection
framework is how to associate unreliable detection results with existing
tracks. In this paper, we propose to handle unreliable detection by collecting
candidates from outputs of both detection and tracking. The intuition behind
generating redundant candidates is that detection and tracks can complement
each other in different scenarios. Detection results of high confidence prevent
tracking drifts in the long term, and predictions of tracks can handle noisy
detection caused by occlusion. In order to apply optimal selection from a
considerable amount of candidates in real-time, we present a novel scoring
function based on a fully convolutional neural network, that shares most
computations on the entire image. Moreover, we adopt a deeply learned
appearance representation, which is trained on large-scale person
re-identification datasets, to improve the identification ability of our
tracker. Extensive experiments show that our tracker achieves real-time and
state-of-the-art performance on a widely used people tracking benchmark.",arxiv
http://arxiv.org/abs/1905.02292v1,2019-05-06T23:37:05Z,2019-05-06T23:37:05Z,Frame-wise Motion and Appearance for Real-time Multiple Object Tracking,"The main challenge of Multiple Object Tracking (MOT) is the efficiency in
associating indefinite number of objects between video frames. Standard motion
estimators used in tracking, e.g., Long Short Term Memory (LSTM), only deal
with single object, while Re-IDentification (Re-ID) based approaches
exhaustively compare object appearances. Both approaches are computationally
costly when they are scaled to a large number of objects, making it very
difficult for real-time MOT. To address these problems, we propose a highly
efficient Deep Neural Network (DNN) that simultaneously models association
among indefinite number of objects. The inference computation of the DNN does
not increase with the number of objects. Our approach, Frame-wise Motion and
Appearance (FMA), computes the Frame-wise Motion Fields (FMF) between two
frames, which leads to very fast and reliable matching among a large number of
object bounding boxes. As auxiliary information is used to fix uncertain
matches, Frame-wise Appearance Features (FAF) are learned in parallel with
FMFs. Extensive experiments on the MOT17 benchmark show that our method
achieved real-time MOT with competitive results as the state-of-the-art
approaches.",arxiv
http://arxiv.org/abs/2005.14695v1,2020-05-29T17:35:23Z,2020-05-29T17:35:23Z,"Non-Rigid Volume to Surface Registration using a Data-Driven
  Biomechanical Model","Non-rigid registration is a key component in soft-tissue navigation. We focus
on laparoscopic liver surgery, where we register the organ model obtained from
a preoperative CT scan to the intraoperative partial organ surface,
reconstructed from the laparoscopic video. This is a challenging task due to
sparse and noisy intraoperative data, real-time requirements and many unknowns
- such as tissue properties and boundary conditions. Furthermore, establishing
correspondences between pre- and intraoperative data can be extremely difficult
since the liver usually lacks distinct surface features and the used imaging
modalities suffer from very different types of noise. In this work, we train a
convolutional neural network to perform both the search for surface
correspondences as well as the non-rigid registration in one step. The network
is trained on physically accurate biomechanical simulations of randomly
generated, deforming organ-like structures. This enables the network to
immediately generalize to a new patient organ without the need to re-train. We
add various amounts of noise to the intraoperative surfaces during training,
making the network robust to noisy intraoperative data. During inference, the
network outputs the displacement field which matches the preoperative volume to
the partial intraoperative surface. In multiple experiments, we show that the
network translates well to real data while maintaining a high inference speed.
Our code is made available online.",arxiv
http://arxiv.org/abs/1706.02631v4,2019-04-15T20:56:19Z,2017-06-08T15:16:36Z,Sliced Wasserstein Generative Models,"In generative modeling, the Wasserstein distance (WD) has emerged as a useful
metric to measure the discrepancy between generated and real data
distributions. Unfortunately, it is challenging to approximate the WD of
high-dimensional distributions. In contrast, the sliced Wasserstein distance
(SWD) factorizes high-dimensional distributions into their multiple
one-dimensional marginal distributions and is thus easier to approximate. In
this paper, we introduce novel approximations of the primal and dual SWD.
Instead of using a large number of random projections, as it is done by
conventional SWD approximation methods, we propose to approximate SWDs with a
small number of parameterized orthogonal projections in an end-to-end deep
learning fashion. As concrete applications of our SWD approximations, we design
two types of differentiable SWD blocks to equip modern generative
frameworks---Auto-Encoders (AE) and Generative Adversarial Networks (GAN). In
the experiments, we not only show the superiority of the proposed generative
models on standard image synthesis benchmarks, but also demonstrate the
state-of-the-art performance on challenging high resolution image and video
generation in an unsupervised manner.",arxiv
http://arxiv.org/abs/1803.03254v1,2018-03-08T18:52:03Z,2018-03-08T18:52:03Z,"GONet: A Semi-Supervised Deep Learning Approach For Traversability
  Estimation","We present semi-supervised deep learning approaches for traversability
estimation from fisheye images. Our method, GONet, and the proposed extensions
leverage Generative Adversarial Networks (GANs) to effectively predict whether
the area seen in the input image(s) is safe for a robot to traverse. These
methods are trained with many positive images of traversable places, but just a
small set of negative images depicting blocked and unsafe areas. This makes the
proposed methods practical. Positive examples can be collected easily by simply
operating a robot through traversable spaces, while obtaining negative examples
is time consuming, costly, and potentially dangerous. Through extensive
experiments and several demonstrations, we show that the proposed
traversability estimation approaches are robust and can generalize to unseen
scenarios. Further, we demonstrate that our methods are memory efficient and
fast, allowing for real-time operation on a mobile robot with single or stereo
fisheye cameras. As part of our contributions, we open-source two new datasets
for traversability estimation. These datasets are composed of approximately 24h
of videos from more than 25 indoor environments. Our methods outperform
baseline approaches for traversability estimation on these new datasets.",arxiv
http://arxiv.org/abs/1904.05408v2,2019-04-13T06:53:02Z,2019-04-10T19:49:43Z,Sliced Wasserstein Generative Models,"In generative modeling, the Wasserstein distance (WD) has emerged as a useful
metric to measure the discrepancy between generated and real data
distributions. Unfortunately, it is challenging to approximate the WD of
high-dimensional distributions. In contrast, the sliced Wasserstein distance
(SWD) factorizes high-dimensional distributions into their multiple
one-dimensional marginal distributions and is thus easier to approximate. In
this paper, we introduce novel approximations of the primal and dual SWD.
Instead of using a large number of random projections, as it is done by
conventional SWD approximation methods, we propose to approximate SWDs with a
small number of parameterized orthogonal projections in an end-to-end deep
learning fashion. As concrete applications of our SWD approximations, we design
two types of differentiable SWD blocks to equip modern generative
frameworks---Auto-Encoders (AE) and Generative Adversarial Networks (GAN). In
the experiments, we not only show the superiority of the proposed generative
models on standard image synthesis benchmarks, but also demonstrate the
state-of-the-art performance on challenging high resolution image and video
generation in an unsupervised manner.",arxiv
http://arxiv.org/abs/1807.08772v1,2018-07-23T18:13:16Z,2018-07-23T18:13:16Z,Identity Preserving Face Completion for Large Ocular Region Occlusion,"We present a novel deep learning approach to synthesize complete face images
in the presence of large ocular region occlusions. This is motivated by recent
surge of VR/AR displays that hinder face-to-face communications. Different from
the state-of-the-art face inpainting methods that have no control over the
synthesized content and can only handle frontal face pose, our approach can
faithfully recover the missing content under various head poses while
preserving the identity. At the core of our method is a novel generative
network with dedicated constraints to regularize the synthesis process. To
preserve the identity, our network takes an arbitrary occlusion-free image of
the target identity to infer the missing content, and its high-level CNN
features as an identity prior to regularize the searching space of generator.
Since the input reference image may have a different pose, a pose map and a
novel pose discriminator are further adopted to supervise the learning of
implicit pose transformations. Our method is capable of generating coherent
facial inpainting with consistent identity over videos with large variations of
head motions. Experiments on both synthesized and real data demonstrate that
our method greatly outperforms the state-of-the-art methods in terms of both
synthesis quality and robustness.",arxiv
http://arxiv.org/abs/1912.00778v1,2019-11-27T15:48:26Z,2019-11-27T15:48:26Z,"Learning a faceted customer segmentation for discovering new business
  opportunities at Intel","For sales and marketing organizations within large enterprises, identifying
and understanding new markets, customers and partners is a key challenge.
Intel's Sales and Marketing Group (SMG) faces similar challenges while growing
in new markets and domains and evolving its existing business. In today's
complex technological and commercial landscape, there is need for intelligent
automation supporting a fine-grained understanding of businesses in order to
help SMG sift through millions of companies across many geographies and
languages and identify relevant directions. We present a system developed in
our company that mines millions of public business web pages, and extracts a
faceted customer representation. We focus on two key customer aspects that are
essential for finding relevant opportunities: industry segments (ranging from
broad verticals such as healthcare, to more specific fields such as 'video
analytics') and functional roles (e.g., 'manufacturer' or 'retail'). To address
the challenge of labeled data collection, we enrich our data with external
information gleaned from Wikipedia, and develop a semi-supervised multi-label,
multi-lingual deep learning model that parses customer website texts and
classifies them into their respective facets. Our system scans and indexes
companies as part of a large-scale knowledge graph that currently holds tens of
millions of connected entities with thousands being fetched, enriched and
connected to the graph by the hour in real time, and also supports knowledge
and insight discovery. In experiments conducted in our company, we are able to
significantly boost the performance of sales personnel in the task of
discovering new customers and commercial partnership opportunities.",arxiv
http://arxiv.org/abs/1902.09777v3,2019-04-24T11:47:13Z,2019-02-26T07:39:11Z,"Single-Image Piece-wise Planar 3D Reconstruction via Associative
  Embedding","Single-image piece-wise planar 3D reconstruction aims to simultaneously
segment plane instances and recover 3D plane parameters from an image. Most
recent approaches leverage convolutional neural networks (CNNs) and achieve
promising results. However, these methods are limited to detecting a fixed
number of planes with certain learned order. To tackle this problem, we propose
a novel two-stage method based on associative embedding, inspired by its recent
success in instance segmentation. In the first stage, we train a CNN to map
each pixel to an embedding space where pixels from the same plane instance have
similar embeddings. Then, the plane instances are obtained by grouping the
embedding vectors in planar regions via an efficient mean shift clustering
algorithm. In the second stage, we estimate the parameter for each plane
instance by considering both pixel-level and instance-level consistencies. With
the proposed method, we are able to detect an arbitrary number of planes.
Extensive experiments on public datasets validate the effectiveness and
efficiency of our method. Furthermore, our method runs at 30 fps at the testing
time, thus could facilitate many real-time applications such as visual SLAM and
human-robot interaction. Code is available at
https://github.com/svip-lab/PlanarReconstruction.",arxiv
http://arxiv.org/abs/2009.01998v1,2020-09-04T03:43:24Z,2020-09-04T03:43:24Z,"SSP-Net: Scalable Sequential Pyramid Networks for Real-Time 3D Human
  Pose Regression","In this paper we propose a highly scalable convolutional neural network,
end-to-end trainable, for real-time 3D human pose regression from still RGB
images. We call this approach the Scalable Sequential Pyramid Networks
(SSP-Net) as it is trained with refined supervision at multiple scales in a
sequential manner. Our network requires a single training procedure and is
capable of producing its best predictions at 120 frames per second (FPS), or
acceptable predictions at more than 200 FPS when cut at test time. We show that
the proposed regression approach is invariant to the size of feature maps,
allowing our method to perform multi-resolution intermediate supervisions and
reaching results comparable to the state-of-the-art with very low resolution
feature maps. We demonstrate the accuracy and the effectiveness of our method
by providing extensive experiments on two of the most important publicly
available datasets for 3D pose estimation, Human3.6M and MPI-INF-3DHP.
Additionally, we provide relevant insights about our decisions on the network
architecture and show its flexibility to meet the best precision-speed
compromise.",arxiv
http://arxiv.org/abs/2009.05792v1,2020-09-12T13:28:28Z,2020-09-12T13:28:28Z,A CNN Based Approach for the Near-Field Photometric Stereo Problem,"Reconstructing the 3D shape of an object using several images under different
light sources is a very challenging task, especially when realistic assumptions
such as light propagation and attenuation, perspective viewing geometry and
specular light reflection are considered. Many of works tackling Photometric
Stereo (PS) problems often relax most of the aforementioned assumptions.
Especially they ignore specular reflection and global illumination effects. In
this work, we propose the first CNN based approach capable of handling these
realistic assumptions in Photometric Stereo. We leverage recent improvements of
deep neural networks for far-field Photometric Stereo and adapt them to near
field setup. We achieve this by employing an iterative procedure for shape
estimation which has two main steps. Firstly we train a per-pixel CNN to
predict surface normals from reflectance samples. Secondly, we compute the
depth by integrating the normal field in order to iteratively estimate light
directions and attenuation which is used to compensate the input images to
compute reflectance samples for the next iteration. To the best of our
knowledge this is the first near-field framework which is able to accurately
predict 3D shape from highly specular objects. Our method outperforms competing
state-of-the-art near-field Photometric Stereo approaches on both synthetic and
real experiments.",arxiv
http://arxiv.org/abs/2012.03680v1,2020-11-12T09:31:09Z,2020-11-12T09:31:09Z,UNOC: Understanding Occlusion for Embodied Presence in Virtual Reality,"Tracking body and hand motions in the 3D space is essential for social and
self-presence in augmented and virtual environments. Unlike the popular 3D pose
estimation setting, the problem is often formulated as inside-out tracking
based on embodied perception (e.g., egocentric cameras, handheld sensors). In
this paper, we propose a new data-driven framework for inside-out body
tracking, targeting challenges of omnipresent occlusions in optimization-based
methods (e.g., inverse kinematics solvers). We first collect a large-scale
motion capture dataset with both body and finger motions using optical markers
and inertial sensors. This dataset focuses on social scenarios and captures
ground truth poses under self-occlusions and body-hand interactions. We then
simulate the occlusion patterns in head-mounted camera views on the captured
ground truth using a ray casting algorithm and learn a deep neural network to
infer the occluded body parts. In the experiments, we show that our method is
able to generate high-fidelity embodied poses by applying the proposed method
on the task of real-time inside-out body tracking, finger motion synthesis, and
3-point inverse kinematics.",arxiv
http://arxiv.org/abs/2101.10811v1,2021-01-26T14:34:49Z,2021-01-26T14:34:49Z,"Semi-synthesis: A fast way to produce effective datasets for stereo
  matching","Stereo matching is an important problem in computer vision which has drawn
tremendous research attention for decades. Recent years, data-driven methods
with convolutional neural networks (CNNs) are continuously pushing stereo
matching to new heights. However, data-driven methods require large amount of
training data, which is not an easy task for real stereo data due to the
annotation difficulties of per-pixel ground-truth disparity. Though synthetic
dataset is proposed to fill the gaps of large data demand, the fine-tuning on
real dataset is still needed due to the domain variances between synthetic data
and real data. In this paper, we found that in synthetic datasets,
close-to-real-scene texture rendering is a key factor to boost up stereo
matching performance, while close-to-real-scene 3D modeling is less important.
We then propose semi-synthetic, an effective and fast way to synthesize large
amount of data with close-to-real-scene texture to minimize the gap between
synthetic data and real data. Extensive experiments demonstrate that models
trained with our proposed semi-synthetic datasets achieve significantly better
performance than with general synthetic datasets, especially on real data
benchmarks with limited training data. With further fine-tuning on the real
dataset, we also achieve SOTA performance on Middlebury and competitive results
on KITTI and ETH3D datasets.",arxiv
http://arxiv.org/abs/2004.09039v2,2020-10-10T19:44:44Z,2020-04-20T03:25:10Z,"X-Ray: Mechanical Search for an Occluded Object by Minimizing Support of
  Learned Occupancy Distributions","For applications in e-commerce, warehouses, healthcare, and home service,
robots are often required to search through heaps of objects to grasp a
specific target object. For mechanical search, we introduce X-Ray, an algorithm
based on learned occupancy distributions. We train a neural network using a
synthetic dataset of RGBD heap images labeled for a set of standard bounding
box targets with varying aspect ratios. X-Ray minimizes support of the learned
distribution as part of a mechanical search policy in both simulated and real
environments. We benchmark these policies against two baseline policies on
1,000 heaps of 15 objects in simulation where the target object is partially or
fully occluded. Results suggest that X-Ray is significantly more efficient, as
it succeeds in extracting the target object 82% of the time, 15% more often
than the best-performing baseline. Experiments on an ABB YuMi robot with 20
heaps of 25 household objects suggest that the learned policy transfers easily
to a physical system, where it outperforms baseline policies by 15% in success
rate with 17% fewer actions. Datasets, videos, and experiments are available at
https://sites.google.com/berkeley.edu/x-ray.",arxiv
http://arxiv.org/abs/2105.02857v2,2021-10-17T20:31:10Z,2021-05-06T17:46:13Z,"Visual Foresight Tree for Object Retrieval from Clutter with
  Nonprehensile Rearrangement","This paper considers the problem of retrieving an object from many tightly
packed objects using a combination of robotic pushing and grasping actions.
Object retrieval in dense clutter is an important skill for robots to operate
in households and everyday environments effectively. The proposed solution,
Visual Foresight Tree (VFT), intelligently rearranges the clutter surrounding a
target object so that it can be grasped easily. Rearrangement with nested
nonprehensile actions is challenging as it requires predicting complex object
interactions in a combinatorially large configuration space of multiple
objects. We first show that a deep neural network can be trained to accurately
predict the poses of the packed objects when the robot pushes one of them. The
predictive network provides visual foresight and is used in a tree search as a
state transition function in the space of scene images. The tree search returns
a sequence of consecutive push actions yielding the best arrangement of the
clutter for grasping the target object. Experiments in simulation and using a
real robot and objects show that the proposed approach outperforms model-free
techniques as well as model-based myopic methods both in terms of success rates
and the number of executed actions, on several challenging tasks. A video
introducing VFT, with robot experiments, is accessible at
https://youtu.be/7cL-hmgvyec. The full source code is available at
https://github.com/arc-l/vft.",arxiv
http://arxiv.org/abs/1405.3134v4,2015-01-07T14:44:40Z,2014-05-13T12:56:05Z,OGLE-III Microlensing Events and the Structure of the Galactic Bulge,"We present and study the largest and the most comprehensive catalog of
microlensing events ever constructed. The sample of standard microlensing
events comprises 3718 unique events from years 2001--2009, with 1409 not
detected before in real-time by the Early Warning System of the Optical
Gravitational Lensing Experiment (OGLE). The search pipeline makes use of
Machine Learning algorithms in order to help find rare phenomena among 150
million objects and derive the detection efficiency. Applications of the
catalog can be numerous, from analyzing individual events to large statistical
studies for the Galactic mass and kinematics distributions and planetary
abundances.
  We derive the maps of the mean Einstein ring crossing time of events spanning
31 sq. deg. toward of the Galactic Center and compare the observed
distributions with the most recent models. We find good agreement within the
observed region and we see the signature of the tilt of the bar in the
microlensing data. However, the asymmetry of the mean time-scales seems to rise
more steeply than predictions, indicating either a somewhat different
orientation of the bar or a larger bar width. The map for the events with
sources in the Galactic bulge shows a dependence of the mean time-scale on the
Galactic latitude, signaling an increasing contribution from disk lenses closer
to the plane, related with the height of the disk. Our data present a perfect
set for comparing and enhancing new models of the central parts of the Milky
Way and creating the 3D picture of the Galaxy.",arxiv
http://arxiv.org/abs/2004.10389v4,2021-07-15T14:46:23Z,2020-04-22T04:04:58Z,"Physics-constrained, low-dimensional models for MHD: First-principles
  and data-driven approaches","Plasmas are highly nonlinear and multi-scale, motivating a hierarchy of
models to understand and describe their behavior. However, there is a scarcity
of plasma models of lower fidelity than magnetohydrodynamics (MHD), although
these reduced models hold promise for understanding key physical mechanisms,
efficient computation, and real-time optimization and control. Galerkin models,
obtained by projection of the MHD equations onto a truncated modal basis, and
data-driven models, obtained by modern machine learning and system
identification, can furnish this gap in the lower levels of the model
hierarchy. This work develops a reduced-order modeling framework for
compressible plasmas, leveraging decades of progress in projection-based and
data-driven modeling of fluids. We begin by formalizing projection-based model
reduction for nonlinear MHD systems. To avoid separate modal decompositions for
the magnetic, velocity, and pressure fields, we introduce an energy inner
product to synthesize all of the fields into a dimensionally-consistent,
reduced-order basis. Next, we obtain an analytic model by Galerkin projection
of the Hall-MHD equations onto these modes. We illustrate how global
conservation laws constrain the model parameters, revealing symmetries that can
be enforced in data-driven models, directly connecting these models to the
underlying physics. We demonstrate the effectiveness of this approach on data
from high-fidelity numerical simulations of a 3D spheromak experiment. This
manuscript builds a bridge to the extensive Galerkin literature in fluid
mechanics, and facilitates future principled development of projection-based
and data-driven models for plasmas.",arxiv
http://arxiv.org/abs/1709.01684v1,2017-09-06T06:17:06Z,2017-09-06T06:17:06Z,Evaluating Content-centric vs User-centric Ad Affect Recognition,"Despite the fact that advertisements (ads) often include strongly emotional
content, very little work has been devoted to affect recognition (AR) from ads.
This work explicitly compares content-centric and user-centric ad AR
methodologies, and evaluates the impact of enhanced AR on computational
advertising via a user study. Specifically, we (1) compile an affective ad
dataset capable of evoking coherent emotions across users; (2) explore the
efficacy of content-centric convolutional neural network (CNN) features for
encoding emotions, and show that CNN features outperform low-level emotion
descriptors; (3) examine user-centered ad AR by analyzing Electroencephalogram
(EEG) responses acquired from eleven viewers, and find that EEG signals encode
emotional information better than content descriptors; (4) investigate the
relationship between objective AR and subjective viewer experience while
watching an ad-embedded online video stream based on a study involving 12
users. To our knowledge, this is the first work to (a) expressly compare user
vs content-centered AR for ads, and (b) study the relationship between modeling
of ad emotions and its impact on a real-life advertising application.",arxiv
http://arxiv.org/abs/1806.01013v2,2018-09-26T11:58:14Z,2018-06-04T08:52:28Z,Synthetic data generation for end-to-end thermal infrared tracking,"The usage of both off-the-shelf and end-to-end trained deep networks have
significantly improved performance of visual tracking on RGB videos. However,
the lack of large labeled datasets hampers the usage of convolutional neural
networks for tracking in thermal infrared (TIR) images. Therefore, most state
of the art methods on tracking for TIR data are still based on handcrafted
features. To address this problem, we propose to use image-to-image translation
models. These models allow us to translate the abundantly available labeled RGB
data to synthetic TIR data. We explore both the usage of paired and unpaired
image translation models for this purpose. These methods provide us with a
large labeled dataset of synthetic TIR sequences, on which we can train
end-to-end optimal features for tracking. To the best of our knowledge we are
the first to train end-to-end features for TIR tracking. We perform extensive
experiments on VOT-TIR2017 dataset. We show that a network trained on a large
dataset of synthetic TIR data obtains better performance than one trained on
the available real TIR data. Combining both data sources leads to further
improvement. In addition, when we combine the network with motion features we
outperform the state of the art with a relative gain of over 10%, clearly
showing the efficiency of using synthetic data to train end-to-end TIR
trackers.",arxiv
http://arxiv.org/abs/1904.10348v2,2020-04-01T16:11:27Z,2019-04-23T14:15:37Z,"Monte-Carlo Tree Search for Efficient Visually Guided Rearrangement
  Planning","We address the problem of visually guided rearrangement planning with many
movable objects, i.e., finding a sequence of actions to move a set of objects
from an initial arrangement to a desired one, while relying on visual inputs
coming from an RGB camera. To do so, we introduce a complete pipeline relying
on two key contributions. First, we introduce an efficient and scalable
rearrangement planning method, based on a Monte-Carlo Tree Search exploration
strategy. We demonstrate that because of its good trade-off between exploration
and exploitation our method (i) scales well with the number of objects while
(ii) finding solutions which require a smaller number of moves compared to the
other state-of-the-art approaches. Note that on the contrary to many
approaches, we do not require any buffer space to be available. Second, to
precisely localize movable objects in the scene, we develop an integrated
approach for robust multi-object workspace state estimation from a single
uncalibrated RGB camera using a deep neural network trained only with synthetic
data. We validate our multi-object visually guided manipulation pipeline with
several experiments on a real UR-5 robotic arm by solving various rearrangement
planning instances, requiring only 60 ms to compute the plan to rearrange 25
objects. In addition, we show that our system is insensitive to camera
movements and can successfully recover from external perturbations.
Supplementary video, source code and pre-trained models are available at
https://ylabbe.github.io/rearrangement-planning.",arxiv
http://arxiv.org/abs/2012.12305v2,2021-07-22T16:53:43Z,2020-12-22T19:27:11Z,"Confronting Abusive Language Online: A Survey from the Ethical and Human
  Rights Perspective","The pervasiveness of abusive content on the internet can lead to severe
psychological and physical harm. Significant effort in Natural Language
Processing (NLP) research has been devoted to addressing this problem through
abusive content detection and related sub-areas, such as the detection of hate
speech, toxicity, cyberbullying, etc. Although current technologies achieve
high classification performance in research studies, it has been observed that
the real-life application of this technology can cause unintended harms, such
as the silencing of under-represented groups. We review a large body of NLP
research on automatic abuse detection with a new focus on ethical challenges,
organized around eight established ethical principles: privacy, accountability,
safety and security, transparency and explainability, fairness and
non-discrimination, human control of technology, professional responsibility,
and promotion of human values. In many cases, these principles relate not only
to situational ethical codes, which may be context-dependent, but are in fact
connected to universal human rights, such as the right to privacy, freedom from
discrimination, and freedom of expression. We highlight the need to examine the
broad social impacts of this technology, and to bring ethical and human rights
considerations to every stage of the application life-cycle, from task
formulation and dataset design, to model training and evaluation, to
application deployment. Guided by these principles, we identify several
opportunities for rights-respecting, socio-technical solutions to detect and
confront online abuse, including `nudging', `quarantining', value sensitive
design, counter-narratives, style transfer, and AI-driven public education
applications.",arxiv
http://arxiv.org/abs/1911.12889v1,2019-11-28T22:49:48Z,2019-11-28T22:49:48Z,"Fruit Detection, Segmentation and 3D Visualisation of Environments in
  Apple Orchards","Robotic harvesting of fruits in orchards is a challenging task, since high
density and overlapping of fruits and branches can heavily impact the success
rate of robotic harvesting. Therefore, the vision system is demanded to provide
comprehensive information of the working environment to guide the manipulator
and gripping system to successful detach the target fruits. In this study, a
deep learning based one-stage detector DaSNet-V2 is developed to perform the
multi-task vision sensing in the working environment of apple orchards.
DaSNet-V2 combines the detection and instance segmentation of fruits and
semantic segmentation of branch into a single network architecture. Meanwhile,
a light-weight backbone network LW-net is utilised in the DaSNet-V2 model to
improve the computational efficiency of the model. In the experiment, DaSNet-V2
is tested and evaluated on the RGB-D images of the orchard. From the experiment
results, DaSNet-V2 with lightweight backbone achieves 0.844, 0.858, and 0.795
on the F 1 score of the detection, and mean intersection of union on the
instance segmentation of fruits and semantic segmentation of branches,
respectively. To provide a direct-viewing of the working environment in
orchards, the obtained sensing results are illustrated by 3D visualisation .
The robustness and efficiency of the DaSNet-V2 in detection and segmentation
are validated by the experiments in the real-environment of apple orchard.",arxiv
http://arxiv.org/abs/2104.14300v1,2021-04-29T12:38:10Z,2021-04-29T12:38:10Z,Capability Iteration Network for Robot Path Planning,"Path planning is an important topic in robotics. Recently, value iteration
based deep learning models have achieved good performance such as Value
Iteration Network(VIN). However, previous methods suffer from slow convergence
and low accuracy on large maps, hence restricted in path planning for agents
with complex kinematics such as legged robots. Therefore, we propose a new
value iteration based path planning method called Capability Iteration
Network(CIN). CIN utilizes sparse reward maps and encodes the capability of the
agent with state-action transition probability, rather than a convolution
kernel in previous models. Furthermore, two training methods including
end-to-end training and training capability module alone are proposed, both of
which speed up convergence greatly. Several path planning experiments in
various scenarios, including on 2D, 3D grid world and real robots with
different map sizes are conducted. The results demonstrate that CIN has higher
accuracy, faster convergence, and lower sensitivity to random seed compared to
previous VI-based models, hence more applicable for real robot path planning.",arxiv
http://arxiv.org/abs/2108.11055v1,2021-08-25T05:51:58Z,2021-08-25T05:51:58Z,Normal Learning in Videos with Attention Prototype Network,"Frame reconstruction (current or future frame) based on Auto-Encoder (AE) is
a popular method for video anomaly detection. With models trained on the normal
data, the reconstruction errors of anomalous scenes are usually much larger
than those of normal ones. Previous methods introduced the memory bank into AE,
for encoding diverse normal patterns across the training videos. However, they
are memory consuming and cannot cope with unseen new scenarios in the testing
data. In this work, we propose a self-attention prototype unit (APU) to encode
the normal latent space as prototypes in real time, free from extra memory
cost. In addition, we introduce circulative attention mechanism to our backbone
to form a novel feature extracting learner, namely Circulative Attention Unit
(CAU). It enables the fast adaption capability on new scenes by only consuming
a few iterations of update. Extensive experiments are conducted on various
benchmarks. The superior performance over the state-of-the-art demonstrates the
effectiveness of our method. Our code is available at
https://github.com/huchao-AI/APN/.",arxiv
http://arxiv.org/abs/1811.09675v1,2018-11-21T01:08:51Z,2018-11-21T01:08:51Z,"CNN based dense underwater 3D scene reconstruction by transfer learning
  using bubble database","Dense 3D shape acquisition of swimming human or live fish is an important
research topic for sports, biological science and so on. For this purpose,
active stereo sensor is usually used in the air, however it cannot be applied
to the underwater environment because of refraction, strong light attenuation
and severe interference of bubbles. Passive stereo is a simple solution for
capturing dynamic scenes at underwater environment, however the shape with
textureless surfaces or irregular reflections cannot be recovered. Recently,
the stereo camera pair with a pattern projector for adding artificial textures
on the objects is proposed. However, to use the system for underwater
environment, several problems should be compensated, i.e., disturbance by
fluctuation and bubbles. Simple solution is to use convolutional neural network
for stereo to cancel the effects of bubbles and/or water fluctuation. Since it
is not easy to train CNN with small size of database with large variation, we
develop a special bubble generation device to efficiently create real bubble
database of multiple size and density. In addition, we propose a transfer
learning technique for multi-scale CNN to effectively remove bubbles and
projected-patterns on the object. Further, we develop a real system and
actually captured live swimming human, which has not been done before.
Experiments are conducted to show the effectiveness of our method compared with
the state of the art techniques.",arxiv
http://arxiv.org/abs/1801.05671v1,2018-01-17T14:14:22Z,2018-01-17T14:14:22Z,"Compact Real-time avoidance on a Humanoid Robot for Human-robot
  Interaction","With robots leaving factories and entering less controlled domains, possibly
sharing the space with humans, safety is paramount and multimodal awareness of
the body surface and the surrounding environment is fundamental. Taking
inspiration from peripersonal space representations in humans, we present a
framework on a humanoid robot that dynamically maintains such a protective
safety zone, composed of the following main components: (i) a human 2D
keypoints estimation pipeline employing a deep learning based algorithm,
extended here into 3D using disparity; (ii) a distributed peripersonal space
representation around the robot's body parts; (iii) a reaching controller that
incorporates all obstacles entering the robot's safety zone on the fly into the
task. Pilot experiments demonstrate that an effective safety margin between the
robot's and the human's body parts is kept. The proposed solution is flexible
and versatile since the safety zone around individual robot and human body
parts can be selectively modulated---here we demonstrate stronger avoidance of
the human head compared to rest of the body. Our system works in real time and
is self-contained, with no external sensory equipment and use of onboard
cameras only.",arxiv
http://arxiv.org/abs/2102.12040v2,2021-07-27T17:21:01Z,2021-02-24T03:10:32Z,"Active Learning to Classify Macromolecular Structures in situ for Less
  Supervision in Cryo-Electron Tomography","Motivation: Cryo-Electron Tomography (cryo-ET) is a 3D bioimaging tool that
visualizes the structural and spatial organization of macromolecules at a
near-native state in single cells, which has broad applications in life
science. However, the systematic structural recognition and recovery of
macromolecules captured by cryo-ET are difficult due to high structural
complexity and imaging limits. Deep learning based subtomogram classification
have played critical roles for such tasks. As supervised approaches, however,
their performance relies on sufficient and laborious annotation on a large
training dataset.
  Results: To alleviate this major labeling burden, we proposed a Hybrid Active
Learning (HAL) framework for querying subtomograms for labelling from a large
unlabeled subtomogram pool. Firstly, HAL adopts uncertainty sampling to select
the subtomograms that have the most uncertain predictions. Moreover, to
mitigate the sampling bias caused by such strategy, a discriminator is
introduced to judge if a certain subtomogram is labeled or unlabeled and
subsequently the model queries the subtomogram that have higher probabilities
to be unlabeled. Additionally, HAL introduces a subset sampling strategy to
improve the diversity of the query set, so that the information overlap is
decreased between the queried batches and the algorithmic efficiency is
improved. Our experiments on subtomogram classification tasks using both
simulated and real data demonstrate that we can achieve comparable testing
performance (on average only 3% accuracy drop) by using less than 30% of the
labeled subtomograms, which shows a very promising result for subtomogram
classification task with limited labeling resources.",arxiv
http://arxiv.org/abs/2107.04523v1,2021-07-09T16:18:12Z,2021-07-09T16:18:12Z,"Learning Cascaded Detection Tasks with Weakly-Supervised Domain
  Adaptation","In order to handle the challenges of autonomous driving, deep learning has
proven to be crucial in tackling increasingly complex tasks, such as 3D
detection or instance segmentation. State-of-the-art approaches for image-based
detection tasks tackle this complexity by operating in a cascaded fashion: they
first extract a 2D bounding box based on which additional attributes, e.g.
instance masks, are inferred. While these methods perform well, a key challenge
remains the lack of accurate and cheap annotations for the growing variety of
tasks. Synthetic data presents a promising solution but, despite the effort in
domain adaptation research, the gap between synthetic and real data remains an
open problem. In this work, we propose a weakly supervised domain adaptation
setting which exploits the structure of cascaded detection tasks. In
particular, we learn to infer the attributes solely from the source domain
while leveraging 2D bounding boxes as weak labels in both domains to explain
the domain shift. We further encourage domain-invariant features through
class-wise feature alignment using ground-truth class information, which is not
available in the unsupervised setting. As our experiments demonstrate, the
approach is competitive with fully supervised settings while outperforming
unsupervised adaptation approaches by a large margin.",arxiv
http://arxiv.org/abs/1903.03340v3,2019-11-11T13:37:32Z,2019-03-08T09:40:51Z,Learning to Estimate Pose and Shape of Hand-Held Objects from RGB Images,"We develop a system for modeling hand-object interactions in 3D from RGB
images that show a hand which is holding a novel object from a known category.
We design a Convolutional Neural Network (CNN) for Hand-held Object Pose and
Shape estimation called HOPS-Net and utilize prior work to estimate the hand
pose and configuration. We leverage the insight that information about the hand
facilitates object pose and shape estimation by incorporating the hand into
both training and inference of the object pose and shape as well as the
refinement of the estimated pose. The network is trained on a large synthetic
dataset of objects in interaction with a human hand. To bridge the gap between
real and synthetic images, we employ an image-to-image translation model
(Augmented CycleGAN) that generates realistically textured objects given a
synthetic rendering. This provides a scalable way of generating annotated data
for training HOPS-Net. Our quantitative experiments show that even noisy hand
parameters significantly help object pose and shape estimation. The qualitative
experiments show results of pose and shape estimation of objects held by a hand
""in the wild"".",arxiv
http://arxiv.org/abs/1811.00793v1,2018-11-02T09:42:15Z,2018-11-02T09:42:15Z,Dealing with Ambiguity in Robotic Grasping via Multiple Predictions,"Humans excel in grasping and manipulating objects because of their life-long
experience and knowledge about the 3D shape and weight distribution of objects.
However, the lack of such intuition in robots makes robotic grasping an
exceptionally challenging task. There are often several equally viable options
of grasping an object. However, this ambiguity is not modeled in conventional
systems that estimate a single, optimal grasp position. We propose to tackle
this problem by simultaneously estimating multiple grasp poses from a single
RGB image of the target object. Further, we reformulate the problem of robotic
grasping by replacing conventional grasp rectangles with grasp belief maps,
which hold more precise location information than a rectangle and account for
the uncertainty inherent to the task. We augment a fully convolutional neural
network with a multiple hypothesis prediction model that predicts a set of
grasp hypotheses in under 60ms, which is critical for real-time robotic
applications. The grasp detection accuracy reaches over 90% for unseen objects,
outperforming the current state of the art on this task.",arxiv
http://arxiv.org/abs/2004.12165v2,2020-07-16T10:06:15Z,2020-04-25T15:07:03Z,CNN based Road User Detection using the 3D Radar Cube,"This letter presents a novel radar based, single-frame, multi-class detection
method for moving road users (pedestrian, cyclist, car), which utilizes
low-level radar cube data. The method provides class information both on the
radar target- and object-level. Radar targets are classified individually after
extending the target features with a cropped block of the 3D radar cube around
their positions, thereby capturing the motion of moving parts in the local
velocity distribution. A Convolutional Neural Network (CNN) is proposed for
this classification step. Afterwards, object proposals are generated with a
clustering step, which not only considers the radar targets' positions and
velocities, but their calculated class scores as well. In experiments on a
real-life dataset we demonstrate that our method outperforms the
state-of-the-art methods both target- and object-wise by reaching an average of
0.70 (baseline: 0.68) target-wise and 0.56 (baseline: 0.48) object-wise F1
score. Furthermore, we examine the importance of the used features in an
ablation study.",arxiv
http://arxiv.org/abs/2107.04644v1,2021-07-09T19:40:20Z,2021-07-09T19:40:20Z,"Self-Supervised Generative Adversarial Network for Depth Estimation in
  Laparoscopic Images","Dense depth estimation and 3D reconstruction of a surgical scene are crucial
steps in computer assisted surgery. Recent work has shown that depth estimation
from a stereo images pair could be solved with convolutional neural networks.
However, most recent depth estimation models were trained on datasets with
per-pixel ground truth. Such data is especially rare for laparoscopic imaging,
making it hard to apply supervised depth estimation to real surgical
applications. To overcome this limitation, we propose SADepth, a new
self-supervised depth estimation method based on Generative Adversarial
Networks. It consists of an encoder-decoder generator and a discriminator to
incorporate geometry constraints during training. Multi-scale outputs from the
generator help to solve the local minima caused by the photometric reprojection
loss, while the adversarial learning improves the framework generation quality.
Extensive experiments on two public datasets show that SADepth outperforms
recent state-of-the-art unsupervised methods by a large margin, and reduces the
gap between supervised and unsupervised depth estimation in laparoscopic
images.",arxiv
http://arxiv.org/abs/2109.01587v1,2021-09-03T15:51:30Z,2021-09-03T15:51:30Z,3D Human Shape Style Transfer,"We consider the problem of modifying/replacing the shape style of a real
moving character with those of an arbitrary static real source character.
Traditional solutions follow a pose transfer strategy, from the moving
character to the source character shape, that relies on skeletal pose
parametrization. In this paper, we explore an alternative approach that
transfers the source shape style onto the moving character. The expected
benefit is to avoid the inherently difficult pose to shape conversion required
with skeletal parametrization applied on real characters. To this purpose, we
consider image style transfer techniques and investigate how to adapt them to
3D human shapes. Adaptive Instance Normalisation (AdaIN) and SPADE
architectures have been demonstrated to efficiently and accurately transfer the
style of an image onto another while preserving the original image structure.
Where AdaIN contributes with a module to perform style transfer through the
statistics of the subjects and SPADE contribute with a residual block
architecture to refine the quality of the style transfer. We demonstrate that
these approaches are extendable to the 3D shape domain by proposing a
convolutional neural network that applies the same principle of preserving the
shape structure (shape pose) while transferring the style of a new subject
shape. The generated results are supervised through a discriminator module to
evaluate the realism of the shape, whilst enforcing the decoder to synthesise
plausible shapes and improve the style transfer for unseen subjects. Our
experiments demonstrate an average of $\approx 56\%$ qualitative and
quantitative improvements over the baseline in shape transfer through
optimization-based and learning-based methods.",arxiv
http://arxiv.org/abs/2108.05616v1,2021-08-12T09:13:06Z,2021-08-12T09:13:06Z,"SSR-PR: Single-shot Super-Resolution Phase Retrieval based two prior
  calibration tests","We propose a novel approach and algorithm based on two preliminary tests of
the optical system elements to enhance the super-resolved complex-valued
imaging. The approach is developed for inverse phase imaging in a single-shot
lensless optical setup. Imaging is based on wavefront modulation by a single
binary phase mask. The preliminary tests compensate errors in the optical
system and correct a carrying wavefront, reducing the gap between real-life
experiments and computational modeling, which improve imaging significantly
both qualitatively and quantitatively. These two tests are performed for
observation of the laser beam and phase mask along, and might be considered as
a preliminary system calibration. The corrected carrying wavefront is embedded
into the proposed iterative Single-shot Super-Resolution Phase Retrieval
(SSR-PR) algorithm. Improved initial diffraction pattern upsampling, and a
combination of sparse and deep learning based filters achieves the
super-resolved reconstructions. Simulations and physical experiments
demonstrate the high-quality super-resolution phase imaging. In the
simulations, we showed that the SSR-PR algorithm corrects the errors of the
proposed optical system and reconstructs phase details 4x smaller than the
sensor pixel size. In physical experiment 2um thick lines of USAF phase-target
were resolved, which is almost 2x smaller than the sensor pixel size and
corresponds to the smallest resolvable group of used test target. For phase
bio-imaging, we provide Buccal Epithelial Cells reconstructed in computational
super-resolution and the quality was of the same level as a digital holographic
system with 40x magnification objective. Furthermore, the single-shot advantage
provides the possibility to record dynamic scenes, where the framerate is
limited only by the used camera. We provide amplitude-phase video clip of a
moving alive single-celled eukaryote.",arxiv
http://arxiv.org/abs/1908.10717v1,2019-08-28T13:31:34Z,2019-08-28T13:31:34Z,Fast Video Object Segmentation via Mask Transfer Network,"Accuracy and processing speed are two important factors that affect the use
of video object segmentation (VOS) in real applications. With the advanced
techniques of deep neural networks, the accuracy has been significantly
improved, however, the speed is still far below the real-time needs because of
the complicated network design, such as the requirement of the first frame
fine-tuning step. To overcome this limitation, we propose a novel mask transfer
network (MTN), which can greatly boost the processing speed of VOS and also
achieve a reasonable accuracy. The basic idea of MTN is to transfer the
reference mask to the target frame via an efficient global pixel matching
strategy. The global pixel matching between the reference frame and the target
frame is to ensure good matching results. To enhance the matching speed, we
perform the matching on a downsampled feature map with 1/32 of the original
frame size. At the same time, to preserve the detailed mask information in such
a small feature map, a mask network is designed to encode the annotated mask
information with 512 channels. Finally, an efficient feature warping method is
used to transfer the encoded reference mask to the target frame. Based on this
design, our method avoids the fine-tuning step on the first frame and does not
rely on the temporal cues and particular object categories. Therefore, it runs
very fast and can be conveniently trained only with images, as well as being
robust to unseen objects. Experiments on the DAVIS datasets demonstrate that
MTN can achieve a speed of 37 fps, and also shows a competitive accuracy in
comparison to the state-of-the-art methods.",arxiv
http://arxiv.org/abs/2003.04070v3,2020-05-25T01:53:18Z,2020-03-09T12:32:16Z,When Person Re-identification Meets Changing Clothes,"Person re-identification (ReID) is now an active research topic for AI-based
video surveillance applications such as specific person search, but the
practical issue that the target person(s) may change clothes (clothes
inconsistency problem) has been overlooked for long. For the first time, this
paper systematically studies this problem. We first overcome the difficulty of
lack of suitable dataset, by collecting a small yet representative real dataset
for testing whilst building a large realistic synthetic dataset for training
and deeper studies. Facilitated by our new datasets, we are able to conduct
various interesting new experiments for studying the influence of clothes
inconsistency. We find that changing clothes makes ReID a much harder problem
in the sense of bringing difficulties to learning effective representations and
also challenges the generalization ability of previous ReID models to identify
persons with unseen (new) clothes. Representative existing ReID models are
adopted to show informative results on such a challenging setting, and we also
provide some preliminary efforts on improving the robustness of existing models
on handling the clothes inconsistency issue in the data. We believe that this
study can be inspiring and helpful for encouraging more researches in this
direction. The dataset is available on the project website:
https://wanfb.github.io/dataset.html.",arxiv
http://arxiv.org/abs/2006.11684v1,2020-06-21T00:38:24Z,2020-06-21T00:38:24Z,"To Explain or Not to Explain: A Study on the Necessity of Explanations
  for Autonomous Vehicles","Explainable AI, in the context of autonomous systems, like self driving cars,
has drawn broad interests from researchers. Recent studies have found that
providing explanations for an autonomous vehicle actions has many benefits,
e.g., increase trust and acceptance, but put little emphasis on when an
explanation is needed and how the content of explanation changes with context.
In this work, we investigate which scenarios people need explanations and how
the critical degree of explanation shifts with situations and driver types.
Through a user experiment, we ask participants to evaluate how necessary an
explanation is and measure the impact on their trust in the self driving cars
in different contexts. We also present a self driving explanation dataset with
first person explanations and associated measure of the necessity for 1103
video clips, augmenting the Berkeley Deep Drive Attention dataset.
Additionally, we propose a learning based model that predicts how necessary an
explanation for a given situation in real time, using camera data inputs. Our
research reveals that driver types and context dictates whether or not an
explanation is necessary and what is helpful for improved interaction and
understanding.",arxiv
http://arxiv.org/abs/2012.07043v2,2021-05-25T13:13:27Z,2020-12-13T11:54:19Z,"Contrastive Learning of Relative Position Regression for One-Shot Object
  Localization in 3D Medical Images","Deep learning networks have shown promising performance for accurate object
localization in medial images, but require large amount of annotated data for
supervised training, which is expensive and expertise burdensome. To address
this problem, we present a one-shot framework for organ and landmark
localization in volumetric medical images, which does not need any annotation
during the training stage and could be employed to locate any landmarks or
organs in test images given a support (reference) image during the inference
stage. Our main idea comes from that tissues and organs from different human
bodies have a similar relative position and context. Therefore, we could
predict the relative positions of their non-local patches, thus locate the
target organ. Our framework is composed of three parts: (1) A projection
network trained to predict the 3D offset between any two patches from the same
volume, where human annotations are not required. In the inference stage, it
takes one given landmark in a reference image as a support patch and predicts
the offset from a random patch to the corresponding landmark in the test
(query) volume. (2) A coarse-to-fine framework contains two projection
networks, providing more accurate localization of the target. (3) Based on the
coarse-to-fine model, we transfer the organ boundingbox (B-box) detection to
locating six extreme points along x, y and z directions in the query volume.
Experiments on multi-organ localization from head-and-neck (HaN) CT volumes
showed that our method acquired competitive performance in real time, which is
more accurate and 10^5 times faster than template matching methods with the
same setting. Code is available: https://github.com/LWHYC/RPR-Loc.",arxiv
http://arxiv.org/abs/2102.11931v3,2021-05-21T20:02:03Z,2021-02-23T20:33:22Z,"Boosting background suppression in the NEXT experiment through
  Richardson-Lucy deconvolution","Next-generation neutrinoless double beta decay experiments aim for half-life
sensitivities of ~$10^{27}$ yr, requiring suppressing backgrounds to <1
count/tonne/yr. For this, any extra background rejection handle, beyond
excellent energy resolution and the use of extremely radiopure materials, is of
utmost importance. The NEXT experiment exploits differences in the spatial
ionization patterns of double beta decay and single-electron events to
discriminate signal from background. While the former display two Bragg peak
dense ionization regions at the opposite ends of the track, the latter
typically have only one such feature. Thus, comparing the energies at the track
extremes provides an additional rejection tool. The unique combination of the
topology-based background discrimination and excellent energy resolution (1%
FWHM at the Q-value of the decay) is the distinguishing feature of NEXT.
Previous studies demonstrated a topological background rejection factor of ~5
when reconstructing electron-positron pairs in the $^{208}$Tl 1.6 MeV double
escape peak (with Compton events as background), recorded in the NEXT-White
demonstrator at the Laboratorio Subterr\'aneo de Canfranc, with 72% signal
efficiency. This was recently improved through the use of a deep convolutional
neural network to yield a background rejection factor of ~10 with 65% signal
efficiency. Here, we present a new reconstruction method, based on the
Richardson-Lucy deconvolution algorithm, which allows reversing the blurring
induced by electron diffusion and electroluminescence light production in the
NEXT TPC. The new method yields highly refined 3D images of reconstructed
events, and, as a result, significantly improves the topological background
discrimination. When applied to real-data 1.6 MeV $e^-e^+$ pairs, it leads to a
background rejection factor of 27 at 57% signal efficiency.",arxiv
http://arxiv.org/abs/1801.04662v2,2018-07-03T14:54:20Z,2018-01-15T04:28:32Z,"Enlarging Context with Low Cost: Efficient Arithmetic Coding with
  Trimmed Convolution","Arithmetic coding is an essential class of coding techniques. One key issue
of arithmetic encoding method is to predict the probability of the current
coding symbol from its context, i.e., the preceding encoded symbols, which
usually can be executed by building a look-up table (LUT). However, the
complexity of LUT increases exponentially with the length of context. Thus,
such solutions are limited to modeling large context, which inevitably
restricts the compression performance. Several recent deep neural network-based
solutions have been developed to account for large context, but are still
costly in computation. The inefficiency of the existing methods are mainly
attributed to that probability prediction is performed independently for the
neighboring symbols, which actually can be efficiently conducted by shared
computation. To this end, we propose a trimmed convolutional network for
arithmetic encoding (TCAE) to model large context while maintaining
computational efficiency. As for trimmed convolution, the convolutional kernels
are specially trimmed to respect the compression order and context dependency
of the input symbols. Benefited from trimmed convolution, the probability
prediction of all symbols can be efficiently performed in one single forward
pass via a fully convolutional network. Furthermore, to speed up the decoding
process, a slope TCAE model is presented to divide the codes from a 3D code map
into several blocks and remove the dependency between the codes inner one block
for parallel decoding, which can 60x speed up the decoding process. Experiments
show that our TCAE and slope TCAE attain better compression ratio in lossless
gray image compression, and can be adopted in CNN-based lossy image compression
to achieve state-of-the-art rate-distortion performance with real-time encoding
speed.",arxiv
http://arxiv.org/abs/1906.07518v1,2019-06-18T12:11:20Z,2019-06-18T12:11:20Z,"Simultaneous Magnetic Particle Imaging and Navigation of large
  superparamagnetic nanoparticles in bifurcation flow experiments","Magnetic Particle Imaging (MPI) has been successfully used to visualize the
distribution of superparamagnetic nanoparticles within 3D volumes with high
sensitivity in real time. Since the magnetic field topology of MPI scanners is
well suited for applying magnetic forces on particles and micron-sized
ferromagnetic devices, MPI has been recently used to navigate micron-sized
particles and micron-sized swimmers. In this work, we analyze the
magnetophoretic mobility and the imaging performance of two different particle
types for Magnetic Particle Imaging/Navigation (MPIN). MPIN constantly switches
between imaging and magnetic modes, enabling quasi-simultaneous navigation and
imaging of particles. We determine the limiting flow velocity to be 8.18 mL/s
using a flow bifurcation experiment, that allows all particles to flow only
through one branch of the bifurcation. Furthermore, we have succeeded in
navigating the particles through the branch of a bifurcation phantom narrowed
by either 60% or 100% stenosis, while imaging their accumulation on the
stenosis. The particles in combination with therapeutic substances have a high
potential for targeted drug delivery and could help to reduce the dose and
improve the efficacy of the drug, e.g. for specific tumor therapy and ischemic
stroke therapy.",arxiv
