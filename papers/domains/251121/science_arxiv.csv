id,updated,published,title,summary,database
http://arxiv.org/abs/2103.13452v1,2021-03-24T19:11:58Z,2021-03-24T19:11:58Z,"A Portable, Self-Contained Neuroprosthetic Hand with Deep Learning-Based
  Finger Control","Objective: Deep learning-based neural decoders have emerged as the prominent
approach to enable dexterous and intuitive control of neuroprosthetic hands.
Yet few studies have materialized the use of deep learning in clinical settings
due to its high computational requirements. Methods: Recent advancements of
edge computing devices bring the potential to alleviate this problem. Here we
present the implementation of a neuroprosthetic hand with embedded deep
learning-based control. The neural decoder is designed based on the recurrent
neural network (RNN) architecture and deployed on the NVIDIA Jetson Nano - a
compacted yet powerful edge computing platform for deep learning inference.
This enables the implementation of the neuroprosthetic hand as a portable and
self-contained unit with real-time control of individual finger movements.
Results: The proposed system is evaluated on a transradial amputee using
peripheral nerve signals (ENG) with implanted intrafascicular microelectrodes.
The experiment results demonstrate the system's capabilities of providing
robust, high-accuracy (95-99%) and low-latency (50-120 msec) control of
individual finger movements in various laboratory and real-world environments.
Conclusion: Modern edge computing platforms enable the effective use of deep
learning-based neural decoders for neuroprosthesis control as an autonomous
system. Significance: This work helps pioneer the deployment of deep neural
networks in clinical applications underlying a new class of wearable biomedical
devices with embedded artificial intelligence.",arxiv
http://arxiv.org/abs/1912.02102v1,2019-12-03T02:11:50Z,2019-12-03T02:11:50Z,"Artificial Intelligence for Low-Resource Communities: Influence
  Maximization in an Uncertain World","The potential of Artificial Intelligence (AI) to tackle challenging problems
that afflict society is enormous, particularly in the areas of healthcare,
conservation and public safety and security. Many problems in these domains
involve harnessing social networks of under-served communities to enable
positive change, e.g., using social networks of homeless youth to raise
awareness about Human Immunodeficiency Virus (HIV) and other STDs.
Unfortunately, most of these real-world problems are characterized by
uncertainties about social network structure and influence models, and previous
research in AI fails to sufficiently address these uncertainties. This thesis
addresses these shortcomings by advancing the state-of-the-art to a new
generation of algorithms for interventions in social networks. In particular,
this thesis describes the design and development of new influence maximization
algorithms which can handle various uncertainties that commonly exist in
real-world social networks. These algorithms utilize techniques from sequential
planning problems and social network theory to develop new kinds of AI
algorithms. Further, this thesis also demonstrates the real-world impact of
these algorithms by describing their deployment in three pilot studies to
spread awareness about HIV among actual homeless youth in Los Angeles. This
represents one of the first-ever deployments of computer science based
influence maximization algorithms in this domain. Our results show that our AI
algorithms improved upon the state-of-the-art by 160% in the real-world. We
discuss research and implementation challenges faced in deploying these
algorithms, and lessons that can be gleaned for future deployment of such
algorithms. The positive results from these deployments illustrate the enormous
potential of AI in addressing societally relevant problems.",arxiv
http://arxiv.org/abs/2012.01356v1,2020-12-02T17:56:44Z,2020-12-02T17:56:44Z,"Coinbot: Intelligent Robotic Coin Bag Manipulation Using Deep
  Reinforcement Learning And Machine Teaching","Given the laborious difficulty of moving heavy bags of physical currency in
the cash center of the bank, there is a large demand for training and deploying
safe autonomous systems capable of conducting such tasks in a collaborative
workspace. However, the deformable properties of the bag along with the large
quantity of rigid-body coins contained within it, significantly increases the
challenges of bag detection, grasping and manipulation by a robotic gripper and
arm. In this paper, we apply deep reinforcement learning and machine learning
techniques to the task of controlling a collaborative robot to automate the
unloading of coin bags from a trolley. To accomplish the task-specific process
of gripping flexible materials like coin bags where the center of the mass
changes during manipulation, a special gripper was implemented in simulation
and designed in physical hardware. Leveraging a depth camera and object
detection using deep learning, a bag detection and pose estimation has been
done for choosing the optimal point of grasping. An intelligent approach based
on deep reinforcement learning has been introduced to propose the best
configuration of the robot end-effector to maximize successful grasping. A
boosted motion planning is utilized to increase the speed of motion planning
during robot operation. Real-world trials with the proposed pipeline have
demonstrated success rates over 96\% in a real-world setting.",arxiv
http://arxiv.org/abs/1910.10045v2,2019-12-26T08:09:25Z,2019-10-22T15:27:30Z,"Explainable Artificial Intelligence (XAI): Concepts, Taxonomies,
  Opportunities and Challenges toward Responsible AI","In the last years, Artificial Intelligence (AI) has achieved a notable
momentum that may deliver the best of expectations over many application
sectors across the field. For this to occur, the entire community stands in
front of the barrier of explainability, an inherent problem of AI techniques
brought by sub-symbolism (e.g. ensembles or Deep Neural Networks) that were not
present in the last hype of AI. Paradigms underlying this problem fall within
the so-called eXplainable AI (XAI) field, which is acknowledged as a crucial
feature for the practical deployment of AI models. This overview examines the
existing literature in the field of XAI, including a prospect toward what is
yet to be reached. We summarize previous efforts to define explainability in
Machine Learning, establishing a novel definition that covers prior conceptual
propositions with a major focus on the audience for which explainability is
sought. We then propose and discuss about a taxonomy of recent contributions
related to the explainability of different Machine Learning models, including
those aimed at Deep Learning methods for which a second taxonomy is built. This
literature analysis serves as the background for a series of challenges faced
by XAI, such as the crossroads between data fusion and explainability. Our
prospects lead toward the concept of Responsible Artificial Intelligence,
namely, a methodology for the large-scale implementation of AI methods in real
organizations with fairness, model explainability and accountability at its
core. Our ultimate goal is to provide newcomers to XAI with a reference
material in order to stimulate future research advances, but also to encourage
experts and professionals from other disciplines to embrace the benefits of AI
in their activity sectors, without any prior bias for its lack of
interpretability.",arxiv
http://arxiv.org/abs/1908.02150v3,2019-10-22T02:23:42Z,2019-08-04T05:19:43Z,Industrial Artificial Intelligence,"Artificial Intelligence (AI) is a cognitive science to enables human to
explore many intelligent ways to model our sensing and reasoning processes.
Industrial AI is a systematic discipline to enable engineers to systematically
develop and deploy AI algorithms with repeating and consistent successes. In
this paper, the key enablers for this transformative technology along with
their significant advantages are discussed. In addition, this research explains
Lighthouse Factories as an emerging status applying to the top manufacturers
that have implemented Industrial AI in their manufacturing ecosystem and gained
significant financial benefits. It is believed that this research will work as
a guideline and roadmap for researchers and industries towards the real-world
implementation of Industrial AI.",arxiv
http://arxiv.org/abs/1705.00346v1,2017-04-30T17:17:44Z,2017-04-30T17:17:44Z,Deep Learning in the Automotive Industry: Applications and Tools,"Deep Learning refers to a set of machine learning techniques that utilize
neural networks with many hidden layers for tasks, such as image
classification, speech recognition, language understanding. Deep learning has
been proven to be very effective in these domains and is pervasively used by
many Internet services. In this paper, we describe different automotive uses
cases for deep learning in particular in the domain of computer vision. We
surveys the current state-of-the-art in libraries, tools and infrastructures
(e.\,g.\ GPUs and clouds) for implementing, training and deploying deep neural
networks. We particularly focus on convolutional neural networks and computer
vision use cases, such as the visual inspection process in manufacturing plants
and the analysis of social media data. To train neural networks, curated and
labeled datasets are essential. In particular, both the availability and scope
of such datasets is typically very limited. A main contribution of this paper
is the creation of an automotive dataset, that allows us to learn and
automatically recognize different vehicle properties. We describe an end-to-end
deep learning application utilizing a mobile app for data collection and
process support, and an Amazon-based cloud backend for storage and training.
For training we evaluate the use of cloud and on-premises infrastructures
(including multiple GPUs) in conjunction with different neural network
architectures and frameworks. We assess both the training times as well as the
accuracy of the classifier. Finally, we demonstrate the effectiveness of the
trained classifier in a real world setting during manufacturing process.",arxiv
http://arxiv.org/abs/2104.09164v1,2021-04-19T09:41:32Z,2021-04-19T09:41:32Z,"HEAR: Human Action Recognition via Neural Networks on Homomorphically
  Encrypted Data","Remote monitoring to support ""aging in place"" is an active area of research.
Advanced computer vision technology based on deep learning can provide near
real-time home monitoring to detect falling and symptoms related to seizure,
and stroke. Affordable webcams, together with cloud computing services (to run
machine learning algorithms), can potentially bring significant social and
health benefits. However, it has not been deployed in practice because of
privacy and security concerns. People may feel uncomfortable sending their
videos of daily activities (with potentially sensitive private information) to
a computing service provider (e.g., on a commercial cloud). In this paper, we
propose a novel strategy to resolve this dilemma by applying fully homomorphic
encryption (FHE) to an alternative representation of human actions (i.e.,
skeleton joints), which guarantees information confidentiality while retaining
high-performance action detection at a low cost. We design an FHE-friendly
neural network for action recognition and present a secure neural network
evaluation strategy to achieve near real-time action detection. Our framework
for private inference achieves an 87.99% recognition accuracy (86.21%
sensitivity and 99.14% specificity in detecting falls) with a latency of 3.1
seconds on real-world datasets. Our evaluation shows that our elaborated and
fine-tuned method reduces the inference latency by 23.81%~74.67% over a
straightforward implementation.",arxiv
http://arxiv.org/abs/2002.01129v3,2021-07-12T21:18:32Z,2020-02-04T05:08:17Z,Bayesian Meta-Prior Learning Using Empirical Bayes,"Adding domain knowledge to a learning system is known to improve results. In
multi-parameter Bayesian frameworks, such knowledge is incorporated as a prior.
On the other hand, various model parameters can have different learning rates
in real-world problems, especially with skewed data. Two often-faced challenges
in Operation Management and Management Science applications are the absence of
informative priors, and the inability to control parameter learning rates. In
this study, we propose a hierarchical Empirical Bayes approach that addresses
both challenges, and that can generalize to any Bayesian framework. Our method
learns empirical meta-priors from the data itself and uses them to decouple the
learning rates of first-order and second-order features (or any other given
feature grouping) in a Generalized Linear Model. As the first-order features
are likely to have a more pronounced effect on the outcome, focusing on
learning first-order weights first is likely to improve performance and
convergence time. Our Empirical Bayes method clamps features in each group
together and uses the deployed model's observed data to empirically compute a
hierarchical prior in hindsight. We report theoretical results for the
unbiasedness, strong consistency, and optimal frequentist cumulative regret
properties of our meta-prior variance estimator. We apply our method to a
standard supervised learning optimization problem, as well as an online
combinatorial optimization problem in a contextual bandit setting implemented
in an Amazon production system. Both during simulations and live experiments,
our method shows marked improvements, especially in cases of small traffic. Our
findings are promising, as optimizing over sparse data is often a challenge.",arxiv
http://arxiv.org/abs/2110.04249v1,2021-10-08T16:58:57Z,2021-10-08T16:58:57Z,How Can AI Recognize Pain and Express Empathy,"Sensory and emotional experiences such as pain and empathy are relevant to
mental and physical health. The current drive for automated pain recognition is
motivated by a growing number of healthcare requirements and demands for social
interaction make it increasingly essential. Despite being a trending area, they
have not been explored in great detail. Over the past decades, behavioral
science and neuroscience have uncovered mechanisms that explain the
manifestations of pain. Recently, also artificial intelligence research has
allowed empathic machine learning methods to be approachable. Generally, the
purpose of this paper is to review the current developments for computational
pain recognition and artificial empathy implementation. Our discussion covers
the following topics: How can AI recognize pain from unimodality and
multimodality? Is it necessary for AI to be empathic? How can we create an AI
agent with proactive and reactive empathy? This article explores the challenges
and opportunities of real-world multimodal pain recognition from a
psychological, neuroscientific, and artificial intelligence perspective.
Finally, we identify possible future implementations of artificial empathy and
analyze how humans might benefit from an AI agent equipped with empathy.",arxiv
http://arxiv.org/abs/2005.01557v1,2020-05-04T15:16:30Z,2020-05-04T15:16:30Z,"Off-the-shelf deep learning is not enough: parsimony, Bayes and
  causality","Deep neural networks (""deep learning"") have emerged as a technology of choice
to tackle problems in natural language processing, computer vision, speech
recognition and gameplay, and in just a few years has led to superhuman level
performance and ushered in a new wave of ""AI."" Buoyed by these successes,
researchers in the physical sciences have made steady progress in incorporating
deep learning into their respective domains. However, such adoption brings
substantial challenges that need to be recognized and confronted. Here, we
discuss both opportunities and roadblocks to implementation of deep learning
within materials science, focusing on the relationship between correlative
nature of machine learning and causal hypothesis driven nature of physical
sciences. We argue that deep learning and AI are now well positioned to
revolutionize fields where causal links are known, as is the case for
applications in theory. When confounding factors are frozen or change only
weakly, this leaves open the pathway for effective deep learning solutions in
experimental domains. Similarly, these methods offer a pathway towards
understanding the physics of real-world systems, either via deriving reduced
representations, deducing algorithmic complexity, or recovering generative
physical models. However, extending deep learning and ""AI"" for models with
unclear causal relationship can produce misleading and potentially incorrect
results. Here, we argue the broad adoption of Bayesian methods incorporating
prior knowledge, development of DL solutions with incorporated physical
constraints, and ultimately adoption of causal models, offers a path forward
for fundamental and applied research. Most notably, while these advances can
change the way science is carried out in ways we cannot imagine, machine
learning is not going to substitute science any time soon.",arxiv
http://arxiv.org/abs/1909.13343v2,2019-10-01T16:06:39Z,2019-09-29T19:15:08Z,"ISTHMUS: Secure, Scalable, Real-time and Robust Machine Learning
  Platform for Healthcare","In recent times, machine learning (ML) and artificial intelligence (AI) based
systems have evolved and scaled across different industries such as finance,
retail, insurance, energy utilities, etc. Among other things, they have been
used to predict patterns of customer behavior, to generate pricing models, and
to predict the return on investments. But the successes in deploying machine
learning models at scale in those industries have not translated into the
healthcare setting. There are multiple reasons why integrating ML models into
healthcare has not been widely successful, but from a technical perspective,
general-purpose commercial machine learning platforms are not a good fit for
healthcare due to complexities in handling data quality issues, mandates to
demonstrate clinical relevance, and a lack of ability to monitor performance in
a highly regulated environment with stringent security and privacy needs. In
this paper, we describe Isthmus, a turnkey, cloud-based platform which
addresses the challenges above and reduces time to market for operationalizing
ML/AI in healthcare. Towards the end, we describe three case studies which shed
light on Isthmus capabilities. These include (1) supporting an end-to-end
lifecycle of a model which predicts trauma survivability at hospital trauma
centers, (2) bringing in and harmonizing data from disparate sources to create
a community data platform for inferring population as well as patient level
insights for Social Determinants of Health (SDoH), and (3) ingesting
live-streaming data from various IoT sensors to build models, which can
leverage real-time and longitudinal information to make advanced time-sensitive
predictions.",arxiv
http://arxiv.org/abs/1803.04873v2,2018-03-14T15:30:00Z,2018-03-13T15:17:30Z,"Using Convolutional Neural Networks for Determining Reticulocyte
  Percentage in Cats","Recent advances in artificial intelligence (AI), specifically in computer
vision (CV) and deep learning (DL), have created opportunities for novel
systems in many fields. In the last few years, deep learning applications have
demonstrated impressive results not only in fields such as autonomous driving
and robotics, but also in the field of medicine, where they have, in some
cases, even exceeded human-level performance. However, despite the huge
potential, adoption of deep learning-based methods is still slow in many areas,
especially in veterinary medicine, where we haven't been able to find any
research papers using modern convolutional neural networks (CNNs) in medical
image processing. We believe that using deep learning-based medical imaging can
enable more accurate, faster and less expensive diagnoses in veterinary
medicine. In order to do so, however, these methods have to be accessible to
everyone in this field, not just to computer scientists. To show the potential
of this technology, we present results on a real-world task in veterinary
medicine that is usually done manually: feline reticulocyte percentage. Using
an open source Keras implementation of the Single-Shot MultiBox Detector (SSD)
model architecture and training it on only 800 labeled images, we achieve an
accuracy of 98.7% at predicting the correct number of aggregate reticulocytes
in microscope images of cat blood smears. The main motivation behind this paper
is to show not only that deep learning can approach or even exceed human-level
performance on a task like this, but also that anyone in the field can
implement it, even without a background in computer science.",arxiv
http://arxiv.org/abs/2106.13219v1,2021-06-24T17:52:43Z,2021-06-24T17:52:43Z,Towards Understanding and Mitigating Social Biases in Language Models,"As machine learning methods are deployed in real-world settings such as
healthcare, legal systems, and social science, it is crucial to recognize how
they shape social biases and stereotypes in these sensitive decision-making
processes. Among such real-world deployments are large-scale pretrained
language models (LMs) that can be potentially dangerous in manifesting
undesirable representational biases - harmful biases resulting from
stereotyping that propagate negative generalizations involving gender, race,
religion, and other social constructs. As a step towards improving the fairness
of LMs, we carefully define several sources of representational biases before
proposing new benchmarks and metrics to measure them. With these tools, we
propose steps towards mitigating social biases during text generation. Our
empirical results and human evaluation demonstrate effectiveness in mitigating
bias while retaining crucial contextual information for high-fidelity text
generation, thereby pushing forward the performance-fairness Pareto frontier.",arxiv
http://arxiv.org/abs/2104.02214v1,2021-04-06T01:04:28Z,2021-04-06T01:04:28Z,"Intelligent Building Control Systems for Thermal Comfort and
  Energy-Efficiency: A Systematic Review of Artificial Intelligence-Assisted
  Techniques","Building operations represent a significant percentage of the total primary
energy consumed in most countries due to the proliferation of Heating,
Ventilation and Air-Conditioning (HVAC) installations in response to the
growing demand for improved thermal comfort. Reducing the associated energy
consumption while maintaining comfortable conditions in buildings are
conflicting objectives and represent a typical optimization problem that
requires intelligent system design. Over the last decade, different
methodologies based on the Artificial Intelligence (AI) techniques have been
deployed to find the sweet spot between energy use in HVAC systems and suitable
indoor comfort levels to the occupants. This paper performs a comprehensive and
an in-depth systematic review of AI-based techniques used for building control
systems by assessing the outputs of these techniques, and their implementations
in the reviewed works, as well as investigating their abilities to improve the
energy-efficiency, while maintaining thermal comfort conditions. This enables a
holistic view of (1) the complexities of delivering thermal comfort to users
inside buildings in an energy-efficient way, and (2) the associated
bibliographic material to assist researchers and experts in the field in
tackling such a challenge. Among the 20 AI tools developed for both energy
consumption and comfort control, functions such as identification and
recognition patterns, optimization, predictive control. Based on the findings
of this work, the application of AI technology in building control is a
promising area of research and still an ongoing, i.e., the performance of
AI-based control is not yet completely satisfactory. This is mainly due in part
to the fact that these algorithms usually need a large amount of high-quality
real-world data, which is lacking in the building or, more precisely, the
energy sector.",arxiv
http://arxiv.org/abs/2105.06457v1,2021-05-13T17:56:04Z,2021-05-13T17:56:04Z,Conversational AI Systems for Social Good: Opportunities and Challenges,"Conversational artificial intelligence (ConvAI) systems have attracted much
academic and commercial attention recently, making significant progress on both
fronts. However, little existing work discusses how these systems can be
developed and deployed for social good. In this paper, we briefly review the
progress the community has made towards better ConvAI systems and reflect on
how existing technologies can help advance social good initiatives from various
angles that are unique for ConvAI, or not yet become common knowledge in the
community. We further discuss about the challenges ahead for ConvAI systems to
better help us achieve these goals and highlight the risks involved in their
development and deployment in the real world.",arxiv
http://arxiv.org/abs/1908.08998v2,2019-10-23T14:39:47Z,2019-08-13T10:15:39Z,AIBench: An Industry Standard Internet Service AI Benchmark Suite,"Today's Internet Services are undergoing fundamental changes and shifting to
an intelligent computing era where AI is widely employed to augment services.
In this context, many innovative AI algorithms, systems, and architectures are
proposed, and thus the importance of benchmarking and evaluating them rises.
However, modern Internet services adopt a microservice-based architecture and
consist of various modules. The diversity of these modules and complexity of
execution paths, the massive scale and complex hierarchy of datacenter
infrastructure, the confidential issues of data sets and workloads pose great
challenges to benchmarking. In this paper, we present the first
industry-standard Internet service AI benchmark suite---AIBench with seventeen
industry partners, including several top Internet service providers. AIBench
provides a highly extensible, configurable, and flexible benchmark framework
that contains loosely coupled modules. We identify sixteen prominent AI problem
domains like learning to rank, each of which forms an AI component benchmark,
from three most important Internet service domains: search engine, social
network, and e-commerce, which is by far the most comprehensive AI benchmarking
effort. On the basis of the AIBench framework, abstracting the real-world data
sets and workloads from one of the top e-commerce providers, we design and
implement the first end-to-end Internet service AI benchmark, which contains
the primary modules in the critical paths of an industry scale application and
is scalable to deploy on different cluster scales. The specifications, source
code, and performance numbers are publicly available from the benchmark council
web site http://www.benchcouncil.org/AIBench/index.html.",arxiv
http://arxiv.org/abs/2007.10784v2,2021-07-10T17:50:27Z,2020-07-16T21:14:45Z,Fast Neural Models for Symbolic Regression at Scale,"Deep learning owes much of its success to the astonishing expressiveness of
neural networks. However, this comes at the cost of complex, black-boxed models
that extrapolate poorly beyond the domain of the training dataset, conflicting
with goals of finding analytic expressions to describe science, engineering and
real world data. Under the hypothesis that the hierarchical modularity of such
laws can be captured by training a neural network, we introduce OccamNet, a
neural network model that finds interpretable, compact, and sparse solutions
for fitting data, \`{a} la Occam's razor. Our model defines a probability
distribution over a non-differentiable function space. We introduce a two-step
optimization method that samples functions and updates the weights with
backpropagation based on cross-entropy matching in an evolutionary strategy: we
train by biasing the probability mass toward better fitting solutions. OccamNet
is able to fit a variety of symbolic laws including simple analytic functions,
recursive programs, implicit functions, simple image classification, and can
outperform noticeably state-of-the-art symbolic regression methods on real
world regression datasets. Our method requires minimal memory footprint, does
not require AI accelerators for efficient training, fits complicated functions
in minutes of training on a single CPU, and demonstrates significant
performance gains when scaled on a GPU. Our implementation, demonstrations and
instructions for reproducing the experiments are available at
https://github.com/druidowm/OccamNet_Public.",arxiv
http://arxiv.org/abs/1812.00825v2,2018-12-04T05:36:36Z,2018-11-21T21:02:50Z,"Microscope 2.0: An Augmented Reality Microscope with Real-time
  Artificial Intelligence Integration","The brightfield microscope is instrumental in the visual examination of both
biological and physical samples at sub-millimeter scales. One key clinical
application has been in cancer histopathology, where the microscopic assessment
of the tissue samples is used for the diagnosis and staging of cancer and thus
guides clinical therapy. However, the interpretation of these samples is
inherently subjective, resulting in significant diagnostic variability.
Moreover, in many regions of the world, access to pathologists is severely
limited due to lack of trained personnel. In this regard, Artificial
Intelligence (AI) based tools promise to improve the access and quality of
healthcare. However, despite significant advances in AI research, integration
of these tools into real-world cancer diagnosis workflows remains challenging
because of the costs of image digitization and difficulties in deploying AI
solutions. Here we propose a cost-effective solution to the integration of AI:
the Augmented Reality Microscope (ARM). The ARM overlays AI-based information
onto the current view of the sample through the optical pathway in real-time,
enabling seamless integration of AI into the regular microscopy workflow. We
demonstrate the utility of ARM in the detection of lymph node metastases in
breast cancer and the identification of prostate cancer with a latency that
supports real-time workflows. We anticipate that ARM will remove barriers
towards the use of AI in microscopic analysis and thus improve the accuracy and
efficiency of cancer diagnosis. This approach is applicable to other microscopy
tasks and AI algorithms in the life sciences and beyond.",arxiv
http://arxiv.org/abs/1806.08212v1,2018-06-20T14:26:52Z,2018-06-20T14:26:52Z,"A Review of Network Inference Techniques for Neural Activation Time
  Series","Studying neural connectivity is considered one of the most promising and
challenging areas of modern neuroscience. The underpinnings of cognition are
hidden in the way neurons interact with each other. However, our experimental
methods of studying real neural connections at a microscopic level are still
arduous and costly. An efficient alternative is to infer connectivity based on
the neuronal activations using computational methods. A reliable method for
network inference, would not only facilitate research of neural circuits
without the need of laborious experiments but also reveal insights on the
underlying mechanisms of the brain. In this work, we perform a review of
methods for neural circuit inference given the activation time series of the
neural population. Approaching it from machine learning perspective, we divide
the methodologies into unsupervised and supervised learning. The methods are
based on correlation metrics, probabilistic point processes, and neural
networks. Furthermore, we add a data mining methodology inspired by influence
estimation in social networks as a new supervised learning approach. For
comparison, we use the small version of the Chalearn Connectomics competition,
that is accompanied with ground truth connections between neurons. The
experiments indicate that unsupervised learning methods perform better,
however, supervised methods could surpass them given enough data and resources.",arxiv
http://arxiv.org/abs/1901.10281v1,2019-01-29T13:43:57Z,2019-01-29T13:43:57Z,Structural Material Property Tailoring Using Deep Neural Networks,"Advances in robotics, artificial intelligence, and machine learning are
ushering in a new age of automation, as machines match or outperform human
performance. Machine intelligence can enable businesses to improve performance
by reducing errors, improving sensitivity, quality and speed, and in some cases
achieving outcomes that go beyond current resource capabilities. Relevant
applications include new product architecture design, rapid material
characterization, and life-cycle management tied with a digital strategy that
will enable efficient development of products from cradle to grave. In
addition, there are also challenges to overcome that must be addressed through
a major, sustained research effort that is based solidly on both inferential
and computational principles applied to design tailoring of functionally
optimized structures. Current applications of structural materials in the
aerospace industry demand the highest quality control of material
microstructure, especially for advanced rotational turbomachinery in aircraft
engines in order to have the best tailored material property. In this paper,
deep convolutional neural networks were developed to accurately predict
processing-structure-property relations from materials microstructures images,
surpassing current best practices and modeling efforts. The models
automatically learn critical features, without the need for manual
specification and/or subjective and expensive image analysis. Further, in
combination with generative deep learning models, a framework is proposed to
enable rapid material design space exploration and property identification and
optimization. The implementation must take account of real-time decision cycles
and the trade-offs between speed and accuracy.",arxiv
http://arxiv.org/abs/1911.08089v2,2019-12-07T03:42:06Z,2019-11-19T04:28:47Z,"""The Human Body is a Black Box"": Supporting Clinical Decision-Making
  with Deep Learning","Machine learning technologies are increasingly developed for use in
healthcare. While research communities have focused on creating
state-of-the-art models, there has been less focus on real world implementation
and the associated challenges to accuracy, fairness, accountability, and
transparency that come from actual, situated use. Serious questions remain
under examined regarding how to ethically build models, interpret and explain
model output, recognize and account for biases, and minimize disruptions to
professional expertise and work cultures. We address this gap in the literature
and provide a detailed case study covering the development, implementation, and
evaluation of Sepsis Watch, a machine learning-driven tool that assists
hospital clinicians in the early diagnosis and treatment of sepsis. We, the
team that developed and evaluated the tool, discuss our conceptualization of
the tool not as a model deployed in the world but instead as a socio-technical
system requiring integration into existing social and professional contexts.
Rather than focusing on model interpretability to ensure a fair and accountable
machine learning, we point toward four key values and practices that should be
considered when developing machine learning to support clinical
decision-making: rigorously define the problem in context, build relationships
with stakeholders, respect professional discretion, and create ongoing feedback
loops with stakeholders. Our work has significant implications for future
research regarding mechanisms of institutional accountability and
considerations for designing machine learning systems. Our work underscores the
limits of model interpretability as a solution to ensure transparency,
accuracy, and accountability in practice. Instead, our work demonstrates other
means and goals to achieve FATML values in design and in practice.",arxiv
http://arxiv.org/abs/1911.08448v4,2020-03-17T16:06:58Z,2019-11-19T18:14:58Z,Artificial intelligence approach to momentum risk-taking,"We propose a mathematical model of momentum risk-taking, which is essentially
real-time risk management focused on short-term volatility of stock markets.
Its implementation, our fully automated momentum equity trading system
presented systematically, proved to be successful in extensive historical and
real-time experiments. Momentum risk-taking is one of the key components of
general decision-making, a challenge for artificial intelligence and machine
learning with deep roots in cognitive science; its variants beyond stock
markets are discussed. We begin with a new algebraic-type theory of news impact
on share-prices, which describes well their power growth, periodicity, and the
market phenomena like price targets and profit-taking. This theory generally
requires Bessel and hypergeometric functions. Its discretization results in
some tables of bids, which are basically expected returns for main investment
horizons, the key in our trading system. The ML procedures we use are similar
to those in neural networking. A preimage of our approach is the new contract
card game provided at the end, a combination of bridge and poker. Relations to
random processes and the fractional Brownian motion are outlined.",arxiv
http://arxiv.org/abs/2006.06865v1,2020-06-11T22:58:36Z,2020-06-11T22:58:36Z,Exploring Algorithmic Fairness in Robust Graph Covering Problems,"Fueled by algorithmic advances, AI algorithms are increasingly being deployed
in settings subject to unanticipated challenges with complex social effects.
Motivated by real-world deployment of AI driven, social-network based suicide
prevention and landslide risk management interventions, this paper focuses on
robust graph covering problems subject to group fairness constraints. We show
that, in the absence of fairness constraints, state-of-the-art algorithms for
the robust graph covering problem result in biased node coverage: they tend to
discriminate individuals (nodes) based on membership in traditionally
marginalized groups. To mitigate this issue, we propose a novel formulation of
the robust graph covering problem with group fairness constraints and a
tractable approximation scheme applicable to real-world instances. We provide a
formal analysis of the price of group fairness (PoF) for this problem, where we
show that uncertainty can lead to greater PoF. We demonstrate the effectiveness
of our approach on several real-world social networks. Our method yields
competitive node coverage while significantly improving group fairness relative
to state-of-the-art methods.",arxiv
http://arxiv.org/abs/2101.02000v1,2021-01-06T13:15:21Z,2021-01-06T13:15:21Z,Weakly-Supervised Multi-Face 3D Reconstruction,"3D face reconstruction plays a very important role in many real-world
multimedia applications, including digital entertainment, social media,
affection analysis, and person identification. The de-facto pipeline for
estimating the parametric face model from an image requires to firstly detect
the facial regions with landmarks, and then crop each face to feed the deep
learning-based regressor. Comparing to the conventional methods performing
forward inference for each detected instance independently, we suggest an
effective end-to-end framework for multi-face 3D reconstruction, which is able
to predict the model parameters of multiple instances simultaneously using
single network inference. Our proposed approach not only greatly reduces the
computational redundancy in feature extraction but also makes the deployment
procedure much easier using the single network model. More importantly, we
employ the same global camera model for the reconstructed faces in each image,
which makes it possible to recover the relative head positions and orientations
in the 3D scene. We have conducted extensive experiments to evaluate our
proposed approach on the sparse and dense face alignment tasks. The
experimental results indicate that our proposed approach is very promising on
face alignment tasks without fully-supervision and pre-processing like
detection and crop. Our implementation is publicly available at
\url{https://github.com/kalyo-zjl/WM3DR}.",arxiv
http://arxiv.org/abs/2103.00959v2,2021-07-23T03:29:00Z,2021-03-01T12:35:16Z,CogDL: Toolkit for Deep Learning on Graphs,"Deep learning on graphs has attracted tremendous attention from the graph
learning community in recent years. It has been widely used in several
real-world applications such as social network analysis and recommender
systems. In this paper, we introduce CogDL, an extensive toolkit for deep
learning on graphs that allows researchers and developers to easily conduct
experiments and build applications. It provides standard training and
evaluation for the most important tasks in the graph domain, including node
classification, graph classification, etc. For each task, it provides
implementations of state-of-the-art models. The models in our toolkit are
divided into two major parts, graph embedding methods and graph neural
networks. Most of the graph embedding methods learn node-level or graph-level
representations in an unsupervised way and preserves the graph properties such
as structural information, while graph neural networks capture node features
and work in semi-supervised or self-supervised settings. All models implemented
in our toolkit can be easily reproducible for leaderboard results. Most models
in CogDL are developed on top of PyTorch, and users can leverage the advantages
of PyTorch to implement their own models. Furthermore, we demonstrate the
effectiveness of CogDL for real-world applications in AMiner, a large academic
mining system.",arxiv
http://arxiv.org/abs/2002.00763v1,2020-01-31T02:28:35Z,2020-01-31T02:28:35Z,Two-path Deep Semi-supervised Learning for Timely Fake News Detection,"News in social media such as Twitter has been generated in high volume and
speed. However, very few of them are labeled (as fake or true news) by
professionals in near real time. In order to achieve timely detection of fake
news in social media, a novel framework of two-path deep semi-supervised
learning is proposed where one path is for supervised learning and the other is
for unsupervised learning. The supervised learning path learns on the limited
amount of labeled data while the unsupervised learning path is able to learn on
a huge amount of unlabeled data. Furthermore, these two paths implemented with
convolutional neural networks (CNN) are jointly optimized to complete
semi-supervised learning. In addition, we build a shared CNN to extract the low
level features on both labeled data and unlabeled data to feed them into these
two paths. To verify this framework, we implement a Word CNN based
semi-supervised learning model and test it on two datasets, namely, LIAR and
PHEME. Experimental results demonstrate that the model built on the proposed
framework can recognize fake news effectively with very few labeled data.",arxiv
http://arxiv.org/abs/2002.05147v1,2020-02-12T18:46:48Z,2020-02-12T18:46:48Z,"Multi-Agent Reinforcement Learning and Human Social Factors in Climate
  Change Mitigation","Many complex real-world problems, such as climate change mitigation, are
intertwined with human social factors. Climate change mitigation, a social
dilemma made difficult by the inherent complexities of human behavior, has an
impact at a global scale. We propose applying multi-agent reinforcement
learning (MARL) in this setting to develop intelligent agents that can
influence the social factors at play in climate change mitigation. There are
ethical, practical, and technical challenges that must be addressed when
deploying MARL in this way. In this paper, we present these challenges and
outline an approach to address them. Understanding how intelligent agents can
be used to impact human social factors is important to prevent their abuse and
can be beneficial in furthering our knowledge of these complex problems as a
whole. The challenges we present are not limited to our specific application
but are applicable to broader MARL. Thus, developing MARL for social factors in
climate change mitigation helps address general problems hindering MARL's
applicability to other real-world problems while also motivating discussion on
the social implications of MARL deployment.",arxiv
http://arxiv.org/abs/1606.03212v1,2016-06-10T07:17:00Z,2016-06-10T07:17:00Z,"Discovery of Latent Factors in High-dimensional Data Using Tensor
  Methods","Unsupervised learning aims at the discovery of hidden structure that drives
the observations in the real world. It is essential for success in modern
machine learning. Latent variable models are versatile in unsupervised learning
and have applications in almost every domain. Training latent variable models
is challenging due to the non-convexity of the likelihood objective. An
alternative method is based on the spectral decomposition of low order moment
tensors. This versatile framework is guaranteed to estimate the correct model
consistently. My thesis spans both theoretical analysis of tensor decomposition
framework and practical implementation of various applications. This thesis
presents theoretical results on convergence to globally optimal solution of
tensor decomposition using the stochastic gradient descent, despite
non-convexity of the objective. This is the first work that gives global
convergence guarantees for the stochastic gradient descent on non-convex
functions with exponentially many local minima and saddle points. This thesis
also presents large-scale deployment of spectral methods carried out on various
platforms. Dimensionality reduction techniques such as random projection are
incorporated for a highly parallel and scalable tensor decomposition algorithm.
We obtain a gain in both accuracies and in running times by several orders of
magnitude compared to the state-of-art variational methods. To solve real world
problems, more advanced models and learning algorithms are proposed. This
thesis discusses generalization of LDA model to mixed membership stochastic
block model for learning user communities in social network, convolutional
dictionary model for learning word-sequence embeddings, hierarchical tensor
decomposition and latent tree structure model for learning disease hierarchy,
and spatial point process mixture model for detecting cell types in
neuroscience.",arxiv
http://arxiv.org/abs/2012.08174v2,2021-03-29T17:15:00Z,2020-12-15T09:49:22Z,"Towards open and expandable cognitive AI architectures for large-scale
  multi-agent human-robot collaborative learning","Learning from Demonstration (LfD) constitutes one of the most robust
methodologies for constructing efficient cognitive robotic systems. Despite the
large body of research works already reported, current key technological
challenges include those of multi-agent learning and long-term autonomy.
Towards this direction, a novel cognitive architecture for multi-agent LfD
robotic learning is introduced, targeting to enable the reliable deployment of
open, scalable and expandable robotic systems in large-scale and complex
environments. In particular, the designed architecture capitalizes on the
recent advances in the Artificial Intelligence (AI) field, by establishing a
Federated Learning (FL)-based framework for incarnating a multi-human
multi-robot collaborative learning environment. The fundamental
conceptualization relies on employing multiple AI-empowered cognitive processes
(implementing various robotic tasks) that operate at the edge nodes of a
network of robotic platforms, while global AI models (underpinning the
aforementioned robotic tasks) are collectively created and shared among the
network, by elegantly combining information from a large number of human-robot
interaction instances. Regarding pivotal novelties, the designed cognitive
architecture a) introduces a new FL-based formalism that extends the
conventional LfD learning paradigm to support large-scale multi-agent
operational settings, b) elaborates previous FL-based self-learning robotic
schemes so as to incorporate the human in the learning loop and c) consolidates
the fundamental principles of FL with additional sophisticated AI-enabled
learning methodologies for modelling the multi-level inter-dependencies among
the robotic tasks. The applicability of the proposed framework is explained
using an example of a real-world industrial case study for agile
production-based Critical Raw Materials (CRM) recovery.",arxiv
http://arxiv.org/abs/2012.10610v3,2021-02-16T17:31:15Z,2020-12-19T07:00:09Z,"SpaceML: Distributed Open-source Research with Citizen Scientists for
  the Advancement of Space Technology for NASA","Traditionally, academic labs conduct open-ended research with the primary
focus on discoveries with long-term value, rather than direct products that can
be deployed in the real world. On the other hand, research in the industry is
driven by its expected commercial return on investment, and hence focuses on a
real world product with short-term timelines. In both cases, opportunity is
selective, often available to researchers with advanced educational
backgrounds. Research often happens behind closed doors and may be kept
confidential until either its publication or product release, exacerbating the
problem of AI reproducibility and slowing down future research by others in the
field. As many research organizations tend to exclusively focus on specific
areas, opportunities for interdisciplinary research reduce. Undertaking
long-term bold research in unexplored fields with non-commercial yet great
public value is hard due to factors including the high upfront risk, budgetary
constraints, and a lack of availability of data and experts in niche fields.
Only a few companies or well-funded research labs can afford to do such
long-term research. With research organizations focused on an exploding array
of fields and resources spread thin, opportunities for the maturation of
interdisciplinary research reduce. Apart from these exigencies, there is also a
need to engage citizen scientists through open-source contributors to play an
active part in the research dialogue. We present a short case study of SpaceML,
an extension of the Frontier Development Lab, an AI accelerator for NASA.
SpaceML distributes open-source research and invites volunteer citizen
scientists to partake in development and deployment of high social value
products at the intersection of space and AI.",arxiv
http://arxiv.org/abs/2009.05835v3,2021-04-03T15:08:50Z,2020-09-12T17:37:36Z,"How Much Can We Really Trust You? Towards Simple, Interpretable Trust
  Quantification Metrics for Deep Neural Networks","A critical step to building trustworthy deep neural networks is trust
quantification, where we ask the question: How much can we trust a deep neural
network? In this study, we take a step towards simple, interpretable metrics
for trust quantification by introducing a suite of metrics for assessing the
overall trustworthiness of deep neural networks based on their behaviour when
answering a set of questions. We conduct a thought experiment and explore two
key questions about trust in relation to confidence: 1) How much trust do we
have in actors who give wrong answers with great confidence? and 2) How much
trust do we have in actors who give right answers hesitantly? Based on insights
gained, we introduce the concept of question-answer trust to quantify
trustworthiness of an individual answer based on confident behaviour under
correct and incorrect answer scenarios, and the concept of trust density to
characterize the distribution of overall trust for an individual answer
scenario. We further introduce the concept of trust spectrum for representing
overall trust with respect to the spectrum of possible answer scenarios across
correctly and incorrectly answered questions. Finally, we introduce
NetTrustScore, a scalar metric summarizing overall trustworthiness. The suite
of metrics aligns with past social psychology studies that study the
relationship between trust and confidence. Leveraging these metrics, we
quantify the trustworthiness of several well-known deep neural network
architectures for image recognition to get a deeper understanding of where
trust breaks down. The proposed metrics are by no means perfect, but the hope
is to push the conversation towards better metrics to help guide practitioners
and regulators in producing, deploying, and certifying deep learning solutions
that can be trusted to operate in real-world, mission-critical scenarios.",arxiv
http://arxiv.org/abs/2103.10975v1,2021-03-19T18:21:38Z,2021-03-19T18:21:38Z,Accelerating GMRES with Deep Learning in Real-Time,"GMRES is a powerful numerical solver used to find solutions to extremely
large systems of linear equations. These systems of equations appear in many
applications in science and engineering. Here we demonstrate a real-time
machine learning algorithm that can be used to accelerate the time-to-solution
for GMRES. Our framework is novel in that is integrates the deep learning
algorithm in an in situ fashion: the AI-accelerator gradually learns how to
optimizes the time to solution without requiring user input (such as a
pre-trained data set). We describe how our algorithm collects data and
optimizes GMRES. We demonstrate our algorithm by implementing an accelerated
(MLGMRES) solver in Python. We then use MLGMRES to accelerate a solver for the
Poisson equation -- a class of linear problems that appears in may
applications.
  Informed by the properties of formal solutions to the Poisson equation, we
test the performance of different neural networks. Our key takeaway is that
networks which are capable of learning non-local relationships perform well,
without needing to be scaled with the input problem size, making them good
candidates for the extremely large problems encountered in high-performance
computing. For the inputs studied, our method provides a roughly 2$\times$
acceleration.",arxiv
http://arxiv.org/abs/2002.05648v3,2020-04-26T04:59:52Z,2020-02-01T01:15:39Z,Politics of Adversarial Machine Learning,"In addition to their security properties, adversarial machine-learning
attacks and defenses have political dimensions. They enable or foreclose
certain options for both the subjects of the machine learning systems and for
those who deploy them, creating risks for civil liberties and human rights. In
this paper, we draw on insights from science and technology studies,
anthropology, and human rights literature, to inform how defenses against
adversarial attacks can be used to suppress dissent and limit attempts to
investigate machine learning systems. To make this concrete, we use real-world
examples of how attacks such as perturbation, model inversion, or membership
inference can be used for socially desirable ends. Although the predictions of
this analysis may seem dire, there is hope. Efforts to address human rights
concerns in the commercial spyware industry provide guidance for similar
measures to ensure ML systems serve democratic, not authoritarian ends",arxiv
http://arxiv.org/abs/2010.11884v1,2020-10-22T17:20:38Z,2020-10-22T17:20:38Z,"AEGIS: A real-time multimodal augmented reality computer vision based
  system to assist facial expression recognition for individuals with autism
  spectrum disorder","The ability to interpret social cues comes naturally for most people, but for
those living with Autism Spectrum Disorder (ASD), some experience a deficiency
in this area. This paper presents the development of a multimodal augmented
reality (AR) system which combines the use of computer vision and deep
convolutional neural networks (CNN) in order to assist individuals with the
detection and interpretation of facial expressions in social settings. The
proposed system, which we call AEGIS (Augmented-reality Expression Guided
Interpretation System), is an assistive technology deployable on a variety of
user devices including tablets, smartphones, video conference systems, or
smartglasses, showcasing its extreme flexibility and wide range of use cases,
to allow integration into daily life with ease. Given a streaming video camera
source, each real-world frame is passed into AEGIS, processed for facial
bounding boxes, and then fed into our novel deep convolutional time windowed
neural network (TimeConvNet). We leverage both spatial and temporal information
in order to provide an accurate expression prediction, which is then converted
into its corresponding visualization and drawn on top of the original video
frame. The system runs in real-time, requires minimal set up and is simple to
use. With the use of AEGIS, we can assist individuals living with ASD to learn
to better identify expressions and thus improve their social experiences.",arxiv
http://arxiv.org/abs/2006.10461v3,2021-08-19T10:22:48Z,2020-06-18T12:16:08Z,"Auxiliary-task learning for geographic data with autoregressive
  embeddings","Machine learning is gaining popularity in a broad range of areas working with
geographic data, such as ecology or atmospheric sciences. Here, data often
exhibit spatial effects, which can be difficult to learn for neural networks.
In this study, we propose SXL, a method for embedding information on the
autoregressive nature of spatial data directly into the learning process using
auxiliary tasks. We utilize the local Moran's I, a popular measure of local
spatial autocorrelation, to ""nudge"" the model to learn the direction and
magnitude of local spatial effects, complementing the learning of the primary
task. We further introduce a novel expansion of Moran's I to multiple
resolutions, thus capturing spatial interactions over longer and shorter
distances simultaneously. The novel multi-resolution Moran's I can be
constructed easily and as a multi-dimensional tensor offers seamless
integration into existing machine learning frameworks. Throughout a range of
experiments using real-world data, we highlight how our method consistently
improves the training of neural networks in unsupervised and supervised
learning tasks. In generative spatial modeling experiments, we propose a novel
loss for auxiliary task GANs utilizing task uncertainty weights. Our proposed
method outperforms domain-specific spatial interpolation benchmarks,
highlighting its potential for downstream applications. This study bridges
expertise from geographic information science and machine learning, showing how
this integration of disciplines can help to address domain-specific challenges.
The code for our experiments is available on Github:
https://github.com/konstantinklemmer/sxl.",arxiv
http://arxiv.org/abs/1804.10134v2,2018-07-28T10:33:47Z,2018-04-26T16:04:30Z,Detection-Tracking for Efficient Person Analysis: The DetTA Pipeline,"In the past decade many robots were deployed in the wild, and people
detection and tracking is an important component of such deployments. On top of
that, one often needs to run modules which analyze persons and extract higher
level attributes such as age and gender, or dynamic information like gaze and
pose. The latter ones are especially necessary for building a reactive, social
robot-person interaction.
  In this paper, we combine those components in a fully modular
detection-tracking-analysis pipeline, called DetTA. We investigate the benefits
of such an integration on the example of head and skeleton pose, by using the
consistent track ID for a temporal filtering of the analysis modules'
observations, showing a slight improvement in a challenging real-world
scenario. We also study the potential of a so-called ""free-flight"" mode, where
the analysis of a person attribute only relies on the filter's predictions for
certain frames. Here, our study shows that this boosts the runtime
dramatically, while the prediction quality remains stable. This insight is
especially important for reducing power consumption and sharing precious
(GPU-)memory when running many analysis components on a mobile platform,
especially so in the era of expensive deep learning methods.",arxiv
http://arxiv.org/abs/2105.06442v1,2021-05-13T17:33:28Z,2021-05-13T17:33:28Z,"An Empirical Comparison of Bias Reduction Methods on Real-World Problems
  in High-Stakes Policy Settings","Applications of machine learning (ML) to high-stakes policy settings -- such
as education, criminal justice, healthcare, and social service delivery -- have
grown rapidly in recent years, sparking important conversations about how to
ensure fair outcomes from these systems. The machine learning research
community has responded to this challenge with a wide array of proposed
fairness-enhancing strategies for ML models, but despite the large number of
methods that have been developed, little empirical work exists evaluating these
methods in real-world settings. Here, we seek to fill this research gap by
investigating the performance of several methods that operate at different
points in the ML pipeline across four real-world public policy and social good
problems. Across these problems, we find a wide degree of variability and
inconsistency in the ability of many of these methods to improve model
fairness, but post-processing by choosing group-specific score thresholds
consistently removes disparities, with important implications for both the ML
research community and practitioners deploying machine learning to inform
consequential policy decisions.",arxiv
http://arxiv.org/abs/2104.02541v1,2021-04-06T14:31:23Z,2021-04-06T14:31:23Z,"Instantaneous Stereo Depth Estimation of Real-World Stimuli with a
  Neuromorphic Stereo-Vision Setup","The stereo-matching problem, i.e., matching corresponding features in two
different views to reconstruct depth, is efficiently solved in biology. Yet, it
remains the computational bottleneck for classical machine vision approaches.
By exploiting the properties of event cameras, recently proposed Spiking Neural
Network (SNN) architectures for stereo vision have the potential of simplifying
the stereo-matching problem. Several solutions that combine event cameras with
spike-based neuromorphic processors already exist. However, they are either
simulated on digital hardware or tested on simplified stimuli. In this work, we
use the Dynamic Vision Sensor 3D Human Pose Dataset (DHP19) to validate a
brain-inspired event-based stereo-matching architecture implemented on a
mixed-signal neuromorphic processor with real-world data. Our experiments show
that this SNN architecture, composed of coincidence detectors and disparity
sensitive neurons, is able to provide a coarse estimate of the input disparity
instantaneously, thereby detecting the presence of a stimulus moving in depth
in real-time.",arxiv
http://arxiv.org/abs/1805.05151v1,2018-05-14T12:54:47Z,2018-05-14T12:54:47Z,Domain Adaptation with Adversarial Training and Graph Embeddings,"The success of deep neural networks (DNNs) is heavily dependent on the
availability of labeled data. However, obtaining labeled data is a big
challenge in many real-world problems. In such scenarios, a DNN model can
leverage labeled and unlabeled data from a related domain, but it has to deal
with the shift in data distributions between the source and the target domains.
In this paper, we study the problem of classifying social media posts during a
crisis event (e.g., Earthquake). For that, we use labeled and unlabeled data
from past similar events (e.g., Flood) and unlabeled data for the current
event. We propose a novel model that performs adversarial learning based domain
adaptation to deal with distribution drifts and graph based semi-supervised
learning to leverage unlabeled data within a single unified deep learning
framework. Our experiments with two real-world crisis datasets collected from
Twitter demonstrate significant improvements over several baselines.",arxiv
http://arxiv.org/abs/2103.00001v3,2021-10-19T18:50:02Z,2021-02-26T21:47:14Z,"Three-dimensional Coherent X-ray Diffraction Imaging via Deep
  Convolutional Neural Networks","As a critical component of coherent X-ray diffraction imaging (CDI), phase
retrieval has been extensively applied in X-ray structural science to recover
the 3D morphological information inside measured particles. Despite meeting all
the oversampling requirements of Sayre and Shannon, current phase retrieval
approaches still have trouble achieving a unique inversion of experimental data
in the presence of noise. Here, we propose to overcome this limitation by
incorporating a 3D Machine Learning (ML) model combining (optional) supervised
learning with transfer learning. The trained ML model can rapidly provide an
immediate result with high accuracy which could benefit real-time experiments,
and the predicted result can be further refined with transfer learning. More
significantly, the proposed ML model can be used without any prior training to
learn the missing phases of an image based on minimization of an appropriate
'loss function' alone. We demonstrate significantly improved performance with
experimental Bragg CDI data over traditional iterative phase retrieval
algorithms.",arxiv
http://arxiv.org/abs/2011.14925v1,2020-11-26T02:37:39Z,2020-11-26T02:37:39Z,"Autonomous Graph Mining Algorithm Search with Best Speed/Accuracy
  Trade-off","Graph data is ubiquitous in academia and industry, from social networks to
bioinformatics. The pervasiveness of graphs today has raised the demand for
algorithms that can answer various questions: Which products would a user like
to purchase given her order list? Which users are buying fake followers to
increase their public reputation? Myriads of new graph mining algorithms are
proposed every year to answer such questions - each with a distinct problem
formulation, computational time, and memory footprint. This lack of unity makes
it difficult for a practitioner to compare different algorithms and pick the
most suitable one for a specific application. These challenges - even more
severe for non-experts - create a gap in which state-of-the-art techniques
developed in academic settings fail to be optimally deployed in real-world
applications. To bridge this gap, we propose AUTOGM, an automated system for
graph mining algorithm development. We first define a unified framework
UNIFIEDGM that integrates various message-passing based graph algorithms,
ranging from conventional algorithms like PageRank to graph neural networks.
Then UNIFIEDGM defines a search space in which five parameters are required to
determine a graph algorithm. Under this search space, AUTOGM explicitly
optimizes for the optimal parameter set of UNIFIEDGM using Bayesian
Optimization. AUTOGM defines a novel budget-aware objective function for the
optimization to incorporate a practical issue - finding the best speed-accuracy
trade-off under a computation budget - into the graph algorithm generation
problem. Experiments on real-world benchmark datasets demonstrate that AUTOGM
generates novel graph mining algorithms with the best speed/accuracy trade-off
compared to existing models with heuristic parameters.",arxiv
http://arxiv.org/abs/1809.02797v2,2018-09-15T08:31:50Z,2018-09-08T13:08:26Z,Fast Gradient Attack on Network Embedding,"Network embedding maps a network into a low-dimensional Euclidean space, and
thus facilitate many network analysis tasks, such as node classification, link
prediction and community detection etc, by utilizing machine learning methods.
In social networks, we may pay special attention to user privacy, and would
like to prevent some target nodes from being identified by such network
analysis methods in certain cases. Inspired by successful adversarial attack on
deep learning models, we propose a framework to generate adversarial networks
based on the gradient information in Graph Convolutional Network (GCN). In
particular, we extract the gradient of pairwise nodes based on the adversarial
network, and select the pair of nodes with maximum absolute gradient to realize
the Fast Gradient Attack (FGA) and update the adversarial network. This process
is implemented iteratively and terminated until certain condition is satisfied,
i.e., the number of modified links reaches certain predefined value.
Comprehensive attacks, including unlimited attack, direct attack and indirect
attack, are performed on six well-known network embedding methods. The
experiments on real-world networks suggest that our proposed FGA behaves better
than some baseline methods, i.e., the network embedding can be easily disturbed
using FGA by only rewiring few links, achieving state-of-the-art attack
performance.",arxiv
http://arxiv.org/abs/2010.06425v1,2020-10-13T14:38:40Z,2020-10-13T14:38:40Z,"Temporal Collaborative Filtering with Graph Convolutional Neural
  Networks","Temporal collaborative filtering (TCF) methods aim at modelling non-static
aspects behind recommender systems, such as the dynamics in users' preferences
and social trends around items. State-of-the-art TCF methods employ recurrent
neural networks (RNNs) to model such aspects. These methods deploy
matrix-factorization-based (MF-based) approaches to learn the user and item
representations. Recently, graph-neural-network-based (GNN-based) approaches
have shown improved performance in providing accurate recommendations over
traditional MF-based approaches in non-temporal CF settings. Motivated by this,
we propose a novel TCF method that leverages GNNs to learn user and item
representations, and RNNs to model their temporal dynamics. A challenge with
this method lies in the increased data sparsity, which negatively impacts
obtaining meaningful quality representations with GNNs. To overcome this
challenge, we train a GNN model at each time step using a set of observed
interactions accumulated time-wise. Comprehensive experiments on real-world
data show the improved performance obtained by our method over several
state-of-the-art temporal and non-temporal CF models.",arxiv
http://arxiv.org/abs/1902.00522v1,2019-02-01T19:02:18Z,2019-02-01T19:02:18Z,"Deep Learning for Multi-Messenger Astrophysics: A Gateway for Discovery
  in the Big Data Era","This report provides an overview of recent work that harnesses the Big Data
Revolution and Large Scale Computing to address grand computational challenges
in Multi-Messenger Astrophysics, with a particular emphasis on real-time
discovery campaigns. Acknowledging the transdisciplinary nature of
Multi-Messenger Astrophysics, this document has been prepared by members of the
physics, astronomy, computer science, data science, software and
cyberinfrastructure communities who attended the NSF-, DOE- and NVIDIA-funded
""Deep Learning for Multi-Messenger Astrophysics: Real-time Discovery at Scale""
workshop, hosted at the National Center for Supercomputing Applications,
October 17-19, 2018. Highlights of this report include unanimous agreement that
it is critical to accelerate the development and deployment of novel,
signal-processing algorithms that use the synergy between artificial
intelligence (AI) and high performance computing to maximize the potential for
scientific discovery with Multi-Messenger Astrophysics. We discuss key aspects
to realize this endeavor, namely (i) the design and exploitation of scalable
and computationally efficient AI algorithms for Multi-Messenger Astrophysics;
(ii) cyberinfrastructure requirements to numerically simulate astrophysical
sources, and to process and interpret Multi-Messenger Astrophysics data; (iii)
management of gravitational wave detections and triggers to enable
electromagnetic and astro-particle follow-ups; (iv) a vision to harness future
developments of machine and deep learning and cyberinfrastructure resources to
cope with the scale of discovery in the Big Data Era; (v) and the need to build
a community that brings domain experts together with data scientists on equal
footing to maximize and accelerate discovery in the nascent field of
Multi-Messenger Astrophysics.",arxiv
http://arxiv.org/abs/1711.10941v1,2017-11-29T16:23:38Z,2017-11-29T16:23:38Z,"Intelligent Traffic Light Control Using Distributed Multi-agent Q
  Learning","The combination of Artificial Intelligence (AI) and Internet-of-Things (IoT),
which is denoted as AI-powered Internet-of-Things (AIoT), is capable of
processing huge amount of data generated from a large number of devices and
handling complex problems in social infrastructures. As AI and IoT technologies
are becoming mature, in this paper, we propose to apply AIoT technologies for
traffic light control, which is an essential component for intelligent
transportation system, to improve the efficiency of smart city's road system.
Specifically, various sensors such as surveillance cameras provide real-time
information for intelligent traffic light control system to observe the states
of both motorized traffic and non-motorized traffic. In this paper, we propose
an intelligent traffic light control solution by using distributed multi-agent
Q learning, considering the traffic information at the neighboring
intersections as well as local motorized and non-motorized traffic, to improve
the overall performance of the entire control system. By using the proposed
multi-agent Q learning algorithm, our solution is targeting to optimize both
the motorized and non-motorized traffic. In addition, we considered many
constraints/rules for traffic light control in the real world, and integrate
these constraints in the learning algorithm, which can facilitate the proposed
solution to be deployed in real operational scenarios. We conducted numerical
simulations for a real-world map with real-world traffic data. The simulation
results show that our proposed solution outperforms existing solutions in terms
of vehicle and pedestrian queue lengths, waiting time at intersections, and
many other key performance metrics.",arxiv
http://arxiv.org/abs/1803.03191v1,2018-03-08T16:33:09Z,2018-03-08T16:33:09Z,"A Bayesian and Machine Learning approach to estimating Influence Model
  parameters for IM-RO","The rise of Online Social Networks (OSNs) has caused an insurmountable amount
of interest from advertisers and researchers seeking to monopolize on its
features. Researchers aim to develop strategies for determining how information
is propagated among users within an OSN that is captured by diffusion or
influence models. We consider the influence models for the IM-RO problem, a
novel formulation to the Influence Maximization (IM) problem based on
implementing Stochastic Dynamic Programming (SDP). In contrast to existing
approaches involving influence spread and the theory of submodular functions,
the SDP method focuses on optimizing clicks and ultimately revenue to
advertisers in OSNs. Existing approaches to influence maximization have been
actively researched over the past decade, with applications to multiple fields,
however, our approach is a more practical variant to the original IM problem.
In this paper, we provide an analysis on the influence models of the IM-RO
problem by conducting experiments on synthetic and real-world datasets. We
propose a Bayesian and Machine Learning approach for estimating the parameters
of the influence models for the (Influence Maximization- Revenue Optimization)
IM-RO problem. We present a Bayesian hierarchical model and implement the
well-known Naive Bayes classifier (NBC), Decision Trees classifier (DTC) and
Random Forest classifier (RFC) on three real-world datasets. Compared to
previous approaches to estimating influence model parameters, our strategy has
the great advantage of being directly implementable in standard software
packages such as WinBUGS/OpenBUGS/JAGS and Apache Spark. We demonstrate the
efficiency and usability of our methods in terms of spreading information and
generating revenue for advertisers in the context of OSNs.",arxiv
http://arxiv.org/abs/2109.13476v1,2021-09-28T04:21:07Z,2021-09-28T04:21:07Z,Fake News Detection using Semi-Supervised Graph Convolutional Network,"Social media becomes the central way for people to obtain and utilise news,
due to its rapidness and inexpensive value of data distribution. Though, such
features of social media platforms also present it a root cause of fake news
distribution, causing adverse consequences on both people and culture. Hence,
detecting fake news has become a significant research interest for bringing
feasible real time solutions to the problem. Most current techniques of fake
news disclosure are supervised, that need large cost in terms of time and
effort to make a certainly interpreted dataset. The proposed framework
concentrates on the text-based detection of fake news items while considering
that only limited number of labels are available. Graphs are functioned
extensively under several purposes of real-world problems on the strength of
their property to structure things easily. Deep neural networks are used to
generate great results within tasks that utilizes graph classification. The
Graph Convolution Network works as a deep learning paradigm which works on
graphs. Our proposed framework deals with limited amount of labelled data; we
go for a semi-supervised learning method. We come up with a semi-supervised
fake news detection technique based on GCN (Graph Convolutional Networks). The
recommended architecture comprises of three basic components: collecting word
embeddings from the news articles in datasets utilising GloVe, building
similarity graph using Word Movers Distance (WMD) and finally applying Graph
Convolution Network (GCN) for binary classification of news articles in
semi-supervised paradigm. The implemented technique is validated on three
different datasets by varying the volume of labelled data achieving 95.27 %
highest accuracy on Real or Fake dataset. Comparison with other contemporary
techniques also reinforced the supremacy of the proposed framework.",arxiv
http://arxiv.org/abs/1907.06011v3,2020-09-14T19:04:01Z,2019-07-13T05:21:05Z,"Extracting Interpretable Physical Parameters from Spatiotemporal Systems
  using Unsupervised Learning","Experimental data is often affected by uncontrolled variables that make
analysis and interpretation difficult. For spatiotemporal systems, this problem
is further exacerbated by their intricate dynamics. Modern machine learning
methods are particularly well-suited for analyzing and modeling complex
datasets, but to be effective in science, the result needs to be interpretable.
We demonstrate an unsupervised learning technique for extracting interpretable
physical parameters from noisy spatiotemporal data and for building a
transferable model of the system. In particular, we implement a
physics-informed architecture based on variational autoencoders that is
designed for analyzing systems governed by partial differential equations
(PDEs). The architecture is trained end-to-end and extracts latent parameters
that parameterize the dynamics of a learned predictive model for the system. To
test our method, we train our model on simulated data from a variety of PDEs
with varying dynamical parameters that act as uncontrolled variables. Numerical
experiments show that our method can accurately identify relevant parameters
and extract them from raw and even noisy spatiotemporal data (tested with
roughly 10% added noise). These extracted parameters correlate well (linearly
with $R^2 > 0.95$) with the ground truth physical parameters used to generate
the datasets. We then apply this method to nonlinear fiber propagation data,
generated by an ab-initio simulation, to demonstrate its capabilities on a more
realistic dataset. Our method for discovering interpretable latent parameters
in spatiotemporal systems will allow us to better analyze and understand
real-world phenomena and datasets, which often have unknown and uncontrolled
variables that alter the system dynamics and cause varying behaviors that are
difficult to disentangle.",arxiv
http://arxiv.org/abs/1910.11779v1,2019-10-25T15:03:11Z,2019-10-25T15:03:11Z,"Toward a better trade-off between performance and fairness with
  kernel-based distribution matching","As recent literature has demonstrated how classifiers often carry unintended
biases toward some subgroups, deploying machine learned models to users demands
careful consideration of the social consequences. How should we address this
problem in a real-world system? How should we balance core performance and
fairness metrics? In this paper, we introduce a MinDiff framework for
regularizing classifiers toward different fairness metrics and analyze a
technique with kernel-based statistical dependency tests. We run a thorough
study on an academic dataset to compare the Pareto frontier achieved by
different regularization approaches, and apply our kernel-based method to two
large-scale industrial systems demonstrating real-world improvements.",arxiv
http://arxiv.org/abs/2105.01636v1,2021-05-04T17:27:59Z,2021-05-04T17:27:59Z,Learning 3D Granular Flow Simulations,"Recently, the application of machine learning models has gained momentum in
natural sciences and engineering, which is a natural fit due to the abundance
of data in these fields. However, the modeling of physical processes from
simulation data without first principle solutions remains difficult. Here, we
present a Graph Neural Networks approach towards accurate modeling of complex
3D granular flow simulation processes created by the discrete element method
LIGGGHTS and concentrate on simulations of physical systems found in real world
applications like rotating drums and hoppers. We discuss how to implement Graph
Neural Networks that deal with 3D objects, boundary conditions, particle -
particle, and particle - boundary interactions such that an accurate modeling
of relevant physical quantities is made possible. Finally, we compare the
machine learning based trajectories to LIGGGHTS trajectories in terms of
particle flows and mixing entropies.",arxiv
http://arxiv.org/abs/2106.02964v1,2021-06-05T21:15:34Z,2021-06-05T21:15:34Z,"A Review of Machine Learning Classification Using Quantum Annealing for
  Real-world Applications","Optimizing the training of a machine learning pipeline helps in reducing
training costs and improving model performance. One such optimizing strategy is
quantum annealing, which is an emerging computing paradigm that has shown
potential in optimizing the training of a machine learning model. The
implementation of a physical quantum annealer has been realized by D-Wave
systems and is available to the research community for experiments. Recent
experimental results on a variety of machine learning applications using
quantum annealing have shown interesting results where the performance of
classical machine learning techniques is limited by limited training data and
high dimensional features. This article explores the application of D-Wave's
quantum annealer for optimizing machine learning pipelines for real-world
classification problems. We review the application domains on which a physical
quantum annealer has been used to train machine learning classifiers. We
discuss and analyze the experiments performed on the D-Wave quantum annealer
for applications such as image recognition, remote sensing imagery,
computational biology, and particle physics. We discuss the possible advantages
and the problems for which quantum annealing is likely to be advantageous over
classical computation.",arxiv
http://arxiv.org/abs/2008.13369v1,2020-08-31T05:12:57Z,2020-08-31T05:12:57Z,"Introducing Representations of Facial Affect in Automated Multimodal
  Deception Detection","Automated deception detection systems can enhance health, justice, and
security in society by helping humans detect deceivers in high-stakes
situations across medical and legal domains, among others. This paper presents
a novel analysis of the discriminative power of dimensional representations of
facial affect for automated deception detection, along with interpretable
features from visual, vocal, and verbal modalities. We used a video dataset of
people communicating truthfully or deceptively in real-world, high-stakes
courtroom situations. We leveraged recent advances in automated emotion
recognition in-the-wild by implementing a state-of-the-art deep neural network
trained on the Aff-Wild database to extract continuous representations of
facial valence and facial arousal from speakers. We experimented with unimodal
Support Vector Machines (SVM) and SVM-based multimodal fusion methods to
identify effective features, modalities, and modeling approaches for detecting
deception. Unimodal models trained on facial affect achieved an AUC of 80%, and
facial affect contributed towards the highest-performing multimodal approach
(adaptive boosting) that achieved an AUC of 91% when tested on speakers who
were not part of training sets. This approach achieved a higher AUC than
existing automated machine learning approaches that used interpretable visual,
vocal, and verbal features to detect deception in this dataset, but did not use
facial affect. Across all videos, deceptive and truthful speakers exhibited
significant differences in facial valence and facial arousal, contributing
computational support to existing psychological theories on affect and
deception. The demonstrated importance of facial affect in our models informs
and motivates the future development of automated, affect-aware machine
learning approaches for modeling and detecting deception and other social
behaviors in-the-wild.",arxiv
http://arxiv.org/abs/2110.13041v1,2021-10-25T15:25:25Z,2021-10-25T15:25:25Z,Applications and Techniques for Fast Machine Learning in Science,"In this community review report, we discuss applications and techniques for
fast machine learning (ML) in science -- the concept of integrating power ML
methods into the real-time experimental data processing loop to accelerate
scientific discovery. The material for the report builds on two workshops held
by the Fast ML for Science community and covers three main areas: applications
for fast ML across a number of scientific domains; techniques for training and
implementing performant and resource-efficient ML algorithms; and computing
architectures, platforms, and technologies for deploying these algorithms. We
also present overlapping challenges across the multiple scientific domains
where common solutions can be found. This community report is intended to give
plenty of examples and inspiration for scientific discovery through integrated
and accelerated ML solutions. This is followed by a high-level overview and
organization of technical advances, including an abundance of pointers to
source material, which can enable these breakthroughs.",arxiv
http://arxiv.org/abs/2104.01757v1,2021-04-05T03:49:45Z,2021-04-05T03:49:45Z,Predicting Mergers and Acquisitions using Graph-based Deep Learning,"The graph data structure is a staple in mathematics, yet graph-based machine
learning is a relatively green field within the domain of data science. Recent
advances in graph-based ML and open source implementations of relevant
algorithms are allowing researchers to apply methods created in academia to
real-world datasets. The goal of this project was to utilize a popular graph
machine learning framework, GraphSAGE, to predict mergers and acquisitions
(M&A) of enterprise companies. The results were promising, as the model
predicted with 81.79% accuracy on a validation dataset. Given the abundance of
data sources and algorithmic decision making within financial data science,
graph-based machine learning offers a performant, yet non-traditional approach
to generating alpha.",arxiv
http://arxiv.org/abs/2108.03713v1,2021-08-08T19:12:04Z,2021-08-08T19:12:04Z,"On the Difficulty of Generalizing Reinforcement Learning Framework for
  Combinatorial Optimization","Combinatorial optimization problems (COPs) on the graph with real-life
applications are canonical challenges in Computer Science. The difficulty of
finding quality labels for problem instances holds back leveraging supervised
learning across combinatorial problems. Reinforcement learning (RL) algorithms
have recently been adopted to solve this challenge automatically. The
underlying principle of this approach is to deploy a graph neural network (GNN)
for encoding both the local information of the nodes and the graph-structured
data in order to capture the current state of the environment. Then, it is
followed by the actor to learn the problem-specific heuristics on its own and
make an informed decision at each state for finally reaching a good solution.
Recent studies on this subject mainly focus on a family of combinatorial
problems on the graph, such as the travel salesman problem, where the proposed
model aims to find an ordering of vertices that optimizes a given objective
function. We use the security-aware phone clone allocation in the cloud as a
classical quadratic assignment problem (QAP) to investigate whether or not deep
RL-based model is generally applicable to solve other classes of such hard
problems. Extensive empirical evaluation shows that existing RL-based model may
not generalize to QAP.",arxiv
http://arxiv.org/abs/1609.02664v1,2016-09-09T06:04:17Z,2016-09-09T06:04:17Z,"Machine Learning with Guarantees using Descriptive Complexity and SMT
  Solvers","Machine learning is a thriving part of computer science. There are many
efficient approaches to machine learning that do not provide strong theoretical
guarantees, and a beautiful general learning theory. Unfortunately, machine
learning approaches that give strong theoretical guarantees have not been
efficient enough to be applicable. In this paper we introduce a logical
approach to machine learning. Models are represented by tuples of logical
formulas and inputs and outputs are logical structures. We present our
framework together with several applications where we evaluate it using SAT and
SMT solvers. We argue that this approach to machine learning is particularly
suited to bridge the gap between efficiency and theoretical soundness. We
exploit results from descriptive complexity theory to prove strong theoretical
guarantees for our approach. To show its applicability, we present experimental
results including learning complexity-theoretic reductions rules for board
games. We also explain how neural networks fit into our framework, although the
current implementation does not scale to provide guarantees for real-world
neural networks.",arxiv
http://arxiv.org/abs/2010.04687v2,2021-01-18T19:52:07Z,2020-10-09T17:16:29Z,"A Series of Unfortunate Counterfactual Events: the Role of Time in
  Counterfactual Explanations","Counterfactual explanations are a prominent example of post-hoc
interpretability methods in the explainable Artificial Intelligence research
domain. They provide individuals with alternative scenarios and a set of
recommendations to achieve a sought-after machine learning model outcome.
Recently, the literature has identified desiderata of counterfactual
explanations, such as feasibility, actionability and sparsity that should
support their applicability in real-world contexts. However, we show that the
literature has neglected the problem of the time dependency of counterfactual
explanations. We argue that, due to their time dependency and because of the
provision of recommendations, even feasible, actionable and sparse
counterfactual explanations may not be appropriate in real-world applications.
This is due to the possible emergence of what we call ""unfortunate
counterfactual events."" These events may occur due to the retraining of machine
learning models whose outcomes have to be explained via counterfactual
explanation. Series of unfortunate counterfactual events frustrate the efforts
of those individuals who successfully implemented the recommendations of
counterfactual explanations. This negatively affects people's trust in the
ability of institutions to provide machine learning-supported decisions
consistently. We introduce an approach to address the problem of the emergence
of unfortunate counterfactual events that makes use of histories of
counterfactual explanations. In the final part of the paper we propose an
ethical analysis of two distinct strategies to cope with the challenge of
unfortunate counterfactual events. We show that they respond to an ethically
responsible imperative to preserve the trustworthiness of credit lending
organizations, the decision models they employ, and the social-economic
function of credit lending.",arxiv
http://arxiv.org/abs/2104.13478v2,2021-05-02T16:16:03Z,2021-04-27T21:09:51Z,"Geometric Deep Learning: Grids, Groups, Graphs, Geodesics, and Gauges","The last decade has witnessed an experimental revolution in data science and
machine learning, epitomised by deep learning methods. Indeed, many
high-dimensional learning tasks previously thought to be beyond reach -- such
as computer vision, playing Go, or protein folding -- are in fact feasible with
appropriate computational scale. Remarkably, the essence of deep learning is
built from two simple algorithmic principles: first, the notion of
representation or feature learning, whereby adapted, often hierarchical,
features capture the appropriate notion of regularity for each task, and
second, learning by local gradient-descent type methods, typically implemented
as backpropagation.
  While learning generic functions in high dimensions is a cursed estimation
problem, most tasks of interest are not generic, and come with essential
pre-defined regularities arising from the underlying low-dimensionality and
structure of the physical world. This text is concerned with exposing these
regularities through unified geometric principles that can be applied
throughout a wide spectrum of applications.
  Such a 'geometric unification' endeavour, in the spirit of Felix Klein's
Erlangen Program, serves a dual purpose: on one hand, it provides a common
mathematical framework to study the most successful neural network
architectures, such as CNNs, RNNs, GNNs, and Transformers. On the other hand,
it gives a constructive procedure to incorporate prior physical knowledge into
neural architectures and provide principled way to build future architectures
yet to be invented.",arxiv
http://arxiv.org/abs/2007.03578v2,2020-07-08T22:53:16Z,2020-07-07T15:55:50Z,"A Vision-based Social Distancing and Critical Density Detection System
  for COVID-19","Social distancing has been proven as an effective measure against the spread
of the infectious COronaVIrus Disease 2019 (COVID-19). However, individuals are
not used to tracking the required 6-feet (2-meters) distance between themselves
and their surroundings. An active surveillance system capable of detecting
distances between individuals and warning them can slow down the spread of the
deadly disease. Furthermore, measuring social density in a region of interest
(ROI) and modulating inflow can decrease social distancing violation occurrence
chance.
  On the other hand, recording data and labeling individuals who do not follow
the measures will breach individuals' rights in free-societies. Here we propose
an Artificial Intelligence (AI) based real-time social distancing detection and
warning system considering four important ethical factors: (1) the system
should never record/cache data, (2) the warnings should not target the
individuals, (3) no human supervisor should be in the detection/warning loop,
and (4) the code should be open-source and accessible to the public. Against
this backdrop, we propose using a monocular camera and deep learning-based
real-time object detectors to measure social distancing. If a violation is
detected, a non-intrusive audio-visual warning signal is emitted without
targeting the individual who breached the social distancing measure. Also, if
the social density is over a critical value, the system sends a control signal
to modulate inflow into the ROI. We tested the proposed method across
real-world datasets to measure its generality and performance. The proposed
method is ready for deployment, and our code is open-sourced.",arxiv
http://arxiv.org/abs/2105.03688v1,2021-05-08T12:48:08Z,2021-05-08T12:48:08Z,"HamNet: Conformation-Guided Molecular Representation with Hamiltonian
  Neural Networks","Well-designed molecular representations (fingerprints) are vital to combine
medical chemistry and deep learning. Whereas incorporating 3D geometry of
molecules (i.e. conformations) in their representations seems beneficial,
current 3D algorithms are still in infancy. In this paper, we propose a novel
molecular representation algorithm which preserves 3D conformations of
molecules with a Molecular Hamiltonian Network (HamNet). In HamNet, implicit
positions and momentums of atoms in a molecule interact in the Hamiltonian
Engine following the discretized Hamiltonian equations. These implicit
coordinations are supervised with real conformations with translation- &
rotation-invariant losses, and further used as inputs to the Fingerprint
Generator, a message-passing neural network. Experiments show that the
Hamiltonian Engine can well preserve molecular conformations, and that the
fingerprints generated by HamNet achieve state-of-the-art performances on
MoleculeNet, a standard molecular machine learning benchmark.",arxiv
http://arxiv.org/abs/1912.09621v1,2019-12-20T02:57:05Z,2019-12-20T02:57:05Z,"Understanding Deep Neural Network Predictions for Medical Imaging
  Applications","Computer-aided detection has been a research area attracting great interest
in the past decade. Machine learning algorithms have been utilized extensively
for this application as they provide a valuable second opinion to the doctors.
Despite several machine learning models being available for medical imaging
applications, not many have been implemented in the real-world due to the
uninterpretable nature of the decisions made by the network. In this paper, we
investigate the results provided by deep neural networks for the detection of
malaria, diabetic retinopathy, brain tumor, and tuberculosis in different
imaging modalities. We visualize the class activation mappings for all the
applications in order to enhance the understanding of these networks. This type
of visualization, along with the corresponding network performance metrics,
would aid the data science experts in better understanding of their models as
well as assisting doctors in their decision-making process.",arxiv
http://arxiv.org/abs/2107.06882v1,2021-07-14T17:55:28Z,2021-07-14T17:55:28Z,"Conservative Objective Models for Effective Offline Model-Based
  Optimization","Computational design problems arise in a number of settings, from synthetic
biology to computer architectures. In this paper, we aim to solve data-driven
model-based optimization (MBO) problems, where the goal is to find a design
input that maximizes an unknown objective function provided access to only a
static dataset of prior experiments. Such data-driven optimization procedures
are the only practical methods in many real-world domains where active data
collection is expensive (e.g., when optimizing over proteins) or dangerous
(e.g., when optimizing over aircraft designs). Typical methods for MBO that
optimize the design against a learned model suffer from distributional shift:
it is easy to find a design that ""fools"" the model into predicting a high
value. To overcome this, we propose conservative objective models (COMs), a
method that learns a model of the objective function that lower bounds the
actual value of the ground-truth objective on out-of-distribution inputs, and
uses it for optimization. Structurally, COMs resemble adversarial training
methods used to overcome adversarial examples. COMs are simple to implement and
outperform a number of existing methods on a wide range of MBO problems,
including optimizing protein sequences, robot morphologies, neural network
weights, and superconducting materials.",arxiv
http://arxiv.org/abs/2006.10111v1,2020-06-17T19:22:58Z,2020-06-17T19:22:58Z,Pendant Drop Tensiometry: A Machine Learning Approach,"Modern pendant drop tensiometry relies on numerical solution of the
Young-Laplace equation and allow to determine the surface tension from a single
picture of a pendant drop with high precision. Most of these techniques solve
the Young-Laplace equation many times over to find the material parameters that
provide a fit to a supplied image of a real droplet. Here we introduce a
machine learning approach to solve this problem in a computationally more
efficient way. We train a deep neural network to determine the surface tension
of a given droplet shape using a large training set of numerically generated
droplet shapes. We show that the deep learning approach is superior to the
current state of the art shape fitting approach in speed and precision, in
particular if shapes in the training set reflect the sensitivity of the droplet
shape with respect to surface tension. In order to derive such an optimized
training set we clarify the role of the Worthington number as quality indicator
in conventional shape fitting and in the machine learning approach. Our
approach demonstrates the capabilities of deep neural networks in the material
parameter determination from rheological deformation experiments in general.",arxiv
http://arxiv.org/abs/1904.07633v1,2019-04-16T13:02:01Z,2019-04-16T13:02:01Z,"HARK Side of Deep Learning -- From Grad Student Descent to Automated
  Machine Learning","Recent advancements in machine learning research, i.e., deep learning,
introduced methods that excel conventional algorithms as well as humans in
several complex tasks, ranging from detection of objects in images and speech
recognition to playing difficult strategic games. However, the current
methodology of machine learning research and consequently, implementations of
the real-world applications of such algorithms, seems to have a recurring
HARKing (Hypothesizing After the Results are Known) issue. In this work, we
elaborate on the algorithmic, economic and social reasons and consequences of
this phenomenon. We present examples from current common practices of
conducting machine learning research (e.g. avoidance of reporting negative
results) and failure of generalization ability of the proposed algorithms and
datasets in actual real-life usage. Furthermore, a potential future trajectory
of machine learning research and development from the perspective of
accountable, unbiased, ethical and privacy-aware algorithmic decision making is
discussed. We would like to emphasize that with this discussion we neither
claim to provide an exhaustive argumentation nor blame any specific institution
or individual on the raised issues. This is simply a discussion put forth by
us, insiders of the machine learning field, reflecting on us.",arxiv
http://arxiv.org/abs/2010.14866v1,2020-10-28T10:25:05Z,2020-10-28T10:25:05Z,"Deterministic and probabilistic deep learning models for inverse design
  of broadband acoustic cloak","Concealing an object from incoming waves (light and/or sound) remained
science fiction for a long time due to the absence of wave-shielding materials
in nature. Yet, the invention of artificial materials and new physical
principles for optical and sound wave manipulation translated this abstract
concept into reality by making an object acoustically invisible. Here, we
present the notion of a machine learning-driven acoustic cloak and demonstrate
an example of such a cloak with a multilayered core-shell configuration.
Importantly, we develop deterministic and probabilistic deep learning models
based on autoencoder-like neural network structure to retrieve the structural
and material properties of the cloaking shell surrounding the object that
suppresses scattering of sound in a broad spectral range, as if it was not
there. The probabilistic model enhances the generalization ability of design
procedure and uncovers the sensitivity of the cloak parameters on the spectral
response for practical implementation. This proposal opens up new avenues to
expedite the design of intelligent cloaking devices for tailored spectral
response and offers a feasible solution for inverse scattering problems.",arxiv
http://arxiv.org/abs/2110.04080v1,2021-10-03T10:52:19Z,2021-10-03T10:52:19Z,Landslide Detection in Real-Time Social Media Image Streams,"Lack of global data inventories obstructs scientific modeling of and response
to landslide hazards which are oftentimes deadly and costly. To remedy this
limitation, new approaches suggest solutions based on citizen science that
requires active participation. However, as a non-traditional data source,
social media has been increasingly used in many disaster response and
management studies in recent years. Inspired by this trend, we propose to
capitalize on social media data to mine landslide-related information
automatically with the help of artificial intelligence (AI) techniques.
Specifically, we develop a state-of-the-art computer vision model to detect
landslides in social media image streams in real time. To that end, we create a
large landslide image dataset labeled by experts and conduct extensive model
training experiments. The experimental results indicate that the proposed model
can be deployed in an online fashion to support global landslide susceptibility
maps and emergency response.",arxiv
http://arxiv.org/abs/2102.00625v1,2021-02-01T04:07:38Z,2021-02-01T04:07:38Z,"Human Perceptions on Moral Responsibility of AI: A Case Study in
  AI-Assisted Bail Decision-Making","How to attribute responsibility for autonomous artificial intelligence (AI)
systems' actions has been widely debated across the humanities and social
science disciplines. This work presents two experiments ($N$=200 each) that
measure people's perceptions of eight different notions of moral responsibility
concerning AI and human agents in the context of bail decision-making. Using
real-life adapted vignettes, our experiments show that AI agents are held
causally responsible and blamed similarly to human agents for an identical
task. However, there was a meaningful difference in how people perceived these
agents' moral responsibility; human agents were ascribed to a higher degree of
present-looking and forward-looking notions of responsibility than AI agents.
We also found that people expect both AI and human decision-makers and advisors
to justify their decisions regardless of their nature. We discuss policy and
HCI implications of these findings, such as the need for explainable AI in
high-stakes scenarios.",arxiv
http://arxiv.org/abs/2103.06450v2,2021-05-21T18:52:44Z,2021-03-11T04:37:29Z,Full Page Handwriting Recognition via Image to Sequence Extraction,"We present a Neural Network based Handwritten Text Recognition (HTR) model
architecture that can be trained to recognize full pages of handwritten or
printed text without image segmentation. Being based on Image to Sequence
architecture, it can extract text present in an image and then sequence it
correctly without imposing any constraints regarding orientation, layout and
size of text and non-text. Further, it can also be trained to generate
auxiliary markup related to formatting, layout and content. We use character
level vocabulary, thereby enabling language and terminology of any subject. The
model achieves a new state-of-art in paragraph level recognition on the IAM
dataset. When evaluated on scans of real world handwritten free form test
answers - beset with curved and slanted lines, drawings, tables, math,
chemistry and other symbols - it performs better than all commercially
available HTR cloud APIs. It is deployed in production as part of a commercial
web application.",arxiv
http://arxiv.org/abs/2007.13505v1,2020-07-16T20:35:46Z,2020-07-16T20:35:46Z,"Modern Hopfield Networks and Attention for Immune Repertoire
  Classification","A central mechanism in machine learning is to identify, store, and recognize
patterns. How to learn, access, and retrieve such patterns is crucial in
Hopfield networks and the more recent transformer architectures. We show that
the attention mechanism of transformer architectures is actually the update
rule of modern Hopfield networks that can store exponentially many patterns. We
exploit this high storage capacity of modern Hopfield networks to solve a
challenging multiple instance learning (MIL) problem in computational biology:
immune repertoire classification. Accurate and interpretable machine learning
methods solving this problem could pave the way towards new vaccines and
therapies, which is currently a very relevant research topic intensified by the
COVID-19 crisis. Immune repertoire classification based on the vast number of
immunosequences of an individual is a MIL problem with an unprecedentedly
massive number of instances, two orders of magnitude larger than currently
considered problems, and with an extremely low witness rate. In this work, we
present our novel method DeepRC that integrates transformer-like attention, or
equivalently modern Hopfield networks, into deep learning architectures for
massive MIL such as immune repertoire classification. We demonstrate that
DeepRC outperforms all other methods with respect to predictive performance on
large-scale experiments, including simulated and real-world virus infection
data, and enables the extraction of sequence motifs that are connected to a
given disease class. Source code and datasets: https://github.com/ml-jku/DeepRC",arxiv
http://arxiv.org/abs/1805.03045v2,2018-06-12T08:10:11Z,2018-05-08T14:15:46Z,"A new method for unveiling Open Clusters in Gaia: new nearby Open
  Clusters confirmed by DR2","The publication of the Gaia Data Release 2 (Gaia DR2) opens a new era in
Astronomy. It includes precise astrometric data (positions, proper motions and
parallaxes) for more than $1.3$ billion sources, mostly stars. To analyse such
a vast amount of new data, the use of data mining techniques and machine
learning algorithms are mandatory. The search for Open Clusters, groups of
stars that were born and move together, located in the disk, is a great example
for the application of these techniques. Our aim is to develop a method to
automatically explore the data space, requiring minimal manual intervention. We
explore the performance of a density based clustering algorithm, DBSCAN, to
find clusters in the data together with a supervised learning method such as an
Artificial Neural Network (ANN) to automatically distinguish between real Open
Clusters and statistical clusters. The development and implementation of this
method to a $5$-Dimensional space ($l$, $b$, $\varpi$, $\mu_{\alpha^*}$,
$\mu_\delta$) to the Tycho-Gaia Astrometric Solution (TGAS) data, and a
posterior validation using Gaia DR2 data, lead to the proposal of a set of new
nearby Open Clusters. We have developed a method to find OCs in astrometric
data, designed to be applied to the full Gaia DR2 archive.",arxiv
http://arxiv.org/abs/1908.10398v1,2019-08-27T18:30:49Z,2019-08-27T18:30:49Z,"A Data-Efficient Deep Learning Approach for Deployable Multimodal Social
  Robots","The deep supervised and reinforcement learning paradigms (among others) have
the potential to endow interactive multimodal social robots with the ability of
acquiring skills autonomously. But it is still not very clear yet how they can
be best deployed in real world applications. As a step in this direction, we
propose a deep learning-based approach for efficiently training a humanoid
robot to play multimodal games---and use the game of `Noughts & Crosses' with
two variants as a case study. Its minimum requirements for learning to perceive
and interact are based on a few hundred example images, a few example
multimodal dialogues and physical demonstrations of robot manipulation, and
automatic simulations. In addition, we propose novel algorithms for robust
visual game tracking and for competitive policy learning with high winning
rates, which substantially outperform DQN-based baselines. While an automatic
evaluation shows evidence that the proposed approach can be easily extended to
new games with competitive robot behaviours, a human evaluation with 130 humans
playing with the Pepper robot confirms that highly accurate visual perception
is required for successful game play.",arxiv
http://arxiv.org/abs/2108.09862v1,2021-08-22T22:44:28Z,2021-08-22T22:44:28Z,"Explainable Machine Learning using Real, Synthetic and Augmented Fire
  Tests to Predict Fire Resistance and Spalling of RC Columns","This paper presents the development of systematic machine learning (ML)
approach to enable explainable and rapid assessment of fire resistance and
fire-induced spalling of reinforced concrete (RC) columns. The developed
approach comprises of an ensemble of three novel ML algorithms namely; random
forest (RF), extreme gradient boosted trees (ExGBT), and deep learning (DL).
These algorithms are trained to account for a wide collection of geometric
characteristics and material properties, as well as loading conditions to
examine fire performance of normal and high strength RC columns by analyzing a
comprehensive database of fire tests comprising of over 494 observations. The
developed ensemble is also capable of presenting quantifiable insights to ML
predictions; thus, breaking free from the notion of 'blackbox' ML and
establishing a solid step towards transparent and explainable ML. Most
importantly, this work tackles the scarcity of available fire tests by
proposing new techniques to leverage the use of real, synthetic and augmented
fire test observations. The developed ML ensemble has been calibrated and
validated for standard and design fire exposures and for one, two, three and
four-sided fire exposures thus; covering a wide range of practical scenarios
present during fire incidents. When fully deployed, the developed ensemble can
analyze over 5,000 RC columns in under 60 seconds thus, providing an attractive
solution for researchers and practitioners. The presented approach can also be
easily extended for evaluating fire resistance and spalling of other structural
members and under varying fire scenarios and loading conditions and hence paves
the way to modernize the state of this research area and practice.",arxiv
http://arxiv.org/abs/2001.05871v1,2020-01-14T19:00:00Z,2020-01-14T19:00:00Z,"""Why is 'Chicago' deceptive?"" Towards Building Model-Driven Tutorials
  for Humans","To support human decision making with machine learning models, we often need
to elucidate patterns embedded in the models that are unsalient, unknown, or
counterintuitive to humans. While existing approaches focus on explaining
machine predictions with real-time assistance, we explore model-driven
tutorials to help humans understand these patterns in a training phase. We
consider both tutorials with guidelines from scientific papers, analogous to
current practices of science communication, and automatically selected examples
from training data with explanations. We use deceptive review detection as a
testbed and conduct large-scale, randomized human-subject experiments to
examine the effectiveness of such tutorials. We find that tutorials indeed
improve human performance, with and without real-time assistance. In
particular, although deep learning provides superior predictive performance
than simple models, tutorials and explanations from simple models are more
useful to humans. Our work suggests future directions for human-centered
tutorials and explanations towards a synergy between humans and AI.",arxiv
http://arxiv.org/abs/2010.10346v2,2021-02-27T18:46:18Z,2020-10-20T15:12:30Z,"Deep Importance Sampling based on Regression for Model Inversion and
  Emulation","Understanding systems by forward and inverse modeling is a recurrent topic of
research in many domains of science and engineering. In this context, Monte
Carlo methods have been widely used as powerful tools for numerical inference
and optimization. They require the choice of a suitable proposal density that
is crucial for their performance. For this reason, several adaptive importance
sampling (AIS) schemes have been proposed in the literature. We here present an
AIS framework called Regression-based Adaptive Deep Importance Sampling
(RADIS). In RADIS, the key idea is the adaptive construction via regression of
a non-parametric proposal density (i.e., an emulator), which mimics the
posterior distribution and hence minimizes the mismatch between proposal and
target densities. RADIS is based on a deep architecture of two (or more) nested
IS schemes, in order to draw samples from the constructed emulator. The
algorithm is highly efficient since employs the posterior approximation as
proposal density, which can be improved adding more support points. As a
consequence, RADIS asymptotically converges to an exact sampler under mild
conditions. Additionally, the emulator produced by RADIS can be in turn used as
a cheap surrogate model for further studies. We introduce two specific RADIS
implementations that use Gaussian Processes (GPs) and Nearest Neighbors (NN)
for constructing the emulator. Several numerical experiments and comparisons
show the benefits of the proposed schemes. A real-world application in remote
sensing model inversion and emulation confirms the validity of the approach.",arxiv
http://arxiv.org/abs/2110.08760v1,2021-10-17T08:41:21Z,2021-10-17T08:41:21Z,"Adapting Membership Inference Attacks to GNN for Graph Classification:
  Approaches and Implications","Graph Neural Networks (GNNs) are widely adopted to analyse non-Euclidean
data, such as chemical networks, brain networks, and social networks, modelling
complex relationships and interdependency between objects. Recently, Membership
Inference Attack (MIA) against GNNs raises severe privacy concerns, where
training data can be leaked from trained GNN models. However, prior studies
focus on inferring the membership of only the components in a graph, e.g., an
individual node or edge. How to infer the membership of an entire graph record
is yet to be explored.
  In this paper, we take the first step in MIA against GNNs for graph-level
classification. Our objective is to infer whether a graph sample has been used
for training a GNN model. We present and implement two types of attacks, i.e.,
training-based attacks and threshold-based attacks from different adversarial
capabilities. We perform comprehensive experiments to evaluate our attacks in
seven real-world datasets using five representative GNN models. Both our
attacks are shown effective and can achieve high performance, i.e., reaching
over 0.7 attack F1 scores in most cases. Furthermore, we analyse the
implications behind the MIA against GNNs. Our findings confirm that GNNs can be
even more vulnerable to MIA than the models with non-graph structures. And
unlike the node-level classifier, MIAs on graph-level classification tasks are
more co-related with the overfitting level of GNNs rather than the statistic
property of their training graphs.",arxiv
http://arxiv.org/abs/2111.00463v1,2021-10-31T10:49:21Z,2021-10-31T10:49:21Z,"FastCover: An Unsupervised Learning Framework for Multi-Hop Influence
  Maximization in Social Networks","Finding influential users in social networks is a fundamental problem with
many possible useful applications. Viewing the social network as a graph, the
influence of a set of users can be measured by the number of neighbors located
within a given number of hops in the network, where each hop marks a step of
influence diffusion. In this paper, we reduce the problem of IM to a
budget-constrained d-hop dominating set problem (kdDSP). We propose a unified
machine learning (ML) framework, FastCover, to solve kdDSP by learning an
efficient greedy strategy in an unsupervised way. As one critical component of
the framework, we devise a novel graph neural network (GNN) architecture, graph
reversed attention network (GRAT), that captures the diffusion process among
neighbors. Unlike most heuristic algorithms and concurrent ML frameworks for
combinatorial optimization problems, FastCover determines the entire seed set
from the nodes' scores computed with only one forward propagation of the GNN
and has a time complexity quasi-linear in the graph size. Experiments on
synthetic graphs and real-world social networks demonstrate that FastCover
finds solutions with better or comparable quality rendered by the concurrent
algorithms while achieving a speedup of over 1000x.",arxiv
http://arxiv.org/abs/1911.04469v1,2019-11-09T19:59:17Z,2019-11-09T19:59:17Z,"A Proposed Artificial intelligence Model for Real-Time Human Action
  Localization and Tracking","In recent years, artificial intelligence (AI) based on deep learning (DL) has
sparked tremendous global interest. DL is widely used today and has expanded
into various interesting areas. It is becoming more popular in cross-subject
research, such as studies of smart city systems, which combine computer science
with engineering applications. Human action detection is one of these areas.
Human action detection is an interesting challenge due to its stringent
requirements in terms of computing speed and accuracy. High-accuracy real-time
object tracking is also considered a significant challenge. This paper
integrates the YOLO detection network, which is considered a state-of-the-art
tool for real-time object detection, with motion vectors and the Coyote
Optimization Algorithm (COA) to construct a real-time human action localization
and tracking system. The proposed system starts with the extraction of motion
information from a compressed video stream and the extraction of appearance
information from RGB frames using an object detector. Then, a fusion step
between the two streams is performed, and the results are fed into the proposed
action tracking model. The COA is used in object tracking due to its accuracy
and fast convergence. The basic foundation of the proposed model is the
utilization of motion vectors, which already exist in a compressed video bit
stream and provide sufficient information to improve the localization of the
target action without requiring high consumption of computational resources
compared with other popular methods of extracting motion information, such as
optical flows. This advantage allows the proposed approach to be implemented in
challenging environments where the computational resources are limited, such as
Internet of Things (IoT) systems.",arxiv
http://arxiv.org/abs/2004.09608v2,2020-04-22T05:23:40Z,2020-04-20T20:14:00Z,"Flow-based Algorithms for Improving Clusters: A Unifying Framework,
  Software, and Performance","Clustering points in a vector space or nodes in a graph is a ubiquitous
primitive in statistical data analysis, and it is commonly used for exploratory
data analysis. In practice, it is often of interest to ""refine"" or ""improve"" a
given cluster that has been obtained by some other method. In this survey, we
focus on principled algorithms for this cluster improvement problem. Many such
cluster improvement algorithms are flow-based methods, by which we mean that
operationally they require the solution of a sequence of maximum flow problems
on a (typically implicitly) modified data graph. These cluster improvement
algorithms are powerful, both in theory and in practice, but they have not been
widely adopted for problems such as community detection, local graph
clustering, semi-supervised learning, etc. Possible reasons for this are: the
steep learning curve for these algorithms; the lack of efficient and easy to
use software; and the lack of detailed numerical experiments on real-world data
that demonstrate their usefulness. Our objective here is to address these
issues. To do so, we guide the reader through the whole process of
understanding how to implement and apply these powerful algorithms. We present
a unifying fractional programming optimization framework that permits us to
distill out in a simple way the crucial components of all these algorithms. It
also makes apparent similarities and differences between related methods.
Viewing these cluster improvement algorithms via a fractional programming
framework suggests directions for future algorithm development. Finally, we
develop efficient implementations of these algorithms in our
LocalGraphClustering python package, and we perform extensive numerical
experiments to demonstrate the performance of these methods on social networks
and image-based data graphs.",arxiv
http://arxiv.org/abs/1612.03217v1,2016-12-09T23:31:35Z,2016-12-09T23:31:35Z,Automatic Lymphocyte Detection in H&E Images with Deep Neural Networks,"Automatic detection of lymphocyte in H&E images is a necessary first step in
lots of tissue image analysis algorithms. An accurate and robust automated
lymphocyte detection approach is of great importance in both computer science
and clinical studies. Most of the existing approaches for lymphocyte detection
are based on traditional image processing algorithms and/or classic machine
learning methods. In the recent years, deep learning techniques have
fundamentally transformed the way that a computer interprets images and have
become a matchless solution in various pattern recognition problems. In this
work, we design a new deep neural network model which extends the fully
convolutional network by combining the ideas in several recent techniques, such
as shortcut links. Also, we design a new training scheme taking the prior
knowledge about lymphocytes into consideration. The training scheme not only
efficiently exploits the limited amount of free-form annotations from
pathologists, but also naturally supports efficient fine-tuning. As a
consequence, our model has the potential of self-improvement by leveraging the
errors collected during real applications. Our experiments show that our deep
neural network model achieves good performance in the images of different
staining conditions or different types of tissues.",arxiv
http://arxiv.org/abs/2104.13386v1,2021-04-27T18:00:02Z,2021-04-27T18:00:02Z,"Deep physical neural networks enabled by a backpropagation algorithm for
  arbitrary physical systems","Deep neural networks have become a pervasive tool in science and engineering.
However, modern deep neural networks' growing energy requirements now
increasingly limit their scaling and broader use. We propose a radical
alternative for implementing deep neural network models: Physical Neural
Networks. We introduce a hybrid physical-digital algorithm called Physics-Aware
Training to efficiently train sequences of controllable physical systems to act
as deep neural networks. This method automatically trains the functionality of
any sequence of real physical systems, directly, using backpropagation, the
same technique used for modern deep neural networks. To illustrate their
generality, we demonstrate physical neural networks with three diverse physical
systems-optical, mechanical, and electrical. Physical neural networks may
facilitate unconventional machine learning hardware that is orders of magnitude
faster and more energy efficient than conventional electronic processors.",arxiv
http://arxiv.org/abs/2003.06769v2,2020-11-23T04:57:39Z,2020-03-15T06:39:59Z,"Multi-AI competing and winning against humans in iterated
  Rock-Paper-Scissors game","Predicting and modeling human behavior and finding trends within human
decision-making processes is a major problem of social science. Rock Paper
Scissors (RPS) is the fundamental strategic question in many game theory
problems and real-world competitions. Finding the right approach to beat a
particular human opponent is challenging. Here we use an AI (artificial
intelligence) algorithm based on Markov Models of one fixed memory length
(abbreviated as ""single AI"") to compete against humans in an iterated RPS game.
We model and predict human competition behavior by combining many Markov Models
with different fixed memory lengths (abbreviated as ""multi-AI""), and develop an
architecture of multi-AI with changeable parameters to adapt to different
competition strategies. We introduce a parameter called ""focus length"" (a
positive number such as 5 or 10) to control the speed and sensitivity for our
multi-AI to adapt to the opponent's strategy change. The focus length is the
number of previous rounds that the multi-AI should look at when determining
which Single-AI has the best performance and should choose to play for the next
game. We experimented with 52 different people, each playing 300 rounds
continuously against one specific multi-AI model, and demonstrated that our
strategy could win against more than 95% of human opponents.",arxiv
http://arxiv.org/abs/2103.16323v2,2021-04-08T09:28:14Z,2021-03-30T13:15:48Z,"Thermal Neural Networks: Lumped-Parameter Thermal Modeling With
  State-Space Machine Learning","With electric power systems becoming more compact and increasingly powerful,
the relevance of thermal stress especially during overload operation is
expected to increase ceaselessly. Whenever critical temperatures cannot be
measured economically on a sensor base, a thermal model lends itself to
estimate those unknown quantities. Thermal models for electric power systems
are usually required to be both, real-time capable and of high estimation
accuracy. Moreover, ease of implementation and time to production play an
increasingly important role. In this work, the thermal neural network (TNN) is
introduced, which unifies both, consolidated knowledge in the form of
heat-transfer-based lumped-parameter models, and data-driven nonlinear function
approximation with supervised machine learning. A quasi-linear
parameter-varying system is identified solely from empirical data, where
relationships between scheduling variables and system matrices are inferred
statistically and automatically. At the same time, a TNN has physically
interpretable states through its state-space representation, is end-to-end
trainable -- similar to deep learning models -- with automatic differentiation,
and requires no material, geometry, nor expert knowledge for its design.
Experiments on an electric motor data set show that a TNN achieves higher
temperature estimation accuracies than previous white-/grey- or black-box
models with a mean squared error of $3.18~\text{K}^2$ and a worst-case error of
$5.84~\text{K}$ at 64 model parameters.",arxiv
http://arxiv.org/abs/1806.03342v1,2018-06-08T20:11:05Z,2018-06-08T20:11:05Z,Discovering Signals from Web Sources to Predict Cyber Attacks,"Cyber attacks are growing in frequency and severity. Over the past year alone
we have witnessed massive data breaches that stole personal information of
millions of people and wide-scale ransomware attacks that paralyzed critical
infrastructure of several countries. Combating the rising cyber threat calls
for a multi-pronged strategy, which includes predicting when these attacks will
occur. The intuition driving our approach is this: during the planning and
preparation stages, hackers leave digital traces of their activities on both
the surface web and dark web in the form of discussions on platforms like
hacker forums, social media, blogs and the like. These data provide predictive
signals that allow anticipating cyber attacks. In this paper, we describe
machine learning techniques based on deep neural networks and autoregressive
time series models that leverage external signals from publicly available Web
sources to forecast cyber attacks. Performance of our framework across ground
truth data over real-world forecasting tasks shows that our methods yield a
significant lift or increase of F1 for the top signals on predicted cyber
attacks. Our results suggest that, when deployed, our system will be able to
provide an effective line of defense against various types of targeted cyber
attacks.",arxiv
http://arxiv.org/abs/2102.09587v1,2021-02-18T19:25:52Z,2021-02-18T19:25:52Z,Interpretable Stability Bounds for Spectral Graph Filters,"Graph-structured data arise in a variety of real-world context ranging from
sensor and transportation to biological and social networks. As a ubiquitous
tool to process graph-structured data, spectral graph filters have been used to
solve common tasks such as denoising and anomaly detection, as well as design
deep learning architectures such as graph neural networks. Despite being an
important tool, there is a lack of theoretical understanding of the stability
properties of spectral graph filters, which are important for designing robust
machine learning models. In this paper, we study filter stability and provide a
novel and interpretable upper bound on the change of filter output, where the
bound is expressed in terms of the endpoint degrees of the deleted and newly
added edges, as well as the spatial proximity of those edges. This upper bound
allows us to reason, in terms of structural properties of the graph, when a
spectral graph filter will be stable. We further perform extensive experiments
to verify intuition that can be gained from the bound.",arxiv
http://arxiv.org/abs/2110.00840v1,2021-10-02T16:52:28Z,2021-10-02T16:52:28Z,"Induction, Popper, and machine learning","Francis Bacon popularized the idea that science is based on a process of
induction by which repeated observations are, in some unspecified way,
generalized to theories based on the assumption that the future resembles the
past. This idea was criticized by Hume and others as untenable leading to the
famous problem of induction. It wasn't until the work of Karl Popper that this
problem was solved, by demonstrating that induction is not the basis for
science and that the development of scientific knowledge is instead based on
the same principles as biological evolution. Today, machine learning is also
taught as being rooted in induction from big data. Solomonoff induction
implemented in an idealized Bayesian agent (Hutter's AIXI) is widely discussed
and touted as a framework for understanding AI algorithms, even though
real-world attempts to implement something like AIXI immediately encounter
fatal problems. In this paper, we contrast frameworks based on induction with
Donald T. Campbell's universal Darwinism. We show that most AI algorithms in
use today can be understood as using an evolutionary trial and error process
searching over a solution space. In this work we argue that a universal
Darwinian framework provides a better foundation for understanding AI systems.
Moreover, at a more meta level the process of development of all AI algorithms
can be understood under the framework of universal Darwinism.",arxiv
http://arxiv.org/abs/2010.12751v1,2020-10-24T03:09:37Z,2020-10-24T03:09:37Z,"Model Extraction Attacks on Graph Neural Networks: Taxonomy and
  Realization","Graph neural networks (GNNs) have been widely used to analyze the
graph-structured data in various application domains, e.g., social networks,
molecular biology, and anomaly detection. With great power, the GNN models,
usually as valuable Intellectual Properties of their owners, also become
attractive targets of the attacker. Recent studies show that machine learning
models are facing a severe threat called Model Extraction Attacks, where a
well-trained private model owned by a service provider can be stolen by the
attacker pretending as a client. Unfortunately, existing works focus on the
models trained on the Euclidean space, e.g., images and texts, while how to
extract a GNN model that contains a graph structure and node features is yet to
be explored. In this paper, we explore and develop model extraction attacks
against GNN models. Given only black-box access to a target GNN model, the
attacker aims to reconstruct a duplicated one via several nodes he obtained
(called attacker nodes). We first systematically formalise the threat modeling
in the context of GNN model extraction and classify the adversarial threats
into seven categories by considering different background knowledge of the
attacker, e.g., attributes and/or neighbor connectives of the attacker nodes.
Then we present the detailed methods which utilize the accessible knowledge in
each threat to implement the attacks. By evaluating over three real-world
datasets, our attacks are shown to extract duplicated models effectively, i.e.,
more than 89% inputs in the target domain have the same output predictions as
the victim model.",arxiv
http://arxiv.org/abs/2105.01774v2,2021-06-18T15:21:10Z,2021-05-04T21:40:04Z,"Envisioning Communities: A Participatory Approach Towards AI for Social
  Good","Research in artificial intelligence (AI) for social good presupposes some
definition of social good, but potential definitions have been seldom suggested
and never agreed upon. The normative question of what AI for social good
research should be ""for"" is not thoughtfully elaborated, or is frequently
addressed with a utilitarian outlook that prioritizes the needs of the majority
over those who have been historically marginalized, brushing aside realities of
injustice and inequity. We argue that AI for social good ought to be assessed
by the communities that the AI system will impact, using as a guide the
capabilities approach, a framework to measure the ability of different policies
to improve human welfare equity. Furthermore, we lay out how AI research has
the potential to catalyze social progress by expanding and equalizing
capabilities. We show how the capabilities approach aligns with a participatory
approach for the design and implementation of AI for social good research in a
framework we introduce called PACT, in which community members affected should
be brought in as partners and their input prioritized throughout the project.
We conclude by providing an incomplete set of guiding questions for carrying
out such participatory AI research in a way that elicits and respects a
community's own definition of social good.",arxiv
http://arxiv.org/abs/1707.06600v2,2017-09-06T17:32:44Z,2017-07-20T16:35:02Z,"A multi-agent reinforcement learning model of common-pool resource
  appropriation","Humanity faces numerous problems of common-pool resource appropriation. This
class of multi-agent social dilemma includes the problems of ensuring
sustainable use of fresh water, common fisheries, grazing pastures, and
irrigation systems. Abstract models of common-pool resource appropriation based
on non-cooperative game theory predict that self-interested agents will
generally fail to find socially positive equilibria---a phenomenon called the
tragedy of the commons. However, in reality, human societies are sometimes able
to discover and implement stable cooperative solutions. Decades of behavioral
game theory research have sought to uncover aspects of human behavior that make
this possible. Most of that work was based on laboratory experiments where
participants only make a single choice: how much to appropriate. Recognizing
the importance of spatial and temporal resource dynamics, a recent trend has
been toward experiments in more complex real-time video game-like environments.
However, standard methods of non-cooperative game theory can no longer be used
to generate predictions for this case. Here we show that deep reinforcement
learning can be used instead. To that end, we study the emergent behavior of
groups of independently learning agents in a partially observed Markov game
modeling common-pool resource appropriation. Our experiments highlight the
importance of trial-and-error learning in common-pool resource appropriation
and shed light on the relationship between exclusion, sustainability, and
inequality.",arxiv
http://arxiv.org/abs/2012.13968v1,2020-12-27T16:03:32Z,2020-12-27T16:03:32Z,"Detecting Medical Misinformation on Social Media Using Multimodal Deep
  Learning","In 2019, outbreaks of vaccine-preventable diseases reached the highest number
in the US since 1992. Medical misinformation, such as antivaccine content
propagating through social media, is associated with increases in vaccine delay
and refusal. Our overall goal is to develop an automatic detector for
antivaccine messages to counteract the negative impact that antivaccine
messages have on the public health. Very few extant detection systems have
considered multimodality of social media posts (images, texts, and hashtags),
and instead focus on textual components, despite the rapid growth of
photo-sharing applications (e.g., Instagram). As a result, existing systems are
not sufficient for detecting antivaccine messages with heavy visual components
(e.g., images) posted on these newer platforms. To solve this problem, we
propose a deep learning network that leverages both visual and textual
information. A new semantic- and task-level attention mechanism was created to
help our model to focus on the essential contents of a post that signal
antivaccine messages. The proposed model, which consists of three branches, can
generate comprehensive fused features for predictions. Moreover, an ensemble
method is proposed to further improve the final prediction accuracy. To
evaluate the proposed model's performance, a real-world social media dataset
that consists of more than 30,000 samples was collected from Instagram between
January 2016 and October 2019. Our 30 experiment results demonstrate that the
final network achieves above 97% testing accuracy and outperforms other
relevant models, demonstrating that it can detect a large amount of antivaccine
messages posted daily. The implementation code is available at
https://github.com/wzhings/antivaccine_detection.",arxiv
http://arxiv.org/abs/1906.05658v1,2019-06-07T08:10:16Z,2019-06-07T08:10:16Z,EKT: Exercise-aware Knowledge Tracing for Student Performance Prediction,"For offering proactive services to students in intelligent education, one of
the fundamental tasks is predicting their performance (e.g., scores) on future
exercises, where it is necessary to track each student's knowledge acquisition
during her exercising activities. However, existing approaches can only exploit
the exercising records of students, and the problem of extracting rich
information existed in the exercise's materials (e.g., knowledge concepts,
exercise content) to achieve both precise predictions of student performance
and interpretable analysis of knowledge acquisition remains underexplored. In
this paper, we present a holistic study of student performance prediction. To
directly achieve the primary goal of prediction, we first propose a general
Exercise-Enhanced Recurrent Neural Network (EERNN) framework by exploring both
student's records and the exercise contents. In EERNN, we simply summarize each
student's state into an integrated vector and trace it with a recurrent neural
network, where we design a bidirectional LSTM to learn the encoding of each
exercise's content. For making predictions, we propose two implementations
under EERNN with different strategies, i.e., EERNNM with Markov property and
EERNNA with Attention mechanism. Then, to explicitly track student's knowledge
acquisition on multiple knowledge concepts, we extend EERNN to an explainable
Exercise-aware Knowledge Tracing (EKT) by incorporating the knowledge concept
effects, where the student's integrated state vector is extended to a knowledge
state matrix. In EKT, we further develop a memory network for quantifying how
much each exercise can affect the mastery of students on concepts during the
exercising process. Finally, we conduct extensive experiments on large-scale
real-world data. The results demonstrate the prediction effectiveness of two
frameworks as well as the superior interpretability of EKT.",arxiv
http://arxiv.org/abs/2004.05953v1,2020-04-13T14:09:21Z,2020-04-13T14:09:21Z,"Software-Defined Network for End-to-end Networked Science at the
  Exascale","Domain science applications and workflow processes are currently forced to
view the network as an opaque infrastructure into which they inject data and
hope that it emerges at the destination with an acceptable Quality of
Experience. There is little ability for applications to interact with the
network to exchange information, negotiate performance parameters, discover
expected performance metrics, or receive status/troubleshooting information in
real time. The work presented here is motivated by a vision for a new smart
network and smart application ecosystem that will provide a more deterministic
and interactive environment for domain science workflows. The Software-Defined
Network for End-to-end Networked Science at Exascale (SENSE) system includes a
model-based architecture, implementation, and deployment which enables
automated end-to-end network service instantiation across administrative
domains. An intent based interface allows applications to express their
high-level service requirements, an intelligent orchestrator and resource
control systems allow for custom tailoring of scalability and real-time
responsiveness based on individual application and infrastructure operator
requirements. This allows the science applications to manage the network as a
first-class schedulable resource as is the current practice for instruments,
compute, and storage systems. Deployment and experiments on production networks
and testbeds have validated SENSE functions and performance. Emulation based
testing verified the scalability needed to support research and education
infrastructures. Key contributions of this work include an architecture
definition, reference implementation, and deployment. This provides the basis
for further innovation of smart network services to accelerate scientific
discovery in the era of big data, cloud computing, machine learning and
artificial intelligence.",arxiv
http://arxiv.org/abs/1907.10323v1,2019-07-24T09:27:11Z,2019-07-24T09:27:11Z,Fairness in Reinforcement Learning,"Decision support systems (e.g., for ecological conservation) and autonomous
systems (e.g., adaptive controllers in smart cities) start to be deployed in
real applications. Although their operations often impact many users or
stakeholders, no fairness consideration is generally taken into account in
their design, which could lead to completely unfair outcomes for some users or
stakeholders. To tackle this issue, we advocate for the use of social welfare
functions that encode fairness and present this general novel problem in the
context of (deep) reinforcement learning, although it could possibly be
extended to other machine learning tasks.",arxiv
http://arxiv.org/abs/1910.06840v3,2020-01-19T09:18:47Z,2019-10-15T14:58:54Z,A Hybrid Compact Neural Architecture for Visual Place Recognition,"State-of-the-art algorithms for visual place recognition, and related visual
navigation systems, can be broadly split into two categories:
computer-science-oriented models including deep learning or image
retrieval-based techniques with minimal biological plausibility, and
neuroscience-oriented dynamical networks that model temporal properties
underlying spatial navigation in the brain. In this letter, we propose a new
compact and high-performing place recognition model that bridges this divide
for the first time. Our approach comprises two key neural models of these
categories: (1) FlyNet, a compact, sparse two-layer neural network inspired by
brain architectures of fruit flies, Drosophila melanogaster, and (2) a
one-dimensional continuous attractor neural network (CANN). The resulting
FlyNet+CANN network incorporates the compact pattern recognition capabilities
of our FlyNet model with the powerful temporal filtering capabilities of an
equally compact CANN, replicating entirely in a hybrid neural implementation
the functionality that yields high performance in algorithmic localization
approaches like SeqSLAM. We evaluate our model, and compare it to three
state-of-the-art methods, on two benchmark real-world datasets with small
viewpoint variations and extreme environmental changes - achieving 87% AUC
results under day to night transitions compared to 60% for Multi-Process
Fusion, 46% for LoST-X and 1% for SeqSLAM, while being 6.5, 310, and 1.5 times
faster, respectively.",arxiv
http://arxiv.org/abs/1810.03032v1,2018-10-06T17:55:26Z,2018-10-06T17:55:26Z,"Constructing Graph Node Embeddings via Discrimination of Similarity
  Distributions","The problem of unsupervised learning node embeddings in graphs is one of the
important directions in modern network science. In this work we propose a novel
framework, which is aimed to find embeddings by \textit{discriminating
distributions of similarities (DDoS)} between nodes in the graph. The general
idea is implemented by maximizing the \textit{earth mover distance} between
distributions of decoded similarities of similar and dissimilar nodes. The
resulting algorithm generates embeddings which give a state-of-the-art
performance in the problem of link prediction in real-world graphs.",arxiv
http://arxiv.org/abs/1811.02213v1,2018-11-06T08:05:24Z,2018-11-06T08:05:24Z,"Hybrid Approach to Automation, RPA and Machine Learning: a Method for
  the Human-centered Design of Software Robots","One of the more prominent trends within Industry 4.0 is the drive to employ
Robotic Process Automation (RPA), especially as one of the elements of the Lean
approach. The full implementation of RPA is riddled with challenges relating
both to the reality of everyday business operations, from SMEs to SSCs and
beyond, and the social effects of the changing job market. To successfully
address these points there is a need to develop a solution that would adjust to
the existing business operations and at the same time lower the negative social
impact of the automation process.
  To achieve these goals we propose a hybrid, human-centered approach to the
development of software robots. This design and implementation method combines
the Living Lab approach with empowerment through participatory design to
kick-start the co-development and co-maintenance of hybrid software robots
which, supported by variety of AI methods and tools, including interactive and
collaborative ML in the cloud, transform menial job posts into higher-skilled
positions, allowing former employees to stay on as robot co-designers and
maintainers, i.e. as co-programmers who supervise the machine learning
processes with the use of tailored high-level RPA Domain Specific Languages
(DSLs) to adjust the functioning of the robots and maintain operational
flexibility.",arxiv
http://arxiv.org/abs/2106.10131v1,2021-06-18T13:47:56Z,2021-06-18T13:47:56Z,Enhancing user creativity: Semantic measures for idea generation,"Human creativity generates novel ideas to solve real-world problems. This
thereby grants us the power to transform the surrounding world and extend our
human attributes beyond what is currently possible. Creative ideas are not just
new and unexpected, but are also successful in providing solutions that are
useful, efficient and valuable. Thus, creativity optimizes the use of available
resources and increases wealth. The origin of human creativity, however, is
poorly understood, and semantic measures that could predict the success of
generated ideas are currently unknown. Here, we analyze a dataset of design
problem-solving conversations in real-world settings by using 49 semantic
measures based on WordNet 3.1 and demonstrate that a divergence of semantic
similarity, an increased information content, and a decreased polysemy predict
the success of generated ideas. The first feedback from clients also enhances
information content and leads to a divergence of successful ideas in creative
problem solving. These results advance cognitive science by identifying
real-world processes in human problem solving that are relevant to the success
of produced solutions and provide tools for real-time monitoring of problem
solving, student training and skill acquisition. A selected subset of
information content (IC S\'anchez-Batet) and semantic similarity
(Lin/S\'anchez-Batet) measures, which are both statistically powerful and
computationally fast, could support the development of technologies for
computer-assisted enhancements of human creativity or for the implementation of
creativity in machines endowed with general artificial intelligence.",arxiv
http://arxiv.org/abs/2003.05861v1,2020-03-12T15:52:49Z,2020-03-12T15:52:49Z,"The Chef's Hat Simulation Environment for Reinforcement-Learning-Based
  Agents","To achieve social interactions within Human-Robot Interaction (HRI)
environments is a very challenging task. Most of the current research focuses
on Wizard-of-Oz approaches, which neglect the recent development of intelligent
robots. On the other hand, real-world scenarios usually do not provide the
necessary control and reproducibility which are needed for learning algorithms.
In this paper, we propose a virtual simulation environment that implements the
Chef's Hat card game, designed to be used in HRI scenarios, to provide a
controllable and reproducible scenario for reinforcement-learning algorithms.",arxiv
http://arxiv.org/abs/2001.06216v2,2020-09-27T04:29:35Z,2020-01-17T09:50:28Z,"GraphLIME: Local Interpretable Model Explanations for Graph Neural
  Networks","Graph structured data has wide applicability in various domains such as
physics, chemistry, biology, computer vision, and social networks, to name a
few. Recently, graph neural networks (GNN) were shown to be successful in
effectively representing graph structured data because of their good
performance and generalization ability. GNN is a deep learning based method
that learns a node representation by combining specific nodes and the
structural/topological information of a graph. However, like other deep models,
explaining the effectiveness of GNN models is a challenging task because of the
complex nonlinear transformations made over the iterations. In this paper, we
propose GraphLIME, a local interpretable model explanation for graphs using the
Hilbert-Schmidt Independence Criterion (HSIC) Lasso, which is a nonlinear
feature selection method. GraphLIME is a generic GNN-model explanation
framework that learns a nonlinear interpretable model locally in the subgraph
of the node being explained. More specifically, to explain a node, we generate
a nonlinear interpretable model from its $N$-hop neighborhood and then compute
the K most representative features as the explanations of its prediction using
HSIC Lasso. Through experiments on two real-world datasets, the explanations of
GraphLIME are found to be of extraordinary degree and more descriptive in
comparison to the existing explanation methods.",arxiv
http://arxiv.org/abs/2002.08361v2,2020-03-13T20:47:01Z,2020-02-19T17:05:50Z,"Phase Imaging with Computational Specificity (PICS) for measuring dry
  mass changes in sub-cellular compartments","Due to its specificity, fluorescence microscopy (FM) has become a
quintessential imaging tool in cell biology. However, photobleaching,
phototoxicity, and related artifacts continue to limit FM's utility. Recently,
it has been shown that artificial intelligence (AI) can transform one form of
contrast into another. We present PICS, a combination of quantitative phase
imaging and AI, which provides information about unlabeled live cells with high
specificity. Our imaging system allows for automatic training, while inference
is built into the acquisition software and runs in real-time. Applying the
computed fluorescence maps back to the QPI data, we measured the growth of both
nuclei and cytoplasm independently, over many days, without loss of viability.
Using a QPI method that suppresses multiple scattering, we measured the dry
mass content of individual cell nuclei within spheroids. In its current
implementation, PICS offers a versatile quantitative technique for continuous
simultaneous monitoring of individual cellular components in biological
applications where long-term label-free imaging is desirable.",arxiv
http://arxiv.org/abs/2106.04008v2,2021-06-09T16:58:52Z,2021-06-07T23:31:47Z,Widening Access to Applied Machine Learning with TinyML,"Broadening access to both computational and educational resources is critical
to diffusing machine-learning (ML) innovation. However, today, most ML
resources and experts are siloed in a few countries and organizations. In this
paper, we describe our pedagogical approach to increasing access to applied ML
through a massive open online course (MOOC) on Tiny Machine Learning (TinyML).
We suggest that TinyML, ML on resource-constrained embedded devices, is an
attractive means to widen access because TinyML both leverages low-cost and
globally accessible hardware, and encourages the development of complete,
self-contained applications, from data collection to deployment. To this end, a
collaboration between academia (Harvard University) and industry (Google)
produced a four-part MOOC that provides application-oriented instruction on how
to develop solutions using TinyML. The series is openly available on the edX
MOOC platform, has no prerequisites beyond basic programming, and is designed
for learners from a global variety of backgrounds. It introduces pupils to
real-world applications, ML algorithms, data-set engineering, and the ethical
considerations of these technologies via hands-on programming and deployment of
TinyML applications in both the cloud and their own microcontrollers. To
facilitate continued learning, community building, and collaboration beyond the
courses, we launched a standalone website, a forum, a chat, and an optional
course-project competition. We also released the course materials publicly,
hoping they will inspire the next generation of ML practitioners and educators
and further broaden access to cutting-edge ML technologies.",arxiv
http://arxiv.org/abs/2001.00660v1,2020-01-02T22:56:15Z,2020-01-02T22:56:15Z,A Parallel Sparse Tensor Benchmark Suite on CPUs and GPUs,"Tensor computations present significant performance challenges that impact a
wide spectrum of applications ranging from machine learning, healthcare
analytics, social network analysis, data mining to quantum chemistry and signal
processing. Efforts to improve the performance of tensor computations include
exploring data layout, execution scheduling, and parallelism in common tensor
kernels. This work presents a benchmark suite for arbitrary-order sparse tensor
kernels using state-of-the-art tensor formats: coordinate (COO) and
hierarchical coordinate (HiCOO) on CPUs and GPUs. It presents a set of
reference tensor kernel implementations that are compatible with real-world
tensors and power law tensors extended from synthetic graph generation
techniques. We also propose Roofline performance models for these kernels to
provide insights of computer platforms from sparse tensor view.",arxiv
http://arxiv.org/abs/2006.15502v1,2020-06-28T04:37:57Z,2020-06-28T04:37:57Z,Scalable Deep Generative Modeling for Sparse Graphs,"Learning graph generative models is a challenging task for deep learning and
has wide applicability to a range of domains like chemistry, biology and social
science. However current deep neural methods suffer from limited scalability:
for a graph with $n$ nodes and $m$ edges, existing deep neural methods require
$\Omega(n^2)$ complexity by building up the adjacency matrix. On the other
hand, many real world graphs are actually sparse in the sense that $m\ll n^2$.
Based on this, we develop a novel autoregressive model, named BiGG, that
utilizes this sparsity to avoid generating the full adjacency matrix, and
importantly reduces the graph generation time complexity to $O((n + m)\log n)$.
Furthermore, during training this autoregressive model can be parallelized with
$O(\log n)$ synchronization stages, which makes it much more efficient than
other autoregressive models that require $\Omega(n)$. Experiments on several
benchmarks show that the proposed approach not only scales to orders of
magnitude larger graphs than previously possible with deep autoregressive graph
generative models, but also yields better graph generation quality.",arxiv
http://arxiv.org/abs/2005.05537v1,2020-05-12T03:46:15Z,2020-05-12T03:46:15Z,"GoGNN: Graph of Graphs Neural Network for Predicting Structured Entity
  Interactions","Entity interaction prediction is essential in many important applications
such as chemistry, biology, material science, and medical science. The problem
becomes quite challenging when each entity is represented by a complex
structure, namely structured entity, because two types of graphs are involved:
local graphs for structured entities and a global graph to capture the
interactions between structured entities. We observe that existing works on
structured entity interaction prediction cannot properly exploit the unique
graph of graphs model. In this paper, we propose a Graph of Graphs Neural
Network, namely GoGNN, which extracts the features in both structured entity
graphs and the entity interaction graph in a hierarchical way. We also propose
the dual-attention mechanism that enables the model to preserve the neighbor
importance in both levels of graphs. Extensive experiments on real-world
datasets show that GoGNN outperforms the state-of-the-art methods on two
representative structured entity interaction prediction tasks:
chemical-chemical interaction prediction and drug-drug interaction prediction.
Our code is available at Github.",arxiv
http://arxiv.org/abs/1810.12081v1,2018-10-29T13:03:38Z,2018-10-29T13:03:38Z,Learning to Teach with Dynamic Loss Functions,"Teaching is critical to human society: it is with teaching that prospective
students are educated and human civilization can be inherited and advanced. A
good teacher not only provides his/her students with qualified teaching
materials (e.g., textbooks), but also sets up appropriate learning objectives
(e.g., course projects and exams) considering different situations of a
student. When it comes to artificial intelligence, treating machine learning
models as students, the loss functions that are optimized act as perfect
counterparts of the learning objective set by the teacher. In this work, we
explore the possibility of imitating human teaching behaviors by dynamically
and automatically outputting appropriate loss functions to train machine
learning models. Different from typical learning settings in which the loss
function of a machine learning model is predefined and fixed, in our framework,
the loss function of a machine learning model (we call it student) is defined
by another machine learning model (we call it teacher). The ultimate goal of
teacher model is cultivating the student to have better performance measured on
development dataset. Towards that end, similar to human teaching, the teacher,
a parametric model, dynamically outputs different loss functions that will be
used and optimized by its student model at different training stages. We
develop an efficient learning method for the teacher model that makes gradient
based optimization possible, exempt of the ineffective solutions such as policy
optimization. We name our method as ""learning to teach with dynamic loss
functions"" (L2T-DLF for short). Extensive experiments on real world tasks
including image classification and neural machine translation demonstrate that
our method significantly improves the quality of various student models.",arxiv
http://arxiv.org/abs/1603.06212v1,2016-03-20T13:32:27Z,2016-03-20T13:32:27Z,"Evaluation of a Tree-based Pipeline Optimization Tool for Automating
  Data Science","As the field of data science continues to grow, there will be an
ever-increasing demand for tools that make machine learning accessible to
non-experts. In this paper, we introduce the concept of tree-based pipeline
optimization for automating one of the most tedious parts of machine
learning---pipeline design. We implement an open source Tree-based Pipeline
Optimization Tool (TPOT) in Python and demonstrate its effectiveness on a
series of simulated and real-world benchmark data sets. In particular, we show
that TPOT can design machine learning pipelines that provide a significant
improvement over a basic machine learning analysis while requiring little to no
input nor prior knowledge from the user. We also address the tendency for TPOT
to design overly complex pipelines by integrating Pareto optimization, which
produces compact pipelines without sacrificing classification accuracy. As
such, this work represents an important step toward fully automating machine
learning pipeline design.",arxiv
http://arxiv.org/abs/1910.09435v1,2019-10-21T15:12:32Z,2019-10-21T15:12:32Z,"Background Rejection in Atmospheric Cherenkov Telescopes using Recurrent
  Convolutional Neural Networks","In this work, we present a new, high performance algorithm for background
rejection in imaging atmospheric Cherenkov telescopes. We build on the already
popular machine-learning techniques used in gamma-ray astronomy by the
application of the latest techniques in machine learning, namely recurrent and
convolutional neural networks, to the background rejection problem. Use of
these machine-learning techniques addresses some of the key challenges
encountered in the currently implemented algorithms and helps to significantly
increase the background rejection performance at all energies.
  We apply these machine learning techniques to the H.E.S.S. telescope array,
first testing their performance on simulated data and then applying the
analysis to two well known gamma-ray sources. With real observational data we
find significantly improved performance over the current standard methods, with
a 20-25\% reduction in the background rate when applying the recurrent neural
network analysis. Importantly, we also find that the convolutional neural
network results are strongly dependent on the sky brightness in the source
region which has important implications for the future implementation of this
method in Cherenkov telescope analysis.",arxiv
http://arxiv.org/abs/1810.02688v2,2018-10-19T13:07:57Z,2018-09-28T08:27:59Z,Wikistat 2.0: Educational Resources for Artificial Intelligence,"Big data, data science, deep learning, artificial intelligence are the key
words of intense hype related with a job market in full evolution, that impose
to adapt the contents of our university professional trainings. Which
artificial intelligence is mostly concerned by the job offers? Which
methodologies and technologies should be favored in the training programs?
Which objectives, tools and educational resources do we needed to put in place
to meet these pressing needs? We answer these questions in describing the
contents and operational resources in the Data Science orientation of the
specialty Applied Mathematics at INSA Toulouse. We focus on basic mathematics
training (Optimization, Probability, Statistics), associated with the practical
implementation of the most performing statistical learning algorithms, with the
most appropriate technologies and on real examples. Considering the huge
volatility of the technologies, it is imperative to train students in
seft-training, this will be their technological watch tool when they will be in
professional activity. This explains the structuring of the educational site
github.com/wikistat into a set of tutorials. Finally, to motivate the thorough
practice of these tutorials, a serious game is organized each year in the form
of a prediction contest between students of Master degrees in Applied
Mathematics for IA.",arxiv
http://arxiv.org/abs/1709.01687v1,2017-09-06T06:42:22Z,2017-09-06T06:42:22Z,"Semi-Supervised Recurrent Neural Network for Adverse Drug Reaction
  Mention Extraction","Social media is an useful platform to share health-related information due to
its vast reach. This makes it a good candidate for public-health monitoring
tasks, specifically for pharmacovigilance. We study the problem of extraction
of Adverse-Drug-Reaction (ADR) mentions from social media, particularly from
twitter. Medical information extraction from social media is challenging,
mainly due to short and highly information nature of text, as compared to more
technical and formal medical reports.
  Current methods in ADR mention extraction relies on supervised learning
methods, which suffers from labeled data scarcity problem. The State-of-the-art
method uses deep neural networks, specifically a class of Recurrent Neural
Network (RNN) which are Long-Short-Term-Memory networks (LSTMs)
\cite{hochreiter1997long}. Deep neural networks, due to their large number of
free parameters relies heavily on large annotated corpora for learning the end
task. But in real-world, it is hard to get large labeled data, mainly due to
heavy cost associated with manual annotation. Towards this end, we propose a
novel semi-supervised learning based RNN model, which can leverage unlabeled
data also present in abundance on social media. Through experiments we
demonstrate the effectiveness of our method, achieving state-of-the-art
performance in ADR mention extraction.",arxiv
http://arxiv.org/abs/2004.02569v1,2020-04-06T11:32:37Z,2020-04-06T11:32:37Z,"Gradient-Based Training and Pruning of Radial Basis Function Networks
  with an Application in Materials Physics","Many applications, especially in physics and other sciences, call for easily
interpretable and robust machine learning techniques. We propose a fully
gradient-based technique for training radial basis function networks with an
efficient and scalable open-source implementation. We derive novel closed-form
optimization criteria for pruning the models for continuous as well as binary
data which arise in a challenging real-world material physics problem. The
pruned models are optimized to provide compact and interpretable versions of
larger models based on informed assumptions about the data distribution.
Visualizations of the pruned models provide insight into the atomic
configurations that determine atom-level migration processes in solid matter;
these results may inform future research on designing more suitable descriptors
for use with machine learning algorithms.",arxiv
http://arxiv.org/abs/2005.13073v1,2020-05-26T22:41:12Z,2020-05-26T22:41:12Z,"Biologically-informed neural networks guide mechanistic modeling from
  sparse experimental data","Biologically-informed neural networks (BINNs), an extension of
physics-informed neural networks [1], are introduced and used to discover the
underlying dynamics of biological systems from sparse experimental data. In the
present work, BINNs are trained in a supervised learning framework to
approximate in vitro cell biology assay experiments while respecting a
generalized form of the governing reaction-diffusion partial differential
equation (PDE). By allowing the diffusion and reaction terms to be multilayer
perceptrons (MLPs), the nonlinear forms of these terms can be learned while
simultaneously converging to the solution of the governing PDE. Further, the
trained MLPs are used to guide the selection of biologically interpretable
mechanistic forms of the PDE terms which provides new insights into the
biological and physical mechanisms that govern the dynamics of the observed
system. The method is evaluated on sparse real-world data from wound healing
assays with varying initial cell densities [2].",arxiv
http://arxiv.org/abs/2104.03961v1,2021-04-08T17:59:07Z,2021-04-08T17:59:07Z,Generalized Approach to Matched Filtering using Neural Networks,"Gravitational wave science is a pioneering field with rapidly evolving data
analysis methodology currently assimilating and inventing deep learning
techniques. The bulk of the sophisticated flagship searches of the field rely
on the time-tested matched filtering principle within their core. In this
paper, we make a key observation on the relationship between the emerging deep
learning and the traditional techniques: matched filtering is formally
equivalent to a particular neural network. This means that a neural network can
be constructed analytically to exactly implement matched filtering, and can be
further trained on data or boosted with additional complexity for improved
performance. This fundamental equivalence allows us to define a ""complexity
standard candle"" allowing us to characterize the relative complexity of the
different approaches to gravitational wave signals in a common framework.
Additionally it also provides a glimpse of an intriguing symmetry that could
provide clues on how neural networks approach the problem of finding signals in
overwhelming noise. Moreover, we show that the proposed neural network
architecture can outperform matched filtering, both with or without knowledge
of a prior on the parameter distribution. When a prior is given, the proposed
neural network can approach the statistically optimal performance. We also
propose and investigate two different neural network architectures MNet-Shallow
and MNet-Deep, both of which implement matched filtering at initialization and
can be trained on data. MNet-Shallow has simpler structure, while MNet-Deep is
more flexible and can deal with a wider range of distributions. Our theoretical
findings are corroborated by experiments using real LIGO data and synthetic
injections. Finally, our results suggest new perspectives on the role of deep
learning in gravitational wave detection.",arxiv
http://arxiv.org/abs/2108.11579v1,2021-08-26T05:00:27Z,2021-08-26T05:00:27Z,Modeling Item Response Theory with Stochastic Variational Inference,"Item Response Theory (IRT) is a ubiquitous model for understanding human
behaviors and attitudes based on their responses to questions. Large modern
datasets offer opportunities to capture more nuances in human behavior,
potentially improving psychometric modeling leading to improved scientific
understanding and public policy. However, while larger datasets allow for more
flexible approaches, many contemporary algorithms for fitting IRT models may
also have massive computational demands that forbid real-world application. To
address this bottleneck, we introduce a variational Bayesian inference
algorithm for IRT, and show that it is fast and scalable without sacrificing
accuracy. Applying this method to five large-scale item response datasets from
cognitive science and education yields higher log likelihoods and higher
accuracy in imputing missing data than alternative inference algorithms. Using
this new inference approach we then generalize IRT with expressive Bayesian
models of responses, leveraging recent advances in deep learning to capture
nonlinear item characteristic curves (ICC) with neural networks. Using an
eigth-grade mathematics test from TIMSS, we show our nonlinear IRT models can
capture interesting asymmetric ICCs. The algorithm implementation is
open-source, and easily usable.",arxiv
http://arxiv.org/abs/1601.07925v1,2016-01-28T21:45:55Z,2016-01-28T21:45:55Z,"Automating biomedical data science through tree-based pipeline
  optimization","Over the past decade, data science and machine learning has grown from a
mysterious art form to a staple tool across a variety of fields in academia,
business, and government. In this paper, we introduce the concept of tree-based
pipeline optimization for automating one of the most tedious parts of machine
learning---pipeline design. We implement a Tree-based Pipeline Optimization
Tool (TPOT) and demonstrate its effectiveness on a series of simulated and
real-world genetic data sets. In particular, we show that TPOT can build
machine learning pipelines that achieve competitive classification accuracy and
discover novel pipeline operators---such as synthetic feature
constructors---that significantly improve classification accuracy on these data
sets. We also highlight the current challenges to pipeline optimization, such
as the tendency to produce pipelines that overfit the data, and suggest future
research paths to overcome these challenges. As such, this work represents an
early step toward fully automating machine learning pipeline design.",arxiv
http://arxiv.org/abs/2007.03639v3,2021-01-11T11:02:34Z,2020-07-07T17:19:56Z,Human Trajectory Forecasting in Crowds: A Deep Learning Perspective,"Since the past few decades, human trajectory forecasting has been a field of
active research owing to its numerous real-world applications: evacuation
situation analysis, deployment of intelligent transport systems, traffic
operations, to name a few. Early works handcrafted this representation based on
domain knowledge. However, social interactions in crowded environments are not
only diverse but often subtle. Recently, deep learning methods have
outperformed their handcrafted counterparts, as they learned about human-human
interactions in a more generic data-driven fashion. In this work, we present an
in-depth analysis of existing deep learning-based methods for modelling social
interactions. We propose two knowledge-based data-driven methods to effectively
capture these social interactions. To objectively compare the performance of
these interaction-based forecasting models, we develop a large scale
interaction-centric benchmark TrajNet++, a significant yet missing component in
the field of human trajectory forecasting. We propose novel performance metrics
that evaluate the ability of a model to output socially acceptable
trajectories. Experiments on TrajNet++ validate the need for our proposed
metrics, and our method outperforms competitive baselines on both real-world
and synthetic datasets.",arxiv
http://arxiv.org/abs/1906.01974v3,2020-03-05T16:58:35Z,2019-06-03T22:43:00Z,"Willump: A Statistically-Aware End-to-end Optimizer for Machine Learning
  Inference","Systems for ML inference are widely deployed today, but they typically
optimize ML inference workloads using techniques designed for conventional data
serving workloads and miss critical opportunities to leverage the statistical
nature of ML. In this paper, we present Willump, an optimizer for ML inference
that introduces two statistically-motivated optimizations targeting ML
applications whose performance bottleneck is feature computation. First,
Willump automatically cascades feature computation for classification queries:
Willump classifies most data inputs using only high-value, low-cost features
selected through empirical observations of ML model performance, improving
query performance by up to 5x without statistically significant accuracy loss.
Second, Willump accurately approximates ML top-K queries, discarding
low-scoring inputs with an automatically constructed approximate model and then
ranking the remainder with a more powerful model, improving query performance
by up to 10x with minimal accuracy loss. Willump automatically tunes these
optimizations' parameters to maximize query performance while meeting an
accuracy target. Moreover, Willump complements these statistical optimizations
with compiler optimizations to automatically generate fast inference code for
ML applications. We show that Willump improves the end-to-end performance of
real-world ML inference pipelines curated from major data science competitions
by up to 16x without statistically significant loss of accuracy.",arxiv
http://arxiv.org/abs/1611.03313v1,2016-11-10T14:32:24Z,2016-11-10T14:32:24Z,X-ray Scattering Image Classification Using Deep Learning,"Visual inspection of x-ray scattering images is a powerful technique for
probing the physical structure of materials at the molecular scale. In this
paper, we explore the use of deep learning to develop methods for automatically
analyzing x-ray scattering images. In particular, we apply Convolutional Neural
Networks and Convolutional Autoencoders for x-ray scattering image
classification. To acquire enough training data for deep learning, we use
simulation software to generate synthetic x-ray scattering images. Experiments
show that deep learning methods outperform previously published methods by 10\%
on synthetic and real datasets.",arxiv
http://arxiv.org/abs/1702.01780v1,2017-02-06T20:10:10Z,2017-02-06T20:10:10Z,"Toward the automated analysis of complex diseases in genome-wide
  association studies using genetic programming","Machine learning has been gaining traction in recent years to meet the demand
for tools that can efficiently analyze and make sense of the ever-growing
databases of biomedical data in health care systems around the world. However,
effectively using machine learning methods requires considerable domain
expertise, which can be a barrier of entry for bioinformaticians new to
computational data science methods. Therefore, off-the-shelf tools that make
machine learning more accessible can prove invaluable for bioinformaticians. To
this end, we have developed an open source pipeline optimization tool
(TPOT-MDR) that uses genetic programming to automatically design machine
learning pipelines for bioinformatics studies. In TPOT-MDR, we implement
Multifactor Dimensionality Reduction (MDR) as a feature construction method for
modeling higher-order feature interactions, and combine it with a new expert
knowledge-guided feature selector for large biomedical data sets. We
demonstrate TPOT-MDR's capabilities using a combination of simulated and real
world data sets from human genetics and find that TPOT-MDR significantly
outperforms modern machine learning methods such as logistic regression and
eXtreme Gradient Boosting (XGBoost). We further analyze the best pipeline
discovered by TPOT-MDR for a real world problem and highlight TPOT-MDR's
ability to produce a high-accuracy solution that is also easily interpretable.",arxiv
http://arxiv.org/abs/2101.05960v1,2021-01-15T04:06:25Z,2021-01-15T04:06:25Z,"DeepWaste: Applying Deep Learning to Waste Classification for a
  Sustainable Planet","Accurate waste disposal, at the point of disposal, is crucial to fighting
climate change. When materials that could be recycled or composted get diverted
into landfills, they cause the emission of potent greenhouse gases such as
methane. Current attempts to reduce erroneous waste disposal are expensive,
inaccurate, and confusing. In this work, we propose DeepWaste, an easy-to-use
mobile app, that utilizes highly optimized deep learning techniques to provide
users instantaneous waste classification into trash, recycling, and compost. We
experiment with several convolution neural network architectures to detect and
classify waste items. Our best model, a deep learning residual neural network
with 50 layers, achieves an average precision of 0.881 on the test set. We
demonstrate the performance and efficiency of our app on a set of real-world
images.",arxiv
http://arxiv.org/abs/2010.09810v1,2020-10-19T19:40:29Z,2020-10-19T19:40:29Z,"Connections between Relational Event Model and Inverse Reinforcement
  Learning for Characterizing Group Interaction Sequences","In this paper we explore previously unidentified connections between
relational event model (REM) from the field of network science and inverse
reinforcement learning (IRL) from the field of machine learning with respect to
their ability to characterize sequences of directed social interaction events
in group settings. REM is a conventional approach to tackle such a problem
whereas the application of IRL is a largely unbeaten path. We begin by
examining the mathematical components of both REM and IRL and find
straightforward analogies between the two methods as well as unique
characteristics of the IRL approach. We demonstrate the special utility of IRL
in characterizing group social interactions with an empirical experiment, in
which we use IRL to infer individual behavioral preferences based on a sequence
of directed communication events from a group of virtual-reality game players
interacting and cooperating to accomplish a shared goal. Our comparison and
experiment introduce fresh perspectives for social behavior analytics and help
inspire new research opportunities at the nexus of social network analysis and
machine learning.",arxiv
http://arxiv.org/abs/2012.10489v2,2021-02-24T04:38:47Z,2020-12-18T19:54:19Z,"XAI4Wind: A Multimodal Knowledge Graph Database for Explainable Decision
  Support in Operations & Maintenance of Wind Turbines","Condition-based monitoring (CBM) has been widely utilised in the wind
industry for monitoring operational inconsistencies and failures in turbines,
with techniques ranging from signal processing and vibration analysis to
artificial intelligence (AI) models using Supervisory Control & Acquisition
(SCADA) data. However, existing studies do not present a concrete basis to
facilitate explainable decision support in operations and maintenance (O&M),
particularly for automated decision support through recommendation of
appropriate maintenance action reports corresponding to failures predicted by
CBM techniques. Knowledge graph databases (KGs) model a collection of
domain-specific information and have played an intrinsic role for real-world
decision support in domains such as healthcare and finance, but have seen very
limited attention in the wind industry. We propose XAI4Wind, a multimodal
knowledge graph for explainable decision support in real-world operational
turbines and demonstrate through experiments several use-cases of the proposed
KG towards O&M planning through interactive query and reasoning and providing
novel insights using graph data science algorithms. The proposed KG combines
multimodal knowledge like SCADA parameters and alarms with natural language
maintenance actions, images etc. By integrating our KG with an Explainable AI
model for anomaly prediction, we show that it can provide effective
human-intelligible O&M strategies for predicted operational inconsistencies in
various turbine sub-components. This can help instil better trust and
confidence in conventionally black-box AI models. We make our KG publicly
available and envisage that it can serve as the building ground for providing
autonomous decision support in the wind industry.",arxiv
http://arxiv.org/abs/2011.05373v1,2020-11-10T20:06:19Z,2020-11-10T20:06:19Z,"Emergent Reciprocity and Team Formation from Randomized Uncertain Social
  Preferences","Multi-agent reinforcement learning (MARL) has shown recent success in
increasingly complex fixed-team zero-sum environments. However, the real world
is not zero-sum nor does it have fixed teams; humans face numerous social
dilemmas and must learn when to cooperate and when to compete. To successfully
deploy agents into the human world, it may be important that they be able to
understand and help in our conflicts. Unfortunately, selfish MARL agents
typically fail when faced with social dilemmas. In this work, we show evidence
of emergent direct reciprocity, indirect reciprocity and reputation, and team
formation when training agents with randomized uncertain social preferences
(RUSP), a novel environment augmentation that expands the distribution of
environments agents play in. RUSP is generic and scalable; it can be applied to
any multi-agent environment without changing the original underlying game
dynamics or objectives. In particular, we show that with RUSP these behaviors
can emerge and lead to higher social welfare equilibria in both classic
abstract social dilemmas like Iterated Prisoner's Dilemma as well in more
complex intertemporal environments.",arxiv
http://arxiv.org/abs/1910.12861v1,2019-10-26T11:50:27Z,2019-10-26T11:50:27Z,Deep Learning for Hyperspectral Image Classification: An Overview,"Hyperspectral image (HSI) classification has become a hot topic in the field
of remote sensing. In general, the complex characteristics of hyperspectral
data make the accurate classification of such data challenging for traditional
machine learning methods. In addition, hyperspectral imaging often deals with
an inherently nonlinear relation between the captured spectral information and
the corresponding materials. In recent years, deep learning has been recognized
as a powerful feature-extraction tool to effectively address nonlinear problems
and widely used in a number of image processing tasks. Motivated by those
successful applications, deep learning has also been introduced to classify
HSIs and demonstrated good performance. This survey paper presents a systematic
review of deep learning-based HSI classification literatures and compares
several strategies for this topic. Specifically, we first summarize the main
challenges of HSI classification which cannot be effectively overcome by
traditional machine learning methods, and also introduce the advantages of deep
learning to handle these problems. Then, we build a framework which divides the
corresponding works into spectral-feature networks, spatial-feature networks,
and spectral-spatial-feature networks to systematically review the recent
achievements in deep learning-based HSI classification. In addition,
considering the fact that available training samples in the remote sensing
field are usually very limited and training deep networks require a large
number of samples, we include some strategies to improve classification
performance, which can provide some guidelines for future studies on this
topic. Finally, several representative deep learning-based classification
methods are conducted on real HSIs in our experiments.",arxiv
http://arxiv.org/abs/1908.04387v3,2019-09-10T17:46:09Z,2019-08-05T02:59:18Z,"Mass Estimation from Images using Deep Neural Network and Sparse Ground
  Truth","Supervised learning is the workhorse for regression and classification tasks,
but the standard approach presumes ground truth for every measurement. In real
world applications, limitations due to expense or general in-feasibility due to
the specific application are common. In the context of agriculture
applications, yield monitoring is one such example where simple-physics based
measurements such as volume or force-impact have been used to quantify mass
flow, which incur error due to sensor calibration. By utilizing semi-supervised
deep learning with gradient aggregation and a sequence of images, in this work
we can accurately estimate a physical quantity (mass) with complex data
structures and sparse ground truth. Using a vision system capturing images of a
sugarcane elevator and running bamboo under controlled testing as a surrogate
material to harvesting sugarcane, mass is accurately predicted from images by
training a DNN using only final load weights. The DNN succeeds in capturing the
complex density physics of random stacking of slender rods internally as part
of the mass prediction model, and surpasses older volumetric-based methods for
mass prediction. Furthermore, by incorporating knowledge about the system
physics through the DNN architecture and penalty terms, improvements in
prediction accuracy and stability, as well as faster learning are obtained. It
is shown that the classic nonlinear regression optimization can be reformulated
with an aggregation term with some independence assumptions to achieve this
feat. Since the number of images for any given run are too large to fit on
typical GPU vRAM, an implementation is shown that compensates for the limited
memory but still achieve fast training times. The same approach presented
herein could be applied to other applications like yield monitoring on grain
combines or other harvesters using vision or other instrumentation.",arxiv
http://arxiv.org/abs/1911.09281v1,2019-11-21T04:19:16Z,2019-11-21T04:19:16Z,"Event Detection in Noisy Streaming Data with Combination of
  Corroborative and Probabilistic Sources","Global physical event detection has traditionally relied on dense coverage of
physical sensors around the world; while this is an expensive undertaking,
there have not been alternatives until recently. The ubiquity of social
networks and human sensors in the field provides a tremendous amount of
real-time, live data about true physical events from around the world. However,
while such human sensor data have been exploited for retrospective large-scale
event detection, such as hurricanes or earthquakes, they has been limited to no
success in exploiting this rich resource for general physical event detection.
  Prior implementation approaches have suffered from the concept drift
phenomenon, where real-world data exhibits constant, unknown, unbounded changes
in its data distribution, making static machine learning models ineffective in
the long term. We propose and implement an end-to-end collaborative drift
adaptive system that integrates corroborative and probabilistic sources to
deliver real-time predictions. Furthermore, out system is adaptive to concept
drift and performs automated continuous learning to maintain high performance.
We demonstrate our approach in a real-time demo available online for landslide
disaster detection, with extensibility to other real-world physical events such
as flooding, wildfires, hurricanes, and earthquakes.",arxiv
http://arxiv.org/abs/2012.02298v2,2021-06-15T06:28:13Z,2020-11-25T17:23:52Z,"Exploration in Online Advertising Systems with Deep Uncertainty-Aware
  Learning","Modern online advertising systems inevitably rely on personalization methods,
such as click-through rate (CTR) prediction. Recent progress in CTR prediction
enjoys the rich representation capabilities of deep learning and achieves great
success in large-scale industrial applications. However, these methods can
suffer from lack of exploration. Another line of prior work addresses the
exploration-exploitation trade-off problem with contextual bandit methods,
which are recently less studied in the industry due to the difficulty in
extending their flexibility with deep models. In this paper, we propose a novel
Deep Uncertainty-Aware Learning (DUAL) method to learn CTR models based on
Gaussian processes, which can provide predictive uncertainty estimations while
maintaining the flexibility of deep neural networks. DUAL can be easily
implemented on existing models and deployed in real-time systems with minimal
extra computational overhead. By linking the predictive uncertainty estimation
ability of DUAL to well-known bandit algorithms, we further present DUAL-based
Ad-ranking strategies to boost up long-term utilities such as the social
welfare in advertising systems. Experimental results on several public datasets
demonstrate the effectiveness of our methods. Remarkably, an online A/B test
deployed in the Alibaba display advertising platform shows an 8.2% social
welfare improvement and an 8.0% revenue lift.",arxiv
http://arxiv.org/abs/2012.07938v1,2020-12-14T20:55:48Z,2020-12-14T20:55:48Z,NVIDIA SimNet^{TM}: an AI-accelerated multi-physics simulation framework,"We present SimNet, an AI-driven multi-physics simulation framework, to
accelerate simulations across a wide range of disciplines in science and
engineering. Compared to traditional numerical solvers, SimNet addresses a wide
range of use cases - coupled forward simulations without any training data,
inverse and data assimilation problems. SimNet offers fast turnaround time by
enabling parameterized system representation that solves for multiple
configurations simultaneously, as opposed to the traditional solvers that solve
for one configuration at a time. SimNet is integrated with parameterized
constructive solid geometry as well as STL modules to generate point clouds.
Furthermore, it is customizable with APIs that enable user extensions to
geometry, physics and network architecture. It has advanced network
architectures that are optimized for high-performance GPU computing, and offers
scalable performance for multi-GPU and multi-Node implementation with
accelerated linear algebra as well as FP32, FP64 and TF32 computations. In this
paper we review the neural network solver methodology, the SimNet architecture,
and the various features that are needed for effective solution of the PDEs. We
present real-world use cases that range from challenging forward multi-physics
simulations with turbulence and complex 3D geometries, to industrial design
optimization and inverse problems that are not addressed efficiently by the
traditional solvers. Extensive comparisons of SimNet results with open source
and commercial solvers show good correlation.",arxiv
http://arxiv.org/abs/1911.06636v2,2020-06-16T09:13:58Z,2019-11-15T13:57:35Z,"Catch & Carry: Reusable Neural Controllers for Vision-Guided Whole-Body
  Tasks","We address the longstanding challenge of producing flexible, realistic
humanoid character controllers that can perform diverse whole-body tasks
involving object interactions. This challenge is central to a variety of
fields, from graphics and animation to robotics and motor neuroscience. Our
physics-based environment uses realistic actuation and first-person perception
-- including touch sensors and egocentric vision -- with a view to producing
active-sensing behaviors (e.g. gaze direction), transferability to real robots,
and comparisons to the biology. We develop an integrated neural-network based
approach consisting of a motor primitive module, human demonstrations, and an
instructed reinforcement learning regime with curricula and task variations. We
demonstrate the utility of our approach for several tasks, including
goal-conditioned box carrying and ball catching, and we characterize its
behavioral robustness. The resulting controllers can be deployed in real-time
on a standard PC. See overview video, https://youtu.be/2rQAW-8gQQk .",arxiv
http://arxiv.org/abs/2010.08600v2,2020-11-16T06:26:16Z,2020-10-16T19:40:08Z,"Robot Navigation in Constrained Pedestrian Environments using
  Reinforcement Learning","Navigating fluently around pedestrians is a necessary capability for mobile
robots deployed in human environments, such as buildings and homes. While
research on social navigation has focused mainly on the scalability with the
number of pedestrians in open spaces, typical indoor environments present the
additional challenge of constrained spaces such as corridors and doorways that
limit maneuverability and influence patterns of pedestrian interaction. We
present an approach based on reinforcement learning (RL) to learn policies
capable of dynamic adaptation to the presence of moving pedestrians while
navigating between desired locations in constrained environments. The policy
network receives guidance from a motion planner that provides waypoints to
follow a globally planned trajectory, whereas RL handles the local
interactions. We explore a compositional principle for multi-layout training
and find that policies trained in a small set of geometrically simple layouts
successfully generalize to more complex unseen layouts that exhibit composition
of the structural elements available during training. Going beyond walls-world
like domains, we show transfer of the learned policy to unseen 3D
reconstructions of two real environments. These results support the
applicability of the compositional principle to navigation in real-world
buildings and indicate promising usage of multi-agent simulation within
reconstructed environments for tasks that involve interaction.",arxiv
http://arxiv.org/abs/2102.10635v2,2021-06-28T16:23:46Z,2021-02-21T16:15:40Z,"AI-Augmented Behavior Analysis for Children with Developmental
  Disabilities: Building Towards Precision Treatment","Autism spectrum disorder is a developmental disorder characterized by
significant social, communication, and behavioral challenges. Individuals
diagnosed with autism, intellectual, and developmental disabilities (AUIDD)
typically require long-term care and targeted treatment and teaching. Effective
treatment of AUIDD relies on efficient and careful behavioral observations done
by trained applied behavioral analysts (ABAs). However, this process
overburdens ABAs by requiring the clinicians to collect and analyze data,
identify the problem behaviors, conduct pattern analysis to categorize and
predict categorical outcomes, hypothesize responsiveness to treatments, and
detect the effects of treatment plans. Successful integration of digital
technologies into clinical decision-making pipelines and the advancements in
automated decision-making using Artificial Intelligence (AI) algorithms
highlights the importance of augmenting teaching and treatments using novel
algorithms and high-fidelity sensors. In this article, we present an
AI-Augmented Learning and Applied Behavior Analytics (AI-ABA) platform to
provide personalized treatment and learning plans to AUIDD individuals. By
defining systematic experiments along with automated data collection and
analysis, AI-ABA can promote self-regulative behavior using reinforcement-based
augmented or virtual reality and other mobile platforms. Thus, AI-ABA could
assist clinicians to focus on making precise data-driven decisions and increase
the quality of individualized interventions for individuals with AUIDD.",arxiv
http://arxiv.org/abs/2001.01861v2,2020-07-30T16:58:22Z,2020-01-07T02:39:02Z,Vamsa: Automated Provenance Tracking in Data Science Scripts,"There has recently been a lot of ongoing research in the areas of fairness,
bias and explainability of machine learning (ML) models due to the self-evident
or regulatory requirements of various ML applications. We make the following
observation: All of these approaches require a robust understanding of the
relationship between ML models and the data used to train them. In this work,
we introduce the ML provenance tracking problem: the fundamental idea is to
automatically track which columns in a dataset have been used to derive the
features/labels of an ML model. We discuss the challenges in capturing such
information in the context of Python, the most common language used by data
scientists. We then present Vamsa, a modular system that extracts provenance
from Python scripts without requiring any changes to the users' code. Using 26K
real data science scripts, we verify the effectiveness of Vamsa in terms of
coverage, and performance. We also evaluate Vamsa's accuracy on a smaller
subset of manually labeled data. Our analysis shows that Vamsa's precision and
recall range from 90.4% to 99.1% and its latency is in the order of
milliseconds for average size scripts. Drawing from our experience in deploying
ML models in production, we also present an example in which Vamsa helps
automatically identify models that are affected by data corruption issues.",arxiv
http://arxiv.org/abs/1907.07958v1,2019-07-18T09:58:27Z,2019-07-18T09:58:27Z,Transfer Learning Across Simulated Robots With Different Sensors,"For a robot to learn a good policy, it often requires expensive equipment
(such as sophisticated sensors) and a prepared training environment conducive
to learning. However, it is seldom possible to perfectly equip robots for
economic reasons, nor to guarantee ideal learning conditions, when deployed in
real-life environments. A solution would be to prepare the robot in the lab
environment, when all necessary material is available to learn a good policy.
After training in the lab, the robot should be able to get by without the
expensive equipment that used to be available to it, and yet still be
guaranteed to perform well on the field. The transition between the lab
(source) and the real-world environment (target) is related to transfer
learning, where the state-space between the source and target tasks differ. We
tackle a simulated task with continuous states and discrete actions presenting
this challenge, using Bootstrapped Dual Policy Iteration, a model-free
actor-critic reinforcement learning algorithm, and Policy Shaping.
Specifically, we train a BDPI agent, embodied by a virtual robot performing a
task in the V-Rep simulator, sensing its environment through several proximity
sensors. The resulting policy is then used by a second agent learning the same
task in the same environment, but with camera images as input. The goal is to
obtain a policy able to perform the task relying on merely camera images.",arxiv
http://arxiv.org/abs/2110.06196v1,2021-10-12T17:49:46Z,2021-10-12T17:49:46Z,GraPE: fast and scalable Graph Processing and Embedding,"Graph Representation Learning methods have enabled a wide range of learning
problems to be addressed for data that can be represented in graph form.
Nevertheless, several real world problems in economy, biology, medicine and
other fields raised relevant scaling problems with existing methods and their
software implementation, due to the size of real world graphs characterized by
millions of nodes and billions of edges. We present GraPE, a software resource
for graph processing and random walk based embedding, that can scale with large
and high-degree graphs and significantly speed up-computation. GraPE comprises
specialized data structures, algorithms, and a fast parallel implementation
that displays everal orders of magnitude improvement in empirical space and
time complexity compared to state of the art software resources, with a
corresponding boost in the performance of machine learning methods for edge and
node label prediction and for the unsupervised analysis of graphs.GraPE is
designed to run on laptop and desktop computers, as well as on high performance
computing clusters",arxiv
http://arxiv.org/abs/2104.04148v2,2021-04-12T03:06:05Z,2021-04-09T01:54:58Z,"Individual Explanations in Machine Learning Models: A Case Study on
  Poverty Estimation","Machine learning methods are being increasingly applied in sensitive societal
contexts, where decisions impact human lives. Hence it has become necessary to
build capabilities for providing easily-interpretable explanations of models'
predictions. Recently in academic literature, a vast number of explanations
methods have been proposed. Unfortunately, to our knowledge, little has been
documented about the challenges machine learning practitioners most often face
when applying them in real-world scenarios. For example, a typical procedure
such as feature engineering can make some methodologies no longer applicable.
The present case study has two main objectives. First, to expose these
challenges and how they affect the use of relevant and novel explanations
methods. And second, to present a set of strategies that mitigate such
challenges, as faced when implementing explanation methods in a relevant
application domain -- poverty estimation and its use for prioritizing access to
social policies.",arxiv
http://arxiv.org/abs/2102.10398v3,2021-02-28T00:47:42Z,2021-02-20T17:35:23Z,All-Chalcogenide Programmable All-Optical Deep Neural Networks,"Deeplearning algorithms are revolutionising many aspects of modern life.
Typically, they are implemented in CMOS-based hardware with severely limited
memory access times and inefficient data-routing. All-optical neural networks
without any electro-optic conversions could alleviate these shortcomings.
However, an all-optical nonlinear activation function, which is a vital
building block for optical neural networks, needs to be developed efficiently
on-chip. Here, we introduce and demonstrate both optical synapse weighting and
all-optical nonlinear thresholding using two different effects in a
chalcogenide material photonic platform. We show how the structural phase
transitions in a wide-bandgap phase-change material enables storing the neural
network weights via non-volatile photonic memory, whilst resonant bond
destabilisation is used as a nonlinear activation threshold without changing
the material. These two different transitions within chalcogenides enable
programmable neural networks with near-zero static power consumption once
trained, in addition to picosecond delays performing inference tasks not
limited by wire charging that limit electrical circuits; for instance, we show
that nanosecond-order weight programming and near-instantaneous weight updates
enable accurate inference tasks within 20 picoseconds in a 3-layer all-optical
neural network. Optical neural networks that bypass electro-optic conversion
altogether hold promise for network-edge machine learning applications where
decision-making in real-time are critical, such as for autonomous vehicles or
navigation systems such as signal pre-processing of LIDAR systems.",arxiv
http://arxiv.org/abs/1810.02080v1,2018-10-04T07:39:31Z,2018-10-04T07:39:31Z,Dual Convolutional Neural Network for Graph of Graphs Link Prediction,"Graphs are general and powerful data representations which can model complex
real-world phenomena, ranging from chemical compounds to social networks;
however, effective feature extraction from graphs is not a trivial task, and
much work has been done in the field of machine learning and data mining. The
recent advances in graph neural networks have made automatic and flexible
feature extraction from graphs possible and have improved the predictive
performance significantly. In this paper, we go further with this line of
research and address a more general problem of learning with a graph of graphs
(GoG) consisting of an external graph and internal graphs, where each node in
the external graph has an internal graph structure. We propose a dual
convolutional neural network that extracts node representations by combining
the external and internal graph structures in an end-to-end manner. Experiments
on link prediction tasks using several chemical network datasets demonstrate
the effectiveness of the proposed method.",arxiv
http://arxiv.org/abs/2108.07903v1,2021-08-17T23:03:55Z,2021-08-17T23:03:55Z,"Spatially and color consistent environment lighting estimation using
  deep neural networks for mixed reality","The representation of consistent mixed reality (XR) environments requires
adequate real and virtual illumination composition in real-time. Estimating the
lighting of a real scenario is still a challenge. Due to the ill-posed nature
of the problem, classical inverse-rendering techniques tackle the problem for
simple lighting setups. However, those assumptions do not satisfy the current
state-of-art in computer graphics and XR applications. While many recent works
solve the problem using machine learning techniques to estimate the environment
light and scene's materials, most of them are limited to geometry or previous
knowledge. This paper presents a CNN-based model to estimate complex lighting
for mixed reality environments with no previous information about the scene. We
model the environment illumination using a set of spherical harmonics (SH)
environment lighting, capable of efficiently represent area lighting. We
propose a new CNN architecture that inputs an RGB image and recognizes, in
real-time, the environment lighting. Unlike previous CNN-based lighting
estimation methods, we propose using a highly optimized deep neural network
architecture, with a reduced number of parameters, that can learn high complex
lighting scenarios from real-world high-dynamic-range (HDR) environment images.
We show in the experiments that the CNN architecture can predict the
environment lighting with an average mean squared error (MSE) of \num{7.85e-04}
when comparing SH lighting coefficients. We validate our model in a variety of
mixed reality scenarios. Furthermore, we present qualitative results comparing
relights of real-world scenes.",arxiv
http://arxiv.org/abs/2004.11838v1,2020-04-14T19:36:11Z,2020-04-14T19:36:11Z,"Analysis of Social Media Data using Multimodal Deep Learning for
  Disaster Response","Multimedia content in social media platforms provides significant information
during disaster events. The types of information shared include reports of
injured or deceased people, infrastructure damage, and missing or found people,
among others. Although many studies have shown the usefulness of both text and
image content for disaster response purposes, the research has been mostly
focused on analyzing only the text modality in the past. In this paper, we
propose to use both text and image modalities of social media data to learn a
joint representation using state-of-the-art deep learning techniques.
Specifically, we utilize convolutional neural networks to define a multimodal
deep learning architecture with a modality-agnostic shared representation.
Extensive experiments on real-world disaster datasets show that the proposed
multimodal architecture yields better performance than models trained using a
single modality (e.g., either text or image).",arxiv
http://arxiv.org/abs/1906.04450v2,2019-08-14T17:52:37Z,2019-06-11T09:02:35Z,"Quantifying Intrinsic Uncertainty in Classification via Deep Dirichlet
  Mixture Networks","With the widespread success of deep neural networks in science and
technology, it is becoming increasingly important to quantify the uncertainty
of the predictions produced by deep learning. In this paper, we introduce a new
method that attaches an explicit uncertainty statement to the probabilities of
classification using deep neural networks. Precisely, we view that the
classification probabilities are sampled from an unknown distribution, and we
propose to learn this distribution through the Dirichlet mixture that is
flexible enough for approximating any continuous distribution on the simplex.
We then construct credible intervals from the learned distribution to assess
the uncertainty of the classification probabilities. Our approach is easy to
implement, computationally efficient, and can be coupled with any deep neural
network architecture. Our method leverages the crucial observation that, in
many classification applications such as medical diagnosis, more than one class
labels are available for each observational unit. We demonstrate the usefulness
of our approach through simulations and a real data example.",arxiv
http://arxiv.org/abs/1808.04511v1,2018-08-14T03:00:29Z,2018-08-14T03:00:29Z,A Record Linkage Model Incorporating Relational Data,"In this paper we introduce a novel Bayesian approach for linking multiple
social networks in order to discover the same real world person having
different accounts across networks. In particular, we develop a latent model
that allow us to jointly characterize the network and linkage structures
relying in both relational and profile data. In contrast to other existing
approaches in the machine learning literature, our Bayesian implementation
naturally provides uncertainty quantification via posterior probabilities for
the linkage structure itself or any function of it. Our findings clearly
suggest that our methodology can produce accurate point estimates of the
linkage structure even in the absence of profile information, and also, in an
identity resolution setting, our results confirm that including relational data
into the matching process improves the linkage accuracy. We illustrate our
methodology using real data from popular social networks such as Twitter,
Facebook, and YouTube.",arxiv
http://arxiv.org/abs/2102.10997v1,2021-02-03T10:52:02Z,2021-02-03T10:52:02Z,"Trust Computational Heuristic for Social Internet of Things: A Machine
  Learning-based Approach","The Internet of Things (IoT) is an evolving network of billions of
interconnected physical objects, such as numerous sensors, smartphones,
wearables, and embedded devices. These physical objects, generally referred to
as the smart objects, when deployed in the real-world aggregates useful
information from their surrounding environment. As-of-late, this notion of IoT
has been extended to incorporate the social networking facets which have led to
the promising paradigm of the `Social Internet of Things' (SIoT). In SIoT, the
devices operate as an autonomous agent and provide an exchange of information
and service discovery in an intelligent manner by establishing social
relationships among them with respect to their owners. Trust plays an important
role in establishing trustworthy relationships among the physical objects and
reduces probable risks in the decision-making process. In this paper, a trust
computational model is proposed to extract individual trust features in a SIoT
environment. Furthermore, a machine learning-based heuristic is used to
aggregate all the trust features in order to ascertain an aggregate trust
score. Simulation results illustrate that the proposed trust-based model
isolates the trustworthy and untrustworthy nodes within the network in an
efficient manner.",arxiv
http://arxiv.org/abs/1707.04826v1,2017-07-16T05:58:40Z,2017-07-16T05:58:40Z,Machine learning application in the life time of materials,"Materials design and development typically takes several decades from the
initial discovery to commercialization with the traditional trial and error
development approach. With the accumulation of data from both experimental and
computational results, data based machine learning becomes an emerging field in
materials discovery, design and property prediction. This manuscript reviews
the history of materials science as a disciplinary the most common machine
learning method used in materials science, and specifically how they are used
in materials discovery, design, synthesis and even failure detection and
analysis after materials are deployed in real application. Finally, the
limitations of machine learning for application in materials science and
challenges in this emerging field is discussed.",arxiv
http://arxiv.org/abs/1801.07413v1,2018-01-23T07:15:15Z,2018-01-23T07:15:15Z,"Greed is Still Good: Maximizing Monotone Submodular+Supermodular
  Functions","We analyze the performance of the greedy algorithm, and also a discrete
semi-gradient based algorithm, for maximizing the sum of a suBmodular and
suPermodular (BP) function (both of which are non-negative monotone
non-decreasing) under two types of constraints, either a cardinality constraint
or $p\geq 1$ matroid independence constraints. These problems occur naturally
in several real-world applications in data science, machine learning, and
artificial intelligence. The problems are ordinarily inapproximable to any
factor (as we show). Using the curvature $\kappa_f$ of the submodular term, and
introducing $\kappa^g$ for the supermodular term (a natural dual curvature for
supermodular functions), however, both of which are computable in linear time,
we show that BP maximization can be efficiently approximated by both the greedy
and the semi-gradient based algorithm. The algorithms yield multiplicative
guarantees of $\frac{1}{\kappa_f}\left[1-e^{-(1-\kappa^g)\kappa_f}\right]$ and
$\frac{1-\kappa^g}{(1-\kappa^g)\kappa_f + p}$ for the two types of constraints
respectively. For pure monotone supermodular constrained maximization, these
yield $1-\kappa^g$ and $(1-\kappa^g)/p$ for the two types of constraints
respectively. We also analyze the hardness of BP maximization and show that our
guarantees match hardness by a constant factor and by $O(\ln(p))$ respectively.
Computational experiments are also provided supporting our analysis.",arxiv
http://arxiv.org/abs/2005.08337v1,2020-05-17T18:46:23Z,2020-05-17T18:46:23Z,A Survey on Unknown Presentation Attack Detection for Fingerprint,"Fingerprint recognition systems are widely deployed in various real-life
applications as they have achieved high accuracy. The widely used applications
include border control, automated teller machine (ATM), and attendance
monitoring systems. However, these critical systems are prone to spoofing
attacks (a.k.a presentation attacks (PA)). PA for fingerprint can be performed
by presenting gummy fingers made from different materials such as silicone,
gelatine, play-doh, ecoflex, 2D printed paper, 3D printed material, or latex.
Biometrics Researchers have developed Presentation Attack Detection (PAD)
methods as a countermeasure to PA. PAD is usually done by training a machine
learning classifier for known attacks for a given dataset, and they achieve
high accuracy in this task. However, generalizing to unknown attacks is an
essential problem from applicability to real-world systems, mainly because
attacks cannot be exhaustively listed in advance. In this survey paper, we
present a comprehensive survey on existing PAD algorithms for fingerprint
recognition systems, specifically from the standpoint of detecting unknown PAD.
We categorize PAD algorithms, point out their advantages/disadvantages, and
future directions for this area.",arxiv
http://arxiv.org/abs/1811.08270v2,2019-02-25T06:50:27Z,2018-11-11T14:15:23Z,Graph Convolutional Neural Networks via Motif-based Attention,"Many real-world problems can be represented as graph-based learning problems.
In this paper, we propose a novel framework for learning spatial and
attentional convolution neural networks on arbitrary graphs. Different from
previous convolutional neural networks on graphs, we first design a
motif-matching guided subgraph normalization method to capture neighborhood
information. Then we implement subgraph-level self-attentional layers to learn
different importances from different subgraphs to solve graph classification
problems. Analogous to image-based attentional convolution networks that
operate on locally connected and weighted regions of the input, we also extend
graph normalization from one-dimensional node sequence to two-dimensional node
grid by leveraging motif-matching, and design self-attentional layers without
requiring any kinds of cost depending on prior knowledge of the graph
structure. Our results on both bioinformatics and social network datasets show
that we can significantly improve graph classification benchmarks over
traditional graph kernel and existing deep models.",arxiv
http://arxiv.org/abs/1906.08230v1,2019-06-19T17:19:31Z,2019-06-19T17:19:31Z,Evaluating Protein Transfer Learning with TAPE,"Protein modeling is an increasingly popular area of machine learning
research. Semi-supervised learning has emerged as an important paradigm in
protein modeling due to the high cost of acquiring supervised protein labels,
but the current literature is fragmented when it comes to datasets and
standardized evaluation techniques. To facilitate progress in this field, we
introduce the Tasks Assessing Protein Embeddings (TAPE), a set of five
biologically relevant semi-supervised learning tasks spread across different
domains of protein biology. We curate tasks into specific training, validation,
and test splits to ensure that each task tests biologically relevant
generalization that transfers to real-life scenarios. We benchmark a range of
approaches to semi-supervised protein representation learning, which span
recent work as well as canonical sequence learning techniques. We find that
self-supervised pretraining is helpful for almost all models on all tasks, more
than doubling performance in some cases. Despite this increase, in several
cases features learned by self-supervised pretraining still lag behind features
extracted by state-of-the-art non-neural techniques. This gap in performance
suggests a huge opportunity for innovative architecture design and improved
modeling paradigms that better capture the signal in biological sequences. TAPE
will help the machine learning community focus effort on scientifically
relevant problems. Toward this end, all data and code used to run these
experiments are available at https://github.com/songlab-cal/tape.",arxiv
http://arxiv.org/abs/1809.07763v4,2020-05-26T15:15:19Z,2018-09-19T19:14:46Z,"auditor: an R Package for Model-Agnostic Visual Validation and
  Diagnostics","Machine learning models have spread to almost every area of life. They are
successfully applied in biology, medicine, finance, physics, and other fields.
With modern software it is easy to train even a~complex model that fits the
training data and results in high accuracy on the test set. The problem arises
when models fail confronted with real-world data.
  This paper describes methodology and tools for model-agnostic audit.
Introduced techniques facilitate assessing and comparing the goodness of fit
and performance of models. In~addition, they may be used for the analysis of
the similarity of residuals and for identification of~outliers and influential
observations. The examination is carried out by diagnostic scores and visual
verification.
  Presented methods were implemented in the auditor package for R. Due to
flexible and~consistent grammar, it is simple to validate models of any
classes.",arxiv
http://arxiv.org/abs/2002.04716v1,2020-02-11T22:18:28Z,2020-02-11T22:18:28Z,"Robust multi-scale multi-feature deep learning for atomic and defect
  identification in Scanning Tunneling Microscopy on H-Si(100) 2x1 surface","The nature of the atomic defects on the hydrogen passivated Si (100) surface
is analyzed using deep learning and scanning tunneling microscopy (STM). A
robust deep learning framework capable of identifying atomic species, defects,
in the presence of non-resolved contaminates, step edges, and noise is
developed. The automated workflow, based on the combination of several networks
for image assessment, atom-finding and defect finding, is developed to perform
the analysis at different levels of description and is deployed on an
operational STM platform. This is further extended to unsupervised
classification of the extracted defects using the mean-shift clustering
algorithm, which utilizes features automatically engineered from the combined
output of neural networks. This combined approach allows the identification of
localized and extended defects on the topographically non-uniform surfaces or
real materials. Our approach is universal in nature and can be applied to other
surfaces for building comprehensive libraries of atomic defects in quantum
materials.",arxiv
http://arxiv.org/abs/2005.00160v2,2020-09-04T01:20:37Z,2020-05-01T00:54:14Z,"PipelineProfiler: A Visual Analytics Tool for the Exploration of AutoML
  Pipelines","In recent years, a wide variety of automated machine learning (AutoML)
methods have been proposed to search and generate end-to-end learning
pipelines. While these techniques facilitate the creation of models for
real-world applications, given their black-box nature, the complexity of the
underlying algorithms, and the large number of pipelines they derive, it is
difficult for their developers to debug these systems. It is also challenging
for machine learning experts to select an AutoML system that is well suited for
a given problem or class of problems. In this paper, we present the
PipelineProfiler, an interactive visualization tool that allows the exploration
and comparison of the solution space of machine learning (ML) pipelines
produced by AutoML systems. PipelineProfiler is integrated with Jupyter
Notebook and can be used together with common data science tools to enable a
rich set of analyses of the ML pipelines and provide insights about the
algorithms that generated them. We demonstrate the utility of our tool through
several use cases where PipelineProfiler is used to better understand and
improve a real-world AutoML system. Furthermore, we validate our approach by
presenting a detailed analysis of a think-aloud experiment with six data
scientists who develop and evaluate AutoML tools.",arxiv
http://arxiv.org/abs/2101.04086v1,2021-01-11T18:29:50Z,2021-01-11T18:29:50Z,"System Design for a Data-driven and Explainable Customer Sentiment
  Monitor","The most important goal of customer services is to keep the customer
satisfied. However, service resources are always limited and must be
prioritized. Therefore, it is important to identify customers who potentially
become unsatisfied and might lead to escalations. Today this prioritization of
customers is often done manually. Data science on IoT data (esp. log data) for
machine health monitoring, as well as analytics on enterprise data for customer
relationship management (CRM) have mainly been researched and applied
independently. In this paper, we present a framework for a data-driven decision
support system which combines IoT and enterprise data to model customer
sentiment. Such decision support systems can help to prioritize customers and
service resources to effectively troubleshoot problems or even avoid them. The
framework is applied in a real-world case study with a major medical device
manufacturer. This includes a fully automated and interpretable machine
learning pipeline designed to meet the requirements defined with domain experts
and end users. The overall framework is currently deployed, learns and
evaluates predictive models from terabytes of IoT and enterprise data to
actively monitor the customer sentiment for a fleet of thousands of high-end
medical devices. Furthermore, we provide an anonymized industrial benchmark
dataset for the research community.",arxiv
http://arxiv.org/abs/2006.09191v2,2020-10-25T23:11:44Z,2020-06-16T14:34:40Z,"Sample-Efficient Optimization in the Latent Space of Deep Generative
  Models via Weighted Retraining","Many important problems in science and engineering, such as drug design,
involve optimizing an expensive black-box objective function over a complex,
high-dimensional, and structured input space. Although machine learning
techniques have shown promise in solving such problems, existing approaches
substantially lack sample efficiency. We introduce an improved method for
efficient black-box optimization, which performs the optimization in the
low-dimensional, continuous latent manifold learned by a deep generative model.
In contrast to previous approaches, we actively steer the generative model to
maintain a latent manifold that is highly useful for efficiently optimizing the
objective. We achieve this by periodically retraining the generative model on
the data points queried along the optimization trajectory, as well as weighting
those data points according to their objective function value. This weighted
retraining can be easily implemented on top of existing methods, and is
empirically shown to significantly improve their efficiency and performance on
synthetic and real-world optimization problems.",arxiv
http://arxiv.org/abs/2107.09822v2,2021-07-22T05:48:12Z,2021-07-21T00:43:32Z,"Bayesian Controller Fusion: Leveraging Control Priors in Deep
  Reinforcement Learning for Robotics","We present Bayesian Controller Fusion (BCF): a hybrid control strategy that
combines the strengths of traditional hand-crafted controllers and model-free
deep reinforcement learning (RL). BCF thrives in the robotics domain, where
reliable but suboptimal control priors exist for many tasks, but RL from
scratch remains unsafe and data-inefficient. By fusing uncertainty-aware
distributional outputs from each system, BCF arbitrates control between them,
exploiting their respective strengths. We study BCF on two real-world robotics
tasks involving navigation in a vast and long-horizon environment, and a
complex reaching task that involves manipulability maximisation. For both these
domains, there exist simple handcrafted controllers that can solve the task at
hand in a risk-averse manner but do not necessarily exhibit the optimal
solution given limitations in analytical modelling, controller miscalibration
and task variation. As exploration is naturally guided by the prior in the
early stages of training, BCF accelerates learning, while substantially
improving beyond the performance of the control prior, as the policy gains more
experience. More importantly, given the risk-aversity of the control prior, BCF
ensures safe exploration and deployment, where the control prior naturally
dominates the action distribution in states unknown to the policy. We
additionally show BCF's applicability to the zero-shot sim-to-real setting and
its ability to deal with out-of-distribution states in the real-world. BCF is a
promising approach for combining the complementary strengths of deep RL and
traditional robotic control, surpassing what either can achieve independently.
The code and supplementary video material are made publicly available at
https://krishanrana.github.io/bcf.",arxiv
http://arxiv.org/abs/2010.07634v3,2021-01-24T10:00:18Z,2020-10-15T10:09:09Z,"Towards Reflectivity profile inversion through Artificial Neural
  Networks","The goal of Specular Neutron and X-ray Reflectometry is to infer materials
Scattering Length Density (SLD) profiles from experimental reflectivity curves.
This paper focuses on investigating an original approach to the ill-posed
non-invertible problem which involves the use of Artificial Neural Networks
(ANN). In particular, the numerical experiments described here deal with large
data sets of simulated reflectivity curves and SLD profiles, and aim to assess
the applicability of Data Science and Machine Learning technology to the
analysis of data generated at neutron scattering large scale facilities. It is
demonstrated that, under certain circumstances, properly trained Deep Neural
Networks are capable of correctly recovering plausible SLD profiles when
presented with never-seen-before simulated reflectivity curves. When the
necessary conditions are met, a proper implementation of the described approach
would offer two main advantages over traditional fitting methods when dealing
with real experiments, namely, 1. sample physical models are described under a
new paradigm: detailed layer-by-layer descriptions (SLDs, thicknesses,
roughnesses) are replaced by parameter free curves $\rho(z)$, allowing a-priori
assumptions to be fed in terms of the sample family to which a given sample
belongs (e.g. ""thin film"", ""lamellar structure"", etc.) 2. the time-to-solution
is shrunk by orders of magnitude, enabling faster batch analyses for large
datasets.",arxiv
http://arxiv.org/abs/2103.16010v1,2021-03-30T00:49:40Z,2021-03-30T00:49:40Z,"Theory-Guided Machine Learning for Process Simulation of Advanced
  Composites","Science-based simulation tools such as Finite Element (FE) models are
routinely used in scientific and engineering applications. While their success
is strongly dependent on our understanding of underlying governing physical
laws, they suffer inherent limitations including trade-off between
fidelity/accuracy and speed. The recent rise of Machine Learning (ML) proposes
a theory-agnostic paradigm. In complex multi-physics problems, however,
creating large enough datasets for successful training of ML models has proven
to be challenging. One promising strategy to bridge the divide between these
approaches and take advantage of their respective strengths is Theory-Guided
Machine Learning (TGML) which aims to integrate physical laws into ML
algorithms. In this paper, three case studies on thermal management during
processing of advanced composites are presented and studied using FE, ML and
TGML. A structured approach to incrementally adding increasingly complex
physics to training of TGML model is presented. The benefits of TGML over ML
models are seen in more accurate predictions, particularly outside the training
region, and ability to train with small datasets. One benefit of TGML over FE
is significant speed improvement to potentially develop real-time feedback
systems. A recent successful implementation of a TGML model to assess
producibility of aerospace composite parts is presented.",arxiv
http://arxiv.org/abs/1910.12750v1,2019-10-28T15:21:48Z,2019-10-28T15:21:48Z,"Deep-Learning-Based Image Segmentation Integrated with Optical
  Microscopy for Automatically Searching for Two-Dimensional Materials","Deep-learning algorithms enable precise image recognition based on
high-dimensional hierarchical image features. Here, we report the development
and implementation of a deep-learning-based image segmentation algorithm in an
autonomous robotic system to search for two-dimensional (2D) materials. We
trained the neural network based on Mask-RCNN on annotated optical microscope
images of 2D materials (graphene, hBN, MoS2, and WTe2). The inference algorithm
is run on a 1024 x 1024 px2 optical microscope images for 200 ms, enabling the
real-time detection of 2D materials. The detection process is robust against
changes in the microscopy conditions, such as illumination and color balance,
which obviates the parameter-tuning process required for conventional
rule-based detection algorithms. Integrating the algorithm with a motorized
optical microscope enables the automated searching and cataloging of 2D
materials. This development will allow researchers to utilize unlimited amounts
of 2D materials simply by exfoliating and running the automated searching
process.",arxiv
http://arxiv.org/abs/1810.03190v1,2018-10-07T17:59:49Z,2018-10-07T17:59:49Z,"Scalable Solutions for Automated Single Pulse Identification and
  Classification in Radio Astronomy","Data collection for scientific applications is increasing exponentially and
is forecasted to soon reach peta- and exabyte scales. Applications which
process and analyze scientific data must be scalable and focus on execution
performance to keep pace. In the field of radio astronomy, in addition to
increasingly large datasets, tasks such as the identification of transient
radio signals from extrasolar sources are computationally expensive. We present
a scalable approach to radio pulsar detection written in Scala that
parallelizes candidate identification to take advantage of in-memory task
processing using Apache Spark on a YARN distributed system. Furthermore, we
introduce a novel automated multiclass supervised machine learning technique
that we combine with feature selection to reduce the time required for
candidate classification. Experimental testing on a Beowulf cluster with 15
data nodes shows that the parallel implementation of the identification
algorithm offers a speedup of up to 5X that of a similar multithreaded
implementation. Further, we show that the combination of automated multiclass
classification and feature selection speeds up the execution performance of the
RandomForest machine learning algorithm by an average of 54% with less than a
2% average reduction in the algorithm's ability to correctly classify pulsars.
The generalizability of these results is demonstrated by using two real-world
radio astronomy data sets.",arxiv
http://arxiv.org/abs/2108.12430v1,2021-08-27T18:00:00Z,2021-08-27T18:00:00Z,"Hardware-accelerated Inference for Real-Time Gravitational-Wave
  Astronomy","The field of transient astronomy has seen a revolution with the first
gravitational-wave detections and the arrival of multi-messenger observations
they enabled. Transformed by the first detection of binary black hole and
binary neutron star mergers, computational demands in gravitational-wave
astronomy are expected to grow by at least a factor of two over the next five
years as the global network of kilometer-scale interferometers are brought to
design sensitivity. With the increase in detector sensitivity, real-time
delivery of gravitational-wave alerts will become increasingly important as an
enabler of multi-messenger followup. In this work, we report a novel
implementation and deployment of deep learning inference for real-time
gravitational-wave data denoising and astrophysical source identification. This
is accomplished using a generic Inference-as-a-Service model that is capable of
adapting to the future needs of gravitational-wave data analysis. Our
implementation allows seamless incorporation of hardware accelerators and also
enables the use of commercial or private (dedicated) as-a-service computing.
Based on our results, we propose a paradigm shift in low-latency and offline
computing in gravitational-wave astronomy. Such a shift can address key
challenges in peak-usage, scalability and reliability, and provide a data
analysis platform particularly optimized for deep learning applications. The
achieved sub-millisecond scale latency will also be relevant for any machine
learning-based real-time control systems that may be invoked in the operation
of near-future and next generation ground-based laser interferometers, as well
as the front-end collection, distribution and processing of data from such
instruments.",arxiv
http://arxiv.org/abs/1908.11588v1,2019-08-30T08:13:22Z,2019-08-30T08:13:22Z,Generating Persuasive Visual Storylines for Promotional Videos,"Video contents have become a critical tool for promoting products in
E-commerce. However, the lack of automatic promotional video generation
solutions makes large-scale video-based promotion campaigns infeasible. The
first step of automatically producing promotional videos is to generate visual
storylines, which is to select the building block footage and place them in an
appropriate order. This task is related to the subjective viewing experience.
It is hitherto performed by human experts and thus, hard to scale. To address
this problem, we propose WundtBackpack, an algorithmic approach to generate
storylines based on available visual materials, which can be video clips or
images. It consists of two main parts, 1) the Learnable Wundt Curve to evaluate
the perceived persuasiveness based on the stimulus intensity of a sequence of
visual materials, which only requires a small volume of data to train; and 2) a
clustering-based backpacking algorithm to generate persuasive sequences of
visual materials while considering video length constraints. In this way, the
proposed approach provides a dynamic structure to empower artificial
intelligence (AI) to organize video footage in order to construct a sequence of
visual stimuli with persuasive power. Extensive real-world experiments show
that our approach achieves close to 10% higher perceived persuasiveness scores
by human testers, and 12.5% higher expected revenue compared to the best
performing state-of-the-art approach.",arxiv
http://arxiv.org/abs/2106.07178v4,2021-10-11T10:02:11Z,2021-06-14T06:04:57Z,A Comprehensive Survey on Graph Anomaly Detection with Deep Learning,"Anomalies represent rare observations (e.g., data records or events) that
deviate significantly from others. Over several decades, research on anomaly
mining has received increasing interests due to the implications of these
occurrences in a wide range of disciplines. Anomaly detection, which aims to
identify rare observations, is among the most vital tasks in the world, and has
shown its power in preventing detrimental events, such as financial fraud,
network intrusion, and social spam. The detection task is typically solved by
identifying outlying data points in the feature space and inherently overlooks
the relational information in real-world data. Graphs have been prevalently
used to represent the structural information, which raises the graph anomaly
detection problem - identifying anomalous graph objects (i.e., nodes, edges and
sub-graphs) in a single graph, or anomalous graphs in a database/set of graphs.
However, conventional anomaly detection techniques cannot tackle this problem
well because of the complexity of graph data. For the advent of deep learning,
graph anomaly detection with deep learning has received a growing attention
recently. In this survey, we aim to provide a systematic and comprehensive
review of the contemporary deep learning techniques for graph anomaly
detection. We compile open-sourced implementations, public datasets, and
commonly-used evaluation metrics to provide affluent resources for future
studies. More importantly, we highlight twelve extensive future research
directions according to our survey results covering unsolved and emerging
research problems and real-world applications. With this survey, our goal is to
create a ""one-stop-shop"" that provides a unified understanding of the problem
categories and existing approaches, publicly available hands-on resources, and
high-impact open challenges for graph anomaly detection using deep learning.",arxiv
http://arxiv.org/abs/1709.04511v4,2018-05-14T13:30:45Z,2017-09-13T19:21:57Z,"A Study of AI Population Dynamics with Million-agent Reinforcement
  Learning","We conduct an empirical study on discovering the ordered collective dynamics
obtained by a population of intelligence agents, driven by million-agent
reinforcement learning. Our intention is to put intelligent agents into a
simulated natural context and verify if the principles developed in the real
world could also be used in understanding an artificially-created intelligent
population. To achieve this, we simulate a large-scale predator-prey world,
where the laws of the world are designed by only the findings or logical
equivalence that have been discovered in nature. We endow the agents with the
intelligence based on deep reinforcement learning (DRL). In order to scale the
population size up to millions agents, a large-scale DRL training platform with
redesigned experience buffer is proposed. Our results show that the population
dynamics of AI agents, driven only by each agent's individual self-interest,
reveals an ordered pattern that is similar to the Lotka-Volterra model studied
in population biology. We further discover the emergent behaviors of collective
adaptations in studying how the agents' grouping behaviors will change with the
environmental resources. Both of the two findings could be explained by the
self-organization theory in nature.",arxiv
http://arxiv.org/abs/2001.00487v1,2020-01-02T15:22:36Z,2020-01-02T15:22:36Z,"Using CNNs For Users Segmentation In Video See-Through Augmented
  Virtuality","In this paper, we present preliminary results on the use of deep learning
techniques to integrate the users self-body and other participants into a
head-mounted video see-through augmented virtuality scenario. It has been
previously shown that seeing users bodies in such simulations may improve the
feeling of both self and social presence in the virtual environment, as well as
user performance. We propose to use a convolutional neural network for real
time semantic segmentation of users bodies in the stereoscopic RGB video
streams acquired from the perspective of the user. We describe design issues as
well as implementation details of the system and demonstrate the feasibility of
using such neural networks for merging users bodies in an augmented virtuality
simulation.",arxiv
http://arxiv.org/abs/2004.13332v1,2020-04-28T06:57:18Z,2020-04-28T06:57:18Z,"The AI Economist: Improving Equality and Productivity with AI-Driven Tax
  Policies","Tackling real-world socio-economic challenges requires designing and testing
economic policies. However, this is hard in practice, due to a lack of
appropriate (micro-level) economic data and limited opportunity to experiment.
In this work, we train social planners that discover tax policies in dynamic
economies that can effectively trade-off economic equality and productivity. We
propose a two-level deep reinforcement learning approach to learn dynamic tax
policies, based on economic simulations in which both agents and a government
learn and adapt. Our data-driven approach does not make use of economic
modeling assumptions, and learns from observational data alone. We make four
main contributions. First, we present an economic simulation environment that
features competitive pressures and market dynamics. We validate the simulation
by showing that baseline tax systems perform in a way that is consistent with
economic theory, including in regard to learned agent behaviors and
specializations. Second, we show that AI-driven tax policies improve the
trade-off between equality and productivity by 16% over baseline policies,
including the prominent Saez tax framework. Third, we showcase several emergent
features: AI-driven tax policies are qualitatively different from baselines,
setting a higher top tax rate and higher net subsidies for low incomes.
Moreover, AI-driven tax policies perform strongly in the face of emergent
tax-gaming strategies learned by AI agents. Lastly, AI-driven tax policies are
also effective when used in experiments with human participants. In experiments
conducted on MTurk, an AI tax policy provides an equality-productivity
trade-off that is similar to that provided by the Saez framework along with
higher inverse-income weighted social welfare.",arxiv
http://arxiv.org/abs/1708.06393v1,2017-08-21T19:54:05Z,2017-08-21T19:54:05Z,"A citizen-science approach to muon events in imaging atmospheric
  Cherenkov telescope data: the Muon Hunter","Event classification is a common task in gamma-ray astrophysics. It can be
treated with rapidly-advancing machine learning algorithms, which have the
potential to outperform traditional analysis methods. However, a major
challenge for machine learning models is extracting reliably labelled training
examples from real data. Citizen science offers a promising approach to tackle
this challenge.
  We present ""Muon Hunter"", a citizen science project hosted on the Zooniverse
platform, where VERITAS data are classified multiple times by individual users
in order to select and parameterize muon events, a product from cosmic ray
induced showers. We use this dataset to train and validate a convolutional
neural-network model to identify muon events for use in monitoring and
calibration. The results of this work and our experience of using the
Zooniverse are presented.",arxiv
http://arxiv.org/abs/2106.12605v1,2021-06-23T18:08:07Z,2021-06-23T18:08:07Z,Deep Fake Detection: Survey of Facial Manipulation Detection Solutions,"Deep Learning as a field has been successfully used to solve a plethora of
complex problems, the likes of which we could not have imagined a few decades
back. But as many benefits as it brings, there are still ways in which it can
be used to bring harm to our society. Deep fakes have been proven to be one
such problem, and now more than ever, when any individual can create a fake
image or video simply using an application on the smartphone, there need to be
some countermeasures, with which we can detect if the image or video is a fake
or real and dispose of the problem threatening the trustworthiness of online
information. Although the Deep fakes created by neural networks, may seem to be
as real as a real image or video, it still leaves behind spatial and temporal
traces or signatures after moderation, these signatures while being invisible
to a human eye can be detected with the help of a neural network trained to
specialize in Deep fake detection. In this paper, we analyze several such
states of the art neural networks (MesoNet, ResNet-50, VGG-19, and Xception
Net) and compare them against each other, to find an optimal solution for
various scenarios like real-time deep fake detection to be deployed in online
social media platforms where the classification should be made as fast as
possible or for a small news agency where the classification need not be in
real-time but requires utmost accuracy.",arxiv
http://arxiv.org/abs/1905.10364v1,2019-05-24T09:19:42Z,2019-05-24T09:19:42Z,"Deep learning based high-resolution incoherent x-ray imaging with a
  single-pixel detector","X-ray ""ghost"" imaging has drawn great attention for its potential to lower
radiation dose in medical diagnosis. For practical implementation, however, the
efficiency and image quality have to be greatly improved. Here we demonstrate a
computational ghost imaging scheme where a bucket detector and specially
designed modulation masks are used, together with a new robust deep learning
algorithm in which a compressed set of Hadamard matrices is incorporated into a
multi-level wavelet convolutional neural network. By this means we have
obtained an image of a real object from only 18.75% of the Nyquist sampling
rate, using a portable tabletop incoherent x-ray source of ~37 {\mu}m diameter.
A high imaging resolution of ~10 {\mu}m is achieved, which represents a
concrete step towards the realization of a practical low cost x-ray ghost
imaging camera for applications in biomedicine, archeology, material science,
and so forth.",arxiv
http://arxiv.org/abs/1902.08638v1,2019-02-22T19:16:32Z,2019-02-22T19:16:32Z,MPP: Model Performance Predictor,"Operations is a key challenge in the domain of machine learning pipeline
deployments involving monitoring and management of real-time prediction
quality. Typically, metrics like accuracy, RMSE etc., are used to track the
performance of models in deployment. However, these metrics cannot be
calculated in production due to the absence of labels. We propose using an ML
algorithm, Model Performance Predictor (MPP), to track the performance of the
models in deployment. We argue that an ensemble of such metrics can be used to
create a score representing the prediction quality in production. This in turn
facilitates formulation and customization of ML alerts, that can be escalated
by an operations team to the data science team. Such a score automates
monitoring and enables ML deployments at scale.",arxiv
http://arxiv.org/abs/1806.06121v1,2018-05-28T12:36:33Z,2018-05-28T12:36:33Z,"Machine learning for prediction of extreme statistics in modulation
  instability","A central area of research in nonlinear science is the study of instabilities
that drive the emergence of extreme events. Unfortunately, experimental
techniques for measuring such phenomena often provide only partial
characterization. For example, real-time studies of instabilities in nonlinear
fibre optics frequently use only spectral data, precluding detailed predictions
about the associated temporal properties. Here, we show how Machine Learning
can overcome this limitation by predicting statistics for the maximum intensity
of temporal peaks in modulation instability based only on spectral
measurements. Specifically, we train a neural network based Machine Learning
model to correlate spectral and temporal properties of optical fibre modulation
instability using data from numerical simulations, and we then use this model
to predict the temporal probability distribution based on high-dynamic range
spectral data from experiments. These results open novel perspectives in all
systems exhibiting chaos and instability where direct time-domain observations
are difficult.",arxiv
http://arxiv.org/abs/1801.00091v2,2018-02-21T22:25:38Z,2017-12-30T06:56:37Z,"PrivySense: $\underline{Pri}$ce $\underline{V}$olatilit$\underline{y}$
  based $\underline{Sen}$timent$\underline{s}$ $\underline{E}$stimation from
  Financial News using Machine Learning","As machine learning ascends the peak of computer science zeitgeist, the usage
and experimentation with sentiment analysis using various forms of textual data
seems pervasive. The effect is especially pronounced in formulating securities
trading strategies, due to a plethora of reasons including the relative ease of
implementation and the abundance of academic research suggesting automated
sentiment analysis can be productively used in trading strategies. The source
data for such analyzers ranges a broad spectrum like social media feeds,
micro-blogs, real-time news feeds, ex-post financial data etc. The abstract
technique underlying these analyzers involve supervised learning of sentiment
classification where the classifier is trained on annotated source corpus, and
accuracy is measured by testing how well the classifiers generalizes on unseen
test data from the corpus. Post training, and validation of fitted models, the
classifiers are used to execute trading strategies, and the corresponding
returns are compared with appropriate benchmark returns (for e.g., the S&P500
returns).
  In this paper, we introduce $\underline{a\ novel\ technique\ of\ using\
price\ volatilities\ to\ empirically\ determine\ the\ sentiment\ in\ news\
data}$, instead of the traditional reverse approach. We also perform meta
sentiment analysis by evaluating the efficacy of existing sentiment classifiers
and the precise definition of sentiment from securities trading context. We
scrutinize the efficacy of using human-annotated sentiment classification and
the tacit assumptions that introduces subjective bias in existing financial
news sentiment classifiers.",arxiv
http://arxiv.org/abs/2103.11799v1,2021-03-14T16:11:30Z,2021-03-14T16:11:30Z,DeepHate: Hate Speech Detection via Multi-Faceted Text Representations,"Online hate speech is an important issue that breaks the cohesiveness of
online social communities and even raises public safety concerns in our
societies. Motivated by this rising issue, researchers have developed many
traditional machine learning and deep learning methods to detect hate speech in
online social platforms automatically. However, most of these methods have only
considered single type textual feature, e.g., term frequency, or using word
embeddings. Such approaches neglect the other rich textual information that
could be utilized to improve hate speech detection. In this paper, we propose
DeepHate, a novel deep learning model that combines multi-faceted text
representations such as word embeddings, sentiments, and topical information,
to detect hate speech in online social platforms. We conduct extensive
experiments and evaluate DeepHate on three large publicly available real-world
datasets. Our experiment results show that DeepHate outperforms the
state-of-the-art baselines on the hate speech detection task. We also perform
case studies to provide insights into the salient features that best aid in
detecting hate speech in online social platforms.",arxiv
http://arxiv.org/abs/2109.09929v1,2021-09-21T03:12:23Z,2021-09-21T03:12:23Z,"A Unified Approach of Detecting Misleading Images via Tracing its
  Instances on Web and Analysing its Past Context for the Verification of
  Content","The verification of multimedia content over social media is one of the
challenging and crucial issues in the current scenario and gaining prominence
in an age where user-generated content and online social web platforms are the
leading sources in shaping and propagating news stories. As these sources allow
users to share their opinions without restriction, opportunistic users often
post misleading/ unreliable content on social media such as Twitter, Facebook,
etc. At present, to lure users towards the news story, the text is often
attached with some multimedia content (images/videos/audios). Verifying these
contents to maintain the credibility and reliability of social media
information is of paramount importance. Motivated by this, we proposed a
generalized system that supports the automatic classification of images into
credible or misleading. In this paper, we investigated machine learning-based
as well as deep learning-based approaches utilized to verify misleading
multimedia content, where the available image traces are used to identify the
credibility of the content. The experiment is performed on the real-world
dataset (Media-eval-2015 dataset) collected from Twitter. It also demonstrates
the efficiency of our proposed approach and features using both Machine and
Deep Learning Model (Bi-directional LSTM). The experiment result reveals that
the Microsoft bings image search engine is quite effective in retrieving titles
and performs better than our study's Google image search engine. It also shows
that gathering clues from attached multimedia content (image) is more effective
than detecting only posted content-based features.",arxiv
http://arxiv.org/abs/2007.04154v1,2020-07-08T14:33:17Z,2020-07-08T14:33:17Z,Robust pricing and hedging via neural SDEs,"Mathematical modelling is ubiquitous in the financial industry and drives key
decision processes. Any given model provides only a crude approximation to
reality and the risk of using an inadequate model is hard to detect and
quantify. By contrast, modern data science techniques are opening the door to
more robust and data-driven model selection mechanisms. However, most machine
learning models are ""black-boxes"" as individual parameters do not have
meaningful interpretation. The aim of this paper is to combine the above
approaches achieving the best of both worlds. Combining neural networks with
risk models based on classical stochastic differential equations (SDEs), we
find robust bounds for prices of derivatives and the corresponding hedging
strategies while incorporating relevant market data. The resulting model called
neural SDE is an instantiation of generative models and is closely linked with
the theory of causal optimal transport. Neural SDEs allow consistent
calibration under both the risk-neutral and the real-world measures. Thus the
model can be used to simulate market scenarios needed for assessing risk
profiles and hedging strategies. We develop and analyse novel algorithms needed
for efficient use of neural SDEs. We validate our approach with numerical
experiments using both local and stochastic volatility models.",arxiv
http://arxiv.org/abs/1901.06242v1,2018-12-01T13:40:03Z,2018-12-01T13:40:03Z,"Data-driven Air Quality Characterisation for Urban Environments: a Case
  Study","The economic and social impact of poor air quality in towns and cities is
increasingly being recognised, together with the need for effective ways of
creating awareness of real-time air quality levels and their impact on human
health. With local authority maintained monitoring stations being
geographically sparse and the resultant datasets also featuring missing labels,
computational data-driven mechanisms are needed to address the data sparsity
challenge. In this paper, we propose a machine learning-based method to
accurately predict the Air Quality Index (AQI), using environmental monitoring
data together with meteorological measurements. To do so, we develop an air
quality estimation framework that implements a neural network that is enhanced
with a novel Non-linear Autoregressive neural network with exogenous input
(NARX), especially designed for time series prediction. The framework is
applied to a case study featuring different monitoring sites in London, with
comparisons against other standard machine-learning based predictive algorithms
showing the feasibility and robust performance of the proposed method for
different kinds of areas within an urban region.",arxiv
http://arxiv.org/abs/1804.07886v1,2018-04-21T04:16:46Z,2018-04-21T04:16:46Z,Social Bots for Online Public Health Interventions,"According to the Center for Disease Control and Prevention, in the United
States hundreds of thousands initiate smoking each year, and millions live with
smoking-related dis- eases. Many tobacco users discuss their habits and
preferences on social media. This work conceptualizes a framework for targeted
health interventions to inform tobacco users about the consequences of tobacco
use. We designed a Twitter bot named Notobot (short for No-Tobacco Bot) that
leverages machine learning to identify users posting pro-tobacco tweets and
select individualized interventions to address their interest in tobacco use.
We searched the Twitter feed for tobacco-related keywords and phrases, and
trained a convolutional neural network using over 4,000 tweets dichotomously
manually labeled as either pro- tobacco or not pro-tobacco. This model achieves
a 90% recall rate on the training set and 74% on test data. Users posting pro-
tobacco tweets are matched with former smokers with similar interests who
posted anti-tobacco tweets. Algorithmic matching, based on the power of peer
influence, allows for the systematic delivery of personalized interventions
based on real anti-tobacco tweets from former smokers. Experimental evaluation
suggests that our system would perform well if deployed. This research offers
opportunities for public health researchers to increase health awareness at
scale. Future work entails deploying the fully operational Notobot system in a
controlled experiment within a public health campaign.",arxiv
http://arxiv.org/abs/2102.09548v2,2021-08-28T19:59:03Z,2021-02-18T18:50:31Z,"Therapeutics Data Commons: Machine Learning Datasets and Tasks for Drug
  Discovery and Development","Therapeutics machine learning is an emerging field with incredible
opportunities for innovatiaon and impact. However, advancement in this field
requires formulation of meaningful learning tasks and careful curation of
datasets. Here, we introduce Therapeutics Data Commons (TDC), the first
unifying platform to systematically access and evaluate machine learning across
the entire range of therapeutics. To date, TDC includes 66 AI-ready datasets
spread across 22 learning tasks and spanning the discovery and development of
safe and effective medicines. TDC also provides an ecosystem of tools and
community resources, including 33 data functions and types of meaningful data
splits, 23 strategies for systematic model evaluation, 17 molecule generation
oracles, and 29 public leaderboards. All resources are integrated and
accessible via an open Python library. We carry out extensive experiments on
selected datasets, demonstrating that even the strongest algorithms fall short
of solving key therapeutics challenges, including real dataset distributional
shifts, multi-scale modeling of heterogeneous data, and robust generalization
to novel data points. We envision that TDC can facilitate algorithmic and
scientific advances and considerably accelerate machine-learning model
development, validation and transition into biomedical and clinical
implementation. TDC is an open-science initiative available at
https://tdcommons.ai.",arxiv
http://arxiv.org/abs/1802.08907v1,2018-02-24T20:34:44Z,2018-02-24T20:34:44Z,Muon Hunter: a Zooniverse project,"The large datasets and often low signal-to-noise inherent to the raw data of
modern astroparticle experiments calls out for increasingly sophisticated event
classification techniques. Machine learning algorithms, such as neural
networks, have the potential to outperform traditional analysis methods, but
come with the major challenge of identifying reliably classified training
samples from real data. Citizen science represents an effective approach to
sort through the large datasets efficiently and meet this challenge. Muon
Hunter is a project hosted on the Zooniverse platform, wherein volunteers sort
through pictures of data from the VERITAS cameras to identify muon ring images.
Each image is classified multiple times to produce a ""clean"" dataset used to
train and validate a convolutional neural network model both able to reject
background events and identify suitable calibration data to monitor the
telescope performance as a function of time.",arxiv
http://arxiv.org/abs/2101.06448v3,2021-01-21T18:16:41Z,2021-01-16T14:20:32Z,"Self-Supervised Multi-Channel Hypergraph Convolutional Network for
  Social Recommendation","Social relations are often used to improve recommendation quality when
user-item interaction data is sparse in recommender systems. Most existing
social recommendation models exploit pairwise relations to mine potential user
preferences. However, real-life interactions among users are very complicated
and user relations can be high-order. Hypergraph provides a natural way to
model complex high-order relations, while its potentials for improving social
recommendation are under-explored. In this paper, we fill this gap and propose
a multi-channel hypergraph convolutional network to enhance social
recommendation by leveraging high-order user relations. Technically, each
channel in the network encodes a hypergraph that depicts a common high-order
user relation pattern via hypergraph convolution. By aggregating the embeddings
learned through multiple channels, we obtain comprehensive user representations
to generate recommendation results. However, the aggregation operation might
also obscure the inherent characteristics of different types of high-order
connectivity information. To compensate for the aggregating loss, we
innovatively integrate self-supervised learning into the training of the
hypergraph convolutional network to regain the connectivity information with
hierarchical mutual information maximization. The experimental results on
multiple real-world datasets show that the proposed model outperforms the SOTA
methods, and the ablation study verifies the effectiveness of the multi-channel
setting and the self-supervised task. The implementation of our model is
available via https://github.com/Coder-Yu/RecQ.",arxiv
http://arxiv.org/abs/2006.06141v2,2020-11-10T15:19:07Z,2020-06-11T01:26:24Z,"On-the-fly Closed-loop Autonomous Materials Discovery via Bayesian
  Active Learning","Active learning - the field of machine learning (ML) dedicated to optimal
experiment design, has played a part in science as far back as the 18th century
when Laplace used it to guide his discovery of celestial mechanics [1]. In this
work we focus a closed-loop, active learning-driven autonomous system on
another major challenge, the discovery of advanced materials against the
exceedingly complex synthesis-processes-structure-property landscape. We
demonstrate autonomous research methodology (i.e. autonomous hypothesis
definition and evaluation) that can place complex, advanced materials in reach,
allowing scientists to fail smarter, learn faster, and spend less resources in
their studies, while simultaneously improving trust in scientific results and
machine learning tools. Additionally, this robot science enables
science-over-the-network, reducing the economic impact of scientists being
physically separated from their labs. We used the real-time closed-loop,
autonomous system for materials exploration and optimization (CAMEO) at the
synchrotron beamline to accelerate the fundamentally interconnected tasks of
rapid phase mapping and property optimization, with each cycle taking seconds
to minutes, resulting in the discovery of a novel epitaxial nanocomposite
phase-change memory material.",arxiv
http://arxiv.org/abs/2109.09343v1,2021-09-20T07:54:09Z,2021-09-20T07:54:09Z,"Latexify Math: Mathematical Formula Markup Revision to Assist
  Collaborative Editing in Math Q&A Sites","Collaborative editing questions and answers plays an important role in
quality control of Mathematics Stack Exchange which is a math Q&A Site. Our
study of post edits in Mathematics Stack Exchange shows that there is a large
number of math-related edits about latexifying formulas, revising LaTeX and
converting the blurred math formula screenshots to LaTeX sequence. Despite its
importance, manually editing one math-related post especially those with
complex mathematical formulas is time-consuming and error-prone even for
experienced users. To assist post owners and editors to do this editing, we
have developed an edit-assistance tool, MathLatexEdit for formula
latexification, LaTeX revision and screenshot transcription. We formulate this
formula editing task as a translation problem, in which an original post is
translated to a revised post. MathLatexEdit implements a deep learning based
approach including two encoder-decoder models for textual and visual LaTeX edit
recommendation with math-specific inference. The two models are trained on
large-scale historical original-edited post pairs and synthesized
screenshot-formula pairs. Our evaluation of MathLatexEdit not only demonstrates
the accuracy of our model, but also the usefulness of MathLatexEdit in editing
real-world posts which are accepted in Mathematics Stack Exchange.",arxiv
http://arxiv.org/abs/1803.00429v2,2018-07-17T08:22:34Z,2018-03-01T15:08:14Z,Learning Human-Aware Path Planning with Fully Convolutional Networks,"This work presents an approach to learn path planning for robot social
navigation by demonstration. We make use of Fully Convolutional Neural Networks
(FCNs) to learn from expert's path demonstrations a map that marks a feasible
path to the goal as a classification problem. The use of FCNs allows us to
overcome the problem of manually designing/identifying the cost-map and
relevant features for the task of robot navigation. The method makes use of
optimal Rapidly-exploring Random Tree planner (RRT*) to overcome eventual
errors in the path prediction; the FCNs prediction is used as cost-map and also
to partially bias the sampling of the configuration space, leading the planner
to behave similarly to the learned expert behavior. The approach is evaluated
in experiments with real trajectories and compared with Inverse Reinforcement
Learning algorithms that use RRT* as underlying planner.",arxiv
http://arxiv.org/abs/2105.08150v1,2021-05-17T20:30:36Z,2021-05-17T20:30:36Z,Modeling the EdNet Dataset with Logistic Regression,"Many of these challenges are won by neural network models created by
full-time artificial intelligence scientists. Due to this origin, they have a
black-box character that makes their use and application less clear to learning
scientists. We describe our experience with competition from the perspective of
educational data mining, a field founded in the learning sciences and connected
with roots in psychology and statistics. We describe our efforts from the
perspectives of learning scientists and the challenges to our methods, some
real and some imagined. We also discuss some basic results in the Kaggle system
and our thoughts on how those results may have been improved. Finally, we
describe how learner model predictions are used to make pedagogical decisions
for students. Their practical use entails a) model predictions and b) a
decision rule (based on the predictions). We point out how increased model
accuracy can be of limited practical utility, especially when paired with
simple decision rules and argue instead for the need to further investigate
optimal decision rules.",arxiv
http://arxiv.org/abs/2011.11761v2,2021-02-11T14:36:36Z,2020-11-16T09:37:27Z,"A robust solution of a statistical inverse problem in multiscale
  computational mechanics using an artificial neural network","This work addresses the inverse identification of apparent elastic properties
of random heterogeneous materials using machine learning based on artificial
neural networks. The proposed neural network-based identification method
requires the construction of a database from which an artificial neural network
can be trained to learn the nonlinear relationship between the hyperparameters
of a prior stochastic model of the random compliance field and some relevant
quantities of interest of an ad hoc multiscale computational model. An initial
database made up with input and target data is first generated from the
computational model, from which a processed database is deduced by conditioning
the input data with respect to the target data using the nonparametric
statistics. Two-and three-layer feedforward artificial neural networks are then
trained from each of the initial and processed databases to construct an
algebraic representation of the nonlinear mapping between the hyperparameters
(network outputs) and the quantities of interest (network inputs). The
performances of the trained artificial neural networks are analyzed in terms of
mean squared error, linear regression fit and probability distribution between
network outputs and targets for both databases. An ad hoc probabilistic model
of the input random vector is finally proposed in order to take into account
uncertainties on the network input and to perform a robustness analysis of the
network output with respect to the input uncertainties level. The capability of
the proposed neural network-based identification method to efficiently solve
the underlying statistical inverse problem is illustrated through two numerical
examples developed within the framework of 2D plane stress linear elasticity,
namely a first validation example on synthetic data obtained through
computational simulations and a second application example on real experimental
data obtained through a physical experiment monitored by digital image
correlation on a real heterogeneous biological material (beef cortical bone).",arxiv
http://arxiv.org/abs/1907.09209v1,2019-07-22T10:04:22Z,2019-07-22T10:04:22Z,"Automatic Calibration of Artificial Neural Networks for Zebrafish
  Collective Behaviours using a Quality Diversity Algorithm","During the last two decades, various models have been proposed for fish
collective motion. These models are mainly developed to decipher the biological
mechanisms of social interaction between animals. They consider very simple
homogeneous unbounded environments and it is not clear that they can simulate
accurately the collective trajectories. Moreover when the models are more
accurate, the question of their scalability to either larger groups or more
elaborate environments remains open. This study deals with learning how to
simulate realistic collective motion of collective of zebrafish, using
real-world tracking data. The objective is to devise an agent-based model that
can be implemented on an artificial robotic fish that can blend into a
collective of real fish. We present a novel approach that uses Quality
Diversity algorithms, a class of algorithms that emphasise exploration over
pure optimisation. In particular, we use CVT-MAP-Elites, a variant of the
state-of-the-art MAP-Elites algorithm for high dimensional search space.
Results show that Quality Diversity algorithms not only outperform classic
evolutionary reinforcement learning methods at the macroscopic level (i.e.
group behaviour), but are also able to generate more realistic biomimetic
behaviours at the microscopic level (i.e. individual behaviour).",arxiv
http://arxiv.org/abs/1107.5462v1,2011-07-27T13:07:39Z,2011-07-27T13:07:39Z,HyFlex: A Benchmark Framework for Cross-domain Heuristic Search,"Automating the design of heuristic search methods is an active research field
within computer science, artificial intelligence and operational research. In
order to make these methods more generally applicable, it is important to
eliminate or reduce the role of the human expert in the process of designing an
effective methodology to solve a given computational search problem.
Researchers developing such methodologies are often constrained on the number
of problem domains on which to test their adaptive, self-configuring
algorithms; which can be explained by the inherent difficulty of implementing
their corresponding domain specific software components.
  This paper presents HyFlex, a software framework for the development of
cross-domain search methodologies. The framework features a common software
interface for dealing with different combinatorial optimisation problems, and
provides the algorithm components that are problem specific. In this way, the
algorithm designer does not require a detailed knowledge the problem domains,
and thus can concentrate his/her efforts in designing adaptive general-purpose
heuristic search algorithms. Four hard combinatorial problems are fully
implemented (maximum satisfiability, one dimensional bin packing, permutation
flow shop and personnel scheduling), each containing a varied set of instance
data (including real-world industrial applications) and an extensive set of
problem specific heuristics and search operators. The framework forms the basis
for the first International Cross-domain Heuristic Search Challenge (CHeSC),
and it is currently in use by the international research community. In summary,
HyFlex represents a valuable new benchmark of heuristic search generality, with
which adaptive cross-domain algorithms are being easily developed, and reliably
compared.",arxiv
http://arxiv.org/abs/1806.05886v2,2021-04-29T17:42:02Z,2018-06-15T10:15:10Z,Automated Image Data Preprocessing with Deep Reinforcement Learning,"Data preparation, i.e. the process of transforming raw data into a format
that can be used for training effective machine learning models, is a tedious
and time-consuming task. For image data, preprocessing typically involves a
sequence of basic transformations such as cropping, filtering, rotating or
flipping images. Currently, data scientists decide manually based on their
experience which transformations to apply in which particular order to a given
image data set. Besides constituting a bottleneck in real-world data science
projects, manual image data preprocessing may yield suboptimal results as data
scientists need to rely on intuition or trial-and-error approaches when
exploring the space of possible image transformations and thus might not be
able to discover the most effective ones. To mitigate the inefficiency and
potential ineffectiveness of manual data preprocessing, this paper proposes a
deep reinforcement learning framework to automatically discover the optimal
data preprocessing steps for training an image classifier. The framework takes
as input sets of labeled images and predefined preprocessing transformations.
It jointly learns the classifier and the optimal preprocessing transformations
for individual images. Experimental results show that the proposed approach not
only improves the accuracy of image classifiers, but also makes them
substantially more robust to noisy inputs at test time.",arxiv
http://arxiv.org/abs/2104.09650v1,2021-04-19T21:32:44Z,2021-04-19T21:32:44Z,"Mapping the Internet: Modelling Entity Interactions in Complex
  Heterogeneous Networks","Even though machine learning algorithms already play a significant role in
data science, many current methods pose unrealistic assumptions on input data.
The application of such methods is difficult due to incompatible data formats,
or heterogeneous, hierarchical or entirely missing data fragments in the
dataset. As a solution, we propose a versatile, unified framework called
`HMill' for sample representation, model definition and training. We review in
depth a multi-instance paradigm for machine learning that the framework builds
on and extends. To theoretically justify the design of key components of HMill,
we show an extension of the universal approximation theorem to the set of all
functions realized by models implemented in the framework. The text also
contains a detailed discussion on technicalities and performance improvements
in our implementation, which is published for download under the MIT License.
The main asset of the framework is its flexibility, which makes modelling of
diverse real-world data sources with the same tool possible. Additionally to
the standard setting in which a set of attributes is observed for each object
individually, we explain how message-passing inference in graphs that represent
whole systems of objects can be implemented in the framework. To support our
claims, we solve three different problems from the cybersecurity domain using
the framework. The first use case concerns IoT device identification from raw
network observations. In the second problem, we study how malicious binary
files can be classified using a snapshot of the operating system represented as
a directed graph. The last provided example is a task of domain blacklist
extension through modelling interactions between entities in the network. In
all three problems, the solution based on the proposed framework achieves
performance comparable to specialized approaches.",arxiv
http://arxiv.org/abs/2103.15348v2,2021-06-21T16:24:36Z,2021-03-29T05:55:08Z,"LayoutParser: A Unified Toolkit for Deep Learning Based Document Image
  Analysis","Recent advances in document image analysis (DIA) have been primarily driven
by the application of neural networks. Ideally, research outcomes could be
easily deployed in production and extended for further investigation. However,
various factors like loosely organized codebases and sophisticated model
configurations complicate the easy reuse of important innovations by a wide
audience. Though there have been on-going efforts to improve reusability and
simplify deep learning (DL) model development in disciplines like natural
language processing and computer vision, none of them are optimized for
challenges in the domain of DIA. This represents a major gap in the existing
toolkit, as DIA is central to academic research across a wide range of
disciplines in the social sciences and humanities. This paper introduces
layoutparser, an open-source library for streamlining the usage of DL in DIA
research and applications. The core layoutparser library comes with a set of
simple and intuitive interfaces for applying and customizing DL models for
layout detection, character recognition, and many other document processing
tasks. To promote extensibility, layoutparser also incorporates a community
platform for sharing both pre-trained models and full document digitization
pipelines. We demonstrate that layoutparser is helpful for both lightweight and
large-scale digitization pipelines in real-word use cases. The library is
publicly available at https://layout-parser.github.io/.",arxiv
http://arxiv.org/abs/2010.14296v1,2020-10-27T13:54:37Z,2020-10-27T13:54:37Z,Fit to Measure: Reasoning about Sizes for Robust Object Recognition,"Service robots can help with many of our daily tasks, especially in those
cases where it is inconvenient or unsafe for us to intervene: e.g., under
extreme weather conditions or when social distance needs to be maintained.
However, before we can successfully delegate complex tasks to robots, we need
to enhance their ability to make sense of dynamic, real world environments. In
this context, the first prerequisite to improving the Visual Intelligence of a
robot is building robust and reliable object recognition systems. While object
recognition solutions are traditionally based on Machine Learning methods,
augmenting them with knowledge based reasoners has been shown to improve their
performance. In particular, based on our prior work on identifying the
epistemic requirements of Visual Intelligence, we hypothesise that knowledge of
the typical size of objects could significantly improve the accuracy of an
object recognition system. To verify this hypothesis, in this paper we present
an approach to integrating knowledge about object sizes in a ML based
architecture. Our experiments in a real world robotic scenario show that this
combined approach ensures a significant performance increase over state of the
art Machine Learning methods.",arxiv
http://arxiv.org/abs/1412.0623v2,2015-04-14T05:29:32Z,2014-12-01T20:11:44Z,Material Recognition in the Wild with the Materials in Context Database,"Recognizing materials in real-world images is a challenging task. Real-world
materials have rich surface texture, geometry, lighting conditions, and
clutter, which combine to make the problem particularly difficult. In this
paper, we introduce a new, large-scale, open dataset of materials in the wild,
the Materials in Context Database (MINC), and combine this dataset with deep
learning to achieve material recognition and segmentation of images in the
wild.
  MINC is an order of magnitude larger than previous material databases, while
being more diverse and well-sampled across its 23 categories. Using MINC, we
train convolutional neural networks (CNNs) for two tasks: classifying materials
from patches, and simultaneous material recognition and segmentation in full
images. For patch-based classification on MINC we found that the best
performing CNN architectures can achieve 85.2% mean class accuracy. We convert
these trained CNN classifiers into an efficient fully convolutional framework
combined with a fully connected conditional random field (CRF) to predict the
material at every pixel in an image, achieving 73.1% mean class accuracy. Our
experiments demonstrate that having a large, well-sampled dataset such as MINC
is crucial for real-world material recognition and segmentation.",arxiv
http://arxiv.org/abs/2004.12161v1,2020-04-25T14:52:11Z,2020-04-25T14:52:11Z,"DAN-SNR: A Deep Attentive Network for Social-Aware Next
  Point-of-Interest Recommendation","Next (or successive) point-of-interest (POI) recommendation has attracted
increasing attention in recent years. Most of the previous studies attempted to
incorporate the spatiotemporal information and sequential patterns of user
check-ins into recommendation models to predict the target user's next move.
However, none of these approaches utilized the social influence of each user's
friends. In this study, we discuss a new topic of next POI recommendation and
present a deep attentive network for social-aware next POI recommendation
called DAN-SNR. In particular, the DAN-SNR makes use of the self-attention
mechanism instead of the architecture of recurrent neural networks to model
sequential influence and social influence in a unified manner. Moreover, we
design and implement two parallel channels to capture short-term user
preference and long-term user preference as well as social influence,
respectively. By leveraging multi-head self-attention, the DAN-SNR can model
long-range dependencies between any two historical check-ins efficiently and
weigh their contributions to the next destination adaptively. Also, we carried
out a comprehensive evaluation using large-scale real-world datasets collected
from two popular location-based social networks, namely Gowalla and Brightkite.
Experimental results indicate that the DAN-SNR outperforms seven competitive
baseline approaches regarding recommendation performance and is of high
efficiency among six neural-network- and attention-based methods.",arxiv
http://arxiv.org/abs/2101.07399v2,2021-02-09T05:03:18Z,2021-01-19T01:24:18Z,"Deep Reinforcement Learning Optimizes Graphene Nanopores for Efficient
  Desalination","Two-dimensional nanomaterials, such as graphene, have been extensively
studied because of their outstanding physical properties. Structure and
geometry optimization of nanopores on such materials is beneficial for their
performances in real-world engineering applications, like water desalination.
However, the optimization process often involves very large number of
experiments or simulations which are expensive and time-consuming. In this
work, we propose a graphene nanopore optimization framework via the combination
of deep reinforcement learning (DRL) and convolutional neural network (CNN) for
efficient water desalination. The DRL agent controls the growth of nanopore by
determining the atom to be removed at each timestep, while the CNN predicts the
performance of nanoporus graphene for water desalination: the water flux and
ion rejection at a certain external pressure. With the synchronous feedback
from CNN-accelerated desalination performance prediction, our DRL agent can
optimize the nanoporous graphene efficiently in an online manner. Molecular
dynamics (MD) simulations on promising DRL-designed graphene nanopores show
that they have higher water flux while maintaining rival ion rejection rate
compared to the normal circular nanopores. Semi-oval shape with rough edges
geometry of DRL-designed pores is found to be the key factor for their high
water desalination performance. Ultimately, this study shows that DRL can be a
powerful tool for material design.",arxiv
http://arxiv.org/abs/1906.10958v3,2019-09-05T02:16:56Z,2019-06-26T10:33:24Z,Signed Graph Attention Networks,"Graph or network data is ubiquitous in the real world, including social
networks, information networks, traffic networks, biological networks and
various technical networks. The non-Euclidean nature of graph data poses the
challenge for modeling and analyzing graph data. Recently, Graph Neural Network
(GNNs) are proposed as a general and powerful framework to handle tasks on
graph data, e.g., node embedding, link prediction and node classification. As a
representative implementation of GNNs, Graph Attention Networks (GATs) are
successfully applied in a variety of tasks on real datasets. However, GAT is
designed to networks with only positive links and fails to handle signed
networks which contain both positive and negative links. In this paper, we
propose Signed Graph Attention Networks (SiGATs), generalizing GAT to signed
networks. SiGAT incorporates graph motifs into GAT to capture two well-known
theories in signed network research, i.e., balance theory and status theory. In
SiGAT, motifs offer us the flexible structural pattern to aggregate and
propagate messages on the signed network to generate node embeddings. We
evaluate the proposed SiGAT method by applying it to the signed link prediction
task. Experimental results on three real datasets demonstrate that SiGAT
outperforms feature-based method, network embedding method and state-of-the-art
GNN-based methods like signed graph convolutional network (SGCN).",arxiv
http://arxiv.org/abs/2105.15079v2,2021-06-10T12:49:03Z,2021-05-31T16:09:26Z,"SA2SL: From Aspect-Based Sentiment Analysis to Social Listening System
  for Business Intelligence","In this paper, we present a process of building a social listening system
based on aspect-based sentiment analysis in Vietnamese from creating a dataset
to building a real application. Firstly, we create UIT-ViSFD, a Vietnamese
Smartphone Feedback Dataset as a new benchmark corpus built based on a strict
annotation schemes for evaluating aspect-based sentiment analysis, consisting
of 11,122 human-annotated comments for mobile e-commerce, which is freely
available for research purposes. We also present a proposed approach based on
the Bi-LSTM architecture with the fastText word embeddings for the Vietnamese
aspect based sentiment task. Our experiments show that our approach achieves
the best performances with the F1-score of 84.48% for the aspect task and
63.06% for the sentiment task, which performs several conventional machine
learning and deep learning systems. Last but not least, we build SA2SL, a
social listening system based on the best performance model on our dataset,
which will inspire more social listening systems in future.",arxiv
http://arxiv.org/abs/2009.05152v1,2020-09-10T21:20:09Z,2020-09-10T21:20:09Z,"CasGCN: Predicting future cascade growth based on information diffusion
  graph","Sudden bursts of information cascades can lead to unexpected consequences
such as extreme opinions, changes in fashion trends, and uncontrollable spread
of rumors. It has become an important problem on how to effectively predict a
cascade' size in the future, especially for large-scale cascades on social
media platforms such as Twitter and Weibo. However, existing methods are
insufficient in dealing with this challenging prediction problem. Conventional
methods heavily rely on either hand crafted features or unrealistic
assumptions. End-to-end deep learning models, such as recurrent neural
networks, are not suitable to work with graphical inputs directly and cannot
handle structural information that is embedded in the cascade graphs. In this
paper, we propose a novel deep learning architecture for cascade growth
prediction, called CasGCN, which employs the graph convolutional network to
extract structural features from a graphical input, followed by the application
of the attention mechanism on both the extracted features and the temporal
information before conducting cascade size prediction. We conduct experiments
on two real-world cascade growth prediction scenarios (i.e., retweet popularity
on Sina Weibo and academic paper citations on DBLP), with the experimental
results showing that CasGCN enjoys a superior performance over several baseline
methods, particularly when the cascades are of large scale.",arxiv
http://arxiv.org/abs/2009.07632v1,2020-08-26T08:58:29Z,2020-08-26T08:58:29Z,"Helping Users Tackle Algorithmic Threats on Social Media: A Multimedia
  Research Agenda","Participation on social media platforms has many benefits but also poses
substantial threats. Users often face an unintended loss of privacy, are
bombarded with mis-/disinformation, or are trapped in filter bubbles due to
over-personalized content. These threats are further exacerbated by the rise of
hidden AI-driven algorithms working behind the scenes to shape users' thoughts,
attitudes, and behavior. We investigate how multimedia researchers can help
tackle these problems to level the playing field for social media users. We
perform a comprehensive survey of algorithmic threats on social media and use
it as a lens to set a challenging but important research agenda for effective
and real-time user nudging. We further implement a conceptual prototype and
evaluate it with experts to supplement our research agenda. This paper calls
for solutions that combat the algorithmic threats on social media by utilizing
machine learning and multimedia content analysis techniques but in a
transparent manner and for the benefit of the users.",arxiv
http://arxiv.org/abs/2103.11972v2,2021-06-23T06:33:03Z,2021-03-22T16:20:21Z,"Explaining Black-Box Algorithms Using Probabilistic Contrastive
  Counterfactuals","There has been a recent resurgence of interest in explainable artificial
intelligence (XAI) that aims to reduce the opaqueness of AI-based
decision-making systems, allowing humans to scrutinize and trust them. Prior
work in this context has focused on the attribution of responsibility for an
algorithm's decisions to its inputs wherein responsibility is typically
approached as a purely associational concept. In this paper, we propose a
principled causality-based approach for explaining black-box decision-making
systems that addresses limitations of existing methods in XAI. At the core of
our framework lies probabilistic contrastive counterfactuals, a concept that
can be traced back to philosophical, cognitive, and social foundations of
theories on how humans generate and select explanations. We show how such
counterfactuals can quantify the direct and indirect influences of a variable
on decisions made by an algorithm, and provide actionable recourse for
individuals negatively affected by the algorithm's decision. Unlike prior work,
our system, LEWIS: (1)can compute provably effective explanations and recourse
at local, global and contextual levels (2)is designed to work with users with
varying levels of background knowledge of the underlying causal model and
(3)makes no assumptions about the internals of an algorithmic system except for
the availability of its input-output data. We empirically evaluate LEWIS on
three real-world datasets and show that it generates human-understandable
explanations that improve upon state-of-the-art approaches in XAI, including
the popular LIME and SHAP. Experiments on synthetic data further demonstrate
the correctness of LEWIS's explanations and the scalability of its recourse
algorithm.",arxiv
http://arxiv.org/abs/2108.03044v1,2021-08-06T10:48:00Z,2021-08-06T10:48:00Z,"Molecule Generation Experience: An Open Platform of Material Design for
  Public Users","Artificial Intelligence (AI)-driven material design has been attracting great
attentions as a groundbreaking technology across a wide spectrum of industries.
Molecular design is particularly important owing to its broad application
domains and boundless creativity attributed to progresses in generative models.
The recent maturity of molecular generative models has stimulated expectations
for practical use among potential users, who are not necessarily familiar with
coding or scripting, such as experimental engineers and students in chemical
domains. However, most of the existing molecular generative models are Python
libraries on GitHub, that are accessible for only IT-savvy users. To fill this
gap, we newly developed a graphical user interface (GUI)-based web application
of molecular generative models, Molecule Generation Experience, that is open to
the general public. This is the first web application of molecular generative
models enabling users to work with built-in datasets to carry out molecular
design. In this paper, we describe the background technology extended from our
previous work. Our new online evaluation and structural filtering algorithms
significantly improved the generation speed by 30 to 1,000 times with a wider
structural variety, satisfying chemical stability and synthetic reality. We
also describe in detail our Kubernetes-based scalable cloud architecture and
user-oriented GUI that are necessary components to achieve a public service.
Finally, we present actual use cases in industrial research to design new
photoacid generators (PAGs) as well as release cases in educational events.",arxiv
http://arxiv.org/abs/2012.11696v2,2021-06-19T00:16:56Z,2020-12-21T21:48:18Z,"Image Captioning as an Assistive Technology: Lessons Learned from VizWiz
  2020 Challenge","Image captioning has recently demonstrated impressive progress largely owing
to the introduction of neural network algorithms trained on curated dataset
like MS-COCO. Often work in this field is motivated by the promise of
deployment of captioning systems in practical applications. However, the
scarcity of data and contexts in many competition datasets renders the utility
of systems trained on these datasets limited as an assistive technology in
real-world settings, such as helping visually impaired people navigate and
accomplish everyday tasks. This gap motivated the introduction of the novel
VizWiz dataset, which consists of images taken by the visually impaired and
captions that have useful, task-oriented information. In an attempt to help the
machine learning computer vision field realize its promise of producing
technologies that have positive social impact, the curators of the VizWiz
dataset host several competitions, including one for image captioning. This
work details the theory and engineering from our winning submission to the 2020
captioning competition. Our work provides a step towards improved assistive
image captioning systems.",arxiv
http://arxiv.org/abs/1902.06942v3,2019-02-28T09:39:00Z,2019-02-19T08:05:54Z,"Air Quality Measurement Based on Double-Channel Convolutional Neural
  Network Ensemble Learning","Environmental air quality affects people's life, obtaining real-time and
accurate environmental air quality has a profound guiding significance for the
development of social activities. At present, environmental air quality
measurement mainly adopts the method that setting air quality detector at
specific monitoring points in cities and timing sampling analysis, which is
easy to be restricted by time and space factors. Some air quality measurement
algorithms related to deep learning mostly adopt a single convolutional neural
network to train the whole image, which will ignore the difference of different
parts of the image. In this paper, we propose a method for air quality
measurement based on double-channel convolutional neural network ensemble
learning to solve the problem of feature extraction for different parts of
environmental images. Our method mainly includes two aspects: ensemble learning
of double-channel convolutional neural network and self-learning weighted
feature fusion. We constructed a double-channel convolutional neural network,
used each channel to train different parts of the environment images for
feature extraction. We propose a feature weight self-learning method, which
weights and concatenates the extracted feature vectors, and uses the fused
feature vectors to measure air quality. Our method can be applied to the two
tasks of air quality grade measurement and air quality index (AQI) measurement.
Moreover, we build an environmental image dataset of random time and location
condition. The experiments show that our method can achieve nearly 82% accuracy
and a small mean absolute error (MAE) on our test dataset. At the same time,
through comparative experiment, we proved that our proposed method gained
considerable improvement in performance compared with single channel
convolutional neural network air quality measurements.",arxiv
http://arxiv.org/abs/2012.06753v2,2020-12-20T13:23:54Z,2020-12-12T08:08:47Z,"Towards Neurohaptics: Brain-Computer Interfaces for Decoding Intuitive
  Sense of Touch","Noninvasive brain-computer interface (BCI) is widely used to recognize users'
intentions. Especially, BCI related to tactile and sensation decoding could
provide various effects on many industrial fields such as manufacturing
advanced touch displays, controlling robotic devices, and more immersive
virtual reality or augmented reality. In this paper, we introduce haptic and
sensory perception-based BCI systems called neurohaptics. It is a preliminary
study for a variety of scenarios using actual touch and touch imagery
paradigms. We designed a novel experimental environment and a device that could
acquire brain signals under touching designated materials to generate natural
touch and texture sensations. Through the experiment, we collected the
electroencephalogram (EEG) signals with respect to four different texture
objects. Seven subjects were recruited for the experiment and evaluated
classification performances using machine learning and deep learning
approaches. Hence, we could confirm the feasibility of decoding actual touch
and touch imagery on EEG signals to develop practical neurohaptics.",arxiv
http://arxiv.org/abs/2007.11808v1,2020-07-23T05:53:36Z,2020-07-23T05:53:36Z,"Deep Reinforcement Learning based Automatic Exploration for Navigation
  in Unknown Environment","This paper investigates the automatic exploration problem under the unknown
environment, which is the key point of applying the robotic system to some
social tasks. The solution to this problem via stacking decision rules is
impossible to cover various environments and sensor properties. Learning based
control methods are adaptive for these scenarios. However, these methods are
damaged by low learning efficiency and awkward transferability from simulation
to reality. In this paper, we construct a general exploration framework via
decomposing the exploration process into the decision, planning, and mapping
modules, which increases the modularity of the robotic system. Based on this
framework, we propose a deep reinforcement learning based decision algorithm
which uses a deep neural network to learning exploration strategy from the
partial map. The results show that this proposed algorithm has better learning
efficiency and adaptability for unknown environments. In addition, we conduct
the experiments on the physical robot, and the results suggest that the learned
policy can be well transfered from simulation to the real robot.",arxiv
http://arxiv.org/abs/1804.03853v4,2019-03-01T02:08:45Z,2018-04-11T07:51:59Z,"Real-world plant species identification based on deep convolutional
  neural networks and visual attention","This paper investigates the issue of real-world identification to fulfill
better species protection. We focus on plant species identification as it is a
classic and hot issue. In tradition plant species identification the samples
are scanned specimen and the background is simple. However, real-world species
recognition is more challenging. We first systematically investigate what is
realistic species recognition and the difference from tradition plant species
recognition. To deal with the challenging task, an interdisciplinary
collaboration is presented based on the latest advances in computer science and
technology. We propose a novel framework and an effective data augmentation
method for deep learning in this paper. We first crop the image in terms of
visual attention before general recognition. Besides, we apply it as a data
augmentation method. We call the novel data augmentation approach attention
cropping (AC). Deep convolutional neural networks are trained to predict
species from a large amount of data. Extensive experiments on traditional
dataset and specific dataset for real-world recognition are conducted to
evaluate the performance of our approach. Experiments first demonstrate that
our approach achieves state-of-the-art results on different types of datasets.
Besides, we also evaluate the performance of data augmentation method AC.
Results show that AC provides superior performance. Compared with the precision
of methods without AC, the results with AC achieve substantial improvement.",arxiv
http://arxiv.org/abs/1801.09471v1,2018-01-29T12:27:50Z,2018-01-29T12:27:50Z,Social Influence (Deep) Learning for Human Behavior Prediction,"Influence propagation in social networks has recently received large
interest. In fact, the understanding of how influence propagates among subjects
in a social network opens the way to a growing number of applications. Many
efforts have been made to quantitatively measure the influence probability
between pairs of subjects. Existing approaches have two main drawbacks: (i)
they assume that the influence probabilities are independent of each other, and
(ii) they do not consider the actions not performed by the subject (but
performed by her/his friends) to learn these probabilities. In this paper, we
propose to address these limitations by employing a deep learning approach. We
introduce a Deep Neural Network (DNN) framework that has the capability for
both modeling social influence and for predicting human behavior. To
empirically validate the proposed framework, we conduct experiments on a
real-life (offline) dataset of an Event-Based Social Network (EBSN). Results
indicate that our approach outperforms existing solutions, by efficiently
resolving the limitations previously described.",arxiv
http://arxiv.org/abs/2007.03177v1,2020-07-07T02:48:34Z,2020-07-07T02:48:34Z,"Modeling and Mitigating Human Annotation Errors to Design Efficient
  Stream Processing Systems with Human-in-the-loop Machine Learning","High-quality human annotations are necessary for creating effective machine
learning-driven stream processing systems. We study hybrid stream processing
systems based on a Human-In-The-Loop Machine Learning (HITL-ML) paradigm, in
which one or many human annotators and an automatic classifier (trained at
least partially by the human annotators) label an incoming stream of instances.
This is typical of many near-real time social media analytics and web
applications, including the annotation of social media posts during emergencies
by digital volunteer groups. From a practical perspective, low-quality human
annotations result in wrong labels for retraining automated classifiers and
indirectly contribute to the creation of inaccurate classifiers.
  Considering human annotation as a psychological process allows us to address
these limitations. We show that human annotation quality is dependent on the
ordering of instances shown to annotators, and can be improved by local changes
in the instance sequence/ordering provided to the annotators, yielding a more
accurate annotation of the stream. We design a theoretically-motivated human
error framework for the human annotation task to study the effect of ordering
instances (i.e., an ""annotation schedule""). Further, we propose an
error-avoidance approach to the active learning (HITL-ML) paradigm for stream
processing applications that is robust to these likely human errors when
deciding a human annotation schedule. We validate the human error framework
using crowdsourcing experiments and evaluate the proposed algorithm against
standard baselines for active learning via extensive experimentation on
classification tasks of filtering relevant social media posts during natural
disasters.",arxiv
http://arxiv.org/abs/2110.10655v1,2021-10-20T16:49:26Z,2021-10-20T16:49:26Z,"Adversarial Socialbot Learning via Multi-Agent Deep Hierarchical
  Reinforcement Learning","Socialbots are software-driven user accounts on social platforms, acting
autonomously (mimicking human behavior), with the aims to influence the
opinions of other users or spread targeted misinformation for particular goals.
As socialbots undermine the ecosystem of social platforms, they are often
considered harmful. As such, there have been several computational efforts to
auto-detect the socialbots. However, to our best knowledge, the adversarial
nature of these socialbots has not yet been studied. This begs a question ""can
adversaries, controlling socialbots, exploit AI techniques to their advantage?""
To this question, we successfully demonstrate that indeed it is possible for
adversaries to exploit computational learning mechanism such as reinforcement
learning (RL) to maximize the influence of socialbots while avoiding being
detected. We first formulate the adversarial socialbot learning as a
cooperative game between two functional hierarchical RL agents. While one agent
curates a sequence of activities that can avoid the detection, the other agent
aims to maximize network influence by selectively connecting with right users.
Our proposed policy networks train with a vast amount of synthetic graphs and
generalize better than baselines on unseen real-life graphs both in terms of
maximizing network influence (up to +18%) and sustainable stealthiness (up to
+40% undetectability) under a strong bot detector (with 90% detection
accuracy). During inference, the complexity of our approach scales linearly,
independent of a network's structure and the virality of news. This makes our
approach a practical adversarial attack when deployed in a real-life setting.",arxiv
http://arxiv.org/abs/1906.10910v2,2019-07-01T09:03:34Z,2019-06-26T08:37:44Z,"Creating A Neural Pedagogical Agent by Jointly Learning to Review and
  Assess","Machine learning plays an increasing role in intelligent tutoring systems as
both the amount of data available and specialization among students grow.
Nowadays, these systems are frequently deployed on mobile applications. Users
on such mobile education platforms are dynamic, frequently being added,
accessing the application with varying levels of focus, and changing while
using the service. The education material itself, on the other hand, is often
static and is an exhaustible resource whose use in tasks such as problem
recommendation must be optimized. The ability to update user models with
respect to educational material in real-time is thus essential; however,
existing approaches require time-consuming re-training of user features
whenever new data is added. In this paper, we introduce a neural pedagogical
agent for real-time user modeling in the task of predicting user response
correctness, a central task for mobile education applications. Our model,
inspired by work in natural language processing on sequence modeling and
machine translation, updates user features in real-time via bidirectional
recurrent neural networks with an attention mechanism over embedded
question-response pairs. We experiment on the mobile education application
SantaTOEIC, which has 559k users, 66M response data points as well as a set of
10k study problems each expert-annotated with topic tags and gathered since
2016. Our model outperforms existing approaches over several metrics in
predicting user response correctness, notably out-performing other methods on
new users without large question-response histories. Additionally, our
attention mechanism and annotated tag set allow us to create an interpretable
education platform, with a smart review system that addresses the
aforementioned issue of varied user attention and problem exhaustion.",arxiv
http://arxiv.org/abs/2006.07333v1,2020-06-12T17:17:01Z,2020-06-12T17:17:01Z,Targeting Learning: Robust Statistics for Reproducible Research,"Targeted Learning is a subfield of statistics that unifies advances in causal
inference, machine learning and statistical theory to help answer
scientifically impactful questions with statistical confidence. Targeted
Learning is driven by complex problems in data science and has been implemented
in a diversity of real-world scenarios: observational studies with missing
treatments and outcomes, personalized interventions, longitudinal settings with
time-varying treatment regimes, survival analysis, adaptive randomized trials,
mediation analysis, and networks of connected subjects. In contrast to the
(mis)application of restrictive modeling strategies that dominate the current
practice of statistics, Targeted Learning establishes a principled standard for
statistical estimation and inference (i.e., confidence intervals and p-values).
This multiply robust approach is accompanied by a guiding roadmap and a
burgeoning software ecosystem, both of which provide guidance on the
construction of estimators optimized to best answer the motivating question.
The roadmap of Targeted Learning emphasizes tailoring statistical procedures so
as to minimize their assumptions, carefully grounding them only in the
scientific knowledge available. The end result is a framework that honestly
reflects the uncertainty in both the background knowledge and the available
data in order to draw reliable conclusions from statistical analyses -
ultimately enhancing the reproducibility and rigor of scientific findings.",arxiv
http://arxiv.org/abs/1904.07998v2,2019-11-11T01:48:59Z,2019-04-16T22:10:19Z,"SynC: A Unified Framework for Generating Synthetic Population with
  Gaussian Copula","Synthetic population generation is the process of combining multiple
socioeconomic and demographic datasets from different sources and/or
granularity levels, and downscaling them to an individual level. Although it is
a fundamental step for many data science tasks, an efficient and standard
framework is absent. In this study, we propose a multi-stage framework called
SynC (Synthetic Population via Gaussian Copula) to fill the gap. SynC first
removes potential outliers in the data and then fits the filtered data with a
Gaussian copula model to correctly capture dependencies and marginal
distributions of sampled survey data. Finally, SynC leverages predictive models
to merge datasets into one and then scales them accordingly to match the
marginal constraints. We make three key contributions in this work: 1) propose
a novel framework for generating individual level data from aggregated data
sources by combining state-of-the-art machine learning and statistical
techniques, 2) demonstrate its value as a feature engineering tool, as well as
an alternative to data collection in situations where gathering is difficult
through two real-world datasets, 3) release an easy-to-use framework
implementation for reproducibility, and 4) ensure the methodology is scalable
at the production level and can easily incorporate new data.",arxiv
http://arxiv.org/abs/1812.03078v1,2018-12-07T15:57:54Z,2018-12-07T15:57:54Z,"Evolutionary Games, Complex Networks and Nonlinear Analysis for
  Epileptic Seizures Forecasting","Epileptic seizures detection and forecasting is nowadays widely recognized as
a problem of great significance and social resonance, and still remains an
open, grand challenge. Furthermore, the development of mobile warning systems
and wearable, non invasive, advisory devices are increasingly and strongly
requested, from the patient community and their families and also from
institutional stakeholders. According to the many recent studies, exploiting
machine learning capabilities upon intracranial EEG (iEEG), in this work we
investigate a combination of novel game theory dynamical model on networks for
brain electrical activity and nonlinear time series analysis based on
recurrences quantification. These two methods are then melted together within a
supervised learning scheme and finally, prediction performances are assessed
using EEG scalp datasets, specifically recorded for this study. Our study
achieved mean sensitivity of 70.9% and a mean time in warning of 20.3%, thus
showing an increase of the improvement over chance metric from 42%, reported in
the most recent study, to 50.5%. Moreover, the real time implementation of the
proposed approach is currently under development on a prototype of a wearable
device.",arxiv
http://arxiv.org/abs/2105.12123v1,2021-05-25T09:45:59Z,2021-05-25T09:45:59Z,Photonic extreme learning machine by free-space optical propagation,"Photonic brain-inspired platforms are emerging as novel analog computing
devices, enabling fast and energy-efficient operations for machine learning.
These artificial neural networks generally require tailored optical elements,
such as integrated photonic circuits, engineered diffractive layers,
nanophotonic materials, or time-delay schemes, which are challenging to train
or stabilize. Here we present a neuromorphic photonic scheme - photonic extreme
learning machines - that can be implemented simply by using an optical encoder
and coherent wave propagation in free space. We realize the concept through
spatial light modulation of a laser beam, with the far field that acts as
feature mapping space. We experimentally demonstrated learning from data on
various classification and regression tasks, achieving accuracies comparable to
digital extreme learning machines. Our findings point out an optical machine
learning device that is easy-to-train, energetically efficient, scalable and
fabrication-constraint free. The scheme can be generalized to a plethora of
photonic systems, opening the route to real-time neuromorphic processing of
optical data.",arxiv
http://arxiv.org/abs/2106.06150v1,2021-06-11T03:30:25Z,2021-06-11T03:30:25Z,Global Neighbor Sampling for Mixed CPU-GPU Training on Giant Graphs,"Graph neural networks (GNNs) are powerful tools for learning from graph data
and are widely used in various applications such as social network
recommendation, fraud detection, and graph search. The graphs in these
applications are typically large, usually containing hundreds of millions of
nodes. Training GNN models on such large graphs efficiently remains a big
challenge. Despite a number of sampling-based methods have been proposed to
enable mini-batch training on large graphs, these methods have not been proved
to work on truly industry-scale graphs, which require GPUs or mixed-CPU-GPU
training. The state-of-the-art sampling-based methods are usually not optimized
for these real-world hardware setups, in which data movement between CPUs and
GPUs is a bottleneck. To address this issue, we propose Global Neighborhood
Sampling that aims at training GNNs on giant graphs specifically for
mixed-CPU-GPU training. The algorithm samples a global cache of nodes
periodically for all mini-batches and stores them in GPUs. This global cache
allows in-GPU importance sampling of mini-batches, which drastically reduces
the number of nodes in a mini-batch, especially in the input layer, to reduce
data copy between CPU and GPU and mini-batch computation without compromising
the training convergence rate or model accuracy. We provide a highly efficient
implementation of this method and show that our implementation outperforms an
efficient node-wise neighbor sampling baseline by a factor of 2X-4X on giant
graphs. It outperforms an efficient implementation of LADIES with small layers
by a factor of 2X-14X while achieving much higher accuracy than LADIES.We also
theoretically analyze the proposed algorithm and show that with cached node
data of a proper size, it enjoys a comparable convergence rate as the
underlying node-wise sampling method.",arxiv
http://arxiv.org/abs/1608.01733v2,2016-10-04T03:49:19Z,2016-08-05T01:12:55Z,"The IPAC Image Subtraction and Discovery Pipeline for the intermediate
  Palomar Transient Factory","We describe the near real-time transient-source discovery engine for the
intermediate Palomar Transient Factory (iPTF), currently in operations at the
Infrared Processing and Analysis Center (IPAC), Caltech. We coin this system
the IPAC/iPTF Discovery Engine (or IDE). We review the algorithms used for
PSF-matching, image subtraction, detection, photometry, and machine-learned
(ML) vetting of extracted transient candidates. We also review the performance
of our ML classifier. For a limiting signal-to-noise ratio of 4 in relatively
unconfused regions, ""bogus"" candidates from processing artifacts and imperfect
image subtractions outnumber real transients by ~ 10:1. This can be
considerably higher for image data with inaccurate astrometric and/or
PSF-matching solutions. Despite this occasionally high contamination rate, the
ML classifier is able to identify real transients with an efficiency (or
completeness) of ~ 97% for a maximum tolerable false-positive rate of 1% when
classifying raw candidates. All subtraction-image metrics, source features, ML
probability-based real-bogus scores, contextual metadata from other surveys,
and possible associations with known Solar System objects are stored in a
relational database for retrieval by the various science working groups. We
review our efforts in mitigating false-positives and our experience in
optimizing the overall system in response to the multitude of science projects
underway with iPTF.",arxiv
http://arxiv.org/abs/2108.05385v1,2021-08-11T18:05:55Z,2021-08-11T18:05:55Z,Physics-Coupled Spatio-Temporal Active Learning for Dynamical Systems,"Spatio-temporal forecasting is of great importance in a wide range of
dynamical systems applications from atmospheric science, to recent COVID-19
spread modeling. These applications rely on accurate predictions of
spatio-temporal structured data reflecting real-world phenomena. A stunning
characteristic is that the dynamical system is not only driven by some physics
laws but also impacted by the localized factor in spatial and temporal regions.
One of the major challenges is to infer the underlying causes, which generate
the perceived data stream and propagate the involved causal dynamics through
the distributed observing units. Another challenge is that the success of
machine learning based predictive models requires massive annotated data for
model training. However, the acquisition of high-quality annotated data is
objectively manual and tedious as it needs a considerable amount of human
intervention, making it infeasible in fields that require high levels of
expertise. To tackle these challenges, we advocate a spatio-temporal
physics-coupled neural networks (ST-PCNN) model to learn the underlying physics
of the dynamical system and further couple the learned physics to assist the
learning of the recurring dynamics. To deal with data-acquisition constraints,
an active learning mechanism with Kriging for actively acquiring the most
informative data is proposed for ST-PCNN training in a partially observable
environment. Our experiments on both synthetic and real-world datasets exhibit
that the proposed ST-PCNN with active learning converges to near optimal
accuracy with substantially fewer instances.",arxiv
http://arxiv.org/abs/1806.06671v1,2018-06-18T13:43:51Z,2018-06-18T13:43:51Z,"Where to Go Next: A Spatio-temporal LSTM model for Next POI
  Recommendation","Next Point-of-Interest (POI) recommendation is of great value for both
location-based service providers and users. Recently Recurrent Neural Networks
(RNNs) have been proved to be effective on sequential recommendation tasks.
However, existing RNN solutions rarely consider the spatio-temporal intervals
between neighbor check-ins, which are essential for modeling user check-in
behaviors in next POI recommendation. In this paper, we propose a new variant
of LSTM, named STLSTM, which implements time gates and distance gates into LSTM
to capture the spatio-temporal relation between successive check-ins.
Specifically, one-time gate and one distance gate are designed to control
short-term interest update, and another time gate and distance gate are
designed to control long-term interest update. Furthermore, to reduce the
number of parameters and improve efficiency, we further integrate coupled input
and forget gates with our proposed model. Finally, we evaluate the proposed
model using four real-world datasets from various location-based social
networks. Our experimental results show that our model significantly
outperforms the state-of-the-art approaches for next POI recommendation.",arxiv
http://arxiv.org/abs/2108.10698v1,2021-08-08T17:44:29Z,2021-08-08T17:44:29Z,Efficacy of BERT embeddings on predicting disaster from Twitter data,"Social media like Twitter provide a common platform to share and communicate
personal experiences with other people. People often post their life
experiences, local news, and events on social media to inform others. Many
rescue agencies monitor this type of data regularly to identify disasters and
reduce the risk of lives. However, it is impossible for humans to manually
check the mass amount of data and identify disasters in real-time. For this
purpose, many research works have been proposed to present words in
machine-understandable representations and apply machine learning methods on
the word representations to identify the sentiment of a text. The previous
research methods provide a single representation or embedding of a word from a
given document. However, the recent advanced contextual embedding method (BERT)
constructs different vectors for the same word in different contexts. BERT
embeddings have been successfully used in different natural language processing
(NLP) tasks, yet there is no concrete analysis of how these representations are
helpful in disaster-type tweet analysis. In this research work, we explore the
efficacy of BERT embeddings on predicting disaster from Twitter data and
compare these to traditional context-free word embedding methods (GloVe,
Skip-gram, and FastText). We use both traditional machine learning methods and
deep learning methods for this purpose. We provide both quantitative and
qualitative results for this study. The results show that the BERT embeddings
have the best results in disaster prediction task than the traditional word
embeddings. Our codes are made freely accessible to the research community.",arxiv
http://arxiv.org/abs/2105.06331v1,2021-05-13T14:37:25Z,2021-05-13T14:37:25Z,Informed Equation Learning,"Distilling data into compact and interpretable analytic equations is one of
the goals of science. Instead, contemporary supervised machine learning methods
mostly produce unstructured and dense maps from input to output. Particularly
in deep learning, this property is owed to the generic nature of simple
standard link functions. To learn equations rather than maps, standard
non-linearities can be replaced with structured building blocks of atomic
functions. However, without strong priors on sparsity and structure,
representational complexity and numerical conditioning limit this direct
approach. To scale to realistic settings in science and engineering, we propose
an informed equation learning system. It provides a way to incorporate expert
knowledge about what are permitted or prohibited equation components, as well
as a domain-dependent structured sparsity prior. Our system then utilizes a
robust method to learn equations with atomic functions exhibiting
singularities, as e.g. logarithm and division. We demonstrate several
artificial and real-world experiments from the engineering domain, in which our
system learns interpretable models of high predictive power.",arxiv
http://arxiv.org/abs/2106.06873v1,2021-06-12T22:22:10Z,2021-06-12T22:22:10Z,Weakly-supervised Graph Meta-learning for Few-shot Node Classification,"Graphs are widely used to model the relational structure of data, and the
research of graph machine learning (ML) has a wide spectrum of applications
ranging from drug design in molecular graphs to friendship recommendation in
social networks. Prevailing approaches for graph ML typically require abundant
labeled instances in achieving satisfactory results, which is commonly
infeasible in real-world scenarios since labeled data for newly emerged
concepts (e.g., new categorizations of nodes) on graphs is limited. Though
meta-learning has been applied to different few-shot graph learning problems,
most existing efforts predominately assume that all the data from those seen
classes is gold-labeled, while those methods may lose their efficacy when the
seen data is weakly-labeled with severe label noise. As such, we aim to
investigate a novel problem of weakly-supervised graph meta-learning for
improving the model robustness in terms of knowledge transfer. To achieve this
goal, we propose a new graph meta-learning framework -- Graph Hallucination
Networks (Meta-GHN) in this paper. Based on a new robustness-enhanced episodic
training, Meta-GHN is meta-learned to hallucinate clean node representations
from weakly-labeled data and extracts highly transferable meta-knowledge, which
enables the model to quickly adapt to unseen tasks with few labeled instances.
Extensive experiments demonstrate the superiority of Meta-GHN over existing
graph meta-learning studies on the task of weakly-supervised few-shot node
classification.",arxiv
http://arxiv.org/abs/1907.07033v1,2019-07-16T14:32:33Z,2019-07-16T14:32:33Z,"Neural Language Model Based Training Data Augmentation for Weakly
  Supervised Early Rumor Detection","The scarcity and class imbalance of training data are known issues in current
rumor detection tasks. We propose a straight-forward and general-purpose data
augmentation technique which is beneficial to early rumor detection relying on
event propagation patterns. The key idea is to exploit massive unlabeled event
data sets on social media to augment limited labeled rumor source tweets. This
work is based on rumor spreading patterns revealed by recent rumor studies and
semantic relatedness between labeled and unlabeled data. A state-of-the-art
neural language model (NLM) and large credibility-focused Twitter corpora are
employed to learn context-sensitive representations of rumor tweets. Six
different real-world events based on three publicly available rumor datasets
are employed in our experiments to provide a comparative evaluation of the
effectiveness of the method. The results show that our method can expand the
size of an existing rumor data set nearly by 200% and corresponding social
context (i.e., conversational threads) by 100% with reasonable quality.
Preliminary experiments with a state-of-the-art deep learning-based rumor
detection model show that augmented data can alleviate over-fitting and class
imbalance caused by limited train data and can help to train complex neural
networks (NNs). With augmented data, the performance of rumor detection can be
improved by 12.1% in terms of F-score. Our experiments also indicate that
augmented training data can help to generalize rumor detection models on unseen
rumors.",arxiv
http://arxiv.org/abs/2010.14000v2,2020-12-08T18:04:58Z,2020-10-27T02:19:40Z,"Graph-based Reinforcement Learning for Active Learning in Real Time: An
  Application in Modeling River Networks","Effective training of advanced ML models requires large amounts of labeled
data, which is often scarce in scientific problems given the substantial human
labor and material cost to collect labeled data. This poses a challenge on
determining when and where we should deploy measuring instruments (e.g.,
in-situ sensors) to collect labeled data efficiently. This problem differs from
traditional pool-based active learning settings in that the labeling decisions
have to be made immediately after we observe the input data that come in a time
series. In this paper, we develop a real-time active learning method that uses
the spatial and temporal contextual information to select representative query
samples in a reinforcement learning framework. To reduce the need for large
training data, we further propose to transfer the policy learned from
simulation data which is generated by existing physics-based models. We
demonstrate the effectiveness of the proposed method by predicting streamflow
and water temperature in the Delaware River Basin given a limited budget for
collecting labeled data. We further study the spatial and temporal distribution
of selected samples to verify the ability of this method in selecting
informative samples over space and time.",arxiv
http://arxiv.org/abs/2102.10477v1,2021-02-20T23:45:24Z,2021-02-20T23:45:24Z,"Neural Sampling Machine with Stochastic Synapse allows Brain-like
  Learning and Inference","Many real-world mission-critical applications require continual online
learning from noisy data and real-time decision making with a defined
confidence level. Probabilistic models and stochastic neural networks can
explicitly handle uncertainty in data and allow adaptive learning-on-the-fly,
but their implementation in a low-power substrate remains a challenge. Here, we
introduce a novel hardware fabric that implements a new class of stochastic NN
called Neural-Sampling-Machine that exploits stochasticity in synaptic
connections for approximate Bayesian inference. Harnessing the inherent
non-linearities and stochasticity occurring at the atomic level in emerging
materials and devices allows us to capture the synaptic stochasticity occurring
at the molecular level in biological synapses. We experimentally demonstrate
in-silico hybrid stochastic synapse by pairing a ferroelectric field-effect
transistor -based analog weight cell with a two-terminal stochastic selector
element. Such a stochastic synapse can be integrated within the
well-established crossbar array architecture for compute-in-memory. We
experimentally show that the inherent stochastic switching of the selector
element between the insulator and metallic state introduces a multiplicative
stochastic noise within the synapses of NSM that samples the conductance states
of the FeFET, both during learning and inference. We perform network-level
simulations to highlight the salient automatic weight normalization feature
introduced by the stochastic synapses of the NSM that paves the way for
continual online learning without any offline Batch Normalization. We also
showcase the Bayesian inferencing capability introduced by the stochastic
synapse during inference mode, thus accounting for uncertainty in data. We
report 98.25%accuracy on standard image classification task as well as
estimation of data uncertainty in rotated samples.",arxiv
http://arxiv.org/abs/1902.03097v1,2019-01-29T11:57:53Z,2019-01-29T11:57:53Z,A semi-supervised approach to message stance classification,"Social media communications are becoming increasingly prevalent; some useful,
some false, whether unwittingly or maliciously. An increasing number of rumours
daily flood the social networks. Determining their veracity in an autonomous
way is a very active and challenging field of research, with a variety of
methods proposed. However, most of the models rely on determining the
constituent messages' stance towards the rumour, a feature known as the ""wisdom
of the crowd"". Although several supervised machine-learning approaches have
been proposed to tackle the message stance classification problem, these have
numerous shortcomings. In this paper we argue that semi-supervised learning is
more effective than supervised models and use two graph-based methods to
demonstrate it. This is not only in terms of classification accuracy, but
equally important, in terms of speed and scalability. We use the Label
Propagation and Label Spreading algorithms and run experiments on a dataset of
72 rumours and hundreds of thousands messages collected from Twitter. We
compare our results on two available datasets to the state-of-the-art to
demonstrate our algorithms' performance regarding accuracy, speed and
scalability for real-time applications.",arxiv
http://arxiv.org/abs/1902.06050v2,2019-12-20T07:53:04Z,2019-02-16T06:03:57Z,"Combination of Domain Knowledge and Deep Learning for Sentiment Analysis
  of Short and Informal Messages on Social Media","Sentiment analysis has been emerging recently as one of the major natural
language processing (NLP) tasks in many applications. Especially, as social
media channels (e.g. social networks or forums) have become significant sources
for brands to observe user opinions about their products, this task is thus
increasingly crucial. However, when applied with real data obtained from social
media, we notice that there is a high volume of short and informal messages
posted by users on those channels. This kind of data makes the existing works
suffer from many difficulties to handle, especially ones using deep learning
approaches. In this paper, we propose an approach to handle this problem. This
work is extended from our previous work, in which we proposed to combine the
typical deep learning technique of Convolutional Neural Networks with domain
knowledge. The combination is used for acquiring additional training data
augmentation and a more reasonable loss function. In this work, we further
improve our architecture by various substantial enhancements, including
negation-based data augmentation, transfer learning for word embeddings, the
combination of word-level embeddings and character-level embeddings, and using
multitask learning technique for attaching domain knowledge rules in the
learning process. Those enhancements, specifically aiming to handle short and
informal messages, help us to enjoy significant improvement in performance once
experimenting on real datasets.",arxiv
http://arxiv.org/abs/1606.01865v2,2016-11-07T20:51:29Z,2016-06-06T19:08:41Z,"Recurrent Neural Networks for Multivariate Time Series with Missing
  Values","Multivariate time series data in practical applications, such as health care,
geoscience, and biology, are characterized by a variety of missing values. In
time series prediction and other related tasks, it has been noted that missing
values and their missing patterns are often correlated with the target labels,
a.k.a., informative missingness. There is very limited work on exploiting the
missing patterns for effective imputation and improving prediction performance.
In this paper, we develop novel deep learning models, namely GRU-D, as one of
the early attempts. GRU-D is based on Gated Recurrent Unit (GRU), a
state-of-the-art recurrent neural network. It takes two representations of
missing patterns, i.e., masking and time interval, and effectively incorporates
them into a deep model architecture so that it not only captures the long-term
temporal dependencies in time series, but also utilizes the missing patterns to
achieve better prediction results. Experiments of time series classification
tasks on real-world clinical datasets (MIMIC-III, PhysioNet) and synthetic
datasets demonstrate that our models achieve state-of-the-art performance and
provides useful insights for better understanding and utilization of missing
values in time series analysis.",arxiv
http://arxiv.org/abs/1711.10339v2,2019-10-23T05:07:55Z,2017-11-27T05:14:19Z,Pulsar Candidate Identification with Artificial Intelligence Techniques,"Discovering pulsars is a significant and meaningful research topic in the
field of radio astronomy. With the advent of astronomical instruments such as
he Five-hundred-meter Aperture Spherical Telescope (FAST) in China, data
volumes and data rates are exponentially growing. This fact necessitates a
focus on artificial intelligence (AI) technologies that can perform the
automatic pulsar candidate identification to mine large astronomical data sets.
Automatic pulsar candidate identification can be considered as a task of
determining potential candidates for further investigation and eliminating
noises of radio frequency interferences or other non-pulsar signals. It is very
hard to raise the performance of DCNN-based pulsar identification because the
limited training samples restrict network structure to be designed deep enough
for learning good features as well as the crucial class imbalance problem due
to very limited number of real pulsar samples. To address these problems, we
proposed a framework which combines deep convolution generative adversarial
network (DCGAN) with support vector machine (SVM) to deal with imbalance class
problem and to improve pulsar identification accuracy. DCGAN is used as sample
generation and feature learning model, and SVM is adopted as the classifier for
predicting candidate's labels in the inference stage. The proposed framework is
a novel technique which not only can solve imbalance class problem but also can
learn discriminative feature representations of pulsar candidates instead of
computing hand-crafted features in preprocessing steps too, which makes it more
accurate for automatic pulsar candidate selection. Experiments on two pulsar
datasets verify the effectiveness and efficiency of our proposed method.",arxiv
http://arxiv.org/abs/2001.10092v1,2020-01-27T21:21:19Z,2020-01-27T21:21:19Z,"Objective Social Choice: Using Auxiliary Information to Improve Voting
  Outcomes","How should one combine noisy information from diverse sources to make an
inference about an objective ground truth? This frequently recurring, normative
question lies at the core of statistics, machine learning, policy-making, and
everyday life. It has been called ""combining forecasts"", ""meta-analysis"",
""ensembling"", and the ""MLE approach to voting"", among other names. Past studies
typically assume that noisy votes are identically and independently distributed
(i.i.d.), but this assumption is often unrealistic. Instead, we assume that
votes are independent but not necessarily identically distributed and that our
ensembling algorithm has access to certain auxiliary information related to the
underlying model governing the noise in each vote. In our present work, we: (1)
define our problem and argue that it reflects common and socially relevant real
world scenarios, (2) propose a multi-arm bandit noise model and count-based
auxiliary information set, (3) derive maximum likelihood aggregation rules for
ranked and cardinal votes under our noise model, (4) propose, alternatively, to
learn an aggregation rule using an order-invariant neural network, and (5)
empirically compare our rules to common voting rules and naive
experience-weighted modifications. We find that our rules successfully use
auxiliary information to outperform the naive baselines.",arxiv
http://arxiv.org/abs/1411.0440v8,2020-04-19T19:58:37Z,2014-11-03T11:50:19Z,Modelling serendipity in a computational context,"The term serendipity describes a creative process that develops, in context,
with the active participation of a creative agent, but not entirely within that
agent's control. While a system cannot be made to perform serendipitously on
demand, we argue that its $\mathit{serendipity\ potential}$ can be increased by
means of a suitable system architecture and other design choices. We distil a
unified description of serendipitous occurrences from historical theorisations
of serendipity and creativity. This takes the form of a framework with six
phases: $\mathit{perception}$, $\mathit{attention}$, $\mathit{interest}$,
$\mathit{explanation}$, $\mathit{bridge}$, and $\mathit{valuation}$. We then
use this framework to organise a survey of literature in cognitive science,
philosophy, and computing, which yields practical definitions of the six
phases, along with heuristics for implementation. We use the resulting model to
evaluate the serendipity potential of four existing systems developed by
others, and two systems previously developed by two of the authors. Most
existing research that considers serendipity in a computing context deals with
serendipity as a service; here we relate theories of serendipity to the
development of autonomous systems and computational creativity practice. We
argue that serendipity is not teleologically blind, and outline representative
directions for future applications of our model. We conclude that it is
feasible to equip computational systems with the potential for serendipity, and
that this could be beneficial in varied computational creativity/AI
applications, particularly those designed to operate responsively in real-world
contexts.",arxiv
http://arxiv.org/abs/2109.09837v1,2021-09-20T20:48:03Z,2021-09-20T20:48:03Z,"Physics-Guided and Physics-Explainable Recurrent Neural Network for Time
  Dynamics in Optical Resonances","Understanding the time evolution of physical systems is crucial to revealing
fundamental characteristics that are hidden in frequency domain. In optical
science, high-quality resonance cavities and enhanced interactions with matters
are at the heart of modern quantum technologies. However, capturing their time
dynamics in real-world scenarios suffers from long data acquisition and low
analysis accuracy due to slow convergence and limited time window. Here, we
report a physics-guided and physics-explainable recurrent neural network to
precisely forecast the time-domain response of resonance features with the
shortest acquired input sequence being 7\% of full length, and to infer
corresponding resonance frequencies. The model is trained in a two-step
multi-fidelity framework for high-accuracy forecast, where the first step is
based on a large amount of low-fidelity physical-model-generated synthetic data
and second step involves a small set of high-fidelity application-oriented
observational data. Through both simulations and experiments, we demonstrate
that the model is universally applicable to a wide range of resonances,
including dielectric metasurfaces, graphene plasmonics, and ultrastrongly
coupled Landau polaritons, where our model accurately captures small signal
features and learns essential physical quantities. The demonstrated machine
learning algorithm offers a new way to accelerate the exploration of physical
phenomena and the design of devices under resonance-enhanced light-matter
interaction.",arxiv
http://arxiv.org/abs/2101.09577v1,2021-01-23T20:23:31Z,2021-01-23T20:23:31Z,"ReliefE: Feature Ranking in High-dimensional Spaces via Manifold
  Embeddings","Feature ranking has been widely adopted in machine learning applications such
as high-throughput biology and social sciences. The approaches of the popular
Relief family of algorithms assign importances to features by iteratively
accounting for nearest relevant and irrelevant instances. Despite their high
utility, these algorithms can be computationally expensive and not-well suited
for high-dimensional sparse input spaces. In contrast, recent embedding-based
methods learn compact, low-dimensional representations, potentially
facilitating down-stream learning capabilities of conventional learners. This
paper explores how the Relief branch of algorithms can be adapted to benefit
from (Riemannian) manifold-based embeddings of instance and target spaces,
where a given embedding's dimensionality is intrinsic to the dimensionality of
the considered data set. The developed ReliefE algorithm is faster and can
result in better feature rankings, as shown by our evaluation on 20 real-life
data sets for multi-class and multi-label classification tasks. The utility of
ReliefE for high-dimensional data sets is ensured by its implementation that
utilizes sparse matrix algebraic operations. Finally, the relation of ReliefE
to other ranking algorithms is studied via the Fuzzy Jaccard Index.",arxiv
http://arxiv.org/abs/2003.11177v2,2020-05-26T23:36:22Z,2020-03-25T01:49:58Z,"Patch-based Non-Local Bayesian Networks for Blind Confocal Microscopy
  Denoising","Confocal microscopy is essential for histopathologic cell visualization and
quantification. Despite its significant role in biology, fluorescence confocal
microscopy suffers from the presence of inherent noise during image
acquisition. Non-local patch-wise Bayesian mean filtering (NLB) was until
recently the state-of-the-art denoising approach. However, classic denoising
methods have been outperformed by neural networks in recent years. In this
work, we propose to exploit the strengths of NLB in the framework of Bayesian
deep learning. We do so by designing a convolutional neural network and
training it to learn parameters of a Gaussian model approximating the prior on
noise-free patches given their nearest, similar yet non-local, neighbors. We
then apply Bayesian reasoning to leverage the prior and information from the
noisy patch in the process of approximating the noise-free patch. Specifically,
we use the closed-form analytic \textit{maximum a posteriori} (MAP) estimate in
the NLB algorithm to obtain the noise-free patch that maximizes the posterior
distribution. The performance of our proposed method is evaluated on confocal
microscopy images with real noise Poisson-Gaussian noise. Our experiments
reveal the superiority of our approach against state-of-the-art unsupervised
denoising techniques.",arxiv
http://arxiv.org/abs/1810.01013v1,2018-10-01T23:32:17Z,2018-10-01T23:32:17Z,"AI for Trustworthiness! Credible User Identification on Social Web for
  Disaster Response Agencies","Although social media provides a vibrant platform to discuss real-world
events, the quantity of information generated can overwhelm decision making
based on that information. By better understanding who is participating in
information sharing, we can more effectively filter information as the event
unfolds. Fine-grained understanding of credible sources can even help develop a
trusted network of users for specific events or situations. Given the culture
of relying on trusted actors for work practices in the humanitarian and
disaster response domain, we propose to identify potential credible users as
organizational and organizational-affiliated user accounts on social media in
realtime for effective information collection and dissemination. Therefore, we
examine social media using AI and Machine Learning methods during three types
of humanitarian or disaster events and identify key actors responding to social
media conversations as organization (business, group, or institution),
organization-affiliated (individual with an organizational affiliation), and
non-affiliated (individual without organizational affiliation) identities. We
propose a credible user classification approach using a diverse set of social,
activity, and descriptive representation features extracted from user profile
metadata. Our extensive experiments showed a contrasting participation behavior
of the user identities by their content practices, such as the use of higher
authoritative content sharing by organization and organization-affiliated
users. This study provides a direction for designing realtime credible content
analytics systems for humanitarian and disaster response agencies.",arxiv
http://arxiv.org/abs/2110.15245v1,2021-10-28T16:04:01Z,2021-10-28T16:04:01Z,"From Machine Learning to Robotics: Challenges and Opportunities for
  Embodied Intelligence","Machine learning has long since become a keystone technology, accelerating
science and applications in a broad range of domains. Consequently, the notion
of applying learning methods to a particular problem set has become an
established and valuable modus operandi to advance a particular field. In this
article we argue that such an approach does not straightforwardly extended to
robotics -- or to embodied intelligence more generally: systems which engage in
a purposeful exchange of energy and information with a physical environment. In
particular, the purview of embodied intelligent agents extends significantly
beyond the typical considerations of main-stream machine learning approaches,
which typically (i) do not consider operation under conditions significantly
different from those encountered during training; (ii) do not consider the
often substantial, long-lasting and potentially safety-critical nature of
interactions during learning and deployment; (iii) do not require ready
adaptation to novel tasks while at the same time (iv) effectively and
efficiently curating and extending their models of the world through targeted
and deliberate actions. In reality, therefore, these limitations result in
learning-based systems which suffer from many of the same operational
shortcomings as more traditional, engineering-based approaches when deployed on
a robot outside a well defined, and often narrow operating envelope. Contrary
to viewing embodied intelligence as another application domain for machine
learning, here we argue that it is in fact a key driver for the advancement of
machine learning technology. In this article our goal is to highlight
challenges and opportunities that are specific to embodied intelligence and to
propose research directions which may significantly advance the
state-of-the-art in robot learning.",arxiv
http://arxiv.org/abs/1812.06186v1,2018-12-13T18:57:53Z,2018-12-13T18:57:53Z,Towards Fast Biomechanical Modeling of Soft Tissue Using Neural Networks,"To date, the simulation of organ deformations for applications like therapy
planning or image-guided interventions is calculated by solving the
elastodynamics equations. While efficient solvers have been proposed for fast
simulations, methods that are both real-time and accurate are still an open
challenge. An ideal, interactive solver would be able to provide physically and
numerically accurate results at high frame rate, which requires efficient force
computation and time integration. Towards this goal, we explore in this paper
for the first time the use of neural networks to directly learn the underlying
biomechanics. Given a 3D mesh of a soft tissue segmented from medical images,
we train a neural network to predict vertex-wise accelerations for a large time
step based on the current state of the system. The model is trained using the
deformation of a bar under torsion, and evaluated on different motions,
geometries, and hyperelastic material models. For predictions of ten times the
original time step we observed a mean error of 0.017mm $\pm$ 0.014 (0.032) at a
mesh size of 50mm x 50mm x 100mm. Predictions at 20dt yield an error of 2.10mm
$\pm$ 1.73 (4.37) and by further increasing the prediction time step the
maximum error rises to 38.3mm due to an artificial stiffening. In all
experiments our proposed method stayed stable, while the reference solver fails
to converge. Our experiments suggest that it is possible to directly learn the
mechanical simulation and open further investigations for the direct
application of machine learning to speed-up biophysics solvers.",arxiv
http://arxiv.org/abs/2104.06410v1,2021-04-13T13:52:58Z,2021-04-13T13:52:58Z,Reward Shaping with Subgoals for Social Navigation,"Social navigation has been gaining attentions with the growth in machine
intelligence. Since reinforcement learning can select an action in the
prediction phase at a low computational cost, it has been formulated in a
social navigation tasks. However, reinforcement learning takes an enormous
number of iterations until acquiring a behavior policy in the learning phase.
This negatively affects the learning of robot behaviors in the real world. In
particular, social navigation includes humans who are unpredictable moving
obstacles in an environment. We proposed a reward shaping method with subgoals
to accelerate learning. The main part is an aggregation method that use
subgoals to shape a reinforcement learning algorithm. We performed a learning
experiment with a social navigation task in which a robot avoided collisions
and then reached its goal. The experimental results show that our method
improved the learning efficiency from a base algorithm in the task.",arxiv
http://arxiv.org/abs/1204.1653v1,2012-04-07T16:34:20Z,2012-04-07T16:34:20Z,Machine Cognition Models: EPAM and GPS,"Through history, the human being tried to relay its daily tasks to other
creatures, which was the main reason behind the rise of civilizations. It
started with deploying animals to automate tasks in the field of
agriculture(bulls), transportation (e.g. horses and donkeys), and even
communication (pigeons). Millenniums after, come the Golden age with
""Al-jazari"" and other Muslim inventors, which were the pioneers of automation,
this has given birth to industrial revolution in Europe, centuries after. At
the end of the nineteenth century, a new era was to begin, the computational
era, the most advanced technological and scientific development that is driving
the mankind and the reason behind all the evolutions of science; such as
medicine, communication, education, and physics. At this edge of technology
engineers and scientists are trying to model a machine that behaves the same as
they do, which pushed us to think about designing and implementing ""Things
that-Thinks"", then artificial intelligence was. In this work we will cover each
of the major discoveries and studies in the field of machine cognition, which
are the ""Elementary Perceiver and Memorizer""(EPAM) and ""The General Problem
Solver""(GPS). The First one focus mainly on implementing the human-verbal
learning behavior, while the second one tries to model an architecture that is
able to solve problems generally (e.g. theorem proving, chess playing, and
arithmetic). We will cover the major goals and the main ideas of each model, as
well as comparing their strengths and weaknesses, and finally giving their
fields of applications. And Finally, we will suggest a real life implementation
of a cognitive machine.",arxiv
http://arxiv.org/abs/2011.08916v1,2020-11-17T20:15:49Z,2020-11-17T20:15:49Z,"Deep Learning Benchmarks and Datasets for Social Media Image
  Classification for Disaster Response","During a disaster event, images shared on social media helps crisis managers
gain situational awareness and assess incurred damages, among other response
tasks. Recent advances in computer vision and deep neural networks have enabled
the development of models for real-time image classification for a number of
tasks, including detecting crisis incidents, filtering irrelevant images,
classifying images into specific humanitarian categories, and assessing the
severity of damage. Despite several efforts, past works mainly suffer from
limited resources (i.e., labeled images) available to train more robust deep
learning models. In this study, we propose new datasets for disaster type
detection, and informativeness classification, and damage severity assessment.
Moreover, we relabel existing publicly available datasets for new tasks. We
identify exact- and near-duplicates to form non-overlapping data splits, and
finally consolidate them to create larger datasets. In our extensive
experiments, we benchmark several state-of-the-art deep learning models and
achieve promising results. We release our datasets and models publicly, aiming
to provide proper baselines as well as to spur further research in the crisis
informatics community.",arxiv
http://arxiv.org/abs/2109.08861v1,2021-09-18T07:17:31Z,2021-09-18T07:17:31Z,"Improving the Deconvolution of Spectrum at Finite Temperature via Neural
  Network","In the study of condensed matter physics, spectral information plays an
important role for understand the mechanism of materials. However, it is
difficult to obtain the spectrum directly through experiments or simulation.
For example, the spectral information deconvoluted by scanning tunneling
spectroscopy suffers from the temperature broadening effect, which is ill-posed
and makes the deconvolution result unstable. To solve this problem, the core
idea of existing methods, such as the maximum entropy method, tends to select
appropriate regularization to suppress unstable oscillations. However, the
choice of regularization is difficult, and the oscillation has not been
completely eliminated. We think non-uniform sampling is the core improvement
direction, combined with stochastic optimization and deep learning, we
introduce a neural network based discretization scheme to solve the
deconvolution problem. Due to the neural network can represent any piece-wise
linear function, our method replace the target spectrum by network and can find
a better approximation solution through optimization accurate and efficient.
Experiments on theoretical datasets about superconductors demonstrate that the
gap is estimated to be more accurate and oscillating less, plugin real
experimental data, our approach can get clearer results for material analysis.",arxiv
http://arxiv.org/abs/2012.15005v2,2021-05-30T01:31:39Z,2020-12-30T02:03:25Z,"Infer-AVAE: An Attribute Inference Model Based on Adversarial
  Variational Autoencoder","User attributes, such as gender and education, face severe incompleteness in
social networks. In order to make this kind of valuable data usable for
downstream tasks like user profiling and personalized recommendation, attribute
inference aims to infer users' missing attribute labels based on observed data.
Recently, variational autoencoder (VAE), an end-to-end deep generative model,
has shown promising performance by handling the problem in a semi-supervised
way. However, VAEs can easily suffer from over-fitting and over-smoothing when
applied to attribute inference. To be specific, VAE implemented with
multi-layer perceptron (MLP) can only reconstruct input data but fail in
inferring missing parts. While using the trending graph neural networks (GNNs)
as encoder has the problem that GNNs aggregate redundant information from
neighborhood and generate indistinguishable user representations, which is
known as over-smoothing. In this paper, we propose an attribute
\textbf{Infer}ence model based on \textbf{A}dversarial \textbf{VAE}
(Infer-AVAE) to cope with these issues. Specifically, to overcome
over-smoothing, Infer-AVAE unifies MLP and GNNs in encoder to learn positive
and negative latent representations respectively. Meanwhile, an adversarial
network is trained to distinguish the two representations and GNNs are trained
to aggregate less noise for more robust representations through adversarial
training. Finally, to relieve over-fitting, mutual information constraint is
introduced as a regularizer for decoder, so that it can make better use of
auxiliary information in representations and generate outputs not limited by
observations. We evaluate our model on 4 real-world social network datasets,
experimental results demonstrate that our model averagely outperforms baselines
by 7.0$\%$ in accuracy.",arxiv
http://arxiv.org/abs/2105.13598v3,2021-08-17T06:31:58Z,2021-05-28T05:54:59Z,End-to-End Deep Fault Tolerant Control,"Ideally, accurate sensor measurements are needed to achieve a good
performance in the closed-loop control of mechatronic systems. As a
consequence, sensor faults will prevent the system from working correctly,
unless a fault-tolerant control (FTC) architecture is adopted. As model-based
FTC algorithms for nonlinear systems are often challenging to design, this
paper focuses on a new method for FTC in the presence of sensor faults, based
on deep learning. The considered approach replaces the phases of fault
detection and isolation and controller design with a single recurrent neural
network, which has the value of past sensor measurements in a given time window
as input, and the current values of the control variables as output. This
end-to-end deep FTC method is applied to a mechatronic system composed of a
spherical inverted pendulum, whose configuration is changed via reaction
wheels, in turn actuated by electric motors. The simulation and experimental
results show that the proposed method can handle abrupt faults occurring in
link position/velocity sensors. The provided supplementary material includes a
video of real-world experiments and the software source code.",arxiv
http://arxiv.org/abs/1805.08395v1,2018-05-22T05:11:59Z,2018-05-22T05:11:59Z,Learning to Optimize via Wasserstein Deep Inverse Optimal Control,"We study the inverse optimal control problem in social sciences: we aim at
learning a user's true cost function from the observed temporal behavior. In
contrast to traditional phenomenological works that aim to learn a generative
model to fit the behavioral data, we propose a novel variational principle and
treat user as a reinforcement learning algorithm, which acts by optimizing his
cost function. We first propose a unified KL framework that generalizes
existing maximum entropy inverse optimal control methods. We further propose a
two-step Wasserstein inverse optimal control framework. In the first step, we
compute the optimal measure with a novel mass transport equation. In the second
step, we formulate the learning problem as a generative adversarial network. In
two real world experiments - recommender systems and social networks, we show
that our framework obtains significant performance gains over both existing
inverse optimal control methods and point process based generative models.",arxiv
http://arxiv.org/abs/1905.12280v2,2019-06-21T10:59:58Z,2019-05-29T09:05:29Z,Lifelong Bayesian Optimization,"Automatic Machine Learning (Auto-ML) systems tackle the problem of automating
the design of prediction models or pipelines for data science. In this paper,
we present Lifelong Bayesian Optimization (LBO), an online, multitask Bayesian
optimization (BO) algorithm designed to solve the problem of model selection
for datasets arriving and evolving over time. To be suitable for ""lifelong""
Bayesian Optimization, an algorithm needs to scale with the ever increasing
number of acquisitions and should be able to leverage past optimizations in
learning the current best model. We cast the problem of model selection as a
black-box function optimization problem. In LBO, we exploit the correlation
between functions by using components of previously learned functions to speed
up the learning process for newly arriving datasets. Experiments on real and
synthetic data show that LBO outperforms standard BO algorithms applied
repeatedly on the data.",arxiv
http://arxiv.org/abs/1911.11455v2,2020-12-18T12:19:27Z,2019-11-26T11:13:51Z,"Neural Latent Space Model for Dynamic Networks and Temporal Knowledge
  Graphs","Although static networks have been extensively studied in machine learning,
data mining, and AI communities for many decades, the study of dynamic networks
has recently taken center stage due to the prominence of social media and its
effects on the dynamics of social networks. In this paper, we propose a
statistical model for dynamically evolving networks, together with a
variational inference approach. Our model, Neural Latent Space Model with
Variational Inference, encodes edge dependencies across different time
snapshots. It represents nodes via latent vectors and uses interaction matrices
to model the presence of edges. These matrices can be used to incorporate
multiple relations in heterogeneous networks by having a separate matrix for
each of the relations. To capture the temporal dynamics, both node vectors and
interaction matrices are allowed to evolve with time. Existing network analysis
methods use representation learning techniques for modelling networks. These
techniques are different for homogeneous and heterogeneous networks because
heterogeneous networks can have multiple types of edges and nodes as opposed to
a homogeneous network. Unlike these, we propose a unified model for homogeneous
and heterogeneous networks in a variational inference framework. Moreover, the
learned node latent vectors and interaction matrices may be interpretable and
therefore provide insights on the mechanisms behind network evolution. We
experimented with a single step and multi-step link forecasting on real-world
networks of homogeneous, bipartite, and heterogeneous nature, and demonstrated
that our model significantly outperforms existing models.",arxiv
http://arxiv.org/abs/2009.09471v1,2020-09-20T16:36:25Z,2020-09-20T16:36:25Z,"SYNC: A Copula based Framework for Generating Synthetic Data from
  Aggregated Sources","A synthetic dataset is a data object that is generated programmatically, and
it may be valuable to creating a single dataset from multiple sources when
direct collection is difficult or costly. Although it is a fundamental step for
many data science tasks, an efficient and standard framework is absent. In this
paper, we study a specific synthetic data generation task called downscaling, a
procedure to infer high-resolution, harder-to-collect information (e.g.,
individual level records) from many low-resolution, easy-to-collect sources,
and propose a multi-stage framework called SYNC (Synthetic Data Generation via
Gaussian Copula). For given low-resolution datasets, the central idea of SYNC
is to fit Gaussian copula models to each of the low-resolution datasets in
order to correctly capture dependencies and marginal distributions, and then
sample from the fitted models to obtain the desired high-resolution subsets.
Predictive models are then used to merge sampled subsets into one, and finally,
sampled datasets are scaled according to low-resolution marginal constraints.
We make four key contributions in this work: 1) propose a novel framework for
generating individual level data from aggregated data sources by combining
state-of-the-art machine learning and statistical techniques, 2) perform
simulation studies to validate SYNC's performance as a synthetic data
generation algorithm, 3) demonstrate its value as a feature engineering tool,
as well as an alternative to data collection in situations where gathering is
difficult through two real-world datasets, 4) release an easy-to-use framework
implementation for reproducibility and scalability at the production level that
easily incorporates new data.",arxiv
http://arxiv.org/abs/2004.14404v2,2020-05-23T01:42:24Z,2020-04-29T18:00:22Z,Meta-Reinforcement Learning for Robotic Industrial Insertion Tasks,"Robotic insertion tasks are characterized by contact and friction mechanics,
making them challenging for conventional feedback control methods due to
unmodeled physical effects. Reinforcement learning (RL) is a promising approach
for learning control policies in such settings. However, RL can be unsafe
during exploration and might require a large amount of real-world training
data, which is expensive to collect. In this paper, we study how to use
meta-reinforcement learning to solve the bulk of the problem in simulation by
solving a family of simulated industrial insertion tasks and then adapt
policies quickly in the real world. We demonstrate our approach by training an
agent to successfully perform challenging real-world insertion tasks using less
than 20 trials of real-world experience. Videos and other material are
available at https://pearl-insertion.github.io/",arxiv
http://arxiv.org/abs/2102.02279v1,2021-02-03T20:19:51Z,2021-02-03T20:19:51Z,Insiders and Outsiders in Research on Machine Learning and Society,"A subset of machine learning research intersects with societal issues,
including fairness, accountability and transparency, as well as the use of
machine learning for social good. In this work, we analyze the scholars
contributing to this research at the intersection of machine learning and
society through the lens of the sociology of science. By analyzing the
authorship of all machine learning papers posted to arXiv, we show that
compared to researchers from overrepresented backgrounds (defined by gender and
race/ethnicity), researchers from underrepresented backgrounds are more likely
to conduct research at this intersection than other kinds of machine learning
research. This state of affairs leads to contention between two perspectives on
insiders and outsiders in the scientific enterprise: outsiders being those
outside the group being studied, and outsiders being those who have not
participated as researchers in an area historically. This contention manifests
as an epistemic question on the validity of knowledge derived from lived
experience in machine learning research, and predicts boundary work that we see
in a real-world example.",arxiv
http://arxiv.org/abs/2004.00104v1,2020-03-31T20:58:14Z,2020-03-31T20:58:14Z,"Improvement of electronic Governance and mobile Governance in
  Multilingual Countries with Digital Etymology using Sanskrit Grammar","With huge improvement of digital connectivity (Wifi,3G,4G) and digital
devices access to internet has reached in the remotest corners now a days.
Rural people can easily access web or apps from PDAs, laptops, smartphones etc.
This is an opportunity of the Government to reach to the citizen in large
number, get their feedback, associate them in policy decision with e governance
without deploying huge man, material or resourses. But the Government of
multilingual countries face a lot of problem in successful implementation of
Government to Citizen (G2C) and Citizen to Government (C2G) governance as the
rural people tend and prefer to interact in their native languages. Presenting
equal experience over web or app to different language group of speakers is a
real challenge. In this research we have sorted out the problems faced by Indo
Aryan speaking netizens which is in general also applicable to any language
family groups or subgroups. Then we have tried to give probable solutions using
Etymology. Etymology is used to correlate the words using their ROOT forms. In
5th century BC Panini wrote Astadhyayi where he depicted sutras or rules -- how
a word is changed according to person,tense,gender,number etc. Later this book
was followed in Western countries also to derive their grammar of comparatively
new languages. We have trained our system for automatic root extraction from
the surface level or morphed form of words using Panian Gramatical rules. We
have tested our system over 10000 bengali Verbs and extracted the root form
with 98% accuracy. We are now working to extend the program to successfully
lemmatize any words of any language and correlate them by applying those rule
sets in Artificial Neural Network.",arxiv
http://arxiv.org/abs/2106.08064v1,2021-06-15T11:42:05Z,2021-06-15T11:42:05Z,"Generating Contrastive Explanations for Inductive Logic Programming
  Based on a Near Miss Approach","In recent research, human-understandable explanations of machine learning
models have received a lot of attention. Often explanations are given in form
of model simplifications or visualizations. However, as shown in cognitive
science as well as in early AI research, concept understanding can also be
improved by the alignment of a given instance for a concept with a similar
counterexample. Contrasting a given instance with a structurally similar
example which does not belong to the concept highlights what characteristics
are necessary for concept membership. Such near misses have been proposed by
Winston (1970) as efficient guidance for learning in relational domains. We
introduce an explanation generation algorithm for relational concepts learned
with Inductive Logic Programming (\textsc{GeNME}). The algorithm identifies
near miss examples from a given set of instances and ranks these examples by
their degree of closeness to a specific positive instance. A modified rule
which covers the near miss but not the original instance is given as an
explanation. We illustrate \textsc{GeNME} with the well known family domain
consisting of kinship relations, the visual relational Winston arches domain
and a real-world domain dealing with file management. We also present a
psychological experiment comparing human preferences of rule-based,
example-based, and near miss explanations in the family and the arches domains.",arxiv
http://arxiv.org/abs/1402.5481v4,2018-07-19T15:36:29Z,2014-02-22T05:10:56Z,From Predictive to Prescriptive Analytics,"In this paper, we combine ideas from machine learning (ML) and operations
research and management science (OR/MS) in developing a framework, along with
specific methods, for using data to prescribe optimal decisions in OR/MS
problems. In a departure from other work on data-driven optimization and
reflecting our practical experience with the data available in applications of
OR/MS, we consider data consisting, not only of observations of quantities with
direct effect on costs/revenues, such as demand or returns, but predominantly
of observations of associated auxiliary quantities. The main problem of
interest is a conditional stochastic optimization problem, given imperfect
observations, where the joint probability distributions that specify the
problem are unknown. We demonstrate that our proposed solution methods, which
are inspired by ML methods such as local regression, CART, and random forests,
are generally applicable to a wide range of decision problems. We prove that
they are tractable and asymptotically optimal even when data is not iid and may
be censored. We extend this to the case where decision variables may directly
affect uncertainty in unknown ways, such as pricing's effect on demand. As an
analogue to R^2, we develop a metric P termed the coefficient of
prescriptiveness to measure the prescriptive content of data and the efficacy
of a policy from an operations perspective. To demonstrate the power of our
approach in a real-world setting we study an inventory management problem faced
by the distribution arm of an international media conglomerate, which ships an
average of 1bil units per year. We leverage internal data and public online
data harvested from IMDb, Rotten Tomatoes, and Google to prescribe operational
decisions that outperform baseline measures. Specifically, the data we collect,
leveraged by our methods, accounts for an 88\% improvement as measured by our
P.",arxiv
http://arxiv.org/abs/2005.05024v1,2020-04-15T19:10:12Z,2020-04-15T19:10:12Z,Intelligent Tutoring Systems for Generation Z's Addiction,"As generation Z's big data is flooding the Internet through social nets,
neural network based data processing is turning an important cornerstone,
showing significant potential for fast extraction of data patterns. Online
course delivery and associated tutoring are transforming into customizable,
on-demand services driven by the learner. Besides automated grading, strong
potential exists for the development and deployment of next generation
intelligent tutoring software agents. Self-adaptive, online tutoring agents
exhibiting ""intelligent-like"" behavior, being capable ""to learn"" from the
learner, will become the next educational superstars. Over the past decade,
computer-based tutoring agents were deployed in a variety of extended reality
environments, from patient rehabilitation to psychological trauma healing. Most
of these agents are driven by a set of conditional control statements and a
large answers/questions pairs dataset. This article provides a brief
introduction on Generation Z's addiction to digital information, highlights
important efforts for the development of intelligent dialogue systems, and
explains the main components and important design decisions for Intelligent
Tutoring System.",arxiv
http://arxiv.org/abs/2005.05287v2,2020-05-25T12:16:12Z,2020-05-11T17:40:58Z,"Using Computer Vision to enhance Safety of Workforce in Manufacturing in
  a Post COVID World","The COVID-19 pandemic forced governments across the world to impose lockdowns
to prevent virus transmissions. This resulted in the shutdown of all economic
activity and accordingly the production at manufacturing plants across most
sectors was halted. While there is an urgency to resume production, there is an
even greater need to ensure the safety of the workforce at the plant site.
Reports indicate that maintaining social distancing and wearing face masks
while at work clearly reduces the risk of transmission. We decided to use
computer vision on CCTV feeds to monitor worker activity and detect violations
which trigger real time voice alerts on the shop floor. This paper describes an
efficient and economic approach of using AI to create a safe environment in a
manufacturing setup. We demonstrate our approach to build a robust social
distancing measurement algorithm using a mix of modern-day deep learning and
classic projective geometry techniques. We have deployed our solution at
manufacturing plants across the Aditya Birla Group (ABG). We have also
described our face mask detection approach which provides a high accuracy
across a range of customized masks.",arxiv
http://arxiv.org/abs/2001.05982v2,2020-06-04T15:13:47Z,2020-01-16T18:32:19Z,"A Common Operating Picture Framework Leveraging Data Fusion and Deep
  Learning","Organizations are starting to realize of the combined power of data and
data-driven algorithmic models to gain insights, situational awareness, and
advance their mission. A common challenge to gaining insights is connecting
inherently different datasets. These datasets (e.g. geocoded features, video
streams, raw text, social network data, etc.) per separate they provide very
narrow answers; however collectively they can provide new capabilities. In this
work, we present a data fusion framework for accelerating solutions for
Processing, Exploitation, and Dissemination (PED). Our platform is a collection
of services that extract information from several data sources (per separate)
by leveraging deep learning and other means of processing. This information is
fused by a set of analytical engines that perform data correlations, searches,
and other modeling operations to combine information from the disparate data
sources. As a result, events of interest are detected, geolocated, logged, and
presented into a common operating picture. This common operating picture allows
the user to visualize in real time all the data sources, per separate and their
collective cooperation. In addition, forensic activities have been implemented
and made available through the framework. Users can review archived results and
compare them to the most recent snapshot of the operational environment. In our
first iteration we have focused on visual data (FMV, WAMI, CCTV/PTZ-Cameras,
open source video, etc.) and AIS data streams (satellite and terrestrial
sources). As a proof-of-concept, in our experiments we show how FMV detections
can be combined with vessel tracking signals from AIS sources to confirm
identity, tip-and-cue aerial reconnaissance, and monitor vessel activity in an
area.",arxiv
http://arxiv.org/abs/2007.13127v1,2020-07-26T13:27:05Z,2020-07-26T13:27:05Z,What Government by Algorithm Might Look Like,"Algocracy is the rule by algorithms. This paper summarises technologies
useful to create algocratic social machines and presents idealistic examples of
their application. In particular, it describes smart contracts and their
implementations, challenges of behaviour mining and prediction, as well as
game-theoretic and AI approaches to mechanism design. The presented idealistic
examples of new algocratic solutions are picked from the reality of a modern
state. The examples are science funding, trade by organisations, regulation of
rental agreements, ranking of significance and sortition. Artificial General
Intelligence is not in the scope of this feasibility study.",arxiv
http://arxiv.org/abs/2005.03912v1,2020-05-08T08:59:31Z,2020-05-08T08:59:31Z,"An Extensive Study on Cross-Dataset Bias and Evaluation Metrics
  Interpretation for Machine Learning applied to Gastrointestinal Tract
  Abnormality Classification","Precise and efficient automated identification of Gastrointestinal (GI) tract
diseases can help doctors treat more patients and improve the rate of disease
detection and identification. Currently, automatic analysis of diseases in the
GI tract is a hot topic in both computer science and medical-related journals.
Nevertheless, the evaluation of such an automatic analysis is often incomplete
or simply wrong. Algorithms are often only tested on small and biased datasets,
and cross-dataset evaluations are rarely performed. A clear understanding of
evaluation metrics and machine learning models with cross datasets is crucial
to bring research in the field to a new quality level. Towards this goal, we
present comprehensive evaluations of five distinct machine learning models
using Global Features and Deep Neural Networks that can classify 16 different
key types of GI tract conditions, including pathological findings, anatomical
landmarks, polyp removal conditions, and normal findings from images captured
by common GI tract examination instruments. In our evaluation, we introduce
performance hexagons using six performance metrics such as recall, precision,
specificity, accuracy, F1-score, and Matthews Correlation Coefficient to
demonstrate how to determine the real capabilities of models rather than
evaluating them shallowly. Furthermore, we perform cross-dataset evaluations
using different datasets for training and testing. With these cross-dataset
evaluations, we demonstrate the challenge of actually building a generalizable
model that could be used across different hospitals. Our experiments clearly
show that more sophisticated performance metrics and evaluation methods need to
be applied to get reliable models rather than depending on evaluations of the
splits of the same dataset, i.e., the performance metrics should always be
interpreted together rather than relying on a single metric.",arxiv
http://arxiv.org/abs/2102.12799v1,2021-02-25T11:53:28Z,2021-02-25T11:53:28Z,"Cognitive network science for understanding online social cognitions: A
  brief review","Social media are digitalising massive amounts of users' cognitions in terms
of timelines and emotional content. Such Big Data opens unprecedented
opportunities for investigating cognitive phenomena like perception,
personality and information diffusion but requires suitable interpretable
frameworks. Since social media data come from users' minds, worthy candidates
for this challenge are cognitive networks, models of cognition giving structure
to mental conceptual associations. This work outlines how cognitive network
science can open new, quantitative ways for understanding cognition through
online media, like: (i) reconstructing how users semantically and emotionally
frame events with contextual knowledge unavailable to machine learning, (ii)
investigating conceptual salience/prominence through knowledge structure in
social discourse; (iii) studying users' personality traits like
openness-to-experience, curiosity, and creativity through language in posts;
(iv) bridging cognitive/emotional content and social dynamics via multilayer
networks comparing the mindsets of influencers and followers. These
advancements combine cognitive-, network- and computer science to understand
cognitive mechanisms in both digital and real-world settings but come with
limitations concerning representativeness, individual variability and data
integration. Such aspects are discussed along the ethical implications of
manipulating socio-cognitive data. In the future, reading cognitions through
networks and social media can expose cognitive biases amplified by online
platforms and relevantly inform policy making, education and markets about
massive, complex cognitive trends.",arxiv
http://arxiv.org/abs/2010.11411v2,2020-11-21T08:45:37Z,2020-10-22T03:27:19Z,"Value Cards: An Educational Toolkit for Teaching Social Impacts of
  Machine Learning through Deliberation","Recently, there have been increasing calls for computer science curricula to
complement existing technical training with topics related to Fairness,
Accountability, Transparency, and Ethics. In this paper, we present Value Card,
an educational toolkit to inform students and practitioners of the social
impacts of different machine learning models via deliberation. This paper
presents an early use of our approach in a college-level computer science
course. Through an in-class activity, we report empirical data for the initial
effectiveness of our approach. Our results suggest that the use of the Value
Cards toolkit can improve students' understanding of both the technical
definitions and trade-offs of performance metrics and apply them in real-world
contexts, help them recognize the significance of considering diverse social
values in the development of deployment of algorithmic systems, and enable them
to communicate, negotiate and synthesize the perspectives of diverse
stakeholders. Our study also demonstrates a number of caveats we need to
consider when using the different variants of the Value Cards toolkit. Finally,
we discuss the challenges as well as future applications of our approach.",arxiv
http://arxiv.org/abs/1909.11849v2,2019-09-27T19:11:48Z,2019-09-26T02:27:11Z,"The Ant Swarm Neuro-Evolution Procedure for Optimizing Recurrent
  Networks","Hand-crafting effective and efficient structures for recurrent neural
networks (RNNs) is a difficult, expensive, and time-consuming process. To
address this challenge, we propose a novel neuro-evolution algorithm based on
ant colony optimization (ACO), called ant swarm neuro-evolution (ASNE), for
directly optimizing RNN topologies. The procedure selects from multiple modern
recurrent cell types such as Delta-RNN, GRU, LSTM, MGU and UGRNN cells, as well
as recurrent connections which may span multiple layers and/or steps of time.
In order to introduce an inductive bias that encourages the formation of
sparser synaptic connectivity patterns, we investigate several variations of
the core algorithm. We do so primarily by formulating different functions that
drive the underlying pheromone simulation process (which mimic L1 and L2
regularization in standard machine learning) as well as by introducing ant
agents with specialized roles (inspired by how real ant colonies operate),
i.e., explorer ants that construct the initial feed forward structure and
social ants which select nodes from the feed forward connections to
subsequently craft recurrent memory structures. We also incorporate a
Lamarckian strategy for weight initialization which reduces the number of
backpropagation epochs required to locally train candidate RNNs, speeding up
the neuro-evolution process. Our results demonstrate that the sparser RNNs
evolved by ASNE significantly outperform traditional one and two layer
architectures consisting of modern memory cells, as well as the well-known NEAT
algorithm. Furthermore, we improve upon prior state-of-the-art results on the
time series dataset utilized in our experiments.",arxiv
http://arxiv.org/abs/2104.04855v2,2021-05-23T22:26:56Z,2021-04-10T20:26:09Z,"Noise-Resilient Quantum Machine Learning for Stability Assessment of
  Power Systems","Transient stability assessment (TSA) is a cornerstone for resilient
operations of today's interconnected power grids. This paper is a confluence of
quantum computing, data science and machine learning to potentially address the
power system TSA challenge. We devise a quantum TSA (qTSA) method to enable
scalable and efficient data-driven transient stability prediction for bulk
power systems, which is the first attempt to tackle the TSA issue with quantum
computing. Our contributions are three-fold: 1) A low-depth, high
expressibility quantum neural network for accurate and noise-resilient TSA; 2)
A quantum natural gradient descent algorithm for efficient qTSA training; 3) A
systematical analysis on qTSA's performance under various quantum factors. qTSA
underpins a foundation of quantum-enabled and data-driven power grid stability
analytics. It renders the intractable TSA straightforward and effortless in the
Hilbert space, and therefore provides stability information for power system
operations. Extensive experiments on quantum simulators and real quantum
computers verify the accuracy, noise-resilience, scalability and universality
of qTSA.",arxiv
http://arxiv.org/abs/2104.09684v1,2021-04-19T23:28:32Z,2021-04-19T23:28:32Z,"Transfer learning suppresses simulation bias in predictive models built
  from sparse, multi-modal data","Many problems in science, engineering, and business require making
predictions based on very few observations. To build a robust predictive model,
these sparse data may need to be augmented with simulated data, especially when
the design space is multidimensional. Simulations, however, often suffer from
an inherent bias. Estimation of this bias may be poorly constrained not only
because of data sparsity, but also because traditional predictive models fit
only one type of observations, such as scalars or images, instead of all
available data modalities, which might have been acquired and simulated at
great cost. We combine recent developments in deep learning to build more
robust predictive models from multimodal data with a recent, novel technique to
suppress the bias, and extend it to take into account multiple data modalities.
First, an initial, simulation-trained, neural network surrogate model learns
important correlations between different data modalities and between simulation
inputs and outputs. Then, the model is partially retrained, or transfer
learned, to fit the observations. Using fewer than 10 inertial confinement
fusion experiments for retraining, we demonstrate that this technique
systematically improves simulation predictions while a simple output
calibration makes predictions worse. We also offer extensive cross-validation
with real and synthetic data to support our findings. The transfer learning
method can be applied to other problems that require transferring knowledge
from simulations to the domain of real observations. This paper opens up the
path to model calibration using multiple data types, which have traditionally
been ignored in predictive models.",arxiv
http://arxiv.org/abs/2103.00740v3,2021-03-03T03:26:02Z,2021-03-01T04:13:21Z,"Towards Enhancing Database Education: Natural Language Generation Meets
  Query Execution Plans","The database systems course is offered as part of an undergraduate computer
science degree program in many major universities. A key learning goal of
learners taking such a course is to understand how SQL queries are processed in
a RDBMS in practice. Since a query execution plan (QEP) describes the execution
steps of a query, learners can acquire the understanding by perusing the QEPs
generated by a RDBMS. Unfortunately, in practice, it is often daunting for a
learner to comprehend these QEPs containing vendor-specific implementation
details, hindering her learning process. In this paper, we present a novel,
end-to-end, generic system called lantern that generates a natural language
description of a qep to facilitate understanding of the query execution steps.
It takes as input an SQL query and its QEP, and generates a natural language
description of the execution strategy deployed by the underlying RDBMS.
Specifically, it deploys a declarative framework called pool that enables
subject matter experts to efficiently create and maintain natural language
descriptions of physical operators used in QEPs. A rule-based framework called
RULE-LANTERN is proposed that exploits pool to generate natural language
descriptions of QEPs. Despite the high accuracy of RULE-LANTERN, our engagement
with learners reveal that, consistent with existing psychology theories,
perusing such rule-based descriptions lead to boredom due to repetitive
statements across different QEPs. To address this issue, we present a novel
deep learning-based language generation framework called NEURAL-LANTERN that
infuses language variability in the generated description by exploiting a set
of paraphrasing tools and word embedding. Our experimental study with real
learners shows the effectiveness of lantern in facilitating comprehension of
QEPs.",arxiv
http://arxiv.org/abs/2107.13473v2,2021-07-30T20:49:01Z,2021-07-28T16:29:58Z,"The Portiloop: a deep learning-based open science tool for closed-loop
  brain stimulation","Electroencephalography (EEG) is a method of measuring the brain's electrical
activity, using non-invasive scalp electrodes. In this article, we propose the
Portiloop, a deep learning-based portable and low-cost device enabling the
neuroscience community to capture EEG, process it in real time, detect patterns
of interest, and respond with precisely-timed stimulation. The core of the
Portiloop is a System on Chip composed of an Analog to Digital Converter (ADC)
and a Field-Programmable Gate Array (FPGA). After being converted to digital by
the ADC, the EEG signal is processed in the FPGA. The FPGA contains an ad-hoc
Artificial Neural Network (ANN) with convolutional and recurrent units,
directly implemented in hardware. The output of the ANN is then used to trigger
the user-defined feedback. We use the Portiloop to develop a real-time sleep
spindle stimulating application, as a case study. Sleep spindles are a specific
type of transient oscillation ($\sim$2.5 s, 12-16 Hz) that are observed in EEG
recordings, and are related to memory consolidation during sleep. We tested the
Portiloop's capacity to detect and stimulate sleep spindles in real time using
an existing database of EEG sleep recordings. With 71% for both precision and
recall as compared with expert labels, the system is able to stimulate spindles
within $\sim$300 ms of their onset, enabling experimental manipulation of early
the entire spindle. The Portiloop can be extended to detect and stimulate other
neural events in EEG. It is fully available to the research community as an
open science project.",arxiv
http://arxiv.org/abs/1908.10407v2,2019-09-02T10:43:50Z,2019-08-27T18:50:50Z,"An Energy Approach to the Solution of Partial Differential Equations in
  Computational Mechanics via Machine Learning: Concepts, Implementation and
  Applications","Partial Differential Equations (PDE) are fundamental to model different
phenomena in science and engineering mathematically. Solving them is a crucial
step towards a precise knowledge of the behaviour of natural and engineered
systems. In general, in order to solve PDEs that represent real systems to an
acceptable degree, analytical methods are usually not enough. One has to resort
to discretization methods. For engineering problems, probably the best known
option is the finite element method (FEM). However, powerful alternatives such
as mesh-free methods and Isogeometric Analysis (IGA) are also available. The
fundamental idea is to approximate the solution of the PDE by means of
functions specifically built to have some desirable properties. In this
contribution, we explore Deep Neural Networks (DNNs) as an option for
approximation. They have shown impressive results in areas such as visual
recognition. DNNs are regarded here as function approximation machines. There
is great flexibility to define their structure and important advances in the
architecture and the efficiency of the algorithms to implement them make DNNs a
very interesting alternative to approximate the solution of a PDE. We
concentrate in applications that have an interest for Computational Mechanics.
Most contributions that have decided to explore this possibility have adopted a
collocation strategy. In this contribution, we concentrate in mechanical
problems and analyze the energetic format of the PDE. The energy of a
mechanical system seems to be the natural loss function for a machine learning
method to approach a mechanical problem. As proofs of concept, we deal with
several problems and explore the capabilities of the method for applications in
engineering.",arxiv
http://arxiv.org/abs/2107.03603v1,2021-07-08T04:52:50Z,2021-07-08T04:52:50Z,"CLAIM: Curriculum Learning Policy for Influence Maximization in Unknown
  Social Networks","Influence maximization is the problem of finding a small subset of nodes in a
network that can maximize the diffusion of information. Recently, it has also
found application in HIV prevention, substance abuse prevention, micro-finance
adoption, etc., where the goal is to identify the set of peer leaders in a
real-world physical social network who can disseminate information to a large
group of people. Unlike online social networks, real-world networks are not
completely known, and collecting information about the network is costly as it
involves surveying multiple people. In this paper, we focus on this problem of
network discovery for influence maximization. The existing work in this
direction proposes a reinforcement learning framework. As the environment
interactions in real-world settings are costly, so it is important for the
reinforcement learning algorithms to have minimum possible environment
interactions, i.e, to be sample efficient. In this work, we propose CLAIM -
Curriculum LeArning Policy for Influence Maximization to improve the sample
efficiency of RL methods. We conduct experiments on real-world datasets and
show that our approach can outperform the current best approach.",arxiv
http://arxiv.org/abs/2006.04258v2,2021-02-23T18:09:22Z,2020-06-07T20:40:47Z,"Investigating Estimated Kolmogorov Complexity as a Means of
  Regularization for Link Prediction","Link prediction in graphs is an important task in the fields of network
science and machine learning. We investigate a flexible means of regularization
for link prediction based on an approximation of the Kolmogorov complexity of
graphs that is differentiable and compatible with recent advances in link
prediction algorithms. Informally, the Kolmogorov complexity of an object is
the length of the shortest computer program that produces the object. Complex
networks are often generated, in part, by simple mechanisms; for example, many
citation networks and social networks are approximately scale-free and can be
explained by preferential attachment. A preference for predicting graphs with
simpler generating mechanisms motivates our choice of Kolmogorov complexity as
a regularization term. In our experiments the regularization method shows good
performance on many diverse real-world networks, however we determine that this
is likely due to an aggregation method rather than any actual estimation of
Kolmogorov complexity.",arxiv
http://arxiv.org/abs/1909.05654v1,2019-09-05T18:12:05Z,2019-09-05T18:12:05Z,"What can computational models learn from human selective attention? A
  review from an audiovisual crossmodal perspective","Selective attention plays an essential role in information acquisition and
utilization from the environment. In the past 50 years, research on selective
attention has been a central topic in cognitive science. Compared with unimodal
studies, crossmodal studies are more complex but necessary to solve real-world
challenges in both human experiments and computational modeling. Although an
increasing number of findings on crossmodal selective attention have shed light
on humans' behavioral patterns and neural underpinnings, a much better
understanding is still necessary to yield the same benefit for computational
intelligent agents. This article reviews studies of selective attention in
unimodal visual and auditory and crossmodal audiovisual setups from the
multidisciplinary perspectives of psychology and cognitive neuroscience, and
evaluates different ways to simulate analogous mechanisms in computational
models and robotics. We discuss the gaps between these fields in this
interdisciplinary review and provide insights about how to use psychological
findings and theories in artificial intelligence from different perspectives.",arxiv
http://arxiv.org/abs/2106.13955v1,2021-06-26T06:47:41Z,2021-06-26T06:47:41Z,Autonomous Deep Quality Monitoring in Streaming Environments,"The common practice of quality monitoring in industry relies on manual
inspection well-known to be slow, error-prone and operator-dependent. This
issue raises strong demand for automated real-time quality monitoring developed
from data-driven approaches thus alleviating from operator dependence and
adapting to various process uncertainties. Nonetheless, current approaches do
not take into account the streaming nature of sensory information while relying
heavily on hand-crafted features making them application-specific. This paper
proposes the online quality monitoring methodology developed from recently
developed deep learning algorithms for data streams, Neural Networks with
Dynamically Evolved Capacity (NADINE), namely NADINE++. It features the
integration of 1-D and 2-D convolutional layers to extract natural features of
time-series and visual data streams captured from sensors and cameras of the
injection molding machines from our own project. Real-time experiments have
been conducted where the online quality monitoring task is simulated on the fly
under the prequential test-then-train fashion - the prominent data stream
evaluation protocol. Comparison with the state-of-the-art techniques clearly
exhibits the advantage of NADINE++ with 4.68\% improvement on average for the
quality monitoring task in streaming environments. To support the reproducible
research initiative, codes, results of NADINE++ along with supplementary
materials and injection molding dataset are made available in
\url{https://github.com/ContinualAL/NADINE-IJCNN2021}.",arxiv
http://arxiv.org/abs/2011.11305v1,2020-11-23T10:05:50Z,2020-11-23T10:05:50Z,"Industrial object, machine part and defect recognition towards fully
  automated industrial monitoring employing deep learning. The case of
  multilevel VGG19","Modern industry requires modern solutions for monitoring the automatic
production of goods. Smart monitoring of the functionality of the mechanical
parts of technology systems or machines is mandatory for a fully automatic
production process. Although Deep Learning has been advancing, allowing for
real-time object detection and other tasks, little has been investigated about
the effectiveness of specially designed Convolutional Neural Networks for
defect detection and industrial object recognition. In the particular study, we
employed six publically available industrial-related datasets containing defect
materials and industrial tools or engine parts, aiming to develop a specialized
model for pattern recognition. Motivated by the recent success of the Virtual
Geometry Group (VGG) network, we propose a modified version of it, called
Multipath VGG19, which allows for more local and global feature extraction,
while the extra features are fused via concatenation. The experiments verified
the effectiveness of MVGG19 over the traditional VGG19. Specifically, top
classification performance was achieved in five of the six image datasets,
while the average classification improvement was 6.95%.",arxiv
http://arxiv.org/abs/1908.01456v1,2019-08-05T03:45:17Z,2019-08-05T03:45:17Z,"A Deep Learning Approach for Tweet Classification and Rescue Scheduling
  for Effective Disaster Management","It is a challenging and complex task to acquire information from different
regions of a disaster-affected area in a timely fashion. The extensive spread
and reach of social media and networks allow people to share information in
real-time. However, the processing of social media data and gathering of
valuable information require a series of operations such as (1) processing each
specific tweet for a text classification, (2) possible location determination
of people needing help based on tweets, and (3) priority calculations of rescue
tasks based on the classification of tweets. These are three primary challenges
in developing an effective rescue scheduling operation using social media data.
In this paper, first, we propose a deep learning model combining attention
based Bi-directional Long Short-Term Memory (BLSTM) and Convolutional Neural
Network (CNN) to classify the tweets under different categories. We use
pre-trained crisis word vectors and global vectors for word representation
(GLoVe) for capturing semantic meaning from tweets. Next, we perform feature
engineering to create an auxiliary feature map which dramatically increases the
model accuracy. In our experiments using real data sets from Hurricanes Harvey
and Irma, it is observed that our proposed approach performs better compared to
other classification methods based on Precision, Recall, F1-score, and
Accuracy, and is highly effective to determine the correct priority of a tweet.
Furthermore, to evaluate the effectiveness and robustness of the proposed
classification model a merged dataset comprises of 4 different datasets from
CrisisNLP and another 15 different disasters data from CrisisLex are used.
Finally, we develop an adaptive multitask hybrid scheduling algorithm
considering resource constraints to perform an effective rescue scheduling
operation considering different rescue priorities.",arxiv
http://arxiv.org/abs/2009.05429v2,2021-01-06T18:29:42Z,2020-09-11T13:28:26Z,"Embodied Visual Navigation with Automatic Curriculum Learning in Real
  Environments","We present NavACL, a method of automatic curriculum learning tailored to the
navigation task. NavACL is simple to train and efficiently selects relevant
tasks using geometric features. In our experiments, deep reinforcement learning
agents trained using NavACL significantly outperform state-of-the-art agents
trained with uniform sampling -- the current standard. Furthermore, our agents
can navigate through unknown cluttered indoor environments to
semantically-specified targets using only RGB images. Obstacle-avoiding
policies and frozen feature networks support transfer to unseen real-world
environments, without any modification or retraining requirements. We evaluate
our policies in simulation, and in the real world on a ground robot and a
quadrotor drone. Videos of real-world results are available in the
supplementary material.",arxiv
http://arxiv.org/abs/2007.05616v1,2020-07-10T21:29:23Z,2020-07-10T21:29:23Z,NaviGAN: A Generative Approach for Socially Compliant Navigation,"Robots navigating in human crowds need to optimize their paths not only for
their task performance but also for their compliance to social norms. One of
the key challenges in this context is the lack of standard metrics for
evaluating and optimizing a socially compliant behavior. Existing works in
social navigation can be grouped according to the differences in their
optimization objectives. For instance, the reinforcement learning approaches
tend to optimize on the \textit{comfort} aspect of the socially compliant
navigation, whereas the inverse reinforcement learning approaches are designed
to achieve \textit{natural} behavior. In this paper, we propose NaviGAN, a
generative navigation algorithm that jointly optimizes both of the
\textit{comfort} and \textit{naturalness} aspects. Our approach is designed as
an adversarial training framework that can learn to generate a navigation path
that is both optimized for achieving a goal and for complying with latent
social rules. A set of experiments has been carried out on multiple datasets to
demonstrate the strengths of the proposed approach quantitatively. We also
perform extensive experiments using a physical robot in a real-world
environment to qualitatively evaluate the trained social navigation behavior.
The video recordings of the robot experiments can be found in the link:
https://youtu.be/61blDymjCpw.",arxiv
http://arxiv.org/abs/1111.6473v1,2011-11-28T15:28:53Z,2011-11-28T15:28:53Z,A kernel-based framework for learning graded relations from data,"Driven by a large number of potential applications in areas like
bioinformatics, information retrieval and social network analysis, the problem
setting of inferring relations between pairs of data objects has recently been
investigated quite intensively in the machine learning community. To this end,
current approaches typically consider datasets containing crisp relations, so
that standard classification methods can be adopted. However, relations between
objects like similarities and preferences are often expressed in a graded
manner in real-world applications. A general kernel-based framework for
learning relations from data is introduced here. It extends existing approaches
because both crisp and graded relations are considered, and it unifies existing
approaches because different types of graded relations can be modeled,
including symmetric and reciprocal relations. This framework establishes
important links between recent developments in fuzzy set theory and machine
learning. Its usefulness is demonstrated through various experiments on
synthetic and real-world data.",arxiv
http://arxiv.org/abs/1903.05431v1,2019-03-13T11:54:04Z,2019-03-13T11:54:04Z,"Resource Abstraction for Reinforcement Learning in Multiagent Congestion
  Problems","Real-world congestion problems (e.g. traffic congestion) are typically very
complex and large-scale. Multiagent reinforcement learning (MARL) is a
promising candidate for dealing with this emerging complexity by providing an
autonomous and distributed solution to these problems. However, there are three
limiting factors that affect the deployability of MARL approaches to congestion
problems. These are learning time, scalability and decentralised coordination
i.e. no communication between the learning agents. In this paper we introduce
Resource Abstraction, an approach that addresses these challenges by allocating
the available resources into abstract groups. This abstraction creates new
reward functions that provide a more informative signal to the learning agents
and aid the coordination amongst them. Experimental work is conducted on two
benchmark domains from the literature, an abstract congestion problem and a
realistic traffic congestion problem. The current state-of-the-art for solving
multiagent congestion problems is a form of reward shaping called difference
rewards. We show that the system using Resource Abstraction significantly
improves the learning speed and scalability, and achieves the highest possible
or near-highest joint performance/social welfare for both congestion problems
in large-scale scenarios involving up to 1000 reinforcement learning agents.",arxiv
http://arxiv.org/abs/1702.08626v1,2017-02-28T03:16:40Z,2017-02-28T03:16:40Z,"Show, Attend and Interact: Perceivable Human-Robot Social Interaction
  through Neural Attention Q-Network","For a safe, natural and effective human-robot social interaction, it is
essential to develop a system that allows a robot to demonstrate the
perceivable responsive behaviors to complex human behaviors. We introduce the
Multimodal Deep Attention Recurrent Q-Network using which the robot exhibits
human-like social interaction skills after 14 days of interacting with people
in an uncontrolled real world. Each and every day during the 14 days, the
system gathered robot interaction experiences with people through a
hit-and-trial method and then trained the MDARQN on these experiences using
end-to-end reinforcement learning approach. The results of interaction based
learning indicate that the robot has learned to respond to complex human
behaviors in a perceivable and socially acceptable manner.",arxiv
http://arxiv.org/abs/2006.14897v2,2020-08-20T05:47:55Z,2020-06-26T10:22:57Z,"Hop Sampling: A Simple Regularized Graph Learning for Non-Stationary
  Environments","Graph representation learning is gaining popularity in a wide range of
applications, such as social networks analysis, computational biology, and
recommender systems. However, different with positive results from many
academic studies, applying graph neural networks (GNNs) in a real-world
application is still challenging due to non-stationary environments. The
underlying distribution of streaming data changes unexpectedly, resulting in
different graph structures (a.k.a., concept drift). Therefore, it is essential
to devise a robust graph learning technique so that the model does not overfit
to the training graphs. In this work, we present Hop Sampling, a
straightforward regularization method that can effectively prevent GNNs from
overfishing. The hop sampling randomly selects the number of propagation steps
rather than fixing it, and by doing so, it encourages the model to learn
meaningful node representation for all intermediate propagation layers and to
experience a variety of plausible graphs that are not in the training set.
Particularly, we describe the use case of our method in recommender systems, a
representative example of the real-world non-stationary case. We evaluated hop
sampling on a large-scale real-world LINE dataset and conducted an online A/B/n
test in LINE Coupon recommender systems of LINE Wallet Tab. Experimental
results demonstrate that the proposed scheme improves the prediction accuracy
of GNNs. We observed hop sampling provides 7.97% and 16.93% improvements for
NDCG and MAP compared to non-regularized GNN models in our online service.
Furthermore, models using hop sampling alleviate the oversmoothing issue in
GNNs enabling a deeper model as well as more diversified representation.",arxiv
http://arxiv.org/abs/1910.07083v1,2019-09-29T22:04:19Z,2019-09-29T22:04:19Z,"Occurence of A Cyber Security Eco-System: A Nature Oriented Project and
  Evaluation of An Indirect Social Experiment","Because of todays technological developments and the influence of digital
systems into every aspect of our lives, importance of cyber security improves
more and more day-by-day. Projects, educational processes and seminars realized
for this aim create and improve awareness among individuals and provide useful
tools for growing equipped generations. The aim of this study is to focus on a
cyber security eco-system, which was self-occurred within the interactive
educational environment designed under the scope of TUBITAK 4004 Nature
Education and Science Schools Projects (with the name of A Cyber Security
Adventure) with the use of important technologies such as virtual reality,
augmented reality, and artificial intelligence. The eco-system occurred within
the interactive educational process where high school students took place
caused both students and the project team to experience an indirect social
experiment environment. In this sense, it is thought that the findings and
comments presented in the study will give important ideas to everyone involved
in cyber security education, life-long learning processes, and the technology
use in software oriented educational tools.",arxiv
http://arxiv.org/abs/2011.14172v1,2020-11-28T17:25:10Z,2020-11-28T17:25:10Z,"Thermodynamic Consistent Neural Networks for Learning Material
  Interfacial Mechanics","For multilayer materials in thin substrate systems, interfacial failure is
one of the most challenges. The traction-separation relations (TSR)
quantitatively describe the mechanical behavior of a material interface
undergoing openings, which is critical to understand and predict interfacial
failures under complex loadings. However, existing theoretical models have
limitations on enough complexity and flexibility to well learn the real-world
TSR from experimental observations. A neural network can fit well along with
the loading paths but often fails to obey the laws of physics, due to a lack of
experimental data and understanding of the hidden physical mechanism. In this
paper, we propose a thermodynamic consistent neural network (TCNN) approach to
build a data-driven model of the TSR with sparse experimental data. The TCNN
leverages recent advances in physics-informed neural networks (PINN) that
encode prior physical information into the loss function and efficiently train
the neural networks using automatic differentiation. We investigate three
thermodynamic consistent principles, i.e., positive energy dissipation,
steepest energy dissipation gradient, and energy conservative loading path. All
of them are mathematically formulated and embedded into a neural network model
with a novel defined loss function. A real-world experiment demonstrates the
superior performance of TCNN, and we find that TCNN provides an accurate
prediction of the whole TSR surface and significantly reduces the violated
prediction against the laws of physics.",arxiv
http://arxiv.org/abs/1501.01209v1,2014-12-11T05:00:26Z,2014-12-11T05:00:26Z,"Reinforcement Learning and Nonparametric Detection of Game-Theoretic
  Equilibrium Play in Social Networks","This paper studies two important signal processing aspects of equilibrium
behavior in non-cooperative games arising in social networks, namely,
reinforcement learning and detection of equilibrium play. The first part of the
paper presents a reinforcement learning (adaptive filtering) algorithm that
facilitates learning an equilibrium by resorting to diffusion cooperation
strategies in a social network. Agents form homophilic social groups, within
which they exchange past experiences over an undirected graph. It is shown
that, if all agents follow the proposed algorithm, their global behavior is
attracted to the correlated equilibria set of the game. The second part of the
paper provides a test to detect if the actions of agents are consistent with
play from the equilibrium of a concave potential game. The theory of revealed
preference from microeconomics is used to construct a non-parametric decision
test and statistical test which only require the probe and associated actions
of agents. A stochastic gradient algorithm is given to optimize the probe in
real time to minimize the Type-II error probabilities of the detection test
subject to specified Type-I error probability. We provide a real-world example
using the energy market, and a numerical example to detect malicious agents in
an online social network.",arxiv
http://arxiv.org/abs/1801.06482v1,2018-01-19T16:27:36Z,2018-01-19T16:27:36Z,"Deep Learning for Detecting Cyberbullying Across Multiple Social Media
  Platforms","Harassment by cyberbullies is a significant phenomenon on the social media.
Existing works for cyberbullying detection have at least one of the following
three bottlenecks. First, they target only one particular social media platform
(SMP). Second, they address just one topic of cyberbullying. Third, they rely
on carefully handcrafted features of the data. We show that deep learning based
models can overcome all three bottlenecks. Knowledge learned by these models on
one dataset can be transferred to other datasets. We performed extensive
experiments using three real-world datasets: Formspring (12k posts), Twitter
(16k posts), and Wikipedia(100k posts). Our experiments provide several useful
insights about cyberbullying detection. To the best of our knowledge, this is
the first work that systematically analyzes cyberbullying detection on various
topics across multiple SMPs using deep learning based models and transfer
learning.",arxiv
http://arxiv.org/abs/1803.03324v1,2018-03-08T22:20:00Z,2018-03-08T22:20:00Z,Learning Deep Generative Models of Graphs,"Graphs are fundamental data structures which concisely capture the relational
structure in many important real-world domains, such as knowledge graphs,
physical and social interactions, language, and chemistry. Here we introduce a
powerful new approach for learning generative models over graphs, which can
capture both their structure and attributes. Our approach uses graph neural
networks to express probabilistic dependencies among a graph's nodes and edges,
and can, in principle, learn distributions over any arbitrary graph. In a
series of experiments our results show that once trained, our models can
generate good quality samples of both synthetic graphs as well as real
molecular graphs, both unconditionally and conditioned on data. Compared to
baselines that do not use graph-structured representations, our models often
perform far better. We also explore key challenges of learning generative
models of graphs, such as how to handle symmetries and ordering of elements
during the graph generation process, and offer possible solutions. Our work is
the first and most general approach for learning generative models over
arbitrary graphs, and opens new directions for moving away from restrictions of
vector- and sequence-like knowledge representations, toward more expressive and
flexible relational data structures.",arxiv
http://arxiv.org/abs/1612.06699v3,2017-06-12T21:38:17Z,2016-12-20T15:04:38Z,Unsupervised Perceptual Rewards for Imitation Learning,"Reward function design and exploration time are arguably the biggest
obstacles to the deployment of reinforcement learning (RL) agents in the real
world. In many real-world tasks, designing a reward function takes considerable
hand engineering and often requires additional sensors to be installed just to
measure whether the task has been executed successfully. Furthermore, many
interesting tasks consist of multiple implicit intermediate steps that must be
executed in sequence. Even when the final outcome can be measured, it does not
necessarily provide feedback on these intermediate steps. To address these
issues, we propose leveraging the abstraction power of intermediate visual
representations learned by deep models to quickly infer perceptual reward
functions from small numbers of demonstrations. We present a method that is
able to identify key intermediate steps of a task from only a handful of
demonstration sequences, and automatically identify the most discriminative
features for identifying these steps. This method makes use of the features in
a pre-trained deep model, but does not require any explicit specification of
sub-goals. The resulting reward functions can then be used by an RL agent to
learn to perform the task in real-world settings. To evaluate the learned
reward, we present qualitative results on two real-world tasks and a
quantitative evaluation against a human-designed reward function. We also show
that our method can be used to learn a real-world door opening skill using a
real robot, even when the demonstration used for reward learning is provided by
a human using their own hand. To our knowledge, these are the first results
showing that complex robotic manipulation skills can be learned directly and
without supervised labels from a video of a human performing the task.
Supplementary material and data are available at
https://sermanet.github.io/rewards",arxiv
http://arxiv.org/abs/2105.02254v1,2021-05-05T18:00:55Z,2021-05-05T18:00:55Z,"ConsisRec: Enhancing GNN for Social Recommendation via Consistent
  Neighbor Aggregation","Social recommendation aims to fuse social links with user-item interactions
to alleviate the cold-start problem for rating prediction. Recent developments
of Graph Neural Networks (GNNs) motivate endeavors to design GNN-based social
recommendation frameworks to aggregate both social and user-item interaction
information simultaneously. However, most existing methods neglect the social
inconsistency problem, which intuitively suggests that social links are not
necessarily consistent with the rating prediction process. Social inconsistency
can be observed from both context-level and relation-level. Therefore, we
intend to empower the GNN model with the ability to tackle the social
inconsistency problem. We propose to sample consistent neighbors by relating
sampling probability with consistency scores between neighbors. Besides, we
employ the relation attention mechanism to assign consistent relations with
high importance factors for aggregation. Experiments on two real-world datasets
verify the model effectiveness.",arxiv
http://arxiv.org/abs/2003.00627v2,2020-03-23T18:46:08Z,2020-03-02T01:55:05Z,Cluster-Based Social Reinforcement Learning,"Social Reinforcement Learning methods, which model agents in large networks,
are useful for fake news mitigation, personalized teaching/healthcare, and
viral marketing, but it is challenging to incorporate inter-agent dependencies
into the models effectively due to network size and sparse interaction data.
Previous social RL approaches either ignore agents dependencies or model them
in a computationally intensive manner. In this work, we incorporate agent
dependencies efficiently in a compact model by clustering users (based on their
payoff and contribution to the goal) and combine this with a method to easily
derive personalized agent-level policies from cluster-level policies. We also
propose a dynamic clustering approach that captures changing user behavior.
Experiments on real-world datasets illustrate that our proposed approach learns
more accurate policy estimates and converges more quickly, compared to several
baselines that do not use agent correlations or only use static clusters.",arxiv
http://arxiv.org/abs/1907.12508v2,2020-04-27T03:37:11Z,2019-07-29T16:20:00Z,"Tackling Ordinal Regression Problem for Heterogeneous Data: Sparse and
  Deep Multi-Task Learning Approaches","Many real-world datasets are labeled with natural orders, i.e., ordinal
labels. Ordinal regression is a method to predict ordinal labels that finds a
wide range of applications in data-rich domains, such as natural, health and
social sciences. Most existing ordinal regression approaches work well for
independent and identically distributed (IID) instances via formulating a
single ordinal regression task. However, for heterogeneous non-IID instances
with well-defined local geometric structures, e.g., subpopulation groups,
multi-task learning (MTL) provides a promising framework to encode task
(subgroup) relatedness, bridge data from all tasks, and simultaneously learn
multiple related tasks in efforts to improve generalization performance. Even
though MTL methods have been extensively studied, there is barely existing work
investigating MTL for heterogeneous data with ordinal labels. We tackle this
important problem via sparse and deep multi-task approaches. Specifically, we
develop a regularized multi-task ordinal regression (MTOR) model for smaller
datasets and a deep neural networks based MTOR model for large-scale datasets.
We evaluate the performance using three real-world healthcare datasets with
applications to multi-stage disease progression diagnosis. Our experiments
indicate that the proposed MTOR models markedly improve the prediction
performance comparing with single-task ordinal regression models.",arxiv
http://arxiv.org/abs/2110.06674v1,2021-10-13T12:18:09Z,2021-10-13T12:18:09Z,Truthful AI: Developing and governing AI that does not lie,"In many contexts, lying -- the use of verbal falsehoods to deceive -- is
harmful. While lying has traditionally been a human affair, AI systems that
make sophisticated verbal statements are becoming increasingly prevalent. This
raises the question of how we should limit the harm caused by AI ""lies"" (i.e.
falsehoods that are actively selected for). Human truthfulness is governed by
social norms and by laws (against defamation, perjury, and fraud). Differences
between AI and humans present an opportunity to have more precise standards of
truthfulness for AI, and to have these standards rise over time. This could
provide significant benefits to public epistemics and the economy, and mitigate
risks of worst-case AI futures.
  Establishing norms or laws of AI truthfulness will require significant work
to: (1) identify clear truthfulness standards; (2) create institutions that can
judge adherence to those standards; and (3) develop AI systems that are
robustly truthful.
  Our initial proposals for these areas include: (1) a standard of avoiding
""negligent falsehoods"" (a generalisation of lies that is easier to assess); (2)
institutions to evaluate AI systems before and after real-world deployment; and
(3) explicitly training AI systems to be truthful via curated datasets and
human interaction.
  A concerning possibility is that evaluation mechanisms for eventual
truthfulness standards could be captured by political interests, leading to
harmful censorship and propaganda. Avoiding this might take careful attention.
And since the scale of AI speech acts might grow dramatically over the coming
decades, early truthfulness standards might be particularly important because
of the precedents they set.",arxiv
http://arxiv.org/abs/1912.03455v1,2019-12-07T07:36:10Z,2019-12-07T07:36:10Z,Digital Twin: Acquiring High-Fidelity 3D Avatar from a Single Image,"We present an approach to generate high fidelity 3D face avatar with a
high-resolution UV texture map from a single image. To estimate the face
geometry, we use a deep neural network to directly predict vertex coordinates
of the 3D face model from the given image. The 3D face geometry is further
refined by a non-rigid deformation process to more accurately capture facial
landmarks before texture projection. A key novelty of our approach is to train
the shape regression network on facial images synthetically generated using a
high-quality rendering engine. Moreover, our shape estimator fully leverages
the discriminative power of deep facial identity features learned from millions
of facial images. We have conducted extensive experiments to demonstrate the
superiority of our optimized 2D-to-3D rendering approach, especially its
excellent generalization property on real-world selfie images. Our proposed
system of rendering 3D avatars from 2D images has a wide range of applications
from virtual/augmented reality (VR/AR) and telepsychiatry to human-computer
interaction and social networks.",arxiv
http://arxiv.org/abs/1106.0134v1,2011-06-01T09:56:49Z,2011-06-01T09:56:49Z,"ProDiGe: PRioritization Of Disease Genes with multitask machine learning
  from positive and unlabeled examples","Elucidating the genetic basis of human diseases is a central goal of genetics
and molecular biology. While traditional linkage analysis and modern
high-throughput techniques often provide long lists of tens or hundreds of
disease gene candidates, the identification of disease genes among the
candidates remains time-consuming and expensive. Efficient computational
methods are therefore needed to prioritize genes within the list of candidates,
by exploiting the wealth of information available about the genes in various
databases. Here we propose ProDiGe, a novel algorithm for Prioritization of
Disease Genes. ProDiGe implements a novel machine learning strategy based on
learning from positive and unlabeled examples, which allows to integrate
various sources of information about the genes, to share information about
known disease genes across diseases, and to perform genome-wide searches for
new disease genes. Experiments on real data show that ProDiGe outperforms
state-of-the-art methods for the prioritization of genes in human diseases.",arxiv
http://arxiv.org/abs/1811.06537v1,2018-11-15T18:59:28Z,2018-11-15T18:59:28Z,"Predicting enterprise cyber incidents using social network analysis on
  the darkweb hacker forums","With rise in security breaches over the past few years, there has been an
increasing need to mine insights from social media platforms to raise alerts of
possible attacks in an attempt to defend conflict during competition. We use
information from the darkweb forums by leveraging the reply network structure
of user interactions with the goal of predicting enterprise cyber attacks. We
use a suite of social network features on top of supervised learning models and
validate them on a binary classification problem that attempts to predict
whether there would be an attack on any given day for an organization. We
conclude from our experiments using information from 53 forums in the darkweb
over a span of 12 months to predict real world organization cyber attacks of 2
different security events that analyzing the path structure between groups of
users is better than just studying network centralities like Pagerank or
relying on the user posting statistics in the forums.",arxiv
http://arxiv.org/abs/1111.6843v4,2012-08-23T14:25:44Z,2011-11-29T15:23:42Z,"Understanding the Social Cascading of Geekspeak and the Upshots for
  Social Cognitive Systems","Barring swarm robotics, a substantial share of current machine-human and
machine-machine learning and interaction mechanisms are being developed and fed
by results of agent-based computer simulations, game-theoretic models, or
robotic experiments based on a dyadic communication pattern. Yet, in real life,
humans no less frequently communicate in groups, and gain knowledge and take
decisions basing on information cumulatively gleaned from more than one single
source. These properties should be taken into consideration in the design of
autonomous artificial cognitive systems construed to interact with learn from
more than one contact or 'neighbour'. To this end, significant practical import
can be gleaned from research applying strict science methodology to human and
social phenomena, e.g. to discovery of realistic creativity potential spans, or
the 'exposure thresholds' after which new information could be accepted by a
cognitive agent. The results will be presented of a project analysing the
social propagation of neologisms in a microblogging service. From local,
low-level interactions and information flows between agents inventing and
imitating discrete lexemes we aim to describe the processes of the emergence of
more global systemic order and dynamics, using the latest methods of complexity
science. Whether in order to mimic them, or to 'enhance' them, parameters
gleaned from complexity science approaches to humans' social and humanistic
behaviour should subsequently be incorporated as points of reference in the
field of robotics and human-machine interaction.",arxiv
http://arxiv.org/abs/2012.15739v2,2021-09-22T15:46:10Z,2020-12-23T02:17:09Z,"Uncertainty Bounds for Multivariate Machine Learning Predictions on
  High-Strain Brittle Fracture","Simulation of the crack network evolution on high strain rate impact
experiments performed in brittle materials is very compute-intensive. The cost
increases even more if multiple simulations are needed to account for the
randomness in crack length, location, and orientation, which is inherently
found in real-world materials. Constructing a machine learning emulator can
make the process faster by orders of magnitude. There has been little work,
however, on assessing the error associated with their predictions. Estimating
these errors is imperative for meaningful overall uncertainty quantification.
In this work, we extend the heteroscedastic uncertainty estimates to bound a
multiple output machine learning emulator. We find that the response prediction
is accurate within its predicted errors, but with a somewhat conservative
estimate of uncertainty.",arxiv
http://arxiv.org/abs/2005.09681v3,2021-08-24T04:33:45Z,2020-05-19T18:05:20Z,Weakly Supervised Representation Learning with Coarse Labels,"With the development of computational power and techniques for data
collection, deep learning demonstrates a superior performance over most
existing algorithms on visual benchmark data sets. Many efforts have been
devoted to studying the mechanism of deep learning. One important observation
is that deep learning can learn the discriminative patterns from raw materials
directly in a task-dependent manner. Therefore, the representations obtained by
deep learning outperform hand-crafted features significantly. However, for some
real-world applications, it is too expensive to collect the task-specific
labels, such as visual search in online shopping. Compared to the limited
availability of these task-specific labels, their coarse-class labels are much
more affordable, but representations learned from them can be suboptimal for
the target task. To mitigate this challenge, we propose an algorithm to learn
the fine-grained patterns for the target task, when only its coarse-class
labels are available. More importantly, we provide a theoretical guarantee for
this. Extensive experiments on real-world data sets demonstrate that the
proposed method can significantly improve the performance of learned
representations on the target task, when only coarse-class information is
available for training. Code is available at
\url{https://github.com/idstcv/CoIns}.",arxiv
http://arxiv.org/abs/1711.05697v4,2019-07-21T22:00:26Z,2017-11-15T17:48:35Z,Motif-based Convolutional Neural Network on Graphs,"This paper introduces a generalization of Convolutional Neural Networks
(CNNs) to graphs with irregular linkage structures, especially heterogeneous
graphs with typed nodes and schemas. We propose a novel spatial convolution
operation to model the key properties of local connectivity and translation
invariance, using high-order connection patterns or motifs. We develop a novel
deep architecture Motif-CNN that employs an attention model to combine the
features extracted from multiple patterns, thus effectively capturing
high-order structural and feature information. Our experiments on
semi-supervised node classification on real-world social networks and multiple
representative heterogeneous graph datasets indicate significant gains of 6-21%
over existing graph CNNs and other state-of-the-art techniques.",arxiv
http://arxiv.org/abs/2107.02639v1,2021-07-06T14:24:43Z,2021-07-06T14:24:43Z,Multi-Level Graph Contrastive Learning,"Graph representation learning has attracted a surge of interest recently,
whose target at learning discriminant embedding for each node in the graph.
Most of these representation methods focus on supervised learning and heavily
depend on label information. However, annotating graphs are expensive to obtain
in the real world, especially in specialized domains (i.e. biology), as it
needs the annotator to have the domain knowledge to label the graph. To
approach this problem, self-supervised learning provides a feasible solution
for graph representation learning. In this paper, we propose a Multi-Level
Graph Contrastive Learning (MLGCL) framework for learning robust representation
of graph data by contrasting space views of graphs. Specifically, we introduce
a novel contrastive view - topological and feature space views. The original
graph is first-order approximation structure and contains uncertainty or error,
while the $k$NN graph generated by encoding features preserves high-order
proximity. Thus $k$NN graph generated by encoding features not only provide a
complementary view, but is more suitable to GNN encoder to extract discriminant
representation. Furthermore, we develop a multi-level contrastive mode to
preserve the local similarity and semantic similarity of graph-structured data
simultaneously. Extensive experiments indicate MLGCL achieves promising results
compared with the existing state-of-the-art graph representation learning
methods on seven datasets.",arxiv
http://arxiv.org/abs/2109.11898v1,2021-09-24T11:44:15Z,2021-09-24T11:44:15Z,"Graph Learning Augmented Heterogeneous Graph Neural Network for Social
  Recommendation","Social recommendation based on social network has achieved great success in
improving the performance of recommendation system. Since social network
(user-user relations) and user-item interactions are both naturally represented
as graph-structured data, Graph Neural Networks (GNNs) have thus been widely
applied for social recommendation. In this work, we propose an end-to-end
heterogeneous global graph learning framework, namely Graph Learning Augmented
Heterogeneous Graph Neural Network (GL-HGNN) for social recommendation. GL-HGNN
aims to learn a heterogeneous global graph that makes full use of user-user
relations, user-item interactions and item-item similarities in a unified
perspective. To this end, we design a Graph Learner (GL) method to learn and
optimize user-user and item-item connections separately. Moreover, we employ a
Heterogeneous Graph Neural Network (HGNN) to capture the high-order complex
semantic relations from our learned heterogeneous global graph. To scale up the
computation of graph learning, we further present the Anchor-based Graph
Learner (AGL) to reduce computational complexity. Extensive experiments on four
real-world datasets demonstrate the effectiveness of our model.",arxiv
http://arxiv.org/abs/1407.3392v2,2015-07-17T20:37:02Z,2014-07-12T14:37:01Z,From Social Network to Semantic Social Network in Recommender System,"Due the success of emerging Web 2.0, and different social network Web sites
such as Amazon and movie lens, recommender systems are creating unprecedented
opportunities to help people browsing the web when looking for relevant
information, and making choices. Generally, these recommender systems are
classified in three categories: content based, collaborative filtering, and
hybrid based recommendation systems. Usually, these systems employ standard
recommendation methods such as artificial neural networks, nearest neighbor, or
Bayesian networks. However, these approaches are limited compared to methods
based on web applications, such as social networks or semantic web. In this
paper, we propose a novel approach for recommendation systems called semantic
social recommendation systems that enhance the analysis of social networks
exploiting the power of semantic social network analysis. Experiments on
real-world data from Amazon examine the quality of our recommendation method as
well as the performance of our recommendation algorithms.",arxiv
http://arxiv.org/abs/2103.12516v1,2021-01-14T13:34:34Z,2021-01-14T13:34:34Z,"Edge-Cloud Collaboration Enabled Video Service Enhancement: A Hybrid
  Human-Artificial Intelligence Scheme","In this paper, a video service enhancement strategy is investigated under an
edge-cloud collaboration framework, where video caching and delivery decisions
are made in the cloud and edge respectively. We aim to guarantee the user
fairness in terms of video coding rate under statistical delay constraint and
edge caching capacity constraint. A hybrid human-artificial intelligence
approach is developed to improve the user hit rate for video caching.
Specifically, individual user interest is first characterized by merging
factorization machine (FM) model and multi-layer perceptron (MLP) model, where
both low-order and high-order features can be well learned simultaneously.
Thereafter, a social aware similarity model is constructed to transferred
individual user interest to group interest, based on which, videos can be
selected to cache. Furthermore, a double bisection exploration scheme is
proposed to optimize wireless resource allocation and video coding rate. The
effectiveness of the proposed video caching scheme and video delivery scheme is
finally validated by extensive experiments with a real-world data set.",arxiv
http://arxiv.org/abs/1809.06537v1,2018-09-18T05:26:40Z,2018-09-18T05:26:40Z,Automatic Judgment Prediction via Legal Reading Comprehension,"Automatic judgment prediction aims to predict the judicial results based on
case materials. It has been studied for several decades mainly by lawyers and
judges, considered as a novel and prospective application of artificial
intelligence techniques in the legal field. Most existing methods follow the
text classification framework, which fails to model the complex interactions
among complementary case materials. To address this issue, we formalize the
task as Legal Reading Comprehension according to the legal scenario. Following
the working protocol of human judges, LRC predicts the final judgment results
based on three types of information, including fact description, plaintiffs'
pleas, and law articles. Moreover, we propose a novel LRC model, AutoJudge,
which captures the complex semantic interactions among facts, pleas, and laws.
In experiments, we construct a real-world civil case dataset for LRC.
Experimental results on this dataset demonstrate that our model achieves
significant improvement over state-of-the-art models. We will publish all
source codes and datasets of this work on \urlgithub.com for further research.",arxiv
http://arxiv.org/abs/1908.05429v1,2019-08-15T05:56:25Z,2019-08-15T05:56:25Z,Domain-adversarial Network Alignment,"Network alignment is a critical task to a wide variety of fields. Many
existing works leverage on representation learning to accomplish this task
without eliminating domain representation bias induced by domain-dependent
features, which yield inferior alignment performance. This paper proposes a
unified deep architecture (DANA) to obtain a domain-invariant representation
for network alignment via an adversarial domain classifier. Specifically, we
employ the graph convolutional networks to perform network embedding under the
domain adversarial principle, given a small set of observed anchors. Then, the
semi-supervised learning framework is optimized by maximizing a posterior
probability distribution of observed anchors and the loss of a domain
classifier simultaneously. We also develop a few variants of our model, such
as, direction-aware network alignment, weight-sharing for directed networks and
simplification of parameter space. Experiments on three real-world social
network datasets demonstrate that our proposed approaches achieve
state-of-the-art alignment results.",arxiv
http://arxiv.org/abs/2011.11081v1,2020-11-22T18:30:23Z,2020-11-22T18:30:23Z,"Deep learning model trained on mobile phone-acquired frozen section
  images effectively detects basal cell carcinoma","Background: Margin assessment of basal cell carcinoma using the frozen
section is a common task of pathology intraoperative consultation. Although
frequently straight-forward, the determination of the presence or absence of
basal cell carcinoma on the tissue sections can sometimes be challenging. We
explore if a deep learning model trained on mobile phone-acquired frozen
section images can have adequate performance for future deployment. Materials
and Methods: One thousand two hundred and forty-one (1241) images of frozen
sections performed for basal cell carcinoma margin status were acquired using
mobile phones. The photos were taken at 100x magnification (10x objective). The
images were downscaled from a 4032 x 3024 pixel resolution to 576 x 432 pixel
resolution. Semantic segmentation algorithm Deeplab V3 with Xception backbone
was used for model training. Results: The model uses an image as input and
produces a 2-dimensional black and white output of prediction of the same
dimension; the areas determined to be basal cell carcinoma were displayed with
white color, in a black background. Any output with the number of white pixels
exceeding 0.5% of the total number of pixels is deemed positive for basal cell
carcinoma. On the test set, the model achieves area under curve of 0.99 for
receiver operator curve and 0.97 for precision-recall curve at the pixel level.
The accuracy of classification at the slide level is 96%. Conclusions: The deep
learning model trained with mobile phone images shows satisfactory performance
characteristics, and thus demonstrates the potential for deploying as a mobile
phone app to assist in frozen section interpretation in real time.",arxiv
http://arxiv.org/abs/1904.01698v1,2019-04-02T22:55:43Z,2019-04-02T22:55:43Z,VRGym: A Virtual Testbed for Physical and Interactive AI,"We propose VRGym, a virtual reality testbed for realistic human-robot
interaction. Different from existing toolkits and virtual reality environments,
the VRGym emphasizes on building and training both physical and interactive
agents for robotics, machine learning, and cognitive science. VRGym leverages
mechanisms that can generate diverse 3D scenes with high realism through
physics-based simulation. We demonstrate that VRGym is able to (i) collect
human interactions and fine manipulations, (ii) accommodate various robots with
a ROS bridge, (iii) support experiments for human-robot interaction, and (iv)
provide toolkits for training the state-of-the-art machine learning algorithms.
We hope VRGym can help to advance general-purpose robotics and machine learning
agents, as well as assisting human studies in the field of cognitive science.",arxiv
http://arxiv.org/abs/adap-org/9807003v1,1998-07-17T01:07:50Z,1998-07-17T01:07:50Z,Development and Evolution of Neural Networks in an Artificial Chemistry,"We present a model of decentralized growth for Artificial Neural Networks
(ANNs) inspired by the development and the physiology of real nervous systems.
In this model, each individual artificial neuron is an autonomous unit whose
behavior is determined only by the genetic information it harbors and local
concentrations of substrates modeled by a simple artificial chemistry. Gene
expression is manifested as axon and dendrite growth, cell division and
differentiation, substrate production and cell stimulation. We demonstrate the
model's power with a hand-written genome that leads to the growth of a simple
network which performs classical conditioning. To evolve more complex
structures, we implemented a platform-independent, asynchronous, distributed
Genetic Algorithm (GA) that allows users to participate in evolutionary
experiments via the World Wide Web.",arxiv
http://arxiv.org/abs/2106.11686v1,2021-06-22T11:40:00Z,2021-06-22T11:40:00Z,Simulation-Driven COVID-19 Epidemiological Modeling with Social Media,"Modern Bayesian approaches and workflows emphasize in how simulation is
important in the context of model developing. Simulation can help researchers
understand how the model behaves in a controlled setting and can be used to
stress the model in different ways before it is exposed to any real data. This
improved understanding could be beneficial in epidemiological models, specially
when dealing with COVID-19. Unfortunately, few researchers perform any
simulations. We present a simulation algorithm that implements a simple
agent-based model for disease transmission that works with a standard
compartment epidemiological model for COVID-19. Our algorithm can be applied in
different parameterizations to reflect several plausible epidemic scenarios.
Additionally, we also model how social media information in the form of daily
symptom mentions can be incorporate into COVID-19 epidemiological models. We
test our social media COVID-19 model with two experiments. The first using
simulated data from our agent-based simulation algorithm and the second with
real data using a machine learning tweet classifier to identify tweets that
mention symptoms from noise. Our results shows how a COVID-19 model can be (1)
used to incorporate social media data and (2) assessed and evaluated with
simulated and real data.",arxiv
http://arxiv.org/abs/1810.03078v1,2018-10-07T03:31:10Z,2018-10-07T03:31:10Z,Graphlet Count Estimation via Convolutional Neural Networks,"Graphlets are defined as k-node connected induced subgraph patterns. For an
undirected graph, 3-node graphlets include close triangle and open triangle.
When k = 4, there are six types of graphlets, e.g., tailed-triangle and clique
are two possible 4-node graphlets. The number of each graphlet, called graphlet
count, is a signature which characterizes the local network structure of a
given graph. Graphlet count plays a prominent role in network analysis of many
fields, most notably bioinformatics and social science.
  However, computing exact graphlet count is inherently difficult and
computational expensive because the number of graphlets grows exponentially
large as the graph size and/or graphlet size k grow. To deal with this
difficulty, many sampling methods were proposed to estimate graphlet count with
bounded error. Nevertheless, these methods require large number of samples to
be statistically reliable, which is still computationally demanding. Moreover,
they have to repeat laborious counting procedure even if a new graph is similar
or exactly the same as previous studied graphs.
  Intuitively, learning from historic graphs can make estimation more accurate
and avoid many repetitive counting to reduce computational cost. Based on this
idea, we propose a convolutional neural network (CNN) framework and two
preprocessing techniques to estimate graphlet count. Extensive experiments on
two types of random graphs and real world biochemistry graphs show that our
framework can offer substantial speedup on estimating graphlet count of new
graphs with high accuracy.",arxiv
http://arxiv.org/abs/1504.00416v1,2015-04-01T23:46:07Z,2015-04-01T23:46:07Z,Nonnegative Multi-level Network Factorization for Latent Factor Analysis,"Nonnegative Matrix Factorization (NMF) aims to factorize a matrix into two
optimized nonnegative matrices and has been widely used for unsupervised
learning tasks such as product recommendation based on a rating matrix.
However, although networks between nodes with the same nature exist, standard
NMF overlooks them, e.g., the social network between users. This problem leads
to comparatively low recommendation accuracy because these networks are also
reflections of the nature of the nodes, such as the preferences of users in a
social network. Also, social networks, as complex networks, have many different
structures. Each structure is a composition of links between nodes and reflects
the nature of nodes, so retaining the different network structures will lead to
differences in recommendation performance. To investigate the impact of these
network structures on the factorization, this paper proposes four multi-level
network factorization algorithms based on the standard NMF, which integrates
the vertical network (e.g., rating matrix) with the structures of horizontal
network (e.g., user social network). These algorithms are carefully designed
with corresponding convergence proofs to retain four desired network
structures. Experiments on synthetic data show that the proposed algorithms are
able to preserve the desired network structures as designed. Experiments on
real-world data show that considering the horizontal networks improves the
accuracy of document clustering and recommendation with standard NMF, and
various structures show their differences in performance on these two tasks.
These results can be directly used in document clustering and recommendation
systems.",arxiv
http://arxiv.org/abs/2005.00825v1,2020-05-02T13:02:54Z,2020-05-02T13:02:54Z,"SIGVerse: A cloud-based VR platform for research on social and embodied
  human-robot interaction","Common sense and social interaction related to daily-life environments are
considerably important for autonomous robots, which support human activities.
One of the practical approaches for acquiring such social interaction skills
and semantic information as common sense in human activity is the application
of recent machine learning techniques. Although recent machine learning
techniques have been successful in realizing automatic manipulation and driving
tasks, it is difficult to use these techniques in applications that require
human-robot interaction experience. Humans have to perform several times over a
long term to show embodied and social interaction behaviors to robots or
learning systems. To address this problem, we propose a cloud-based immersive
virtual reality (VR) platform which enables virtual human-robot interaction to
collect the social and embodied knowledge of human activities in a variety of
situations. To realize the flexible and reusable system, we develop a real-time
bridging mechanism between ROS and Unity, which is one of the standard
platforms for developing VR applications. We apply the proposed system to a
robot competition field named RoboCup@Home to confirm the feasibility of the
system in a realistic human-robot interaction scenario. Through demonstration
experiments at the competition, we show the usefulness and potential of the
system for the development and evaluation of social intelligence through
human-robot interaction. The proposed VR platform enables robot systems to
collect social experiences with several users in a short time. The platform
also contributes in providing a dataset of social behaviors, which would be a
key aspect for intelligent service robots to acquire social interaction skills
based on machine learning techniques.",arxiv
http://arxiv.org/abs/2009.07436v2,2021-07-08T13:30:36Z,2020-09-16T02:50:22Z,Weakly-Supervised Online Hashing,"With the rapid development of social websites, recent years have witnessed an
explosive growth of social images with user-provided tags which continuously
arrive in a streaming fashion. Due to the fast query speed and low storage
cost, hashing-based methods for image search have attracted increasing
attention. However, existing hashing methods for social image retrieval are
based on batch mode which violates the nature of social images, i.e., social
images are usually generated periodically or collected in a stream fashion.
Although there exist many online image hashing methods, they either adopt
unsupervised learning which ignore the relevant tags, or are designed in the
supervised manner which needs high-quality labels. In this paper, to overcome
the above limitations, we propose a new method named Weakly-supervised Online
Hashing (WOH). In order to learn high-quality hash codes, WOH exploits the weak
supervision by considering the semantics of tags and removing the noise.
Besides, We develop a discrete online optimization algorithm for WOH, which is
efficient and scalable. Extensive experiments conducted on two real-world
datasets demonstrate the superiority of WOH compared with several
state-of-the-art hashing baselines.",arxiv
http://arxiv.org/abs/1802.03725v2,2018-11-07T17:20:05Z,2018-02-11T11:20:49Z,A Generative Model for Dynamic Networks with Applications,"Networks observed in real world like social networks, collaboration networks
etc., exhibit temporal dynamics, i.e. nodes and edges appear and/or disappear
over time. In this paper, we propose a generative, latent space based,
statistical model for such networks (called dynamic networks). We consider the
case where the number of nodes is fixed, but the presence of edges can vary
over time. Our model allows the number of communities in the network to be
different at different time steps. We use a neural network based methodology to
perform approximate inference in the proposed model and its simplified version.
Experiments done on synthetic and real world networks for the task of community
detection and link prediction demonstrate the utility and effectiveness of our
model as compared to other similar existing approaches.",arxiv
http://arxiv.org/abs/0903.1193v3,2009-06-19T22:22:29Z,2009-03-06T11:56:54Z,Reality as Simplicity,"The aim of this paper is to study the relevance of simplicity and its formal
representation as Kolmogorov or algorithmic complexity in the cognitive
sciences. The discussion is based on two premises: 1) all human experience is
generated in the brain, 2) the brain has only access to information. Taken
together, these two premises lead us to conclude that all the elements of what
we call `reality' are derived mental constructs based on information and
compression, i.e., algorithmic models derived from the search for simplicity in
data. Naturally, these premises apply to humans in real or virtual environments
as well as robots or other cognitive systems. Based on this, it is further
hypothesized that there is a hierarchy of processing levels where simplicity
and compression play a major role. As applications, I illustrate first the
relevance of compression and simplicity in fundamental neuroscience with an
analysis of the Mismatch Negativity paradigm. Then I discuss the applicability
to Presence research, which studies how to produce real-feeling experiences in
mediated interaction, and use Bayesian modeling to define in a formal way
different aspects of the illusion of Presence. The idea is put forth that given
alternative models (interpretations) for a given mediated interaction, a brain
will select the simplest one it can construct weighted by prior models. In the
final section the universality of these ideas and applications in robotics,
machine learning, biology and education is discussed. I emphasize that there is
a common conceptual thread based on the idea of simplicity, which suggests a
common study approach.",arxiv
http://arxiv.org/abs/2101.02390v3,2021-03-27T11:45:02Z,2021-01-07T06:15:07Z,SDGNN: Learning Node Representation for Signed Directed Networks,"Network embedding is aimed at mapping nodes in a network into low-dimensional
vector representations. Graph Neural Networks (GNNs) have received widespread
attention and lead to state-of-the-art performance in learning node
representations. However, most GNNs only work in unsigned networks, where only
positive links exist. It is not trivial to transfer these models to signed
directed networks, which are widely observed in the real world yet less
studied. In this paper, we first review two fundamental sociological theories
(i.e., status theory and balance theory) and conduct empirical studies on
real-world datasets to analyze the social mechanism in signed directed
networks. Guided by related sociological theories, we propose a novel Signed
Directed Graph Neural Networks model named SDGNN to learn node embeddings for
signed directed networks. The proposed model simultaneously reconstructs link
signs, link directions, and signed directed triangles. We validate our model's
effectiveness on five real-world datasets, which are commonly used as the
benchmark for signed network embedding. Experiments demonstrate the proposed
model outperforms existing models, including feature-based methods, network
embedding methods, and several GNN methods.",arxiv
http://arxiv.org/abs/1903.02174v1,2019-03-06T04:58:43Z,2019-03-06T04:58:43Z,Graph Neural Networks for User Identity Linkage,"The increasing popularity and diversity of social media sites has encouraged
more and more people to participate in multiple online social networks to enjoy
their services. Each user may create a user identity to represent his or her
unique public figure in every social network. User identity linkage across
online social networks is an emerging task and has attracted increasing
attention, which could potentially impact various domains such as
recommendations and link predictions. The majority of existing work focuses on
mining network proximity or user profile data for discovering user identity
linkages. With the recent advancements in graph neural networks (GNNs), it
provides great potential to advance user identity linkage since users are
connected in social graphs, and learning latent factors of users and items is
the key. However, predicting user identity linkages based on GNNs faces
challenges. For example, the user social graphs encode both \textit{local}
structure such as users' neighborhood signals, and \textit{global} structure
with community properties. To address these challenges simultaneously, in this
paper, we present a novel graph neural network framework ({\m}) for user
identity linkage. In particular, we provide a principled approach to jointly
capture local and global information in the user-user social graph and propose
the framework {\m}, which jointly learning user representations for user
identity linkage. Extensive experiments on real-world datasets demonstrate the
effectiveness of the proposed framework.",arxiv
http://arxiv.org/abs/1902.07243v2,2019-11-23T01:47:49Z,2019-02-19T19:22:54Z,Graph Neural Networks for Social Recommendation,"In recent years, Graph Neural Networks (GNNs), which can naturally integrate
node information and topological structure, have been demonstrated to be
powerful in learning on graph data. These advantages of GNNs provide great
potential to advance social recommendation since data in social recommender
systems can be represented as user-user social graph and user-item graph; and
learning latent factors of users and items is the key. However, building social
recommender systems based on GNNs faces challenges. For example, the user-item
graph encodes both interactions and their associated opinions; social relations
have heterogeneous strengths; users involve in two graphs (e.g., the user-user
social graph and the user-item graph). To address the three aforementioned
challenges simultaneously, in this paper, we present a novel graph neural
network framework (GraphRec) for social recommendations. In particular, we
provide a principled approach to jointly capture interactions and opinions in
the user-item graph and propose the framework GraphRec, which coherently models
two graphs and heterogeneous strengths. Extensive experiments on two real-world
datasets demonstrate the effectiveness of the proposed framework GraphRec. Our
code is available at \url{https://github.com/wenqifan03/GraphRec-WWW19}",arxiv
http://arxiv.org/abs/1905.13612v1,2019-05-31T13:35:34Z,2019-05-31T13:35:34Z,Leveraging Trust and Distrust in Recommender Systems via Deep Learning,"The data scarcity of user preferences and the cold-start problem often appear
in real-world applications and limit the recommendation accuracy of
collaborative filtering strategies. Leveraging the selections of social friends
and foes can efficiently face both problems. In this study, we propose a
strategy that performs social deep pairwise learning. Firstly, we design a
ranking loss function incorporating multiple ranking criteria based on the
choice in users, and the choice in their friends and foes to improve the
accuracy in the top-k recommendation task. We capture the nonlinear
correlations between user preferences and the social information of trust and
distrust relationships via a deep learning strategy. In each backpropagation
step, we follow a social negative sampling strategy to meet the multiple
ranking criteria of our ranking loss function. We conduct comprehensive
experiments on a benchmark dataset from Epinions, among the largest publicly
available that has been reported in the relevant literature. The experimental
results demonstrate that the proposed model beats other state-of-the art
methods, attaining an 11.49% average improvement over the most competitive
model. We show that our deep learning strategy plays an important role in
capturing the nonlinear correlations between user preferences and the social
information of trust and distrust relationships, and demonstrate the importance
of our social negative sampling strategy on the proposed model.",arxiv
http://arxiv.org/abs/1907.11625v5,2020-02-20T08:17:06Z,2019-07-08T19:59:40Z,"Influence maximization in unknown social networks: Learning Policies for
  Effective Graph Sampling","A serious challenge when finding influential actors in real-world social
networks is the lack of knowledge about the structure of the underlying
network. Current state-of-the-art methods rely on hand-crafted sampling
algorithms; these methods sample nodes and their neighbours in a carefully
constructed order and choose opinion leaders from this discovered network to
maximize influence spread in the (unknown) complete network. In this work, we
propose a reinforcement learning framework for network discovery that
automatically learns useful node and graph representations that encode
important structural properties of the network. At training time, the method
identifies portions of the network such that the nodes selected from this
sampled subgraph can effectively influence nodes in the complete network. The
realization of such transferable network structure based adaptable policies is
attributed to the meticulous design of the framework that encodes relevant node
and graph signatures driven by an appropriate reward scheme. We experiment with
real-world social networks from four different domains and show that the
policies learned by our RL agent provide a 10-36% improvement over the current
state-of-the-art method.",arxiv
http://arxiv.org/abs/2001.03855v1,2020-01-12T05:25:02Z,2020-01-12T05:25:02Z,"Hyperparameters optimization for Deep Learning based emotion prediction
  for Human Robot Interaction","To enable humanoid robots to share our social space we need to develop
technology for easy interaction with the robots using multiple modes such as
speech, gestures and share our emotions with them. We have targeted this
research towards addressing the core issue of emotion recognition problem which
would require less computation resources and much lesser number of network
hyperparameters which will be more adaptive to be computed on low resourced
social robots for real time communication. More specifically, here we have
proposed an Inception module based Convolutional Neural Network Architecture
which has achieved improved accuracy of upto 6% improvement over the existing
network architecture for emotion classification when combinedly tested over
multiple datasets when tried over humanoid robots in real - time. Our proposed
model is reducing the trainable Hyperparameters to an extent of 94% as compared
to vanilla CNN model which clearly indicates that it can be used in real time
based application such as human robot interaction. Rigorous experiments have
been performed to validate our methodology which is sufficiently robust and
could achieve high level of accuracy. Finally, the model is implemented in a
humanoid robot, NAO in real time and robustness of the model is evaluated.",arxiv
http://arxiv.org/abs/2106.03233v1,2021-06-06T20:06:44Z,2021-06-06T20:06:44Z,A Pre-training Oracle for Predicting Distances in Social Networks,"In this paper, we propose a novel method to make distance predictions in
real-world social networks. As predicting missing distances is a difficult
problem, we take a two-stage approach. Structural parameters for families of
synthetic networks are first estimated from a small set of measurements of a
real-world network and these synthetic networks are then used to pre-train the
predictive neural networks. Since our model first searches for the most
suitable synthetic graph parameters which can be used as an ""oracle"" to create
arbitrarily large training data sets, we call our approach ""Oracle Search
Pre-training"" (OSP). For example, many real-world networks exhibit a Power law
structure in their node degree distribution, so a Power law model can provide a
foundation for the desired oracle to generate synthetic pre-training networks,
if the appropriate Power law graph parameters can be estimated. Accordingly, we
conduct experiments on real-world Facebook, Email, and Train Bombing networks
and show that OSP outperforms models without pre-training, models pre-trained
with inaccurate parameters, and other distance prediction schemes such as
Low-rank Matrix Completion. In particular, we achieve a prediction error of
less than one hop with only 1% of sampled distances from the social network.
OSP can be easily extended to other domains such as random networks by choosing
an appropriate model to generate synthetic training data, and therefore
promises to impact many different network learning problems.",arxiv
http://arxiv.org/abs/2008.03226v1,2020-06-28T20:59:03Z,2020-06-28T20:59:03Z,"The Photoswitch Dataset: A Molecular Machine Learning Benchmark for the
  Advancement of Synthetic Chemistry","The space of synthesizable molecules is greater than $10^{60}$, meaning only
a vanishingly small fraction of these molecules have ever been realized in the
lab. In order to prioritize which regions of this space to explore next,
synthetic chemists need access to accurate molecular property predictions.
While great advances in molecular machine learning have been made, there is a
dearth of benchmarks featuring properties that are useful for the synthetic
chemist. Focussing directly on the needs of the synthetic chemist, we introduce
the Photoswitch Dataset, a new benchmark for molecular machine learning where
improvements in model performance can be immediately observed in the throughput
of promising molecules synthesized in the lab. Photoswitches are a versatile
class of molecule for medical and renewable energy applications where a
molecule's efficacy is governed by its electronic transition wavelengths. We
demonstrate superior performance in predicting these wavelengths compared to
both time-dependent density functional theory (TD-DFT), the incumbent first
principles quantum mechanical approach, as well as a panel of human experts.
Our baseline models are currently being deployed in the lab as part of the
decision process for candidate synthesis. It is our hope that this benchmark
can drive real discoveries in photoswitch chemistry and that future benchmarks
can be introduced to pivot learning algorithm development to benefit more
expansive areas of synthetic chemistry.",arxiv
http://arxiv.org/abs/2110.07728v1,2021-10-07T17:48:57Z,2021-10-07T17:48:57Z,Pre-training Molecular Graph Representation with 3D Geometry,"Molecular graph representation learning is a fundamental problem in modern
drug and material discovery. Molecular graphs are typically modeled by their 2D
topological structures, but it has been recently discovered that 3D geometric
information plays a more vital role in predicting molecular functionalities.
However, the lack of 3D information in real-world scenarios has significantly
impeded the learning of geometric graph representation. To cope with this
challenge, we propose the Graph Multi-View Pre-training (GraphMVP) framework
where self-supervised learning (SSL) is performed by leveraging the
correspondence and consistency between 2D topological structures and 3D
geometric views. GraphMVP effectively learns a 2D molecular graph encoder that
is enhanced by richer and more discriminative 3D geometry. We further provide
theoretical insights to justify the effectiveness of GraphMVP. Finally,
comprehensive experiments show that GraphMVP can consistently outperform
existing graph SSL methods.",arxiv
http://arxiv.org/abs/1905.06945v1,2018-11-19T02:22:56Z,2018-11-19T02:22:56Z,"Uncertainty quantification of molecular property prediction using
  Bayesian neural network models","In chemistry, deep neural network models have been increasingly utilized in a
variety of applications such as molecular property predictions, novel molecule
designs, and planning chemical reactions. Despite the rapid increase in the use
of state-of-the-art models and algorithms, deep neural network models often
produce poor predictions in real applications because model performance is
highly dependent on the quality of training data. In the field of molecular
analysis, data are mostly obtained from either complicated chemical experiments
or approximate mathematical equations, and then quality of data may be
questioned.In this paper, we quantify uncertainties of prediction using
Bayesian neural networks in molecular property predictions. We estimate both
model-driven and data-driven uncertainties, demonstrating the usefulness of
uncertainty quantification as both a quality checker and a confidence indicator
with the three experiments. Our results manifest that uncertainty
quantification is necessary for more reliable molecular applications and
Bayesian neural network models can be a practical approach.",arxiv
http://arxiv.org/abs/1809.03632v1,2018-09-10T23:20:00Z,2018-09-10T23:20:00Z,Detecting Gang-Involved Escalation on Social Media Using Context,"Gang-involved youth in cities such as Chicago have increasingly turned to
social media to post about their experiences and intents online. In some
situations, when they experience the loss of a loved one, their online
expression of emotion may evolve into aggression towards rival gangs and
ultimately into real-world violence. In this paper, we present a novel system
for detecting Aggression and Loss in social media. Our system features the use
of domain-specific resources automatically derived from a large unlabeled
corpus, and contextual representations of the emotional and semantic content of
the user's recent tweets as well as their interactions with other users.
Incorporating context in our Convolutional Neural Network (CNN) leads to a
significant improvement.",arxiv
http://arxiv.org/abs/2012.14325v1,2020-12-22T09:54:04Z,2020-12-22T09:54:04Z,Digital me ontology and ethics,"This paper addresses ontology and ethics of an AI agent called digital me. We
define digital me as autonomous, decision-making, and learning agent,
representing an individual and having practically immortal own life. It is
assumed that digital me is equipped with the big-five personality model,
ensuring that it provides a model of some aspects of a strong AI:
consciousness, free will, and intentionality. As computer-based personality
judgments are more accurate than those made by humans, digital me can judge the
personality of the individual represented by the digital me, other individuals'
personalities, and other digital me-s. We describe seven ontological qualities
of digital me: a) double-layer status of Digital Being versus digital me, b)
digital me versus real me, c) mind-digital me and body-digital me, d) digital
me versus doppelganger (shadow digital me), e) non-human time concept, f)
social quality, g) practical immortality. We argue that with the advancement of
AI's sciences and technologies, there exist two digital me thresholds. The
first threshold defines digital me having some (rudimentarily) form of
consciousness, free will, and intentionality. The second threshold assumes that
digital me is equipped with moral learning capabilities, implying that, in
principle, digital me could develop their own ethics which significantly
differs from human's understanding of ethics. Finally we discuss the
implications of digital me metaethics, normative and applied ethics, the
implementation of the Golden Rule in digital me-s, and we suggest two sets of
normative principles for digital me: consequentialist and duty based digital me
principles.",arxiv
http://arxiv.org/abs/2101.06919v1,2021-01-18T08:09:53Z,2021-01-18T08:09:53Z,Unsupervised Link and Unlink Prediction on Dynamic Networks,"Understanding and characterizing the process deriving the creation and
dissolution of social interactions is a fundamental challenge for social
network analysis. In the dynamic setting, it is essential to be able to, given
the collection of link states of the network in a certain period, accurately
predict the link and unlink states of the network in a future time. Addressing
this task is more complicated compared to its static counterpart especially for
increasingly large networks due to the prohibitive expensiveness of
computational complexity. Consequently, mainstreams of current researches in
unsupervised settings ignore the temporal information. Additionally, only a few
approaches study on unlink prediction, which is also important to understand
the evolution of social networks.
  In this work, we address such mining tasks by unsupervised learning, and
propose a model for link and unlink prediction with temporal information
(LUPT). Given a sequence of snapshots of network over time, LUPT utilizes the
spectral diffusion by variants of local random walks to calculate the
probability vector started from each node at each snapshot. Then, it calculates
the similarity score for each of the nodes by the probability vectors of all
the previous snapshots. Finally, LUPT predicts the link and unlink states by
ranking the similarity scores according to the link and unlink tasks,
respectively. Experiments on real-world networks demonstrate that LUPT provides
superior results compared to the baseline methods in both link and unlink
prediction tasks.",arxiv
http://arxiv.org/abs/1703.07823v2,2017-06-19T20:59:29Z,2017-03-22T19:09:12Z,Fake News Mitigation via Point Process Based Intervention,"We propose the first multistage intervention framework that tackles fake news
in social networks by combining reinforcement learning with a point process
network activity model. The spread of fake news and mitigation events within
the network is modeled by a multivariate Hawkes process with additional
exogenous control terms. By choosing a feature representation of states,
defining mitigation actions and constructing reward functions to measure the
effectiveness of mitigation activities, we map the problem of fake news
mitigation into the reinforcement learning framework. We develop a policy
iteration method unique to the multivariate networked point process, with the
goal of optimizing the actions for maximal total reward under budget
constraints. Our method shows promising performance in real-time intervention
experiments on a Twitter network to mitigate a surrogate fake news campaign,
and outperforms alternatives on synthetic datasets.",arxiv
http://arxiv.org/abs/2109.05821v1,2021-09-13T09:46:50Z,2021-09-13T09:46:50Z,Cyber-Security in the Emerging World of Smart Everything,"The fourth industrial revolution (4IR) is a revolution many authors believe
have come to stay. It is a revolution that has been fast blurring the line
between physical, digital and biological technologies. These disruptive
technologies largely rely on high-speed internet connectivity, Cloud
technologies, Augmented Reality, Additive Manufacturing, Data science and
Artificial Intelligence. Most developed economies have embraced the it while
the developing economies are struggling to adopt 4IR because they lack the
requisite skills, knowledge and technology. Thus, this study investigates
Nigeria as one of the developing economies to understand her readiness for 4IR
and the level of preparedness to mitigate the sophisticated cyber-attacks that
comes with it. The investigation adopted quantitative research approach and
developed an online questionnaire that was shared amongst the population of
interest that includes academic, industry experts and relevant stakeholders.
The questionnaire returned 116 valid responses which were analysed with
descriptive statistical tools in SPSS. Results suggest that 60 of the
respondents opined that Nigerian government at are not showing enough evidence
to demonstrate her preparedness to leverage these promised potentials by
developing 4IR relevant laws, strong institutional frameworks and policies.
They lack significant development capacity to mitigate risks associated with
digital ecosystem and cyber ecosystem that are ushered in by the 4IR. In the
universities, 52 of the courses offered at the undergraduate and 42 at the
post-graduate levels are relevant in the development of skills required in the
revolution. The study recommends that the government at all levels make
adequate efforts in developing the countrys intangible assets. In all, this
paper posits that successful implementation of these could equip Nigeria to
embrace the 4IR in all its aspects.",arxiv
http://arxiv.org/abs/2001.02214v1,2020-01-07T18:26:38Z,2020-01-07T18:26:38Z,"Attributed Multi-Relational Attention Network for Fact-checking URL
  Recommendation","To combat fake news, researchers mostly focused on detecting fake news and
journalists built and maintained fact-checking sites (e.g., Snopes.com and
Politifact.com). However, fake news dissemination has been greatly promoted via
social media sites, and these fact-checking sites have not been fully utilized.
To overcome these problems and complement existing methods against fake news,
in this paper we propose a deep-learning based fact-checking URL recommender
system to mitigate impact of fake news in social media sites such as Twitter
and Facebook. In particular, our proposed framework consists of a
multi-relational attentive module and a heterogeneous graph attention network
to learn complex/semantic relationship between user-URL pairs, user-user pairs,
and URL-URL pairs. Extensive experiments on a real-world dataset show that our
proposed framework outperforms eight state-of-the-art recommendation models,
achieving at least 3~5.3% improvement.",arxiv
http://arxiv.org/abs/2105.01976v1,2021-05-05T11:01:27Z,2021-05-05T11:01:27Z,"GRAPHOPT: constrained optimization-based parallelization of irregular
  graphs","Sparse, irregular graphs show up in various applications like linear algebra,
machine learning, engineering simulations, robotic control, etc. These graphs
have a high degree of parallelism, but their execution on parallel threads of
modern platforms remains challenging due to the irregular data dependencies.
The parallel execution performance can be improved by efficiently partitioning
the graphs such that the communication and thread synchronization overheads are
minimized without hurting the utilization of the threads. To achieve this, this
paper proposes GRAPHOPT, a tool that models the graph parallelization as a
constrained optimization problem and uses the open Google OR-Tools solver to
find good partitions. Several scalability techniques are developed to handle
large real-world graphs with millions of nodes and edges. Extensive experiments
are performed on the graphs of sparse matrix triangular solves (linear algebra)
and sum-product networks (machine learning). GRAPHOPT achieves an average
speedup of 1.4x and 6.1x over the Intel Math Kernel Library and a commonly-used
heuristic partitioning technique, respectively, demonstrating the effectiveness
of the constrained optimization-based graph parallelization.",arxiv
http://arxiv.org/abs/1802.06108v3,2019-12-31T20:08:44Z,2018-02-16T20:22:41Z,"Modeling the Formation of Social Conventions from Embodied Real-Time
  Interactions","What is the role of real-time control and learning in the formation of social
conventions? To answer this question, we propose a computational model that
matches human behavioral data in a social decision-making game that was
analyzed both in discrete-time and continuous-time setups. Furthermore, unlike
previous approaches, our model takes into account the role of sensorimotor
control loops in embodied decision-making scenarios. For this purpose, we
introduce the Control-based Reinforcement Learning (CRL) model. CRL is grounded
in the Distributed Adaptive Control (DAC) theory of mind and brain, where
low-level sensorimotor control is modulated through perceptual and behavioral
learning in a layered structure. CRL follows these principles by implementing a
feedback control loop handling the agent's reactive behaviors (pre-wired
reflexes), along with an adaptive layer that uses reinforcement learning to
maximize long-term reward. We test our model in a multi-agent game-theoretic
task in which coordination must be achieved to find an optimal solution. We
show that CRL is able to reach human-level performance on standard
game-theoretic metrics such as efficiency in acquiring rewards and fairness in
reward distribution.",arxiv
http://arxiv.org/abs/1910.07643v3,2021-01-23T02:46:40Z,2019-10-16T23:06:34Z,Dynamic Graph Convolutional Networks Using the Tensor M-Product,"Many irregular domains such as social networks, financial transactions,
neuron connections, and natural language constructs are represented using graph
structures. In recent years, a variety of graph neural networks (GNNs) have
been successfully applied for representation learning and prediction on such
graphs. In many of the real-world applications, the underlying graph changes
over time, however, most of the existing GNNs are inadequate for handling such
dynamic graphs. In this paper we propose a novel technique for learning
embeddings of dynamic graphs using a tensor algebra framework. Our method
extends the popular graph convolutional network (GCN) for learning
representations of dynamic graphs using the recently proposed tensor M-product
technique. Theoretical results presented establish a connection between the
proposed tensor approach and spectral convolution of tensors. The proposed
method TM-GCN is consistent with the Message Passing Neural Network (MPNN)
framework, accounting for both spatial and temporal message passing. Numerical
experiments on real-world datasets demonstrate the performance of the proposed
method for edge classification and link prediction tasks on dynamic graphs. We
also consider an application related to the COVID-19 pandemic, and show how our
method can be used for early detection of infected individuals from contact
tracing data.",arxiv
http://arxiv.org/abs/2105.04098v1,2021-05-10T03:58:34Z,2021-05-10T03:58:34Z,"SRLF: A Stance-aware Reinforcement Learning Framework for Content-based
  Rumor Detection on Social Media","The rapid development of social media changes the lifestyle of people and
simultaneously provides an ideal place for publishing and disseminating rumors,
which severely exacerbates social panic and triggers a crisis of social trust.
Early content-based methods focused on finding clues from the text and user
profiles for rumor detection. Recent studies combine the stances of users'
comments with news content to capture the difference between true and false
rumors. Although the user's stance is effective for rumor detection, the manual
labeling process is time-consuming and labor-intensive, which limits the
application of utilizing it to facilitate rumor detection.
  In this paper, we first finetune a pre-trained BERT model on a small labeled
dataset and leverage this model to annotate weak stance labels for users'
comment data to overcome the problem mentioned above. Then, we propose a novel
Stance-aware Reinforcement Learning Framework (SRLF) to select high-quality
labeled stance data for model training and rumor detection. Both the stance
selection and rumor detection tasks are optimized simultaneously to promote
both tasks mutually. We conduct experiments on two commonly used real-world
datasets. The experimental results demonstrate that our framework outperforms
the state-of-the-art models significantly, which confirms the effectiveness of
the proposed framework.",arxiv
http://arxiv.org/abs/1712.08636v1,2017-12-22T19:58:01Z,2017-12-22T19:58:01Z,Find the Conversation Killers: a Predictive Study of Thread-ending Posts,"How to improve the quality of conversations in online communities has
attracted considerable attention recently. Having engaged, urbane, and reactive
online conversations has a critical effect on the social life of Internet
users. In this study, we are particularly interested in identifying a post in a
multi-party conversation that is unlikely to be further replied to, which
therefore kills that thread of the conversation. For this purpose, we propose a
deep learning model called the ConverNet. ConverNet is attractive due to its
capability of modeling the internal structure of a long conversation and its
appropriate encoding of the contextual information of the conversation, through
effective integration of attention mechanisms. Empirical experiments on
real-world datasets demonstrate the effectiveness of the proposal model. For
the widely concerned topic, our analysis also offers implications for improving
the quality and user experience of online conversations.",arxiv
http://arxiv.org/abs/2109.11345v1,2021-09-21T13:42:46Z,2021-09-21T13:42:46Z,Graph Neural Netwrok with Interaction Pattern for Group Recommendation,"With the development of social platforms, people are more and more inclined
to combine into groups to participate in some activities, so group
recommendation has gradually become a problem worthy of research. For group
recommendation, an important issue is how to obtain the characteristic
representation of the group and the item through personal interaction history,
and obtain the group's preference for the item. For this problem, we proposed
the model GIP4GR (Graph Neural Network with Interaction Pattern For Group
Recommendation). Specifically, our model use the graph neural network framework
with powerful representation capabilities to represent the interaction between
group-user-items in the topological structure of the graph, and at the same
time, analyze the interaction pattern of the graph to adjust the feature output
of the graph neural network, the feature representations of groups, and items
are obtained to calculate the group's preference for items. We conducted a lot
of experiments on two real-world datasets to illustrate the superior
performance of our model.",arxiv
http://arxiv.org/abs/2003.11117v1,2020-03-24T21:17:44Z,2020-03-24T21:17:44Z,"COVID-19 and Computer Audition: An Overview on What Speech & Sound
  Analysis Could Contribute in the SARS-CoV-2 Corona Crisis","At the time of writing, the world population is suffering from more than
10,000 registered COVID-19 disease epidemic induced deaths since the outbreak
of the Corona virus more than three months ago now officially known as
SARS-CoV-2. Since, tremendous efforts have been made worldwide to counter-steer
and control the epidemic by now labelled as pandemic. In this contribution, we
provide an overview on the potential for computer audition (CA), i.e., the
usage of speech and sound analysis by artificial intelligence to help in this
scenario. We first survey which types of related or contextually significant
phenomena can be automatically assessed from speech or sound. These include the
automatic recognition and monitoring of breathing, dry and wet coughing or
sneezing sounds, speech under cold, eating behaviour, sleepiness, or pain to
name but a few. Then, we consider potential use-cases for exploitation. These
include risk assessment and diagnosis based on symptom histograms and their
development over time, as well as monitoring of spread, social distancing and
its effects, treatment and recovery, and patient wellbeing. We quickly guide
further through challenges that need to be faced for real-life usage. We come
to the conclusion that CA appears ready for implementation of (pre-)diagnosis
and monitoring tools, and more generally provides rich and significant, yet so
far untapped potential in the fight against COVID-19 spread.",arxiv
http://arxiv.org/abs/2010.04143v1,2020-10-08T17:41:26Z,2020-10-08T17:41:26Z,Deep SVBRDF Estimation on Real Materials,"Recent work has demonstrated that deep learning approaches can successfully
be used to recover accurate estimates of the spatially-varying BRDF (SVBRDF) of
a surface from as little as a single image. Closer inspection reveals, however,
that most approaches in the literature are trained purely on synthetic data,
which, while diverse and realistic, is often not representative of the richness
of the real world. In this paper, we show that training such networks
exclusively on synthetic data is insufficient to achieve adequate results when
tested on real data. Our analysis leverages a new dataset of real materials
obtained with a novel portable multi-light capture apparatus. Through an
extensive series of experiments and with the use of a novel deep learning
architecture, we explore two strategies for improving results on real data:
finetuning, and a per-material optimization procedure. We show that adapting
network weights to real data is of critical importance, resulting in an
approach which significantly outperforms previous methods for SVBRDF estimation
on real materials. Dataset and code are available at
https://lvsn.github.io/real-svbrdf",arxiv
http://arxiv.org/abs/1907.00498v4,2020-07-08T22:48:11Z,2019-06-30T23:46:30Z,"Proof of Witness Presence: Blockchain Consensus for Augmented Democracy
  in Smart Cities","Smart Cities evolve into complex and pervasive urban environments with a
citizens' mandate to meet sustainable development goals. Repositioning
democratic values of citizens' choices in these complex ecosystems has turned
out to be imperative in an era of social media filter bubbles, fake news and
opportunities for manipulating electoral results with such means. This paper
introduces a new paradigm of augmented democracy that promises actively
engaging citizens in a more informed decision-making augmented into public
urban space. The proposed concept is inspired by a digital revive of the
Ancient Agora of Athens, an arena of public discourse, a Polis where citizens
assemble to actively deliberate and collectively decide about public matters.
The core contribution of the proposed paradigm is the concept of proving
witness presence: making decision-making subject of providing secure evidence
and testifying for choices made in the physical space. This paper shows how the
challenge of proving witness presence can be tackled with blockchain consensus
to empower citizens' trust and overcome security vulnerabilities of GPS
localization. Moreover, a novel platform for collective decision-making and
crowd-sensing in urban space is introduced: Smart Agora. It is shown how
real-time collective measurements over citizens' choices can be made in a fully
decentralized and privacy-preserving way. Witness presence is tested by
deploying a decentralized system for crowd-sensing the sustainable use of
transport means. Furthermore, witness presence of cycling risk is validated
using official accident data from public authorities, which are compared
against wisdom of the crowd. The paramount role of dynamic consensus,
self-governance and ethically aligned artificial intelligence in the augmented
democracy paradigm is outlined.",arxiv
http://arxiv.org/abs/1903.04307v1,2019-03-08T15:05:50Z,2019-03-08T15:05:50Z,"A cooperative game for automated learning of elasto-plasticity knowledge
  graphs and models with AI-guided experimentation","We introduce a multi-agent meta-modeling game to generate data, knowledge,
and models that make predictions on constitutive responses of elasto-plastic
materials. We introduce a new concept from graph theory where a modeler agent
is tasked with evaluating all the modeling options recast as a directed
multigraph and find the optimal path that links the source of the directed
graph (e.g. strain history) to the target (e.g. stress) measured by an
objective function. Meanwhile, the data agent, which is tasked with generating
data from real or virtual experiments (e.g. molecular dynamics, discrete
element simulations), interacts with the modeling agent sequentially and uses
reinforcement learning to design new experiments to optimize the prediction
capacity. Consequently, this treatment enables us to emulate an idealized
scientific collaboration as selections of the optimal choices in a decision
tree search done automatically via deep reinforcement learning.",arxiv
http://arxiv.org/abs/1411.0778v1,2014-11-04T03:48:20Z,2014-11-04T03:48:20Z,"Detecting Suicidal Ideation in Chinese Microblogs with Psychological
  Lexicons","Suicide is among the leading causes of death in China. However, technical
approaches toward preventing suicide are challenging and remaining under
development. Recently, several actual suicidal cases were preceded by users who
posted microblogs with suicidal ideation to Sina Weibo, a Chinese social media
network akin to Twitter. It would therefore be desirable to detect suicidal
ideations from microblogs in real-time, and immediately alert appropriate
support groups, which may lead to successful prevention. In this paper, we
propose a real-time suicidal ideation detection system deployed over Weibo,
using machine learning and known psychological techniques. Currently, we have
identified 53 known suicidal cases who posted suicide notes on Weibo prior to
their deaths.We explore linguistic features of these known cases using a
psychological lexicon dictionary, and train an effective suicidal Weibo post
detection model. 6714 tagged posts and several classifiers are used to verify
the model. By combining both machine learning and psychological knowledge, SVM
classifier has the best performance of different classifiers, yielding an
F-measure of 68:3%, a Precision of 78:9%, and a Recall of 60:3%.",arxiv
http://arxiv.org/abs/1907.06853v1,2019-07-16T06:05:29Z,2019-07-16T06:05:29Z,Deep Social Collaborative Filtering,"Recommender systems are crucial to alleviate the information overload problem
in online worlds. Most of the modern recommender systems capture users'
preference towards items via their interactions based on collaborative
filtering techniques. In addition to the user-item interactions, social
networks can also provide useful information to understand users' preference as
suggested by the social theories such as homophily and influence. Recently,
deep neural networks have been utilized for social recommendations, which
facilitate both the user-item interactions and the social network information.
However, most of these models cannot take full advantage of the social network
information. They only use information from direct neighbors, but distant
neighbors can also provide helpful information. Meanwhile, most of these models
treat neighbors' information equally without considering the specific
recommendations. However, for a specific recommendation case, the information
relevant to the specific item would be helpful. Besides, most of these models
do not explicitly capture the neighbor's opinions to items for social
recommendations, while different opinions could affect the user differently. In
this paper, to address the aforementioned challenges, we propose DSCF, a Deep
Social Collaborative Filtering framework, which can exploit the social
relations with various aspects for recommender systems. Comprehensive
experiments on two-real world datasets show the effectiveness of the proposed
framework.",arxiv
http://arxiv.org/abs/2004.00403v1,2020-04-01T12:56:13Z,2020-04-01T12:56:13Z,Two-shot Spatially-varying BRDF and Shape Estimation,"Capturing the shape and spatially-varying appearance (SVBRDF) of an object
from images is a challenging task that has applications in both computer vision
and graphics. Traditional optimization-based approaches often need a large
number of images taken from multiple views in a controlled environment. Newer
deep learning-based approaches require only a few input images, but the
reconstruction quality is not on par with optimization techniques. We propose a
novel deep learning architecture with a stage-wise estimation of shape and
SVBRDF. The previous predictions guide each estimation, and a joint refinement
network later refines both SVBRDF and shape. We follow a practical mobile image
capture setting and use unaligned two-shot flash and no-flash images as input.
Both our two-shot image capture and network inference can run on mobile
hardware. We also create a large-scale synthetic training dataset with
domain-randomized geometry and realistic materials. Extensive experiments on
both synthetic and real-world datasets show that our network trained on a
synthetic dataset can generalize well to real-world images. Comparisons with
recent approaches demonstrate the superior performance of the proposed
approach.",arxiv
http://arxiv.org/abs/2105.11485v2,2021-05-26T13:37:55Z,2021-05-24T18:15:37Z,"Finding flares in Kepler and TESS data with recurrent deep neural
  networks","Stellar flares are an important aspect of magnetic activity -- both for
stellar evolution and circumstellar habitability viewpoints - but automatically
and accurately finding them is still a challenge to researchers in the Big Data
era of astronomy. We present an experiment to detect flares in space-borne
photometric data using deep neural networks. Using a set of artificial data and
real photometric data we trained a set of neural networks, and found that the
best performing architectures were the recurrent neural networks (RNNs) using
Long Short-Term Memory (LSTM) layers. The best trained network detected flares
over {$5\sigma$ with $\gtrsim80$\% recall and precision} and was also capable
to distinguish typical false signals (e.g. maxima of RR Lyr stars) from real
flares. Testing the network trained on Kepler data on TESS light curves showed
that the neural net is able to generalize and find flares - with similar
effectiveness - in completely new data having previously unseen sampling and
characteristics.",arxiv
http://arxiv.org/abs/2010.16356v1,2020-10-30T16:25:26Z,2020-10-30T16:25:26Z,Cooperation dynamics of generalized reciprocity on complex networks,"Recent studies suggest that the emergence of cooperative behavior can be
explained by generalized reciprocity, a behavioral mechanism based on the
principle of ""help anyone if helped by someone"". In complex systems, the
cooperative dynamics is largely determined by the network structure which
dictates the interactions among neighboring individuals. Despite an abundance
of studies, the role of the network structure in in promoting cooperation
through generalized reciprocity remains an under-explored phenomenon. In this
doctoral thesis, we utilize basic tools from the dynamical systems theory, and
develop a unifying framework for investigating the cooperation dynamics of
generalized reciprocity on complex networks. We use this framework to present a
theoretical overview on the role of generalized reciprocity in promoting
cooperation in three distinct interaction structures: i) social dilemmas, ii)
multidimensional networks, and iii) fluctuating environments. The results
suggest that cooperation through generalized reciprocity always emerges as the
unique attractor in which the overall level of cooperation is maximized, while
simultaneously exploitation of the participating individuals is prevented. The
effect of the network structure is captured by a local centrality measure which
uniquely quantifies the propensity of the network structure to cooperation, by
dictating the degree of cooperation displayed both at microscopic and
macroscopic level. As a consequence, the implementation of our results may go
beyond explaining the evolution of cooperation. In particular, they can be
directly applied in domains that deal with the development of artificial
systems able to adequately mimic reality, such as reinforcement learning.",arxiv
http://arxiv.org/abs/1312.0317v1,2013-12-02T03:21:28Z,2013-12-02T03:21:28Z,Evolutionary Dynamics of Information Diffusion over Social Networks,"Current social networks are of extremely large-scale generating tremendous
information flows at every moment. How information diffuse over social networks
has attracted much attention from both industry and academics. Most of the
existing works on information diffusion analysis are based on machine learning
methods focusing on social network structure analysis and empirical data
mining. However, the dynamics of information diffusion, which are heavily
influenced by network users' decisions, actions and their socio-economic
interactions, is generally ignored by most of existing works. In this paper, we
propose an evolutionary game theoretic framework to model the dynamic
information diffusion process in social networks. Specifically, we derive the
information diffusion dynamics in complete networks, uniform degree and
non-uniform degree networks, with the highlight of two special networks,
Erd\H{o}s-R\'enyi random network and the Barab\'asi-Albert scale-free network.
We find that the dynamics of information diffusion over these three kinds of
networks are scale-free and the same with each other when the network scale is
sufficiently large. To verify our theoretical analysis, we perform simulations
for the information diffusion over synthetic networks and real-world Facebook
networks. Moreover, we also conduct experiment on Twitter hashtags dataset,
which shows that the proposed game theoretic model can well fit and predict the
information diffusion over real social networks.",arxiv
http://arxiv.org/abs/1901.05356v1,2019-01-16T15:56:19Z,2019-01-16T15:56:19Z,"How to Host a Data Competition: Statistical Advice for Design and
  Analysis of a Data Competition","Data competitions rely on real-time leaderboards to rank competitor entries
and stimulate algorithm improvement. While such competitions have become quite
popular and prevalent, particularly in supervised learning formats, their
implementations by the host are highly variable. Without careful planning, a
supervised learning competition is vulnerable to overfitting, where the winning
solutions are so closely tuned to the particular set of provided data that they
cannot generalize to the underlying problem of interest to the host. This paper
outlines some important considerations for strategically designing relevant and
informative data sets to maximize the learning outcome from hosting a
competition based on our experience. It also describes a post-competition
analysis that enables robust and efficient assessment of the strengths and
weaknesses of solutions from different competitors, as well as greater
understanding of the regions of the input space that are well-solved. The
post-competition analysis, which complements the leaderboard, uses exploratory
data analysis and generalized linear models (GLMs). The GLMs not only expand
the range of results we can explore, they also provide more detailed analysis
of individual sub-questions including similarities and differences between
algorithms across different types of scenarios, universally easy or hard
regions of the input space, and different learning objectives. When coupled
with a strategically planned data generation approach, the methods provide
richer and more informative summaries to enhance the interpretation of results
beyond just the rankings on the leaderboard. The methods are illustrated with a
recently completed competition to evaluate algorithms capable of detecting,
identifying, and locating radioactive materials in an urban environment.",arxiv
http://arxiv.org/abs/1910.04807v3,2019-10-29T15:32:47Z,2019-10-10T18:41:54Z,Link Prediction via Graph Attention Network,"Link prediction aims to infer missing links or predicting the future ones
based on currently observed partial networks, it is a fundamental problem in
network science with tremendous real-world applications. However, conventional
link prediction approaches neither have high prediction accuracy nor being
capable of revealing the hidden information behind links. To address this
problem, we generalize the latest techniques in deep learning on graphs and
present a new link prediction model - DeepLinker. Instead of learning node
representation with the node label information, DeepLinker uses the links as
supervised information. Experiments on five graphs show that DeepLinker can not
only achieve the state-of-the-art link prediction accuracy, but also acquire
the efficient node representations and node centrality ranking as the
byproducts. Although the representations are obtained without any supervised
node label information, they still perform well on node ranking and node
classification tasks.",arxiv
http://arxiv.org/abs/1911.02623v1,2019-11-06T20:52:02Z,2019-11-06T20:52:02Z,Map Enhanced Route Travel Time Prediction using Deep Neural Networks,"Travel time estimation is a fundamental problem in transportation science
with extensive literature. The study of these techniques has intensified due to
availability of many publicly available large trip datasets. Recently developed
deep learning based models have improved the generality and performance and
have focused on estimating times for individual sub-trajectories and
aggregating them to predict the travel time of the entire trajectory. However,
these techniques ignore the road network information. In this work, we propose
and study techniques for incorporating road networks along with historical
trips' data into travel time prediction. We incorporate both node embeddings as
well as road distance into the existing model. Experiments on large real-world
benchmark datasets suggest improved performance, especially when the train data
is small. As expected, the proposed method performs better than the baseline
when there is a larger difference between road distance and Vincenty distance
between start and end points.",arxiv
http://arxiv.org/abs/2012.05996v1,2020-12-10T21:48:26Z,2020-12-10T21:48:26Z,"How to enhance quantum generative adversarial learning of noisy
  information","Quantum Machine Learning is where nowadays machine learning meets quantum
information science. In order to implement this new paradigm for novel quantum
technologies, we still need a much deeper understanding of its underlying
mechanisms, before proposing new algorithms to feasibly address real problems.
In this context, quantum generative adversarial learning is a promising
strategy to use quantum devices for quantum estimation or generative machine
learning tasks. However, the convergence behaviours of its training process,
which is crucial for its practical implementation on quantum processors, have
not been investigated in detail yet. Indeed here we show how different training
problems may occur during the optimization process, such as the emergence of
limit cycles. The latter may remarkably extend the convergence time in the
scenario of mixed quantum states playing a crucial role in the already
available noisy intermediate scale quantum devices. Then, we propose new
strategies to achieve a faster convergence in any operating regime. Our results
pave the way for new experimental demonstrations of such hybrid
classical-quantum protocols allowing to evaluate the potential advantages over
their classical counterparts.",arxiv
http://arxiv.org/abs/1411.5731v1,2014-11-21T00:39:43Z,2014-11-21T00:39:43Z,Visual Sentiment Prediction with Deep Convolutional Neural Networks,"Images have become one of the most popular types of media through which users
convey their emotions within online social networks. Although vast amount of
research is devoted to sentiment analysis of textual data, there has been very
limited work that focuses on analyzing sentiment of image data. In this work,
we propose a novel visual sentiment prediction framework that performs image
understanding with Deep Convolutional Neural Networks (CNN). Specifically, the
proposed sentiment prediction framework performs transfer learning from a CNN
with millions of parameters, which is pre-trained on large-scale data for
object recognition. Experiments conducted on two real-world datasets from
Twitter and Tumblr demonstrate the effectiveness of the proposed visual
sentiment analysis framework.",arxiv
http://arxiv.org/abs/1906.07318v3,2019-11-07T08:58:37Z,2019-06-18T00:31:59Z,Deep Active Learning for Anchor User Prediction,"Predicting pairs of anchor users plays an important role in the cross-network
analysis. Due to the expensive costs of labeling anchor users for training
prediction models, we consider in this paper the problem of minimizing the
number of user pairs across multiple networks for labeling as to improve the
accuracy of the prediction. To this end, we present a deep active learning
model for anchor user prediction (DALAUP for short). However, active learning
for anchor user sampling meets the challenges of non-i.i.d. user pair data
caused by network structures and the correlation among anchor or non-anchor
user pairs. To solve the challenges, DALAUP uses a couple of neural networks
with shared-parameter to obtain the vector representations of user pairs, and
ensembles three query strategies to select the most informative user pairs for
labeling and model training. Experiments on real-world social network data
demonstrate that DALAUP outperforms the state-of-the-art approaches.",arxiv
http://arxiv.org/abs/2105.04493v1,2021-05-10T16:33:58Z,2021-05-10T16:33:58Z,Graph Feature Gating Networks,"Graph neural networks (GNNs) have received tremendous attention due to their
power in learning effective representations for graphs. Most GNNs follow a
message-passing scheme where the node representations are updated by
aggregating and transforming the information from the neighborhood. Meanwhile,
they adopt the same strategy in aggregating the information from different
feature dimensions. However, suggested by social dimension theory and spectral
embedding, there are potential benefits to treat the dimensions differently
during the aggregation process. In this work, we investigate to enable
heterogeneous contributions of feature dimensions in GNNs. In particular, we
propose a general graph feature gating network (GFGN) based on the graph signal
denoising problem and then correspondingly introduce three graph filters under
GFGN to allow different levels of contributions from feature dimensions.
Extensive experiments on various real-world datasets demonstrate the
effectiveness and robustness of the proposed frameworks.",arxiv
http://arxiv.org/abs/1911.12063v1,2019-11-27T10:32:18Z,2019-11-27T10:32:18Z,"Following Social Groups: Socially Compliant Autonomous Navigation in
  Dense Crowds","In densely populated environments, socially compliant navigation is critical
for autonomous robots as driving close to people is unavoidable. This manner of
social navigation is challenging given the constraints of human comfort and
social rules. Traditional methods based on hand-craft cost functions to achieve
this task have difficulties to operate in the complex real world. Other
learning-based approaches fail to address the naturalness aspect from the
perspective of collective formation behaviors. We present an autonomous
navigation system capable of operating in dense crowds and utilizing
information of social groups. The underlying system incorporates a deep neural
network to track social groups and join the flow of a social group in
facilitating the navigation. A collision avoidance layer in the system further
ensures navigation safety. In experiments, our method generates socially
compliant behaviors as state-of-the-art methods. More importantly, the system
is capable of navigating safely in a densely populated area (10+ people in a
10m x 20m area) following crowd flows to reach the goal.",arxiv
http://arxiv.org/abs/1807.05698v2,2018-07-28T14:31:53Z,2018-07-16T06:49:22Z,"Recurrent Squeeze-and-Excitation Context Aggregation Net for Single
  Image Deraining","Rain streaks can severely degrade the visibility, which causes many current
computer vision algorithms fail to work. So it is necessary to remove the rain
from images. We propose a novel deep network architecture based on deep
convolutional and recurrent neural networks for single image deraining. As
contextual information is very important for rain removal, we first adopt the
dilated convolutional neural network to acquire large receptive field. To
better fit the rain removal task, we also modify the network. In heavy rain,
rain streaks have various directions and shapes, which can be regarded as the
accumulation of multiple rain streak layers. We assign different alpha-values
to various rain streak layers according to the intensity and transparency by
incorporating the squeeze-and-excitation block. Since rain streak layers
overlap with each other, it is not easy to remove the rain in one stage. So we
further decompose the rain removal into multiple stages. Recurrent neural
network is incorporated to preserve the useful information in previous stages
and benefit the rain removal in later stages. We conduct extensive experiments
on both synthetic and real-world datasets. Our proposed method outperforms the
state-of-the-art approaches under all evaluation metrics. Codes and
supplementary material are available at our project webpage:
https://xialipku.github.io/RESCAN .",arxiv
http://arxiv.org/abs/1706.00163v3,2017-06-05T14:54:55Z,2017-06-01T04:35:33Z,Coding Method for Parallel Iterative Linear Solver,"Computationally intensive distributed and parallel computing is often
bottlenecked by a small set of slow workers known as stragglers. In this paper,
we utilize the emerging idea of ""coded computation"" to design a novel
error-correcting-code inspired technique for solving linear inverse problems
under specific iterative methods in a parallelized implementation affected by
stragglers. Example applications include inverse problems in machine learning
on graphs, such as personalized PageRank and sampling on graphs. We provably
show that our coded-computation technique can reduce the mean-squared error
under a computational deadline constraint. In fact, the ratio of mean-squared
error of replication-based and coded techniques diverges to infinity as the
deadline increases. Our experiments for personalized PageRank performed on real
systems and real social networks show that this ratio can be as large as
$10^4$. Further, unlike coded-computation techniques proposed thus far, our
strategy combines outputs of all workers, including the stragglers, to produce
more accurate estimates at the computational deadline. This also ensures that
the accuracy degrades ""gracefully"" in the event that the number of stragglers
is large.",arxiv
http://arxiv.org/abs/2102.03517v1,2021-02-06T05:33:04Z,2021-02-06T05:33:04Z,Privacy-Preserving Feature Selection with Secure Multiparty Computation,"Existing work on privacy-preserving machine learning with Secure Multiparty
Computation (MPC) is almost exclusively focused on model training and on
inference with trained models, thereby overlooking the important data
pre-processing stage. In this work, we propose the first MPC based protocol for
private feature selection based on the filter method, which is independent of
model training, and can be used in combination with any MPC protocol to rank
features. We propose an efficient feature scoring protocol based on Gini
impurity to this end. To demonstrate the feasibility of our approach for
practical data science, we perform experiments with the proposed MPC protocols
for feature selection in a commonly used machine-learning-as-a-service
configuration where computations are outsourced to multiple servers, with
semi-honest and with malicious adversaries. Regarding effectiveness, we show
that secure feature selection with the proposed protocols improves the accuracy
of classifiers on a variety of real-world data sets, without leaking
information about the feature values or even which features were selected.
Regarding efficiency, we document runtimes ranging from several seconds to an
hour for our protocols to finish, depending on the size of the data set and the
security settings.",arxiv
http://arxiv.org/abs/1906.09032v2,2019-11-27T13:03:55Z,2019-06-21T09:55:40Z,"Popularity Prediction on Social Platforms with Coupled Graph Neural
  Networks","Predicting the popularity of online content on social platforms is an
important task for both researchers and practitioners. Previous methods mainly
leverage demographics, temporal and structural patterns of early adopters for
popularity prediction. However, most existing methods are less effective to
precisely capture the cascading effect in information diffusion, in which early
adopters try to activate potential users along the underlying network. In this
paper, we consider the problem of network-aware popularity prediction,
leveraging both early adopters and social networks for popularity prediction.
We propose to capture the cascading effect explicitly, modeling the activation
state of a target user given the activation state and influence of his/her
neighbors. To achieve this goal, we propose a novel method, namely CoupledGNN,
which uses two coupled graph neural networks to capture the interplay between
node activation states and the spread of influence. By stacking graph neural
network layers, our proposed method naturally captures the cascading effect
along the network in a successive manner. Experiments conducted on both
synthetic and real-world Sina Weibo datasets demonstrate that our method
significantly outperforms the state-of-the-art methods for popularity
prediction.",arxiv
http://arxiv.org/abs/2005.02131v2,2020-10-05T20:38:16Z,2020-05-05T13:22:35Z,Stealing Links from Graph Neural Networks,"Graph data, such as chemical networks and social networks, may be deemed
confidential/private because the data owner often spends lots of resources
collecting the data or the data contains sensitive information, e.g., social
relationships. Recently, neural networks were extended to graph data, which are
known as graph neural networks (GNNs). Due to their superior performance, GNNs
have many applications, such as healthcare analytics, recommender systems, and
fraud detection. In this work, we propose the first attacks to steal a graph
from the outputs of a GNN model that is trained on the graph. Specifically,
given a black-box access to a GNN model, our attacks can infer whether there
exists a link between any pair of nodes in the graph used to train the model.
We call our attacks link stealing attacks. We propose a threat model to
systematically characterize an adversary's background knowledge along three
dimensions which in total leads to a comprehensive taxonomy of 8 different link
stealing attacks. We propose multiple novel methods to realize these 8 attacks.
Extensive experiments on 8 real-world datasets show that our attacks are
effective at stealing links, e.g., AUC (area under the ROC curve) is above 0.95
in multiple cases. Our results indicate that the outputs of a GNN model reveal
rich information about the structure of the graph used to train the model.",arxiv
http://arxiv.org/abs/2009.09171v1,2020-09-19T05:48:01Z,2020-09-19T05:48:01Z,"Stochastic Threshold Model Trees: A Tree-Based Ensemble Method for
  Dealing with Extrapolation","In the field of chemistry, there have been many attempts to predict the
properties of unknown compounds from statistical models constructed using
machine learning. In an area where many known compounds are present (the
interpolation area), an accurate model can be constructed. In contrast, data in
areas where there are no known compounds (the extrapolation area) are generally
difficult to predict. However, in the development of new materials, it is
desirable to search this extrapolation area and discover compounds with
unprecedented physical properties. In this paper, we propose Stochastic
Threshold Model Trees (STMT), an extrapolation method that reflects the trend
of the data, while maintaining the accuracy of conventional interpolation
methods. The behavior of STMT is confirmed through experiments using both
artificial and real data. In the case of the real data, although there is no
significant overall improvement in accuracy, there is one compound for which
the prediction accuracy is notably improved, suggesting that STMT reflects the
data trends in the extrapolation area. We believe that the proposed method will
contribute to more efficient searches in situations such as new material
development.",arxiv
http://arxiv.org/abs/2106.03569v4,2021-08-26T09:52:20Z,2021-06-07T12:50:52Z,Socially-Aware Self-Supervised Tri-Training for Recommendation,"Self-supervised learning (SSL), which can automatically generate ground-truth
samples from raw data, holds vast potential to improve recommender systems.
Most existing SSL-based methods perturb the raw data graph with uniform
node/edge dropout to generate new data views and then conduct the
self-discrimination based contrastive learning over different views to learn
generalizable representations. Under this scheme, only a bijective mapping is
built between nodes in two different views, which means that the
self-supervision signals from other nodes are being neglected. Due to the
widely observed homophily in recommender systems, we argue that the supervisory
signals from other nodes are also highly likely to benefit the representation
learning for recommendation. To capture these signals, a general socially-aware
SSL framework that integrates tri-training is proposed in this paper.
Technically, our framework first augments the user data views with the user
social information. And then under the regime of tri-training for multi-view
encoding, the framework builds three graph encoders (one for recommendation)
upon the augmented views and iteratively improves each encoder with
self-supervision signals from other users, generated by the other two encoders.
Since the tri-training operates on the augmented views of the same data sources
for self-supervision signals, we name it self-supervised tri-training.
Extensive experiments on multiple real-world datasets consistently validate the
effectiveness of the self-supervised tri-training framework for improving
recommendation. The code is released at https://github.com/Coder-Yu/QRec.",arxiv
http://arxiv.org/abs/2010.09814v1,2020-10-19T19:47:38Z,2020-10-19T19:47:38Z,"Social Hierarchy-based Distributed Economic Model Predictive Control of
  Floating Offshore Wind Farms","This paper implements a recently developed social hierarchy-based distributed
economic model predictive control (DEMPC) algorithm in floating offshore wind
farms for the purpose of power maximization. The controller achieves this
objective using the concept of yaw and induction-based turbine repositioning
(YITuR), which minimizes the overlap areas between adjacent floating wind
turbine rotors in real-time to minimize the wake effect. Floating wind farm
dynamics and performance are predicted numerically using FOWFSim-Dyn. To ensure
fast decision-making by the DEMPC algorithm, feed-forward neural networks are
used to estimate floating wind turbine dynamics during the process of dynamic
optimization. For simulated wind farms with layouts ranging from 1-by-2 to
1-by-5, an increase of 20% in energy production is predicted when using YITuR
instead of greedy operation. Increased variability in wind speed and direction
is also studied and is shown to diminish controller performance due to rising
errors in neural network predictions.",arxiv
http://arxiv.org/abs/1501.00802v1,2015-01-05T09:55:37Z,2015-01-05T09:55:37Z,Detecting Malicious Content on Facebook,"Online Social Networks (OSNs) witness a rise in user activity whenever an
event takes place. Malicious entities exploit this spur in user-engagement
levels to spread malicious content that compromises system reputation and
degrades user experience. It also generates revenue from advertisements,
clicks, etc. for the malicious entities. Facebook, the world's biggest social
network, is no exception and has recently been reported to face much abuse
through scams and other type of malicious content, especially during news
making events. Recent studies have reported that spammers earn $200 million
just by posting malicious links on Facebook. In this paper, we characterize
malicious content posted on Facebook during 17 events, and discover that
existing efforts to counter malicious content by Facebook are not able to stop
all malicious content from entering the social graph. Our findings revealed
that malicious entities tend to post content through web and third party
applications while legitimate entities prefer mobile platforms to post content.
In addition, we discovered a substantial amount of malicious content generated
by Facebook pages. Through our observations, we propose an extensive feature
set based on entity profile, textual content, metadata, and URL features to
identify malicious content on Facebook in real time and at zero-hour. This
feature set was used to train multiple machine learning models and achieved an
accuracy of 86.9%. The intent is to catch malicious content that is currently
evading Facebook's detection techniques. Our machine learning model was able to
detect more than double the number of malicious posts as compared to existing
malicious content detection techniques. Finally, we built a real world solution
in the form of a REST based API and a browser plug-in to identify malicious
Facebook posts in real time.",arxiv
http://arxiv.org/abs/2103.06109v2,2021-04-29T09:01:45Z,2021-03-10T15:03:48Z,Session-based Social and Dependency-aware Software Recommendation,"With the increase of complexity of modern software, social collaborative
coding and reuse of open source software packages become more and more popular,
which thus greatly enhances the development efficiency and software quality.
However, the explosive growth of open source software packages exposes
developers to the challenge of information overload. While this can be
addressed by conventional recommender systems, they usually do not consider
particular constraints of social coding such as social influence among
developers and dependency relations among software packages. In this paper, we
aim to model the dynamic interests of developers with both social influence and
dependency constraints, and propose the Session-based Social and
Dependency-aware software Recommendation (SSDRec) model. This model integrates
recurrent neural network (RNN) and graph attention network (GAT) into a unified
framework. A RNN is employed to model the short-term dynamic interests of
developers in each session and two GATs are utilized to capture social
influence from friends and dependency constraints from dependent software
packages, respectively. Extensive experiments are conducted on real-world
datasets and the results demonstrate that our model significantly outperforms
the competitive baselines.",arxiv
http://arxiv.org/abs/1804.05259v1,2018-04-14T18:22:29Z,2018-04-14T18:22:29Z,"Intrinsically motivated reinforcement learning for human-robot
  interaction in the real-world","For a natural social human-robot interaction, it is essential for a robot to
learn the human-like social skills. However, learning such skills is
notoriously hard due to the limited availability of direct instructions from
people to teach a robot. In this paper, we propose an intrinsically motivated
reinforcement learning framework in which an agent gets the intrinsic
motivation-based rewards through the action-conditional predictive model. By
using the proposed method, the robot learned the social skills from the
human-robot interaction experiences gathered in the real uncontrolled
environments. The results indicate that the robot not only acquired human-like
social skills but also took more human-like decisions, on a test dataset, than
a robot which received direct rewards for the task achievement.",arxiv
http://arxiv.org/abs/1706.09916v2,2017-10-20T04:25:23Z,2017-06-29T18:33:06Z,Graph Convolution: A High-Order and Adaptive Approach,"In this paper, we presented a novel convolutional neural network framework
for graph modeling, with the introduction of two new modules specially designed
for graph-structured data: the $k$-th order convolution operator and the
adaptive filtering module. Importantly, our framework of High-order and
Adaptive Graph Convolutional Network (HA-GCN) is a general-purposed
architecture that fits various applications on both node and graph centrics, as
well as graph generative models. We conducted extensive experiments on
demonstrating the advantages of our framework. Particularly, our HA-GCN
outperforms the state-of-the-art models on node classification and molecule
property prediction tasks. It also generates 32% more real molecules on the
molecule generation task, both of which will significantly benefit real-world
applications such as material design and drug screening.",arxiv
http://arxiv.org/abs/2108.10587v1,2021-08-24T09:03:03Z,2021-08-24T09:03:03Z,Pooling Architecture Search for Graph Classification,"Graph classification is an important problem with applications across many
domains, like chemistry and bioinformatics, for which graph neural networks
(GNNs) have been state-of-the-art (SOTA) methods. GNNs are designed to learn
node-level representation based on neighborhood aggregation schemes, and to
obtain graph-level representation, pooling methods are applied after the
aggregation operation in existing GNN models to generate coarse-grained graphs.
However,due to highly diverse applications of graph classification, and the
performance of existing pooling methods vary on different graphs. In other
words, it is a challenging problem to design a universal pooling architecture
to perform well in most cases, leading to a demand for data-specific pooling
methods in real-world applications. To address this problem, we propose to use
neural architecture search (NAS) to search for adaptive pooling architectures
for graph classification. Firstly we designed a unified framework consisting of
four modules: Aggregation, Pooling, Readout, and Merge, which can cover
existing human-designed pooling methods for graph classification. Based on this
framework, a novel search space is designed by incorporating popular operations
in human-designed architectures. Then to enable efficient search, a coarsening
strategy is proposed to continuously relax the search space, thus a
differentiable search method can be adopted. Extensive experiments on six
real-world datasets from three domains are conducted, and the results
demonstrate the effectiveness and efficiency of the proposed framework.",arxiv
http://arxiv.org/abs/2102.03616v1,2021-02-06T17:11:09Z,2021-02-06T17:11:09Z,"A Data Augmented Bayesian Network for Node Failure Prediction in Optical
  Networks","Failures in optical network backbone can cause significant interruption in
internet data traffic. Hence, it is very important to reduce such network
outages. Prediction of such failures would be a step forward to avoid such
disruption of internet services for users as well as operators. Several
research proposals are available in the literature which are applications of
data science and machine learning techniques. Most of the techniques rely on
significant amount of real time data collection. Network devices are assumed to
be equipped to collect data and these are then analysed by different algorithms
to predict failures. Every network element which is already deployed in the
field may not have these data gathering or analysis techniques designed into
them initially. However, such mechanisms become necessary later when they are
already deployed in the field. This paper proposes a Bayesian network based
failure prediction of network nodes, e.g., routers etc., using very basic
information from the log files of the devices and applying power law based data
augmentation to complement for scarce real time information. Numerical results
show that network node failure prediction can be performed with high accuracy
using the proposed mechanism.",arxiv
http://arxiv.org/abs/2001.08853v5,2020-10-26T10:18:52Z,2020-01-24T00:20:35Z,"MONSTOR: An Inductive Approach for Estimating and Maximizing Influence
  over Unseen Networks","Influence maximization (IM) is one of the most important problems in social
network analysis. Its objective is to find a given number of seed nodes that
maximize the spread of information through a social network. Since it is an
NP-hard problem, many approximate/heuristic methods have been developed, and a
number of them repeat Monte Carlo (MC) simulations over and over to reliably
estimate the influence (i.e., the number of infected nodes) of a seed set. In
this work, we present an inductive machine learning method, called Monte Carlo
Simulator (MONSTOR), for estimating the influence of given seed nodes in social
networks unseen during training. To the best of our knowledge, MONSTOR is the
first inductive method for this purpose. MONSTOR can greatly accelerate
existing IM algorithms by replacing repeated MC simulations. In our
experiments, MONSTOR provided highly accurate estimates, achieving 0.998 or
higher Pearson and Spearman correlation coefficients in unseen real-world
social networks. Moreover, IM algorithms equipped with MONSTOR are more
accurate than state-of-the-art competitors in 63% of IM use cases.",arxiv
http://arxiv.org/abs/1904.05003v1,2019-04-10T04:53:20Z,2019-04-10T04:53:20Z,Semi-Supervised Graph Classification: A Hierarchical Graph Perspective,"Node classification and graph classification are two graph learning problems
that predict the class label of a node and the class label of a graph
respectively. A node of a graph usually represents a real-world entity, e.g., a
user in a social network, or a protein in a protein-protein interaction
network. In this work, we consider a more challenging but practically useful
setting, in which a node itself is a graph instance. This leads to a
hierarchical graph perspective which arises in many domains such as social
network, biological network and document collection. For example, in a social
network, a group of people with shared interests forms a user group, whereas a
number of user groups are interconnected via interactions or common members. We
study the node classification problem in the hierarchical graph where a `node'
is a graph instance, e.g., a user group in the above example. As labels are
usually limited in real-world data, we design two novel semi-supervised
solutions named \underline{SE}mi-supervised gr\underline{A}ph
c\underline{L}assification via \underline{C}autious/\underline{A}ctive
\underline{I}teration (or SEAL-C/AI in short). SEAL-C/AI adopt an iterative
framework that takes turns to build or update two classifiers, one working at
the graph instance level and the other at the hierarchical graph level. To
simplify the representation of the hierarchical graph, we propose a novel
supervised, self-attentive graph embedding method called SAGE, which embeds
graph instances of arbitrary size into fixed-length vectors. Through
experiments on synthetic data and Tencent QQ group data, we demonstrate that
SEAL-C/AI not only outperform competing methods by a significant margin in
terms of accuracy/Macro-F1, but also generate meaningful interpretations of the
learned representations.",arxiv
http://arxiv.org/abs/1709.00389v1,2017-08-30T04:24:06Z,2017-08-30T04:24:06Z,End-to-end Learning for Short Text Expansion,"Effectively making sense of short texts is a critical task for many real
world applications such as search engines, social media services, and
recommender systems. The task is particularly challenging as a short text
contains very sparse information, often too sparse for a machine learning
algorithm to pick up useful signals. A common practice for analyzing short text
is to first expand it with external information, which is usually harvested
from a large collection of longer texts. In literature, short text expansion
has been done with all kinds of heuristics. We propose an end-to-end solution
that automatically learns how to expand short text to optimize a given learning
task. A novel deep memory network is proposed to automatically find relevant
information from a collection of longer documents and reformulate the short
text through a gating mechanism. Using short text classification as a
demonstrating task, we show that the deep memory network significantly
outperforms classical text expansion methods with comprehensive experiments on
real world data sets.",arxiv
http://arxiv.org/abs/1910.12202v4,2020-09-14T02:38:16Z,2019-10-27T07:42:15Z,CONNA: Addressing Name Disambiguation on The Fly,"Name disambiguation is a key and also a very tough problem in many online
systems such as social search and academic search. Despite considerable
research, a critical issue that has not been systematically studied is
disambiguation on the fly -- to complete the disambiguation in the real-time.
This is very challenging, as the disambiguation algorithm must be accurate,
efficient, and error tolerance. In this paper, we propose a novel framework --
CONNA -- to train a matching component and a decision component jointly via
reinforcement learning. The matching component is responsible for finding the
top matched candidate for the given paper, and the decision component is
responsible for deciding on assigning the top matched person or creating a new
person. The two components are intertwined and can be bootstrapped via jointly
training. Empirically, we evaluate CONNA on two name disambiguation datasets.
Experimental results show that the proposed framework can achieve a
1.21%-19.84% improvement on F1-score using joint training of the matching and
the decision components. The proposed CONNA has been successfully deployed on
AMiner -- a large online academic search system.",arxiv
http://arxiv.org/abs/1704.02012v1,2017-04-06T20:25:36Z,2017-04-06T20:25:36Z,"A Software-equivalent SNN Hardware using RRAM-array for Asynchronous
  Real-time Learning","Spiking Neural Network (SNN) naturally inspires hardware implementation as it
is based on biology. For learning, spike time dependent plasticity (STDP) may
be implemented using an energy efficient waveform superposition on memristor
based synapse. However, system level implementation has three challenges.
First, a classic dilemma is that recognition requires current reading for short
voltage$-$spikes which is disturbed by large voltage$-$waveforms that are
simultaneously applied on the same memristor for real$-$time learning i.e. the
simultaneous read$-$write dilemma. Second, the hardware needs to exactly
replicate software implementation for easy adaptation of algorithm to hardware.
Third, the devices used in hardware simulations must be realistic. In this
paper, we present an approach to address the above concerns. First, the
learning and recognition occurs in separate arrays simultaneously in
real$-$time, asynchronously $-$ avoiding non$-$biomimetic clocking based
complex signal management. Second, we show that the hardware emulates software
at every stage by comparison of SPICE (circuit$-$simulator) with MATLAB
(mathematical SNN algorithm implementation in software) implementations. As an
example, the hardware shows 97.5 per cent accuracy in classification which is
equivalent to software for a Fisher$-$Iris dataset. Third, the STDP is
implemented using a model of synaptic device implemented using HfO2 memristor.
We show that an increasingly realistic memristor model slightly reduces the
hardware performance (85 per cent), which highlights the need to engineer RRAM
characteristics specifically for SNN.",arxiv
http://arxiv.org/abs/2011.01579v2,2021-01-29T12:10:26Z,2020-11-03T09:09:51Z,Fake News Detection through Graph Comment Advanced Learning,"Disinformation has long been regarded as a severe social problem, where fake
news is one of the most representative issues. What is worse, today's highly
developed social media makes fake news widely spread at incredible speed,
bringing in substantial harm to various aspects of human life. Yet, the
popularity of social media also provides opportunities to better detect fake
news. Unlike conventional means which merely focus on either content or user
comments, effective collaboration of heterogeneous social media information,
including content and context factors of news, users' comments and the
engagement of social media with users, will hopefully give rise to better
detection of fake news.
  Motivated by the above observations, a novel detection framework, namely
graph comment-user advanced learning framework (GCAL) is proposed in this
paper. User-comment information is crucial but not well studied in fake news
detection. Thus, we model user-comment context through network representation
learning based on heterogeneous graph neural network. We conduct experiments on
two real-world datasets, which demonstrate that the proposed joint model
outperforms 8 state-of-the-art baseline methods for fake news detection (at
least 4% in Accuracy, 7% in Recall and 5% in F1). Moreover, the proposed method
is also explainable.",arxiv
http://arxiv.org/abs/2103.08976v1,2021-03-16T11:08:30Z,2021-03-16T11:08:30Z,Dual Side Deep Context-aware Modulation for Social Recommendation,"Social recommendation is effective in improving the recommendation
performance by leveraging social relations from online social networking
platforms. Social relations among users provide friends' information for
modeling users' interest in candidate items and help items expose to potential
consumers (i.e., item attraction). However, there are two issues haven't been
well-studied: Firstly, for the user interests, existing methods typically
aggregate friends' information contextualized on the candidate item only, and
this shallow context-aware aggregation makes them suffer from the limited
friends' information. Secondly, for the item attraction, if the item's past
consumers are the friends of or have a similar consumption habit to the
targeted user, the item may be more attractive to the targeted user, but most
existing methods neglect the relation enhanced context-aware item attraction.
To address the above issues, we proposed DICER (Dual Side Deep Context-aware
Modulation for SocialRecommendation). Specifically, we first proposed a novel
graph neural network to model the social relation and collaborative relation,
and on top of high-order relations, a dual side deep context-aware modulation
is introduced to capture the friends' information and item attraction.
Empirical results on two real-world datasets show the effectiveness of the
proposed model and further experiments are conducted to help understand how the
dual context-aware modulation works.",arxiv
http://arxiv.org/abs/1204.6535v6,2014-08-31T03:30:09Z,2012-04-30T02:19:26Z,"Citations, Sequence Alignments, Contagion, and Semantics: On Acyclic
  Structures and their Randomness","Datasets from several domains, such as life-sciences, semantic web, machine
learning, natural language processing, etc. are naturally structured as acyclic
graphs. These datasets, particularly those in bio-informatics and computational
epidemiology, have grown tremendously over the last decade or so. Increasingly,
as a consequence, there is a need to build and evaluate various strategies for
processing acyclic structured graphs. Most of the proposed research models the
real world acyclic structures as random graphs, i.e., they are generated by
randomly selecting a subset of edges from all possible edges. Unfortunately the
graphs thus generated have predictable and degenerate structures, i.e., the
resulting graphs will always have almost the same degree distribution and very
short paths.
  Specifically, we show that if $O(n \log n \log n)$ edges are added to a
binary tree of $n$ nodes then with probability more than $O(1/(\log n)^{1/n})$
the depth of all but $O({\log \log n} ^{\log \log n})$ vertices of the dag
collapses to 1. Experiments show that irregularity, as measured by distribution
of length of random walks from root to leaves, is also predictable and small.
The degree distribution and random walk length properties of real world graphs
from these domains are significantly different from random graphs of similar
vertex and edge size.",arxiv
http://arxiv.org/abs/2011.05632v2,2020-11-12T01:21:35Z,2020-11-11T08:42:30Z,"Exploratory Grasping: Asymptotically Optimal Algorithms for Grasping
  Challenging Polyhedral Objects","There has been significant recent work on data-driven algorithms for learning
general-purpose grasping policies. However, these policies can consistently
fail to grasp challenging objects which are significantly out of the
distribution of objects in the training data or which have very few high
quality grasps. Motivated by such objects, we propose a novel problem setting,
Exploratory Grasping, for efficiently discovering reliable grasps on an unknown
polyhedral object via sequential grasping, releasing, and toppling. We
formalize Exploratory Grasping as a Markov Decision Process, study the
theoretical complexity of Exploratory Grasping in the context of reinforcement
learning and present an efficient bandit-style algorithm, Bandits for Online
Rapid Grasp Exploration Strategy (BORGES), which leverages the structure of the
problem to efficiently discover high performing grasps for each object stable
pose. BORGES can be used to complement any general-purpose grasping algorithm
with any grasp modality (parallel-jaw, suction, multi-fingered, etc) to learn
policies for objects in which they exhibit persistent failures. Simulation
experiments suggest that BORGES can significantly outperform both
general-purpose grasping pipelines and two other online learning algorithms and
achieves performance within 5% of the optimal policy within 1000 and 8000
timesteps on average across 46 challenging objects from the Dex-Net adversarial
and EGAD! object datasets, respectively. Initial physical experiments suggest
that BORGES can improve grasp success rate by 45% over a Dex-Net baseline with
just 200 grasp attempts in the real world. See https://tinyurl.com/exp-grasping
for supplementary material and videos.",arxiv
http://arxiv.org/abs/1708.04664v1,2017-08-05T13:15:36Z,2017-08-05T13:15:36Z,"A Novel data Pre-processing method for multi-dimensional and non-uniform
  data","We are in the era of data analytics and data science which is on full bloom.
There is abundance of all kinds of data for example biometrics based data,
satellite images data, chip-seq data, social network data, sensor based data
etc. from a variety of sources. This data abundance is the result of the fact
that storage cost is getting cheaper day by day, so people as well as almost
all business or scientific organizations are storing more and more data. Most
of the real data is multi-dimensional, non-uniform, and big in size, such that
it requires a unique pre-processing before analyzing it. In order to make data
useful for any kind of analysis, pre-processing is a very important step. This
paper presents a unique and novel pre-processing method for multi-dimensional
and non-uniform data with the aim of making it uniform and reduced in size
without losing much of its value. We have chosen biometric signature data to
demonstrate the proposed method as it qualifies for the attributes of being
multi-dimensional, non-uniform and big in size. Biometric signature data does
not only captures the structural characteristics of a signature but also its
behavioral characteristics that are captured using a dynamic signature capture
device. These features like pen pressure, pen tilt angle, time taken to sign a
document when collected in real-time turn out to be of varying dimensions. This
feature data set along with the structural data needs to be pre-processed in
order to use it to train a machine learning based model for signature
verification purposes. We demonstrate the success of the proposed method over
other methods using experimental results for biometric signature data but the
same can be implemented for any other data with similar properties from a
different domain.",arxiv
http://arxiv.org/abs/1908.04472v1,2019-08-13T03:19:46Z,2019-08-13T03:19:46Z,Exploiting Multi-domain Visual Information for Fake News Detection,"The increasing popularity of social media promotes the proliferation of fake
news. With the development of multimedia technology, fake news attempts to
utilize multimedia contents with images or videos to attract and mislead
readers for rapid dissemination, which makes visual contents an important part
of fake news. Fake-news images, images attached in fake news posts,include not
only fake images which are maliciously tampered but also real images which are
wrongly used to represent irrelevant events. Hence, how to fully exploit the
inherent characteristics of fake-news images is an important but challenging
problem for fake news detection. In the real world, fake-news images may have
significantly different characteristics from real-news images at both physical
and semantic levels, which can be clearly reflected in the frequency and pixel
domain, respectively. Therefore, we propose a novel framework Multi-domain
Visual Neural Network (MVNN) to fuse the visual information of frequency and
pixel domains for detecting fake news. Specifically, we design a CNN-based
network to automatically capture the complex patterns of fake-news images in
the frequency domain; and utilize a multi-branch CNN-RNN model to extract
visual features from different semantic levels in the pixel domain. An
attention mechanism is utilized to fuse the feature representations of
frequency and pixel domains dynamically. Extensive experiments conducted on a
real-world dataset demonstrate that MVNN outperforms existing methods with at
least 9.2% in accuracy, and can help improve the performance of multimodal fake
news detection by over 5.2%.",arxiv
http://arxiv.org/abs/2110.15727v1,2021-10-11T07:42:41Z,2021-10-11T07:42:41Z,"Calling to CNN-LSTM for Rumor Detection: A Deep Multi-channel Model for
  Message Veracity Classification in Microblogs","Reputed by their low-cost, easy-access, real-time and valuable information,
social media also wildly spread unverified or fake news. Rumors can notably
cause severe damage on individuals and the society. Therefore, rumor detection
on social media has recently attracted tremendous attention. Most rumor
detection approaches focus on rumor feature analysis and social features, i.e.,
metadata in social media. Unfortunately, these features are data-specific and
may not always be available, e.g., when the rumor has just popped up and not
yet propagated. In contrast, post contents (including images or videos) play an
important role and can indicate the diffusion purpose of a rumor. Furthermore,
rumor classification is also closely related to opinion mining and sentiment
analysis. Yet, to the best of our knowledge, exploiting images and sentiments
is little investigated.Considering the available multimodal features from
microblogs, notably, we propose in this paper an end-to-end model called
deepMONITOR that is based on deep neural networks and allows quite accurate
automated rumor verification, by utilizing all three characteristics: post
textual and image contents, as well as sentiment. deepMONITOR concatenates
image features with the joint text and sentiment features to produce a
reliable, fused classification. We conduct extensive experiments on two
large-scale, real-world datasets. The results show that deepMONITOR achieves a
higher accuracy than state-of-the-art methods.",arxiv
http://arxiv.org/abs/1901.07868v3,2020-02-08T12:42:08Z,2019-01-23T13:25:16Z,Constant Time Graph Neural Networks,"The recent advancements in graph neural networks (GNNs) have led to
state-of-the-art performances in various applications, including
chemo-informatics, question-answering systems, and recommender systems.
However, scaling up these methods to huge graphs, such as social networks and
Web graphs, remains a challenge. In particular, the existing methods for
accelerating GNNs either are not theoretically guaranteed in terms of the
approximation error or incur at least a linear time computation cost. In this
study, we reveal the query complexity of the uniform node sampling scheme for
Message Passing Neural Networks including GraphSAGE, graph attention networks
(GATs), and graph convolutional networks (GCNs). Surprisingly, our analysis
reveals that the complexity of the node sampling method is completely
independent of the number of the nodes, edges, and neighbors of the input and
depends only on the error tolerance and confidence probability while providing
a theoretical guarantee for the approximation error. To the best of our
knowledge, this is the first paper to provide a theoretical guarantee of
approximation for GNNs within constant time. Through experiments with synthetic
and real-world datasets, we investigated the speed and precision of the node
sampling scheme and validated our theoretical results.",arxiv
http://arxiv.org/abs/1908.01207v1,2019-08-03T17:52:13Z,2019-08-03T17:52:13Z,Predicting Dynamic Embedding Trajectory in Temporal Interaction Networks,"Modeling sequential interactions between users and items/products is crucial
in domains such as e-commerce, social networking, and education. Representation
learning presents an attractive opportunity to model the dynamic evolution of
users and items, where each user/item can be embedded in a Euclidean space and
its evolution can be modeled by an embedding trajectory in this space. However,
existing dynamic embedding methods generate embeddings only when users take
actions and do not explicitly model the future trajectory of the user/item in
the embedding space. Here we propose JODIE, a coupled recurrent neural network
model that learns the embedding trajectories of users and items. JODIE employs
two recurrent neural networks to update the embedding of a user and an item at
every interaction. Crucially, JODIE also models the future embedding trajectory
of a user/item. To this end, it introduces a novel projection operator that
learns to estimate the embedding of the user at any time in the future. These
estimated embeddings are then used to predict future user-item interactions. To
make the method scalable, we develop a t-Batch algorithm that creates
time-consistent batches and leads to 9x faster training. We conduct six
experiments to validate JODIE on two prediction tasks---future interaction
prediction and state change prediction---using four real-world datasets. We
show that JODIE outperforms six state-of-the-art algorithms in these tasks by
at least 20% in predicting future interactions and 12% in state change
prediction.",arxiv
http://arxiv.org/abs/2105.02653v2,2021-05-13T15:37:31Z,2021-04-29T13:59:21Z,"Correcting Classification: A Bayesian Framework Using Explanation
  Feedback to Improve Classification Abilities","Neural networks (NNs) have shown high predictive performance, however, with
shortcomings. Firstly, the reasons behind the classifications are not fully
understood. Several explanation methods have been developed, but they do not
provide mechanisms for users to interact with the explanations. Explanations
are social, meaning they are a transfer of knowledge through interactions.
Nonetheless, current explanation methods contribute only to one-way
communication. Secondly, NNs tend to be overconfident, providing unreasonable
uncertainty estimates on out-of-distribution observations. We overcome these
difficulties by training a Bayesian convolutional neural network (CNN) that
uses explanation feedback. After training, the model presents explanations of
training sample classifications to an annotator. Based on the provided
information, the annotator can accept or reject the explanations by providing
feedback. Our proposed method utilizes this feedback for fine-tuning to correct
the model such that the explanations and classifications improve. We use
existing CNN architectures to demonstrate the method's effectiveness on one toy
dataset (decoy MNIST) and two real-world datasets (Dogs vs. Cats and ISIC skin
cancer). The experiments indicate that few annotated explanations and
fine-tuning epochs are needed to improve the model and predictive performance,
making the model more trustworthy and understandable.",arxiv
http://arxiv.org/abs/2106.15910v1,2021-06-30T08:57:01Z,2021-06-30T08:57:01Z,Graph Signal Restoration Using Nested Deep Algorithm Unrolling,"Graph signal processing is a ubiquitous task in many applications such as
sensor, social, transportation and brain networks, point cloud processing, and
graph neural networks. Graph signals are often corrupted through sensing
processes, and need to be restored for the above applications. In this paper,
we propose two graph signal restoration methods based on deep algorithm
unrolling (DAU). First, we present a graph signal denoiser by unrolling
iterations of the alternating direction method of multiplier (ADMM). We then
propose a general restoration method for linear degradation by unrolling
iterations of Plug-and-Play ADMM (PnP-ADMM). In the second method, the unrolled
ADMM-based denoiser is incorporated as a submodule. Therefore, our restoration
method has a nested DAU structure. Thanks to DAU, parameters in the proposed
denoising/restoration methods are trainable in an end-to-end manner. Since the
proposed restoration methods are based on iterations of a (convex) optimization
algorithm, the method is interpretable and keeps the number of parameters small
because we only need to tune graph-independent regularization parameters. We
solve two main problems in existing graph signal restoration methods: 1)
limited performance of convex optimization algorithms due to fixed parameters
which are often determined manually. 2) large number of parameters of graph
neural networks that result in difficulty of training. Several experiments for
graph signal denoising and interpolation are performed on synthetic and
real-world data. The proposed methods show performance improvements to several
existing methods in terms of root mean squared error in both tasks.",arxiv
http://arxiv.org/abs/2003.01351v1,2020-03-03T06:09:15Z,2020-03-03T06:09:15Z,DETECT: Deep Trajectory Clustering for Mobility-Behavior Analysis,"Identifying mobility behaviors in rich trajectory data is of great economic
and social interest to various applications including urban planning, marketing
and intelligence. Existing work on trajectory clustering often relies on
similarity measurements that utilize raw spatial and/or temporal information of
trajectories. These measures are incapable of identifying similar moving
behaviors that exhibit varying spatio-temporal scales of movement. In addition,
the expense of labeling massive trajectory data is a barrier to supervised
learning models. To address these challenges, we propose an unsupervised neural
approach for mobility behavior clustering, called the Deep Embedded TrajEctory
ClusTering network (DETECT). DETECT operates in three parts: first it
transforms the trajectories by summarizing their critical parts and augmenting
them with context derived from their geographical locality (e.g., using POIs
from gazetteers). In the second part, it learns a powerful representation of
trajectories in the latent space of behaviors, thus enabling a clustering
function (such as $k$-means) to be applied. Finally, a clustering oriented loss
is directly built on the embedded features to jointly perform feature
refinement and cluster assignment, thus improving separability between mobility
behaviors. Exhaustive quantitative and qualitative experiments on two
real-world datasets demonstrate the effectiveness of our approach for mobility
behavior analyses.",arxiv
http://arxiv.org/abs/1904.11679v2,2020-09-16T18:42:11Z,2019-04-26T05:52:05Z,Fake News Early Detection: An Interdisciplinary Study,"Massive dissemination of fake news and its potential to erode democracy has
increased the demand for accurate fake news detection. Recent advancements in
this area have proposed novel techniques that aim to detect fake news by
exploring how it propagates on social networks. Nevertheless, to detect fake
news at an early stage, i.e., when it is published on a news outlet but not yet
spread on social media, one cannot rely on news propagation information as it
does not exist. Hence, there is a strong need to develop approaches that can
detect fake news by focusing on news content. In this paper, a theory-driven
model is proposed for fake news detection. The method investigates news content
at various levels: lexicon-level, syntax-level, semantic-level and
discourse-level. We represent news at each level, relying on well-established
theories in social and forensic psychology. Fake news detection is then
conducted within a supervised machine learning framework. As an
interdisciplinary research, our work explores potential fake news patterns,
enhances the interpretability in fake news feature engineering, and studies the
relationships among fake news, deception/disinformation, and clickbaits.
Experiments conducted on two real-world datasets indicate the proposed method
can outperform the state-of-the-art and enable fake news early detection when
there is limited content information.",arxiv
http://arxiv.org/abs/2111.07027v1,2021-11-13T03:38:09Z,2021-11-13T03:38:09Z,"Adaptive Similarity Function with Structural Features of Network
  Embedding for Missing Link Prediction","Link prediction is a fundamental problem of data science, which usually calls
for unfolding the mechanisms that govern the micro-dynamics of networks. In
this regard, using features obtained from network embedding for predicting
links has drawn widespread attention. Though edge features-based or node
similarity-based methods have been proposed to solve the link prediction
problem, many technical challenges still exist due to the unique structural
properties of networks, especially when the networks are sparse. From the graph
mining perspective, we first give empirical evidence of the inconsistency
between heuristic and learned edge features. Then we propose a novel link
prediction framework, AdaSim, by introducing an Adaptive Similarity function
using features obtained from network embedding based on random walks. The node
feature representations are obtained by optimizing a graph-based objective
function. Instead of generating edge features using binary operators, we
perform link prediction solely leveraging the node features of the network. We
define a flexible similarity function with one tunable parameter, which serves
as a penalty of the original similarity measure. The optimal value is learned
through supervised learning thus is adaptive to data distribution. To evaluate
the performance of our proposed algorithm, we conduct extensive experiments on
eleven disparate networks of the real world. Experimental results show that
AdaSim achieves better performance than state-of-the-art algorithms and is
robust to different sparsities of the networks.",arxiv
http://arxiv.org/abs/1408.1228v3,2016-04-01T09:00:05Z,2014-08-06T09:52:13Z,Location Prediction: Communities Speak Louder than Friends,"Humans are social animals, they interact with different communities of
friends to conduct different activities. The literature shows that human
mobility is constrained by their social relations. In this paper, we
investigate the social impact of a person's communities on his mobility,
instead of all friends from his online social networks. This study can be
particularly useful, as certain social behaviors are influenced by specific
communities but not all friends. To achieve our goal, we first develop a
measure to characterize a person's social diversity, which we term `community
entropy'. Through analysis of two real-life datasets, we demonstrate that a
person's mobility is influenced only by a small fraction of his communities and
the influence depends on the social contexts of the communities. We then
exploit machine learning techniques to predict users' future movement based on
their communities' information. Extensive experiments demonstrate the
prediction's effectiveness.",arxiv
http://arxiv.org/abs/2003.10548v1,2020-03-23T21:04:43Z,2020-03-23T21:04:43Z,spsurv: An R package for semi-parametric survival analysis,"Software development innovations and advances in computing have enabled more
complex and less costly computations in medical research (survival analysis),
engineering studies (reliability analysis), and social sciences event analysis
(historical analysis). As a result, many semi-parametric modeling efforts
emerged when it comes to time-to-event data analysis. In this context, this
work presents a flexible Bernstein polynomial (BP) based framework for survival
data modeling. This innovative approach is applied to existing families of
models such as proportional hazards (PH), proportional odds (PO), and
accelerated failure time (AFT) models to estimate unknown baseline functions.
Along with this contribution, this work also presents new automated routines in
R, taking advantage of algorithms available in Stan. The proposed computation
routines are tested and explored through simulation studies based on artificial
datasets. The tools implemented to fit the proposed statistical models are
combined and organized in an R package. Also, the BP based proportional hazards
(BPPH), proportional odds (BPPO), and accelerated failure time (BPAFT) models
are illustrated in real applications related to cancer trial data using maximum
likelihood (ML) estimation and Markov chain Monte Carlo (MCMC) methods.",arxiv
http://arxiv.org/abs/1703.04699v1,2017-03-14T20:23:48Z,2017-03-14T20:23:48Z,"A fully end-to-end deep learning approach for real-time simultaneous 3D
  reconstruction and material recognition","This paper addresses the problem of simultaneous 3D reconstruction and
material recognition and segmentation. Enabling robots to recognise different
materials (concrete, metal etc.) in a scene is important for many tasks, e.g.
robotic interventions in nuclear decommissioning. Previous work on 3D semantic
reconstruction has predominantly focused on recognition of everyday domestic
objects (tables, chairs etc.), whereas previous work on material recognition
has largely been confined to single 2D images without any 3D reconstruction.
Meanwhile, most 3D semantic reconstruction methods rely on computationally
expensive post-processing, using Fully-Connected Conditional Random Fields
(CRFs), to achieve consistent segmentations. In contrast, we propose a deep
learning method which performs 3D reconstruction while simultaneously
recognising different types of materials and labelling them at the pixel level.
Unlike previous methods, we propose a fully end-to-end approach, which does not
require hand-crafted features or CRF post-processing. Instead, we use only
learned features, and the CRF segmentation constraints are incorporated inside
the fully end-to-end learned system. We present the results of experiments, in
which we trained our system to perform real-time 3D semantic reconstruction for
23 different materials in a real-world application. The run-time performance of
the system can be boosted to around 10Hz, using a conventional GPU, which is
enough to achieve real-time semantic reconstruction using a 30fps RGB-D camera.
To the best of our knowledge, this work is the first real-time end-to-end
system for simultaneous 3D reconstruction and material recognition.",arxiv
http://arxiv.org/abs/1711.09409v1,2017-11-26T15:44:49Z,2017-11-26T15:44:49Z,"BL-MNE: Emerging Heterogeneous Social Network Embedding through Broad
  Learning with Aligned Autoencoder","Network embedding aims at projecting the network data into a low-dimensional
feature space, where the nodes are represented as a unique feature vector and
network structure can be effectively preserved. In recent years, more and more
online application service sites can be represented as massive and complex
networks, which are extremely challenging for traditional machine learning
algorithms to deal with. Effective embedding of the complex network data into
low-dimension feature representation can both save data storage space and
enable traditional machine learning algorithms applicable to handle the network
data. Network embedding performance will degrade greatly if the networks are of
a sparse structure, like the emerging networks with few connections. In this
paper, we propose to learn the embedding representation for a target emerging
network based on the broad learning setting, where the emerging network is
aligned with other external mature networks at the same time. To solve the
problem, a new embedding framework, namely ""Deep alIgned autoencoder based
eMbEdding"" (DIME), is introduced in this paper. DIME handles the diverse link
and attribute in a unified analytic based on broad learning, and introduces the
multiple aligned attributed heterogeneous social network concept to model the
network structure. A set of meta paths are introduced in the paper, which
define various kinds of connections among users via the heterogeneous link and
attribute information. The closeness among users in the networks are defined as
the meta proximity scores, which will be fed into DIME to learn the embedding
vectors of users in the emerging network. Extensive experiments have been done
on real-world aligned social networks, which have demonstrated the
effectiveness of DIME in learning the emerging network embedding vectors.",arxiv
http://arxiv.org/abs/2004.14251v1,2020-04-29T14:59:58Z,2020-04-29T14:59:58Z,"Action Sequence Predictions of Vehicles in Urban Environments using Map
  and Social Context","This work studies the problem of predicting the sequence of future actions
for surround vehicles in real-world driving scenarios. To this aim, we make
three main contributions. The first contribution is an automatic method to
convert the trajectories recorded in real-world driving scenarios to action
sequences with the help of HD maps. The method enables automatic dataset
creation for this task from large-scale driving data. Our second contribution
lies in applying the method to the well-known traffic agent tracking and
prediction dataset Argoverse, resulting in 228,000 action sequences.
Additionally, 2,245 action sequences were manually annotated for testing. The
third contribution is to propose a novel action sequence prediction method by
integrating past positions and velocities of the traffic agents, map
information and social context into a single end-to-end trainable neural
network. Our experiments prove the merit of the data creation method and the
value of the created dataset - prediction performance improves consistently
with the size of the dataset and shows that our action prediction method
outperforms comparing models.",arxiv
http://arxiv.org/abs/2105.06725v1,2021-05-14T09:16:28Z,2021-05-14T09:16:28Z,Meta-Inductive Node Classification across Graphs,"Semi-supervised node classification on graphs is an important research
problem, with many real-world applications in information retrieval such as
content classification on a social network and query intent classification on
an e-commerce query graph. While traditional approaches are largely
transductive, recent graph neural networks (GNNs) integrate node features with
network structures, thus enabling inductive node classification models that can
be applied to new nodes or even new graphs in the same feature space. However,
inter-graph differences still exist across graphs within the same domain. Thus,
training just one global model (e.g., a state-of-the-art GNN) to handle all new
graphs, whilst ignoring the inter-graph differences, can lead to suboptimal
performance.
  In this paper, we study the problem of inductive node classification across
graphs. Unlike existing one-model-fits-all approaches, we propose a novel
meta-inductive framework called MI-GNN to customize the inductive model to each
graph under a meta-learning paradigm. That is, MI-GNN does not directly learn
an inductive model; it learns the general knowledge of how to train a model for
semi-supervised node classification on new graphs. To cope with the differences
across graphs, MI-GNN employs a dual adaptation mechanism at both the graph and
task levels. More specifically, we learn a graph prior to adapt for the
graph-level differences, and a task prior to adapt for the task-level
differences conditioned on a graph. Extensive experiments on five real-world
graph collections demonstrate the effectiveness of our proposed model.",arxiv
http://arxiv.org/abs/1804.08369v1,2018-04-23T12:33:59Z,2018-04-23T12:33:59Z,Gaussian Material Synthesis,"We present a learning-based system for rapid mass-scale material synthesis
that is useful for novice and expert users alike. The user preferences are
learned via Gaussian Process Regression and can be easily sampled for new
recommendations. Typically, each recommendation takes 40-60 seconds to render
with global illumination, which makes this process impracticable for real-world
workflows. Our neural network eliminates this bottleneck by providing
high-quality image predictions in real time, after which it is possible to pick
the desired materials from a gallery and assign them to a scene in an intuitive
manner. Workflow timings against Disney's ""principled"" shader reveal that our
system scales well with the number of sought materials, thus empowering even
novice users to generate hundreds of high-quality material models without any
expertise in material modeling. Similarly, expert users experience a
significant decrease in the total modeling time when populating a scene with
materials. Furthermore, our proposed solution also offers controllable
recommendations and a novel latent space variant generation step to enable the
real-time fine-tuning of materials without requiring any domain expertise.",arxiv
http://arxiv.org/abs/2106.03415v1,2021-06-07T08:32:19Z,2021-06-07T08:32:19Z,"Leveraging Tripartite Interaction Information from Live Stream
  E-Commerce for Improving Product Recommendation","Recently, a new form of online shopping becomes more and more popular, which
combines live streaming with E-Commerce activity. The streamers introduce
products and interact with their audiences, and hence greatly improve the
performance of selling products. Despite of the successful applications in
industries, the live stream E-commerce has not been well studied in the data
science community. To fill this gap, we investigate this brand-new scenario and
collect a real-world Live Stream E-Commerce (LSEC) dataset. Different from
conventional E-commerce activities, the streamers play a pivotal role in the
LSEC events. Hence, the key is to make full use of rich interaction information
among streamers, users, and products. We first conduct data analysis on the
tripartite interaction data and quantify the streamer's influence on users'
purchase behavior. Based on the analysis results, we model the tripartite
information as a heterogeneous graph, which can be decomposed to multiple
bipartite graphs in order to better capture the influence. We propose a novel
Live Stream E-Commerce Graph Neural Network framework (LSEC-GNN) to learn the
node representations of each bipartite graph, and further design a multi-task
learning approach to improve product recommendation. Extensive experiments on
two real-world datasets with different scales show that our method can
significantly outperform various baseline approaches.",arxiv
http://arxiv.org/abs/2106.09929v1,2021-06-18T06:06:35Z,2021-06-18T06:06:35Z,Graph-based Joint Pandemic Concern and Relation Extraction on Twitter,"Public concern detection provides potential guidance to the authorities for
crisis management before or during a pandemic outbreak. Detecting people's
concerns and attention from online social media platforms has been widely
acknowledged as an effective approach to relieve public panic and prevent a
social crisis. However, detecting concerns in time from massive information in
social media turns out to be a big challenge, especially when sufficient
manually labeled data is in the absence of public health emergencies, e.g.,
COVID-19. In this paper, we propose a novel end-to-end deep learning model to
identify people's concerns and the corresponding relations based on Graph
Convolutional Network and Bi-directional Long Short Term Memory integrated with
Concern Graph. Except for the sequential features from BERT embeddings, the
regional features of tweets can be extracted by the Concern Graph module, which
not only benefits the concern detection but also enables our model to be high
noise-tolerant. Thus, our model can address the issue of insufficient manually
labeled data. We conduct extensive experiments to evaluate the proposed model
by using both manually labeled tweets and automatically labeled tweets. The
experimental results show that our model can outperform the state-of-art models
on real-world datasets.",arxiv
http://arxiv.org/abs/2110.09899v2,2021-10-20T18:20:25Z,2021-10-17T19:28:47Z,POLE: Polarized Embedding for Signed Networks,"From the 2016 U.S. presidential election to the 2021 Capitol riots to the
spread of misinformation related to COVID-19, many have blamed social media for
today's deeply divided society. Recent advances in machine learning for signed
networks hold the promise to guide small interventions with the goal of
reducing polarization in social media. However, existing models are especially
ineffective in predicting conflicts (or negative links) among users. This is
due to a strong correlation between link signs and the network structure, where
negative links between polarized communities are too sparse to be predicted
even by state-of-the-art approaches. To address this problem, we first design a
partition-agnostic polarization measure for signed graphs based on the signed
random-walk and show that many real-world graphs are highly polarized. Then, we
propose POLE (POLarized Embedding for signed networks), a signed embedding
method for polarized graphs that captures both topological and signed
similarities jointly via signed autocovariance. Through extensive experiments,
we show that POLE significantly outperforms state-of-the-art methods in signed
link prediction, particularly for negative links with gains of up to one order
of magnitude.",arxiv
http://arxiv.org/abs/2110.13957v2,2021-10-29T15:05:04Z,2021-10-26T18:44:37Z,Unbiased Graph Embedding with Biased Graph Observations,"Graph embedding techniques have been increasingly employed in real-world
machine learning tasks on graph-structured data, such as social recommendations
and protein structure modeling. Since the generation of a graph is inevitably
affected by some sensitive node attributes (such as gender and age of users in
a social network), the learned graph representations can inherit such sensitive
information and introduce undesirable biases in downstream tasks. Most existing
works on debiasing graph representations add ad-hoc constraints on the learned
embeddings to restrict their distributions, which however compromise the
utility of resulting graph representations in downstream tasks.
  In this paper, we propose a principled new way for obtaining unbiased
representations by learning from an underlying bias-free graph that is not
influenced by sensitive attributes. Based on this new perspective, we propose
two complementary methods for uncovering such an underlying graph with the goal
of introducing minimum impact on the utility of learned representations in
downstream tasks. Both our theoretical justification and extensive experiment
comparisons against state-of-the-art solutions demonstrate the effectiveness of
our proposed methods.",arxiv
http://arxiv.org/abs/1905.09558v1,2019-05-23T09:49:37Z,2019-05-23T09:49:37Z,"MR-GNN: Multi-Resolution and Dual Graph Neural Network for Predicting
  Structured Entity Interactions","Predicting interactions between structured entities lies at the core of
numerous tasks such as drug regimen and new material design. In recent years,
graph neural networks have become attractive. They represent structured
entities as graphs and then extract features from each individual graph using
graph convolution operations. However, these methods have some limitations: i)
their networks only extract features from a fix-sized subgraph structure (i.e.,
a fix-sized receptive field) of each node, and ignore features in substructures
of different sizes, and ii) features are extracted by considering each entity
independently, which may not effectively reflect the interaction between two
entities. To resolve these problems, we present MR-GNN, an end-to-end graph
neural network with the following features: i) it uses a multi-resolution based
architecture to extract node features from different neighborhoods of each
node, and, ii) it uses dual graph-state long short-term memory networks
(L-STMs) to summarize local features of each graph and extracts the interaction
features between pairwise graphs. Experiments conducted on real-world datasets
show that MR-GNN improves the prediction of state-of-the-art methods.",arxiv
http://arxiv.org/abs/1710.02595v2,2017-10-10T05:05:58Z,2017-10-06T21:42:15Z,Intelligent Pothole Detection and Road Condition Assessment,"Poor road conditions are a public nuisance, causing passenger discomfort,
damage to vehicles, and accidents. In the U.S., road-related conditions are a
factor in 22,000 of the 42,000 traffic fatalities each year. Although we often
complain about bad roads, we have no way to detect or report them at scale. To
address this issue, we developed a system to detect potholes and assess road
conditions in real-time. Our solution is a mobile application that captures
data on a car's movement from gyroscope and accelerometer sensors in the phone.
To assess roads using this sensor data, we trained SVM models to classify road
conditions with 93% accuracy and potholes with 92% accuracy, beating the base
rate for both problems. As the user drives, the models use the sensor data to
classify whether the road is good or bad, and whether it contains potholes.
Then, the classification results are used to create data-rich maps that
illustrate road conditions across the city. Our system will empower civic
officials to identify and repair damaged roads which inconvenience passengers
and cause accidents. This paper details our data science process for collecting
training data on real roads, transforming noisy sensor data into useful
signals, training and evaluating machine learning models, and deploying those
models to production through a real-time classification app. It also highlights
how cities can use our system to crowdsource data and deliver road repair
resources to areas in need.",arxiv
http://arxiv.org/abs/2009.03823v1,2020-09-08T15:43:16Z,2020-09-08T15:43:16Z,"QSAN: A Quantum-probability based Signed Attention Network for
  Explainable False Information Detection","False information detection on social media is challenging as it commonly
requires tedious evidence-collecting but lacks available comparative
information. Clues mined from user comments, as the wisdom of crowds, could be
of considerable benefit to this task. However, it is non-trivial to capture the
complex semantics from the contents and comments in consideration of their
implicit correlations. Although deep neural networks have good expressive
power, one major drawback is the lack of explainability. In this paper, we
focus on how to learn from the post contents and related comments in social
media to understand and detect the false information more effectively, with
explainability. We thus propose a Quantum-probability based Signed Attention
Network (QSAN) that integrates the quantum-driven text encoding and a novel
signed attention mechanism in a unified framework. QSAN is not only able to
distinguish important comments from the others, but also can exploit the
conflicting social viewpoints in the comments to facilitate the detection.
Moreover, QSAN is advantageous with its explainability in terms of transparency
due to quantum physics meanings and the attention weights. Extensive
experiments on real-world datasets show that our approach outperforms
state-of-the-art baselines and can provide different kinds of user comments to
explain why a piece of information is detected as false.",arxiv
http://arxiv.org/abs/2104.06095v4,2021-04-24T05:45:42Z,2021-04-13T11:00:48Z,"Relevance-Aware Anomalous Users Detection in Social Network via Graph
  Neural Network","Anomalous users detection in social network is an imperative task for
security problems. Motivated by the great power of Graph Neural Networks(GNNs),
many current researches adopt GNN-based detectors to reveal the anomalous
users. However, the increasing scale of social activities, explosive growth of
users and manifold technical disguise render the user detection a difficult
task. In this paper, we propose an innovate Relevance-aware Anomalous Users
Detection model (RAU-GNN) to obtain a fine-grained detection result. RAU-GNN
first extracts multiple relations of all types of users in social network,
including both benign and anomalous users, and accordingly constructs the
multiple user relation graph. Secondly, we employ relevance-aware GNN framework
to learn the hidden features of users, and discriminate the anomalous users
after discriminating. Concretely, by integrating Graph Convolution Network(GCN)
and Graph Attention Network(GAT), we design a GCN-based relation fusion layer
to aggregate initial information from different relations, and a GAT-based
embedding layer to obtain the high-level embeddings. Lastly, we feed the
learned representations to the following GNN layer in order to consolidate the
node embedding by aggregating the final users' embeddings. We conduct extensive
experiment on real-world datasets. The experimental results show that our
approach can achieve high accuracy for anomalous users detection.",arxiv
http://arxiv.org/abs/2106.03251v1,2021-06-06T21:17:27Z,2021-06-06T21:17:27Z,"DyDiff-VAE: A Dynamic Variational Framework for Information Diffusion
  Prediction","This paper describes a novel diffusion model, DyDiff-VAE, for information
diffusion prediction on social media. Given the initial content and a sequence
of forwarding users, DyDiff-VAE aims to estimate the propagation likelihood for
other potential users and predict the corresponding user rankings. Inferring
user interests from diffusion data lies the foundation of diffusion prediction,
because users often forward the information in which they are interested or the
information from those who share similar interests. Their interests also evolve
over time as the result of the dynamic social influence from neighbors and the
time-sensitive information gained inside/outside the social media. Existing
works fail to model users' intrinsic interests from the diffusion data and
assume user interests remain static along the time. DyDiff-VAE advances the
state of the art in two directions: (i) We propose a dynamic encoder to infer
the evolution of user interests from observed diffusion data. (ii) We propose a
dual attentive decoder to estimate the propagation likelihood by integrating
information from both the initial cascade content and the forwarding user
sequence. Extensive experiments on four real-world datasets from Twitter and
Youtube demonstrate the advantages of the proposed model; we show that it
achieves 43.3% relative gains over the best baseline on average. Moreover, it
has the lowest run-time compared with recurrent neural network based models.",arxiv
http://arxiv.org/abs/2007.14471v1,2020-07-22T13:33:44Z,2020-07-22T13:33:44Z,Learning to predict metal deformations in hot-rolling processes,"Hot-rolling is a metal forming process that produces a workpiece with a
desired target cross-section from an input workpiece through a sequence of
plastic deformations; each deformation is generated by a stand composed of
opposing rolls with a specific geometry. In current practice, the rolling
sequence (i.e., the sequence of stands and the geometry of their rolls) needed
to achieve a given final cross-section is designed by experts based on previous
experience, and iteratively refined in a costly trial-and-error process. Finite
Element Method simulations are increasingly adopted to make this process more
efficient and to test potential rolling sequences, achieving good accuracy at
the cost of long simulation times, limiting the practical use of the approach.
We propose a supervised learning approach to predict the deformation of a given
workpiece by a set of rolls with a given geometry; the model is trained on a
large dataset of procedurally-generated FEM simulations, which we publish as
supplementary material. The resulting predictor is four orders of magnitude
faster than simulations, and yields an average Jaccard Similarity Index of
0.972 (against ground truth from simulations) and 0.925 (against real-world
measured deformations); we additionally report preliminary results on using the
predictor for automatic planning of rolling sequences.",arxiv
http://arxiv.org/abs/2107.11646v2,2021-07-28T07:16:15Z,2021-07-24T16:28:06Z,Hand Image Understanding via Deep Multi-Task Learning,"Analyzing and understanding hand information from multimedia materials like
images or videos is important for many real world applications and remains
active in research community. There are various works focusing on recovering
hand information from single image, however, they usually solve a single task,
for example, hand mask segmentation, 2D/3D hand pose estimation, or hand mesh
reconstruction and perform not well in challenging scenarios. To further
improve the performance of these tasks, we propose a novel Hand Image
Understanding (HIU) framework to extract comprehensive information of the hand
object from a single RGB image, by jointly considering the relationships
between these tasks. To achieve this goal, a cascaded multi-task learning (MTL)
backbone is designed to estimate the 2D heat maps, to learn the segmentation
mask, and to generate the intermediate 3D information encoding, followed by a
coarse-to-fine learning paradigm and a self-supervised learning strategy.
Qualitative experiments demonstrate that our approach is capable of recovering
reasonable mesh representations even in challenging situations. Quantitatively,
our method significantly outperforms the state-of-the-art approaches on various
widely-used datasets, in terms of diverse evaluation metrics.",arxiv
http://arxiv.org/abs/1909.11822v1,2019-09-25T23:52:57Z,2019-09-25T23:52:57Z,"DisCo: Physics-Based Unsupervised Discovery of Coherent Structures in
  Spatiotemporal Systems","Extracting actionable insight from complex unlabeled scientific data is an
open challenge and key to unlocking data-driven discovery in science.
Complementary and alternative to supervised machine learning approaches,
unsupervised physics-based methods based on behavior-driven theories hold great
promise. Due to computational limitations, practical application on real-world
domain science problems has lagged far behind theoretical development. We
present our first step towards bridging this divide - DisCo - a
high-performance distributed workflow for the behavior-driven local causal
state theory. DisCo provides a scalable unsupervised physics-based
representation learning method that decomposes spatiotemporal systems into
their structurally relevant components, which are captured by the latent local
causal state variables. Complex spatiotemporal systems are generally highly
structured and organize around a lower-dimensional skeleton of coherent
structures, and in several firsts we demonstrate the efficacy of DisCo in
capturing such structures from observational and simulated scientific data. To
the best of our knowledge, DisCo is also the first application software
developed entirely in Python to scale to over 1000 machine nodes, providing
good performance along with ensuring domain scientists' productivity. We
developed scalable, performant methods optimized for Intel many-core processors
that will be upstreamed to open-source Python library packages. Our capstone
experiment, using newly developed DisCo workflow and libraries, performs
unsupervised spacetime segmentation analysis of CAM5.1 climate simulation data,
processing an unprecedented 89.5 TB in 6.6 minutes end-to-end using 1024 Intel
Haswell nodes on the Cori supercomputer obtaining 91% weak-scaling and 64%
strong-scaling efficiency.",arxiv
http://arxiv.org/abs/2104.14210v1,2021-04-29T08:59:36Z,2021-04-29T08:59:36Z,"Biased Edge Dropout for Enhancing Fairness in Graph Representation
  Learning","Graph representation learning has become a ubiquitous component in many
scenarios, ranging from social network analysis to energy forecasting in smart
grids. In several applications, ensuring the fairness of the node (or graph)
representations with respect to some protected attributes is crucial for their
correct deployment. Yet, fairness in graph deep learning remains
under-explored, with few solutions available. In particular, the tendency of
similar nodes to cluster on several real-world graphs (i.e., homophily) can
dramatically worsen the fairness of these procedures. In this paper, we propose
a biased edge dropout algorithm (FairDrop) to counter-act homophily and improve
fairness in graph representation learning. FairDrop can be plugged in easily on
many existing algorithms, is efficient, adaptable, and can be combined with
other fairness-inducing solutions. After describing the general algorithm, we
demonstrate its application on two benchmark tasks, specifically, as a random
walk model for producing node embeddings, and to a graph convolutional network
for link prediction. We prove that the proposed algorithm can successfully
improve the fairness of all models up to a small or negligible drop in
accuracy, and compares favourably with existing state-of-the-art solutions. In
an ablation study, we demonstrate that our algorithm can flexibly interpolate
between biasing towards fairness and an unbiased edge dropout. Furthermore, to
better evaluate the gains, we propose a new dyadic group definition to measure
the bias of a link prediction task when paired with group-based fairness
metrics. In particular, we extend the metric used to measure the bias in the
node embeddings to take into account the graph structure.",arxiv
http://arxiv.org/abs/1701.00038v1,2016-12-31T00:27:42Z,2016-12-31T00:27:42Z,Sparsity enabled cluster reduced-order models for control,"Characterizing and controlling nonlinear, multi-scale phenomena play
important roles in science and engineering. Cluster-based reduced-order
modeling (CROM) was introduced to exploit the underlying low-dimensional
dynamics of complex systems. CROM builds a data-driven discretization of the
Perron-Frobenius operator, resulting in a probabilistic model for ensembles of
trajectories. A key advantage of CROM is that it embeds nonlinear dynamics in a
linear framework, and uncertainty can be managed with data assimilation. CROM
is typically computed on high-dimensional data, however, access to and
computations on this full-state data limit the online implementation of CROM
for prediction and control. Here, we address this key challenge by identifying
a small subset of critical measurements to learn an efficient CROM, referred to
as sparsity-enabled CROM. In particular, we leverage compressive measurements
to faithfully embed the cluster geometry and preserve the probabilistic
dynamics. Further, we show how to identify fewer optimized sensor locations
tailored to a specific problem that outperform random measurements. Both of
these sparsity-enabled sensing strategies significantly reduce the burden of
data acquisition and processing for low-latency in-time estimation and control.
We illustrate this unsupervised learning approach on three different
high-dimensional nonlinear dynamical systems from fluids with increasing
complexity, with one application in flow control. Sparsity-enabled CROM is a
critical facilitator for real-time implementation on high-dimensional systems
where full-state information may be inaccessible.",arxiv
http://arxiv.org/abs/1910.02550v2,2019-10-14T17:29:36Z,2019-10-06T23:22:56Z,ClearGrasp: 3D Shape Estimation of Transparent Objects for Manipulation,"Transparent objects are a common part of everyday life, yet they possess
unique visual properties that make them incredibly difficult for standard 3D
sensors to produce accurate depth estimates for. In many cases, they often
appear as noisy or distorted approximations of the surfaces that lie behind
them. To address these challenges, we present ClearGrasp -- a deep learning
approach for estimating accurate 3D geometry of transparent objects from a
single RGB-D image for robotic manipulation. Given a single RGB-D image of
transparent objects, ClearGrasp uses deep convolutional networks to infer
surface normals, masks of transparent surfaces, and occlusion boundaries. It
then uses these outputs to refine the initial depth estimates for all
transparent surfaces in the scene. To train and test ClearGrasp, we construct a
large-scale synthetic dataset of over 50,000 RGB-D images, as well as a
real-world test benchmark with 286 RGB-D images of transparent objects and
their ground truth geometries. The experiments demonstrate that ClearGrasp is
substantially better than monocular depth estimation baselines and is capable
of generalizing to real-world images and novel objects. We also demonstrate
that ClearGrasp can be applied out-of-the-box to improve grasping algorithms'
performance on transparent objects. Code, data, and benchmarks will be
released. Supplementary materials available on the project website:
https://sites.google.com/view/cleargrasp",arxiv
http://arxiv.org/abs/1705.02047v3,2018-02-14T00:38:17Z,2017-05-04T23:47:12Z,Matrix Completion via Factorizing Polynomials,"Predicting unobserved entries of a partially observed matrix has found wide
applicability in several areas, such as recommender systems, computational
biology, and computer vision. Many scalable methods with rigorous theoretical
guarantees have been developed for algorithms where the matrix is factored into
low-rank components, and embeddings are learned for the row and column
entities. While there has been recent research on incorporating explicit side
information in the low-rank matrix factorization setting, often implicit
information can be gleaned from the data, via higher-order interactions among
entities. Such implicit information is especially useful in cases where the
data is very sparse, as is often the case in real-world datasets. In this
paper, we design a method to learn embeddings in the context of recommendation
systems, using the observation that higher powers of a graph transition
probability matrix encode the probability that a random walker will hit that
node in a given number of steps. We develop a coordinate descent algorithm to
solve the resulting optimization, that makes explicit computation of the higher
order powers of the matrix redundant, preserving sparsity and making
computations efficient. Experiments on several datasets show that our method,
that can use higher order information, outperforms methods that only use
explicitly available side information, those that use only second-order
implicit information and in some cases, methods based on deep neural networks
as well.",arxiv
http://arxiv.org/abs/2007.13004v1,2020-07-25T20:07:28Z,2020-07-25T20:07:28Z,Learning Attribute-Structure Co-Evolutions in Dynamic Graphs,"Most graph neural network models learn embeddings of nodes in static
attributed graphs for predictive analysis. Recent attempts have been made to
learn temporal proximity of the nodes. We find that real dynamic attributed
graphs exhibit complex co-evolution of node attributes and graph structure.
Learning node embeddings for forecasting change of node attributes and birth
and death of links over time remains an open problem. In this work, we present
a novel framework called CoEvoGNN for modeling dynamic attributed graph
sequence. It preserves the impact of earlier graphs on the current graph by
embedding generation through the sequence. It has a temporal self-attention
mechanism to model long-range dependencies in the evolution. Moreover, CoEvoGNN
optimizes model parameters jointly on two dynamic tasks, attribute inference
and link prediction over time. So the model can capture the co-evolutionary
patterns of attribute change and link formation. This framework can adapt to
any graph neural algorithms so we implemented and investigated three methods
based on it: CoEvoGCN, CoEvoGAT, and CoEvoSAGE. Experiments demonstrate the
framework (and its methods) outperform strong baselines on predicting an entire
unseen graph snapshot of personal attributes and interpersonal links in dynamic
social graphs and financial graphs.",arxiv
http://arxiv.org/abs/2008.10880v1,2020-08-25T08:27:11Z,2020-08-25T08:27:11Z,Improving Fair Predictions Using Variational Inference In Causal Models,"The importance of algorithmic fairness grows with the increasing impact
machine learning has on people's lives. Recent work on fairness metrics shows
the need for causal reasoning in fairness constraints. In this work, a
practical method named FairTrade is proposed for creating flexible prediction
models which integrate fairness constraints on sensitive causal paths. The
method uses recent advances in variational inference in order to account for
unobserved confounders. Further, a method outline is proposed which uses the
causal mechanism estimates to audit black box models. Experiments are conducted
on simulated data and on a real dataset in the context of detecting unlawful
social welfare. This research aims to contribute to machine learning techniques
which honour our ethical and legal boundaries.",arxiv
http://arxiv.org/abs/astro-ph/0210511v1,2002-10-23T14:06:24Z,2002-10-23T14:06:24Z,"SkyDOT (Sky Database for Objects in the Time Domain): A Virtual
  Observatory for Variability Studies at LANL","The mining of Virtual Observatories (VOs) is becoming a powerful new method
for discovery in astronomy. Here we report on the development of SkyDOT (Sky
Database for Objects in the Time domain), a new Virtual Observatory, which is
dedicated to the study of sky variability. The site will confederate a number
of massive variability surveys and enable exploration of the time domain in
astronomy. We discuss the architecture of the database and the functionality of
the user interface. An important aspect of SkyDOT is that it is continuously
updated in near real time so that users can access new observations in a timely
manner. The site will also utilize high level machine learning tools that will
allow sophisticated mining of the archive. Another key feature is the real time
data stream provided by RAPTOR (RAPid Telescopes for Optical Response), a new
sky monitoring experiment under construction at Los Alamos National Laboratory
(LANL).",arxiv
http://arxiv.org/abs/1403.6652v2,2014-06-27T17:17:25Z,2014-03-26T12:30:07Z,DeepWalk: Online Learning of Social Representations,"We present DeepWalk, a novel approach for learning latent representations of
vertices in a network. These latent representations encode social relations in
a continuous vector space, which is easily exploited by statistical models.
DeepWalk generalizes recent advancements in language modeling and unsupervised
feature learning (or deep learning) from sequences of words to graphs. DeepWalk
uses local information obtained from truncated random walks to learn latent
representations by treating walks as the equivalent of sentences. We
demonstrate DeepWalk's latent representations on several multi-label network
classification tasks for social networks such as BlogCatalog, Flickr, and
YouTube. Our results show that DeepWalk outperforms challenging baselines which
are allowed a global view of the network, especially in the presence of missing
information. DeepWalk's representations can provide $F_1$ scores up to 10%
higher than competing methods when labeled data is sparse. In some experiments,
DeepWalk's representations are able to outperform all baseline methods while
using 60% less training data. DeepWalk is also scalable. It is an online
learning algorithm which builds useful incremental results, and is trivially
parallelizable. These qualities make it suitable for a broad class of real
world applications such as network classification, and anomaly detection.",arxiv
http://arxiv.org/abs/2103.07597v1,2021-03-13T02:05:26Z,2021-03-13T02:05:26Z,"DeepGroup: Representation Learning for Group Recommendation with
  Implicit Feedback","Group recommender systems facilitate group decision making for a set of
individuals (e.g., a group of friends, a team, a corporation, etc.). Many of
these systems, however, either assume that (i) user preferences can be elicited
(or inferred) and then aggregated into group preferences or (ii) group
preferences are partially observed/elicited. We focus on making recommendations
for a new group of users whose preferences are unknown, but we are given the
decisions/choices of other groups. By formulating this problem as group
recommendation from group implicit feedback, we focus on two of its practical
instances: group decision prediction and reverse social choice. Given a set of
groups and their observed decisions, group decision prediction intends to
predict the decision of a new group of users, whereas reverse social choice
aims to infer the preferences of those users involved in observed group
decisions. These two problems are of interest to not only group recommendation,
but also to personal privacy when the users intend to conceal their personal
preferences but have participated in group decisions. To tackle these two
problems, we propose and study DeepGroup -- a deep learning approach for group
recommendation with group implicit data. We empirically assess the predictive
power of DeepGroup on various real-world datasets, group conditions (e.g.,
homophily or heterophily), and group decision (or voting) rules. Our extensive
experiments not only demonstrate the efficacy of DeepGroup, but also shed light
on the privacy-leakage concerns of some decision making processes.",arxiv
http://arxiv.org/abs/2011.02838v1,2020-10-11T15:04:34Z,2020-10-11T15:04:34Z,"Real-time parameter inference in reduced-order flame models with
  heteroscedastic Bayesian neural network ensembles","The estimation of model parameters with uncertainties from observed data is a
ubiquitous inverse problem in science and engineering. In this paper, we
suggest an inexpensive and easy to implement parameter estimation technique
that uses a heteroscedastic Bayesian Neural Network trained using anchored
ensembling. The heteroscedastic aleatoric error of the network models the
irreducible uncertainty due to parameter degeneracies in our inverse problem,
while the epistemic uncertainty of the Bayesian model captures uncertainties
which may arise from an input observation's out-of-distribution nature. We use
this tool to perform real-time parameter inference in a 6 parameter G-equation
model of a ducted, premixed flame from observations of acoustically excited
flames. We train our networks on a library of 2.1 million simulated flame
videos. Results on the test dataset of simulated flames show that the network
recovers flame model parameters, with the correlation coefficient between
predicted and true parameters ranging from 0.97 to 0.99, and well-calibrated
uncertainty estimates. The trained neural networks are then used to infer model
parameters from real videos of a premixed Bunsen flame captured using a
high-speed camera in our lab. Re-simulation using inferred parameters shows
excellent agreement between the real and simulated flames. Compared to Ensemble
Kalman Filter-based tools that have been proposed for this problem in the
combustion literature, our neural network ensemble achieves better
data-efficiency and our sub-millisecond inference times represent a savings on
computational costs by several orders of magnitude. This allows us to calibrate
our reduced-order flame model in real-time and predict the thermoacoustic
instability behaviour of the flame more accurately.",arxiv
http://arxiv.org/abs/2106.04880v1,2021-06-09T08:03:57Z,2021-06-09T08:03:57Z,Self-Improved Retrosynthetic Planning,"Retrosynthetic planning is a fundamental problem in chemistry for finding a
pathway of reactions to synthesize a target molecule. Recently, search
algorithms have shown promising results for solving this problem by using deep
neural networks (DNNs) to expand their candidate solutions, i.e., adding new
reactions to reaction pathways. However, the existing works on this line are
suboptimal; the retrosynthetic planning problem requires the reaction pathways
to be (a) represented by real-world reactions and (b) executable using
""building block"" molecules, yet the DNNs expand reaction pathways without fully
incorporating such requirements. Motivated by this, we propose an end-to-end
framework for directly training the DNNs towards generating reaction pathways
with the desirable properties. Our main idea is based on a self-improving
procedure that trains the model to imitate successful trajectories found by
itself. We also propose a novel reaction augmentation scheme based on a forward
reaction model. Our experiments demonstrate that our scheme significantly
improves the success rate of solving the retrosynthetic problem from 86.84% to
96.32% while maintaining the performance of DNN for predicting valid reactions.",arxiv
http://arxiv.org/abs/1611.08135v1,2016-11-24T11:01:32Z,2016-11-24T11:01:32Z,"Question Retrieval for Community-based Question Answering via
  Heterogeneous Network Integration Learning","Community based question answering platforms have attracted substantial users
to share knowledge and learn from each other. As the rapid enlargement of CQA
platforms, quantities of overlapped questions emerge, which makes users
confounded to select a proper reference. It is urgent for us to take effective
automated algorithms to reuse historical questions with corresponding answers.
In this paper we focus on the problem with question retrieval, which aims to
match historical questions that are relevant or semantically equivalent to
resolve one s query directly. The challenges in this task are the lexical gaps
between questions for the word ambiguity and word mismatch problem.
Furthermore, limited words in queried sentences cause sparsity of word
features. To alleviate these challenges, we propose a novel framework named
HNIL which encodes not only the question contents but also the askers social
interactions to enhance the question embedding performance. More specifically,
we apply random walk based learning method with recurrent neural network to
match the similarities between askers question and historical questions
proposed by other users. Extensive experiments on a large scale dataset from a
real world CQA site show that employing the heterogeneous social network
information outperforms the other state of the art solutions in this task.",arxiv
http://arxiv.org/abs/2101.11206v1,2021-01-27T05:05:25Z,2021-01-27T05:05:25Z,"Adversarial Active Learning based Heterogeneous Graph Neural Network for
  Fake News Detection","The explosive growth of fake news along with destructive effects on politics,
economy, and public safety has increased the demand for fake news detection.
Fake news on social media does not exist independently in the form of an
article. Many other entities, such as news creators, news subjects, and so on,
exist on social media and have relationships with news articles. Different
entities and relationships can be modeled as a heterogeneous information
network (HIN). In this paper, we attempt to solve the fake news detection
problem with the support of a news-oriented HIN. We propose a novel fake news
detection framework, namely Adversarial Active Learning-based Heterogeneous
Graph Neural Network (AA-HGNN) which employs a novel hierarchical attention
mechanism to perform node representation learning in the HIN. AA-HGNN utilizes
an active learning framework to enhance learning performance, especially when
facing the paucity of labeled data. An adversarial selector will be trained to
query high-value candidates for the active learning framework. When the
adversarial active learning is completed, AA-HGNN detects fake news by
classifying news article nodes. Experiments with two real-world fake news
datasets show that our model can outperform text-based models and other
graph-based models when using less labeled data benefiting from the adversarial
active learning. As a model with generalizability, AA-HGNN also has the ability
to be widely used in other node classification-related applications on
heterogeneous graphs.",arxiv
http://arxiv.org/abs/2108.08134v1,2021-08-18T13:21:02Z,2021-08-18T13:21:02Z,Hyperbolic Hypergraphs for Sequential Recommendation,"Hypergraphs have been becoming a popular choice to model complex,
non-pairwise, and higher-order interactions for recommender system. However,
compared with traditional graph-based methods, the constructed hypergraphs are
usually much sparser, which leads to a dilemma when balancing the benefits of
hypergraphs and the modelling difficulty. Moreover, existing sequential
hypergraph recommendation overlooks the temporal modelling among user
relationships, which neglects rich social signals from the recommendation data.
To tackle the above shortcomings of the existing hypergraph-based sequential
recommendations, we propose a novel architecture named Hyperbolic Hypergraph
representation learning method for Sequential Recommendation (H2SeqRec) with
pre-training phase. Specifically, we design three self-supervised tasks to
obtain the pre-training item embeddings to feed or fuse into the following
recommendation architecture (with two ways to use the pre-trained embeddings).
In the recommendation phase, we learn multi-scale item embeddings via a
hierarchical structure to capture multiple time-span information. To alleviate
the negative impact of sparse hypergraphs, we utilize a hyperbolic space-based
hypergraph convolutional neural network to learn the dynamic item embeddings.
Also, we design an item enhancement module to capture dynamic social
information at each timestamp to improve effectiveness. Extensive experiments
are conducted on two real-world datasets to prove the effectiveness and high
performance of the model.",arxiv
http://arxiv.org/abs/1808.06354v1,2018-08-20T09:13:53Z,2018-08-20T09:13:53Z,Signed Graph Convolutional Network,"Due to the fact much of today's data can be represented as graphs, there has
been a demand for generalizing neural network models for graph data. One recent
direction that has shown fruitful results, and therefore growing interest, is
the usage of graph convolutional neural networks (GCNs). They have been shown
to provide a significant improvement on a wide range of tasks in network
analysis, one of which being node representation learning. The task of learning
low-dimensional node representations has shown to increase performance on a
plethora of other tasks from link prediction and node classification, to
community detection and visualization. Simultaneously, signed networks (or
graphs having both positive and negative links) have become ubiquitous with the
growing popularity of social media. However, since previous GCN models have
primarily focused on unsigned networks (or graphs consisting of only positive
links), it is unclear how they could be applied to signed networks due to the
challenges presented by negative links. The primary challenges are based on
negative links having not only a different semantic meaning as compared to
positive links, but their principles are inherently different and they form
complex relations with positive links. Therefore we propose a dedicated and
principled effort that utilizes balance theory to correctly aggregate and
propagate the information across layers of a signed GCN model. We perform
empirical experiments comparing our proposed signed GCN against
state-of-the-art baselines for learning node representations in signed
networks. More specifically, our experiments are performed on four real-world
datasets for the classical link sign prediction problem that is commonly used
as the benchmark for signed network embeddings algorithms.",arxiv
http://arxiv.org/abs/1801.08629v1,2018-01-25T23:23:04Z,2018-01-25T23:23:04Z,"Forecasting Suspicious Account Activity at Large-Scale Online Service
  Providers","In the face of large-scale automated social engineering attacks to large
online services, fast detection and remediation of compromised accounts are
crucial to limit the spread of new attacks and to mitigate the overall damage
to users, companies, and the public at large. We advocate a fully automated
approach based on machine learning: we develop an early warning system that
harnesses account activity traces to predict which accounts are likely to be
compromised in the future and generate suspicious activity. We hypothesize that
this early warning is key for a more timely detection of compromised accounts
and consequently faster remediation. We demonstrate the feasibility and
applicability of the system through an experiment at a large-scale online
service provider using four months of real-world production data encompassing
hundreds of millions of users. We show that - even using only login data to
derive features with low computational cost, and a basic model selection
approach - our classifier can be tuned to achieve good classification precision
when used for forecasting. Our system correctly identifies up to one month in
advance the accounts later flagged as suspicious with precision, recall, and
false positive rates that indicate the mechanism is likely to prove valuable in
operational settings to support additional layers of defense.",arxiv
http://arxiv.org/abs/1810.11973v1,2018-10-29T06:30:54Z,2018-10-29T06:30:54Z,Feature Bagging for Steganographer Identification,"Traditional steganalysis algorithms focus on detecting the existence of
steganography in a single object. In practice, one may face a complex scenario
where one or some of multiple users also called actors are guilty of using
steganography, which is defined as the steganographer identification problem
(SIP). This requires steganalysis experts to design effective and robust
detection algorithms to identify the guilty actor(s). The mainstream works use
clustering, ensemble and anomaly detection, where distances in high dimensional
space between features of actors are determined to find out the outlier(s)
corresponding to steganographer(s). However, in high dimensional space, feature
points could be sparse such that distances between feature points may become
relatively similar to each other, which cannot benefit the detection. Moreover,
it is well-known in machine learning that combining techniques such as boosting
and bagging can be effective in improving detection performance. This motivates
the authors in this paper to present a feature bagging approach to SIP. The
proposed work merges results from multiple detection sub-models, each of which
feature space is randomly sampled from the raw full dimensional space. We
create a new dataset called ImgNetEase including 5108 images downloaded from a
social website to mimic the real-world scenario. We extract PEV-274 features
from images, and take nsF5 as the steganographic algorithm for evaluation.
Experiments have shown that our work improves the detection accuracy
significantly on created dataset in most cases, which has shown the superiority
and applicability.",arxiv
http://arxiv.org/abs/1909.10492v1,2019-09-23T17:20:16Z,2019-09-23T17:20:16Z,Modeling Peoples Voting Behavior with Poll Information,"Despite the prevalence of voting systems in the real world there is no
consensus among researchers of how people vote strategically, even in simple
voting settings. This paper addresses this gap by comparing different
approaches that have been used to model strategic voting, including expected
utility maximization, heuristic decisionmaking, and bounded rationality models.
The models are applied to data collected from hundreds of people in controlled
voting experiments, where people vote after observing non-binding poll
information. We introduce a new voting model, the Attainability- Utility (AU)
heuristic, which weighs the popularity of a candidate according to the poll,
with the utility of the candidate to the voter. We argue that the AU model is
cognitively plausible, and show that it is able to predict peoples voting
behavior significantly better than other models from the literature. It was
almost at par with (and sometimes better than) a machine learning algorithm
that uses substantially more information. Our results provide new insights into
the strategic considerations of voters, that undermine the prevalent
assumptions of much theoretical work in social choice.",arxiv
http://arxiv.org/abs/2005.12762v2,2020-05-27T13:32:15Z,2020-05-26T14:34:07Z,"Exploring aspects of similarity between spoken personal narratives by
  disentangling them into narrative clause types","Sharing personal narratives is a fundamental aspect of human social behavior
as it helps share our life experiences. We can tell stories and rely on our
background to understand their context, similarities, and differences. A
substantial effort has been made towards developing storytelling machines or
inferring characters' features. However, we don't usually find models that
compare narratives. This task is remarkably challenging for machines since
they, as sometimes we do, lack an understanding of what similarity means. To
address this challenge, we first introduce a corpus of real-world spoken
personal narratives comprising 10,296 narrative clauses from 594 video
transcripts. Second, we ask non-narrative experts to annotate those clauses
under Labov's sociolinguistic model of personal narratives (i.e., action,
orientation, and evaluation clause types) and train a classifier that reaches
84.7% F-score for the highest-agreed clauses. Finally, we match stories and
explore whether people implicitly rely on Labov's framework to compare
narratives. We show that actions followed by the narrator's evaluation of these
are the aspects non-experts consider the most. Our approach is intended to help
inform machine learning methods aimed at studying or representing personal
narratives.",arxiv
http://arxiv.org/abs/2011.03113v1,2020-11-05T21:59:37Z,2020-11-05T21:59:37Z,Evaluating the Performance of Twitter-based Exploit Detectors,"Patch prioritization is a crucial aspect of information systems security, and
knowledge of which vulnerabilities were exploited in the wild is a powerful
tool to help systems administrators accomplish this task. The analysis of
social media for this specific application can enhance the results and bring
more agility by collecting data from online discussions and applying machine
learning techniques to detect real-world exploits. In this paper, we use a
technique that combines Twitter data with public database information to
classify vulnerabilities as exploited or not-exploited. We analyze the behavior
of different classifying algorithms, investigate the influence of different
antivirus data as ground truth, and experiment with various time window sizes.
Our findings suggest that using a Light Gradient Boosting Machine (LightGBM)
can benefit the results, and for most cases, the statistics related to a tweet
and the users who tweeted are more meaningful than the text tweeted. We also
demonstrate the importance of using ground-truth data from security companies
not mentioned in previous works.",arxiv
http://arxiv.org/abs/2009.02617v1,2020-09-05T23:55:24Z,2020-09-05T23:55:24Z,"Artefact removal in ground truth and noise model deficient sub-cellular
  nanoscopy images using auto-encoder deep learning","Image denoising or artefact removal using deep learning is possible in the
availability of supervised training dataset acquired in real experiments or
synthesized using known noise models. Neither of the conditions can be
fulfilled for nanoscopy (super-resolution optical microscopy) images that are
generated from microscopy videos through statistical analysis techniques. Due
to several physical constraints, supervised dataset cannot be measured. Due to
non-linear spatio-temporal mixing of data and valuable statistics of
fluctuations from fluorescent molecules which compete with noise statistics,
noise or artefact models in nanoscopy images cannot be explicitly learnt.
Therefore, such problem poses unprecedented challenges to deep learning. Here,
we propose a robust and versatile simulation-supervised training approach of
deep learning auto-encoder architectures for the highly challenging nanoscopy
images of sub-cellular structures inside biological samples. We show the proof
of concept for one nanoscopy method and investigate the scope of
generalizability across structures, noise models, and nanoscopy algorithms not
included during simulation-supervised training. We also investigate a variety
of loss functions and learning models and discuss the limitation of existing
performance metrics for nanoscopy images. We generate valuable insights for
this highly challenging and unsolved problem in nanoscopy, and set the
foundation for application of deep learning problems in nanoscopy for life
sciences.",arxiv
http://arxiv.org/abs/1512.01818v5,2016-07-14T22:51:39Z,2015-12-06T18:52:51Z,"SentiBench - a benchmark comparison of state-of-the-practice sentiment
  analysis methods","In the last few years thousands of scientific papers have investigated
sentiment analysis, several startups that measure opinions on real data have
emerged and a number of innovative products related to this theme have been
developed. There are multiple methods for measuring sentiments, including
lexical-based and supervised machine learning methods. Despite the vast
interest on the theme and wide popularity of some methods, it is unclear which
one is better for identifying the polarity (i.e., positive or negative) of a
message. Accordingly, there is a strong need to conduct a thorough
apple-to-apple comparison of sentiment analysis methods, \textit{as they are
used in practice}, across multiple datasets originated from different data
sources. Such a comparison is key for understanding the potential limitations,
advantages, and disadvantages of popular methods. This article aims at filling
this gap by presenting a benchmark comparison of twenty-four popular sentiment
analysis methods (which we call the state-of-the-practice methods). Our
evaluation is based on a benchmark of eighteen labeled datasets, covering
messages posted on social networks, movie and product reviews, as well as
opinions and comments in news articles. Our results highlight the extent to
which the prediction performance of these methods varies considerably across
datasets. Aiming at boosting the development of this research area, we open the
methods' codes and datasets used in this article, deploying them in a benchmark
system, which provides an open API for accessing and comparing sentence-level
sentiment analysis methods.",arxiv
http://arxiv.org/abs/2105.11914v2,2021-08-24T13:58:11Z,2021-05-25T13:17:35Z,Theory and Design of Super-resolution Haptic Skins,"Haptic feedback is important to make robots more dexterous and effective in
unstructured environments. High-resolution haptic sensors are still not widely
available, and their application is often bound by the resolution-robustness
dilemma. A route towards high-resolution and robust skin embeds a few sensor
units (taxels) into a flexible surface material and uses signal processing to
achieve sensing with super-resolution accuracy. We propose a theory for
geometric super-resolution to guide the development of haptic sensors of this
kind and link it to machine learning techniques for signal processing. This
theory is based on sensor isolines and allows us to predict force sensitivity
and accuracy in contact position and force magnitude as a spatial quantity. We
evaluate the influence of different factors, such as elastic properties of the
material, structure design, and transduction methods, using finite element
simulations and by implementing real sensors. We empirically determine sensor
isolines and validate the theory in two custom-built sensors with barometric
units for 1D and 2D measurement surfaces. Using machine learning methods for
the inference of contact information, our sensors obtain an unparalleled
average super-resolution factor of over 100 and 1200, respectively. Our theory
can guide future haptic sensor designs and inform various design choices.",arxiv
http://arxiv.org/abs/1912.01160v2,2020-02-10T02:38:59Z,2019-12-03T02:34:11Z,Neighborhood Cognition Consistent Multi-Agent Reinforcement Learning,"Social psychology and real experiences show that cognitive consistency plays
an important role to keep human society in order: if people have a more
consistent cognition about their environments, they are more likely to achieve
better cooperation. Meanwhile, only cognitive consistency within a neighborhood
matters because humans only interact directly with their neighbors. Inspired by
these observations, we take the first step to introduce \emph{neighborhood
cognitive consistency} (NCC) into multi-agent reinforcement learning (MARL).
Our NCC design is quite general and can be easily combined with existing MARL
methods. As examples, we propose neighborhood cognition consistent deep
Q-learning and Actor-Critic to facilitate large-scale multi-agent cooperations.
Extensive experiments on several challenging tasks (i.e., packet routing, wifi
configuration, and Google football player control) justify the superior
performance of our methods compared with state-of-the-art MARL approaches.",arxiv
http://arxiv.org/abs/1908.04172v2,2019-08-29T19:16:10Z,2019-08-12T14:30:13Z,"nGraph-HE2: A High-Throughput Framework for Neural Network Inference on
  Encrypted Data","In previous work, Boemer et al. introduced nGraph-HE, an extension to the
Intel nGraph deep learning (DL) compiler, that enables data scientists to
deploy models with popular frameworks such as TensorFlow and PyTorch with
minimal code changes. However, the class of supported models was limited to
relatively shallow networks with polynomial activations. Here, we introduce
nGraph-HE2, which extends nGraph-HE to enable privacy-preserving inference on
standard, pre-trained models using their native activation functions and number
fields (typically real numbers). The proposed framework leverages the CKKS
scheme, whose support for real numbers is friendly to data science, and a
client-aided model using a two-party approach to compute activation functions.
  We first present CKKS-specific optimizations, enabling a 3x-88x runtime
speedup for scalar encoding, and doubling the throughput through a novel use of
CKKS plaintext packing into complex numbers. Second, we optimize
ciphertext-plaintext addition and multiplication, yielding 2.6x-4.2x runtime
speedup. Third, we exploit two graph-level optimizations: lazy rescaling and
depth-aware encoding, which allow us to significantly improve performance.
  Together, these optimizations enable state-of-the-art throughput of 1,998
images/s on the CryptoNets network. Using the client-aided model, we also
present homomorphic evaluation of (to our knowledge) the largest network to
date, namely, pre-trained MobileNetV2 models on the ImageNet dataset, with
60.4\percent/82.7\percent\ top-1/top-5 accuracy and an amortized runtime of 381
ms/image.",arxiv
http://arxiv.org/abs/2110.00869v1,2021-10-02T19:52:48Z,2021-10-02T19:52:48Z,BdSL36: A Dataset for Bangladeshi Sign Letters Recognition,"Bangladeshi Sign Language (BdSL) is a commonly used medium of communication
for the hearing-impaired people in Bangladesh. A real-time BdSL interpreter
with no controlled lab environment has a broad social impact and an interesting
avenue of research as well. Also, it is a challenging task due to the variation
in different subjects (age, gender, color, etc.), complex features, and
similarities of signs and clustered backgrounds. However, the existing dataset
for BdSL classification task is mainly built in a lab friendly setup which
limits the application of powerful deep learning technology. In this paper, we
introduce a dataset named BdSL36 which incorporates background augmentation to
make the dataset versatile and contains over four million images belonging to
36 categories. Besides, we annotate about 40,000 images with bounding boxes to
utilize the potentiality of object detection algorithms. Furthermore, several
intensive experiments are performed to establish the baseline performance of
our BdSL36. Moreover, we employ beta testing of our classifiers at the user
level to justify the possibilities of real-world application with this dataset.
We believe our BdSL36 will expedite future research on practical sign letter
classification. We make the datasets and all the pre-trained models available
for further researcher.",arxiv
http://arxiv.org/abs/2005.10224v2,2021-06-05T22:47:16Z,2020-05-20T17:41:40Z,The Random Feature Model for Input-Output Maps between Banach Spaces,"Well known to the machine learning community, the random feature model is a
parametric approximation to kernel interpolation or regression methods. It is
typically used to approximate functions mapping a finite-dimensional input
space to the real line. In this paper, we instead propose a methodology for use
of the random feature model as a data-driven surrogate for operators that map
an input Banach space to an output Banach space. Although the methodology is
quite general, we consider operators defined by partial differential equations
(PDEs); here, the inputs and outputs are themselves functions, with the input
parameters being functions required to specify the problem, such as initial
data or coefficients, and the outputs being solutions of the problem. Upon
discretization, the model inherits several desirable attributes from this
infinite-dimensional viewpoint, including mesh-invariant approximation error
with respect to the true PDE solution map and the capability to be trained at
one mesh resolution and then deployed at different mesh resolutions. We view
the random feature model as a non-intrusive data-driven emulator, provide a
mathematical framework for its interpretation, and demonstrate its ability to
efficiently and accurately approximate the nonlinear parameter-to-solution maps
of two prototypical PDEs arising in physical science and engineering
applications: viscous Burgers' equation and a variable coefficient elliptic
equation.",arxiv
http://arxiv.org/abs/1810.08517v1,2018-10-19T14:21:14Z,2018-10-19T14:21:14Z,"Developing a seismic pattern interpretation network (SpiNet) for
  automated seismic interpretation","Seismic interpretation is now serving as a fundamental tool for depicting
subsurface geology and assisting activities in various domains, such as
environmental engineering and petroleum exploration. However, most of the
existing interpretation techniques are designed for interpreting a certain
seismic pattern (e.g., faults and salt domes) in a given seismic dataset at one
time; correspondingly, the rest patterns would be ignored. Interpreting all the
important seismic patterns becomes feasible with the aid of multiple
classification techniques. When implementing them into the seismic domain,
however, the major drawback is the low efficiency particularly for a large
dataset, since the classification need to be repeated at every seismic sample.
To resolve such limitation, this study first present a seismic pattern
interpretation dataset (SpiDat), which tentatively categorizes 12
commonly-observed seismic patterns based on their signal intensity and lateral
geometry, including these of important geologic implications such as faults,
salt domes, gas chimneys, and depositional sequences. Then we propose a seismic
pattern interpretation network (SpiNet) based on the state-of-the-art
deconvolutional neural network, which is capable of automatically recognizing
and annotating the 12 defined seismic patterns in real time. The impacts of the
proposed SpiNet come in two folds. First, applying the SpiNet to a seismic cube
allows interpreters to quickly identify the important seismic patterns as input
to advanced interpretation and modeling. Second, the SpiNet paves the
foundation for deriving more task-oriented seismic interpretation networks,
such as fault detection. It is concluded that the proposed SpiNet holds great
potentials for assisting the major seismic interpretation challenges and
advancing it further towards cognitive seismic data analysis.",arxiv
http://arxiv.org/abs/1712.07727v1,2017-12-20T22:09:38Z,2017-12-20T22:09:38Z,PERS: A Personalized and Explainable POI Recommender System,"The Location-Based Social Networks (LBSN) (e.g., Facebook) have many factors
(for instance, ratings, check-in time, etc.) that play a crucial role for the
Point-of-Interest (POI) recommendations. Unlike ratings, the reviews can help
users to elaborate their opinion and share the extent of consumption experience
in terms of the relevant factors of interest (aspects). Though some of the
existing recommendation systems have been using the user reviews, most of them
are less transparent and non-interpretable. These reasons have induced
considerable attention towards explainable and interpretable recommendation. To
the best of our knowledge, this is the first paper to exploit the user reviews
to incorporate the sentiment and opinions on different aspects for the
personalized and explainable POI recommendation. In this paper, we propose a
model termed as PERS (Personalized Explainable POI Recommender System) which
models the review-aspect category correlation by exploiting deep neural
network, formulates the user-aspect category bipartite relation as a bipartite
graph, and models the explainable recommendation using bipartite core-based and
ranking-based methods. The major contributions of this paper are: (i) it models
users and locations based on the aspects posted by user via reviews, (ii) it
exploits a deep neural network to model the review-aspect category correlation,
(iii) it provisions the incorporation of multiple contexts (e.g., categorical,
spatial, etc.) in the POI recommendation model, (iv) it formulates the
preference of users' on aspect category as a bipartite relation, represents it
as a location-aspect category bipartite graph, and models the explainable
recommendation with the notion of ordered dense subgraph extraction using
bipartite core-based and ranking-based approaches, and (v) it evaluates the
generated recommendation with three real-world datasets.",arxiv
http://arxiv.org/abs/1706.08685v1,2017-06-27T06:38:53Z,2017-06-27T06:38:53Z,"Material Recognition CNNs and Hierarchical Planning for Biped Robot
  Locomotion on Slippery Terrain","In this paper we tackle the problem of visually predicting surface friction
for environments with diverse surfaces, and integrating this knowledge into
biped robot locomotion planning. The problem is essential for autonomous robot
locomotion since diverse surfaces with varying friction abound in the real
world, from wood to ceramic tiles, grass or ice, which may cause difficulties
or huge energy costs for robot locomotion if not considered. We propose to
estimate friction and its uncertainty from visual estimation of material
classes using convolutional neural networks, together with probability
distribution functions of friction associated with each material. We then
robustly integrate the friction predictions into a hierarchical (footstep and
full-body) planning method using chance constraints, and optimize the same
trajectory costs at both levels of the planning method for consistency. Our
solution achieves fully autonomous perception and locomotion on slippery
terrain, which considers not only friction and its uncertainty, but also
collision, stability and trajectory cost. We show promising friction prediction
results in real pictures of outdoor scenarios, and planning experiments on a
real robot facing surfaces with different friction.",arxiv
http://arxiv.org/abs/2002.10632v1,2020-02-25T02:43:36Z,2020-02-25T02:43:36Z,"Genetic Algorithm-Guided Deep Learning of Grain Boundary Diagrams:
  Addressing the Challenge of Five Degrees of Freedom","Grain boundaries (GBs) often control the processing and properties of
polycrystalline materials. Here, a potentially transformative research is
represented by constructing GB property diagrams as functions of temperature
and bulk composition, also called ""complexion diagrams,"" as a general materials
science tool on par with phase diagrams. However, a GB has five macroscopic
(crystallographic) degrees of freedom (DOFs). It is essentially a ""mission
impossible"" to construct property diagrams for GBs as a function of five DOFs
by either experiments or modeling. Herein, we combine isobaric
semi-grand-canonical ensemble hybrid Monte Carlo and molecular dynamics (hybrid
MC/MD) simulations with a genetic algorithm (GA) and deep neural network (DNN)
models to tackle this grand challenge. The DNN prediction is ~108 faster than
atomistic simulations, thereby enabling the construction of the property
diagrams for millions of distinctly different GBs of five DOFs. Notably,
excellent prediction accuracies have been achieved for not only symmetric-tilt
and twist GBs, but also asymmetric-tilt and mixed tilt-twist GBs; the latter
are more complex and much less understood, but they are ubiquitous and often
limit the performance properties of real polycrystals as the weak links. The
data-driven prediction of GB properties as function of temperature, bulk
composition, and five crystallographic DOFs (i.e., in a 7D space) opens a new
paradigm.",arxiv
http://arxiv.org/abs/1804.05271v3,2019-02-17T03:42:14Z,2018-04-14T20:21:48Z,"Adaptive Federated Learning in Resource Constrained Edge Computing
  Systems","Emerging technologies and applications including Internet of Things (IoT),
social networking, and crowd-sourcing generate large amounts of data at the
network edge. Machine learning models are often built from the collected data,
to enable the detection, classification, and prediction of future events. Due
to bandwidth, storage, and privacy concerns, it is often impractical to send
all the data to a centralized location. In this paper, we consider the problem
of learning model parameters from data distributed across multiple edge nodes,
without sending raw data to a centralized place. Our focus is on a generic
class of machine learning models that are trained using gradient-descent based
approaches. We analyze the convergence bound of distributed gradient descent
from a theoretical point of view, based on which we propose a control algorithm
that determines the best trade-off between local update and global parameter
aggregation to minimize the loss function under a given resource budget. The
performance of the proposed algorithm is evaluated via extensive experiments
with real datasets, both on a networked prototype system and in a larger-scale
simulated environment. The experimentation results show that our proposed
approach performs near to the optimum with various machine learning models and
different data distributions.",arxiv
http://arxiv.org/abs/2106.13711v1,2021-06-22T21:21:29Z,2021-06-22T21:21:29Z,Multimodal Emergent Fake News Detection via Meta Neural Process Networks,"Fake news travels at unprecedented speeds, reaches global audiences and puts
users and communities at great risk via social media platforms. Deep learning
based models show good performance when trained on large amounts of labeled
data on events of interest, whereas the performance of models tends to degrade
on other events due to domain shift. Therefore, significant challenges are
posed for existing detection approaches to detect fake news on emergent events,
where large-scale labeled datasets are difficult to obtain. Moreover, adding
the knowledge from newly emergent events requires to build a new model from
scratch or continue to fine-tune the model, which can be challenging,
expensive, and unrealistic for real-world settings. In order to address those
challenges, we propose an end-to-end fake news detection framework named
MetaFEND, which is able to learn quickly to detect fake news on emergent events
with a few verified posts. Specifically, the proposed model integrates
meta-learning and neural process methods together to enjoy the benefits of
these approaches. In particular, a label embedding module and a hard attention
mechanism are proposed to enhance the effectiveness by handling categorical
information and trimming irrelevant posts. Extensive experiments are conducted
on multimedia datasets collected from Twitter and Weibo. The experimental
results show our proposed MetaFEND model can detect fake news on never-seen
events effectively and outperform the state-of-the-art methods.",arxiv
http://arxiv.org/abs/2105.06631v4,2021-09-15T14:46:28Z,2021-05-14T03:49:59Z,Ordering-Based Causal Discovery with Reinforcement Learning,"It is a long-standing question to discover causal relations among a set of
variables in many empirical sciences. Recently, Reinforcement Learning (RL) has
achieved promising results in causal discovery from observational data.
However, searching the space of directed graphs and enforcing acyclicity by
implicit penalties tend to be inefficient and restrict the existing RL-based
method to small scale problems. In this work, we propose a novel RL-based
approach for causal discovery, by incorporating RL into the ordering-based
paradigm. Specifically, we formulate the ordering search problem as a
multi-step Markov decision process, implement the ordering generating process
with an encoder-decoder architecture, and finally use RL to optimize the
proposed model based on the reward mechanisms designed for~each ordering. A
generated ordering would then be processed using variable selection to obtain
the final causal graph. We analyze the consistency and computational complexity
of the proposed method, and empirically show that a pretrained model can be
exploited to accelerate training. Experimental results on both synthetic and
real data sets shows that the proposed method achieves a much improved
performance over existing RL-based method.",arxiv
http://arxiv.org/abs/1607.08723v4,2018-02-14T15:56:51Z,2016-07-29T08:33:10Z,"Cognitive Science in the era of Artificial Intelligence: A roadmap for
  reverse-engineering the infant language-learner","During their first years of life, infants learn the language(s) of their
environment at an amazing speed despite large cross cultural variations in
amount and complexity of the available language input. Understanding this
simple fact still escapes current cognitive and linguistic theories. Recently,
spectacular progress in the engineering science, notably, machine learning and
wearable technology, offer the promise of revolutionizing the study of
cognitive development. Machine learning offers powerful learning algorithms
that can achieve human-like performance on many linguistic tasks. Wearable
sensors can capture vast amounts of data, which enable the reconstruction of
the sensory experience of infants in their natural environment. The project of
'reverse engineering' language development, i.e., of building an effective
system that mimics infant's achievements appears therefore to be within reach.
Here, we analyze the conditions under which such a project can contribute to
our scientific understanding of early language development. We argue that
instead of defining a sub-problem or simplifying the data, computational models
should address the full complexity of the learning situation, and take as input
the raw sensory signals available to infants. This implies that (1) accessible
but privacy-preserving repositories of home data be setup and widely shared,
and (2) models be evaluated at different linguistic levels through a benchmark
of psycholinguist tests that can be passed by machines and humans alike, (3)
linguistically and psychologically plausible learning architectures be scaled
up to real data using probabilistic/optimization principles from machine
learning. We discuss the feasibility of this approach and present preliminary
results.",arxiv
http://arxiv.org/abs/1902.08329v1,2019-02-22T01:40:51Z,2019-02-22T01:40:51Z,E-LSTM-D: A Deep Learning Framework for Dynamic Network Link Prediction,"Predicting the potential relations between nodes in networks, known as link
prediction, has long been a challenge in network science. However, most studies
just focused on link prediction of static network, while real-world networks
always evolve over time with the occurrence and vanishing of nodes and links.
Dynamic network link prediction thus has been attracting more and more
attention since it can better capture the evolution nature of networks, but
still most algorithms fail to achieve satisfied prediction accuracy. Motivated
by the excellent performance of Long Short-Term Memory (LSTM) in processing
time series, in this paper, we propose a novel Encoder-LSTM-Decoder (E-LSTM-D)
deep learning model to predict dynamic links end to end. It could handle long
term prediction problems, and suits the networks of different scales with
fine-tuned structure. To the best of our knowledge, it is the first time that
LSTM, together with an encoder-decoder architecture, is applied to link
prediction in dynamic networks. This new model is able to automatically learn
structural and temporal features in a unified framework, which can predict the
links that never appear in the network before. The extensive experiments show
that our E-LSTM-D model significantly outperforms newly proposed dynamic
network link prediction methods and obtain the state-of-the-art results.",arxiv
http://arxiv.org/abs/2103.02014v3,2021-06-11T02:19:04Z,2021-03-02T20:36:04Z,Online Adversarial Attacks,"Adversarial attacks expose important vulnerabilities of deep learning models,
yet little attention has been paid to settings where data arrives as a stream.
In this paper, we formalize the online adversarial attack problem, emphasizing
two key elements found in real-world use-cases: attackers must operate under
partial knowledge of the target model, and the decisions made by the attacker
are irrevocable since they operate on a transient data stream. We first
rigorously analyze a deterministic variant of the online threat model by
drawing parallels to the well-studied $k$-secretary problem in theoretical
computer science and propose Virtual+, a simple yet practical online algorithm.
Our main theoretical result show Virtual+ yields provably the best competitive
ratio over all single-threshold algorithms for $k<5$ -- extending previous
analysis of the $k$-secretary problem. We also introduce the \textit{stochastic
$k$-secretary} -- effectively reducing online blackbox transfer attacks to a
$k$-secretary problem under noise -- and prove theoretical bounds on the
performance of \textit{any} online algorithms adapted to this setting. Finally,
we complement our theoretical results by conducting experiments on both MNIST
and CIFAR-10 with both vanilla and robust classifiers, revealing not only the
necessity of online algorithms in achieving near-optimal performance but also
the rich interplay of a given attack strategy towards online attack selection,
enabling simple strategies like FGSM to outperform classically strong whitebox
adversaries.",arxiv
http://arxiv.org/abs/1812.05473v3,2020-04-05T17:42:14Z,2018-12-13T15:10:44Z,Learning Features of Network Structures Using Graphlets,"Networks are fundamental to the study of complex systems, ranging from social
contacts, message transactions, to biological regulations and economical
networks. In many realistic applications, these networks may vary over time.
Modeling and analyzing such temporal properties is of additional interest as it
can provide a richer characterization of relations between nodes in networks.
In this paper, we explore the role of \emph{graphlets} in network
classification for both static and temporal networks. Graphlets are small
non-isomorphic induced subgraphs representing connected patterns in a network
and their frequency can be used to assess network structures. We show that
graphlet features, which are not captured by state-of-the-art methods, play a
significant role in enhancing the performance of network classification. To
that end, we propose two novel graphlet-based techniques, \emph{gl2vec} for
network embedding, and \emph{gl-DCNN} for diffusion-convolutional neural
networks. We demonstrate the efficacy and usability of \emph{gl2vec} and
\emph{gl-DCNN} through extensive experiments using several real-world static
and temporal networks. We find that features learned from graphlets can bring
notable performance increases to state-of-the-art methods in network analysis.",arxiv
http://arxiv.org/abs/1710.00818v4,2019-05-19T07:35:29Z,2017-09-30T09:05:27Z,"Continuous-Time Relationship Prediction in Dynamic Heterogeneous
  Information Networks","Online social networks, World Wide Web, media and technological networks, and
other types of so-called information networks are ubiquitous nowadays. These
information networks are inherently heterogeneous and dynamic. They are
heterogeneous as they consist of multi-typed objects and relations, and they
are dynamic as they are constantly evolving over time. One of the challenging
issues in such heterogeneous and dynamic environments is to forecast those
relationships in the network that will appear in the future. In this paper, we
try to solve the problem of continuous-time relationship prediction in dynamic
and heterogeneous information networks. This implies predicting the time it
takes for a relationship to appear in the future, given its features that have
been extracted by considering both heterogeneity and temporal dynamics of the
underlying network. To this end, we first introduce a feature extraction
framework that combines the power of meta-path-based modeling and recurrent
neural networks to effectively extract features suitable for relationship
prediction regarding heterogeneity and dynamicity of the networks. Next, we
propose a supervised non-parametric approach, called Non-Parametric Generalized
Linear Model (NP-GLM), which infers the hidden underlying probability
distribution of the relationship building time given its features. We then
present a learning algorithm to train NP-GLM and an inference method to answer
time-related queries. Extensive experiments conducted on synthetic data and
three real-world datasets, namely Delicious, MovieLens, and DBLP, demonstrate
the effectiveness of NP-GLM in solving continuous-time relationship prediction
problem vis-a-vis competitive baselines",arxiv
http://arxiv.org/abs/2007.06903v1,2020-07-14T08:31:05Z,2020-07-14T08:31:05Z,"H-VGRAE: A Hierarchical Stochastic Spatial-Temporal Embedding Method for
  Robust Anomaly Detection in Dynamic Networks","Detecting anomalous edges and nodes in dynamic networks is critical in
various areas, such as social media, computer networks, and so on. Recent
approaches leverage network embedding technique to learn how to generate node
representations for normal training samples and detect anomalies deviated from
normal patterns. However, most existing network embedding approaches learn
deterministic node representations, which are sensitive to fluctuations of the
topology and attributes due to the high flexibility and stochasticity of
dynamic networks. In this paper, a stochastic neural network, named by
Hierarchical Variational Graph Recurrent Autoencoder (H-VGRAE), is proposed to
detect anomalies in dynamic networks by the learned robust node representations
in the form of random variables. H-VGRAE is a semi-supervised model to capture
normal patterns in training set by maximizing the likelihood of the adjacency
matrix and node attributes via variational inference. Comparing with existing
methods, H-VGRAE has three main advantages: 1) H-VGRAE learns robust node
representations through stochasticity modeling and the extraction of
multi-scale spatial-temporal features; 2) H-VGRAE can be extended to deep
structure with the increase of the dynamic network scale; 3) the anomalous edge
and node can be located and interpreted from the probabilistic perspective.
Extensive experiments on four real-world datasets demonstrate the
outperformance of H-VGRAE on anomaly detection in dynamic networks compared
with state-of-the-art competitors.",arxiv
http://arxiv.org/abs/2010.14047v1,2020-10-27T04:21:39Z,2020-10-27T04:21:39Z,"Embedding Dynamic Attributed Networks by Modeling the Evolution
  Processes","Network embedding has recently emerged as a promising technique to embed
nodes of a network into low-dimensional vectors. While fairly successful, most
existing works focus on the embedding techniques for static networks. But in
practice, there are many networks that are evolving over time and hence are
dynamic, e.g., the social networks. To address this issue, a high-order
spatio-temporal embedding model is developed to track the evolutions of dynamic
networks. Specifically, an activeness-aware neighborhood embedding method is
first proposed to extract the high-order neighborhood information at each given
timestamp. Then, an embedding prediction framework is further developed to
capture the temporal correlations, in which the attention mechanism is employed
instead of recurrent neural networks (RNNs) for its efficiency in computing and
flexibility in modeling. Extensive experiments are conducted on four real-world
datasets from three different areas. It is shown that the proposed method
outperforms all the baselines by a substantial margin for the tasks of dynamic
link prediction and node classification, which demonstrates the effectiveness
of the proposed methods on tracking the evolutions of dynamic networks.",arxiv
http://arxiv.org/abs/2105.07944v1,2021-05-17T15:33:25Z,2021-05-17T15:33:25Z,TCL: Transformer-based Dynamic Graph Modelling via Contrastive Learning,"Dynamic graph modeling has recently attracted much attention due to its
extensive applications in many real-world scenarios, such as recommendation
systems, financial transactions, and social networks. Although many works have
been proposed for dynamic graph modeling in recent years, effective and
scalable models are yet to be developed. In this paper, we propose a novel
graph neural network approach, called TCL, which deals with the
dynamically-evolving graph in a continuous-time fashion and enables effective
dynamic node representation learning that captures both the temporal and
topology information. Technically, our model contains three novel aspects.
First, we generalize the vanilla Transformer to temporal graph learning
scenarios and design a graph-topology-aware transformer. Secondly, on top of
the proposed graph transformer, we introduce a two-stream encoder that
separately extracts representations from temporal neighborhoods associated with
the two interaction nodes and then utilizes a co-attentional transformer to
model inter-dependencies at a semantic level. Lastly, we are inspired by the
recently developed contrastive learning and propose to optimize our model by
maximizing mutual information (MI) between the predictive representations of
two future interaction nodes. Benefiting from this, our dynamic representations
can preserve high-level (or global) semantics about interactions and thus is
robust to noisy interactions. To the best of our knowledge, this is the first
attempt to apply contrastive learning to representation learning on dynamic
graphs. We evaluate our model on four benchmark datasets for interaction
prediction and experiment results demonstrate the superiority of our model.",arxiv
http://arxiv.org/abs/2108.03084v2,2021-08-10T13:06:30Z,2021-08-06T12:38:42Z,"Transferring Knowledge Distillation for Multilingual Social Event
  Detection","Recently published graph neural networks (GNNs) show promising performance at
social event detection tasks. However, most studies are oriented toward
monolingual data in languages with abundant training samples. This has left the
more common multilingual settings and lesser-spoken languages relatively
unexplored. Thus, we present a GNN that incorporates cross-lingual word
embeddings for detecting events in multilingual data streams. The first exploit
is to make the GNN work with multilingual data. For this, we outline a
construction strategy that aligns messages in different languages at both the
node and semantic levels. Relationships between messages are established by
merging entities that are the same but are referred to in different languages.
Non-English message representations are converted into English semantic space
via the cross-lingual word embeddings. The resulting message graph is then
uniformly encoded by a GNN model. In special cases where a lesser-spoken
language needs to be detected, a novel cross-lingual knowledge distillation
framework, called CLKD, exploits prior knowledge learned from similar threads
in English to make up for the paucity of annotated data. Experiments on both
synthetic and real-world datasets show the framework to be highly effective at
detection in both multilingual data and in languages where training samples are
scarce.",arxiv
http://arxiv.org/abs/2007.02747v2,2021-07-09T12:13:36Z,2020-07-06T13:41:19Z,"GAG: Global Attributed Graph Neural Network for Streaming Session-based
  Recommendation","Streaming session-based recommendation (SSR) is a challenging task that
requires the recommender system to do the session-based recommendation (SR) in
the streaming scenario. In the real-world applications of e-commerce and social
media, a sequence of user-item interactions generated within a certain period
are grouped as a session, and these sessions consecutively arrive in the form
of streams. Most of the recent SR research has focused on the static setting
where the training data is first acquired and then used to train a
session-based recommender model. They need several epochs of training over the
whole dataset, which is infeasible in the streaming setting. Besides, they can
hardly well capture long-term user interests because of the neglect or the
simple usage of the user information. Although some streaming recommendation
strategies have been proposed recently, they are designed for streams of
individual interactions rather than streams of sessions. In this paper, we
propose a Global Attributed Graph (GAG) neural network model with a Wasserstein
reservoir for the SSR problem. On one hand, when a new session arrives, a
session graph with a global attribute is constructed based on the current
session and its associate user. Thus, the GAG can take both the global
attribute and the current session into consideration to learn more
comprehensive representations of the session and the user, yielding a better
performance in the recommendation. On the other hand, for the adaptation to the
streaming session scenario, a Wasserstein reservoir is proposed to help
preserve a representative sketch of the historical data. Extensive experiments
on two real-world datasets have been conducted to verify the superiority of the
GAG model compared with the state-of-the-art methods.",arxiv
http://arxiv.org/abs/2105.11152v2,2021-06-06T05:14:45Z,2021-05-24T08:35:48Z,"Dynamic Hawkes Processes for Discovering Time-evolving Communities'
  States behind Diffusion Processes","Sequences of events including infectious disease outbreaks, social network
activities, and crimes are ubiquitous and the data on such events carry
essential information about the underlying diffusion processes between
communities (e.g., regions, online user groups). Modeling diffusion processes
and predicting future events are crucial in many applications including
epidemic control, viral marketing, and predictive policing. Hawkes processes
offer a central tool for modeling the diffusion processes, in which the
influence from the past events is described by the triggering kernel. However,
the triggering kernel parameters, which govern how each community is influenced
by the past events, are assumed to be static over time. In the real world, the
diffusion processes depend not only on the influences from the past, but also
the current (time-evolving) states of the communities, e.g., people's awareness
of the disease and people's current interests. In this paper, we propose a
novel Hawkes process model that is able to capture the underlying dynamics of
community states behind the diffusion processes and predict the occurrences of
events based on the dynamics. Specifically, we model the latent dynamic
function that encodes these hidden dynamics by a mixture of neural networks.
Then we design the triggering kernel using the latent dynamic function and its
integral. The proposed method, termed DHP (Dynamic Hawkes Processes), offers a
flexible way to learn complex representations of the time-evolving communities'
states, while at the same time it allows to computing the exact likelihood,
which makes parameter learning tractable. Extensive experiments on four
real-world event datasets show that DHP outperforms five widely adopted methods
for event prediction.",arxiv
http://arxiv.org/abs/1409.7699v3,2018-12-30T17:43:46Z,2014-09-26T20:00:14Z,"The Overlooked Potential of Generalized Linear Models in Astronomy-II:
  Gamma regression and photometric redshifts","Machine learning techniques offer a precious tool box for use within
astronomy to solve problems involving so-called big data. They provide a means
to make accurate predictions about a particular system without prior knowledge
of the underlying physical processes of the data. In this article, and the
companion papers of this series, we present the set of Generalized Linear
Models (GLMs) as a fast alternative method for tackling general astronomical
problems, including the ones related to the machine learning paradigm. To
demonstrate the applicability of GLMs to inherently positive and continuous
physical observables, we explore their use in estimating the photometric
redshifts of galaxies from their multi-wavelength photometry. Using the gamma
family with a log link function we predict redshifts from the PHoto-z Accuracy
Testing simulated catalogue and a subset of the Sloan Digital Sky Survey from
Data Release 10. We obtain fits that result in catastrophic outlier rates as
low as ~1% for simulated and ~2% for real data. Moreover, we can easily obtain
such levels of precision within a matter of seconds on a normal desktop
computer and with training sets that contain merely thousands of galaxies. Our
software is made publicly available as an user-friendly package developed in
Python, R and via an interactive web application
(https://cosmostatisticsinitiative.shinyapps.io/CosmoPhotoz). This software
allows users to apply a set of GLMs to their own photometric catalogues and
generates publication quality plots with minimum effort from the user. By
facilitating their ease of use to the astronomical community, this paper series
aims to make GLMs widely known and to encourage their implementation in future
large-scale projects, such as the Large Synoptic Survey Telescope.",arxiv
http://arxiv.org/abs/1901.02495v1,2019-01-08T20:08:42Z,2019-01-08T20:08:42Z,"Presence-absence estimation in audio recordings of tropical frog
  communities","One non-invasive way to study frog communities is by analyzing long-term
samples of acoustic material containing calls. This immense task has been
optimized by the development of Machine Learning tools to extract ecological
information. We explored a likelihood-ratio audio detector based on Gaussian
mixture model classification of 10 frog species, and applied it to estimate
presence-absence in audio recordings from an actual amphibian monitoring
performed at Yasun\'i National Park in the Ecuadorian Amazonia. A modified
filter-bank was used to extract 20 cepstral features that model the spectral
content of frog calls. Experiments were carried out to investigate the
hyperparameters and the minimum frog-call time needed to train an accurate GMM
classifier. With 64 Gaussians and 12 seconds of training time, the classifier
achieved an average weighted error rate of 0.9% on the 10-fold cross-validation
for nine species classification, as compared to 3% with MFCC and 1.8% with PLP
features. For testing, 10 GMMs were trained using all the available
training-validation dataset to study 23.5 hours in 141, 10-minute long samples
of unidentified real-world audio recorded at two frog communities in 2001 with
analog equipment. To evaluate automatic presence-absence estimation, we
characterized the audio samples with 10 binary variables each corresponding to
a frog species, and manually labeled a sub-set of 18 samples using headphones.
A recall of 87.5% and precision of 100% with average accuracy of 96.66%
suggests good generalization ability of the algorithm, and provides evidence of
the validity of this approach to study real-world audio recorded in a tropical
acoustic environment. Finally, we applied the algorithm to the available
corpus, and show its potentiality to gain insights into the temporal
reproductive behavior of frogs.",arxiv
http://arxiv.org/abs/2011.03922v1,2020-11-08T08:24:06Z,2020-11-08T08:24:06Z,Learning World Transition Model for Socially Aware Robot Navigation,"Moving in dynamic pedestrian environments is one of the important
requirements for autonomous mobile robots. We present a model-based
reinforcement learning approach for robots to navigate through crowded
environments. The navigation policy is trained with both real interaction data
from multi-agent simulation and virtual data from a deep transition model that
predicts the evolution of surrounding dynamics of mobile robots. The model
takes laser scan sequence and robot's own state as input and outputs steering
control. The laser sequence is further transformed into stacked local obstacle
maps disentangled from robot's ego motion to separate the static and dynamic
obstacles, simplifying the model training. We observe that our method can be
trained with significantly less real interaction data in simulator but achieve
similar level of success rate in social navigation task compared with other
methods. Experiments were conducted in multiple social scenarios both in
simulation and on real robots, the learned policy can guide the robots to the
final targets successfully while avoiding pedestrians in a socially compliant
manner. Code is available at
https://github.com/YuxiangCui/model-based-social-navigation",arxiv
http://arxiv.org/abs/1607.02168v1,2016-07-07T20:47:05Z,2016-07-07T20:47:05Z,Discovering Boolean Gates in Slime Mould,"Slime mould of Physarum polycephalum is a large cell exhibiting rich spatial
non-linear electrical characteristics. We exploit the electrical properties of
the slime mould to implement logic gates using a flexible hardware platform
designed for investigating the electrical properties of a substrate (MECOBO).
We apply arbitrary electrical signals to `configure' the slime mould, i.e.
change shape of its body and, measure the slime mould's electrical response. We
show that it is possible to find configurations that allow the Physarum to act
as any 2-input Boolean gate. The occurrence frequency of the gates discovered
in the slime was analysed and compared to complexity hierarchies of logical
gates obtained in other unconventional materials. The search for gates was
performed by both sweeping across configurations in the real material as well
as training a neural network-based model and searching the gates therein using
gradient descent.",arxiv
http://arxiv.org/abs/2108.01998v1,2021-08-02T03:56:35Z,2021-08-02T03:56:35Z,Adversarial Energy Disaggregation for Non-intrusive Load Monitoring,"Energy disaggregation, also known as non-intrusive load monitoring (NILM),
challenges the problem of separating the whole-home electricity usage into
appliance-specific individual consumptions, which is a typical application of
data analysis. {NILM aims to help households understand how the energy is used
and consequently tell them how to effectively manage the energy, thus allowing
energy efficiency which is considered as one of the twin pillars of sustainable
energy policy (i.e., energy efficiency and renewable energy).} Although NILM is
unidentifiable, it is widely believed that the NILM problem can be addressed by
data science. Most of the existing approaches address the energy disaggregation
problem by conventional techniques such as sparse coding, non-negative matrix
factorization, and hidden Markov model. Recent advances reveal that deep neural
networks (DNNs) can get favorable performance for NILM since DNNs can
inherently learn the discriminative signatures of the different appliances. In
this paper, we propose a novel method named adversarial energy disaggregation
(AED) based on DNNs. We introduce the idea of adversarial learning into NILM,
which is new for the energy disaggregation task. Our method trains a generator
and multiple discriminators via an adversarial fashion. The proposed method not
only learns shard representations for different appliances, but captures the
specific multimode structures of each appliance. Extensive experiments on
real-world datasets verify that our method can achieve new state-of-the-art
performance.",arxiv
http://arxiv.org/abs/2004.00622v1,2020-04-01T17:59:59Z,2020-04-01T17:59:59Z,Evading Deepfake-Image Detectors with White- and Black-Box Attacks,"It is now possible to synthesize highly realistic images of people who don't
exist. Such content has, for example, been implicated in the creation of
fraudulent social-media profiles responsible for dis-information campaigns.
Significant efforts are, therefore, being deployed to detect
synthetically-generated content. One popular forensic approach trains a neural
network to distinguish real from synthetic content.
  We show that such forensic classifiers are vulnerable to a range of attacks
that reduce the classifier to near-0% accuracy. We develop five attack case
studies on a state-of-the-art classifier that achieves an area under the ROC
curve (AUC) of 0.95 on almost all existing image generators, when only trained
on one generator. With full access to the classifier, we can flip the lowest
bit of each pixel in an image to reduce the classifier's AUC to 0.0005; perturb
1% of the image area to reduce the classifier's AUC to 0.08; or add a single
noise pattern in the synthesizer's latent space to reduce the classifier's AUC
to 0.17. We also develop a black-box attack that, with no access to the target
classifier, reduces the AUC to 0.22. These attacks reveal significant
vulnerabilities of certain image-forensic classifiers.",arxiv
http://arxiv.org/abs/1907.07228v1,2019-07-16T19:38:46Z,2019-07-16T19:38:46Z,"Modeling Human Annotation Errors to Design Bias-Aware Systems for Social
  Stream Processing","High-quality human annotations are necessary to create effective machine
learning systems for social media. Low-quality human annotations indirectly
contribute to the creation of inaccurate or biased learning systems. We show
that human annotation quality is dependent on the ordering of instances shown
to annotators (referred as 'annotation schedule'), and can be improved by local
changes in the instance ordering provided to the annotators, yielding a more
accurate annotation of the data stream for efficient real-time social media
analytics. We propose an error-mitigating active learning algorithm that is
robust with respect to some cases of human errors when deciding an annotation
schedule. We validate the human error model and evaluate the proposed algorithm
against strong baselines by experimenting on classification tasks of relevant
social media posts during crises. According to these experiments, considering
the order in which data instances are presented to human annotators leads to
both an increase in accuracy for machine learning and awareness toward some
potential biases in human learning that may affect the automated classifier.",arxiv
http://arxiv.org/abs/1504.05809v1,2015-04-22T13:59:49Z,2015-04-22T13:59:49Z,"LOAD: Local Orientation Adaptive Descriptor for Texture and Material
  Classification","In this paper, we propose a novel local feature, called Local Orientation
Adaptive Descriptor (LOAD), to capture regional texture in an image. In LOAD,
we proposed to define point description on an Adaptive Coordinate System (ACS),
adopt a binary sequence descriptor to capture relationships between one point
and its neighbors and use multi-scale strategy to enhance the discriminative
power of the descriptor. The proposed LOAD enjoys not only discriminative power
to capture the texture information, but also has strong robustness to
illumination variation and image rotation. Extensive experiments on benchmark
data sets of texture classification and real-world material recognition show
that the proposed LOAD yields the state-of-the-art performance. It is worth to
mention that we achieve a 65.4\% classification accuracy-- which is, to the
best of our knowledge, the highest record by far --on Flickr Material Database
by using a single feature. Moreover, by combining LOAD with the feature
extracted by Convolutional Neural Networks (CNN), we obtain significantly
better performance than both the LOAD and CNN. This result confirms that the
LOAD is complementary to the learning-based features.",arxiv
http://arxiv.org/abs/2110.02405v1,2021-10-05T23:23:51Z,2021-10-05T23:23:51Z,Echo-Reconstruction: Audio-Augmented 3D Scene Reconstruction,"Reflective and textureless surfaces such as windows, mirrors, and walls can
be a challenge for object and scene reconstruction. These surfaces are often
poorly reconstructed and filled with depth discontinuities and holes, making it
difficult to cohesively reconstruct scenes that contain these planar
discontinuities. We propose Echoreconstruction, an audio-visual method that
uses the reflections of sound to aid in geometry and audio reconstruction for
virtual conferencing, teleimmersion, and other AR/VR experience. The mobile
phone prototype emits pulsed audio, while recording video for RGB-based 3D
reconstruction and audio-visual classification. Reflected sound and images from
the video are input into our audio (EchoCNN-A) and audio-visual (EchoCNN-AV)
convolutional neural networks for surface and sound source detection, depth
estimation, and material classification. The inferences from these
classifications enhance scene 3D reconstructions containing open spaces and
reflective surfaces by depth filtering, inpainting, and placement of unmixed
sound sources in the scene. Our prototype, VR demo, and experimental results
from real-world and virtual scenes with challenging surfaces and sound indicate
high success rates on classification of material, depth estimation, and
closed/open surfaces, leading to considerable visual and audio improvement in
3D scenes (see Figure 1).",arxiv
http://arxiv.org/abs/2103.09054v1,2021-03-07T14:59:12Z,2021-03-07T14:59:12Z,Sentiment Analysis for Troll Detection on Weibo,"The impact of social media on the modern world is difficult to overstate.
Virtually all companies and public figures have social media accounts on
popular platforms such as Twitter and Facebook. In China, the micro-blogging
service provider, Sina Weibo, is the most popular such service. To influence
public opinion, Weibo trolls -- the so called Water Army -- can be hired to
post deceptive comments. In this paper, we focus on troll detection via
sentiment analysis and other user activity data on the Sina Weibo platform. We
implement techniques for Chinese sentence segmentation, word embedding, and
sentiment score calculation. In recent years, troll detection and sentiment
analysis have been studied, but we are not aware of previous research that
considers troll detection based on sentiment analysis. We employ the resulting
techniques to develop and test a sentiment analysis approach for troll
detection, based on a variety of machine learning strategies. Experimental
results are generated and analyzed. A Chrome extension is presented that
implements our proposed technique, which enables real-time troll detection when
a user browses Sina Weibo.",arxiv
http://arxiv.org/abs/2106.01970v1,2021-06-03T16:18:01Z,2021-06-03T16:18:01Z,"NeRFactor: Neural Factorization of Shape and Reflectance Under an
  Unknown Illumination","We address the problem of recovering the shape and spatially-varying
reflectance of an object from posed multi-view images of the object illuminated
by one unknown lighting condition. This enables the rendering of novel views of
the object under arbitrary environment lighting and editing of the object's
material properties. The key to our approach, which we call Neural Radiance
Factorization (NeRFactor), is to distill the volumetric geometry of a Neural
Radiance Field (NeRF) [Mildenhall et al. 2020] representation of the object
into a surface representation and then jointly refine the geometry while
solving for the spatially-varying reflectance and the environment lighting.
Specifically, NeRFactor recovers 3D neural fields of surface normals, light
visibility, albedo, and Bidirectional Reflectance Distribution Functions
(BRDFs) without any supervision, using only a re-rendering loss, simple
smoothness priors, and a data-driven BRDF prior learned from real-world BRDF
measurements. By explicitly modeling light visibility, NeRFactor is able to
separate shadows from albedo and synthesize realistic soft or hard shadows
under arbitrary lighting conditions. NeRFactor is able to recover convincing 3D
models for free-viewpoint relighting in this challenging and underconstrained
capture setup for both synthetic and real scenes. Qualitative and quantitative
experiments show that NeRFactor outperforms classic and deep learning-based
state of the art across various tasks. Our code and data are available at
people.csail.mit.edu/xiuming/projects/nerfactor/.",arxiv
http://arxiv.org/abs/2106.12372v2,2021-06-25T08:09:48Z,2021-06-23T13:09:58Z,Real-time Neural Radiance Caching for Path Tracing,"We present a real-time neural radiance caching method for path-traced global
illumination. Our system is designed to handle fully dynamic scenes, and makes
no assumptions about the lighting, geometry, and materials. The data-driven
nature of our approach sidesteps many difficulties of caching algorithms, such
as locating, interpolating, and updating cache points. Since pretraining neural
networks to handle novel, dynamic scenes is a formidable generalization
challenge, we do away with pretraining and instead achieve generalization via
adaptation, i.e. we opt for training the radiance cache while rendering. We
employ self-training to provide low-noise training targets and simulate
infinite-bounce transport by merely iterating few-bounce training updates. The
updates and cache queries incur a mild overhead -- about 2.6ms on full HD
resolution -- thanks to a streaming implementation of the neural network that
fully exploits modern hardware. We demonstrate significant noise reduction at
the cost of little induced bias, and report state-of-the-art, real-time
performance on a number of challenging scenarios.",arxiv
http://arxiv.org/abs/1806.00749v1,2018-06-03T08:09:58Z,2018-06-03T08:09:58Z,TI-CNN: Convolutional Neural Networks for Fake News Detection,"With the development of social networks, fake news for various commercial and
political purposes has been appearing in large numbers and gotten widespread in
the online world. With deceptive words, people can get infected by the fake
news very easily and will share them without any fact-checking. For instance,
during the 2016 US president election, various kinds of fake news about the
candidates widely spread through both official news media and the online social
networks. These fake news is usually released to either smear the opponents or
support the candidate on their side. The erroneous information in the fake news
is usually written to motivate the voters' irrational emotion and enthusiasm.
Such kinds of fake news sometimes can bring about devastating effects, and an
important goal in improving the credibility of online social networks is to
identify the fake news timely. In this paper, we propose to study the fake news
detection problem. Automatic fake news identification is extremely hard, since
pure model based fact-checking for news is still an open problem, and few
existing models can be applied to solve the problem. With a thorough
investigation of a fake news data, lots of useful explicit features are
identified from both the text words and images used in the fake news. Besides
the explicit features, there also exist some hidden patterns in the words and
images used in fake news, which can be captured with a set of latent features
extracted via the multiple convolutional layers in our model. A model named as
TI-CNN (Text and Image information based Convolutinal Neural Network) is
proposed in this paper. By projecting the explicit and latent features into a
unified feature space, TI-CNN is trained with both the text and image
information simultaneously. Extensive experiments carried on the real-world
fake news datasets have demonstrate the effectiveness of TI-CNN.",arxiv
http://arxiv.org/abs/0810.4945v1,2008-10-27T21:39:04Z,2008-10-27T21:39:04Z,New Approaches to Object Classification in Synoptic Sky Surveys,"Digital synoptic sky surveys pose several new object classification
challenges. In surveys where real-time detection and classification of
transient events is a science driver, there is a need for an effective
elimination of instrument-related artifacts which can masquerade as transient
sources in the detection pipeline, e.g., unremoved large cosmic rays,
saturation trails, reflections, crosstalk artifacts, etc. We have implemented
such an Artifact Filter, using a supervised neural network, for the real-time
processing pipeline in the Palomar-Quest (PQ) survey. After the training phase,
for each object it takes as input a set of measured morphological parameters
and returns the probability of it being a real object. Despite the relatively
low number of training cases for many kinds of artifacts, the overall artifact
classification rate is around 90%, with no genuine transients misclassified
during our real-time scans. Another question is how to assign an optimal
star-galaxy classification in a multi-pass survey, where seeing and other
conditions change between different epochs, potentially producing inconsistent
classifications for the same object. We have implemented a star/galaxy
multipass classifier that makes use of external and a priori knowledge to find
the optimal classification from the individually derived ones. Both these
techniques can be applied to other, similar surveys and data sets.",arxiv
http://arxiv.org/abs/2104.00631v2,2021-09-08T03:06:30Z,2021-04-01T17:22:50Z,Residual Model Learning for Microrobot Control,"A majority of microrobots are constructed using compliant materials that are
difficult to model analytically, limiting the utility of traditional
model-based controllers. Challenges in data collection on microrobots and large
errors between simulated models and real robots make current model-based
learning and sim-to-real transfer methods difficult to apply. We propose a
novel framework residual model learning (RML) that leverages approximate models
to substantially reduce the sample complexity associated with learning an
accurate robot model. We show that using RML, we can learn a model of the
Harvard Ambulatory MicroRobot (HAMR) using just 12 seconds of passively
collected interaction data. The learned model is accurate enough to be
leveraged as ""proxy-simulator"" for learning walking and turning behaviors using
model-free reinforcement learning algorithms. RML provides a general framework
for learning from extremely small amounts of interaction data, and our
experiments with HAMR clearly demonstrate that RML substantially outperforms
existing techniques.",arxiv
http://arxiv.org/abs/2106.16118v1,2021-06-30T15:18:14Z,2021-06-30T15:18:14Z,"SimNet: Enabling Robust Unknown Object Manipulation from Pure Synthetic
  Data via Stereo","Robot manipulation of unknown objects in unstructured environments is a
challenging problem due to the variety of shapes, materials, arrangements and
lighting conditions. Even with large-scale real-world data collection, robust
perception and manipulation of transparent and reflective objects across
various lighting conditions remain challenging. To address these challenges we
propose an approach to performing sim-to-real transfer of robotic perception.
The underlying model, SimNet, is trained as a single multi-headed neural
network using simulated stereo data as input and simulated object segmentation
masks, 3D oriented bounding boxes (OBBs), object keypoints, and disparity as
output. A key component of SimNet is the incorporation of a learned stereo
sub-network that predicts disparity. SimNet is evaluated on 2D car detection,
unknown object detection, and deformable object keypoint detection and
significantly outperforms a baseline that uses a structured light RGB-D sensor.
By inferring grasp positions using the OBB and keypoint predictions, SimNet can
be used to perform end-to-end manipulation of unknown objects in both easy and
hard scenarios using our fleet of Toyota HSR robots in four home environments.
In unknown object grasping experiments, the predictions from the baseline RGB-D
network and SimNet enable successful grasps of most of the easy objects.
However, the RGB-D baseline only grasps 35% of the hard (e.g., transparent)
objects, while SimNet grasps 95%, suggesting that SimNet can enable robust
manipulation of unknown objects, including transparent objects, in unknown
environments.",arxiv
http://arxiv.org/abs/2001.07427v1,2020-01-21T10:17:39Z,2020-01-21T10:17:39Z,Circuit Implementation of a Four-Dimensional Topological Insulator,"The classification of topological insulators predicts the existence of
high-dimensional topological phases that cannot occur in real materials, as
these are limited to three or fewer spatial dimensions. We use electric
circuits to experimentally implement a four-dimensional (4D) topological
lattice. The lattice dimensionality is established by circuit connections, and
not by mapping to a lower-dimensional system. On the lattice's
three-dimensional surface, we observe topological surface states that are
associated with a nonzero second Chern number but vanishing first Chern
numbers. The 4D lattice belongs to symmetry class AI, which refers to
time-reversal-invariant and spinless systems with no special spatial symmetry.
Class AI is topologically trivial in one to three spatial dimensions, so 4D is
the lowest possible dimension for achieving a topological insulator in this
class. This work paves the way to the use of electric circuits for exploring
high-dimensional topological models.",arxiv
http://arxiv.org/abs/2001.09938v1,2019-10-22T15:57:20Z,2019-10-22T15:57:20Z,"Autonomous discovery of battery electrolytes with robotic
  experimentation and machine-learning","Innovations in batteries take years to formulate and commercialize, requiring
extensive experimentation during the design and optimization phases. We
approached the design and selection of a battery electrolyte through a
black-box optimization algorithm directly integrated into a robotic test-stand.
We report here the discovery of a novel battery electrolyte by this experiment
completely guided by the machine-learning software without human intervention.
Motivated by the recent trend toward super-concentrated aqueous electrolytes
for high-performance batteries, we utilize Dragonfly - a Bayesian
machine-learning software package - to search mixtures of commonly used lithium
and sodium salts for super-concentrated aqueous electrolytes with wide
electrochemical stability windows. Dragonfly autonomously managed the robotic
test-stand, recommending electrolyte designs to test and receiving experimental
feedback in real time. In 40 hours of continuous experimentation over a
four-dimensional design space with millions of potential candidates, Dragonfly
discovered a novel, mixed-anion aqueous sodium electrolyte with a wider
electrochemical stability window than state-of-the-art sodium electrolyte. A
human-guided design process may have missed this optimal electrolyte. This
result demonstrates the possibility of integrating robotics with
machine-learning to rapidly and autonomously discover novel battery materials.",arxiv
http://arxiv.org/abs/1906.06283v3,2021-01-26T10:23:30Z,2019-06-14T16:45:01Z,Support vector machines on the D-Wave quantum annealer,"Kernel-based support vector machines (SVMs) are supervised machine learning
algorithms for classification and regression problems. We introduce a method to
train SVMs on a D-Wave 2000Q quantum annealer and study its performance in
comparison to SVMs trained on conventional computers. The method is applied to
both synthetic data and real data obtained from biology experiments. We find
that the quantum annealer produces an ensemble of different solutions that
often generalizes better to unseen data than the single global minimum of an
SVM trained on a conventional computer, especially in cases where only limited
training data is available. For cases with more training data than currently
fits on the quantum annealer, we show that a combination of classifiers for
subsets of the data almost always produces stronger joint classifiers than the
conventional SVM for the same parameters.",arxiv
http://arxiv.org/abs/2006.14063v2,2020-07-02T20:48:51Z,2020-06-24T21:30:55Z,Practical applications of metric space magnitude and weighting vectors,"Metric space magnitude, an active subject of research in algebraic topology,
originally arose in the context of biology, where it was used to represent the
effective number of distinct species in an environment. In a more general
setting, the magnitude of a metric space is a real number that aims to quantify
the effective number of distinct points in the space. The contribution of each
point to a metric space's global magnitude, which is encoded by the {\em
weighting vector}, captures much of the underlying geometry of the original
metric space.
  Surprisingly, when the metric space is Euclidean, the weighting vector also
serves as an effective tool for boundary detection. This allows the weighting
vector to serve as the foundation of novel algorithms for classic machine
learning tasks such as classification, outlier detection and active learning.
We demonstrate, using experiments and comparisons on classic benchmark
datasets, the promise of the proposed magnitude and weighting vector-based
approaches.",arxiv
http://arxiv.org/abs/1903.05575v1,2019-03-13T16:19:27Z,2019-03-13T16:19:27Z,"On the Efficacy and High-Performance Implementation of Quaternion Matrix
  Multiplication","Quaternion symmetry is ubiquitous in the physical sciences. As such, much
work has been afforded over the years to the development of efficient schemes
to exploit this symmetry using real and complex linear algebra. Recent years
have also seen many advances in the formal theoretical development of
explicitly quaternion linear algebra with promising applications in image
processing and machine learning. Despite these advances, there do not currently
exist optimized software implementations of quaternion linear algebra. The
leverage of optimized linear algebra software is crucial in the achievement of
high levels of performance on modern computing architectures, and thus provides
a central tool in the development of high-performance scientific software. In
this work, a case will be made for the efficacy of high-performance quaternion
linear algebra software for appropriate problems. In this pursuit, an optimized
software implementation of quaternion matrix multiplication will be presented
and will be shown to outperform a vendor tuned implementation for the analogous
complex matrix operation. The results of this work pave the path for further
development of high-performance quaternion linear algebra software which will
improve the performance of the next generation of applicable scientific
applications.",arxiv
http://arxiv.org/abs/2111.03776v1,2021-11-07T13:02:52Z,2021-11-07T13:02:52Z,"Wireless Edge-Empowered Metaverse: A Learning-Based Incentive Mechanism
  for Virtual Reality","The Metaverse is regarded as the next-generation Internet paradigm that
allows humans to play, work, and socialize in an alternative virtual world with
immersive experience, for instance, via head-mounted display for Virtual
Reality (VR) rendering. With the help of ubiquitous wireless connections and
powerful edge computing technologies, VR users in wireless edge-empowered
Metaverse can immerse in the virtual through the access of VR services offered
by different providers. However, VR applications are computation- and
communication-intensive. The VR service providers (SPs) have to optimize the VR
service delivery efficiently and economically given their limited communication
and computation resources. An incentive mechanism can be thus applied as an
effective tool for managing VR services between providers and users. Therefore,
in this paper, we propose a learning-based Incentive Mechanism framework for VR
services in the Metaverse. First, we propose the quality of perception as the
metric for VR users immersing in the virtual world. Second, for quick trading
of VR services between VR users (i.e., buyers) and VR SPs (i.e., sellers), we
design a double Dutch auction mechanism to determine optimal pricing and
allocation rules in this market. Third, for auction communication reduction, we
design a deep reinforcement learning-based auctioneer to accelerate this
auction process. Experimental results demonstrate that the proposed framework
can achieve near-optimal social welfare while reducing at least half of the
auction information exchange cost than baseline methods.",arxiv
http://arxiv.org/abs/2103.01933v2,2021-03-19T20:13:29Z,2021-03-02T18:44:57Z,"PHASE: PHysically-grounded Abstract Social Events for Machine Social
  Perception","The ability to perceive and reason about social interactions in the context
of physical environments is core to human social intelligence and human-machine
cooperation. However, no prior dataset or benchmark has systematically
evaluated physically grounded perception of complex social interactions that go
beyond short actions, such as high-fiving, or simple group activities, such as
gathering. In this work, we create a dataset of physically-grounded abstract
social events, PHASE, that resemble a wide range of real-life social
interactions by including social concepts such as helping another agent. PHASE
consists of 2D animations of pairs of agents moving in a continuous space
generated procedurally using a physics engine and a hierarchical planner.
Agents have a limited field of view, and can interact with multiple objects, in
an environment that has multiple landmarks and obstacles. Using PHASE, we
design a social recognition task and a social prediction task. PHASE is
validated with human experiments demonstrating that humans perceive rich
interactions in the social events, and that the simulated agents behave
similarly to humans. As a baseline model, we introduce a Bayesian inverse
planning approach, SIMPLE (SIMulation, Planning and Local Estimation), which
outperforms state-of-the-art feed-forward neural networks. We hope that PHASE
can serve as a difficult new challenge for developing new models that can
recognize complex social interactions.",arxiv
http://arxiv.org/abs/2110.12749v1,2021-10-25T09:24:00Z,2021-10-25T09:24:00Z,"A GPU based single-pulse search pipeline (GSP) with database and its
  application to the commensal radio astronomy FAST survey (CRAFTS)","We developed a GPU based single-pulse search pipeline (GSP) with
candidate-archiving database. Largely based upon the infrastructure of Open
source pulsar search and analysis toolkit (PRESTO), GSP implements GPU
acceleration of the de-dispersion and integrates a candidate-archiving
database. We applied GSP to the data streams from the commensal radio astronomy
FAST survey (CRAFTS), which resulted in a quasi-real-time processing. The
integrated candidate database facilitates synergistic usage of multiple
machine-learning tools and thus improves efficient identification of radio
pulsars such as rotating radio transients (RRATs) and Fast Radio Bursts (FRBs).
We first tested GSP on pilot CRAFTS observations with the FAST Ultra-Wide Band
(UWB) receiver. GSP detected all pulsars known from the the Parkes multibeam
pulsar survey in the respective sky area covered by the FAST-UWB. GSP also
discovered 13 new pulsars. We measured the computational efficiency of GSP to
be ~120 times faster than the original PRESTO and ~60 times faster than a
MPI-parallelized version of PRESTO.",arxiv
http://arxiv.org/abs/1908.09951v1,2019-08-26T22:49:35Z,2019-08-26T22:49:35Z,"An Emotional Analysis of False Information in Social Media and News
  Articles","Fake news is risky since it has been created to manipulate the readers'
opinions and beliefs. In this work, we compared the language of false news to
the real one of real news from an emotional perspective, considering a set of
false information types (propaganda, hoax, clickbait, and satire) from social
media and online news articles sources. Our experiments showed that false
information has different emotional patterns in each of its types, and emotions
play a key role in deceiving the reader. Based on that, we proposed a LSTM
neural network model that is emotionally-infused to detect false news.",arxiv
http://arxiv.org/abs/2012.09973v1,2020-12-17T23:21:53Z,2020-12-17T23:21:53Z,High Dimensional Level Set Estimation with Bayesian Neural Network,"Level Set Estimation (LSE) is an important problem with applications in
various fields such as material design, biotechnology, machine operational
testing, etc. Existing techniques suffer from the scalability issue, that is,
these methods do not work well with high dimensional inputs. This paper
proposes novel methods to solve the high dimensional LSE problems using
Bayesian Neural Networks. In particular, we consider two types of LSE problems:
(1) \textit{explicit} LSE problem where the threshold level is a fixed
user-specified value, and, (2) \textit{implicit} LSE problem where the
threshold level is defined as a percentage of the (unknown) maximum of the
objective function. For each problem, we derive the corresponding theoretic
information based acquisition function to sample the data points so as to
maximally increase the level set accuracy. Furthermore, we also analyse the
theoretical time complexity of our proposed acquisition functions, and suggest
a practical methodology to efficiently tune the network hyper-parameters to
achieve high model accuracy. Numerical experiments on both synthetic and
real-world datasets show that our proposed method can achieve better results
compared to existing state-of-the-art approaches.",arxiv
http://arxiv.org/abs/2107.04171v1,2021-07-09T01:34:46Z,2021-07-09T01:34:46Z,Excavation Learning for Rigid Objects in Clutter,"Autonomous excavation for hard or compact materials, especially irregular
rigid objects, is challenging due to high variance of geometric and physical
properties of objects, and large resistive force during excavation. In this
paper, we propose a novel learning-based excavation planning method for rigid
objects in clutter. Our method consists of a convolutional neural network to
predict the excavation success and a sampling-based optimization method for
planning high-quality excavation trajectories leveraging the learned prediction
model. To reduce the sim2real gap for excavation learning, we propose a
voxel-based representation of the excavation scene. We perform excavation
experiments in both simulation and real world to evaluate the learning-based
excavation planners. We further compare with two heuristic baseline excavation
planners and one data-driven scene-independent planner. The experimental
results show that our method can plan high-quality excavations for rigid
objects in clutter and outperforms the baseline methods by large margins. As
far as we know, our work presents the first learning-based excavation planner
for cluttered and irregular rigid objects.",arxiv
http://arxiv.org/abs/2108.04867v1,2021-08-10T18:37:54Z,2021-08-10T18:37:54Z,AuraSense: Robot Collision Avoidance by Full Surface Proximity Detection,"Perceiving obstacles and avoiding collisions is fundamental to the safe
operation of a robot system, particularly when the robot must operate in highly
dynamic human environments. Proximity detection using on-robot sensors can be
used to avoid or mitigate impending collisions. However, existing proximity
sensing methods are orientation and placement dependent, resulting in blind
spots even with large numbers of sensors. In this paper, we introduce the
phenomenon of the Leaky Surface Wave (LSW), a novel sensing modality, and
present AuraSense, a proximity detection system using the LSW. AuraSense is the
first system to realize no-dead-spot proximity sensing for robot arms. It
requires only a single pair of piezoelectric transducers, and can easily be
applied to off-the-shelf robots with minimal modifications. We further
introduce a set of signal processing techniques and a lightweight neural
network to address the unique challenges in using the LSW for proximity
sensing. Finally, we demonstrate a prototype system consisting of a single
piezoelectric element pair on a robot manipulator, which validates our design.
We conducted several micro benchmark experiments and performed more than 2000
on-robot proximity detection trials with various potential robot arm materials,
colliding objects, approach patterns, and robot movement patterns. AuraSense
achieves 100% and 95.3% true positive proximity detection rates when the arm
approaches static and mobile obstacles respectively, with a true negative rate
over 99%, showing the real-world viability of this system.",arxiv
http://arxiv.org/abs/1106.6107v2,2011-11-05T08:45:39Z,2011-06-30T02:26:04Z,"Evolution of cooperation facilitated by reinforcement learning with
  adaptive aspiration levels","Repeated interaction between individuals is the main mechanism for
maintaining cooperation in social dilemma situations. Variants of tit-for-tat
(repeating the previous action of the opponent) and the win-stay lose-shift
strategy are known as strong competitors in iterated social dilemma games. On
the other hand, real repeated interaction generally allows plasticity (i.e.,
learning) of individuals based on the experience of the past. Although
plasticity is relevant to various biological phenomena, its role in repeated
social dilemma games is relatively unexplored. In particular, if
experience-based learning plays a key role in promotion and maintenance of
cooperation, learners should evolve in the contest with nonlearners under
selection pressure. By modeling players using a simple reinforcement learning
model, we numerically show that learning enables the evolution of cooperation.
We also show that numerically estimated adaptive dynamics appositely predict
the outcome of evolutionary simulations. The analysis of the adaptive dynamics
enables us to capture the obtained results as an affirmative example of the
Baldwin effect, where learning accelerates the evolution to optimality.",arxiv
http://arxiv.org/abs/1910.01453v1,2019-09-28T03:03:09Z,2019-09-28T03:03:09Z,"D2D-LSTM based Prediction of the D2D Diffusion Path in Mobile Social
  Networks","Recently, how to expand data transmission to reduce cell data and repeated
cell transmission has received more and more research attention. In mobile
social networks, content popularity prediction has always been an important
part of traffic offloading and expanding data dissemination. However, current
mainstream content popularity prediction methods only use the number of
downloads and shares or the distribution of user interests, which do not
consider important time and geographic location information in mobile social
networks, and all of data is from OSN which is not same as MSN. In this work,
we propose D2D Long Short-Term Memory (D2D-LSTM), a deep neural network based
on LSTM, which is designed to predict a complete D2D diffusion path. Our work
is the first attempt in the world to use real data of MSN to predict diffusion
path with deep neural networks which conforms to the D2D structure. Compared to
linear sequence networks, only learn users' social features without time
distribution or GPS distribution and files' content features, our model can
predict the propagation path more accurately (up to 85.858\%) and can reach
convergence faster (less than 100 steps) because of the neural network that
conforms to the D2D structure and combines user social features and files
features. Moreover, we can simulate generating a D2D propagation tree. After
experiment and comparison, it is found to be very similar to the ground-truth
trees. Finally, we define a user prototype refinement that can more accurately
describe the propagation sharing habits of a prototype user (including content
preferences, time preferences, and geographic location preferences), and
experimentally validate the predictions when the user prototype is added to
1000 classes, it is almost identical to the 50 categories.",arxiv
http://arxiv.org/abs/1608.01987v1,2016-08-05T19:55:57Z,2016-08-05T19:55:57Z,Human collective intelligence as distributed Bayesian inference,"Collective intelligence is believed to underly the remarkable success of
human society. The formation of accurate shared beliefs is one of the key
components of human collective intelligence. How are accurate shared beliefs
formed in groups of fallible individuals? Answering this question requires a
multiscale analysis. We must understand both the individual decision mechanisms
people use, and the properties and dynamics of those mechanisms in the
aggregate. As of yet, mathematical tools for such an approach have been
lacking. To address this gap, we introduce a new analytical framework: We
propose that groups arrive at accurate shared beliefs via distributed Bayesian
inference. Distributed inference occurs through information processing at the
individual level, and yields rational belief formation at the group level. We
instantiate this framework in a new model of human social decision-making,
which we validate using a dataset we collected of over 50,000 users of an
online social trading platform where investors mimic each others' trades using
real money in foreign exchange and other asset markets. We find that in this
setting people use a decision mechanism in which popularity is treated as a
prior distribution for which decisions are best to make. This mechanism is
boundedly rational at the individual level, but we prove that in the aggregate
implements a type of approximate ""Thompson sampling""---a well-known and highly
effective single-agent Bayesian machine learning algorithm for sequential
decision-making. The perspective of distributed Bayesian inference therefore
reveals how collective rationality emerges from the boundedly rational decision
mechanisms people use.",arxiv
http://arxiv.org/abs/1909.05189v3,2020-08-20T14:35:49Z,2019-09-11T16:40:03Z,ORES: Lowering Barriers with Participatory Machine Learning in Wikipedia,"Algorithmic systems---from rule-based bots to machine learning
classifiers---have a long history of supporting the essential work of content
moderation and other curation work in peer production projects. From
counter-vandalism to task routing, basic machine prediction has allowed open
knowledge projects like Wikipedia to scale to the largest encyclopedia in the
world, while maintaining quality and consistency. However, conversations about
how quality control should work and what role algorithms should play have
generally been led by the expert engineers who have the skills and resources to
develop and modify these complex algorithmic systems. In this paper, we
describe ORES: an algorithmic scoring service that supports real-time scoring
of wiki edits using multiple independent classifiers trained on different
datasets. ORES decouples several activities that have typically all been
performed by engineers: choosing or curating training data, building models to
serve predictions, auditing predictions, and developing interfaces or automated
agents that act on those predictions. This meta-algorithmic system was designed
to open up socio-technical conversations about algorithms in Wikipedia to a
broader set of participants. In this paper, we discuss the theoretical
mechanisms of social change ORES enables and detail case studies in
participatory machine learning around ORES from the 5 years since its
deployment.",arxiv
http://arxiv.org/abs/1407.3502v1,2014-07-13T19:09:30Z,2014-07-13T19:09:30Z,"Automated Real-Time Classification and Decision Making in Massive Data
  Streams from Synoptic Sky Surveys","The nature of scientific and technological data collection is evolving
rapidly: data volumes and rates grow exponentially, with increasing complexity
and information content, and there has been a transition from static data sets
to data streams that must be analyzed in real time. Interesting or anomalous
phenomena must be quickly characterized and followed up with additional
measurements via optimal deployment of limited assets. Modern astronomy
presents a variety of such phenomena in the form of transient events in digital
synoptic sky surveys, including cosmic explosions (supernovae, gamma ray
bursts), relativistic phenomena (black hole formation, jets), potentially
hazardous asteroids, etc. We have been developing a set of machine learning
tools to detect, classify and plan a response to transient events for astronomy
applications, using the Catalina Real-time Transient Survey (CRTS) as a
scientific and methodological testbed. The ability to respond rapidly to the
potentially most interesting events is a key bottleneck that limits the
scientific returns from the current and anticipated synoptic sky surveys.
Similar challenge arise in other contexts, from environmental monitoring using
sensor networks to autonomous spacecraft systems. Given the exponential growth
of data rates, and the time-critical response, we need a fully automated and
robust approach. We describe the results obtained to date, and the possible
future developments.",arxiv
http://arxiv.org/abs/1601.04385v1,2016-01-18T02:06:45Z,2016-01-18T02:06:45Z,Real-Time Data Mining of Massive Data Streams from Synoptic Sky Surveys,"The nature of scientific and technological data collection is evolving
rapidly: data volumes and rates grow exponentially, with increasing complexity
and information content, and there has been a transition from static data sets
to data streams that must be analyzed in real time. Interesting or anomalous
phenomena must be quickly characterized and followed up with additional
measurements via optimal deployment of limited assets. Modern astronomy
presents a variety of such phenomena in the form of transient events in digital
synoptic sky surveys, including cosmic explosions (supernovae, gamma ray
bursts), relativistic phenomena (black hole formation, jets), potentially
hazardous asteroids, etc. We have been developing a set of machine learning
tools to detect, classify and plan a response to transient events for astronomy
applications, using the Catalina Real-time Transient Survey (CRTS) as a
scientific and methodological testbed. The ability to respond rapidly to the
potentially most interesting events is a key bottleneck that limits the
scientific returns from the current and anticipated synoptic sky surveys.
Similar challenge arise in other contexts, from environmental monitoring using
sensor networks to autonomous spacecraft systems. Given the exponential growth
of data rates, and the time-critical response, we need a fully automated and
robust approach. We describe the results obtained to date, and the possible
future developments.",arxiv
http://arxiv.org/abs/1904.04081v1,2019-04-08T13:59:36Z,2019-04-08T13:59:36Z,Heterogeneous Multi-task Metric Learning across Multiple Domains,"Distance metric learning (DML) plays a crucial role in diverse machine
learning algorithms and applications. When the labeled information in target
domain is limited, transfer metric learning (TML) helps to learn the metric by
leveraging the sufficient information from other related domains. Multi-task
metric learning (MTML), which can be regarded as a special case of TML,
performs transfer across all related domains. Current TML tools usually assume
that the same feature representation is exploited for different domains.
However, in real-world applications, data may be drawn from heterogeneous
domains. Heterogeneous transfer learning approaches can be adopted to remedy
this drawback by deriving a metric from the learned transformation across
different domains. But they are often limited in that only two domains can be
handled. To appropriately handle multiple domains, we develop a novel
heterogeneous multi-task metric learning (HMTML) framework. In HMTML, the
metrics of all different domains are learned together. The transformations
derived from the metrics are utilized to induce a common subspace, and the
high-order covariance among the predictive structures of these domains is
maximized in this subspace. There do exist a few heterogeneous transfer
learning approaches that deal with multiple domains, but the high-order
statistics (correlation information), which can only be exploited by
simultaneously examining all domains, is ignored in these approaches. Compared
with them, the proposed HMTML can effectively explore such high-order
information, thus obtaining more reliable feature transformations and metrics.
Effectiveness of our method is validated by the extensive and intensive
experiments on text categorization, scene classification, and social image
annotation.",arxiv
http://arxiv.org/abs/2008.00707v2,2020-09-08T15:46:08Z,2020-08-03T08:22:04Z,"Heterogeneous Treatment and Spillover Effects under Clustered Network
  Interference","The bulk of causal inference studies rules out the presence of interference
between units. However, in many real-world settings units are interconnected by
social, physical or virtual ties and the effect of a treatment can spill from
one unit to other connected individuals in the network. In these settings,
interference should be taken into account to avoid biased estimates of the
treatment effect, but it can also be leveraged to save resources and provide
the intervention to a lower percentage of the population where the treatment is
more effective and where the effect can spill over to other susceptible
individuals. In fact, different people might respond differently not only to
the treatment received but also to the treatment received by their network
contacts. Understanding the heterogeneity of treatment and spillover effects
can help policy-makers in the scale-up phase of the intervention, it can guide
the design of targeting strategies with the ultimate goal of making the
interventions more cost-effective, and it might even allow generalizing the
level of treatment spillover effects in other populations. In this paper, we
develop a machine learning method that makes use of tree-based algorithms and
an Horvitz-Thompson estimator to assess the heterogeneity of treatment and
spillover effects with respect to individual, neighborhood and network
characteristics in the context of clustered network interference. We illustrate
how the proposed binary tree methodology performs in a Monte Carlo simulation
study. Additionally, we provide an application on a randomized experiment aimed
at assessing the heterogeneous effects of information sessions on the uptake of
a new weather insurance policy in rural China.",arxiv
http://arxiv.org/abs/1512.07454v1,2015-12-23T12:44:09Z,2015-12-23T12:44:09Z,Evaluation-as-a-Service: Overview and Outlook,"Evaluation in empirical computer science is essential to show progress and
assess technologies developed. Several research domains such as information
retrieval have long relied on systematic evaluation to measure progress: here,
the Cranfield paradigm of creating shared test collections, defining search
tasks, and collecting ground truth for these tasks has persisted up until now.
In recent years, however, several new challenges have emerged that do not fit
this paradigm very well: extremely large data sets, confidential data sets as
found in the medical domain, and rapidly changing data sets as often
encountered in industry. Also, crowdsourcing has changed the way that industry
approaches problem-solving with companies now organizing challenges and handing
out monetary awards to incentivize people to work on their challenges,
particularly in the field of machine learning.
  This white paper is based on discussions at a workshop on
Evaluation-as-a-Service (EaaS). EaaS is the paradigm of not providing data sets
to participants and have them work on the data locally, but keeping the data
central and allowing access via Application Programming Interfaces (API),
Virtual Machines (VM) or other possibilities to ship executables. The objective
of this white paper are to summarize and compare the current approaches and
consolidate the experiences of these approaches to outline the next steps of
EaaS, particularly towards sustainable research infrastructures.
  This white paper summarizes several existing approaches to EaaS and analyzes
their usage scenarios and also the advantages and disadvantages. The many
factors influencing EaaS are overviewed, and the environment in terms of
motivations for the various stakeholders, from funding agencies to challenge
organizers, researchers and participants, to industry interested in supplying
real-world problems for which they require solutions.",arxiv
http://arxiv.org/abs/2005.13035v1,2020-05-26T20:52:44Z,2020-05-26T20:52:44Z,"Detection of exomoons in simulated light curves with a regularized
  convolutional neural network","Many moons have been detected around planets in our Solar System, but none
has been detected unambiguously around any of the confirmed extrasolar planets.
We test the feasibility of a supervised convolutional neural network to
classify photometric transit light curves of planet-host stars and identify
exomoon transits, while avoiding false positives caused by stellar variability
or instrumental noise. Convolutional neural networks are known to have
contributed to improving the accuracy of classification tasks. The network
optimization is typically performed without studying the effect of noise on the
training process. Here we design and optimize a 1D convolutional neural network
to classify photometric transit light curves. We regularize the network by the
total variation loss in order to remove unwanted variations in the data
features. Using numerical experiments, we demonstrate the benefits of our
network, which produces results comparable to or better than the standard
network solutions. Most importantly, our network clearly outperforms a
classical method used in exoplanet science to identify moon-like signals. Thus
the proposed network is a promising approach for analyzing real transit light
curves in the future.",arxiv
http://arxiv.org/abs/1812.02289v1,2018-12-06T01:50:47Z,2018-12-06T01:50:47Z,Learning Dynamic Embeddings from Temporal Interactions,"Modeling a sequence of interactions between users and items (e.g., products,
posts, or courses) is crucial in domains such as e-commerce, social networking,
and education to predict future interactions. Representation learning presents
an attractive solution to model the dynamic evolution of user and item
properties, where each user/item can be embedded in a euclidean space and its
evolution can be modeled by dynamic changes in embedding. However, existing
embedding methods either generate static embeddings, treat users and items
independently, or are not scalable.
  Here we present JODIE, a coupled recurrent model to jointly learn the dynamic
embeddings of users and items from a sequence of user-item interactions. JODIE
has three components. First, the update component updates the user and item
embedding from each interaction using their previous embeddings with the two
mutually-recursive Recurrent Neural Networks. Second, a novel projection
component is trained to forecast the embedding of users at any future time.
Finally, the prediction component directly predicts the embedding of the item
in a future interaction. For models that learn from a sequence of interactions,
traditional training data batching cannot be done due to complex user-user
dependencies. Therefore, we present a novel batching algorithm called t-Batch
that generates time-consistent batches of training data that can run in
parallel, giving massive speed-up.
  We conduct six experiments on two prediction tasks---future interaction
prediction and state change prediction---using four real-world datasets. We
show that JODIE outperforms six state-of-the-art algorithms in these tasks by
up to 22.4%. Moreover, we show that JODIE is highly scalable and up to 9.2x
faster than comparable models. As an additional experiment, we illustrate that
JODIE can predict student drop-out from courses five interactions in advance.",arxiv
http://arxiv.org/abs/1912.11757v1,2019-12-26T02:52:47Z,2019-12-26T02:52:47Z,Multi-Label Graph Convolutional Network Representation Learning,"Knowledge representation of graph-based systems is fundamental across many
disciplines. To date, most existing methods for representation learning
primarily focus on networks with simplex labels, yet real-world objects (nodes)
are inherently complex in nature and often contain rich semantics or labels,
e.g., a user may belong to diverse interest groups of a social network,
resulting in multi-label networks for many applications. The multi-label
network nodes not only have multiple labels for each node, such labels are
often highly correlated making existing methods ineffective or fail to handle
such correlation for node representation learning. In this paper, we propose a
novel multi-label graph convolutional network (ML-GCN) for learning node
representation for multi-label networks. To fully explore label-label
correlation and network topology structures, we propose to model a multi-label
network as two Siamese GCNs: a node-node-label graph and a label-label-node
graph. The two GCNs each handle one aspect of representation learning for nodes
and labels, respectively, and they are seamlessly integrated under one
objective function. The learned label representations can effectively preserve
the inner-label interaction and node label properties, and are then aggregated
to enhance the node representation learning under a unified training framework.
Experiments and comparisons on multi-label node classification validate the
effectiveness of our proposed approach.",arxiv
http://arxiv.org/abs/2103.01745v1,2021-03-02T14:26:00Z,2021-03-02T14:26:00Z,"IdentityDP: Differential Private Identification Protection for Face
  Images","Because of the explosive growth of face photos as well as their widespread
dissemination and easy accessibility in social media, the security and privacy
of personal identity information becomes an unprecedented challenge. Meanwhile,
the convenience brought by advanced identity-agnostic computer vision
technologies is attractive. Therefore, it is important to use face images while
taking careful consideration in protecting people's identities. Given a face
image, face de-identification, also known as face anonymization, refers to
generating another image with similar appearance and the same background, while
the real identity is hidden. Although extensive efforts have been made,
existing face de-identification techniques are either insufficient in
photo-reality or incapable of well-balancing privacy and utility. In this
paper, we focus on tackling these challenges to improve face de-identification.
We propose IdentityDP, a face anonymization framework that combines a
data-driven deep neural network with a differential privacy (DP) mechanism.
This framework encompasses three stages: facial representations
disentanglement, $\epsilon$-IdentityDP perturbation and image reconstruction.
Our model can effectively obfuscate the identity-related information of faces,
preserve significant visual similarity, and generate high-quality images that
can be used for identity-agnostic computer vision tasks, such as detection,
tracking, etc. Different from the previous methods, we can adjust the balance
of privacy and utility through the privacy budget according to pratical demands
and provide a diversity of results without pre-annotations. Extensive
experiments demonstrate the effectiveness and generalization ability of our
proposed anonymization framework.",arxiv
http://arxiv.org/abs/2104.10891v1,2021-04-22T06:43:02Z,2021-04-22T06:43:02Z,"Computer Vision-based Social Distancing Surveillance Solution with
  Optional Automated Camera Calibration for Large Scale Deployment","Social distancing has been suggested as one of the most effective measures to
break the chain of viral transmission in the current COVID-19 pandemic. We
herein describe a computer vision-based AI-assisted solution to aid compliance
with social distancing norms. The solution consists of modules to detect and
track people and to identify distance violations. It provides the flexibility
to choose between a tool-based mode or an automated mode of camera calibration,
making the latter suitable for large-scale deployments. In this paper, we
discuss different metrics to assess the risk associated with social distancing
violations and how we can differentiate between transient or persistent
violations. Our proposed solution performs satisfactorily under different test
scenarios, processes video feed at real-time speed as well as addresses data
privacy regulations by blurring faces of detected people, making it ideal for
deployments.",arxiv
http://arxiv.org/abs/1809.09657v2,2019-03-22T08:39:24Z,2018-09-25T18:49:40Z,"High-resolution, yet statistically relevant, analysis of damage in DP
  steel using artificial intelligence","High performance materials, from natural bone over ancient damascene steel to
modern superalloys, typically possess a complex structure at the microscale.
Their properties exceed those of the individual components and their
knowledge-based improvement therefore requires understanding beyond that of the
components' individual behaviour. Electron microscopy has been instrumental in
unravelling the most important mechanisms of co-deformation and in-situ
deformation experiments have emerged as a popular and accessible technique.
However, a challenge remains: to achieve high spatial resolution and
statistical relevance in combination. Here, we overcome this limitation by
using panoramic imaging and machine learning to study damage in a dual-phase
steel. This high-throughput approach not only gives us strain and
microstructure dependent insights across a large area of this heterogeneous
material, but also encourages us to expand current research past interpretation
of exemplary cases of distinct damage sites towards the less clear-cut reality.",arxiv
http://arxiv.org/abs/2110.05762v2,2021-11-15T18:31:11Z,2021-10-12T06:31:54Z,"Detecting Damage Building Using Real-time Crowdsourced Images and
  Transfer Learning","After significant earthquakes, we can see images posted on social media
platforms by individuals and media agencies owing to the mass usage of
smartphones these days. These images can be utilized to provide information
about the shaking damage in the earthquake region both to the public and
research community, and potentially to guide rescue work. This paper presents
an automated way to extract the damaged building images after earthquakes from
social media platforms such as Twitter and thus identify the particular user
posts containing such images. Using transfer learning and ~6500 manually
labelled images, we trained a deep learning model to recognize images with
damaged buildings in the scene. The trained model achieved good performance
when tested on newly acquired images of earthquakes at different locations and
ran in near real-time on Twitter feed after the 2020 M7.0 earthquake in Turkey.
Furthermore, to better understand how the model makes decisions, we also
implemented the Grad-CAM method to visualize the important locations on the
images that facilitate the decision.",arxiv
http://arxiv.org/abs/1911.03074v2,2020-03-05T17:02:11Z,2019-11-08T06:29:31Z,"Mapless Navigation among Dynamics with Social-safety-awareness: a
  reinforcement learning approach from 2D laser scans","We propose a method to tackle the problem of mapless collision-avoidance
navigation where humans are present using 2D laser scans. Our proposed method
uses ego-safety to measure collision from the robot's perspective while
social-safety to measure the impact of our robot's actions on surrounding
pedestrians. Specifically, the social-safety part predicts the intrusion impact
of our robot's action into the interaction area with surrounding humans. We
train the policy using reinforcement learning on a simple simulator and
directly evaluate the learned policy in Gazebo and real robot tests.
Experiments show the learned policy can be smoothly transferred without any
fine tuning. We observe that our method demonstrates time-efficient path
planning behavior with high success rate in mapless navigation tasks.
Furthermore, we test our method in a navigation among dynamic crowds task
considering both low and high volume traffic. Our learned policy demonstrates
cooperative behavior that actively drives our robot into traffic flows while
showing respect to nearby pedestrians. Evaluation videos are at
https://sites.google.com/view/ssw-batman",arxiv
http://arxiv.org/abs/1502.03394v1,2015-02-11T18:14:04Z,2015-02-11T18:14:04Z,For whom will the Bayesian agents vote?,"Within an agent-based model where moral classifications are socially learned,
we ask if a population of agents behaves in a way that may be compared with
conservative or liberal positions in the real political spectrum. We assume
that agents first experience a formative period, in which they adjust their
learning style acting as supervised Bayesian adaptive learners. The formative
phase is followed by a period of social influence by reinforcement learning. By
comparing data generated by the agents with data from a sample of 15000 Moral
Foundation questionnaires we found the following. 1. The number of information
exchanges in the formative phase correlates positively with statistics
identifying liberals in the social influence phase. This is consistent with
recent evidence that connects the dopamine receptor D4-7R gene, political
orientation and early age social clique size. 2. The learning algorithms that
result from the formative phase vary in the way they treat novelty and
corroborative information with more conservative-like agents treating it more
equally than liberal-like agents. This is consistent with the correlation
between political affiliation and the Openness personality trait reported in
the literature. 3. Under the increase of a model parameter interpreted as an
external pressure, the statistics of liberal agents resemble more those of
conservative agents, consistent with reports on the consequences of external
threats on measures of conservatism. We also show that in the social influence
phase liberal-like agents readapt much faster than conservative-like agents
when subjected to changes on the relevant set of moral issues. This suggests a
verifiable dynamical criterium for attaching liberal or conservative labels to
groups.",arxiv
http://arxiv.org/abs/2105.11852v1,2021-05-25T11:50:05Z,2021-05-25T11:50:05Z,"GCNBoost: Artwork Classification by Label Propagation through a
  Knowledge Graph","The rise of digitization of cultural documents offers large-scale contents,
opening the road for development of AI systems in order to preserve, search,
and deliver cultural heritage. To organize such cultural content also means to
classify them, a task that is very familiar to modern computer science.
Contextual information is often the key to structure such real world data, and
we propose to use it in form of a knowledge graph. Such a knowledge graph,
combined with content analysis, enhances the notion of proximity between
artworks so it improves the performances in classification tasks. In this
paper, we propose a novel use of a knowledge graph, that is constructed on
annotated data and pseudo-labeled data. With label propagation, we boost
artwork classification by training a model using a graph convolutional network,
relying on the relationships between entities of the knowledge graph. Following
a transductive learning framework, our experiments show that relying on a
knowledge graph modeling the relations between labeled data and unlabeled data
allows to achieve state-of-the-art results on multiple classification tasks on
a dataset of paintings, and on a dataset of Buddha statues. Additionally, we
show state-of-the-art results for the difficult case of dealing with unbalanced
data, with the limitation of disregarding classes with extremely low degrees in
the knowledge graph.",arxiv
http://arxiv.org/abs/1705.03451v2,2017-05-26T13:17:17Z,2017-05-09T17:55:15Z,Proceedings of the Workshop on Data Mining for Oil and Gas,"The process of exploring and exploiting Oil and Gas (O&G) generates a lot of
data that can bring more efficiency to the industry. The opportunities for
using data mining techniques in the ""digital oil-field"" remain largely
unexplored or uncharted. With the high rate of data expansion, companies are
scrambling to develop ways to develop near-real-time predictive analytics, data
mining and machine learning capabilities, and are expanding their data storage
infrastructure and resources. With these new goals, come the challenges of
managing data growth, integrating intelligence tools, and analyzing the data to
glean useful insights. Oil and Gas companies need data solutions to
economically extract value from very large volumes of a wide variety of data
generated from exploration, well drilling and production devices and sensors.
  Data mining for oil and gas industry throughout the lifecycle of the
reservoir includes the following roles: locating hydrocarbons, managing
geological data, drilling and formation evaluation, well construction, well
completion, and optimizing production through the life of the oil field. For
each of these phases during the lifecycle of oil field, data mining play a
significant role. Based on which phase were talking about, knowledge creation
through scientific models, data analytics and machine learning, a effective,
productive, and on demand data insight is critical for decision making within
the organization.
  The significant challenges posed by this complex and economically vital field
justify a meeting of data scientists that are willing to share their experience
and knowledge. Thus, the Worskhop on Data Mining for Oil and Gas (DM4OG) aims
to provide a quality forum for researchers that work on the significant
challenges arising from the synergy between data science, machine learning, and
the modeling and optimization problems in the O&G industry.",arxiv
http://arxiv.org/abs/1803.02952v2,2018-03-15T01:00:25Z,2018-03-08T03:18:41Z,Touch Your Heart: A Tone-aware Chatbot for Customer Care on Social Media,"Chatbot has become an important solution to rapidly increasing customer care
demands on social media in recent years. However, current work on chatbot for
customer care ignores a key to impact user experience - tones. In this work, we
create a novel tone-aware chatbot that generates toned responses to user
requests on social media. We first conduct a formative research, in which the
effects of tones are studied. Significant and various influences of different
tones on user experience are uncovered in the study. With the knowledge of
effects of tones, we design a deep learning based chatbot that takes tone
information into account. We train our system on over 1.5 million real customer
care conversations collected from Twitter. The evaluation reveals that our
tone-aware chatbot generates as appropriate responses to user requests as human
agents. More importantly, our chatbot is perceived to be even more empathetic
than human agents.",arxiv
http://arxiv.org/abs/2005.00589v2,2020-12-02T04:45:25Z,2020-05-01T20:14:49Z,"Global Table Extractor (GTE): A Framework for Joint Table Identification
  and Cell Structure Recognition Using Visual Context","Documents are often used for knowledge sharing and preservation in business
and science, within which are tables that capture most of the critical data.
Unfortunately, most documents are stored and distributed as PDF or scanned
images, which fail to preserve logical table structure. Recent vision-based
deep learning approaches have been proposed to address this gap, but most still
cannot achieve state-of-the-art results. We present Global Table Extractor
(GTE), a vision-guided systematic framework for joint table detection and cell
structured recognition, which could be built on top of any object detection
model. With GTE-Table, we invent a new penalty based on the natural cell
containment constraint of tables to train our table network aided by cell
location predictions. GTE-Cell is a new hierarchical cell detection network
that leverages table styles. Further, we design a method to automatically label
table and cell structure in existing documents to cheaply create a large corpus
of training and test data. We use this to enhance PubTabNet with cell labels
and create FinTabNet, real-world and complex scientific and financial datasets
with detailed table structure annotations to help train and test structure
recognition. Our framework surpasses previous state-of-the-art results on the
ICDAR 2013 and ICDAR 2019 table competition in both table detection and cell
structure recognition with a significant 5.8% improvement in the full table
extraction system. Further experiments demonstrate a greater than 45%
improvement in cell structure recognition when compared to a vanilla RetinaNet
object detection model in our new out-of-domain FinTabNet.",arxiv
http://arxiv.org/abs/1804.03547v2,2018-04-11T15:20:31Z,2018-04-10T14:07:45Z,"A real-time and unsupervised face Re-Identification system for
  Human-Robot Interaction","In the context of Human-Robot Interaction (HRI), face Re-Identification (face
Re-ID) aims to verify if certain detected faces have already been observed by
robots. The ability of distinguishing between different users is crucial in
social robots as it will enable the robot to tailor the interaction strategy
toward the users' individual preferences. So far face recognition research has
achieved great success, however little attention has been paid to the realistic
applications of Face Re-ID in social robots. In this paper, we present an
effective and unsupervised face Re-ID system which simultaneously re-identifies
multiple faces for HRI. This Re-ID system employs Deep Convolutional Neural
Networks to extract features, and an online clustering algorithm to determine
the face's ID. Its performance is evaluated on two datasets: the TERESA video
dataset collected by the TERESA robot, and the YouTube Face Dataset (YTF
Dataset). We demonstrate that the optimised combination of techniques achieves
an overall 93.55% accuracy on TERESA dataset and an overall 90.41% accuracy on
YTF dataset. We have implemented the proposed method into a software module in
the HCI^2 Framework for it to be further integrated into the TERESA robot, and
has achieved real-time performance at 10~26 Frames per second.",arxiv
http://arxiv.org/abs/1912.02085v1,2019-12-04T16:28:52Z,2019-12-04T16:28:52Z,Protecting Geolocation Privacy of Photo Collections,"People increasingly share personal information, including their photos and
photo collections, on social media. This information, however, can compromise
individual privacy, particularly as social media platforms use it to infer
detailed models of user behavior, including tracking their location. We
consider the specific issue of location privacy as potentially revealed by
posting photo collections, which facilitate accurate geolocation with the help
of deep learning methods even in the absence of geotags. One means to limit
associated inadvertent geolocation privacy disclosure is by carefully pruning
select photos from photo collections before these are posted publicly. We study
this problem formally as a combinatorial optimization problem in the context of
geolocation prediction facilitated by deep learning. We first demonstrate the
complexity both by showing that a natural greedy algorithm can be arbitrarily
bad and by proving that the problem is NP-Hard. We then exhibit an important
tractable special case, as well as a more general approach based on
mixed-integer linear programming. Through extensive experiments on real photo
collections, we demonstrate that our approaches are indeed highly effective at
preserving geolocation privacy.",arxiv
http://arxiv.org/abs/1810.02648v3,2019-01-25T15:37:49Z,2018-10-05T12:39:37Z,LiveCap: Real-time Human Performance Capture from Monocular Video,"We present the first real-time human performance capture approach that
reconstructs dense, space-time coherent deforming geometry of entire humans in
general everyday clothing from just a single RGB video. We propose a novel
two-stage analysis-by-synthesis optimization whose formulation and
implementation are designed for high performance. In the first stage, a skinned
template model is jointly fitted to background subtracted input video, 2D and
3D skeleton joint positions found using a deep neural network, and a set of
sparse facial landmark detections. In the second stage, dense non-rigid 3D
deformations of skin and even loose apparel are captured based on a novel
real-time capable algorithm for non-rigid tracking using dense photometric and
silhouette constraints. Our novel energy formulation leverages automatically
identified material regions on the template to model the differing non-rigid
deformation behavior of skin and apparel. The two resulting non-linear
optimization problems per-frame are solved with specially-tailored
data-parallel Gauss-Newton solvers. In order to achieve real-time performance
of over 25Hz, we design a pipelined parallel architecture using the CPU and two
commodity GPUs. Our method is the first real-time monocular approach for
full-body performance capture. Our method yields comparable accuracy with
off-line performance capture techniques, while being orders of magnitude
faster.",arxiv
http://arxiv.org/abs/2108.06886v1,2021-08-16T03:59:39Z,2021-08-16T03:59:39Z,"Decentralized Multi-AGV Task Allocation based on Multi-Agent
  Reinforcement Learning with Information Potential Field Rewards","Automated Guided Vehicles (AGVs) have been widely used for material handling
in flexible shop floors. Each product requires various raw materials to
complete the assembly in production process. AGVs are used to realize the
automatic handling of raw materials in different locations. Efficient AGVs task
allocation strategy can reduce transportation costs and improve distribution
efficiency. However, the traditional centralized approaches make high demands
on the control center's computing power and real-time capability. In this
paper, we present decentralized solutions to achieve flexible and
self-organized AGVs task allocation. In particular, we propose two improved
multi-agent reinforcement learning algorithms, MADDPG-IPF (Information
Potential Field) and BiCNet-IPF, to realize the coordination among AGVs
adapting to different scenarios. To address the reward-sparsity issue, we
propose a reward shaping strategy based on information potential field, which
provides stepwise rewards and implicitly guides the AGVs to different material
targets. We conduct experiments under different settings (3 AGVs and 6 AGVs),
and the experiment results indicate that, compared with baseline methods, our
work obtains up to 47\% task response improvement and 22\% training iterations
reduction.",arxiv
http://arxiv.org/abs/1708.04030v1,2017-08-14T08:09:02Z,2017-08-14T08:09:02Z,"Link Classification and Tie Strength Ranking in Online Social Networks
  with Exogenous Interaction Networks","Online social networks (OSNs) have become the main medium for connecting
people, sharing knowledge and information, and for communication. The social
connections between people using these OSNs are formed as virtual links (e.g.,
friendship and following connections) that connect people. These links are the
heart of today's OSNs as they facilitate all of the activities that the members
of a social network can do. However, many of these networks suffer from noisy
links, i.e., links that do not reflect a real relationship or links that have a
low intensity, that change the structure of the network and prevent accurate
analysis of these networks. Hence, a process for assessing and ranking the
links in a social network is crucial in order to sustain a healthy and real
network. Here, we define link assessment as the process of identifying noisy
and non-noisy links in a network. In this paper, we address the problem of link
assessment and link ranking in social networks using external interaction
networks. In addition to a friendship social network, additional exogenous
interaction networks are utilized to make the assessment process more
meaningful. We employed machine learning classifiers for assessing and ranking
the links in the social network of interest using the data from exogenous
interaction networks. The method was tested with two different datasets, each
containing the social network of interest, with the ground truth, along with
the exogenous interaction networks. The results show that it is possible to
effectively assess the links of a social network using only the structure of a
single network of the exogenous interaction networks, and also using the
structure of the whole set of exogenous interaction networks. The experiments
showed that some classifiers do better than others regarding both link
classification and link ranking.",arxiv
http://arxiv.org/abs/1901.05147v1,2019-01-16T06:10:45Z,2019-01-16T06:10:45Z,The Winning Solution to the IEEE CIG 2017 Game Data Mining Competition,"Machine learning competitions such as those organized by Kaggle or KDD
represent a useful benchmark for data science research. In this work, we
present our winning solution to the Game Data Mining competition hosted at the
2017 IEEE Conference on Computational Intelligence and Games (CIG 2017). The
contest consisted of two tracks, and participants (more than 250, belonging to
both industry and academia) were to predict which players would stop playing
the game, as well as their remaining lifetime. The data were provided by a
major worldwide video game company, NCSoft, and came from their successful
massively multiplayer online game Blade and Soul. Here, we describe the long
short-term memory approach and conditional inference survival ensemble model
that made us win both tracks of the contest, as well as the validation
procedure that we followed in order to prevent overfitting. In particular,
choosing a survival method able to deal with censored data was crucial to
accurately predict the moment in which each player would leave the game, as
censoring is inherent in churn. The selected models proved to be robust against
evolving conditions---since there was a change in the business model of the
game (from subscription-based to free-to-play) between the two sample datasets
provided---and efficient in terms of time cost. Thanks to these features and
also to their a ability to scale to large datasets, our models could be readily
implemented in real business settings.",arxiv
http://arxiv.org/abs/1907.01522v1,2019-06-28T22:22:41Z,2019-06-28T22:22:41Z,Tucker Tensor Decomposition on FPGA,"Tensor computation has emerged as a powerful mathematical tool for solving
high-dimensional and/or extreme-scale problems in science and engineering. The
last decade has witnessed tremendous advancement of tensor computation and its
applications in machine learning and big data. However, its hardware
optimization on resource-constrained devices remains an (almost) unexplored
field. This paper presents an hardware accelerator for a classical tensor
computation framework, Tucker decomposition. We study three modules of this
architecture: tensor-times-matrix (TTM), matrix singular value decomposition
(SVD), and tensor permutation, and implemented them on Xilinx FPGA for
prototyping. In order to further reduce the computing time, a warm-start
algorithm for the Jacobi iterations in SVD is proposed. A fixed-point simulator
is used to evaluate the performance of our design. Some synthetic data sets and
a real MRI data set are used to validate the design and evaluate its
performance. We compare our work with state-of-the-art software toolboxes
running on both CPU and GPU, and our work shows 2.16 - 30.2x speedup on the
cardiac MRI data set.",arxiv
http://arxiv.org/abs/1907.12817v2,2019-07-31T07:14:32Z,2019-07-30T10:00:52Z,"Increasing Scalability of Process Mining using Event Dataframes: How
  Data Structure Matters","Process Mining is a branch of Data Science that aims to extract
process-related information from event data contained in information systems,
that is steadily increasing in amount. Many algorithms, and a general-purpose
open source framework (ProM 6), have been developed in the last years for
process discovery, conformance checking, machine learning on event data.
However, in very few cases scalability has been a target, prioritizing the
quality of the output over the execution speed and the optimization of
resources. This is making progressively more difficult to apply process mining
with mainstream workstations on real-life event data with any open source
process mining framework. Hence, exploring more scalable storage techniques,
in-memory data structures, more performant algorithms is a strictly incumbent
need. In this paper, we propose the usage of mainstream columnar storages and
dataframes to increase the scalability of process mining. These can replace the
classic event log structures in most tasks, but require completely different
implementations with regards to mainstream process mining algorithms.
Dataframes will be defined, some algorithms on such structures will be
presented and their complexity will be calculated.",arxiv
http://arxiv.org/abs/2001.00621v1,2019-12-17T08:58:24Z,2019-12-17T08:58:24Z,"BehavDT: A Behavioral Decision Tree Learning to Build User-Centric
  Context-Aware Predictive Model","This paper formulates the problem of building a context-aware predictive
model based on user diverse behavioral activities with smartphones. In the area
of machine learning and data science, a tree-like model as that of decision
tree is considered as one of the most popular classification techniques, which
can be used to build a data-driven predictive model. The traditional decision
tree model typically creates a number of leaf nodes as decision nodes that
represent context-specific rigid decisions, and consequently may cause
overfitting problem in behavior modeling. However, in many practical scenarios
within the context-aware environment, the generalized outcomes could play an
important role to effectively capture user behavior. In this paper, we propose
a behavioral decision tree, ""BehavDT"" context-aware model that takes into
account user behavior-oriented generalization according to individual
preference level. The BehavDT model outputs not only the generalized decisions
but also the context-specific decisions in relevant exceptional cases. The
effectiveness of our BehavDT model is studied by conducting experiments on
individual user real smartphone datasets. Our experimental results show that
the proposed BehavDT context-aware model is more effective when compared with
the traditional machine learning approaches, in predicting user diverse
behaviors considering multi-dimensional contexts.",arxiv
http://arxiv.org/abs/2012.03501v1,2020-12-07T07:51:23Z,2020-12-07T07:51:23Z,Adaptive Local Bayesian Optimization Over Multiple Discrete Variables,"In the machine learning algorithms, the choice of the hyperparameter is often
an art more than a science, requiring labor-intensive search with expert
experience. Therefore, automation on hyperparameter optimization to exclude
human intervention is a great appeal, especially for the black-box functions.
Recently, there have been increasing demands of solving such concealed tasks
for better generalization, though the task-dependent issue is not easy to
solve. The Black-Box Optimization challenge (NeurIPS 2020) required competitors
to build a robust black-box optimizer across different domains of standard
machine learning problems. This paper describes the approach of team KAIST OSI
in a step-wise manner, which outperforms the baseline algorithms by up to
+20.39%. We first strengthen the local Bayesian search under the concept of
region reliability. Then, we design a combinatorial kernel for a Gaussian
process kernel. In a similar vein, we combine the methodology of Bayesian and
multi-armed bandit,(MAB) approach to select the values with the consideration
of the variable types; the real and integer variables are with Bayesian, while
the boolean and categorical variables are with MAB. Empirical evaluations
demonstrate that our method outperforms the existing methods across different
tasks.",arxiv
http://arxiv.org/abs/2101.10595v2,2021-03-16T12:18:42Z,2021-01-26T07:09:36Z,"Probability Trajectory: One New Movement Description for Trajectory
  Prediction","Trajectory prediction is a fundamental and challenging task for numerous
applications, such as autonomous driving and intelligent robots. Currently,
most of existing work treat the pedestrian trajectory as a series of fixed
two-dimensional coordinates. However, in real scenarios, the trajectory often
exhibits randomness, and has its own probability distribution. Inspired by this
observed fact, also considering other movement characteristics of pedestrians,
we propose one simple and intuitive movement description, probability
trajectory, which maps the coordinate points of pedestrian trajectory into
two-dimensional Gaussian distribution in images. Based on this unique
description, we develop one novel trajectory prediction method, called social
probability. The method combines the new probability trajectory and powerful
convolution recurrent neural networks together. Both the input and output of
our method are probability trajectories, which provide the recurrent neural
network with sufficient spatial and random information of moving pedestrians.
And the social probability extracts spatio-temporal features directly on the
new movement description to generate robust and accurate predicted results. The
experiments on public benchmark datasets show the effectiveness of the proposed
method.",arxiv
http://arxiv.org/abs/2010.10910v1,2020-10-21T11:44:04Z,2020-10-21T11:44:04Z,Complaint Identification in Social Media with Transformer Networks,"Complaining is a speech act extensively used by humans to communicate a
negative inconsistency between reality and expectations. Previous work on
automatically identifying complaints in social media has focused on using
feature-based and task-specific neural network models. Adapting
state-of-the-art pre-trained neural language models and their combinations with
other linguistic information from topics or sentiment for complaint prediction
has yet to be explored. In this paper, we evaluate a battery of neural models
underpinned by transformer networks which we subsequently combine with
linguistic information. Experiments on a publicly available data set of
complaints demonstrate that our models outperform previous state-of-the-art
methods by a large margin achieving a macro F1 up to 87.",arxiv
http://arxiv.org/abs/1704.03507v2,2018-01-22T10:28:48Z,2017-04-11T19:35:13Z,"Unsupervised Learning of Parsimonious General-Purpose Embeddings for
  User and Location Modelling","Many social network applications depend on robust representations of
spatio-temporal data. In this work, we present an embedding model based on
feed-forward neural networks which transforms social media check-ins into dense
feature vectors encoding geographic, temporal, and functional aspects for
modelling places, neighborhoods, and users. We employ the embedding model in a
variety of applications including location recommendation, urban functional
zone study, and crime prediction. For location recommendation, we propose a
Spatio-Temporal Embedding Similarity algorithm (STES) based on the embedding
model.
  In a range of experiments on real life data collected from Foursquare, we
demonstrate our model's effectiveness at characterizing places and people and
its applicability in aforementioned problem domains. Finally, we select eight
major cities around the globe and verify the robustness and generality of our
model by porting pre-trained models from one city to another, thereby
alleviating the need for costly local training.",arxiv
http://arxiv.org/abs/1905.04815v1,2019-05-13T00:20:31Z,2019-05-13T00:20:31Z,"Programmable Spectrometry -- Per-pixel Classification of Materials using
  Learned Spectral Filters","Many materials have distinct spectral profiles. This facilitates estimation
of the material composition of a scene at each pixel by first acquiring its
hyperspectral image, and subsequently filtering it using a bank of spectral
profiles. This process is inherently wasteful since only a set of linear
projections of the acquired measurements contribute to the classification task.
We propose a novel programmable camera that is capable of producing images of a
scene with an arbitrary spectral filter. We use this camera to optically
implement the spectral filtering of the scene's hyperspectral image with the
bank of spectral profiles needed to perform per-pixel material classification.
This provides gains both in terms of acquisition speed --- since only the
relevant measurements are acquired --- and in signal-to-noise ratio --- since
we invariably avoid narrowband filters that are light inefficient. Given
training data, we use a range of classical and modern techniques including SVMs
and neural networks to identify the bank of spectral profiles that facilitate
material classification. We verify the method in simulations on standard
datasets as well as real data using a lab prototype of the camera.",arxiv
http://arxiv.org/abs/2104.11360v1,2021-04-23T00:46:35Z,2021-04-23T00:46:35Z,"Normalized multivariate time series causality analysis and causal graph
  reconstruction","Causality analysis is an important problem lying at the heart of science, and
is of particular importance in data science and machine learning. An endeavor
during the past 16 years viewing causality as real physical notion so as to
formulate it from first principles, however, seems to go unnoticed. This study
introduces to the community this line of work, with a long-due generalization
of the information flow-based bivariate time series causal inference to
multivariate series, based on the recent advance in theoretical development.
The resulting formula is transparent, and can be implemented as a
computationally very efficient algorithm for application. It can be normalized,
and tested for statistical significance. Different from the previous work along
this line where only information flows are estimated, here an algorithm is also
implemented to quantify the influence of a unit to itself. While this forms a
challenge in some causal inferences, here it comes naturally, and hence the
identification of self-loops in a causal graph is fulfilled automatically as
the causalities along edges are inferred.
  To demonstrate the power of the approach, presented here are two applications
in extreme situations. The first is a network of multivariate processes buried
in heavy noises (with the noise-to-signal ratio exceeding 100), and the second
a network with nearly synchronized chaotic oscillators. In both graphs,
confounding processes exist. While it seems to be a huge challenge to
reconstruct from given series these causal graphs, an easy application of the
algorithm immediately reveals the desideratum. Particularly, the confounding
processes have been accurately differentiated. Considering the surge of
interest in the community, this study is very timely.",arxiv
http://arxiv.org/abs/1509.05066v1,2015-09-16T21:10:12Z,2015-09-16T21:10:12Z,Processing Analytical Workloads Incrementally,"Analysis of large data collections using popular machine learning and
statistical algorithms has been a topic of increasing research interest. A
typical analysis workload consists of applying an algorithm to build a model on
a data collection and subsequently refining it based on the results.
  In this paper we introduce model materialization and incremental model reuse
as first class citizens in the execution of analysis workloads. We materialize
built models instead of discarding them in a way that can be reused in
subsequent computations. At the same time we consider manipulating an existing
model (adding or deleting data from it) in order to build a new one. We discuss
our approach in the context of popular machine learning models. We specify the
details of how to incrementally maintain models as well as outline the suitable
optimizations required to optimally use models and their incremental
adjustments to build new ones. We detail our techniques for linear regression,
naive bayes and logistic regression and present the suitable algorithms and
optimizations to handle these models in our framework.
  We present the results of a detailed performance evaluation, using real and
synthetic data sets. Our experiments analyze the various trade offs inherent in
our approach and demonstrate vast performance benefits.",arxiv
http://arxiv.org/abs/2006.08209v1,2020-06-15T08:19:14Z,2020-06-15T08:19:14Z,Multimodal fusion for sea level anomaly forecasting,"The accumulated remote sensing data of altimeters and scatterometers have
provided a new opportunity to forecast the ocean states and improve the
knowledge in ocean/atmosphere exchanges. Few previous studies have focused on
sea level anomaly (SLA) multi-step forecasting by multivariate deep learning
for different modalities. For this paper, a novel multimodal fusion approach
named MMFnet is used for SLA multi-step forecasting in South China Sea (SCS).
First, a grid forecasting network is trained by an improved Convolutional Long
Short-Term Memory (ConvLSTM) network on daily multiple remote sensing data from
1993 to 2016. Then, an in-situ forecasting network is trained by an improved
LSTM network, which is decomposed by the ensemble empirical mode decomposition
(EEMD-LSTM), on real-time, in-situ and remote sensing data. Finally, the two
single-modal networks are fused by an ocean data assimilation scheme. During
the test period from 2017 to 2019, the average RMSE of the MMFnet (single-modal
ConvLSTM) is 4.03 cm (4.51 cm), the 15th-day anomaly correlation coefficient is
0.78 (0.67), the performance of MMFnet is much higher than those of current
state-of-the-art dynamical (HYCOM) and statistical (ConvLSTM, Persistence and
daily Climatology) forecasting systems. Sensitivity experiments analysis
indicates that, the MMFnet, which added CCMP SCAT products and OISST for SLA
forecasting, has improved the forecast range over a week and can effectively
produce 15-day SLA forecasting with reasonable accuracies.In an extension of
the validation over the North Pacific Ocean, MMFnet can calculate the
forecasting results in a few minutes, and we find good agreement in amplitude
and distribution of SLA variability between MMFnet and other classical
operational model products.",arxiv
http://arxiv.org/abs/1503.02129v3,2015-06-18T03:37:44Z,2015-03-07T03:35:26Z,Learning Scale-Free Networks by Dynamic Node-Specific Degree Prior,"Learning the network structure underlying data is an important problem in
machine learning. This paper introduces a novel prior to study the inference of
scale-free networks, which are widely used to model social and biological
networks. The prior not only favors a desirable global node degree
distribution, but also takes into consideration the relative strength of all
the possible edges adjacent to the same node and the estimated degree of each
individual node.
  To fulfill this, ranking is incorporated into the prior, which makes the
problem challenging to solve. We employ an ADMM (alternating direction method
of multipliers) framework to solve the Gaussian Graphical model regularized by
this prior. Our experiments on both synthetic and real data show that our prior
not only yields a scale-free network, but also produces many more correctly
predicted edges than the others such as the scale-free inducing prior, the
hub-inducing prior and the $l_1$ norm.",arxiv
http://arxiv.org/abs/2101.03717v2,2021-01-13T00:06:45Z,2021-01-11T05:57:32Z,"Constraint 2021: Machine Learning Models for COVID-19 Fake News
  Detection Shared Task","In this system paper we present our contribution to the Constraint 2021
COVID-19 Fake News Detection Shared Task, which poses the challenge of
classifying COVID-19 related social media posts as either fake or real. In our
system, we address this challenge by applying classical machine learning
algorithms together with several linguistic features, such as n-grams,
readability, emotional tone and punctuation. In terms of pre-processing, we
experiment with various steps like stop word removal, stemming/lemmatization,
link removal and more. We find our best performing system to be based on a
linear SVM, which obtains a weighted average F1 score of 95.19% on test data,
which lands a place in the middle of the leaderboard (place 80 of 167).",arxiv
http://arxiv.org/abs/1707.05223v1,2017-07-17T15:18:36Z,2017-07-17T15:18:36Z,A transient search using combined human and machine classifications,"Large modern surveys require efficient review of data in order to find
transient sources such as supernovae, and to distinguish such sources from
artefacts and noise. Much effort has been put into the development of automatic
algorithms, but surveys still rely on human review of targets. This paper
presents an integrated system for the identification of supernovae in data from
Pan-STARRS1, combining classifications from volunteers participating in a
citizen science project with those from a convolutional neural network. The
unique aspect of this work is the deployment, in combination, of both human and
machine classifications for near real-time discovery in an astronomical
project. We show that the combination of the two methods outperforms either one
used individually. This result has important implications for the future
development of transient searches, especially in the era of LSST and other
large-throughput surveys.",arxiv
http://arxiv.org/abs/1911.03219v1,2019-11-08T12:42:22Z,2019-11-08T12:42:22Z,"Language Grounding through Social Interactions and Curiosity-Driven
  Multi-Goal Learning","Autonomous reinforcement learning agents, like children, do not have access
to predefined goals and reward functions. They must discover potential goals,
learn their own reward functions and engage in their own learning trajectory.
Children, however, benefit from exposure to language, helping to organize and
mediate their thought. We propose LE2 (Language Enhanced Exploration), a
learning algorithm leveraging intrinsic motivations and natural language (NL)
interactions with a descriptive social partner (SP). Using NL descriptions from
the SP, it can learn an NL-conditioned reward function to formulate goals for
intrinsically motivated goal exploration and learn a goal-conditioned policy.
By exploring, collecting descriptions from the SP and jointly learning the
reward function and the policy, the agent grounds NL descriptions into real
behavioral goals. From simple goals discovered early to more complex goals
discovered by experimenting on simpler ones, our agent autonomously builds its
own behavioral repertoire. This naturally occurring curriculum is supplemented
by an active learning curriculum resulting from the agent's intrinsic
motivations. Experiments are presented with a simulated robotic arm that
interacts with several objects including tools.",arxiv
http://arxiv.org/abs/2003.03971v1,2020-03-09T08:52:53Z,2020-03-09T08:52:53Z,"DeepCP: Deep Learning Driven Cascade Prediction Based Autonomous Content
  Placement in Closed Social Network","Online social networks (OSNs) are emerging as the most popular mainstream
platform for content cascade diffusion. In order to provide satisfactory
quality of experience (QoE) for users in OSNs, much research dedicates to
proactive content placement by using the propagation pattern, user's personal
profiles and social relationships in open social network scenarios (e.g.,
Twitter and Weibo). In this paper, we take a new direction of popularity-aware
content placement in a closed social network (e.g., WeChat Moment) where user's
privacy is highly enhanced. We propose a novel data-driven holistic deep
learning framework, namely DeepCP, for joint diffusion-aware cascade prediction
and autonomous content placement without utilizing users' personal and social
information. We first devise a time-window LSTM model for content popularity
prediction and cascade geo-distribution estimation. Accordingly, we further
propose a novel autonomous content placement mechanism CP-GAN which adopts the
generative adversarial network (GAN) for agile placement decision making to
reduce the content access latency and enhance users' QoE. We conduct extensive
experiments using cascade diffusion traces in WeChat Moment (WM). Evaluation
results corroborate that the proposed DeepCP framework can predict the content
popularity with a high accuracy, generate efficient placement decision in a
real-time manner, and achieve significant content access latency reduction over
existing schemes.",arxiv
http://arxiv.org/abs/1403.0156v1,2014-03-02T04:14:23Z,2014-03-02T04:14:23Z,Sleep Analytics and Online Selective Anomaly Detection,"We introduce a new problem, the Online Selective Anomaly Detection (OSAD), to
model a specific scenario emerging from research in sleep science. Scientists
have segmented sleep into several stages and stage two is characterized by two
patterns (or anomalies) in the EEG time series recorded on sleep subjects.
These two patterns are sleep spindle (SS) and K-complex. The OSAD problem was
introduced to design a residual system, where all anomalies (known and unknown)
are detected but the system only triggers an alarm when non-SS anomalies
appear. The solution of the OSAD problem required us to combine techniques from
both machine learning and control theory. Experiments on data from real
subjects attest to the effectiveness of our approach.",arxiv
http://arxiv.org/abs/2008.02321v2,2021-02-25T03:04:37Z,2020-08-05T19:00:36Z,"Can I Pour into It? Robot Imagining Open Containability Affordance of
  Previously Unseen Objects via Physical Simulations","Open containers, i.e., containers without covers, are an important and
ubiquitous class of objects in human life. In this letter, we propose a novel
method for robots to ""imagine"" the open containability affordance of a
previously unseen object via physical simulations. We implement our imagination
method on a UR5 manipulator. The robot autonomously scans the object with an
RGB-D camera. The scanned 3D model is used for open containability imagination
which quantifies the open containability affordance by physically simulating
dropping particles onto the object and counting how many particles are retained
in it. This quantification is used for open-container vs. non-open-container
binary classification (hereafter referred to as open container classification).
If the object is classified as an open container, the robot further imagines
pouring into the object, again using physical simulations, to obtain the
pouring position and orientation for real robot autonomous pouring. We evaluate
our method on open container classification and autonomous pouring of granular
material on a dataset containing 130 previously unseen objects with 57 object
categories. Although our proposed method uses only 11 objects for simulation
calibration (training), its open container classification aligns well with
human judgements. In addition, our method endows the robot with the capability
to autonomously pour into the 55 containers in the dataset with a very high
success rate. We also compare to a deep learning method. Results show that our
method achieves the same performance as the deep learning method on open
container classification and outperforms it on autonomous pouring. Moreover,
our method is fully explainable.",arxiv
http://arxiv.org/abs/1808.06277v1,2018-08-20T00:54:29Z,2018-08-20T00:54:29Z,An Efficient Approach for Geo-Multimedia Cross-Modal Retrieval,"Due to the rapid development of mobile Internet techniques, cloud computation
and popularity of online social networking and location-based services, massive
amount of multimedia data with geographical information is generated and
uploaded to the Internet. In this paper, we propose a novel type of cross-modal
multimedia retrieval called geo-multimedia cross-modal retrieval which aims to
search out a set of geo-multimedia objects based on geographical distance
proximity and semantic similarity between different modalities. Previous
studies for cross-modal retrieval and spatial keyword search cannot address
this problem effectively because they do not consider multimedia data with
geo-tags and do not focus on this type of query. In order to address this
problem efficiently, we present the definition of $k$NN geo-multimedia
cross-modal query at the first time and introduce relevant conceptions such as
cross-modal semantic representation space. To bridge the semantic gap between
different modalities, we propose a method named cross-modal semantic matching
which contains two important component, i.e., CorrProj and LogsTran, which aims
to construct a common semantic representation space for cross-modal semantic
similarity measurement. Besides, we designed a framework based on deep learning
techniques to implement common semantic representation space construction. In
addition, a novel hybrid indexing structure named GMR-Tree combining
geo-multimedia data and R-Tree is presented and a efficient $k$NN search
algorithm called $k$GMCMS is designed. Comprehensive experimental evaluation on
real and synthetic dataset clearly demonstrates that our solution outperforms
the-state-of-the-art methods.",arxiv
http://arxiv.org/abs/1511.09120v4,2017-12-18T14:04:48Z,2015-11-30T00:44:41Z,Coresets for Kinematic Data: From Theorems to Real-Time Systems,"A coreset (or core-set) of a dataset is its semantic compression with respect
to a set of queries, such that querying the (small) coreset provably yields an
approximate answer to querying the original (full) dataset. In the last decade,
coresets provided breakthroughs in theoretical computer science for
approximation algorithms, and more recently, in the machine learning community
for learning ""Big data"". However, we are not aware of real-time systems that
compute coresets in a rate of dozens of frames per second. In this paper we
suggest a framework to turn theorems to such systems using coresets. We begin
with a proof of independent interest, that any set of $n$ matrices in
$\mathbb{R}^{d\times d}$ whose sum is $S$, has a positively weighted subset
whose sum has the same center of mass (mean) and orientation (left+right
singular vectors) as $S$, and consists of $O(dr)$ matrices (independent of
$n$), where $r\leq d$ is the rank of $S$. We provide an algorithm that computes
this (core) set in one pass over possibly infinite stream of matrices in
$d^{O(1)}$ time per matrix insertion. By maintaining such a coreset for
kinematic (moving) set of $n$ points, we can run pose-estimation algorithms,
such as Kabsch or PnP, on the small coresets, instead of the $n$ points, in
real-time using weak devices, while obtaining the same results. This enabled us
to implement a low-cost ($<\$100$) IoT wireless system that tracks a toy (and
harmless) quadcopter which guides guests to a desired room (in a hospital,
mall, hotel, museum, etc.) with no help of additional human or remote
controller. We hope that our framework will encourage researchers outside the
theoretical community to design and use coresets in future systems and papers.
To this end, we provide extensive experimental results on both synthetic and
real data, as well as a link to the open code of our system and algorithms.",arxiv
http://arxiv.org/abs/2104.00864v1,2021-04-02T03:04:24Z,2021-04-02T03:04:24Z,"Constrained non-negative matrix factorization enabling real-time
  insights of $\textit{in situ}$ and high-throughput experiments","Non-negative Matrix Factorization (NMF) methods offer an appealing
unsupervised learning method for real-time analysis of streaming spectral data
in time-sensitive data collection, such as $\textit{in situ}$ characterization
of materials. However, canonical NMF methods are optimized to reconstruct a
full dataset as closely as possible, with no underlying requirement that the
reconstruction produces components or weights representative of the true
physical processes. In this work, we demonstrate how constraining NMF weights
or components, provided as known or assumed priors, can provide significant
improvement in revealing true underlying phenomena. We present a PyTorch based
method for efficiently applying constrained NMF and demonstrate this on several
synthetic examples. When applied to streaming experimentally measured spectral
data, an expert researcher-in-the-loop can provide and dynamically adjust the
constraints. This set of interactive priors to the NMF model can, for example,
contain known or identified independent components, as well as functional
expectations about the mixing of components. We demonstrate this application on
measured X-ray diffraction and pair distribution function data from $\textit{in
situ}$ beamline experiments. Details of the method are described, and general
guidance provided to employ constrained NMF in extraction of critical
information and insights during $\textit{in situ}$ and high-throughput
experiments.",arxiv
http://arxiv.org/abs/1906.04477v4,2020-06-08T14:48:29Z,2019-06-11T10:09:35Z,Causal Discovery with Reinforcement Learning,"Discovering causal structure among a set of variables is a fundamental
problem in many empirical sciences. Traditional score-based casual discovery
methods rely on various local heuristics to search for a Directed Acyclic Graph
(DAG) according to a predefined score function. While these methods, e.g.,
greedy equivalence search, may have attractive results with infinite samples
and certain model assumptions, they are usually less satisfactory in practice
due to finite data and possible violation of assumptions. Motivated by recent
advances in neural combinatorial optimization, we propose to use Reinforcement
Learning (RL) to search for the DAG with the best scoring. Our encoder-decoder
model takes observable data as input and generates graph adjacency matrices
that are used to compute rewards. The reward incorporates both the predefined
score function and two penalty terms for enforcing acyclicity. In contrast with
typical RL applications where the goal is to learn a policy, we use RL as a
search strategy and our final output would be the graph, among all graphs
generated during training, that achieves the best reward. We conduct
experiments on both synthetic and real datasets, and show that the proposed
approach not only has an improved search ability but also allows a flexible
score function under the acyclicity constraint.",arxiv
http://arxiv.org/abs/1810.10659v1,2018-10-25T00:12:44Z,2018-10-25T00:12:44Z,"Combinatorial Optimization with Graph Convolutional Networks and Guided
  Tree Search","We present a learning-based approach to computing solutions for certain
NP-hard problems. Our approach combines deep learning techniques with useful
algorithmic elements from classic heuristics. The central component is a graph
convolutional network that is trained to estimate the likelihood, for each
vertex in a graph, of whether this vertex is part of the optimal solution. The
network is designed and trained to synthesize a diverse set of solutions, which
enables rapid exploration of the solution space via tree search. The presented
approach is evaluated on four canonical NP-hard problems and five datasets,
which include benchmark satisfiability problems and real social network graphs
with up to a hundred thousand nodes. Experimental results demonstrate that the
presented approach substantially outperforms recent deep learning work, and
performs on par with highly optimized state-of-the-art heuristic solvers for
some NP-hard problems. Experiments indicate that our approach generalizes
across datasets, and scales to graphs that are orders of magnitude larger than
those used during training.",arxiv
http://arxiv.org/abs/2103.06327v1,2021-03-10T20:14:41Z,2021-03-10T20:14:41Z,"Closed Loop Predictive Control of Adaptive Optics Systems with
  Convolutional Neural Networks","Predictive wavefront control is an important and rapidly developing field of
adaptive optics (AO). Through the prediction of future wavefront effects, the
inherent AO system servo-lag caused by the measurement, computation, and
application of the wavefront correction can be significantly mitigated. This
lag can impact the final delivered science image, including reduced strehl and
contrast, and inhibits our ability to reliably use faint guidestars. We
summarize here a novel method for training deep neural networks for predictive
control based on an adversarial prior. Unlike previous methods in the
literature, which have shown results based on previously generated data or for
open-loop systems, we demonstrate our network's performance simulated in closed
loop. Our models are able to both reduce effects induced by servo-lag and push
the faint end of reliable control with natural guidestars, improving K-band
Strehl performance compared to classical methods by over 55% for 16th magnitude
guide stars on an 8-meter telescope. We further show that LSTM based approaches
may be better suited in high-contrast scenarios where servo-lag error is most
pronounced, while traditional feed forward models are better suited for high
noise scenarios. Finally, we discuss future strategies for implementing our
system in real-time and on astronomical telescope systems.",arxiv
http://arxiv.org/abs/2103.16584v1,2021-03-30T18:01:06Z,2021-03-30T18:01:06Z,"Parameterized Hypercomplex Graph Neural Networks for Graph
  Classification","Despite recent advances in representation learning in hypercomplex (HC)
space, this subject is still vastly unexplored in the context of graphs.
Motivated by the complex and quaternion algebras, which have been found in
several contexts to enable effective representation learning that inherently
incorporates a weight-sharing mechanism, we develop graph neural networks that
leverage the properties of hypercomplex feature transformation. In particular,
in our proposed class of models, the multiplication rule specifying the algebra
itself is inferred from the data during training. Given a fixed model
architecture, we present empirical evidence that our proposed model
incorporates a regularization effect, alleviating the risk of overfitting. We
also show that for fixed model capacity, our proposed method outperforms its
corresponding real-formulated GNN, providing additional confirmation for the
enhanced expressivity of HC embeddings. Finally, we test our proposed
hypercomplex GNN on several open graph benchmark datasets and show that our
models reach state-of-the-art performance while consuming a much lower memory
footprint with 70& fewer parameters. Our implementations are available at
https://github.com/bayer-science-for-a-better-life/phc-gnn.",arxiv
http://arxiv.org/abs/1810.06637v2,2019-05-02T15:03:20Z,2018-10-15T19:46:00Z,"Nonlinear System Identification of Soft Robot Dynamics Using Koopman
  Operator Theory","Soft robots are challenging to model due in large part to the nonlinear
properties of soft materials. Fortunately, this softness makes it possible to
safely observe their behavior under random control inputs, making them amenable
to large-scale data collection and system identification. This paper implements
and evaluates a system identification method based on Koopman operator theory
in which models of nonlinear dynamical systems are constructed via linear
regression of observed data by exploiting the fact that every nonlinear system
has a linear representation in the infinite-dimensional space of real-valued
functions called observables. The approach does not suffer from some of the
shortcomings of other nonlinear system identification methods, which typically
require the manual tuning of training parameters and have limited convergence
guarantees. A dynamic model of a pneumatic soft robot arm is constructed via
this method, and used to predict the behavior of the real system. The total
normalized-root-mean-square error (NRMSE) of its predictions is lower than that
of several other identified models including a neural network, NLARX, nonlinear
Hammerstein-Wiener, and linear state space model.",arxiv
http://arxiv.org/abs/2010.03203v1,2020-10-07T06:23:38Z,2020-10-07T06:23:38Z,"RealSmileNet: A Deep End-To-End Network for Spontaneous and Posed Smile
  Recognition","Smiles play a vital role in the understanding of social interactions within
different communities, and reveal the physical state of mind of people in both
real and deceptive ways. Several methods have been proposed to recognize
spontaneous and posed smiles. All follow a feature-engineering based pipeline
requiring costly pre-processing steps such as manual annotation of face
landmarks, tracking, segmentation of smile phases, and hand-crafted features.
The resulting computation is expensive, and strongly dependent on
pre-processing steps. We investigate an end-to-end deep learning model to
address these problems, the first end-to-end model for spontaneous and posed
smile recognition. Our fully automated model is fast and learns the feature
extraction processes by training a series of convolution and ConvLSTM layer
from scratch. Our experiments on four datasets demonstrate the robustness and
generalization of the proposed model by achieving state-of-the-art
performances.",arxiv
http://arxiv.org/abs/1610.07432v1,2016-10-24T14:37:27Z,2016-10-24T14:37:27Z,"Virtual Embodiment: A Scalable Long-Term Strategy for Artificial
  Intelligence Research","Meaning has been called the ""holy grail"" of a variety of scientific
disciplines, ranging from linguistics to philosophy, psychology and the
neurosciences. The field of Artifical Intelligence (AI) is very much a part of
that list: the development of sophisticated natural language semantics is a
sine qua non for achieving a level of intelligence comparable to humans.
Embodiment theories in cognitive science hold that human semantic
representation depends on sensori-motor experience; the abundant evidence that
human meaning representation is grounded in the perception of physical reality
leads to the conclusion that meaning must depend on a fusion of multiple
(perceptual) modalities. Despite this, AI research in general, and its
subdisciplines such as computational linguistics and computer vision in
particular, have focused primarily on tasks that involve a single modality.
Here, we propose virtual embodiment as an alternative, long-term strategy for
AI research that is multi-modal in nature and that allows for the kind of
scalability required to develop the field coherently and incrementally, in an
ethically responsible fashion.",arxiv
http://arxiv.org/abs/2108.04822v1,2021-08-10T07:53:09Z,2021-08-10T07:53:09Z,Self-supervised Consensus Representation Learning for Attributed Graph,"Attempting to fully exploit the rich information of topological structure and
node features for attributed graph, we introduce self-supervised learning
mechanism to graph representation learning and propose a novel Self-supervised
Consensus Representation Learning (SCRL) framework. In contrast to most
existing works that only explore one graph, our proposed SCRL method treats
graph from two perspectives: topology graph and feature graph. We argue that
their embeddings should share some common information, which could serve as a
supervisory signal. Specifically, we construct the feature graph of node
features via k-nearest neighbor algorithm. Then graph convolutional network
(GCN) encoders extract features from two graphs respectively. Self-supervised
loss is designed to maximize the agreement of the embeddings of the same node
in the topology graph and the feature graph. Extensive experiments on real
citation networks and social networks demonstrate the superiority of our
proposed SCRL over the state-of-the-art methods on semi-supervised node
classification task. Meanwhile, compared with its main competitors, SCRL is
rather efficient.",arxiv
http://arxiv.org/abs/1302.5235v2,2013-03-01T10:21:08Z,2013-02-21T10:06:35Z,"Predicting the Temporal Dynamics of Information Diffusion in Social
  Networks","Online social networks play a major role in the spread of information at very
large scale and it becomes essential to provide means to analyse this
phenomenon. In this paper we address the issue of predicting the temporal
dynamics of the information diffusion process. We develop a graph-based
approach built on the assumption that the macroscopic dynamics of the spreading
process are explained by the topology of the network and the interactions that
occur through it, between pairs of users, on the basis of properties at the
microscopic level. We introduce a generic model, called T-BaSIC, and describe
how to estimate its parameters from users behaviours using machine learning
techniques. Contrary to classical approaches where the parameters are fixed in
advance, T-BaSIC's parameters are functions depending of time, which permit to
better approximate and adapt to the diffusion phenomenon observed in online
social networks. Our proposal has been validated on real Twitter datasets.
Experiments show that our approach is able to capture the particular patterns
of diffusion depending of the studied sub-networks of users and topics. The
results corroborate the ""two-step"" theory (1955) that states that information
flows from media to a few ""opinion leaders"" who then transfer it to the mass
population via social networks and show that it applies in the online context.
This work also highlights interesting recommendations for future
investigations.",arxiv
http://arxiv.org/abs/2106.14742v1,2021-06-28T14:17:22Z,2021-06-28T14:17:22Z,TENT: Tensorized Encoder Transformer for Temperature Forecasting,"Reliable weather forecasting is of great importance in science, business and
society. The best performing data-driven models for weather prediction tasks
rely on recurrent or convolutional neural networks, where some of which
incorporate attention mechanisms. In this work, we introduce a new model based
on the Transformer architecture for weather forecasting. The proposed Tensorial
Encoder Transformer (TENT) model is equipped with tensorial attention and thus
it exploits the spatiotemporal structure of weather data by processing it in
multidimensional tensorial format. We show that compared to the encoder part of
the original transformer and 3D convolutional neural networks, the proposed
TENT model can better model the underlying complex pattern of weather data for
the studied temperature prediction task. Experiments on two real-life weather
datasets are performed. The datasets consist of historical measurements from
USA, Canada and European cities. The first dataset contains hourly measurements
of weather attributes for 30 cities in USA and Canada from October 2012 to
November 2017. The second dataset contains daily measurements of weather
attributes of 18 cities across Europe from May 2005 to April 2020. We use
attention scores calculated from our attention mechanism to shed light on the
decision-making process of our model and have insight knowledge on the most
important cities for the task.",arxiv
http://arxiv.org/abs/2105.08841v1,2021-05-18T21:43:51Z,2021-05-18T21:43:51Z,A Deep Learning Method for AGILE-GRID GRB Detection,"The follow-up of external science alerts received from Gamma-Ray Bursts (GRB)
and Gravitational Waves (GW) detectors is one of the AGILE Team's current major
activities. The AGILE team developed an automated real-time analysis pipeline
to analyse AGILE Gamma-Ray Imaging Detector (GRID) data to detect possible
counterparts in the energy range 0.1-10 GeV. This work presents a new approach
for detecting GRBs using a Convolutional Neural Network (CNN) to classify the
AGILE-GRID intensity maps improving the GRBs detection capability over the
Li&Ma method, currently used by the AGILE team. The CNN is trained with large
simulated datasets of intensity maps. The AGILE complex observing pattern due
to the so-called 'spinning mode' is studied to prepare datasets to test and
evaluate the CNN. A GRB emission model is defined from the Second Fermi-LAT GRB
catalogue and convoluted with the AGILE observing pattern. Different p-value
distributions are calculated evaluating with the CNN millions of
background-only maps simulated varying the background level. The CNN is then
used on real data to analyse the AGILE-GRID data archive, searching for GRB
detections using the trigger time and position taken from the Swift-BAT,
Fermi-GBM, and Fermi-LAT GRB catalogues. From these catalogues, the CNN detects
21 GRBs with a significance $\geq 3 \sigma$, while the Li&Ma method detects
only two GRBs. The results shown in this work demonstrate that the CNN is more
effective in detecting GRBs than the Li&Ma method in this context and can be
implemented into the AGILE-GRID real-time analysis pipeline.",arxiv
http://arxiv.org/abs/1801.03226v1,2018-01-10T03:17:45Z,2018-01-10T03:17:45Z,Adaptive Graph Convolutional Neural Networks,"Graph Convolutional Neural Networks (Graph CNNs) are generalizations of
classical CNNs to handle graph data such as molecular data, point could and
social networks. Current filters in graph CNNs are built for fixed and shared
graph structure. However, for most real data, the graph structures varies in
both size and connectivity. The paper proposes a generalized and flexible graph
CNN taking data of arbitrary graph structure as input. In that way a
task-driven adaptive graph is learned for each graph data while training. To
efficiently learn the graph, a distance metric learning is proposed. Extensive
experiments on nine graph-structured datasets have demonstrated the superior
performance improvement on both convergence speed and predictive accuracy.",arxiv
http://arxiv.org/abs/1812.06443v1,2018-12-16T11:00:37Z,2018-12-16T11:00:37Z,"""When and Where?"": Behavior Dominant Location Forecasting with
  Micro-blog Streams","The proliferation of smartphones and wearable devices has increased the
availability of large amounts of geospatial streams to provide significant
automated discovery of knowledge in pervasive environments, but most prominent
information related to altering interests have not yet adequately capitalized.
In this paper, we provide a novel algorithm to exploit the dynamic fluctuations
in user's point-of-interest while forecasting the future place of visit with
fine granularity. Our proposed algorithm is based on the dynamic formation of
collective personality communities using different languages, opinions,
geographical and temporal distributions for finding out optimized equivalent
content. We performed extensive empirical experiments involving, real-time
streams derived from 0.6 million stream tuples of micro-blog comprising 1945
social person fusion with graph algorithm and feed-forward neural network model
as a predictive classification model. Lastly, The framework achieves 62.10%
mean average precision on 1,20,000 embeddings on unlabeled users and
surprisingly 85.92% increment on the state-of-the-art approach.",arxiv
http://arxiv.org/abs/1802.05121v1,2018-02-14T14:47:56Z,2018-02-14T14:47:56Z,Co-training for Extraction of Adverse Drug Reaction Mentions from Tweets,"Adverse drug reactions (ADRs) are one of the leading causes of mortality in
health care. Current ADR surveillance systems are often associated with a
substantial time lag before such events are officially published. On the other
hand, online social media such as Twitter contain information about ADR events
in real-time, much before any official reporting. Current state-of-the-art
methods in ADR mention extraction use Recurrent Neural Networks (RNN), which
typically need large labeled corpora. Towards this end, we propose a
semi-supervised method based on co-training which can exploit a large pool of
unlabeled tweets to augment the limited supervised training data, and as a
result enhance the performance. Experiments with 0.1M tweets show that the
proposed approach outperforms the state-of-the-art methods for the ADR mention
extraction task by 5% in terms of F1 score.",arxiv
http://arxiv.org/abs/2004.11648v1,2020-04-24T10:42:49Z,2020-04-24T10:42:49Z,"GCAN: Graph-aware Co-Attention Networks for Explainable Fake News
  Detection on Social Media","This paper solves the fake news detection problem under a more realistic
scenario on social media. Given the source short-text tweet and the
corresponding sequence of retweet users without text comments, we aim at
predicting whether the source tweet is fake or not, and generating explanation
by highlighting the evidences on suspicious retweeters and the words they
concern. We develop a novel neural network-based model, Graph-aware
Co-Attention Networks (GCAN), to achieve the goal. Extensive experiments
conducted on real tweet datasets exhibit that GCAN can significantly outperform
state-of-the-art methods by 16% in accuracy on average. In addition, the case
studies also show that GCAN can produce reasonable explanations.",arxiv
http://arxiv.org/abs/2005.07427v2,2020-05-25T08:38:54Z,2020-05-15T09:17:08Z,"Structural Temporal Graph Neural Networks for Anomaly Detection in
  Dynamic Graphs","Detecting anomalies in dynamic graphs is a vital task, with numerous
practical applications in areas such as security, finance, and social media.
Previous network embedding based methods have been mostly focusing on learning
good node representations, whereas largely ignoring the subgraph structural
changes related to the target nodes in dynamic graphs. In this paper, we
propose StrGNN, an end-to-end structural temporal Graph Neural Network model
for detecting anomalous edges in dynamic graphs. In particular, we first
extract the $h$-hop enclosing subgraph centered on the target edge and propose
the node labeling function to identify the role of each node in the subgraph.
Then, we leverage graph convolution operation and Sortpooling layer to extract
the fixed-size feature from each snapshot/timestamp. Based on the extracted
features, we utilize Gated recurrent units (GRUs) to capture the temporal
information for anomaly detection. Extensive experiments on six benchmark
datasets and a real enterprise security system demonstrate the effectiveness of
StrGNN.",arxiv
http://arxiv.org/abs/2005.13754v1,2020-05-28T02:56:17Z,2020-05-28T02:56:17Z,COVID-19 and Your Smartphone: BLE-based Smart Contact Tracing,"Contact tracing is of paramount importance when it comes to preventing the
spreading of infectious diseases. Contact tracing is usually performed manually
by authorized personnel. Manual contact tracing is an inefficient, error-prone,
time-consuming process of limited utility to the population at large as those
in close contact with infected individuals are informed hours, if not days,
later. This paper introduces an alternative way to manual contact tracing. The
proposed Smart Contact Tracing (SCT) system utilizes the smartphone's Bluetooth
Low Energy (BLE) signals and machine learning classifier to accurately and
quickly determined the contact profile. SCT's contribution is two-fold: a)
classification of the user's contact as high/low-risk using precise proximity
sensing, and b) user anonymity using a privacy-preserving communications
protocol. SCT leverages BLE's non-connectable advertising feature to broadcast
a signature packet when the user is in the public space. Both broadcasted and
observed signatures are stored in the user's smartphone and they are only
uploaded to a secure signature database when a user is confirmed by public
health authorities to be infected. Using received signal strength (RSS) each
smartphone estimates its distance from other user's phones and issues real-time
alerts when social distancing rules are violated. The paper includes extensive
experimentation utilizing real-life smartphone positions and a comparative
evaluation of five machine learning classifiers. Reported results indicate that
a decision tree classifier outperforms other states of the art classification
methods in terms of accuracy. Lastly, to facilitate research in this area, and
to contribute to the timely development of advanced solutions the entire data
set of six experiments with about 123,000 data points is made publicly
available.",arxiv
http://arxiv.org/abs/2111.08004v1,2021-11-13T13:58:43Z,2021-11-13T13:58:43Z,Bag of Tricks and A Strong baseline for Image Copy Detection,"Image copy detection is of great importance in real-life social media. In
this paper, a bag of tricks and a strong baseline are proposed for image copy
detection. Unsupervised pre-training substitutes the commonly-used supervised
one. Beyond that, we design a descriptor stretching strategy to stabilize the
scores of different queries. Experiments demonstrate that the proposed method
is effective. The proposed baseline ranks third out of 526 participants on the
Facebook AI Image Similarity Challenge: Descriptor Track. The code and trained
models are available at
https://github.com/WangWenhao0716/ISC-Track2-Submission.",arxiv
http://arxiv.org/abs/2012.01653v1,2020-12-03T02:23:48Z,2020-12-03T02:23:48Z,Deep Spectral CNN for Laser Induced Breakdown Spectroscopy,"This work proposes a spectral convolutional neural network (CNN) operating on
laser induced breakdown spectroscopy (LIBS) signals to learn to (1) disentangle
spectral signals from the sources of sensor uncertainty (i.e., pre-process) and
(2) get qualitative and quantitative measures of chemical content of a sample
given a spectral signal (i.e., calibrate). Once the spectral CNN is trained, it
can accomplish either task through a single feed-forward pass, with real-time
benefits and without any additional side information requirements including
dark current, system response, temperature and detector-to-target range. Our
experiments demonstrate that the proposed method outperforms the existing
approaches used by the Mars Science Lab for pre-processing and calibration for
remote sensing observations from the Mars rover, 'Curiosity'.",arxiv
http://arxiv.org/abs/2106.03019v1,2021-06-06T03:17:29Z,2021-06-06T03:17:29Z,"Machine Learning Based Anxiety Detection in Older Adults using Wristband
  Sensors and Context Feature","This paper explores a novel method for anxiety detection in older adults
using simple wristband sensors such as Electrodermal Activity (EDA) and
Photoplethysmogram (PPG) and a context-based feature. The proposed method for
anxiety detection combines features from a single physiological signal with an
experimental context-based feature to improve the performance of the anxiety
detection model. The experimental data for this work is obtained from a
year-long experiment on 41 healthy older adults (26 females and 15 males) in
the age range 60-80 with mean age 73.36+-5.25 during a Trier Social Stress Test
(TSST) protocol. The anxiety level ground truth was obtained from State-Trait
Anxiety Inventory (STAI), which is regarded as the gold standard to measure
perceived anxiety. EDA and Blood Volume Pulse (BVP) signals were recorded using
a wrist-worn EDA and PPG sensor respectively. 47 features were computed from
EDA and BVP signal, out of which a final set of 24 significantly correlated
features were selected for analysis. The phases of the experimental study are
encoded as unique integers to generate the context feature vector. A
combination of features from a single sensor with the context feature vector is
used for training a machine learning model to distinguish between anxious and
not-anxious states. Results and analysis showed that the EDA and BVP machine
learning models that combined the context feature along with the physiological
features achieved 3.37% and 6.41% higher accuracy respectively than the models
that used only physiological features. Further, end-to-end processing of EDA
and BVP signals was simulated for real-time anxiety level detection. This work
demonstrates the practicality of the proposed anxiety detection method in
facilitating long-term monitoring of anxiety in older adults using low-cost
consumer devices.",arxiv
http://arxiv.org/abs/1512.08512v2,2016-04-30T03:03:04Z,2015-12-28T20:56:50Z,Visually Indicated Sounds,"Objects make distinctive sounds when they are hit or scratched. These sounds
reveal aspects of an object's material properties, as well as the actions that
produced them. In this paper, we propose the task of predicting what sound an
object makes when struck as a way of studying physical interactions within a
visual scene. We present an algorithm that synthesizes sound from silent videos
of people hitting and scratching objects with a drumstick. This algorithm uses
a recurrent neural network to predict sound features from videos and then
produces a waveform from these features with an example-based synthesis
procedure. We show that the sounds predicted by our model are realistic enough
to fool participants in a ""real or fake"" psychophysical experiment, and that
they convey significant information about material properties and physical
interactions.",arxiv
http://arxiv.org/abs/2111.03821v1,2021-11-06T07:30:00Z,2021-11-06T07:30:00Z,ROFT: Real-Time Optical Flow-Aided 6D Object Pose and Velocity Tracking,"6D object pose tracking has been extensively studied in the robotics and
computer vision communities. The most promising solutions, leveraging on deep
neural networks and/or filtering and optimization, exhibit notable performance
on standard benchmarks. However, to our best knowledge, these have not been
tested thoroughly against fast object motions. Tracking performance in this
scenario degrades significantly, especially for methods that do not achieve
real-time performance and introduce non negligible delays. In this work, we
introduce ROFT, a Kalman filtering approach for 6D object pose and velocity
tracking from a stream of RGB-D images. By leveraging real-time optical flow,
ROFT synchronizes delayed outputs of low frame rate Convolutional Neural
Networks for instance segmentation and 6D object pose estimation with the RGB-D
input stream to achieve fast and precise 6D object pose and velocity tracking.
We test our method on a newly introduced photorealistic dataset, Fast-YCB,
which comprises fast moving objects from the YCB model set, and on the dataset
for object and hand pose estimation HO-3D. Results demonstrate that our
approach outperforms state-of-the-art methods for 6D object pose tracking,
while also providing 6D object velocity tracking. A video showing the
experiments is provided as supplementary material.",arxiv
http://arxiv.org/abs/2110.14636v1,2021-10-27T08:01:10Z,2021-10-27T08:01:10Z,"Emoji-aware Co-attention Network with EmoGraph2vec Model for Sentiment
  Anaylsis","In social media platforms, emojis have an extremely high occurrence in
computer-mediated communications. Many emojis are used to strengthen the
emotional expressions and the emojis that co-occurs in a sentence also have a
strong sentiment connection. However, when it comes to emoji representation
learning, most studies have only utilized the fixed descriptions provided by
the Unicode Consortium, without consideration of actual usage scenario. As for
the sentiment analysis task, many researchers ignore the emotional impact of
the interaction between text and emojis. It results that the emotional
semantics of emojis cannot be fully explored. In this work, we propose a method
to learn emoji representations called EmoGraph2vec and design an emoji-aware
co-attention network that learns the mutual emotional semantics between text
and emojis on short texts of social media. In EmoGraph2vec, we form an emoji
co-occurrence network on real social data and enrich the semantic information
based on an external knowledge base EmojiNet to obtain emoji node embeddings.
Our model designs a co-attention mechanism to incorporate the text and emojis,
and integrates a squeeze-and-excitation (SE) block into a convolutional neural
network as a classifier. Finally, we use the transfer learning method to
increase converge speed and achieve higher accuracy. Experimental results show
that the proposed model can outperform several baselines for sentiment analysis
on benchmark datasets. Additionally, we conduct a series of ablation and
comparison experiments to investigate the effectiveness of our model.",arxiv
http://arxiv.org/abs/1902.06673v1,2019-02-10T15:21:45Z,2019-02-10T15:21:45Z,Fake News Detection on Social Media using Geometric Deep Learning,"Social media are nowadays one of the main news sources for millions of people
around the globe due to their low cost, easy access and rapid dissemination.
This however comes at the cost of dubious trustworthiness and significant risk
of exposure to 'fake news', intentionally written to mislead the readers.
Automatically detecting fake news poses challenges that defy existing
content-based analysis approaches. One of the main reasons is that often the
interpretation of the news requires the knowledge of political or social
context or 'common sense', which current NLP algorithms are still missing.
Recent studies have shown that fake and real news spread differently on social
media, forming propagation patterns that could be harnessed for the automatic
fake news detection. Propagation-based approaches have multiple advantages
compared to their content-based counterparts, among which is language
independence and better resilience to adversarial attacks. In this paper we
show a novel automatic fake news detection model based on geometric deep
learning. The underlying core algorithms are a generalization of classical CNNs
to graphs, allowing the fusion of heterogeneous data such as content, user
profile and activity, social graph, and news propagation. Our model was trained
and tested on news stories, verified by professional fact-checking
organizations, that were spread on Twitter. Our experiments indicate that
social network structure and propagation are important features allowing highly
accurate (92.7% ROC AUC) fake news detection. Second, we observe that fake news
can be reliably detected at an early stage, after just a few hours of
propagation. Third, we test the aging of our model on training and testing data
separated in time. Our results point to the promise of propagation-based
approaches for fake news detection as an alternative or complementary strategy
to content-based approaches.",arxiv
http://arxiv.org/abs/2011.14618v1,2020-11-30T08:42:13Z,2020-11-30T08:42:13Z,"CovidExplorer: A Multi-faceted AI-based Search and Visualization Engine
  for COVID-19 Information","The entire world is engulfed in the fight against the COVID-19 pandemic,
leading to a significant surge in research experiments, government policies,
and social media discussions. A multi-modal information access and data
visualization platform can play a critical role in supporting research aimed at
understanding and developing preventive measures for the pandemic. In this
paper, we present a multi-faceted AI-based search and visualization engine,
CovidExplorer. Our system aims to help researchers understand current
state-of-the-art COVID-19 research, identify research articles relevant to
their domain, and visualize real-time trends and statistics of COVID-19 cases.
In contrast to other existing systems, CovidExplorer also brings in
India-specific topical discussions on social media to study different aspects
of COVID-19. The system, demo video, and the datasets are available at
http://covidexplorer.in.",arxiv
http://arxiv.org/abs/2006.06704v1,2020-06-11T18:04:10Z,2020-06-11T18:04:10Z,End-to-end Sinkhorn Autoencoder with Noise Generator,"In this work, we propose a novel end-to-end sinkhorn autoencoder with noise
generator for efficient data collection simulation. Simulating processes that
aim at collecting experimental data is crucial for multiple real-life
applications, including nuclear medicine, astronomy and high energy physics.
Contemporary methods, such as Monte Carlo algorithms, provide high-fidelity
results at a price of high computational cost. Multiple attempts are taken to
reduce this burden, e.g. using generative approaches based on Generative
Adversarial Networks or Variational Autoencoders. Although such methods are
much faster, they are often unstable in training and do not allow sampling from
an entire data distribution. To address these shortcomings, we introduce a
novel method dubbed end-to-end Sinkhorn Autoencoder, that leverages sinkhorn
algorithm to explicitly align distribution of encoded real data examples and
generated noise. More precisely, we extend autoencoder architecture by adding a
deterministic neural network trained to map noise from a known distribution
onto autoencoder latent space representing data distribution. We optimise the
entire model jointly. Our method outperforms competing approaches on a
challenging dataset of simulation data from Zero Degree Calorimeters of ALICE
experiment in LHC. as well as standard benchmarks, such as MNIST and CelebA.",arxiv
http://arxiv.org/abs/2007.14432v1,2020-07-28T18:47:21Z,2020-07-28T18:47:21Z,"A Convolutional Neural Network for gaze preference detection: A
  potential tool for diagnostics of autism spectrum disorder in children","Early diagnosis of autism spectrum disorder (ASD) is known to improve the
quality of life of affected individuals. However, diagnosis is often delayed
even in wealthier countries including the US, largely due to the fact that gold
standard diagnostic tools such as the Autism Diagnostic Observation Schedule
(ADOS) and the Autism Diagnostic Interview-Revised (ADI-R) are time consuming
and require expertise to administer. This trend is even more pronounced lower
resources settings due to a lack of trained experts. As a result, alternative,
less technical methods that leverage the unique ways in which children with ASD
react to visual stimulation in a controlled environment have been developed to
help facilitate early diagnosis. Previous studies have shown that, when exposed
to a video that presents both social and abstract scenes side by side, a child
with ASD will focus their attention towards the abstract images on the screen
to a greater extent than a child without ASD. Such differential responses make
it possible to implement an algorithm for the rapid diagnosis of ASD based on
eye tracking against different visual stimuli. Here we propose a convolutional
neural network (CNN) algorithm for gaze prediction using images extracted from
a one-minute stimulus video. Our model achieved a high accuracy rate and
robustness for prediction of gaze direction with independent persons and
employing a different camera than the one used during testing. In addition to
this, the proposed algorithm achieves a fast response time, providing a near
real-time evaluation of ASD. Thereby, by applying the proposed method, we could
significantly reduce the diagnosis time and facilitate the diagnosis of ASD in
low resource regions.",arxiv
http://arxiv.org/abs/1609.00086v1,2016-09-01T01:58:50Z,2016-09-01T01:58:50Z,"A novel online multi-label classifier for high-speed streaming data
  applications","In this paper, a high-speed online neural network classifier based on extreme
learning machines for multi-label classification is proposed. In multi-label
classification, each of the input data sample belongs to one or more than one
of the target labels. The traditional binary and multi-class classification
where each sample belongs to only one target class forms the subset of
multi-label classification. Multi-label classification problems are far more
complex than binary and multi-class classification problems, as both the number
of target labels and each of the target labels corresponding to each of the
input samples are to be identified. The proposed work exploits the high-speed
nature of the extreme learning machines to achieve real-time multi-label
classification of streaming data. A new threshold-based online sequential
learning algorithm is proposed for high speed and streaming data classification
of multi-label problems. The proposed method is experimented with six different
datasets from different application domains such as multimedia, text, and
biology. The hamming loss, accuracy, training time and testing time of the
proposed technique is compared with nine different state-of-the-art methods.
Experimental studies shows that the proposed technique outperforms the existing
multi-label classifiers in terms of performance and speed.",arxiv
http://arxiv.org/abs/2110.01767v2,2021-11-09T07:25:14Z,2021-10-05T00:56:51Z,Scalable Relational Query Processing on Big Matrix Data,"The use of large-scale machine learning methods is becoming ubiquitous in
many applications ranging from business intelligence to self-driving cars.
These methods require a complex computation pipeline consisting of various
types of operations, e.g., relational operations for pre-processing or
post-processing the dataset, and matrix operations for core model computations.
Many existing systems focus on efficiently processing matrix-only operations,
and assume that the inputs to the relational operators are already pre-computed
and are materialized as intermediate matrices. However, the input to a
relational operator may be complex in machine learning pipelines, and may
involve various combinations of matrix operators. Hence, it is critical to
realize scalable and efficient relational query processors that directly
operate on big matrix data. This paper presents new efficient and scalable
relational query processing techniques on big matrix data for in-memory
distributed clusters. The proposed techniques leverage algebraic transformation
rules to rewrite query execution plans into ones with lower computation costs.
A distributed query plan optimizer exploits the sparsity-inducing property of
merge functions as well as Bloom join strategies for efficiently evaluating
various flavors of the join operation. Furthermore, optimized partitioning
schemes for the input matrices are developed to facilitate the performance of
join operations based on a cost model that minimizes the communication
overhead.The proposed relational query processing techniques are prototyped in
Apache Spark. Experiments on both real and synthetic data demonstrate that the
proposed techniques achieve up to two orders of magnitude performance
improvement over state-of-the-art systems on a wide range of applications.",arxiv
http://arxiv.org/abs/2006.08893v1,2020-06-16T03:09:21Z,2020-06-16T03:09:21Z,An Intelligent Group Event Recommendation System in Social networks,"The importance of contexts has been widely recognized in recommender systems
for individuals. However, most existing group recommendation models in
Event-Based Social Networks (EBSNs) focus on how to aggregate group members'
preferences to form group preferences. In these models, the influence of
contexts on groups is considered but simply defined in a manual way, which
cannot model the complex and deep interactions between contexts and groups. In
this paper, we propose an Attention-based Context-aware Group Event
Recommendation model (ACGER) in EBSNs. ACGER models the deep, non-linear
influence of contexts on users, groups, and events through multi-layer neural
networks. Especially, a novel attention mechanism is designed to enable the
influence weights of contexts on users/groups change dynamically with the
events concerned. Considering that groups may have completely different
behavior patterns from group members, we propose that the preference of a group
need to be obtained from indirect and direct perspectives (called indirect
preference and direct preference respectively). In order to obtain the indirect
preference, we propose a method of aggregating preferences based on attention
mechanism. Compared with existing predefined strategies, this method can
flexibly adapt the strategy according to the events concerned by the group. In
order to obtain the direct preference, we employ neural networks to directly
learn it from group-event interactions. Furthermore, to make full use of rich
user-event interactions in EBSNs, we integrate the context-aware individual
recommendation task into ACGER, which enhances the accuracy of learning of user
embeddings and event embeddings. Extensive experiments on two real datasets
from Meetup show that our model ACGER significantly outperforms the
state-of-the-art models.",arxiv
http://arxiv.org/abs/1704.05973v1,2017-04-20T01:22:57Z,2017-04-20T01:22:57Z,"Call Attention to Rumors: Deep Attention Based Recurrent Neural Networks
  for Early Rumor Detection","The proliferation of social media in communication and information
dissemination has made it an ideal platform for spreading rumors. Automatically
debunking rumors at their stage of diffusion is known as \textit{early rumor
detection}, which refers to dealing with sequential posts regarding disputed
factual claims with certain variations and highly textual duplication over
time. Thus, identifying trending rumors demands an efficient yet flexible model
that is able to capture long-range dependencies among postings and produce
distinct representations for the accurate early detection. However, it is a
challenging task to apply conventional classification algorithms to rumor
detection in earliness since they rely on hand-crafted features which require
intensive manual efforts in the case of large amount of posts. This paper
presents a deep attention model on the basis of recurrent neural networks (RNN)
to learn \textit{selectively} temporal hidden representations of sequential
posts for identifying rumors. The proposed model delves soft-attention into the
recurrence to simultaneously pool out distinct features with particular focus
and produce hidden representations that capture contextual variations of
relevant posts over time. Extensive experiments on real datasets collected from
social media websites demonstrate that (1) the deep attention based RNN model
outperforms state-of-the-arts that rely on hand-crafted features; (2) the
introduction of soft attention mechanism can effectively distill relevant parts
to rumors from original posts in advance; (3) the proposed method detects
rumors more quickly and accurately than competitors.",arxiv
http://arxiv.org/abs/2002.12683v2,2020-03-02T10:47:53Z,2020-02-28T12:44:34Z,"RP-DNN: A Tweet level propagation context based deep neural networks for
  early rumor detection in Social Media","Early rumor detection (ERD) on social media platform is very challenging when
limited, incomplete and noisy information is available. Most of the existing
methods have largely worked on event-level detection that requires the
collection of posts relevant to a specific event and relied only on
user-generated content. They are not appropriate to detect rumor sources in the
very early stages, before an event unfolds and becomes widespread. In this
paper, we address the task of ERD at the message level. We present a novel
hybrid neural network architecture, which combines a task-specific
character-based bidirectional language model and stacked Long Short-Term Memory
(LSTM) networks to represent textual contents and social-temporal contexts of
input source tweets, for modelling propagation patterns of rumors in the early
stages of their development. We apply multi-layered attention models to jointly
learn attentive context embeddings over multiple context inputs. Our
experiments employ a stringent leave-one-out cross-validation (LOO-CV)
evaluation setup on seven publicly available real-life rumor event data sets.
Our models achieve state-of-the-art(SoA) performance for detecting unseen
rumors on large augmented data which covers more than 12 events and 2,967
rumors. An ablation study is conducted to understand the relative contribution
of each component of our proposed model.",arxiv
http://arxiv.org/abs/2012.03680v1,2020-11-12T09:31:09Z,2020-11-12T09:31:09Z,UNOC: Understanding Occlusion for Embodied Presence in Virtual Reality,"Tracking body and hand motions in the 3D space is essential for social and
self-presence in augmented and virtual environments. Unlike the popular 3D pose
estimation setting, the problem is often formulated as inside-out tracking
based on embodied perception (e.g., egocentric cameras, handheld sensors). In
this paper, we propose a new data-driven framework for inside-out body
tracking, targeting challenges of omnipresent occlusions in optimization-based
methods (e.g., inverse kinematics solvers). We first collect a large-scale
motion capture dataset with both body and finger motions using optical markers
and inertial sensors. This dataset focuses on social scenarios and captures
ground truth poses under self-occlusions and body-hand interactions. We then
simulate the occlusion patterns in head-mounted camera views on the captured
ground truth using a ray casting algorithm and learn a deep neural network to
infer the occluded body parts. In the experiments, we show that our method is
able to generate high-fidelity embodied poses by applying the proposed method
on the task of real-time inside-out body tracking, finger motion synthesis, and
3-point inverse kinematics.",arxiv
http://arxiv.org/abs/2003.06705v1,2020-03-14T21:11:02Z,2020-03-14T21:11:02Z,Identifying Individual Dogs in Social Media Images,"We present the results of an initial study focused on developing a visual AI
solution able to recognize individual dogs in unconstrained (wild) images
occurring on social media.
  The work described here is part of joint project done with Pet2Net, a social
network focused on pets and their owners. In order to detect and recognize
individual dogs we combine transfer learning and object detection approaches on
Inception v3 and SSD Inception v2 architectures respectively and evaluate the
proposed pipeline using a new data set containing real data that the users
uploaded to Pet2Net platform. We show that it can achieve 94.59% accuracy in
identifying individual dogs. Our approach has been designed with simplicity in
mind and the goal of easy deployment on all the images uploaded to Pet2Net
platform.
  A purely visual approach to identifying dogs in images, will enhance Pet2Net
features aimed at finding lost dogs, as well as form the basis of future work
focused on identifying social relationships between dogs, which cannot be
inferred from other data collected by the platform.",arxiv
http://arxiv.org/abs/2004.04565v3,2020-05-23T15:16:31Z,2020-04-09T14:37:43Z,CovidSens: A Vision on Reliable Social Sensing for COVID-19,"With the spiraling pandemic of the Coronavirus Disease 2019 (COVID-19), it
has becoming inherently important to disseminate accurate and timely
information about the disease. Due to the ubiquity of Internet connectivity and
smart devices, social sensing is emerging as a dynamic AI-driven sensing
paradigm to extract real-time observations from online users. In this paper, we
propose CovidSens, a vision of social sensing based risk alert systems to
spontaneously obtain and analyze social data to infer COVID-19 propagation.
CovidSens can actively help to keep the general public informed about the
COVID-19 spread and identify risk-prone areas. The CovidSens concept is
motivated by three observations: 1) people actively share their experience of
COVID-19 via online social media, 2) official warning channels and news
agencies are relatively slower than people reporting on social media, and 3)
online users are frequently equipped with powerful mobile devices that can
perform data processing and analytics. We envision unprecedented opportunities
to leverage posts generated by ordinary people to build real-time sensing and
analytic system for gathering and circulating COVID-19 propagation data.
Specifically, the vision of CovidSens attempts to answer the questions: How to
distill reliable information on COVID-19 with prevailing rumors and
misinformation? How to inform the general public about the state of the spread
timely and effectively? How to leverage the computational power on edge devices
to construct fully integrated edge-based social sensing platforms? In this
vision paper, we discuss the roles of CovidSens and identify potential
challenges in developing reliable social sensing based risk alert systems. We
envision that approaches originating from multiple disciplines can be effective
in addressing the challenges. Finally, we outline a few research directions for
future work in CovidSens.",arxiv
http://arxiv.org/abs/1105.5449v1,2011-05-27T01:48:39Z,2011-05-27T01:48:39Z,AntNet: Distributed Stigmergetic Control for Communications Networks,"This paper introduces AntNet, a novel approach to the adaptive learning of
routing tables in communications networks. AntNet is a distributed, mobile
agents based Monte Carlo system that was inspired by recent work on the ant
colony metaphor for solving optimization problems. AntNet's agents concurrently
explore the network and exchange collected information. The communication among
the agents is indirect and asynchronous, mediated by the network itself. This
form of communication is typical of social insects and is called stigmergy. We
compare our algorithm with six state-of-the-art routing algorithms coming from
the telecommunications and machine learning fields. The algorithms' performance
is evaluated over a set of realistic testbeds. We run many experiments over
real and artificial IP datagram networks with increasing number of nodes and
under several paradigmatic spatial and temporal traffic distributions. Results
are very encouraging. AntNet showed superior performance under all the
experimental conditions with respect to its competitors. We analyze the main
characteristics of the algorithm and try to explain the reasons for its
superiority.",arxiv
http://arxiv.org/abs/2006.12334v3,2021-03-09T23:10:27Z,2020-06-22T15:26:30Z,Graph Learning for Inverse Landscape Genetics,"The problem of inferring unknown graph edges from numerical data at a graph's
nodes appears in many forms across machine learning. We study a version of this
problem that arises in the field of \emph{landscape genetics}, where genetic
similarity between organisms living in a heterogeneous landscape is explained
by a weighted graph that encodes the ease of dispersal through that landscape.
Our main contribution is an efficient algorithm for \emph{inverse landscape
genetics}, which is the task of inferring this graph from measurements of
genetic similarity at different locations (graph nodes). Inverse landscape
genetics is important in discovering impediments to species dispersal that
threaten biodiversity and long-term species survival. In particular, it is
widely used to study the effects of climate change and human development.
Drawing on influential work that models organism dispersal using graph
\emph{effective resistances} (McRae 2006), we reduce the inverse landscape
genetics problem to that of inferring graph edges from noisy measurements of
these resistances, which can be obtained from genetic similarity data. Building
on the NeurIPS 2018 work of Hoskins et al. 2018 on learning edges in social
networks, we develop an efficient first-order optimization method for solving
this problem. Despite its non-convex nature, experiments on synthetic and real
genetic data establish that our method provides fast and reliable convergence,
significantly outperforming existing heuristics used in the field. By providing
researchers with a powerful, general purpose algorithmic tool, we hope our work
will have a positive impact on accelerating work on landscape genetics.",arxiv
http://arxiv.org/abs/2008.07364v1,2020-08-07T22:01:50Z,2020-08-07T22:01:50Z,"Predicting Individual Treatment Effects of Large-scale Team Competitions
  in a Ride-sharing Economy","Millions of drivers worldwide have enjoyed financial benefits and work
schedule flexibility through a ride-sharing economy, but meanwhile they have
suffered from the lack of a sense of identity and career achievement. Equipped
with social identity and contest theories, financially incentivized team
competitions have been an effective instrument to increase drivers'
productivity, job satisfaction, and retention, and to improve revenue over cost
for ride-sharing platforms. While these competitions are overall effective, the
decisive factors behind the treatment effects and how they affect the outcomes
of individual drivers have been largely mysterious. In this study, we analyze
data collected from more than 500 large-scale team competitions organized by a
leading ride-sharing platform, building machine learning models to predict
individual treatment effects. Through a careful investigation of features and
predictors, we are able to reduce out-sample prediction error by more than 24%.
Through interpreting the best-performing models, we discover many novel and
actionable insights regarding how to optimize the design and the execution of
team competitions on ride-sharing platforms. A simulated analysis demonstrates
that by simply changing a few contest design options, the average treatment
effect of a real competition is expected to increase by as much as 26%. Our
procedure and findings shed light on how to analyze and optimize large-scale
online field experiments in general.",arxiv
http://arxiv.org/abs/2105.02725v1,2021-05-06T14:45:34Z,2021-05-06T14:45:34Z,CrossWalk: Fairness-enhanced Node Representation Learning,"The potential for machine learning systems to amplify social inequities and
unfairness is receiving increasing popular and academic attention. Much recent
work has focused on developing algorithmic tools to assess and mitigate such
unfairness. However, there is little work on enhancing fairness in graph
algorithms. Here, we develop a simple, effective and general method, CrossWalk,
that enhances fairness of various graph algorithms, including influence
maximization, link prediction and node classification, applied to node
embeddings. CrossWalk is applicable to any random walk based node
representation learning algorithm, such as DeepWalk and Node2Vec. The key idea
is to bias random walks to cross group boundaries, by upweighting edges which
(1) are closer to the groups' peripheries or (2) connect different groups in
the network. CrossWalk pulls nodes that are near groups' peripheries towards
their neighbors from other groups in the embedding space, while preserving the
necessary structural information from the graph. Extensive experiments show the
effectiveness of our algorithm to enhance fairness in various graph algorithms,
including influence maximization, link prediction and node classification in
synthetic and real networks, with only a very small decrease in performance.",arxiv
http://arxiv.org/abs/1605.05422v2,2016-05-24T06:38:18Z,2016-05-18T02:46:14Z,Optimization Beyond Prediction: Prescriptive Price Optimization,"This paper addresses a novel data science problem, prescriptive price
optimization, which derives the optimal price strategy to maximize future
profit/revenue on the basis of massive predictive formulas produced by machine
learning. The prescriptive price optimization first builds sales forecast
formulas of multiple products, on the basis of historical data, which reveal
complex relationships between sales and prices, such as price elasticity of
demand and cannibalization. Then, it constructs a mathematical optimization
problem on the basis of those predictive formulas. We present that the
optimization problem can be formulated as an instance of binary quadratic
programming (BQP). Although BQP problems are NP-hard in general and
computationally intractable, we propose a fast approximation algorithm using a
semi-definite programming (SDP) relaxation, which is closely related to the
Goemans-Williamson's Max-Cut approximation. Our experiments on simulation and
real retail datasets show that our prescriptive price optimization
simultaneously derives the optimal prices of tens/hundreds products with
practical computational time, that potentially improve 8.2% of gross profit of
those products.",arxiv
http://arxiv.org/abs/1705.06499v2,2018-05-15T02:32:51Z,2017-05-18T09:55:17Z,"A Non-monotone Alternating Updating Method for A Class of Matrix
  Factorization Problems","In this paper we consider a general matrix factorization model which covers a
large class of existing models with many applications in areas such as machine
learning and imaging sciences. To solve this possibly nonconvex, nonsmooth and
non-Lipschitz problem, we develop a non-monotone alternating updating method
based on a potential function. Our method essentially updates two blocks of
variables in turn by inexactly minimizing this potential function, and updates
another auxiliary block of variables using an explicit formula. The special
structure of our potential function allows us to take advantage of efficient
computational strategies for non-negative matrix factorization to perform the
alternating minimization over the two blocks of variables. A suitable line
search criterion is also incorporated to improve the numerical performance.
Under some mild conditions, we show that the line search criterion is well
defined, and establish that the sequence generated is bounded and any cluster
point of the sequence is a stationary point. Finally, we conduct some numerical
experiments using real datasets to compare our method with some existing
efficient methods for non-negative matrix factorization and matrix completion.
The numerical results show that our method can outperform these methods for
these specific applications.",arxiv
http://arxiv.org/abs/2010.08356v2,2021-02-17T19:35:59Z,2020-10-16T12:48:30Z,Optimizing persistent homology based functions,"Solving optimization tasks based on functions and losses with a topological
flavor is a very active, growing field of research in data science and
Topological Data Analysis, with applications in non-convex optimization,
statistics and machine learning. However, the approaches proposed in the
literature are usually anchored to a specific application and/or topological
construction, and do not come with theoretical guarantees. To address this
issue, we study the differentiability of a general map associated with the most
common topological construction, that is, the persistence map. Building on real
analytic geometry arguments, we propose a general framework that allows us to
define and compute gradients for persistence-based functions in a very simple
way. We also provide a simple, explicit and sufficient condition for
convergence of stochastic subgradient methods for such functions. This result
encompasses all the constructions and applications of topological optimization
in the literature. Finally, we provide associated code, that is easy to handle
and to mix with other non-topological methods and constraints, as well as some
experiments showcasing the versatility of our approach.",arxiv
http://arxiv.org/abs/2003.01904v2,2020-07-28T07:01:11Z,2020-03-04T05:47:11Z,"Multidimensional Analysis of Excitonic Spectra of Monolayers of Tungsten
  Disulphide: Towards Computer Aided Identification of Structural and
  Environmental Perturbations of 2D Materials","Despite 2D materials holding great promise for a broad range of applications,
the proliferation of devices and their fulfillment of real-life demands are
still far from being realized. Experimentally obtainable samples commonly
experience a wide range of perturbations (ripples and wrinkles, point and line
defects, grain boundaries, strain field, doping, water intercalation,
oxidation, edge reconstructions) significantly deviating the properties from
idealistic models. These perturbations, in general, can be entangled or occur
in groups with each group forming a complex perturbation making the
interpretations of observable physical properties and the disentanglement of
simultaneously acting effects a highly non-trivial task even for an experienced
researcher. Here we generalise statistical correlation analysis of excitonic
spectra of monolayer WS2, acquired by hyperspectral absorption and
photoluminescence imaging, to a multidimensional case, and examine
multidimensional correlations via unsupervised machine learning algorithms.
Using principle component analysis we are able to identify 4 dominant
components that are correlated with tensile strain, disorder induced by
adsorption or intercalation of environmental molecules, multi-layer regions and
charge doping, respectively. This approach has the potential to determine the
local environment of WS2 monolayers or other 2D materials from simple optical
measurements, and paves the way towards advanced, machine-aided,
characterisation of monolayer matter.",arxiv
http://arxiv.org/abs/1906.03423v1,2019-06-08T08:56:02Z,2019-06-08T08:56:02Z,News Labeling as Early as Possible: Real or Fake?,"Making disguise between real and fake news propagation through online social
networks is an important issue in many applications. The time gap between the
news release time and detection of its label is a significant step towards
broadcasting the real information and avoiding the fake. Therefore, one of the
challenging tasks in this area is to identify fake and real news in early
stages of propagation. However, there is a trade-off between minimizing the
time gap and maximizing accuracy. Despite recent efforts in detection of fake
news, there has been no significant work that explicitly incorporates early
detection in its model. In this paper, we focus on accurate early labeling of
news, and propose a model by considering earliness both in modeling and
prediction. The proposed method utilizes recurrent neural networks with a novel
loss function, and a new stopping rule. Given the context of news, we first
embed it with a class-specific text representation. Then, we utilize the
available public profile of users, and speed of news diffusion, for early
labeling of the news. Experiments on real datasets demonstrate the
effectiveness of our model both in terms of early labelling and accuracy,
compared to the state of the art baseline and models.",arxiv
http://arxiv.org/abs/1801.03604v1,2018-01-11T01:23:50Z,2018-01-11T01:23:50Z,Conversational AI: The Science Behind the Alexa Prize,"Conversational agents are exploding in popularity. However, much work remains
in the area of social conversation as well as free-form conversation over a
broad range of domains and topics. To advance the state of the art in
conversational AI, Amazon launched the Alexa Prize, a 2.5-million-dollar
university competition where sixteen selected university teams were challenged
to build conversational agents, known as socialbots, to converse coherently and
engagingly with humans on popular topics such as Sports, Politics,
Entertainment, Fashion and Technology for 20 minutes. The Alexa Prize offers
the academic community a unique opportunity to perform research with a live
system used by millions of users. The competition provided university teams
with real user conversational data at scale, along with the user-provided
ratings and feedback augmented with annotations by the Alexa team. This enabled
teams to effectively iterate and make improvements throughout the competition
while being evaluated in real-time through live user interactions. To build
their socialbots, university teams combined state-of-the-art techniques with
novel strategies in the areas of Natural Language Understanding, Context
Modeling, Dialog Management, Response Generation, and Knowledge Acquisition. To
support the efforts of participating teams, the Alexa Prize team made
significant scientific and engineering investments to build and improve
Conversational Speech Recognition, Topic Tracking, Dialog Evaluation, Voice
User Experience, and tools for traffic management and scalability. This paper
outlines the advances created by the university teams as well as the Alexa
Prize team to achieve the common goal of solving the problem of Conversational
AI.",arxiv
http://arxiv.org/abs/2104.04184v2,2021-07-19T12:56:08Z,2021-04-09T04:30:04Z,"Robust Training of Social Media Image Classification Models for Rapid
  Disaster Response","Images shared on social media help crisis managers gain situational awareness
and assess incurred damages, among other response tasks. As the volume and
velocity of such content are typically high, real-time image classification has
become an urgent need for a faster disaster response. Recent advances in
computer vision and deep neural networks have enabled the development of models
for real-time image classification for a number of tasks, including detecting
crisis incidents, filtering irrelevant images, classifying images into specific
humanitarian categories, and assessing the severity of the damage. To develop
robust real-time models, it is necessary to understand the capability of the
publicly available pre-trained models for these tasks, which remains to be
under-explored in the crisis informatics literature. In this study, we address
such limitations by investigating ten different network architectures for four
different tasks using the largest publicly available datasets for these tasks.
We also explore various data augmentation strategies, semi-supervised
techniques, and a multitask learning setup. In our extensive experiments, we
achieve promising results.",arxiv
http://arxiv.org/abs/2106.07114v1,2021-06-14T00:12:27Z,2021-06-14T00:12:27Z,"Intelligent Agent for Hurricane Emergency Identification and Text
  Information Extraction from Streaming Social Media Big Data","This paper presents our research on leveraging social media Big Data and AI
to support hurricane disaster emergency response. The current practice of
hurricane emergency response for rescue highly relies on emergency call
centres. The more recent Hurricane Harvey event reveals the limitations of the
current systems. We use Hurricane Harvey and the associated Houston flooding as
the motivating scenario to conduct research and develop a prototype as a
proof-of-concept of using an intelligent agent as a complementary role to
support emergency centres in hurricane emergency response. This intelligent
agent is used to collect real-time streaming tweets during a natural disaster
event, to identify tweets requesting rescue, to extract key information such as
address and associated geocode, and to visualize the extracted information in
an interactive map in decision supports. Our experiment shows promising
outcomes and the potential application of the research in support of hurricane
emergency response.",arxiv
http://arxiv.org/abs/2111.07090v1,2021-11-13T10:56:58Z,2021-11-13T10:56:58Z,"D^2LV: A Data-Driven and Local-Verification Approach for Image Copy
  Detection","Image copy detection is of great importance in real-life social media. In
this paper, a data-driven and local-verification (D^2LV) approach is proposed
to compete for Image Similarity Challenge: Matching Track at NeurIPS'21. In
D^2LV, unsupervised pre-training substitutes the commonly-used supervised one.
When training, we design a set of basic and six advanced transformations, and a
simple but effective baseline learns robust representation. During testing, a
global-local and local-global matching strategy is proposed. The strategy
performs local-verification between reference and query images. Experiments
demonstrate that the proposed method is effective. The proposed approach ranks
first out of 1,103 participants on the Facebook AI Image Similarity Challenge:
Matching Track. The code and trained models are available at
https://github.com/WangWenhao0716/ISC-Track1-Submission.",arxiv
http://arxiv.org/abs/1811.09675v1,2018-11-21T01:08:51Z,2018-11-21T01:08:51Z,"CNN based dense underwater 3D scene reconstruction by transfer learning
  using bubble database","Dense 3D shape acquisition of swimming human or live fish is an important
research topic for sports, biological science and so on. For this purpose,
active stereo sensor is usually used in the air, however it cannot be applied
to the underwater environment because of refraction, strong light attenuation
and severe interference of bubbles. Passive stereo is a simple solution for
capturing dynamic scenes at underwater environment, however the shape with
textureless surfaces or irregular reflections cannot be recovered. Recently,
the stereo camera pair with a pattern projector for adding artificial textures
on the objects is proposed. However, to use the system for underwater
environment, several problems should be compensated, i.e., disturbance by
fluctuation and bubbles. Simple solution is to use convolutional neural network
for stereo to cancel the effects of bubbles and/or water fluctuation. Since it
is not easy to train CNN with small size of database with large variation, we
develop a special bubble generation device to efficiently create real bubble
database of multiple size and density. In addition, we propose a transfer
learning technique for multi-scale CNN to effectively remove bubbles and
projected-patterns on the object. Further, we develop a real system and
actually captured live swimming human, which has not been done before.
Experiments are conducted to show the effectiveness of our method compared with
the state of the art techniques.",arxiv
http://arxiv.org/abs/2102.12040v2,2021-07-27T17:21:01Z,2021-02-24T03:10:32Z,"Active Learning to Classify Macromolecular Structures in situ for Less
  Supervision in Cryo-Electron Tomography","Motivation: Cryo-Electron Tomography (cryo-ET) is a 3D bioimaging tool that
visualizes the structural and spatial organization of macromolecules at a
near-native state in single cells, which has broad applications in life
science. However, the systematic structural recognition and recovery of
macromolecules captured by cryo-ET are difficult due to high structural
complexity and imaging limits. Deep learning based subtomogram classification
have played critical roles for such tasks. As supervised approaches, however,
their performance relies on sufficient and laborious annotation on a large
training dataset.
  Results: To alleviate this major labeling burden, we proposed a Hybrid Active
Learning (HAL) framework for querying subtomograms for labelling from a large
unlabeled subtomogram pool. Firstly, HAL adopts uncertainty sampling to select
the subtomograms that have the most uncertain predictions. Moreover, to
mitigate the sampling bias caused by such strategy, a discriminator is
introduced to judge if a certain subtomogram is labeled or unlabeled and
subsequently the model queries the subtomogram that have higher probabilities
to be unlabeled. Additionally, HAL introduces a subset sampling strategy to
improve the diversity of the query set, so that the information overlap is
decreased between the queried batches and the algorithmic efficiency is
improved. Our experiments on subtomogram classification tasks using both
simulated and real data demonstrate that we can achieve comparable testing
performance (on average only 3% accuracy drop) by using less than 30% of the
labeled subtomograms, which shows a very promising result for subtomogram
classification task with limited labeling resources.",arxiv
http://arxiv.org/abs/1802.05130v1,2018-02-14T14:53:06Z,2018-02-14T14:53:06Z,"Multi-Task Learning for Extraction of Adverse Drug Reaction Mentions
  from Tweets","Adverse drug reactions (ADRs) are one of the leading causes of mortality in
health care. Current ADR surveillance systems are often associated with a
substantial time lag before such events are officially published. On the other
hand, online social media such as Twitter contain information about ADR events
in real-time, much before any official reporting. Current state-of-the-art in
ADR mention extraction uses Recurrent Neural Networks (RNN), which typically
need large labeled corpora. Towards this end, we propose a multi-task learning
based method which can utilize a similar auxiliary task (adverse drug event
detection) to enhance the performance of the main task, i.e., ADR extraction.
Furthermore, in the absence of auxiliary task dataset, we propose a novel joint
multi-task learning method to automatically generate weak supervision dataset
for the auxiliary task when a large pool of unlabeled tweets is available.
Experiments with 0.48M tweets show that the proposed approach outperforms the
state-of-the-art methods for the ADR mention extraction task by 7.2% in terms
of F1 score.",arxiv
http://arxiv.org/abs/1805.03900v1,2018-05-10T09:22:56Z,2018-05-10T09:22:56Z,Improv Chat: Second Response Generation for Chatbot,"Existing research on response generation for chatbot focuses on \textbf{First
Response Generation} which aims to teach the chatbot to say the first response
(e.g. a sentence) appropriate to the conversation context (e.g. the user's
query). In this paper, we introduce a new task \textbf{Second Response
Generation}, termed as Improv chat, which aims to teach the chatbot to say the
second response after saying the first response with respect the conversation
context, so as to lighten the burden on the user to keep the conversation
going. Specifically, we propose a general learning based framework and develop
a retrieval based system which can generate the second responses with the
users' query and the chatbot's first response as input. We present the approach
to building the conversation corpus for Improv chat from public forums and
social networks, as well as the neural networks based models for response
matching and ranking. We include the preliminary experiments and results in
this paper. This work could be further advanced with better deep matching
models for retrieval base systems or generative models for generation based
systems as well as extensive evaluations in real-life applications.",arxiv
http://arxiv.org/abs/2106.11865v1,2021-06-22T15:32:50Z,2021-06-22T15:32:50Z,"NetFense: Adversarial Defenses against Privacy Attacks on Neural
  Networks for Graph Data","Recent advances in protecting node privacy on graph data and attacking graph
neural networks (GNNs) gain much attention. The eye does not bring these two
essential tasks together yet. Imagine an adversary can utilize the powerful
GNNs to infer users' private labels in a social network. How can we
adversarially defend against such privacy attacks while maintaining the utility
of perturbed graphs? In this work, we propose a novel research task,
adversarial defenses against GNN-based privacy attacks, and present a graph
perturbation-based approach, NetFense, to achieve the goal. NetFense can
simultaneously keep graph data unnoticeability (i.e., having limited changes on
the graph structure), maintain the prediction confidence of targeted label
classification (i.e., preserving data utility), and reduce the prediction
confidence of private label classification (i.e., protecting the privacy of
nodes). Experiments conducted on single- and multiple-target perturbations
using three real graph data exhibit that the perturbed graphs by NetFense can
effectively maintain data utility (i.e., model unnoticeability) on targeted
label classification and significantly decrease the prediction confidence of
private label classification (i.e., privacy protection). Extensive studies also
bring several insights, such as the flexibility of NetFense, preserving local
neighborhoods in data unnoticeability, and better privacy protection for
high-degree nodes.",arxiv
http://arxiv.org/abs/2110.13596v1,2021-10-26T11:53:43Z,2021-10-26T11:53:43Z,"TME-BNA: Temporal Motif-Preserving Network Embedding with Bicomponent
  Neighbor Aggregation","Evolving temporal networks serve as the abstractions of many real-life
dynamic systems, e.g., social network and e-commerce. The purpose of temporal
network embedding is to map each node to a time-evolving low-dimension vector
for downstream tasks, e.g., link prediction and node classification. The
difficulty of temporal network embedding lies in how to utilize the topology
and time information jointly to capture the evolution of a temporal network. In
response to this challenge, we propose a temporal motif-preserving network
embedding method with bicomponent neighbor aggregation, named TME-BNA.
Considering that temporal motifs are essential to the understanding of topology
laws and functional properties of a temporal network, TME-BNA constructs
additional edge features based on temporal motifs to explicitly utilize complex
topology with time information. In order to capture the topology dynamics of
nodes, TME-BNA utilizes Graph Neural Networks (GNNs) to aggregate the
historical and current neighbors respectively according to the timestamps of
connected edges. Experiments are conducted on three public temporal network
datasets, and the results show the effectiveness of TME-BNA.",arxiv
http://arxiv.org/abs/1706.02586v3,2018-08-29T19:51:04Z,2017-06-08T13:52:23Z,"DSOS and SDSOS Optimization: More Tractable Alternatives to Sum of
  Squares and Semidefinite Optimization","In recent years, optimization theory has been greatly impacted by the advent
of sum of squares (SOS) optimization. The reliance of this technique on
large-scale semidefinite programs however, has limited the scale of problems to
which it can be applied. In this paper, we introduce DSOS and SDSOS
optimization as linear programming and second-order cone programming-based
alternatives to sum of squares optimization that allow one to trade off
computation time with solution quality. These are optimization problems over
certain subsets of sum of squares polynomials (or equivalently subsets of
positive semidefinite matrices), which can be of interest in general
applications of semidefinite programming where scalability is a limitation. We
show that some basic theorems from SOS optimization which rely on results from
real algebraic geometry are still valid for DSOS and SDSOS optimization.
Furthermore, we show with numerical experiments from diverse application
areas---polynomial optimization, statistics and machine learning, derivative
pricing, and control theory---that with reasonable tradeoffs in accuracy, we
can handle problems at scales that are currently significantly beyond the reach
of traditional sum of squares approaches. Finally, we provide a review of
recent techniques that bridge the gap between our DSOS/SDSOS approach and the
SOS approach at the expense of additional running time. The Supplementary
Material of the paper introduces an accompanying MATLAB package for DSOS and
SDSOS optimization.",arxiv
http://arxiv.org/abs/2009.02647v1,2020-09-06T05:27:50Z,2020-09-06T05:27:50Z,"Utilizing Citation Network Structure to Predict Citation Counts: A Deep
  Learning Approach","With the advancement of science and technology, the number of academic papers
published in the world each year has increased almost exponentially. While a
large number of research papers highlight the prosperity of science and
technology, they also give rise to some problems. As we all know, academic
papers are the most intuitive embodiment of the research results of scholars,
which can reflect the level of researchers. It is also the evaluation standard
for decision-making such as promotion and allocation of funds. Therefore, how
to measure the quality of an academic paper is very important. The most common
standard for measuring academic papers is the number of citation counts of
papers, because this indicator is widely used in the evaluation of scientific
publications, and it also serves as the basis for many other indicators (such
as the h-index). Therefore, it is very important to be able to accurately
predict the citation counts of academic papers.
  This paper proposes an end-to-end deep learning network, DeepCCP, which
combines the effect of information cascade and looks at the citation counts
prediction problem from the perspective of information cascade prediction.
DeepCCP directly uses the citation network formed in the early stage of the
paper as the input, and the output is the citation counts of the corresponding
paper after a period of time. DeepCCP only uses the structure and temporal
information of the citation network, and does not require other additional
information, but it can still achieve outstanding performance. According to
experiments on 6 real data sets, DeepCCP is superior to the state-of-the-art
methods in terms of the accuracy of citation count prediction.",arxiv
http://arxiv.org/abs/2011.01223v2,2021-04-16T01:23:23Z,2020-11-01T06:46:01Z,Comprehensible Counterfactual Explanation on Kolmogorov-Smirnov Test,"The Kolmogorov-Smirnov (KS) test is popularly used in many applications, such
as anomaly detection, astronomy, database security and AI systems. One
challenge remained untouched is how we can obtain an explanation on why a test
set fails the KS test. In this paper, we tackle the problem of producing
counterfactual explanations for test data failing the KS test. Concept-wise, we
propose the notion of most comprehensible counterfactual explanations, which
accommodates both the KS test data and the user domain knowledge in producing
explanations. Computation-wise, we develop an efficient algorithm MOCHE (for
MOst CompreHensible Explanation) that avoids enumerating and checking an
exponential number of subsets of the test set failing the KS test. MOCHE not
only guarantees to produce the most comprehensible counterfactual explanations,
but also is orders of magnitudes faster than the baselines. Experiment-wise, we
present a systematic empirical study on a series of benchmark real datasets to
verify the effectiveness, efficiency and scalability of most comprehensible
counterfactual explanations and MOCHE.",arxiv
http://arxiv.org/abs/1811.00821v5,2020-01-23T09:13:20Z,2018-11-02T11:12:49Z,OrthoNet: Multilayer Network Data Clustering,"Network data appears in very diverse applications, like biological, social,
or sensor networks. Clustering of network nodes into categories or communities
has thus become a very common task in machine learning and data mining. Network
data comes with some information about the network edges. In some cases, this
network information can even be given with multiple views or multiple layers,
each one representing a different type of relationship between the network
nodes. Increasingly often, network nodes also carry a feature vector. We
propose in this paper to extend the node clustering problem, that commonly
considers only the network information, to a problem where both the network
information and the node features are considered together for learning a
clustering-friendly representation of the feature space. Specifically, we
design a generic two-step algorithm for multilayer network data clustering. The
first step aggregates the different layers of network information into a graph
representation given by the geometric mean of the network Laplacian matrices.
The second step uses a neural net to learn a feature embedding that is
consistent with the structure given by the network layers. We propose a novel
algorithm for efficiently training the neural net via stochastic gradient
descent, which encourages the neural net outputs to span the leading
eigenvectors of the aggregated Laplacian matrix, in order to capture the
pairwise interactions on the network, and provide a clustering-friendly
representation of the feature space. We demonstrate with an extensive set of
experiments on synthetic and real datasets that our method leads to a
significant improvement w.r.t. state-of-the-art multilayer graph clustering
algorithms, as it judiciously combines nodes features and network information
in the node embedding algorithms.",arxiv
http://arxiv.org/abs/1909.04840v2,2020-01-29T15:43:44Z,2019-09-11T03:28:55Z,A Deep Learning Approach to Grasping the Invisible,"We study an emerging problem named ""grasping the invisible"" in robotic
manipulation, in which a robot is tasked to grasp an initially invisible target
object via a sequence of pushing and grasping actions. In this problem, pushes
are needed to search for the target and rearrange cluttered objects around it
to enable effective grasps. We propose to solve the problem by formulating a
deep learning approach in a critic-policy format. The target-oriented motion
critic, which maps both visual observations and target information to the
expected future rewards of pushing and grasping motion primitives, is learned
via deep Q-learning. We divide the problem into two subtasks, and two policies
are proposed to tackle each of them, by combining the critic predictions and
relevant domain knowledge. A Bayesian-based policy accounting for past action
experience performs pushing to search for the target; once the target is found,
a classifier-based policy coordinates target-oriented pushing and grasping to
grasp the target in clutter. The motion critic and the classifier are trained
in a self-supervised manner through robot-environment interactions. Our system
achieves a 93% and 87% task success rate on each of the two subtasks in
simulation and an 85% task success rate in real robot experiments on the whole
problem, which outperforms several baselines by large margins. Supplementary
material is available at https://sites.google.com/umn.edu/grasping-invisible.",arxiv
http://arxiv.org/abs/2110.00468v1,2021-10-01T15:03:03Z,2021-10-01T15:03:03Z,"New Evolutionary Computation Models and their Applications to Machine
  Learning","Automatic Programming is one of the most important areas of computer science
research today. Hardware speed and capability have increased exponentially, but
the software is years behind. The demand for software has also increased
significantly, but it is still written in old fashion: by using humans.
  There are multiple problems when the work is done by humans: cost, time,
quality. It is costly to pay humans, it is hard to keep them satisfied for a
long time, it takes a lot of time to teach and train them and the quality of
their output is in most cases low (in software, mostly due to bugs).
  The real advances in human civilization appeared during the industrial
revolutions. Before the first revolution, most people worked in agriculture.
Today, very few percent of people work in this field.
  A similar revolution must appear in the computer programming field.
Otherwise, we will have so many people working in this field as we had in the
past working in agriculture.
  How do people know how to write computer programs? Very simple: by learning.
Can we do the same for software? Can we put the software to learn how to write
software?
  It seems that is possible (to some degree) and the term is called Machine
Learning. It was first coined in 1959 by the first person who made a computer
perform a serious learning task, namely, Arthur Samuel.
  However, things are not so easy as in humans (well, truth to be said - for
some humans it is impossible to learn how to write software). So far we do not
have software that can learn perfectly to write software. We have some
particular cases where some programs do better than humans, but the examples
are sporadic at best. Learning from experience is difficult for computer
programs. Instead of trying to simulate how humans teach humans how to write
computer programs, we can simulate nature.",arxiv
http://arxiv.org/abs/2012.12305v2,2021-07-22T16:53:43Z,2020-12-22T19:27:11Z,"Confronting Abusive Language Online: A Survey from the Ethical and Human
  Rights Perspective","The pervasiveness of abusive content on the internet can lead to severe
psychological and physical harm. Significant effort in Natural Language
Processing (NLP) research has been devoted to addressing this problem through
abusive content detection and related sub-areas, such as the detection of hate
speech, toxicity, cyberbullying, etc. Although current technologies achieve
high classification performance in research studies, it has been observed that
the real-life application of this technology can cause unintended harms, such
as the silencing of under-represented groups. We review a large body of NLP
research on automatic abuse detection with a new focus on ethical challenges,
organized around eight established ethical principles: privacy, accountability,
safety and security, transparency and explainability, fairness and
non-discrimination, human control of technology, professional responsibility,
and promotion of human values. In many cases, these principles relate not only
to situational ethical codes, which may be context-dependent, but are in fact
connected to universal human rights, such as the right to privacy, freedom from
discrimination, and freedom of expression. We highlight the need to examine the
broad social impacts of this technology, and to bring ethical and human rights
considerations to every stage of the application life-cycle, from task
formulation and dataset design, to model training and evaluation, to
application deployment. Guided by these principles, we identify several
opportunities for rights-respecting, socio-technical solutions to detect and
confront online abuse, including `nudging', `quarantining', value sensitive
design, counter-narratives, style transfer, and AI-driven public education
applications.",arxiv
http://arxiv.org/abs/1904.13017v2,2021-10-06T07:17:01Z,2019-04-30T02:12:00Z,"Hyperspectral Unmixing via Deep Autoencoder Networks for a Generalized
  Linear-Mixture/Nonlinear-Fluctuation Model","Spectral unmixing is an important task in hyperspectral image processing for
separating the mixed spectral data pertaining to various materials observed
individual pixels. Recently, nonlinear spectral unmixing has received
particular attention because a linear mixture is not appropriate under many
conditions. However, existing nonlinear unmixing approaches are often based on
specific assumptions regarding the inherent nonlinearity, and they can be
ineffective when applied to conditions deviating from the original assumptions.
Therefore, these approaches are not well suited to scenes with unknown
nonlinearity characteristics. This paper presents an unsupervised nonlinear
spectral unmixing method based on a deep autoencoder network that applies to a
generalized linear-mixture/nonlinear fluctuation model, consisting of a linear
mixture component and an additive nonlinear mixture component that depends on
both endmembers and abundances. The proposed approach benefits from the
universal modeling ability of deep neural networks to learn the inherent
nonlinearity of the nonlinear mixture component from the data itself via the
autoencoder network, rather than relying on an assumed form. Extensive
experiments with numerically synthetic, labeled laboratory-created data and
real airborne data, illustrate the generality and effectiveness of this
approach compared with state-of-the-art methods.",arxiv
http://arxiv.org/abs/2001.06576v1,2020-01-18T02:05:54Z,2020-01-18T02:05:54Z,"Inference for Network Structure and Dynamics from Time Series Data via
  Graph Neural Network","Network structures in various backgrounds play important roles in social,
technological, and biological systems. However, the observable network
structures in real cases are often incomplete or unavailable due to measurement
errors or private protection issues. Therefore, inferring the complete network
structure is useful for understanding complex systems. The existing studies
have not fully solved the problem of inferring network structure with partial
or no information about connections or nodes. In this paper, we tackle the
problem by utilizing time series data generated by network dynamics. We regard
the network inference problem based on dynamical time series data as a problem
of minimizing errors for predicting future states and proposed a novel
data-driven deep learning model called Gumbel Graph Network (GGN) to solve the
two kinds of network inference problems: Network Reconstruction and Network
Completion. For the network reconstruction problem, the GGN framework includes
two modules: the dynamics learner and the network generator. For the network
completion problem, GGN adds a new module called the States Learner to infer
missing parts of the network. We carried out experiments on discrete and
continuous time series data. The experiments show that our method can
reconstruct up to 100% network structure on the network reconstruction task.
While the model can also infer the unknown parts of the structure with up to
90% accuracy when some nodes are missing. And the accuracy decays with the
increase of the fractions of missing nodes. Our framework may have wide
application areas where the network structure is hard to obtained and the time
series data is rich.",arxiv
http://arxiv.org/abs/1907.10932v1,2019-07-25T09:46:36Z,2019-07-25T09:46:36Z,Object Perception and Grasping in Open-Ended Domains,"Nowadays service robots are leaving the structured and completely known
environments and entering human-centric settings. For these robots, object
perception and grasping are two challenging tasks due to the high demand for
accurate and real-time responses. Although many problems have already been
understood and solved successfully, many challenges still remain. Open-ended
learning is one of these challenges waiting for many improvements. Cognitive
science revealed that humans learn to recognize object categories and grasp
affordances ceaselessly over time. This ability allows adapting to new
environments by enhancing their knowledge from the accumulation of experiences
and the conceptualization of new object categories. Inspired by this, an
autonomous robot must have the ability to process visual information and
conduct learning and recognition tasks in an open-ended fashion. In this
context, ""open-ended"" implies that the set of object categories to be learned
is not known in advance, and the training instances are extracted from online
experiences of a robot, and become gradually available over time, rather than
being completely available at the beginning of the learning process.
  In my research, I mainly focus on interactive open-ended learning approaches
to recognize multiple objects and their grasp affordances concurrently. In
particular, I try to address the following research questions: (i) What is the
importance of open-ended learning for autonomous robots? (ii) How robots could
learn incrementally from their own experiences as well as from interaction with
humans? (iii) What are the limitations of Deep Learning approaches to be used
in an open-ended manner? (iv) How to evaluate open-ended learning approaches
and what are the right metrics to do so?",arxiv
http://arxiv.org/abs/2105.03299v2,2021-05-11T07:41:25Z,2021-05-07T14:52:03Z,"Leveraging Multiple Relations for Fashion Trend Forecasting Based on
  Social Media","Fashion trend forecasting is of great research significance in providing
useful suggestions for both fashion companies and fashion lovers. Although
various studies have been devoted to tackling this challenging task, they only
studied limited fashion elements with highly seasonal or simple patterns, which
could hardly reveal the real complex fashion trends. Moreover, the mainstream
solutions for this task are still statistical-based and solely focus on
time-series data modeling, which limit the forecast accuracy. Towards
insightful fashion trend forecasting, previous work [1] proposed to analyze
more fine-grained fashion elements which can informatively reveal fashion
trends. Specifically, it focused on detailed fashion element trend forecasting
for specific user groups based on social media data. In addition, it proposed a
neural network-based method, namely KERN, to address the problem of fashion
trend modeling and forecasting. In this work, to extend the previous work, we
propose an improved model named Relation Enhanced Attention Recurrent (REAR)
network. Compared to KERN, the REAR model leverages not only the relations
among fashion elements but also those among user groups, thus capturing more
types of correlations among various fashion trends. To further improve the
performance of long-range trend forecasting, the REAR method devises a sliding
temporal attention mechanism, which is able to capture temporal patterns on
future horizons better. Extensive experiments and more analysis have been
conducted on the FIT and GeoStyle datasets to evaluate the performance of REAR.
Experimental and analytical results demonstrate the effectiveness of the
proposed REAR model in fashion trend forecasting, which also show the
improvement of REAR compared to the KERN.",arxiv
http://arxiv.org/abs/1807.01514v1,2018-07-04T10:55:38Z,2018-07-04T10:55:38Z,Generating Synthetic but Plausible Healthcare Record Datasets,"Generating datasets that ""look like"" given real ones is an interesting tasks
for healthcare applications of ML and many other fields of science and
engineering. In this paper we propose a new method of general application to
binary datasets based on a method for learning the parameters of a latent
variable moment that we have previously used for clustering patient datasets.
We compare our method with a recent proposal (MedGan) based on generative
adversarial methods and find that the synthetic datasets we generate are
globally more realistic in at least two senses: real and synthetic instances
are harder to tell apart by Random Forests, and the MMD statistic. The most
likely explanation is that our method does not suffer from the ""mode collapse""
which is an admitted problem of GANs. Additionally, the generative models we
generate are easy to interpret, unlike the rather obscure GANs. Our experiments
are performed on two patient datasets containing ICD-9 diagnostic codes: the
publicly available MIMIC-III dataset and a dataset containing admissions for
congestive heart failure during 7 years at Hospital de Sant Pau in Barcelona.",arxiv
http://arxiv.org/abs/2102.11931v3,2021-05-21T20:02:03Z,2021-02-23T20:33:22Z,"Boosting background suppression in the NEXT experiment through
  Richardson-Lucy deconvolution","Next-generation neutrinoless double beta decay experiments aim for half-life
sensitivities of ~$10^{27}$ yr, requiring suppressing backgrounds to <1
count/tonne/yr. For this, any extra background rejection handle, beyond
excellent energy resolution and the use of extremely radiopure materials, is of
utmost importance. The NEXT experiment exploits differences in the spatial
ionization patterns of double beta decay and single-electron events to
discriminate signal from background. While the former display two Bragg peak
dense ionization regions at the opposite ends of the track, the latter
typically have only one such feature. Thus, comparing the energies at the track
extremes provides an additional rejection tool. The unique combination of the
topology-based background discrimination and excellent energy resolution (1%
FWHM at the Q-value of the decay) is the distinguishing feature of NEXT.
Previous studies demonstrated a topological background rejection factor of ~5
when reconstructing electron-positron pairs in the $^{208}$Tl 1.6 MeV double
escape peak (with Compton events as background), recorded in the NEXT-White
demonstrator at the Laboratorio Subterr\'aneo de Canfranc, with 72% signal
efficiency. This was recently improved through the use of a deep convolutional
neural network to yield a background rejection factor of ~10 with 65% signal
efficiency. Here, we present a new reconstruction method, based on the
Richardson-Lucy deconvolution algorithm, which allows reversing the blurring
induced by electron diffusion and electroluminescence light production in the
NEXT TPC. The new method yields highly refined 3D images of reconstructed
events, and, as a result, significantly improves the topological background
discrimination. When applied to real-data 1.6 MeV $e^-e^+$ pairs, it leads to a
background rejection factor of 27 at 57% signal efficiency.",arxiv
