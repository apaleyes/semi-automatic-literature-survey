id,updated,published,title,summary,database
http://arxiv.org/abs/2001.09778v2,2020-02-06T14:46:51Z,2020-01-22T15:39:42Z,"Artificial intelligence in medicine and healthcare: a review and
  classification of current and near-future applications and their ethical and
  social Impact","This paper provides an overview of the current and near-future applications
of Artificial Intelligence (AI) in Medicine and Health Care and presents a
classification according to their ethical and societal aspects, potential
benefits and pitfalls, and issues that can be considered controversial and are
not deeply discussed in the literature.
  This work is based on an analysis of the state of the art of research and
technology, including existing software, personal monitoring devices, genetic
tests and editing tools, personalized digital models, online platforms,
augmented reality devices, and surgical and companion robotics. Motivated by
our review, we present and describe the notion of 'extended personalized
medicine', we then review existing applications of AI in medicine and
healthcare and explore the public perception of medical AI systems, and how
they show, simultaneously, extraordinary opportunities and drawbacks that even
question fundamental medical concepts. Many of these topics coincide with
urgent priorities recently defined by the World Health Organization for the
coming decade. In addition, we study the transformations of the roles of
doctors and patients in an age of ubiquitous information, identify the risk of
a division of Medicine into 'fake-based', 'patient-generated', and
'scientifically tailored', and draw the attention of some aspects that need
further thorough analysis and public debate.",arxiv
http://arxiv.org/abs/2102.01998v1,2021-02-03T10:56:58Z,2021-02-03T10:56:58Z,"Unbox the Black-box for the Medical Explainable AI via Multi-modal and
  Multi-centre Data Fusion: A Mini-Review, Two Showcases and Beyond","Explainable Artificial Intelligence (XAI) is an emerging research topic of
machine learning aimed at unboxing how AI systems' black-box choices are made.
This research field inspects the measures and models involved in
decision-making and seeks solutions to explain them explicitly. Many of the
machine learning algorithms can not manifest how and why a decision has been
cast. This is particularly true of the most popular deep neural network
approaches currently in use. Consequently, our confidence in AI systems can be
hindered by the lack of explainability in these black-box models. The XAI
becomes more and more crucial for deep learning powered applications,
especially for medical and healthcare studies, although in general these deep
neural networks can return an arresting dividend in performance. The
insufficient explainability and transparency in most existing AI systems can be
one of the major reasons that successful implementation and integration of AI
tools into routine clinical practice are uncommon. In this study, we first
surveyed the current progress of XAI and in particular its advances in
healthcare applications. We then introduced our solutions for XAI leveraging
multi-modal and multi-centre data fusion, and subsequently validated in two
showcases following real clinical scenarios. Comprehensive quantitative and
qualitative analyses can prove the efficacy of our proposed XAI solutions, from
which we can envisage successful applications in a broader range of clinical
questions.",arxiv
http://arxiv.org/abs/1912.12115v1,2019-12-27T14:39:58Z,2019-12-27T14:39:58Z,Split Learning for collaborative deep learning in healthcare,"Shortage of labeled data has been holding the surge of deep learning in
healthcare back, as sample sizes are often small, patient information cannot be
shared openly, and multi-center collaborative studies are a burden to set up.
Distributed machine learning methods promise to mitigate these problems. We
argue for a split learning based approach and apply this distributed learning
method for the first time in the medical field to compare performance against
(1) centrally hosted and (2) non collaborative configurations for a range of
participants. Two medical deep learning tasks are used to compare split
learning to conventional single and multi center approaches: a binary
classification problem of a data set of 9000 fundus photos, and multi-label
classification problem of a data set of 156,535 chest X-rays. The several
distributed learning setups are compared for a range of 1-50 distributed
participants. Performance of the split learning configuration remained constant
for any number of clients compared to a single center study, showing a marked
difference compared to the non collaborative configuration after 2 clients (p <
0.001) for both sets. Our results affirm the benefits of collaborative training
of deep neural networks in health care. Our work proves the significant benefit
of distributed learning in healthcare, and paves the way for future real-world
implementations.",arxiv
http://arxiv.org/abs/1909.13343v2,2019-10-01T16:06:39Z,2019-09-29T19:15:08Z,"ISTHMUS: Secure, Scalable, Real-time and Robust Machine Learning
  Platform for Healthcare","In recent times, machine learning (ML) and artificial intelligence (AI) based
systems have evolved and scaled across different industries such as finance,
retail, insurance, energy utilities, etc. Among other things, they have been
used to predict patterns of customer behavior, to generate pricing models, and
to predict the return on investments. But the successes in deploying machine
learning models at scale in those industries have not translated into the
healthcare setting. There are multiple reasons why integrating ML models into
healthcare has not been widely successful, but from a technical perspective,
general-purpose commercial machine learning platforms are not a good fit for
healthcare due to complexities in handling data quality issues, mandates to
demonstrate clinical relevance, and a lack of ability to monitor performance in
a highly regulated environment with stringent security and privacy needs. In
this paper, we describe Isthmus, a turnkey, cloud-based platform which
addresses the challenges above and reduces time to market for operationalizing
ML/AI in healthcare. Towards the end, we describe three case studies which shed
light on Isthmus capabilities. These include (1) supporting an end-to-end
lifecycle of a model which predicts trauma survivability at hospital trauma
centers, (2) bringing in and harmonizing data from disparate sources to create
a community data platform for inferring population as well as patient level
insights for Social Determinants of Health (SDoH), and (3) ingesting
live-streaming data from various IoT sensors to build models, which can
leverage real-time and longitudinal information to make advanced time-sensitive
predictions.",arxiv
http://arxiv.org/abs/2012.06354v1,2020-12-10T13:56:00Z,2020-12-10T13:56:00Z,Privacy-preserving medical image analysis,"The utilisation of artificial intelligence in medicine and healthcare has led
to successful clinical applications in several domains. The conflict between
data usage and privacy protection requirements in such systems must be resolved
for optimal results as well as ethical and legal compliance. This calls for
innovative solutions such as privacy-preserving machine learning (PPML). We
present PriMIA (Privacy-preserving Medical Image Analysis), a software
framework designed for PPML in medical imaging. In a real-life case study we
demonstrate significantly better classification performance of a securely
aggregated federated learning model compared to human experts on unseen
datasets. Furthermore, we show an inference-as-a-service scenario for
end-to-end encrypted diagnosis, where neither the data nor the model are
revealed. Lastly, we empirically evaluate the framework's security against a
gradient-based model inversion attack and demonstrate that no usable
information can be recovered from the model.",arxiv
http://arxiv.org/abs/2012.05410v1,2020-12-10T02:08:47Z,2020-12-10T02:08:47Z,Artificial Intelligence at the Edge,"The Internet of Things (IoT) and edge computing applications aim to support a
variety of societal needs, including the global pandemic situation that the
entire world is currently experiencing and responses to natural disasters.
  The need for real-time interactive applications such as immersive video
conferencing, augmented/virtual reality, and autonomous vehicles, in education,
healthcare, disaster recovery and other domains, has never been higher. At the
same time, there have been recent technological breakthroughs in highly
relevant fields such as artificial intelligence (AI)/machine learning (ML),
advanced communication systems (5G and beyond), privacy-preserving
computations, and hardware accelerators. 5G mobile communication networks
increase communication capacity, reduce transmission latency and error, and
save energy -- capabilities that are essential for new applications. The
envisioned future 6G technology will integrate many more technologies,
including for example visible light communication, to support groundbreaking
applications, such as holographic communications and high precision
manufacturing. Many of these applications require computations and analytics
close to application end-points: that is, at the edge of the network, rather
than in a centralized cloud. AI techniques applied at the edge have tremendous
potential both to power new applications and to need more efficient operation
of edge infrastructure. However, it is critical to understand where to deploy
AI systems within complex ecosystems consisting of advanced applications and
the specific real-time requirements towards AI systems.",arxiv
http://arxiv.org/abs/2010.02715v1,2020-10-03T23:18:05Z,2020-10-03T23:18:05Z,"Assessing Automated Machine Learning service to detect COVID-19 from
  X-Ray and CT images: A Real-time Smartphone Application case study","The recent outbreak of SARS COV-2 gave us a unique opportunity to study for a
non interventional and sustainable AI solution. Lung disease remains a major
healthcare challenge with high morbidity and mortality worldwide. The
predominant lung disease was lung cancer. Until recently, the world has
witnessed the global pandemic of COVID19, the Novel coronavirus outbreak. We
have experienced how viral infection of lung and heart claimed thousands of
lives worldwide. With the unprecedented advancement of Artificial Intelligence
in recent years, Machine learning can be used to easily detect and classify
medical imagery. It is much faster and most of the time more accurate than
human radiologists. Once implemented, it is more cost-effective and
time-saving. In our study, we evaluated the efficacy of Microsoft Cognitive
Service to detect and classify COVID19 induced pneumonia from other
Viral/Bacterial pneumonia based on X-Ray and CT images. We wanted to assess the
implication and accuracy of the Automated ML-based Rapid Application
Development (RAD) environment in the field of Medical Image diagnosis. This
study will better equip us to respond with an ML-based diagnostic Decision
Support System(DSS) for a Pandemic situation like COVID19. After optimization,
the trained network achieved 96.8% Average Precision which was implemented as a
Web Application for consumption. However, the same trained network did not
perform the same like Web Application when ported to Smartphone for Real-time
inference. Which was our main interest of study. The authors believe, there is
scope for further study on this issue. One of the main goal of this study was
to develop and evaluate the performance of AI-powered Smartphone-based
Real-time Application. Facilitating primary diagnostic services in less
equipped and understaffed rural healthcare centers of the world with unreliable
internet service.",arxiv
http://arxiv.org/abs/1803.04873v2,2018-03-14T15:30:00Z,2018-03-13T15:17:30Z,"Using Convolutional Neural Networks for Determining Reticulocyte
  Percentage in Cats","Recent advances in artificial intelligence (AI), specifically in computer
vision (CV) and deep learning (DL), have created opportunities for novel
systems in many fields. In the last few years, deep learning applications have
demonstrated impressive results not only in fields such as autonomous driving
and robotics, but also in the field of medicine, where they have, in some
cases, even exceeded human-level performance. However, despite the huge
potential, adoption of deep learning-based methods is still slow in many areas,
especially in veterinary medicine, where we haven't been able to find any
research papers using modern convolutional neural networks (CNNs) in medical
image processing. We believe that using deep learning-based medical imaging can
enable more accurate, faster and less expensive diagnoses in veterinary
medicine. In order to do so, however, these methods have to be accessible to
everyone in this field, not just to computer scientists. To show the potential
of this technology, we present results on a real-world task in veterinary
medicine that is usually done manually: feline reticulocyte percentage. Using
an open source Keras implementation of the Single-Shot MultiBox Detector (SSD)
model architecture and training it on only 800 labeled images, we achieve an
accuracy of 98.7% at predicting the correct number of aggregate reticulocytes
in microscope images of cat blood smears. The main motivation behind this paper
is to show not only that deep learning can approach or even exceed human-level
performance on a task like this, but also that anyone in the field can
implement it, even without a background in computer science.",arxiv
http://arxiv.org/abs/1711.06517v2,2018-05-22T05:54:17Z,2017-11-17T12:59:22Z,Wikipedia for Smart Machines and Double Deep Machine Learning,"Very important breakthroughs in data centric deep learning algorithms led to
impressive performance in transactional point applications of Artificial
Intelligence (AI) such as Face Recognition, or EKG classification. With all due
appreciation, however, knowledge blind data only machine learning algorithms
have severe limitations for non-transactional AI applications, such as medical
diagnosis beyond the EKG results. Such applications require deeper and broader
knowledge in their problem solving capabilities, e.g. integrating anatomy and
physiology knowledge with EKG results and other patient findings. Following a
review and illustrations of such limitations for several real life AI
applications, we point at ways to overcome them. The proposed Wikipedia for
Smart Machines initiative aims at building repositories of software structures
that represent humanity science & technology knowledge in various parts of
life; knowledge that we all learn in schools, universities and during our
professional life. Target readers for these repositories are smart machines;
not human. AI software developers will have these Reusable Knowledge structures
readily available, hence, the proposed name ReKopedia. Big Data is by now a
mature technology, it is time to focus on Big Knowledge. Some will be derived
from data, some will be obtained from mankind gigantic repository of knowledge.
Wikipedia for smart machines along with the new Double Deep Learning approach
offer a paradigm for integrating datacentric deep learning algorithms with
algorithms that leverage deep knowledge, e.g. evidential reasoning and
causality reasoning. For illustration, a project is described to produce
ReKopedia knowledge modules for medical diagnosis of about 1,000 disorders.
Data is important, but knowledge deep, basic, and commonsense is equally
important.",arxiv
http://arxiv.org/abs/2107.10230v4,2021-08-13T05:13:51Z,2021-07-21T17:28:46Z,"Multi-institution encrypted medical imaging AI validation without data
  sharing","Adoption of artificial intelligence medical imaging applications is often
impeded by barriers between healthcare systems and algorithm developers given
that access to both private patient data and commercial model IP is important
to perform pre-deployment evaluation. This work investigates a framework for
secure, privacy-preserving and AI-enabled medical imaging inference using
CrypTFlow2, a state-of-the-art end-to-end compiler allowing cryptographically
secure 2-party Computation (2PC) protocols between the machine learning model
vendor and target patient data owner. A common DenseNet-121 chest x-ray
diagnosis model was evaluated on multi-institutional chest radiographic imaging
datasets both with and without CrypTFlow2 on two test sets spanning seven sites
across the US and India, and comprising 1,149 chest x-ray images. We measure
comparative AUROC performance between secure and insecure inference in multiple
pathology classification tasks, and explore model output distributional shifts
and resource constraints introduced by secure model inference. Secure inference
with CrypTFlow2 demonstrated no significant difference in AUROC for all
diagnoses, and model outputs from secure and insecure inference methods were
distributionally equivalent. The use of CrypTFlow2 may allow off-the-shelf
secure 2PC between healthcare systems and AI model vendors for medical imaging,
without changes in performance, and can facilitate scalable pre-deployment
infrastructure for real-world secure model evaluation without exposure to
patient data or model IP.",arxiv
http://arxiv.org/abs/2011.03274v1,2020-11-06T10:41:39Z,2020-11-06T10:41:39Z,"Trust Issues: Uncertainty Estimation Does Not Enable Reliable OOD
  Detection On Medical Tabular Data","When deploying machine learning models in high-stakes real-world environments
such as health care, it is crucial to accurately assess the uncertainty
concerning a model's prediction on abnormal inputs. However, there is a
scarcity of literature analyzing this problem on medical data, especially on
mixed-type tabular data such as Electronic Health Records. We close this gap by
presenting a series of tests including a large variety of contemporary
uncertainty estimation techniques, in order to determine whether they are able
to identify out-of-distribution (OOD) patients. In contrast to previous work,
we design tests on realistic and clinically relevant OOD groups, and run
experiments on real-world medical data. We find that almost all techniques fail
to achieve convincing results, partly disagreeing with earlier findings.",arxiv
http://arxiv.org/abs/2109.07846v1,2021-09-16T10:22:31Z,2021-09-16T10:22:31Z,"Telehealthcare and Covid-19: A Noninvasive & Low Cost Invasive, Scalable
  and Multimodal Real-Time Smartphone Application for Early Diagnosis of
  SARS-CoV-2 Infection","The global coronavirus pandemic overwhelmed many health care systems,
enforcing lockdown and encouraged work from home to control the spread of the
virus and prevent overrunning of hospitalized patients. This prompted a sharp
widespread use of telehealth to provide low-risk care for patients.
Nevertheless, a continuous mutation into new variants and widespread
unavailability of test kits, especially in developing countries, possess the
challenge to control future potential waves of infection. In this paper, we
propose a novel Smartphone application-based platform for early diagnosis of
possible Covid-19 infected patients. The application provides three modes of
diagnosis from possible symptoms, cough sound, and specific blood biomarkers.
When a user chooses a particular setting and provides the necessary
information, it sends the data to a trained machine learning (ML) model
deployed in a remote server using the internet. The ML algorithm then predicts
the possibility of contracting Covid-19 and sends the feedback to the user. The
entire procedure takes place in real-time. Our machine learning models can
identify Covid-19 patients with an accuracy of 100%, 95.65%, and 77.59% from
blood parameters, cough sound, and symptoms respectively. Moreover, the ML
sensitivity for blood and sound is 100%, which indicates correct identification
of Covid positive patients. This is significant in limiting the spread of the
virus. The multimodality offers multiplex diagnostic methods to better classify
possible infectees and together with the instantaneous nature of our technique,
demonstrates the power of telehealthcare as an easy and widespread low-cost
scalable diagnostic solution for future pandemics.",arxiv
http://arxiv.org/abs/2009.01657v1,2020-08-27T20:53:26Z,2020-08-27T20:53:26Z,"A free web service for fast COVID-19 classification of chest X-Ray
  images","The coronavirus outbreak became a major concern for society worldwide.
Technological innovation and ingenuity are essential to fight COVID-19 pandemic
and bring us one step closer to overcome it. Researchers over the world are
working actively to find available alternatives in different fields, such as
the Healthcare System, pharmaceutic, health prevention, among others. With the
rise of artificial intelligence (AI) in the last 10 years, IA-based
applications have become the prevalent solution in different areas because of
its higher capability, being now adopted to help combat against COVID-19. This
work provides a fast detection system of COVID-19 characteristics in X-Ray
images based on deep learning (DL) techniques. This system is available as a
free web deployed service for fast patient classification, alleviating the high
demand for standards method for COVID-19 diagnosis. It is constituted of two
deep learning models, one to differentiate between X-Ray and non-X-Ray images
based on Mobile-Net architecture, and another one to identify chest X-Ray
images with characteristics of COVID-19 based on the DenseNet architecture. For
real-time inference, it is provided a pair of dedicated GPUs, which reduce the
computational time. The whole system can filter out non-chest X-Ray images, and
detect whether the X-Ray presents characteristics of COVID-19, highlighting the
most sensitive regions.",arxiv
http://arxiv.org/abs/1804.09997v1,2018-04-26T11:37:03Z,2018-04-26T11:37:03Z,PANDA: Facilitating Usable AI Development,"Recent advances in artificial intelligence (AI) and machine learning have
created a general perception that AI could be used to solve complex problems,
and in some situations over-hyped as a tool that can be so easily used.
Unfortunately, the barrier to realization of mass adoption of AI on various
business domains is too high because most domain experts have no background in
AI. Developing AI applications involves multiple phases, namely data
preparation, application modeling, and product deployment. The effort of AI
research has been spent mostly on new AI models (in the model training stage)
to improve the performance of benchmark tasks such as image recognition. Many
other factors such as usability, efficiency and security of AI have not been
well addressed, and therefore form a barrier to democratizing AI. Further, for
many real world applications such as healthcare and autonomous driving,
learning via huge amounts of possibility exploration is not feasible since
humans are involved. In many complex applications such as healthcare, subject
matter experts (e.g. Clinicians) are the ones who appreciate the importance of
features that affect health, and their knowledge together with existing
knowledge bases are critical to the end results. In this paper, we take a new
perspective on developing AI solutions, and present a solution for making AI
usable. We hope that this resolution will enable all subject matter experts
(eg. Clinicians) to exploit AI like data scientists.",arxiv
http://arxiv.org/abs/1804.05296v3,2019-02-04T06:03:22Z,2018-04-15T02:33:08Z,Adversarial Attacks Against Medical Deep Learning Systems,"The discovery of adversarial examples has raised concerns about the practical
deployment of deep learning systems. In this paper, we demonstrate that
adversarial examples are capable of manipulating deep learning systems across
three clinical domains. For each of our representative medical deep learning
classifiers, both white and black box attacks were highly successful. Our
models are representative of the current state of the art in medical computer
vision and, in some cases, directly reflect architectures already seeing
deployment in real world clinical settings. In addition to the technical
contribution of our paper, we synthesize a large body of knowledge about the
healthcare system to argue that medicine may be uniquely susceptible to
adversarial attacks, both in terms of monetary incentives and technical
vulnerability. To this end, we outline the healthcare economy and the
incentives it creates for fraud and provide concrete examples of how and why
such attacks could be realistically carried out. We urge practitioners to be
aware of current vulnerabilities when deploying deep learning systems in
clinical settings, and encourage the machine learning community to further
investigate the domain-specific characteristics of medical learning systems.",arxiv
http://arxiv.org/abs/2111.11789v1,2021-11-23T11:06:27Z,2021-11-23T11:06:27Z,"End-to-End Optimized Arrhythmia Detection Pipeline using Machine
  Learning for Ultra-Edge Devices","Atrial fibrillation (AF) is the most prevalent cardiac arrhythmia worldwide,
with 2% of the population affected. It is associated with an increased risk of
strokes, heart failure and other heart-related complications. Monitoring
at-risk individuals and detecting asymptomatic AF could result in considerable
public health benefits, as individuals with asymptomatic AF could take
preventive measures with lifestyle changes. With increasing affordability to
wearables, personalized health care is becoming more accessible. These
personalized healthcare solutions require accurate classification of
bio-signals while being computationally inexpensive. By making inferences
on-device, we avoid issues inherent to cloud-based systems such as latency and
network connection dependency. We propose an efficient pipeline for real-time
Atrial Fibrillation Detection with high accuracy that can be deployed in
ultra-edge devices. The feature engineering employed in this research catered
to optimizing the resource-efficient classifier used in the proposed pipeline,
which was able to outperform the best performing standard ML model by
$10^5\times$ in terms of memory footprint with a mere trade-off of 2%
classification accuracy. We also obtain higher accuracy of approximately 6%
while consuming 403$\times$ lesser memory and being 5.2$\times$ faster compared
to the previous state-of-the-art (SoA) embedded implementation.",arxiv
http://arxiv.org/abs/2110.04249v1,2021-10-08T16:58:57Z,2021-10-08T16:58:57Z,How Can AI Recognize Pain and Express Empathy,"Sensory and emotional experiences such as pain and empathy are relevant to
mental and physical health. The current drive for automated pain recognition is
motivated by a growing number of healthcare requirements and demands for social
interaction make it increasingly essential. Despite being a trending area, they
have not been explored in great detail. Over the past decades, behavioral
science and neuroscience have uncovered mechanisms that explain the
manifestations of pain. Recently, also artificial intelligence research has
allowed empathic machine learning methods to be approachable. Generally, the
purpose of this paper is to review the current developments for computational
pain recognition and artificial empathy implementation. Our discussion covers
the following topics: How can AI recognize pain from unimodality and
multimodality? Is it necessary for AI to be empathic? How can we create an AI
agent with proactive and reactive empathy? This article explores the challenges
and opportunities of real-world multimodal pain recognition from a
psychological, neuroscientific, and artificial intelligence perspective.
Finally, we identify possible future implementations of artificial empathy and
analyze how humans might benefit from an AI agent equipped with empathy.",arxiv
http://arxiv.org/abs/1812.00825v2,2018-12-04T05:36:36Z,2018-11-21T21:02:50Z,"Microscope 2.0: An Augmented Reality Microscope with Real-time
  Artificial Intelligence Integration","The brightfield microscope is instrumental in the visual examination of both
biological and physical samples at sub-millimeter scales. One key clinical
application has been in cancer histopathology, where the microscopic assessment
of the tissue samples is used for the diagnosis and staging of cancer and thus
guides clinical therapy. However, the interpretation of these samples is
inherently subjective, resulting in significant diagnostic variability.
Moreover, in many regions of the world, access to pathologists is severely
limited due to lack of trained personnel. In this regard, Artificial
Intelligence (AI) based tools promise to improve the access and quality of
healthcare. However, despite significant advances in AI research, integration
of these tools into real-world cancer diagnosis workflows remains challenging
because of the costs of image digitization and difficulties in deploying AI
solutions. Here we propose a cost-effective solution to the integration of AI:
the Augmented Reality Microscope (ARM). The ARM overlays AI-based information
onto the current view of the sample through the optical pathway in real-time,
enabling seamless integration of AI into the regular microscopy workflow. We
demonstrate the utility of ARM in the detection of lymph node metastases in
breast cancer and the identification of prostate cancer with a latency that
supports real-time workflows. We anticipate that ARM will remove barriers
towards the use of AI in microscopic analysis and thus improve the accuracy and
efficiency of cancer diagnosis. This approach is applicable to other microscopy
tasks and AI algorithms in the life sciences and beyond.",arxiv
http://arxiv.org/abs/2010.07029v2,2021-02-27T19:06:48Z,2020-09-29T21:32:28Z,"Basic principles and concept design of a real-time clinical decision
  support system for managing medical emergencies on missions to Mars","Space agencies and private companies prepare the beginning of human space
exploration for the 2030s with missions to put the first human on the Mars
surface. The absence of gravity and radiation, along with distance, isolation
and hostile environments, are expected to increase medical events where
previously unseen manifestations may arise. The current healthcare strategy
based on telemedicine and the possibility to stabilize and transport the
injured crewmember to a terrestrial definitive medical facility is not
applicable in exploration class missions. Therefore, the need for deploying the
full autonomous capability to solve medical emergencies may guide the design of
future onboard healthcare systems. We present ten basic principles and concept
design of a software suite to bring onboard decision support to help the crew
dealing with medical emergencies taking into consideration physiological
disturbances in space and spaceflight restrictions. 1) give real-time support
for emergency medical decision making, 2) give patient-specific advice for
executive problem-solving, 3) take into account available information from life
support and monitoring of crewmembers, 4) be fully autonomous from remote
facilities, 5) continuously adapt predictions to physiological disturbance and
changing conditions, 6) optimize emergency medical decision making in terms of
mission fundamental priorities, 7) take into account medical supplies and
equipment on board, 8) apply health standards for the level of care V, 9)
implement ethics responsibilities for spaceflights, and 10) apply ethical
standards for artificial intelligence. Based on these principles, we propose an
autonomous clinical decision support system (CDSS) to provide real-time advice
for emergency medical interventions on board of space exploration missions.",arxiv
http://arxiv.org/abs/2005.08076v1,2020-05-16T19:42:16Z,2020-05-16T19:42:16Z,"A Deep Learning based Wearable Healthcare IoT Device for AI-enabled
  Hearing Assistance Automation","With the recent booming of artificial intelligence (AI), particularly deep
learning techniques, digital healthcare is one of the prevalent areas that
could gain benefits from AI-enabled functionality. This research presents a
novel AI-enabled Internet of Things (IoT) device operating from the ESP-8266
platform capable of assisting those who suffer from impairment of hearing or
deafness to communicate with others in conversations. In the proposed solution,
a server application is created that leverages Google's online speech
recognition service to convert the received conversations into texts, then
deployed to a micro-display attached to the glasses to display the conversation
contents to deaf people, to enable and assist conversation as normal with the
general population. Furthermore, in order to raise alert of traffic or
dangerous scenarios, an 'urban-emergency' classifier is developed using a deep
learning model, Inception-v4, with transfer learning to detect/recognize
alerting/alarming sounds, such as a horn sound or a fire alarm, with texts
generated to alert the prospective user. The training of Inception-v4 was
carried out on a consumer desktop PC and then implemented into the AI based IoT
application. The empirical results indicate that the developed prototype system
achieves an accuracy rate of 92% for sound recognition and classification with
real-time performance.",arxiv
http://arxiv.org/abs/2012.11952v1,2020-12-22T12:11:42Z,2020-12-22T12:11:42Z,"A Feasibility study for Deep learning based automated brain tumor
  segmentation using Magnetic Resonance Images","Deep learning algorithms have accounted for the rapid acceleration of
research in artificial intelligence in medical image analysis, interpretation,
and segmentation with many potential applications across various sub
disciplines in medicine. However, only limited number of research which
investigates these application scenarios, are deployed into the clinical sector
for the evaluation of the real requirement and the practical challenges of the
model deployment. In this research, a deep convolutional neural network (CNN)
based classification network and Faster RCNN based localization network were
developed for brain tumor MR image classification and tumor localization. A
typical edge detection algorithm called Prewitt was used for tumor segmentation
task, based on the output of the tumor localization. Overall performance of the
proposed tumor segmentation architecture, was analyzed using objective quality
parameters including Accuracy, Boundary Displacement Error (BDE), Dice score
and confidence interval. A subjective quality assessment of the model was
conducted based on the Double Stimulus Impairment Scale (DSIS) protocol using
the input of medical expertise. It was observed that the confidence level of
our segmented output was in a similar range to that of experts. Also, the
Neurologists have rated the output of our model as highly accurate
segmentation.",arxiv
http://arxiv.org/abs/2101.03989v1,2021-01-11T15:54:48Z,2021-01-11T15:54:48Z,Technology Readiness Levels for Machine Learning Systems,"The development and deployment of machine learning (ML) systems can be
executed easily with modern tools, but the process is typically rushed and
means-to-an-end. The lack of diligence can lead to technical debt, scope creep
and misaligned objectives, model misuse and failures, and expensive
consequences. Engineering systems, on the other hand, follow well-defined
processes and testing standards to streamline development for high-quality,
reliable results. The extreme is spacecraft systems, where mission critical
measures and robustness are ingrained in the development process. Drawing on
experience in both spacecraft engineering and ML (from research through product
across domain areas), we have developed a proven systems engineering approach
for machine learning development and deployment. Our ""Machine Learning
Technology Readiness Levels"" (MLTRL) framework defines a principled process to
ensure robust, reliable, and responsible systems while being streamlined for ML
workflows, including key distinctions from traditional software engineering.
Even more, MLTRL defines a lingua franca for people across teams and
organizations to work collaboratively on artificial intelligence and machine
learning technologies. Here we describe the framework and elucidate it with
several real world use-cases of developing ML methods from basic research
through productization and deployment, in areas such as medical diagnostics,
consumer computer vision, satellite imagery, and particle physics.",arxiv
http://arxiv.org/abs/2001.09346v2,2020-03-04T19:22:37Z,2020-01-25T18:43:47Z,"CorGAN: Correlation-Capturing Convolutional Generative Adversarial
  Networks for Generating Synthetic Healthcare Records","Deep learning models have demonstrated high-quality performance in areas such
as image classification and speech processing. However, creating a deep
learning model using electronic health record (EHR) data, requires addressing
particular privacy challenges that are unique to researchers in this domain.
This matter focuses attention on generating realistic synthetic data while
ensuring privacy. In this paper, we propose a novel framework called
correlation-capturing Generative Adversarial Network (CorGAN), to generate
synthetic healthcare records. In CorGAN we utilize Convolutional Neural
Networks to capture the correlations between adjacent medical features in the
data representation space by combining Convolutional Generative Adversarial
Networks and Convolutional Autoencoders. To demonstrate the model fidelity, we
show that CorGAN generates synthetic data with performance similar to that of
real data in various Machine Learning settings such as classification and
prediction. We also give a privacy assessment and report on statistical
analysis regarding realistic characteristics of the synthetic data. The
software of this work is open-source and is available at:
https://github.com/astorfi/cor-gan.",arxiv
http://arxiv.org/abs/2107.05989v1,2021-07-13T11:17:00Z,2021-07-13T11:17:00Z,"Emotion Recognition for Healthcare Surveillance Systems Using Neural
  Networks: A Survey","Recognizing the patient's emotions using deep learning techniques has
attracted significant attention recently due to technological advancements.
Automatically identifying the emotions can help build smart healthcare centers
that can detect depression and stress among the patients in order to start the
medication early. Using advanced technology to identify emotions is one of the
most exciting topics as it defines the relationships between humans and
machines. Machines learned how to predict emotions by adopting various methods.
In this survey, we present recent research in the field of using neural
networks to recognize emotions. We focus on studying emotions' recognition from
speech, facial expressions, and audio-visual input and show the different
techniques of deploying these algorithms in the real world. These three emotion
recognition techniques can be used as a surveillance system in healthcare
centers to monitor patients. We conclude the survey with a presentation of the
challenges and the related future work to provide an insight into the
applications of using emotion recognition.",arxiv
http://arxiv.org/abs/2008.08525v1,2020-08-19T16:05:58Z,2020-08-19T16:05:58Z,"""Name that manufacturer"". Relating image acquisition bias with task
  complexity when training deep learning models: experiments on head CT","As interest in applying machine learning techniques for medical images
continues to grow at a rapid pace, models are starting to be developed and
deployed for clinical applications. In the clinical AI model development
lifecycle (described by Lu et al. [1]), a crucial phase for machine learning
scientists and clinicians is the proper design and collection of the data
cohort. The ability to recognize various forms of biases and distribution
shifts in the dataset is critical at this step. While it remains difficult to
account for all potential sources of bias, techniques can be developed to
identify specific types of bias in order to mitigate their impact. In this work
we analyze how the distribution of scanner manufacturers in a dataset can
contribute to the overall bias of deep learning models. We evaluate
convolutional neural networks (CNN) for both classification and segmentation
tasks, specifically two state-of-the-art models: ResNet [2] for classification
and U-Net [3] for segmentation. We demonstrate that CNNs can learn to
distinguish the imaging scanner manufacturer and that this bias can
substantially impact model performance for both classification and segmentation
tasks. By creating an original synthesis dataset of brain data mimicking the
presence of more or less subtle lesions we also show that this bias is related
to the difficulty of the task. Recognition of such bias is critical to develop
robust, generalizable models that will be crucial for clinical applications in
real-world data distributions.",arxiv
http://arxiv.org/abs/2107.08574v1,2021-07-19T01:29:16Z,2021-07-19T01:29:16Z,"A Modulation Layer to Increase Neural Network Robustness Against Data
  Quality Issues","Data quality is a common problem in machine learning, especially in
high-stakes settings such as healthcare. Missing data affects accuracy,
calibration, and feature attribution in complex patterns. Developers often
train models on carefully curated datasets to minimize missing data bias;
however, this reduces the usability of such models in production environments,
such as real-time healthcare records. Making machine learning models robust to
missing data is therefore crucial for practical application. While some
classifiers naturally handle missing data, others, such as deep neural
networks, are not designed for unknown values. We propose a novel neural
network modification to mitigate the impacts of missing data. The approach is
inspired by neuromodulation that is performed by biological neural networks.
Our proposal replaces the fixed weights of a fully-connected layer with a
function of an additional input (reliability score) at each input, mimicking
the ability of cortex to up- and down-weight inputs based on the presence of
other data. The modulation function is jointly learned with the main task using
a multi-layer perceptron. We tested our modulating fully connected layer on
multiple classification, regression, and imputation problems, and it either
improved performance or generated comparable performance to conventional neural
network architectures concatenating reliability to the inputs. Models with
modulating layers were more robust against degradation of data quality by
introducing additional missingness at evaluation time. These results suggest
that explicitly accounting for reduced information quality with a modulating
fully connected layer can enable the deployment of artificial intelligence
systems in real-time settings.",arxiv
http://arxiv.org/abs/1908.04537v2,2019-08-14T09:20:57Z,2019-08-13T08:49:33Z,"Icebreaker: Element-wise Active Information Acquisition with Bayesian
  Deep Latent Gaussian Model","In this paper we introduce the ice-start problem, i.e., the challenge of
deploying machine learning models when only little or no training data is
initially available, and acquiring each feature element of data is associated
with costs. This setting is representative for the real-world machine learning
applications. For instance, in the health-care domain, when training an AI
system for predicting patient metrics from lab tests, obtaining every single
measurement comes with a high cost. Active learning, where only the label is
associated with a cost does not apply to such problem, because performing all
possible lab tests to acquire a new training datum would be costly, as well as
unnecessary due to redundancy. We propose Icebreaker, a principled framework to
approach the ice-start problem. Icebreaker uses a full Bayesian Deep Latent
Gaussian Model (BELGAM) with a novel inference method. Our proposed method
combines recent advances in amortized inference and stochastic gradient MCMC to
enable fast and accurate posterior inference. By utilizing BELGAM's ability to
fully quantify model uncertainty, we also propose two information acquisition
functions for imputation and active prediction problems. We demonstrate that
BELGAM performs significantly better than the previous VAE (Variational
autoencoder) based models, when the data set size is small, using both machine
learning benchmarks and real-world recommender systems and health-care
applications. Moreover, based on BELGAM, Icebreaker further improves the
performance and demonstrate the ability to use minimum amount of the training
data to obtain the highest test time performance.",arxiv
http://arxiv.org/abs/1807.03043v5,2019-02-25T21:06:08Z,2018-07-09T11:12:16Z,Convolutional Recurrent Neural Networks for Glucose Prediction,"Control of blood glucose is essential for diabetes management. Current
digital therapeutic approaches for subjects with Type 1 diabetes mellitus
(T1DM) such as the artificial pancreas and insulin bolus calculators leverage
machine learning techniques for predicting subcutaneous glucose for improved
control. Deep learning has recently been applied in healthcare and medical
research to achieve state-of-the-art results in a range of tasks including
disease diagnosis, and patient state prediction among others. In this work, we
present a deep learning model that is capable of forecasting glucose levels
with leading accuracy for simulated patient cases (RMSE = 9.38$\pm$0.71 [mg/dL]
over a 30-minute horizon, RMSE = 18.87$\pm$2.25 [mg/dL] over a 60-minute
horizon) and real patient cases (RMSE = 21.07$\pm$2.35 [mg/dL] for 30-minute,
RMSE = 33.27$\pm$4.79\% for 60-minute). In addition, the model provides
competitive performance in providing effective prediction horizon ($PH_{eff}$)
with minimal time lag both in a simulated patient dataset ($PH_{eff}$ =
29.0$\pm$0.7 for 30-min and $PH_{eff}$ = 49.8$\pm$2.9 for 60-min) and in a real
patient dataset ($PH_{eff}$ = 19.3$\pm$3.1 for 30-min and $PH_{eff}$ =
29.3$\pm$9.4 for 60-min). This approach is evaluated on a dataset of 10
simulated cases generated from the UVa/Padova simulator and a clinical dataset
of 10 real cases each containing glucose readings, insulin bolus, and meal
(carbohydrate) data. Performance of the recurrent convolutional neural network
is benchmarked against four algorithms. The proposed algorithm is implemented
on an Android mobile phone, with an execution time of $6$ms on a phone compared
to an execution time of $780$ms on a laptop.",arxiv
http://arxiv.org/abs/2109.14885v1,2021-09-30T07:05:20Z,2021-09-30T07:05:20Z,"Out-of-Distribution Detection for Medical Applications: Guidelines for
  Practical Evaluation","Detection of Out-of-Distribution (OOD) samples in real time is a crucial
safety check for deployment of machine learning models in the medical field.
Despite a growing number of uncertainty quantification techniques, there is a
lack of evaluation guidelines on how to select OOD detection methods in
practice. This gap impedes implementation of OOD detection methods for
real-world applications. Here, we propose a series of practical considerations
and tests to choose the best OOD detector for a specific medical dataset. These
guidelines are illustrated on a real-life use case of Electronic Health Records
(EHR). Our results can serve as a guide for implementation of OOD detection
methods in clinical practice, mitigating risks associated with the use of
machine learning models in healthcare.",arxiv
http://arxiv.org/abs/1912.12397v1,2019-12-28T04:05:15Z,2019-12-28T04:05:15Z,"Natural language processing of MIMIC-III clinical notes for identifying
  diagnosis and procedures with neural networks","Coding diagnosis and procedures in medical records is a crucial process in
the healthcare industry, which includes the creation of accurate billings,
receiving reimbursements from payers, and creating standardized patient care
records. In the United States, Billing and Insurance related activities cost
around $471 billion in 2012 which constitutes about 25% of all the U.S hospital
spending. In this paper, we report the performance of a natural language
processing model that can map clinical notes to medical codes, and predict
final diagnosis from unstructured entries of history of present illness,
symptoms at the time of admission, etc. Previous studies have demonstrated that
deep learning models perform better at such mapping when compared to
conventional machine learning models. Therefore, we employed state-of-the-art
deep learning method, ULMFiT on the largest emergency department clinical notes
dataset MIMIC III which has 1.2M clinical notes to select for the top-10 and
top-50 diagnosis and procedure codes. Our models were able to predict the
top-10 diagnoses and procedures with 80.3% and 80.5% accuracy, whereas the
top-50 ICD-9 codes of diagnosis and procedures are predicted with 70.7% and
63.9% accuracy. Prediction of diagnosis and procedures from unstructured
clinical notes benefit human coders to save time, eliminate errors and minimize
costs. With promising scores from our present model, the next step would be to
deploy this on a small-scale real-world scenario and compare it with human
coders as the gold standard. We believe that further research of this approach
can create highly accurate predictions that can ease the workflow in a clinical
setting.",arxiv
http://arxiv.org/abs/2008.02567v1,2020-08-06T10:51:56Z,2020-08-06T10:51:56Z,"An Intelligent Non-Invasive Real Time Human Activity Recognition System
  for Next-Generation Healthcare","Human motion detection is getting considerable attention in the field of
Artificial Intelligence (AI) driven healthcare systems. Human motion can be
used to provide remote healthcare solutions for vulnerable people by
identifying particular movements such as falls, gait and breathing disorders.
This can allow people to live more independent lifestyles and still have the
safety of being monitored if more direct care is needed. At present wearable
devices can provide real time monitoring by deploying equipment on a person's
body. However, putting devices on a person's body all the time make it
uncomfortable and the elderly tends to forget it to wear as well in addition to
the insecurity of being tracked all the time. This paper demonstrates how human
motions can be detected in quasi-real-time scenario using a non-invasive
method. Patterns in the wireless signals presents particular human body motions
as each movement induces a unique change in the wireless medium. These changes
can be used to identify particular body motions. This work produces a dataset
that contains patterns of radio wave signals obtained using software defined
radios (SDRs) to establish if a subject is standing up or sitting down as a
test case. The dataset was used to create a machine learning model, which was
used in a developed application to provide a quasi-real-time classification of
standing or sitting state. The machine learning model was able to achieve 96.70
% accuracy using the Random Forest algorithm using 10 fold cross validation. A
benchmark dataset of wearable devices was compared to the proposed dataset and
results showed the proposed dataset to have similar accuracy of nearly 90 %.
The machine learning models developed in this paper are tested for two
activities but the developed system is designed and applicable for detecting
and differentiating x number of activities.",arxiv
http://arxiv.org/abs/1809.01852v3,2019-03-07T04:27:19Z,2018-09-06T07:30:13Z,"GAMENet: Graph Augmented MEmory Networks for Recommending Medication
  Combination","Recent progress in deep learning is revolutionizing the healthcare domain
including providing solutions to medication recommendations, especially
recommending medication combination for patients with complex health
conditions. Existing approaches either do not customize based on patient health
history, or ignore existing knowledge on drug-drug interactions (DDI) that
might lead to adverse outcomes. To fill this gap, we propose the Graph
Augmented Memory Networks (GAMENet), which integrates the drug-drug
interactions knowledge graph by a memory module implemented as a graph
convolutional networks, and models longitudinal patient records as the query.
It is trained end-to-end to provide safe and personalized recommendation of
medication combination. We demonstrate the effectiveness and safety of GAMENet
by comparing with several state-of-the-art methods on real EHR data. GAMENet
outperformed all baselines in all effectiveness measures, and also achieved
3.60% DDI rate reduction from existing EHR data.",arxiv
http://arxiv.org/abs/2111.01338v2,2021-11-03T14:49:34Z,2021-11-02T02:54:30Z,"Federated Split Vision Transformer for COVID-19 CXR Diagnosis using
  Task-Agnostic Training","Federated learning, which shares the weights of the neural network across
clients, is gaining attention in the healthcare sector as it enables training
on a large corpus of decentralized data while maintaining data privacy. For
example, this enables neural network training for COVID-19 diagnosis on chest
X-ray (CXR) images without collecting patient CXR data across multiple
hospitals. Unfortunately, the exchange of the weights quickly consumes the
network bandwidth if highly expressive network architecture is employed.
So-called split learning partially solves this problem by dividing a neural
network into a client and a server part, so that the client part of the network
takes up less extensive computation resources and bandwidth. However, it is not
clear how to find the optimal split without sacrificing the overall network
performance. To amalgamate these methods and thereby maximize their distinct
strengths, here we show that the Vision Transformer, a recently developed deep
learning architecture with straightforward decomposable configuration, is
ideally suitable for split learning without sacrificing performance. Even under
the non-independent and identically distributed data distribution which
emulates a real collaboration between hospitals using CXR datasets from
multiple sources, the proposed framework was able to attain performance
comparable to data-centralized training. In addition, the proposed framework
along with heterogeneous multi-task clients also improves individual task
performances including the diagnosis of COVID-19, eliminating the need for
sharing large weights with innumerable parameters. Our results affirm the
suitability of Transformer for collaborative learning in medical imaging and
pave the way forward for future real-world implementations.",arxiv
http://arxiv.org/abs/2104.09164v1,2021-04-19T09:41:32Z,2021-04-19T09:41:32Z,"HEAR: Human Action Recognition via Neural Networks on Homomorphically
  Encrypted Data","Remote monitoring to support ""aging in place"" is an active area of research.
Advanced computer vision technology based on deep learning can provide near
real-time home monitoring to detect falling and symptoms related to seizure,
and stroke. Affordable webcams, together with cloud computing services (to run
machine learning algorithms), can potentially bring significant social and
health benefits. However, it has not been deployed in practice because of
privacy and security concerns. People may feel uncomfortable sending their
videos of daily activities (with potentially sensitive private information) to
a computing service provider (e.g., on a commercial cloud). In this paper, we
propose a novel strategy to resolve this dilemma by applying fully homomorphic
encryption (FHE) to an alternative representation of human actions (i.e.,
skeleton joints), which guarantees information confidentiality while retaining
high-performance action detection at a low cost. We design an FHE-friendly
neural network for action recognition and present a secure neural network
evaluation strategy to achieve near real-time action detection. Our framework
for private inference achieves an 87.99% recognition accuracy (86.21%
sensitivity and 99.14% specificity in detecting falls) with a latency of 3.1
seconds on real-world datasets. Our evaluation shows that our elaborated and
fine-tuned method reduces the inference latency by 23.81%~74.67% over a
straightforward implementation.",arxiv
http://arxiv.org/abs/1912.02102v1,2019-12-03T02:11:50Z,2019-12-03T02:11:50Z,"Artificial Intelligence for Low-Resource Communities: Influence
  Maximization in an Uncertain World","The potential of Artificial Intelligence (AI) to tackle challenging problems
that afflict society is enormous, particularly in the areas of healthcare,
conservation and public safety and security. Many problems in these domains
involve harnessing social networks of under-served communities to enable
positive change, e.g., using social networks of homeless youth to raise
awareness about Human Immunodeficiency Virus (HIV) and other STDs.
Unfortunately, most of these real-world problems are characterized by
uncertainties about social network structure and influence models, and previous
research in AI fails to sufficiently address these uncertainties. This thesis
addresses these shortcomings by advancing the state-of-the-art to a new
generation of algorithms for interventions in social networks. In particular,
this thesis describes the design and development of new influence maximization
algorithms which can handle various uncertainties that commonly exist in
real-world social networks. These algorithms utilize techniques from sequential
planning problems and social network theory to develop new kinds of AI
algorithms. Further, this thesis also demonstrates the real-world impact of
these algorithms by describing their deployment in three pilot studies to
spread awareness about HIV among actual homeless youth in Los Angeles. This
represents one of the first-ever deployments of computer science based
influence maximization algorithms in this domain. Our results show that our AI
algorithms improved upon the state-of-the-art by 160% in the real-world. We
discuss research and implementation challenges faced in deploying these
algorithms, and lessons that can be gleaned for future deployment of such
algorithms. The positive results from these deployments illustrate the enormous
potential of AI in addressing societally relevant problems.",arxiv
http://arxiv.org/abs/1801.09271v1,2018-01-28T19:29:50Z,2018-01-28T19:29:50Z,"Deep Reinforcement Learning for Dynamic Treatment Regimes on Medical
  Registry Data","This paper presents the first deep reinforcement learning (DRL) framework to
estimate the optimal Dynamic Treatment Regimes from observational medical data.
This framework is more flexible and adaptive for high dimensional action and
state spaces than existing reinforcement learning methods to model real-life
complexity in heterogeneous disease progression and treatment choices, with the
goal of providing doctor and patients the data-driven personalized decision
recommendations. The proposed DRL framework comprises (i) a supervised learning
step to predict the most possible expert actions, and (ii) a deep reinforcement
learning step to estimate the long-term value function of Dynamic Treatment
Regimes. Both steps depend on deep neural networks.
  As a key motivational example, we have implemented the proposed framework on
a data set from the Center for International Bone Marrow Transplant Research
(CIBMTR) registry database, focusing on the sequence of prevention and
treatments for acute and chronic graft versus host disease after
transplantation. In the experimental results, we have demonstrated promising
accuracy in predicting human experts' decisions, as well as the high expected
reward function in the DRL-based dynamic treatment regimes.",arxiv
http://arxiv.org/abs/2104.14870v1,2021-04-30T09:53:28Z,2021-04-30T09:53:28Z,"Action in Mind: A Neural Network Approach to Action Recognition and
  Segmentation","Recognizing and categorizing human actions is an important task with
applications in various fields such as human-robot interaction, video analysis,
surveillance, video retrieval, health care system and entertainment industry.
This thesis presents a novel computational approach for human action
recognition through different implementations of multi-layer architectures
based on artificial neural networks. Each system level development is designed
to solve different aspects of the action recognition problem including online
real-time processing, action segmentation and the involvement of objects. The
analysis of the experimental results are illustrated and described in six
articles. The proposed action recognition architecture of this thesis is
composed of several processing layers including a preprocessing layer, an
ordered vector representation layer and three layers of neural networks. It
utilizes self-organizing neural networks such as Kohonen feature maps and
growing grids as the main neural network layers. Thus the architecture presents
a biological plausible approach with certain features such as topographic
organization of the neurons, lateral interactions, semi-supervised learning and
the ability to represent high dimensional input space in lower dimensional
maps. For each level of development the system is trained with the input data
consisting of consecutive 3D body postures and tested with generalized input
data that the system has never met before. The experimental results of
different system level developments show that the system performs well with
quite high accuracy for recognizing human actions.",arxiv
http://arxiv.org/abs/1907.07296v4,2019-10-03T19:38:48Z,2019-07-17T00:50:37Z,"Explaining Vulnerabilities to Adversarial Machine Learning through
  Visual Analytics","Machine learning models are currently being deployed in a variety of
real-world applications where model predictions are used to make decisions
about healthcare, bank loans, and numerous other critical tasks. As the
deployment of artificial intelligence technologies becomes ubiquitous, it is
unsurprising that adversaries have begun developing methods to manipulate
machine learning models to their advantage. While the visual analytics
community has developed methods for opening the black box of machine learning
models, little work has focused on helping the user understand their model
vulnerabilities in the context of adversarial attacks. In this paper, we
present a visual analytics framework for explaining and exploring model
vulnerabilities to adversarial attacks. Our framework employs a multi-faceted
visualization scheme designed to support the analysis of data poisoning attacks
from the perspective of models, data instances, features, and local structures.
We demonstrate our framework through two case studies on binary classifiers and
illustrate model vulnerabilities with respect to varying attack strategies.",arxiv
http://arxiv.org/abs/2110.12900v1,2021-10-21T01:38:35Z,2021-10-21T01:38:35Z,"Automated Scoring System of HER2 in Pathological Images under the
  Microscope","Breast cancer is the most common cancer among women worldwide. The human
epidermal growth factor receptor 2(HER2) with immunohistochemical(IHC) is
widely used for pathological evaluation to provide the appropriate therapy for
patients with breast cancer. However, the deficiency of pathologists is
extremely significant in the current society, and visual diagnosis of the HER2
overexpression is subjective and susceptible to inter-observer variation.
Recently, with the rapid development of artificial intelligence(AI) in disease
diagnosis, several automated HER2 scoring methods using traditional computer
vision or machine learning methods indicate the improvement of the HER2
diagnostic accuracy, but the unreasonable interpretation in pathology, as well
as the expensive and ethical issues for annotation, make these methods still
have a long way to deploy in hospitals to ease pathologists' burden in real. In
this paper, we propose a HER2 automated scoring system that strictly follows
the HER2 scoring guidelines simulating the real workflow of HER2 scores
diagnosis by pathologists. Unlike the previous work, our method takes the
positive control of HER2 into account to make sure the assay performance for
each slide, eliminating work for repeated comparison and checking for the
current field of view(FOV) and positive control FOV, especially for the
borderline cases. Besides, for each selected FOV under the microscope, our
system provides real-time HER2 scores analysis and visualizations of the
membrane staining intensity and completeness corresponding with the cell
classification. Our rigorous workflow along with the flexible interactive
adjustion in demand substantially assists pathologists to finish the HER2
diagnosis faster and improves the robustness and accuracy. The proposed system
will be embedded in our Thorough Eye platform for deployment in hospitals.",arxiv
http://arxiv.org/abs/2106.01613v1,2021-06-03T06:20:18Z,2021-06-03T06:20:18Z,"NODE-GAM: Neural Generalized Additive Model for Interpretable Deep
  Learning","Deployment of machine learning models in real high-risk settings (e.g.
healthcare) often depends not only on model's accuracy but also on its
fairness, robustness and interpretability. Generalized Additive Models (GAMs)
have a long history of use in these high-risk domains, but lack desirable
features of deep learning such as differentiability and scalability. In this
work, we propose a neural GAM (NODE-GAM) and neural GA$^2$M (NODE-GA$^2$M) that
scale well to large datasets, while remaining interpretable and accurate. We
show that our proposed models have comparable accuracy to other
non-interpretable models, and outperform other GAMs on large datasets. We also
show that our models are more accurate in self-supervised learning setting when
access to labeled data is limited.",arxiv
http://arxiv.org/abs/1712.01785v3,2017-12-16T00:30:53Z,2017-12-05T17:49:18Z,"Towards Practical Verification of Machine Learning: The Case of Computer
  Vision Systems","Due to the increasing usage of machine learning (ML) techniques in security-
and safety-critical domains, such as autonomous systems and medical diagnosis,
ensuring correct behavior of ML systems, especially for different corner cases,
is of growing importance. In this paper, we propose a generic framework for
evaluating security and robustness of ML systems using different real-world
safety properties. We further design, implement and evaluate VeriVis, a
scalable methodology that can verify a diverse set of safety properties for
state-of-the-art computer vision systems with only blackbox access. VeriVis
leverage different input space reduction techniques for efficient verification
of different safety properties. VeriVis is able to find thousands of safety
violations in fifteen state-of-the-art computer vision systems including ten
Deep Neural Networks (DNNs) such as Inception-v3 and Nvidia's Dave self-driving
system with thousands of neurons as well as five commercial third-party vision
APIs including Google vision and Clarifai for twelve different safety
properties. Furthermore, VeriVis can successfully verify local safety
properties, on average, for around 31.7% of the test images. VeriVis finds up
to 64.8x more violations than existing gradient-based methods that, unlike
VeriVis, cannot ensure non-existence of any violations. Finally, we show that
retraining using the safety violations detected by VeriVis can reduce the
average number of violations up to 60.2%.",arxiv
http://arxiv.org/abs/2101.10869v2,2021-01-29T06:26:14Z,2021-01-23T09:49:33Z,"A Raspberry Pi-based Traumatic Brain Injury Detection System for
  Single-Channel Electroencephalogram","Traumatic Brain Injury (TBI) is a common cause of death and disability.
However, existing tools for TBI diagnosis are either subjective or require
extensive clinical setup and expertise. The increasing affordability and
reduction in size of relatively high-performance computing systems combined
with promising results from TBI related machine learning research make it
possible to create compact and portable systems for early detection of TBI.
This work describes a Raspberry Pi based portable, real-time data acquisition,
and automated processing system that uses machine learning to efficiently
identify TBI and automatically score sleep stages from a single-channel
Electroen-cephalogram (EEG) signal. We discuss the design, implementation, and
verification of the system that can digitize EEG signal using an Analog to
Digital Converter (ADC) and perform real-time signal classification to detect
the presence of mild TBI (mTBI). We utilize Convolutional Neural Networks (CNN)
and XGBoost based predictive models to evaluate the performance and demonstrate
the versatility of the system to operate with multiple types of predictive
models. We achieve a peak classification accuracy of more than 90% with a
classification time of less than 1 s across 16 s - 64 s epochs for TBI vs
control conditions. This work can enable development of systems suitable for
field use without requiring specialized medical equipment for early TBI
detection applications and TBI research. Further, this work opens avenues to
implement connected, real-time TBI related health and wellness monitoring
systems.",arxiv
http://arxiv.org/abs/2110.15127v1,2021-10-28T14:02:16Z,2021-10-28T14:02:16Z,"Lightweight Mobile Automated Assistant-to-physician for Global
  Lower-resource Areas","Importance: Lower-resource areas in Africa and Asia face a unique set of
healthcare challenges: the dual high burden of communicable and
non-communicable diseases; a paucity of highly trained primary healthcare
providers in both rural and densely populated urban areas; and a lack of
reliable, inexpensive internet connections. Objective: To address these
challenges, we designed an artificial intelligence assistant to help primary
healthcare providers in lower-resource areas document demographic and medical
sign/symptom data and to record and share diagnostic data in real-time with a
centralized database. Design: We trained our system using multiple data sets,
including US-based electronic medical records (EMRs) and open-source medical
literature and developed an adaptive, general medical assistant system based on
machine learning algorithms. Main outcomes and Measure: The application
collects basic information from patients and provides primary care providers
with diagnoses and prescriptions suggestions. The application is unique from
existing systems in that it covers a wide range of common diseases, signs, and
medication typical in lower-resource countries; the application works with or
without an active internet connection. Results: We have built and implemented
an adaptive learning system that assists trained primary care professionals by
means of an Android smartphone application, which interacts with a central
database and collects real-time data. The application has been tested by dozens
of primary care providers. Conclusions and Relevance: Our application would
provide primary healthcare providers in lower-resource areas with a tool that
enables faster and more accurate documentation of medical encounters. This
application could be leveraged to automatically populate local or national EMR
systems.",arxiv
http://arxiv.org/abs/1804.00308v3,2021-09-28T14:55:41Z,2018-04-01T15:56:43Z,"Manipulating Machine Learning: Poisoning Attacks and Countermeasures for
  Regression Learning","As machine learning becomes widely used for automated decisions, attackers
have strong incentives to manipulate the results and models generated by
machine learning algorithms. In this paper, we perform the first systematic
study of poisoning attacks and their countermeasures for linear regression
models. In poisoning attacks, attackers deliberately influence the training
data to manipulate the results of a predictive model. We propose a
theoretically-grounded optimization framework specifically designed for linear
regression and demonstrate its effectiveness on a range of datasets and models.
We also introduce a fast statistical attack that requires limited knowledge of
the training process. Finally, we design a new principled defense method that
is highly resilient against all poisoning attacks. We provide formal guarantees
about its convergence and an upper bound on the effect of poisoning attacks
when the defense is deployed. We evaluate extensively our attacks and defenses
on three realistic datasets from health care, loan assessment, and real estate
domains.",arxiv
http://arxiv.org/abs/2103.15908v2,2021-03-31T00:46:39Z,2021-03-29T19:38:04Z,"pH-RL: A personalization architecture to bring reinforcement learning to
  health practice","While reinforcement learning (RL) has proven to be the approach of choice for
tackling many complex problems, it remains challenging to develop and deploy RL
agents in real-life scenarios successfully. This paper presents pH-RL
(personalization in e-Health with RL) a general RL architecture for
personalization to bring RL to health practice. pH-RL allows for various levels
of personalization in health applications and allows for online and batch
learning. Furthermore, we provide a general-purpose implementation framework
that can be integrated with various healthcare applications. We describe a
step-by-step guideline for the successful deployment of RL policies in a mobile
application. We implemented our open-source RL architecture and integrated it
with the MoodBuster mobile application for mental health to provide messages to
increase daily adherence to the online therapeutic modules. We then performed a
comprehensive study with human participants over a sustained period. Our
experimental results show that the developed policies learn to select
appropriate actions consistently using only a few days' worth of data.
Furthermore, we empirically demonstrate the stability of the learned policies
during the study.",arxiv
http://arxiv.org/abs/2103.16898v1,2021-03-31T08:31:07Z,2021-03-31T08:31:07Z,"Perun: Secure Multi-Stakeholder Machine Learning Framework with GPU
  Support","Confidential multi-stakeholder machine learning (ML) allows multiple parties
to perform collaborative data analytics while not revealing their intellectual
property, such as ML source code, model, or datasets. State-of-the-art
solutions based on homomorphic encryption incur a large performance overhead.
Hardware-based solutions, such as trusted execution environments (TEEs),
significantly improve the performance in inference computations but still
suffer from low performance in training computations, e.g., deep neural
networks model training, because of limited availability of protected memory
and lack of GPU support.
  To address this problem, we designed and implemented Perun, a framework for
confidential multi-stakeholder machine learning that allows users to make a
trade-off between security and performance. Perun executes ML training on
hardware accelerators (e.g., GPU) while providing security guarantees using
trusted computing technologies, such as trusted platform module and integrity
measurement architecture. Less compute-intensive workloads, such as inference,
execute only inside TEE, thus at a lower trusted computing base. The evaluation
shows that during the ML training on CIFAR-10 and real-world medical datasets,
Perun achieved a 161x to 1560x speedup compared to a pure TEE-based approach.",arxiv
http://arxiv.org/abs/2002.05229v1,2020-02-12T20:35:31Z,2020-02-12T20:35:31Z,"Data Efficient Training for Reinforcement Learning with Adaptive
  Behavior Policy Sharing","Deep Reinforcement Learning (RL) is proven powerful for decision making in
simulated environments. However, training deep RL model is challenging in real
world applications such as production-scale health-care or recommender systems
because of the expensiveness of interaction and limitation of budget at
deployment. One aspect of the data inefficiency comes from the expensive
hyper-parameter tuning when optimizing deep neural networks. We propose
Adaptive Behavior Policy Sharing (ABPS), a data-efficient training algorithm
that allows sharing of experience collected by behavior policy that is
adaptively selected from a pool of agents trained with an ensemble of
hyper-parameters. We further extend ABPS to evolve hyper-parameters during
training by hybridizing ABPS with an adapted version of Population Based
Training (ABPS-PBT). We conduct experiments with multiple Atari games with up
to 16 hyper-parameter/architecture setups. ABPS achieves superior overall
performance, reduced variance on top 25% agents, and equivalent performance on
the best agent compared to conventional hyper-parameter tuning with independent
training, even though ABPS only requires the same number of environmental
interactions as training a single agent. We also show that ABPS-PBT further
improves the convergence speed and reduces the variance.",arxiv
http://arxiv.org/abs/1905.02940v1,2019-05-08T07:26:27Z,2019-05-08T07:26:27Z,"A new direction to promote the implementation of artificial intelligence
  in natural clinical settings","Artificial intelligence (AI) researchers claim that they have made great
`achievements' in clinical realms. However, clinicians point out the so-called
`achievements' have no ability to implement into natural clinical settings. The
root cause for this huge gap is that many essential features of natural
clinical tasks are overlooked by AI system developers without medical
background. In this paper, we propose that the clinical benchmark suite is a
novel and promising direction to capture the essential features of the
real-world clinical tasks, hence qualifies itself for guiding the development
of AI systems, promoting the implementation of AI in real-world clinical
practice.",arxiv
http://arxiv.org/abs/1906.00108v1,2019-05-31T22:28:40Z,2019-05-31T22:28:40Z,"ActiveHARNet: Towards On-Device Deep Bayesian Active Learning for Human
  Activity Recognition","Various health-care applications such as assisted living, fall detection
etc., require modeling of user behavior through Human Activity Recognition
(HAR). HAR using mobile- and wearable-based deep learning algorithms have been
on the rise owing to the advancements in pervasive computing. However, there
are two other challenges that need to be addressed: first, the deep learning
model should support on-device incremental training (model updation) from
real-time incoming data points to learn user behavior over time, while also
being resource-friendly; second, a suitable ground truthing technique (like
Active Learning) should help establish labels on-the-fly while also selecting
only the most informative data points to query from an oracle. Hence, in this
paper, we propose ActiveHARNet, a resource-efficient deep ensembled model which
supports on-device Incremental Learning and inference, with capabilities to
represent model uncertainties through approximations in Bayesian Neural
Networks using dropout. This is combined with suitable acquisition functions
for active learning. Empirical results on two publicly available wrist-worn HAR
and fall detection datasets indicate that ActiveHARNet achieves considerable
efficiency boost during inference across different users, with a substantially
low number of acquired pool points (at least 60% reduction) during incremental
learning on both datasets experimented with various acquisition functions, thus
demonstrating deployment and Incremental Learning feasibility.",arxiv
http://arxiv.org/abs/2107.11263v1,2021-07-23T14:22:26Z,2021-07-23T14:22:26Z,Compressed Ultrasound Imaging:from Sub-Nyquist Rates to Super-Resolution,"The multi-billion dollar, worldwide medical ultrasound (US) market continues
to grow annually. Its non-ionizing nature, real-time capabilities and
relatively low cost, compared to other imaging modalities, have led to
significant applications in many different fields, including cardiology,
angiology, obstetrics and emergency medicine. Facilitated by ongoing
innovations, US continues to change rules and norms regarding patient
screening, diagnosis and surgery. This huge and promising market is constantly
driven by new imaging and processing techniques. From 3D images to
sophisticated software, hardware and portability improvements, it is clear that
the status of US as one of the leading medical imaging technologies is ensured
for many years ahead. However, as imaging systems evolve, new engineering
challenges emerge. Acquisition, transmission and processing of huge amounts of
data are common for all ultrasound-based imaging modalities. Moreover,
achieving higher resolution is constantly on demand, as improved diagnosis
could be achieved by better visualization of organs and blood vessels deep
within tissues. In this article, our goal is to motivate further interest and
research in emerging processing techniques, as well as their applications in
medical ultrasound, enabled by recent advancements in signal processing
algorithms and deep learning. We address some of the primary challenges and
potential remedies from a signal processing perspective, by exploiting the
inherent structure of the received US signal.",arxiv
http://arxiv.org/abs/2011.00739v2,2021-04-06T17:11:52Z,2020-10-30T17:32:18Z,"Mutual Information-based Disentangled Neural Networks for Classifying
  Unseen Categories in Different Domains: Application to Fetal Ultrasound
  Imaging","Deep neural networks exhibit limited generalizability across images with
different entangled domain features and categorical features. Learning
generalizable features that can form universal categorical decision boundaries
across domains is an interesting and difficult challenge. This problem occurs
frequently in medical imaging applications when attempts are made to deploy and
improve deep learning models across different image acquisition devices, across
acquisition parameters or if some classes are unavailable in new training
databases. To address this problem, we propose Mutual Information-based
Disentangled Neural Networks (MIDNet), which extract generalizable categorical
features to transfer knowledge to unseen categories in a target domain. The
proposed MIDNet adopts a semi-supervised learning paradigm to alleviate the
dependency on labeled data. This is important for real-world applications where
data annotation is time-consuming, costly and requires training and expertise.
We extensively evaluate the proposed method on fetal ultrasound datasets for
two different image classification tasks where domain features are respectively
defined by shadow artifacts and image acquisition devices. Experimental results
show that the proposed method outperforms the state-of-the-art on the
classification of unseen categories in a target domain with sparsely labeled
training data.",arxiv
http://arxiv.org/abs/2005.12573v3,2021-05-08T11:45:24Z,2020-05-26T08:46:32Z,"Learning Global and Local Features of Normal Brain Anatomy for
  Unsupervised Abnormality Detection","In real-world clinical practice, overlooking unanticipated findings can
result in serious consequences. However, supervised learning, which is the
foundation for the current success of deep learning, only encourages models to
identify abnormalities that are defined in datasets in advance. Therefore,
abnormality detection must be implemented in medical images that are not
limited to a specific disease category. In this study, we demonstrate an
unsupervised learning framework for pixel-wise abnormality detection in brain
magnetic resonance imaging captured from a patient population with metastatic
brain tumor. Our concept is as follows: If an image reconstruction network can
faithfully reproduce the global features of normal anatomy, then the abnormal
lesions in unseen images can be identified based on the local difference from
those reconstructed as normal by a discriminative network. Both networks are
trained on a dataset comprising only normal images without labels. In addition,
we devise a metric to evaluate the anatomical fidelity of the reconstructed
images and confirm that the overall detection performance is improved when the
image reconstruction network achieves a higher score. For evaluation,
clinically significant abnormalities are comprehensively segmented. The results
show that the area under the receiver operating characteristics curve values
for metastatic brain tumors, extracranial metastatic tumors, postoperative
cavities, and structural changes are 0.78, 0.61, 0.91, and 0.60, respectively.",arxiv
http://arxiv.org/abs/2011.03395v2,2020-11-24T19:16:02Z,2020-11-06T14:53:13Z,"Underspecification Presents Challenges for Credibility in Modern Machine
  Learning","ML models often exhibit unexpectedly poor behavior when they are deployed in
real-world domains. We identify underspecification as a key reason for these
failures. An ML pipeline is underspecified when it can return many predictors
with equivalently strong held-out performance in the training domain.
Underspecification is common in modern ML pipelines, such as those based on
deep learning. Predictors returned by underspecified pipelines are often
treated as equivalent based on their training domain performance, but we show
here that such predictors can behave very differently in deployment domains.
This ambiguity can lead to instability and poor model behavior in practice, and
is a distinct failure mode from previously identified issues arising from
structural mismatch between training and deployment domains. We show that this
problem appears in a wide variety of practical ML pipelines, using examples
from computer vision, medical imaging, natural language processing, clinical
risk prediction based on electronic health records, and medical genomics. Our
results show the need to explicitly account for underspecification in modeling
pipelines that are intended for real-world deployment in any domain.",arxiv
http://arxiv.org/abs/1912.09848v1,2019-12-20T14:35:49Z,2019-12-20T14:35:49Z,"Prediction of Physical Load Level by Machine Learning Analysis of Heart
  Activity after Exercises","The assessment of energy expenditure in real life is of great importance for
monitoring the current physical state of people, especially in work, sport,
elderly care, health care, and everyday life even. This work reports about
application of some machine learning methods (linear regression, linear
discriminant analysis, k-nearest neighbors, decision tree, random forest,
Gaussian naive Bayes, support-vector machine) for monitoring energy
expenditures in athletes. The classification problem was to predict the known
level of the in-exercise loads (in three categories by calories) by the heart
rate activity features measured during the short period of time (1 minute only)
after training, i.e by features of the post-exercise load. The results obtained
shown that the post-exercise heart activity features preserve the information
of the in-exercise training loads and allow us to predict their actual
in-exercise levels. The best performance can be obtained by the random forest
classifier with all 8 heart rate features (micro-averaged area under curve
value AUCmicro = 0.87 and macro-averaged one AUCmacro = 0.88) and the k-nearest
neighbors classifier with 4 most important heart rate features (AUCmicro = 0.91
and AUCmacro = 0.89). The limitations and perspectives of the ML methods used
are outlined, and some practical advices are proposed as to their improvement
and implementation for the better prediction of in-exercise energy
expenditures.",arxiv
http://arxiv.org/abs/2012.10020v1,2020-12-18T02:37:49Z,2020-12-18T02:37:49Z,"EVA: Generating Longitudinal Electronic Health Records Using Conditional
  Variational Autoencoders","Researchers require timely access to real-world longitudinal electronic
health records (EHR) to develop, test, validate, and implement machine learning
solutions that improve the quality and efficiency of healthcare. In contrast,
health systems value deeply patient privacy and data security. De-identified
EHRs do not adequately address the needs of health systems, as de-identified
data are susceptible to re-identification and its volume is also limited.
Synthetic EHRs offer a potential solution. In this paper, we propose EHR
Variational Autoencoder (EVA) for synthesizing sequences of discrete EHR
encounters (e.g., clinical visits) and encounter features (e.g., diagnoses,
medications, procedures). We illustrate that EVA can produce realistic EHR
sequences, account for individual differences among patients, and can be
conditioned on specific disease conditions, thus enabling disease-specific
studies. We design efficient, accurate inference algorithms by combining
stochastic gradient Markov Chain Monte Carlo with amortized variational
inference. We assess the utility of the methods on large real-world EHR
repositories containing over 250, 000 patients. Our experiments, which include
user studies with knowledgeable clinicians, indicate the generated EHR
sequences are realistic. We confirmed the performance of predictive models
trained on the synthetic data are similar with those trained on real EHRs.
Additionally, our findings indicate that augmenting real data with synthetic
EHRs results in the best predictive performance - improving the best baseline
by as much as 8% in top-20 recall.",arxiv
http://arxiv.org/abs/2105.06791v2,2021-10-31T00:08:57Z,2021-05-14T12:16:47Z,"Agree to Disagree: When Deep Learning Models With Identical
  Architectures Produce Distinct Explanations","Deep Learning of neural networks has progressively become more prominent in
healthcare with models reaching, or even surpassing, expert accuracy levels.
However, these success stories are tainted by concerning reports on the lack of
model transparency and bias against some medical conditions or patients'
sub-groups. Explainable methods are considered the gateway to alleviate many of
these concerns. In this study we demonstrate that the generated explanations
are volatile to changes in model training that are perpendicular to the
classification task and model structure. This raises further questions about
trust in deep learning models for healthcare. Mainly, whether the models
capture underlying causal links in the data or just rely on spurious
correlations that are made visible via explanation methods. We demonstrate that
the output of explainability methods on deep neural networks can vary
significantly by changes of hyper-parameters, such as the random seed or how
the training set is shuffled. We introduce a measure of explanation consistency
which we use to highlight the identified problems on the MIMIC-CXR dataset. We
find explanations of identical models but with different training setups have a
low consistency: $\approx$ 33% on average. On the contrary, kernel methods are
robust against any orthogonal changes, with explanation consistency at 94%. We
conclude that current trends in model explanation are not sufficient to
mitigate the risks of deploying models in real life healthcare applications.",arxiv
http://arxiv.org/abs/2105.01401v1,2021-05-04T10:27:20Z,2021-05-04T10:27:20Z,"A Review of Confidentiality Threats Against Embedded Neural Network
  Models","Utilization of Machine Learning (ML) algorithms, especially Deep Neural
Network (DNN) models, becomes a widely accepted standard in many domains more
particularly IoT-based systems. DNN models reach impressive performances in
several sensitive fields such as medical diagnosis, smart transport or security
threat detection, and represent a valuable piece of Intellectual Property. Over
the last few years, a major trend is the large-scale deployment of models in a
wide variety of devices. However, this migration to embedded systems is slowed
down because of the broad spectrum of attacks threatening the integrity,
confidentiality and availability of embedded models. In this review, we cover
the landscape of attacks targeting the confidentiality of embedded DNN models
that may have a major impact on critical IoT systems, with a particular focus
on model extraction and data leakage. We highlight the fact that Side-Channel
Analysis (SCA) is a relatively unexplored bias by which model's confidentiality
can be compromised. Input data, architecture or parameters of a model can be
extracted from power or electromagnetic observations, testifying a real need
from a security point of view.",arxiv
http://arxiv.org/abs/2005.06342v1,2020-05-09T05:54:28Z,2020-05-09T05:54:28Z,"sCrop: A Internet-of-Agro-Things (IoAT) Enabled Solar Powered Smart
  Device for Automatic Plant Disease Prediction","Internet-of-Things (IoT) is omnipresent, ranging from home solutions to
turning wheels for the fourth industrial revolution. This article presents the
novel concept of Internet-of-Agro-Things (IoAT) with an example of automated
plant disease prediction. It consists of solar enabled sensor nodes which help
in continuous sensing and automating agriculture. The existing solutions have
implemented a battery powered sensor node. On the contrary, the proposed system
has adopted the use of an energy efficient way of powering using solar energy.
It is observed that around 80% of the crops are attacked with microbial
diseases in traditional agriculture. To prevent this, a health maintenance
system is integrated with the sensor node, which captures the image of the crop
and performs an analysis with the trained Convolutional Neural Network (CNN)
model. The deployment of the proposed system is demonstrated in a real-time
environment using a microcontroller, solar sensor nodes with a camera module,
and an mobile application for the farmers visualization of the farms. The
deployed prototype was deployed for two months and has achieved a robust
performance by sustaining in varied weather conditions and continued to remain
rust-free. The proposed deep learning framework for plant disease prediction
has achieved an accuracy of 99.2% testing accuracy.",arxiv
http://arxiv.org/abs/1906.04450v2,2019-08-14T17:52:37Z,2019-06-11T09:02:35Z,"Quantifying Intrinsic Uncertainty in Classification via Deep Dirichlet
  Mixture Networks","With the widespread success of deep neural networks in science and
technology, it is becoming increasingly important to quantify the uncertainty
of the predictions produced by deep learning. In this paper, we introduce a new
method that attaches an explicit uncertainty statement to the probabilities of
classification using deep neural networks. Precisely, we view that the
classification probabilities are sampled from an unknown distribution, and we
propose to learn this distribution through the Dirichlet mixture that is
flexible enough for approximating any continuous distribution on the simplex.
We then construct credible intervals from the learned distribution to assess
the uncertainty of the classification probabilities. Our approach is easy to
implement, computationally efficient, and can be coupled with any deep neural
network architecture. Our method leverages the crucial observation that, in
many classification applications such as medical diagnosis, more than one class
labels are available for each observational unit. We demonstrate the usefulness
of our approach through simulations and a real data example.",arxiv
http://arxiv.org/abs/2002.03763v2,2020-02-22T16:56:25Z,2020-02-03T22:58:28Z,Learning Numerical Observers using Unsupervised Domain Adaptation,"Medical imaging systems are commonly assessed by use of objective image
quality measures. Supervised deep learning methods have been investigated to
implement numerical observers for task-based image quality assessment. However,
labeling large amounts of experimental data to train deep neural networks is
tedious, expensive, and prone to subjective errors. Computer-simulated image
data can potentially be employed to circumvent these issues; however, it is
often difficult to computationally model complicated anatomical structures,
noise sources, and the response of real world imaging systems. Hence, simulated
image data will generally possess physical and statistical differences from the
experimental image data they seek to emulate. Within the context of machine
learning, these differences between the sets of two images is referred to as
domain shift. In this study, we propose and investigate the use of an
adversarial domain adaptation method to mitigate the deleterious effects of
domain shift between simulated and experimental image data for deep
learning-based numerical observers (DL-NOs) that are trained on simulated
images but applied to experimental ones. In the proposed method, a DL-NO will
initially be trained on computer-simulated image data and subsequently adapted
for use with experimental image data, without the need for any labeled
experimental images. As a proof of concept, a binary signal detection task is
considered. The success of this strategy as a function of the degree of domain
shift present between the simulated and experimental image data is
investigated.",arxiv
http://arxiv.org/abs/2101.05766v1,2021-01-14T18:17:11Z,2021-01-14T18:17:11Z,Ajalon: Simplifying the Authoring of Wearable Cognitive Assistants,"Wearable Cognitive Assistance (WCA) amplifies human cognition in real time
through a wearable device and low-latency wireless access to edge computing
infrastructure. It is inspired by, and broadens, the metaphor of GPS navigation
tools that provide real-time step-by-step guidance, with prompt error detection
and correction. WCA applications are likely to be transformative in education,
health care, industrial troubleshooting, manufacturing, and many other areas.
Today, WCA application development is difficult and slow, requiring skills in
areas such as machine learning and computer vision that are not widespread
among software developers. This paper describes Ajalon, an authoring toolchain
for WCA applications that reduces the skill and effort needed at each step of
the development pipeline. Our evaluation shows that Ajalon significantly
reduces the effort needed to create new WCA applications.",arxiv
http://arxiv.org/abs/2104.06910v1,2021-04-14T15:00:39Z,2021-04-14T15:00:39Z,"Towards a framework for evaluating the safety, acceptability and
  efficacy of AI systems for health: an initial synthesis","The potential presented by Artificial Intelligence (AI) for healthcare has
long been recognised by the technical community. More recently, this potential
has been recognised by policymakers, resulting in considerable public and
private investment in the development of AI for healthcare across the globe.
Despite this, excepting limited success stories, real-world implementation of
AI systems into front-line healthcare has been limited. There are numerous
reasons for this, but a main contributory factor is the lack of internationally
accepted, or formalised, regulatory standards to assess AI safety and impact
and effectiveness. This is a well-recognised problem with numerous ongoing
research and policy projects to overcome it. Our intention here is to
contribute to this problem-solving effort by seeking to set out a minimally
viable framework for evaluating the safety, acceptability and efficacy of AI
systems for healthcare. We do this by conducting a systematic search across
Scopus, PubMed and Google Scholar to identify all the relevant literature
published between January 1970 and November 2020 related to the evaluation of:
output performance; efficacy; and real-world use of AI systems, and
synthesising the key themes according to the stages of evaluation: pre-clinical
(theoretical phase); exploratory phase; definitive phase; and post-market
surveillance phase (monitoring). The result is a framework to guide AI system
developers, policymakers, and regulators through a sufficient evaluation of an
AI system designed for use in healthcare.",arxiv
http://arxiv.org/abs/2011.14966v1,2020-11-30T16:38:18Z,2020-11-30T16:38:18Z,"Depression Status Estimation by Deep Learning based Hybrid Multi-Modal
  Fusion Model","Preliminary detection of mild depression could immensely help in effective
treatment of the common mental health disorder. Due to the lack of proper
awareness and the ample mix of stigmas and misconceptions present within the
society, mental health status estimation has become a truly difficult task. Due
to the immense variations in character level traits from person to person,
traditional deep learning methods fail to generalize in a real world setting.
In our study we aim to create a human allied AI workflow which could
efficiently adapt to specific users and effectively perform in real world
scenarios. We propose a Hybrid deep learning approach that combines the essence
of one shot learning, classical supervised deep learning methods and human
allied interactions for adaptation. In order to capture maximum information and
make efficient diagnosis video, audio, and text modalities are utilized. Our
Hybrid Fusion model achieved a high accuracy of 96.3% on the Dataset; and
attained an AUC of 0.9682 which proves its robustness in discriminating classes
in complex real-world scenarios making sure that no cases of mild depression
are missed during diagnosis. The proposed method is deployed in a cloud-based
smartphone application for robust testing. With user-specific adaptations and
state of the art methodologies, we present a state-of-the-art model with user
friendly experience.",arxiv
http://arxiv.org/abs/1702.01780v1,2017-02-06T20:10:10Z,2017-02-06T20:10:10Z,"Toward the automated analysis of complex diseases in genome-wide
  association studies using genetic programming","Machine learning has been gaining traction in recent years to meet the demand
for tools that can efficiently analyze and make sense of the ever-growing
databases of biomedical data in health care systems around the world. However,
effectively using machine learning methods requires considerable domain
expertise, which can be a barrier of entry for bioinformaticians new to
computational data science methods. Therefore, off-the-shelf tools that make
machine learning more accessible can prove invaluable for bioinformaticians. To
this end, we have developed an open source pipeline optimization tool
(TPOT-MDR) that uses genetic programming to automatically design machine
learning pipelines for bioinformatics studies. In TPOT-MDR, we implement
Multifactor Dimensionality Reduction (MDR) as a feature construction method for
modeling higher-order feature interactions, and combine it with a new expert
knowledge-guided feature selector for large biomedical data sets. We
demonstrate TPOT-MDR's capabilities using a combination of simulated and real
world data sets from human genetics and find that TPOT-MDR significantly
outperforms modern machine learning methods such as logistic regression and
eXtreme Gradient Boosting (XGBoost). We further analyze the best pipeline
discovered by TPOT-MDR for a real world problem and highlight TPOT-MDR's
ability to produce a high-accuracy solution that is also easily interpretable.",arxiv
http://arxiv.org/abs/2001.11363v1,2020-01-29T17:23:16Z,2020-01-29T17:23:16Z,"REST: Robust and Efficient Neural Networks for Sleep Monitoring in the
  Wild","In recent years, significant attention has been devoted towards integrating
deep learning technologies in the healthcare domain. However, to safely and
practically deploy deep learning models for home health monitoring, two
significant challenges must be addressed: the models should be (1) robust
against noise; and (2) compact and energy-efficient. We propose REST, a new
method that simultaneously tackles both issues via 1) adversarial training and
controlling the Lipschitz constant of the neural network through spectral
regularization while 2) enabling neural network compression through sparsity
regularization. We demonstrate that REST produces highly-robust and efficient
models that substantially outperform the original full-sized models in the
presence of noise. For the sleep staging task over single-channel
electroencephalogram (EEG), the REST model achieves a macro-F1 score of 0.67
vs. 0.39 achieved by a state-of-the-art model in the presence of Gaussian noise
while obtaining 19x parameter reduction and 15x MFLOPS reduction on two large,
real-world EEG datasets. By deploying these models to an Android application on
a smartphone, we quantitatively observe that REST allows models to achieve up
to 17x energy reduction and 9x faster inference. We open-source the code
repository with this paper: https://github.com/duggalrahul/REST.",arxiv
http://arxiv.org/abs/2107.08189v1,2021-07-17T05:53:24Z,2021-07-17T05:53:24Z,"BEDS-Bench: Behavior of EHR-models under Distributional Shift--A
  Benchmark","Machine learning has recently demonstrated impressive progress in predictive
accuracy across a wide array of tasks. Most ML approaches focus on
generalization performance on unseen data that are similar to the training data
(In-Distribution, or IND). However, real world applications and deployments of
ML rarely enjoy the comfort of encountering examples that are always IND. In
such situations, most ML models commonly display erratic behavior on
Out-of-Distribution (OOD) examples, such as assigning high confidence to wrong
predictions, or vice-versa. Implications of such unusual model behavior are
further exacerbated in the healthcare setting, where patient health can
potentially be put at risk. It is crucial to study the behavior and robustness
properties of models under distributional shift, understand common failure
modes, and take mitigation steps before the model is deployed. Having a
benchmark that shines light upon these aspects of a model is a first and
necessary step in addressing the issue. Recent work and interest in increasing
model robustness in OOD settings have focused more on image modality, while the
Electronic Health Record (EHR) modality is still largely under-explored. We aim
to bridge this gap by releasing BEDS-Bench, a benchmark for quantifying the
behavior of ML models over EHR data under OOD settings. We use two open access,
de-identified EHR datasets to construct several OOD data settings to run tests
on, and measure relevant metrics that characterize crucial aspects of a model's
OOD behavior. We evaluate several learning algorithms under BEDS-Bench and find
that all of them show poor generalization performance under distributional
shift in general. Our results highlight the need and the potential to improve
robustness of EHR models under distributional shift, and BEDS-Bench provides
one way to measure progress towards that goal.",arxiv
http://arxiv.org/abs/2109.02915v1,2021-09-07T08:04:02Z,2021-09-07T08:04:02Z,"Few-shot Learning in Emotion Recognition of Spontaneous Speech Using a
  Siamese Neural Network with Adaptive Sample Pair Formation","Speech-based machine learning (ML) has been heralded as a promising solution
for tracking prosodic and spectrotemporal patterns in real-life that are
indicative of emotional changes, providing a valuable window into one's
cognitive and mental state. Yet, the scarcity of labelled data in ambulatory
studies prevents the reliable training of ML models, which usually rely on
""data-hungry"" distribution-based learning. Leveraging the abundance of labelled
speech data from acted emotions, this paper proposes a few-shot learning
approach for automatically recognizing emotion in spontaneous speech from a
small number of labelled samples. Few-shot learning is implemented via a metric
learning approach through a siamese neural network, which models the relative
distance between samples rather than relying on learning absolute patterns of
the corresponding distributions of each emotion. Results indicate the
feasibility of the proposed metric learning in recognizing emotions from
spontaneous speech in four datasets, even with a small amount of labelled
samples. They further demonstrate superior performance of the proposed metric
learning compared to commonly used adaptation methods, including network
fine-tuning and adversarial learning. Findings from this work provide a
foundation for the ambulatory tracking of human emotion in spontaneous speech
contributing to the real-life assessment of mental health degradation.",arxiv
http://arxiv.org/abs/1911.08090v1,2019-11-19T04:33:05Z,2019-11-19T04:33:05Z,Deep Detector Health Management under Adversarial Campaigns,"Machine learning models are vulnerable to adversarial inputs that induce
seemingly unjustifiable errors. As automated classifiers are increasingly used
in industrial control systems and machinery, these adversarial errors could
grow to be a serious problem. Despite numerous studies over the past few years,
the field of adversarial ML is still considered alchemy, with no practical
unbroken defenses demonstrated to date, leaving PHM practitioners with few
meaningful ways of addressing the problem. We introduce turbidity detection as
a practical superset of the adversarial input detection problem, coping with
adversarial campaigns rather than statistically invisible one-offs. This
perspective is coupled with ROC-theoretic design guidance that prescribes an
inexpensive domain adaptation layer at the output of a deep learning model
during an attack campaign. The result aims to approximate the Bayes optimal
mitigation that ameliorates the detection model's degraded health. A
proactively reactive type of prognostics is achieved via Monte Carlo simulation
of various adversarial campaign scenarios, by sampling from the model's own
turbidity distribution to quickly deploy the correct mitigation during a
real-world campaign.",arxiv
http://arxiv.org/abs/1905.10364v1,2019-05-24T09:19:42Z,2019-05-24T09:19:42Z,"Deep learning based high-resolution incoherent x-ray imaging with a
  single-pixel detector","X-ray ""ghost"" imaging has drawn great attention for its potential to lower
radiation dose in medical diagnosis. For practical implementation, however, the
efficiency and image quality have to be greatly improved. Here we demonstrate a
computational ghost imaging scheme where a bucket detector and specially
designed modulation masks are used, together with a new robust deep learning
algorithm in which a compressed set of Hadamard matrices is incorporated into a
multi-level wavelet convolutional neural network. By this means we have
obtained an image of a real object from only 18.75% of the Nyquist sampling
rate, using a portable tabletop incoherent x-ray source of ~37 {\mu}m diameter.
A high imaging resolution of ~10 {\mu}m is achieved, which represents a
concrete step towards the realization of a practical low cost x-ray ghost
imaging camera for applications in biomedicine, archeology, material science,
and so forth.",arxiv
http://arxiv.org/abs/1906.10670v2,2020-11-11T05:26:52Z,2019-06-25T17:09:34Z,"Improving performance of deep learning models with axiomatic attribution
  priors and expected gradients","Recent research has demonstrated that feature attribution methods for deep
networks can themselves be incorporated into training; these attribution priors
optimize for a model whose attributions have certain desirable properties --
most frequently, that particular features are important or unimportant. These
attribution priors are often based on attribution methods that are not
guaranteed to satisfy desirable interpretability axioms, such as completeness
and implementation invariance. Here, we introduce attribution priors to
optimize for higher-level properties of explanations, such as smoothness and
sparsity, enabled by a fast new attribution method formulation called expected
gradients that satisfies many important interpretability axioms. This improves
model performance on many real-world tasks where previous attribution priors
fail. Our experiments show that the gains from combining higher-level
attribution priors with expected gradients attributions are consistent across
image, gene expression, and health care data sets. We believe this work
motivates and provides the necessary tools to support the widespread adoption
of axiomatic attribution priors in many areas of applied machine learning. The
implementations and our results have been made freely available to academic
communities.",arxiv
http://arxiv.org/abs/2003.12828v2,2020-06-24T16:39:37Z,2020-03-28T16:07:41Z,Learning medical triage from clinicians using Deep Q-Learning,"Medical Triage is of paramount importance to healthcare systems, allowing for
the correct orientation of patients and allocation of the necessary resources
to treat them adequately. While reliable decision-tree methods exist to triage
patients based on their presentation, those trees implicitly require human
inference and are not immediately applicable in a fully automated setting. On
the other hand, learning triage policies directly from experts may correct for
some of the limitations of hard-coded decision-trees. In this work, we present
a Deep Reinforcement Learning approach (a variant of DeepQ-Learning) to triage
patients using curated clinical vignettes. The dataset, consisting of 1374
clinical vignettes, was created by medical doctors to represent real-life
cases. Each vignette is associated with an average of 3.8 expert triage
decisions given by medical doctors relying solely on medical history. We show
that this approach is on a par with human performance, yielding safe triage
decisions in 94% of cases, and matching expert decisions in 85% of cases. The
trained agent learns when to stop asking questions, acquires optimized decision
policies requiring less evidence than supervised approaches, and adapts to the
novelty of a situation by asking for more information. Overall, we demonstrate
that a Deep Reinforcement Learning approach can learn effective medical triage
policies directly from expert decisions, without requiring expert knowledge
engineering. This approach is scalable and can be deployed in healthcare
settings or geographical regions with distinct triage specifications, or where
trained experts are scarce, to improve decision making in the early stage of
care.",arxiv
http://arxiv.org/abs/1908.05376v1,2019-08-15T00:06:23Z,2019-08-15T00:06:23Z,"Maximum Relevance and Minimum Redundancy Feature Selection Methods for a
  Marketing Machine Learning Platform","In machine learning applications for online product offerings and marketing
strategies, there are often hundreds or thousands of features available to
build such models. Feature selection is one essential method in such
applications for multiple objectives: improving the prediction accuracy by
eliminating irrelevant features, accelerating the model training and prediction
speed, reducing the monitoring and maintenance workload for feature data
pipeline, and providing better model interpretation and diagnosis capability.
However, selecting an optimal feature subset from a large feature space is
considered as an NP-complete problem. The mRMR (Minimum Redundancy and Maximum
Relevance) feature selection framework solves this problem by selecting the
relevant features while controlling for the redundancy within the selected
features. This paper describes the approach to extend, evaluate, and implement
the mRMR feature selection methods for classification problem in a marketing
machine learning platform at Uber that automates creation and deployment of
targeting and personalization models at scale. This study first extends the
existing mRMR methods by introducing a non-linear feature redundancy measure
and a model-based feature relevance measure. Then an extensive empirical
evaluation is performed for eight different feature selection methods, using
one synthetic dataset and three real-world marketing datasets at Uber to cover
different use cases. Based on the empirical results, the selected mRMR method
is implemented in production for the marketing machine learning platform. A
description of the production implementation is provided and an online
experiment deployed through the platform is discussed.",arxiv
http://arxiv.org/abs/2106.10352v1,2021-06-18T20:54:18Z,2021-06-18T20:54:18Z,"Semi-supervised Optimal Transport with Self-paced Ensemble for
  Cross-hospital Sepsis Early Detection","The utilization of computer technology to solve problems in medical scenarios
has attracted considerable attention in recent years, which still has great
potential and space for exploration. Among them, machine learning has been
widely used in the prediction, diagnosis and even treatment of Sepsis. However,
state-of-the-art methods require large amounts of labeled medical data for
supervised learning. In real-world applications, the lack of labeled data will
cause enormous obstacles if one hospital wants to deploy a new Sepsis detection
system. Different from the supervised learning setting, we need to use known
information (e.g., from another hospital with rich labeled data) to help build
a model with acceptable performance, i.e., transfer learning. In this paper, we
propose a semi-supervised optimal transport with self-paced ensemble framework
for Sepsis early detection, called SPSSOT, to transfer knowledge from the other
that has rich labeled data. In SPSSOT, we first extract the same clinical
indicators from the source domain (e.g., hospital with rich labeled data) and
the target domain (e.g., hospital with little labeled data), then we combine
the semi-supervised domain adaptation based on optimal transport theory with
self-paced under-sampling to avoid a negative transfer possibly caused by
covariate shift and class imbalance. On the whole, SPSSOT is an end-to-end
transfer learning method for Sepsis early detection which can automatically
select suitable samples from two domains respectively according to the number
of iterations and align feature space of two domains. Extensive experiments on
two open clinical datasets demonstrate that comparing with other methods, our
proposed SPSSOT, can significantly improve the AUC values with only 1% labeled
data in the target domain in two transfer learning scenarios, MIMIC
$rightarrow$ Challenge and Challenge $rightarrow$ MIMIC.",arxiv
http://arxiv.org/abs/2002.02897v2,2020-02-15T14:34:48Z,2020-02-07T16:55:21Z,"MDLdroid: a ChainSGD-reduce Approach to Mobile Deep Learning for
  Personal Mobile Sensing","Personal mobile sensing is fast permeating our daily lives to enable activity
monitoring, healthcare and rehabilitation. Combined with deep learning, these
applications have achieved significant success in recent years. Different from
conventional cloud-based paradigms, running deep learning on devices offers
several advantages including data privacy preservation and low-latency response
for both model inference and update. Since data collection is costly in
reality, Google's Federated Learning offers not only complete data privacy but
also better model robustness based on multiple user data. However, personal
mobile sensing applications are mostly user-specific and highly affected by
environment. As a result, continuous local changes may seriously affect the
performance of a global model generated by Federated Learning. In addition,
deploying Federated Learning on a local server, e.g., edge server, may quickly
reach the bottleneck due to resource constraint and serious failure by attacks.
Towards pushing deep learning on devices, we present MDLdroid, a novel
decentralized mobile deep learning framework to enable resource-aware on-device
collaborative learning for personal mobile sensing applications. To address
resource limitation, we propose a ChainSGD-reduce approach which includes a
novel chain-directed Synchronous Stochastic Gradient Descent algorithm to
effectively reduce overhead among multiple devices. We also design an
agent-based multi-goal reinforcement learning mechanism to balance resources in
a fair and efficient manner. Our evaluations show that our model training on
off-the-shelf mobile devices achieves 2x to 3.5x faster than single-device
training, and 1.5x faster than the master-slave approach.",arxiv
http://arxiv.org/abs/2107.04882v1,2021-07-10T18:00:40Z,2021-07-10T18:00:40Z,"Out of Distribution Detection and Adversarial Attacks on Deep Neural
  Networks for Robust Medical Image Analysis","Deep learning models have become a popular choice for medical image analysis.
However, the poor generalization performance of deep learning models limits
them from being deployed in the real world as robustness is critical for
medical applications. For instance, the state-of-the-art Convolutional Neural
Networks (CNNs) fail to detect adversarial samples or samples drawn
statistically far away from the training distribution. In this work, we
experimentally evaluate the robustness of a Mahalanobis distance-based
confidence score, a simple yet effective method for detecting abnormal input
samples, in classifying malaria parasitized cells and uninfected cells. Results
indicated that the Mahalanobis confidence score detector exhibits improved
performance and robustness of deep learning models, and achieves
stateof-the-art performance on both out-of-distribution (OOD) and adversarial
samples.",arxiv
http://arxiv.org/abs/1911.08089v2,2019-12-07T03:42:06Z,2019-11-19T04:28:47Z,"""The Human Body is a Black Box"": Supporting Clinical Decision-Making
  with Deep Learning","Machine learning technologies are increasingly developed for use in
healthcare. While research communities have focused on creating
state-of-the-art models, there has been less focus on real world implementation
and the associated challenges to accuracy, fairness, accountability, and
transparency that come from actual, situated use. Serious questions remain
under examined regarding how to ethically build models, interpret and explain
model output, recognize and account for biases, and minimize disruptions to
professional expertise and work cultures. We address this gap in the literature
and provide a detailed case study covering the development, implementation, and
evaluation of Sepsis Watch, a machine learning-driven tool that assists
hospital clinicians in the early diagnosis and treatment of sepsis. We, the
team that developed and evaluated the tool, discuss our conceptualization of
the tool not as a model deployed in the world but instead as a socio-technical
system requiring integration into existing social and professional contexts.
Rather than focusing on model interpretability to ensure a fair and accountable
machine learning, we point toward four key values and practices that should be
considered when developing machine learning to support clinical
decision-making: rigorously define the problem in context, build relationships
with stakeholders, respect professional discretion, and create ongoing feedback
loops with stakeholders. Our work has significant implications for future
research regarding mechanisms of institutional accountability and
considerations for designing machine learning systems. Our work underscores the
limits of model interpretability as a solution to ensure transparency,
accuracy, and accountability in practice. Instead, our work demonstrates other
means and goals to achieve FATML values in design and in practice.",arxiv
http://arxiv.org/abs/1807.06419v1,2018-07-13T17:23:54Z,2018-07-13T17:23:54Z,On Ternary Coding and Three-Valued Logic,"Mathematically, ternary coding is more efficient than binary coding. It is
little used in computation because technology for binary processing is already
established and the implementation of ternary coding is more complicated, but
remains relevant in algorithms that use decision trees and in communications.
In this paper we present a new comparison of binary and ternary coding and
their relative efficiencies are computed both for number representation and
decision trees. The implications of our inability to use optimal representation
through mathematics or logic are examined. Apart from considerations of
representation efficiency, ternary coding appears preferable to binary coding
in classification of many real-world problems of artificial intelligence (AI)
and medicine. We examine the problem of identifying appropriate three classes
for domain-specific applications.",arxiv
http://arxiv.org/abs/2103.15933v1,2021-03-29T20:10:51Z,2021-03-29T20:10:51Z,Learning Under Adversarial and Interventional Shifts,"Machine learning models are often trained on data from one distribution and
deployed on others. So it becomes important to design models that are robust to
distribution shifts. Most of the existing work focuses on optimizing for either
adversarial shifts or interventional shifts. Adversarial methods lack
expressivity in representing plausible shifts as they consider shifts to joint
distributions in the data. Interventional methods allow more expressivity but
provide robustness to unbounded shifts, resulting in overly conservative
models. In this work, we combine the complementary strengths of the two
approaches and propose a new formulation, RISe, for designing robust models
against a set of distribution shifts that are at the intersection of
adversarial and interventional shifts. We employ the distributionally robust
optimization framework to optimize the resulting objective in both supervised
and reinforcement learning settings. Extensive experimentation with synthetic
and real world datasets from healthcare demonstrate the efficacy of the
proposed approach.",arxiv
http://arxiv.org/abs/2105.06442v1,2021-05-13T17:33:28Z,2021-05-13T17:33:28Z,"An Empirical Comparison of Bias Reduction Methods on Real-World Problems
  in High-Stakes Policy Settings","Applications of machine learning (ML) to high-stakes policy settings -- such
as education, criminal justice, healthcare, and social service delivery -- have
grown rapidly in recent years, sparking important conversations about how to
ensure fair outcomes from these systems. The machine learning research
community has responded to this challenge with a wide array of proposed
fairness-enhancing strategies for ML models, but despite the large number of
methods that have been developed, little empirical work exists evaluating these
methods in real-world settings. Here, we seek to fill this research gap by
investigating the performance of several methods that operate at different
points in the ML pipeline across four real-world public policy and social good
problems. Across these problems, we find a wide degree of variability and
inconsistency in the ability of many of these methods to improve model
fairness, but post-processing by choosing group-specific score thresholds
consistently removes disparities, with important implications for both the ML
research community and practitioners deploying machine learning to inform
consequential policy decisions.",arxiv
http://arxiv.org/abs/2006.06385v1,2020-06-11T13:00:02Z,2020-06-11T13:00:02Z,"TensorFlow with user friendly Graphical Framework for object detection
  API","TensorFlow is an open-source framework for deep learning dataflow and
contains application programming interfaces (APIs) of voice analysis, natural
language process, and computer vision. Especially, TensorFlow object detection
API in computer vision field has been widely applied to technologies of
agriculture, engineering, and medicine but barriers to entry of the framework
usage is still high through command-line interface (CLI) and code for amateurs
and beginners of information technology (IT) field. Therefore, this is aim to
develop an user friendly Graphical Framework for object detection API on
TensorFlow which is called TensorFlow Graphical Framework (TF-GraF). The
TF-GraF provides independent virtual environments according to user accounts in
server-side, additionally, execution of data preprocessing, training, and
evaluation without CLI in client-side. Furthermore, hyperparameter setting,
real-time observation of training process, object visualization of test images,
and metrics evaluations of test data can also be operated via TF-GraF.
Especially, TF-GraF supports flexible model selection of SSD, Faster-RCNN,
RFCN, and Mask-RCNN including convolutional neural networks (inceptions and
ResNets) through GUI environment. Consequently, TF-GraF allows anyone, even
without any previous knowledge of deep learning frameworks, to design, train
and deploy machine intelligence models without coding. Since TF-GraF takes care
of setting and configuration, it allows anyone to use deep learning technology
for their project without spending time to install complex software and
environment.",arxiv
http://arxiv.org/abs/1905.03554v1,2019-05-09T11:52:10Z,2019-05-09T11:52:10Z,1D Convolutional Neural Networks and Applications: A Survey,"During the last decade, Convolutional Neural Networks (CNNs) have become the
de facto standard for various Computer Vision and Machine Learning operations.
CNNs are feed-forward Artificial Neural Networks (ANNs) with alternating
convolutional and subsampling layers. Deep 2D CNNs with many hidden layers and
millions of parameters have the ability to learn complex objects and patterns
providing that they can be trained on a massive size visual database with
ground-truth labels. With a proper training, this unique ability makes them the
primary tool for various engineering applications for 2D signals such as images
and video frames. Yet, this may not be a viable option in numerous applications
over 1D signals especially when the training data is scarce or
application-specific. To address this issue, 1D CNNs have recently been
proposed and immediately achieved the state-of-the-art performance levels in
several applications such as personalized biomedical data classification and
early diagnosis, structural health monitoring, anomaly detection and
identification in power electronics and motor-fault detection. Another major
advantage is that a real-time and low-cost hardware implementation is feasible
due to the simple and compact configuration of 1D CNNs that perform only 1D
convolutions (scalar multiplications and additions). This paper presents a
comprehensive review of the general architecture and principals of 1D CNNs
along with their major engineering applications, especially focused on the
recent progress in this field. Their state-of-the-art performance is
highlighted concluding with their unique properties. The benchmark datasets and
the principal 1D CNN software used in those applications are also publically
shared in a dedicated website.",arxiv
http://arxiv.org/abs/2108.12242v1,2021-08-27T12:47:19Z,2021-08-27T12:47:19Z,Deep learning models are not robust against noise in clinical text,"Artificial Intelligence (AI) systems are attracting increasing interest in
the medical domain due to their ability to learn complicated tasks that require
human intelligence and expert knowledge. AI systems that utilize
high-performance Natural Language Processing (NLP) models have achieved
state-of-the-art results on a wide variety of clinical text processing
benchmarks. They have even outperformed human accuracy on some tasks. However,
performance evaluation of such AI systems have been limited to accuracy
measures on curated and clean benchmark datasets that may not properly reflect
how robustly these systems can operate in real-world situations. In order to
address this challenge, we introduce and implement a wide variety of
perturbation methods that simulate different types of noise and variability in
clinical text data. While noisy samples produced by these perturbation methods
can often be understood by humans, they may cause AI systems to make erroneous
decisions. Conducting extensive experiments on several clinical text processing
tasks, we evaluated the robustness of high-performance NLP models against
various types of character-level and word-level noise. The results revealed
that the NLP models performance degrades when the input contains small amounts
of noise. This study is a significant step towards exposing vulnerabilities of
AI models utilized in clinical text processing systems. The proposed
perturbation methods can be used in performance evaluation tests to assess how
robustly clinical NLP models can operate on noisy data, in real-world settings.",arxiv
http://arxiv.org/abs/2110.03660v1,2021-08-17T18:01:12Z,2021-08-17T18:01:12Z,"Developing Medical AI : a cloud-native audio-visual data collection
  study","Designing Artificial Intelligence (AI) solutions that can operate in
real-world situations is a highly complex task. Deploying such solutions in the
medical domain is even more challenging. The promise of using AI to improve
patient care and reduce cost has encouraged many companies to undertake such
endeavours. For our team, the goal has been to improve early identification of
deteriorating patients in the hospital. Identifying patient deterioration in
lower acuity wards relies, to a large degree on the attention and intuition of
clinicians, rather than on the presence of physiological monitoring devices. In
these care areas, an automated tool which could continuously observe patients
and notify the clinical staff of suspected deterioration, would be extremely
valuable. In order to develop such an AI-enabled tool, a large collection of
patient images and audio correlated with corresponding vital signs, past
medical history and clinical outcome would be indispensable. To the best of our
knowledge, no such public or for-pay data set currently exists. This lack of
audio-visual data led to the decision to conduct exactly such study. The main
contributions of this paper are, the description of a protocol for audio-visual
data collection study, a cloud-architecture for efficiently processing and
consuming such data, and the design of a specific data collection device.",arxiv
http://arxiv.org/abs/1711.08149v3,2018-10-15T15:26:47Z,2017-11-22T06:32:13Z,"Accurate Real Time Localization Tracking in A Clinical Environment using
  Bluetooth Low Energy and Deep Learning","Deep learning has started to revolutionize several different industries, and
the applications of these methods in medicine are now becoming more
commonplace. This study focuses on investigating the feasibility of tracking
patients and clinical staff wearing Bluetooth Low Energy (BLE) tags in a
radiation oncology clinic using artificial neural networks (ANNs) and
convolutional neural networks (CNNs). The performance of these networks was
compared to relative received signal strength indicator (RSSI) thresholding and
triangulation. By utilizing temporal information, a combined CNN+ANN network
was capable of correctly identifying the location of the BLE tag with an
accuracy of 99.9%. It outperformed a CNN model (accuracy = 94%), a thresholding
model employing majority voting (accuracy = 95%), and a triangulation
classifier utilizing majority voting (accuracy = 95%). Future studies will seek
to deploy this affordable real time location system in hospitals to improve
clinical workflow, efficiency, and patient safety.",arxiv
http://arxiv.org/abs/2007.08653v1,2020-07-14T22:47:12Z,2020-07-14T22:47:12Z,Dementia Prediction Applying Variational Quantum Classifier,"Dementia is the fifth cause of death worldwide with 10 million new cases
every year. Healthcare applications using machine learning techniques have
almost reached the physical limits while more data is becoming available
resulting from the increasing rate of diagnosis. Recent research in Quantum
Machine Learning (QML) techniques have found different approaches that may be
useful to accelerate the training process of existing machine learning models
and provide an alternative to learn more complex patterns. This work aims to
report a real-world application of a Quantum Machine Learning Algorithm, in
particular, we found that using the implemented version for Variational Quantum
Classiffication (VQC) in IBM's framework Qiskit allows predicting dementia in
elderly patients, this approach proves to provide more consistent results when
compared with a classical Support Vector Machine (SVM) with a linear kernel
using different number of features.",arxiv
http://arxiv.org/abs/1902.01506v3,2019-06-24T07:19:55Z,2019-02-05T00:59:44Z,"Learning to Prescribe Interventions for Tuberculosis Patients Using
  Digital Adherence Data","Digital Adherence Technologies (DATs) are an increasingly popular method for
verifying patient adherence to many medications. We analyze data from one city
served by 99DOTS, a phone-call-based DAT deployed for Tuberculosis (TB)
treatment in India where nearly 3 million people are afflicted with the disease
each year. The data contains nearly 17,000 patients and 2.1M dose records. We
lay the groundwork for learning from this real-world data, including a method
for avoiding the effects of unobserved interventions in training data used for
machine learning. We then construct a deep learning model, demonstrate its
interpretability, and show how it can be adapted and trained in different
clinical scenarios to better target and improve patient care. In the real-time
risk prediction setting our model could be used to proactively intervene with
21% more patients and before 76% more missed doses than current heuristic
baselines. For outcome prediction, our model performs 40% better than baseline
methods, allowing cities to target more resources to clinics with a heavier
burden of patients at risk of failure. Finally, we present a case study
demonstrating how our model can be trained in an end-to-end decision focused
learning setting to achieve 15% better solution quality in an example decision
problem faced by health workers.",arxiv
http://arxiv.org/abs/2012.02048v1,2020-12-03T16:28:45Z,2020-12-03T16:28:45Z,"Ethical Testing in the Real World: Evaluating Physical Testing of
  Adversarial Machine Learning","This paper critically assesses the adequacy and representativeness of
physical domain testing for various adversarial machine learning (ML) attacks
against computer vision systems involving human subjects. Many papers that
deploy such attacks characterize themselves as ""real world."" Despite this
framing, however, we found the physical or real-world testing conducted was
minimal, provided few details about testing subjects and was often conducted as
an afterthought or demonstration. Adversarial ML research without
representative trials or testing is an ethical, scientific, and health/safety
issue that can cause real harms. We introduce the problem and our methodology,
and then critique the physical domain testing methodologies employed by papers
in the field. We then explore various barriers to more inclusive physical
testing in adversarial ML and offer recommendations to improve such testing
notwithstanding these challenges.",arxiv
http://arxiv.org/abs/1807.08942v1,2018-07-24T07:50:18Z,2018-07-24T07:50:18Z,Example Mining for Incremental Learning in Medical Imaging,"Incremental Learning is well known machine learning approach wherein the
weights of the learned model are dynamically and gradually updated to
generalize on new unseen data without forgetting the existing knowledge.
Incremental learning proves to be time as well as resource-efficient solution
for deployment of deep learning algorithms in real world as the model can
automatically and dynamically adapt to new data as and when annotated data
becomes available. The development and deployment of Computer Aided Diagnosis
(CAD) tools in medical domain is another scenario, where incremental learning
becomes very crucial as collection and annotation of a comprehensive dataset
spanning over multiple pathologies and imaging machines might take years.
However, not much has so far been explored in this direction. In the current
work, we propose a robust and efficient method for incremental learning in
medical imaging domain. Our approach makes use of Hard Example Mining technique
(which is commonly used as a solution to heavy class imbalance) to
automatically select a subset of dataset to fine-tune the existing network
weights such that it adapts to new data while retaining existing knowledge. We
develop our approach for incremental learning of our already under test model
for detecting dental caries. Further, we apply our approach to one publicly
available dataset and demonstrate that our approach reaches the accuracy of
training on entire dataset at once, while availing the benefits of incremental
learning scenario.",arxiv
http://arxiv.org/abs/2008.05232v2,2020-11-23T20:38:50Z,2020-08-12T11:03:57Z,Learning to Detect Anomalous Wireless Links in IoT Networks,"After decades of research, the Internet of Things (IoT) is finally permeating
real-life and helps improve the efficiency of infrastructures and processes as
well as our health. As a massive number of IoT devices are deployed, they
naturally incur great operational costs to ensure intended operations. To
effectively handle such intended operations in massive IoT networks, automatic
detection of malfunctioning, namely anomaly detection, becomes a critical but
challenging task. In this paper, motivated by a real-world experimental IoT
deployment, we introduce four types of wireless network anomalies that are
identified at the link layer. We study the performance of threshold- and
machine learning (ML)-based classifiers to automatically detect these
anomalies. We examine the relative performance of three supervised and three
unsupervised ML techniques on both non-encoded and encoded (autoencoder)
feature representations. Our results demonstrate that; i) selected supervised
approaches are able to detect anomalies with F1 scores of above 0.98, while
unsupervised ones are also capable of detecting the said anomalies with F1
scores of, on average, 0.90, and ii) OC-SVM outperforms all the other
unsupervised ML approaches reaching at F1 scores of 0.99 for SuddenD, 0.95 for
SuddenR, 0.93 for InstaD and 0.95 for SlowD.",arxiv
http://arxiv.org/abs/2103.03472v1,2021-03-05T04:51:02Z,2021-03-05T04:51:02Z,"A Novel Framework for Threat Analysis of Machine Learning-based Smart
  Healthcare Systems","Smart healthcare systems (SHSs) are providing fast and efficient disease
treatment leveraging wireless body sensor networks (WBSNs) and implantable
medical devices (IMDs)-based internet of medical things (IoMT). In addition,
IoMT-based SHSs are enabling automated medication, allowing communication among
myriad healthcare sensor devices. However, adversaries can launch various
attacks on the communication network and the hardware/firmware to introduce
false data or cause data unavailability to the automatic medication system
endangering the patient's life. In this paper, we propose SHChecker, a novel
threat analysis framework that integrates machine learning and formal analysis
capabilities to identify potential attacks and corresponding effects on an
IoMT-based SHS. Our framework can provide us with all potential attack vectors,
each representing a set of sensor measurements to be altered, for an SHS given
a specific set of attack attributes, allowing us to realize the system's
resiliency, thus the insight to enhance the robustness of the model. We
implement SHChecker on a synthetic and a real dataset, which affirms that our
framework can reveal potential attack vectors in an IoMT system. This is a
novel effort to formally analyze supervised and unsupervised machine learning
models for black-box SHS threat analysis.",arxiv
http://arxiv.org/abs/2007.13404v2,2020-10-29T16:23:12Z,2020-07-27T09:50:11Z,"YOLOpeds: Efficient Real-Time Single-Shot Pedestrian Detection for Smart
  Camera Applications","Deep Learning-based object detectors can enhance the capabilities of smart
camera systems in a wide spectrum of machine vision applications including
video surveillance, autonomous driving, robots and drones, smart factory, and
health monitoring. Pedestrian detection plays a key role in all these
applications and deep learning can be used to construct accurate
state-of-the-art detectors. However, such complex paradigms do not scale easily
and are not traditionally implemented in resource-constrained smart cameras for
on-device processing which offers significant advantages in situations when
real-time monitoring and robustness are vital. Efficient neural networks can
not only enable mobile applications and on-device experiences but can also be a
key enabler of privacy and security allowing a user to gain the benefits of
neural networks without needing to send their data to the server to be
evaluated. This work addresses the challenge of achieving a good trade-off
between accuracy and speed for efficient deployment of deep-learning-based
pedestrian detection in smart camera applications. A computationally efficient
architecture is introduced based on separable convolutions and proposes
integrating dense connections across layers and multi-scale feature fusion to
improve representational capacity while decreasing the number of parameters and
operations. In particular, the contributions of this work are the following: 1)
An efficient backbone combining multi-scale feature operations, 2) a more
elaborate loss function for improved localization, 3) an anchor-less approach
for detection, The proposed approach called YOLOpeds is evaluated using the
PETS2009 surveillance dataset on 320x320 images. Overall, YOLOpeds provides
real-time sustained operation of over 30 frames per second with detection rates
in the range of 86% outperforming existing deep learning models.",arxiv
http://arxiv.org/abs/2011.07555v1,2020-11-15T15:27:51Z,2020-11-15T15:27:51Z,Towards Compliant Data Management Systems for Healthcare ML,"The increasing popularity of machine learning approaches and the rising
awareness of data protection and data privacy presents an opportunity to build
truly secure and trustworthy healthcare systems. Regulations such as GDPR and
HIPAA present broad guidelines and frameworks, but the implementation can
present technical challenges. Compliant data management systems require
enforcement of a number of technical and administrative safeguards. While
policies can be set for both safeguards there is limited availability to
understand compliance in real time. Increasingly, machine learning
practitioners are becoming aware of the importance of keeping track of
sensitive data. With sensitivity over personally identifiable, health or
commercially sensitive information there would be value in understanding
assessment of the flow of data in a more dynamic fashion. We review how data
flows within machine learning projects in healthcare from source to storage to
use in training algorithms and beyond. Based on this, we design engineering
specifications and solutions for versioning of data. Our objective is to design
tools to detect and track sensitive data across machines and users across the
life cycle of a project, prioritizing efficiency, consistency and ease of use.
We build a prototype of the solution that demonstrates the difficulties in this
domain. Together, these represent first efforts towards building a compliant
data management system for healthcare machine learning projects.",arxiv
http://arxiv.org/abs/1706.02501v1,2017-06-08T10:10:44Z,2017-06-08T10:10:44Z,Unlocking the Potential of Simulators: Design with RL in Mind,"Using Reinforcement Learning (RL) in simulation to construct policies useful
in real life is challenging. This is often attributed to the sequential
decision making aspect: inaccuracies in simulation accumulate over multiple
steps, hence the simulated trajectories diverge from what would happen in
reality.
  In our work we show the need to consider another important aspect: the
mismatch in simulating control. We bring attention to the need for modeling
control as well as dynamics, since oversimplifying assumptions about applying
actions of RL policies could make the policies fail on real-world systems.
  We design a simulator for solving a pivoting task (of interest in Robotics)
and demonstrate that even a simple simulator designed with RL in mind
outperforms high-fidelity simulators when it comes to learning a policy that is
to be deployed on a real robotic system. We show that a phenomenon that is hard
to model - friction - could be exploited successfully, even when RL is
performed using a simulator with a simple dynamics and noise model. Hence, we
demonstrate that as long as the main sources of uncertainty are identified, it
could be possible to learn policies applicable to real systems even using a
simple simulator.
  RL-compatible simulators could open the possibilities for applying a wide
range of RL algorithms in various fields. This is important, since currently
data sparsity in fields like healthcare and education frequently forces
researchers and engineers to only consider sample-efficient RL approaches.
Successful simulator-aided RL could increase flexibility of experimenting with
RL algorithms and help applying RL policies to real-world settings in fields
where data is scarce. We believe that lessons learned in Robotics could help
other fields design RL-compatible simulators, so we summarize our experience
and conclude with suggestions.",arxiv
http://arxiv.org/abs/2002.03478v3,2020-08-11T06:51:45Z,2020-02-10T00:26:43Z,"Interpretable Off-Policy Evaluation in Reinforcement Learning by
  Highlighting Influential Transitions","Off-policy evaluation in reinforcement learning offers the chance of using
observational data to improve future outcomes in domains such as healthcare and
education, but safe deployment in high stakes settings requires ways of
assessing its validity. Traditional measures such as confidence intervals may
be insufficient due to noise, limited data and confounding. In this paper we
develop a method that could serve as a hybrid human-AI system, to enable human
experts to analyze the validity of policy evaluation estimates. This is
accomplished by highlighting observations in the data whose removal will have a
large effect on the OPE estimate, and formulating a set of rules for choosing
which ones to present to domain experts for validation. We develop methods to
compute exactly the influence functions for fitted Q-evaluation with two
different function classes: kernel-based and linear least squares, as well as
importance sampling methods. Experiments on medical simulations and real-world
intensive care unit data demonstrate that our method can be used to identify
limitations in the evaluation process and make evaluation more robust.",arxiv
http://arxiv.org/abs/2104.07820v2,2021-04-29T00:11:58Z,2021-04-15T23:38:39Z,"Machine Learning Approaches for Type 2 Diabetes Prediction and Care
  Management","Prediction of diabetes and its various complications has been studied in a
number of settings, but a comprehensive overview of problem setting for
diabetes prediction and care management has not been addressed in the
literature. In this document we seek to remedy this omission in literature with
an encompassing overview of diabetes complication prediction as well as
situating this problem in the context of real world healthcare management. We
illustrate various problems encountered in real world clinical scenarios via
our own experience with building and deploying such models. In this manuscript
we illustrate a Machine Learning (ML) framework for addressing the problem of
predicting Type 2 Diabetes Mellitus (T2DM) together with a solution for risk
stratification, intervention and management. These ML models align with how
physicians think about disease management and mitigation, which comprises these
four steps: Identify, Stratify, Engage, Measure.",arxiv
http://arxiv.org/abs/2109.02418v2,2021-10-17T07:11:58Z,2021-09-06T12:58:25Z,Multitask Balanced and Recalibrated Network for Medical Code Prediction,"Human coders assign standardized medical codes to clinical documents
generated during patients' hospitalization, which is error-prone and
labor-intensive. Automated medical coding approaches have been developed using
machine learning methods such as deep neural networks. Nevertheless, automated
medical coding is still challenging because of the imbalanced class problem,
complex code association, and noise in lengthy documents. To solve these
issues, we propose a novel neural network called Multitask Balanced and
Recalibrated Neural Network. Significantly, the multitask learning scheme
shares the relationship knowledge between different code branches to capture
the code association. A recalibrated aggregation module is developed by
cascading convolutional blocks to extract high-level semantic features that
mitigate the impact of noise in documents. Also, the cascaded structure of the
recalibrated module can benefit the learning from lengthy notes. To solve the
class imbalanced problem, we deploy the focal loss to redistribute the
attention of low and high-frequency medical codes. Experimental results show
that our proposed model outperforms competitive baselines on a real-world
clinical dataset MIMIC-III.",arxiv
http://arxiv.org/abs/2007.15153v1,2020-07-29T23:43:15Z,2020-07-29T23:43:15Z,"Fast, Structured Clinical Documentation via Contextual Autocomplete","We present a system that uses a learned autocompletion mechanism to
facilitate rapid creation of semi-structured clinical documentation. We
dynamically suggest relevant clinical concepts as a doctor drafts a note by
leveraging features from both unstructured and structured medical data. By
constraining our architecture to shallow neural networks, we are able to make
these suggestions in real time. Furthermore, as our algorithm is used to write
a note, we can automatically annotate the documentation with clean labels of
clinical concepts drawn from medical vocabularies, making notes more structured
and readable for physicians, patients, and future algorithms. To our knowledge,
this system is the only machine learning-based documentation utility for
clinical notes deployed in a live hospital setting, and it reduces keystroke
burden of clinical concepts by 67% in real environments.",arxiv
http://arxiv.org/abs/2004.11958v1,2020-04-24T19:36:37Z,2020-04-24T19:36:37Z,"The Plant Pathology 2020 challenge dataset to classify foliar disease of
  apples","Apple orchards in the U.S. are under constant threat from a large number of
pathogens and insects. Appropriate and timely deployment of disease management
depends on early disease detection. Incorrect and delayed diagnosis can result
in either excessive or inadequate use of chemicals, with increased production
costs, environmental, and health impacts. We have manually captured 3,651
high-quality, real-life symptom images of multiple apple foliar diseases, with
variable illumination, angles, surfaces, and noise. A subset, expert-annotated
to create a pilot dataset for apple scab, cedar apple rust, and healthy leaves,
was made available to the Kaggle community for 'Plant Pathology Challenge';
part of the Fine-Grained Visual Categorization (FGVC) workshop at CVPR 2020
(Computer Vision and Pattern Recognition). We also trained an off-the-shelf
convolutional neural network (CNN) on this data for disease classification and
achieved 97% accuracy on a held-out test set. This dataset will contribute
towards development and deployment of machine learning-based automated plant
disease classification algorithms to ultimately realize fast and accurate
disease detection. We will continue to add images to the pilot dataset for a
larger, more comprehensive expert-annotated dataset for future Kaggle
competitions and to explore more advanced methods for disease classification
and quantification.",arxiv
http://arxiv.org/abs/1912.09621v1,2019-12-20T02:57:05Z,2019-12-20T02:57:05Z,"Understanding Deep Neural Network Predictions for Medical Imaging
  Applications","Computer-aided detection has been a research area attracting great interest
in the past decade. Machine learning algorithms have been utilized extensively
for this application as they provide a valuable second opinion to the doctors.
Despite several machine learning models being available for medical imaging
applications, not many have been implemented in the real-world due to the
uninterpretable nature of the decisions made by the network. In this paper, we
investigate the results provided by deep neural networks for the detection of
malaria, diabetic retinopathy, brain tumor, and tuberculosis in different
imaging modalities. We visualize the class activation mappings for all the
applications in order to enhance the understanding of these networks. This type
of visualization, along with the corresponding network performance metrics,
would aid the data science experts in better understanding of their models as
well as assisting doctors in their decision-making process.",arxiv
http://arxiv.org/abs/2101.03170v2,2021-09-17T17:17:45Z,2021-01-07T20:18:43Z,"BDNNSurv: Bayesian deep neural networks for survival analysis using
  pseudo values","There has been increasing interest in modeling survival data using deep
learning methods in medical research. In this paper, we proposed a Bayesian
hierarchical deep neural networks model for modeling and prediction of survival
data. Compared with previously studied methods, the new proposal can provide
not only point estimate of survival probability but also quantification of the
corresponding uncertainty, which can be of crucial importance in predictive
modeling and subsequent decision making. The favorable statistical properties
of point and uncertainty estimates were demonstrated by simulation studies and
real data analysis. The Python code implementing the proposed approach was
provided.",arxiv
http://arxiv.org/abs/2010.10969v2,2021-01-06T10:07:56Z,2020-10-21T13:00:05Z,"Incorporating Interpretable Output Constraints in Bayesian Neural
  Networks","Domains where supervised models are deployed often come with task-specific
constraints, such as prior expert knowledge on the ground-truth function, or
desiderata like safety and fairness. We introduce a novel probabilistic
framework for reasoning with such constraints and formulate a prior that
enables us to effectively incorporate them into Bayesian neural networks
(BNNs), including a variant that can be amortized over tasks. The resulting
Output-Constrained BNN (OC-BNN) is fully consistent with the Bayesian framework
for uncertainty quantification and is amenable to black-box inference. Unlike
typical BNN inference in uninterpretable parameter space, OC-BNNs widen the
range of functional knowledge that can be incorporated, especially for model
users without expertise in machine learning. We demonstrate the efficacy of
OC-BNNs on real-world datasets, spanning multiple domains such as healthcare,
criminal justice, and credit scoring.",arxiv
http://arxiv.org/abs/2101.03889v2,2021-02-16T13:44:56Z,2020-12-21T10:25:27Z,A Comprehensive Survey of 6G Wireless Communications,"While fifth-generation (5G) communications are being rolled out worldwide,
sixth-generation (6G) communications have attracted much attention from both
the industry and the academia. Compared with 5G, 6G will have a wider frequency
band, higher transmission rate, spectrum efficiency, greater connection
capacity, shorter delay, broader coverage, and more robust anti-interference
capability to satisfy various network requirements. This survey presents an
insightful understanding of 6G wireless communications by introducing
requirements, features, critical technologies, challenges, and applications.
First, we give an overview of 6G from perspectives of technologies, security
and privacy, and applications. Subsequently, we introduce various 6G
technologies and their existing challenges in detail, e.g., artificial
intelligence (AI), intelligent surfaces, THz, space-air-ground-sea integrated
network, cell-free massive MIMO, etc. Because of these technologies, 6G is
expected to outperform existing wireless communication systems regarding the
transmission rate, latency, global coverage, etc. Next, we discuss security and
privacy techniques that can be applied to protect data in 6G. Since edge
devices are expected to gain popularity soon, the vast amount of generated data
and frequent data exchange make the leakage of data easily. Finally, we predict
real-world applications built on the technologies and features of 6G; for
example, smart healthcare, smart city, and smart manufacturing will be
implemented by taking advantage of AI.",arxiv
http://arxiv.org/abs/2104.08178v1,2021-04-14T20:32:39Z,2021-04-14T20:32:39Z,"Design of an Efficient, Ease-of-use and Affordable Artificial
  Intelligence based Nucleic Acid Amplification Diagnosis Technology for
  Tuberculosis and Multi-drug Resistant Tuberculosis","Current technologies that facilitate diagnosis for simultaneous detection of
Mycobacterium tuberculosis and its resistance to first-line anti-tuberculosis
drugs (Isoniazid and Rifampicim) are designed for lab-based settings and are
unaffordable for large scale testing implementations. The suitability of a TB
diagnosis instrument, generally required in low-resource settings, to be
implementable in point-of-care last mile public health centres depends on
manufacturing cost, ease-of-use, automation and portability. This paper
discusses a portable, low-cost, machine learning automated Nucleic acid
amplification testing (NAAT) device that employs the use of a smartphone-based
fluorescence detection using novel image processing and chromaticity detection
algorithms. To test the instrument, real time polymerase chain reaction (qPCR)
experiment on cDNA dilution spanning over two concentrations (40 ng/uL and 200
ng/uL) was performed and sensitive detection of multiplexed positive control
assay was verified.",arxiv
http://arxiv.org/abs/1904.13342v1,2019-04-30T16:12:55Z,2019-04-30T16:12:55Z,PYRO-NN: Python Reconstruction Operators in Neural Networks,"Purpose: Recently, several attempts were conducted to transfer deep learning
to medical image reconstruction. An increasingly number of publications follow
the concept of embedding the CT reconstruction as a known operator into a
neural network. However, most of the approaches presented lack an efficient CT
reconstruction framework fully integrated into deep learning environments. As a
result, many approaches are forced to use workarounds for mathematically
unambiguously solvable problems. Methods: PYRO-NN is a generalized framework to
embed known operators into the prevalent deep learning framework Tensorflow.
The current status includes state-of-the-art parallel-, fan- and cone-beam
projectors and back-projectors accelerated with CUDA provided as Tensorflow
layers. On top, the framework provides a high level Python API to conduct FBP
and iterative reconstruction experiments with data from real CT systems.
Results: The framework provides all necessary algorithms and tools to design
end-to-end neural network pipelines with integrated CT reconstruction
algorithms. The high level Python API allows a simple use of the layers as
known from Tensorflow. To demonstrate the capabilities of the layers, the
framework comes with three baseline experiments showing a cone-beam short scan
FDK reconstruction, a CT reconstruction filter learning setup, and a TV
regularized iterative reconstruction. All algorithms and tools are referenced
to a scientific publication and are compared to existing non deep learning
reconstruction frameworks. The framework is available as open-source software
at \url{https://github.com/csyben/PYRO-NN}. Conclusions: PYRO-NN comes with the
prevalent deep learning framework Tensorflow and allows to setup end-to-end
trainable neural networks in the medical image reconstruction context. We
believe that the framework will be a step towards reproducible research",arxiv
http://arxiv.org/abs/2008.13369v1,2020-08-31T05:12:57Z,2020-08-31T05:12:57Z,"Introducing Representations of Facial Affect in Automated Multimodal
  Deception Detection","Automated deception detection systems can enhance health, justice, and
security in society by helping humans detect deceivers in high-stakes
situations across medical and legal domains, among others. This paper presents
a novel analysis of the discriminative power of dimensional representations of
facial affect for automated deception detection, along with interpretable
features from visual, vocal, and verbal modalities. We used a video dataset of
people communicating truthfully or deceptively in real-world, high-stakes
courtroom situations. We leveraged recent advances in automated emotion
recognition in-the-wild by implementing a state-of-the-art deep neural network
trained on the Aff-Wild database to extract continuous representations of
facial valence and facial arousal from speakers. We experimented with unimodal
Support Vector Machines (SVM) and SVM-based multimodal fusion methods to
identify effective features, modalities, and modeling approaches for detecting
deception. Unimodal models trained on facial affect achieved an AUC of 80%, and
facial affect contributed towards the highest-performing multimodal approach
(adaptive boosting) that achieved an AUC of 91% when tested on speakers who
were not part of training sets. This approach achieved a higher AUC than
existing automated machine learning approaches that used interpretable visual,
vocal, and verbal features to detect deception in this dataset, but did not use
facial affect. Across all videos, deceptive and truthful speakers exhibited
significant differences in facial valence and facial arousal, contributing
computational support to existing psychological theories on affect and
deception. The demonstrated importance of facial affect in our models informs
and motivates the future development of automated, affect-aware machine
learning approaches for modeling and detecting deception and other social
behaviors in-the-wild.",arxiv
http://arxiv.org/abs/1712.09347v1,2017-12-25T02:08:39Z,2017-12-25T02:08:39Z,"Smart Fog: Fog Computing Framework for Unsupervised Clustering Analytics
  in Wearable Internet of Things","The increasing use of wearables in smart telehealth generates heterogeneous
medical big data. Cloud and fog services process these data for assisting
clinical procedures. IoT based ehealthcare have greatly benefited from
efficient data processing. This paper proposed and evaluated use of low
resource machine learning on Fog devices kept close to the wearables for smart
healthcare. In state of the art telecare systems, the signal processing and
machine learning modules are deployed in the cloud for processing physiological
data. We developed a prototype of Fog-based unsupervised machine learning big
data analysis for discovering patterns in physiological data. We employed Intel
Edison and Raspberry Pi as Fog computer in proposed architecture. We performed
validation studies on real-world pathological speech data from in home
monitoring of patients with Parkinson's disease (PD). Proposed architecture
employed machine learning for analysis of pathological speech data obtained
from smartwatches worn by the patients with PD. Results showed that proposed
architecture is promising for low-resource clinical machine learning. It could
be useful for other applications within wearable IoT for smart telehealth
scenarios by translating machine learning approaches from the cloud backend to
edge computing devices such as Fog.",arxiv
http://arxiv.org/abs/1910.01177v1,2019-10-02T19:24:31Z,2019-10-02T19:24:31Z,Improving Differentially Private Models with Active Learning,"Broad adoption of machine learning techniques has increased privacy concerns
for models trained on sensitive data such as medical records. Existing
techniques for training differentially private (DP) models give rigorous
privacy guarantees, but applying these techniques to neural networks can
severely degrade model performance. This performance reduction is an obstacle
to deploying private models in the real world. In this work, we improve the
performance of DP models by fine-tuning them through active learning on public
data. We introduce two new techniques - DIVERSEPUBLIC and NEARPRIVATE - for
doing this fine-tuning in a privacy-aware way. For the MNIST and SVHN datasets,
these techniques improve state-of-the-art accuracy for DP models while
retaining privacy guarantees.",arxiv
http://arxiv.org/abs/2105.01852v2,2021-10-22T15:54:55Z,2021-05-05T03:28:57Z,Deep Learning for Needle Detection in a Cannulation Simulator,"Cannulation for hemodialysis is the act of inserting a needle into a
surgically created vascular access (e.g., an arteriovenous fistula) for the
purpose of dialysis. The main risk associated with cannulation is infiltration,
the puncture of the wall of the vascular access after entry, which can cause
medical complications. Simulator-based training allows clinicians to gain
cannulation experience without putting patients at risk. In this paper, we
propose to use deep-learning-based techniques for detecting, based on video,
whether the needle tip is in or has infiltrated the simulated fistula. Three
categories of deep neural networks are investigated in this work: modified
pre-trained models based on VGG-16 and ResNet-50, light convolutional neural
networks (light CNNs), and convolutional recurrent neural networks (CRNNs).
CRNNs consist of convolutional layers and a long short-term memory (LSTM)
layer. A data set of cannulation experiments was collected and analyzed. The
results show that both the light CNN and the CRNN achieve better performance
than the pre-trained baseline models. The CRNN was implemented in real time on
commodity hardware for use in the cannulation simulator, and the performance
was verified. Deep-learning video analysis is a viable method for detecting
needle state in a low cost cannulation simulator. Our data sets and code are
released at https://github.com/axin233/DL_for_Needle_Detection_Cannulation",arxiv
http://arxiv.org/abs/2003.09800v1,2020-03-22T03:58:05Z,2020-03-22T03:58:05Z,Forecasting and evaluating intervention of Covid-19 in the World,"When the Covid-19 pandemic enters dangerous new phase, whether and when to
take aggressive public health interventions to slow down the spread of
COVID-19. To develop the artificial intelligence (AI) inspired methods for
real-time forecasting and evaluating intervention strategies to curb the spread
of Covid-19 in the World. A modified auto-encoder for modeling the transmission
dynamics of the epidemics is developed and applied to the surveillance data of
cumulative and new Covid-19 cases and deaths from WHO, as of March 16, 2020.
The average errors of 5-step forecasting were 2.5%. The total peak number of
cumulative cases and new cases, and the maximum number of cumulative cases in
the world with later intervention (comprehensive public health intervention is
implemented 4 weeks later) could reach 75,249,909, 10,086,085, and 255,392,154,
respectively. The case ending time was January 10, 2021. However, the total
peak number of cumulative cases and new cases and the maximum number of
cumulative cases in the world with one week later intervention were reduced to
951,799, 108,853 and 1,530,276, respectively. Duration time of the Covid-19
spread would be reduced from 356 days to 232 days. The case ending time was
September 8, 2020. We observed that delaying intervention for one month caused
the maximum number of cumulative cases to increase 166.89 times, and the number
of deaths increase from 53,560 to 8,938,725. We will face disastrous
consequences if immediate action to intervene is not taken.",arxiv
http://arxiv.org/abs/1907.01734v1,2019-07-03T04:37:31Z,2019-07-03T04:37:31Z,"AMI-Net+: A Novel Multi-Instance Neural Network for Medical Diagnosis
  from Incomplete and Imbalanced Data","In medical real-world study (RWS), how to fully utilize the fragmentary and
scarce information in model training to generate the solid diagnosis results is
a challenging task. In this work, we introduce a novel multi-instance neural
network, AMI-Net+, to train and predict from the incomplete and extremely
imbalanced data. It is more effective than the state-of-art method, AMI-Net.
First, we also implement embedding, multi-head attention and gated
attention-based multi-instance pooling to capture the relations of symptoms
themselves and with the given disease. Besides, we propose var-ious
improvements to AMI-Net, that the cross-entropy loss is replaced by focal loss
and we propose a novel self-adaptive multi-instance pooling method on
instance-level to obtain the bag representation. We validate the performance of
AMI-Net+ on two real-world datasets, from two different medical domains.
Results show that our approach outperforms other base-line models by a
considerable margin.",arxiv
http://arxiv.org/abs/1902.02808v1,2019-02-07T19:15:51Z,2019-02-07T19:15:51Z,ML Health: Fitness Tracking for Production Models,"Deployment of machine learning (ML) algorithms in production for extended
periods of time has uncovered new challenges such as monitoring and management
of real-time prediction quality of a model in the absence of labels. However,
such tracking is imperative to prevent catastrophic business outcomes resulting
from incorrect predictions. The scale of these deployments makes manual
monitoring prohibitive, making automated techniques to track and raise alerts
imperative. We present a framework, ML Health, for tracking potential drops in
the predictive performance of ML models in the absence of labels. The framework
employs diagnostic methods to generate alerts for further investigation. We
develop one such method to monitor potential problems when production data
patterns do not match training data distributions. We demonstrate that our
method performs better than standard ""distance metrics"", such as RMSE,
KL-Divergence, and Wasserstein at detecting issues with mismatched data sets.
Finally, we present a working system that incorporates the ML Health approach
to monitor and manage ML deployments within a realistic full production ML
lifecycle.",arxiv
http://arxiv.org/abs/2108.07856v1,2021-08-17T20:01:33Z,2021-08-17T20:01:33Z,"OncoPetNet: A Deep Learning based AI system for mitotic figure counting
  on H&E stained whole slide digital images in a large veterinary diagnostic
  lab setting","Background: Histopathology is an important modality for the diagnosis and
management of many diseases in modern healthcare, and plays a critical role in
cancer care. Pathology samples can be large and require multi-site sampling,
leading to upwards of 20 slides for a single tumor, and the human-expert tasks
of site selection and and quantitative assessment of mitotic figures are time
consuming and subjective. Automating these tasks in the setting of a digital
pathology service presents significant opportunities to improve workflow
efficiency and augment human experts in practice. Approach: Multiple
state-of-the-art deep learning techniques for histopathology image
classification and mitotic figure detection were used in the development of
OncoPetNet. Additionally, model-free approaches were used to increase speed and
accuracy. The robust and scalable inference engine leverages Pytorch's
performance optimizations as well as specifically developed speed up techniques
in inference. Results: The proposed system, demonstrated significantly improved
mitotic counting performance for 41 cancer cases across 14 cancer types
compared to human expert baselines. In 21.9% of cases use of OncoPetNet led to
change in tumor grading compared to human expert evaluation. In deployment, an
effective 0.27 min/slide inference was achieved in a high throughput veterinary
diagnostic pathology service across 2 centers processing 3,323 digital whole
slide images daily. Conclusion: This work represents the first successful
automated deployment of deep learning systems for real-time expert-level
performance on important histopathology tasks at scale in a high volume
clinical practice. The resulting impact outlines important considerations for
model development, deployment, clinical decision making, and informs best
practices for implementation of deep learning systems in digital histopathology
practices.",arxiv
http://arxiv.org/abs/2108.00974v1,2021-08-02T15:22:05Z,2021-08-02T15:22:05Z,"Evaluating Federated Learning for Intrusion Detection in Internet of
  Things: Review and Challenges","The application of Machine Learning (ML) techniques to the well-known
intrusion detection systems (IDS) is key to cope with increasingly
sophisticated cybersecurity attacks through an effective and efficient
detection process. In the context of the Internet of Things (IoT), most
ML-enabled IDS approaches use centralized approaches where IoT devices share
their data with data centers for further analysis. To mitigate privacy concerns
associated with centralized approaches, in recent years the use of Federated
Learning (FL) has attracted a significant interest in different sectors,
including healthcare and transport systems. However, the development of
FL-enabled IDS for IoT is in its infancy, and still requires research efforts
from various areas, in order to identify the main challenges for the deployment
in real-world scenarios. In this direction, our work evaluates a FL-enabled IDS
approach based on a multiclass classifier considering different data
distributions for the detection of different attacks in an IoT scenario. In
particular, we use three different settings that are obtained by partitioning
the recent ToN\_IoT dataset according to IoT devices' IP address and types of
attack. Furthermore, we evaluate the impact of different aggregation functions
according to such setting by using the recent IBMFL framework as FL
implementation. Additionally, we identify a set of challenges and future
directions based on the existing literature and the analysis of our evaluation
results.",arxiv
http://arxiv.org/abs/2102.10435v1,2021-02-20T20:17:07Z,2021-02-20T20:17:07Z,"MHDeep: Mental Health Disorder Detection System based on Body-Area and
  Deep Neural Networks","Mental health problems impact quality of life of millions of people around
the world. However, diagnosis of mental health disorders is a challenging
problem that often relies on self-reporting by patients about their behavioral
patterns. Therefore, there is a need for new strategies for diagnosis of mental
health problems. The recent introduction of body-area networks consisting of a
plethora of accurate sensors embedded in smartwatches and smartphones and deep
neural networks (DNNs) points towards a possible solution. However, disease
diagnosis based on WMSs and DNNs, and their deployment on edge devices, remains
a challenging problem. To this end, we propose a framework called MHDeep that
utilizes commercially available WMSs and efficient DNN models to diagnose three
important mental health disorders: schizoaffective, major depressive, and
bipolar. MHDeep uses eight different categories of data obtained from sensors
integrated in a smartwatch and smartphone. Due to limited available data,
MHDeep uses a synthetic data generation module to augment real data with
synthetic data drawn from the same probability distribution. We use the
synthetic dataset to pre-train the DNN models, thus imposing a prior on the
weights. We use a grow-and-prune DNN synthesis approach to learn both the
architecture and weights during the training process. We use three different
data partitions to evaluate the MHDeep models trained with data collected from
74 individuals. We conduct data instance level and patient level evaluations.
MHDeep achieves an average test accuracy of 90.4%, 87.3%, and 82.4%,
respectively, for classifications between healthy instances and schizoaffective
disorder instances, major depressive disorder instances, and bipolar disorder
instances. At the patient level, MHDeep DNNs achieve an accuracy of 100%, 100%,
and 90.0% for the three mental health disorders, respectively.",arxiv
http://arxiv.org/abs/1804.07886v1,2018-04-21T04:16:46Z,2018-04-21T04:16:46Z,Social Bots for Online Public Health Interventions,"According to the Center for Disease Control and Prevention, in the United
States hundreds of thousands initiate smoking each year, and millions live with
smoking-related dis- eases. Many tobacco users discuss their habits and
preferences on social media. This work conceptualizes a framework for targeted
health interventions to inform tobacco users about the consequences of tobacco
use. We designed a Twitter bot named Notobot (short for No-Tobacco Bot) that
leverages machine learning to identify users posting pro-tobacco tweets and
select individualized interventions to address their interest in tobacco use.
We searched the Twitter feed for tobacco-related keywords and phrases, and
trained a convolutional neural network using over 4,000 tweets dichotomously
manually labeled as either pro- tobacco or not pro-tobacco. This model achieves
a 90% recall rate on the training set and 74% on test data. Users posting pro-
tobacco tweets are matched with former smokers with similar interests who
posted anti-tobacco tweets. Algorithmic matching, based on the power of peer
influence, allows for the systematic delivery of personalized interventions
based on real anti-tobacco tweets from former smokers. Experimental evaluation
suggests that our system would perform well if deployed. This research offers
opportunities for public health researchers to increase health awareness at
scale. Future work entails deploying the fully operational Notobot system in a
controlled experiment within a public health campaign.",arxiv
http://arxiv.org/abs/2108.08762v1,2021-08-19T16:06:16Z,2021-08-19T16:06:16Z,"Dynamic Difficulty Adjustment in Virtual Reality Exergames through
  Experience-driven Procedural Content Generation","Virtual Reality (VR) games that feature physical activities have been shown
to increase players' motivation to do physical exercise. However, for such
exercises to have a positive healthcare effect, they have to be repeated
several times a week. To maintain player motivation over longer periods of
time, games often employ Dynamic Difficulty Adjustment (DDA) to adapt the
game's challenge according to the player's capabilities. For exercise games,
this is mostly done by tuning specific in-game parameters like the speed of
objects. In this work, we propose to use experience-driven Procedural Content
Generation for DDA in VR exercise games by procedurally generating levels that
match the player's current capabilities. Not only finetuning specific
parameters but creating completely new levels has the potential to decrease
repetition over longer time periods and allows for the simultaneous adaptation
of the cognitive and physical challenge of the exergame. As a proof-of-concept,
we implement an initial prototype in which the player must traverse a maze that
includes several exercise rooms, whereby the generation of the maze is realized
by a neural network. Passing those exercise rooms requires the player to
perform physical activities. To match the player's capabilities, we use Deep
Reinforcement Learning to adjust the structure of the maze and to decide which
exercise rooms to include in the maze. We evaluate our prototype in an
exploratory user study utilizing both biodata and subjective questionnaires.",arxiv
http://arxiv.org/abs/1901.06242v1,2018-12-01T13:40:03Z,2018-12-01T13:40:03Z,"Data-driven Air Quality Characterisation for Urban Environments: a Case
  Study","The economic and social impact of poor air quality in towns and cities is
increasingly being recognised, together with the need for effective ways of
creating awareness of real-time air quality levels and their impact on human
health. With local authority maintained monitoring stations being
geographically sparse and the resultant datasets also featuring missing labels,
computational data-driven mechanisms are needed to address the data sparsity
challenge. In this paper, we propose a machine learning-based method to
accurately predict the Air Quality Index (AQI), using environmental monitoring
data together with meteorological measurements. To do so, we develop an air
quality estimation framework that implements a neural network that is enhanced
with a novel Non-linear Autoregressive neural network with exogenous input
(NARX), especially designed for time series prediction. The framework is
applied to a case study featuring different monitoring sites in London, with
comparisons against other standard machine-learning based predictive algorithms
showing the feasibility and robust performance of the proposed method for
different kinds of areas within an urban region.",arxiv
http://arxiv.org/abs/2111.01950v2,2021-11-10T10:08:13Z,2021-11-02T23:51:06Z,"Machine-Learning Identification of Hemodynamics in Coronary Arteries in
  the Presence of Stenosis","Prediction of the blood flow characteristics is of utmost importance for
understanding the behavior of the blood arterial network, especially in the
presence of vascular diseases such as stenosis. Computational fluid dynamics
(CFD) has provided a powerful and efficient tool to determine these
characteristics including the pressure and velocity fields within the network.
Despite numerous studies in the field, the extremely high computational cost of
CFD has led the researchers to develop new platforms including Machine Learning
approaches that instead provide faster analyses at a much lower cost. In this
study, we put forth a Deep Neural Network framework to predict flow behavior in
a coronary arterial network with different properties in the presence of any
abnormality like stenosis. To this end, an artificial neural network (ANN)
model is trained using synthetic data so that it can predict the pressure and
velocity within the arterial network. The data required to train the neural
network were obtained from the CFD analysis of several geometries of arteries
with specific features in ABAQUS software. Blood pressure drop caused by
stenosis, which is one of the most important factors in the diagnosis of heart
diseases, can be predicted using our proposed model knowing the geometrical and
flow boundary conditions of any section of the coronary arteries. The
efficiency of the model was verified using three real geometries of LAD's
vessels. The proposed approach precisely predicts the hemodynamic behavior of
the blood flow. The average accuracy of the pressure prediction was 98.7% and
the average velocity magnitude accuracy was 93.2%. According to the results of
testing the model on three patient-specific geometries, model can be considered
as an alternative to finite element methods as well as other hard-to-implement
and time-consuming numerical simulations.",arxiv
http://arxiv.org/abs/2101.02780v1,2021-01-07T22:01:30Z,2021-01-07T22:01:30Z,"SHARKS: Smart Hacking Approaches for RisK Scanning in Internet-of-Things
  and Cyber-Physical Systems based on Machine Learning","Cyber-physical systems (CPS) and Internet-of-Things (IoT) devices are
increasingly being deployed across multiple functionalities, ranging from
healthcare devices and wearables to critical infrastructures, e.g., nuclear
power plants, autonomous vehicles, smart cities, and smart homes. These devices
are inherently not secure across their comprehensive software, hardware, and
network stacks, thus presenting a large attack surface that can be exploited by
hackers. In this article, we present an innovative technique for detecting
unknown system vulnerabilities, managing these vulnerabilities, and improving
incident response when such vulnerabilities are exploited. The novelty of this
approach lies in extracting intelligence from known real-world CPS/IoT attacks,
representing them in the form of regular expressions, and employing machine
learning (ML) techniques on this ensemble of regular expressions to generate
new attack vectors and security vulnerabilities. Our results show that 10 new
attack vectors and 122 new vulnerability exploits can be successfully generated
that have the potential to exploit a CPS or an IoT ecosystem. The ML
methodology achieves an accuracy of 97.4% and enables us to predict these
attacks efficiently with an 87.2% reduction in the search space. We demonstrate
the application of our method to the hacking of the in-vehicle network of a
connected car. To defend against the known attacks and possible novel exploits,
we discuss a defense-in-depth mechanism for various classes of attacks and the
classification of data targeted by such attacks. This defense mechanism
optimizes the cost of security measures based on the sensitivity of the
protected resource, thus incentivizing its adoption in real-world CPS/IoT by
cybersecurity practitioners.",arxiv
http://arxiv.org/abs/2102.09548v2,2021-08-28T19:59:03Z,2021-02-18T18:50:31Z,"Therapeutics Data Commons: Machine Learning Datasets and Tasks for Drug
  Discovery and Development","Therapeutics machine learning is an emerging field with incredible
opportunities for innovatiaon and impact. However, advancement in this field
requires formulation of meaningful learning tasks and careful curation of
datasets. Here, we introduce Therapeutics Data Commons (TDC), the first
unifying platform to systematically access and evaluate machine learning across
the entire range of therapeutics. To date, TDC includes 66 AI-ready datasets
spread across 22 learning tasks and spanning the discovery and development of
safe and effective medicines. TDC also provides an ecosystem of tools and
community resources, including 33 data functions and types of meaningful data
splits, 23 strategies for systematic model evaluation, 17 molecule generation
oracles, and 29 public leaderboards. All resources are integrated and
accessible via an open Python library. We carry out extensive experiments on
selected datasets, demonstrating that even the strongest algorithms fall short
of solving key therapeutics challenges, including real dataset distributional
shifts, multi-scale modeling of heterogeneous data, and robust generalization
to novel data points. We envision that TDC can facilitate algorithmic and
scientific advances and considerably accelerate machine-learning model
development, validation and transition into biomedical and clinical
implementation. TDC is an open-science initiative available at
https://tdcommons.ai.",arxiv
http://arxiv.org/abs/2003.12881v1,2020-03-28T19:57:55Z,2020-03-28T19:57:55Z,"Streamlined Empirical Bayes Fitting of Linear Mixed Models in Mobile
  Health","To effect behavior change a successful algorithm must make high-quality
decisions in real-time. For example, a mobile health (mHealth) application
designed to increase physical activity must make contextually relevant
suggestions to motivate users. While machine learning offers solutions for
certain stylized settings, such as when batch data can be processed offline,
there is a dearth of approaches which can deliver high-quality solutions under
the specific constraints of mHealth. We propose an algorithm which provides
users with contextualized and personalized physical activity suggestions. This
algorithm is able to overcome a challenge critical to mHealth that complex
models be trained efficiently. We propose a tractable streamlined empirical
Bayes procedure which fits linear mixed effects models in large-data settings.
Our procedure takes advantage of sparsity introduced by hierarchical random
effects to efficiently learn the posterior distribution of a linear mixed
effects model. A key contribution of this work is that we provide explicit
updates in order to learn both fixed effects, random effects and
hyper-parameter values. We demonstrate the success of this approach in a mobile
health (mHealth) reinforcement learning application, a domain in which fast
computations are crucial for real time interventions. Not only is our approach
computationally efficient, it is also easily implemented with closed form
matrix algebraic updates and we show improvements over state of the art
approaches both in speed and accuracy of up to 99% and 56% respectively.",arxiv
http://arxiv.org/abs/2011.11557v1,2020-11-23T17:11:50Z,2020-11-23T17:11:50Z,"Planar 3D Transfer Learning for End to End Unimodal MRI Unbalanced Data
  Segmentation","We present a novel approach of 2D to 3D transfer learning based on mapping
pre-trained 2D convolutional neural network weights into planar 3D kernels. The
method is validated by the proposed planar 3D res-u-net network with encoder
transferred from the 2D VGG-16, which is applied for a single-stage unbalanced
3D image data segmentation. In particular, we evaluate the method on the MICCAI
2016 MS lesion segmentation challenge dataset utilizing solely fluid-attenuated
inversion recovery (FLAIR) sequence without brain extraction for training and
inference to simulate real medical praxis. The planar 3D res-u-net network
performed the best both in sensitivity and Dice score amongst end to end
methods processing raw MRI scans and achieved comparable Dice score to a
state-of-the-art unimodal not end to end approach. Complete source code was
released under the open-source license, and this paper complies with the
Machine learning reproducibility checklist. By implementing practical transfer
learning for 3D data representation, we could segment heavily unbalanced data
without selective sampling and achieved more reliable results using less
training data in a single modality. From a medical perspective, the unimodal
approach gives an advantage in real praxis as it does not require
co-registration nor additional scanning time during an examination. Although
modern medical imaging methods capture high-resolution 3D anatomy scans
suitable for computer-aided detection system processing, deployment of
automatic systems for interpretation of radiology imaging is still rather
theoretical in many medical areas. Our work aims to bridge the gap by offering
a solution for partial research questions.",arxiv
http://arxiv.org/abs/1805.08960v1,2018-05-23T04:52:06Z,2018-05-23T04:52:06Z,ICADx: Interpretable computer aided diagnosis of breast masses,"In this study, a novel computer aided diagnosis (CADx) framework is devised
to investigate interpretability for classifying breast masses. Recently, a deep
learning technology has been successfully applied to medical image analysis
including CADx. Existing deep learning based CADx approaches, however, have a
limitation in explaining the diagnostic decision. In real clinical practice,
clinical decisions could be made with reasonable explanation. So current deep
learning approaches in CADx are limited in real world deployment. In this
paper, we investigate interpretability in CADx with the proposed interpretable
CADx (ICADx) framework. The proposed framework is devised with a generative
adversarial network, which consists of interpretable diagnosis network and
synthetic lesion generative network to learn the relationship between
malignancy and a standardized description (BI-RADS). The lesion generative
network and the interpretable diagnosis network compete in an adversarial
learning so that the two networks are improved. The effectiveness of the
proposed method was validated on public mammogram database. Experimental
results showed that the proposed ICADx framework could provide the
interpretability of mass as well as mass classification. It was mainly
attributed to the fact that the proposed method was effectively trained to find
the relationship between malignancy and interpretations via the adversarial
learning. These results imply that the proposed ICADx framework could be a
promising approach to develop the CADx system.",arxiv
http://arxiv.org/abs/2106.03211v2,2021-06-10T22:04:36Z,2021-06-06T18:57:30Z,Distributed Learning and its Application for Time-Series Prediction,"Extreme events are occurrences whose magnitude and potential cause extensive
damage on people, infrastructure, and the environment. Motivated by the extreme
nature of the current global health landscape, which is plagued by the
coronavirus pandemic, we seek to better understand and model extreme events.
Modeling extreme events is common in practice and plays an important role in
time-series prediction applications. Our goal is to (i) compare and investigate
the effect of some common extreme events modeling methods to explore which
method can be practical in reality and (ii) accelerate the deep learning
training process, which commonly uses deep recurrent neural network (RNN), by
implementing the asynchronous local Stochastic Gradient Descent (SGD) framework
among multiple compute nodes. In order to verify our distributed extreme events
modeling, we evaluate our proposed framework on a stock data set S\&P500, with
a standard recurrent neural network. Our intuition is to explore the (best)
extreme events modeling method which could work well under the distributed deep
learning setting. Moreover, by using asynchronous distributed learning, we aim
to significantly reduce the communication cost among the compute nodes and
central server, which is the main bottleneck of almost all distributed learning
frameworks.
  We implement our proposed work and evaluate its performance on representative
data sets, such as S&P500 stock in $5$-year period. The experimental results
validate the correctness of the design principle and show a significant
training duration reduction upto $8$x, compared to the baseline single compute
node. Our results also show that our proposed work can achieve the same level
of test accuracy, compared to the baseline setting.",arxiv
http://arxiv.org/abs/2008.01774v2,2020-11-04T02:36:36Z,2020-08-04T19:20:31Z,"An artificial intelligence system for predicting the deterioration of
  COVID-19 patients in the emergency department","During the coronavirus disease 2019 (COVID-19) pandemic, rapid and accurate
triage of patients at the emergency department is critical to inform
decision-making. We propose a data-driven approach for automatic prediction of
deterioration risk using a deep neural network that learns from chest X-ray
images and a gradient boosting model that learns from routine clinical
variables. Our AI prognosis system, trained using data from 3,661 patients,
achieves an area under the receiver operating characteristic curve (AUC) of
0.786 (95% CI: 0.745-0.830) when predicting deterioration within 96 hours. The
deep neural network extracts informative areas of chest X-ray images to assist
clinicians in interpreting the predictions and performs comparably to two
radiologists in a reader study. In order to verify performance in a real
clinical setting, we silently deployed a preliminary version of the deep neural
network at New York University Langone Health during the first wave of the
pandemic, which produced accurate predictions in real-time. In summary, our
findings demonstrate the potential of the proposed system for assisting
front-line physicians in the triage of COVID-19 patients.",arxiv
http://arxiv.org/abs/2106.06048v3,2021-11-07T15:41:11Z,2021-06-04T14:30:39Z,"Optimizing Bayesian Recurrent Neural Networks on an FPGA-based
  Accelerator","Neural networks have demonstrated their outstanding performance in a wide
range of tasks. Specifically recurrent architectures based on long-short term
memory (LSTM) cells have manifested excellent capability to model time
dependencies in real-world data. However, standard recurrent architectures
cannot estimate their uncertainty which is essential for safety-critical
applications such as in medicine. In contrast, Bayesian recurrent neural
networks (RNNs) are able to provide uncertainty estimation with improved
accuracy. Nonetheless, Bayesian RNNs are computationally and memory demanding,
which limits their practicality despite their advantages. To address this
issue, we propose an FPGA-based hardware design to accelerate Bayesian
LSTM-based RNNs. To further improve the overall algorithmic-hardware
performance, a co-design framework is proposed to explore the most fitting
algorithmic-hardware configurations for Bayesian RNNs. We conduct extensive
experiments on healthcare applications to demonstrate the improvement of our
design and the effectiveness of our framework. Compared with GPU
implementation, our FPGA-based design can achieve up to 10 times speedup with
nearly 106 times higher energy efficiency. To the best of our knowledge, this
is the first work targeting acceleration of Bayesian RNNs on FPGAs.",arxiv
http://arxiv.org/abs/2009.11433v1,2020-09-24T01:04:18Z,2020-09-24T01:04:18Z,Unifying data for fine-grained visual species classification,"Wildlife monitoring is crucial to nature conservation and has been done by
manual observations from motion-triggered camera traps deployed in the field.
Widespread adoption of such in-situ sensors has resulted in unprecedented data
volumes being collected over the last decade. A significant challenge exists to
process and reliably identify what is in these images efficiently. Advances in
computer vision are poised to provide effective solutions with custom AI models
built to automatically identify images of interest and label the species in
them. Here we outline the data unification effort for the Wildlife Insights
platform from various conservation partners, and the challenges involved. Then
we present an initial deep convolutional neural network model, trained on 2.9M
images across 465 fine-grained species, with a goal to reduce the load on human
experts to classify species in images manually. The long-term goal is to enable
scientists to make conservation recommendations from near real-time analysis of
species abundance and population health.",arxiv
http://arxiv.org/abs/2010.01165v2,2021-03-25T13:21:50Z,2020-10-02T19:01:02Z,"Multi-domain Clinical Natural Language Processing with MedCAT: the
  Medical Concept Annotation Toolkit","Electronic health records (EHR) contain large volumes of unstructured text,
requiring the application of Information Extraction (IE) technologies to enable
clinical analysis. We present the open-source Medical Concept Annotation
Toolkit (MedCAT) that provides: a) a novel self-supervised machine learning
algorithm for extracting concepts using any concept vocabulary including
UMLS/SNOMED-CT; b) a feature-rich annotation interface for customising and
training IE models; and c) integrations to the broader CogStack ecosystem for
vendor-agnostic health system deployment. We show improved performance in
extracting UMLS concepts from open datasets (F1:0.448-0.738 vs 0.429-0.650).
Further real-world validation demonstrates SNOMED-CT extraction at 3 large
London hospitals with self-supervised training over ~8.8B words from ~17M
clinical records and further fine-tuning with ~6K clinician annotated examples.
We show strong transferability (F1 > 0.94) between hospitals, datasets, and
concept types indicating cross-domain EHR-agnostic utility for accelerated
clinical and research use cases.",arxiv
http://arxiv.org/abs/1204.1653v1,2012-04-07T16:34:20Z,2012-04-07T16:34:20Z,Machine Cognition Models: EPAM and GPS,"Through history, the human being tried to relay its daily tasks to other
creatures, which was the main reason behind the rise of civilizations. It
started with deploying animals to automate tasks in the field of
agriculture(bulls), transportation (e.g. horses and donkeys), and even
communication (pigeons). Millenniums after, come the Golden age with
""Al-jazari"" and other Muslim inventors, which were the pioneers of automation,
this has given birth to industrial revolution in Europe, centuries after. At
the end of the nineteenth century, a new era was to begin, the computational
era, the most advanced technological and scientific development that is driving
the mankind and the reason behind all the evolutions of science; such as
medicine, communication, education, and physics. At this edge of technology
engineers and scientists are trying to model a machine that behaves the same as
they do, which pushed us to think about designing and implementing ""Things
that-Thinks"", then artificial intelligence was. In this work we will cover each
of the major discoveries and studies in the field of machine cognition, which
are the ""Elementary Perceiver and Memorizer""(EPAM) and ""The General Problem
Solver""(GPS). The First one focus mainly on implementing the human-verbal
learning behavior, while the second one tries to model an architecture that is
able to solve problems generally (e.g. theorem proving, chess playing, and
arithmetic). We will cover the major goals and the main ideas of each model, as
well as comparing their strengths and weaknesses, and finally giving their
fields of applications. And Finally, we will suggest a real life implementation
of a cognitive machine.",arxiv
http://arxiv.org/abs/2107.11003v1,2021-07-23T02:41:51Z,2021-07-23T02:41:51Z,"Model Selection for Offline Reinforcement Learning: Practical
  Considerations for Healthcare Settings","Reinforcement learning (RL) can be used to learn treatment policies and aid
decision making in healthcare. However, given the need for generalization over
complex state/action spaces, the incorporation of function approximators (e.g.,
deep neural networks) requires model selection to reduce overfitting and
improve policy performance at deployment. Yet a standard validation pipeline
for model selection requires running a learned policy in the actual
environment, which is often infeasible in a healthcare setting. In this work,
we investigate a model selection pipeline for offline RL that relies on
off-policy evaluation (OPE) as a proxy for validation performance. We present
an in-depth analysis of popular OPE methods, highlighting the additional
hyperparameters and computational requirements (fitting/inference of auxiliary
models) when used to rank a set of candidate policies. We compare the utility
of different OPE methods as part of the model selection pipeline in the context
of learning to treat patients with sepsis. Among all the OPE methods we
considered, fitted Q evaluation (FQE) consistently leads to the best validation
ranking, but at a high computational cost. To balance this trade-off between
accuracy of ranking and computational efficiency, we propose a simple two-stage
approach to accelerate model selection by avoiding potentially unnecessary
computation. Our work serves as a practical guide for offline RL model
selection and can help RL practitioners select policies using real-world
datasets. To facilitate reproducibility and future extensions, the code
accompanying this paper is available online at
https://github.com/MLD3/OfflineRL_ModelSelection.",arxiv
http://arxiv.org/abs/2012.14704v1,2020-12-29T11:10:12Z,2020-12-29T11:10:12Z,"Advances in deep learning methods for pavement surface crack detection
  and identification with visible light visual images","Compared to NDT and health monitoring method for cracks in engineering
structures, surface crack detection or identification based on visible light
images is non-contact, with the advantages of fast speed, low cost and high
precision. Firstly, typical pavement (concrete also) crack public data sets
were collected, and the characteristics of sample images as well as the random
variable factors, including environmental, noise and interference etc., were
summarized. Subsequently, the advantages and disadvantages of three main crack
identification methods (i.e., hand-crafted feature engineering, machine
learning, deep learning) were compared. Finally, from the aspects of model
architecture, testing performance and predicting effectiveness, the development
and progress of typical deep learning models, including self-built CNN,
transfer learning(TL) and encoder-decoder(ED), which can be easily deployed on
embedded platform, were reviewed. The benchmark test shows that: 1) It has been
able to realize real-time pixel-level crack identification on embedded
platform: the entire crack detection average time cost of an image sample is
less than 100ms, either using the ED method (i.e., FPCNet) or the TL method
based on InceptionV3. It can be reduced to less than 10ms with TL method based
on MobileNet (a lightweight backbone base network). 2) In terms of accuracy, it
can reach over 99.8% on CCIC which is easily identified by human eyes. On
SDNET2018, some samples of which are difficult to be identified, FPCNet can
reach 97.5%, while TL method is close to 96.1%.
  To the best of our knowledge, this paper for the first time comprehensively
summarizes the pavement crack public data sets, and the performance and
effectiveness of surface crack detection and identification deep learning
methods for embedded platform, are reviewed and evaluated.",arxiv
http://arxiv.org/abs/2106.13219v1,2021-06-24T17:52:43Z,2021-06-24T17:52:43Z,Towards Understanding and Mitigating Social Biases in Language Models,"As machine learning methods are deployed in real-world settings such as
healthcare, legal systems, and social science, it is crucial to recognize how
they shape social biases and stereotypes in these sensitive decision-making
processes. Among such real-world deployments are large-scale pretrained
language models (LMs) that can be potentially dangerous in manifesting
undesirable representational biases - harmful biases resulting from
stereotyping that propagate negative generalizations involving gender, race,
religion, and other social constructs. As a step towards improving the fairness
of LMs, we carefully define several sources of representational biases before
proposing new benchmarks and metrics to measure them. With these tools, we
propose steps towards mitigating social biases during text generation. Our
empirical results and human evaluation demonstrate effectiveness in mitigating
bias while retaining crucial contextual information for high-fidelity text
generation, thereby pushing forward the performance-fairness Pareto frontier.",arxiv
http://arxiv.org/abs/2107.02359v3,2021-07-15T18:35:40Z,2021-07-06T02:44:40Z,"Leveraging Clinical Context for User-Centered Explainability: A Diabetes
  Use Case","Academic advances of AI models in high-precision domains, like healthcare,
need to be made explainable in order to enhance real-world adoption. Our past
studies and ongoing interactions indicate that medical experts can use AI
systems with greater trust if there are ways to connect the model inferences
about patients to explanations that are tied back to the context of use.
Specifically, risk prediction is a complex problem of diagnostic and
interventional importance to clinicians wherein they consult different sources
to make decisions. To enable the adoption of the ever improving AI risk
prediction models in practice, we have begun to explore techniques to
contextualize such models along three dimensions of interest: the patients'
clinical state, AI predictions about their risk of complications, and
algorithmic explanations supporting the predictions. We validate the importance
of these dimensions by implementing a proof-of-concept (POC) in type-2 diabetes
(T2DM) use case where we assess the risk of chronic kidney disease (CKD) - a
common T2DM comorbidity. Within the POC, we include risk prediction models for
CKD, post-hoc explainers of the predictions, and other natural-language modules
which operationalize domain knowledge and CPGs to provide context. With primary
care physicians (PCP) as our end-users, we present our initial results and
clinician feedback in this paper. Our POC approach covers multiple knowledge
sources and clinical scenarios, blends knowledge to explain data and
predictions to PCPs, and received an enthusiastic response from our medical
expert.",arxiv
http://arxiv.org/abs/2104.10715v1,2021-04-21T18:28:13Z,2021-04-21T18:28:13Z,Uncertainty-Aware Boosted Ensembling in Multi-Modal Settings,"Reliability of machine learning (ML) systems is crucial in safety-critical
applications such as healthcare, and uncertainty estimation is a widely
researched method to highlight the confidence of ML systems in deployment.
Sequential and parallel ensemble techniques have shown improved performance of
ML systems in multi-modal settings by leveraging the feature sets together. We
propose an uncertainty-aware boosting technique for multi-modal ensembling in
order to focus on the data points with higher associated uncertainty estimates,
rather than the ones with higher loss values. We evaluate this method on
healthcare tasks related to Dementia and Parkinson's disease which involve
real-world multi-modal speech and text data, wherein our method shows an
improved performance. Additional analysis suggests that introducing
uncertainty-awareness into the boosted ensembles decreases the overall entropy
of the system, making it more robust to heteroscedasticity in the data, as well
as better calibrating each of the modalities along with high quality prediction
intervals. We open-source our entire codebase at
https://github.com/usarawgi911/Uncertainty-aware-boosting",arxiv
http://arxiv.org/abs/2104.12844v1,2021-04-26T19:47:03Z,2021-04-26T19:47:03Z,"LCS-DIVE: An Automated Rule-based Machine Learning Visualization
  Pipeline for Characterizing Complex Associations in Classification","Machine learning (ML) research has yielded powerful tools for training
accurate prediction models despite complex multivariate associations (e.g.
interactions and heterogeneity). In fields such as medicine, improved
interpretability of ML modeling is required for knowledge discovery,
accountability, and fairness. Rule-based ML approaches such as Learning
Classifier Systems (LCSs) strike a balance between predictive performance and
interpretability in complex, noisy domains. This work introduces the LCS
Discovery and Visualization Environment (LCS-DIVE), an automated LCS model
interpretation pipeline for complex biomedical classification. LCS-DIVE
conducts modeling using a new scikit-learn implementation of ExSTraCS, an LCS
designed to overcome noise and scalability in biomedical data mining yielding
human readable IF:THEN rules as well as feature-tracking scores for each
training sample. LCS-DIVE leverages feature-tracking scores and/or rules to
automatically guide characterization of (1) feature importance (2) underlying
additive, epistatic, and/or heterogeneous patterns of association, and (3)
model-driven heterogeneous instance subgroups via clustering, visualization
generation, and cluster interrogation. LCS-DIVE was evaluated over a diverse
set of simulated genetic and benchmark datasets encoding a variety of complex
multivariate associations, demonstrating its ability to differentiate between
them and then applied to characterize associations within a real-world study of
pancreatic cancer.",arxiv
http://arxiv.org/abs/1805.00917v3,2018-11-19T22:57:54Z,2018-05-02T17:26:09Z,A Scalable Discrete-Time Survival Model for Neural Networks,"There is currently great interest in applying neural networks to prediction
tasks in medicine. It is important for predictive models to be able to use
survival data, where each patient has a known follow-up time and
event/censoring indicator. This avoids information loss when training the model
and enables generation of predicted survival curves. In this paper, we describe
a discrete-time survival model that is designed to be used with neural
networks, which we refer to as Nnet-survival. The model is trained with the
maximum likelihood method using minibatch stochastic gradient descent (SGD).
The use of SGD enables rapid convergence and application to large datasets that
do not fit in memory. The model is flexible, so that the baseline hazard rate
and the effect of the input data on hazard probability can vary with follow-up
time. It has been implemented in the Keras deep learning framework, and source
code for the model and several examples is available online. We demonstrate the
performance of the model on both simulated and real data and compare it to
existing models Cox-nnet and Deepsurv.",arxiv
http://arxiv.org/abs/2110.01863v1,2021-10-05T07:55:19Z,2021-10-05T07:55:19Z,"DeepEdge: A Deep Reinforcement Learning based Task Orchestrator for Edge
  Computing","The improvements in the edge computing technology pave the road for
diversified applications that demand real-time interaction. However, due to the
mobility of the end-users and the dynamic edge environment, it becomes
challenging to handle the task offloading with high performance. Moreover,
since each application in mobile devices has different characteristics, a task
orchestrator must be adaptive and have the ability to learn the dynamics of the
environment. For this purpose, we develop a deep reinforcement learning based
task orchestrator, DeepEdge, which learns to meet different task requirements
without needing human interaction even under the heavily-loaded stochastic
network conditions in terms of mobile users and applications. Given the dynamic
offloading requests and time-varying communication conditions, we successfully
model the problem as a Markov process and then apply the Double Deep Q-Network
(DDQN) algorithm to implement DeepEdge. To evaluate the robustness of DeepEdge,
we experiment with four different applications including image rendering,
infotainment, pervasive health, and augmented reality in the network under
various loads. Furthermore, we compare the performance of our agent with the
four different task offloading approaches in the literature. Our results show
that DeepEdge outperforms its competitors in terms of the percentage of
satisfactorily completed tasks.",arxiv
http://arxiv.org/abs/2002.04700v4,2020-03-15T03:27:52Z,2020-02-11T21:42:22Z,"A Single RGB Camera Based Gait Analysis with a Mobile Tele-Robot for
  Healthcare","With the increasing awareness of high-quality life, there is a growing need
for health monitoring devices running robust algorithms in home environment.
Health monitoring technologies enable real-time analysis of users' health
status, offering long-term healthcare support and reducing hospitalization
time. The purpose of this work is twofold, the software focuses on the analysis
of gait, which is widely adopted for joint correction and assessing any lower
limb or spinal problem. On the hardware side, we design a novel marker-less
gait analysis device using a low-cost RGB camera mounted on a mobile
tele-robot. As gait analysis with a single camera is much more challenging
compared to previous works utilizing multi-cameras, a RGB-D camera or wearable
sensors, we propose using vision-based human pose estimation approaches. More
specifically, based on the output of two state-of-the-art human pose estimation
models (Openpose and VNect), we devise measurements for four bespoke gait
parameters: inversion/eversion, dorsiflexion/plantarflexion, ankle and foot
progression angles. We thereby classify walking patterns into normal,
supination, pronation and limp. We also illustrate how to run the purposed
machine learning models in low-resource environments such as a single
entry-level CPU. Experiments show that our single RGB camera method achieves
competitive performance compared to state-of-the-art methods based on depth
cameras or multi-camera motion capture system, at smaller hardware costs.",arxiv
http://arxiv.org/abs/1610.09704v1,2016-10-30T20:09:46Z,2016-10-30T20:09:46Z,Feature-Augmented Neural Networks for Patient Note De-identification,"Patient notes contain a wealth of information of potentially great interest
to medical investigators. However, to protect patients' privacy, Protected
Health Information (PHI) must be removed from the patient notes before they can
be legally released, a process known as patient note de-identification. The
main objective for a de-identification system is to have the highest possible
recall. Recently, the first neural-network-based de-identification system has
been proposed, yielding state-of-the-art results. Unlike other systems, it does
not rely on human-engineered features, which allows it to be quickly deployed,
but does not leverage knowledge from human experts or from electronic health
records (EHRs). In this work, we explore a method to incorporate
human-engineered features as well as features derived from EHRs to a
neural-network-based de-identification system. Our results show that the
addition of features, especially the EHR-derived features, further improves the
state-of-the-art in patient note de-identification, including for some of the
most sensitive PHI types such as patient names. Since in a real-life setting
patient notes typically come with EHRs, we recommend developers of
de-identification systems to leverage the information EHRs contain.",arxiv
http://arxiv.org/abs/1811.06672v1,2018-11-16T03:59:22Z,2018-11-16T03:59:22Z,Detecting Irregular Patterns in IoT Streaming Data for Fall Detection,"Detecting patterns in real time streaming data has been an interesting and
challenging data analytics problem. With the proliferation of a variety of
sensor devices, real-time analytics of data from the Internet of Things (IoT)
to learn regular and irregular patterns has become an important machine
learning problem to enable predictive analytics for automated notification and
decision support. In this work, we address the problem of learning an irregular
human activity pattern, fall, from streaming IoT data from wearable sensors. We
present a deep neural network model for detecting fall based on accelerometer
data giving 98.75 percent accuracy using an online physical activity monitoring
dataset called ""MobiAct"", which was published by Vavoulas et al. The initial
model was developed using IBM Watson studio and then later transferred and
deployed on IBM Cloud with the streaming analytics service supported by IBM
Streams for monitoring real-time IoT data. We also present the systems
architecture of the real-time fall detection framework that we intend to use
with mbientlabs wearable health monitoring sensors for real time patient
monitoring at retirement homes or rehabilitation clinics.",arxiv
http://arxiv.org/abs/1802.10458v2,2018-11-01T12:43:02Z,2018-02-27T02:24:06Z,"A High GOPs/Slice Time Series Classifier for Portable and Embedded
  Biomedical Applications","Nowadays a diverse range of physiological data can be captured continuously
for various applications in particular wellbeing and healthcare. Such data
require efficient methods for classification and analysis. Deep learning
algorithms have shown remarkable potential regarding such analyses, however,
the use of these algorithms on low-power wearable devices is challenged by
resource constraints such as area and power consumption. Most of the available
on-chip deep learning processors contain complex and dense hardware
architectures in order to achieve the highest possible throughput. Such a trend
in hardware design may not be efficient in applications where on-node
computation is required and the focus is more on the area and power efficiency
as in the case of portable and embedded biomedical devices. This paper presents
an efficient time-series classifier capable of automatically detecting
effective features and classifying the input signals in real-time. In the
proposed classifier, throughput is traded off with hardware complexity and cost
using resource sharing techniques. A Convolutional Neural Network (CNN) is
employed to extract input features and then a Long-Short-Term-Memory (LSTM)
architecture with ternary weight precision classifies the input signals
according to the extracted features. Hardware implementation on a Xilinx FPGA
confirm that the proposed hardware can accurately classify multiple complex
biomedical time series data with low area and power consumption and outperform
all previously presented state-of-the-art records. Most notably, our classifier
reaches 1.3$\times$ higher GOPs/Slice than similar state of the art FPGA-based
accelerators.",arxiv
http://arxiv.org/abs/2011.11719v3,2021-09-02T10:10:08Z,2020-11-23T20:51:22Z,"Explainable-by-design Semi-Supervised Representation Learning for
  COVID-19 Diagnosis from CT Imaging","Our motivating application is a real-world problem: COVID-19 classification
from CT imaging, for which we present an explainable Deep Learning approach
based on a semi-supervised classification pipeline that employs variational
autoencoders to extract efficient feature embedding. We have optimized the
architecture of two different networks for CT images: (i) a novel conditional
variational autoencoder (CVAE) with a specific architecture that integrates the
class labels inside the encoder layers and uses side information with shared
attention layers for the encoder, which make the most of the contextual clues
for representation learning, and (ii) a downstream convolutional neural network
for supervised classification using the encoder structure of the CVAE. With the
explainable classification results, the proposed diagnosis system is very
effective for COVID-19 classification. Based on the promising results obtained
qualitatively and quantitatively, we envisage a wide deployment of our
developed technique in large-scale clinical studies.Code is available at
https://git.etrovub.be/AVSP/ct-based-covid-19-diagnostic-tool.git.",arxiv
http://arxiv.org/abs/1810.04538v1,2018-10-10T14:04:08Z,2018-10-10T14:04:08Z,"Secure Deep Learning Engineering: A Software Quality Assurance
  Perspective","Over the past decades, deep learning (DL) systems have achieved tremendous
success and gained great popularity in various applications, such as
intelligent machines, image processing, speech processing, and medical
diagnostics. Deep neural networks are the key driving force behind its recent
success, but still seem to be a magic black box lacking interpretability and
understanding. This brings up many open safety and security issues with
enormous and urgent demands on rigorous methodologies and engineering practice
for quality enhancement. A plethora of studies have shown that the
state-of-the-art DL systems suffer from defects and vulnerabilities that can
lead to severe loss and tragedies, especially when applied to real-world
safety-critical applications. In this paper, we perform a large-scale study and
construct a paper repository of 223 relevant works to the quality assurance,
security, and interpretation of deep learning. We, from a software quality
assurance perspective, pinpoint challenges and future opportunities towards
universal secure deep learning engineering. We hope this work and the
accompanied paper repository can pave the path for the software engineering
community towards addressing the pressing industrial demand of secure
intelligent applications.",arxiv
http://arxiv.org/abs/2111.03890v1,2021-11-06T13:54:07Z,2021-11-06T13:54:07Z,"Demystifying Deep Learning Models for Retinal OCT Disease Classification
  using Explainable AI","In the world of medical diagnostics, the adoption of various deep learning
techniques is quite common as well as effective, and its statement is equally
true when it comes to implementing it into the retina Optical Coherence
Tomography (OCT) sector, but (i)These techniques have the black box
characteristics that prevent the medical professionals to completely trust the
results generated from them (ii)Lack of precision of these methods restricts
their implementation in clinical and complex cases (iii)The existing works and
models on the OCT classification are substantially large and complicated and
they require a considerable amount of memory and computational power, reducing
the quality of classifiers in real-time applications. To meet these problems,
in this paper a self-developed CNN model has been proposed which is
comparatively smaller and simpler along with the use of Lime that introduces
Explainable AI to the study and helps to increase the interpretability of the
model. This addition will be an asset to the medical experts for getting major
and detailed information and will help them in making final decisions and will
also reduce the opacity and vulnerability of the conventional deep learning
models.",arxiv
http://arxiv.org/abs/2007.08146v1,2020-07-16T07:10:21Z,2020-07-16T07:10:21Z,"Enhanced detection of fetal pose in 3D MRI by Deep Reinforcement
  Learning with physical structure priors on anatomy","Fetal MRI is heavily constrained by unpredictable and substantial fetal
motion that causes image artifacts and limits the set of viable diagnostic
image contrasts. Current mitigation of motion artifacts is predominantly
performed by fast, single-shot MRI and retrospective motion correction.
Estimation of fetal pose in real time during MRI stands to benefit prospective
methods to detect and mitigate fetal motion artifacts where inferred fetal
motion is combined with online slice prescription with low-latency decision
making. Current developments of deep reinforcement learning (DRL), offer a
novel approach for fetal landmarks detection. In this task 15 agents are
deployed to detect 15 landmarks simultaneously by DRL. The optimization is
challenging, and here we propose an improved DRL that incorporates priors on
physical structure of the fetal body. First, we use graph communication layers
to improve the communication among agents based on a graph where each node
represents a fetal-body landmark. Further, additional reward based on the
distance between agents and physical structures such as the fetal limbs is used
to fully exploit physical structure. Evaluation of this method on a repository
of 3-mm resolution in vivo data demonstrates a mean accuracy of landmark
estimation within 10 mm of ground truth as 87.3%, and a mean error of 6.9 mm.
The proposed DRL for fetal pose landmark search demonstrates a potential
clinical utility for online detection of fetal motion that guides real-time
mitigation of motion artifacts as well as health diagnosis during MRI of the
pregnant mother.",arxiv
http://arxiv.org/abs/1811.01627v1,2018-11-05T11:49:26Z,2018-11-05T11:49:26Z,"Real-time Driver Drowsiness Detection for Android Application Using Deep
  Neural Networks Techniques","Road crashes and related forms of accidents are a common cause of injury and
death among the human population. According to 2015 data from the World Health
Organization, road traffic injuries resulted in approximately 1.25 million
deaths worldwide, i.e. approximately every 25 seconds an individual will
experience a fatal crash. While the cost of traffic accidents in Europe is
estimated at around 160 billion Euros, driver drowsiness accounts for
approximately 100,000 accidents per year in the United States alone as reported
by The American National Highway Traffic Safety Administration (NHTSA). In this
paper, a novel approach towards real-time drowsiness detection is proposed.
This approach is based on a deep learning method that can be implemented on
Android applications with high accuracy. The main contribution of this work is
the compression of heavy baseline model to a lightweight model. Moreover,
minimal network structure is designed based on facial landmark key point
detection to recognize whether the driver is drowsy. The proposed model is able
to achieve an accuracy of more than 80%. Keywords: Driver Monitoring System;
Drowsiness Detection; Deep Learning; Real-time Deep Neural Network; Android.",arxiv
http://arxiv.org/abs/1509.08644v1,2015-09-29T08:54:48Z,2015-09-29T08:54:48Z,"Neural-based machine translation for medical text domain. Based on
  European Medicines Agency leaflet texts","The quality of machine translation is rapidly evolving. Today one can find
several machine translation systems on the web that provide reasonable
translations, although the systems are not perfect. In some specific domains,
the quality may decrease. A recently proposed approach to this domain is neural
machine translation. It aims at building a jointly-tuned single neural network
that maximizes translation performance, a very different approach from
traditional statistical machine translation. Recently proposed neural machine
translation models often belong to the encoder-decoder family in which a source
sentence is encoded into a fixed length vector that is, in turn, decoded to
generate a translation. The present research examines the effects of different
training methods on a Polish-English Machine Translation system used for
medical data. The European Medicines Agency parallel text corpus was used as
the basis for training of neural and statistical network-based translation
systems. The main machine translation evaluation metrics have also been used in
analysis of the systems. A comparison and implementation of a real-time medical
translator is the main focus of our experiments.",arxiv
http://arxiv.org/abs/1905.00288v3,2020-07-23T18:16:09Z,2019-05-01T12:50:26Z,Beyond Mobile Apps: A Survey of Technologies for Mental Well-being,"Mental health problems are on the rise globally and strain national health
systems worldwide. Mental disorders are closely associated with fear of stigma,
structural barriers such as financial burden, and lack of available services
and resources which often prohibit the delivery of frequent clinical advice and
monitoring. Technologies for mental well-being exhibit a range of attractive
properties, which facilitate the delivery of state-of-the-art clinical
monitoring. This review article provides an overview of traditional techniques
followed by their technological alternatives, sensing devices, behaviour
changing tools, and feedback interfaces. The challenges presented by these
technologies are then discussed with data collection, privacy, and battery life
being some of the key issues which need to be carefully considered for the
successful deployment of mental health toolkits. Finally, the opportunities
this growing research area presents are discussed including the use of portable
tangible interfaces combining sensing and feedback technologies. Capitalising
on the data these ubiquitous devices can record, state of the art machine
learning algorithms can lead to the development of robust clinical decision
support tools towards diagnosis and improvement of mental well-being delivery
in real-time.",arxiv
http://arxiv.org/abs/2101.06175v1,2021-01-15T15:36:22Z,2021-01-15T15:36:22Z,PaddleSeg: A High-Efficient Development Toolkit for Image Segmentation,"Image Segmentation plays an essential role in computer vision and image
processing with various applications from medical diagnosis to autonomous car
driving. A lot of segmentation algorithms have been proposed for addressing
specific problems. In recent years, the success of deep learning techniques has
tremendously influenced a wide range of computer vision areas, and the modern
approaches of image segmentation based on deep learning are becoming prevalent.
In this article, we introduce a high-efficient development toolkit for image
segmentation, named PaddleSeg. The toolkit aims to help both developers and
researchers in the whole process of designing segmentation models, training
models, optimizing performance and inference speed, and deploying models.
Currently, PaddleSeg supports around 20 popular segmentation models and more
than 50 pre-trained models from real-time and high-accuracy levels. With
modular components and backbone networks, users can easily build over one
hundred models for different requirements. Furthermore, we provide
comprehensive benchmarks and evaluations to show that these segmentation
algorithms trained on our toolkit have more competitive accuracy. Also, we
provide various real industrial applications and practical cases based on
PaddleSeg. All codes and examples of PaddleSeg are available at
https://github.com/PaddlePaddle/PaddleSeg.",arxiv
http://arxiv.org/abs/2101.04086v1,2021-01-11T18:29:50Z,2021-01-11T18:29:50Z,"System Design for a Data-driven and Explainable Customer Sentiment
  Monitor","The most important goal of customer services is to keep the customer
satisfied. However, service resources are always limited and must be
prioritized. Therefore, it is important to identify customers who potentially
become unsatisfied and might lead to escalations. Today this prioritization of
customers is often done manually. Data science on IoT data (esp. log data) for
machine health monitoring, as well as analytics on enterprise data for customer
relationship management (CRM) have mainly been researched and applied
independently. In this paper, we present a framework for a data-driven decision
support system which combines IoT and enterprise data to model customer
sentiment. Such decision support systems can help to prioritize customers and
service resources to effectively troubleshoot problems or even avoid them. The
framework is applied in a real-world case study with a major medical device
manufacturer. This includes a fully automated and interpretable machine
learning pipeline designed to meet the requirements defined with domain experts
and end users. The overall framework is currently deployed, learns and
evaluates predictive models from terabytes of IoT and enterprise data to
actively monitor the customer sentiment for a fleet of thousands of high-end
medical devices. Furthermore, we provide an anonymized industrial benchmark
dataset for the research community.",arxiv
http://arxiv.org/abs/2108.04815v1,2021-08-10T17:58:01Z,2021-08-10T17:58:01Z,"The Effect of the Loss on Generalization: Empirical Study on Synthetic
  Lung Nodule Data","Convolutional Neural Networks (CNNs) are widely used for image classification
in a variety of fields, including medical imaging. While most studies deploy
cross-entropy as the loss function in such tasks, a growing number of
approaches have turned to a family of contrastive learning-based losses. Even
though performance metrics such as accuracy, sensitivity and specificity are
regularly used for the evaluation of CNN classifiers, the features that these
classifiers actually learn are rarely identified and their effect on the
classification performance on out-of-distribution test samples is
insufficiently explored. In this paper, motivated by the real-world task of
lung nodule classification, we investigate the features that a CNN learns when
trained and tested on different distributions of a synthetic dataset with
controlled modes of variation. We show that different loss functions lead to
different features being learned and consequently affect the generalization
ability of the classifier on unseen data. This study provides some important
insights into the design of deep learning solutions for medical imaging tasks.",arxiv
http://arxiv.org/abs/1809.07806v2,2019-06-14T01:39:12Z,2018-09-20T19:03:14Z,Understanding Behavior of Clinical Models under Domain Shifts,"The hypothesis that computational models can be reliable enough to be adopted
in prognosis and patient care is revolutionizing healthcare. Deep learning, in
particular, has been a game changer in building predictive models, thus leading
to community-wide data curation efforts. However, due to inherent variabilities
in population characteristics and biological systems, these models are often
biased to the training datasets. This can be limiting when models are deployed
in new environments, when there are systematic domain shifts not known a
priori. In this paper, we propose to emulate a large class of domain shifts,
that can occur in clinical settings, with a given dataset, and argue that
evaluating the behavior of predictive models in light of those shifts is an
effective way to quantify their reliability. More specifically, we develop an
approach for building realistic scenarios, based on analysis of \textit{disease
landscapes} in multi-label classification. Using the openly available MIMIC-III
EHR dataset for phenotyping, for the first time, our work sheds light into data
regimes where deep clinical models can fail to generalize. This work emphasizes
the need for novel validation mechanisms driven by real-world domain shifts in
AI for healthcare.",arxiv
http://arxiv.org/abs/2012.06448v2,2020-12-19T10:02:04Z,2020-12-11T16:16:32Z,"An Unsupervised Reconstruction Method For Low-Dose CT Using Deep
  Generative Regularization Prior","Low-dose CT imaging requires reconstruction from noisy indirect measurements
which can be defined as an ill-posed linear inverse problem. In addition to
conventional FBP method in CT imaging, recent compressed sensing based methods
exploit handcrafted priors which are mostly simplistic and hard to determine.
More recently, deep learning (DL) based methods have become popular in medical
imaging field. In CT imaging, DL based methods try to learn a function that
maps low-dose images to normal-dose images. Although the results of these
methods are promising, their success mostly depends on the availability of
high-quality massive datasets. In this study, we proposed a method that does
not require any training data or a learning process. Our method exploits such
an approach that deep convolutional neural networks (CNNs) generate patterns
easier than the noise, therefore randomly initialized generative neural
networks can be suitable priors to be used in regularizing the reconstruction.
In the experiments, the proposed method is implemented with different loss
function variants. Both analytical CT phantoms and real-world CT images are
used with different views. Conventional FBP method, a popular iterative method
(SART), and TV regularized SART are used in the comparisons. We demonstrated
that our method with different loss function variants outperforms the other
methods both qualitatively and quantitatively.",arxiv
http://arxiv.org/abs/1907.10554v2,2020-03-26T16:11:35Z,2019-07-24T16:47:47Z,"Development of a Real-time Indoor Location System using Bluetooth Low
  Energy Technology and Deep Learning to Facilitate Clinical Applications","An indoor, real-time location system (RTLS) can benefit both hospitals and
patients by improving clinical efficiency through data-driven optimization of
procedures. Bluetooth-based RTLS systems are cost-effective but lack accuracy
and robustness because Bluetooth signal strength is subject to fluctuation. We
developed a machine learning-based solution using a Long Short-Term Memory
(LSTM) network followed by a Multilayer Perceptron classifier and a posterior
constraint algorithm to improve RTLS performance. Training and validation
datasets showed that most machine learning models perform well in classifying
individual location zones, although LSTM was most reliable. However, when faced
with data indicating cross-zone trajectories, all models showed erratic zone
switching. Thus, we implemented a history-based posterior constraint algorithm
to reduce the variability in exchange for a slight decrease in responsiveness.
This network increases robustness at the expense of latency. When latency is
less of a concern, we computed the latency-corrected accuracy which is 100% for
our testing data, significantly improved from LSTM without constraint which is
96.2%. The balance between robustness and responsiveness can be considered and
adjusted on a case-by-case basis, according to the specific needs of downstream
clinical applications. This system was deployed and validated in an academic
medical center. Industry best practices enabled system scaling without
substantial compromises to performance or cost.",arxiv
http://arxiv.org/abs/1710.08299v1,2017-09-26T05:20:12Z,2017-09-26T05:20:12Z,An In-field Automatic Wheat Disease Diagnosis System,"Crop diseases are responsible for the major production reduction and economic
losses in agricultural industry world- wide. Monitoring for health status of
crops is critical to control the spread of diseases and implement effective
management. This paper presents an in-field automatic wheat disease diagnosis
system based on a weakly super- vised deep learning framework, i.e. deep
multiple instance learning, which achieves an integration of identification for
wheat diseases and localization for disease areas with only image-level
annotation for training images in wild conditions. Furthermore, a new in-field
image dataset for wheat disease, Wheat Disease Database 2017 (WDD2017), is
collected to verify the effectiveness of our system. Under two different
architectures, i.e. VGG-FCN-VD16 and VGG-FCN-S, our system achieves the mean
recognition accuracies of 97.95% and 95.12% respectively over 5-fold
cross-validation on WDD2017, exceeding the results of 93.27% and 73.00% by two
conventional CNN frameworks, i.e. VGG-CNN-VD16 and VGG-CNN-S. Experimental
results demonstrate that the proposed system outperforms conventional CNN
architectures on recognition accuracy under the same amount of parameters,
meanwhile main- taining accurate localization for corresponding disease areas.
Moreover, the proposed system has been packed into a real-time mobile app to
provide support for agricultural disease diagnosis.",arxiv
http://arxiv.org/abs/1911.06633v1,2019-11-15T13:50:27Z,2019-11-15T13:50:27Z,"HealthFog: An Ensemble Deep Learning based Smart Healthcare System for
  Automatic Diagnosis of Heart Diseases in Integrated IoT and Fog Computing
  Environments","Cloud computing provides resources over the Internet and allows a plethora of
applications to be deployed to provide services for different industries. The
major bottleneck being faced currently in these cloud frameworks is their
limited scalability and hence inability to cater to the requirements of
centralized Internet of Things (IoT) based compute environments. The main
reason for this is that latency-sensitive applications like health monitoring
and surveillance systems now require computation over large amounts of data
(Big Data) transferred to centralized database and from database to cloud data
centers which leads to drop in performance of such systems. The new paradigms
of fog and edge computing provide innovative solutions by bringing resources
closer to the user and provide low latency and energy-efficient solutions for
data processing compared to cloud domains. Still, the current fog models have
many limitations and focus from a limited perspective on either accuracy of
results or reduced response time but not both. We proposed a novel framework
called HealthFog for integrating ensemble deep learning in Edge computing
devices and deployed it for a real-life application of automatic Heart Disease
analysis. HealthFog delivers healthcare as a fog service using IoT devices and
efficiently manages the data of heart patients, which comes as user requests.
Fog-enabled cloud framework, FogBus is used to deploy and test the performance
of the proposed model in terms of power consumption, network bandwidth,
latency, jitter, accuracy and execution time. HealthFog is configurable to
various operation modes that provide the best Quality of Service or prediction
accuracy, as required, in diverse fog computation scenarios and for different
user requirements.",arxiv
http://arxiv.org/abs/2008.05381v1,2020-08-12T15:29:11Z,2020-08-12T15:29:11Z,"Improving the Performance of Fine-Grain Image Classifiers via Generative
  Data Augmentation","Recent advances in machine learning (ML) and computer vision tools have
enabled applications in a wide variety of arenas such as financial analytics,
medical diagnostics, and even within the Department of Defense. However, their
widespread implementation in real-world use cases poses several challenges: (1)
many applications are highly specialized, and hence operate in a \emph{sparse
data} domain; (2) ML tools are sensitive to their training sets and typically
require cumbersome, labor-intensive data collection and data labelling
processes; and (3) ML tools can be extremely ""black box,"" offering users little
to no insight into the decision-making process or how new data might affect
prediction performance. To address these challenges, we have designed and
developed Data Augmentation from Proficient Pre-Training of Robust Generative
Adversarial Networks (DAPPER GAN), an ML analytics support tool that
automatically generates novel views of training images in order to improve
downstream classifier performance. DAPPER GAN leverages high-fidelity
embeddings generated by a StyleGAN2 model (trained on the LSUN cars dataset) to
create novel imagery for previously unseen classes. We experimentally evaluate
this technique on the Stanford Cars dataset, demonstrating improved vehicle
make and model classification accuracy and reduced requirements for real data
using our GAN based data augmentation framework. The method's validity was
supported through an analysis of classifier performance on both augmented and
non-augmented datasets, achieving comparable or better accuracy with up to 30\%
less real data across visually similar classes. To support this method, we
developed a novel augmentation method that can manipulate semantically
meaningful dimensions (e.g., orientation) of the target object in the embedding
space.",arxiv
http://arxiv.org/abs/2108.10205v1,2021-08-02T13:09:53Z,2021-08-02T13:09:53Z,"Power transformer faults diagnosis using undestructive methods (Roger
  and IEC) and artificial neural network for dissolved gas analysis applied on
  the functional transformer in the Algerian north-eastern: a comparative study","Nowadays, power transformer aging and failures are viewed with great
attention in power transmission industry. Dissolved gas analysis (DGA) is
classified among the biggest widely used methods used within the context of
asset management policy to detect the incipient faults in their earlier stage
in power transformers. Up to now, several procedures have been employed for the
lecture of DGA results. Among these useful means, we find Key Gases, Rogers
Ratios, IEC Ratios, the historical technique less used today Doernenburg
Ratios, the two types of Duval Pentagons methods, several versions of the Duval
Triangles method and Logarithmic Nomograph. Problem. DGA data extracted from
different units in service served to verify the ability and reliability of
these methods in assessing the state of health of the power transformer. Aim.
An improving the quality of diagnostics of electrical power transformer by
artificial neural network tools based on two conventional methods in the case
of a functional power transformer at S\'etif province in East North of Algeria.
Methodology. Design an inelegant tool for power transformer diagnosis using
neural networks based on traditional methods IEC and Rogers, which allows to
early detection faults, to increase the reliability, of the entire electrical
energy system from transport to consumers and improve a continuity and quality
of service. Results. The solution of the problem was carried out by using
feed-forward back-propagation neural networks implemented in MATLAB-Simulink
environment. Four real power transformers working under different environment
and climate conditions such as: desert, humid, cold were taken into account.
The practical results of the diagnosis of these power transformers by the DGA
are presented. Practical value.....",arxiv
http://arxiv.org/abs/1911.05521v1,2019-11-13T14:56:36Z,2019-11-13T14:56:36Z,"Real-time ultra-low power ECG anomaly detection using an event-driven
  neuromorphic processor","Accurate detection of pathological conditions in human subjects can be
achieved through off-line analysis of recorded biological signals such as
electrocardiograms (ECGs). However, human diagnosis is time-consuming and
expensive, as it requires the time of medical professionals. This is especially
inefficient when indicative patterns in the biological signals are infrequent.
Moreover, patients with suspected pathologies are often monitored for extended
periods, requiring the storage and examination of large amounts of
non-pathological data, and entailing a difficult visual search task for
diagnosing professionals.
  In this work we propose a compact and sub-mW low power neural processing
system that can be used to perform on-line and real-time preliminary diagnosis
of pathological conditions, to raise warnings for the existence of possible
pathological conditions, or to trigger an off-line data recording system for
further analysis by a medical professional. We apply the system to real-time
classification of ECG data for distinguishing between healthy heartbeats and
pathological rhythms.
  Multi-channel analog ECG traces are encoded as asynchronous streams of binary
events and processed using a spiking recurrent neural network operated in a
reservoir computing paradigm. An event-driven neuron output layer is then
trained to recognize one of several pathologies. Finally, the filtered activity
of this output layer is used to generate a binary trigger signal indicating the
presence or absence of a pathological pattern.
  We validate the approach proposed using a Dynamic Neuromorphic Asynchronous
Processor (DYNAP) chip, implemented using a standard 180 nm CMOS VLSI process,
and present experimental results measured from the chip.",arxiv
http://arxiv.org/abs/2108.04358v1,2021-07-31T01:54:20Z,2021-07-31T01:54:20Z,"Convolutional Nets for Diabetic Retinopathy Screening in Bangladeshi
  Patients","Diabetes is one of the most prevalent chronic diseases in Bangladesh, and as
a result, Diabetic Retinopathy (DR) is widespread in the population. DR, an eye
illness caused by diabetes, can lead to blindness if it is not identified and
treated in its early stages. Unfortunately, diagnosis of DR requires medically
trained professionals, but Bangladesh has limited specialists in comparison to
its population. Moreover, the screening process is often expensive, prohibiting
many from receiving timely and proper diagnosis. To address the problem, we
introduce a deep learning algorithm which screens for different stages of DR.
We use a state-of-the-art CNN architecture to diagnose patients based on
retinal fundus imagery. This paper is an experimental evaluation of the
algorithm we developed for DR diagnosis and screening specifically for
Bangladeshi patients. We perform this validation study using separate pools of
retinal image data of real patients from a hospital and field studies in
Bangladesh. Our results show that the algorithm is effective at screening
Bangladeshi eyes even when trained on a public dataset which is out of domain,
and can accurately determine the stage of DR as well, achieving an overall
accuracy of 92.27\% and 93.02\% on two validation sets of Bangladeshi eyes. The
results confirm the ability of the algorithm to be used in real clinical
settings and applications due to its high accuracy and classwise metrics. Our
algorithm is implemented in the application Drishti, which is used to screen
for DR in patients living in rural areas in Bangladesh, where access to
professional screening is limited.",arxiv
http://arxiv.org/abs/2110.08820v2,2021-10-19T16:49:58Z,2021-10-17T13:42:37Z,On-board Fault Diagnosis of a Laboratory Mini SR-30 Gas Turbine Engine,"Inspired by recent progress in machine learning, a data-driven fault
diagnosis and isolation (FDI) scheme is explicitly developed for failure in the
fuel supply system and sensor measurements of the laboratory gas turbine
system. A passive approach of fault diagnosis is implemented where a model is
trained using machine learning classifiers to detect a given set of fault
scenarios in real-time on which it is trained. Towards the end, a comparative
study is presented for well-known classification techniques, namely Support
vector classifier, linear discriminant analysis, K-neighbor, and decision
trees. Several simulation studies were carried out to demonstrate and
illustrate the proposed fault diagnosis scheme's advantages, capabilities, and
performance.",arxiv
http://arxiv.org/abs/2011.02000v5,2021-05-05T21:17:45Z,2020-11-03T20:41:59Z,"Detection of Maternal and Fetal Stress from the Electrocardiogram with
  Self-Supervised Representation Learning","In the pregnant mother and her fetus, chronic prenatal stress results in
entrainment of the fetal heartbeat by the maternal heartbeat, quantified by the
fetal stress index (FSI). Deep learning (DL) is capable of pattern detection in
complex medical data with high accuracy in noisy real-life environments, but
little is known about DL's utility in non-invasive biometric monitoring during
pregnancy. A recently established self-supervised learning (SSL) approach to DL
provides emotional recognition from electrocardiogram (ECG). We hypothesized
that SSL will identify chronically stressed mother-fetus dyads from the raw
maternal abdominal electrocardiograms (aECG), containing fetal and maternal
ECG. Chronically stressed mothers and controls matched at enrolment at 32 weeks
of gestation were studied. We validated the chronic stress exposure by
psychological inventory, maternal hair cortisol and FSI. We tested two variants
of SSL architecture, one trained on the generic ECG features for emotional
recognition obtained from public datasets and another transfer-learned on a
subset of our data. Our DL models accurately detect the chronic stress exposure
group (AUROC=0.982+/-0.002), the individual psychological stress score
(R2=0.943+/-0.009) and FSI at 34 weeks of gestation (R2=0.946+/-0.013), as well
as the maternal hair cortisol at birth reflecting chronic stress exposure
(0.931+/-0.006). The best performance was achieved with the DL model trained on
the public dataset and using maternal ECG alone. The present DL approach
provides a novel source of physiological insights into complex multi-modal
relationships between different regulatory systems exposed to chronic stress.
The final DL model can be deployed in low-cost regular ECG biosensors as a
simple, ubiquitous early stress detection and monitoring tool during pregnancy.
This discovery should enable early behavioral interventions.",arxiv
http://arxiv.org/abs/2006.03647v2,2020-06-23T16:54:09Z,2020-06-05T19:33:19Z,"Deployment-Efficient Reinforcement Learning via Model-Based Offline
  Optimization","Most reinforcement learning (RL) algorithms assume online access to the
environment, in which one may readily interleave updates to the policy with
experience collection using that policy. However, in many real-world
applications such as health, education, dialogue agents, and robotics, the cost
or potential risk of deploying a new data-collection policy is high, to the
point that it can become prohibitive to update the data-collection policy more
than a few times during learning. With this view, we propose a novel concept of
deployment efficiency, measuring the number of distinct data-collection
policies that are used during policy learning. We observe that na\""{i}vely
applying existing model-free offline RL algorithms recursively does not lead to
a practical deployment-efficient and sample-efficient algorithm. We propose a
novel model-based algorithm, Behavior-Regularized Model-ENsemble (BREMEN) that
can effectively optimize a policy offline using 10-20 times fewer data than
prior works. Furthermore, the recursive application of BREMEN is able to
achieve impressive deployment efficiency while maintaining the same or better
sample efficiency, learning successful policies from scratch on simulated
robotic environments with only 5-10 deployments, compared to typical values of
hundreds to millions in standard RL baselines. Codes and pre-trained models are
available at https://github.com/matsuolab/BREMEN .",arxiv
http://arxiv.org/abs/1611.04049v1,2016-11-12T22:08:15Z,2016-11-12T22:08:15Z,Prognostics of Surgical Site Infections using Dynamic Health Data,"Surgical Site Infection (SSI) is a national priority in healthcare research.
Much research attention has been attracted to develop better SSI risk
prediction models. However, most of the existing SSI risk prediction models are
built on static risk factors such as comorbidities and operative factors. In
this paper, we investigate the use of the dynamic wound data for SSI risk
prediction. There have been emerging mobile health (mHealth) tools that can
closely monitor the patients and generate continuous measurements of many
wound-related variables and other evolving clinical variables. Since existing
prediction models of SSI have quite limited capacity to utilize the evolving
clinical data, we develop the corresponding solution to equip these mHealth
tools with decision-making capabilities for SSI prediction with a seamless
assembly of several machine learning models to tackle the analytic challenges
arising from the spatial-temporal data. The basic idea is to exploit the
low-rank property of the spatial-temporal data via the bilinear formulation,
and further enhance it with automatic missing data imputation by the matrix
completion technique. We derive efficient optimization algorithms to implement
these models and demonstrate the superior performances of our new predictive
model on a real-world dataset of SSI, compared to a range of state-of-the-art
methods.",arxiv
http://arxiv.org/abs/1701.05130v1,2017-01-18T16:17:35Z,2017-01-18T16:17:35Z,"On the Performance of Network Parallel Training in Artificial Neural
  Networks","Artificial Neural Networks (ANNs) have received increasing attention in
recent years with applications that span a wide range of disciplines including
vital domains such as medicine, network security and autonomous transportation.
However, neural network architectures are becoming increasingly complex and
with an increasing need to obtain real-time results from such models, it has
become pivotal to use parallelization as a mechanism for speeding up network
training and deployment. In this work we propose an implementation of Network
Parallel Training through Cannon's Algorithm for matrix multiplication. We show
that increasing the number of processes speeds up training until the point
where process communication costs become prohibitive; this point varies by
network complexity. We also show through empirical efficiency calculations that
the speedup obtained is superlinear.",arxiv
http://arxiv.org/abs/1205.6910v1,2012-05-31T08:22:19Z,2012-05-31T08:22:19Z,"A New Architecture of a Ubiquitous Health Monitoring System: A Prototype
  Of Cloud Mobile Health Monitoring System","Wireless Body Area Sensor Networks (WBASN) is an emerging technology which
uses wireless sensors to implement real-time wearable health monitoring of
patients to enhance independent living. In this paper we propose a prototype of
cloud mobile health monitoring system. The system uses WBASN and Smartphone
application that uses cloud computing, location data and a neural network to
determine the state of patients.",arxiv
http://arxiv.org/abs/2109.00516v1,2021-08-31T17:51:15Z,2021-08-31T17:51:15Z,Multistage Pruning of CNN Based ECG Classifiers for Edge Devices,"Using smart wearable devices to monitor patients electrocardiogram (ECG) for
real-time detection of arrhythmias can significantly improve healthcare
outcomes. Convolutional neural network (CNN) based deep learning has been used
successfully to detect anomalous beats in ECG. However, the computational
complexity of existing CNN models prohibits them from being implemented in
low-powered edge devices. Usually, such models are complex with lots of model
parameters which results in large number of computations, memory, and power
usage in edge devices. Network pruning techniques can reduce model complexity
at the expense of performance in CNN models. This paper presents a novel
multistage pruning technique that reduces CNN model complexity with negligible
loss in performance compared to existing pruning techniques. An existing CNN
model for ECG classification is used as a baseline reference. At 60% sparsity,
the proposed technique achieves 97.7% accuracy and an F1 score of 93.59% for
ECG classification tasks. This is an improvement of 3.3% and 9% for accuracy
and F1 Score respectively, compared to traditional pruning with fine-tuning
approach. Compared to the baseline model, we also achieve a 60.4% decrease in
run-time complexity.",arxiv
http://arxiv.org/abs/2005.01180v2,2020-05-05T11:49:27Z,2020-05-03T20:27:10Z,MAGES 3.0: Tying the knot of medical VR,"In this work, we present MAGES 3.0, a novel Virtual Reality (VR)-based
authoring SDK platform for accelerated surgical training and assessment. The
MAGES Software Development Kit (SDK) allows code-free prototyping of any VR
psychomotor simulation of medical operations by medical professionals, who
urgently need a tool to solve the issue of outdated medical training. Our
platform encapsulates the following novel algorithmic techniques: a)
collaborative networking layer with Geometric Algebra (GA) interpolation engine
b) supervised machine learning analytics module for real-time recommendations
and user profiling c) GA deformable cutting and tearing algorithm d) on-the-go
configurable soft body simulation for deformable surfaces.",arxiv
http://arxiv.org/abs/1908.06943v2,2020-04-24T15:13:00Z,2019-08-15T15:46:40Z,"Resolving challenges in deep learning-based analyses of
  histopathological images using explanation methods","Deep learning has recently gained popularity in digital pathology due to its
high prediction quality. However, the medical domain requires explanation and
insight for a better understanding beyond standard quantitative performance
evaluation. Recently, explanation methods have emerged, which are so far still
rarely used in medicine. This work shows their application to generate heatmaps
that allow to resolve common challenges encountered in deep learning-based
digital histopathology analyses. These challenges comprise biases typically
inherent to histopathology data. We study binary classification tasks of tumor
tissue discrimination in publicly available haematoxylin and eosin slides of
various tumor entities and investigate three types of biases: (1) biases which
affect the entire dataset, (2) biases which are by chance correlated with class
labels and (3) sampling biases. While standard analyses focus on patch-level
evaluation, we advocate pixel-wise heatmaps, which offer a more precise and
versatile diagnostic instrument and furthermore help to reveal biases in the
data. This insight is shown to not only detect but also to be helpful to remove
the effects of common hidden biases, which improves generalization within and
across datasets. For example, we could see a trend of improved area under the
receiver operating characteristic curve by 5% when reducing a labeling bias.
Explanation techniques are thus demonstrated to be a helpful and highly
relevant tool for the development and the deployment phases within the life
cycle of real-world applications in digital pathology.",arxiv
http://arxiv.org/abs/2108.03823v3,2021-09-24T04:53:18Z,2021-08-09T05:58:49Z,Towards to Robust and Generalized Medical Image Segmentation Framework,"To mitigate the radiologist's workload, computer-aided diagnosis with the
capability to review and analyze medical images is gradually deployed. Deep
learning-based region of interest segmentation is among the most exciting use
cases. However, this paradigm is restricted in real-world clinical applications
due to poor robustness and generalization. The issue is more sinister with a
lack of training data. In this paper, we address the challenge from the
representation learning point of view. We investigate that the collapsed
representations, as one of the main reasons which caused poor robustness and
generalization, could be avoided through transfer learning. Therefore, we
propose a novel two-stage framework for robust generalized segmentation. In
particular, an unsupervised Tile-wise AutoEncoder (T-AE) pretraining
architecture is coined to learn meaningful representation for improving the
generalization and robustness of the downstream tasks. Furthermore, the learned
knowledge is transferred to the segmentation benchmark. Coupled with an image
reconstruction network, the representation keeps to be decoded, encouraging the
model to capture more semantic features. Experiments of lung segmentation on
multi chest X-ray datasets are conducted. Empirically, the related experimental
results demonstrate the superior generalization capability of the proposed
framework on unseen domains in terms of high performance and robustness to
corruption, especially under the scenario of the limited training data.",arxiv
http://arxiv.org/abs/2110.04563v1,2021-10-09T13:11:21Z,2021-10-09T13:11:21Z,"Automatic Recognition of Abdominal Organs in Ultrasound Images based on
  Deep Neural Networks and K-Nearest-Neighbor Classification","Abdominal ultrasound imaging has been widely used to assist in the diagnosis
and treatment of various abdominal organs. In order to shorten the examination
time and reduce the cognitive burden on the sonographers, we present a
classification method that combines the deep learning techniques and
k-Nearest-Neighbor (k-NN) classification to automatically recognize various
abdominal organs in the ultrasound images in real time. Fine-tuned deep neural
networks are used in combination with PCA dimension reduction to extract
high-level features from raw ultrasound images, and a k-NN classifier is
employed to predict the abdominal organ in the image. We demonstrate the
effectiveness of our method in the task of ultrasound image classification to
automatically recognize six abdominal organs. A comprehensive comparison of
different configurations is conducted to study the influence of different
feature extractors and classifiers on the classification accuracy. Both
quantitative and qualitative results show that with minimal training effort,
our method can ""lazily"" recognize the abdominal organs in the ultrasound images
in real time with an accuracy of 96.67%. Our implementation code is publicly
available at: https://github.com/LeeKeyu/abdominal_ultrasound_classification.",arxiv
http://arxiv.org/abs/2012.01153v1,2020-12-02T12:53:19Z,2020-12-02T12:53:19Z,Towards Intelligent Reconfigurable Wireless Physical Layer (PHY),"Next-generation wireless networks are getting significant attention because
they promise 10-factor enhancement in mobile broadband along with the potential
to enable new heterogeneous services. Services include massive machine type
communications desired for Industrial 4.0 along with ultra-reliable low latency
services for remote healthcare and vehicular communications. In this paper, we
present the design of an intelligent and reconfigurable physical layer (PHY) to
bring these services to reality. First, we design and implement the
reconfigurable PHY via a hardware-software co-design approach on system-on-chip
consisting of the ARM processor and field-programmable gate array (FPGA). The
reconfigurable PHY is then made intelligent by augmenting it with online
machine learning (OML) based decision-making algorithm. Such PHY can learn the
environment (for example, wireless channel) and dynamically adapt the
transceivers' configuration (i.e., modulation scheme, word-length) and select
the wireless channel on-the-fly. Since the environment is unknown and changes
with time, we make the OML architecture reconfigurable to enable dynamic switch
between various OML algorithms on-the-fly. We have demonstrated the functional
correctness of the proposed architecture for different environments and
word-lengths. The detailed throughput, latency, and complexity analysis
validate the feasibility and importance of the proposed intelligent and
reconfigurable PHY in next-generation networks.",arxiv
http://arxiv.org/abs/1806.07777v1,2018-06-20T14:56:10Z,2018-06-20T14:56:10Z,"Generative Adversarial Networks for Image-to-Image Translation on
  Multi-Contrast MR Images - A Comparison of CycleGAN and UNIT","In medical imaging, a general problem is that it is costly and time consuming
to collect high quality data from healthy and diseased subjects. Generative
adversarial networks (GANs) is a deep learning method that has been developed
for synthesizing data. GANs can thereby be used to generate more realistic
training data, to improve classification performance of machine learning
algorithms. Another application of GANs is image-to-image translations, e.g.
generating magnetic resonance (MR) images from computed tomography (CT) images,
which can be used to obtain multimodal datasets from a single modality. Here,
we evaluate two unsupervised GAN models (CycleGAN and UNIT) for image-to-image
translation of T1- and T2-weighted MR images, by comparing generated synthetic
MR images to ground truth images. We also evaluate two supervised models; a
modification of CycleGAN and a pure generator model. A small perceptual study
was also performed to evaluate how visually realistic the synthesized images
are. It is shown that the implemented GAN models can synthesize visually
realistic MR images (incorrectly labeled as real by a human). It is also shown
that models producing more visually realistic synthetic images not necessarily
have better quantitative error measurements, when compared to ground truth
data. Code is available at https://github.com/simontomaskarlsson/GAN-MRI",arxiv
http://arxiv.org/abs/2103.16223v2,2021-06-16T09:09:50Z,2021-03-30T10:11:09Z,"LemgoRL: An open-source Benchmark Tool to Train Reinforcement Learning
  Agents for Traffic Signal Control in a real-world simulation scenario","Sub-optimal control policies in intersection traffic signal controllers (TSC)
contribute to congestion and lead to negative effects on human health and the
environment. Reinforcement learning (RL) for traffic signal control is a
promising approach to design better control policies and has attracted
considerable research interest in recent years. However, most work done in this
area used simplified simulation environments of traffic scenarios to train
RL-based TSC. To deploy RL in real-world traffic systems, the gap between
simplified simulation environments and real-world applications has to be
closed. Therefore, we propose LemgoRL, a benchmark tool to train RL agents as
TSC in a realistic simulation environment of Lemgo, a medium-sized town in
Germany. In addition to the realistic simulation model, LemgoRL encompasses a
traffic signal logic unit that ensures compliance with all regulatory and
safety requirements. LemgoRL offers the same interface as the well-known OpenAI
gym toolkit to enable easy deployment in existing research work. Our benchmark
tool drives the development of RL algorithms towards real-world applications.
We provide LemgoRL as an open-source tool at https://github.com/rl-ina/lemgorl.",arxiv
http://arxiv.org/abs/2011.10839v1,2020-11-21T18:26:44Z,2020-11-21T18:26:44Z,"Deep Learning-Based Computer Vision for Real Time Intravenous Drip
  Infusion Monitoring","This paper explores the use of deep learning-based computer vision for
real-time monitoring of the flow in intravenous (IV) infusions. IV infusions
are among the most common therapies in hospitalized patients and, given that
both over-infusion and under-infusion can cause severe damages, monitoring the
flow rate of the fluid being administered to patients is very important for
their safety. The proposed system uses a camera to film the IV drip infusion
kit and a deep learning-based algorithm to classify acquired frames into two
different states: frames with a drop that has just begun to take shape and
frames with a well-formed drop. The alternation of these two states is used to
count drops and derive a measurement of the flow rate of the drip. The usage of
a camera as sensing element makes the proposed system safe in medical
environments and easier to be integrated into current health facilities.
Experimental results are reported in the paper that confirm the accuracy of the
system and its capability to produce real-time estimates. The proposed method
can be therefore effectively adopted to implement IV infusion monitoring and
control systems.",arxiv
http://arxiv.org/abs/2005.06612v1,2020-05-05T12:01:12Z,2020-05-05T12:01:12Z,"An Investigation of COVID-19 Spreading Factors with Explainable AI
  Techniques","Since COVID-19 was first identified in December 2019, various public health
interventions have been implemented across the world. As different measures are
implemented at different countries at different times, we conduct an assessment
of the relative effectiveness of the measures implemented in 18 countries and
regions using data from 22/01/2020 to 02/04/2020. We compute the top one and
two measures that are most effective for the countries and regions studied
during the period. Two Explainable AI techniques, SHAP and ECPI, are used in
our study; such that we construct (machine learning) models for predicting the
instantaneous reproduction number ($R_t$) and use the models as surrogates to
the real world and inputs that the greatest influence to our models are seen as
measures that are most effective. Across-the-board, city lockdown and contact
tracing are the two most effective measures. For ensuring $R_t<1$, public
wearing face masks is also important. Mass testing alone is not the most
effective measure although when paired with other measures, it can be
effective. Warm temperature helps for reducing the transmission.",arxiv
http://arxiv.org/abs/2101.11461v1,2021-01-18T17:10:39Z,2021-01-18T17:10:39Z,Machine learning with limited data,"Thanks to the availability of powerful computing resources, big data and deep
learning algorithms, we have made great progress on computer vision in the last
few years. Computer vision systems begin to surpass humans in some tasks, such
as object recognition, object detection, face recognition and pose estimation.
Lots of computer vision algorithms have been deployed to real world
applications and started to improve our life quality. However, big data and
labels are not always available. Sometimes we only have very limited labeled
data, such as medical images which requires experts to label them. In this
paper, we study few shot image classification, in which we only have very few
labeled data. Machine learning with little data is a big challenge. To tackle
this challenge, we propose two methods and test their effectiveness thoroughly.
One method is to augment image features by mixing the style of these images.
The second method is applying spatial attention to explore the relations
between patches of images. We also find that domain shift is a critical issue
in few shot learning when the training domain and testing domain are different.
So we propose a more realistic cross-domain few-shot learning with unlabeled
data setting, in which some unlabeled data is available in the target domain.
We propose two methods in this setting. Our first method transfers the style
information of the unlabeled target dataset to the samples in the source
dataset and trains a model with stylized images and original images. Our second
method proposes a unified framework to fully utilize all the data. Both of our
methods surpass the baseline method by a large margin.",arxiv
http://arxiv.org/abs/2109.00115v1,2021-08-31T23:38:17Z,2021-08-31T23:38:17Z,"Uncertainty Quantified Deep Learning for Predicting Dice Coefficient of
  Digital Histopathology Image Segmentation","Deep learning models (DLMs) can achieve state of the art performance in
medical image segmentation and classification tasks. However, DLMs that do not
provide feedback for their predictions such as Dice coefficients (Dice) have
limited deployment potential in real world clinical settings. Uncertainty
estimates can increase the trust of these automated systems by identifying
predictions that need further review but remain computationally prohibitive to
deploy. In this study, we use a DLM with randomly initialized weights and Monte
Carlo dropout (MCD) to segment tumors from microscopic Hematoxylin and Eosin
(H&E) dye stained prostate core biopsy RGB images. We devise a novel approach
that uses multiple clinical region based uncertainties from a single image
(instead of the entire image) to predict Dice of the DLM model output by linear
models. Image level uncertainty maps were generated and showed correspondence
between imperfect model segmentation and high levels of uncertainty associated
with specific prostate tissue regions with or without tumors. Results from this
study suggest that linear models can learn coefficients of uncertainty
quantified deep learning and correlations ((Spearman's correlation (p<0.05)) to
predict Dice scores of specific regions of medical images.",arxiv
http://arxiv.org/abs/1909.02511v2,2019-09-27T21:48:31Z,2019-09-05T16:31:40Z,"CT Data Curation for Liver Patients: Phase Recognition in Dynamic
  Contrast-Enhanced CT","As the demand for more descriptive machine learning models grows within
medical imaging, bottlenecks due to data paucity will exacerbate. Thus,
collecting enough large-scale data will require automated tools to harvest
data/label pairs from messy and real-world datasets, such as hospital PACS.
This is the focus of our work, where we present a principled data curation tool
to extract multi-phase CT liver studies and identify each scan's phase from a
real-world and heterogenous hospital PACS dataset. Emulating a typical
deployment scenario, we first obtain a set of noisy labels from our
institutional partners that are text mined using simple rules from DICOM tags.
We train a deep learning system, using a customized and streamlined 3D SE
architecture, to identify non-contrast, arterial, venous, and delay phase
dynamic CT liver scans, filtering out anything else, including other types of
liver contrast studies. To exploit as much training data as possible, we also
introduce an aggregated cross entropy loss that can learn from scans only
identified as ""contrast"". Extensive experiments on a dataset of 43K scans of
7680 patient imaging studies demonstrate that our 3DSE architecture, armed with
our aggregated loss, can achieve a mean F1 of 0.977 and can correctly harvest
up to 92.7% of studies, which significantly outperforms the text-mined and
standard-loss approach, and also outperforms other, and more complex, model
architectures.",arxiv
http://arxiv.org/abs/2008.12949v2,2021-01-14T12:55:11Z,2020-08-29T09:54:05Z,VR-Caps: A Virtual Environment for Capsule Endoscopy,"Current capsule endoscopes and next-generation robotic capsules for diagnosis
and treatment of gastrointestinal diseases are complex cyber-physical platforms
that must orchestrate complex software and hardware functions. The desired
tasks for these systems include visual localization, depth estimation, 3D
mapping, disease detection and segmentation, automated navigation, active
control, path realization and optional therapeutic modules such as targeted
drug delivery and biopsy sampling. Data-driven algorithms promise to enable
many advanced functionalities for capsule endoscopes, but real-world data is
challenging to obtain. Physically-realistic simulations providing synthetic
data have emerged as a solution to the development of data-driven algorithms.
In this work, we present a comprehensive simulation platform for capsule
endoscopy operations and introduce VR-Caps, a virtual active capsule
environment that simulates a range of normal and abnormal tissue conditions
(e.g., inflated, dry, wet etc.) and varied organ types, capsule endoscope
designs (e.g., mono, stereo, dual and 360{\deg}camera), and the type, number,
strength, and placement of internal and external magnetic sources that enable
active locomotion. VR-Caps makes it possible to both independently or jointly
develop, optimize, and test medical imaging and analysis software for the
current and next-generation endoscopic capsule systems. To validate this
approach, we train state-of-the-art deep neural networks to accomplish various
medical image analysis tasks using simulated data from VR-Caps and evaluate the
performance of these models on real medical data. Results demonstrate the
usefulness and effectiveness of the proposed virtual platform in developing
algorithms that quantify fractional coverage, camera trajectory, 3D map
reconstruction, and disease classification.",arxiv
http://arxiv.org/abs/2106.07708v1,2021-06-14T18:58:09Z,2021-06-14T18:58:09Z,"CathAI: Fully Automated Interpretation of Coronary Angiograms Using
  Neural Networks","Coronary heart disease (CHD) is the leading cause of adult death in the
United States and worldwide, and for which the coronary angiography procedure
is the primary gateway for diagnosis and clinical management decisions. The
standard-of-care for interpretation of coronary angiograms depends upon ad-hoc
visual assessment by the physician operator. However, ad-hoc visual
interpretation of angiograms is poorly reproducible, highly variable and bias
prone. Here we show for the first time that fully-automated angiogram
interpretation to estimate coronary artery stenosis is possible using a
sequence of deep neural network algorithms. The algorithmic pipeline we
developed--called CathAI--achieves state-of-the art performance across the
sequence of tasks required to accomplish automated interpretation of
unselected, real-world angiograms. CathAI (Algorithms 1-2) demonstrated
positive predictive value, sensitivity and F1 score of >=90% to identify the
projection angle overall and >=93% for left or right coronary artery angiogram
detection, the primary anatomic structures of interest. To predict obstructive
coronary artery stenosis (>=70% stenosis), CathAI (Algorithm 4) exhibited an
area under the receiver operating characteristic curve (AUC) of 0.862 (95% CI:
0.843-0.880). When externally validated in a healthcare system in another
country, CathAI AUC was 0.869 (95% CI: 0.830-0.907) to predict obstructive
coronary artery stenosis. Our results demonstrate that multiple purpose-built
neural networks can function in sequence to accomplish the complex series of
tasks required for automated analysis of real-world angiograms. Deployment of
CathAI may serve to increase standardization and reproducibility in coronary
stenosis assessment, while providing a robust foundation to accomplish future
tasks for algorithmic angiographic interpretation.",arxiv
http://arxiv.org/abs/2102.07510v2,2021-03-19T10:10:08Z,2021-02-15T12:19:28Z,Plug-and-Play gradient-based denoisers applied to CT image enhancement,"Blur and noise corrupting Computed Tomography (CT) images can hide or distort
small but important details, negatively affecting the diagnosis. In this paper,
we present a novel gradient-based Plug-and-Play algorithm, constructed on the
Half-Quadratic Splitting scheme, and we apply it to restore CT images. In
particular, we consider different schemes encompassing external and internal
denoisers as priors, defined on the image gradient domain. The internal prior
is based on the Total Variation functional. The external denoiser is
implemented by a deep Convolutional Neural Network (CNN) trained on the
gradient domain (and not on the image one, as in state-of-the-art works). We
also prove a general fixed-point convergence theorem under weak assumptions on
both internal and external denoisers. The experiments confirm the effectiveness
of the proposed framework in restoring blurred noisy CT images, both in
simulated and real medical settings. The achieved enhancements in the restored
images are really remarkable, if compared to the results of many
state-of-the-art methods.",arxiv
http://arxiv.org/abs/2103.15245v3,2021-04-03T14:55:20Z,2021-03-28T23:36:56Z,"Game Theory Based Privacy Preserving Approach for Collaborative Deep
  Learning in IoT","The exponential growth of Internet of Things (IoT) has become a transcending
force in creating innovative smart devices and connected domains including
smart homes, healthcare, transportation and manufacturing. With billions of IoT
devices, there is a huge amount of data continuously being generated,
transmitted, and stored at various points in the IoT architecture. Deep
learning is widely being used in IoT applications to extract useful insights
from IoT data. However, IoT users have security and privacy concerns and prefer
not to share their personal data with third party applications or stakeholders.
In order to address user privacy concerns, Collaborative Deep Learning (CDL)
has been largely employed in data-driven applications which enables multiple
IoT devices to train their models locally on edge gateways. In this chapter, we
first discuss different types of deep learning approaches and how these
approaches can be employed in the IoT domain. We present a privacy-preserving
collaborative deep learning approach for IoT devices which can achieve benefits
from other devices in the system. This learning approach is analyzed from the
behavioral perspective of mobile edge devices using a game-theoretic model. We
analyze the Nash Equilibrium in N-player static game model. We further present
a novel fair collaboration strategy among edge IoT devices using cluster based
approach to solve the CDL game, which enforces mobile edge devices for
cooperation. We also present implementation details and evaluation analysis in
a real-world smart home deployment.",arxiv
http://arxiv.org/abs/1809.07763v4,2020-05-26T15:15:19Z,2018-09-19T19:14:46Z,"auditor: an R Package for Model-Agnostic Visual Validation and
  Diagnostics","Machine learning models have spread to almost every area of life. They are
successfully applied in biology, medicine, finance, physics, and other fields.
With modern software it is easy to train even a~complex model that fits the
training data and results in high accuracy on the test set. The problem arises
when models fail confronted with real-world data.
  This paper describes methodology and tools for model-agnostic audit.
Introduced techniques facilitate assessing and comparing the goodness of fit
and performance of models. In~addition, they may be used for the analysis of
the similarity of residuals and for identification of~outliers and influential
observations. The examination is carried out by diagnostic scores and visual
verification.
  Presented methods were implemented in the auditor package for R. Due to
flexible and~consistent grammar, it is simple to validate models of any
classes.",arxiv
http://arxiv.org/abs/2003.05731v4,2021-03-05T01:55:27Z,2020-03-11T00:22:50Z,"SUOD: Accelerating Large-Scale Unsupervised Heterogeneous Outlier
  Detection","Outlier detection (OD) is a key machine learning (ML) task for identifying
abnormal objects from general samples with numerous high-stake applications
including fraud detection and intrusion detection. Due to the lack of ground
truth labels, practitioners often have to build a large number of unsupervised,
heterogeneous models (i.e., different algorithms with varying hyperparameters)
for further combination and analysis, rather than relying on a single model.
How to accelerate the training and scoring on new-coming samples by
outlyingness (referred as prediction throughout the paper) with a large number
of unsupervised, heterogeneous OD models? In this study, we propose a modular
acceleration system, called SUOD, to address it. The proposed system focuses on
three complementary acceleration aspects (data reduction for high-dimensional
data, approximation for costly models, and taskload imbalance optimization for
distributed environment), while maintaining performance accuracy. Extensive
experiments on more than 20 benchmark datasets demonstrate SUOD's effectiveness
in heterogeneous OD acceleration, along with a real-world deployment case on
fraudulent claim analysis at IQVIA, a leading healthcare firm. We open-source
SUOD for reproducibility and accessibility.",arxiv
http://arxiv.org/abs/1908.00149v1,2019-07-31T23:53:50Z,2019-07-31T23:53:50Z,"Response time optimization for drone-delivered automated external
  defibrillators","Out-of-hospital cardiac arrest (OHCA) claims over 400,000 lives each year in
North America and is one of the most time-sensitive medical emergencies.
Drone-delivered automated external defibrillators (AEDs) have the potential to
be a transformative innovation in the provision of emergency care for OHCA. In
this paper, we propose a simulation-optimization framework to minimize the
total number of drones required to meet a pre-specified response time goal,
while guaranteeing a sufficient number of drones are located at each base. To
do this, we develop a location-queuing model that is based on the p-median
architecture, where each base constitutes an explicit M/M/d queue, and that
incorporates estimated baseline response times to the demand points. We then
develop a reformulation technique that exploits the baseline response times,
allowing us to solve real-world instances to optimality using an off-the-shelf
solver. To test our model, we develop a two-stage machine learning approach to
simulate both the locations and baseline response times for future OHCAs. We
demonstrate the application of our framework using eight years of real data
from an area covering 26,000 square kilometres around Toronto, Canada. A modest
number of drones are required to significantly reduce response times in all
regions. Furthermore, an objective function focused on improving the 90th
percentile is well-suited for use in practice because the model reduces the
entire response time distribution, while providing equitable coverage in both
cities and rural areas. Overall, this paper provides a realistic framework that
can be leveraged by healthcare providers seeking to implement a drone network.",arxiv
http://arxiv.org/abs/2110.06196v1,2021-10-12T17:49:46Z,2021-10-12T17:49:46Z,GraPE: fast and scalable Graph Processing and Embedding,"Graph Representation Learning methods have enabled a wide range of learning
problems to be addressed for data that can be represented in graph form.
Nevertheless, several real world problems in economy, biology, medicine and
other fields raised relevant scaling problems with existing methods and their
software implementation, due to the size of real world graphs characterized by
millions of nodes and billions of edges. We present GraPE, a software resource
for graph processing and random walk based embedding, that can scale with large
and high-degree graphs and significantly speed up-computation. GraPE comprises
specialized data structures, algorithms, and a fast parallel implementation
that displays everal orders of magnitude improvement in empirical space and
time complexity compared to state of the art software resources, with a
corresponding boost in the performance of machine learning methods for edge and
node label prediction and for the unsupervised analysis of graphs.GraPE is
designed to run on laptop and desktop computers, as well as on high performance
computing clusters",arxiv
http://arxiv.org/abs/2012.07278v1,2020-12-14T05:54:55Z,2020-12-14T05:54:55Z,"Learning how to approve updates to machine learning algorithms in
  non-stationary settings","Machine learning algorithms in healthcare have the potential to continually
learn from real-world data generated during healthcare delivery and adapt to
dataset shifts. As such, the FDA is looking to design policies that can
autonomously approve modifications to machine learning algorithms while
maintaining or improving the safety and effectiveness of the deployed models.
However, selecting a fixed approval strategy, a priori, can be difficult
because its performance depends on the stationarity of the data and the quality
of the proposed modifications. To this end, we investigate a
learning-to-approve approach (L2A) that uses accumulating monitoring data to
learn how to approve modifications. L2A defines a family of strategies that
vary in their ""optimism''---where more optimistic policies have faster approval
rates---and searches over this family using an exponentially weighted average
forecaster. To control the cumulative risk of the deployed model, we give L2A
the option to abstain from making a prediction and incur some fixed abstention
cost instead. We derive bounds on the average risk of the model deployed by
L2A, assuming the distributional shifts are smooth. In simulation studies and
empirical analyses, L2A tailors the level of optimism for each problem-setting:
It learns to abstain when performance drops are common and approve beneficial
modifications quickly when the distribution is stable.",arxiv
http://arxiv.org/abs/2103.13511v1,2021-03-24T22:33:38Z,2021-03-24T22:33:38Z,Addressing catastrophic forgetting for medical domain expansion,"Model brittleness is a key concern when deploying deep learning models in
real-world medical settings. A model that has high performance at one
institution may suffer a significant decline in performance when tested at
other institutions. While pooling datasets from multiple institutions and
retraining may provide a straightforward solution, it is often infeasible and
may compromise patient privacy. An alternative approach is to fine-tune the
model on subsequent institutions after training on the original institution.
Notably, this approach degrades model performance at the original institution,
a phenomenon known as catastrophic forgetting. In this paper, we develop an
approach to address catastrophic forget-ting based on elastic weight
consolidation combined with modulation of batch normalization statistics under
two scenarios: first, for expanding the domain from one imaging system's data
to another imaging system's, and second, for expanding the domain from a large
multi-institutional dataset to another single institution dataset. We show that
our approach outperforms several other state-of-the-art approaches and provide
theoretical justification for the efficacy of batch normalization modulation.
The results of this study are generally applicable to the deployment of any
clinical deep learning model which requires domain expansion.",arxiv
http://arxiv.org/abs/2009.12406v1,2020-09-25T19:15:26Z,2020-09-25T19:15:26Z,"Why have a Unified Predictive Uncertainty? Disentangling it using Deep
  Split Ensembles","Understanding and quantifying uncertainty in black box Neural Networks (NNs)
is critical when deployed in real-world settings such as healthcare. Recent
works using Bayesian and non-Bayesian methods have shown how a unified
predictive uncertainty can be modelled for NNs. Decomposing this uncertainty to
disentangle the granular sources of heteroscedasticity in data provides rich
information about its underlying causes. We propose a conceptually simple
non-Bayesian approach, deep split ensemble, to disentangle the predictive
uncertainties using a multivariate Gaussian mixture model. The NNs are trained
with clusters of input features, for uncertainty estimates per cluster. We
evaluate our approach on a series of benchmark regression datasets, while also
comparing with unified uncertainty methods. Extensive analyses using dataset
shits and empirical rule highlight our inherently well-calibrated models. Our
work further demonstrates its applicability in a multi-modal setting using a
benchmark Alzheimer's dataset and also shows how deep split ensembles can
highlight hidden modality-specific biases. The minimal changes required to NNs
and the training procedure, and the high flexibility to group features into
clusters makes it readily deployable and useful. The source code is available
at https://github.com/wazeerzulfikar/deep-split-ensembles",arxiv
http://arxiv.org/abs/2106.07875v1,2021-06-15T04:24:59Z,2021-06-15T04:24:59Z,S-LIME: Stabilized-LIME for Model Explanation,"An increasing number of machine learning models have been deployed in domains
with high stakes such as finance and healthcare. Despite their superior
performances, many models are black boxes in nature which are hard to explain.
There are growing efforts for researchers to develop methods to interpret these
black-box models. Post hoc explanations based on perturbations, such as LIME,
are widely used approaches to interpret a machine learning model after it has
been built. This class of methods has been shown to exhibit large instability,
posing serious challenges to the effectiveness of the method itself and harming
user trust. In this paper, we propose S-LIME, which utilizes a hypothesis
testing framework based on central limit theorem for determining the number of
perturbation points needed to guarantee stability of the resulting explanation.
Experiments on both simulated and real world data sets are provided to
demonstrate the effectiveness of our method.",arxiv
http://arxiv.org/abs/1910.03060v1,2019-10-07T20:06:38Z,2019-10-07T20:06:38Z,Impact of Inference Accelerators on hardware selection,"As opportunities for AI-assisted healthcare grow steadily, model deployment
faces challenges due to the specific characteristics of the industry. The
configuration choice for a production device can impact model performance while
influencing operational costs. Moreover, in healthcare some situations might
require fast, but not real time, inference. We study different configurations
and conduct a cost-performance analysis to determine the optimized hardware for
the deployment of a model subject to healthcare domain constraints. We observe
that a naive performance comparison may not lead to an optimal configuration
selection. In fact, given realistic domain constraints, CPU execution might be
preferable to GPU accelerators. Hence, defining beforehand precise expectations
for model deployment is crucial.",arxiv
http://arxiv.org/abs/2001.09282v2,2020-04-08T08:52:44Z,2020-01-25T09:13:39Z,"Limited Angle Tomography reconstruction for non-standard MBI system by
  means of parallel-hole and pinhole optics","The purpose of the present work is the study of reconstruction properties of
a new Molecular Breast Imaging (MBI) device for the early diagnosis of breast
cancer, in Limited Angle Tomography (LAT), by using two asymmetric detector
heads with different collimators. The detectors face each other in
anti-parallel viewing direction and, mild-compressing the breast phantom, they
are able to reconstruct the inner tumour of the phantoms with only a limited
number of projections using a dedicated maximum-likelihood expectation
maximization (ML-EM) algorithm. Phantoms, MBI system, as well as Monte Carlo
simulator using Geant 4 Application for Tomographic Emission (GATE) software,
are briefly described. MBI system's model has been implemented in IDL
(Interactive Data Visualization), in order to evaluate the best LAT
configuration of the system and its reconstruction ability by varying tumour's
size, depth and uptake. LAT setup in real and simulated configurations, as well
as the ML-EM method and the preliminary reconstruction results, are discussed.",arxiv
http://arxiv.org/abs/2011.10823v2,2021-06-23T08:49:24Z,2020-11-21T16:45:02Z,"A System for Automatic Rice Disease Detection from Rice Paddy Images
  Serviced via a Chatbot","A LINE Bot System to diagnose rice diseases from actual paddy field images
was developed and presented in this paper. It was easy-to-use and automatic
system designed to help rice farmers improve the rice yield and quality. The
targeted images were taken from the actual paddy environment without special
sample preparation. We used a deep learning neural networks technique to detect
rice diseases from the images. We developed an object detection model training
and refinement process to improve the performance of our previous research on
rice leave diseases detection. The process was based on analyzing the model's
predictive results and could be repeatedly used to improve the quality of the
database in the next training of the model. The deployment model for our LINE
Bot system was created from the selected best performance technique in our
previous paper, YOLOv3, trained by refined training data set. The performance
of the deployment model was measured on 5 target classes and found that the
Average True Positive Point improved from 91.1% in the previous paper to 95.6%
in this study. Therefore, we used this deployment model for Rice Disease LINE
Bot system. Our system worked automatically real-time to suggest primary
diagnosis results to the users in the LINE group, which included rice farmers
and rice disease specialists. They could communicate freely via chat. In the
real LINE Bot deployment, the model's performance was measured by our own
defined measurement Average True Positive Point and was found to be an average
of 78.86%. The system was fast and took only 2-3 s for detection process in our
system server.",arxiv
http://arxiv.org/abs/2012.02308v2,2021-03-08T10:30:50Z,2020-12-03T22:18:24Z,Concept-based model explanations for Electronic Health Records,"Recurrent Neural Networks (RNNs) are often used for sequential modeling of
adverse outcomes in electronic health records (EHRs) due to their ability to
encode past clinical states. These deep, recurrent architectures have displayed
increased performance compared to other modeling approaches in a number of
tasks, fueling the interest in deploying deep models in clinical settings. One
of the key elements in ensuring safe model deployment and building user trust
is model explainability. Testing with Concept Activation Vectors (TCAV) has
recently been introduced as a way of providing human-understandable
explanations by comparing high-level concepts to the network's gradients. While
the technique has shown promising results in real-world imaging applications,
it has not been applied to structured temporal inputs. To enable an application
of TCAV to sequential predictions in the EHR, we propose an extension of the
method to time series data. We evaluate the proposed approach on an open EHR
benchmark from the intensive care unit, as well as synthetic data where we are
able to better isolate individual effects.",arxiv
http://arxiv.org/abs/1905.06004v1,2019-05-15T07:48:21Z,2019-05-15T07:48:21Z,Domain Adaptive Transfer Learning for Fault Diagnosis,"Thanks to digitization of industrial assets in fleets, the ambitious goal of
transferring fault diagnosis models fromone machine to the other has raised
great interest. Solving these domain adaptive transfer learning tasks has the
potential to save large efforts on manually labeling data and modifying models
for new machines in the same fleet. Although data-driven methods have shown
great potential in fault diagnosis applications, their ability to generalize on
new machines and new working conditions are limited because of their tendency
to overfit to the training set in reality. One promising solution to this
problem is to use domain adaptation techniques. It aims to improve model
performance on the target new machine. Inspired by its successful
implementation in computer vision, we introduced Domain-Adversarial Neural
Networks (DANN) to our context, along with two other popular methods existing
in previous fault diagnosis research. We then carefully justify the
applicability of these methods in realistic fault diagnosis settings, and offer
a unified experimental protocol for a fair comparison between domain adaptation
methods for fault diagnosis problems.",arxiv
http://arxiv.org/abs/2104.09876v1,2021-04-20T10:16:04Z,2021-04-20T10:16:04Z,"IIoT-Enabled Health Monitoring for Integrated Heat Pump System Using
  Mixture Slow Feature Analysis","The sustaining evolution of sensing and advancement in communications
technologies have revolutionized prognostics and health management for various
electrical equipment towards data-driven ways. This revolution delivers a
promising solution for the health monitoring problem of heat pump (HP) system,
a vital device widely deployed in modern buildings for heating use, to timely
evaluate its operation status to avoid unexpected downtime. Many HPs were
practically manufactured and installed many years ago, resulting in fewer
sensors available due to technology limitations and cost control at that time.
It raises a dilemma to safeguard HPs at an affordable cost. We propose a hybrid
scheme by integrating industrial Internet-of-Things (IIoT) and intelligent
health monitoring algorithms to handle this challenge. To start with, an IIoT
network is constructed to sense and store measurements. Specifically,
temperature sensors are properly chosen and deployed at the inlet and outlet of
the water tank to measure water temperature. Second, with temperature
information, we propose an unsupervised learning algorithm named mixture slow
feature analysis (MSFA) to timely evaluate the health status of the integrated
HP. Characterized by frequent operation switches of different HPs due to the
variable demand for hot water, various heating patterns with different heating
speeds are observed. Slowness, a kind of dynamics to measure the varying speed
of steady distribution, is properly considered in MSFA for both heating pattern
division and health evaluation. Finally, the efficacy of the proposed method is
verified through a real integrated HP with five connected HPs installed ten
years ago. The experimental results show that MSFA is capable of accurately
identifying health status of the system, especially failure at a preliminary
stage compared to its competing algorithms.",arxiv
http://arxiv.org/abs/2105.14052v1,2021-05-28T18:37:12Z,2021-05-28T18:37:12Z,"Targeted Deep Learning: Framework, Methods, and Applications","Deep learning systems are typically designed to perform for a wide range of
test inputs. For example, deep learning systems in autonomous cars are supposed
to deal with traffic situations for which they were not specifically trained.
In general, the ability to cope with a broad spectrum of unseen test inputs is
called generalization. Generalization is definitely important in applications
where the possible test inputs are known but plentiful or simply unknown, but
there are also cases where the possible inputs are few and unlabeled but known
beforehand. For example, medicine is currently interested in targeting
treatments to individual patients; the number of patients at any given time is
usually small (typically one), their diagnoses/responses/... are still unknown,
but their general characteristics (such as genome information, protein levels
in the blood, and so forth) are known before the treatment. We propose to call
deep learning in such applications targeted deep learning. In this paper, we
introduce a framework for targeted deep learning, and we devise and test an
approach for adapting standard pipelines to the requirements of targeted deep
learning. The approach is very general yet easy to use: it can be implemented
as a simple data-preprocessing step. We demonstrate on a variety of real-world
data that our approach can indeed render standard deep learning faster and more
accurate when the test inputs are known beforehand.",arxiv
http://arxiv.org/abs/1811.07738v3,2019-04-23T07:51:28Z,2018-11-19T14:51:56Z,"M2U-Net: Effective and Efficient Retinal Vessel Segmentation for
  Resource-Constrained Environments","In this paper, we present a novel neural network architecture for retinal
vessel segmentation that improves over the state of the art on two benchmark
datasets, is the first to run in real time on high resolution images, and its
small memory and processing requirements make it deployable in mobile and
embedded systems. The M2U-Net has a new encoder-decoder architecture that is
inspired by the U-Net. It adds pretrained components of MobileNetV2 in the
encoder part and novel contractive bottleneck blocks in the decoder part that,
combined with bilinear upsampling, drastically reduce the parameter count to
0.55M compared to 31.03M in the original U-Net. We have evaluated its
performance against a wide body of previously published results on three public
datasets. On two of them, the M2U-Net achieves new state-of-the-art performance
by a considerable margin. When implemented on a GPU, our method is the first to
achieve real-time inference speeds on high-resolution fundus images. We also
implemented our proposed network on an ARM-based embedded system where it
segments images in between 0.6 and 15 sec, depending on the resolution. Thus,
the M2U-Net enables a number of applications of retinal vessel structure
extraction, such as early diagnosis of eye diseases, retinal biometric
authentication systems, and robot assisted microsurgery.",arxiv
http://arxiv.org/abs/2011.04128v1,2020-11-09T01:17:58Z,2020-11-09T01:17:58Z,"Stable predictions for health related anticausal prediction tasks
  affected by selection biases: the need to deconfound the test set features","In health related machine learning applications, the training data often
corresponds to a non-representative sample from the target populations where
the learners will be deployed. In anticausal prediction tasks, selection biases
often make the associations between confounders and the outcome variable
unstable across different target environments. As a consequence, the
predictions from confounded learners are often unstable, and might fail to
generalize in shifted test environments. Stable prediction approaches aim to
solve this problem by producing predictions that are stable across unknown test
environments. These approaches, however, are sometimes applied to the training
data alone with the hope that training an unconfounded model will be enough to
generate stable predictions in shifted test sets. Here, we show that this is
insufficient, and that improved stability can be achieved by deconfounding the
test set features as well. We illustrate these observations using both
synthetic data and real world data from a mobile health study.",arxiv
