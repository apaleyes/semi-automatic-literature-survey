id,datePublished,description,title,downloadUrl,publisher,doi,journals,database
357234107,2020-03-05T00:00:00,"Abstract In this paper, we describe development of a mobile robot which does unsupervised learning for recognizing an environment from action sequences. We call this novel recognition approach action-based environment modeling (AEM). Most studies on recognizing an environment have tried to build precise geometric maps with high sensitive and global sensors. However such precise and global information may be hardly obtained in a real environment, and may be unnecessary to recognize an environment. Furthermore unsupervised-learning is necessary for recognition in an unknown environment without help of a teacher. Thus we attempt to build a mobile robot which does unsupervised-learning to recognize environments with low sensitive and local sensors. The mobile robot is behavior-based and does wall-following in enclosures (called rooms). Then the sequences of actions executed in each room are transformed into environment vectors for self-organizing maps. Learning without a teacher is done, and the robot becomes able to identify rooms. Moreover, we develop a method to identify environments independent of a start point using a partial sequence. We have fully implemented the system with a real mobile robot, and made experiments for evaluating the ability. As a result, we found out that the environment recognition was done well and our method was adaptive to noisy environments",Recognizing environments from action sequences using self-organizing maps,https://core.ac.uk/download/357234107.pdf,,,,core
323323388,2020-08-13T00:00:00,"The use of Reinforcement Learning (RL) is still restricted to simulation or
to enhance human-operated systems through recommendations. Real-world
environments (e.g. industrial robots or power grids) are generally designed
with safety constraints in mind implemented in the shape of valid actions masks
or contingency controllers. For example, the range of motion and the angles of
the motors of a robot can be limited to physical boundaries. Violating
constraints thus results in rejected actions or entering in a safe mode driven
by an external controller, making RL agents incapable of learning from their
mistakes. In this paper, we propose a simple modification of a state-of-the-art
deep RL algorithm (DQN), enabling learning from forbidden actions. To do so,
the standard Q-learning update is enhanced with an extra safety loss inspired
by structured classification. We empirically show that it reduces the number of
hit constraints during the learning phase and accelerates convergence to
near-optimal policies compared to using standard DQN. Experiments are done on a
Visual Grid World Environment and Text-World domain.Comment: Accepted at Internationnal Joint Conference on Neural Networks
  (IJCNN'2020","I'm sorry Dave, I'm afraid I can't do that, Deep Q-learning from
  forbidden action",http://arxiv.org/abs/1910.02078,,,,core
355105498,2020-11-16T00:00:00,"This paper discusses the creative and technical approaches in a performative robot project called Embodied Musicking Robots (2018 – present). The core approach of this project is Human-centred AI (HC-AI) which focuses on the design, development and deployment of intelligent systems that co-operate with humans in real-time in a ‘deep and meaningful way’. HC-AI is ‘defined by two goals: (1) the AI system must continually improve by learning from humans while (2) creating an effective and fulfilling human-robot interaction experience’ [1]. This project applies these core goals as a central philosophy from which the concepts of Creative AI and Experien-tial Learning in the context of performative robots are developed. At the centre of this discussion is the articulation of a shift in thinking of what constitutes Creative AI and new HC-AI forms of computational learning from inside the flow of the shared experience between robot and human. The central case study investigates the technical solutions and artistic po-tential of AI driven robots co-creating with an improvising human musician in real-time. This project is ongoing and is part of the Creative AI Research Group in the Institute of Creative Technologies at De Montfort Universit",Creative AI and Musicking Robots,https://core.ac.uk/download/355105498.pdf,,,,core
475608342,2021-01-01T00:00:00,"As deep learning for resource-constrained systems become more popular, we see an increased number of intelligent embedded systems such as IoT devices, robots, autonomous vehicles, and the plethora of portable, wearable, and mobile devices that are feature-packed with a wide variety of machine learning tasks. However, the performance of DNNs (deep neural networks) running on an embedded system is significantly limited by the platform's CPU, memory, and battery-size; and their scope is limited to simplistic inference tasks only.      This dissertation proposes on-device deep learning algorithms and supporting hardware designs, enabling embedded systems to efficiently perform deep intelligent tasks (i.e., deep neural networks) that are high-memory-footprint, compute-intensive, and energy-hungry beyond their limited computing resources. We name such on-device deep intelligence on embedded systems as Embedded Deep Intelligence. Specifically, we introduce resource-aware learning strategies devised to overcome the four fundamental constraints of embedded systems imposed on the way towards Embedded Deep Intelligence, i.e., in-memory multitask learning via introducing the concept of Neural Weight Virtualization, adaptive real-time learning via introducing the concept of SubFlow, opportunistic accelerated learning via introducing the concept of Neuro.ZERO, and energy-aware intermittent learning, which tackles the problems of the small size of memory, dynamic timing constraint, low-computing capability, and limited energy, respectively.       Once deployed in the field with the proposed resource-aware learning strategies, embedded systems are not only able to perform deep inference tasks on sensor data but also update and re-train their learning models at run-time without requiring any help from any external system. Such an on-device learning capability of Embedded Deep Intelligence makes an embedded intelligent system real-time, privacy-aware, secure, autonomous, untethered, responsive, and adaptive without concern for its limited resources.Doctor of Philosoph",Enabling Deep Intelligence on Embedded Systems,https://core.ac.uk/download/475608342.pdf,University of North Carolina at Chapel Hill Graduate School,10.17615/azw9-g349,,core
395661195,2020-01-01T00:00:00,"Comisión Europea. Joint Research Centre. 
Serie: JRC Science for Police ReportThis report reviews and classifies the current and near-future applications of Artificial Intelligence (AI) in Medicine and Healthcare according to their ethical and societal impact and the availability level of the various technological implementations. It provides conceptual foundations for well-informed policy-oriented work, research, and forward-looking activities that address the opportunities and challenges created in the field of AI in Medicine and Healthcare. This report is aimed for policy developers, but it also makes contributions that are of interest for researchers studying the impact and the future of AI on Healthcare, for scientific and technological stakeholders in this field and for the general public.This report is based on an analysis of the state of the art of research and technology, including software, personal monitoring devices, genetic tests and editing tools, personalized digital models, online platforms, augmented reality devices, and surgical and companion robotics. From this analysis, it is presented the concept of “extended personalized medicine”, and it is explored the public perception of medical AI systems, and how they show, simultaneously, extraordinary opportunities and drawbacks. In addition, this report addresses the transformation of the roles of doctors and patients in an age of ubiquitous information and identifies three main paradigms in AI-supported Medicine: “fake-based”, “patient-generated”, and “scientifically tailored” views.This Report presents:- An updated overview of the many aspects related to the social impact of Artificial Intelligence and its applications in Medicine and Health. A new ‘Technology Availability Scale’ is defined to evaluate and compare their current status.- Recent examples of the growing social concerns and debates in the general press, social media and other web-bases sources.- A ‘Visual Overview of AI and AI-mediated technologies in Medicine and Healthcare’, in which two figures show, respeComisión Europea. Joint Research Centr","Artificial Intelligence in Medicine and Healthcare: applications, availability and societal impact",https://core.ac.uk/download/395661195.pdf,'Wiley',,,core
334990296,2020-08-01T07:00:00,"The increasing number of spacefaring nations and agendas, miniaturization of subsystems, and trend toward integrated systems are no doubt influencing the evolution of space systems. The diversification of space architectures has surged at an unprecedented rate in recent history with initial deployments of planned mega-constellations. This paper explores HIVE-a reconfigurable small satellite system primed to revolutionize the concept of modular space systems and future space architectures.
Based on a mass producible functioning unit consisting of nested rings, HIVE is a comprehensive satellite design harnessing advancement in robotics, software and machine learning, precision scale manufacturing, and novel materials with multifunctional properties. HIVE is addressing solutions for detailed design of interconnected hardware, engineering analysis for multi-payload applications, and policy to accomplish modularized, in-space deployment and reconfiguration.
The HIVE unit design lends itself to the “infinite possibilities” of space mission architectures and presents a revolutionary way to design, integrate, and operate missions from space. This paper provides and overview of the HIVE concept development and provides examples of applications for HIVE to showcase the range of possible systems and architectural advantages; such as space domain awareness, large service structure, and planetary surface infrastructure. Finally, we will discuss technology transfer and possible pathways to making a resilient, adaptable, and continually upgradable space infrastructure a reality",HIVE: A Space Architecture Concept,https://core.ac.uk/download/334990296.pdf,DigitalCommons@USU,,,core
389431508,2020-10-16T00:00:00,"Intelligent assistive robots can potentially support elderly persons and caregivers in their everyday
lives and facilitate a closer man and machine collaboration as an essential part of the yet to come 5-th industrial revolution. In contrast to classical robotic applications where robots were mostly designed for repetitive tasks, assistive robots will face a variety of different tasks in close contact with everyday users. In particular, it is difficult to foresee the variety of applications beforehand since they depend on a person's individual needs and preferences. This renders preprogramming of all tasks for assistive robots difficult and gives need to explore methods of how robots can learn new tasks at hand during deployment time. Learning from and during direct interaction with humans provides hereby a potentially powerful tool for an assistive robot to acquire new skills and incorporate prior human knowledge during the exploration of novel tasks. Such an interactive learning process can not only help the robot to acquire new skills or profit from human prior knowledge but also facilitates the participation of inexperienced users or coworkers which can lead to a higher acceptance of the robot. However, while on the one hand human presence and assistance can be beneficial during the learning process, on the other hand, close contact with inexperienced users also imposes challenges. In shared workspaces or in close contact with everyday users a robot should be able to adapt learned skills to achieve as little disturbance of humans as possible. It becomes also important to evaluate human preferences about such adaptation strategies, their understanding of interactive learning processes and different ways for human input into learning. To come closer to the goal of intelligent assistive robots is therefore important to develop novel interactive learning methods and evaluate them in different robotic applications. 
This thesis focusses on three main challenges related to the development of assistive intelligent robots and their interaction with everyday users. The different parts of the thesis contribute not only novel theoretical methods but additionally also evaluations on different robotic tasks with users, that had zero or only little prior experience with robots. 
The first challenge is to enable robots to learn cooperative skills from a potentially open-ended stream of human demonstrations in an incremental fashion. While learning new skills from human demonstrations has already been exploited in the literature it remains challenging to learn skill libraries from incrementally incoming demonstrations and when the total number of skills is not known beforehand. Therefore, in the first part of the thesis, we introduce an approach for online and incremental learning of a library for collaborative skills. Here, we follow a Mixture of Experts based approach and incrementally learn a library of collaborative skills and a gating model from coupled human-robot trajectories. Once trained, the gating model can decide which skill to choose as an appropriate response to a human motion, based on prior demonstrations and activate the corresponding robot skill. In contrast to existing batch learning methods, our method does not require the total number of skills to be known a priori and can learn new skills as well as update existing skills from multiple human demonstrations. The cooperative skills are represented as Probabilistic Interaction Primitives which can capture variance and inherent correlations in the demonstrations. We evaluate our method with different human subjects in a task where a robot assists the subjects in making a salad. We also evaluate hereby how learned skills transfer between different subjects.
Second, intelligent assistive robots should be able to adapt learned skills to humans when working in close contact or shared workspaces. For Probabilistic Movement Primitives (ProMPs), which were chosen as a skill representation in this thesis, such methods for online adaptation were missing in the literature so far. Hereby, it is in particular important to also evaluate the perceived level of safety and comfort of humans according to different adaptation strategies. To this end, we present two methods for online adaptation of learned skills in a shared workspace setting. Here, we introduce two novel online adaptation methods for ProMPs, namely spatial deformation and temporal scaling. Spatial deformation avoids collisions by dynamically changing the shape of the movement primitive, while at the same time staying close to the demonstrated motions. In temporal scaling, we adapt the ProMP's velocity profile to avoid time-dependent collisions. To achieve intention aware adaptation in shared workspaces we combine both methods with a goal-directed prediction model for human motions. This prediction model can also be learned online from human motions. We conducted experiments for both novel adaptation methods in comparison to non-adaptive behavior with inexperienced users and evaluated influences on task performance as well as subjective metrics such as comfort and perceived level of safety.
The third challenge that we consider in this thesis is how a library of learned skills can be used in practice to solve sequential robotic tasks. While hereby reinforcement learning offers a powerful tool for reward-driven learning and self-improvement, in real robotic applications it often suffers from costly and time-consuming sample collection. Here, human input might be beneficial to speed up and guide the learning. Therefore, it is important to enable and compare different ways how human input can be incorporated in reinforcement learning algorithms. 
In this thesis, we present an approach, which incorporates multiple forms of human input into reinforcement learning for sequential tasks. Since depending on the task human input might not always be correct, we additionally introduce the concept of self-confidence for the robot, such that it becomes able to question human input. We evaluate which input channels humans prefer during interaction and how well they accept suggestions or rejections of the robot if the robot becomes confident in its own decisions.
To summarize, the different parts of the thesis contribute to the development of intelligent assistive robots that can learn from imitating humans, adapt the learned skills dynamically to humans in shared workspaces and profit and learn from human input during self-driven learning of how to sequence skills into more complex tasks. The three main contributions to the state of the art are hereby: First, a novel approach to incrementally learn a library for collaborative skills when the total number of skills is not known a priori. Second, two novel methods for online adaptation of ProMPs and their combination with a goal-directed prediction model to enable intention aware online adaptation in shared workspaces. And third, an approach that combines multiple forms of human input with a reinforcement learning algorithm and a novel concept of self-confidence to learn and improve the sequencing of skills into more complex tasks",Interactive Machine Learning for Assistive Robots,,,10.25534/tuprints-00014184,,core
441356408,2020-07-01T00:00:00,"Abstract Over the years, technology has changed the way we produce and have access to our food through the development of applications, robotics, data analysis, and processing techniques. The implementation of these approaches by the food industry ensure quality and affordability, reducing at the same time the costs of keeping the food fresh and increase productivity. A system, as the one presented herein, for raw food categorization is needed in future food industries to automate food classification according to type, the process of algorithm approaches that will be applied to every different food origin and also for serving disabled people. The purpose of this work was to develop a machine learning workflow based on supervised PLS regression and SVM classification, towards automated raw food categorization from FTIR. The system exhibited high efficiency in multi-class classification of 7 different types of raw food. The selected food samples, were diverse in terms of storage conditions (temperature, storage time and packaging), while the variability within each food was also taken into account by several different batches; leading in a classifier able to embed this variation towards increased robustness and efficiency, ready for real life applications targeting to the digital transformation of the food industry",A machine learning workflow for raw food spectroscopic classification in a future industry,,'Springer Science and Business Media LLC',10.1038/s41598-020-68156-2,"[{'title': 'Scientific Reports', 'identifiers': ['2045-2322', 'issn:2045-2322']}]",core
479489243,2021-11-01T00:00:00,"This paper introduces a novel proprioceptive state estimator for legged
robots based on a learned displacement measurement from IMU data. Recent
research in pedestrian tracking has shown that motion can be inferred from
inertial data using convolutional neural networks. A learned inertial
displacement measurement can improve state estimation in challenging scenarios
where leg odometry is unreliable, such as slipping and compressible terrains.
Our work learns to estimate a displacement measurement from IMU data which is
then fused with traditional leg odometry. Our approach greatly reduces the
drift of proprioceptive state estimation, which is critical for legged robots
deployed in vision and lidar denied environments such as foggy sewers or dusty
mines. We compared results from an EKF and an incremental fixed-lag factor
graph estimator using data from several real robot experiments crossing
challenging terrains. Our results show a reduction of relative pose error by
37% in challenging scenarios when compared to a traditional kinematic-inertial
estimator without learned measurement. We also demonstrate a 22% reduction in
error when used with vision systems in visually degraded environments such as
an underground mine.Comment: To be presented at 5th Annual Conference on Robot Learning (CoRL),
  202",Learning Inertial Odometry for Dynamic Legged Robot State Estimation,http://arxiv.org/abs/2111.00789,,,,core
186295629,2020-01-30T00:00:00,"We introduce Probabilistic Object Detection, the task of detecting objects in
images and accurately quantifying the spatial and semantic uncertainties of the
detections. Given the lack of methods capable of assessing such probabilistic
object detections, we present the new Probability-based Detection Quality
measure (PDQ).Unlike AP-based measures, PDQ has no arbitrary thresholds and
rewards spatial and label quality, and foreground/background separation quality
while explicitly penalising false positive and false negative detections. We
contrast PDQ with existing mAP and moLRP measures by evaluating
state-of-the-art detectors and a Bayesian object detector based on Monte Carlo
Dropout. Our experiments indicate that conventional object detectors tend to be
spatially overconfident and thus perform poorly on the task of probabilistic
object detection. Our paper aims to encourage the development of new object
detection approaches that provide detections with accurately estimated spatial
and label uncertainties and are of critical importance for deployment on robots
and embodied AI systems in the real world.Comment: 21 pages, 25 figures, to appear in the proceedings of the winter
  conference on applications of computer vision WACV 202",Probabilistic Object Detection: Definition and Evaluation,http://arxiv.org/abs/1811.10800,,,,core
387288691,2020-12-06T00:00:00,"Autonomous navigation is a long-standing field of robotics research, which
provides an essential capability for mobile robots to execute a series of tasks
on the same environments performed by human everyday. In this chapter, we
present a set of algorithms to train and deploy deep networks for autonomous
navigation of mobile robots using the Robot Operation System (ROS). We describe
three main steps to tackle this problem: i) collecting data in simulation
environments using ROS and Gazebo; ii) designing deep network for autonomous
navigation, and iii) deploying the learned policy on mobile robots in both
simulation and real-world. Theoretically, we present deep learning
architectures for robust navigation in normal environments (e.g., man-made
houses, roads) and complex environments (e.g., collapsed cities, or natural
caves). We further show that the use of visual modalities such as RGB, Lidar,
and point cloud is essential to improve the autonomy of mobile robots. Our
project website and demonstration video can be found at
https://sites.google.com/site/autonomousnavigationros.Comment: 18 pages. arXiv admin note: substantial text overlap with
  arXiv:2007.1594","Autonomous Navigation with Mobile Robots using Deep Learning and the
  Robot Operating System",http://arxiv.org/abs/2012.02417,,,,core
334935031,2020-04-21T00:00:00,"Universal grasping of a diverse range of previously unseen objects from heaps
is a grand challenge in e-commerce order fulfillment, manufacturing, and home
service robotics. Recently, deep learning based grasping approaches have
demonstrated results that make them increasingly interesting for industrial
deployments. This paper explores the problem from an automation systems
point-of-view. We develop a robotics grasping system using Dex-Net, which is
fully integrated at the controller level. Two neural networks are deployed on a
novel industrial AI hardware acceleration module close to a PLC with a power
footprint of less than 10 W for the overall system. The software is tightly
integrated with the hardware allowing for fast and efficient data processing
and real-time communication. The success rate of grasping an object form a bin
is up to 95 percent with more than 350 picks per hour, if object and receptive
bins are in close proximity. The system was presented at the Hannover Fair 2019
(world s largest industrial trade fair) and other events, where it performed
over 5,000 grasps per event","Industrial Robot Grasping with Deep Learning using a Programmable Logic
  Controller (PLC)",http://arxiv.org/abs/2004.10251,,,,core
323307956,2020-03-02T12:58:19,"Deep- and reinforcement-learning techniques have increasingly required large sets of real data to achieve stable convergence and generalization, in the context of image-recognition, object-detection or motion-control strategies. On this subject, the research community lacks robust approaches to overcome unavailable real-world extensive data by means of realistic synthetic-information and domain-adaptation techniques. In this work, synthetic-learning strategies have been used for the vision-based autonomous following of a noncooperative multirotor. The complete maneuver was learned with synthetic images and high-dimensional low-level continuous robot states, with deep- and reinforcement-learning techniques for object detection and motion control, respectively. A novel motion-control strategy for object following is introduced where the camera gimbal movement is coupled with the multirotor motion during the multirotor following. Results confirm that our present framework can be used to deploy a vision-based task in real flight using synthetic data. It was extensively validated in both simulated and real-flight scenarios, providing proper results (following a multirotor up to 1.3 m/s in simulation and 0.3 m/s in real flights). Keywords: multirotor; UAV; following; synthetic learning; reinforcement learning; deep learnin",Vision-based multirotor following using synthetic learning techniques,,'MDPI AG',10.3390/s19214794,"[{'title': 'Sensors', 'identifiers': ['issn:1424-8220', '1424-8220']}]",core
322706060,2020-03-12T00:00:00,"Machine and reinforcement learning (RL) are increasingly being applied to plan and control the behavior of autonomous systems interacting with the physical world. Examples include self-driving vehicles, distributed sensor networks, and agile robots. However, when machine learning is to be applied in these new settings, the algorithms had better come with the same type of reliability, robustness, and safety bounds that are hallmarks of control theory, or failures could be catastrophic. Thus, as learning algorithms are increasingly and more aggressively deployed in safety critical settings, it is imperative that control theorists join the conversation. The goal of this tutorial paper is to provide a starting point for control theorists wishing to work on learning related problems, by covering recent advances bridging learning and control theory, and by placing these results within an appropriate historical context of system identification and adaptive control",From self-tuning regulators to reinforcement learning and back again,,'Institute of Electrical and Electronics Engineers (IEEE)',10.1109/CDC40024.2019.9029916,,core
288813933,2020-03-27T00:00:00,"Η εκτίμηση του Κέντρου Μάζας (CoM) διαδραματίζει κρίσιμο ρόλο στη ρομποτική βάδιση. Οι περισσότεροι σχεδιαστές κίνησης και ελεγκτές βάδισης πραγματικού χρό¬νου υποθέτουν ότι η θέση και η ταχύτητα του CoM είναι διαθέσιμες για ανατροφοδό¬τηση ανά πάσα στιγμή. Σε αυτή τη διατριβή παρουσιάζουμε έναν από τους πρώτους τρισδιάστατους εκτιμητές κατάστασης CoM για το περπάτημα των ανθρωποειδών ρομπότ. Ο προτεινόμενος εκτιμητής συνδυάζει αποτελεσματικά τις μετρήσεις από αισθητήρες πίεσης στα πόδια, κωδικοποιητές στις αρθρώσεις και αδρανειακής μο¬νάδας (IMU) στο σώμα με ένα Εκτεταμένο Φίλτρο Κάλμαν (EKF) για την ακριβή εκτίμηση τόσο της θέσης και της ταχύτητας του CoM αλλά και των εξωτερικών δυ¬νάμεων που δρουν πάνω σε αυτό. Επιπλέον, λαμβάνει υπόψιν την ανωμαλότητα του εδάφους και την στροφορμή του σώματος με αποτέλεσμα να συνδυάζει το μετωπικό με το πλευρικό επίπεδο κίνησης, χωρίς να βασίζεται σε αισθητήρες δύναμης / ροπής (F/T) στα πόδια.
Ωστόσο, είναι κοινή πρακτική να επιχειρείται η μετατροπή των μετρήσεων σε ένα αδρανειακό σύστημα αναφοράς ώστε η εκτίμηση του CoM να γίνεται σε σχέση με αυτό. Κατά συνέπεια, για την επίτευξη του παραπάνω είναι υποχρεωτικό να συνε¬κτιμηθούν η βάση και το πόδι στήριξης του ρομπότ. Για το σκοπό αυτό, επεκτείνουμε έναν καθιερωμένο στη βιβλιογραφία εκτιμητή αιωρούμενης μάζας με τη δυναμική του ποδιού στήριξης χρησιμοποιώντας μετρήσεις κινηματικής και αδρανειακής μονάδας με το Φίλτρο Κάλμαν Σφάλματος Κατάστασης (ESKF) για την κατάλληλη διαχείριση της υπερ-παραμετροποίησης των περιστροφών. Με αυτό το τρόπο,δημιουργείται ένα σύστημα σειριακής εκτίμησης κατάστασης που αποτελείται από έναν εκτιμητή βάσης και έναν εκτιμητή CoM το οποίο ονομάζουμε State Estimation RObot Walking (SEROW). Επιπλέον, για να διορθώσουμε την κινηματική απόκλιση που προκαλείται από την ολίσθηση των ποδιών κατά το περπάτημα, χρησιμοποιούμε μετρήσεις Οπτι¬κής Οδομετρίας (VO) και/ή Οδομετρίας LIDAR (LO). Δυστυχώς, τέτοιες μετρήσεις υποφέρουν από ακραίες τιμές σε ένα δυναμικό περιβάλλον, αφού κατά τον υπολογι¬σμό τους χρησιμοποιείται η υπόθεση ότι μόνο το ρομπότ βρίσκεται σε κίνηση και ο κόσμος γύρω του είναι στατικός. Για αυτό το λόγο, εισάγουμε το Σθεναρό Γκαουσια-νό Φίλτρο Κάλμαν Σφάλματος Κατάστασης (RGESKF) για την αυτόματη ανίχνευση και απόρριψη των ακραίων μετρήσεων. Το προτεινόμενο φίλτρο δεν βασίζεται σε πρότερη γνώση σχετικά με τις κατανομές των μετρήσεων και δεν χρησιμοποιεί ειδι¬κά ρυθμισμένα κατώφλια. Ως εκ τούτου,το SEROW γίνεται ένα σθεναρό σύστημα εκτίμησης κατάστασης, κατάλληλο για δυναμικά ανθρώπινα περιβάλλοντα. Προ¬κειμένου να ενισχυθούν περαιτέρω οι ερευνητικές προσπάθειες, οο SEROW δίνεται ελεύθερα στη ρομποτική κοινότητα ως ένα πακέτο ROS/C++ ανοικτού κώδικα.
Τα σύγχρονα συστήματα ελέγχου και εκτίμησης κατάστασης ανθρωποειδών ρο¬μπότ υποθέτουν ότι η κατάσταση επαφής ποδιών-εδάφους είναι γνωστή εκ των προ¬τέρων. Η ανίχνευση τέτοιων επαφών είναι ένα σημαντικό και σε μεγάλο βαθμό ανεξερεύνητο θέμα στη σύγχρονη ρομποτική έρευνα. Σε αυτή τη διατριβή, διατυ¬πώνουμε μια ευρύτερη ερώτηση: σε ποια φάση βάδισης βρίσκεται το ρομπότ; Ποςς το σκοπό αυτό, προτείνουμε ένα ολιστικό πλαίσιο βασισμένο σε μη-επιβλεπόμενη μάθηση από δεδομένα ιδιοδεκτικής αίσθησης που αντιμετωπίζει με ακρίβεια και α¬ποτελεσματικότητα αυτό το πρόβλημα. Συγκεκριμένα, ανιχνεύουμε με ακρίβεια μια από τις τρεις φάσεις βάδισης, την Αριστερή Υποστήριξη (LSS), την Διπλή Υποστήριξη (DS) και τη Δεξιά Υποστήριξη (RSS), χρησιμοποιώντας μετρήσεις από κωδικοποιητές, IMU και F/T. Αρχικά, πραγματοποιείται μείωση των διαστάσεων με Ανάλυση Κύριων Στοιχείων (PCA) ή με αυτόματους κωδικοποιητές ώστε να εξαχθούν χρήσιμα χαρα¬κτηριστικά, μια συμπαγής αναπαράσταση και να μειωθεί ο θόρυβος στα δεδομένα. Στη συνέχεια, πραγματοποιείται μια ομαδοποίηση στον χώρο χαμηλών διαστάσεων με Γκαουσιανά Μοντέλα Μίγματος (GMMs). Ως αποτέλεσμα λαμβάνονται τρία πυ¬κνά συμπλέγματα που αντιστοιχούν στις φάσεις της βάδισης. Αυτό σημαίνει ότι η δυναμική της φάσης του βαδίσματος είναι χαμηλής διάστασης το οποίο λειτουργεί ως άλλη μια ένδειξη στο ότι ολόκληρη η διαδικασία της βάδισης είναι χαμηλής διά¬στασης. Επιπλέον, δεδομένου ότι το προτεινόμενο πλαίσιο χρησιμοποιεί μετρήσεις από αισθητήρες που είναι συνήθως διαθέσιμοι στα σημερινά ανθρωποειδή ρομπότ, προσφέρουμε στη ρομποτική κοινότητα το Gait-Phase Estimation Module (GEM), μια ανοικτού κώδικα εφαρμογή σε ROS/Python.
Το SEROW και το GEM έχουν αξιολογηθεί ποσοτικά και ποιοτικά αφορικά με την ακρίβεια και την αποδοτικότητα τους τόσο σε προσομοίωση όσο και σε πραγματικές συνθήκες.Αρχικά , χρησιμοποιήθηκε ένα προσομοιωμένο ρομπότ στο MATLAB και το ανθρωποειδές ρομπότ Valkyrie της NASA στο ROS/Gazebo για να τεκμηριωθούν τα προτεινόμενα σχήματα στο βάδισμα πάνω σε ανομοιόμορφο/ανώμαλο έδαφος. τηη συνέχεια, τα προτεινόμενα σχήματα ενσωματώθηκαν στο α) μικρού μεγέθους ανθρω-ποειδές ρομπότ NAO v4.0 και β) στο πλήρους μεγέθους ανθρωποειδές WALK-MAN v2.0 για περεταίρω πειραματική επικύρωση. Με το NAO, οο SEROW εφαρμόστηκε στο ρομπότ για να παράσχει την απαραίτητη ανατροφοδότηση στον σχεδιασμό της κίνησης και τη σταθεροποίηση του βηματισμού σε πραγματικό χρόνο. Με αυτό το τρόπο επιτεύχθηκε πολυκατευθυντική βάδιση ακόμη και σε εξωτερικά/ανομοιογενή εδάφη. Επιπλέον,το SEROW χρησιμοποιήθηκε στον σχεδιασμό βημάτων για την πλοήγηση και επίσης στο Visual SLAM με το ίδιο ρομπότ. Όσον αφορά το WALK¬MAN v2.0, το SEROW εφαρμόστηκε με δεδομένα κινηματικής, αδρανειακής μονάδας και F/T για να παρέχει ανατροφοδότηση βάσης και CoM σε πραγματικό χρόνο. Στην εκτίμηση λήφθηκε υπόψη και το VO για την διόρθωση της κινηματικής απόκλισης κατά το περπάτημα. Με αυτό το τρόπο διευκολύνεται σημαντικά ο πιθανός σχεδια¬σμός βημάτων. Τλοςς, το GEM χρησιμοποιήθηκε επίσης για την εκτίμηση της φάσης της βάδισης στο δυναμικό περπάτημα του WALK-MAN.
Συνοψίζοντας, σε αυτή τη διατριβή προτείνεται ένας σθεναρός μη-γραμμικός ε¬κτιμητής κατάστασης για το βάδισμα ανθρωποειδών ρομπότ. Παρόλα αυτά, το προ¬τεινόμενο σύστημα μπορεί εύκολα να επεκταθεί και σε άλλους τύπους ρομπότ με πόδια, όπως τα τετράποδα, μιας και διαθέτουν τις ίδιες βασικές αρχές κίνησης.Center of Mass (CoM) estimation realizes a crucial role in legged locomotion. Most walking
pattern generators and real-time gait stabilizers commonly assume that the CoM position
and velocity are available for feedback. In this thesis we present one of the first
3D-CoM state estimators for humanoid robot walking. The proposed estimation scheme
fuses effectively joint encoder, inertial, and feet pressure measurements with an Extended
Kalman Filter (EKF) to accurately estimate the 3D-CoM position, velocity, and external
forces acting on the CoM. Furthermore, it directly considers the presence of uneven terrain
and the body’s angular momentum rate and thus effectively couples the frontal with
the lateral plane dynamics, without relying on feet Force/Torque (F/T) sensing.
Nevertheless, it is common practice to transform the measurements to a world frame
of reference and estimate the CoM with respect to the world frame. Consequently, the
robot’s base and support foot pose are mandatory and need to be co-estimated. To this
end, we extend a well-established in literature floating mass estimator to account for the
support foot dynamics and fuse kinematic-inertial measurements with the Error State
Kalman Filter (ESKF) to appropriately handle the overparametrization of rotations. In
such a way, a cascade state estimation scheme consisting of a base and a CoM estimator
is formed and coined State Estimation RObot Walking (SEROW). Additionally, we employ
Visual Odometry (VO) and/or LIDAR Odometry (LO) measurements to correct the kinematic
drift caused by slippage during walking. Unfortunately, such measurements suffer
from outliers in a dynamic environment, since frequently it is assumed that only the
robot is inmotion and the world around is static. Thus, we introduce the Robust Gaussian
ESKF (RGESKF) to automatically detect and reject outliers without relying on any prior
knowledge on measurement distributions or finely tuned thresholds. Therefore, SEROW
is robustified and is suitable for dynamic human environments. In order to reinforce further
research endeavors, SEROW is released to the robotic community as an open-source
ROS/C++ package.
Up to date control and state estimation schemes readily assume that feet contact status
is known a priori. Contact detection is an important and largely unexplored topic in
contemporary humanoid robotics research. In this thesis, we elaborate on a broader question:
in which gait phase is the robot currently in? To this end, we propose a holistic framework
based on unsupervised learning from proprioceptive sensing that accurately and efficiently
addresses this problem. More specifically, we robustly detect one of the three gaitphases,
namely Left Single Support (LSS), Double Support (DS), and Right Single Support (RSS) utilizing joint encoder, IMU, and F/T measurements. Initially, dimensionality reduction
with Principal Components Analysis (PCA) or autoencoders is performed to extract
useful features, obtain a compact representation, and reduce the noise. Next, clustering
is performed on the low-dimensional latent space with GaussianMixtureModels (GMMs)
and three dense clusters corresponding to the gait-phases are obtained. Interestingly, it is
demonstrated that the gait phase dynamics are low-dimensional which is another indication
pointing towards locomotion being a low dimensional skill. Accordingly, given that
the proposed framework utilizes measurements fromsensors that are commonly available
on humanoids nowadays, we offer the Gait-phase Estimation Module (GEM), an opensource
ROS/Python implementation to the robotic community.
SEROW and GEM have been quantitatively and qualitatively assessed in terms of accuracy
and efficiency both in simulation and under real-world conditions. Initially, a simulated
robot in MATLAB and NASA’s Valkyrie humanoid robot in ROS/Gazebo were employed
to establish the proposed schemes with uneven/rough terrain gaits. Subsequently,
the proposed schemes were integrated on a) the small size NAO humanoid robot v4.0 and
b) the adult size WALK-MAN v2.0 for experimental validation. With NAO, SEROW was implemented
on the robot to provide the necessary feedback for motion planning and realtime
gait stabilization to achieve omni-directional locomotion even on outdoor/uneven
terrains. Additionally, SEROW was used in footstep planning and also in Visual SLAM
with the same robot. Regarding WALK-MAN v2.0, SEROW was executed onboard with
kinematic-inertial and F/T data to provide base and CoM feedback in real-time. Furthermore,
VO has also been considered to correct the kinematic drift while walking and facilitate
possible footstep planning. GEM was also employed to estimate the gait phase in
WALK-MAN’s dynamic gaits.
Summarizing, a robust nonlinear state estimator is proposed for humanoid robot walking.
Nevertheless, this scheme can be readily extended to other type of legged robots such as quadrupeds, since they share the same fundamental principles",Σθεναρή μη γραμμική εκτίμηση κατάστασης ανθρωποειδών ρομπότ,,,,,core
348700928,2020-07-05T00:00:00,"The deep supervised and reinforcement learning paradigms (among others) have the potential to endow interactive multimodal social robots with the ability of acquiring skills autonomously. But it is still not very clear yet how they can be best deployed in real world applications. As a step in this direction, we propose a deep learning-based approach for efficiently training a humanoid robot to play multimodal games---and use the game of `Noughts \& Crosses' with two variants as a case study. Its minimum requirements for learning to perceive and interact are based on a few hundred example images, a few  example multimodal dialogues and physical demonstrations of robot manipulation, and automatic simulations. In addition, we propose novel algorithms for robust visual game tracking and for competitive policy learning with high winning rates, which substantially outperform DQN-based baselines. While an automatic evaluation shows evidence that the proposed approach can be easily extended to new games with competitive robot behaviours, a human evaluation with 130 humans playing with the {\it Pepper} robot confirms that highly accurate visual perception is required for successful game play",A Data-Efficient Deep Learning Approach for Deployable Multimodal Social Robots,https://core.ac.uk/download/348700928.pdf,'Elsevier BV',10.1016/j.neucom.2018.09.104,,core
462870252,2021-07-12T00:00:00,"Humanoid robots could be versatile and intuitive human avatars that operate
remotely in inaccessible places: the robot could reproduce in the remote
location the movements of an operator equipped with a wearable motion capture
device while sending visual feedback to the operator. While substantial
progress has been made on transferring (""retargeting"") human motions to
humanoid robots, a major problem preventing the deployment of such systems in
real applications is the presence of communication delays between the human
input and the feedback from the robot: even a few hundred milliseconds of delay
can irreversibly disturb the operator, let alone a few seconds. To overcome
these delays, we introduce a system in which a humanoid robot executes commands
before it actually receives them, so that the visual feedback appears to be
synchronized to the operator, whereas the robot executed the commands in the
past. To do so, the robot continuously predicts future commands by querying a
machine learning model that is trained on past trajectories and conditioned on
the last received commands. In our experiments, an operator was able to
successfully control a humanoid robot (32 degrees of freedom) with stochastic
delays up to 2 seconds in several whole-body manipulation tasks, including
reaching different targets, picking up, and placing a box at distinct
locations.Comment: Video: https://www.youtube.com/watch?v=N3u4ot3aIy",Prescient teleoperation of humanoid robots,http://arxiv.org/abs/2107.01281,,,,core
323145851,2020-03-01T00:00:00,"The use of Small Unmanned Aerial Vehicles (sUAVs) has grown exponentially owing to an increasing number of autonomous capabilities. Automated functions include the return to home at critical energy levels, collision avoidance, take-off and landing, and target tracking. However, sUAVs applications in real-world and time-critical scenarios, such as Search and Rescue (SAR) is still limited. In SAR applications, the overarching aim of autonomous sUAV navigation is the quick localisation, identification and quantification of victims to prioritise emergency response in affected zones. Traditionally, sUAV pilots are exposed to prolonged use of visual systems to interact with the environment, which causes fatigue and sensory overloads. Nevertheless, the search for victims onboard a sUAV is challenging because of noise in the data, low image resolution, illumination conditions, and partial (or full) occlusion between the victims and surrounding structures. This paper presents an autonomous Sequential Decision Process (SDP) for sUAV navigation that incorporates target detection uncertainty from vision-based cameras. The SDP is modelled as a Partially Observable Markov Decision Process (POMDP) and solved online using the Adaptive Belief Tree (ABT) algorithm. In particular, a detailed model of target detection uncertainty from deep learning-based models is shown. The presented formulation is tested under Software in the Loop (SITL) through Gazebo, Robot Operating System (ROS), and PX4 firmware. A Hardware in the Loop (HITL) implementation is also presented using an Intel Myriad Vision Processing Unit (VPU) device and ROS. Tests are conducted in a simulated SAR GPS-denied scenario, aimed to find a person at different levels of location and pose uncertainty.</p",Autonomous UAV navigation for active perception of targets in uncertain and cluttered environments,,'Institute of Electrical and Electronics Engineers (IEEE)',10.1109/AERO47225.2020.9172808,,core
328809510,2020-03-01T00:00:00,"We introduce Probabilistic Object Detection, the task of detecting objects in images and accurately quantifying the spatial and semantic uncertainties of the detections. Given the lack of methods capable of assessing such probabilistic object detections, we present the new Probability-based Detection Quality measure (PDQ). Unlike AP-based measures, PDQ has no arbitrary thresholds and rewards spatial and label quality, and foreground/background separation quality while explicitly penalising false positive and false negative detections. We contrast PDQ with existing mAP and moLRP measures by evaluating state-of-the-art detectors and a Bayesian object detector based on Monte Carlo Dropout. Our experiments indicate that conventional object detectors tend to be spatially overconfident and thus perform poorly on the task of probabilistic object detection. Our paper aims to encourage the development of new object detection approaches that provide detections with accurately estimated spatial and label uncertainties and are of critical importance for deployment on robots and embodied AI systems in the real world",Probabilistic object detection:Definition and evaluation,,'Institute of Electrical and Electronics Engineers (IEEE)',10.1109/WACV45572.2020.9093599,,core
359967943,2020-01-01T00:00:00,"Data-driven modelling and synthesis of motion is an active research area with applications that include animation, games, and social robotics. This paper introduces a new class of probabilistic, generative, and controllable motion-data models based on normalising flows. Models of this kind can describe highly complex distributions, yet can be trained efficiently using exact maximum likelihood, unlike GANs or VAEs. Our proposed model is autoregressive and uses LSTMs to enable arbitrarily long time-dependencies. Importantly, is is also causal, meaning that each pose in the output sequence is generated without access to poses or control inputs from future time steps; this absence of algorithmic latency is important for interactive applications with real-time motion control. The approach can in principle be applied to any type of motion since it does not make restrictive, task-specific assumptions regarding the motion or the character morphology. We evaluate the models on motion-capture datasets of human and quadruped locomotion. Objective and subjective results show that randomly-sampled motion from the proposed method outperforms task-agnostic baselines and attains a motion quality close to recorded motion capture.QC 20200929VR proj. 2018-05409 (StyleBot)SSF no. RIT15-0107 (EACare)Wallenberg AI, Autonomous Systems and Software Program (WASP",MoGlow: Probabilistic and controllable motion synthesis using normalising flows,,'Association for Computing Machinery (ACM)',10.1145/3414685.3417836,,core
337304742,2020-10-31T00:00:00,"The results reported here represent the first stage in the development of a full-featured laboratory system aimed at studying machine learning algorithms. The relevance of the current work is predetermined by the lack of network small-size mobile robots and appropriate control software that would make it possible to conduct field experiments in real time. This paper reports the selection of network data transmission technology for managing mobile robots in real time. Based on the chosen data transmission protocol, a complete stack of technologies of the network model of a multi-agent system of mobile robots has been proposed. This has made it possible to build a network model of the system that visualizes and investigates machine learning algorithms. In accordance with the requirements set by the OSI network model for constructing such systems, the model includes the following levels:1) the lower level of data collection and controlling elements – mobile robots;2) the top level of the model includes a user interface server and a business logic support server.Based on the built diagram of the protocol stack and the network model, the software and hardware implementation of the obtained results has been carried out. This paper employed the JavaScript library React with a SPA technology (Single Page Application), a Virtual DOM technology (Document Object Model), stored in the device's RAM and synchronized with the actual DOM. That has made it possible to simplify the process of control over the clients and reduce network traffic.The model provides the opportunity to:1) manage the prototypes of robot clients in real time;2) reduce the use of network traffic, compared to other data transmission technologies;3) reduce the load on the CPU processors of robots and servers; 4) virtually simulate an experiment;5) investigate the implementation of machine learning algorithmsПредставленные результаты работы являются первым этапом разработки полнофункциональной лабораторной системы исследования алгоритмов машинного обучения. Актуальность работы обусловлена отсутствием сетевых малогабаритных мобильных роботов и соответствующего управляющего программного обеспечения, что позволило бы проводить натурные эксперименты в реальном времени. В работе осуществлен подбор сетевой технологии передачи данных для управления мобильными роботами в реальном времени. На основе выбранного протокола передачи данных предложен полный стек технологий сетевой модели мультиагентной системы мобильных роботов. Это позволило построить сетевую модель системы визуализации и исследования алгоритмов машинного обучения. В соответствии с требованиями сетевой модели OSI по построению подобных систем, модель включает в себя следующие уровни:1. Нижний уровень сбора данных и исполнительных механизмов – мобильные работы;2. Верхний уровень модели состоит из сервера пользовательского интерфейса и сервера поддержки бизнес-логики.Основываясь на построенной диаграмме стека протоколов и сетевой модели, осуществлена программно-аппаратная реализация полученных результатов. В работе использованы JavaScript библиотека React с технологией SPA (Single Page Application), технология Virtual DOM (Document Object Model), хранящаяся в оперативной памяти устройства и синхронизируемая с реальным DOM. Это позволило упростить процесс управления клиентами и уменьшить сетевой трафик.Модель позволяет:1) управлять прототипами роботов-клиентов в реальном времени;2) уменшить использование сетевого трафика, в сравнении с другими технологиями передачи данных;3) уменьшить нагрузку на центральные процессоры роботов и серверов;4) выполнять виртуальную симуляцию эксперимента;5) исследовать выполнение алгоритмов машинного обученияПредставлені результати роботи є першим етапом розробки повнофункціональної лабораторної системи дослідження алгоритмів машинного навчання. Актуальність роботи зумовлена відсутністю мережевих малогабаритних мобільних роботів та відповідного керуючого програмного забезпечення, що дозволило б проводити натурні експерименти в реальному часі. В роботі здійснено підбір мережевої технології передачі даних для керування мобільними роботами в реальному часі. На основі обраного протоколу передачі даних запропоновано повний стек технологій мережевої моделі мультиагентної системи мобільних роботів. Це дозволило побудувати мережеву модель системи візуалізації та дослідження алгоритмів машинного навчання. Відповідно до вимог мережевої моделі OSI щодо побудови подібних систем, модель включає в себе наступні рівні:1) нижній рівень збору даних та виконавчих механізмів – мобільні роботи;2) верхній рівень моделі – складається з серверу користувацького інтерфейсу та серверу підтримки бізнес-логіки.Базуючись на побудованих діаграмі стеку протоколів та мережевій моделі здійснена програмно-апаратна реалізація отриманих результатів. У роботі використано JavaScript бібліотека React з технологією SPA (Single Page Application), технологію Virtual DOM (Document Object Model), що зберігається в оперативній пам’яті пристрою і синхронізується з реальним DOM. Це дозволило спростити процес керування клієнтами та зменшити мережевий трафік.Модель надає можливість: 1) керувати прототипами роботів-клієнтів в реальному часі;2) зменшити використання мережевого трафіку, в порівнянні з іншими технологіями передачі даних;3) зменшити навантаження на центральні процесори роботів та серверів;4) виконувати віртуальну симуляцію експерименту;5) досліджувати виконання алгоритмів машинного навчання",Побудова моделі мережевої взаємодії складових мультиагентної системи мобільних роботів,https://core.ac.uk/download/337304742.pdf,'Private Company Technology Center',10.15587/1729-4061.2020.213989,,core
479917899,2021-06-22T09:30:10,"The subject matter of this research paper discusses the nature of the relationship between law and artificial intelligence. The paper presents the nature of this relationship according to a critical study comparing the French legislative reality with its Qatari counterpart, in light of the European civil law rules on robotics of 2017, and the Comprehensive European industrial policy on artificial intelligence and robotics of 2019. The study is carried out in two main axes that formed the two main sections of this study. The first path dealt with the protection of  artificial intelligence , by protecting the results of this intelligence at various levels; in its concrete form  Robot  and intangible  Logarithms and Software.  The second path addressed the protection of the  society in its various components  in terms of ensuring that our basic rights and freedoms are not violated, in addition to determining the nature of legal liability resulting from the illegal use of this intelligence. The research concluded that there is a clear deficiency of legislation - albeit with some variation - in the nature of the legal treatment of many points raised by artificial intelligence in both the French and the Qatari civil legislations, whether in relation to the legal rules concerning the protection of intellectual property rights, especially those related to the protection of intellectual property rights of artificial intelligence, or to the recognition of the legal personality of the tangible form of artificial intelligence represented by robotics. On the other hand, the research stressed the need to codify the process of using artificial intelligence in order to ensure respect for the latter and those responsible for the basic rights and freedoms of individuals. The research also showed that, in the absence of the cognitive aspect of artificial intelligence, the main pillar of the idea of its legal liability in both the French and Qatari legislations, the error remains a human error of unconventional concept, which requires both legislators to reconsider the legal rules governing their notion of legal liability, consistent with the specificity and nature of this liability. Therefore, the research recommended the need to ensure the ethics of using artificial intelligence in both legislations, through the development of a code of ethical work outlining the various aspects of its work, as well as the rights and duties ensuing, in order to ensure the re-harmonization of various related legal texts within the spirit and philosophy of the relationship between the artificial intelligence and the law, and the nature of the legal system for both the French and Qatari legislators",الذكاء الاصطناعي والقانون-دراسة نقدية مقارنة في التشريعين المدني الفرنسي و القطري -في ضوء القواعد الأوربية في القانون المدني للإنسآلة لعام 2017 والسياسة الصناعية الأوربية للذكاء الاصطناعي والإنسآلات لعام 2019,https://core.ac.uk/download/479917899.pdf,Digital Commons @ BAU,,,core
14703831,2052-03-02T00:00:00,"La tesi affronta la progettazione e la sintesi di un controllo robusto per un amplificatore di forza ad elevate prestazioni denominato Body Extender.
Il Body Extender è un esoscheletro corpo intero a 22 gradi di libertà progettato e costruito dal laboratorio PERCRO della Scuola Superiore Sant'Anna di Pisa. Il sistema è concepito per essere indossato da un operatore umano e muoversi in coerenza con i suoi movimenti, sgravando i pesi e le inerzie della sua struttura e dei carichi trasportati. Il livello di compensazione del carico viene inoltre ridotto a una percentuale arbitraria, in modo da consentire una manipolazione naturale per l'operatore. Per tali motivi nella progettazione del controllo si è dovuto tenere conto della presenza dell'uomo che causa notevoli variazioni dei parametri (massa e rigidezza), della elasticità della trasmissione, e delle differenti situazioni di esercizio della macchina, quali variazioni di carico e interferenza con l'ambiente. Il funzionamento della macchina deve garantire una adattività a tutti i suddetti effetti.
Il controllo realizza un comportamento dell'esoscheletro pari ad un amplificatore di forze; nel caso in cui il movimento dell'operatore sia libero risulta invece trasparente per l'utilizzatore. La struttura del controllo presenta due anelli principali: uno più interno che si occupa dell'inseguimento delle traiettorie, implementato con un controllo a dinamica inversa adattivo; mentre l'anello esterno genera il riferimento simulando la pura massa virtuale agli organi di presa. L'anello interno di controllo compensa inoltre l'elasticità concentrata ai giunti annullandone gli effetti dinamici nell'intorno della traiettoria. L'anello esterno utilizza la forza misurata dal sensore per generare un riferimento in accelerazione che viene opportunamente integrato per determinare i riferimenti di velocità e posizione. Un ultimo modulo si occupa della stima delle forze esterne per la corretta amplificazione di forza e per il funzionamento del controllo in posizione. Considerando la natura non stazionaria del modello da identificare, la stima dei carichi viene effettuata con un operatore a media mobile basato su un algoritmo di regressione non lineare.
L'analisi della stabilità del sistema è stata effettuata tramite il luogo delle radici, e quindi verificata nei parametri incerti con il software Mathematica. E' stata inoltre programmata un interfaccia in python per il log e la visualizzazione realtime delle variabili di stato del sistama. L'implementazione del controllo progettata in ambiente Simulink di MATLAB usa il real-time Workshop opportunamente configurato, e può essere eseguire gli schemi direttamente nel robot. Sono state effettuate prove sperimentali su un modello di ridotta complessità per verificare le proprietà dinamiche degli algoritmi sviluppati. Sono infine stati effettuati test sulla struttura completa. I risultati delle prove sperimentali hanno evidenziato un accordo con l'analisi teorica realizzata sia in termini di stabilità sia di prestazione",Controllo robusto di forza per una struttura robotica articolata di tipo Body Extender,https://core.ac.uk/download/14703831.pdf,'Pisa University Press',,,core
385962751,2020-04-29T00:00:00,"Service robotic systems have many practical applications in areas like assisted living for older-adults, security and surveillance. As a remotely controlled system, they must be able to move through their surrounding environment, communicate the sensed information with the&nbsp;controller or even with each other. Through network connectivity, these robots can also be controlled semi-autonomously by a user while viewing the surrounding scenes through augmented reality using an onboard camera. The objective of this project is to explore the notion of AI for such remote control applications. The service robot can be deployed in a known indoor living environment containing various stationery and moveable items. To demonstrate the effectiveness of this proposal, a camera is moved in a known environment where the graphic objects are overlaid on the live video stream in order to assist the remote navigation.&nbsp;
&nbsp;
&nbsp;
&nbsp",Study of AI-Based System for Remote Navigation of Service Robotics,,SFU Library,,,core
343943298,2020-01-01T08:00:00,"As we are entering the big data era, data are becoming high-volume, complex, and heterogeneous. Nowadays, almost every field and sector of the modern society is being impacted by big data, ranging from energy system to health care, and from business to government. The excessive amount of data contains potential and highly useful values. Extracting values from these data and utilize it to support the decision-making process is of paramount importance to improve productivity in business and enhance the well-being of our society. However, data-driven decision-making also arises with many challenges, such as feature selection, data fusion, and real-time decision-making.
To overcome these challenges, this dissertation will develop deep learning algorithms and frameworks for data-driven decision-making. This work is composed of three major parts: automatic feature extraction, heterogeneous data fusion, and deep reinforcement learning (DRL) based data-driven frameworks. This dissertation will first investigate the automatic feature extraction to avoid the burden of the “feature engineering” process. To make sure the feature extractor can be deployed to different scenarios, domain adaptive feature extraction will be studied. This dissertation will then examine the heterogeneous data fusion problem where feature refinement, normalization, transformation, and fusion will be investigated. With the automatic feature extraction and heterogeneous data fusion, this dissertation will develop data-driven decision-making frameworks for real-world applications, including robot-assisted pedestrian regulation and real-time electric vehicle (EV) charging management. Numerous experiments will be conducted to verify the effectiveness of the proposed frameworks",DEEP LEARNING FOR DATA-DRIVEN DECISION-MAKING,https://core.ac.uk/download/343943298.pdf,DigitalCommons@URI,,,core
334915222,2020-02-22T00:00:00,"Deep reinforcement learning (RL) uses model-free techniques to optimize
task-specific control policies. Despite having emerged as a promising approach
for complex problems, RL is still hard to use reliably for real-world
applications. Apart from challenges such as precise reward function tuning,
inaccurate sensing and actuation, and non-deterministic response, existing RL
methods do not guarantee behavior within required safety constraints that are
crucial for real robot scenarios. In this regard, we introduce guided
constrained policy optimization (GCPO), an RL framework based upon our
implementation of constrained proximal policy optimization (CPPO) for tracking
base velocity commands while following the defined constraints. We also
introduce schemes which encourage state recovery into constrained regions in
case of constraint violations. We present experimental results of our training
method and test it on the real ANYmal quadruped robot. We compare our approach
against the unconstrained RL method and show that guided constrained RL offers
faster convergence close to the desired optimum resulting in an optimal, yet
physically feasible, robotic control behavior without the need for precise
reward function tuning.Comment: 8 pages, 8 figures, 5 tables, 1 algorithm, accepted to IEEE Robotics
  and Automation Letters (RA-L), January 2020 with presentation at
  International Conference on Robotics and Automation (ICRA) 202","Guided Constrained Policy Optimization for Dynamic Quadrupedal Robot
  Locomotion",http://arxiv.org/abs/2002.09676,,,,core
351125143,2020-11-04T08:00:00,"Deep neural networks (DNNs) have achieved significant success in many applications, such as computer vision, natural language processing, robots, and self-driving cars. With the growing demand for more complex real-world applications, more complicated neural networks have been proposed. However, high capacity models result in two major problems: long training times and high inference delays, making the neural networks hard to train and infeasible to deploy for time-intensive applications or resource-limited devices. In this work, we propose multiple techniques to accelerate the training and inference speed as well as model performance
The first technique we study is model parallelization on generative adversarial networks (GANs). Multiple orthogonal generators with shared memory are employed to capture the whole data distribution space. This method can not only improve the model performance but also alleviate the mode collapse problem that is common in GANs. The second technique we investigate is the automatic network pruning. To reduce the floating-point operations (FLOPs) to a proper level without compromising accuracy, we propose a better generalized and easy-to-use pruning method, which prunes the network through optimizing a set of trainable auxiliary parameters instead of original weights. Weakly coupled gradient update rules are proposed to keep consistency with pruning tasks. The third technique is to remove the redundancy of the complicated model based on the need of applications. We treat the chemical reaction prediction as a translation problem and apply a low capacity neuron translation model to this problem. The fourth technique is to combine distillation with Differentiable Architecture Search to stabilize and improve the searching procedure. Intermediate results as well as the output logits are transferred from the teacher network to the student network. For the application of the speedup technique, we introduce neural network pruning into Materials Genomics. We propose attention based AutoPrune for the kernel pruning of a continuous filtering neural network for molecular property prediction and achieves better performance and more compact size",Speedup Techniques for Deep Neural Networks,https://core.ac.uk/download/351125143.pdf,OpenCommons@UConn,,,core
388606178,2021-03-29T00:00:00,"Creating robots that can perform general-purpose service tasks in a
human-populated environment has been a longstanding grand challenge for AI and
Robotics research. One particularly valuable skill that is relevant to a wide
variety of tasks is the ability to locate and retrieve objects upon request.
This paper models this skill as a Scavenger Hunt (SH) game, which we formulate
as a variation of the NP-hard stochastic traveling purchaser problem. In this
problem, the goal is to find a set of objects as quickly as possible, given
probability distributions of where they may be found. We investigate the
performance of several solution algorithms for the SH problem, both in
simulation and on a real mobile robot. We use Reinforcement Learning (RL) to
train an agent to plan a minimal cost path, and show that the RL agent can
outperform a range of heuristic algorithms, achieving near optimal performance.
In order to stimulate research on this problem, we introduce a publicly
available software stack and associated website that enable users to upload
scavenger hunts which robots can download, perform, and learn from to
continually improve their performance on future hunts.Comment: 6 pages + references + Appendi",A Scavenger Hunt for Service Robots,http://arxiv.org/abs/2103.05225,,,,core
389180549,2021-01-13T00:00:00,"Research background: Rapid technical progress supports the development of advanced technologies, which increases the requirements of customers who emphasize the quality of products. We can expect revolutionary changes in every business area, but especially in production and logistics, as they will be most subject to the implementation of artificial intelligence, robotics, augmented reality, 3D printing and other technologies related to Industry 4.0 as a representative of The Fourth Industrial Revolution. Based on these facts, every enterprise is looking for new ways to reduce costs not only in production but also logistics, speed up and simplify the production and logistics process. One of the effective ways to achieve this is to use and implement progressive technologies in logistics. The aim of the paper was to examine progressive technologies in logistics that are used in Slovak automotive enterprises and then analyse selected technologies. The paper is divided into two parts. The first part of the paper is devoted to a literary view of progressive technologies in logistics. The second part provides an insight into a comparative analysis of selected technologies in logistics of 65 Slovak automotive enterprises. The paper used several research methods, which include: literature search, system analysis, comparative analysis, induction, deduction and visualization method. Progressive technologies in today’s conditions are a prerequisite for effective results in every enterprise in the form of economy and efficiency. The use and continuous improvement of progressive technologies in automotive enterprises must be one of the necessary activities in the management of the enterprise in order to maintain competitiveness in the market.
Purpose of the article: The aim of the paper was to examine progressive technologies in logistics that are used in Slovak automotive enterprises and then analyse selected technologies. The paper is divided into two parts. The first part of the paper is devoted to a literary view of progressive technologies in logistics. The second part provides an insight into a comparative analysis of selected technologies in logistics of 65 Slovak automobile enterprises.
Methods: The paper used several research methods, which include: literature search, system analysis, comparative analysis, induction, deduction and visualization method.
Findings & Value added: Progressive technologies in today’s conditions are a prerequisite for effective results in every enterprise in the form of economy and efficiency. The use and continuous improvement of advanced technologies in automotive enterprises must be one of the necessary activities in the management of the enterprise in order to maintain competitiveness in the market",The Use of Progressive Technologies in Logistics of Slovak Automotive Enterprises,,'EDP Sciences',10.1051/shsconf/20219204022,,core
355868684,2020-01-01T00:00:00,"Data-driven modelling and synthesis of motion is an active research area with applications that include animation, games, and social robotics. This paper introduces a new class of probabilistic, generative, and controllable motion-data models based on normalising flows. Models of this kind can describe highly complex distributions, yet can be trained efficiently using exact maximum likelihood, unlike GANs or VAEs. Our proposed model is autoregressive and uses LSTMs to enable arbitrarily long time-dependencies. Importantly, is is also causal, meaning that each pose in the output sequence is generated without access to poses or control inputs from future time steps; this absence of algorithmic latency is important for interactive applications with real-time motion control. The approach can in principle be applied to any type of motion since it does not make restrictive, task-specific assumptions regarding the motion or the character morphology. We evaluate the models on motion-capture datasets of human and quadruped locomotion. Objective and subjective results show that randomly-sampled motion from the proposed method outperforms task-agnostic baselines and attains a motion quality close to recorded motion capture.QC 20200929VR proj. 2018-05409 (StyleBot)SSF no. RIT15-0107 (EACare)Wallenberg AI, Autonomous Systems and Software Program (WASP",MoGlow: Probabilistic and controllable motion synthesis using normalising flows,,"New York, NY, USA",,,core
372703950,2090-07-16T00:00:00,"The work focuses on an obstacle detection framework for an autonomous robotics platform. State of the art classical and Deep Learning existing frameworks and approaches are first explored. A probabilistic framework that includes 3D reconstruction techniques, stereo matching and UV-disparity projection is designed. The system is integrated with appearance cues, edges related metrics and ground plane homography estimation to robustly segment obstacles in a wide range of scenarios. A synthetic and real datasets are generated to test the performances and accuracy of the proposed algorithms. The framework is deployed on a autonomous mobile robot where real time performances are achieved, using CUDA computing capabilities of a NVIDIA Xavier embedded platform",Small Obstacles Detection for Autonomous Navigation,,'Pisa University Press',,,core
479136994,2021-01-01T08:00:00,"Mobile robots are already in use for mapping, agriculture, entertainment, and the delivery of goods and people. As robotic systems continue to become more affordable, large numbers of mobile robots may be deployed concurrently to accomplish tasks faster and more efficiently. Practical deployments of very large teams will require scalable algorithms to enable the distributed cooperation of autonomous agents. This thesis focuses on the three main algorithmic obstacles to the scalability of robot teams: coordination, control, and communication. To address these challenges, we design graph-based abstractions that allow us to apply Graph Neural Networks (GNNs).First, a team of robots must continually coordinate to divide up mission requirements among all agents. We focus on the case studies of exploration and coverage to develop a spatial GNN controller that can coordinate a team of dozens of agents as they visit thousands of landmarks. A routing problem of this size is intractable for existing optimization-based approaches. Second, a robot in a team must be able to execute the trajectory that will accomplish its given sub-task. In large teams with high densities of robots, planning and execution of safe, collision-free trajectories requires the joint optimization over all agent trajectories, which may be impractical in large teams. We present two approaches to scalable control: a) a controller for flocking that uses delayed communication formalized via a GNN; and b) an inverse optimal planning method that learns from real air traffic data. Third, robot teams may need to operate in harsh environments without existing communication infrastructure, requiring the formation of ad-hoc networks to exchange information. Many algorithms for control of multi-robot teams operate under the assumption that low-latency, global state information necessary to coordinate agent actions can readily be disseminated among the team. Our approach leverages GNNs to control the connectivity within the ad-hoc network and to provide the data distribution infrastructure necessary for countless multi-robot algorithms. Finally, this thesis develops a framework for distributed learning to be used when centralized information is unavailable during training. Our approach allows robots to train controllers independently and then share their experiences by composing multiple models represented in a Reproducing Kernel Hilbert Space",Scalable Learning In Distributed Robot Teams,https://core.ac.uk/download/479136994.pdf,ScholarlyCommons,,,core
388916678,2021-01-01T00:00:00,"3D object recognition has been a cutting-edge research topic since the popularization of depth cameras. These cameras enhance the perception of the environment and so are particularly suitable for autonomous robot navigation applications. Advanced deep learning approaches for 3D object recognition are based on complex algorithms and demand powerful hardware resources. However, autonomous robots and powered wheelchairs have limited resources, which affects the implementation of these algorithms for real-time performance. We propose to use instead a 3D voxel-based extension of the 2D histogram of oriented gradients (3DVHOG) as a handcrafted object descriptor for 3D object recognition in combination with a pose normalization method for rotational invariance and a supervised object classifier. The experimental goal is to reduce the overall complexity and the system hardware requirements, and thus enable a feasible real-time hardware implementation. This article compares the 3DVHOG object recognition rates with those of other 3D recognition approaches, using the ModelNet10 object data set as a reference. We analyze the recognition accuracy for 3DVHOG using a variety of voxel grid selections, different numbers of neurons (N-h ) in the single hidden layer feedforward neural network, and feature dimensionality reduction using principal component analysis. The experimental results show that the 3DVHOG descriptor achieves a recognition accuracy of 84.91% with a total processing time of 21.4 ms. Despite the lower recognition accuracy, this is close to the current state-of-the-art approaches for deep learning while enabling real-time performance",Processing chain for 3D histogram of gradients based real-time object recognition,,'SAGE Publications',10.1177/1729881420978363,,core
428288418,2021-08-01T07:00:00,"This study proposes a digital twin (DT) model for a two-arm collaborative robot that can be deployed to simulate human arm motions using the reverse engineering process. A collaborative robot named ABB Yumi – IRB14000 was considered for this study. The purpose of the experiment was to find the best version of the digital twin model by applying translation and rotation constraints in every part of the CAD model of the robot. After adding features to the robot part files, Virtual Reality Modeling Language (VRML) format files were being created to assemble it in 3D world Editor for DT formation and a grid layout was created that contained the control panel of the collaborative model digital twin to connect it with the real world. Finally, a cyber-physical system (CPS) interface was built to replicate human motion. Deep reinforcement learning will be implemented using these two models for human motion simulation",Digital twin model of two-arm collaborative robot for human arms motion simulation using reverse engineering,,UTC Scholar,,,core
401534877,2021-02-01T00:00:00,"Sensory feedback is essential for the control of soft robotic systems and to enable deployment in a variety of different tasks. Proprioception refers to sensing the robot’s own state and is of crucial importance in order to deploy soft robotic systems outside of laboratory environments, i.e. where no external sensing, such as motion capture systems, is available. A vision-based sensing approach for a soft robotic arm made from fabric is presented, leveraging the high-resolution sensory feedback provided by cameras. No mechanical interaction between the sensor and the soft structure is required and consequently the compliance of the soft system is preserved. The integration of a camera into an inflatable, fabric-based bellow actuator is discussed. Three actuators, each featuring an integrated camera, are used to control the spherical robotic arm and simultaneously provide sensory feedback of the two rotational degrees of freedom. A convolutional neural network architecture predicts the two angles describing the robot’s orientation from the camera images. Ground truth data is provided by a motion capture system during the training phase of the supervised learning approach and its evaluation thereafter. The camera-based sensing approach is able to provide estimates of the orientation in real-time with an accuracy of about one degree. The reliability of the sensing approach is demonstrated by using the sensory feedback to control the orientation of the robotic arm in closed-loop.ISSN:2296-914",A Vision-Based Sensing Approach for a Spherical Soft Robotic Arm,,'Frontiers Media SA',10.3389/frobt.2021.630935,,core
436601343,2021-06-18T00:00:00,"This paper addresses the problem of policy selection in domains with abundant
logged data, but with a very restricted interaction budget. Solving this
problem would enable safe evaluation and deployment of offline reinforcement
learning policies in industry, robotics, and healthcare domain among others.
Several off-policy evaluation (OPE) techniques have been proposed to assess the
value of policies using only logged data. However, there is still a big gap
between the evaluation by OPE and the full online evaluation in the real
environment. To reduce this gap, we introduce a novel \emph{active offline
policy selection} problem formulation, which combined logged data and limited
online interactions to identify the best policy. We rely on the advances in OPE
to warm start the evaluation. We build upon Bayesian optimization to
iteratively decide which policies to evaluate in order to utilize the limited
environment interactions wisely. Many candidate policies could be proposed,
thus, we focus on making our approach scalable and introduce a kernel function
to model similarity between policies. We use several benchmark environments to
show that the proposed approach improves upon state-of-the-art OPE estimates
and fully online policy evaluation with limited budget. Additionally, we show
that each component of the proposed method is important, it works well with
various number and quality of OPE estimates and even with a large number of
candidate policies",Active Offline Policy Selection,http://arxiv.org/abs/2106.10251,,,,core
337296398,2021-03-07T00:00:00,"Learning from Hallucination (LfH) is a recent machine learning paradigm for
autonomous navigation, which uses training data collected in completely safe
environments and adds numerous imaginary obstacles to make the environment
densely constrained, to learn navigation planners that produce feasible
navigation even in highly constrained (more dangerous) spaces. However, LfH
requires hallucinating the robot perception during deployment to match with the
hallucinated training data, which creates a need for sometimes-infeasible prior
knowledge and tends to generate very conservative planning. In this work, we
propose a new LfH paradigm that does not require runtime hallucination -- a
feature we call ""sober deployment"" -- and can therefore adapt to more realistic
navigation scenarios. This novel Hallucinated Learning and Sober Deployment
(HLSD) paradigm is tested in a benchmark testbed of 300 simulated navigation
environments with a wide range of difficulty levels, and in the real-world. In
most cases, HLSD outperforms both the original LfH method and a classical
navigation planner","Agile Robot Navigation through Hallucinated Learning and Sober
  Deployment",http://arxiv.org/abs/2010.08098,,,,core
462872231,2021-07-14T00:00:00,"In this work we present a general, two-stage reinforcement learning approach
for going from a single demonstration trajectory to a robust policy that can be
deployed on hardware without any additional training. The demonstration is used
in the first stage as a starting point to facilitate initial exploration. In
the second stage, the relevant task reward is optimized directly and a policy
robust to environment uncertainties is computed. We demonstrate and examine in
detail performance and robustness of our approach on highly dynamic hopping and
bounding tasks on a real quadruped robot","Model-free Reinforcement Learning for Robust Locomotion Using Trajectory
  Optimization for Exploration",http://arxiv.org/abs/2107.06629,,,,core
480185838,2021-11-03T00:00:00,"Vision-based control has found a key place in the research to tackle the
requirement of the state feedback when controlling a continuum robot under
physical sensing limitations. Traditional visual servoing requires feature
extraction and tracking while the imaging device captures the images, which
limits the controller's efficiency. We hypothesize that employing deep learning
models and implementing direct visual servoing can effectively resolve the
issue by eliminating the tracking requirement and controlling the continuum
robot without requiring an exact system model. In this paper, we control a
single-section tendon-driven continuum robot utilizing a modified VGG-16 deep
learning network and an eye-in-hand direct visual servoing approach. The
proposed algorithm is first developed in Blender using only one input image of
the target and then implemented on a real robot. The convergence and accuracy
of the results in normal, shadowed, and occluded scenes reflected by the sum of
absolute difference between the normalized target and captured images prove the
effectiveness and robustness of the proposed controller.Comment: 7 pages, 10 figures, 1 table, Submitted to 2022 IEEE International
  Conference on Soft Robotics (RoboSoft",Deep Direct Visual Servoing of Tendon-Driven Continuum Robots,http://arxiv.org/abs/2111.02580,,,,core
402899368,2021-02-05T03:05:15,"© 2013 IEEE. We present a novel implementation of a Rock-Paper-Scissors (RPS) game interaction with a social robot. The framework is tailored to be computationally lightweight, as well as entertaining and visually appealing through collaboration with designers and animators. The fundamental gesture recognition pipeline employs a Leap motion device and two separate machine learning architectures to evaluate kinematic hand data on-the-fly. The first architecture is used to recognize and segment human motion activity in order to initialize the RPS play, and the second architecture is used to classify hand gestures into rock, paper or scissors. The employed tabletop robot interacts in the RPS play through unique animated gestural movements and vocalizations designed by animators which communicate the robot's choices as well as cognitive reflection on winning, losing and draw states. Performance of both learning architectures is carefully evaluated with respect to accuracy, reliability and run time performance under different feature and classifier types. Moreover, we assess our system during an interactive RPS play between robot and human. Experimental results show that the proposed system is robust to user variations and play style in real environment conditions. As such, it offers a powerful application for the subsequent exploration of social human-machine interaction",Developing a Lightweight Rock-Paper-Scissors Framework for Human-Robot Collaborative Gaming,https://opus.lib.uts.edu.au/bitstream/10453/145863/2/DevelopingaLightweightRockPaperScissorsFrameworkforHumanRobotCollaborativeGaming.pdf,'Institute of Electrical and Electronics Engineers (IEEE)',10.1109/ACCESS.2020.3033550,"[{'title': 'IEEE Access', 'identifiers': ['issn:2169-3536', '2169-3536']}]",core
291793288,10000-01-01,"Recent advances in the broad field of artificial intelligence (AI) has brought
much excitement and many expectations.
However, there is a strong need to understand
 intelligence, and through understanding it we can help achieve true
machine intelligence, one that is not only able to complete certain difficult tasks but also reason about the world. To study intelligence, we look at ourselves and especially at infants. At a very young age, we can consciously and easily perform tasks that involve understanding of both languages and vision, two of the channels through which we acquire most of our information from the external world.                  
How do we do so? How do children learn their first language?
 In our lab, we believe intelligence and learning should be interactive.
 We learn from interaction with the real world through our five senses.
We also believe a massive number of well-defined labels does not exist for
children. In order to study this idea of unlabeled data and learning through
interaction, for this thesis we implemented a system that enabled a human robot to associate 
 visual information with speech information and to learn
to describe a new object with vocabularies acquired during training. Several
machine learning models were implemented on the iCub humanoid platform. Specifically,
 Gaussian Mixture Models and K-means were implemented for
the vision part of the experiment, and Hidden Markov Models were used for
the speech.U of I Onlyundergraduate senior thesis not recommended for open acces",Language acquisition and object recognition with Bert,,,,,core
479017238,2021-09-01T00:00:00,"Socially assistive robots (SAR) hold significant potential to assist older adults and people with dementia in human engagement and clinical contexts by supporting mental health and independence at home. While SAR research has recently experienced prolific growth, long-term trust, clinical translation and patient benefit remain immature. Affective human-robot interactions are unresolved and the deployment of robots with conversational abilities is fundamental for robustness and humanrobot engagement. In this paper, we review the state of the art within the past two decades, design trends, and current applications of conversational affective SAR for ageing and dementia support. A horizon scanning of AI voice technology for healthcare, including ubiquitous smart speakers, is further introduced to address current gaps inhibiting home use. We discuss the role of user-centred approaches in the design of voice systems, including the capacity to handle communication breakdowns for effective use by target populations. We summarise the state of development in interactions using speech and natural language processing, which forms a baseline for longitudinal health monitoring and cognitive assessment. Drawing from this foundation, we identify open challenges and propose future directions to advance conversational affective social robots for: 1) user engagement, 2) deployment in real-world settings, and 3) clinical translation",Conversational affective social robots for ageing and dementia support,,'Institute of Electrical and Electronics Engineers (IEEE)',10.1109/tcds.2021.3115228,"[{'title': 'IEEE Transactions on Cognitive and Developmental Systems', 'identifiers': ['2379-8920', 'issn:2379-8920']}]",core
199173119,2088-07-20T00:00:00,"During the last years, modern electronic power wheelchairs have been equipped by manipulators to compensate the deficit in the manual skills of the users due to accidents or disabling diseases. Such robotic arms are designed to perform simple operations, like knocking on a door or pressing buttons in a lift panel, turning on the light in a room, etc.
Owing to the benefits introduced by such manipulators in terms of mobility and autonomy increment, the research has been focusing on the realization of robotic arms able to perform very complex tasks like interaction with small objects.
However, for highly impaired users exploiting all the potentialities offered by such manipulators is often hard, especially because the robot arm is often controlled by the same joystick controller of the power-wheelchair. For such reason, a high number of different Human Machine Interfaces (HMIs) have been developed for users with different impairments. For example in case of severe upper-link disabilities, Brain Computer Interfaces (BCI) can be used which is very promising thanks to its usability by the users having no possibility of moving their arms. However, such HMI requires the presence of numerous electrodes placed on the user's body or dedicated helmets resulting more invasive than other interfaces. For such reason, for less severe disabilities different HMIs are more recommended.
In this thesis, we propose a user independent low cost robotic arm with 5-Degrees of Freedom(DOFs) equipped by a monocular camera and a proximity sensor, controlled by and augmented HMI featuring a real-time AI algorithm which improves the users experience. The offered HMI is based on a touch-screen in which the user can visualize the camera frames and press the desired object. Starting from the user selections, the AI extracts some image features which are elaborated by a tracking algorithm. The autonomy of the robotic arm is reached by a closed-feedback loop which exploits image features extracted by the camera images, to actuate the motors of the manipulator. The main advantages of this work is due to particular Computer Vision algorithm that uses an Artificial Intelligence (AI) to perform objects recognition and their coordinates extraction in the image frames. In fact, thanks to the presence of the AI, objects, like the buttons of a lift panel, are autonomously recognized and shown to the users surrounded by a bounding box. This allows the implementation of a robust HMIs activated by the click of the user on a touchscreen. The robustness of the HMIs is increased since it accepts clicks only inside the shown bounding boxes, avoiding to start the approach in case of user errors. For more severe impairments, a Manual raster scanning and a Time raster scanning HMIs have been implemented.
The software of the system is modular, versatile and easy to maintain thanks to its model based organization. It has been implemented inside the Robotic Operative System (ROS) environment. ROS is an open-source and multi-platform framework that manages multiple tasks as nodes of a network that exchange messages in form of topics or services. Each node is a single elaboration unit or module, usually represented by a process, in which the messages are received and sent, according to the previous and next module. In this way, a future reconfiguration of the software can be carried out without changing the entire software organization. It also permits to test new single nodes inside the existent ones, providing the message are maintained.
The entire software, with the only exception of the AI and the tracking algorithm, runs on a Raspberry PI 3 platform",Low cost eye-in-hand robotic-arm featuring AI-based human machine interface for people with disabilities,,'Pisa University Press',,,core
479314553,2021-01-01T00:00:00,"One of the biggest challenges hindering a table tennis robot to play as well as a professional player is the ball&#x2019;s accurate motion control, which depends on various factors such as the incoming ball&#x2019;s position, linear, spin velocity and so forth. Unfortunately, some factors are almost impossible to be directly measured in real practice, such as the ball&#x2019;s spin velocity, which is difficult to be estimated from vision due to the little texture on the ball&#x2019;s surface. To perform accurate motion control in table tennis, this study proposes to learn a ball stroke strategy to guarantee desirable &#x201C;target landing location&#x201D; and the &#x201C;over-net height&#x201D; which are two key indicators to evaluate the quality of a stroke. To overcome the spin velocity challenge, a deep reinforcement learning (DRL) based stroke approach is developed with the spin velocity estimation capability, through which the system can predict the relative spin velocity of the ball and stroke it back accurately by iteratively learning from the robot-environment interactions. To pre-train the DRL-based strategy effectively, this paper develops a virtual table tennis playing environment, through which various simulated data can be collected. For the real table tennis robot implementation, experimental results demonstrate the superior performance of the proposed control strategy compared to that of the traditional aerodynamics-based method with an average landing error around 80mm and the landing-within-table probability higher than 70&#x0025;",Ball Motion Control in the Table Tennis Robot System Using Time-Series Deep Reinforcement Learning,,'Institute of Electrical and Electronics Engineers (IEEE)',10.1109/ACCESS.2021.3093340,"[{'title': 'IEEE Access', 'identifiers': ['issn:2169-3536', '2169-3536']}]",core
387291020,2021-04-02T00:00:00,"Localizing the camera in a known indoor environment is a key building block
for scene mapping, robot navigation, AR, etc. Recent advances estimate the
camera pose via optimization over the 2D/3D-3D correspondences established
between the coordinates in 2D/3D camera space and 3D world space. Such a
mapping is estimated with either a convolution neural network or a decision
tree using only the static input image sequence, which makes these approaches
vulnerable to dynamic indoor environments that are quite common yet challenging
in the real world. To address the aforementioned issues, in this paper, we
propose a novel outlier-aware neural tree which bridges the two worlds, deep
learning and decision tree approaches. It builds on three important blocks: (a)
a hierarchical space partition over the indoor scene to construct the decision
tree; (b) a neural routing function, implemented as a deep classification
network, employed for better 3D scene understanding; and (c) an outlier
rejection module used to filter out dynamic points during the hierarchical
routing process. Our proposed algorithm is evaluated on the RIO-10 benchmark
developed for camera relocalization in dynamic indoor environments. It achieves
robust neural routing through space partitions and outperforms the
state-of-the-art approaches by around 30% on camera pose accuracy, while
running comparably fast for evaluation","Robust Neural Routing Through Space Partitions for Camera Relocalization
  in Dynamic Indoor Environments",http://arxiv.org/abs/2012.04746,,,,core
479164234,2021-07-03T00:00:00,"This thesis tackles the problem of predicting in real-time when a grasp, autonomously executed by a soft robotic hand, is going to fail, providing the most probable direction of sliding, and proposes a solution to compensate for the potential failure by activating reactive multi-arm primitives. For the prediction part deep learning techniques are implemented in real-time, while, regarding the compensation, primitives are based on human-like motion planning. Using a robotic manipulator, that moves like a human, allows the scenario to be more similar to reality, and in particular, the introduction of a second one, to compensate for grasp failure, is crucial for future applications of collaborative robots. In this work, starting from the studies developed by my partner Irene Valdambrini, who proposed some solutions to predict a grasp failure in off-line mode, first of all, a Neural Network is selected. Two main aspects have to be considered: the test accuracy parameter for detecting grasp failure and the robot velocity constraints to execute the re-grasp action. This network works online and takes as input a multidimensional continuum stream of raw signals coming with a frequency of 70 Hz from 15 Inertial Measurement Units. Data are published on two ROS topics and through a dynamic buffer are provided to Neural Network, which requires a longer time to do inference, about 5 Hz. The network is trained to predict the occurrence in the near future of a slippage event and in particular its direction. Before the 1st robot grasps the object the IMUs data are not significant to predict the correct output, so network output starts to be published on ROS topic when the robot starts to lift the object. If the output predicted is the same for five consecutive times the 2nd robot starts to plan and to execute the re-grasp motion. This strategy is adopted to find a trade-off between a reliable Neural Network output and the time that the 2nd robot takes to do the re-grasp action, about 2 seconds. Based on the output predicted, two different re-grasp primitives will be performed: bottom, if the object slips perpendicularly to the palm of the hand (type 1), or lateral if it slips parallelly (type 2). The grasp and re-grasp trajectories are planned through the MoveIt! software by implementing the RRT Connect algorithm, that finds these quickly, and are executed by robots using an impedance control. Therefore, based on training experiments and outcomes, to verify the Neural Network in real-time, 20 tests are conducted for each typology of grasp: success, failure type 1, and failure type 2. The results show that the network detects 78.33$\%$ of failures online while in off-line mode 83.61$\%$ and the re-grasp is executed successfully 87.5$\%$ times. Finally, to generalize the network, it is tested with daily life objects of different shapes. For each new object, 10 experiments are conducted and the result obtained is the detection of slipping with 77.05$\%$ of accuracy with 88.63$\%$ of successfully re-grasp. Starting from cylinder and sphere, the network is tested to detect slipping considering new shapes. This analysis has been possible using a soft hand that, thanks to its physical adaptability, is able to grasp objects with different shapes and surfaces, while grasping a pan, the contact surface between object and hand is smaller than the sphere case. The network generalization is also possible using IMUs data as sensor strategy. This work shows consistent results which could be improved by introducing a system of vision to detect the object to grasp and to develop autonomous grasping, or detecting failures but also studying hand-object contact to obtain useful information to provide to the network",Learning grasp failure recovery: online prediction and re-grasp execution,,'Pisa University Press',,,core
199172960,2088-07-16T00:00:00,"In agricoltura prima dell’introduzione delle moderne tecnologie, come il sistema satellitare globale di navigazione (GNSS), la raccolta e l’elaborazione dei dati richiedevano molto tempo e molto lavoro. Oggi, grazie all’introduzione di tecnologie avanzate, le informazioni riguardanti le macchine agricole possono essere raccolte in tempi ridotti. Lo scopo di questa tesi e stato quello di monitorare le traiettorie e le caratteristiche operative di un robot rasaerba in aree di diversa forma e di uguale superficie utilizzando un dispositivo real-time kinematic (RTK) – GNSS. Tali informazioni sono state successivamente estratte facilmente utilizzando uno specifico software, progettato e sviluppato per questo scopo. Il software permette di calcolare la distanza percorsa, la velocità di avanzamento, l’area di taglio e il numero di intersezioni delle traiettorie del robot. Questi dati sono stati analizzati con lo scopo di fornire informazioni utili sul funzionamento dei robot rasaerba ai produttori, imprenditori e professionisti. La traiettoria percorsa dal robot rasaerba era casuale e l’area di tappeto erboso per ognuna delle forme (quadrato, triangolo, rettangolo, circonferenza) misurava 135 m2 e non vi erano ostacoli all'interno. La distanza percorsa dal robot rasaerba, la velocità media di avanzamento e le intersezioni delle traiettorie sono risultate influenzate dall'interazione tra il tempo di taglio e la forma del tappeto erboso. La superficie totale è stata tagliata completamente dopo due ore di lavoro in tutte le forme. L’efficienza di taglio è diminuita all'aumentare del tempo, come conseguenza dell’aumento delle sovrapposizioni",Studio delle traiettorie dei rasaerba autonomi,,'Pisa University Press',,,core
388365551,2021-03-07T00:00:00,"Real-time adaptation is imperative to the control of robots operating in
complex, dynamic environments. Adaptive control laws can endow even nonlinear
systems with good trajectory tracking performance, provided that any uncertain
dynamics terms are linearly parameterizable with known nonlinear features.
However, it is often difficult to specify such features a priori, such as for
aerodynamic disturbances on rotorcraft or interaction forces between a
manipulator arm and various objects. In this paper, we turn to data-driven
modeling with neural networks to learn, offline from past data, an adaptive
controller with an internal parametric model of these nonlinear features. Our
key insight is that we can better prepare the controller for deployment with
control-oriented meta-learning of features in closed-loop simulation, rather
than regression-oriented meta-learning of features to fit input-output data.
Specifically, we meta-learn the adaptive controller with closed-loop tracking
simulation as the base-learner and the average tracking error as the
meta-objective. With a nonlinear planar rotorcraft subject to wind, we
demonstrate that our adaptive controller outperforms other controllers trained
with regression-oriented meta-learning when deployed in closed-loop for
trajectory tracking control",Adaptive-Control-Oriented Meta-Learning for Nonlinear Systems,http://arxiv.org/abs/2103.04490,,,,core
478100547,2021-04-20T14:56:41,"The purpose of this volume is to encourage and inspire the continual invention of robot manipulators for science and the good of humanity. The concepts of artificial intelligence combined with the engineering and technology of feedback control, have great potential for new, useful and exciting machines. The concept of eclecticism for the design, development, simulation and implementation of a real time controller for an intelligent, vision guided robots is now being explored. The dream of an eclectic perceptual, creative controller that can select its own tasks and perform autonomous operations with reliability and dependability is starting to evolve. We have not yet reached this stage but a careful study of the contents will start one on the exciting journey that could lead to many inventions and successful solutions",Advances in Robot Manipulators,,'IntechOpen',10.5772/238,,core
464903810,2021-07-15T00:00:00,"Learning multimodal representations involves integrating information from
multiple heterogeneous sources of data. It is a challenging yet crucial area
with numerous real-world applications in multimedia, affective computing,
robotics, finance, human-computer interaction, and healthcare. Unfortunately,
multimodal research has seen limited resources to study (1) generalization
across domains and modalities, (2) complexity during training and inference,
and (3) robustness to noisy and missing modalities. In order to accelerate
progress towards understudied modalities and tasks while ensuring real-world
robustness, we release MultiBench, a systematic and unified large-scale
benchmark spanning 15 datasets, 10 modalities, 20 prediction tasks, and 6
research areas. MultiBench provides an automated end-to-end machine learning
pipeline that simplifies and standardizes data loading, experimental setup, and
model evaluation. To enable holistic evaluation, MultiBench offers a
comprehensive methodology to assess (1) generalization, (2) time and space
complexity, and (3) modality robustness. MultiBench introduces impactful
challenges for future research, including scalability to large-scale multimodal
datasets and robustness to realistic imperfections. To accompany this
benchmark, we also provide a standardized implementation of 20 core approaches
in multimodal learning. Simply applying methods proposed in different research
areas can improve the state-of-the-art performance on 9/15 datasets. Therefore,
MultiBench presents a milestone in unifying disjoint efforts in multimodal
research and paves the way towards a better understanding of the capabilities
and limitations of multimodal models, all the while ensuring ease of use,
accessibility, and reproducibility. MultiBench, our standardized code, and
leaderboards are publicly available, will be regularly updated, and welcomes
inputs from the community.Comment: Code: https://github.com/pliang279/MultiBench and Website:
  https://cmu-multicomp-lab.github.io/multibench",MultiBench: Multiscale Benchmarks for Multimodal Representation Learning,http://arxiv.org/abs/2107.07502,,,,core
478236672,2021-09-29T00:00:00,"Developing robust vision-guided controllers for quadrupedal robots in complex
environments, with various obstacles, dynamical surroundings and uneven
terrains, is very challenging. While Reinforcement Learning (RL) provides a
promising paradigm for agile locomotion skills with vision inputs in
simulation, it is still very challenging to deploy the RL policy in the real
world. Our key insight is that aside from the discrepancy in the domain gap, in
visual appearance between the simulation and the real world, the latency from
the control pipeline is also a major cause of difficulty. In this paper, we
propose Multi-Modal Delay Randomization (MMDR) to address this issue when
training RL agents. Specifically, we simulate the latency of real hardware by
using past observations, sampled with randomized periods, for both
proprioception and vision. We train the RL policy for end-to-end control in a
physical simulator without any predefined controller or reference motion, and
directly deploy it on the real A1 quadruped robot running in the wild. We
evaluate our method in different outdoor environments with complex terrains and
obstacles. We demonstrate the robot can smoothly maneuver at a high speed,
avoid the obstacles, and show significant improvement over the baselines. Our
project page with videos is at https://mehooz.github.io/mmdr-wild/.Comment: Project page: https://mehooz.github.io/mmdr-wild","Vision-Guided Quadrupedal Locomotion in the Wild with Multi-Modal Delay
  Randomization",http://arxiv.org/abs/2109.14549,,,,core
479164236,2021-07-03T00:00:00,"This paper addresses the challenge of predicting grasping errors and slip direction in soft hands before they happen by combining Deep Learning with a distributed Inertial Measurement Units based sensing strategy. We first propose two neural architectures that we have implemented and tested with data from previous work collected with an articulated soft hand, the Pisa/IIT SoftHand, and an IMUs glove. The first architecture (regressor) implements a prediction at time t of the Kth sensor data at time t+1. The second (classifier) classifies the entire data sequence as belonging to one of N classes and the choice falls on this model because it is computationally less expensive than the regressor. The general idea behind this is that the data is pruned at the time of failure and then the entire sequence is given as input to the neural network with a backward pruning from the last time. The final data corresponds to a series of experiments in which failure events have not yet occurred, in this way the classification of the data is equivalent to a prediction of the failure. In order to achieve a correspondence between the time of prediction and the backward cut, it is necessary to know the frequency of the experiments.
Previous works have some limitations that are improved in this work: a lack of data experiments, a non-generalized setup, the time of failure is detected manually with the possibility of human error and only one direction of slip, so it is necessary to collect new data.
The new experimental setup is created to collect more robust data to be sent to the network during training. The Sensus apparatus includes: an optical motion tracking 3D system for offline detection of fault timing and a glove equipped with 17 IMUs to provide information about slippage to the network. Two robotic platforms are involved in this work, the first, mentioned earlier, is Pisa/IIT SoftHand and the second is the robotic arm with 7 D.O.F. Panda from Franka Emika. The failures are caused by the weight added to the grasped object to obtain generalized failures, and the objects are built with a 3D printer to give them the best shape. The experiments collected in this setup are 1800, divided into rough and smooth surfaces, and are classified into three groups: Success, Lateral Slippage, and Central Slippage, and each is preprocessed before being input to the neural network.
During model selection, some neural networks are tested to predict failure events before they occur. The network that is best suited for a temporal sequence of data is a Recurrent Neural Network and the two gated networks are tested: LSTM(Long Short Term Memory) and GRU (Gated Recurrent Units). The falls are detected in a window of 1-3 seconds before they occur with a test accuracy between 86-76%. These results are good offline, but the real challenge is to test whether or not the percentage accuracy could be as high in real time.
The next goal will be to find a predictive model to test in real time and test the network with a second robot to retrieve objects that are about to fall",Learning grasp failure recovery: from raw data to grasp failure prediction,,'Pisa University Press',,,core
480006062,2021-01-01T00:00:00,"Affordance detection in computer vision allows segmenting an object into parts according to functions that those parts afford. Most solutions for affordance detection are developed in robotics using deep learning architectures that require substantial computing power. Therefore, these approaches are not convenient for application in embedded systems with limited resources. For instance, computer vision is used in smart prosthetic limbs, and in this context, affordance detection could be employed to determine the graspable segments of an object, which is a critical information for selecting a grasping strategy. This work proposes an affordance detection strategy based on hardware-aware deep learning solutions. Experimental results confirmed that the proposed solution achieves comparable accuracy with respect to the state-of-the-art approaches. In addition, the model was implemented on real-time embedded devices obtaining a high FPS rate, with limited power consumption. Finally, the experimental assessment in realistic conditions demonstrated that the developed method is robust and reliable. As a major outcome, the paper proposes and characterizes the first complete embedded solution for affordance detection in embedded devices. Such a solution could be used to substantially improve computer vision based prosthesis control but it is also highly relevant for other applications (e.g., resource-constrained robotic systems)",Hardware-Aware Affordance Detection for Application in Portable Embedded Systems,,'Institute of Electrical and Electronics Engineers (IEEE)',10.1109/ACCESS.2021.3109733,,core
479176460,2021-10-28T00:00:00,"Machine learning has long since become a keystone technology, accelerating
science and applications in a broad range of domains. Consequently, the notion
of applying learning methods to a particular problem set has become an
established and valuable modus operandi to advance a particular field. In this
article we argue that such an approach does not straightforwardly extended to
robotics -- or to embodied intelligence more generally: systems which engage in
a purposeful exchange of energy and information with a physical environment. In
particular, the purview of embodied intelligent agents extends significantly
beyond the typical considerations of main-stream machine learning approaches,
which typically (i) do not consider operation under conditions significantly
different from those encountered during training; (ii) do not consider the
often substantial, long-lasting and potentially safety-critical nature of
interactions during learning and deployment; (iii) do not require ready
adaptation to novel tasks while at the same time (iv) effectively and
efficiently curating and extending their models of the world through targeted
and deliberate actions. In reality, therefore, these limitations result in
learning-based systems which suffer from many of the same operational
shortcomings as more traditional, engineering-based approaches when deployed on
a robot outside a well defined, and often narrow operating envelope. Contrary
to viewing embodied intelligence as another application domain for machine
learning, here we argue that it is in fact a key driver for the advancement of
machine learning technology. In this article our goal is to highlight
challenges and opportunities that are specific to embodied intelligence and to
propose research directions which may significantly advance the
state-of-the-art in robot learning","From Machine Learning to Robotics: Challenges and Opportunities for
  Embodied Intelligence",http://arxiv.org/abs/2110.15245,,,,core
392156880,2021-03-19T00:00:00,"Self-adaptation approaches usually rely on closed-loop controllers that avoid
human intervention from adaptation. While such fully automated approaches have
proven successful in many application domains, there are situations where human
involvement in the adaptation process is beneficial or even necessary. For such
""human-in-the-loop"" adaptive systems, two major challenges, namely transparency
and controllability, have to be addressed to include the human in the
self-adaptation loop. Transparency means that relevant context information
about the adaptive systems and its context is represented based on a digital
twin enabling the human an immersive and realistic view. Concerning
controllability, the decision-making and adaptation operations should be
managed in a natural and interactive way. As existing human-in-the-loop
adaptation approaches do not fully cover these aspects, we investigate
alternative human-in-the-loop strategies by using a combination of digital
twins and virtual reality (VR) interfaces. Based on the concept of the digital
twin, we represent a self-adaptive system and its respective context in a
virtual environment. With the help of a VR interface, we support an immersive
and realistic human involvement in the self-adaptation loop by mirroring the
physical entities of the real world to the VR interface. For integrating the
human in the decision-making and adaptation process, we have implemented and
analyzed two different human-in-the-loop strategies in VR: a procedural control
where the human can control the decision making-process and adaptations through
VR interactions (human-controlled) and a declarative control where the human
specifies the goal state and the configuration is delegated to an AI planner
(mixed-initiative). We illustrate and evaluate our approach based on an
autonomic robot system that is accessible and controlled through a VR
interface.Comment: Submitted to SEAMS 202","Enhancing Human-in-the-Loop Adaptive Systems through Digital Twins and
  VR Interfaces",http://arxiv.org/abs/2103.10804,,,,core
479192011,2021-05-01T00:00:00,"Abstract Visual simultaneous localization and mapping (vSLAM), one of the most important applications in autonomous vehicles and robots to estimate the position and pose using inexpensive visual sensors, suffers from error accumulation for long‐term navigation without loop closure detection. Recently, deep neural networks (DNNs) are leveraged to achieve high accuracy for loop closure detection, however the execution time is much slower than those employing handcrafted visual features. In this paper, a parallel loop searching and verifying method for loop closure detection with both high accuracy and high speed, which combines two parallel tasks using handcrafted and DNN features, respectively, is proposed. A fast loop searching is proposed to link the bag‐of‐words features and histogram for higher accuracy, and it splits the images into multiple grids for high parallelism; meanwhile, a DNN feature extractor is utilized for further verification. A loop state control method based on a finite state machine to control these tasks is designed, wherein the loop closure detection is described as a context‐related procedure. The framework is implemented on a real machine, and the top‐2 best accuracy and fastest execution time of 80‐543 frames per second (min: 1.84ms, and max: 12.45ms) are achieved on several public benchmarks compared with some existing algorithms",PLSAV: Parallel loop searching and verifying for loop closure detection,,'Institution of Engineering and Technology (IET)',10.1049/itr2.12054,"[{'title': 'IET Intelligent Transport Systems', 'identifiers': ['issn:1751-956X', 'issn:1751-9578', '1751-9578', '1751-956x']}]",core
327944559,2021-01-21T00:00:00,"Dexterous object manipulation remains an open problem in robotics, despite
the rapid progress in machine learning during the past decade. We argue that a
hindrance is the high cost of experimentation on real systems, in terms of both
time and money. We address this problem by proposing an open-source robotic
platform which can safely operate without human supervision. The hardware is
inexpensive (about \SI{5000}[\$]{}) yet highly dynamic, robust, and capable of
complex interaction with external objects. The software operates at 1-kilohertz
and performs safety checks to prevent the hardware from breaking. The
easy-to-use front-end (in C++ and Python) is suitable for real-time control as
well as deep reinforcement learning. In addition, the software framework is
largely robot-agnostic and can hence be used independently of the hardware
proposed herein. Finally, we illustrate the potential of the proposed platform
through a number of experiments, including real-time optimal control, deep
reinforcement learning from scratch, throwing, and writing",TriFinger: An Open-Source Robot for Learning Dexterity,http://arxiv.org/abs/2008.03596,,,,core
443972052,2021-07-01T00:00:00,"Autonomous Driving (AD) related features provide new forms of mobility that
are also beneficial for other kind of intelligent and autonomous systems like
robots, smart transportation, and smart industries. For these applications, the
decisions need to be made fast and in real-time. Moreover, in the quest for
electric mobility, this task must follow low power policy, without affecting
much the autonomy of the mean of transport or the robot. These two challenges
can be tackled using the emerging Spiking Neural Networks (SNNs). When deployed
on a specialized neuromorphic hardware, SNNs can achieve high performance with
low latency and low power consumption. In this paper, we use an SNN connected
to an event-based camera for facing one of the key problems for AD, i.e., the
classification between cars and other objects. To consume less power than
traditional frame-based cameras, we use a Dynamic Vision Sensor (DVS). The
experiments are made following an offline supervised learning rule, followed by
mapping the learnt SNN model on the Intel Loihi Neuromorphic Research Chip. Our
best experiment achieves an accuracy on offline implementation of 86%, that
drops to 83% when it is ported onto the Loihi Chip. The Neuromorphic Hardware
implementation has maximum 0.72 ms of latency for every sample, and consumes
only 310 mW. To the best of our knowledge, this work is the first
implementation of an event-based car classifier on a Neuromorphic Chip.Comment: Accepted for publication at IJCNN 202","CarSNN: An Efficient Spiking Neural Network for Event-Based Autonomous
  Cars on the Loihi Neuromorphic Research Processor",http://arxiv.org/abs/2107.00401,,,,core
478868057,2021-09-15T15:40:54,"The objective of the proposed research is to develop methodologies, support algorithms and software-hardware infrastructure for detection, diagnosis, and correction of failures for actuators, sensors and control software in linear and nonlinear state variable systems with the help of multiple checks employed in the system. This objective is motivated by the proliferation of autonomous sense-and-control real-time systems, such as intelligent robots and self-driven cars which must maintain a minimum level of performance in the presence of electro-mechanical degradation of system-level components in the field as well as external attacks in the form of transient errors. A key focus is on rapid recovery from the effects of such anomalies and impairments with minimal impact on system performance while maintaining low implementation overhead as opposed to traditional schemes for recovery that rely on duplication or triplication. On-line detection, diagnosis and correction techniques are investigated and rely on analysis of system under test response signatures to real-time stimulus. For on-line error detection and diagnosis, linear and nonlinear state space encodings of the system under test are used and specific properties of the codes, as well as machine learning model based approaches were used are analyzed in real-time. Recovery is initiated by copying check model values to correct error for sensor and control software malfunction, and by redesigning the controller parameter on-the-fly for actuators to restore system performance. Future challenges that need to be addressed include viability studies of the proposed techniques on mobile autonomous system in distributed setting as well as application to systems with soft as well as hard real-time performance constraints.Ph.D",REAL-TIME ERROR DETECTION AND CORRECTION FOR ROBUST OPERATION OF AUTONOMOUS SYSTEMS USING ENCODED STATE CHECKS,https://core.ac.uk/download/478868057.pdf,Georgia Institute of Technology,,,core
199172230,2088-05-07T00:00:00,"The introduction of robots and automation in industrial processes brought many benefits to manufacturing industries, by improving both quality and quantity of production. Automation-assisted production is however mostly open-loop and requires checkpoints to perform product quality analysis. Early vision-based systems were therefore developed since the nineties. This approach worked very well for those activities in which the critical issues can be formally expressed by means of geometrical properties (e.g. measures) or the presence/absence of well-known features on the inspected objects.
However, to date, there are still many quality-control activities that cannot be performed by automated vision inspection machines. Their use is limited by the need to express the checks to be performed as a predefined sequence of actions in which each of them must be carefully designed to fit the specific production requirements. Nevertheless, many of the checks which can be easily learned by humans, are extremely difficult to be formally described using a set of static rules. The major difference relies on the fact that humans can exploit their experience as a relevant evaluation criterion while assessing the quality of a product. On the other hand, implementing such feature on an automated vision inspection machine requires the development of novel algorithms which can be trained and improved with time and experience. Early attempts using traditional Artificial Neural Networks (ANNs) and other learning tools dramatically failed due to the complexity of modeling in rigid mathematical structures the vast set of rules required to model the expertise.
In the last few years, the research on Deep Neural Network (DNN) brought new interest in the field of machine learning based on ANNs. In particular, it has been shown that, given enough data, a DNN can be successfully used to perform complex computer vision tasks such as object classification, object detection and image segmentation. Those networks showed several interesting features, such as experience-based learning, scalability, and performances similar (and in some cases even superior) to human beings. Moreover, with respect to previous ANN approaches, these networks have the advantage that they do not require anymore explicit efforts to describe optimized features extraction algorithms.
This Thesis work has the aim to map the human learning ability from experience into an equivalent learning-from-data capability of a DNN. The effectiveness of this approach has been proved in a real application for the inspection of welding defects on the assembly line of fuel injectors. We designed and assessed an automated defect analysis system based on deep learning. The Thesis covered all the development steps from early concept design, specification design, development of a prototype, software design and finally the development of a complete system which will be integrated into the assembly line. The hardware and software have been designed for all the system components, from the acquisition of the images to the communication of the analysis results to the enterprise PLC. Most of the efforts have been put in the design of the network architecture and on the definition of the training procedure. Starting from state-of-the-art deep architectures and using the transfer learning technique, it has been possible to train a network with about 7 millions parameters using a reduced number of injectors images, obtaining an accuracy of 97.22%.
The realization of this Thesis has been made possible thanks to the collaboration with Continental Automotive Italy S.p.A and their interest in the Industry 4.0 research topics to bring intelligence in the manufacturing processes.
So far, the system described in this work has been successfully tested on the assembly line and will enter into production starting from June 2018. We already provided intelligence in the system so that during the early phases of operation, it will collect new images to extend the existing dataset and to improve further its performance. With this Thesis work, we showed that deep neural networks can successfully perform quality inspection tasks which are actually done by humans. The approach, the system and the methodology described here can be also easily extended and applied to other applications",Automated Defect Analysis of Mechanical Components using Deep Learning-Based Computer Vision,,'Pisa University Press',,,core
388710373,2021-07-04T00:00:00,"Offline reinforcement learning proposes to learn policies from large
collected datasets without interacting with the physical environment. These
algorithms have made it possible to learn useful skills from data that can then
be deployed in the environment in real-world settings where interactions may be
costly or dangerous, such as autonomous driving or factories. However, current
algorithms overfit to the dataset they are trained on and exhibit poor
out-of-distribution generalization to the environment when deployed. In this
paper, we study the effectiveness of performing data augmentations on the state
space, and study 7 different augmentation schemes and how they behave with
existing offline RL algorithms. We then combine the best data performing
augmentation scheme with a state-of-the-art Q-learning technique, and improve
the function approximation of the Q-networks by smoothening out the learned
state-action space. We experimentally show that using this Surprisingly Simple
Self-Supervision technique in RL (S4RL), we significantly improve over the
current state-of-the-art algorithms on offline robot learning environments such
as MetaWorld [1] and RoboSuite [2,3], and benchmark datasets such as D4RL [4]","S4RL: Surprisingly Simple Self-Supervision for Offline Reinforcement
  Learning",http://arxiv.org/abs/2103.06326,,,,core
478192963,2021-01-01T00:00:00,"In this article, we show that learned policies can be applied to solve legged locomotion control tasks with extensive flight phases, such as those encountered in space exploration. Using an off-the-shelf deep reinforcement learning algorithm, we train a neural network to control a jumping quadruped robot while solely using its limbs for attitude control. We present tasks of increasing complexity leading to a combination of 3-D (re)orientation and landing locomotion behaviors of a quadruped robot traversing simulated low-gravity celestial bodies. We show that our approach easily generalizes across these tasks and successfully trains policies for each case. Using sim-to-real transfer, we deploy trained policies in the real world on the SpaceBok robot placed on an experimental testbed designed for 2-D microgravity experiments. The experimental results demonstrate that repetitive controlled jumping and landing with natural agility is possible.ISSN:1552-3098ISSN:1042-296XISSN:1941-046",Cat-Like Jumping and Landing of Legged Robots in Low Gravity Using Deep Reinforcement Learning,,'Institute of Electrical and Electronics Engineers (IEEE)',10.1109/tro.2021.3084374,,core
474896089,2021-07-22T00:00:00,"We present Bayesian Controller Fusion (BCF): a hybrid control strategy that
combines the strengths of traditional hand-crafted controllers and model-free
deep reinforcement learning (RL). BCF thrives in the robotics domain, where
reliable but suboptimal control priors exist for many tasks, but RL from
scratch remains unsafe and data-inefficient. By fusing uncertainty-aware
distributional outputs from each system, BCF arbitrates control between them,
exploiting their respective strengths. We study BCF on two real-world robotics
tasks involving navigation in a vast and long-horizon environment, and a
complex reaching task that involves manipulability maximisation. For both these
domains, there exist simple handcrafted controllers that can solve the task at
hand in a risk-averse manner but do not necessarily exhibit the optimal
solution given limitations in analytical modelling, controller miscalibration
and task variation. As exploration is naturally guided by the prior in the
early stages of training, BCF accelerates learning, while substantially
improving beyond the performance of the control prior, as the policy gains more
experience. More importantly, given the risk-aversity of the control prior, BCF
ensures safe exploration and deployment, where the control prior naturally
dominates the action distribution in states unknown to the policy. We
additionally show BCF's applicability to the zero-shot sim-to-real setting and
its ability to deal with out-of-distribution states in the real-world. BCF is a
promising approach for combining the complementary strengths of deep RL and
traditional robotic control, surpassing what either can achieve independently.
The code and supplementary video material are made publicly available at
https://krishanrana.github.io/bcf.Comment: Under review for The International Journal of Robotics Research
  (IJRR). Project page: https://krishanrana.github.io/bc","Bayesian Controller Fusion: Leveraging Control Priors in Deep
  Reinforcement Learning for Robotics",http://arxiv.org/abs/2107.09822,,,,core
478614405,2021-10-09T00:00:00,"A critical need in assistive robotics, such as assistive wheelchairs for
navigation, is a need to learn task intent and safety guarantees through user
interactions in order to ensure safe task performance. For tasks where the
objectives from the user are not easily defined, learning from user
demonstrations has been a key step in enabling learning. However, most robot
learning from demonstration (LfD) methods primarily rely on optimal
demonstration in order to successfully learn a control policy, which can be
challenging to acquire from novice users. Recent work does use suboptimal and
failed demonstrations to learn about task intent; few focus on learning safety
guarantees to prevent repeat failures experienced, essential for assistive
robots. Furthermore, interactive human-robot learning aims to minimize effort
from the human user to facilitate deployment in the real-world. As such,
requiring users to label the unsafe states or keyframes from the demonstrations
should not be a necessary requirement for learning. Here, we propose an
algorithm to learn a safety value function from a set of suboptimal and failed
demonstrations that is used to generate a real-time safety control filter.
Importantly, we develop a credit assignment method that extracts the failure
states from the failed demonstrations without requiring human labelling or
prespecified knowledge of unsafe regions. Furthermore, we extend our
formulation to allow for user-specific safety functions, by incorporating
user-defined safety rankings from which we can generate safety level sets
according to the users' preferences. By using both suboptimal and failed
demonstrations and the developed credit assignment formulation, we enable
learning a safety value function with minimal effort needed from the user,
making it more feasible for widespread use in human-robot interactive learning
tasks.Comment: Presented at AI-HRI symposium as part of AAAI-FSS 2021
  (arXiv:2109.10836",Credit Assignment Safety Learning from Human Demonstrations,http://arxiv.org/abs/2110.04633,,,,core
440360570,2021-06-01T00:00:00,"Abstract With the increasing application of computer vision technology in autonomous driving, robot, and other mobile devices, more and more attention has been paid to the implementation of target detection and tracking algorithms on embedded platforms. The real-time performance and robustness of algorithms are two hot research topics and challenges in this field. In order to solve the problems of poor real-time tracking performance of embedded systems using convolutional neural networks and low robustness of tracking algorithms for complex scenes, this paper proposes a fast and accurate real-time video detection and tracking algorithm suitable for embedded systems. The algorithm combines the object detection model of single-shot multibox detection in deep convolution networks and the kernel correlation filters tracking algorithm, what is more, it accelerates the single-shot multibox detection model using field-programmable gate arrays, which satisfies the real-time performance of the algorithm on the embedded platform. To solve the problem of model contamination after the kernel correlation filters algorithm fails to track in complex scenes, an improvement in the validity detection mechanism of tracking results is proposed that solves the problem of the traditional kernel correlation filters algorithm not being able to robustly track for a long time. In order to solve the problem that the missed rate of the single-shot multibox detection model is high under the conditions of motion blur or illumination variation, a strategy to reduce missed rate is proposed that effectively reduces the missed detection. The experimental results on the embedded platform show that the algorithm can achieve real-time tracking of the object in the video and can automatically reposition the object to continue tracking after the object tracking fails",Real-time embedded object detection and tracking system in Zynq SoC,,'Springer Science and Business Media LLC',10.1186/s13640-021-00561-7,"[{'title': 'EURASIP Journal on Image and Video Processing', 'identifiers': ['issn:1687-5281', '1687-5281']}]",core
443971503,2021-06-30T00:00:00,"Robot programming typically makes use of a set of mechanical skills that is
acquired by machine learning. Because there is in general no guarantee that
machine learning produces robot programs that are free of surprising behavior,
the safe execution of a robot program must utilize monitoring modules that take
sensor data as inputs in real time to ensure the correctness of the skill
execution. Owing to the fact that sensors and monitoring algorithms are usually
subject to physical restrictions and that effective robot programming is
sensitive to the selection of skill parameters, these considerations may lead
to different sensor input qualities such as the view coverage of a vision
system that determines whether a skill can be successfully deployed in
performing a task. Choosing improper skill parameters may cause the monitoring
modules to delay or miss the detection of important events such as a mechanical
failure. These failures may reduce the throughput in robotic manufacturing and
could even cause a destructive system crash. To address above issues, we
propose a sensing quality-aware robot programming system that automatically
computes the sensing qualities as a function of the robot's environment and
uses the information to guide non-expert users to select proper skill
parameters in the programming phase. We demonstrate our system framework on a
6DOF robot arm for an object pick-up task.Comment: 7 pages, 9 figures, 1 table; accepted for presentation in IEEE ICRA
  2021(IEEE International Conference on Robotics and Automation","SQRP: Sensing Quality-aware Robot Programming System for Non-expert
  Programmers",http://arxiv.org/abs/2107.00127,,,,core
402209797,2021-02-01T00:00:00,"COVID-19 has severely impacted mental health in vulnerable demographics, in particular older adults, who face unprecedented isolation. Consequences, while globally severe, are acutely pronounced in low- and middle-income countries (LMICs) confronting pronounced gaps in resources and clinician accessibility. Social robots are well-recognized for their potential to support mental health, yet user compliance (i.e., trust) demands seamless affective human-robot interactions; natural ‘human-like’ conversations are required in simple, inexpensive, deployable platforms. We present the design, development, and pilot testing of a multimodal robotic framework fusing verbal (contextual speech) and nonverbal (facial expressions) social cues, aimed to improve engagement in human-robot interaction and ultimately facilitate mental health telemedicine during and beyond the COVID-19 pandemic. We report the design optimization of a hybrid face robot, which combines digital facial expressions based on mathematical affect space mapping with static 3D facial features. We further introduce a contextual virtual assistant with integrated cloud-based AI coupled to the robot’s facial representation of emotions, such that the robot adapts its emotional response to users’ speech in real-time. Experiments with healthy participants demonstrate emotion recognition exceeding 90% for happy, tired, sad, angry, surprised and stern/disgusted robotic emotions. When separated, stern and disgusted are occasionally transposed (70%+ accuracy overall) but are easily distinguishable from other emotions. A qualitative user experience analysis indicates overall enthusiastic and engaging reception to human-robot multimodal interaction with the new framework. The robot has been modified to enable clinical telemedicine for cognitive engagement with older adults and people with dementia (PwD) in LMICs. The mechanically simple and low-cost social robot has been deployed in pilot tests to support older individuals and PwD at the Schizophrenia Research Foundation (SCARF) in Chennai, India. A procedure for deployment addressing challenges in cultural acceptance, end-user acclimatization and resource allocation is further introduced. Results indicate strong promise to stimulate human-robot psychosocial interaction through the hybrid-face robotic system. Future work is targeting deployment for telemedicine to mitigate the mental health impact of COVID-19 on older adults and PwD in both LMICs and higher income regions",Robotic telemedicine for mental health: a multimodal approach to improve human-robot engagement,,'Frontiers Media SA',10.3389/frobt.2021.618866,"[{'title': 'Frontiers in Robotics and AI', 'identifiers': ['issn:2296-9144', '2296-9144']}]",core
334863759,2021-01-25T00:00:00,"Visual exploration and smart data collection via autonomous vehicles is an
attractive topic in various disciplines. Disturbances like wind significantly
influence both the power consumption of the flying robots and the performance
of the camera. We propose a reinforcement learning approach which combines the
effects of the power consumption and the object detection modules to develop a
policy for object detection in large areas with limited battery life. The
learning model enables dynamic learning of the negative rewards of each action
based on the drag forces that is resulted by the motion of the flying robot
with respect to the wind field. The algorithm is implemented in a near-real
world simulation environment both for the planar motion and flight in different
altitudes. The trained agent often performed a trade-off between detecting the
objects with high accuracy and increasing the area coverage within its battery
life. The developed exploration policy outperformed the complete coverage
algorithm by minimizing the traveled path while finding the target objects. The
performance of the algorithms under various wind fields was evaluated in planar
and 3D motion. During an exploration task with sparsely distributed goals and
within a UAV's battery life, the proposed architecture could detect more than
twice the amount of goal objects compared to the coverage path planning
algorithm in moderate wind field. In high wind intensities, the energy-aware
algorithm could detect 4 times the amount of goal objects when compared to its
complete coverage counterpart.Comment: 20 Pages, 14 figure","Visual Exploration and Energy-aware Path Planning via Reinforcement
  Learning",http://arxiv.org/abs/1909.12217,,,,core
467112496,2021-07-17T00:00:00,"Autonomous car racing is a challenging task in the robotic control area.
Traditional modular methods require accurate mapping, localization and
planning, which makes them computationally inefficient and sensitive to
environmental changes. Recently, deep-learning-based end-to-end systems have
shown promising results for autonomous driving/racing. However, they are
commonly implemented by supervised imitation learning (IL), which suffers from
the distribution mismatch problem, or by reinforcement learning (RL), which
requires a huge amount of risky interaction data. In this work, we present a
general deep imitative reinforcement learning approach (DIRL), which
successfully achieves agile autonomous racing using visual inputs. The driving
knowledge is acquired from both IL and model-based RL, where the agent can
learn from human teachers as well as perform self-improvement by safely
interacting with an offline world model. We validate our algorithm both in a
high-fidelity driving simulation and on a real-world 1/20-scale RC-car with
limited onboard computation. The evaluation results demonstrate that our method
outperforms previous IL and RL methods in terms of sample efficiency and task
performance. Demonstration videos are available at
https://caipeide.github.io/autorace-dirl/Comment: 8 pages, 8 figures. IEEE Robotics and Automation Letters (RA-L) &
  IROS 202","Vision-Based Autonomous Car Racing Using Deep Imitative Reinforcement
  Learning",http://arxiv.org/abs/2107.08325,,,,core
478867932,2021-09-01T00:00:00,"Presented online via Bluejeans Events on September 1, 2021 at 12:15 p.m.Jie Tan is currently the Tech Lead Manager of the Robot Locomotion and Safety teams at Google Brain Robotics. His research enables robots to automatically learn skills to accomplish complex tasks in the real world. Dr. Tan's research interests include AI safety, machine learning, robotics, power grids and simulation.Runtime: 51:54 minutesDeep Reinforcement Learning (DRL) holds the promise of designing complex robotic controllers automatically. In this talk, I will discuss two different approaches to apply deep reinforcement learning to learn locomotion controllers for legged robots. The first approach is through sim-to-real transfer. Due to safety concerns and limited data, most of the training is conducted in simulation. However, controllers learned in simulation usually perform poorly on real robots. I will present a set of techniques to overcome this sim-to-real gap. The second approach is to directly learn in the real world. Due to the complexity and diversity of the real environments, building a simulation that can faithfully model the real world is not always feasible. Having the ability to learn on the fly and adapt quickly in real-world scenarios is crucial for large-scale deployment of robots. I will discuss the challenges of training legged robots in the real world and various ways to address these challenges",Learning Locomotion: From Simulation to Real World,,,,,core
389073049,2021-03-14T00:00:00,"We present Success weighted by Completion Time (SCT), a new metric for
evaluating navigation performance for mobile robots. Several related works on
navigation have used Success weighted by Path Length (SPL) as the primary
method of evaluating the path an agent makes to a goal location, but SPL is
limited in its ability to properly evaluate agents with complex dynamics. In
contrast, SCT explicitly takes the agent's dynamics model into consideration,
and aims to accurately capture how well the agent has approximated the fastest
navigation behavior afforded by its dynamics. While several embodied navigation
works use point-turn dynamics, we focus on unicycle-cart dynamics for our
agent, which better exemplifies the dynamics model of popular mobile robotics
platforms (e.g., LoCoBot, TurtleBot, Fetch, etc.). We also present
RRT*-Unicycle, an algorithm for unicycle dynamics that estimates the fastest
collision-free path and completion time from a starting pose to a goal location
in an environment containing obstacles. We experiment with deep reinforcement
learning and reward shaping to train and compare the navigation performance of
agents with different dynamics models. In evaluating these agents, we show that
in contrast to SPL, SCT is able to capture the advantages in navigation speed a
unicycle model has over a simpler point-turn model of dynamics. Lastly, we show
that we can successfully deploy our trained models and algorithms outside of
simulation in the real world. We embody our agents in an real robot to navigate
an apartment, and show that they can generalize in a zero-shot manner","Success Weighted by Completion Time: A Dynamics-Aware Evaluation
  Criteria for Embodied Navigation",http://arxiv.org/abs/2103.08022,,,,core
392156949,2021-03-19T00:00:00,"Artificial intelligence-powered pocket-sized air robots have the potential to
revolutionize the Internet-of-Things ecosystem, acting as autonomous,
unobtrusive, and ubiquitous smart sensors. With a few cm$^{2}$ form-factor,
nano-sized unmanned aerial vehicles (UAVs) are the natural befit for indoor
human-drone interaction missions, as the pose estimation task we address in
this work. However, this scenario is challenged by the nano-UAVs' limited
payload and computational power that severely relegates the onboard brain to
the sub-100 mW microcontroller unit-class. Our work stands at the intersection
of the novel parallel ultra-low-power (PULP) architectural paradigm and our
general development methodology for deep neural network (DNN) visual pipelines,
i.e., covering from perception to control. Addressing the DNN model design,
from training and dataset augmentation to 8-bit quantization and deployment, we
demonstrate how a PULP-based processor, aboard a nano-UAV, is sufficient for
the real-time execution (up to 135 frame/s) of our novel DNN, called
PULP-Frontnet. We showcase how, scaling our model's memory and computational
requirement, we can significantly improve the onboard inference (top energy
efficiency of 0.43 mJ/frame) with no compromise in the quality-of-result vs. a
resource-unconstrained baseline (i.e., full-precision DNN). Field experiments
demonstrate a closed-loop top-notch autonomous navigation capability, with a
heavily resource-constrained 27-gram Crazyflie 2.1 nano-quadrotor. Compared
against the control performance achieved using an ideal sensing setup, onboard
relative pose inference yields excellent drone behavior in terms of median
absolute errors, such as positional (onboard: 41 cm, ideal: 26 cm) and angular
(onboard: 3.7$^{\circ}$, ideal: 4.1$^{\circ}$).Comment: 15 pages, 15 figures, 4 tables. This work has been submitted to the
  IEEE for possible publication. Copyright may be transferred without notice,
  after which this version may no longer be accessibl","Fully Onboard AI-powered Human-Drone Pose Estimation on Ultra-low Power
  Autonomous Flying Nano-UAVs",http://arxiv.org/abs/2103.10873,,,,core
389409070,2021-03-05T00:00:00,"Automatic interpretation of human actions from realistic videos attracts increasing research attention owing to its growing demand in real-world deployments such as biometrics, intelligent robotics, and surveillance. In this research, we propose an ensemble model of evolving deep networks comprising Convolutional Neural Networks (CNNs) and bidirectional Long Short-Term Memory (BLSTM) networks for human action recognition. A swarm intelligence (SI)-based algorithm is also proposed for identifying the optimal hyper-parameters of the deep networks. The SI algorithm plays a crucial role for determining the BLSTM network and learning configurations such as the learning and dropout rates and the number of hidden neurons, in order to establish effective deep features that accurately represent the temporal dynamics of human actions. The proposed SI algorithm incorporates hybrid crossover operators implemented by sine, cosine, and tanh functions for multiple elite offspring signal generation, as well as geometric search coefficients extracted from a three-dimensional super-ellipse surface. Moreover, it employs a versatile search process led by the yielded promising offspring solutions to overcome stagnation. Diverse CNN–BLSTM networks with distinctive hyper-parameter settings are devised. An ensemble model is subsequently constructed by aggregating a set of three optimized CNN–BLSTM​ networks based on the average prediction probabilities. Evaluated using several publicly available human action data sets, our evolving ensemble deep networks illustrate statistically significant superiority over those with default and optimal settings identified by other search methods. The proposed SI algorithm also shows great superiority over several other methods for solving diverse high-dimensional unimodal and multimodal optimization functions with artificial landscapes",Intelligent human action recognition using an ensemble model of evolving deep networks with swarm-based optimization.,,'Elsevier BV',10.1016/j.knosys.2021.106918,,core
478614469,2021-10-09T00:00:00,"Interactive reinforcement learning, where humans actively assist during an
agent's learning process, has the promise to alleviate the sample complexity
challenges of practical algorithms. However, the inner workings and state of
the robot are typically hidden from the teacher when humans provide feedback.
To create a common ground between the human and the learning robot, in this
paper, we propose an Augmented Reality (AR) system that reveals the hidden
state of the learning to the human users. This paper describes our system's
design and implementation and concludes with a discussion on two directions for
future work which we are pursuing: 1) use of our system in AI education
activities at the K-12 level; and 2) development of a framework for an AR-based
human-in-the-loop reinforcement learning, where the human teacher can see
sensory and cognitive representations of the robot overlaid in the real world.Comment: AAAI-FSS 2021 (arXiv:2109.10836","An Augmented Reality Platform for Introducing Reinforcement Learning to
  K-12 Students with Robots",http://arxiv.org/abs/2110.04697,,,,core
478137911,2021-05-01T15:42:12,"Fuzzy Logic is a good model for the human ability to compute words. It is based on the theory of fuzzy set. A fuzzy set is different from a classical set because it breaks the Law of the Excluded Middle. In fact, an item may belong to a fuzzy set and its complement at the same time and with the same or different degree of membership. The degree of membership of an item in a fuzzy set can be any real number included between 0 and 1. This property enables us to deal with all those statements of which truths are a matter of degree. Fuzzy logic plays a relevant role in the field of Artificial Intelligence because it enables decision-making in complex situations, where there are many intertwined variables involved. Traditionally, fuzzy logic is implemented through software on a computer or, even better, through analog electronic circuits. Recently, the idea of using molecules and chemical reactions to process fuzzy logic has been promoted. In fact, the molecular word is fuzzy in its essence. The overlapping of quantum states, on the one hand, and the conformational heterogeneity of large molecules, on the other, enable context-specific functions to emerge in response to changing environmental conditions. Moreover, analog input–output relationships, involving not only electrical but also other physical and chemical variables can be exploited to build fuzzy logic systems. The development of “fuzzy chemical systems” is tracing a new path in the field of artificial intelligence. This new path shows that artificially intelligent systems can be implemented not only through software and electronic circuits but also through solutions of properly chosen chemical compounds. The design of chemical artificial intelligent systems and chemical robots promises to have a significant impact on science, medicine, economy, security, and wellbeing. Therefore, it is my great pleasure to announce a Special Issue of Molecules entitled “The Fuzziness in Molecular, Supramolecular, and Systems Chemistry.” All researchers who experience the Fuzziness of the molecular world or use Fuzzy logic to understand Chemical Complex Systems will be interested in this book","The Fuzziness in Molecular, Supramolecular, and Systems Chemistry",https://core.ac.uk/download/478137911.pdf,'MDPI AG',10.3390/books978-3-03943-179-3,,core
479487443,2021-05-12T00:00:00,"The research field of Embodied AI has witnessed substantial progress in
visual navigation and exploration thanks to powerful simulating platforms and
the availability of 3D data of indoor and photorealistic environments. These
two factors have opened the doors to a new generation of intelligent agents
capable of achieving nearly perfect PointGoal Navigation. However, such
architectures are commonly trained with millions, if not billions, of frames
and tested in simulation. Together with great enthusiasm, these results yield a
question: how many researchers will effectively benefit from these advances? In
this work, we detail how to transfer the knowledge acquired in simulation into
the real world. To that end, we describe the architectural discrepancies that
damage the Sim2Real adaptation ability of models trained on the Habitat
simulator and propose a novel solution tailored towards the deployment in
real-world scenarios. We then deploy our models on a LoCoBot, a Low-Cost Robot
equipped with a single Intel RealSense camera. Different from previous work,
our testing scene is unavailable to the agent in simulation. The environment is
also inaccessible to the agent beforehand, so it cannot count on scene-specific
semantic priors. In this way, we reproduce a setting in which a research group
(potentially from other fields) needs to employ the agent visual navigation
capabilities as-a-Service. Our experiments indicate that it is possible to
achieve satisfying results when deploying the obtained model in the real world.
Our code and models are available at https://github.com/aimagelab/LoCoNav",Out of the Box: Embodied Navigation in the Real World,http://arxiv.org/abs/2105.05873,'Springer Science and Business Media LLC',10.1007/978-3-030-89128-2_5,,core
390127702,2021-04-01T00:00:00,"International audienceSimulating frictional contacts remains a challenging research topic in robotics. Recently, differentiable physics emerged and has proven to be a key element in modelbased Reinforcement Learning (RL) and optimal control fields. However, most of the current formulations deploy coarse approximations of the underlying physical principles. Indeed, the classic simulators lose precision by casting the Nonlinear Complementarity Problem (NCP) of frictional contact into a Linear Complementarity Problem (LCP) to simplify computations. Moreover, such methods deploy non-smooth operations and cannot be automatically differentiated. In this paper, we propose (i) an extension of the staggered projections algorithm for more accurate solutions of the problem of contacts with friction. Based on this formulation, we introduce (ii) a differentiable simulator and an efficient way to compute the analytical derivatives of the involved optimization problems. Finally, (iii) we validate the proposed framework with a set of experiments to present a possible application of our differentiable simulator. In particular, using our approach we demonstrate accurate estimation of friction coefficients and object masses both in synthetic and real experiments",Differentiable simulation for physical system identification,,'Institute of Electrical and Electronics Engineers (IEEE)',10.1109/LRA.2021.3062323,,core
387276892,2021-03-28T00:00:00,"Recent advances in incorporating neural networks into particle filters
provide the desired flexibility to apply particle filters in large-scale
real-world applications. The dynamic and measurement models in this framework
are learnable through the differentiable implementation of particle filters.
Past efforts in optimising such models often require the knowledge of true
states which can be expensive to obtain or even unavailable in practice. In
this paper, in order to reduce the demand for annotated data, we present an
end-to-end learning objective based upon the maximisation of a
pseudo-likelihood function which can improve the estimation of states when
large portion of true states are unknown. We assess performance of the proposed
method in state estimation tasks in robotics with simulated and real-world
datasets.Comment: Accepted in ICRA 202",End-To-End Semi-supervised Learning for Differentiable Particle Filters,http://arxiv.org/abs/2011.05748,,,,core
227186025,,"In this paper we present a new methodology for robot learning that combines ideas from statistical generalization and reinforcement learning. First we apply statistical generalization to compute an approximation for the optimal control policy as defined by training movements that solve the given task in a number of specific situations. This way we obtain a manifold of movements, which dimensionality is usually much smaller than the dimensionality of a full space of movement primitives. Next we refine the policy by means of reinforcement learning on the approximating manifold, which results in a learning problem constrained to the low dimensional manifold. We show that in some situations, learning on the low dimensional manifold can be implemented as an error learning algorithm. We apply golden section search to refine the control policy. Furthermore, we propose a reinforcement learning algorithm with an extended parameter set, which combines learning in constrained domain with learning in full space of parametric movement primitives, which makes it possible to explore actions outside of the initial approximating manifold. The proposed approach was tested for learning of pouring action both in simulation and on a real robotSistemų analizės katedraVytauto Didžiojo universiteta",Applying statistical generalization to determine search direction for reinforcement learning of movement primitives,,'Institute of Electrical and Electronics Engineers (IEEE)',10.1109/HUMANOIDS.2012.6651500,,core
395676178,2021-01-01T00:00:00,"International audienceSimulating frictional contacts remains a challenging research topic in robotics. Recently, differentiable physics emerged and has proven to be a key element in modelbased Reinforcement Learning (RL) and optimal control fields. However, most of the current formulations deploy coarse approximations of the underlying physical principles. Indeed, the classic simulators lose precision by casting the Nonlinear Complementarity Problem (NCP) of frictional contact into a Linear Complementarity Problem (LCP) to simplify computations. Moreover, such methods deploy non-smooth operations and cannot be automatically differentiated. In this paper, we propose (i) an extension of the staggered projections algorithm for more accurate solutions of the problem of contacts with friction. Based on this formulation, we introduce (ii) a differentiable simulator and an efficient way to compute the analytical derivatives of the involved optimization problems. Finally, (iii) we validate the proposed framework with a set of experiments to present a possible application of our differentiable simulator. In particular, using our approach we demonstrate accurate estimation of friction coefficients and object masses both in synthetic and real experiments",Differentiable simulation for physical system identification,https://core.ac.uk/download/395676178.pdf,'Institute of Electrical and Electronics Engineers (IEEE)',10.1109/LRA.2021.3062323,,core
475269103,2021-05-18T00:00:00,"Positioning is a need for many applications related to mapping and navigation either in civilian or military domains. The significant developments in satellite-based techniques, sensors, telecommunications, computer hardware and software, image processing, etc. positively influenced to solve the positioning problem efficiently and instantaneously. Accordingly, the mentioned development empowered the applications and advancement of autonomous navigation. One of the most interesting developed positioning techniques is what is called in robotics as the Simultaneous Localization and Mapping SLAM. The SLAM problem solution has witnessed a quick improvement in the last decades either using active sensors like the RAdio Detection And Ranging (Radar) and Light Detection and Ranging (LiDAR) or passive sensors like cameras. Definitely, positioning and mapping is one of the main tasks for Geomatics engineers, and therefore it's of high importance for them to understand the SLAM topic which is not easy because of the huge documentation and algorithms available and the various SLAM solutions in terms of the mathematical models, complexity, the sensors used, and the type of applications. In this paper, a clear and simplified explanation is introduced about SLAM from a Geomatical viewpoint avoiding going into the complicated algorithmic details behind the presented techniques. In this way, a general overview of SLAM is presented showing the relationship between its different components and stages like the core part of the front-end and back-end and their relation to the SLAM paradigm. Furthermore, we explain the major mathematical techniques of filtering and pose graph optimization either using visual or LiDAR SLAM and introduce a summary of the deep learning efficient contribution to the SLAM problem. Finally, we address examples of some existing practical applications of SLAM in our reality",The simultaneous localization and mapping (SLAM): An overview,,'Interdisciplinary Publishing Academia',10.38094/sgej1027,,core
392158380,2021-03-23T00:00:00,"As the number of the robot's degrees of freedom increases, the implementation
of robot motion becomes more complex and difficult. In this study, we focus on
learning 6DOF-grasping motion and consider dividing the grasping motion into
multiple tasks. We propose to combine imitation and reinforcement learning in
order to facilitate a more efficient learning of the desired motion. In order
to collect demonstration data as teacher data for the imitation learning, we
created a virtual reality (VR) interface that allows humans to operate the
robot intuitively. Moreover, by dividing the motion into simpler tasks, we
simplify the design of reward functions for reinforcement learning and show in
our experiments a reduction in the steps required to learn the grasping motion",Learning 6DoF Grasping Using Reward-Consistent Demonstration,http://arxiv.org/abs/2103.12321,,,,core
432884549,2021-06-17T00:00:00,"In this article, we show that learned policies can be applied to solve legged
locomotion control tasks with extensive flight phases, such as those
encountered in space exploration. Using an off-the-shelf deep reinforcement
learning algorithm, we trained a neural network to control a jumping quadruped
robot while solely using its limbs for attitude control. We present tasks of
increasing complexity leading to a combination of three-dimensional
(re-)orientation and landing locomotion behaviors of a quadruped robot
traversing simulated low-gravity celestial bodies. We show that our approach
easily generalizes across these tasks and successfully trains policies for each
case. Using sim-to-real transfer, we deploy trained policies in the real world
on the SpaceBok robot placed on an experimental testbed designed for
two-dimensional micro-gravity experiments. The experimental results demonstrate
that repetitive, controlled jumping and landing with natural agility is
possible.Comment: Published in IEEE Transactions on Robotics:
  https://ieeexplore.ieee.org/document/9453856 Video:
  https://youtu.be/KQhlZa42fe","Cat-like Jumping and Landing of Legged Robots in Low-gravity Using Deep
  Reinforcement Learning",http://arxiv.org/abs/2106.09357,'Institute of Electrical and Electronics Engineers (IEEE)',10.1109/TRO.2021.3084374,,core
288003301,2022-03-01T00:00:00,"Initially, robots were developed with the aim of making our life easier, carrying
out repetitive or dangerous tasks for humans. Although they were able
to perform these tasks, the latest generation of robots are being designed
to take a step further, by performing more complex tasks that have been
carried out by smart animals or humans up to date. To this end, inspiration
needs to be taken from biological examples. For instance, insects are able
to optimally solve complex environment navigation problems, and many researchers
have started to mimic how these insects behave. Recent interest in
neuromorphic engineering has motivated us to present a real-time, neuromorphic,
spike-based Central Pattern Generator of application in neurorobotics,
using an arthropod-like robot. A Spiking Neural Network was designed and
implemented on SpiNNaker. The network models a complex, online-change
capable Central Pattern Generator which generates three gaits for a hexapod
robot locomotion. Recon gurable hardware was used to manage both
the motors of the robot and the real-time communication interface with the
Spiking Neural Networks. Real-time measurements con rm the simulation
results, and locomotion tests show that NeuroPod can perform the gaits
without any balance loss or added delay.Ministerio de Economía y Competitividad TEC2016-77785-",NeuroPod: a real-time neuromorphic spiking CPG applied to robotics,https://core.ac.uk/download/288003301.pdf,'Elsevier BV',,,core
478617377,2021-01-01T00:00:00,"Autonomy plays a key role for more scalable, precise and economic future robotic space missions. Teleoperated space robotic tasks are affected by the communication delay between the spacecraft and the ground station. In the context of robotics in-space assembly and the PULSAR project, a technical demonstrator of the autonomous assembly of a telescope's primary mirror, a learning-based method for such operation is proposed in this work. Conventional robotics assembly methods usually rely on pre-defined motions and strategies, and are engineered for a single use case. Learning-based approaches allow to use the same method for different geometries with little efforts. In this work, a reinforcement learning environment where an industrial robotic arm performs the assembly operation is modelled. Then, open source software components are used to implement the proposed design and validate it in a simulated environment with a precise physics engine. The experiments in the simulator show that the training converges and the trained reinforcement learning agents are able to successfully perform the assemblies of different peg-in-hole geometries and the parts designed for the PULSAR project. These results make reinforcement learning methods worth considering for future real-world experiments and potential in-space assembly missions",Learning Robust Strategies For In-Space Autonomous Assembly,,,,,core
478867994,2021-09-15T15:44:17,"The Optimal Transport (OT) problem naturally arises in various machine learning problems, where one needs to align data from multiple sources. For example, the training data and application scenarios oftentimes have a domain gap, e.g., the training data is annotated photos collected in the daytime, yet the application scenario is in dark hours. In this case, we need to align the two datasets, so that the annotation information can be shared across them. During my Ph.D. study, I propose scalable algorithms for efficient OT computation, and its novel applications in end-to-end learning.

For OT computation, I consider both discrete cases and continuous cases. For the discrete cases, I develop an Inexact Proximal point method for exact Optimal Transport problem (IPOT) with the proximal operator approximately evaluated at each iteration using projections to the probability simplex. The algorithm (a) converges to exact Wasserstein distance with theoretical guarantee and robust regularization parameter selection, (b) alleviates numerical stability issue, (c) has similar computational complexity to Sinkhorn, and (d) avoids the shrinking problem when apply to generative models. Furthermore, a new algorithm is proposed based on IPOT to obtain sharper Wasserstein barycenter.

For the continuous cases, I propose an implicit generative learning-based framework called SPOT (Scalable Push-forward of Optimal Transport). Specifically, we approximate the optimal transport plan by a pushforward of a reference distribution, and cast the optimal transport problem into a minimax problem. We then can solve OT problems efficiently using primal dual stochastic gradient-type algorithms.

To explore the connections between OT and end-to-end learning, I developed a differentiable top-k operator, and a differentiable permutation step.

For the top-k operation, i.e., finding the k largest or smallest elements from a collection of scores, is an important model component used in information retrieval, machine learning, and data mining. However, if the top-k operation is implemented in an algorithmic way, e.g., using bubble algorithm, the resulting model cannot be trained in an end-to-end way using prevalent gradient descent algorithms. This is because these implementations typically involve swapping indices, whose gradient cannot be computed. Moreover, the corresponding mapping from the input scores to the indicator vector of whether this element belongs to the top-k set is essentially discontinuous. To address the issue, we propose a smoothed approximation, namely the SOFT (Scalable Optimal transport-based diFferenTiable) top-k operator. Specifically, our SOFT top-k operator approximates the output of the top-k operation as the solution of an Entropic Optimal Transport (EOT) problem. The gradient of the SOFT operator can then be efficiently approximated based on the optimality conditions of EOT problem. We apply the proposed operator to the k-nearest neighbors and beam search algorithms, and demonstrate improved performance.

For the differentiable permutation step, I connect optimal transport to a variant of regression problem, where the correspondence between input and output data is not available. Such shuffled data is commonly observed in many real-world problems. Taking flow cytometry as an example, the measuring instruments may not be able to maintain the correspondence between the samples and the measurements. Due to the combinatorial nature of the problem, most existing methods are only applicable when the sample size is small, and limited to linear regression models. To overcome such bottlenecks, we propose a new computational framework -- ROBOT -- for the shuffled regression problem, which is applicable to large data and complex nonlinear models. Specifically, we reformulate the regression without correspondence as a continuous optimization problem. Then by exploiting the interaction between the regression model and the data correspondence, we develop a hypergradient approach based on differentiable programming techniques. Such a hypergradient approach essentially views the data correspondence as an operator of the regression, and therefore allows us to find a better descent direction for the model parameter by differentiating through the data correspondence. ROBOT can be further extended to the inexact correspondence setting, where there may not be an exact alignment between the input and output data. Thorough numerical experiments show that ROBOT achieves better performance than existing methods in both linear and nonlinear regression tasks, including real-world applications such as flow cytometry and multi-object tracking.Ph.D",On Computation and Application of Optimal Transport,https://core.ac.uk/download/478867994.pdf,Georgia Institute of Technology,,,core
430680333,2021-10-31T00:00:00,"Simulation has recently become key for deep reinforcement learning to safely
and efficiently acquire general and complex control policies from visual and
proprioceptive inputs. Tactile information is not usually considered despite
its direct relation to environment interaction. In this work, we present a
suite of simulated environments tailored towards tactile robotics and
reinforcement learning. A simple and fast method of simulating optical tactile
sensors is provided, where high-resolution contact geometry is represented as
depth images. Proximal Policy Optimisation (PPO) is used to learn successful
policies across all considered tasks. A data-driven approach enables
translation of the current state of a real tactile sensor to corresponding
simulated depth images. This policy is implemented within a real-time control
loop on a physical robot to demonstrate zero-shot sim-to-real policy transfer
on several physically-interactive tasks requiring a sense of touch",Tactile Sim-to-Real Policy Transfer via Real-to-Sim Image Translation,http://arxiv.org/abs/2106.08796,,,,core
478156438,2021-01-01T00:00:00,"This doctoral dissertation explores intelligent systems and services for image and video analysis. In view of scientific challenges for developing innovative solutions with a broad social impact, it investigates applications in biomedicine and computer-assisted navigation of visually impaired individuals. In this context, it focuses on machine learning, particularly the investigation of methods to improve the efficiency and the effectiveness of deep artificial neural network architectures, such as the Convolutional Neural Networks (CNNs).In Convolutional Neural Networks (CNNs) the input data can contain uncertainties, such as noise, color and geometric ubiquities, that is naturally propagated from the input layer to the convolution layers of the network affecting the quality of the extracted features. To cope with this problem, a novel pooling operation based on (type-1) fuzzy sets is proposed, named Fuzzy Pooling, which can be used as a drop-in replacement of the current, crisp, pooling layers of CNN architectures. Several experiments using publicly available datasets show that the proposed approach can enhance the classification performance of a CNN.Aiming to improve the effectiveness of CNNs, especially in the context of medical image analysis, a novel architecture named Look Behind Fully Convolutional Neural Network (LB-FCN) is proposed. The architecture is capable of extracting multi-scale image features by using blocks of parallel convolutional layers with different filter sizes. These blocks are connected by look-behind connections, so that the features they produce are combined with features extracted from behind layers, thus preserving the respective information. Furthermore, it has a smaller number of free parameters than conventional CNN architectures, which makes it suitable for training with smaller datasets. This is particularly useful in medical image analysis, since data availability is usually limited, due to ethicolegal constraints. Experiments on publicly available gastrointestinal image datasets show higher classification performance compared to state-of-the-art machine and deep learning methodologies. The architecture is capable of generalizing well even when the training dataset is different than the one on which it is tested. To investigate that, a novel cross-dataset experimental study was performed on various publicly available gastrointestinal tract image datasets, containing images from different modalities, including Wireless Capsule Endoscopy (WCE) and flexible endoscopy. The number of training samples in CNN training is directly linked to their generalization performance. When the training samples are limited, such as in the case of medical images, the generalization performance is negatively affected. A typical approach to mediate this problem is to use data augmentation techniques, which image rotation and translation. While effective, this technique still requires a substantial amount of training samples to be available. To battle this problem, in the context of inflammatory conditions detection in WCE images, a novel approach is presented that uses Generative Adversarial Networks (GANs) to generate artificial images. More specifically the study trained two GANs, one to generate healthy small bowel images and another, images with inflammatory conditions. The images are then used to train a CNN architecture and validate its performance in real images. The results from this study show that the substitution of real with artificially generated endoscopic images for CNN training can be a viable option.While CNNs have a remarkable performance in computer vision problems, usually, they are computationally expensive. This limits their usage in high-end expensive devices with multiple graphical processing units (GPUs). To mediate the problem, a typical approach is to reduce the number of floating-point operations (FLOPs) required for inference, at the expense of generalization performance. In this context, a novel LB-FCN inspired CNN architecture was proposed, named LB-FCN light. The architecture features a relatively low number of free parameters and FLOPs, while managing to maintain high generalization performance. The performance of the network is validated in the problem of staircase detection in indoor and outdoor environments, with application on assisted navigation of visually impaired individuals. The results from the experimental evaluation of LB-FCN light indicate its advantageous performance over the relevant state-of-the-art architectures.The development of easy-to-use machine learning (ML) application frameworks has enabled the development of advanced artificial intelligence (AI) applications with only a few lines of self-explanatory code. However, the deployment of ML algorithms as a service for remote high throughput ML task execution, involving complex data-processing pipelines can still be challenging, especially with respect to production ML use cases. To cope with this issue, a novel system architecture is presented, which enables Algorithm-agnostic, Scalable ML (ASML) task execution for high throughput applications. It aims to provide an answer to the research question of how to design and implement an abstraction framework, suitable for the deployment of end-to-end ML pipelines in a generic and standard way. The architecture manages horizontal scaling, task scheduling, reporting, monitoring and execution of multi-client ML tasks using modular, extensible components that abstract the execution details of the underlying algorithms. Applications of ASML are investigated for the analysis of image streams in the context of medical image analysis and assisted navigation of visually impaired individuals. The results of the experiments performed demonstrate its capacity for parallel, mission critical, task execution. Assistive navigation systems require the development, assessment, and optimization of different algorithms for obstacle detection, recognition, and avoidance, as well as path planning. This is a painstaking and costly process that requires repetitive measurements under stable conditions, which is usually difficult to achieve. To this end, a novel digital twin framework for the simulation and evaluation of assistive navigation systems is presented. The framework can replicate relevant real-life situations, enabling the evaluation and optimization of algorithms through adjustable and cost-effective simulations. The utility and the effectiveness of the framework are demonstrated with an indicative simulation study in the context of a camera-based wearable system for the navigation of visually impaired individuals in an outdoor cultural space.The work presented in this dissertation includes methods with both theoretical and practical impact, that can be used as the basis for further research, and the applications presented can be used as paradigms for applications on different domains, such as telemedicine, robotics, and intelligent transportation systems.Η παρούσα διδακτορική διατριβή διερευνά πρωτότυπα έξυπνα συστήματα και υπηρεσίες ανάλυσης εικόνας και βίντεο. Λαμβάνοντας υπόψη τις επιστημονικές προκλήσεις για την ανάπτυξη καινοτόμων λύσεων με ευρύ κοινωνικό αντίκτυπο, διερευνά εφαρμογές στη βιοϊατρική και την καθοδήγηση ατόμων με προβλήματα όρασης. Σε αυτό το πλαίσιο, επικεντρώνεται στη μηχανική μάθηση, εστιάζοντας στη διερεύνηση μεθόδων για τη βελτίωση της αποδοτικότητας και αποτελεσματικότητας των αρχιτεκτονικών βαθέων τεχνητών νευρικών δικτύων, όπως τα Συνελικτικά Νευρωνικά Δίκτυα (Convolutional Neural Networks, CNN).Τα δεδομένα εισόδου των CNN μπορούν να περιέχουν αβεβαιότητες, όπως θόρυβος, χρώμα και γεωμετρική απροσδιοριστία, που μεταδίδονται από το επίπεδο εισόδου στα συνελικτικά επίπεδα του δικτύου επηρεάζοντας την ποιότητα των εξαγόμενων χαρακτηριστικών. Προκειμένου να αντιμετωπιστεί αυτό το πρόβλημα, προτείνεται μια νέα λειτουργία συγκέντρωσης (pooling) βασισμένη σε ασαφή σύνολα (τύπου-1), με όνομα Fuzzy Pooling, η οποία μπορεί να χρησιμοποιηθεί για την αντικατάσταση των υπαρχόντων επιπέδων pooling των CNN αρχιτεκτονικών. Πειράματα σε δημοσίως διαθέσιμα δεδομένα έδειξαν ότι η χρήση της  προτεινόμενη προσέγγισης μπορεί να χρησιμοποιηθεί για την βελτίωση της απόδοσης ταξινόμησης των CNN.Με στόχο τη βελτίωση της αποτελεσματικότητας των CNN, και ειδικότερα στο πλαίσιο της ανάλυσης ιατρικών εικόνων, προτάθηκε μια νέα αρχιτεκτονική CNN που ονομάζεται Look Behind Fully Convolutional Neural Network (LB-FCN). Η αρχιτεκτονική είναι ικανή να εξαγάγει χαρακτηριστικά πολλαπλών κλιμάκων χρησιμοποιώντας σύνολα (μπλοκ) παράλληλων συνελικτικών στρωμάτων με διαφορετικά μεγέθη φίλτρου. Τα σύνολα αυτά, συνδέονται με οπίσθιες συνδέσεις, με στόχο τον συνδυασμό των παραγόμενων χαρακτηριστικών με τα χαρακτηριστικά  εισόδου, διατηρώντας έτσι τις αντίστοιχες πληροφορίες. Επιπλέον, η αρχιτεκτονική έχει μικρότερο πλήθος ελεύθερων παραμέτρων σε σχέση με συμβατικές αρχιτεκτονικές CNN, γεγονός που επιτρέπει την εκπαίδευσή της με μικρό πλήθος δεδομένων εκπαίδευσης. Αυτό είναι ιδιαίτερα χρήσιμο στην ανάλυση ιατρικών εικόνας, δεδομένου ότι η διαθεσιμότητα δεδομένων εκπαίδευσης είναι συνήθως περιορισμένη, λόγω βιοηθικών και νομικών περιορισμών. Πειράματα σε δημοσίως διαθέσιμα δεδομένα εικόνων του γαστρεντερικού συστήματος, παρουσιάζουν υψηλή απόδοση ταξινόμησης σε σύγκριση με άλλες σύγχρονες προσεγγίσεις. Η αρχιτεκτονική είναι ικανή να γενικεύει καλά ακόμη και όταν το δεδομένα εκπαίδευσης προέρχονται από διαφορετικά σύνολα δεδομένων από αυτά στα οποίο δοκιμάζεται. Σε αυτό το πλαίσιο, πραγματοποιήθηκε πειραματική μελέτη σε πληθώρα δημοσίων διαθέσιμων συνόλων δεδομένων γαστρεντερικού συστήματος, απαρτιζόμενα από εικόνες που έχουν ληφθεί κάνοντας χρήση διαφορετικών ιατρικών οργάνων, όπως ενδοσκοπικής κάψουλας και εύκαμπτου ενδοσκοπίου. Η δυνατότητα γενίκευσης των CNN συνδέεται άμεσα με το διαθέσιμο πλήθος δειγμάτων εκπαίδευσης. Όταν τα δείγματα εκπαίδευσης είναι περιορισμένα, όπως στην περίπτωση ιατρικών εικόνων, η δυνατότητα γενίκευσης επηρεάζεται αρνητικά. Μια τυπική προσέγγιση για τον περιορισμό αυτού του προβλήματος είναι η χρήση τεχνικών επαύξησης δεδομένων, τροποποιώντας τα υπάρχοντα δεδομένα. Αν και η τεχνική αυτή είναι αποτελεσματική και πάλι απαιτείται σημαντικό πλήθος δεδομένων εκπαίδευσης. Για την καταπολέμηση αυτού του προβλήματος, στο πλαίσιο της ανίχνευσης φλεγμονών σε εικόνες που προέρχονται από ενδοσκοπική κάψουλα, παρουσιάζεται μια προσέγγιση που χρησιμοποιεί Παραγωγικά Αντιπαραθετικά Δίκτυα (Generative Adversarial Networks, GAN) για τη δημιουργία συνθετικών εικόνων. Πιο συγκεκριμένα, η μελέτη βασίζεται στην εκπαίδευση δύο GAN, ένα για να την παραγωγή υγιών εικόνων του λεπτού εντέρου και ένα άλλο, για την παραγωγή εικόνων με φλεγμονές. Οι παραγόμενες εικόνες στη συνέχεια χρησιμοποιούνται για την εκπαίδευση ενός CNN με στόχο την αξιολόγηση της αποδοτικότητάς του σε πραγματικές εικόνες. Τα αποτελέσματα αυτής της μελέτης δείχνουν ότι η αντικατάσταση πραγματικών με τεχνητά παραγόμενων ενδοσκοπικών εικόνων για εκπαίδευση στο CNN μπορεί να είναι μια βιώσιμη επιλογή.Η αξιοσημείωτη απόδοση των CNN στον τομέα της υπολογιστικής όρασης, συνήθως, συνοδεύεται από αυξημένο υπολογιστικό κόστος. Αυτό περιορίζει τη χρήση τους σε συσκευές υψηλών υπολογιστικών προδιαγραφών εξοπλισμένες με πολλαπλές κάρτες γραφικών. Για την αντιμετώπιση αυτού του προβλήματος, μια τυπική προσέγγιση είναι η μείωση των απαιτούμενων αριθμητικών πράξεων, σε βάρος της απόδοσης γενίκευσης. Σε αυτό το πλαίσιο, προτάθηκε μια νέα αρχιτεκτονική CNN, εμπνευσμένη από την LB-FCN, με όνομα LB-FCN light. Η αρχιτεκτονική διαθέτει χαμηλό αριθμό ελεύθερων παραμέτρων και πράξεων, ενώ παράλληλα διατηρεί υψηλή απόδοση γενίκευσης. Η απόδοση του δικτύου διερευνήθηκε στο πρόβλημα της ανίχνευσης σκαλών σε εσωτερικούς και εξωτερικούς χώρους, με εφαρμογές στην υποβοηθούμενη πλοήγηση ατόμων με προβλήματα όρασης. Τα αποτελέσματα από την πειραματική αξιολόγηση του LB-FCN light δείχνουν πως απόδοσή του είναι υψηλότερη σε σύγκριση με άλλες, σύγχρονες αρχιτεκτονικές CNNs. Η ανάπτυξη εύχρηστων πλαισίων εφαρμογών μηχανικής μάθησης, δίνει την δυνατότητα ανάπτυξης προηγμένων εφαρμογών τεχνητής νοημοσύνης με μόνο λίγες γραμμές κώδικα. Ωστόσο, η εγκατάσταση αλγορίθμων μηχανικής μάθησης σε απομακρυσμένο περιβάλλον υψηλής απόδοσης, που περιλαμβάνει περίπλοκα επίπεδα επεξεργασίας δεδομένων, εξακολουθεί να είναι δύσκολη, ειδικά όταν τα περιβάλλοντα αυτά προορίζονται για χρήση από επιχειρήσεις. Για την αντιμετώπιση αυτού του προβλήματος, παρουσιάζεται μια νέα αρχιτεκτονική συστήματος, η οποία επιτρέπει την εκτέλεση εργασιών μηχανικής μάθησης για εφαρμογές υψηλής απόδοσης, με όνομα Algorithm-agnostic, Scalable Machine Learning (ASML). Στόχος της αρχιτεκτονικής είναι να δώσει μια απάντηση στο ερευνητικό πρόβλημα της σχεδίας και ανάπτυξης πλαισίου εφαρμογής, κατάλληλο για την ανάπτυξη διεργασιών μηχανικής μάθησης με γενικό και τυποποιημένο τρόπο, ανεξάρτητο του αλγορίθμου μηχανικής μάθησης. Η αρχιτεκτονική διαχειρίζεται την οριζόντια κλιμάκωση, τον προγραμματισμό εργασιών, την αναφορά, την παρακολούθηση και την εκτέλεση εργασιών μηχανικής μάθησης, με δυνατότητα χρήσης από πολλαπλούς χρήστες, χρησιμοποιώντας ανεξάρτητα και επεκτάσιμα στοιχεία που αποκρύπτουν τις λεπτομέρειες εκτέλεσης των υποκείμενων αλγορίθμων. Η δυνατότητες της αρχιτεκτονικής διερευνήθηκαν σε εφαρμογές ανάλυσης ροών εικόνων από ιατρικά δεδομένα και στα πλαίσια της υποβοηθούμενης πλοήγηση ατόμων με προβλήματα όρασης. Τα αποτελέσματα των πειραμάτων που πραγματοποιήθηκαν δείχνουν ότι η αρχιτεκτονική είναι κατάλληλη για παράλληλη χρήση και σε κρίσιμα συστήματα.Τα συστήματα υποβοηθούμενης πλοήγησης απαιτούν την ανάπτυξη, αξιολόγηση και βελτιστοποίηση διαφορετικών αλγορίθμων για την ανίχνευση εμποδίων, την αναγνώριση και την αποφυγή τους, καθώς και τον σχεδιασμό διαδρομών. Η διαδικασία αυτή είναι ιδιαιτέρως επίπονη και δαπανηρή και απαιτεί επαναλαμβανόμενες μετρήσεις υπό σταθερές συνθήκες, κάτι που συνήθως είναι δύσκολο να επιτευχθεί. Για το σκοπό αυτό, παρουσιάζεται ένα πρωτότυπο πλαίσιο εφαρμογής για την προσομοίωση και την αξιολόγηση συστημάτων υποβοήθησης πλοήγησης. Το πλαίσιο αυτό μπορεί να αναπαράγει πραγματικές καταστάσεις, επιτρέποντας την αξιολόγηση και βελτιστοποίηση αλγορίθμων μέσω ρυθμιζόμενων και οικονομικά αποδοτικών προσομοιώσεων. Η χρησιμότητα και η αποτελεσματικότητα του πλαισίου αποδεικνύονται με μια ενδεικτική μελέτη προσομοίωσης στο πλαίσιο ενός φορητού συστήματος που βασίζεται σε κάμερα για την πλοήγηση ατόμων με προβλήματα όρασης σε έναν υπαίθριο χώρο πολιτιστικού ενδιαφέροντος.Το έργο που παρουσιάστηκε στην παρούσα διατριβή περιλαμβάνει μεθόδους με θεωρητικό και πρακτικό αντίκτυπο, οι οποίες μπορούν να χρησιμοποιηθούν ως βάση για περαιτέρω έρευνα. Οι εφαρμογές που παρουσιάζονται μπορούν να χρησιμοποιηθούν ως πρότυπα για εφαρμογές σε διαφορετικούς τομείς, όπως τηλεϊατρική, ρομποτική και έξυπνα συστήματα μετακίνησης",Ευφυή συστήματα και υπηρεσίες για την ανάλυση εικόνων και βίντεο,,Πανεπιστήμιο Θεσσαλίας,,,core
389072519,2021-05-28T00:00:00,"Learning continuously during all model lifetime is fundamental to deploy
machine learning solutions robust to drifts in the data distribution. Advances
in Continual Learning (CL) with recurrent neural networks could pave the way to
a large number of applications where incoming data is non stationary, like
natural language processing and robotics. However, the existing body of work on
the topic is still fragmented, with approaches which are application-specific
and whose assessment is based on heterogeneous learning protocols and datasets.
In this paper, we organize the literature on CL for sequential data processing
by providing a categorization of the contributions and a review of the
benchmarks. We propose two new benchmarks for CL with sequential data based on
existing datasets, whose characteristics resemble real-world applications. We
also provide a broad empirical evaluation of CL and Recurrent Neural Networks
in class-incremental scenario, by testing their ability to mitigate forgetting
with a number of different strategies which are not specific to sequential data
processing. Our results highlight the key role played by the sequence length
and the importance of a clear specification of the CL scenario.Comment: In submissio","Continual Learning for Recurrent Neural Networks: an Empirical
  Evaluation",http://arxiv.org/abs/2103.07492,,,,core
387309355,2021-01-15T00:00:00,"There are many practitioners that create software to buy and sell financial
assets in an autonomous way. There are some digital platforms that allow the
development, test and deployment of trading agents (or robots) in simulated or
real markets. Some of these work focus on very short horizons of investment,
while others deal with longer periods. The spectrum of used AI techniques in
finance field is wide. There are many cases, where the developers are
successful in creating robots with great performance in historical price series
(so called backtesting). Furthermore, some platforms make available thousands
of robots that [allegedly] are able to be profitable in real markets. These
strategies may be created with some simple idea or using complex machine
learning schemes. Nevertheless, when they are used in real markets or with data
not used in their training or evaluation frequently they present very poor
performance. In this paper, we propose a method for testing Foreign Exchange
(FX) trading strategies that can provide realistic expectations about
strategy's performance. This method addresses many pitfalls that can fool even
experience practitioners and researchers. We present the results of applying
such method in several famous autonomous strategies in many different financial
assets. Analyzing these results, we can realize that it is very hard to build a
reliable strategy and many published strategies are far from being reliable
vehicles of investment. These facts can be maliciously used by those who try to
sell such robots, by advertising such great (and non repetitive) results, while
hiding the bad but meaningful results. The proposed method can be used to
select among potential robots, establishes minimal periods and requirements for
the test executions. In this way, the method helps to tell if you really have a
great trading strategy or you are just fooling yourself.Comment: there were some warning, (references not found!), but the references
  are all there. I think it needs to run latex twice!","Is it a great Autonomous FX Trading Strategy or you are just fooling
  yourself",http://arxiv.org/abs/2101.07217,,,,core
467111636,2021-07-20T00:00:00,"In the process of intelligently segmenting foods in images using deep neural
networks for diet management, data collection and labeling for network training
are very important but labor-intensive tasks. In order to solve the
difficulties of data collection and annotations, this paper proposes a food
segmentation method applicable to real-world through synthetic data. To perform
food segmentation on healthcare robot systems, such as meal assistance robot
arm, we generate synthetic data using the open-source 3D graphics software
Blender placing multiple objects on meal plate and train Mask R-CNN for
instance segmentation. Also, we build a data collection system and verify our
segmentation model on real-world food data. As a result, on our real-world
dataset, the model trained only synthetic data is available to segment food
instances that are not trained with 52.2% mask AP@all, and improve performance
by +6.4%p after fine-tuning comparing to the model trained from scratch. In
addition, we also confirm the possibility and performance improvement on the
public dataset for fair analysis. Our code and pre-trained weights are
avaliable online at: https://github.com/gist-ailab/Food-Instance-SegmentationComment: Accepted by UR2021(Ubiquitous Robots 2021) conferenc",Deep Learning based Food Instance Segmentation using Synthetic Data,http://arxiv.org/abs/2107.07191,,,,core
429949797,2021-06-01T00:00:00,"Health information technology can support the development of national learning health and care systems, which can be defined as health and care systems that continuously use data-enabled infrastructure to support policy and planning, public health, and personalisation of care. The COVID-19 pandemic has offered an opportunity to assess how well equipped the UK is to leverage health information technology and apply the principles of a national learning health and care system in response to a major public health shock. With the experience acquired during the pandemic, each country within the UK should now re-evaluate their digital health and care strategies. After leaving the EU, UK countries now need to decide to what extent they wish to engage with European efforts to promote interoperability between electronic health records. Major priorities for strengthening health information technology in the UK include achieving the optimal balance between top-down and bottom-up implementation, improving usability and interoperability, developing capacity for handling, processing, and analysing data, addressing privacy and security concerns, and encouraging digital inclusivity. Current and future opportunities include integrating electronic health records across health and care providers, investing in health data science research, generating real-world data, developing artificial intelligence and robotics, and facilitating public–private partnerships. Many ethical challenges and unintended consequences of implementation of health information technology exist. To address these, there is a need to develop regulatory frameworks for the development, management, and procurement of artificial intelligence and health information technology systems, create public–private partnerships, and ethically and safely apply artificial intelligence in the National Health Service",Health information technology and digital innovation for national learning health and care systems,https://core.ac.uk/download/429949797.pdf,'Elsevier BV',10.1016/S2589-7500(21)00005-4,,core
479906933,2021-11-15T12:07:35,"The presented paper study using augmented reality for converting industrial chemical spraying robot into soap bubbles robot as an interactive game augmented robot. Also, a fuzzy logic control system (FLCS) designed and structed as an artificial intelligence tool for soap bubbles robot, and it’s implemented to programming logic control (automated operating system). Fuzzy system used for predicting level of soap-water mixture inside the robot to conform best robot performance",Using Fuzzy Logic Control System as an Artificial Intelligence Tool to Design Soap Bubbles Robot as a Type of Interactive Games,https://core.ac.uk/download/479906933.pdf,Arab Journals Platform,,,core
158324758,,"Recent advances in the broad field of artificial intelligence (AI) has brought
much excitement and many expectations.
However, there is a strong need to understand
 intelligence, and through understanding it we can help achieve true
machine intelligence, one that is not only able to complete certain difficult tasks but also reason about the world. To study intelligence, we look at ourselves and especially at infants. At a very young age, we can consciously and easily perform tasks that involve understanding of both languages and vision, two of the channels through which we acquire most of our information from the external world.                  
How do we do so? How do children learn their first language?
 In our lab, we believe intelligence and learning should be interactive.
 We learn from interaction with the real world through our five senses.
We also believe a massive number of well-defined labels does not exist for
children. In order to study this idea of unlabeled data and learning through
interaction, for this thesis we implemented a system that enabled a human robot to associate 
 visual information with speech information and to learn
to describe a new object with vocabularies acquired during training. Several
machine learning models were implemented on the iCub humanoid platform. Specifically,
 Gaussian Mixture Models and K-means were implemented for
the vision part of the experiment, and Hidden Markov Models were used for
the speech.U of I Onlyundergraduate senior thesis not recommended for open acces",Language acquisition and object recognition with Bert,,,,,core
479305301,2021-07-01T00:00:00,"The publication presents a picture of modern steelworks that is evolving from steelworks 3.0 to steelworks 4.0. The paper was created on the basis of secondary sources of information (desk research). The entire publication concerns the emerging opportunities for the development of the steel producers to Industry 4.0 and the changes already implemented in the steel plants. The collected information shows the support environment for changes in the steel sector (EU programs), the levels of evolution of steel mills, along with the areas of change in the steel industry and implemented investment projects. The work consists of a theoretical part based on a literature review and a practical part based on case studies. The work ends with a discussion in which the staged and segmented nature of the changes introduced in the analyzed sector is emphasized. Based on the three case studies described in the paper, a comparative analysis was conducted between them. When we tried to compare methods used in the three analyzed steel producers (capital groups): ArcelorMittal, Thyssenkrupp, and Tata Steel Group, it can be seen that in all organizations, the main problem connected with steelworks 4.0 transition is the digitalization of all processes within an organization and in the entire supply chain. This is realized using various tools and methods but they are concentrated on using technologies and methods such as artificial intelligence, drones, virtual reality, full automatization, and industrial robots. The effects are connected to better relations with customers, which leads to an increase in customer satisfaction and the organizations’ profit. The steel industry will undergo further strong changes, bringing it closer to Industry 4.0 because it occupies an important place in the economies of many countries due to the strong dependence of steel producers on the markets of the recipients (steel consumers). Steel is the basic material needed to make many products, and its properties have been valued for centuries. In addition, steel mills with positive economic, social, and environmental aspects are part of the concept of sustainability for industries and economies",Transitioning of Steel Producers to the Steelworks 4.0—Literature Review with Case Studies,,'MDPI AG',10.3390/en14144109,"[{'title': 'Energies', 'identifiers': ['issn:1996-1073', '1996-1073']}]",core
322385964,2021-05-28T00:00:00,"In this work, the target is the coordination problem among the members of a robot soccer team. In order to solve this problem several methods which are extensions of a marketdriven approach are implemented. In this work these approaches are studied and compared in detail. The first developed method was the method with static role assignment. Since it has many drawbacks, a novel market-driven approach was implemented to increase the team success by using the full benefits of collaboration. In this first version, roles are fixed, and the agents are assigned suitable roles according to the available cost functions to increase success, in the current situation. This strategy was quite successful and takes good results in during the matches done by other teams, but there are different teams with different game strategies like in the real life case, so there is a need to change the game strategy (e.g. playing offensive or defensive) according to the opponent team strategy. So the original MarketTeam is extended by the addition of reinforcement-based learning method, which allows the team to learn new strategies, as it plays matches with other teams, and use a dynamic strategy to choose the roles for the players. Later this strategy which uses marketbased cost values and other domain specific values in its state vector is further extended to eliminate the drawbacks, and increase success. The results show that reinforcement learning is a good solution for role assignment problem in the robot soccer domain. However, encoding of the problem into the learner is an important issue. When the configuration space is quite large, the policy may not cover all possible states. As a result, the agent is forced to select random actions and the system performance decreases. The communication problem is not addressed in this work. It is assumed that each agent can broadcast limited amount of data. The controller simply collects available data from any other agent. The data may be noisy. Since, at each frame the communication data is refreshed, the error is not cumulative. The solution can also be used in other highly dynamic environments where it is possible to introduce some reinforcement measures for the team. In the robot soccer domain, the reinforcement measures are the goals scored by either our team or the opponent team",Market-Driven Multi-Agent Collaboration in Robot Soccer Domain,https://core.ac.uk/download/322385964.pdf,'IntechOpen',10.5772/4661,,core
387274292,2021-07-03T00:00:00,"The success of deep reinforcement learning (RL) and imitation learning (IL)
in vision-based robotic manipulation typically hinges on the expense of large
scale data collection. With simulation, data to train a policy can be collected
efficiently at scale, but the visual gap between sim and real makes deployment
in the real world difficult. We introduce RetinaGAN, a generative adversarial
network (GAN) approach to adapt simulated images to realistic ones with
object-detection consistency. RetinaGAN is trained in an unsupervised manner
without task loss dependencies, and preserves general object structure and
texture in adapted images. We evaluate our method on three real world tasks:
grasping, pushing, and door opening. RetinaGAN improves upon the performance of
prior sim-to-real methods for RL-based object instance grasping and continues
to be effective even in the limited data regime. When applied to a pushing task
in a similar visual domain, RetinaGAN demonstrates transfer with no additional
real data requirements. We also show our method bridges the visual gap for a
novel door opening task using imitation learning in a new visual domain. Visit
the project website at https://retinagan.github.io/Comment: International Conference on Robotics and Automation (ICRA) 202",RetinaGAN: An Object-aware Approach to Sim-to-Real Transfer,http://arxiv.org/abs/2011.03148,,,,core
475672898,2021-08-11T00:00:00,"Home automation projects have been developed for some time, having evolved into the socalled smart environments. These environments are characterised by the presence of sets of sensors and actuators, connected in order to respond appropriately and proactively to different situations. The integration of intelligent environments with robots allows for the introduction of additional sensing capabilities, besides performing tasks with greater flexibility and less mechanical complexity than traditional monolithic robots. To endow such environments with truly autonomous behaviours, algorithms must extract semantically meaningful information from whichever sensor data is available. Human activity recognition is one of the most active fields of research within this context. In this project, the design and evaluations of learning techniques for human activity recognition was addressed, considering different sensor modalities. Two types of neural networks, based on combinations of Convolutional Neural Networks to Recurrent Networks with Long Short-Term Memory or Temporal Convolutional Networks, were proposed and evaluated on two public datasets for multimodal activity recognition from videos and inertial sensors. The resulting framework was then introduced to a new dataset, the HWU-USP activities dataset, collected as part of this work, in an actual environment endowed with videos, inertial units, and ambient sensors. This design allowed for assessing the influence of ambient sensors, synchronised to the inertial and video data, to the accuracy of the results, which has proven to be a promising approach. Also, the new dataset provided complex activities with long-term dependencies, evaluated through segment-wise classifiers simulating the results for real-time applications. In a second moment, works were developed on neurophysiological data from primates induced to Parkinsons disease. Those studies ranged from data analysis and classification, using neural networks, to the construction of a computational model of the affected structures within the brain. Although different from the studies on activity recognition and assistive technologies, which were the focus of this thesis, these works were related in the nature of the techniques used, and their results were part of the application scenario developed next. Finally, an application scenario was designed and implemented as a robot simulation, so that the developed module could be evaluated in practical situations. For the behaviour selection mechanism, a bioinspired approach based on computational models of the basal ganglia-thalamus-cortex circuit was evaluated and compared to non-bioinspired approaches based on simple heuristics.Projetos de automação residencial têm sido desenvolvidos há algum tempo, tendo evoluído para os chamados ambientes inteligentes. Esses ambientes são caracterizados pela presença de conjuntos de sensores e atuadores, conectados de forma a responder adequada e proativamente a diferentes situações. A integração de ambientes inteligentes com robôs permite a introdução de capacidades adicionais de sensoriamento, além da realização de tarefas com maior flexibilidade e menor complexidade mecânica do que os robôs monolíticos tradicionais. Para dotar tais ambientes de comportamentos verdadeiramente autônomos, algoritmos devem extrair informações semanticamente significativas de quaisquer dados sensoriais disponíveis. Reconhecimento de atividade humana é um dos campos de pesquisa mais ativos dentro deste contexto. Neste projeto, foi abordado o projeto e avaliação de técnicas de aprendizado para reconhecimento da atividade humana, considerando diferentes modalidades de sensores. Dois tipos de redes neurais, baseadas em combinações de Redes Neurais Convolucionais com Redes Recorrentes com Memória de Curto e Longo Prazo ou Redes Convolucionais Temporais, foram propostas e avaliadas em duas bases de dados públicas para reconhecimento de atividade multimodal de vídeos e sensores inerciais. A estrutura resultante foi então empregada a um novo conjunto de dados, o HWU-USP activities dataset, coletado como parte deste trabalho, em um ambiente real dotado de vídeos, unidades inerciais e sensores ambientais. Foi avaliada a influência dos sensores ambientais, sincronizados aos dados inerciais e de vídeo, na acurácia dos resultados, tendo se mostrado uma abordagem promissora. Além disso, o novo conjunto de dados foi provido de atividades complexas com dependências de longo prazo, avaliadas por meio de classificadores baseados em segmentos de comprimento limitado, simulando os resultados para aplicações de tempo real. Em um segundo momento, foram desenvolvidos trabalhos sobre dados neurofisiológicos de primatas induzidos à doença de Parkinson, indo de análises e classificação dos dados, com uso de redes neurais, até a construção de um modelo computacional das estruturas acometidas dentro do cérebro. Embora distinta dos estudos sobre reconhecimento de atividades e tecnologias assistivas, focos desta tese, esses trabalhos foram relacionados na natureza das técnicas empregadas, e seus resultados fizeram parte do cenário de aplicação desenvolvido em seguida. Por fim, foi projetado e implementado um cenário de aplicação na forma de simulação robótica, de modo que o módulo desenvolvido pudesse ser avaliado em situações práticas. Para o mecanismo de seleção de comportamento, uma abordagem bioinspirada baseada em modelos computacionais do circuito núcleos da base-tálamo-córtex foi avaliada e comparada a abordagens não bioinspiradas baseadas em heurísticas simples",Reconhecimento de atividades e abordagens bioinspiradas para robótica em ambientes inteligentes,,"'Universidade de Sao Paulo, Agencia USP de Gestao da Informacao Academica (AGUIA)'",10.11606/T.55.2021.tde-11082021-112227,,core
419112712,2021-01-01T00:00:00,"Visual navigation is essential for many applications in robotics, from manipulation, through mobile robotics to automated driving. Deep reinforcement learning (DRL) provides an elegant map-free approach integrating image processing, localization, and planning in one module, which can be trained and therefore optimized for a given environment. However, to date, DRL-based visual navigation was validated exclusively in simulation, where the simulator provides information that is not available in the real world, e.g., the robot's position or segmentation masks. This precludes the use of the learned policy on a real robot. Therefore, we present a novel approach that enables a direct deployment of the trained policy on real robots. We have designed a new powerful simulator capable of domain randomization. To facilitate the training, we propose visual auxiliary tasks and a tailored reward scheme. The policy is fine-tuned on images collected from real-world environments. We have evaluated the method on a mobile robot in a real office environment. The training took approximately 30 hours on a single GPU. In 30 navigation experiments, the robot reached a 0.3-meter neighbourhood of the goal in more than 86.7% of cases. This result makes the proposed method directly applicable to tasks like mobile manipulation.Learning & Autonomous Contro",Visual Navigation in Real-World Indoor Environments Using End-to-End Deep Reinforcement Learning,,'Institute of Electrical and Electronics Engineers (IEEE)',10.1109/LRA.2021.3068106,,core
478917255,2021-06-08T00:00:00,"Substrate independence and mind-body functionalism claim that thinking does not depend on any particular kind of physical implementation. But real-world information processing depends on energy and energy depends on material substrates. Biological evidence for these claims comes from ecology and neuroscience, while computational evidence comes from neuromorphic computing and deep learning. Attention to energy requirements undermines the use of substrate independence to support claims about the feasibility of artificial intelligence, the moral standing of robots, the possibility that we may be living in a computer simulation, the plausibility of transferring minds into computers, and the autonomy of psychology from neuroscience",Energy Requirements Undermine Substrate Independence and Mind-Body Functionalism,,,,,core
365287462,2020-12-22T08:00:00,"As governments and private companies alike race to achieve the vision of a smart city — where artificial intelligence (AI) technology is used to enable self-driving cars, cashier-less shopping experiences and connected home devices from thermostats to robot vacuum cleaners — advancements are being made in both software and hardware to enable increasingly real-time, accurate inference at the edge. One hardware solution adopted for this purpose is the LiDAR sensor, which utilizes infrared lasers to accurately detect and map its surroundings in 3D. On the software side, developers have turned to artificial neural networks to make predictions and recommendations with high accuracy. These neural networks have the potential, particularly run on purpose-built hardware such as GPUs and TPUs, to make inferences in near real-time, allowing the AI models to serve as a usable interface for real-world interactions with other AI-powered devices, or with human users. This paper aims to example the joint use of LiDAR sensors and AI to understand its importance in smart city environments",SJSU ScholarWorks,LiDAR Object Detection Utilizing Existing CNNs for Smart Cities,,,,core
365390124,2020-11-16T00:00:00,"International audienceThe temporal evolution of individual grip force profiles of a novice using a robotic system for minimally invasive endoscopic surgery is analyzed on the basis of thousands of individual sensor data recorded in real time through a wearable wireless sensor glove system. The spatio-temporal grip force profiles from specific sensor locations in the dominant hand performing a four-step pick-and-drop simulator task reveal skill-relevant differences in force deployment by the small finger (fine grip force control) and the middle finger (gross grip force contribution) by comparison with the profiles of a highly proficient expert. Cross-disciplinary insight from systems neuroscience, cognitive behavioral science, and robotics, with implications for biologically inspired AI for human-robot interaction, highlight the functional significance of spatio-temporal grip force profiling",HAL CCSD,Wearable wireless biosensors for spatiotemporal grip force profiling in real time,,,,core
333595120,2020-08-26T00:00:00,"Cílem práce je návrh a implementace prototypu řídícího programu pro ovládání mobilního robotu, který bude umožňovat interakci člověka s robotem pomocí gest. Pro práci byl zvolen sensor Microsoft Kinect. Sledování gest je založeno na strojovém učení s využitím umělých neuronových sítí. Před tvorbou řídicího programu byla provedena analýza použitého sensoru a použitých metod. Implementace byla inspirována rešerší prací, které se věnovali rozlišování gest s využitím neuronových sítí. V práci je popsán návrh a implementace řídicího programu, který může být ovládán sedmi různými gesty, a testování přesnosti použitých metod. Výsledkem práce je funkční prototyp řídícího programu, který není integrován s robotem.The aim of this thesis is design and implementation of prototype program for gesture control of mobile robot, which will enable interaction between human and robot. Sensor Microsoft Kinect was chosen for this work. Gesture recognition is based on machine learning with the use of artificial neural networks. Prior to the program development a analysis of used sensor and methods was carried out. Implementation was inspired by several papers focused on gesture recognition based on neural networks. In the thesis we introduce the design and implementation of a program that can be controlled by seven different gestures. Furthermore the accuracy of the used methods is tested. The result of the thesis is a functional prototype of control program, that is not intergrated with a real robot",Czech Technical University in Prague. Computing and Information Centre.,Gesture Control of Mobile Robot,,,,core
357291093,2020-04-02T00:00:00,". Abstract Design for manufacturing is often difficult for mechanical parts since significant manufacturing knowledge is required to adjust part designs for manufacturability. The traditional trial and error approach usually leads to expensive iterations and compromises the quality of the final design. The authors believe the appropriate way to handle product design for manufacturing problems is not to formulate a large design problem that exhaustively incorporates design and manufacturing issues, but to separate the design and manufacturing activities and provide support for collaboration between engineering teams. In this paper, the Collaborative Multidisciplinary Decision-making Methodology (CMDM) is used to solve a product design and manufacturing problem. First, the compromise Decision Support Problem is used as a mathematical model of each engineering teams&apos; design decisions and as a medium for information exchange. Second, game theoretic principles are employed to resolve couplings or interactions between the teams&apos; decisions. Third, design capability indices are used to maintain design freedom at the early stages of product realization in order to accommodate unexpected downstream design changes. A plastic robot arm design and manufacturing scenario is presented to demonstrate the application of this methodology and its effectiveness for solving a complex design for manufacturing problem in a streamlined manner, with minimal expensive iterations. Keywords: Collaborative Design, Design for Manufacturing, Game Theory, and Multidisciplinary Decision Making 2 Design for manufacturing Concurrent engineering involves separating product realization activities so that design activities can be executed independently while simultaneously incorporating relevant information from downstream domains such as manufacturing, assembly, or recycling  Alternatively, a manufacturing team that understands the purpose of a design and its functional requirements may be more capable of adjusting it to facilitate manufacturing. The authors believe the appropriate way to handle complex product design problems such as DfM is not to formulate large design problems but to support cooperation and collaboration between multidisciplinary engineering teams. Towards this end, the Collaborative Multidisciplinary Decision-making Methodology (CMDM) is established (Xiao et al. 2005)  to enable collaborative decision making between design and manufacturing teams through &apos;collaboration by separation&apos;. Separation signifies that the responsibility for DfM is transferred from the design team to the downstream manufacturing team; whereas, collaboration signifies that satisfactory systems-level solutions are coordinated with minimal information exchange and iteration. Unlike many mathematical multidisciplinary optimization (MDO) approaches (Balling and SobieszczanskiSobieski 1996 1. Exchanging Information. The information required for decision-making in an activity must be transferred completely from one team to another, and the recipient teams should be able to understand the team&apos;s intentions without requiring additional information flows or causing iterations. The compromise Decision Support Problem (DSP)  2. Accommodating interactions between activities. Some activities in a DfM process may be coupled, such that each design team makes decisions that affect the decisions of other teams. Game theory is used in the CMDM to model different degrees of collaboration and manage interactions between engineering teams, with little or no expensive, systems-level iterations. 3. Maintaining feasible and satisfactory overall designs. When design activities are separated, design teams must make decisions without full knowledge of their impact on downstream activities. If single point solutions are exchanged, downstream designers are prevented from adjusting designs for feasibility or satisfactory local performance, and iterations often ensue. With set-based approaches, however, ranges or sets of solutions are shared and gradually narrowed during the design process, thereby reducing or eliminating the need for global, systems-level iterations  In a product realization problem, the dependencies between any two activities, such as designmanufacturing, design-design, or manufacturing-manufacturing, may be interactive or sequential. Game theory is used to resolve the interactive couplings and design capability indices are used to handle the sequential relationships. The authors believe the sequential relationships are more significant in DfM problems, mostly due to the upstream/downstream nature of the designmanufacturing relationship. As shown in  &lt;Figure 1 goes about here&gt; Collaborative multidisciplinary decision making methodology The CMDM is implemented in three steps: Step 1 Representing decision making information in a compromise DSP which serves as an information medium to eliminate iterations caused by information exchange and communication; Step 2 Representing cooperation styles among engineering teams with game theoretic protocols to eliminate iterations caused by interdisciplinary interactions; and Step 3 Reformulating the compromise DSPs using design capability indices for finding superior ranged set of solutions that eliminate or reduce costly iterations caused by unexpected downstream requirements and constraints. Step 1, modeling product realization activities using compromise DSPs In order to resolve the first challenge, that of exchanging information, Section 1, a compromise Decision Support Problem, DSP, is used. A compromise DSP is a multi-objective decision modela hybrid formulation based on mathematical programming and goal programming -that is used to find the values of design variables that satisfy a set of constraints and achieve a set of conflicting goals as closely as possible  For a given product realization activity, a compromise DSP is capable of representing a team&apos;s decision-making knowledge, as well as the design rationale underlying its decision. A team&apos;s decision is represented with a feasible design space, a set of design objectives, and a tradeoff strategy between these design objectives. As shown in  The compromise DSP resolves the first challenge of exchanging information, but it does not address the second challenge of enabling the separation of activities. There are three possible relationships between any two compromise DSPs; they may be solved concurrently, sequentially, or as coupled problems. Given the disk brakes in a passenger vehicle as an example, there is no direct information exchange between the brake design and exhaust system design. From a decisionmaking perspective, the two compromise DSPs (brake and exhaust system) do not share any unknown variables. They can be solved concurrently, and the solution remains the same regardless of the teams&apos; cooperation styles. Meanwhile, the brake pad cannot be designed without knowledge of the rotor design team&apos;s results, whereas the rotor has to be designed with knowledge of the geometric shape, surface finish, and other details of the brake pad. This situation is reflected as shared variables between the rotor and brake pad design compromise DSPs. Neither compromise 6 DSP can be solved independently, and the result is always affected by the two design teams&apos; cooperation styles, namely, which team solves its compromise DSP first. Game theoretic protocols are used to address the second challenge of separating the coupled activities. In addition, a manufacturing team must design the fixtures, determining the processing parameters based on the final rotor and brake pad designs. The manufacturing compromise DSP includes variables that are determined only by solving the design compromise DSPs. This is a sequential process and the teams&apos; cooperation styles do not affect the solution. However, the downstream manufacturing team may need to modify the design, causing potential iterations. Design capability indices are used to address this third challenge. Since design and manufacturing activities are separated and the responsibility for DfM is transferred from the upstream design team to the downstream manufacturing team, the third challenge becomes more significant in this study. Step 2, representing cooperation styles using game theory The second challenge is resolving couplings between activities. Traditionally, a trial and error approach is used to solve coupled compromise DSPs. Since a team has to make assumptions about another team&apos;s decisions to initiate the trial and error process, this traditional approach may not guarantee consensus (convergence) and usually fails to achieve superior results. Game theory facilitates interaction among multiple engineers without integrating a product realization process into a single large optimization problem or causing iterations. There are three game protocols representing different types of interactions between teams (or players in game theory terminology): cooperative, noncooperative, and leader/follower. Rao and colleagues  In a leader/follower game, the leader makes a set of rational decisions by predicting the A is a subset of X A which must be determined using information from team B, and x B B must be determined using  The feasibility of the solution is ensured by using a BRC to predict the follower&apos;s behavior. Generally, a leader-follower game protocol facilitates collaborative decision making without requiring iteration, hence the coupled activities can be accomplished separately. This solves the second challenge. Step 3, maintaining design freedom using design capability indices The third challenge is to eliminate, or at least reduce, costly iterations between upstream and downstream activities by having the upstream team identify ranges of design variables, rather than single point values, that are as broad as possible without deviating from a desired range of 8 performance, as shown in  &lt;Figure 3 goes about here&gt; As presented in  In the design variable set, X, if any design variable is discrete, say x j , the location and deviation of the performance measures have to be conservatively estimated using: In many cases, calculating the min/max values of a performance measure requires exhaustive search, but the performance range estimated in this manner will cover all the possible values even though they are not continuous. If a performance variable is discrete, the design capability indices are not applicable. Given that all the performance variables are continuous, design capability indices are embedded into the compromise DSP by formulating the design goals using C dk , adding constraints C dk ≥1, and formulating the deviation function to maximize the overachievements of C dk . Moreover, the constraints are re-formulated using Equation  Clearly, constraint g k (X) must be differentiable. If any design variable is discrete, the constraints can be calculated using Equation (4). The bounds of design variables are still formulated using constant values. The resulting compromise DSP is shown in  &lt;Figure 4 goes about here&gt; In  Step 1, the challenge is to provide a method for exchanging information, and it is solved by representing the decision making information in compromise DSPs. In Step 2, the challenge is accommodating interactions between activities, and it is addressed with game theory. In Step 3, the challenge is to maintain feasible and satisfactory overall designs, and it is addressed by reformulating the compromise DSPs using design capability indices. The CMDM provides a normative framework that facilitates collaborative product realization by separating the decision making activities. A robot arm design and manufacturing scenario The authors have developed a distributed product realization environment called the Rapid Tooling  &lt;Figure 5 goes about here&gt; A design team, a rapid tooling team, and an injection molding team are assigned to this task. Correspondingly, the product realization process is partitioned into three activities, as shown in  In this scenario, DfM includes not only adjusting the geometric shape, but also the entire rapid tooling activity. Designing the mold pairs requires knowledge about rapid tooling; hence, it is very difficult for the design team. Since the robot arm is designed without knowledge of the downstream manufacturing process, the design team will have to modify its design based on feedback from manufacturing experts. For a simple product realization process like this one, it is possible to collect all the manufacturing related information and formulate a large design problem to solve all the design variables, i.e., robot arm geometry shape, mold geometry shape, and some manufacturing parameters, like that in Concurrent Engineering. For complex real-world problems, this implies a design problem containing large numbers of design variables and complex analyses; therefore it is not practical to solve as a single problem. The traditional approach to solving a DfM problem is a trial and error approach, which will cause extensive information exchange and iteration. In this example, all three of the challenges addressed by the CMDM exist. Information exchange is required between these activities (challenge 1). Rapid tooling and injection molding are coupled activities; thus iterations exist between them (challenge 2). The geometric shape of the 12 robot arm may have to be modified in order to fabricate the batch with given time and cost; this forces the upstream design team to redesign the geometry (challenge 3). Engineering teams&apos; compromise DSPs The first step of the CMDM is modeling each activity as a standard compromise DSP as shown in  Three design goals are determined based on the customer requirements: (i) the maximum deformation under working load should be as close to 0.5mm as possible, (ii) the maximum von Mises stress under working load should be as close to 6MPa as possible, and (iii) the weight of the robot arm should be as close to 3.5g as possible. The design compromise DSP is shown in  Please note at this step, no design capability indices or game protocol is yet involved. Mathematical equations for deformation, stress, and weight can be found in (Xiao 2003). &lt;Figure 6 goes about here&gt; The rapid tooling team designs the injection mold halves,  On the other hand, the mold life determines the number of mold halves, N m , that must be built in the rapid tooling activity, which thus affects all of the process parameters in this activity. Therefore, injection molding and rapid tooling are coupled activities. As shown in  If we combine all compromise DSPs into one problem with a variable set that includes all of the variables of design, rapid tooling, and injection molding and an objective set that includes all of the objectives with the same weights, the results are shown in  = 9.25mm, t = 3.10mm. In this case, all design goals achieve their target values and the overall deviation is 0. Then, the rapid tooling and injection molding teams make decisions based on this result. Since these two activities are coupled, the rapid tooling team assumes a value of ML, and expects to acquire converged results after several iterations. Unfortunately in this case, rapid tooling and injection molding teams&apos; solutions do not converge. The reason is the design team makes decisions only considering its own design goals; hence the thickness of robot arm t is too large and the mold life becomes so short that the rapid tooling team must build 10 pairs of molds. This violates the constraints of time and cost. Therefore, the product design must be modified. For simplicity, the intermediate results are not listed here. After several rounds of iterations, the converged results from the traditional approach are as shown in  In the trial and error process, iterations happen not only between the coupled rapid tooling and injection molding activities, but also with the upstream design activity. Furthermore, the number and styles of iterations are affected by some unpredictable or uncontrollable factors, such as the teams&apos; experience, and the initial values the teams choose to start the iterative process. So far, this case has demonstrated the difficulties of DfM, and why the traditional approach cannot guarantee the superiority of the final result. By using the CMDM to separate the activities, we expect to change the process shown in  &lt;Table 1 goes about here&gt; &lt;Table 2 goes about here&gt; Resolving couplings using Leader/Follower protocol Since the design team&apos;s decision is not coupled with the decisions of either of the manufacturing teams, as shown in  when LT = 2 mils (7) ML=1696. 10-135.46d-311.63t+423.03Θ +61.17(d-8.11  et al. 1996), which is fundamentally different from using them to approximate the BRCs. Beyond predicting a player&apos;s behavior using its BRC, game protocols also govern issues such as the sequence of the players&apos; decision making activities and control over specific variables. All of these factors are determined by the players&apos; cooperation styles. If the injection molding team is selected as the leader, the BRC T is: When 150&gt; ML ≥75, two pairs of molds have to be build, thus LT = 8 mils has to be selected in order to meet the time constraint. Here, we do not consider the situation that several pairs of molds can be built simultaneously in an SLA3500 machine. It can also be observed from BRC T that Θ remains 0 because the rapid tooling team does not know how the draft angle will affect mold life, and strives to reduce surface roughness, SF, of the robot arm which is achieved with Θ = 0. Obviously, the injection molding team will be unable to eject the parts with a zero draft angle. This is the main reason why the traditional trial and error approach does not converge between the rapid tooling and injection molding teams. Compromise DSP for a ranged set of decisions As described in Section 1, the third step is reformulating the compromise DSPs using design capability indices. In the design activity, all the design variables are continuous; therefore the 16 locations and deviations of the performance variables are calculated using Equation (2). The design team&apos;s compromise DSP for a ranged set of decisions is shown in  &lt;Table 5 goes about here&gt; &lt;Figure 14 goes about here&gt; In  The CMDM is especially useful in the early stages of product design, when little is known about the product and approximating a set of correct designs is much more efficient than conducting rounds and rounds of guessing and correcting. The advantage of the CMDM in DfM is that design and manufacturing activities are separated systematically; hence the product realization activities are accomplished in a more streamlined process as shown in  18 Design freedom in the process The reason the CMDM results in more superior results than the traditional trial and error approach is the design freedom in the product realization process. A design freedom metric is presented in  where n is the number of performance measures of the system. For the i th performance measure, TR i is the target range, PR i is the feasible performance range, and PR i,initial is the initial feasible performance range. In  At the second row, the performance ranges and design freedom at the initial state of this product realization process are listed. The initial design freedom is 0.734, which is smaller than 1 due to the natural limitation of this process and the couplings between activities. In the trial and error process, when the design team makes a specific decision, design freedom is quantified as 0.368, shown at the third row of  Step 3. The injection molding team finally makes its decision at Step 4. It is observed that when the design team makes a ranged set of decisions, design freedom is 0.504 at Step 2. At this moment of the product realization process, the rapid tooling and injection molding teams can  Closure In this paper, the idea of collaboration by separation is tested in product DfM problems. The CMDM is used to enable the separation without causing costly information exchange and iterations. Generally, the compromise DSP is used as an information medium to separate the activities at the information communication level, game theoretical principles separate the coupled activities, and design capability indices separate upstream and downstream activities. The robot arm design and manufacturing process demonstrates that by using the CMDM, a complex product realization process is implemented in a streamlined manner, with each engineering team focusing on its areas of expertise. With the CMDM, final results are obtained with fewer iterations between design teams and significantly less deviation from target performance, relative to using the traditional trial and error approach",,Sobieszczanski-Sobieski and Haftka,,,,core
323255279,2020-05-28T00:00:00,"Deep Reinforcement Learning has been successfully applied in various computer
games [8]. However, it is still rarely used in real-world applications,
especially for the navigation and continuous control of real mobile robots
[13]. Previous approaches lack safety and robustness and/or need a structured
environment. In this paper we present our proof of concept for autonomous
self-learning robot navigation in an unknown environment for a real robot
without a map or planner. The input for the robot is only the fused data from a
2D laser scanner and a RGB-D camera as well as the orientation to the goal. The
map of the environment is unknown. The output actions of an Asynchronous
Advantage Actor-Critic network (GA3C) are the linear and angular velocities for
the robot. The navigator/controller network is pretrained in a high-speed,
parallel, and self-implemented simulation environment to speed up the learning
process and then deployed to the real robot. To avoid overfitting, we train
relatively small networks, and we add random Gaussian noise to the input laser
data. The sensor data fusion with the RGB-D camera allows the robot to navigate
in real environments with real 3D obstacle avoidance and without the need to
fit the environment to the sensory capabilities of the robot. To further
increase the robustness, we train on environments of varying difficulties and
run 32 training instances simultaneously. Video: supplementary File / YouTube,
Code: GitHubComment: 7 pages, repor",,"Deep Reinforcement learning for real autonomous mobile robot navigation
  in indoor environments",http://arxiv.org/abs/2005.13857,,,core
387292687,2020-12-11T00:00:00,"Sensory feedback is essential for the control of soft robotic systems and to
enable deployment in a variety of different tasks. Proprioception refers to
sensing the robot's own state and is of crucial importance in order to deploy
soft robotic systems outside of laboratory environments, i.e. where no external
sensing, such as motion capture systems, is available.
  A vision-based sensing approach for a soft robotic arm made from fabric is
presented, leveraging the high-resolution sensory feedback provided by cameras.
No mechanical interaction between the sensor and the soft structure is required
and consequently, the compliance of the soft system is preserved. The
integration of a camera into an inflatable, fabric-based bellow actuator is
discussed. Three actuators, each featuring an integrated camera, are used to
control the spherical robotic arm and simultaneously provide sensory feedback
of the two rotational degrees of freedom. A convolutional neural network
architecture predicts the two angles describing the robot's orientation from
the camera images. Ground truth data is provided by a motion capture system
during the training phase of the supervised learning approach and its
evaluation thereafter.
  The camera-based sensing approach is able to provide estimates of the
orientation in real-time with an accuracy of about one degree. The reliability
of the sensing approach is demonstrated by using the sensory feedback to
control the orientation of the robotic arm in closed-loop.Comment: This work has been submitted to the Frontiers Journal for
  publication. Copyright may be transferred without notice, after which this
  version may no longer be accessibl",,A Vision-based Sensing Approach for a Spherical Soft Robotic Arm,http://arxiv.org/abs/2012.06413,,,core
334868855,2020-03-02T00:00:00,"Visual navigation tasks in real-world environments often require both
self-motion and place recognition feedback. While deep reinforcement learning
has shown success in solving these perception and decision-making problems in
an end-to-end manner, these algorithms require large amounts of experience to
learn navigation policies from high-dimensional data, which is generally
impractical for real robots due to sample complexity. In this paper, we address
these problems with two main contributions. We first leverage place recognition
and deep learning techniques combined with goal destination feedback to
generate compact, bimodal image representations that can then be used to
effectively learn control policies from a small amount of experience. Second,
we present an interactive framework, CityLearn, that enables for the first time
training and deployment of navigation algorithms across city-sized, realistic
environments with extreme visual appearance changes. CityLearn features more
than 10 benchmark datasets, often used in visual place recognition and
autonomous driving research, including over 100 recorded traversals across 60
cities around the world. We evaluate our approach on two CityLearn
environments, training our navigation policy on a single traversal. Results
show our method can be over 2 orders of magnitude faster than when using raw
images, and can also generalize across extreme visual changes including day to
night and summer to winter transitions.Comment: Preprint version of article accepted to ICRA 202",,"CityLearn: Diverse Real-World Environments for Sample-Efficient
  Navigation Policy Learning",http://arxiv.org/abs/1910.04335,,,core
357617392,2020-05-01T00:00:00,"Abstract. For many reasons, it is desirable to use robots in courses such as introductory computer science, artificial intelligence, and cognitive science, yet the knowledge normally required by students to make effective use of these tools is often prohibitive in such courses with well established curricula. We have developed a user interface that allows students with no prior experience or training in robotics to experiment with behavior networks in real robots, and then brings them down through the software and then the hardware involved. The interface is still in the early stages of development, and has been tested somewhat in a Cognitive Science course, but more widespread use is expected",,Teaching Bottom-Up AI from the Top Down,,,,core
386380658,2020-12-01T08:00:00,"With the rise of (semi)autonomous vehicles and continuum robotics technology and applications, there has been an increasing interest in controller and haptic interface designs. The presence of nonlinearities in the vehicle dynamics is the main challenge in the selection of control algorithms for real-time regulation and tracking of (semi)autonomous vehicles. Moreover, control of continuum structures with infinite dimensions proves to be difficult due to their complex dynamics plus the soft and flexible nature of the manipulator body. The trajectory tracking and control of automobile and robotic systems requires control algorithms that can effectively deal with the nonlinearities of the system without the need for approximation, modeling uncertainties, and input disturbances. Control strategies based on a linearized model are often inadequate in meeting precise performance requirements. To cope with these challenges, one must consider nonlinear techniques. Nonlinear control systems provide tools and methodologies for enabling the design and realization of (semi)autonomous vehicle and continuum robots with extended specifications based on the operational mission profiles. This dissertation provides an insight into various nonlinear controllers developed for (semi)autonomous vehicles and continuum robots as a guideline for future applications in the automobile and soft robotics field. A comprehensive assessment of the approaches and control strategies, as well as insight into the future areas of research in this field, are presented.First, two vehicle haptic interfaces, including a robotic grip and a joystick, both of which are accompanied by nonlinear sliding mode control, have been developed and studied on a steer-by-wire platform integrated with a virtual reality driving environment. An operator-in-the-loop evaluation that included 30 human test subjects was used to investigate these haptic steering interfaces over a prescribed series of driving maneuvers through real time data logging and post-test questionnaires. A conventional steering wheel with a robust sliding mode controller was used for all the driving events for comparison. Test subjects operated these interfaces for a given track comprised of a double lane-change maneuver and a country road driving event. Subjective and objective results demonstrate that the driver’s experience can be enhanced up to 75.3% with a robotic steering input when compared to the traditional steering wheel during extreme maneuvers such as high-speed driving and sharp turn (e.g., hairpin turn) passing.  Second, a cellphone-inspired portable human-machine-interface (HMI) that incorporated the directional control of the vehicle as well as the brake and throttle functionality into a single holistic device will be presented. A nonlinear adaptive control technique and an optimal control approach based on driver intent were also proposed to accompany the mechatronic system for combined longitudinal and lateral vehicle guidance. Assisting the disabled drivers by excluding extensive arm and leg movements ergonomically, the device has been tested in a driving simulator platform. Human test subjects evaluated the mechatronic system with various control configurations through obstacle avoidance and city road driving test, and a conventional set of steering wheel and pedals were also utilized for comparison. Subjective and objective results from the tests demonstrate that the mobile driving interface with the proposed control scheme can enhance the driver’s performance by up to 55.8% when compared to the traditional driving system during aggressive maneuvers. The system’s superior performance during certain vehicle maneuvers and approval received from the participants demonstrated its potential as an alternative driving adaptation for disabled drivers. Third, a novel strategy is designed for trajectory control of a multi-section continuum robot in three-dimensional space to achieve accurate orientation, curvature, and section length tracking. The formulation connects the continuum manipulator dynamic behavior to a virtual discrete-jointed robot whose degrees of freedom are directly mapped to those of a continuum robot section under the hypothesis of constant curvature. Based on this connection, a computed torque control architecture is developed for the virtual robot, for which inverse kinematics and dynamic equations are constructed and exploited, with appropriate transformations developed for implementation on the continuum robot. The control algorithm is validated in a realistic simulation and implemented on a six degree-of-freedom two-section OctArm continuum manipulator. Both simulation and experimental results show that the proposed method could manage simultaneous extension/contraction, bending, and torsion actions on multi-section continuum robots with decent tracking performance (e.g. steady state arc length and curvature tracking error of 3.3mm and 130mm-1, respectively). Last, semi-autonomous vehicles equipped with assistive control systems may experience degraded lateral behaviors when aggressive driver steering commands compete with high levels of autonomy. This challenge can be mitigated with effective operator intent recognition, which can configure automated systems in context-specific situations where the driver intends to perform a steering maneuver. In this article, an ensemble learning-based driver intent recognition strategy has been developed. A nonlinear model predictive control algorithm has been designed and implemented to generate haptic feedback for lateral vehicle guidance, assisting the drivers in accomplishing their intended action. To validate the framework, operator-in-the-loop testing with 30 human subjects was conducted on a steer-by-wire platform with a virtual reality driving environment. The roadway scenarios included lane change, obstacle avoidance, intersection turns, and highway exit. The automated system with learning-based driver intent recognition was compared to both the automated system with a finite state machine-based driver intent estimator and the automated system without any driver intent prediction for all driving events. Test results demonstrate that semi-autonomous vehicle performance can be enhanced by up to 74.1% with a learning-based intent predictor. The proposed holistic framework that integrates human intelligence, machine learning algorithms, and vehicle control can help solve the driver-system conflict problem leading to safer vehicle operations",Clemson University Libraries,Nonlinear Modeling and Control of Driving Interfaces and Continuum Robots for System Performance Gains,https://core.ac.uk/download/386380658.pdf,,,core
343446702,2020-02-20T00:00:00,"Artificial intelligence (AI) is intelligence demonstrated by machines, in contrast to the natural intelligence demonstrated by humans. Examples of AI research include reasoning, knowledge representation, planning, learning, natural language processing, perception, and the ability to move and manipulate objects, which is usually regarded as intelligent control. In recent years, the applications of AI to robotics have experimented with exponential growth. AI plays a crucial role in the path planning of robots, allowing fast responses to changes in complex environments. It also plays a leading role in modeling and intelligent control of robots by allowing a more complex feedback analysis, self-tuning applications, and on-the-fly adaptation to environmental changes.



Changing industrial environments like flexible manufacturing facilities and automated warehouses where robots are intended to work side by side with humans are benefiting directly from advancements in complex path planning and autonomous decision making based on AI-powered algorithms. On the consumer side, applications like cleaning robots and delivery robots are also becoming part of our daily lives. The implementation of AI-powered path planning and control algorithms drastically improves the efficiency and practicality of these robots, as the environments in which these robots must operate is highly dynamic and needs constant adaptation.



This Research Topic is organized under the section “Robotic Control Systems” within Frontiers in Robotics and AI. The first article by Tan et al. is focused on designing mechanisms and algorithms for robotics, which serves as a platform for path planning and control. Current robot designs have been taking inspiration from games and entertainment artifacts (GEAs). However, there is a lack of systematic and general processes for implementing a GEA-inspired design in robotics. In this article, a systematic robot design paradigm is proposed based on the inspiration of GEAs. Both problem-driven and solution-driven processes can be followed to make use of analogies of GEAs so that robotic solutions can be obtained for real-world problems. The application of the design paradigm is demonstrated by using a reconfigurable floor cleaning robot and its path planning algorithm.



Due to the capacity of reasoning, AI plays a crucial role in achieving safe human-robot interaction (HRI) for collaborative robots. The article by Du et al. combines different AI technologies to achieve active collision avoidance for safe HRI. A Microsoft's motion sensing input device named Kinect is employed to detect anyone who enters the workspace of the robot so that the skeleton data of the human can be calculated in real-time. An expert system with collision avoidance knowledge is employed to analyze the behavior of the human for active collision avoidance. An artificial potential field method is adopted to plan a new path for the robot such that it can bypass the human in real-time. Experiments show that by applying these AI-powered algorithms, the proposed system can safeguard the human by detecting the human and analyzing the motion of the human.



An important issue for collaborative robots is to learn the compositionality of human activities, i.e., to recognize both activities and their comprising actions. Even a small set of actions and objects can create a large combination of possible activities. Most existing approaches in this topic address action and activity recognitions separately. The article by Mici et al. suggests learning human activities concurrently on two levels of semantic and temporal complexity: Transitive actions such as reaching and opening a cereal box, and high-level activities such as having breakfast. The learning model consists of a hierarchy of growing-when-required (GWR) networks which can process and learn inherent spatiotemporal dependencies of multiple visual cues abstracted from human body skeletal representation and interaction with objects. GWR means that new network nodes are added only when the number of iterations is an integer multiple of some pre-defined constant. The proposed architecture semantically segments input RGB-D sequences of high-level activities into their component actions without supervision. Experiments show that the proposed approach possesses a superior ability regarding the classification of high-level activities.



AI is also useful for collaborative robots to assist humans in co-manipulation and teleoperation tasks under demonstrated trajectories. Most existing approaches in this topic are not applicable when the solutions of demonstrations are suboptimal or when the generalization capabilities of the learned models cannot cope with changing environments. The article by Ewerton et al. presents a reinforcement learning-based approach to solve the problem above. The proposed approach makes use of the concept of relevance functions and is initialized by a probability distribution of demonstrated trajectories. Gaussian Process regression is applied to cope with the changes in dynamic environments. Experiments demonstrate that the proposed algorithm embedded in a 7-degree of Freedom (DoF) robot arm performs well under a dynamic environment.



The last article by Galati and Giulio serves more as an outreach of this Research Topic, where a physical model-based approach for terrain characterization is presented for a tracked skid-steer vehicle. A set of physics-based parameters, including the equivalent track, the power spectral density for the vertical accelerations, drive motor electrical currents, and motor currents, is employed to characterize the terrain properties. The proposed algorithm predicts the type of terrain that the robot traverses based on the parameter set. Experiments under various surfaces verify the effectiveness of the proposed approach for autonomous robots. The results of this article also indicate that the intelligent integration of model-based and AI-based techniques will be promising in robotic applications.YP was funded by the Fundamental Research Funds for the

Central Universities of China (Grant No. 19lgzd40). CY was

partially supported by the Engineering and Physical Sciences

Research Council (Grant EP/S001913).Peer reviewe",'Frontiers Media SA',"Editorial: AI for Robot Modeling, Path Planning, and Intelligent Control",https://core.ac.uk/download/343446702.pdf,"[{'title': 'Frontiers in Robotics and AI', 'identifiers': ['issn:2296-9144', '2296-9144']}]",10.3389/frobt.2020.00019,core
387279872,2020-11-30T00:00:00,"Robots have limited adaptation ability compared to humans and animals in the
case of damage. However, robot damages are prevalent in real-world
applications, especially for robots deployed in extreme environments. The
fragility of robots greatly limits their widespread application. We propose an
adversarial reinforcement learning framework, which significantly increases
robot robustness over joint damage cases in both manipulation tasks and
locomotion tasks. The agent is trained iteratively under the joint damage cases
where it has poor performance. We validate our algorithm on a three-fingered
robot hand and a quadruped robot. Our algorithm can be trained only in
simulation and directly deployed on a real robot without any fine-tuning. It
also demonstrates exceeding success rates over arbitrary joint damage cases.Comment: 7 pages, 7 figure",,Fault-Aware Robust Control via Adversarial Reinforcement Learning,http://arxiv.org/abs/2011.08728,,,core
387274064,2020-11-04T00:00:00,"In this paper, we present Asynchronous implementation of Deep Neural
Network-based Model Reference Adaptive Control (DMRAC). We evaluate this new
neuro-adaptive control architecture through flight tests on a small quadcopter.
We demonstrate that a single DMRAC controller can handle significant
nonlinearities due to severe system faults and deliberate wind disturbances
while executing high-bandwidth attitude control. We also show that the
architecture has long-term learning abilities across different flight regimes,
and can generalize to fly different flight trajectories than those on which it
was trained. These results demonstrating the efficacy of this architecture for
high bandwidth closed-loop attitude control of unstable and nonlinear robots
operating in adverse situations. To achieve these results, we designed a
software+communication architecture to ensure online real-time inference of the
deep network on a high-bandwidth computation-limited platform. We expect that
this architecture will benefit other deep learning in the closed-loop
experiments on robots.Comment: Accepted in CORL2020. arXiv admin note: text overlap with
  arXiv:1909.0860",,Asynchronous Deep Model Reference Adaptive Control,http://arxiv.org/abs/2011.02920,,,core
387290484,2020-12-07T00:00:00,"With deep reinforcement learning (RL) methods achieving results that exceed
human capabilities in games, robotics, and simulated environments, continued
scaling of RL training is crucial to its deployment in solving complex
real-world problems. However, improving the performance scalability and power
efficiency of RL training through understanding the architectural implications
of CPU-GPU systems remains an open problem. In this work we investigate and
improve the performance and power efficiency of distributed RL training on
CPU-GPU systems by approaching the problem not solely from the GPU
microarchitecture perspective but following a holistic system-level analysis
approach. We quantify the overall hardware utilization on a state-of-the-art
distributed RL training framework and empirically identify the bottlenecks
caused by GPU microarchitectural, algorithmic, and system-level design choices.
We show that the GPU microarchitecture itself is well-balanced for
state-of-the-art RL frameworks, but further investigation reveals that the
number of actors running the environment interactions and the amount of
hardware resources available to them are the primary performance and power
efficiency limiters. To this end, we introduce a new system design metric,
CPU/GPU ratio, and show how to find the optimal balance between CPU and GPU
resources when designing scalable and efficient CPU-GPU systems for RL
training.Comment: To appear in the proceedings of the 6th Workshop on Energy Efficient
  Machine Learning and Cognitive Computing (EMC2) 202",,"The Architectural Implications of Distributed Reinforcement Learning on
  CPU-GPU Systems",http://arxiv.org/abs/2012.04210,,,core
334486504,2020-09-15T00:00:00,"International audienceThe main concepts and techniques of multi-agent oriented programming, which supports the multi-agent systems paradigm at the programming level.A multi-agent system is an organized ensemble of autonomous, intelligent, goal-oriented entities called agents, communicating with each other and interacting within an environment. This book introduces the main concepts and techniques of multi-agent oriented programming, (MAOP) which supports the multi-agent systems paradigm at the programming level. MAOP provides a structured approach based on three integrated dimensions, which the book examines in detail: the agent dimension, used to design the individual (interacting) entities; the environment dimension, which allows the development of shared resources and connections to the real world; and the organization dimension, which structures the interactions among the autonomous agents and the shared environment.The book puts the approach into practice using the JaCaMo programming model and platform. It employs an easy-to-follow, step-by-step style, showing solutions to increasingly complex scenarios. The book also discusses the integration of MAOP into existing technologies and application domains, including mobile computing, web-based computing, and robotics. Finally, it considers artificial intelligence (AI)–related classical problems from an MAOP perspective and discusses an agent-oriented approach to software engineering",The MIT Press,Multi-Agent Oriented Programming: Programming Multi-Agent Systems Using JaCaMo,,,,core
360730425,2020-11-15T00:00:00,"Amyotrophic Lateral Sclerosis (ALS) is a fast-progressive neurodegenerative disease leading to progressive physical immobility with usually normal or mild cognitive and/or behavioural involvement. Many patients are relatively young, instructed, sensitive to new technologies, and professionally active when developing the first symptoms. Older patients usually require more time, encouragement, reinforcement and a closer support but, nevertheless, selecting user-friendly devices, provided earlier in the course of the disease, and engaging motivated carers may overcome many technological barriers. ALS may be considered a model for neurodegenerative diseases to further develop and test new technologies. From multidisciplinary teleconsults to telemonitoring of the respiratory function, telemedicine has the potentiality to embrace other fields, including nutrition, physical mobility, and the interaction with the environment. Brain-computer interfaces and eye tracking expanded the field of augmentative and alternative communication in ALS but their potentialities go beyond communication, to cognition and robotics. Virtual reality and different forms of artificial intelligence present further interesting possibilities that deserve to be investigated. COVID-19 pandemic is an unprecedented opportunity to speed up the development and implementation of new technologies in clinical practice, improving the daily living of both ALS patients and carers. The present work reviews the current technologies for ALS patients already in place or being under evaluation with published publications, prompted by the COVID-19 pandemic",'Elsevier BV',New technologies and Amyotrophic Lateral Sclerosis &#8211; Which step forward rushed by the COVID-19 pandemic?,,,10.1016/j.jns.2020.117081,core
397483610,2020-12-29T00:00:00,"The effect of AI on how people are viewed and handled in society is important and profound. However, a vicious cycle is maintained with AI's algorithms design and implementation. Among others, predictive models, machine learning and AI algorithms train and test themselves using datasets, as a result, they “learn” mainly based on the data input in a model. Nowadays and in this context, it seems that there is a growing scientific dialogue concerning bias in training AI (Falco, 2019; Lu, 2019; Straw, 2020) as well as whether datasets, on which decisions are made, only represent fractions of reality (Günther et al., 2017).  The technology often captures and reproduces regulated and restrictive beliefs regarding gender and race, which are then repetitively strengthened: Gender relations be materialized by inventions and, through their enrolment and incorporation of machinery, masculinity, and femininity gain of turn their importance and character. When robots progress in certain cognitive functions, their comparatively weak abilities will definitely get better. This list incorporates the innovative approach to the dilemma, empathy, negotiation, and belief. Automation and AI will also replace many of today's workers at the same time creating new opportunities for specialized personnel– so that is why women need to get into this emerging sector and ensure that they can secure new jobs when their jobs are squeezed.  In addition, AI may provide the ability to alter male and female epistemological assumptions. The narration of ""hard"" and ""soft"" intelligence, for instance, is often described as male and female. The rise and development of AI is also seen as pushing economic growth and strengthening political influence. In politics, UK politics still dominates the ambition of economic development by technical advancement. Jude Browne states (Clementine Collett & Sarah Dillon) that a national AI agency equivalent to Human Fertilization and Embryology (HFEA) has yet to be set up by the government of the UK that will fill the divide between national, experts and government, for example. Browne claims that it includes the dominance, primarily guided by the goals of economic wealth, of private interest over the public interest. There is a possibility that economic growth and political influence play an important part in influencing AI laws and policies at the cost of other motivations, which are more morally equal. Consequently, a dual-purpose must be incorporated into an equitable AI policy. Firstly, to ensure there is no rise in social and economic disparity due to the advancement of AI technology. Secondly, to call AI to cut this down. AI must first and foremost enable us to promote our democratic liberties, enhance social harmony, and enhance unity, rather than jeopardise our individual trajectories and networks of solidarity",'National Documentation Centre (EKT)',Using AI Changes the Paradigm of Women's Participation in Politics,https://core.ac.uk/download/397483610.pdf,,10.12681/hapscpbs.26479,core
265418942,2020-01-01T00:00:00,"Robot-assisted endobronchial intervention requires accurate localisation based on both intra- and pre-operative data. Most existing methods achieve this by registering 2D videos with 3D CT models according to a defined similarity metric with local features. Instead, we formulate the bronchoscopic localisation as a learning-based global localisation using deep neural networks. The proposed network consists of two generative architectures and one auxiliary learning component. The cycle generative architecture bridges the domain variance between the real bronchoscopic videos and virtual views derived from pre-operative CT data so that the proposed approach can be trained through a large number of generated virtual images but deployed through real images. The auxiliary learning architecture leverages complementary relative pose regression to constrain the search space, ensuring consistent global pose predictions. Most importantly, the uncertainty of each global pose is obtained through variational inference by sampling within the learned underlying probability distribution. Detailed validation results demonstrate the localisation accuracy with reasonable uncertainty achieved and its potential clinical value",'Institute of Electrical and Electronics Engineers (IEEE)',Generative localisation with uncertainty estimation through video-CT data for bronchoscopic biopsy,https://core.ac.uk/download/265418942.pdf,,10.1109/LRA.2019.2955941,core
346528583,2020-01-01T00:00:00,"В данной статье исследована параллельная реализация метода машинного
обучения с циклическим фрактальным кодированием и использованием словаря
доменных блоков, адаптированный для применения на мобильных платформах, с
оптимизацией производительности и объема хранимых фрактальных образов
изображений. Основная идея метода заключается в применении метода фрактального
сжатия на основе систем итерированных функций для понижения размерности исходных
изображений, и использовании циклического фрактального кодирования для
представления класса изображений в целом. В параллельной реализации метода
используется технология CUDA на аппаратной мобильной платформе NVIDIA Jetson
Nano, в результате исследований метода получено, что время распознавания в среднем
составило 76 мс, что в 2.8 раза быстрее последовательной реализации. Достигнутые
показатели производительности являются приемлемыми для использования в системах
обработки изображений в реальном времени на мобильных платформах, в т.ч. для БПЛА
и наземных автономных роботов. In this article the parallel implementation of the method of machine learning with
cyclic fractal coding and the use of domain block dictionary, adapted for use on mobile
platforms, with optimization of performance and volume of stored fractal images is
investigated. The main idea of the method is to use the fractal compression method based on
systems of iterated functions to lower the dimension of the original images, and to use cyclic
fractal coding to represent the class of images as a whole. In the parallel implementation of the
method, CUDA technology is used on the NVIDIA Jetson Nano hardware mobile platform, as
a result of research of the method, it was found that the recognition time on average was 76 ms,
which is 2.8 times faster than sequential implementation. The achieved performance indicators
are acceptable for use in real-time image processing systems on mobile platforms, including
for UAVs and ground-based autonomous robots",,High performance implementation of machine learning method based on fractal compression,,,,core
326805041,2020-06-22T11:39:13,"Orientador: Paulo Roberto Gardel KurkaTese (doutorado) - Universidade Estadual de Campinas, Faculdade de Engenharia MecânicaResumo: Esta tese apresenta um novo método utilizando técnicas de visão computacional e aprendizado de máquina para estimar a odometria de um robô utilizando apenas informações da textura do chão. Foram testados 7 diferentes detectores de pontos característicos na imagem, um algoritmo de rastreamento de pontos e uma estrutura de rede neural com três diferentes algoritmos de treinamento de forma a determinar a metodologia com maior precisão e menor tempo de processamento. O processo inicia com a extração de pontos característicos na imagem através do detector de pontos característicos FAST, juntamente com o rastreamento destes através do algoritmo de fluxo óptico de Lucas-Kanade-Tomasi e então a odometria é calculada de três maneiras: algebricamente, algebricamente com melhoria por redes neurais e utilizando apenas redes neurais. Os experimentos foram realizados em um ambiente virtual desenvolvido na plataforma de software Blender com implementação de diferentes texturas do chão e em um ambiente real. Os resultados mostram que a metodologia proposta é promissora para aplicação em tempo real para certas texturas, considerando uma configuração de hardware apropriadaAbstract: This thesis presents the implementation of computer vision and machine learning techniques to estimate the odometry of a differential robot using only floor texture information. Seven feature detectors, one feature tracker and one neural network structure with three different training algorithms were tested to determine the fastest and most precise methodology. The process works firstly by extracting characteristic points in the floor image using FAST feature detector, then these points are tracked with the Lucas-Kanade-Tomasi optical flow and the odometry is calculated in three different ways: algebraic, algebraic with a neural network improver and neural network only. The experiments were done in a virtual environment developed in the Blender software platform with different floor textures and in a real environment. The results show that the methodology presented is promising for real time applications and certain textures, considering an appropriate hardware configurationDoutoradoMecatrônicaDoutor em Engenharia MecânicaFAPEA",[s.n.],Visual odometry using floor texture,https://core.ac.uk/download/326805041.pdf,,,core
289968510,2020,"Today, Artificial Intelligence is one of the most important technologies, ubiquitous in our daily lives. Deep Neural Networks (DNN's) have come up as state of art for various machine intelligence applications such as object detection, image classification, face recognition and performs myriad of activities with exceptional prediction accuracy. AI in this contemporary world is moving towards embedded platforms for inference on the edge. This is essential to avoid latency, enhance data security and realize real-time performance. However, these DNN algorithms are computational and memory intensive. Consequently, exploiting immense energy, compute resources and memory-bandwidth making it difficult to be deployed in embedded devices. To solve this problem and realize an on-device AI acceleration, dedicated energy-efficient hardware accelerators are paramount. This thesis involves the implementation of such a dedicated deep learning accelerator on the FPGA. The NVIDIA's Deep Learning Accelerator (NVDLA), is encompassed in this research to explore SoC designs for integrated inference acceleration. NVDLA, an open-source architecture, standardizes deep learning inference acceleration on hardware. It optimizes inference acceleration all across the full stack from application through hardware to achieve energy efficiency synergy with the demanding throughput requirements. Therefore, the following thesis probes into the NVDLA framework to perceive the consistent workflow across the whole hardware-software programming hierarchies. Besides, the hardware design parameters, optimization features and system configurations of the NVDLA systems are analyzed for efficient implementations. Also, a comparative study of the diverse NVDLA SoC implementations (nv\_small and nv\_medium) with respect to performance metrics such as power, area, and throughput are discussed. Our approach engages prototyping of Nvidia’s Deep Learning Accelerator on a Zynq Ultrascale+ ZCU104 FPGA to examine its system functionality. The Hardware design of the system is carried out using Xilinx's Vivado Design Suite 2018.3 in Verilog. While the on-device software runs Linux kernel 4.14 on Zynq MPSoC. Thus, the software ecosystem is built with PetaLinux tools from Xilinx. The entire system architecture is validated using the pre-built regression tests that verify individual CNN layers. Besides these NVDLA hardware design also runs pre-compiled AlexNet as a benchmark for performance evaluation and comparisonToday, Artificial Intelligence is at the edge. This edge or endpoint device is becoming more sophisticated with the evolution of Internet of Things (IoT) and 5G. For instance, these devices are employed in different applications such as autonomous cars, drones, and other IoT gadgets. At present, a self-driving car is a data center on wheels, a drone is a data center on wings as well as robots are data centers with arms and legs. All these mechanisms collect vast real-world information that demands to be processed in real-time. Here in these applications, there is no time to send data to the cloud for processing and wait for action. As the decision making needs to be instantaneous. There is a shift in transforming the processing to the edge devices. The edge acceleration brings computation and data storage closer to the device. With the evolution of specialized hardware’s providing increased computational capabilities, the AI models are processed on the edge. As a result, the overall system latency gets reduced, the bandwidth costs for data transfers are lowered and the data processing is done locally enhances privacy concerns. For example, autonomous cars require a spontaneous reaction (in seconds) to avoid potential hazards on the road. Consider the situation where a self-driving car is collecting real word information like images, videos, in this case, assume it’s sensing for a stop sign. If the system sends the specific image information to the cloud for processing and waits for a decision to stop. By that response time, the autonomous vehicle could have already blown through the stop sign running over several people. Therefore, it is paramount to process the data in real-time which could be accomplished using dedicated hardware for processing locally. This thesis primarily explores those hardware architectures for efficient processing of AI algorithms and their corresponding software execution environment setup. The particular thesis was carried out as a joint collaboration between Ericsson and Lund University. Here Nvidia’s Deep Learning Accelerator architecture is engaged as a target to comprehend the complete system incorporating a hardware-software co-design. The particular architecture is an essential characteristic of NVIDIA’s Xavier Drive chip which is utilized in their autonomous drive platforms. This thesis is addressed to a variety of audiences who are passionate about Deep Learning, Computer Architecture, and System-on-Chip Design. The thesis illustrates a comprehensive implementation of an AI accelerator to envision AI processing on the edge",Lunds universitet/Institutionen för elektro- och informationsteknik,Implementation of a Deep Learning Inference Accelerator on the FPGA.,,,,core
334900771,2020-01-07T00:00:00,"Recently, vision-based control has gained traction by leveraging the power of
machine learning. In this work, we couple a model predictive control (MPC)
framework to a visual pipeline. We introduce deep optical flow (DOF) dynamics,
which is a combination of optical flow and robot dynamics. Using the DOF
dynamics, MPC explicitly incorporates the predicted movement of relevant pixels
into the planned trajectory of a robot. Our implementation of DOF is
memory-efficient, data-efficient, and computationally cheap so that it can be
computed in real-time for use in an MPC framework. The suggested Pixel Model
Predictive Control (PixelMPC) algorithm controls the robot to accomplish a
high-speed racing task while maintaining visibility of the important features
(gates). This improves the reliability of vision-based estimators for
localization and can eventually lead to safe autonomous flight. The proposed
algorithm is tested in a photorealistic simulation with a high-speed drone
racing task",,"Aggressive Perception-Aware Navigation using Deep Optical Flow Dynamics
  and PixelMPC",http://arxiv.org/abs/2001.02307,,,core
334906530,2020-02-06T00:00:00,"This paper provides an overview of the current and near-future applications
of Artificial Intelligence (AI) in Medicine and Health Care and presents a
classification according to their ethical and societal aspects, potential
benefits and pitfalls, and issues that can be considered controversial and are
not deeply discussed in the literature.
  This work is based on an analysis of the state of the art of research and
technology, including existing software, personal monitoring devices, genetic
tests and editing tools, personalized digital models, online platforms,
augmented reality devices, and surgical and companion robotics. Motivated by
our review, we present and describe the notion of 'extended personalized
medicine', we then review existing applications of AI in medicine and
healthcare and explore the public perception of medical AI systems, and how
they show, simultaneously, extraordinary opportunities and drawbacks that even
question fundamental medical concepts. Many of these topics coincide with
urgent priorities recently defined by the World Health Organization for the
coming decade. In addition, we study the transformations of the roles of
doctors and patients in an age of ubiquitous information, identify the risk of
a division of Medicine into 'fake-based', 'patient-generated', and
'scientifically tailored', and draw the attention of some aspects that need
further thorough analysis and public debate",,"Artificial intelligence in medicine and healthcare: a review and
  classification of current and near-future applications and their ethical and
  social Impact",http://arxiv.org/abs/2001.09778,,,core
359945624,2020-09-15T00:00:00,"Visual navigation tasks in real-world environments often require both self-motion and place recognition feedback. While deep reinforcement learning has shown success in solving these perception and decision-making problems in an end-to-end manner, these algorithms require large amounts of experience to learn navigation policies from high-dimensional data, which is generally impractical for real robots due to sample complexity. In this paper, we address these problems with two main contributions. We first leverage place recognition and deep learning techniques combined with goal destination feedback to generate compact, bimodal image representations that can then be used to effectively learn control policies from a small amount of experience. Second, we present an interactive framework, CityLearn, that enables for the first time training and deployment of navigation algorithms across city-sized, realistic environments with extreme visual appearance changes. CityLearn features more than 10 benchmark datasets, often used in visual place recognition and autonomous driving research, including over 100 recorded traversals across 60 cities around the world. We evaluate our approach on two CityLearn environments, training our navigation policy on a single traversal per dataset. Results show our method can be over 2 orders of magnitude faster than when using raw images, and can also generalize across extreme visual changes including day to night and summer to winter transitions.</p",'Institute of Electrical and Electronics Engineers (IEEE)',CityLearn:Diverse Real-World Environments for Sample-Efficient Navigation Policy Learning,,,10.1109/ICRA40945.2020.9197336,core
428920125,2020-03-23T00:00:00,"This paper is the first step of an attempt to equip social robots with emotion recognition capabilities comparable to those of humans. Most of the recent deep learning solutions for facial expression recognition under-perform when deployed in Human-Robot-Interaction scenarios, although they are capable of breaking records on the most varied benchmarks on facial expression recognition. The main reason for that we believe is that they are using techniques that are developed for recognition of static pictures, while in real-life scenarios, we infer emotions from intervals of expression. Utilising on the feature of CNN to form regions of interests that are similar to human gaze patterns, we use recordings from human-gaze patterns to train such a network to infer facial emotions from 3 seconds video footage of humans expressing 6 basic emotions",'Association for Computing Machinery (ACM)',Improving emotional expression recognition of robots using regions of interest from human data,,,10.1145/3371382.3378359,core
327204412,2020-05-01T07:00:00,"It is important to be able to establish formal performance bounds for autonomous systems. However, formal verification techniques require a model of the environment in which the system operates; a challenge for autonomous systems, especially those expected to operate over longer timescales. This paper describes work in progress to automate the monitor and repair of ROS-based autonomous robot software written for an a-priori partially known and possibly incorrect environment model. A taint analysis method is used to automatically extract the data-flow sequence from input topic to publish topic, and instrument that code. A unique reinforcement learning approximation of MDP utility is calculated, an empirical and non-invasive characterization of the inherent objectives of the software designers. By comparing design (a-priori) utility with deploy (deployed system) utility, we show, using a small but real ROS example, that it’s possible to monitor a performance criterion and relate violations of the criterion to parts of the software. The software is then patched using automated software repair techniques and evaluated against the original off-line utility",Fordham Research Commons,Using Taint Analysis and Reinforcement Learning (TARL) to Repair Autonomous Robot Software,,,,core
323921209,2020-06-07T00:00:00,"This article proposes a novel unsupervised learning framework for detecting
the number of tunnel junctions in subterranean environments based on acquired
2D point clouds. The implementation of the framework provides valuable
information for high level mission planners to navigate an aerial platform in
unknown areas or robot homing missions. The framework utilizes spectral
clustering, which is capable of uncovering hidden structures from connected
data points lying on non-linear manifolds. The spectral clustering algorithm
computes a spectral embedding of the original 2D point cloud by utilizing the
eigen decomposition of a matrix that is derived from the pairwise similarities
of these points. We validate the developed framework using multiple data-sets,
collected from multiple realistic simulations, as well as from real flights in
underground environments, demonstrating the performance and merits of the
proposed methodology",,"Unsupervised Learning for Subterranean Junction Recognition Based on 2D
  Point Cloud",http://arxiv.org/abs/2006.04225,,,core
334862700,2020-03-11T00:00:00,"In this work we focus on improving the efficiency and generalisation of
learned navigation strategies when transferred from its training environment to
previously unseen ones. We present an extension of the residual reinforcement
learning framework from the robotic manipulation literature and adapt it to the
vast and unstructured environments that mobile robots can operate in. The
concept is based on learning a residual control effect to add to a typical
sub-optimal classical controller in order to close the performance gap, whilst
guiding the exploration process during training for improved data efficiency.
We exploit this tight coupling and propose a novel deployment strategy,
switching Residual Reactive Navigation (sRRN), which yields efficient
trajectories whilst probabilistically switching to a classical controller in
cases of high policy uncertainty. Our approach achieves improved performance
over end-to-end alternatives and can be incorporated as part of a complete
navigation stack for cluttered indoor navigation tasks in the real world. The
code and training environment for this project is made publicly available at
https://sites.google.com/view/srrn/home.Comment: Accepted as a conference paper at ICRA2020. Project site available at
  https://sites.google.com/view/srrn/hom",,"Residual Reactive Navigation: Combining Classical and Learned Navigation
  Strategies For Deployment in Unknown Environments",http://arxiv.org/abs/1909.10972,,,core
334870931,2020-01-19T00:00:00,"State-of-the-art algorithms for visual place recognition, and related visual
navigation systems, can be broadly split into two categories:
computer-science-oriented models including deep learning or image
retrieval-based techniques with minimal biological plausibility, and
neuroscience-oriented dynamical networks that model temporal properties
underlying spatial navigation in the brain. In this letter, we propose a new
compact and high-performing place recognition model that bridges this divide
for the first time. Our approach comprises two key neural models of these
categories: (1) FlyNet, a compact, sparse two-layer neural network inspired by
brain architectures of fruit flies, Drosophila melanogaster, and (2) a
one-dimensional continuous attractor neural network (CANN). The resulting
FlyNet+CANN network incorporates the compact pattern recognition capabilities
of our FlyNet model with the powerful temporal filtering capabilities of an
equally compact CANN, replicating entirely in a hybrid neural implementation
the functionality that yields high performance in algorithmic localization
approaches like SeqSLAM. We evaluate our model, and compare it to three
state-of-the-art methods, on two benchmark real-world datasets with small
viewpoint variations and extreme environmental changes - achieving 87% AUC
results under day to night transitions compared to 60% for Multi-Process
Fusion, 46% for LoST-X and 1% for SeqSLAM, while being 6.5, 310, and 1.5 times
faster, respectively.Comment: Preprint version of article published in IEEE Robotics and Automation
  Letter",'Institute of Electrical and Electronics Engineers (IEEE)',A Hybrid Compact Neural Architecture for Visual Place Recognition,http://arxiv.org/abs/1910.06840,,10.1109/LRA.2020.2967324,core
334914254,2020-02-11T00:00:00,"Individualized manufacturing is becoming an important approach as a means to
fulfill increasingly diverse and specific consumer requirements and
expectations. While there are various solutions to the implementation of the
manufacturing process, such as additive manufacturing, the subsequent automated
assembly remains a challenging task. As an approach to this problem, we aim to
teach a collaborative robot to successfully perform pick and place tasks by
implementing reinforcement learning. For the assembly of an individualized
product in a constantly changing manufacturing environment, the simulated
geometric and dynamic parameters will be varied. Using reinforcement learning
algorithms capable of meta-learning, the tasks will first be trained in
simulation. They will then be performed in a real-world environment where new
factors are introduced that were not simulated in training to confirm the
robustness of the algorithms. The robot will gain its input data from tactile
sensors, area scan cameras, and 3D cameras used to generate heightmaps of the
environment and the objects. The selection of machine learning algorithms and
hardware components as well as further research questions to realize the
outlined production scenario are the results of the presented work",,"Towards Intelligent Pick and Place Assembly of Individualized Products
  Using Reinforcement Learning",http://arxiv.org/abs/2002.08333,,,core
334927984,2020-03-31T00:00:00,"Deep learning and reinforcement learning methods have been shown to enable
learning of flexible and complex robot controllers. However, the reliance on
large amounts of training data often requires data collection to be carried out
in simulation, with a number of sim-to-real transfer methods being developed in
recent years. In this paper, we study these techniques for tactile sensing
using the TacTip optical tactile sensor, which consists of a deformable tip
with a camera observing the positions of pins inside this tip. We designed a
model for soft body simulation which was implemented using the Unity physics
engine, and trained a neural network to predict the locations and angles of
edges when in contact with the sensor. Using domain randomisation techniques
for sim-to-real transfer, we show how this framework can be used to accurately
predict edges with less than 1 mm prediction error in real-world testing,
without any real-world data at all.Comment: Accepted for publication at ICRA 2020. Website:
  https://www.robot-learning.uk/sim-to-real-tactile-icra-202",,Sim-to-Real Transfer for Optical Tactile Sensing,http://arxiv.org/abs/2004.00136,,,core
334934332,2020-04-20T00:00:00,"The question of how an effective and efficient communication system can
emerge in a population of agents that need to solve a particular task attracts
more and more attention from researchers in many fields, including artificial
intelligence, linguistics and statistical physics. A common methodology for
studying this question consists of carrying out multi-agent experiments in
which a population of agents takes part in a series of scripted and
task-oriented communicative interactions, called 'language games'. While each
individual language game is typically played by two agents in the population, a
large series of games allows the population to converge on a shared
communication system. Setting up an experiment in which a rich system for
communicating about the real world emerges is a major enterprise, as it
requires a variety of software components for running multi-agent experiments,
for interacting with sensors and actuators, for conceptualising and
interpreting semantic structures, and for mapping between these semantic
structures and linguistic utterances. The aim of this paper is twofold. On the
one hand, it introduces a high-level robot interface that extends the Babel
software system, presenting for the first time a toolkit that provides flexible
modules for dealing with each subtask involved in running advanced grounded
language game experiments. On the other hand, it provides a practical guide to
using the toolkit for implementing such experiments, taking a grounded colour
naming game experiment as a didactic example.Comment: This paper was officially published at the 'Language Learning for
  Artificial Agents (L2A2) Symposium' of the 2019 Artificial Intelligence and
  Simulation of Behaviour (AISB) Conventio",,"A Practical Guide to Studying Emergent Communication through Grounded
  Language Games",http://arxiv.org/abs/2004.09218,,,core
328391295,2020-01-01T00:00:00,"The goal of Geometric Algebra Applications Vol. II: Robot Modeling and Control is to present a unified mathematical treatment of diverse problems in the general domain of robotics and associated fields using Clifford, or geometric algebra. By treating a wide spectrum of problems in a common language, this Volume II offers both new insights and new solutions that should be useful to scientists, and engineers working in different areas related with robotics. Topics and features -Introduces a no specialists to Clifford, or geometric, algebra and by examples encourages the reader to learn to compute using geometric entities and geometric formulations. -A study in depth for applications of Lie group theory, Lie algebra, spinors and versors and the algebra of incidence using the universal geometric algebra generated by reciprocal null cones. -Includes a thorough study of kinematics, differential kinematics and dynamics using geometric algebra. The Euler Lagrange and Hamiltonians equations for dynamics are developed using conformal geometric algebra and the recursive Newton-Euler using screw theory in the motor algebra framework. A thorough study of robot modeling and nonlinear controllers. -Thorough discussion of several applications in computer vision, graphics, neurocomputing, quantum computing, robotics and control engineering using the geometric algebra framework. -209 exercises and hints for the development of future computer software packages for extensive calculations in geometric algebra. A entire section is dedicated to explain how one should write the subroutines in C++, Matlab and Maple to carry out efficient geometric computations in the geometric algebra framework. Furthermore it is shown how program code can be optimized for real time computations. -The book is an essential resource for applied physicists, computer scientists, AI researchers, roboticists and mechanical and electrical engineers, it clarifies and demonstrates the importance of geometric computing for building autonomous systems and push forward advances in cognitive systems research",'Springer Science and Business Media LLC',Geometric algebra applications,,,10.1007/978-3-030-34978-3,core
362229542,2020-11-22T00:00:00,"International audienceDeep reinforcement learning (DRL) techniques give robotics research an AI boost in many applications. In order to simultaneously accommodate the complex robotic behaviour simulation and DRL algorithm verification, a new simulation platform, namely the RobotDrlSim, is proposed. First, we design a standardized API interfacing mechanism for coordinating diverse environments on RobotDrlSim platform, where PyBullet simulator is equipped with an API to form a physical engine for robotics simulation. Second, benchmark DRL models are included in the baseline library for evaluation. Third, real-time human-robot interactions can be captured and imported to drive the RobotDrlSim tasks, which provide big data-stream for reinforcement learning. Experimentations show that cutting-edge DRL algorithms developed in python can be seamlessly deployed to the robots, and human interactions can be availed in training the robots. RobotDrlSim is valid for efficiently developing DRL algorithms for artificial intelligence models of robots, and it is especially suitable for the robot educational purposes",HAL CCSD,RobotDrlSim: A real time robot simulation platform for reinforcement learning and human interactive demonstration learning,https://core.ac.uk/download/362229542.pdf,,,core
132306374,2020-06-22T00:00:00,"La conoscenza della topologia e morfologia di un particolare ambiente ostile è da sempre considerata un vantaggio tattico ed è anche una condizione necessaria in caso di ricerche e soccorso.
Il problema si complica nel caso di ambienti interni in cui non è disponibile un sistema di posizionamento globale. L'abilità di avere un team di robot che mappano un luogo sconosciuto permette di avere tutte le informazioni necessarie in tempi più brevi rispetto alle attuali tecnologie.
Lo scopo di questo lavoro è ottenere un sistema flessibile e robusto che permetta ai robot di navigare, localizzarsi, esplorare e mappare l'ambiente circostante.
In particolare sono stati integrati diversi sistemi allo stato dell'arte ed è stato creato un framework che si occupa di gestire e convertire le informazioni.
La soluzione trovata è utilizzabile anche in scenari multi robot. Infatti è stato ideato un metodo di condivisione delle informazioni ottenute da ciascun robot nel momento in cui viene stabilito il collegamento con un altro. Viene anche considerata la possibilità di spegnere e riaccendere il drone in un punto diverso, continuando il lavoro di esplorazione.
Il framework proposto è un sistema real-time privo della necessità di unità di calcolo esterne. L'intero sistema è stato valutato con un prototipo multirotore e diverse configurazioni hardware, considerando la qualità della mappa ricostruita. Basandosi sui dati ottenuti dalle sperimentazioni si è dimostrata un promettente, robusto e sufficientemente preciso modulo di navigazione. Le prospettive future riguardano la costruzione di un payload con tutto il necessario che possa essere montato su qualsiasi drone in commercio.
Knowledge of topology and morphology of a particular dangerous environment has always been considered a tactical advantage and it is also a necessary condition in case of search and rescue mission in disaster scenarios. Environment reconstruction is much more complex indoor scenarios, indeed this is a current challenge from both research and technological points of view.
Currently systems that are able to map an unknown environment are mainly human--driven. The aim of this work is to get a flexible and robust system that enable robots to navigate in unknown indoor environments where no GNSS signal is present.
In particular, we are interested in the design of a novel payload for robots in order to transform standard human--driven devices in autonomous robots able to localize themselves in unknown indoor environment with the use of optical/acoustic sensors whether they are ground, aerial or underwater robots.
In this work, several state-of-the-art algorithms have been integrated to obtain a robust autonomous indoor navigation system. In particular the integration involves the use of a vehicle control system, a mapping and localization system (through the use of vision sensors) and a path planning algorithm.
The framework has been designed modularly so that different subsystems may be used with respect to the one implemented and reported in this work. Moreover, the developed generic robust framework is adaptable to different scenarios with different robots.
Another useful feature is that the system is designed to refine the map of the reconstruction of the environment during navigation and is able to maintain information about its position and the reconstructed map even in case of a switch off.
It is worth mentioning that the proposed integrated framework is suitable also for the localization of multi--robot systems. Indeed the developed mapping and localization system allows robots to share information on the reconstructed environment to build a common global map in which any robot is able to localize itself",'Pisa University Press',Progettazione e realizzazione di un drone per la navigazione autonoma in ambiente indoor,,,,core
328897284,2020-08-26T00:00:00,"In order for humans to have meaningful interactions with a robotic system, this system should be capable of grounding semantic representations to their real-world representations, learn spatial relationships and
communicate using spoken human language. End users need to be able to query the system what objects it already has knowledge of, for more efficient learning. Such systems exist, but require large sample sizes, thus not allowing end users to teach the system more objects when needed.
To overcome this problem, we developed a non-mobile system dubbed Kille, that uses a 3D camera, SIFT features and machine learning to allow a tutor to teach the system objects and spatial relations. The system is built upon the ROS (Robot Operating System) framework and uses Opendial software as a dialogue system,
for which a ROS support was written as part of this project. We describe the hardware of the system, the software used and developed, and we evaluate its performance. Our results show that Kille performs well on small learning sets, considering the low sample size it uses
to learn. In contrast to other approaches, we focus on learning by a tutor presenting objects and not by providing a dataset. Recognition of spatial relations works well, however no definitive conclusions can be drawn. This is largely due to the small number of participants and the subjective nature of spatial relations",,Kille: Learning Objects and Spatial Relations with Kinect,https://core.ac.uk/download/328897284.pdf,,,core
329132270,2020-08-26T00:00:00,"With advances in the field of machine learning, precisely algorithms for
recommendation systems, robot assistants are envisioned to become more present
in the hospitality industry. Additionally, the COVID-19 pandemic has also
highlighted the need to have more service robots in our everyday lives, to
minimise the risk of human to-human transmission. One such example would be
coffee shops, which have become intrinsic to our everyday lives. However,
serving an excellent cup of coffee is not a trivial feat as a coffee blend
typically comprises rich aromas, indulgent and unique flavours and a lingering
aftertaste. Our work addresses this by proposing a computational model which
recommends optimal coffee beans resulting from the user's preferences.
Specifically, given a set of coffee bean properties (objective features), we
apply different supervised learning techniques to predict coffee qualities
(subjective features). We then consider an unsupervised learning method to
analyse the relationship between coffee beans in the subjective feature space.
Evaluated on a real coffee beans dataset based on digitised reviews, our
results illustrate that the proposed computational model gives up to 92.7
percent recommendation accuracy for coffee beans prediction. From this, we
propose how this computational model can be deployed on a service robot to
reliably predict customers' coffee bean preferences, starting from the user
inputting their coffee preferences to the robot recommending the coffee beans
that best meet the user's likings.Comment: Extended version of submission to ACM HAI 202",,At Your Service: Coffee Beans Recommendation From a Robot Assistant,http://arxiv.org/abs/2008.13585,,,core
326409611,2020-08-04T00:00:00,"Learning to control robots without requiring engineered models has been a
long-term goal, promising diverse and novel applications. Yet, reinforcement
learning has only achieved limited impact on real-time robot control due to its
high demand of real-world interactions. In this work, by leveraging a learnt
probabilistic model of drone dynamics, we learn a thrust-attitude controller
for a quadrotor through model-based reinforcement learning. No prior knowledge
of the flight dynamics is assumed; instead, a sequential latent variable model,
used generatively and as an online filter, is learnt from raw sensory input.
The controller and value function are optimised entirely by propagating
stochastic analytic gradients through generated latent trajectories. We show
that ""learning to fly"" can be achieved with less than 30 minutes of experience
with a single drone, and can be deployed solely using onboard computational
resources and sensors, on a self-built drone",,Learning to Fly via Deep Model-Based Reinforcement Learning,http://arxiv.org/abs/2003.08876,,,core
335000914,2020-01-01T00:00:00,"Human - robot Cooperation (HRC) is the research topic which aims at the complementary combination between the robot abilities and the human skills. The robots can assist humans by increasing their capabilities in precision, speed, and force. In addition, the robots could reduce the stress and the tiredness of the human operator and hence improve its working conditions. The humans could contribute to the cooperation in terms of experience, knowledge of executing a task, intuition, easy learning and adaptation, and easy understanding of control strategies. This thesis is composed from three parts: the first part about the safety of human-robot co-manipulation, the second part is about the control method for human-robot co-manipulation. In the third part, a method for improving the performance of human-robot co-manipulation is presented. Towards the safety of the human-robot co-manipulation, a multilayer feedforward Neural Network based approach is proposed for human-robot collision detection and collided link identification taking safety standards into consideration. The topology of one neural network is designed considering the coupled dynamics of the robot and trained, with and without external contacts, by Levenberg-Marquardt algorithm to detect unwanted collisions of the human operator with the manipulator and the link that is collided. Two neural networks architectures are implemented; one architecture with one hidden layer is using both the intrinsic joint position and torque sensors of the manipulator. This architecture can be applied to the collaborative robots. The other neural network architecture with two hidden layers is using only the intrinsic position sensors of the manipulator. This architecture can be applied to any robot. The experimental results prove that the developed system is considerably efficient and very fast in detecting the collisions in the safe region and identifying the collided link along the entire workspace of the joints motion of the manipulator. Separate/Uncoupled neural networks, one for each joint, are also designed and trained using the same data and their performance are compared with the coupled one. Quantitative and qualitative comparison between the both neural networks architectures is also included. In the second part of the thesis, an approach is presented for variable admittance control in human-robot co-manipulation depending on the online training of neural network. The virtual damping, the virtual inertia, or both are tuned and investigated in improving the human-robot co-manipulation. The design of the variable virtual controller is analyzed, and the choice of the neural network type and their inputs and output are justified. In case of adjusting the virtual inertia only or the virtual damping only, the multilayer feedforward neural network (MLFFNN) is selected, and the error backpropagation analysis is used to train the network. In case of adjusting both the virtual damping and the virtual inertia simultaneously, a Jordan recurrent neural network (JRNN) is designed and the real-time recurrent learning algorithm for the training process. The training of any network (whether the MLFFNN or the JRNN) is happening indirectly and based on the velocity error between the reference velocity of the minimum jerk trajectory model and the actual velocity of the robot. The proposed variable admittance controller performance is experimentally investigated, and its generalization is evaluated by conducting cooperative tasks with help of multiple subjects using the KUKA LWR manipulator under different conditions and tasks than the ones used for the neural network training. Finally, a comparative study is presented between the proposed variable controller with previous published ones. Moreover, the comparison between the variable admittance controller, where both the virtual damping and the virtual inertia are adjusted simultaneously, and variable admittance controller where only the virtual damping or the virtual inertia is adjusted. Towards improving the performance of human-robot co-manipulation, an approach for the evaluation of human-robot collaboration towards high performance is introduced and implemented. The human arm and the manipulator are modelled as a closed kinematic chain (CKC) and the proposed task performance criterion is investigated based on the manipulability index of the closed kinematic chain. The selected task is a straight motion in which the robot end-effector is guided by the human operator via an admittance controller. The best location of the selected task is determined by the maximization of the minimal manipulability along the path. Evaluation criteria for the performance are adapted considering the ergonomics literature. In the experimental set-up with a KUKA LWR manipulator, multiple subjects repeat the specified motion to evaluate the introduced approach experimentally.Η αλληλεπίδραση ανθρώπου – ρομπότ είναι μια ερευνητική περιοχή που στοχεύει στον επιτυχή συνδυασμό των δυνατοτήτων ρομπότ και ανθρώπου. Τα ρομπότ μπορούν να βοηθήσουν τους ανθρώπους σε ζητήματα ακρίβειας, ταχύτητας και δύναμης. Επιπρόσθετα, τα ρομπότ μπορούν περιορίσουν συμπτώματα όπως το άγχος και η κούραση του χρήστη, βελτιώνοντας έτσι τις συνθήκες εργασίας του. Βέβαια, καθοριστικός παράγοντας στην υλοποίηση των παραπάνω είναι ο άνθρωπος, όπου μέσα από την εμπειρία και την τεχνογνωσία εκτέλεσης καθημερινών καθηκόντων, τη διαίσθηση του, την εύκολη μάθηση την κατανόηση και την προσαρμογή  του με τις μεθόδους ελέγχου, μπορεί να εκπαιδεύσει ένα ρομπότ. Ο κορμός αυτής της διδακτορικής διατριβής χωρίζεται σε 3 μέρη: το πρώτο μέρος έχει να κάνει με την ασφάλεια ενώ στο δεύτερο μέρος αναλύονται οι τεχνικές ελέγχου αλληλεπίδρασης ανθρώπου – ρομπότ. Στο τρίτο μέρος, παρουσιάζονται τεχνικές βελτίωσης της αλληλεπίδρασης. Όσο αφορά την ασφαλή αλληλεπίδραση ανθρώπου-ρομπότ, προτείνεται μια προσέγγιση πολυστρωματικού πρόσθιας φόρτωσης νευρωνικού δικτύου (multilayer feedforward Neural Network) για τον εντοπισμό της σύγκρουσης ανθρώπου-ρομπότ καθώς και της αναγνώρισης του σχετικού συνδέσμου (link), λαμβάνοντας υπόψιν τα πρότυπα ασφαλείας. Η τοπολογία ενός νευρωνικού δικτύου σχεδιάζεται με βάση την δυναμική σύζευξης (coupled dynamics) του ρομπότ, η οποία εκπαιδεύεται με και χωρίς εξωτερικές επαφές σύμφωνα με τον αλγόριθμο του Levenberg-Marquardt που ανιχνεύει ανεπιθύμητες συγκρούσεις μεταξύ χειριστή και του εκάστοτε συνδέσμου του ρομπότ. Δυο αρχιτεκτονικές νευρωνικών δικτύων αναπτύσσονται: Η πρώτη εξαρτάται από τις θέσεις των αρθρώσεων και των αισθητήρων ροπής του ρομποτικού βραχίονα. Αυτή η αρχιτεκτονική είναι ιδιαίτερα ωφέλιμη σε συνεργατικά ρομπότ. Η δεύτερη αρχιτεκτονική βασίζεται μόνο στους αισθητήρες θέσης του ρομποτικού βραχίονα. Η δεύτερη μέθοδος μπορεί να εφαρμοστεί σε οποιοδήποτε βιομηχανικό ρομπότ. Τα πειραματικά αποτελέσματα αποδεικνύουν ότι οι προτεινόμενες μέθοδοι είναι ιδιαίτερα αποδοτικές και πολύ γρήγορες στον εντοπισμό συγκρούσεων σε ασφαλείς περιοχές, προσδιορίζοντας τον σύνδεσμο σε ολόκληρο τον χώρο εργασίας. Ανεξάρτητα (χωρίς συσχέτιση με άλλους συνδέσμους) νευρωνικά δίκτυα, ένα για κάθε άρθρωση, σχεδιάζονται και εκπαιδεύονται, χρησιμοποιώντας τα ίδια δεδομένα, και η απόδοση τους συγκρίνεται με τα εξαρτώμενα νευρωνικά δίκτυα ποσοτικά αλλά και ποιοτικά. Στο δεύτερο μέρος της διατριβής, παρουσιάζεται μια μέθοδος ελέγχου μεταβλητής εισόδου (variable admittance control) για την συνεργασία ανθρώπου – ρομπότ, εξαρτώμενη από την σύγχρονη (online) εκπαίδευση ενός νευρωνικού δικτύου. Η εικονική απόσβεση, η εικονική αδράνεια ή και οι δύο ρυθμίζονται και διερευνώνται για τη βελτίωση της συνεργασίας ανθρώπου-ρομπότ. Ο σχεδιασμός του μεταβλητού ελεγκτή αναλύεται καθώς επίσης και η επιλογή του νευρωνικού δικτύου, συμπεριλαμβανομένου των εισόδων και εξόδων του. Σε περίπτωση ρύθμισης μόνο της εικονικής αδράνειας ή μόνο της εικονικής απόσβεσης επιλέγεται ένα πολυστρωματικό πρόσθιας φόρτωσης νευρωνικό δίκτυο (multilayer feedforward Neural Network), όπου το σφάλμα (backpropagation error) λαμβάνεται υπόψιν κατά την εκπαίδευση του νευρωνικού δικτύου. Σε περίπτωση ρύθμισης τόσο της εικονικής απόσβεσης όσο και της εικονικής αδράνειας ταυτόχρονα, χρησιμοποιείται το επαναλαμβανόμενο νευρωνικό δίκτυο (RNN) Jordan συνοδευόμενος από τον αντίστοιχο αλγόριθμο μάθησης σε πραγματικό χρόνο. Υλοποιείται έμμεση εκπαίδευση του δικτύου με βάση το σφάλμα ταχύτητας μεταξύ της ταχύτητας αναφοράς του μοντέλου στην περίπτωση που ακολουθεί ελάχιστα απότομη τροχιά και της πραγματικής ταχύτητας του ρομπότ. Η προτεινόμενη απόδοση του ελεγκτή μεταβλητής εισόδου (variable admittance controller) διερευνάται πειραματικά και η γενίκευση της αξιολογείται με συνεργατικές εργασίες με τη βοήθεια πολλαπλών χειριστών του ρομποτικού βραχίονα KUKA LWR υπό διαφορετικές συνθήκες και εργασίες οι οποίες αξιοποιούνται στην εκπαίδευση του νευρικού δικτύου. Επιπλέον, γίνεται σύγκριση μεταξύ των παραλλαγών του ελεγκτή μεταβλητής εισόδου (variable admittance controller), όπου τόσο η εικονική απόσβεση όσο και η εικονική αδράνεια ρυθμίζονται ταυτόχρονα ή ξεχωριστά. Για τη βελτίωση της αλληλεπίδρασης ανθρώπου-ρομπότ, προτείνεται μια προσέγγιση για την αξιολόγηση της συνεργασίας ανθρώπου-ρομπότ με στόχο την επίτευξη υψηλών επιδόσεων. Τόσο το ανθρώπινο χέρι όσο και ο ρομποτικός βραχίονας μοντελοποιούνται ως μια κλειστή κινηματική αλυσίδα (closed kinematic chain) και τα κριτήρια αξιολόγησης της απόδοσης αναλύονται βάσει της δυνατότητας χειρισμού. Η επιλεγμένη εργασία είναι μια ευθεία κίνηση στην οποία το άκρο του ρομπότ οδηγείται από τον άνθρωπο μέσω ενός ελεγκτή εισόδου (admittance controller). Η καλύτερη θέση της επιλεγμένης εργασίας καθορίζεται με τη μεγιστοποίηση της ελάχιστης δυνατότητας χειρισμού κατά μήκος της διαδρομής. Τα κριτήρια αξιολόγησης για την απόδοση υιοθετούνται λαμβάνοντας υπόψη εργονομικές μεθόδους. Στην πειραματική διάταξη με τον βραχίονα KUKA LWR, πολλοί εθελοντές χειριστές επαναλαμβάνουν την καθορισμένη κίνηση και αξιολογούν εμπειρικά την παραπάνω προσέγγιση",Πανεπιστήμιο Πατρών,Έξυπνη ρύθμιση ελέγχου και αντίστασης για αποτελεσματική συνεργασία ανθρώπου-ρομπότ,,,,core
334462062,2020-09-04T14:54:14,"Intelligent robotic systems are becoming essential for space applications, industries, nuclear plants and for harsh environments in general, such as the European Organization for Nuclear Research (CERN) particles accelerator complex and experiments. Robotics technology has huge potential benefits for people and its ultimate scope depends on the way this technology is  used. In order to increase safety and machine availability, robots can perform repetitive, unplanned and dangerous tasks, which humans either prefer to avoid or are unable to carry out due to hazards, size constraints, or the extreme environments in which they take place. Nowadays, mechatronic systems use mature technologies that allow their robust and safe use, even in collaboration with human workers. Over the past years, the progress of robots has been based on the development of smart sensors, artificial intelligence and modular mechanical systems. Due to the multiple challenges that hazardous and unstructured environments have for the application of autonomous industrial systems, there is still a high demand for intelligent and teleoperation systems that give the control of a robot (slave) to a human operator via haptic input devices (master), as well as using human-supervised telerobotic control techniques. Modern techniques like simulation and virtual reality systems can facilitate the preparation of ad-hoc mechatronic tools and robotic intervention including recovery scenarios and failure mode analysis.  The basic contribution of this thesis is the development of a novel robotic framework for autonomous inspections and supervised teleoperations in harsh environments. The proposed framework covers all aspects of a robotic intervention, from the specification and operator training, the choice of the robot and its material in accordance with possible radiological contamination risks, to the realization of the intervention, including procedures and recovery scenarios. In a second set of contributions, new methods for mutirobots maintenance operations are developed, including intervention preparation and best practices for remote handling and advanced tools. The third set of contributions is built on a novel multimodal user-friendly human-robot interface that allows  operator training using virtual reality systems and technicians not expert in robot operation to perform inspection/maintenance tasks. In this thesis, we exploit a robotic system able to navigate autonomously and to inspect unknown environments in a safe way. A new real-time control system has been implemented in order to guarantee a fast response to environmental changes and adaptation to different type of scenarios the robot may find in a semi-structured and hazardous environment. The proposed new robotic control system  has been integrated on different robots, tested and validated with several robotic interventions in the CERN hazardous particle accelerator complex",,A Novel Robotic Framework for Safe Inspection and Telemanipulation in Hazardous and Unstructured Environments,,,,core
334906890,2020-01-28T00:00:00,"In this paper, we present a factor-graph LiDAR-SLAM system which incorporates
a state-of-the-art deeply learned feature-based loop closure detector to enable
a legged robot to localize and map in industrial environments. These facilities
can be badly lit and comprised of indistinct metallic structures, thus our
system uses only LiDAR sensing and was developed to run on the quadruped
robot's navigation PC. Point clouds are accumulated using an inertial-kinematic
state estimator before being aligned using ICP registration. To close loops we
use a loop proposal mechanism which matches individual segments between clouds.
We trained a descriptor offline to match these segments. The efficiency of our
method comes from carefully designing the network architecture to minimize the
number of parameters such that this deep learning method can be deployed in
real-time using only the CPU of a legged robot, a major contribution of this
work. The set of odometry and loop closure factors are updated using pose graph
optimization. Finally we present an efficient risk alignment prediction method
which verifies the reliability of the registrations. Experimental results at an
industrial facility demonstrated the robustness and flexibility of our system,
including autonomous following paths derived from the SLAM map.Comment: 8 pages, 9 figures, accepted for IEEE International Conference on
  Robotics and Automation (ICRA 2020",,"Online LiDAR-SLAM for Legged Robots with Robust Registration and
  Deep-Learned Loop Closure",http://arxiv.org/abs/2001.10249,,,core
343617593,2020-01-01T00:00:00,"Nowadays, robots are heavily used in factories for different tasks, most of them including grasping and manipulation of generic objects in unstructured scenarios. In order to better mimic a human operator involved in a grasping action, where he/she needs to identify the object and detect an optimal grasp by means of visual information, a widely adopted sensing solution is Artificial Vision. Nonetheless, state-of-art applications need long training and fine-tuning for manually build the object's model that is used at run-time during the normal operations, which reduce the overall operational throughput of the robotic system. To overcome such limits, the paper presents a framework based on Deep Convolutional Neural Networks (DCNN) to predict both single and multiple grasp poses for multiple objects all at once, using a single RGB image as input. Thanks to a novel loss function, our framework is trained in an end-to-end fashion and matches state-of-art accuracy with a substantially smaller architecture, which gives unprecedented real-time performances during experimental tests, and makes the application reliable for working on real robots. The system has been implemented using the ROS framework and tested on a Baxter collaborative robot",'Elsevier BV',Deep learning-based method for vision-guided robotic grasping of unknown objects,,,10.1016/j.aei.2020.101052,core
334578353,2020-09-16T00:00:00,"Industry 4.0 is the fourth generation of industry which will theoretically revolutionize manufacturing methods through the integration of machine learning and artificial intelligence approaches on the factory floor to obtain robustness and sped-up process changes. In particular, the use of the digital twin in a manufacturing environment makes it possible to test such approaches in a timely manner using a realistic 3D environment that limits incurring safety issues and danger of damage to resources. To obtain superior performance in an industry 4.0 setup, a modified version of a binary gravitational search algorithm is introduced which benefits from an exclusive or (XOR) operator and a repository to improve the exploration property of the algorithm. Mathematical analysis of the proposed optimization approach is performed which resulted in two theorems which show that the proposed modification to the velocity vector can direct particles to the best particles. The use of repository in this algorithm provides a guideline to direct the particles to the best solutions more rapidly. The proposed algorithm is evaluated on some benchmark optimization problems covering a diverse range of functions including unimodal and multimodal as well as those which suffer from multiple local minima. The proposed algorithm is compared against several existing binary optimization algorithms including existing versions of a binary gravitational search algorithm, improved binary optimization, binary particle swarm optimization, binary grey wolf optimization and binary dragonfly optimization. To show that the proposed approach is an effective method to deal with real world binary optimization problems raised in an industry 4.0 environment, it is then applied to optimize the assembly task of an industrial robot assembling an industrial calculator. The optimal movements obtained are then implemented on a real robot. Furthermore, the digital twin of a universal robot is developed, and its path planning is done in the presence of obstacles using the proposed optimization algorithm. The obtained path is then inspected by human expert and validated. It is shown that the proposed approach can effectively solve such optimization problems which arises in industry 4.0 environment",'MDPI AG',XOR Binary Gravitational Search Algorithm with Repository: Industry 4.0 Applications,https://core.ac.uk/download/334578353.pdf,,10.3390/app10186451,core
334411689,2020-02-17T00:00:00,"Deep reinforcement learning has the potential to train robots to perform complex tasks in the real world without requiring accurate models of the robot or its environment. A practical approach is to train agents in simulation, and then transfer them to the real world. One popular method for achieving transferability is to use domain randomisation, which involves randomly perturbing various aspects of a simulated environment in order to make trained agents robust to the reality gap. However, less work has gone into understanding such agents - which are deployed in the real world - beyond task performance. In this work we examine such agents, through qualitative and quantitative comparisons between agents trained with and without visual domain randomisation. We train agents for Fetch and Jaco robots on a visuomotor control task and evaluate how well they generalise using different testing conditions. Finally, we investigate the internals of the trained agents by using a suite of interpretability techniques. Our results show that the primary outcome of domain randomisation is more robust, entangled representations, accompanied with larger weights with greater spatial structure; moreover, the types of changes are heavily influenced by the task setup and presence of additional proprioceptive inputs. Additionally, we demonstrate that our domain randomised agents require higher sample complexity, can overfit and more heavily rely on recurrent processing. Furthermore, even with an improved saliency method introduced in this work, we show that qualitative studies may not always correspond with quantitative measures, necessitating the combination of inspection tools in order to provide sufficient insights into the behaviour of trained agents",'Center for Open Science',Analysing deep reinforcement learning agents trained with domain randomisation,,,,core
328200774,2020-08-18T00:00:00,"A physical selfie stick extends the user's reach, enabling the acquisition of
personal photos that include more of the background scene. Similarly, a
quadcopter can capture photos from vantage points unattainable by the user; but
teleoperating a quadcopter to good viewpoints is a difficult task. This paper
presents a natural interface for quadcopter photography, the SelfieDroneStick
that allows the user to guide the quadcopter to the optimal vantage point based
on the phone's sensors. Users specify the composition of their desired
long-range selfies using their smartphone, and the quadcopter autonomously
flies to a sequence of vantage points from where the desired shots can be
taken. The robot controller is trained from a combination of real-world images
and simulated flight data. This paper describes two key innovations required to
deploy deep reinforcement learning models on a real robot: 1) an abstract state
representation for transferring learning from simulation to the hardware
platform, and 2) reward shaping and staging paradigms for training the
controller. Both of these improvements were found to be essential in learning a
robot controller from simulation that transfers successfully to the real robot",,Selfie Drone Stick: A Natural Interface for Quadcopter Photography,http://arxiv.org/abs/1909.06491,,,core
323880754,2020-04-01T00:00:00,"International audienceAn open problem in the cognitive dimensions of navigation concerns how previous exploratory experience is reorganized in order to allow the creation of novel efficient navigation trajectories. This behavior is revealed in the ""traveling salesrat problem"" (TSP) when rats discover the shortest path linking baited food wells after a few exploratory traversals. We have recently published a model of navigation sequence learning, where sharp wave ripple replay of hippocampal place cells transmit ""snippets"" of the recent trajectories that the animal has explored to the prefrontal cortex (PFC) (Cazin et al. in PLoS Comput Biol 15:e1006624, 2019). PFC is modeled as a recurrent reservoir network that is able to assemble these snippets into the efficient sequence (trajectory of spatial locations coded by place cell activation). The model of hippocampal replay generates a distribution of snippets as a function of their proximity to a reward, thus implementing a form of spatial credit assignment that solves the TSP task. The integrative PFC reservoir reconstructs the efficient TSP sequence based on exposure to this distribution of snippets that favors paths that are most proximal to rewards. While this demonstrates the theoretical feasibility of the PFC-HIPP interaction, the integration of such a dynamic system into a real-time sensory-motor system remains a challenge. In the current research, we test the hypothesis that the PFC reservoir model can operate in a real-time sensory-motor loop. Thus, the main goal of the paper is to validate the model in simulated and real robot scenarios. Place cell activation encoding the current position of the simulated and physical rat robot feeds the PFC reservoir which generates the successor place cell activation that represents the next step in the reproduced sequence in the readout. This is input to the robot, which advances to the coded location and then generates de novo the current place cell activation. This allows demonstration of the crucial role of embodiment. If the spatial code readout from PFC is played back directly into PFC, error can accumulate, and the system can diverge from desired trajectories. This required a spatial filter to decode the PFC code to a location and then recode a new place cell code for that location. In the robot, the place cell vector output of PFC is used to physically displace the robot and then generate a new place cell coded input to the PFC, replacing part of the software recoding procedure that was required otherwise. We demonstrate how this integrated sensory-motor system can learn simple navigation sequences and then, importantly, how it can synthesize novel efficient sequences based on prior experience, as previously demonstrated (Cazin et al. 2019). This contributes to the understanding of hippocampal replay in novel navigation sequence formation and the important role of embodiment.Prefrontal cortex; Hippocampus; Navigation; Replay; Sharp wave ripple; Reservoir computing; Traveling sales person; Reinforcement learning ;Spatial Cognition; Memory Consolidation; Natural-Language; Reverse Replay; Grid Cells; Model; Reactivation; Cortex; State; Representatio",'Springer Science and Business Media LLC',Real-time sensory–motor integration of hippocampal place cell replay and prefrontal sequence learning in simulated and physical rat robots for novel path optimization,,,10.1007/s00422-020-00820-2,core
327076110,2020-10-29T00:00:00,"Deep Learning-based object detectors can enhance the capabilities of smart
camera systems in a wide spectrum of machine vision applications including
video surveillance, autonomous driving, robots and drones, smart factory, and
health monitoring. Pedestrian detection plays a key role in all these
applications and deep learning can be used to construct accurate
state-of-the-art detectors. However, such complex paradigms do not scale easily
and are not traditionally implemented in resource-constrained smart cameras for
on-device processing which offers significant advantages in situations when
real-time monitoring and robustness are vital. Efficient neural networks can
not only enable mobile applications and on-device experiences but can also be a
key enabler of privacy and security allowing a user to gain the benefits of
neural networks without needing to send their data to the server to be
evaluated. This work addresses the challenge of achieving a good trade-off
between accuracy and speed for efficient deployment of deep-learning-based
pedestrian detection in smart camera applications. A computationally efficient
architecture is introduced based on separable convolutions and proposes
integrating dense connections across layers and multi-scale feature fusion to
improve representational capacity while decreasing the number of parameters and
operations. In particular, the contributions of this work are the following: 1)
An efficient backbone combining multi-scale feature operations, 2) a more
elaborate loss function for improved localization, 3) an anchor-less approach
for detection, The proposed approach called YOLOpeds is evaluated using the
PETS2009 surveillance dataset on 320x320 images. Overall, YOLOpeds provides
real-time sustained operation of over 30 frames per second with detection rates
in the range of 86% outperforming existing deep learning models",'Institution of Engineering and Technology (IET)',"YOLOpeds: Efficient Real-Time Single-Shot Pedestrian Detection for Smart
  Camera Applications",http://arxiv.org/abs/2007.13404,,10.1049/iet-cvi.2019.0897,core
334919860,2020-04-20T00:00:00,"Model Predictive Control (MPC) is a powerful control technique that handles
constraints, takes the system's dynamics into account, and optimizes for a
given cost function. In practice, however, it often requires an expert to craft
and tune this cost function and find trade-offs between different state
penalties to satisfy simple high level objectives. In this paper, we use
Reinforcement Learning and in particular value learning to approximate the
value function given only high level objectives, which can be sparse and
binary. Building upon previous works, we present improvements that allowed us
to successfully deploy the method on a real world unmanned ground vehicle. Our
experiments show that our method can learn the cost function from scratch and
without human intervention, while reaching a performance level similar to that
of an expert-tuned MPC. We perform a quantitative comparison of these methods
with standard MPC approaches both in simulation and on the real robot.Comment: 14 pages, 6 figures, submitted to L4DC 202",,"Practical Reinforcement Learning For MPC: Learning from sparse
  objectives in under an hour on a real robot",http://arxiv.org/abs/2003.03200,,,core
334911372,2020-02-11T00:00:00,"This paper proposes an end-to-end deep reinforcement learning approach for
mobile robot navigation with dynamic obstacles avoidance. Using experience
collected in a simulation environment, a convolutional neural network (CNN) is
trained to predict proper steering actions of a robot from its egocentric local
occupancy maps, which accommodate various sensors and fusion algorithms. The
trained neural network is then transferred and executed on a real-world mobile
robot to guide its local path planning. The new approach is evaluated both
qualitatively and quantitatively in simulation and real-world robot
experiments. The results show that the map-based end-to-end navigation model is
easy to be deployed to a robotic platform, robust to sensor noise and
outperforms other existing DRL-based models in many indicators.Comment: 6 pages, 7 figures, accepted by ICNSC 202",,Robot Navigation with Map-Based Deep Reinforcement Learning,http://arxiv.org/abs/2002.04349,,,core
401950305,2020-08-05T00:00:00,"The purpose of the research is to improve theoretical and practical aspects of digital strategic concept forming modern Ukrainian society in the context of technological development, opportunities and breakthrough changes. Objectives of the research: 1) to analyze information, artificial intelligence and machine learning as powerful levers to attract huge amounts of data that revolutionize our lives and production; 2) show the impact of automation on job creation in various areas of work and talent management in digital age; 3) to find out the place and role of information as a digital strategic basis of the new information revolution. Methodology - used systemic and institutional methods that allowed to bring everything into a reliable system for managing complex areas of interdependent activities, which allows you to discover and analyze the components and consistently connect them with each other. The institutional method is necessary for the formation of a holistic view as how the institutional subsystem affects information subsystem functioning. Methods - information scientific modes - general principles - creative relationship of correlation as subject with specific information reality, as well as fixing the results of the subject in gnosis to the world, nature, society, power institutions and their interaction with each other. The result of the research. As a result of the research, information, artificial intelligence and machine learning were analyzed as powerful levers for attracting a huge amount of data that revolutionizes our lives and production; an automation influence on job formation in various fields of work and talent management in digital age is studied; the place and role of information as a digital strategic basis of the new information revolution are clarified. Practical recommendations: 1) to introduce digital strategic concept of modern Ukrainian society in the context of the breakthrough changes development in the organization; 2) to form a new digital thinking taking into account the new digital culture in order to counteract the negative technologies of the new day; 3) to form computerization with the help of artificial intelligence systems and robots, which will increase productivity, remove barriers to innovation, create new opportunities for small businesses, startups, reduce barriers to market entry, implement software as a serviceАктуальність теми дослідження. Актуальність дослідження у тому, що сучасні умови діджиталізації вимагають формування моделі цифрової стратегії сучасного світу у контексті розвитку технологій, можливостей і проривних змін, що виступають як конкурентоспроможна сила, в основі якої експоненціональний розвиток, цифрова інформація, довгострокові впливи на економіку, бізнес, суспільство, людину, національне і глобальне. Мета дослідження – удосконалення теоретичних і практичних аспектів формування концепції цифрової стратегії сучасного українського суспільства у контексті розвитку технологій, можливостей і проривних змін. Завдання дослідження: 1) проаналізувати інформацію, штучний інтелект і машинне навчання як потужні важелі для залучення величезної кількості даних, що революціонізують наше життя і виробництво; 2) показати вплив автоматизації на формування робочих місць у різноманітних сферах праці та управління талантами у цифрову епоху; 3) з’ясувати місце і роль інформації як основи цифрової стратегії нової інформаційної революції. Методологія - використано системний та інституціональний методи, що дозволили все привести в надійну систему для керування складними сферами взаємозалежної діяльності, яка дозволяє розкривати й аналізувати складові компоненти і послідовно сполучати їх один з одним. Інституційний метод необхідний для формування цілісного уявлення про те, як інституціональна підсистема впливає на функціонування інформаційної підсистеми. Методи - модуси інформаціології – загальні принципи-постулати креативного відношення кореляції суб’єкта-креатора до конкретно-наявної інформаціологічної дійсності, а також фіксації результатів діяльності суб’єкта у гностизації до світу, природи, соціуму, інститутів влади та їх взаємодії один з одним. Результат дослідження. У результаті проведеного дослідження проаналізовано інформацію, штучний інтелект і машинне навчання як потужні важелі для залучення величезної кількості даних, що революціонізують наше життя і виробництво; досліджено вплив автоматизації на формування робочих місць у різноманітних сферах праці та управління талантами у цифрову епоху; з’ясовано місце і роль інформації як основи цифрової стратегії нової інформаційної революції. Практичні рекомендації: 1) упроваджувати концепцію цифрової стратегії сучасного українського суспільства у контексті розвитку проривних змін в організації; 2) формувати нове цифрове мислення з врахуванням нової цифрової культури, щоб протидіяти негативним технологіям нового дня; 3) формувати комп’ютеризацію за допомогою систем штучного інтелекту та роботів, що приведуть до підвищення продуктивності, усунення перешкод для інновацій, появі нових можливостей для малого бізнесу, стартапів, зниження бар’єрів для входження на ринку, реалізації програмного забезпечення як послуг",'Journal of Babylon Center for Humanities Studies',"ФОРМУВАННЯ КОНЦЕПЦІЇ ЦИФРОВОЇ СТРАТЕГІЇ СУЧАСНОГО УКРАЇНСЬКОГО СУСПІЛЬСТВА У КОНТЕКСТІ РОЗВИТКУ ТЕХНОЛОГІЙ, МОЖЛИВОСТЕЙ І ПРОРИВНИХ ЗМІН",,,,core
334926325,2020-03-27T00:00:00,"Wireless power transfer (WPT) is a promising technology to prolong the
lifetime of the sensors and communication devices, i.e., workers, in completing
crowdsourcing tasks by providing continuous and cost-effective energy supplies.
In this paper, we propose a wireless powered spatial crowdsourcing framework
which consists of two mutually dependent phases: task allocation phase and data
crowdsourcing phase. In the task allocation phase, we propose a Stackelberg
game based mechanism for the spatial crowdsourcing platform to efficiently
allocate spatial tasks and wireless charging power to each worker. In the data
crowdsourcing phase, the workers may have an incentive to misreport its real
working location to improve its utility, which causes adverse effects to the
spatial crowdsourcing platform. To address this issue, we present three
strategyproof deployment mechanisms for the spatial crowdsourcing platform to
place a mobile base station, e.g., vehicle or robot, which is responsible for
transferring the wireless power and collecting the crowdsourced data. As the
benchmark, we first apply the classical median mechanism and evaluate its
worst-case performance. Then, we design a conventional strategyproof deployment
mechanism to improve the expected utility of the spatial crowdsourcing platform
under the condition that the workers' locations follow a known geographical
distribution. For a more general case with only the historical location data
available, we propose a deep learning based strategyproof deployment mechanism
to maximize the spatial crowdsourcing platform's utility. Extensive
experimental results based on synthetic and real-world datasets reveal the
effectiveness of the proposed framework in allocating tasks and charging power
to workers while avoiding the dishonest worker's manipulation.Comment: 14 pages. arXiv admin note: substantial text overlap with
  arXiv:1907.0892",'Institute of Electrical and Electronics Engineers (IEEE)',Mechanism Design for Wireless Powered Spatial Crowdsourcing Networks,http://arxiv.org/abs/2003.12228,,10.1109/TVT.2019.2952926,core
337296900,2020-11-16T00:00:00,"Navigating fluently around pedestrians is a necessary capability for mobile
robots deployed in human environments, such as buildings and homes. While
research on social navigation has focused mainly on the scalability with the
number of pedestrians in open spaces, typical indoor environments present the
additional challenge of constrained spaces such as corridors and doorways that
limit maneuverability and influence patterns of pedestrian interaction. We
present an approach based on reinforcement learning (RL) to learn policies
capable of dynamic adaptation to the presence of moving pedestrians while
navigating between desired locations in constrained environments. The policy
network receives guidance from a motion planner that provides waypoints to
follow a globally planned trajectory, whereas RL handles the local
interactions. We explore a compositional principle for multi-layout training
and find that policies trained in a small set of geometrically simple layouts
successfully generalize to more complex unseen layouts that exhibit composition
of the structural elements available during training. Going beyond walls-world
like domains, we show transfer of the learned policy to unseen 3D
reconstructions of two real environments. These results support the
applicability of the compositional principle to navigation in real-world
buildings and indicate promising usage of multi-agent simulation within
reconstructed environments for tasks that involve interaction",,"Robot Navigation in Constrained Pedestrian Environments using
  Reinforcement Learning",http://arxiv.org/abs/2010.08600,,,core
343943352,2020-01-01T08:00:00,"As we are entering the big data era, data are becoming high-volume, complex, and heterogeneous. Nowadays, almost every field and sector of the modern society is being impacted by big data, ranging from energy system to health care, and from business to government. The excessive amount of data contains potential and highly useful values. Extracting values from these data and utilize it to support the decision-making process is of paramount importance to improve productivity in business and enhance the well-being of our society. However, data-driven decision-making also arises with many challenges, such as feature selection, data fusion, and real-time decision-making. To overcome these challenges, this dissertation will develop deep learning algorithms and frameworks for data-driven decision-making. This work is composed of three major parts: automatic feature extraction, heterogeneous data fusion, and deep reinforcement learning (DRL) based data-driven frameworks. This dissertation will first investigate the automatic feature extraction to avoid the burden of the  feature engineering\u27\u27 process. To make sure the feature extractor can be deployed to different scenarios, domain adaptive feature extraction will be studied. This dissertation will then examine the heterogeneous data fusion problem where feature refinement, normalization, transformation, and fusion will be investigated. With the automatic feature extraction and heterogeneous data fusion, this dissertation will develop data-driven decision-making frameworks for real-world applications, including robot-assisted pedestrian regulation and real-time electric vehicle (EV) charging management. Numerous experiments will be conducted to verify the effectiveness of the proposed frameworks",DigitalCommons@URI,Deep Learning for Data-Driven Decision-Making,,,,core
355099865,2020-01-01T00:00:00,"Abstract: Poor management practices of road transport assets posed a challenge to the sustainable development of the transport system in developing countries like Nigeria. Studies in the past focused mainly on the performance of road construction process. However, few studies have evaluated the effect of the fourth industrial revolution (4.0IR) on the road transport assets in developing countries such as Nigeria. The current study aimed at assessing the effect of the fourth industrial revolution towards improving the management practice of road transport assets. Survey instruments were administered to project and facility managers in the Nigerian road construction sector of the economy using a proportionate random sampling technique. Partial least square structural equation modelling was used for data analysis utilising the Warp 7.0 PLS-SEM software algorithm. The software calculates p-values with WarpPLS based on non-parametric algorithms, resampling or stable algorithms and thus does not require that the variables to be normally distributed. The study concluded that 4.0IR drivers have a moderate effect change on the management practice of road transport assets in Nigeria at the moment. The findings imply that management of road assets in Nigeria would moderately improve due to 4.0IR technologies resulting in transport, safety and general efficiency and effectiveness of road networks in Nigeria. The study identified 4.0IR drivers to include; robotics, mobility, virtual and augmented reality, internet of things and cloud computing, machine learning, artificial intelligence, blockchain, 3D printing drones that are built with an attached 3D printer, (the drone hangs a 3D printing nozzle that's fed plastic, concrete mix or other material from a tube connected to the top of the drone's printing path that precisely plotted by software, for a promised printing accuracy of 0.1mm),and digital engineering. This study emanated from the government reports and past studies in the area of road transport asset management practice which the study investigated the major causes of poor practices and assessed the effect of the fourth industrial revolution on the practice",,Effect of the Fourth Industrial Revolution on Road Transport Asset Management Practice in Nigeria,,,,core
355100275,2020-01-01T00:00:00,"Abstract: DC motor is extensively used in various industrial applications such as robotics, automobiles, toys and many other motoring applications. This is attributable to their extraordinary flexibility, durability and low implementation cost. To obtain the desired output based on the use of the DC motor, it is imperative to control the speed, position, torque and other variables of the DC motor. Many classical techniques have been utilized in the past to control the DC motor, however, such classical methods typically take a long time, particularly when used for complex nonlinear systems. The use of metaheuristic algorithms as a way of implementing artificial intelligence (AI) in this field has proven to be highly effective in overcoming these shortcomings. In recent decades, metaheuristic algorithms have become increasingly prevalent due to their tremendous success in addressing several real-world optimization challenges in various areas of human activities, ranging from economic, pharmaceutical and industrial applications to intellectual applications. This review presents the use of different types of metaheuristic algorithm techniques in optimizing the parameters of the proportional-integralderived (PID) controller in order to control the DC motor. For a more robust review, the application of various forms of PID controller, as well as different types of DC motors, is considered",,Optimization of PID controller with metaheuristic algorithms for DC motor drives : review,,,,core
359041876,2020-11-20T11:14:13,"The use of robots in search and rescue is gaining particular interest, but singular skills are required to ensure efficient deployments in real missions.
To face this problem, there is a need to develop more intuitive control interfaces.
Moreover, to ensure high performance during high cognitive demanding tasks, such as in search and rescue missions, there is a need to combine both humanâs and robotâs skills.
Respectively, humans and robots can adapt to new situations and optimize the execution of repetitive tasks. In this regard, novel share-control techniques have been developed to adapt the human-robot interaction, but to dynamically adapt this interaction, the information about the human state is missing.

To address these problems, I first developed a novel wearable system that enhances the control of drones providing a more intuitive flying experience. 
As shown in chapter 2, this wearable system tracks the upper body movements and translates them into commands for a drone.
The system has been tested with a simulator and demonstrated for the teleoperation of a real drone.
Moreover, to ensure enduring operations, I proposed a method that drastically reduces communication, and consequently, improves energy efficiency by 11.9%.

Second, in chapter 3, I presented a machine-learning approach for monitoring the cognitive workload level of a drone operator involved in search and rescue missions.
My model combines the information of different features extracted from physiological signals, such as respiratory activity, electrocardiogram, photoplethysmogram, and skin temperature, acquired in a non-invasive way.
To reduce both subject and day inter-variability of the signals, I explored different feature normalization techniques.
Moreover, I adjusted the learning method for support vector machines to allow subject-specific optimizations.
On a test set acquired from 34 volunteers, the proposed model distinguished between low and high cognitive workloads with an average accuracy of 87.3% and 91.2%, while controlling a drone simulator using both a traditional controller and the proposed FlyJacket design, respectively.

Third, in chapter 4, I presented the integration of the method developed for cognitive workload monitoring, on a new single wearable embedded system, that also integrates the proposed drone controller design.
On the hardware side, it includes a multi-channel physiological signals acquisition and a low-power processing platform that is suited for cognitive workload monitoring.
On the software side, the proposed system includes novel energy-aware bio-signal processing and embedded machine learning methods.
Moreover, to exploit the trade-offs between the required accuracy of the available energy of the system, I presented a new application of the concept of a scalable machine-learning method with different power-saving levels.
Results showed that the proposed self-aware approach yields an increase of 78% of the battery lifetime without really affecting the classification accuracy.

The proposed system, comprising a drone controller integrating a unit for cognitive workload monitoring, lays the foundations for the development of new-generation human-robot interfaces.
With the information about the human state, we can close the loop of traditional share-control techniques, which will be able to dynamically adapt the level of interaction with semi-autonomous machines based on the need for the operator","Lausanne, EPFL",Wearable and Self-Aware Machine Learning System for Online Cognitive Workload Monitoring and Drone Control,,,10.5075/epfl-thesis-10279,core
286777864,2020-01-01T00:00:00,"The major issue during crisis events occurrence is the immediate reaction and the provision of emergency services to humans in imminent danger. As human survival chances are negatively correlated with detection time, the response time is a vital factor in Search and Rescue (SAR) operations, thus potential delays may have dramatic and even lethal results. Moreover, the safety of rescue workers is another major issue and must be ensured in any circumstance. Unmanned Aerial Vehicles (UAVs) technology can provide a critical and multifaceted support in SAR missions. Given the fact that the faster these operations are planned and executed, the greater the chances of success they get, these aerial robots are ideal candidates for such applications owed to their ability of being nimble, quick-moving and easily programmed to exhibit autonomous behaviors. Hence, with the aim to increase the prospects of distressed people to remain alive, robotic rescue systems—either remote controlled or autonomous ones—must be instantaneously available, to provide a robust helping hand to rescuers, thus boosting their performance and ensuring their own safety. The doctoral dissertation in hand deals with the study and implementation of a novel fully autonomous UAV-based rescue platform for life-saving services, by combining Global Navigation Satellite System (GNSS) methods and state-of-the-art deep learning techniques. Specifically, the proposed Autonomous Aerial Rescue System (AARS), namely the ROLFER (RObotic Lifeguard For Emergency Rescue), is capable of precisely locating and providing instantaneous emergency services to humans in peril during critical events. The main system’s components are: a completely autonomous rescue UAV and its bearing equipment, the Ground Control Station (GCS) including all the apparent software and hardware for the system’s proper functionality, as well as the wearable equipment carried by humans in peril for the distress signal transmission. The rescue system’s Key Performance Indicators (KPI) are to: (i) be fully autonomous, i.e. guided solely by data provided by the distressed human’s wearable equipment without requiring to include human-in-the-loop, (ii) provide fast reaction as possible, i.e. to minimize the intervention time a UAV needs to provide invaluable assistance to a human in the context of an emergency and (iii) precisely detect the human in peril as well to accurately release the rescue equipment. The importance of not wasting the critical response time, as well as taking into serious consideration any other vital parameter during a SAR operation is obvious. The main objectives of this research are summarized as follows: The study, design and implementation of a fully autonomous aerial robotic system for the provision of life-saving services to humans in peril, The deployment of two human rescue scenarios in both maritime and terrestrial environments, The detection of human in peril using several accurate advanced Global Positioning System (GPS)/Global Navigation Satellite System (GNSS)-based localization methods, The detection, tracking and localization of human in peril using advanced image processing techniques as well as the precise rescue apparatus release, The hazard identification and risk analysis of the proposed AARS for the elimination of potential threats to both human in peril and rescue workers, by applying System Theoretic Process Analysis (STPA). Thus, the structure of the dissertation in hand is mainly based on the aforementioned objectives. In particular, Chapter 1 introduces the utilization of robotic systems in SAR operations including some crucial information about UAVs, a review of the related literature, as well the contribution of this dissertation within the state of art. Chapter 2 presents the proposed aerial rescue support system’s architecture and functionality, concerning details about both software and hardware configuration, as well as the system’s performance evaluation in terms of both accuracy and intervention time. Moreover, Chapter 3 addresses the issue of precise localization of the distressed human by implementing advanced geolocalization techniques (e.g. Real Time Kinematics (RTK)-GPS). Thus, an enhanced positioning system which grounds on a specially customized wearable device is proposed, allowing the precise positioning of human in peril. Both hardware and software configuration of the designed novel prototype are described. Experimental results from real flight tests of the proposed AARS and the effectiveness of the implemented moving-base RTK-GPS-based wearable apparatus are also discussed in detail. Chapter 4 presents the vision-based punctual and precise detection of humans in peril from the proposed AARS. In particular, human detection is implemented based on two different approaches:a)by utilizing deep learning techniques on-board the autonomous system’s rescue UAV for the human in peril in maritime environmentb)by combining RTK-GPS techniques and advanced image processing algorithms on the proposed rescue system’s GCS for the human in peril in terrestrial environment. Two suitable datasets, including human in peril in both maritime and terrestrial environments have been created for the computer vision algorithms training. Hardware and software configuration, as well experimental results for the accurate human detection and rescue apparatus fall down are presented in detail.  In Chapter 5, a hazard identification and risk analysis of the proposed AARS, for the elimination of potential threats to both human in peril and rescue workers from the proposed system functionality is presented. The objectives of this analysis are a) to introduce the idea of analyzing the system by dividing it in separate operational modes and b) to feature the advantages of applying a systemic STPA-based safety approach on a SAR mission recruiting the use of UAVs. The dissertation is concluded in Chapter 6 where our findings are summarized and future work is mentioned.The doctoral dissertation in hand has been conducted in the Laboratory of Robotics and Automation of the Department of Production and Management of DUTh, which expertizes in such systems implementations.Το μεγαλύτερο πρόβλημα κατά την διάρκεια καταστάσεων εκτάκτου ανάγκης είναι η άμεση αντίδραση και η παροχή βοηθειών πρώτης ανάγκης στα υπό κίνδυνο άτομα. Λαμβάνοντας υπ’ όψιν ότι οι πιθανότητες επιβίωσης του υπό κίνδυνο ατόμου μειώνονται δραματικά με την πάροδο του χρόνου ανίχνευσης του, ο χρόνος αντίδρασης αποτελεί καίριας σημασίας παράγοντα κατά τη διάρκεια των επιχειρήσεων έρευνας και διάσωσης καθώς ενδεχόμενες καθυστερήσεις μπορεί να οδηγήσουν σε δραματικά αποτελέσματα. Επιπλέον, η ασφάλεια των μελών των ομάδων διάσωσης αποτελεί ένα άλλο μείζoν θέμα και θα πρέπει να διασφαλιστεί σε κάθε περίπτωση. Τα μη-επανδρωμένα εναέρια οχήματα (UAVs) μπορούν να παρέχουν αποφασιστικής σημασίας και πολυπρόσωπη στήριξη στις επιχειρήσεις έρευνας και διάσωσης αφού πρόκειται για γρήγορα και ευέλικτα εναέρια ρομποτικά συστήματα τα οποία μπορούν να προγραμματιστούν εύκολα προκειμένου να λειτουργούν πλήρως αυτόνομα. Συνεπώς, έχοντας ως στόχο να αυξήσουν τις πιθανότητες διάσωσης των υπό κίνδυνο ατόμων, τα ρομποτικά εναέρια οχήματα-είτε τηλεκατευθυνόμενα είτε πλήρως αυτόνομα- οφείλουν να είναι πάντα διαθέσιμα να προσφέρουν ένα χρήσιμο χέρι βοηθείας στις διασωστικές ομάδες ενισχύοντας παράλληλα την απόδοση τους και εξασφαλίζοντας την σωματική ακεραιότητα των εργαζόμενων σε αυτές. Η παρούσα διδακτορική διατριβή πραγματεύεται την μελέτη και την υλοποίηση ενός καινοτόμου πλήρως αυτόνομου ρομποτικού συστήματος διάσωσης βασισμένο στη τεχνολογία των UAVs. Η υλοποίηση και η λειτουργικότητα του προτεινόμενου συστήματος βασίζεται στην συνδυαστική χρήση προηγμένων μεθόδων γεωεντοπισμού και αλγορίθμων βαθιάς μάθησης. Συγκεκριμένα, το προτεινόμενο σύστημα με το ακρωνύμιο ROLFER (Robotic Lifeguard For Emergency Rescue), καθιστά εφικτό τον ακριβή εντοπισμό και την παροχή του κατάλληλου εξοπλισμού βοήθειας στα υπό κίνδυνο άτομα κατά τη διάρκεια καταστάσεων εκτάκτου ανάγκης. Τα κύρια μέρη του προτεινόμενου συστήματος είναι: το πλήρως αυτόνομο διασωστικό UAV με τον απαραίτητο φερόμενο εξοπλισμό βοήθειας, ο επίγειος σταθμός βάσης με το απαραίτητο υλικό (hardware) και λογισμικό (software) για την εύρυθμη λειτουργία του συστήματος, καθώς επίσης και ο κατάλληλος φορητός εξοπλισμός του χρήστη (άτομο σε κίνδυνο) για την εκπομπή του σήματος κινδύνου. Επιπλέον, η μελέτη και υλοποίηση ενός σεναρίου διάσωσης σε υδάτινο και χερσαίο περιβάλλον εξετάζεται διεξοδικά. Οι κύριοι στόχοι της παρούσας διδακτορικής διατριβής συνοψίζονται ως εξής: Η μελέτη, ο σχεδιασμός  και η υλοποίηση ενός πλήρως αυτόνομου ρομποτικού συστήματος εναέριας παροχής βοήθειας για την παροχή απαραίτητου εξοπλισμού σε άτομα που βρίσκονται σε κίνδυνο, Η ανάπτυξη σεναρίου διάσωσης σε θαλάσσιο και χερσαίο περιβάλλον, Ο εντοπισμός του ατόμου σε κίνδυνο με τη χρήση διαφόρων προηγμένων μεθόδων που βασίζονται στη χρήση γεωεντοπισμού (παγκόσμιο σύστημα δορυφορικής πλοήγησης), Ο εντοπισμός, παρακολούθηση και χωροθέτηση του ατόμου σε κίνδυνο με τη χρήση προηγμένων τεχνικών επεξεργασίας εικόνας για την απελευθέρωση του κατάλληλου εξοπλισμού με υψηλή ακρίβεια, Η ανάλυση επικινδυνότητας του προτεινόμενου εναέριου ρομποτικού συστήματος με εφαρμογή της εξειδικευμένης μεθόδου STPA, για την εύρεση των προδιαγραφών ασφαλείας του καθώς και για την ελαχιστοποίηση των πιθανών απειλών τόσο για το άτομο που κινδυνεύει όσο και για την ομάδα διάσωσης. Συνεπώς, η δομή της παρούσας διδακτορικής διατριβής βασίζεται στους παραπάνω σκοπούς. Συγκεκριμένα, στο Κεφάλαιο 1 γίνεται εισαγωγή της χρήσης των ρομποτικών συστημάτων στις επιχειρήσεις έρευνας και διάσωσης, συμπεριλαμβάνοντας χρήσιμες πληροφορίες σχετικά με τα μη-επανδρωμένα εναέρια οχήματα (UAVs), εκτενή ανασκόπηση της σχετικής βιβλιογραφίας, καθώς επίσης  και την συνεισφορά της παρούσας διατριβής στην επιστήμη. Στο Κεφάλαιο 2 παρουσιάζεται η δομή και η αρχιτεκτονική του προτεινόμενου διασωστικού συστήματος συμπεριλαμβανομένων και των λεπτομερειών τόσο του υλικού (hardware) όσο και του λογισμικού (software), καθώς επίσης και την αξιολόγηση του σε θέματα σχετικά με την ακρίβεια και του χρόνου παρέμβασης του. Στο Κεφάλαιο 3 εξετάζεται ο ακριβής εντοπισμός της θέσης του υπό κίνδυνο ατόμου με χρήση προηγμένων τεχνικών γεωεντοπισμού (RTK-GPS). Συγκεκριμένα παρουσιάζεται ένα βελτιωμένο σύστημα εντοπισμού το οποίο βασίζεται σε μια προσαρμοσμένη φορητή συσκευή που επιτρέπει τον εντοπισμό της θέσης του ατόμου σε κίνδυνο με ακρίβεια μερικών εκατοστών. Αποτελέσματα από την αξιολόγηση της χρήσης της συγκεκριμένης πρωτότυπης συσκευής περιγράφονται λεπτομερώς. Η υλοποίηση και εφαρμογή προηγμένων αλγορίθμων επεξεργασίας εικόνας για την βέλτιστη δυνατή ακρίβεια εντοπισμού της θέσης του ατόμου που κινδυνεύει παρουσιάζεται στο Κεφάλαιο 4. Συγκεκριμένα, η υλοποίηση αυτή πραγματοποιείται με δύο διαφορετικές προσεγγίσεις: α) με την χρήση αλγορίθμων βαθιάς μάθησης ενσωματωμένων στο αυτόνομο μη-επανδρωμένο εναέριο όχημα, για τον εντοπισμό του υπό-κίνδυνο ατόμου σε υδάτινο περιβάλλον, β) με συνδυαστική χρήση προηγμένων μεθόδων γεωεντοπισμού (RTK-GPS) και προηγμένων αλγορίθμων υπολογιστικής όρασης στον επίγειο σταθμό βάσης του συστήματος, για τον εντοπισμό του ατόμου σε χερσαίο περιβάλλον. Για το σκοπό αυτό, έχουν δημιουργηθεί δύο διαφορετικά σετ δεδομένων (dataset) που εμπεριέχουν εικόνες ατόμων σε κίνδυνο τόσο σε χερσαίο όσο και σε υδάτινο περιβάλλον. Με βάση αυτά τα σετ δεδομένων πραγματοποιήθηκε η εκπαίδευση των αλγορίθμων υπολογιστικής όρασης. Επιπλέον, παρουσιάζονται αναλυτικά πειραματικά αποτελέσματα από δοκιμές του συστήματος σε πραγματικές συνθήκες σχετικά με την ακρίβεια του εντοπισμού του ανθρώπου όσο και της απελευθέρωσης του διασωστικού εξοπλισμού. Τέλος, το Κεφάλαιο 5 παρουσιάζει την ανάλυση επικινδυνότητας του προτεινόμενου συστήματος για την απαλοιφή πιθανών απειλών και κινδύνων που θα μπορούσε να ελλοχεύει η χρήση του τόσο για το υπό κίνδυνο άτομο όσο και για τις διασωστικές ομάδες. Σκοπός της ανάλυσης αυτής είναι α) να εισάγει την ιδέα της ανάλυσης του συστήματος διαιρώντας το σε επιμέρους φάσεις λειτουργίας και β) να παρουσιάσει τα πλεονεκτήματα της εφαρμογής αναλύσεων επικινδυνότητας που βασίζονται στη συστημική θεωρία και συγκεκριμένα της STPA, στον τομέα των αποστολών έρευνας και διάσωσης με χρήση UAVs. Η παρούσα διατριβή ολοκληρώνεται στο Κεφάλαιο 6 όπου παρουσιάζονται τα συμπεράσματα και τα ευρήματα της υλοποίησης του συστήματος καθώς και η πιθανή μελλοντική δουλειά . Η παρούσα διδακτορική διατριβή εκπονήθηκε στο Εργαστήριο Ρομποτικής και Αυτοματισμού του Τμήματος Μηχανικών Παραγωγής και Διοίκησης του Δ.Π.Θ. που ειδικεύεται σε τέτοιου είδους υλοποιήσεις συστημάτων",Δημοκρίτειο Πανεπιστήμιο Θράκης (ΔΠΘ),Μελέτη και υλοποίηση πλήρως αυτόνομου ιπτάμενου ρομποτικού συστήματος για την παροχή υπηρεσιών εκτάκτου ανάγκης,,,,core
386415663,2020-12-17T00:00:00,"Digitalisierung hat sich in Wirtschaft, Wissenschaft und Gesellschaft als der Change Maker schlechthin etabliert. Infrastrukturen, Arbeitsweisen und Kompetenzen stehen im Vordergrund vieler Debatten und bestimmen mehr und mehr die Zukunftsfähigkeit ganzer Branchen. Wir haben uns offenbar auf den permanenten Wandel bei zunehmender Beschleunigung eingelassen. Aber: Wo geht die Reise tatsächlich hin? Konstituieren sich Gemeinschaften ausschließlich im Wechselspiel hybrider Realitäten? Sind große Datenmengen Bedrohung oder Chance? Können wir diese überhaupt verarbeiten oder bedarf es dafür grundlegend veränderter Werkzeuge und Methoden – wie Visual Analytics, Virtuelle Rekonstruktion, Virtual Engineering, virtueller Assistenten und kooperativer VR? Waren IT-Innovationen bis vor kurzem etwas für Digital-Experten*innen so sind hybride Gemeinschaften in virtuellen Realitäten mittlerweile Alltag. Doch worauf müssen sich Führungskräfte einstellen? Digitalisierung bedeutet neue Möglichkeiten für Öffnung, Transparenz und Partizipation. Kommt es in diesem Zuge auch zu einem Revival humanzentrierter Managementaktivitäten? [... aus der Einleitung]:Gemeinschaften in Neuen Medien. Von hybriden Realitäten zu hybriden Gemeinschaften XXIX
Communities in New Media. From hybrid realities to hybrid communities XXXIV

A Eingeladene Vorträge 1
A.1 Interaktive Online Formate zur Wissensteilung: Systematisierung und Handlungsempfehlung für geeignete IT-Tools 1
A.2 Von der Hand in den Kopf in die Stadt 9

B Erfahrungen mit digitaler Praxis 10
B.1 Den Gottesdienst von zu Hause mitfeiern 10
B.2 Konzeption und Evaluation des Kompetenzzentrums Medien 19
B.3 Supporting Learning in Art History – Artificial Intelligence in Digital Humanities Education 28
B.4 Detecting Treasures in Museums with Artificial Intelligence 36

C Digitale Entwicklung in Wirtschaft und Industrie 49
C.1 Triebkräfte der digitalen Partizipation: Was Online-Community-Mitglieder zur proaktiven Beteiligung motiviert 49
C.2 Online-Panel: Communities und Netzwerke als Treiber des digitalen Wandels: Erfahrungen, Perspektiven und Ausblick 60
C.3 Digitale Innovationen im Handwerk 65
C.4 Online-Panel: Conversational Platforms als strategisches Digitalisierungsinstrument 75

D Partizipation 80
D.1 Ein systematisch gestalteter Softwareprototyp zur Erhöhung der Partizipationsbereitschaft 80
D.2 Gamification as a Means to Improve Stakeholder Management in Urban Planning Participation 90
D.3 The Right Reaction: Entwicklung und Evaluation eines emotionsbasierten Software-Prototypen 99

E Cases of digitizing higher education – a global perspective 110
E.1 EdTec Implementation in a global higher education network. Empirical data from a field study in South Asia 110
E.2 Use-Case Studie eines auf der Nutzung von Handlungsfehlern basierenden AR-Lernsystems zur kritischen Reflexion der technischen Umsetzbarkeit 126
E.3 Organizational models in virtual teaching cooperation – documentation and evaluation of organisational didactics in a collaborative higher education project 133
E.4 Ein Fall für zwei Hochschulen: Entwicklung eines modularen Manuals zur Gestaltung von Fallstudienseminaren im virtuellen Raum 144

F Future learning in der beruflichen Bildung 150
F.1 Potenziale für das technologiebasierte Lehren und Lernen in der Weiterbildung 150
F.2 Mediennutzungskonzepte an Berufsschulen – Webseitenanalyse zur Selbstdarstellung der digitalen Kompetenz 164
F.3 Spielend leicht Veränderungen lernen – Serious Games in der Schulungsumgebung von Unternehmen 173
F.4 Game-Based Learning in der beruflichen Bildung 179

G Methoden und Technologien des Assessments 186
G.1 Itempool-Management mit Microsoft Excel: Eine UX-Studie 186
G.2 KiWI-Kompetenzmodellentwicklung in der Wirtschaftsinformatik 195
G.3 „Nichts als die Wahrheit?“ – eine empirische Untersuchung des Zusammenhangs zwischen persönlichkeits- und nutzerbezogenen Faktoren und der Suggestibilität für Fake News im Internet 204
G.4 Decision-making style and trusting stance at the workplace: a socio-cultural approach 217

H Exploring Digital Realities empirically 226
H.1 Who gets the fame, who is to blame? Empirical exploration of responsibility attribution in HCI 226
H.2 VibTacX: A taxonomy for vibro-tactile patterns 236
H.3 Das Robot Impression Inventory – Ein modulares Instrument zur Erfassung des subjektiven Eindrucks von Robotern 244
H.4 Augmented Reality Passenger Information on Mobile Public Displays 250

I Teaching in Open Education 258
I.1 Parcours on Gamification – Ein Train-the-Trainer-Konzept zur Steigerung der Gamification-Readiness 258
I.2 Lehren mit OER: Förderung von Kompetenzen für Lehrende an Hochschulen für offene Bildung auf spielerischem Weg 264
I.3 Digitale Lehr und Lernunterstützung an deutschen Universitäten – Anforderungen und Rahmenbedingungen für die Implementierung einer Mentoring Workbench 279
I.4 Nach dem sog. MOOC-Hype: Welche kritischen Fragen an die Hochschullehre bleiben 289
I.5 Conducting Oral Examinations Virtually using MS Teams – An Insightful Experience Report 294

J Digitale Lern- und Spielkulturen 299
J.1 Spielerischer Zugang zu MINT-Studiengängen – das Serious Game des Learn&Play Projekts als Anwendungsbeispiel 299
J.2 Entwicklung und Evaluation digitaler Lernspiele – Wissenschaftliche Befunde jenseits des Entertainment 306
J.3 Ausgespielt? Zu Risiken und Nebenwirkungen von Gamification 318

K Betriebliche Weiterbildung 332
K.1 Leading Digital Change – Management of Hybridity and Change in Education and Social Service Institutions 332
K.2 Use Cases of Enterprise Social Software in Consulting: A Practice Perspective 342
K.3 Betriebliche Weiterbildung in sächsischen Klein- und Kleinstunter nehmen – arbeitsplatzintegriert und digital gestützt? 353
K.4 Wie „Change Maker“ Visionen für den digitalen Wandel an Bildungs einrichtungen des Handels entwickeln und umsetzen – ein Praxisbeispiel 364

L Digitalisierung im Lehramtsstudium 370
L.1 Anknüpfungspunkte zur Integration informatischer Inhalte und Kompetenzen in der Grundschule am Beispiel sächsischer Lehrpläne 370
L.2 Digitalisierungsbezogene Kompetenzen von Lehrenden in den Lehramtsstudiengängen – Entwicklung eines Kompetenzrahmens 377
L.3 DigiBlock – E-Learning im Blockpraktikum A im Lehramt an berufsbildenden Schulen 385

M Lehren und Lernen 391
M.1 Jump starting e-learning: the impact of COVID-19 on perceived learning success – A real-time case study 391
M.2 Online-Lehre im Lockdown: Analyse des Nutzungsverhaltens von kollaborativen Werkzeugen durch Studierende und Lehrende im Fachhochschul- und Berufsschulkontext 403
M.3 Teaching in a crisis? Guidance for digital education in Pandemic Times 413
M.4 Mit dem MINTcoach auf Mission 422
M.5 Onboarding in Virtuellen Kollaborativen Umgebungen – Implikationen für Lehre und Betrieb 432
M.6 Modulare Selbstlernangebote auf Basis von Videotutorien zur Vermittlung digitaler Forschungsmethoden in den Geisteswissenschaften – Forschungsstand und curriculare Perspektiven 441

N Wissenskollaboration im betrieblichen Kontext 452
N.1 Digitalisierung als Treiber in der beruflichen Bildung – Entwicklung eines Instruments zur Erfassung von Indikatoren für die Akzeptanz von virtuellen Lernortkooperationen 452
N.2 Digitaler Wissenstransfer in der beruflichen Bildung – Potentiale eines Online-Berichtsheftes 470
Autorenverzeichnis 476Digitisation has established itself as the change maker par excellence in business, science and society. Infrastructures, working methods and skills are at the forefront of many debates and increasingly determine the future viability of entire industries. We have obviously embraced the permanent change with increasing acceleration. But: Where is the journey really going? Do communities constitute themselves exclusively in the interplay of hybrid realities? Are large amounts of data a threat or an opportunity? Can we process them at all or do we need fundamentally different tools and methods – such as visual analytics, virtual reconstruction, virtual engineering, virtual assistants and cooperative VR? Until recently, IT innovations were something for digital experts*, but hybrid communities in virtual realities are now part of everyday life. But what do managers have to prepare for? Digitalisation means new opportunities for openness, transparency and participation. Will this also lead to a revival of human-centred management activities? [... from the introduction]:Gemeinschaften in Neuen Medien. Von hybriden Realitäten zu hybriden Gemeinschaften XXIX
Communities in New Media. From hybrid realities to hybrid communities XXXIV

A Eingeladene Vorträge 1
A.1 Interaktive Online Formate zur Wissensteilung: Systematisierung und Handlungsempfehlung für geeignete IT-Tools 1
A.2 Von der Hand in den Kopf in die Stadt 9

B Erfahrungen mit digitaler Praxis 10
B.1 Den Gottesdienst von zu Hause mitfeiern 10
B.2 Konzeption und Evaluation des Kompetenzzentrums Medien 19
B.3 Supporting Learning in Art History – Artificial Intelligence in Digital Humanities Education 28
B.4 Detecting Treasures in Museums with Artificial Intelligence 36

C Digitale Entwicklung in Wirtschaft und Industrie 49
C.1 Triebkräfte der digitalen Partizipation: Was Online-Community-Mitglieder zur proaktiven Beteiligung motiviert 49
C.2 Online-Panel: Communities und Netzwerke als Treiber des digitalen Wandels: Erfahrungen, Perspektiven und Ausblick 60
C.3 Digitale Innovationen im Handwerk 65
C.4 Online-Panel: Conversational Platforms als strategisches Digitalisierungsinstrument 75

D Partizipation 80
D.1 Ein systematisch gestalteter Softwareprototyp zur Erhöhung der Partizipationsbereitschaft 80
D.2 Gamification as a Means to Improve Stakeholder Management in Urban Planning Participation 90
D.3 The Right Reaction: Entwicklung und Evaluation eines emotionsbasierten Software-Prototypen 99

E Cases of digitizing higher education – a global perspective 110
E.1 EdTec Implementation in a global higher education network. Empirical data from a field study in South Asia 110
E.2 Use-Case Studie eines auf der Nutzung von Handlungsfehlern basierenden AR-Lernsystems zur kritischen Reflexion der technischen Umsetzbarkeit 126
E.3 Organizational models in virtual teaching cooperation – documentation and evaluation of organisational didactics in a collaborative higher education project 133
E.4 Ein Fall für zwei Hochschulen: Entwicklung eines modularen Manuals zur Gestaltung von Fallstudienseminaren im virtuellen Raum 144

F Future learning in der beruflichen Bildung 150
F.1 Potenziale für das technologiebasierte Lehren und Lernen in der Weiterbildung 150
F.2 Mediennutzungskonzepte an Berufsschulen – Webseitenanalyse zur Selbstdarstellung der digitalen Kompetenz 164
F.3 Spielend leicht Veränderungen lernen – Serious Games in der Schulungsumgebung von Unternehmen 173
F.4 Game-Based Learning in der beruflichen Bildung 179

G Methoden und Technologien des Assessments 186
G.1 Itempool-Management mit Microsoft Excel: Eine UX-Studie 186
G.2 KiWI-Kompetenzmodellentwicklung in der Wirtschaftsinformatik 195
G.3 „Nichts als die Wahrheit?“ – eine empirische Untersuchung des Zusammenhangs zwischen persönlichkeits- und nutzerbezogenen Faktoren und der Suggestibilität für Fake News im Internet 204
G.4 Decision-making style and trusting stance at the workplace: a socio-cultural approach 217

H Exploring Digital Realities empirically 226
H.1 Who gets the fame, who is to blame? Empirical exploration of responsibility attribution in HCI 226
H.2 VibTacX: A taxonomy for vibro-tactile patterns 236
H.3 Das Robot Impression Inventory – Ein modulares Instrument zur Erfassung des subjektiven Eindrucks von Robotern 244
H.4 Augmented Reality Passenger Information on Mobile Public Displays 250

I Teaching in Open Education 258
I.1 Parcours on Gamification – Ein Train-the-Trainer-Konzept zur Steigerung der Gamification-Readiness 258
I.2 Lehren mit OER: Förderung von Kompetenzen für Lehrende an Hochschulen für offene Bildung auf spielerischem Weg 264
I.3 Digitale Lehr und Lernunterstützung an deutschen Universitäten – Anforderungen und Rahmenbedingungen für die Implementierung einer Mentoring Workbench 279
I.4 Nach dem sog. MOOC-Hype: Welche kritischen Fragen an die Hochschullehre bleiben 289
I.5 Conducting Oral Examinations Virtually using MS Teams – An Insightful Experience Report 294

J Digitale Lern- und Spielkulturen 299
J.1 Spielerischer Zugang zu MINT-Studiengängen – das Serious Game des Learn&Play Projekts als Anwendungsbeispiel 299
J.2 Entwicklung und Evaluation digitaler Lernspiele – Wissenschaftliche Befunde jenseits des Entertainment 306
J.3 Ausgespielt? Zu Risiken und Nebenwirkungen von Gamification 318

K Betriebliche Weiterbildung 332
K.1 Leading Digital Change – Management of Hybridity and Change in Education and Social Service Institutions 332
K.2 Use Cases of Enterprise Social Software in Consulting: A Practice Perspective 342
K.3 Betriebliche Weiterbildung in sächsischen Klein- und Kleinstunter nehmen – arbeitsplatzintegriert und digital gestützt? 353
K.4 Wie „Change Maker“ Visionen für den digitalen Wandel an Bildungs einrichtungen des Handels entwickeln und umsetzen – ein Praxisbeispiel 364

L Digitalisierung im Lehramtsstudium 370
L.1 Anknüpfungspunkte zur Integration informatischer Inhalte und Kompetenzen in der Grundschule am Beispiel sächsischer Lehrpläne 370
L.2 Digitalisierungsbezogene Kompetenzen von Lehrenden in den Lehramtsstudiengängen – Entwicklung eines Kompetenzrahmens 377
L.3 DigiBlock – E-Learning im Blockpraktikum A im Lehramt an berufsbildenden Schulen 385

M Lehren und Lernen 391
M.1 Jump starting e-learning: the impact of COVID-19 on perceived learning success – A real-time case study 391
M.2 Online-Lehre im Lockdown: Analyse des Nutzungsverhaltens von kollaborativen Werkzeugen durch Studierende und Lehrende im Fachhochschul- und Berufsschulkontext 403
M.3 Teaching in a crisis? Guidance for digital education in Pandemic Times 413
M.4 Mit dem MINTcoach auf Mission 422
M.5 Onboarding in Virtuellen Kollaborativen Umgebungen – Implikationen für Lehre und Betrieb 432
M.6 Modulare Selbstlernangebote auf Basis von Videotutorien zur Vermittlung digitaler Forschungsmethoden in den Geisteswissenschaften – Forschungsstand und curriculare Perspektiven 441

N Wissenskollaboration im betrieblichen Kontext 452
N.1 Digitalisierung als Treiber in der beruflichen Bildung – Entwicklung eines Instruments zur Erfassung von Indikatoren für die Akzeptanz von virtuellen Lernortkooperationen 452
N.2 Digitaler Wissenstransfer in der beruflichen Bildung – Potentiale eines Online-Berichtsheftes 470
Autorenverzeichnis 47",TUDpress,Gemeinschaften in Neuen Medien. Von hybriden Realitäten zu hybriden Gemeinschaften: 23. Workshop GeNeMe'20 Gemeinschaften in Neuen Medien,https://core.ac.uk/download/386415663.pdf,,,core
401560600,2020-12-28T00:00:00,"The world has entered the era of the fourth industrial revolution – a period in which digitalization plays perhaps the most significant role for production, and innovative technologies such as virtual reality, internet of things, artificial intelligence and robotics are fundamentally changing the way people work and the way they live. The backbone of modern society is a rapidly growing network of electronic knowledge and tools that includes manufacturers, suppliers, sellers, buyers and users of information in electronic form. The information sphere of the state directs its economic and innovative potential, and thus significantly affects other spheres, such as competitiveness in the international arena and the quality of citizens’ life. Today Ukraine is at a unique stage of development when there is a chance to make the so-called “digital leap” in key spheres of the economy. That is, to quickly move to a new stage of development in these spheres, bypassing intermediate stages, and starting to use modern systems at once, bypassing several generations of technologies. The percent of the digital economy in Ukraine is gradually steadily increasing, but the pace of its development is still low. In order to keep up with the world's leading economies forever, it is necessary to begin large-scale digitization of all industries as soon as possible, investing as much as possible in the development of digital infrastructures, innovations and modern technologies. Moreover, the country has potential, especially in the IT sphere, where Ukraine's position is quite good. Technologically, Ukraine is still in the last century because the state has very low domestic demand for technology. The actual tasks for Ukraine in this sphere, on the one hand, are the implementation of its own digital potential, and on the other, the implementation of relevant EU documents and projects into national legislation. In addition, it is important to develop the spheres of science and education, without which it is impossible to count on progress in the development of the information society and the knowledge economy. Every year, technologies go forward, new trends are gaining momentum, affecting all the schemes by which people do business in the digital world. And in order to stay ahead of the competition or even just to “stay in the game”, you need to learn to work with new tools, track trends and be flexible enough to adapt to these changes",'Stepan Gzhytskyi National University of Veterinary Medicine and  Biotechnologies Lviv',Digitalization in Ukraine’s economy in the context of world digitization,https://core.ac.uk/download/401560600.pdf,,10.32718/nvlvet-e9602,core
395067275,2020-10-24T00:00:00,"Learning-based approaches often outperform hand-coded algorithmic solutions for many problems in robotics. However, learning long-horizon tasks on real robot hardware can be intractable, and transferring a learned policy from simulation to reality is still extremely challenging. We present a novel approach to model-free reinforcement learning that can leverage existing sub-optimal solutions as an algorithmic prior during training and deployment. During training, our gated fusion approach enables the prior to guide the initial stages of exploration, increasing sample-efficiency and enabling learning from sparse long-horizon reward signals. Importantly, the policy can learn to improve beyond the performance of the sub-optimal prior since the prior's influence is annealed gradually. During deployment, the policy's uncertainty provides a reliable strategy for transferring a simulation-trained policy to the real world by falling back to the prior controller in uncertain states. We show the efficacy of our Multiplicative Controller Fusion approach on the task of robot navigation and demonstrate safe transfer from simulation to the real world without any fine-tuning. The code for this project is made publicly available at https://sites.google.com/view/mcf-nav/home.</p",'Institute of Electrical and Electronics Engineers (IEEE)',Multiplicative controller fusion:Leveraging algorithmic priors for sample-efficient reinforcement learning and safe sim-to-real transfer,,,10.1109/IROS45743.2020.9341372,core
304693959,2020-01-01T00:00:00,"Surgical action recognition and temporal segmentation is a building block needed to provide some degrees of autonomy to surgical robots. In this paper, we present a deep learning model that relies on videos and kinematic data to output in real-time the current action in a surgical procedure. The proposed neural network architecture is composed of two sub-networks: a Spatial-Kinematic Network, which produces high-level features by processing images and kinematic data, and a Temporal Convolutional Network, which filters such features temporally over a sliding window to stabilize their changes over time. Since we are interested in applications to real-time supervisory control of robots, we focus on an efficient and causal implementation, i.e. the prediction at sample k only depends on previous observations. We tested our causal architecture on the publicly available JIGSAWS dataset, outperforming comparable state-of-the-art non-causal algorithms up to 8.6% in the edit score",,A Multi-Modal Learning System for On-Line Surgical Action Segmentation,,,,core
322699373,2020-01-01T00:00:00,"Collision-free path finding is crucial for multi-agent traversing environments like gaming systems. An efficient and accurate technique is proposed for avoiding collisions with potential obstacles in virtual and real time environments. Potential field is a coherent technique but it eventuates with various problems like static map usage and pre-calculated potential field map of the environment. It is unsuitable for dynamically changing or unknown environments. Agents can get stuck inside a local minima incompetent in escaping without a workaround implementation. This paper presents efficient and accurate solutions to find collision free path using potential field for dynamic gaming and real time robot navigation. A surfing game in two testing environments with a Gamecar and a physical robot called Robocar is created with dynamic and solid obstacles. Sensor like proximity, line and ultrasonic are used along with the camera as different agents for path finding. The proposed intelligent agent (IA) technique is compared with other path planing algorithms and games in terms of time complexity, cost metrics, decision making complexity, action repertoire, interagent communication, reactivity and temporally continuous. It traverses for 135 meters(m) in 55.8 seconds(s) covering 20 goals and 419.3 m in 8.7 minutes while avoiding 10 local minimas successfully. Proposed technique shows comparable results to path finding with techniques using neural networks and A* algorithm. Experimental results prove the efficiency with run time overload, time complexity and resource consumption of the proposed technique",'Institute of Electrical and Electronics Engineers (IEEE)',Collision-free path finding for dynamic gaming and real time robot navigation,,"[{'title': None, 'identifiers': ['issn:2375-0197', 'issn:1082-3409', '2375-0197', '1082-3409']}]",10.1109/ICTAI.2019.00023,core
350068153,2020-03-04T00:00:00,"The problem of mapping a real environment and recognizing the objects contained in this environment is a problem in the Computer Vision area and has received attention with the advances of SLAM solutions and robust object recognition solutions. The problem with Simultaneous Localization and Mapping (SLAM) robotics is to create a (generally geometric) map of the scene while estimating the viewer’s pose. The solutions to this problem are used in several areas where a map of the environment is desirable and extract geometric information from it. Object recognition allows us to identify the object in the scene according to the object classes of the reference database. For recognition in 2D images, the best solutions are based on convolutional neural networks. However, to obtain the 3D geometric information of the objects in the scene, other techniques are necessary that vary according to the model of the reference 3D object. In this work, we present a new approach to deal with the pose estimation of 3D objects from images of static scenes of indoor environments. We also propose a new integration between an object detector and a monocular SLAM solution based on keyframes. As results we demonstrate an improvement in the estimation of the camera’s trajectory in relation to the original method and a use of the system implemented in the creation of virtual environments.O problema de mapeamento de um ambiente real e reconhecimento dos objetos contidos neste ambiente é um problema da área de Visão Computacional e tem recebido atenção com o avanço de soluções SLAM e soluções robustas de reconhecimento de objetos. O problema da robótica de Localização e Mapeamento Simultâneos (Simultaneous Localization and Mapping - SLAM) consiste em criar um mapa (geralmente geométrico) da cena ao mesmo tempo em que estima a pose do observador. As soluções para este problema são utilizadas em diversas áreas onde se deseja mapear um ambiente e extrair informações geométricas deste. O reconhecimento de objetos permite identificar o objeto na cena conforme as classes de objetos da base de dados de referência. Para o reconhecimento em imagens 2D, as melhores soluções são baseadas em redes neurais convolucionais. Entretanto, para a obtenção das informações geométricas 3D dos objetos na cena, são necessárias outras técnicas que variam conforme o modelo do objeto 3D de referência. Neste trabalho, será apresentada uma nova abordagem para lidar com a estimativa de pose de objetos 3D a partir de imagens de cenas estáticas de ambientes internos. Para isso, uma integração entre um detector de objetos em imagens e uma solução SLAM monocular baseada em keyframes foi desenvoldida. Como resultados, demonstramos uma melhoria na estimativa da trajetória da câmera em relação ao método original e uma utilização do sistema implementado na criação de ambientes virtuais",'Revista Educacao em Foco (UFJF)',VEM-SLAM - Virtual environment modelling through SLAM,,,,core
345074841,2020-09-01T00:00:00,"This article proposes a novel unsupervised learning framework for detecting the number of tunnel junctions in subterranean environments based on acquired 2D point clouds. The implementation of the framework provides valuable information for high level mission planners to navigate an aerial platform in unknown areas or robot homing missions. The framework utilizes spectral clustering, which is capable of uncovering hidden structures from connected data points lying on non-linear manifolds. The spectral clustering algorithm computes a spectral embedding of the original 2D point cloud by utilizing the eigen decomposition of a matrix that is derived from the pairwise similarities of these points. We validate the developed framework using multiple data-sets, collected from multiple realistic simulations, as well as from real flights in underground environments, demonstrating the performance and merits of the proposed methodology",'Institute of Electrical and Electronics Engineers (IEEE)',Unsupervised Learning for Subterranean Junction Recognition Based on 2D Point Cloud,https://core.ac.uk/download/345074841.pdf,,,core
334975840,2020-01-01T00:00:00,"For self-driving vehicles, aerial drones, and autonomous robots to be successfully deployed in the real-world, they must be able to navigate complex environments and track objects. While Artificial Intelligence and Machine Vision have made significant progress in dynamic scene understanding, they are not yet as robust and computationally efficient as humans or other primates in these tasks. For example, the current state-of-the-art visual tracking methods become inaccurate when applied to random test videos. We suggest that ideas from cortical visual processing can inspire real world solutions for motion perception and tracking that are robust and efficient. In this context, the following contributions are made in this dissertation. First, a method for estimating 6DoF ego-motion and pixel-wise object motion is introduced, based on a learned overcomplete motion field basis set. The method uses motion field constraints for training and a novel differentiable sparsity regularizer to achieve state-of-the-art ego and object-motion performances on benchmark datasets. Second, a Convolutional Neural Network (CNN) that learns hidden neural representations analogous to the response characteristics of dorsal Medial Superior Temporal area (MSTd) neurons for optic flow and object motion is presented. The findings suggest that goal driven training of CNNs might automatically result in the MSTd-like response properties of model neurons. Third, a recurrent neural network model of predictive smooth pursuit eye movements is presented that generates similar pursuit initiation and predictive pursuit behaviors as observed in humans. The model provides the computational mechanisms of formation and rapid update of an internal model of target velocity, commonly attributed to zero lag tracking and smooth pursuit of occluded objects. Finally, a spike based stereo depth algorithm is presented that reconstructs dynamic visual scenes at 400 frames-per-second with one watt of power consumption when implemented using the IBM TrueNorth processor. Taken together, the presented models and implementations provide the computations for motion perception in the dorsal visual pathway in the brain and inform ideas for efficient computational vision systems","eScholarship, University of California",Brain Inspired Neural Network Models of Visual Motion Perception and Tracking in Dynamic Scenes,,,,core
362674141,2020-11-27T00:00:00,"Industry 4.0 technologies integrate devices and data, bringing flexibility and efficiency,
derived from decentralization of information sources and processing, which is fundamental
to further advance applications. This work aims to introduce Cyber Physical Systems
(CPS) working with decision affecting passive objects, a system with distributed data,
and automatic planners, to achieve a self-sufficient process manipulator that does not
require external goal insertion and can self-adjust given an exception, in real-time. This
solution is more flexible and autonomous than state machines. Applying the technologies
introduced in Industry 4.0 and methods that were previously treated separately, such as
symbolic artificial intelligence and robot kinematics, the system can perform perception,
planning and actuation processes. This system is capable of extracting information inside
passive passive entities in the physical domain by using Radio Frequency IDentification
(RFID) to acquire predicates, data, about each object current and objective states using
the Predicate inside RFID Database (PRD) tool. This data is treated to produce a domain
snapshot, by joining distributed information and generating a problem definition, through
the Grouped Individual State Predicates (GISP) methodology. This problem definition
may then be fed into a planning module, implemented on an Edge or Cloud server, where
discrete-action and trajectory planning are concatenated to output control references,
using a generic symbolic planner and a numeric trajectory generator. Then, the active
agent may actuate, verify for exceptions and update the passive objects information if the
obtained state is perceived with no exceptions, else it must reiterate to satisfy the global
goal. This work structures the adaptive discrete event control architecture with a RFID
database containing parts of predicate logic.Pesquisa sem auxílio de agências de fomentoTrabalho de Conclusão de Curso (Graduação)As tecnologias da indústria 4.0 integram dispositivos e dados, trazendo flexibilidade e
eficiência, derivada da decentralização das fontes de informação e processamento, que é
fundamental para o avanço das aplicações. Esse trabalho busca introduzir sistemas ciber
físicos trabalhando com objetos passivos com capacidade de afetar decisões, um sistema
com informação distribuida, e planejadores automáticos, para alcançar um manipulador
de processos autossuficiente que não requer inserção externa de objetivos e que pode
auto-ajustar-se de acordo com exceções, em tempo de execução. Essa é uma solução
mais flexível e autônoma que o uso de máquinas de estado. Aplicando as tecnologias
introduzidas na Indústria 4.0 e métodos que eram tratados em separado, como inteligência
artificial simbólica e cinemática de robôs, o sistema pode realizar os processos de percepção,
planejamento e atuação. Esse sistema é capaz de extrair informação de entidades passivas
no domínio físico utilizando Identificação por Rádio Frequência (RFID) para adquirir
predicados, dados, sobre os estados corrente e objetivo de cada objeto através da ferramenta
PRD (Predicados dentro de base de Dados RFID). Esses dados são tratados para produzir
um retrato do domínio, através da união da informação distribuída e produção de uma
definição de problema usando a metodologia GISP (Predicados de Estado Individual
Agrupados). Essa definição de problema pode ser alimentada no módulo de planejamento,
implementado num servidor local ou na nuvem, onde planemento de ações discretas e
trajetória são concatenados para retornar referências de controle, usando um planejador
simbólico genérico e um gerador numérico de trajetória. Então, o agente ativo pode atuar,
verificar exceções e atualizar a informação nos objetos passivos caso o estado obtido seja
percebido livre de exceções, senão deve reiterar até que se satisfaça o objetivo global. Esse
trabalho estrutura a arquitetura de controle adaptativa a eventos discretos com base de dados RFID contendo partes de lógica de predicados",Engenharia Mecatrônica,Introdução ao controle adaptativo a eventos discretos em base de dados RFID com planejador automático aplicado a sistemas robóticos,https://core.ac.uk/download/362674141.pdf,,,core
352884845,2020-01-01T08:00:00,"“In a worker-centered intelligent manufacturing system, sensing and understanding of the worker’s behavior are the primary tasks, which are essential for automatic performance evaluation & optimization, intelligent training & assistance, and human-robot collaboration. In this study, a worker-centered training & assistant system is proposed for intelligent manufacturing, which is featured with self-awareness and active-guidance. To understand the hand behavior, a method is proposed for complex hand gesture recognition using Convolutional Neural Networks (CNN) with multiview augmentation and inference fusion, from depth images captured by Microsoft Kinect. To sense and understand the worker in a more comprehensive way, a multi-modal approach is proposed for worker activity recognition using Inertial Measurement Unit (IMU) signals obtained from a Myo armband and videos from a visual camera. To automatically learn the importance of different sensors, a novel attention-based approach is proposed to human activity recognition using multiple IMU sensors worn at different body locations. To deploy the developed algorithms to the factory floor, a real-time assembly operation recognition system is proposed with fog computing and transfer learning. The proposed worker-centered training & assistant system has been validated and demonstrated the feasibility and great potential for applying to the manufacturing industry for frontline workers. Our developed approaches have been evaluated: 1) the multi-view approach outperforms the state-of-the-arts on two public benchmark datasets, 2) the multi-modal approach achieves an accuracy of 97% on a worker activity dataset including 6 activities and achieves the best performance on a public dataset, 3) the attention-based method outperforms the state-of-the-art methods on five publicly available datasets, and 4) the developed transfer learning model achieves a real-time recognition accuracy of 95% on a dataset including 10 worker operations”--Abstract, page iv",Scholars\u27 Mine,Human behavior understanding for worker-centered intelligent manufacturing,https://core.ac.uk/download/352884845.pdf,,,core
386386346,2020-01-01T08:00:00,"Reinforcement learning has been applied to solve several real world challenging problems, from robotics to data center cooling. Similarly, adaption of reinforcement learning for multi-agent systems facilitated applications such as optimal multi-robot control and analysis of social-dilemmas. In this dissertation, we show that multi-agent reinforcement learning algorithms suffer from several stability issues such as multi-scenario learning, unstable training in dual-reward setting, overestimation bias and value function collapse, and provide solutions to each of these problems respectively. Several contributions of this dissertation have been formalized within the framework of a defensive escort team problems, a scenario where a team of learning robots is deployed as a defensive escort team to protect a high value payload. The goal here is to automatically learn an optimal formation around the payload that will minimize potential physical threats from nearby bystanders. We first formalize the defensive escort team problem as a security game problem based on game theoretic principles. Motivated from our defensive escort team problem, we present a distributed multi-agent reinforcement learning based solution to solve large scale security games and show that an optimal formation can automatically be learnt only with self play. In addition,  The multi-scenario instability problem arises when reinforcement learning agents are deployed on environments other than they are trained on. To tackle this issue, we propose  Multi-Agent Universal Policy Gradients: A novel multi-agent reinforcement learning algorithm inspired from universal value function approximators that generalizes over set of multiple scenarios. Our results show that our proposed solution works better than scenario-dependent policies. The instability in training arises when reinforcement learning agents try to maximize entangled multi-objective reward function. This is a challenging task for current state-of-the-art multi-agent reinforcement algorithms that are designed to either maximize the global reward of the team or the individual local rewards. The problem is exacerbated when either of the rewards is sparse leading to unstable learning. To address this problem, we present Decomposed Multi-Agent Deep Deterministic Policy Gradient (DE-MADDPG): a novel cooperative multi-agent reinforcement learning framework that simultaneously learns to maximize the global and local rewards. Overestimation bias is one of the major issues in reinforcement learning that contributes towards learning sub-optimal policies. Several techniques have been proposed that utilize an ensemble of neural networks to address the overestimation bias in reinforcement learning. However, the neural networks in the ensemble collapse to the same representation space, therefore, invalidating the use of ensemble neural networks to address the overestimation bias. To mitigate this issue, we propose five regularization techniques to maximize the representation diversity in the ensemble of neural networks. Although several contributions of this dissertation have been formalized within the framework of defensive escort team problem, the techniques developed in this dissertation are directly applicable to scenarios where groups of robots and humans need to navigate in a shared space such as robotic guide dogs for visually impaired humans. The contribution in the last chapter are more generic and can be applied to any reinforcement learning algorithm that uses and ensemble of neural networks",'Information Bulletin on Variable Stars (IBVS)',Multi-agent Reinforcement Learning for Defensive Escort Teams,,,,core
327204410,2020-05-01T07:00:00,"It is important to be able to establish formal performance bounds for autonomous systems. However, formal verification techniques require a model of the environment in which the system operates; a challenge for autonomous systems, especially those expected to operate over longer timescales. This paper describes work in progress to automate the monitor and repair of ROS-based autonomous robot software written for an a-priori partially known and possibly incorrect environment model. A taint analysis method is used to automatically extract the data-flow sequence from input topic to publish topic, and instrument that code. A unique reinforcement learning approximation of MDP utility is calculated, an empirical and non-invasive characterization of the inherent objectives of the software designers. By comparing design (a-priori) utility with deploy (deployed system) utility, we show, using a small but real ROS example, that it’s possible to monitor a performance criterion and relate violations of the criterion to parts of the software. The software is then patched using automated software repair techniques and evaluated against the original off-line utility",Fordham Research Commons,Using Taint Analysis and Reinforcement Learning (TARL) to Repair Autonomous Robot Software,,,,core
334986205,2020-09-01T00:00:00,"This article proposes a novel unsupervised learning framework for detecting the number of tunnel junctions in subterranean environments based on acquired 2D point clouds. The implementation of the framework provides valuable information for high level mission planners to navigate an aerial platform in unknown areas or robot homing missions. The framework utilizes spectral clustering, which is capable of uncovering hidden structures from connected data points lying on non-linear manifolds. The spectral clustering algorithm computes a spectral embedding of the original 2D point cloud by utilizing the eigen decomposition of a matrix that is derived from the pairwise similarities of these points. We validate the developed framework using multiple data-sets, collected from multiple realistic simulations, as well as from real flights in underground environments, demonstrating the performance and merits of the proposed methodology",'Institute of Electrical and Electronics Engineers (IEEE)',Unsupervised Learning for Subterranean Junction Recognition Based on 2D Point Cloud,https://core.ac.uk/download/334986205.pdf,,,core
340076325,2020-01-01T00:00:00,"Schilling M, Rohlfing KJ, Vogt P, Yu C, Spranger M. Guest Editorial Special Issue on Multidisciplinary Perspectives on Mechanisms of Language Learning. IEEE transactions on cognitive and developmental systems. 2020;12(2):134-138.Humans excel at learning from other humans [item 1) in the Appendix). Language facilitates such learning and plays a crucial role. On the one hand, it coordinates our interactions and cooperative behavior [item 2) in the Appendix). On the other hand, language and communication allow to directly incorporate novel knowledge gathered from social interaction or from reading [item 3) in the Appendix). It has been a long-standing goal of artificial intelligence to leverage such communicative abilities [item 4) in the Appendix) for robots and smart software agents which would, at first, simplify our interactions with machines through a more human-like way of coordinating between humans and robots. But furthermore, this would allow us to easily teach these machines, increasing their abilities and skills further which would allow them to become real partners and companions",'Institute of Electrical and Electronics Engineers (IEEE)',Guest Editorial Special Issue on Multidisciplinary Perspectives on Mechanisms of Language Learning,,,10.1109/TCDS.2020.2991470,core
390245731,2020-11-18T00:00:00,"International audienceNeuro-inspired computing employs technologies that enable brain-inspired computing hardware for more efficient and adaptive intelligent systems. Mimicking the human brain and nervous system, these computing architectures are excellent candidates for solving complexand large-scale associative learning problems. In the framework of EU H2020 NeurONN project, we implement and explore energy-efficient neuromorphic computing based on oscillatory neural networks (ONN) [1] using metal-insulator-transition (MIT) for emulating “neurons” and 2D material memristors for emulating “synapses” to achieve a truly neuro-inspired computing paradigm for enabling AI at the edge [2-4].To show the proof-of-concept, we have implemented a digital version of the ONN on a programmable logic component (FPGA). We use the FPGA logic resources to implement an ONN of sixty neurons. The ONN is a Hopfield-type neural network, and it works like anassociative memory. During the learning stage, the synapse weight values are calculated from several reference images.To display ONN functionality in real applications, we will present two demonstrators:The first demonstrator implements a pattern recognition application. The ONN learns several digits and then it tries to recognize fuzzy digits in an image. The test bench includes a camera for image acquisition, the images are then scaled down to the right size and sent to the neuralnetwork which associates the fuzzy digit with a digit it has learned.For the second demonstrator, we have designed an ONN that allows a mobile robot to make decisions to avoid obstacles (see figure below). The robot is equipped with three proximity sensors. The values of these sensors are coded in the form of a 60 pixels image.ONN learns from 8 reference images, each one corresponding to the encoded data from proximity sensors. There are 3 proximity sensors embedded on the robot. The data coming from the sensors are encoded into 6 values. Thus, there is a total of 216 decision possibilitiesbased on the sensor data. ONN provided a 100% success rate to make a decision (i.e. avoid obstacles) based on the received images by ONN and the memorized images on ONN",HAL CCSD,Using Oscillatory Neural Network for Pattern Recognition and Mobile Robot Control,,,,core
360675799,2020-01-01T00:00:00,"Background: Near-infrared fluorescence (NIRF) imaging with indocyanine green (ICG) has been recently adopted in pediatric minimally invasive surgery (MIS) in order to improve intra-operative visualization of anatomic structures and facilitate surgery. Objective: This study aimed to report our preliminary experience using ICG technology in pediatric urology using laparoscopy and robotics. Study design: ICG technology was adopted in 57 laparoscopic or robotic urological procedures performed in our unit over a 24-month period: 41 (38 laparoscopic - 3 robotic) left varicocele repairs with intra-operative lymphography and 16 renal procedures (12 laparoscopic - 4 robotic) including 9 partial nephrectomies, 3 nephrectomies and 4 renal cyst deroofings. Results: The ICG solution was injected intravenously in renal procedures or into the testis body in case of varicocele repair. Regarding the timing of the administration, the ICG injection was performed intra-operatively in all cases and allowed the visualization of the anatomic structures in a matter of 30\u201360 s. The dosage of ICG was 0.3 mg/mL/kg in all indications. All procedures were completed laparoscopically or robotically without conversions. No adverse and allergic reactions to ICG and other complications occurred postoperatively. Discussion: This paper describes for the first time in pediatric urology that ICG-guided NIRF imaging may be helpful in laparoscopic and robotic procedures. In case of varicocele repair, ICG-enhanced fluorescence allowed to perform a lymphatic-sparing procedure and avoid the risk of postoperative hydrocele. In case of partial nephrectomy, ICG-guided NIRF was helpful to visualize the vascularization of the non-functioning moiety, identify the dissection plane between the two moieties (Fig. 1) and check the perfusion of the residual parenchyma after resection of the non-functioning pole. In case of renal cyst deroofing, ICG-guided NIRF aided to identify the avascular cyst dome and to guide its resection. No real benefits of using ICG-enhanced fluorescence were observed during nephrectomy. Conclusion: Our preliminary experience confirmed the safety and efficacy of ICG technology in pediatric urology and highlighted its potential advantages as adjunctive surgical technology in patients undergoing laparoscopic or robotic urological procedures. Use of NIRF was also cost-effective as no added costs were required except for the ICG dye (cost 40 eur per bottle). The most common and useful applications in pediatric urology included varicocele repair, partial nephrectomy ad renal cyst deroofing. The main limitation is the specific equipment needed in laparoscopy, that is not available in all centers whereas the robot is equipped with the Firefly\uae software for NIRF",'Elsevier BV',Near-Infrared fluorescence imaging using indocyanine green (ICG): Emerging applications in pediatric urology,,,10.1016/j.jpurol.2020.07.008,core
375437458,2020-04-13T00:00:00,"The emergence of Artificial Intelligence (AI) is creating new dimensions and redefining the concept and meaning of work in industrial settings. Documented success has been reported where AI is transforming industrial scenes such as scaling large operation processes, speed of execution, flexibility of processes where rigid manufacturing by dumb robots is replaced with smart individualized production following real-time customer choices, decision-making in which a huge amount of data can be quickly available at the fingertips of workers on the factory floor or even prevent problems before they happen, and personalization where AI uses data to deliver personalized user experience. According to the market research firm Trac tica, the global AI software market is expected to experience massive growth in the coming years, with revenues increasing from around US 9.5 billion in 2018 to an expected US 118.6 billion by 2025",'Institute of Electrical and Electronics Engineers (IEEE)',Future trends in I&M: Human-machine co-creation in the rise of AI,https://core.ac.uk/download/375437458.pdf,,10.1109/MIM.2020.9062691,core
365068422,2020-12-22T08:00:00,"As governments and private companies alike race to achieve the vision of a smart city — where artificial intelligence (AI) technology is used to enable self-driving cars, cashier-less shopping experiences and connected home devices from thermostats to robot vacuum cleaners — advancements are being made in both software and hardware to enable increasingly real-time, accurate inference at the edge. One hardware solution adopted for this purpose is the LiDAR sensor, which utilizes infrared lasers to accurately detect and map its surroundings in 3D. On the software side, developers have turned to artificial neural networks to make predictions and recommendations with high accuracy. These neural networks have the potential, particularly run on purpose-built hardware such as GPUs and TPUs, to make inferences in near real-time, allowing the AI models to serve as a usable interface for real-world interactions with other AI-powered devices, or with human users. This paper aims to example the joint use of LiDAR sensors and AI to understand its importance in smart city environments",SJSU ScholarWorks,LiDAR Object Detection Utilizing Existing CNNs for Smart Cities,,,,core
390088733,2020-01-01T00:00:00,"Limberg C, Wersing H, Ritter H. Beyond Cross-Validation—Accuracy Estimation for Incremental and Active Learning Models. Machine Learning and Knowledge Extraction. 2020;2(3):327-346.For incremental machine-learning applications it is often important to robustly estimate the system accuracy during training, especially if humans perform the supervised teaching. Cross-validation and interleaved test/train error are here the standard supervised approaches. We propose a novel semi-supervised accuracy estimation approach that clearly outperforms these two methods. We introduce the Configram Estimation (CGEM) approach to predict the accuracy of any classifier that delivers confidences. By calculating classification confidences for unseen samples, it is possible to train an offline regression model, capable of predicting the classifier’s accuracy on novel data in a semi-supervised fashion. We evaluate our method with several diverse classifiers and on analytical and real-world benchmark data sets for both incremental and active learning. The results show that our novel method improves accuracy estimation over standard methods and requires less supervised training data after deployment of the model. We demonstrate the application of our approach to a challenging robot object recognition task, where the human teacher can use our method to judge sufficient training",'MDPI AG',Beyond Cross-Validation—Accuracy Estimation for Incremental and Active Learning Models,,,10.3390/make2030018,core
343462063,2020-11-04T08:00:00,"Deep neural networks (DNNs) have achieved significant success in many applications, such as computer vision, natural language processing, robots, and self-driving cars. With the growing demand for more complex real-world applications, more complicated neural networks have been proposed. However, high capacity models result in two major problems: long training times and high inference delays, making the neural networks hard to train and infeasible to deploy for time-intensive applications or resource-limited devices. In this work, we propose multiple techniques to accelerate the training and inference speed as well as model performance
The first technique we study is model parallelization on generative adversarial networks (GANs). Multiple orthogonal generators with shared memory are employed to capture the whole data distribution space. This method can not only improve the model performance but also alleviate the mode collapse problem that is common in GANs. The second technique we investigate is the automatic network pruning. To reduce the floating-point operations (FLOPs) to a proper level without compromising accuracy, we propose a better generalized and easy-to-use pruning method, which prunes the network through optimizing a set of trainable auxiliary parameters instead of original weights. Weakly coupled gradient update rules are proposed to keep consistency with pruning tasks. The third technique is to remove the redundancy of the complicated model based on the need of applications. We treat the chemical reaction prediction as a translation problem and apply a low capacity neuron translation model to this problem. The fourth technique is to combine distillation with Differentiable Architecture Search to stabilize and improve the searching procedure. Intermediate results as well as the output logits are transferred from the teacher network to the student network. For the application of the speedup technique, we introduce neural network pruning into Materials Genomics. We propose attention based AutoPrune for the kernel pruning of a continuous filtering neural network for molecular property prediction and achieves better performance and more compact size",OpenCommons@UConn,Speedup Techniques for Deep Neural Networks,,,,core
360847191,2020-01-01T00:00:00,"Data-driven modelling and synthesis of motion is an active research area with applications that include animation, games, and social robotics. This paper introduces a new class of probabilistic, generative, and controllable motion-data models based on normalising flows. Models of this kind can describe highly complex distributions, yet can be trained efficiently using exact maximum likelihood, unlike GANs or VAEs. Our proposed model is autoregressive and uses LSTMs to enable arbitrarily long time-dependencies. Importantly, is is also causal, meaning that each pose in the output sequence is generated without access to poses or control inputs from future time steps; this absence of algorithmic latency is important for interactive applications with real-time motion control. The approach can in principle be applied to any type of motion since it does not make restrictive, task-specific assumptions regarding the motion or the character morphology. We evaluate the models on motion-capture datasets of human and quadruped locomotion. Objective and subjective results show that randomly-sampled motion from the proposed method outperforms task-agnostic baselines and attains a motion quality close to recorded motion capture.QC 20200929VR proj. 2018-05409 (StyleBot)SSF no. RIT15-0107 (EACare)Wallenberg AI, Autonomous Systems and Software Program (WASP",'Association for Computing Machinery (ACM)',MoGlow: Probabilistic and controllable motion synthesis using normalising flows,,,10.1145/3414685.3417836,core
322748047,2020-05-08T00:00:00,"This report reviews and classifies the current and near-future applications of Artificial Intelligence (AI) in Medicine and Healthcare according to their ethical and societal impact and the availability level of the various technological implementations. It provides conceptual foundations for well-informed policy-oriented work, research, and forward-looking activities that address the opportunities and challenges created in the field of AI in Medicine and Healthcare. This report is aimed for policy developers, but it also makes contributions that are of interest for researchers studying the impact and the future of AI on Healthcare, for scientific and technological stakeholders in this field and for the general public.
This report is based on an analysis of the state of the art of research and technology, including software, personal monitoring devices, genetic tests and editing tools, personalized digital models, online platforms, augmented reality devices, and surgical and companion robotics. From this analysis, it is presented the concept of “extended personalized medicine”, and it is explored the public perception of medical AI systems, and how they show, simultaneously, extraordinary opportunities and drawbacks. In addition, this report addresses the transformation of the roles of doctors and patients in an age of ubiquitous information and identifies three main paradigms in AI-supported Medicine: “fake-based”, “patient-generated”, and “scientifically tailored” views.
This Report presents:
- An updated overview of the many aspects related to the social impact of Artificial Intelligence and its applications in Medicine and Health. A new ‘Technology Availability Scale’ is defined to evaluate and compare their current status.
- Recent examples of the growing social concerns and debates in the general press, social media and other web-bases sources.
- A ‘Visual Overview of AI and AI-mediated technologies in Medicine and Healthcare’, in which two figures show, respectively, a (newly proposed) classification according to their ethical and social impact, and the most relevant ethical and social aspects considered for such classification. Some key questions, controversies, significant, and conflicting issues are outlined for each aspect.
- A ‘Structured Overview’, with a sorted list of technologies and their implementations, including perspectives, conflicting views and potential pitfalls, and a corresponding, extensive list of references.
- A conclusive set of policy challenges, namely the need of informed citizens, key aspects (of AI and AI-mediated technologies in Medicine and Healthcare) to evaluate, and some recommendations towards a European leadership in this sector.
- We finally relate our study with an update on the use of AI technologies to fight the SARS-CoV-2 virus and COVID-19 pandemic disease.JRC.A.5-Scientific Developmen",'Publications Office of the European Union',"Artificial Intelligence in Medicine and Healthcare: applications, availability and societal impact",https://core.ac.uk/download/322748047.pdf,,10.2760/047666,core
334900621,2020-01-06T00:00:00,"With the advancements in high volume, low precision computational technology
and applied research on cognitive artificially intelligent heuristic systems,
machine learning solutions through neural networks with real-time learning has
seen an immense interest in the research community as well the industry. This
paper involves research, development and experimental analysis of a neural
network implemented on a robot with an arm through which evolves to learn to
walk in a straight line or as required. The neural network learns using the
algorithms of Gradient Descent and Backpropagation. Both the implementation and
training of the neural network is done locally on the robot on a raspberry pi 3
so that its learning process is completely independent. The neural network is
first tested on a custom simulator developed on MATLAB and then implemented on
the raspberry computer. Data at each generation of the evolving network is
stored, and analysis both mathematical and graphical is done on the data.
Impact of factors like the learning rate and error tolerance on the learning
process and final output is analyzed.Comment: 8 pages, 14 figure",'eSAT Publishing House',Self learning robot using real-time neural networks,http://arxiv.org/abs/2001.02103,,10.15623/ijret.2018.0710009,core
344346767,2020-01-01T00:00:00,"Robots are becoming interactive and robust enough to be adopted outside laboratories and in industrial scenarios as well as interacting with humans in social activities. However, the design of engaging robot-based applications requires the availability of usable, flexible and accessible development frameworks, which can be adopted and mastered by researchers and practitioners in social sciences and adult end users as a whole. This paper surveys Visual Programming Environments aimed at enabling a paradigm fostering the so-called End-User Development of applications involving robots with social capabilities. The focus of this article is on those Visual Programming Environments that are designed to support social research goals as well as to cater for professional needs of people not trained in more traditional text-based computer programming languages. This survey excludes interfaces aimed at supporting expert programmers, at allowing industrial robots to perform typical industrial tasks (such as pick and place operations), and at teaching children how to code. After having performed a systematic search, sixteen programming environments have been included in this survey. Our goal is two-fold: first, to present these software tools with their technical features and Authoring Artificial Intelligence modeling approaches, and second, to present open challenges in the development of Visual Programming Environments for end users and social researchers, which can be informative and valuable to the community. The results show that the most recent such tools are adopting distributed and Component-Based Software Engineering approaches and web technologies. However, few of them have been designed to enable the independence of end users from high-tech scribes. Moreover, findings indicate the need for (i) more objective and comparative evaluations, as well as usability and user experience studies with real end users; and (ii) validations of these tools for designing applications aimed at working ""in-the-wild"" rather than only in laboratories and structured settings",'Elsevier BV',Visual Pogramming Environments for End-User Development of intelligent and social robots : a systematic review,https://core.ac.uk/download/344346767.pdf,"[{'title': 'Journal of Computer Languages', 'identifiers': ['2590-1184', 'issn:2590-1184']}]",10.1016/j.cola.2020.100970,core
357322010,2020-04-03T00:00:00,"ABSTRACT To speed up the convergence of reinforcement learning (RL) algorithms by more efficient use of computer simulations, three algorithmic techniques are proposed: Time Manipulation, Time Hopping, and Eligibility Propagation. They are evaluated on various robot control tasks. The proposed Time Manipulation [1] is a concept of manipulating the time inside a simulation and using it as a tool to speed up the learning process. It is applicable to a subset of RL problems whose goal is to learn a control policy to avoid failure events. Time Manipulation works by turning back the time of the simulation on failure events, thus avoiding redundant state transitions and exploring deeper the state space. This is impossible to be done in the real world, but it can easily be done in a simulation. In order to evaluate the proposed algorithm, experiments on a classical control benchmark problem are conducted: an inverted pendulum balancing robot task. The aim of the RL algorithm is to find a control policy which can prevent the pendulum from falling by moving the robot left or right, without hitting the edges of the given track. The experimental results show that Time Manipulation speeds up the learning process by 260%. It also improves the state space exploration by 12%, because it allows the RL algorithm to explore better the state space in proximity of failure states. The proposed Time Hopping [2] is a generalization of Time Manipulation, able to make arbitrary &quot;hops&quot; between states and this way traverse rapidly throughout the entire state space. Time Hopping extends the applicability of time manipulations to include not only failure-avoidance problems, but also continuous optimization problems, by creating new mechanisms to trigger the time manipulation events, to make prediction about the possible future rewards, and to select promising time hopping targets. The proposed implementation of the Time Hopping technique consists of 3 components: Hopping trigger (decides when the hopping starts), Target selection (decides where does it hop to), and Hopping (performs the actual hopping). For the implementation of the Hopping trigger component, a Gamma pruning technique is proposed, which detects and prunes unpromising exploratory paths. For the Target selection component, a Best Lasso Target Selection technique is proposed, which selects a target among the proximity of the current best policy. The evaluation of Time Hopping is performed on a biped crawling robot task. The crawling robot has 2 limbs, each with 2 segments, for a total of 4 degrees of freedom (DOF), 80 possible actions at each time step, and 13689 possible robot states. The goal of the learning process is to find a crawling motion with the maximum speed. The reward function for this task is defined as the horizontal displacement of the robot after every action. The experimental results show that Time Hopping accelerates the learning process more than 7 times. A very strong point of Time Hopping is that it is completely transparent for the RL algorithm, which offers various opportunities for combining Time Hopping with other approaches for speeding up the learning process. In addition, it can also be used as a tool for re-shaping the state probability distribution as desired  The proposed Eligibility Propagation [4] is a mechanism to further speed up Time Hopping. It provides similar abilities to what eligibility traces provide for conventional RL. Eligibility traces are one of the basic mechanisms for temporal credit assignment in reinforcement learning. An eligibility trace is a temporary record of the occurrence of an event, such as the visiting of a state or the taking of an action. Eligibility Propagation uses the transitions graph to obtain all predecessor states of an updated state. Regardless of the actual order in which Time Hopping visits the states, this oriented graph contains a record of the correct chronological sequence of state transitions. Once this oriented graph is available, it is used to propagate state value updates in the opposite direction of the state transition edges, thus making the propagation flow logically backwards in time. The evaluation of Eligibility Propagation is performed on the same biped crawling robot task as for Time Hopping. The results show that Time Hopping with Eligibility Propagation achieves 99% of the maximum possible speed almost 3 times faster than Time Hopping alone, and more than 4 times faster than conventional Q-learning. This significant speedup of the learning process is achieved despite the additional computational overhead of maintaining the transitions graph. The reason for this is the improved Gamma-pruning based on more precise future reward predictions. All experiments are conducted on a custom developed Javabased software application system, and a custom developed 2D robot physics simulation engine. The significant speed-ups achieved by the proposed algorithms make them very suitable for a wide range of robot control problems, where reducing the computational cost is importan",,Time Hopping Technique for Reinforcement Learning and its Application to Robot Control,,,,core
387275025,2020-11-07T00:00:00,"The optimal tracking problem is addressed in the robotics literature by using
a variety of robust and adaptive control approaches. However, these schemes are
associated with implementation limitations such as applicability in uncertain
dynamical environments with complete or partial model-based control structures,
complexity and integrity in discrete-time environments, and scalability in
complex coupled dynamical systems. An online adaptive learning mechanism is
developed to tackle the above limitations and provide a generalized solution
platform for a class of tracking control problems. This scheme minimizes the
tracking errors and optimizes the overall dynamical behavior using simultaneous
linear feedback control strategies. Reinforcement learning approaches based on
value iteration processes are adopted to solve the underlying Bellman
optimality equations. The resulting control strategies are updated in real time
in an interactive manner without requiring any information about the dynamics
of the underlying systems. Means of adaptive critics are employed to
approximate the optimal solving value functions and the associated control
strategies in real time. The proposed adaptive tracking mechanism is
illustrated in simulation to control a flexible wing aircraft under uncertain
aerodynamic learning environment.Comment: 29 pages, 15 figures. The manuscript has been published in MDPI
  Robotic",'MDPI AG',"Online Multi-Objective Model-Independent Adaptive Tracking Mechanism for
  Dynamical Systems",http://arxiv.org/abs/2011.03881,,10.3390/robotics8040082,core
323983835,2020-07-04T00:00:00,"Deep Reinforcement Learning (RL) is a promising approach for adaptive robot
control, but its current application to robotics is currently hindered by high
sample requirements. To alleviate this issue, we propose to exploit the
symmetries present in robotic tasks. Intuitively, symmetries from observed
trajectories define transformations that leave the space of feasible RL
trajectories invariant and can be used to generate new feasible trajectories,
which could be used for training. Based on this data augmentation idea, we
formulate a general framework, called Invariant Transform Experience Replay
that we present with two techniques: (i) Kaleidoscope Experience Replay
exploits reflectional symmetries and (ii) Goal-augmented Experience Replay
which takes advantage of lax goal definitions. In the Fetch tasks from OpenAI
Gym, our experimental results show significant increases in learning rates and
success rates. Particularly, we attain a 13, 3, and 5 times speedup in the
pushing, sliding, and pick-and-place tasks respectively in the multi-goal
setting. Performance gains are also observed in similar tasks with obstacles
and we successfully deployed a trained policy on a real Baxter robot. Our work
demonstrates that invariant transformations on RL trajectories are a promising
methodology to speed up learning in deep RL.Comment: 8 pages, 11 figures, additional 3 pages for appendix. IEEE Robotics
  and Automation Letters (RAL), 2020. Also in: Intelligent Robots and Systems
  (IROS",'Institute of Electrical and Electronics Engineers (IEEE)',"Invariant Transform Experience Replay: Data Augmentation for Deep
  Reinforcement Learning",http://arxiv.org/abs/1909.10707,,10.1109/LRA.2020.3013937,core
322450177,2020-05-06T00:00:00,"With the increase in use of Unmanned Aerial Vehicles (UAVs)/drones, it is
important to detect and identify causes of failure in real time for proper
recovery from a potential crash-like scenario or post incident forensics
analysis. The cause of crash could be either a fault in the sensor/actuator
system, a physical damage/attack, or a cyber attack on the drone's software. In
this paper, we propose novel architectures based on deep Convolutional and Long
Short-Term Memory Neural Networks (CNNs and LSTMs) to detect (via Autoencoder)
and classify drone mis-operations based on sensor data. The proposed
architectures are able to learn high-level features automatically from the raw
sensor data and learn the spatial and temporal dynamics in the sensor data. We
validate the proposed deep-learning architectures via simulations and
experiments on a real drone. Empirical results show that our solution is able
to detect with over 90% accuracy and classify various types of drone
mis-operations (with about 99% accuracy (simulation data) and upto 88% accuracy
(experimental data)).Comment: IEEE International Conference on Robotics and Automation (ICRA), May
  2020, 6+1 page",,"On-board Deep-learning-based Unmanned Aerial Vehicle Fault Cause
  Detection and Identification",http://arxiv.org/abs/2005.00336,,,core
363402556,2020-01-01T00:00:00,"Зайцева С. С. Соціальнокомунікаційні аспекти висвітлення наукових
новин та інновацій в інтернет-медіа України. – Кваліфікаційна наукова праця на
правах рукопису.
Дисертація на здобуття наукового ступеня кандидата наук із соціальних
комунікацій за спеціальністю 27.00.01 – теорія та історія соціальних комунікацій. –
Класичний приватний університет, Запоріжжя, 2020.
Робота присвячена з’ясуванню соціально-комунікаційних особливостей
висвітлення наукових новин та інновацій в інтернет-медіа України. На основі теорії
порядку денного й теорії дифузії інновацій за допомогою контент-аналізу та
структурованого якісного аналізу визначено, які теми висвітлюють інтернет-видання
України, як висвітлюють та як при цьому використовують сучасні можливості
зворотного зв’язку. Дослідження проведено на матеріалі сімох найбільш
відвідуваних новинних інтернет-медіа України: «Корреспондент.net» «Obozrevatel»,
«Сьогодні», «Телеканал 24», «ТСН.ua», «Українська правда», «Цензор.НЕТ»,
з 1 січня 2014 р. – до 31 грудня 2018 р. Встановлено, що новинні медіа приділяють
мало уваги темі наукових новин та інновацій (0,1% матеріалів). У фокусі висвітлення
– медицина, історія та психологія. Інтерес медіа зосереджений на науково-
практичних здобутках; досягненнях США та країн Західної Європи. Близько 91%
матеріалів запозичені, переважно із західних та російських джерел, часто з
помилками, скороченнями. Значна частина тем порушена з метою розважити, а не
сприяти всебічному інформуванню про науку. Третина публікацій мають заголовки,
які цілком або частково не відповідають змісту основного тексту. Висвітлення
чинників дифузії інновацій (за Е. Роджерсом) забезпечено на невисокому рівні.
Найбільше реалізованим є чинник спостережності, найменше – випробовність. Лише
в двох інтернет-виданнях із семи використовують можливість суспільного
обговорення новин науки та інновацій у коментарях.Зайцева С. С. Социальнокоммуникационные аспекты освещения
научных новостей и инноваций в интернет-медиа Украины. – Квалификационная
научная работа на правах рукописи.
Диссертация на соискание научной степени кандидата наук из социальных
коммуникаций по специальности 27.00.01 – теория и история социальных
коммуникаций. – Классический приватный университет, Запорожье, 2020.
Работа посвящена определению социально-коммуникационных особенностей
освещения научных новостей и инноваций в интернет-медиа Украины. На основании
теории повестки дня и теории диффузии инноваций с помощью контент-анализа и
структурированного качественного анализа определено, какие темы освещают
интернет-издания Украины, как освещают и как при этом используют современные
возможности обратной связи. Исследование проведено на материале семи самых
посещаемых новостных интернет-медиа Украины: «Корреспондент.net»
«Obozrevatel», «Сегодня», «Телеканал 24», «ТСН.ua», «Украинская правда»,
«Цензор.НЕТ» – с 1 января 2014 г. – по 31 декабря 2018 г. Результаты исследования
показали, что новостные интернет-медиа уделяют мало внимания теме научных
новостей и инноваций (0,01% материалов). В фокусе – медицина, история и
психология. Интерес массмедиа сконцентрирован на: научно-практических
разработках; достижениях США и стран Западной Европы. Около 91% материалов
позаимствованы из других источников, преимущественно западных и российских,
часто с ошибками, сокращениями. Значительная часть тем освещена с целью
развлечения, а не всестороннего информирования о науке. Треть публикаций имеют
заглавия, которые полностью или частично не соответствуют содержанию основного
текста. Освещение факторов диффузии инноваций (по Э. Роджерсу) обеспечивается
на невысоком уровне. Более всего реализован фактор наблюдения, менее всего –
испытуемости. Только в двух интернет-изданиях из семи используется возможность
общественного обсуждения новостей науки и инноваций в комментариях.Zaitseva S. S. Social communication aspects of scientific news and innovations
coverage in the digital media of Ukraine. – Qualified scientific work on the rights of the
manuscript.
The thesis for assignment of a Scholarly Degree of Candidate in Social
Communications by speciality 27.00.01 – Theory and History of Social Communications. –
Classical Private University, Zaporizhzhia, 2020.
The work is devoted to the elucidation of social and communication features of
covering the scientific innovations in the Internet media of Ukraine. On the basis of the
agenda theory and diffusion theory of innovations with the help of content analysis and
structured qualitative analysis, it is determined what topics are covered by the Ukrainian
Internet publication, how they are covered and how modern feedback opportunities in
Internet publications are used. The research is conducted on the material of the seven most
visited Internet news media of Ukraine: «Correspondent.net», «Obozrevatel», «Today»,
«TV channel 24», «TSN.ua», «Ukrainska Pravda», «Censor.net» – from 1 January 2014 till
December 31, 2018.
About 0.1% of the total number of materials turned out to be allocated to the
covering scientific news and innovations (for comparison: in the British, Danish, Spanish
media – 4-7%). Medicine and health care are devoted to 42.7%, history – 10.6%,
psychology – 10.6%. The topics of minimal interest were agronomy, anthropology,
mathematics, meteorology, pedagogy, sociology, chemistry.
Preference is given to topics related to people's daily lives, habits, basic needs –
nutrition, safety (including health), sexual relations, sleep, belonging and more. It is noted
that the majority (90.9%) of materials on scientific achievements are unoriginal: they are
submitted with reference to the source or a chain from several previous source. Only 9.1% of
materials do not indicate the source of the borrowing, that is, they are submitted as original
(although they may not be). The greatest influence in the formation of the agenda regarding
science in the Internet media of Ukraine has Western («The Daily Mail», «BBC», «The
Independent», etc.) and the Russian media («Lenta. ru», «Gazeta. ru», «RIA Novosti», etc.).
Online publications in Ukraine talk mostly about the achievements of Western
universities, primarily the United States (28%). The discoveries and inventions of the UK
and Ukraine are devoted to 8.1% of materials.
The most frequently mentioned organizations are Harvard University (4%), NASA
(2.9%), University of California (2.5%), and Massachusetts Institute of Technology (2.5%).
Among the Ukrainian institutes are the National Academy of Sciences of Ukraine – 1.8%
and Kiev National University Shevchenko – 1.5%. References to the scientific source
contain only 20.7% of the materials, the names of the authors of inventions or discoveries
are indicated in 20.9%.
The study also considers the headings of publications on scientific news and
innovations in terms of their main functions in the online publication. Described features of
using attention-grabbing techniques such as simplification, invasiveness, negativization,
provocativeness; reference to the main text, numbers, and questions are in news headlines
about science. The most common were links to the main text – 75.2%; invasiveness,
provocation, simplification are also common. To determine the function of informing, the
headings were divided into three types: fully correspond to the essence of the publication,
partially correspond or do not correspond. Full compliance was observed in 69.2% of cases,
partial – in 21.9%, non-compliance – 9.1. It is emphasized that the inconsistency of the
headlines to the publications content (as a consequence – inaccurate information) in the
case of the studying topic becomes an acute problem when it comes to innovations that
people can put into practice, especially medical ones.
In addition to the agenda, the study also looks at the diffusion factors (by E. Rogers)
in media reports on scientific news and innovations. As for the «relative advantage» factor,
it is found that most often it is not specified at all or is unclear – the total figure is 69.6%.
Almost all cases where relative advantage was indicated (clearly or indistinctly) relate to
practical scientific achievements.
The data obtained showed that compatibility of the analyzed editions is often not
indicated at all (48.7%). If indicated, it is more often accurate than inaccurate – 34.8% and
16.6% respectively.
When assessing the factor of «complexity», the level of presentation complexity of
information is considered. A simple language about scientific news and innovation is
reported in 82% of the material, with a high rate in all the publications analyzed. But this
indicator was high because, in many materials, journalists did not disclose the essence of
innovation at all/ The main reason for the complications is the use of highly specialized
terms without explanation. Difficult lighting is more characteristic of theoretical scientific
achievements.
It is found that trialability (fourth factor) is rarely reported in the analyzed editions:
only 18.5% of the material. The most common topics within this group were information
technology (drones, robots, educational tools, etc.) – 52.5%, as well as medicine and health
care – 25%
The «observability» factor was evaluated taking into account the presence and
quality of visual elements accompanying the publication of scientific novelty or innovation.
Just over a third (35.2%) of publications was found to be accompanied by informative
visualization tools. The most common way of non-informative illustration is with stock
photos such as Getty Images, Pixabay and more.
Through the same type of stock photo illustrations to materials (which do not really
serve illustrative functions) the mass media can form stereotypical visual perceptions of
science. Nanotechnology (85.7%) of materials within the theme provided with informative
visual component), artificial intelligence (80%), robotics (77.8%), information technology
(72.7%), military defense (70.7%) have the highest level of observation.
Discussing scientific news and innovations in society is also essential for diffusion.
Each of the analysis publications has at least two tools: buttons for distribution of
publications on social networks and forms for commenting. Despite the availability of tools
for discussing science and innovation news, in most publications (Obozrevatel, TV Channel
24, TSN.ua, Today), they are not used or used improperly. With the hired commentators,
whose statements are only remotely related to the informational drive, instead aimed at
inciting aggression and improving website analytics. Discussions on the essence of mass
media reports took place only in the publications «Censor.NET» and, to a less extent,
«Ukrainian Pravda». The main areas of discussion in both editions are the relative
advantage of innovation and compatibility. A characteristic feature of commenting on
science and innovation news is that discussions involving «real», unpaid participants are
deployed solely in connection with scientific and practical innovation",Класичний приватний університет,Соціальнокомунікаційні аспекти висвітлення наукових новин та інновацій в інтернет-медіа України,,,,core
304167278,2020-01-01T00:00:00,"On the pursuit of autonomous flying robots, the scientific community
has been developing onboard real-time algorithms for localisation,
mapping and planning. Despite recent progress, the available solutions
still lack accuracy and robustness in many aspects. While mapping for
autonomous cars had a substantive boost using deep-learning techniques
to enhance LIDAR measurements using image-based depth completion, the
large viewpoint variations experienced by aerial vehicles are still
posing major challenges for learning-based mapping approaches. In this
paper, we propose a depth completion and uncertainty estimation
approach that better handles the challenges of aerial platforms, such
as large viewpoint and depth variations, and limited computing
resources. The core of our method is a novel compact network that
performs both depth completion and confidence estimation using an
image-guided approach. Real-time performance onboard a GPU suitable for
small flying robots is achieved by sharing deep features between both
tasks. Experiments demonstrate that our network outperforms the
state-of-the-art in depth completion and uncertainty estimation for
single-view methods on mobile GPUs. We further present a new
photorealistic aerial depth completion dataset that exhibits more
challenging depth completion scenarios than the established indoor and
car driving datasets. The dataset includes an open-source,
visual-inertial UAV simulator for photo-realistic data generation. Our
results show that our network trained on this dataset can be directly
deployed on real-world outdoor aerial public datasets without
fine-tuning or style transfer.ISSN:2377-376",'Institute of Electrical and Electronics Engineers (IEEE)',Aerial Single-View Depth Completion with Image-Guided Uncertainty Estimation,,,10.1109/LRA.2020.2967296,core
328195607,2020-08-16T00:00:00,"Does progress in simulation translate to progress on robots? If one method
outperforms another in simulation, how likely is that trend to hold in reality
on a robot? We examine this question for embodied PointGoal navigation,
developing engineering tools and a research paradigm for evaluating a simulator
by its sim2real predictivity. First, we develop Habitat-PyRobot Bridge (HaPy),
a library for seamless execution of identical code on simulated agents and
robots, transferring simulation-trained agents to a LoCoBot platform with a
one-line code change. Second, we investigate the sim2real predictivity of
Habitat-Sim for PointGoal navigation. We 3D-scan a physical lab space to create
a virtualized replica, and run parallel tests of 9 different models in reality
and simulation. We present a new metric called Sim-vs-Real Correlation
Coefficient (SRCC) to quantify predictivity. We find that SRCC for Habitat as
used for the CVPR19 challenge is low (0.18 for the success metric), suggesting
that performance differences in this simulator-based challenge do not persist
after physical deployment. This gap is largely due to AI agents learning to
exploit simulator imperfections, abusing collision dynamics to 'slide' along
walls, leading to shortcuts through otherwise non-navigable space. Naturally,
such exploits do not work in the real world. Our experiments show that it is
possible to tune simulation parameters to improve sim2real predictivity (e.g.
improving $SRCC_{Succ}$ from 0.18 to 0.844), increasing confidence that
in-simulation comparisons will translate to deployed systems in reality",'Institute of Electrical and Electronics Engineers (IEEE)',"Sim2Real Predictivity: Does Evaluation in Simulation Predict Real-World
  Performance?",http://arxiv.org/abs/1912.06321,,10.1109/LRA.2020.3013848,core
323329229,2020-03-11T00:00:00,"Assistive robots collaborating with people demand strong Human-Robot interaction capabilities. In this way, recognizing the person the robot has to interact with is paramount to provide a personalized service and reach a satisfactory end-user experience. 
To this end, face recognition: a non-intrusive, automatic mechanism of identification using biometric identifiers from an user's face, has gained relevance in the recent years, as the advances in machine learning and the creation of huge public datasets have considerably improved the state-of-the-art performance.
 In this work we study different open-source implementations of the typical components of state-of-the-art face recognition pipelines, including face detection, feature extraction and classification, and propose a recognition system integrating the most suitable methods for their utilization in assistant robots. 
 Concretely, for face detection we have considered MTCNN, OpenCV's DNN, and OpenPose, while for feature extraction we have analyzed InsightFace and Facenet.
 We have made public an implementation of the proposed recognition framework, ready to be used by any robot running the Robot Operating System (ROS).
 The methods in the spotlight have been compared in terms of accuracy and performance in common benchmark datasets, namely FDDB and LFW, to aid the choice of the final system implementation, which has been tested in a real robotic platform.This work is supported by the Universidad de Málaga. Campus de Excelencia Internacional Andalucía Tech, the research projects WISER ([DPI2017-84827-R]),funded by the Spanish Government, and financed by European RegionalDevelopment’s funds (FEDER), and MoveCare ([ICT-26-2016b-GA-732158]), funded by the European H2020 program, and by a postdoc contract from the I-PPIT-UMA program financed by the University of Málaga",,A face recognition system for assistive robots,,,,core
334894947,2020-02-17T00:00:00,"Deep reinforcement learning has the potential to train robots to perform
complex tasks in the real world without requiring accurate models of the robot
or its environment. A practical approach is to train agents in simulation, and
then transfer them to the real world. One popular method for achieving
transferability is to use domain randomisation, which involves randomly
perturbing various aspects of a simulated environment in order to make trained
agents robust to the reality gap. However, less work has gone into
understanding such agents - which are deployed in the real world - beyond task
performance. In this work we examine such agents, through qualitative and
quantitative comparisons between agents trained with and without visual domain
randomisation. We train agents for Fetch and Jaco robots on a visuomotor
control task and evaluate how well they generalise using different testing
conditions. Finally, we investigate the internals of the trained agents by
using a suite of interpretability techniques. Our results show that the primary
outcome of domain randomisation is more robust, entangled representations,
accompanied with larger weights with greater spatial structure; moreover, the
types of changes are heavily influenced by the task setup and presence of
additional proprioceptive inputs. Additionally, we demonstrate that our domain
randomised agents require higher sample complexity, can overfit and more
heavily rely on recurrent processing. Furthermore, even with an improved
saliency method introduced in this work, we show that qualitative studies may
not always correspond with quantitative measures, necessitating the combination
of inspection tools in order to provide sufficient insights into the behaviour
of trained agents",,"Analysing Deep Reinforcement Learning Agents Trained with Domain
  Randomisation",http://arxiv.org/abs/1912.08324,,,core
334967437,2020-01-01T00:00:00,"While the teleoperation framework has been successfully implemented for the surgical robots, especially for soft tissue interventions, the main challenge is that the surgeons are responsible for all actions taken, and all decisions made, during the entire surgery. As the robotics technology explores more complicated surgical interventions, teleoperation framework can become increasingly overwhelmingfor the human surgeons, who inherit limited sensing and motor control bandwidth, and can result in degraded surgical performance. Introduction of automation and intelligence into the robot-assisted interventions, where some of the surgical responsibilities are delegated to the AI agent, can substantially improve this framework and enhance the overall surgical outcome. Amongst many challenges of bringing autonomy into the surgical interventions, the main two technological ones pertain to the complexity of soft tissue environments and the inaccuracies of the surgical robotics systems. This dissertation aims at addressing these two challenges and proposes various solutions for different surgical robotic systems with applications to laparoscopic, orthopedic, and opthalmologic surgeries. Regarding planning of surgical subtasks, suturing and tissue manipulation which occur frequently in soft tissue surgeries are considered. For suturing task, two novel optimization-based needle motion planning algorithms, Fixed Center Motion (FCM) and Moving Center Motion (MCM), are proposed where the tissue trauma is minimized and a wide variety of suturing criteria (i.e., adequate depth) are met. An extensive simulations for each method were provided to (I) confirm the mathematical formulations and (II) to obtain optimal strategies under various suturing conditions. The FCM needle planning was deployed on Raven IV system with an open loop controller (i.e, no vision feedback) and experiment results confirmed the simulation and optimization outcomes. Regarding the tissue manipulation task, a new synergic learning method where human knowledge contributes to selecting intuitive features of tissue manipulation while the algorithm learns to take optimal actions is proposed. The method was tested on four different configurations in the simulations and the robot was able to successfully accomplish the task of tissue manipulation autonomously for all. To improve estimation and control accuracy of three surgical robotics systems, multiple frameworks are proposed. For the first category which pertain to cable-driven serial manipulators used for soft tissue surgeries, a 6 DoF visual servo controller using robot-camera calibration and realtime vision feedback was developed. The framework enabled the Raven IV surgical system to perform autonomous suturing task for various suturing trajectories and tissue compliance. For the second category which pertain to continuum manipulators with applications to orthopedic surgery and bronchoscopy, a novel stochastic sensor fusion algorithm, called Simultaneous Sensor Calibration and Deformation Estimation (SCADE), was introduced. SCADE addresses the problem of estimating calibration bias of FBG sensors as well as the shape/tip of the continuum surgical manipulators simultaneously in real-time. The algorithm was tested to estimate the tip position of a continuum manipulator within free and obstructed environments and showed superior performance compared to estimations from FBG sensor. For the third category which pertain to a robot-assisted cataract surgery system, a new hardware and software solution was proposed to estimate the tip location of surgical tools inside the eye during cataract surgery. The framework was developed andtested using a total of 31 pig eyes and results demonstrated efficacy of the proposed solution","eScholarship, University of California","Towards Building Autonomy and Intelligence for Surgical Robotic Systems using Trajectory Optimization, Stochastic Estimation, Vision-based Control, and Machine Learning Algorithms",,,,core
289186554,2020-01-01T00:00:00,"Robots deployed in human-centric environments, such as a person's home in a home-assistance setting or inside a person's body in a surgical setting, have the potential to have a large, positive impact on human quality of life. However, for robots to operate in such environments they must be able to move efficiently while avoiding colliding with obstacles such as objects in the person's home or sensitive anatomical structures in the person's body. Robot motion planning aims to compute safe and efficient motions for robots that avoid obstacles, but home assistance and surgical robots come with unique challenges that can make this difficult. For instance, many state of the art surgical robots have computationally expensive kinematic models, i.e., it can be computationally expensive to predict their shape as they move. Some of these robots have hybrid dynamics, i.e., they consist of multiple stages that behave differently. Additionally, it can be difficult to plan motions for robots while leveraging real-world sensor data, such as point clouds.      In this dissertation, we demonstrate and empirically evaluate methods for overcoming these challenges to compute high-quality and safe motions for robots in home-assistance and surgical settings. First, we present a motion planning method for a continuum, parallel surgical manipulator that accounts for its computationally expensive kinematics. We then leverage this motion planner to optimize its kinematic design chosen prior to a surgical procedure. Next, we present a motion planning method for a 3-stage lung tumor biopsy robot that accounts for its hybrid dynamics and evaluate the robot and planner in simulation and in inflated porcine lung tissue. Next, we present a motion planning method for a home-assistance robot that leverages real-world, point-cloud obstacle representations. We then expand this method to work with a type of continuum surgical manipulator, a concentric tube robot, with point-cloud anatomical representations. Finally, we present a data-driven machine learning method for more accurately estimating the shape of concentric tube robots. By effectively addressing challenges associated with home assistance and surgical robots operating in human-centric environments, we take steps toward enabling robots to have a positive impact on human quality of life.Doctor of Philosoph",University of North Carolina at Chapel Hill Graduate School,Integrating Optimization and Sampling for Robot Motion Planning with Applications in Healthcare,https://core.ac.uk/download/289186554.pdf,,10.17615/j14c-rw49,core
394994330,2020-08-31T00:00:00,"“© © 2020 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted component of this work in other works.”Traditional robotic control suits require profound task-specific knowledge for designing, building and testing control software. The rise of Deep Learning has enabled end-to-end solutions to be learned entirely from data, requiring minimal knowledge about the application area. We design a learning scheme to train end-to-end linear dynamical systems (LDS)s by gradient descent in imitation learning robotic domains. We introduce a new regularization loss component together with a learning algorithm that improves the stability of the learned autonomous system, by forcing the eigenvalues of the internal state updates of an LDS to be negative reals. We evaluate our approach on a series of real-life and simulated robotic experiments, in comparison to linear and nonlinear Recurrent Neural Network (RNN) architectures. Our results show that our stabilizing method significantly improves test performance of LDS, enabling such linear models to match the performance of contemporary nonlinear RNN architectures. A video of the obstacle avoidance performance of our method on a mobile robot, in unseen environments, compared to other methods can be viewed at https://youtu.be/mhEsCoNao5E.M.L. is supported in parts by the Austrian Science Fund (FWF) under grant Z211-N23 (Wittgenstein Award). R.H., and R.G. are partially supported by the Horizon-2020 ECSEL Project grant No. 783163 (iDev40), and the Austrian Research Promotion Agency (FFG), Project No. 860424. R.H. and D.R. is partially supported by the Boeing Company.",,Gershgorin Loss Stabilizes the Recurrent Neural Network Compartment of an End-to-end Robot Learning Scheme,,,10.34726/242,core
395666093,2020-01-01T00:00:00,"In recent years, incremental sampling-based motion planning algorithms have been widely used to solve robot motion planning problems
in high-dimensional configuration spaces. In particular, the Rapidly-exploring Random Tree (RRT) algorithm and its asymptotically-optimal
counterpart called RRT* are popular algorithms used in real-life applications due to its desirable properties. Such algorithms are inherently
iterative, but certain modules such as the collision-checking procedure can be parallelized providing significant speedup with respect to sequential
implementations. In this paper, the RRT and RRT* algorithms have been adapted to a bioinspired computational framework called Membrane
Computing whose models of computation, a.k.a. P systems, run in a non-deterministic and massively parallel way. A large number of robotic
applications are currently using a variant of P systems called Enzymatic Numerical P systems (ENPS) for reactive controlling, but there is a lack
of solutions for motion planning in the framework. The novel models in this work have been designed using the ENPS framework. In order to test
and validate the ENPS models for RRT and RRT*, we present two ad-hoc implementations able to emulate the computation of the models using
OpenMP and CUDA. Finally, we show the speedup of our solutions with respect to sequential baseline implementations. The results show a
speedup up to 6x using OpenMP with 8 cores against the sequential implementation and up to 24x using CUDA against the best multi-threading
configuration.Ministerio de Industria, Economía y Competitividad TIN2017- 89842-P (MABICAP)National Natural Science Foundation of China No. 61972324National Natural Science Foundation of China No. 61672437National Natural Science Foundation of China No. 61702428Beijing Advanced Innovation Center for Intelligent Robots and Systems 2019IRS14Artificial Intelligence Key Laboratory of Sichuan Province 2019RYJ06New Generation Artificial Intelligence Science and Technology Major Project of Sichuan Province 2018GZDZX0043Sichuan Science and Technology Program 2018GZ0185Sichuan Science and Technology Program 2018GZ008",'IOS Press',A membrane parallel rapidly-exploring random tree algorithm for robotic motion planning,,,,core
337294926,2020-12-28T00:00:00,"Inferring the depth of images is a fundamental inverse problem within the
field of Computer Vision since depth information is obtained through 2D images,
which can be generated from infinite possibilities of observed real scenes.
Benefiting from the progress of Convolutional Neural Networks (CNNs) to explore
structural features and spatial image information, Single Image Depth
Estimation (SIDE) is often highlighted in scopes of scientific and
technological innovation, as this concept provides advantages related to its
low implementation cost and robustness to environmental conditions. In the
context of autonomous vehicles, state-of-the-art CNNs optimize the SIDE task by
producing high-quality depth maps, which are essential during the autonomous
navigation process in different locations. However, such networks are usually
supervised by sparse and noisy depth data, from Light Detection and Ranging
(LiDAR) laser scans, and are carried out at high computational cost, requiring
high-performance Graphic Processing Units (GPUs). Therefore, we propose a new
lightweight and fast supervised CNN architecture combined with novel feature
extraction models which are designed for real-world autonomous navigation. We
also introduce an efficient surface normals module, jointly with a simple
geometric 2.5D loss function, to solve SIDE problems. We also innovate by
incorporating multiple Deep Learning techniques, such as the use of
densification algorithms and additional semantic, surface normals and depth
information to train our framework. The method introduced in this work focuses
on robotic applications in indoor and outdoor environments and its results are
evaluated on the competitive and publicly available NYU Depth V2 and KITTI
Depth datasets.Comment: 29 pages, 16 figures. Preprint published in the Elsevier's Robotics
  and Autonomous Systems journal on November 23, 202",'Elsevier BV',"On Deep Learning Techniques to Boost Monocular Depth Estimation for
  Autonomous Navigation",http://arxiv.org/abs/2010.06626,,10.1016/j.robot.2020.103701,core
326450695,2020-01-01T00:00:00,"Tracking moving objects like pedestrian have a wide range of applications for intelligent autonomous vehicle. While object detection using modern deep learning-based approach such as Convolutional Neural Network (CNN) have advanced significantly, autonomous system such as ground mobile robots still face problems in real-time implementation. These methods include CNN based human tracking due to occlusions, illumination changes and camera view variations. The aim of this project is to identify the optimal deep learning algorithm, in terms of speed and accuracy. Human tracking by detection using selected real-time object detector is also implemented. Firstly, in terms of performance speed, in terms of Frames per Second (FPS), One-stage Detectors, Single Shot Detector (SSD) achieve 19 FPS, You Only Look Once (YOLOv3) achieve 22 FPS as compared to Two-stage Detectors, such as Mask Region-Convolutional Neural Network (Mask R-CNN) which achieve 1 FPS, with this, it is concluded that One-stage Detectors is more suitable for real time implementation. In addition, Multiple Object Tracking Precision (MOTP) is used as a measure for detection accuracy, it is found that SSD achieved 68.4%, YOLOv3 achieved 72.9% and Mask R-CNN achieved 74.0%. Thus, YOLOv3 is used in this project as it is the optimal object detector. Next, the detection output coordinates as bounding boxes (in pixels) from the YOLOv3 is passed to the tracking algorithm that associate detections over frames to create a trajectory of any tracked person. The decision rule to associate detection over frames is computed by Hungarian Algorithm (Kuhn-Munkres) that takes on motion affinity and appearance similarity as inputs. For motion affinity, Intersection Over Union (IOU) provides information on physical proximity between the new detections and the tracked persons. The appearance similarity is measured by computing Euclidean distance between the last CNN features of the detected person. Promising results were obtained in the various tests carried out. Finally, the tracking algorithm is tested on mobile robot with Robot Operating System (ROS) framework, with images captured from Orbbec Astra RGB-D camera which is passed into object detection algorithm. The tracking is performed subsequently in real-time. Two major challenges faced during testing were ID switching and missing detections. While the latter is limited by the detection algorithm, ID switch occurs when two similar looking persons are very close and tracking algorithm are unable to differentiate them.Bachelor of Engineering (Electrical and Electronic Engineering",'Nanyang Technological University',Human motion tracking using deep learning,,,,core
288813934,2020-03-27T00:00:00,"Η παρούσα διδακτορική διατριβή αφορά τη μελέτη, ανάπτυξη και εφαρμογή, μεθόδων
Μηχανικής Μάθησης μέσω Παρατήρησης (Learning from Demonstration) με στόχο την
ρομποτική αναπαραγωγή δράσεων χειρισμού. Η μεθοδολογία αυτή στηρίζεται στην
δημιουργία μιας αντιστοίχισης (mapping) μεταξύ της κινηματικής του ανθρώπινου χεριού και
ενός ρομποτικού βραχίονα, ή πιο συγκεκριμένα μεταξύ του πολυδιάστατου χώρου των
κινήσεων του ανθρώπου (human actor) με τον επίσης πολυδιάστατο χώρο δράσης του
ρομπότ. Η συσχέτιση των ανθρώπινων ενεργειών με αντίστοιχες ρομποτικές, επιτυγχάνεται
μέσω μιας άδηλης αναπαράστασης, που ονομάζεται λανθάνουσα απεικόνιση χώρου (latent
space). Πιο συγκεκριμένα, μελετάμε την αμοιβαία αλληλεπίδραση της αντίληψης και της 
δράσης, προκειμένου να διδάξουμε τα ρομπότ μια ποικιλία από νέες κινήσεις χειρός. Ως εκ
τούτου, υλοποιήθηκε ένα μεθοδολογικό πλαίσιο μάθησης μέσω παρατήρησης, το οποίο
ονομάζεται IMFO (Imitation Framework by Observation), που διευκολύνει την αναπαραγωγή
μαθημένων και νέων κινήσεων χειρισμού από ένα ρομπότ (manipulation tasks) και,
παράλληλα, έχει ευρεία εφαρμογή σε σενάρια αλληλεπίδρασης ανθρώπου-ρομπότ (HRI) σε
καθημερινά περιβάλλοντα.
Επιπλέον, σε αυτή τη διατριβή, εξετάζουμε το ρόλο της χρονικής διάρκειας εκτέλεσης μιας
κίνησης μέσα από τη διαδικασία μάθησης από παρατήρηση, ενισχύοντας το διαμορφωμένο
πλαίσιο IMFO με την δυνατότητα αναπαράστασης και αναπαραγωγής τόσο των χωρικών όσο
και των χρονικών χαρακτηριστικών των ανθρώπινων κινήσεων. Σε αντίθεση με άλλες
μεθόδους μάθησης μέσω παρατήρησης (LfD) που περιγράφουν την εκτελούμενη δράση μόνο
με βάση τα χωρικά χαρακτηριστικά της, η προτεινόμενη μεθοδολογία ενισχύει την
αναπαραγωγή των χωροχρονικών πτυχών μιας κίνησης επιτρέποντας την αποτελεσματική
εφαρμογή της σε πιο σύνθετα σενάρια HRI, όπου η χρονική αλληλουχία των δράσεων είναι
σημαντική. Επιπρόσθετα, εισάγεται ένα σύνολο καλά καθορισμένων μετρικών αξιολόγησης
(evaluation metrics) για να αποτιμηθεί η εγκυρότητα της προτεινόμενης προσέγγισης
λαμβάνοντας υπόψη τη χρονική και χωρική συνέπεια των αναπαραγόμενων συμπεριφορών.
Μια αξιοσημείωτη επέκταση του προαναφερθέντος πλαισίου αναφέρεται στην εκμάθηση
της δύναμης που επιβάλλεται από τον χρήστη για την επιτυχημένη εκτέλεση λεπτών
χειρισμών. Αυτή η διαδικασία παρουσιάζεται επίσης στην παρούσα διατριβή μέσω ενός
νέου πλαισίου εποπτευόμενης μάθησης, το οποίο ονομάζεται SLF (Supervised Learning
scheme for Force-based manipulation). Το SLF διατυπώνεται ως μία διαδικασία τριών
σταδίων: (α) επιβλεπόμενη διαδικασία εκτέλεσης κινήσεων χειρισμού σε προσομοίωση για
την απόκτηση επαρκών δεδομένων, (β) διαδικασία εκπαίδευσης (training) για τη
διευκόλυνση της μάθησης κινήσεων χειρισμού με την κατάλληλη προσαρμογή του καρπού
και της δύναμη πιασίματος και μεταφοράς και (γ) εκτέλεση της κίνησης από ρομποτικό
βραχίονα σε προσομοίωση. Στη συνέχεια, με τη χρήση της μεθόδου sim-to-real transfer,
επιτυγχάνεται αναπαραγωγή των μαθημένων δράσεων σε πραγματικά περιβάλλοντα
γενικεύοντας την εφαρμογή του πλαισίου μάθησης σε επιπλέον συνθήκες χειρισμού
εύθραυστων αντικειμένων. Τα αποτελέσματα με τη χρήση του ρομποτικού βραχίονα YuMi,
σε πειράματα με διαφορετικά αντικείμενα με παρόμοιους συντελεστές τριβής, και
εναλλακτικές πόζες πιασίματος, αποδεικνύουν ότι το ρομπότ είναι σε θέση να αναπαράγει
αποτελεσματικά απαιτητικές κινήσεις μεταφοράς και χειρισμού μετά την ολοκλήρωση της
διαδικασίας μάθησης.
Συνοπτικά, η παρούσα διατριβή μελετά την διαδικασία μάθησης μέσω παρατήρησης
συνεισφέροντας με μια νέα προσέγγιση που εισάγει την μελέτη δράσεων χειρισμού
αντικειμένων μέσα από έναν χώρο μειωμένων διαστάσεων, για την εύκολη και συμπαγή
κωδικοποίηση των επιμέρους χαρακτηριστικών των δράσεων. Ταυτόχρονα μελετώνται τα
χρονικά χαρακτηριστικά των κινήσεων ώστε να ενισχυθεί η εφαρμογή της μεθόδου σε
σύνθετες, πραγματικές συνθήκες που απαιτούν χρονική ακρίβεια αναπαραγωγής. Τέλος, η
διαμόρφωση μιας γενικευμένης διαδικασίας εποπτευόμενης μάθησης για τον χειρισμό
εύθραυστων αντικείμενων αναβαθμίζει περαιτέρω το αρχικό πλαίσιο μάθησης.The current PhD thesis addresses the formulation and implementation of a methodological
framework for robot Learning from Demonstration (LfD). The latter refers to methodologies
that develop behavioral policies from example state-to-action mappings. To this
end, we study the reciprocal interaction of perception and action, in order to teach robots
a repertoire of novel action behaviors. Based on that, we design, develop and implement
a robust imitation framework, termed IMFO (IMitation Framework by Observation), that
facilitates imitation learning and relevant applications in human-robot interaction (HRI)
tasks. IMFO can cope with the reproduction of learned (i.e. previously observed) actions,
aswell as novel ones. Mapping of human actions to the respective robotic ones is achieved
via an indeterminate depiction, termed latent space representation. The latter accomplishes
a compact, yet precise abstraction of action trajectories, effectively representing
high dimensional raw actions in a low dimensional space.
Moreover, throughout this thesis, we examine the role of time in LfD by enhancing
the aforementioned framework with the notion of learning both the spatial and temporal
characteristics of human motions. Accordingly, learned actions can be subsequently reproduced
in the context of more complex time-informed HRI scenarios. Unlike previous
LfD methods that cope only with the spatial traits of an action, the formulated scheme
effectively encompasses spatial and temporal aspects. Extensive experimentation with a
variety of real robotic platforms demonstrates the robustness and applicability of the introduced
integrated LfD scheme.
Learned actions are reproduced under the high level control of a time-informed task
planner. During the implementation of the studied scenarios, temporal and physical constraints
may impose speed adaptations in the performed actions. The employed latent
space representation readily supports such variations, giving rise to novel actions in the
temporal domain. Experimental results demonstrate the effectiveness of the proposed
enhanced imitation scheme in the implementation of HRI scenarios. Additionally, a set
of well defined evaluation metrics are introduced to assess the validity of the proposed
approach considering the temporal and spatial consistency of the reproduced behaviors.
A noteworthy extension of the above regards force-based object grasping for executing
sensitive manipulation tasks. This is also treated in the current thesis via a novel supervised
learning scheme, termed SLF (Supervised Learning for Force-based manipulation).
SLF is formulated as a three-stage process: (a) supervised trial-execution in simulation
to acquire sufficient training data; (b) training to facilitate grasp learning with suitable
robot-arm pose and lifting force; (c) grasp execution in simulation. Subsequently, following
sim-to-real transfer, operation in real environments is achieved in addition to simulated
ones, generalizing also for objects not included in the trial sessions. The proposed
learning scheme is demonstrated in object lifting tasks where the applied force varies for
different objects with similar contact friction coefficients, and likewise the grasping pose.
Experimental results on the manipulator YuMi show that the robot is able to effectively
reproduce demanding lifting and manipulation tasks after learning is accomplished.
In summary, our thesis has studied LfD and has contributed with a novel approach that
introduced latent space representations to encode the action characteristics. A framework
implementation (IMFO) of our approach allowed extensive experimentation and also conduction
of HRI scenarios. The inclusion of temporal aspects in our approach enhanced it
to cope with complex, real-life interactions. Finally, the extension of IMFO with forcebased
grasping facilitated manipulation tasks with sensitive objects",,Μάθηση μέσω παρατήρησης για την επίτευξη ρομποτικών δράσεων χειρισμού,,,,core
387285024,2020-11-27T00:00:00,"Open-ended learning is a core research field of machine learning and robotics
aiming to build learning machines and robots able to autonomously acquire
knowledge and skills and to reuse them to solve novel tasks. The multiple
challenges posed by open-ended learning have been operationalized in the
robotic competition REAL 2020. This requires a simulated camera-arm-gripper
robot to (a) autonomously learn to interact with objects during an intrinsic
phase where it can learn how to move objects and then (b) during an extrinsic
phase, to re-use the acquired knowledge to accomplish externally given goals
requiring the robot to move objects to specific locations unknown during the
intrinsic phase. Here we present a 'baseline architecture' for solving the
challenge, provided as baseline model for REAL 2020. Few models have all the
functionalities needed to solve the REAL 2020 benchmark and none has been
tested with it yet. The architecture we propose is formed by three components:
(1) Abstractor: abstracting sensory input to learn relevant control variables
from images; (2) Explorer: generating experience to learn goals and actions;
(3) Planner: formulating and executing action plans to accomplish the
externally provided goals. The architecture represents the first model to solve
the simpler REAL 2020 'Round 1' allowing the use of a simple parameterised push
action. On Round 2, the architecture was used with a more general action
(sequence of joints positions) achieving again higher than chance level
performance. The baseline software is well documented and available for
download and use at https://github.com/AIcrowd/REAL2020_starter_kit.Comment: 21 pages, 8 figure",,"An open-ended learning architecture to face the REAL 2020 simulated
  robot competition",http://arxiv.org/abs/2011.13880,,,core
334903391,2020-01-16T00:00:00,"Augmented Reality has been subject to various integration efforts within
industries due to its ability to enhance human machine interaction and
understanding. Neural networks have achieved remarkable results in areas of
computer vision, which bear great potential to assist and facilitate an
enhanced Augmented Reality experience. However, most neural networks are
computationally intensive and demand huge processing power thus, are not
suitable for deployment on Augmented Reality devices. In this work we propose a
method to deploy state of the art neural networks for real time 3D object
localization on augmented reality devices. As a result, we provide a more
automated method of calibrating the AR devices with mobile robotic systems. To
accelerate the calibration process and enhance user experience, we focus on
fast 2D detection approaches which are extracting the 3D pose of the object
fast and accurately by using only 2D input. The results are implemented into an
Augmented Reality application for intuitive robot control and sensor data
visualization. For the 6D annotation of 2D images, we developed an annotation
tool, which is, to our knowledge, the first open source tool to be available.
We achieve feasible results which are generally applicable to any AR device
thus making this work promising for further research in combining high
demanding neural networks with Internet of Things devices.Comment: 6 pages,5 figures. arXiv admin note: text overlap with
  arXiv:1912.1210",,"A Markerless Deep Learning-based 6 Degrees of Freedom PoseEstimation for
  with Mobile Robots using RGB Data",http://arxiv.org/abs/2001.05703,,,core
395677188,2020-07-17T00:00:00,"International audienceThe use of Reinforcement Learning (RL) is still restricted to simulation or to enhance human-operated systems through recommendations. Real-world environments (e.g. industrial robots or power grids) are generally designed with safety constraints in mind implemented in the shape of valid actions masks or contingency controllers. For example, the range of motion and the angles of the motors of a robot can be limited to physical boundaries. Violating constraints thus results in rejected actions or entering in a safe mode driven by an external controller, making RL agents incapable of learning from their mistakes. In this paper, we propose a simple modification of a state-of-the-art deep RL algorithm (DQN), enabling learning from forbidden actions. To do so, the standard Q-learning update is enhanced with an extra safety loss inspired by structured classification. We empirically show that it reduces the number of hit constraints during the learning phase and accelerates convergence to near-optimal policies compared to using standard DQN. Experiments are done on a Visual Grid World Environment and Text-World domain",HAL CCSD,"""I'm sorry Dave, I'm afraid I can't do that"" Deep Q-Learning From Forbidden Actions",https://core.ac.uk/download/395677188.pdf,,,core
351907779,2020-01-01T00:00:00,"The main concepts and techniques of multi-agent oriented programming, which supports the multi-agent systems paradigm at the programming level.

A multi-agent system is an organized ensemble of autonomous, intelligent, goal-oriented entities called agents, communicating with each other and interacting within an environment. This book introduces the main concepts and techniques of multi-agent oriented programming, (MAOP) which supports the multi-agent systems paradigm at the programming level. MAOP provides a structured approach based on three integrated dimensions, which the book examines in detail: the agent dimension, used to design the individual (interacting) entities; the environment dimension, which allows the development of shared resources and connections to the real world; and the organization dimension, which structures the interactions among the autonomous agents and the shared environment.

The book puts the approach into practice using the JaCaMo programming model and platform. It employs an easy-to-follow, step-by-step style, showing solutions to increasingly complex scenarios. The book also discusses the integration of MAOP into existing technologies and application domains, including mobile computing, web-based computing, and robotics. Finally, it considers artificial intelligence (AI)\u2013related classical problems from an MAOP perspective and discusses an agent-oriented approach to software engineering",place:Cambridge,Multi-Agent Oriented Programming -- Programming Multi-Agent Systems Using JaCaMo,,,,core
186320024,2019-01-29T00:00:00,"Advances in robotics, artificial intelligence, and machine learning are
ushering in a new age of automation, as machines match or outperform human
performance. Machine intelligence can enable businesses to improve performance
by reducing errors, improving sensitivity, quality and speed, and in some cases
achieving outcomes that go beyond current resource capabilities. Relevant
applications include new product architecture design, rapid material
characterization, and life-cycle management tied with a digital strategy that
will enable efficient development of products from cradle to grave. In
addition, there are also challenges to overcome that must be addressed through
a major, sustained research effort that is based solidly on both inferential
and computational principles applied to design tailoring of functionally
optimized structures. Current applications of structural materials in the
aerospace industry demand the highest quality control of material
microstructure, especially for advanced rotational turbomachinery in aircraft
engines in order to have the best tailored material property. In this paper,
deep convolutional neural networks were developed to accurately predict
processing-structure-property relations from materials microstructures images,
surpassing current best practices and modeling efforts. The models
automatically learn critical features, without the need for manual
specification and/or subjective and expensive image analysis. Further, in
combination with generative deep learning models, a framework is proposed to
enable rapid material design space exploration and property identification and
optimization. The implementation must take account of real-time decision cycles
and the trade-offs between speed and accuracy",Structural Material Property Tailoring Using Deep Neural Networks,http://arxiv.org/abs/1901.10281,,,,core
334851887,2019-08-27T00:00:00,"The deep supervised and reinforcement learning paradigms (among others) have
the potential to endow interactive multimodal social robots with the ability of
acquiring skills autonomously. But it is still not very clear yet how they can
be best deployed in real world applications. As a step in this direction, we
propose a deep learning-based approach for efficiently training a humanoid
robot to play multimodal games---and use the game of `Noughts & Crosses' with
two variants as a case study. Its minimum requirements for learning to perceive
and interact are based on a few hundred example images, a few example
multimodal dialogues and physical demonstrations of robot manipulation, and
automatic simulations. In addition, we propose novel algorithms for robust
visual game tracking and for competitive policy learning with high winning
rates, which substantially outperform DQN-based baselines. While an automatic
evaluation shows evidence that the proposed approach can be easily extended to
new games with competitive robot behaviours, a human evaluation with 130 humans
playing with the Pepper robot confirms that highly accurate visual perception
is required for successful game play","A Data-Efficient Deep Learning Approach for Deployable Multimodal Social
  Robots",http://arxiv.org/abs/1908.10398,'Elsevier BV',10.1016/j.neucom.2018.09.104,,core
186317260,2019-01-22T00:00:00,"The ability to recover from a fall is an essential feature for a legged robot
to navigate in challenging environments robustly. Until today, there has been
very little progress on this topic. Current solutions mostly build upon
(heuristically) predefined trajectories, resulting in unnatural behaviors and
requiring considerable effort in engineering system-specific components. In
this paper, we present an approach based on model-free Deep Reinforcement
Learning (RL) to control recovery maneuvers of quadrupedal robots using a
hierarchical behavior-based controller. The controller consists of four neural
network policies including three behaviors and one behavior selector to
coordinate them. Each of them is trained individually in simulation and
deployed directly on a real system. We experimentally validate our approach on
the quadrupedal robot ANYmal, which is a dog-sized quadrupedal system with 12
degrees of freedom. With our method, ANYmal manifests dynamic and reactive
recovery behaviors to recover from an arbitrary fall configuration within less
than 5 seconds. We tested the recovery maneuver more than 100 times, and the
success rate was higher than 97 %","Robust Recovery Controller for a Quadrupedal Robot using Deep
  Reinforcement Learning",http://arxiv.org/abs/1901.07517,,,,core
334829637,2019-09-22T00:00:00,"Machine and reinforcement learning (RL) are increasingly being applied to
plan and control the behavior of autonomous systems interacting with the
physical world. Examples include self-driving vehicles, distributed sensor
networks, and agile robots. However, when machine learning is to be applied in
these new settings, the algorithms had better come with the same type of
reliability, robustness, and safety bounds that are hallmarks of control
theory, or failures could be catastrophic. Thus, as learning algorithms are
increasingly and more aggressively deployed in safety critical settings, it is
imperative that control theorists join the conversation. The goal of this
tutorial paper is to provide a starting point for control theorists wishing to
work on learning related problems, by covering recent advances bridging
learning and control theory, and by placing these results within an appropriate
historical context of system identification and adaptive control.Comment: Tutorial paper, 2019 IEEE Conference on Decision and Control, to
  appea",From self-tuning regulators to reinforcement learning and back again,http://arxiv.org/abs/1906.11392,,,,core
228089876,2019-01-01T00:00:00,"Reinforcement Learning methods are capable of solving complex problems, but resulting policies might perform poorly in environments that are even slightly different. In robotics especially, training and deployment conditions often vary and data collection is expensive, making retraining undesirable. Simulation training allows for feasible training times, but on the other hand suffer from a reality-gap when applied in real-world settings. This raises the need of efficient adaptation of policies acting in new environments. We consider the problem of transferring knowledge within a family of similar Markov decision processes. We assume that Q-functions are generated by some low-dimensional latent variable. Given such a Q-function, we can find a master policy that can adapt given different values of this latent variable. Our method learns both the generative mapping and an approximate posterior of the latent variables, enabling identification of policies for new tasks by searching only in the latent space, rather than the space of all policies. The low-dimensional space, and master policy found by our method enables policies to quickly adapt to new environments. We demonstrate the method on both a pendulum swing-up task in simulation, and for simulation-to-real transfer on a pushing task.QC 20190916Factories of the Future (FACT",Vpe : Variational policy embedding for transfer reinforcement learning,,'Institute of Electrical and Electronics Engineers (IEEE)',10.1109/ICRA.2019.8793556,,core
231796070,2019-01-01T00:00:00,"Reinforcement Learning methods are capable of solving complex problems, but resulting policies might perform poorly in environments that are even slightly different. In robotics especially, training and deployment conditions often vary and data collection is expensive, making retraining undesirable. Simulation training allows for feasible training times, but on the other hand suffer from a reality-gap when applied in real-world settings. This raises the need of efficient adaptation of policies acting in new environments. We consider the problem of transferring knowledge within a family of similar Markov decision processes. We assume that Q-functions are generated by some low-dimensional latent variable. Given such a Q-function, we can find a master policy that can adapt given different values of this latent variable. Our method learns both the generative mapping and an approximate posterior of the latent variables, enabling identification of policies for new tasks by searching only in the latent space, rather than the space of all policies. The low-dimensional space, and master policy found by our method enables policies to quickly adapt to new environments. We demonstrate the method on both a pendulum swing-up task in simulation, and for simulation-to-real transfer on a pushing task.QC 20190916Factories of the Future (FACT",Vpe : Variational policy embedding for transfer reinforcement learning,,'Institute of Electrical and Electronics Engineers (IEEE)',10.1109/ICRA.2019.8793556,,core
334836948,2019-07-16T00:00:00,"Vision-based depth reconstruction is a challenging problem extensively
studied in computer vision but still lacking universal solution. Reconstructing
depth from single image is particularly valuable to mobile robotics as it can
be embedded to the modern vision-based simultaneous localization and mapping
(vSLAM) methods providing them with the metric information needed to construct
accurate maps in real scale. Typically, depth reconstruction is done nowadays
via fully-convolutional neural networks (FCNNs). In this work we experiment
with several FCNN architectures and introduce a few enhancements aimed at
increasing both the effectiveness and the efficiency of the inference. We
experimentally determine the solution that provides the best
performance/accuracy tradeoff and is able to run on NVidia Jetson with the
framerates exceeding 16FPS for 320 x 240 input. We also evaluate the suggested
models by conducting monocular vSLAM of unknown indoor environment on NVidia
Jetson TX2 in real-time. Open-source implementation of the models and the
inference node for Robot Operating System (ROS) are available at
https://github.com/CnnDepth/tx2_fcnn_node.Comment: Camera-ready version as submitted to ECMR 201",Real-time Vision-based Depth Reconstruction with NVidia Jetson,http://arxiv.org/abs/1907.07210,,,,core
304653591,2019-06-05T00:00:00,"Current state-of-the-art machine learning systems efficiently identify known classes of objects with a high rate of success. However, their inference capabilities become limited in the presence of objects for which little or no training data exists, or when extreme environmental conditions drastically affect the scene appearance. Therefore, their effectiveness when deployed in the context of exploring and interacting with previously unencountered environments with novel entities, is often limited. This research is focused on complementing such operations by discovering previously unclassified objects, while also refining their localization in images to facilitate robust tracking and extraction. The ultimate purpose is to facilitate the automated enhancement of the trained classification system with new class information via incremental retraining. The proposed unclassified object detection pipeline involves a multi-step architecture that starts by generating object proposals from a hierarchical set of graph segmentations across multiple color and frequency domains. Subsequently these are filtered, further refined, and fused by exploiting visual attention prediction models that estimate pixel-wise saliency in color images. This bottom-up approach aims to remain generic, while managing to propose a meaningfully small set of high quality regions within the image context so as to optimize their evaluation for retraining. The presented unclassified object discovery results come from experimental datasets taken from the viewpoint of an aerial robot in a variety of operating conditions and environments. They are given with reference to the YOLOv3 state-of-the-art detection algorithm to indicate the complementarity of our approach in managing to isolate unknown entities during real-time operation",Unsupervised Detection of Unclassified Objects via Graph-based Segmentations and Image Saliency Fusion for Automated Incremental Learning,,,,,core
151254364,2019-02-01T00:00:00,"The ability to interpret a scene is an important capability for a robot that
is supposed to interact with its environment. The knowledge of what is in front
of the robot is, for example, relevant for navigation, manipulation, or
planning. Semantic segmentation labels each pixel of an image with a class
label and thus provides a detailed semantic annotation of the surroundings to
the robot. Convolutional neural networks (CNNs) are popular methods for
addressing this type of problem. The available software for training and the
integration of CNNs for real robots, however, is quite fragmented and often
difficult to use for non-experts, despite the availability of several
high-quality open-source frameworks for neural network implementation and
training. In this paper, we propose a tool called Bonnet, which addresses this
fragmentation problem by building a higher abstraction that is specific for the
semantic segmentation task. It provides a modular approach to simplify the
training of a semantic segmentation CNN independently of the used dataset and
the intended task. Furthermore, we also address the deployment on a real
robotic platform. Thus, we do not propose a new CNN approach in this paper.
Instead, we provide a stable and easy-to-use tool to make this technology more
approachable in the context of autonomous systems. In this sense, we aim at
closing a gap between computer vision research and its use in robotics
research. We provide an open-source codebase for training and deployment. The
training interface is implemented in Python using TensorFlow and the deployment
interface provides a C++ library that can be easily integrated in an existing
robotics codebase, a ROS node, and two standalone applications for label
prediction in images and videos.Comment: To be published in to IEEE International Conference on Robotics and
  Automation 201","Bonnet: An Open-Source Training and Deployment Framework for Semantic
  Segmentation in Robotics using CNNs",http://arxiv.org/abs/1802.08960,,,,core
220589792,2019-01-01T00:00:00,"Abstract The Robotic Process Automation (RPA) is a new wave of the future technologies. Robotic Process Automation is one of the most advanced technologies in the area of computers science, electronic and communications, mechanical engineering and information technology. It is a combination of both hardware and software, networking and automation for doing things very simple. In this light, the research manuscript investigated the secondary data - which is available in google, academic and research databases. The investigation went for totally 6 months, i.e., 1-1-2018 to 30-6-2018. A very few empirical articles, white papers, blogs and were found RPA and came across to compose this research manuscript. This study is exploratory in nature because of the contemporary phenomenon. The keywords used in searching of the database were Robotic Process Automation, RPA, Robots, Artificial Intelligence, Blue Prism. The study finally discovered that Robots and Robotic Process Automation technologies are becoming compulsory as a part to do business operations in the organizations across the globe. Robotic Process Automation can bring immediate value to the core business processes including employee payroll, employee status changes, new hire recruitment and on boarding, accounts receivable and payable, invoice processing, inventory management, report creation, software installations, data migration, and vendor on boarding etc. to name a few applications. Besides, the Robotic Process Automation has abundant applications including healthcare and pharmaceuticals, financial services, outsourcing, retail, telecom, energy and utilities, real estate and FMCG and many more sectors. To put in the right place of RPA in business operations, their many allied technologies are working at the background level, artificial intelligence, machine learning, deep learning, data analytics, HR analytics, virtual reality (second life), home automation, blockchain technologies, 4D printing etc. Moreover, it covers the content of different start-ups companies and existing companies - their RPA applications used across the world. This manuscript will be a good guideline for the academicians, researchers, students, and practitioners to get an overall idea",The Future Digital Work Force: Robotic Process Automation (RPA),,,,,core
186266456,2019-01-25T00:00:00,"Autonomous robotics and artificial intelligence techniques can be used to
support human personnel in the event of critical incidents. These incidents can
pose great danger to human life. Some examples of such assistance include:
multi-robot surveying of the scene; collection of sensor data and scene
imagery, real-time risk assessment and analysis; object identification and
anomaly detection; and retrieval of relevant supporting documentation such as
standard operating procedures (SOPs). These incidents, although often rare, can
involve chemical, biological, radiological/nuclear or explosive (CBRNE)
substances and can be of high consequence. Real-world training and deployment
of these systems can be costly and sometimes not feasible. For this reason, we
have developed a realistic 3D model of a CBRNE scenario to act as a testbed for
an initial set of assisting AI tools that we have developed.Comment: arXiv admin note: substantial text overlap with arXiv:1806.0449","A Virtual Testbed for Critical Incident Investigation with Autonomous
  Remote Aerial Vehicle Surveying, Artificial Intelligence, and Decision
  Support",http://arxiv.org/abs/1809.06244,,,,core
328264402,2019-12-07T00:00:00,"Digital  disruptive  technologies  are  an  integral  component  of  the  modern  world.  These  technologies  are  transforming  the  globalindustries from traditional to more innovative and adaptive. However, the state of global real estate is yet to improve and is currently lagging the technology curve. Because of this lag, useful information is either not made available to the end-users or is shared too late that is raising concerns  among  the  online real  estate  platform  users.  This  results  in  larger  vacancy  rates  and  post-occupancy  regrets  among  the  service consumers. The current study based on the concepts of Technology Acceptance Models (TAM), presents a conceptual Real Estate Stakeholders Technology Acceptance Model (RESTAM) for addressing the key needs of the four important stakeholders of the real estate industry including the end-users or consumers, government & regulatory authorities, agents & agencies and complementary industries. Based on comprehensive literature review of 213 articles, the needs of these stakeholders are assessed and addressed through the Big9 technologies namely drones, the internet of things (IoT), clouds, software as a service (SaaS), big data, 3D scanning, wearable technologies, virtual and augmented realities (VR & AR), and artificial intelligence and robotics. The resulting RESTAM framework with a specific focus on the online platform based real estate users are expected to lay the foundation for introducing the missing technology acceptance model for real estate stakeholders whereby these Big9 disruptive technologies are implemented in real estate industry to uplift it from traditional to smart real estate. This will reduce the post-occupancy regrets of the real estate service users and improve the relations between various real estate stakeholders","2nd International Conference on Sustainable Development in Civil Engineering (ICSDC 2019), Jamshoro Pakistan",https://www.researchgate.net/profile/Fahim_Ullah3/publication/337772796_Real_Estate_Stakeholders_Technology_Acceptance_Model_RESTAM_User-focused_Big9_Disruptive_Technologies_for_Smart_Real_Estate_Management/links/5de99409a6fdcc28370939dc/Real-Estate-Stakeholders-Technology-Acceptance-Model-RESTAM-User-focused-Big9-Disruptive-Technologies-for-Smart-Real-Estate-Management.pdf,"MUET, Pakistan",,,core
232931862,2019-01-01T00:00:00,"Reinforcement Learning methods are capable of solving complex problems, but resulting policies might perform poorly in environments that are even slightly different. In robotics especially, training and deployment conditions often vary and data collection is expensive, making retraining undesirable. Simulation training allows for feasible training times, but on the other hand suffer from a reality-gap when applied in real-world settings. This raises the need of efficient adaptation of policies acting in new environments. We consider the problem of transferring knowledge within a family of similar Markov decision processes. We assume that Q-functions are generated by some low-dimensional latent variable. Given such a Q-function, we can find a master policy that can adapt given different values of this latent variable. Our method learns both the generative mapping and an approximate posterior of the latent variables, enabling identification of policies for new tasks by searching only in the latent space, rather than the space of all policies. The low-dimensional space, and master policy found by our method enables policies to quickly adapt to new environments. We demonstrate the method on both a pendulum swing-up task in simulation, and for simulation-to-real transfer on a pushing task.QC 20190916Factories of the Future (FACT",Vpe : Variational policy embedding for transfer reinforcement learning,,'Institute of Electrical and Electronics Engineers (IEEE)',10.1109/ICRA.2019.8793556,,core
160786583,2019-04-19T00:00:00,"The recent drive towards achieving greater autonomy and intelligence in
robotics has led to high levels of complexity. Autonomous robots increasingly
depend on third party off-the-shelf components and complex machine-learning
techniques. This trend makes it challenging to provide strong design-time
certification of correct operation.
  To address these challenges, we present SOTER, a robotics programming
framework with two key components: (1) a programming language for implementing
and testing high-level reactive robotics software and (2) an integrated runtime
assurance (RTA) system that helps enable the use of uncertified components,
while still providing safety guarantees. SOTER provides language primitives to
declaratively construct a RTA module consisting of an advanced,
high-performance controller (uncertified), a safe, lower-performance controller
(certified), and the desired safety specification. The framework provides a
formal guarantee that a well-formed RTA module always satisfies the safety
specification, without completely sacrificing performance by using higher
performance uncertified components whenever safe. SOTER allows the complex
robotics software stack to be constructed as a composition of RTA modules,
where each uncertified component is protected using a RTA module.
  To demonstrate the efficacy of our framework, we consider a real-world
case-study of building a safe drone surveillance system. Our experiments both
in simulation and on actual drones show that the SOTER-enabled RTA ensures the
safety of the system, including when untrusted third-party components have bugs
or deviate from the desired behavior","SOTER: A Runtime Assurance Framework for Programming Safe Robotics
  Systems",http://arxiv.org/abs/1808.07921,,,,core
299955648,2019-01-01T00:00:00,"The project calls for the application of engineering and programming knowledge to develop and implement an autonomous vehicle, the Nanyang Venture 11 (NV-11) which will be taking part in the Shell Eco-marathon Asia 2019. Throughout the project, different task had been allocated to collaborate as a team to build the NV-11. A 3-phase Brushless DC (BLDC) electric motor controller that was designed by the previous student had a few flaws which were the instantaneous current surge that cause the components on the circuitry board to be damaged. The implementation of the Soft Start code improved the performance and prevent it from any sudden current spikes occurring. The software kit by Renesas Electronics Corporation was used to program the microprocessor board to determine the clockwise and anti-clockwise rotation of the vehicle wheels. The NVIDIA Jetson TX2 is a supercomputer which is an embedded Artificial Intelligence computing device that acts as the brain on the autonomous vehicle NV-11. It operates using Ubuntu 16.04 Operating System and has the compatibility of using the Robot Operating System (ROS) as a platform that provides a set of software libraries and tools to build an application. A virtual environment was designed as a simulation to test the functionality of the autonomous vehicle using the data from the exterior sensors like the Lidar sensors, Zed camera, and Radar sensors. Using TensorFlow which is an open-source software library for machine learning to gather data for obstacles avoidance and detection. The Tinkerforge IMU Brick 2.0 is a device which is equipped with a 3-axis accelerometer, magnetometer (compass) and gyroscope. It provides the quaternions data which is a mathematical notation for representing orientations and rotations of the vehicle in three dimensions. It was implemented as part of the simulation to determine the exact location in the real world, comparing with the virtual world.Bachelor of Engineering (Electrical and Electronic Engineering",Implementation of electrical and simulation for autonomous driving,,,,,core
201224611,2019-02-01T00:00:00,"Autonomously following a man-made trail in the wild is a challenging problem for robotic systems. Recently, deep learning-based approaches have cast the trail following problem as an image classification task and have achieved great success in the vision-based trail-following problem. However, the existing research only focuses on the trail-following task with a single-robot system. In contrast, many robotic tasks in reality, such as search and rescue, are conducted by a group of robots. While these robots are grouped to move in the wild, they can cooperate to lead to a more robust performance and perform the trail-following task in a better manner. Concretely, each robot can periodically exchange the vision data with other robots and make decisions based both on its local view and the information from others. This paper proposes a sensor fusion-based cooperative trail-following method, which enables a group of robots to implement the trail-following task by fusing the sensor data of each robot. Our method allows each robot to face the same direction from different altitudes to fuse the vision data feature on the collective level and then take action respectively. Besides, considering the quality of service requirement of the robotic software, our method limits the condition to implementing the sensor data fusion process by using the “threshold” mechanism. Qualitative and quantitative experiments on the real-world dataset have shown that our method can significantly promote the recognition accuracy and lead to a more robust performance compared with the single-robot system",Sensor Fusion-Based Cooperative Trail Following for Autonomous Multi-Robot System,,'MDPI AG',10.3390/s19040823,"[{'title': 'Sensors', 'identifiers': ['issn:1424-8220', '1424-8220']}]",core
200810936,2019-03-12T00:00:00,"The 5th edition of the International Conference on Cloud and Robotics (ICCR
2018 - http://cloudrobotics.info) will be held on November 12-14 2018 in Paris
and Saint-Quentin, France. The conference is a co-event with GDR ALROB and the
industry exposition Robonumerique (http://www.robonumerique.fr).
  The domain of cloud robotics aims to converge robots with computation,
storage and communication resources provided by the cloud. The cloud may
complement robotic resources in several ways, including crowd-sourcing
knowledge databases, context information, computational offloading or
data-intensive information processing for artificial intelligence. Today, the
paradigms of cloud/fog/edge computing propose software architecture solutions
for robots to share computations or offload them to ambiant and networked
resources. Yet, combining distant computations with the real time constraints
of robotics is very challenging. As the challenges in this domain are
multi-disciplinary and similar in other research areas, Cloud Robotics aims at
building bridges among experts from academia and industry working in different
fields, such as robotics, cyber-physical systems, automotive, aerospace,
machine learning, artificial intelligence, software architecture, big data
analytics, Internet-of-Things, networked control and distributed cloud systems","Proceedings of the Fifth International Conference on Cloud and Robotics
  (ICCR2018)",http://arxiv.org/abs/1903.04824,,,,core
231839019,2019-12-04T00:00:00,"Building an efficient and reliable collision perception visual system is a challenging problem for future robots and autonomous vehicles. The biological visual neural networks, which have evolved over millions of years in nature, and are working perfectly in the real world, could be ideal models for designing artificial vision systems. In the locust's visual pathways, a lobula giant movement detector, i.e. the LGMD2, has been identified as a looming perception neuron that responds most strongly to darker approaching objects relative to their backgrounds, similar situations which many ground vehicles and robots are often facing with. However, little has been done on modelling the LGMD2 and investigating its potential in robotics and vehicles. In this research, we build an LGMD2 visual neural network which possesses the similar collision selectivity of an LGMD2 neuron in locust, via the modelling of biased-ON and OFF pathways splitting visual signals into parallel ON/OFF channels. With stronger-inhibition (bias) in the ON pathway, this model responds selectively to darker looming objects. The proposed model has been tested systematically with a range of stimuli including real-world scenarios. It has also been implemented in a micro mobile robot and tested with real-time experiments. The experimental results have verified the effectiveness and robustness of the proposed model for detecting darker looming objects against various dynamic and cluttered backgrounds",A Robust Collision Perception Visual Neural Network with Specific Selectivity to Darker Objects,https://core.ac.uk/download/231839019.pdf,'Institute of Electrical and Electronics Engineers (IEEE)',10.1109/TCYB.2019.2946090,,core
237407160,2019-10-30T15:35:41,"Deep learning has been successful in a variety of applications, such as object recognition, video games, and machine translation. Deep neural networks can automatically learn important features given large training datasets. However, the success of deep learning in robotic systems in the real world is still limited mainly because obtaining large datasets and labeling are costly. As a result, much of the successful work in deep learning has been limited to domains where large datasets are readily available or easily collected. To address this issue, I propose a framework for acquiring re-usable skills efficiently combining intrinsic motivation and the control basis framework --- a developmental architecture implemented using a landscape of attractors. A deep neural classifier is used to predict probabilistic control affordances representing controller states accessible by way of actions. Information theoretic motivation with embedding distance influences exploration in a way that supports learning control affordances efficiently. The learned affordance is used as a filter to help a robot experience more positive outcomes of controllers when a robot learns a new affordance. I conduct quantitative experiments using a dynamic simulator and the results show that the proposed learning framework enables a mobile manipulator to learn deep sensorimotor skills more efficiently in a self-supervised learning manner",Efficient Self-supervised Deep Sensorimotor Learning in Robotics,https://core.ac.uk/download/237407160.pdf,ScholarWorks@UMass Amherst,,,core
297665105,2019-01-01T00:00:00,"Emergency personnel, such as firefighters, bomb technicians, and urban search and rescue specialists, can be exposed to a variety of extreme hazards during the response to natural and human-made disasters. In many of these scenarios, a risk factor is the presence of hazardous airborne chemicals. The recent and rapid advances in robotics and sensor technologies allow emergency responders to deal with such hazards from relatively safe distances. Mobile robots with gas-sensing capabilities allow to convey useful information such as the possible source positions of different chemicals in the emergency area. However, common gas sampling procedures for laboratory use are not applicable due to the complexity of the environment and the need for fast deployment and analysis. In addition, conventional gas identification approaches, based on supervised learning, cannot handle situations when the number and identities of the present chemicals are unknown. For the purpose of emergency response, all the information concluded from the gas detection events during the robot exploration should be delivered in real time. To address these challenges, we developed an online gas-sensing system using an electronic nose. Our system can automatically perform unsupervised learning and update the discrimination model as the robot is exploring a given environment. The online gas discrimination results are further integrated with geometrical information to derive a multi-compound gas spatial distribution map. The proposed system is deployed on a robot built to operate in harsh environments for supporting fire brigades, and is validated in several different real-world experiments of discriminating and mapping multiple chemical compounds in an indoor open environment. Our results show that the proposed system achieves high accuracy in gas discrimination in an online, unsupervised, and computationally efficient manner. The subsequently created gas distribution maps accurately indicate the presence of different chemicals in the environment, which is of practical significance for emergency response.Funding Agency:European Commission  645101</p",Towards Gas Discrimination and Mapping in Emergency Response Scenarios Using a Mobile Robot with an Electronic Nose,,'MDPI AG',10.3390/s19030685,,core
333871880,2019-01-01T00:00:00,"A search and rescue drone are an unmanned aircraft used by emergency services, such as police officers, firefighters or volunteer rescue teams, ideal for searching over vast areas for missing persons and crime victims in need of rescue and in any environment. Unmanned aerial vehicles (UAVs) can provide real-time visual information and data in the aftermath of an earthquake or hurricane. They can also become an eye in the sky to locate a lost person in the mountain for example. This report is about the system that can help to automatically detect the human body in lying position. The benefit of this project is we can use drone for search and rescue operation (SAR) and disaster victim identification (DVI). This is because this drone can detect human body in lying position exact location in the disaster area so that the rescuer team can easily find and locate the missing victim. Most of the disaster area is very dangerous for human to enter to give the first quick rescue. The search and rescue team also need to prepare on the suitable attire to wear before enter the disaster area. This drone is very helpful because it provides mobility, easy to setup, real-time visual information and location and suitable for any type of disaster. The existing product in market does not implement artificial intelligence (AI) for the object detection. They just monitor and observe the screen themselves in order to get information about survivor from the disaster. A market survey was conducted and the survey has shown that a majority of the respondent were agree that the existing method for SAR operation consumes a lot of time start from the set-up of the operation until the rescue operation. The survey also collected data of respondent that really hope this system was going to use for the SAR operation. Our target markets are Malaysian Fire Fighter, Malaysian Civil Defense and university researchers. We are now competing with SnakeBot and BEAR (Battlefield Extraction-Assist Robot), as they also provide the services for search and rescue operation. We estimated our profit per month is RM 48,500 and annually is RM 582,000. However, this system is not fully ready yet to be implemented for the SAR operation. There is still a lot room for improvement needed to be done","Drone assisted detection system (deADS) / Muhammad Nur Haziq Abdul Shukor, Mohammad Aidil Shah Sajat and 'Abdurrouuf Anuar",,,,,core
211998416,2019-06-04T07:47:58,"Legged robots pose one of the greatest challenges in robotics. Dynamic and agile maneuvers of animals cannot be imitated by existing methods that are crafted by humans. A compelling alternative is reinforcement learning, which requires minimal craftsmanship and promotes the natural evolution of a control policy. However, so far, reinforcement learning research for legged robots is mainly limited to simulation, and only few and comparably simple examples have been deployed on real systems. The primary reason is that training with real robots, particularly with dynamically balancing systems, is complicated and expensive. In the present work, we introduce a method for training a neural network policy in simulation and transferring it to a state-of-the-art legged system, thereby leveraging fast, automated, and cost-effective data generation schemes. The approach is applied to the ANYmal robot, a sophisticated medium-dog–sized quadrupedal system. Using policies trained in simulation, the quadrupedal machine achieves locomotion skills that go beyond what had been achieved with prior methods: ANYmal is capable of precisely and energy-efficiently following high-level body velocity commands, running faster than before, and recovering from falling even in complex configurations",Learning agile and dynamic motor skills for legged robots,,'American Association for the Advancement of Science (AAAS)',10.1126/scirobotics.aau5872,,core
200836328,2019-05-10T00:00:00,"Nano-size unmanned aerial vehicles (UAVs), with few centimeters of diameter
and sub-10 Watts of total power budget, have so far been considered incapable
of running sophisticated visual-based autonomous navigation software without
external aid from base-stations, ad-hoc local positioning infrastructure, and
powerful external computation servers. In this work, we present what is, to the
best of our knowledge, the first 27g nano-UAV system able to run aboard an
end-to-end, closed-loop visual pipeline for autonomous navigation based on a
state-of-the-art deep-learning algorithm, built upon the open-source CrazyFlie
2.0 nano-quadrotor. Our visual navigation engine is enabled by the combination
of an ultra-low power computing device (the GAP8 system-on-chip) with a novel
methodology for the deployment of deep convolutional neural networks (CNNs). We
enable onboard real-time execution of a state-of-the-art deep CNN at up to
18Hz. Field experiments demonstrate that the system's high responsiveness
prevents collisions with unexpected dynamic obstacles up to a flight speed of
1.5m/s. In addition, we also demonstrate the capability of our visual
navigation engine of fully autonomous indoor navigation on a 113m previously
unseen path. To share our key findings with the embedded and robotics
communities and foster further developments in autonomous nano-UAVs, we
publicly release all our code, datasets, and trained networks.Comment: Accepted for publication in Proceeding of International Conference on
  Distributed Computing in Sensor Systems (DCOSS 2019). arXiv admin note: text
  overlap with arXiv:1805.0183","An Open Source and Open Hardware Deep Learning-powered Visual Navigation
  Engine for Autonomous Nano-UAVs",http://arxiv.org/abs/1905.04166,'Institute of Electrical and Electronics Engineers (IEEE)',10.1109/DCOSS.2019.00111,,core
267812972,2019-08-01T00:00:00,"Text in EnglishIn recent years, there have been tremendous advances in information technology, robotics, communication technology, nanotechnology, and artificial intelligence, resulting in the merging  of physical, digital, and biological worlds that have come to be known as the ""fourth industrial revolution”. In this context, the present study engages such technology in the green economy and to tackle the techno-economic environmental impact assessments challenges associated with floating solar system applications in the agricultural sector of South Africa. In response, this exploratory study aimed to examine the development of a Geographical Information System (GIS)-based support platform for Environmental Impact Assessment (EIA) and due-diligence analyses for future planned agricultural floating solar systems, especially with the goal to address the vast differences between the environmental impacts for land-based and water-based photovoltaic energy systems.
A research gap was identified in the planning processes for implementing floating solar
systems in South Africa’s agricultural sector. This inspired the development of a novel GIS-based modelling tool to assist with floating solar system type energy infrastructure planning in the renewable energy discourse. In this context, there are significant challenges and future research avenues for technical and environmental performance modelling in the new sustainable energy transformation. The present dissertation and geographical research ventured into the conceptualisation, designing and development of a software GIS-based decision support tool to assist environmental impact practitioners, project owners and landscape architects to perform environmental scoping and environmental due-diligence analysis for planned floating solar systems in the local agricultural sector. In terms of the aims and objectives of the research, this project aims at the design and development of a dedicated GIS toolset to determine the environmental feasibility around the use of floating solar systems in agricultural applications in South Africa. In this context, the research objectives of this study included the use of computational modelling and simulation techniques to theoretically determine the energy yield predictions and computing environmental impacts/offsets for future planned agricultural floating solar systems in South
Africa. The toolset succeeded in determining these aspects in applications where floating
solar systems would substitute Eskom grid power. The study succeeded in developing a
digital GIS-based computer simulation model for floating solar systems capable of (a) predicting the anticipated energy yield, (b) calculating the environmental offsets achieved by substituting coal-fired generation by floating solar panels, (c) determining the environmental impact and land-use preservation benefits of any floating solar system, and (d) relating these metrics to water-energy-land-food (WELF) nexus parameters suitable for user project viability analysis and decision support. The research project has demonstrated how the proposed GIS toolset supports the body of geographical knowledge in the fields of Energy and Environmental Geography. The new toolset, called EIAcloudGIS, was developed to assist in solving challenges around
energy and environmental sustainability analysis when planning new floating solar installations on farms in South Africa. Experiments conducted during the research showed how the geographical study in general, and the toolset in particular, succeeded in solving a real-world problem. Through the formulation and development of GIS-based computer simulation models embedded into GIS layers, this new tool practically supports the National Environmental Management Act (NEMA Act No. 107 of 1998), and in particular, associated EIA processes. The tool also simplifies and semi-automates certain aspects of environmental impact analysis processes for newly envisioned and planned floating solar installations in South Africa.GeographyM.Sc. (Geography",Development of a GIS-based decision support tool for environmental impact assessment and due-diligence analyses of planned agricultural floating solar systems,https://core.ac.uk/download/267812972.pdf,,,,core
186267692,2019-10-08T00:00:00,"Current end-to-end deep Reinforcement Learning (RL) approaches require
jointly learning perception, decision-making and low-level control from very
sparse reward signals and high-dimensional inputs, with little capability of
incorporating prior knowledge. This results in prohibitively long training
times for use on real-world robotic tasks. Existing algorithms capable of
extracting task-level representations from high-dimensional inputs, e.g. object
detection, often produce outputs of varying lengths, restricting their use in
RL methods due to the need for neural networks to have fixed length inputs. In
this work, we propose a framework that combines deep sets encoding, which
allows for variable-length abstract representations, with modular RL that
utilizes these representations, decoupling high-level decision making from
low-level control. We successfully demonstrate our approach on the robot
manipulation task of object sorting, showing that this method can learn
effective policies within mere minutes of highly simplified simulation. The
learned policies can be directly deployed on a robot without further training,
and generalize to variations of the task unseen during training",Sim-to-Real Transfer of Robot Learning with Variable Length Inputs,http://arxiv.org/abs/1809.07480,,,,core
186318395,2019-01-24T00:00:00,"Legged robots pose one of the greatest challenges in robotics. Dynamic and
agile maneuvers of animals cannot be imitated by existing methods that are
crafted by humans. A compelling alternative is reinforcement learning, which
requires minimal craftsmanship and promotes the natural evolution of a control
policy. However, so far, reinforcement learning research for legged robots is
mainly limited to simulation, and only few and comparably simple examples have
been deployed on real systems. The primary reason is that training with real
robots, particularly with dynamically balancing systems, is complicated and
expensive. In the present work, we introduce a method for training a neural
network policy in simulation and transferring it to a state-of-the-art legged
system, thereby leveraging fast, automated, and cost-effective data generation
schemes. The approach is applied to the ANYmal robot, a sophisticated
medium-dog-sized quadrupedal system. Using policies trained in simulation, the
quadrupedal machine achieves locomotion skills that go beyond what had been
achieved with prior methods: ANYmal is capable of precisely and
energy-efficiently following high-level body velocity commands, running faster
than before, and recovering from falling even in complex configurations",Learning agile and dynamic motor skills for legged robots,http://arxiv.org/abs/1901.08652,'American Association for the Advancement of Science (AAAS)',10.1126/scirobotics.aau5872,,core
163029050,2019-07-05T00:00:00,"The deep supervised and reinforcement learning paradigms (among others) have the potential to endow interactive multimodal social robots with the ability of acquiring skills autonomously. But it is still not very clear yet how they can be best deployed in real world applications. As a step in this direction, we propose a deep learning-based approach for efficiently training a humanoid robot to play multimodal games---and use the game of `Noughts \& Crosses' with two variants as a case study. Its minimum requirements for learning to perceive and interact are based on a few hundred example images, a few  example multimodal dialogues and physical demonstrations of robot manipulation, and automatic simulations. In addition, we propose novel algorithms for robust visual game tracking and for competitive policy learning with high winning rates, which substantially outperform DQN-based baselines. While an automatic evaluation shows evidence that the proposed approach can be easily extended to new games with competitive robot behaviours, a human evaluation with 130 humans playing with the {\it Pepper} robot confirms that highly accurate visual perception is required for successful game play",A Data-Efficient Deep Learning Approach for Deployable Multimodal Social Robots,https://core.ac.uk/download/163029050.pdf,'Elsevier BV',10.1016/j.neucom.2018.09.104,,core
185550496,2019,"The paper aims at investigating the present debate on the state of the art and the

regulatory perspectives for Robotics and Artificial Intelligence, with particular attention

to the opportunity to establish an independent body with monitoring functions,

evaluation of existing rules, elaboration of regulatory proposals, as well as quasi-jurisdictional

and adjudication functions. The challenges faced in the experimentation

of robotic systems in operational environments tackle the passage of certain technologies

from laboratory to reality, from design and prototype development to its

concrete deployment, with all the consequences in terms of acceptance of the technologies

themselves. Another problem concerns the experimentation of special and

derogatory pilot rules in their effective functioning. Some references to the Japanese

experience are given in the present essay, to present a case that leads to hypothesize,

in conclusion, normative experiments on Robotics and AI in the European environment,

especially under the coordinating role of an authority acting at the European

level, developing regulatory guidelines, monitoring the results of the experiments, and

identifying the best practices","La regolazione della Robotica e dell'Intelligenza artificiale:il dibattito, le proposte, le prospettive. Alcuni spunti di riflessione.",,,,,core
160782091,2019-02-12T00:00:00,"We study active object tracking, where a tracker takes visual observations
(i.e., frame sequences) as input and produces the corresponding camera control
signals as output (e.g., move forward, turn left, etc.). Conventional methods
tackle tracking and camera control tasks separately, and the resulting system
is difficult to tune jointly. These methods also require significant human
efforts for image labeling and expensive trial-and-error system tuning in the
real world. To address these issues, we propose, in this paper, an end-to-end
solution via deep reinforcement learning. A ConvNet-LSTM function approximator
is adopted for the direct frame-to-action prediction. We further propose an
environment augmentation technique and a customized reward function, which are
crucial for successful training. The tracker trained in simulators (ViZDoom and
Unreal Engine) demonstrates good generalization behaviors in the case of unseen
object moving paths, unseen object appearances, unseen backgrounds, and
distracting objects. The system is robust and can restore tracking after
occasional lost of the target being tracked. We also find that the tracking
ability, obtained solely from simulators, can potentially transfer to
real-world scenarios. We demonstrate successful examples of such transfer, via
experiments over the VOT dataset and the deployment of a real-world robot using
the proposed active tracker trained in simulation.Comment: To appear in Transactions on Pattern Analysis and Machine
  Intelligence. arXiv admin note: text overlap with arXiv:1705.1056","End-to-end Active Object Tracking and Its Real-world Deployment via
  Reinforcement Learning",http://arxiv.org/abs/1808.03405,,,,core
186301898,2019-02-15T00:00:00,"Robots are increasingly used to carry out critical missions in extreme
environments that are hazardous for humans. This requires a high degree of
operational autonomy under uncertain conditions, and poses new challenges for
assuring the robot's safety and reliability. In this paper, we develop a
framework for probabilistic model checking on a layered Markov model to verify
the safety and reliability requirements of such robots, both at pre-mission
stage and during runtime. Two novel estimators based on conservative Bayesian
inference and imprecise probability model with sets of priors are introduced to
learn the unknown transition parameters from operational data. We demonstrate
our approach using data from a real-world deployment of unmanned underwater
vehicles in extreme environments.Comment: Version accepted at the 33rd AAAI Conference on Artificial
  Intelligence, Honolulu, Hawaii, 201",Probabilistic Model Checking of Robots Deployed in Extreme Environments,http://arxiv.org/abs/1812.04128,,,,core
334848376,2019-08-18T00:00:00,"Scene Classification has been addressed with numerous techniques in computer
vision literature. However, with the increasing number of scene classes in
datasets in the field, it has become difficult to achieve high accuracy in the
context of robotics. In this paper, we implement an approach which combines
traditional deep learning techniques with natural language processing methods
to generate a word embedding based Scene Classification algorithm. We use the
key idea that context (objects in the scene) of an image should be
representative of the scene label meaning a group of objects could assist to
predict the scene class. Objects present in the scene are represented by
vectors and the images are re-classified based on the objects present in the
scene to refine the initial classification by a Convolutional Neural Network
(CNN). In our approach we address indoor Scene Classification task using a
model trained with a reduced pre-processed version of the Places365 dataset and
an empirical analysis is done on a real-world dataset that we built by
capturing image sequences using a GoPro camera. We also report results obtained
on a subset of the Places365 dataset using our approach and additionally show a
deployment of our approach on a robot operating in a real-world environment","Scene Classification in Indoor Environments for Robots using Context
  Based Word Embeddings",http://arxiv.org/abs/1908.06422,,,,core
334837625,2019-07-18T00:00:00,"For a robot to learn a good policy, it often requires expensive equipment
(such as sophisticated sensors) and a prepared training environment conducive
to learning. However, it is seldom possible to perfectly equip robots for
economic reasons, nor to guarantee ideal learning conditions, when deployed in
real-life environments. A solution would be to prepare the robot in the lab
environment, when all necessary material is available to learn a good policy.
After training in the lab, the robot should be able to get by without the
expensive equipment that used to be available to it, and yet still be
guaranteed to perform well on the field. The transition between the lab
(source) and the real-world environment (target) is related to transfer
learning, where the state-space between the source and target tasks differ. We
tackle a simulated task with continuous states and discrete actions presenting
this challenge, using Bootstrapped Dual Policy Iteration, a model-free
actor-critic reinforcement learning algorithm, and Policy Shaping.
Specifically, we train a BDPI agent, embodied by a virtual robot performing a
task in the V-Rep simulator, sensing its environment through several proximity
sensors. The resulting policy is then used by a second agent learning the same
task in the same environment, but with camera images as input. The goal is to
obtain a policy able to perform the task relying on merely camera images",Transfer Learning Across Simulated Robots With Different Sensors,http://arxiv.org/abs/1907.07958,,,,core
196234290,2019-05-01T00:00:00,"abstract: Convolutional neural networks boast a myriad of applications in artificial intelligence, but one of the most common uses for such networks is image extraction. The ability of convolutional layers to extract and combine data features for the purpose of image analysis can be leveraged for pose estimation on an object - detecting the presence and attitude of corners and edges allows a convolutional neural network to identify how an object is positioned. This task can assist in working to grasp an object correctly in robotics applications, or to track an object more accurately in 3D space. However, the effectiveness of pose estimation may change based on properties of the object; the pose of a complex object, complexity being determined by internal occlusions, similar faces, etcetera, can be difficult to resolve.
This thesis is part of a collaboration between ASU’s Interactive Robotics Laboratory and NASA’s Jet Propulsion Laboratory. In this thesis, the training pipeline from Sharma’s paper “Pose Estimation for Non-Cooperative Spacecraft Rendezvous Using Convolutional Neural Networks” was modified to perform pose estimation on a complex object - specifically, a segment of a hollow truss. After initial attempts to replicate the architecture used in the paper and train solely on synthetic images, a combination of synthetic dataset generation and transfer learning on an ImageNet-pretrained AlexNet model was implemented to mitigate the difficulty of gathering large amounts of real-world data. Experimentation with pose estimation accuracy and hyperparameters of the model resulted in gradual test accuracy improvement, and future work is suggested to improve pose estimation for complex objects with some form of rotational symmetry",Pose Estimation with Convolutional Neural Networks,,,,,core
233572274,2019-01-01T00:00:00,"Nano-size unmanned aerial vehicles (UAVs), with few centimeters of diameter and sub-10 Watts of total power budget, have so far been considered incapable of running sophisticated visual-based autonomous navigation software without external aid from base-stations, ad-hoc local positioning infrastructure, and powerful external computation servers. In this work, we present what is, to the best of our knowledge, the first 27g nano-UAV system able to run aboard an end-to-end, closed-loop visual pipeline for autonomous navigation based on a state-of-the-art deep-learning algorithm, built upon the open-source CrazyFlie 2.0 nano-quadrotor. Our visual navigation engine is enabled by the combination of an ultra-low power computing device (the GAP8 system-on-chip) with a novel methodology for the deployment of deep convolutional neural networks (CNNs). We enable onboard real-time execution of a state-of-the-art deep CNN at up to 18Hz. Field experiments demonstrate that the system's high responsiveness prevents collisions with unexpected dynamic obstacles up to a flight speed of 1.5m/s. In addition, we also demonstrate the capability of our visual navigation engine of fully autonomous indoor navigation on a 113m previously unseen path. To share our key findings with the embedded and robotics communities and foster further developments in autonomous nano-UAVs, we publicly release all our code, datasets, and trained networks",An Open Source and Open Hardware Deep Learning-Powered Visual Navigation Engine for Autonomous Nano-UAVs,https://core.ac.uk/download/233572274.pdf,'Institute of Electrical and Electronics Engineers (IEEE)',10.1109/DCOSS.2019.00111,,core
478099534,2019-01-23T14:53:43,"How can neural and morphological computations be effectively combined and realized in embodied closed-loop systems (e.g., robots) such that they can become more like living creatures in their level of performance? Understanding this will lead to new technologies and a variety of applications. To tackle this research question, here, we bring together experts from different fields (including Biology, Computational Neuroscience, Robotics, and Artificial Intelligence) to share their recent findings and ideas and to update our research community. This eBook collects 17 cutting edge research articles, covering neural and morphological computations as well as the transfer of results to real world applications, like prosthesis and orthosis control and neuromorphic hardware implementation",Neural Computation in Embodied Closed-Loop Systems for the Generation of Complex Behavior: From Biology to Technology,,'Frontiers Media SA',10.3389/978-2-88945-605-5,,core
275590394,2019-12-05T00:00:00,"Point cloud acquisition and processing workflows are usually application-dependent following a classic progression from data gathering to deliverable creation. While the collection step may be specific to the sensor at hands, point-cloud-as-a-deliverable upsurges, becoming one de-facto choice for many industries. This task-oriented scenario mainly considers these as a spatial reference – which is used by experts to create other deliverables – thus being a project’s closest link to reality. It brings accurate real-world information which could allow decision-making based on digital-reality instead of interpreted or not up-to-date information. However, there are several considerations to address for a suitable integration. Point clouds are often very large depending on how much data is collected – usually in the realms of Gigabytes, if not Terabytes – and are destined to be archived as a reusable support to create new type of data and products. This can lead to a dead-end with exponential storage needs, incompatibility between outputs, loss of information and complicated collaboration. These practices also show limited to no attempt to generalize a framework which could in turn play as a common ground for further interoperability and generalization. This lack is counterproductive and could lead in term to a chaotic data repartition among actors and worsen the dependency to several outsourced service, each aiming an application independently. 
This primarily emphasize a strong need to study interoperable scenarios in which one point cloud could be used by many users from different domains, each having a different need (E.g. the object of interest can be a building or only the roof of this building). This will in turn introduce new constraints at the acquisition level to define the needed exhaustivity of the 3D representation for use with reasoning engines. Of course, this serialize additional challenges for interconnecting processes and insuring a compatibility with the different sources, volumes and other data-driven parameters. 
Secondly, robotics research has made a leap forward providing autonomous 3D recording systems, where we obtain a 3D point cloud of environments with no human intervention. Of course, following this idea to develop autonomous surveying demands that the data can be used for decision-making. The collected point cloud without context does not permit to take a valid decision, and the knowledge of experts is needed to extract the necessary information and to creates a viable data support for decision-making. Automating this process for fully autonomous cognitive decision systems is very tempting but poses many challenges mainly link to Knowledge Extraction (KE), Knowledge Integration (KI) and Knowledge Representation (KR) from point cloud. Therefore, point cloud structuration must be specifically designed to allow the computer to use it as a base for information extraction using reasoning and agent-based systems. Interoperable approaches which permits several actors to leverage one common information system (E.g. Facility Management 4.0) based on a digital twin is a great exploration motor. In this continuum, the presentation feeds a broader reflexion to go from a human-centered process to an autonomous workflow which highlights a need to improve automation, data management and interaction to speed-up inference processes, crucial to the development of point clouds in 3D capture workflows.
The presentation primarily aims at providing all the necessary information for the development of an infrastructure: The Smart Point Cloud (SPC). It permits to handle point cloud data, manage heterogeneity, process and group points that retain a relationship regarding a specific domain ontology that allow to query and reason for decision-making tools including smart modelling. The resulting implementation of the SPC is based on new meta-models that permit to structure the information (3D geometry and semantics) and leverage available knowledge for accessing decision-making support tools and reasoning capabilities. At the frontier between a point cloud GIS system and a spatial infrastructure for agent-based decision support systems, its flexibility allows to evolve with future developments using artificial intelligence and new machine learning approaches. The proposed modular infrastructure includes Knowledge Discovery processes with Knowledge Integration and Knowledge Representation as ontologies, proving efficient context-specific adaptation.Peer reviewe",The Smart Point Cloud Model: Integration of point intelligence,https://orbi.uliege.be/bitstream/2268/242190/1/20191205_PCP2019_PUBLIC_lowres.pdf,,,,core
323497487,2019-01-01T00:00:00,"© . This manuscript version is made available under the CC-BY-NC-ND 4.0 license http://creativecommons.org/licenses/by-nc-nd/4.0/LEGION (Locally Excitatory, Globally Inhibitory Oscillator Network) topology has demonstrated good capabilities in scene segmentation applications. However, the implementation of LEGION algorithm requires machines with high performance to process a set of complex differential equations limiting its use in practical real-time applications. Recently, several authors have proposed alternative methods based on spiking neural networks (SNN) to create oscillatory neural networks with low computational complexity and highly feasible to be implemented on digital hardware to perform adaptive segmentation of images. Nevertheless, existing SNN with LEGION configuration focus on the membrane model leaving aside the behavior of the synapses although they play an important role in the synchronization of several segments by self-adapting their weights. In this work, we propose a SNN-LEGION configuration along with normalized weight of the synapses to self-adapt the SNN network to synchronize several segments of any size and shape at the same time. The proposed SNN-LEGION method involves a global inhibitor, which is in charge of performing the segmentation process between different objects with different sizes and shapes on time. To validate the proposal, the SNN-LEGION method is implemented on an optimized scalable neuromorphic architecture. Our preliminary results demonstrate that the proposed normalization process of the synaptic weights along with the SNN-LEGION configuration keep the capacity of the LEGION network to separate the segments on time, which can be useful in video processing applications such as vision processing systems for mobile robots, offering lower computational complexity and area consumption compared with previously reported solutions.The authors would like to thank the Consejo Nacional de Ciencia y Tecnologia (CONACyT) and the IPN for the financial support to realize this work under project SIP-20180251. This work was also supported in part by the Spanish Ministry of Science and Innovation and the European Social Fund (ESF) under Projects TEC2011-27047 and TEC2015-67278-R.Peer ReviewedPostprint (author's final draft",LEGION-based image segmentation by means of spiking neural networks using normalized synaptic weights implemented on a compact scalable neuromorphic architecture,,'Elsevier BV',10.1016/j.neucom.2019.04.037,"[{'title': 'Neurocomputing', 'identifiers': ['0925-2312', 'issn:0925-2312']}]",core
334898481,2019-12-30T00:00:00,"In this paper, with a view toward fast deployment of learned locomotion gaits
in low-cost hardware, we generate a library of walking trajectories, namely,
forward trot, backward trot, side-step, and turn in our custom-built quadruped
robot, Stoch 2, using reinforcement learning. There are existing approaches
that determine optimal policies for each time step, whereas we determine an
optimal policy, in the form of end-foot trajectories, for each half walking
step i.e., swing phase and stance phase. The way-points for the foot
trajectories are obtained from a linear policy, i.e., a linear function of the
states of the robot, and cubic splines are used to interpolate between these
points. Augmented Random Search, a model-free and gradient-free learning
algorithm is used to learn the policy in simulation. This learned policy is
then deployed on hardware, yielding a trajectory in every half walking step.
Different locomotion patterns are learned in simulation by enforcing a
preconfigured phase shift between the trajectories of different legs. The
transition from one gait to another is achieved by using a low-pass filter for
the phase, and the sim-to-real transfer is improved by a linear transformation
of the states obtained through regression.Comment: 7 pages, 11 figures, 1 tabl",Gait Library Synthesis for Quadruped Robots via Augmented Random Search,http://arxiv.org/abs/1912.12907,,,,core
237427583,2019-11-20T00:00:00,"The thesis studies building blocks for robot skill learning. Using these key components, learning frameworks can be constructed which provide robots with the capability to acquire a motion and manipulation skill autonomously. We study skill learning in two contexts: in-contact and free-space motions. In brief, this thesis investigates how to: (1) learn a policy for in-contact tasks; (2) generalize a free-space motion policy to new situations using a contextual skill model (CSM); and (3) transfer the CSM from simulation to real world. 

Learning an in-contact task such as wood planing from scratch can be time-consuming and dangerous. This problem can be avoided by imitating a policy from a human demonstration. However, a mere imitation may not satisfy the objective of the corresponding in-contact task. The thesis proposes a reinforcement learning (RL) framework for improving the performance of an imitated in-contact policy. The policy search for in-contact tasks has been achieved by making the motion compliant which allows for exploration in the force profile. 

Generalizing a policy to new situations is fundamental to skill learning as it alleviates the need to learn a new policy in every novel situation. Generalizing a policy refers to synthesizing a function mapping the policy to new situations. The function is referred to as a contextual policy or contextual skill model (CSM). The thesis proposes a parametric CSM. Experiments demonstrated that the parametric CSM can extract a global pattern from a database (DB) of policy parameters leading to significantly better extrapolation capability than with non-parametric CSMs. Furthermore, the underlying model of the CSM is fitted to the DB using a novel model selection approach to better represent the underlying regularities of the task. In order to speed the process of learning, the prediction uncertainty of the CSM is calculated using empirical Bayes (EB) and employed for guiding the exploration process of a model-free policy search. In addition, the most promising task is selected using a novel task manager, allowing for better future generalization performance achieved with minimum effort. In essence, the thesis presents an incremental learning framework,the main components of which are as follows: CSM, policy search, model selection, DB, EB, and a task manager implemented using active learning.

Learning a policy in a simulated environment and transferring it to the real world will alleviate the need to learn from scratch or from a demonstration. The thesis proposes to transfer a CSM instead of transferring a single control policy. We developed a simulation-to-real transfer framework which learns a source CSM in simulation incrementally and transfers it to the real world incrementally. Transference of the source CSM has been achieved using sample policies from the target environment. Experiments indicated that one sample policy is sufficient to transfer a CSM to the target environment. The target CSM improved the extrapolation capability significantly better than zero-shot transfer",Incremental and Transfer Learning of Contextual Skill Model for Robots,,'Institute of Electrical and Electronics Engineers (IEEE)',10.1109/HUMANOIDS.2016.7803277,,core
201299050,2019-02-01T00:00:00Z,"Abstract Robotic weed control has seen increased research of late with its potential for boosting productivity in agriculture. Majority of works focus on developing robotics for croplands, ignoring the weed management problems facing rangeland stock farmers. Perhaps the greatest obstacle to widespread uptake of robotic weed control is the robust classification of weed species in their natural environment. The unparalleled successes of deep learning make it an ideal candidate for recognising various weed species in the complex rangeland environment. This work contributes the first large, public, multiclass image dataset of weed species from the Australian rangelands; allowing for the development of robust classification methods to make robotic weed control viable. The DeepWeeds dataset consists of 17,509 labelled images of eight nationally significant weed species native to eight locations across northern Australia. This paper presents a baseline for classification performance on the dataset using the benchmark deep learning models, Inception-v3 and ResNet-50. These models achieved an average classification accuracy of 95.1% and 95.7%, respectively. We also demonstrate real time performance of the ResNet-50 architecture, with an average inference time of 53.4 ms per image. These strong results bode well for future field implementation of robotic weed control methods in the Australian rangelands",DeepWeeds: A Multiclass Weed Species Image Dataset for Deep Learning,,Nature Publishing Group,10.1038/s41598-018-38343-3,"[{'title': None, 'identifiers': ['2045-2322', 'issn:2045-2322']}]",core
229331689,2019-01-01T08:00:00,"A study is presented on intelligent robotic navigation through simultaneous localization and mapping (SLAM) enhanced with convolutional neural networks (CNNs). The study included re-training a pre-trained CNN network for object detection, recognition and depth estimation in a laboratory setting, implementing a feature-based monocular SLAM algorithm (ORB-SLAM) within robot operating system (ROS) framework and integrating both the re-trained CNN and ORB-SLAM to intelligently guide a robot during navigation to reach target objects while avoiding obstacles. The visual SLAM (ORB-SLAM) enhanced with CNN for object detection, recognition and depth estimation was adapted and implemented in real-time. A Kobuki Turtlebot with Xbox 360 camera along with its on-board laptop (CPU based) was selected as the mobile robotic platform for the implementation within ROS framework. The proposed system successfully combined the capabilities of ORB-SLAM with CNN for real-time autonomous navigation of the robot in an enclosed environment. The power of edge computing with graphics processing unit (GPU) based hardware platform Jetson TX2 along with open-source software library TensorFlow suitable for implementation of deep learning architectures including CNN were utilized within ROS for real-time operation. The effectiveness of the system is illustrated through case studies that required the robot to avoid obstacles while locating designated objects within the map including a maze in a laboratory setting. In each case, the robot was able to plan a path towards the target objects using the map it saved from the ORB-SLAM-CNN implementation. The map was continuously updated accommodating changes in the environment while the robot was navigating towards the target objects. Details of algorithms developed, hardware and software support, real-time implementation, and results of different case studies are presented along with recommendations for future work",Intelligent Robotic Navigation Through Simultaneous Localization and Mapping with Convolutional Neural Networks,,Digital Commons@Georgia Southern,,,core
343443508,2019-08-15T00:00:00,"Search and Rescue (SAR) missions represent an important challenge in the robotics research field as they usually involve exceedingly variable-nature scenarios which require a high-level of autonomy and versatile decision-making capabilities. This challenge becomes even more relevant in the case of aerial robotic platforms owing to their limited payload and computational capabilities. In this paper, we present a fully-autonomous aerial robotic solution, for executing complex SAR missions in unstructured indoor environments. The proposed system is based on the combination of a complete hardware configuration and a flexible system architecture which allows the execution of high-level missions in a fully unsupervised manner (i.e. without human intervention). In order to obtain flexible and versatile behaviors from the proposed aerial robot, several learning-based capabilities have been integrated for target recognition and interaction. The target recognition capability includes a supervised learning classifier based on a computationally-efficient Convolutional Neural Network (CNN) model trained for target/background classification, while the capability to interact with the target for rescue operations introduces a novel Image-Based Visual Servoing (IBVS) algorithm which integrates a recent deep reinforcement learning method named Deep Deterministic Policy Gradients (DDPG). In order to train the aerial robot for performing IBVS tasks, a reinforcement learning framework has been developed, which integrates a deep reinforcement learning agent (e.g. DDPG) with a Gazebo-based simulator for aerial robotics. The proposed system has been validated in a wide range of simulation flights, using Gazebo and PX4 Software-In-The-Loop, and real flights in cluttered indoor environments, demonstrating the versatility of the proposed system in complex SAR missions.This work was supported by the Spanish Ministry of Science (Project DPI2014-60139-R).Peer reviewe",A Fully-Autonomous Aerial Robot for Search and Rescue Applications in Indoor Environments using Learning-Based Techniques,,'Springer Science and Business Media LLC',10.1007/s10846-018-0898-1,"[{'title': 'Journal of Intelligent & Robotic Systems', 'identifiers': ['issn:0921-0296', '0921-0296', 'issn:1573-0409', '1573-0409']}]",core
289162380,2019-01-01T00:00:00,"Emergency personnel, such as firefighters, bomb technicians, and urban search and rescue specialists, can be exposed to a variety of extreme hazards during the response to natural and human-made disasters. In many of these scenarios, a risk factor is the presence of hazardous airborne chemicals. The recent and rapid advances in robotics and sensor technologies allow emergency responders to deal with such hazards from relatively safe distances. Mobile robots with gas-sensing capabilities allow to convey useful information such as the possible source positions of different chemicals in the emergency area. However, common gas sampling procedures for laboratory use are not applicable due to the complexity of the environment and the need for fast deployment and analysis. In addition, conventional gas identification approaches, based on supervised learning, cannot handle situations when the number and identities of the present chemicals are unknown. For the purpose of emergency response, all the information concluded from the gas detection events during the robot exploration should be delivered in real time. To address these challenges, we developed an online gas-sensing system using an electronic nose. Our system can automatically perform unsupervised learning and update the discrimination model as the robot is exploring a given environment. The online gas discrimination results are further integrated with geometrical information to derive a multi-compound gas spatial distribution map. The proposed system is deployed on a robot built to operate in harsh environments for supporting fire brigades, and is validated in several different real-world experiments of discriminating and mapping multiple chemical compounds in an indoor open environment. Our results show that the proposed system achieves high accuracy in gas discrimination in an online, unsupervised, and computationally efficient manner. The subsequently created gas distribution maps accurately indicate the presence of different chemicals in the environment, which is of practical significance for emergency response.Funding Agency:European Commission  645101</p",Towards Gas Discrimination and Mapping in Emergency Response Scenarios Using a Mobile Robot with an Electronic Nose,,'MDPI AG',10.3390/s19030685,,core
289956463,2019,"This thesis describes the development of an autonomous weeding vehicle for the agricultural industry. It has been done as a project at RMIT University in Melbourne and the aim was to develop a design for the vehicle which could later be used to develop a commercial product. The current trend in the agricultural industry is larger machines that can benefit from economies of scale. These heavy machines are causing three major problems for farmers; serious subsoil compaction, longer disruptions due to single vehicle failure and an inability to efficiently deal with weeds which have resulted in a rapid increase of herbicide resistant weeds. This project is an attempt at solving these problems by integrating the already existing stationary farming robot FarmBot with a lightweight, cost efficient, aesthetically pleasing and durable frame on wheels that can operate autonomously on farms and is being powered by solar energy. The project started by researching the agricultural industry and benchmarking similar products, which gave an understanding of the problem and how it could best be solved. Since many different parts had to be developed, it was decided that the complex problem was to be divided into several sub problems which were then solved parallel with each other. The different concepts were evaluated and a final product was developed visually using CAD. The finished vehicle was primarily made out of parts that can be ordered off the shelves as well as parts that can be 3D printed. It weighs 154 kg, the parts costs around USD 5300 and it is equipped to be able to operate autonomously for at least a day at a time. It is very easy to assemble and has a modular design, which simplifies further development.Smarter, more advanced farming methods are needed to stop the rapid increase of herbicide resistant weeds and overuse of herbicides. Autonomous farming robots are the future of farming, and by using machine learning and advanced image analysis a robot is being developed that can target specific weeds and exterminate them in the most optimal way, reducing herbicide usage by up to 90%. The current farming methods are in a dire need of a revolution. Due to broadacre spraying weeds are starting to become resistant to herbicides at an alarming rate, the run-off from herbicide usage has severe consequences for the environment and heavy machinery is reducing the crop yield for farmers due to soil compaction. In Melbourne, Australia a small team is working on disrupting the industry by creating a lightweight autonomous farming robot that can target the weeds and exterminate them by using the optimal amount and type of herbicide. The foundation for this robot is the open source CNC farming robot FarmBot that can take care of a garden plot without human interaction. By using the intuitive web-based interface people can set up their plot and control the robot through most devices. It can plant seeds, measure the soil moisture level, exterminate weeds, water the plants and even notify you when they’re ripe for picking. It navigates by using a coordinate system, which means it can perform its tasks with millimeter accuracy. The FarmBot is a stationary robot, but if you strip it of non-essential parts and functions you have an inexpensive and lightweight robot that which can be mounted on a frame capable of autonomously navigating a farm. This is exactly what was envisioned by Hormoz Marzbani, lecturer at Royal Melbourne Institute of Technology when he first set eyes on the FarmBot. By utilizing an already existing product the project got a jumpstart, and a first prototype was quickly built to be able to test out the steering and image analysis. When it was decided that the prototype worked well, it was time for the next phase of the project, the design phase. Master students Lisa Ralsgård and Simon Axbom, Lund University, were then put in charge of developing a complete design for the robot in a CAD software. Numerous autonomous weeding robots have been developed, but they all have one thing in common, they are all very expensive. The goal of this project was to exploit that business opportunity so it was important that the robot was inexpensive to manufacture. During a 20-week period concepts were continuously generated and evaluated to be able to design a robot that would meet all the project requirements as well as the requirements from the farmers. In the end they managed to come up with a solar powered design that enabled the robot to operate autonomously without needing to be refill herbicide for more than 30 hours. With a slimmed down design it weighs only 154 kg and the total cost for parts is less than $5500. By only using off the shelf parts, except for the cover, all the parts could essentially be ordered online and assembled by anyone. The next phase of the project is to order all the parts and assemble them into a working prototype. The design has been tested theoretically, but it’s time to see how well it performs in the real, harsh agricultural environment. If the next prototype phase goes smoothly, a new modern way of managing weeds is close on the horizon",Design of an Autonomous Weeding Vehicle,,Lunds universitet/Produktutveckling,,,core
224400295,2019-01-01T00:00:00,"In an era where humans are increasingly being replaced or augmented by technological innovation, how might the humanist tradition offer us guiding questions for ethics of war today? My talk will explore the U.S. discourse of how improved battlefield technology is believed to make Western war an inherently more ethical space that eases the liberal conscience in killing. Drawing on the logics and practices of U.S. war making, my talk will address three phases of the transition from an ethics of practical judgment and due care to a computational techno-ethics of war. First, it traces the rise of smart bombs alongside collateral damage estimation software. Second, I examine the machine-learning processes that constructs ‘legitimate targets’ in US drone strikes via heterogeneous correlations of SIM card metadata. Third, I survey the consequences of a quantified global battlefield and the improbability of ‘meaningful human control’ over artificially intelligent ‘killer robots’. War by algorithm ultimately removes us from the act of killing while proffering a more ethical ‘science of warfare’. These practices enable decision-makers to tick the ethical box of due care with technology that is believed to be objective and neutral, yet in reality, has simply buried bias deep within the algorithmic code. Not only do these technologies of war and big data shape our capacity to think ethically, but fundamentally call us to reassess how complex ethico-political dilemmas of war could be replaced by computation. What is at stake is the erosion of effective constraints on the use of lethal force because this techno-rationalization of a quantified risk assessment has supplanted ethical decision-making, the site of the body, and emotions in contemporary conflict. Ultimately, I will argue that the science of humanity of Giambattista Vico, allows us to rethink algorithmic epistemologies of war in novel ways that bring the human back to the forefront of ethical decision-making in the 21st Century",War by Algorithm: Giambattista Vico and Ethics of War in the Techno-Logical Era,https://core.ac.uk/download/224400295.pdf,"eScholarship, University of California",,,core
201463447,2019-01-01T00:00:00,"Robot navigation is a fundamental problem in robotics and various approaches have been developed to cope with this problem. Despite the great success of previous approaches, learning-based methods are receiving growing interest in the research community. They have shown great efficiency in solving navigation tasks and offer considerable promise to build intelligent navigation systems. This paper presents a goal-directed robot navigation system that integrates global planning based on goal-directed end-to-end learning and local planning based on reinforcement learning (RL). The proposed system aims to navigate the robot to desired goal positions while also being adaptive to changes in the environment. The global planner is trained to imitate an expert&rsquo;s navigation between different positions by goal-directed end-to-end learning, where both the goal representations and local observations are incorporated to generate actions. However, it is trained in a supervised fashion and is weak in dealing with changes in the environment. To solve this problem, a local planner based on deep reinforcement learning (DRL) is designed. The local planner is first implemented in a simulator and then transferred to the real world. It works complementarily to deal with situations that have not been met during training the global planner and is able to generalize over different situations. The experimental results on a robot platform demonstrate the effectiveness of the proposed navigation system",Towards Goal-Directed Navigation Through Combining Learning Based Global and Local Planners,,'MDPI AG',10.3390/s19010176,"[{'title': 'Sensors', 'identifiers': ['issn:1424-8220', '1424-8220']}]",core
287205891,2019-01-01T08:00:00,"A study is presented on visual navigation of wheeled mobile robots (WMR) using deep reinforcement learning in unknown and dynamic environments. Two versions of deep reinforcement learning (DRL) algorithms, namely, value-learning based deep Q-network (DQN) and policy gradient based asynchronous advantage actor critic (A3C) have been considered in this study. Both DRL algorithms have been implemented using RGB and depth images as inputs to generate outputs for the WMR for autonomous navigation in both simulation and real-time. The initial DRL networks were generated and trained progressively in simulation environments using OpenAI Gym Gazebo within robot operating system (ROS) framework for a popular experimental WMR, namely, Kobuki TurtleBot2 with Asus Xtion depth camera. The real-time implementation of the trained DRL networks in ROS framework was achieved using onboard edge computing hardware platform of NVIDIA Jetson TX2 through software framework of TensorFlow. For object detection, classification, and target identification, a pre-trained deep neural network, namely, ResNet50 was used after further training with reduced classification categories for target-driven visual mapless navigation of Turlebot2 through DRL. The simulation based training of DQN and A3C networks was successfully transferred with online learning in real-time navigation of Turlebot2 in physical environments. The performance of A3C was simulated with multiple computation threads (4, 6, and 8) on a desktop. The simulated navigation performance, in terms of the minimum, the average, and the maximum rewards, and the completion tine was compared for DQN and A3C networks for three simulation environments. The performance of A3C with multiple threads (4, 6, and 8) was better than DQN, as expected. The performance of A3C also improved with the number of threads. The real-time implementation results of A3C with 8 threads in unknown and dynamic environments with target objects were promising. Details of the methodology, simulation and real-time implementation results are presented and recommendations for future work are outlined",Visual Navigation of Wheeled Mobile Robots Using Deep Reinforcement Learning,,Digital Commons@Georgia Southern,,,core
146473775,2019-08-13T00:00:00,"Everyday robotics are challenged to deal with autonomous product handling in
applications like logistics or retail, possibly causing damage on the items
during manipulation. Traditionally, most approaches try to minimize physical
interaction with goods. However, this paper proposes to take into account any
unintended object motion and to learn damage-minimizing manipulation strategies
in a self-supervised way. The presented approach consists of a simulation-based
planning method for an optimal manipulation sequence with respect to possible
damage. The planned manipulation sequences are generalized to new, unseen
scenes in the same application scenario using machine learning. This learned
manipulation strategy is continuously refined in a self-supervised,
simulation-in-the-loop optimization cycle during load-free times of the system,
commonly known as mental simulation. In parallel, the generated manipulation
strategies can be deployed in near-real time in an anytime fashion. The
approach is validated on an industrial container-unloading scenario and on a
retail shelf-replenishment scenario","Self-Supervised Damage-Avoiding Manipulation Strategy Optimization via
  Mental Simulation",http://arxiv.org/abs/1712.07452,'Springer Science and Business Media LLC',10.1007/s11370-019-00286-7,,core
334906654,2019-10-22T00:00:00,"Innovations in batteries take years to formulate and commercialize, requiring
extensive experimentation during the design and optimization phases. We
approached the design and selection of a battery electrolyte through a
black-box optimization algorithm directly integrated into a robotic test-stand.
We report here the discovery of a novel battery electrolyte by this experiment
completely guided by the machine-learning software without human intervention.
Motivated by the recent trend toward super-concentrated aqueous electrolytes
for high-performance batteries, we utilize Dragonfly - a Bayesian
machine-learning software package - to search mixtures of commonly used lithium
and sodium salts for super-concentrated aqueous electrolytes with wide
electrochemical stability windows. Dragonfly autonomously managed the robotic
test-stand, recommending electrolyte designs to test and receiving experimental
feedback in real time. In 40 hours of continuous experimentation over a
four-dimensional design space with millions of potential candidates, Dragonfly
discovered a novel, mixed-anion aqueous sodium electrolyte with a wider
electrochemical stability window than state-of-the-art sodium electrolyte. A
human-guided design process may have missed this optimal electrolyte. This
result demonstrates the possibility of integrating robotics with
machine-learning to rapidly and autonomously discover novel battery materials.Comment: 23 pages, 4 figures, 10 pages of Extended Dat","Autonomous discovery of battery electrolytes with robotic
  experimentation and machine-learning",http://arxiv.org/abs/2001.09938,,,,core
328783920,2019-10-08T16:16:31,"Humans are able to seamlessly integrate tactile and visual stimuli with their intuitions to explore and execute complex manipulation skills. They not only see but also feel their actions. Most current robotic learning methodologies exploit recent progress in computer vision and deep learning to acquire data-hungry pixel-to-action policies. These methodologies do not exploit intuitive latent structure in physics or tactile signatures. Tactile reasoning is omnipresent in the animal kingdom, yet it is underdeveloped in robotic manipulation. Tactile stimuli are only acquired through invasive interaction, and interpretation of the data stream together with visual stimuli is challenging. Here, we propose a methodology to emulate hierarchical reasoning and multisensory fusion in a robot that learns to play Jenga, a complex game that requires physical interaction to be played effectively. The game mechanics were formulated as a generative process using a temporal hierarchical Bayesian model, with representations for both behavioral archetypes and noisy block states. This model captured descriptive latent structures, and the robot learned probabilistic models of these relationships in force and visual domains through a short exploration phase. Once learned, the robot used this representation to infer block behavior patterns and states as it played the game. Using its inferred beliefs, the robot adjusted its behavior with respect to both its current actions and its game strategy, similar to the way humans play the game. We evaluated the performance of the approach against three standard baselines and show its fidelity on a real-world implementation of the game","See, feel, act: hierarchical learning for complex manipulation skills with multisensory fusion",,'American Association for the Advancement of Science (AAAS)',10.1126/scirobotics.aav3123,"[{'title': 'Science Robotics', 'identifiers': ['2470-9476', 'issn:2470-9476']}]",core
266990259,2019-01-01T00:00:00,"In the article, the author analyzes the problems of human-computer communication in the context of artificial intelligence, augmented reality and a Turing methodology for comparing the capabilities of artificial and natural intelligence in a dialogue. It is argued that the tool with which the computer and humans communicate is of no less importance than the computer program with which the dialogue is conducted. As an example of the implementation of such visualization, the project “E.LENA” of a digital television anchor created at the Sberbank Robotics Laboratory was considered. The author gives his own version of Turing's specific test to study the features of non- verbal communication interfaces between a person and a computer. The key idea formulated by the author for further discussion is to provide artificial intelligence with visual avatar in order to avoid the alienation of man and computer and the loss of semantic information during its transmission from computer to person",Технологические предпосылки неразличимости человека и его компьютерной имитации,https://core.ac.uk/download/266990259.pdf,,,,core
300005449,2019-01-01T00:00:00,"Object recognition and 6D pose estimation are imperative for robots to relate to the real world. However, due to occlusion, clutter and the properties of various objects in a scene, it might be challenging and tedious for a robot to recognize and estimate the 6D pose of objects. Various methods have been presented throughout the years with concern to this topic. However, many of these methods have its set back and limitations. Due to these reasons, rose the motivation to develop a robust and versatile real time system capable of accurate object recognition and 6D pose estimation with respect to the industrial standards. Over the years, with the advancement in technology, computing power have improved drastically. Algorithms, techniques and methods that were once infeasible to implement due to high computational power requirements could now be done with ease. One such implementation is none other than deep learning. It is now the current state-of-the-art technology. Since, this project is in relation with computer vision, the deep learning architecture proposed would be a convolutional neural network. Hence, the dataset used would consist of images. A deep learning framework known as PoseCNN is explored for object recognition and 6D pose estimation capabilities. In this project, a detailed literature review of PoseCNN, as well as comparisons with current approaches, will be reviewed. Finally, the results obtained using the YCB dataset would be presented.Bachelor of Engineering (Electrical and Electronic Engineering",Object recognition and 6D pose estimation using deep learning,,,,,core
301271596,2019-01-01T00:00:00,"The book covers a variety of topics in Information and Communications Technology (ICT) and their impact on innovation and business. The authors discuss various innovations, business and industrial motivations, and impact on humans and the interplay between those factors in terms of finance, demand, and competition. Topics discussed include the convergence of Machine to Machine (M2M), Internet of Things (IoT), Social, and Big Data. They also discuss AI and its integration into technologies from machine learning, predictive analytics, security software, to intelligent agents, and many more. Contributions come from academics and professionals around the world.



Covers the most recent practices in ICT related topics pertaining to technological growth, innovation, and business; Presents a survey on the most recent technological areas revolutionizing how humans communicate and interact; Features four sections: IoT, Wireless Ad Hoc & Sensor Networks, Fog Computing, and Big Data Analytics.(Chapter) The recent advancements in robotic systems set new challenges for robotic simulation software, particularly for planning. It requires the realistic behavior of the robots and the objects in the simulation environment by incorporating their dynamics. Furthermore, it requires the capability of reasoning about the action effects. To cope with these challenges, this study proposes an open-source simulation tool for knowledge-oriented physics-based motion planning by extending The Kautham Project, a C++ based open-source simulation tool for motion planning. The proposed simulation tool provides a flexible way to incorporate the physics, knowledge and reasoning in planning process. Moreover, it provides ROS-based interface to handle the manipulation actions (such as push/pull) and an easy way to communicate with the real robotsPeer Reviewe",A tool for knowledge-oriented physics-based motion planning and simulation,,'Springer Science and Business Media LLC',10.1007/978-3-319-99966-1,,core
334844348,2019-08-05T00:00:00,"In this paper, we propose Augmented Reality Semi-automatic labeling (ARS), a
semi-automatic method which leverages on moving a 2D camera by means of a
robot, proving precise camera tracking, and an augmented reality pen to define
initial object bounding box, to create large labeled datasets with minimal
human intervention. By removing the burden of generating annotated data from
humans, we make the Deep Learning technique applied to computer vision, that
typically requires very large datasets, truly automated and reliable. With the
ARS pipeline, we created effortlessly two novel datasets, one on
electromechanical components (industrial scenario) and one on fruits
(daily-living scenario), and trained robustly two state-of-the-art object
detectors, based on convolutional neural networks, such as YOLO and SSD. With
respect to the conventional manual annotation of 1000 frames that takes us
slightly more than 10 hours, the proposed approach based on ARS allows
annotating 9 sequences of about 35000 frames in less than one hour, with a gain
factor of about 450. Moreover, both the precision and recall of object
detection is increased by about 15\% with respect to manual labeling. All our
software is available as a ROS package in a public repository alongside the
novel annotated datasets",Semi-Automatic Labeling for Deep Learning in Robotics,http://arxiv.org/abs/1908.01862,,,,core
334898971,2019-12-31T00:00:00,"This paper illustrates the MIR (Mobile Intelligent Robotics) Vehicle: a
feasible option of transforming an electric ride-on-car into a modular Graphics
Processing Unit (GPU) powered autonomous platform equipped with the capability
that supports test and deployment of various intelligent autonomous vehicles
algorithms. To use a platform for research, two components must be provided:
perception and control. The sensors such as incremental encoders, an Inertial
Measurement Unit (IMU), a camera, and a LIght Detection And Ranging (LIDAR)
must be able to be installed on the platform to add the capability of
environmental perception. A microcontroller-powered control box is designed to
properly respond to the environmental changes by regulating drive and steering
motors. This drive-by-wire capability is controlled by a GPU powered laptop
computer where high-level perception algorithms are processed and complex
actions are generated by various methods including behavior cloning using deep
neural networks. The main goal of this paper is to provide an adequate and
comprehensive approach for fabricating a cost-effective platform that would
contribute to the research quality from the wider community. The proposed
platform is to use a modular and hierarchical software architecture where the
lower and simpler motor controls are taken care of by microcontroller programs,
and the higher and complex algorithms are processed by a GPU powered laptop
computer. The platform uses the Robot Operating System (ROS) as middleware to
maintain the modularity of the perceptions and decision-making modules. It is
expected that the level three and above autonomous vehicle systems and Advanced
Driver Assistance Systems (ADAS) can be tested on and deployed to the platform
with a decent real-time system behavior due to the capabilities and
affordability of the proposed platform.Comment: 20 pages, 16 figure","MIR-Vehicle: Cost-Effective Research Platform for Autonomous Vehicle
  Applications",http://arxiv.org/abs/2001.00048,,,,core
351965595,2019-01-01T00:00:00,"The main goal of this work is learning a local path planning policy for mobile robots from a single depth camera input. We formulate the end-to-end local planning problem as a Partially Observable Markov Decision Process and solve it using a Deep Reinforcement Learning algorithm. The main challenges of this setting comes from 1) the short-sightedness of reaction-based planners, and 2) the limited field-of-view of depth camera that significantly degrades the planner’s performance. We resolve these problems by memory-based Deep Reinforcement Learning. This framework represents a policy as a network with a memory unit that can remember past observations. As a result, the trained policy can generate collision-safe trajectories based on not only a current observation but also previous observations. We also address sample ineciency of end-to-end learning by 1) a two-stream feature extraction with pre-trained autoencoder and 2) Asymmetric Actor-Critic method. These methods were demonstrated to be effective for fast convergence by our ablation study results. Finally we bridge the reality gap between real depth image and simulated depth image by real-time depth completion algorithm and pre-training autoencoder with both real images and simulate images. In the quantitative evaluation, our policy with memory units outperforms standard CNN policy. Notably, the policy with Temporal Convolutional layers learned much faster than the policy with conventional LSTM. In the following real robot experiments, we deployed the trained policy to the quadrupedal robot ANYmal with Intel RealSense depth camera. Our policy generated collision-safe paths reactively in both stationary and dynamic environments",End-to-End Collision Avoidance from Depth Input with Memory-based Deep  Reinforcement Learning,,ETH Zurich,10.3929/ethz-b-000444961,,core
334604620,2019-12-01T00:00:00,"Current end-to-end deep Reinforcement Learning (RL) approaches require jointly learning perception, decision-making and low-level control from very sparse reward signals and high-dimensional inputs, with little capability of incorporating prior knowledge. This results in prohibitively long training times for use on real-world robotic tasks. Existing algorithms capable of extracting task-level representations from high-dimensional inputs, e.g. object detection, often produce outputs of varying lengths, restricting their use in RL methods due to the need for neural networks to have fixed length inputs. In this work, we propose a framework that combines deep sets encoding, which allows for variable-length abstract representations, with modular RL that utilizes these representations, decoupling high-level decision making from low-level control. We successfully demonstrate our approach on the robot manipulation task of object sorting, showing that this method can learn effective policies within mere minutes of highly simplified simulation. The learned policies can be directly deployed on a robot without further training, and generalize to variations of the task unseen during training.</p",Sim-to-real transfer of robot learning with variable length inputs,,Australian Robotics and Automation Association (ARAA),,,core
333563431,2020,"The pervasive use of artificial intelligence and neural networks in several different research fields has noticeably improved multiple aspects of human life. The application of these techniques to machines has made them progressively more “intelligent” and able to solve tasks considered extremely complex for a human being. This technological evolution has deeply influenced the way we interact with machines. Purely symbolic artificial intelligence and techniques like ontologies, have also been successfully used in the past applied to robotics, but have also shown some limitations and failings in the knowledge construction task. In fact, the exhibited “intelligence” is rarely the result of a real autonomous decision, but it is rather hard-encoded in the machine. While a number of approaches have already been proposed in literature concerning knowledge acquisition from the surrounding environment, they are either exclusively based on low-level features or they involve solely high-level semantics-based attributes. Moreover, they often don’t use a general high-level knowledge base for grounding the acquired knowledge. In this contexts, the use of semantics technologies, such as ontologies, is mostly employed for action-oriented tasks. In this article we propose an extension of a novel approach for knowledge acquisition based on a general semantic knowledge-base and the fusion of semantics and visual information by means of neural networks and ontologies. The proposed approach has been implemented on a humanoid robotic platform and the experimental results are shown and discussed",Knowledge Acquisition and Design Using Semantics and Perception: A Case Study for Autonomous Robots,,'Springer Science and Business Media LLC',10.1007/s11063-020-10311-x,,core
387273517,2020-11-04T00:00:00,"Multi-agent path finding in formation has many potential real-world
applications like mobile warehouse robots. However, previous multi-agent path
finding (MAPF) methods hardly take formation into consideration. Furthermore,
they are usually centralized planners and require the whole state of the
environment. Other decentralized partially observable approaches to MAPF are
reinforcement learning (RL) methods. However, these RL methods encounter
difficulties when learning path finding and formation problem at the same time.
In this paper, we propose a novel decentralized partially observable RL
algorithm that uses a hierarchical structure to decompose the multi objective
task into unrelated ones. It also calculates a theoretical weight that makes
every task reward has equal influence on the final RL value function.
Additionally, we introduce a communication method that helps agents cooperate
with each other. Experiments in simulation show that our method outperforms
other end-to-end RL methods and our method can naturally scale to large world
sizes where centralized planner struggles. We also deploy and validate our
method in a real world scenario","Moving Forward in Formation: A Decentralized Hierarchical Learning
  Approach to Multi-Agent Moving Together",http://arxiv.org/abs/2011.02373,,,,core
386352618,2020-01-01T08:00:00,"Bots are software applications that complete tasks automatically. A bot\u27s communication is disembodied, so humans can mistake it for a real person, and their misbelief can be exploited by the bot owner to deploy malware or phish personal data. Bots also pose as consumers posting online product reviews or spread (often fake) news, and a bot owner can coordinate multiple social-network accounts to trick a network\u27s  trending  algorithms, boosting the visibility of specific content, sowing and exacerbating controversy, or fabricating an impression of mass individual consensus. California\u27s 2019 Bolstering Online Transparency Act (the  CA Bot Act\u27) imposes conspicuous disclosure requirements on bots when they communicate or interact with humans in California. Call it Isaac Asimov\u27s fourth Rule of Robotics: A robot may not pretend to be a human being. By requiring bots to  self-identify  as such, the CA Bot Act is a pioneer in laws regulating artificial intelligence. Most of its criticism points to the act\u27s lack of an enforcement mechanism to incentivize compliance. Accordingly, this Article lays out a map to sanction violations of the act with civil actions under California\u27s Unfair Competition Law and statutory tort law of fraudulent deceit. It outlines what is prohibited, who can be sued, and who has standing to sue, then addresses First Amendment limits on unmasking John Doe defendants via subpoena. For many reasons, attempts to hold CA Bot Act violators liable are most likely to prevail in the commercial arena. But a willful use of bots to undermine a political election or prevent voting might also be a worthy target. Ultimately, the law could be strengthened with an articulated enforcement provision. But if the CA Bot Act aims a first salvo against malicious online bots, this Article hopes to spark the powder",People v. Robots: A Roadmap for Enforcing California\u27s New Online Bot Disclosure Act,,Scholarship@Vanderbilt Law,,,core
333913632,2020-09-15T00:00:00,"International audienceThe main concepts and techniques of multi-agent oriented programming, which supports the multi-agent systems paradigm at the programming level.A multi-agent system is an organized ensemble of autonomous, intelligent, goal-oriented entities called agents, communicating with each other and interacting within an environment. This book introduces the main concepts and techniques of multi-agent oriented programming, (MAOP) which supports the multi-agent systems paradigm at the programming level. MAOP provides a structured approach based on three integrated dimensions, which the book examines in detail: the agent dimension, used to design the individual (interacting) entities; the environment dimension, which allows the development of shared resources and connections to the real world; and the organization dimension, which structures the interactions among the autonomous agents and the shared environment.The book puts the approach into practice using the JaCaMo programming model and platform. It employs an easy-to-follow, step-by-step style, showing solutions to increasingly complex scenarios. The book also discusses the integration of MAOP into existing technologies and application domains, including mobile computing, web-based computing, and robotics. Finally, it considers artificial intelligence (AI)–related classical problems from an MAOP perspective and discusses an agent-oriented approach to software engineering",Multi-Agent Oriented Programming: Programming Multi-Agent Systems Using JaCaMo,,The MIT Press,,,core
327123119,2020-07-30T00:00:00,"Object navigation is defined as navigating to an object of a given label in a
complex, unexplored environment. In its general form, this problem poses
several challenges for Robotics: semantic exploration of unknown environments
in search of an object and low-level control. In this work we study
object-guided exploration and low-level control, and present an end-to-end
trained navigation policy achieving a success rate of 0.68 and SPL of 0.58 on
unseen, visually complex scans of real homes. We propose a highly scalable
implementation of an off-policy Reinforcement Learning algorithm, distributed
Soft Actor Critic, which allows the system to utilize 98M experience steps in
24 hours on 8 GPUs. Our system learns to control a differential drive mobile
base in simulation from a stack of high dimensional observations commonly used
on robotic platforms. The learned policy is capable of object-guided
exploratory behaviors and low-level control learned from pure experiences in
realistic environments","Learning Object-conditioned Exploration using Distributed Soft Actor
  Critic",http://arxiv.org/abs/2007.14545,,,,core
327254068,2020-07-31T00:00:00,"Energy-efficient mapless navigation is crucial for mobile robots as they
explore unknown environments with limited on-board resources. Although the
recent deep reinforcement learning (DRL) approaches have been successfully
applied to navigation, their high energy consumption limits their use in
several robotic applications. Here, we propose a neuromorphic approach that
combines the energy-efficiency of spiking neural networks with the optimality
of DRL and benchmark it in learning control policies for mapless navigation.
Our hybrid framework, spiking deep deterministic policy gradient (SDDPG),
consists of a spiking actor network (SAN) and a deep critic network, where the
two networks were trained jointly using gradient descent. The co-learning
enabled synergistic information exchange between the two networks, allowing
them to overcome each other's limitations through a shared representation
learning. To evaluate our approach, we deployed the trained SAN on Intel's
Loihi neuromorphic processor. When validated on simulated and real-world
complex environments, our method on Loihi consumed 75 times less energy per
inference as compared to DDPG on Jetson TX2, and also exhibited a higher rate
of successful navigation to the goal, which ranged from 1% to 4.2% and depended
on the forward-propagation timestep size. These results reinforce our ongoing
efforts to design brain-inspired algorithms for controlling autonomous robots
with neuromorphic hardware.Comment: 8 pages, 7 figure","Reinforcement co-Learning of Deep and Spiking Neural Networks for
  Energy-Efficient Mapless Navigation with Neuromorphic Hardware",http://arxiv.org/abs/2003.01157,,,,core
323308900,2020-06-03T00:00:00,"Deep reinforcement learning has great potential to acquire complex, adaptive
behaviors for autonomous agents automatically. However, the underlying neural
network polices have not been widely deployed in real-world applications,
especially in these safety-critical tasks (e.g., autonomous driving). One of
the reasons is that the learned policy cannot perform flexible and resilient
behaviors as traditional methods to adapt to diverse environments. In this
paper, we consider the problem that a mobile robot learns adaptive and
resilient behaviors for navigating in unseen uncertain environments while
avoiding collisions. We present a novel approach for uncertainty-aware
navigation by introducing an uncertainty-aware predictor to model the
environmental uncertainty, and we propose a novel uncertainty-aware navigation
network to learn resilient behaviors in the prior unknown environments. To
train the proposed uncertainty-aware network more stably and efficiently, we
present the temperature decay training paradigm, which balances exploration and
exploitation during the training process. Our experimental evaluation
demonstrates that our approach can learn resilient behaviors in diverse
environments and generate adaptive trajectories according to environmental
uncertainties.Comment: accepted to ICRA 202",Learning Resilient Behaviors for Navigation Under Uncertainty,http://arxiv.org/abs/1910.09998,,,,core
233572457,2020-01-01T00:00:00,"In this article, we propose an augmented reality semiautomatic labeling (ARS), a semiautomatic method which leverages on moving a 2-D camera by means of a robot, proving precise camera tracking, and an augmented reality pen (ARP) to define initial object bounding box, to create large labeled data sets with minimal human intervention. By removing the burden of generating annotated data from humans, we make the deep learning technique applied to computer vision, which typically requires very large data sets, truly automated and reliable. With the ARS pipeline, we created two novel data sets effortlessly, one on electromechanical components (industrial scenario) and other on fruits (daily-living scenario) and trained two state-of-the-art object detectors robustly, based on convolutional neural networks, such as you only look once (YOLO) and single shot detector (SSD). With respect to conventional manual annotation of 1000 frames that takes us slightly more than 10 h, the proposed approach based on ARS allows to annotate 9 sequences of about 35,000 frames in less than 1 h, with a gain factor of about 450. Moreover, both the precision and recall of object detection is increased by about 15% with respect to manual labeling. All our software is available as a robot operating system (ROS) package in a public repository alongside with the novel annotated data sets",Semiautomatic Labeling for Deep Learning in Robotics,,'Institute of Electrical and Electronics Engineers (IEEE)',10.1109/TASE.2019.2938316,,core
333537718,2020-08-28T10:22:15,"Robots are entering our daily lives from self-driving cars to health-care robots. Historically, pre-programmed robots were vulnerable to changing conditions in daily life, primarily because of a lack of ability to generate novel, non-preset flexible solutions. Thus there is a need for robotics to incorporate adaptation, which is a trait of higher order natural species. This adaptation allows higher-order natural species to change their behaviours and internal mechanisms based on experience with often dynamic environment. The ability to adapt emerged through evolutionary processes.
Evolutionary Robotics is an approach to create autonomous robots that are capable of automatically generating artificial behaviors and morphologies to achieve adaptation. Evolutionary robotics has the potential to automatically synthesize controllers for real autonomous robots and generate solutions to complete tasks in the uncertain real-world. Compared to the inflexibility of pre-programmed robots, evolutionary robots are able to learn flexible solutions to given tasks through evolutionary methods.

Cognitive robotics, a branch of artificial cognitive systems research, is such an attempt to create autonomous robots by applying bio-inspired methods. As the robot interacts with environment, an underlying cognitive system can learn its own solutions toward task completion. This learning-solution-from-interaction approach, also termed as a Reinforcement Learning (RL) approach, is widely applied in cognitive robotics to learn the solutions automatically. Ideally, the solutions can emerge in the cognitive system through the trial-and-error process of the RL approach without introducing human bias.

This thesis aims to develop an evolutionary cognitive architecture (system) for a robot that can learn adaptive solutions to complete tasks. Inspired by emotion theories, this work proposes Affective Computing Multilayer Cognitive Architecture (ACMCA), a universal cognitive architecture, which is able to learn diverse solutions. Extending from previous work, ACMCA has a five-layer structure, where each layer aims to achieve different components of the solutions. The position of this thesis is that introducing a novel emotion inspired multilayer architecture that produces task solutions through subsumption operations and underlying appropriate machine learning algorithms will allow a robot to complete admissible tasks.

ACMCA’s five layers are: primary reinforcer layer, secondary reinforcer layer, core affect state layer, strategy layer, and behaviour layer. This five-layer decomposition also meets the traditional decomposition of a mobile control system into functional modules (e.g. perception, modelling, planning, task execution, and motor control). Each layer contains computing nodes as functional modules that process various Stimuli, Actions, and their consequential Outcomes of the cognitive system. In this work, 17 computing nodes and their connections in ACMCA represent the solutions that a mobile robot has learned to complete navigation tasks in complex scenarios.

Inspired by the Constructive Theory 1 and the robotic subsumption system, this work proposes a contingency-based subsumption approach to construct ACMCA. This contingency is termed Stimuli-Action-Outcome Contingency (SAOC), which is extended from the Action-Outcome (AO) contingency of Construction Theory. SAOCs are represented by “if-then” rules, termed SAOC rules, which encapsulate Stimuli, Actions, and their consequential Outcomes, providing clear symbolic interpretations. That is, the symbolic meaning of a SAOC rule can be interpreted as: if the input stimulus is perceived, the output action will be advocated as a cognitive response, expecting the outcome of the action with an estimation of relevance. As low-level computing nodes encapsulate Stimulus, Actions, and Outcomes, high-level computing nodes can subsume these low-level ones through the form of SAOC rules. Therefore, the proposed ACMCA can be constructed by subsumption layers of Stimuli-Action-Outcome Contingency (SAOC) rules.

This work applies machine learning techniques to facilitate ACMCA’s real-world robotic implementation. This work selects Accuracy-based Learning Classifier Systems (XCS) algorithms as the underlying machine learning techniques that are deployed at computing nodes for the contingency-based subsumption operations. The mitosis approach of XCS and the XCS with a Combined Reward method (XCSCR) are two novel variants of XCS algorithm. They are proposed to amend two challenges that occur when the standard XCS approaches are applied for robotic applications. The mitosis approach introduces an accuracy pressure into the algorithm’s evolutionary process, improving the algorithms’ performance in robotic applications where noisy interferences exist. The XCSCR enables the policy to emerge earlier and more frequently than the existing benchmark approaches in multistep problems. Therefore, a robot with the XCSCR can handle a multistep scenario more effectively than those with the benchmarked algorithms.

This work conducts five experiments to test the capability of ACMCA and its underlying algorithms in learning solutions for robotic navigation tasks. The five experiments are conducted as follows: reflex-learning, IR-tuning, deliberation-establishing, emotion model, and combined reward assignment. As the results of the experiments, three different affective patterns have emerged in the first three experiments, an emotion model has emerged in the fourth experiments, and the fifth experiment explores ACMCA’s potential implementation in the life-long learning scenario.

These results demonstrate that ACMCA, a novel emotion inspired multilayer architecture, can produce task solutions through contingency-based subsumption operations and underlying appropriate machine learning algorithms, allowing a robot to complete admissible tasks through evolutionary processes. The contingency-based subsumption operations can establish three contingencies and one emotion model between the subsumed components by multiple RL agents which deploy the proposed mitosis approach of XCS algorithms. These three emotion patterns and emotion model can consistently improve the robot’s navigation performance with interpretable explanations. These two variants of XCS algorithms can amend shortfalls of the standard XCS approach in real-world robotic implementations. It has been demonstrated that the diverse solutions learned by ACMCA improve the navigation performance of the robot in terms of higher flexibility, reduction in continuous collisions and shorter navigation time consumption",An Affective Computing Multilayer Cognitive Architecture for Evolutionary Cognitive Robotics,,'Victoria University of Wellington Library',,,core
326230945,2020-06-18T00:00:00,"Neural Networks (NNs) have increasingly apparent safety implications
commensurate with their proliferation in real-world applications: both
unanticipated as well as adversarial misclassifications can result in fatal
outcomes. As a consequence, techniques of formal verification have been
recognized as crucial to the design and deployment of safe NNs. In this paper,
we introduce a new approach to formally verify the most commonly considered
safety specification for ReLU NNs -- i.e. polytopic specifications on the input
and output of the network. Like some other approaches, ours uses a relaxed
convex program to mitigate the combinatorial complexity of the problem.
However, unique in our approach is the way we exploit the geometry of neuronal
activation regions to further prune the search space of relaxed neuron
activations. In particular, conditioning on neurons from input layer to output
layer, we can regard each relaxed neuron as having the simplest possible
geometry for its activation region: a half-space.This paradigm can be leveraged
to create a verification algorithm that is not only faster in general than
competing approaches, but is also able to verify considerably more safety
properties. For example, our approach completes the standard MNIST verification
test bench 2.7-50 times faster than competing algorithms while still proving
14-30% more properties. We also used our framework to verify the safety of a
neural network controlled autonomous robot in a structured environment, and
observed a 1900 times speed up compared to existing methods","Effective Formal Verification of Neural Networks using the Geometry of
  Linear Regions",http://arxiv.org/abs/2006.10864,,,,core
362230362,2020-11-25T00:00:00,"International audienceIn this paper, we propose a method for coarse camera pose computation which is robust to viewing conditions and does not require a detailed model of the scene. This method meets the growing need of easy deployment of robotics or augmented reality applications in any environments, especially those for which no accurate 3D model nor huge amount of ground truth data are available. It exploits the ability of deep learning techniques to reliably detect objects regardless of viewing conditions. Previous works have also shown that abstracting the geometry of a scene of objects by an ellipsoid cloud allows to compute the camera pose accurately enough for various application needs. Though promising, these approaches use the ellipses fitted to the detection bounding boxes as an approximation of the im-aged objects. In this paper, we go one step further and propose a learning-based method which detects improved elliptic approximations of objects which are coherent with the 3D ellipsoid in terms of perspective projection. Experiments prove that the accuracy of the computed pose significantly increases thanks to our method and is more robust to the variability of the boundaries of the detection boxes. This is achieved with very little effort in terms of training data acquisition-a few hundred calibrated images of which only three need manual object annotation",3D-Aware Ellipse Prediction for Object-Based Camera Pose Estimation,https://core.ac.uk/download/362230362.pdf,HAL CCSD,,,core
327075346,2020-07-24T00:00:00,"We consider an autonomous exploration problem in which a range-sensing mobile
robot is tasked with accurately mapping the landmarks in an a priori unknown
environment efficiently in real-time; it must choose sensing actions that both
curb localization uncertainty and achieve information gain. For this problem,
belief space planning methods that forward-simulate robot sensing and
estimation may often fail in real-time implementation, scaling poorly with
increasing size of the state, belief and action spaces. We propose a novel
approach that uses graph neural networks (GNNs) in conjunction with deep
reinforcement learning (DRL), enabling decision-making over graphs containing
exploration information to predict a robot's optimal sensing action in belief
space. The policy, which is trained in different random environments without
human intervention, offers a real-time, scalable decision-making process whose
high-performance exploratory sensing actions yield accurate maps and high rates
of information gain","Autonomous Exploration Under Uncertainty via Deep Reinforcement Learning
  on Graphs",http://arxiv.org/abs/2007.12640,,,,core
323030379,2020-07-20T00:00:00,"Reproducing the diverse and agile locomotion skills of animals has been a
longstanding challenge in robotics. While manually-designed controllers have
been able to emulate many complex behaviors, building such controllers involves
a time-consuming and difficult development process, often requiring substantial
expertise of the nuances of each skill. Reinforcement learning provides an
appealing alternative for automating the manual effort involved in the
development of controllers. However, designing learning objectives that elicit
the desired behaviors from an agent can also require a great deal of
skill-specific expertise. In this work, we present an imitation learning system
that enables legged robots to learn agile locomotion skills by imitating
real-world animals. We show that by leveraging reference motion data, a single
learning-based approach is able to automatically synthesize controllers for a
diverse repertoire behaviors for legged robots. By incorporating sample
efficient domain adaptation techniques into the training process, our system is
able to learn adaptive policies in simulation that can then be quickly adapted
for real-world deployment. To demonstrate the effectiveness of our system, we
train an 18-DoF quadruped robot to perform a variety of agile behaviors ranging
from different locomotion gaits to dynamic hops and turns",Learning Agile Robotic Locomotion Skills by Imitating Animals,http://arxiv.org/abs/2004.00784,,,,core
322453654,2020-05-07T00:00:00,"It is important to be able to establish formal performance bounds for
autonomous systems. However, formal verification techniques require a model of
the environment in which the system operates; a challenge for autonomous
systems, especially those expected to operate over longer timescales. This
paper describes work in progress to automate the monitor and repair of
ROS-based autonomous robot software written for an a-priori partially known and
possibly incorrect environment model. A taint analysis method is used to
automatically extract the data-flow sequence from input topic to publish topic,
and instrument that code. A unique reinforcement learning approximation of MDP
utility is calculated, an empirical and non-invasive characterization of the
inherent objectives of the software designers. By comparing off-line (a-priori)
utility with on-line (deployed system) utility, we show, using a small but real
ROS example, that it's possible to monitor a performance criterion and relate
violations of the criterion to parts of the software. The software is then
patched using automated software repair techniques and evaluated against the
original off-line utility.Comment: IEEE Workshop on Assured IEEE Workshop on Assured Autonomous Systems,
  May, 202","Using Taint Analysis and Reinforcement Learning (TARL) to Repair
  Autonomous Robot Software",http://arxiv.org/abs/2005.03813,,,,core
475205979,2020-12-01T00:00:00,"Intelligent robots require advanced vision capabilities to perceive and interact with the real physical world. While computer vision has made great strides in recent years, its predominant paradigm still focuses on building deep-learning networks or handcrafted features to achieve semantic labeling or instance segmentation separately and independently. However, the two tasks should be synergistically unified in the recognition flow since they have a complementary nature in scene understanding.This dissertation presents the detection of instances in multiple scene understanding levels. Representations that enable intelligent systems to not only recognize what is seen (e.g. Does that pixel represent a chair?), but also predict contextual information about the complete 3D scene as a whole (e.g. How big is the chair? Is the chair placed next to a table?). More specifically, it presents a flow of understanding from local information to global fitness. First, we investigate in the 3D geometry information of instances. A new approach of generating tight cuboids for objects is presented. Then, we take advantage of the trained semantic labeling networks by using the intermediate layer output as a per-category local detector. Instance hypotheses are generated to help traditional optimization methods to get a higher instance segmentation accuracy. After that, to bring the local detection results to holistic scene understanding, our method optimizes object instance segmentation considering both the spacial fitness and the relational compatibility. The context information is implemented using graphical models which represent the scene level object placement in three ways: horizontal, vertical and non-placement hanging relations. Finally, the context information is implemented to a network structure. A deep learning-based re-inferencing frame work is proposed to boost any pixel-level labeling outputs using our local collaborative object presence (LoCOP) feature as the global-to-local guidance.This dissertation demonstrates that uniting pixel-level detection and instance segmentation not only significantly improves the overall performance for localized and individualized analysis, but also paves the way for holistic scene understanding",Holistic indoor scene understanding by context supported instance segmentation,https://core.ac.uk/download/475205979.pdf,,,,core
329131898,2020-08-30T00:00:00,"In recent years, industrial robots have been installed in various industries
to handle advanced manufacturing and high precision tasks. However, further
integration of industrial robots is hampered by their limited flexibility,
adaptability and decision making skills compared to human operators. Assembly
tasks are especially challenging for robots since they are contact-rich and
sensitive to even small uncertainties. While reinforcement learning (RL) offers
a promising framework to learn contact-rich control policies from scratch, its
applicability to high-dimensional continuous state-action spaces remains rather
limited due to high brittleness and sample complexity. To address those issues,
we propose different pruning methods that facilitate convergence and
generalization. In particular, we divide the task into free and contact-rich
sub-tasks, perform the control in Cartesian rather than joint space, and
parameterize the control policy. Those pruning methods are naturally
implemented within the framework of dynamic movement primitives (DMP). To
handle contact-rich tasks, we extend the DMP framework by introducing a
coupling term that acts like the human wrist and provides active compliance
under contact with the environment. We demonstrate that the proposed method can
learn insertion skills that are invariant to space, size, shape, and closely
related scenarios, while handling large uncertainties. Finally we demonstrate
that the learned policy can be easily transferred from simulations to real
world and achieve similar performance on UR5e robot.Comment: 27 pages, 10 Figure","Deep Reinforcement Learning for Contact-Rich Skills Using Compliant
  Movement Primitives",http://arxiv.org/abs/2008.13223,,,,core
334919266,2020-03-04T00:00:00,"We present a novel technique called Dynamic Experience Replay (DER) that
allows Reinforcement Learning (RL) algorithms to use experience replay samples
not only from human demonstrations but also successful transitions generated by
RL agents during training and therefore improve training efficiency. It can be
combined with an arbitrary off-policy RL algorithm, such as DDPG or DQN, and
their distributed versions. We build upon Ape-X DDPG and demonstrate our
approach on robotic tight-fitting joint assembly tasks, based on force/torque
and Cartesian pose observations. In particular, we run experiments on two
different tasks: peg-in-hole and lap-joint. In each case, we compare different
replay buffer structures and how DER affects them. Our ablation studies show
that Dynamic Experience Replay is a crucial ingredient that either largely
shortens the training time in these challenging environments or solves the
tasks that the vanilla Ape-X DDPG cannot solve. We also show that our policies
learned purely in simulation can be deployed successfully on the real robot.
The video presenting our experiments is available at
https://sites.google.com/site/dynamicexperiencereplayComment: 10 pages, 5 figures, presented at 2019 Conference on Robot Learning
  (CoRL",Dynamic Experience Replay,http://arxiv.org/abs/2003.02372,,,,core
334911622,2020-03-14T00:00:00,"With the increasing awareness of high-quality life, there is a growing need
for health monitoring devices running robust algorithms in home environment.
Health monitoring technologies enable real-time analysis of users' health
status, offering long-term healthcare support and reducing hospitalization
time. The purpose of this work is twofold, the software focuses on the analysis
of gait, which is widely adopted for joint correction and assessing any lower
limb or spinal problem. On the hardware side, we design a novel marker-less
gait analysis device using a low-cost RGB camera mounted on a mobile
tele-robot. As gait analysis with a single camera is much more challenging
compared to previous works utilizing multi-cameras, a RGB-D camera or wearable
sensors, we propose using vision-based human pose estimation approaches. More
specifically, based on the output of two state-of-the-art human pose estimation
models (Openpose and VNect), we devise measurements for four bespoke gait
parameters: inversion/eversion, dorsiflexion/plantarflexion, ankle and foot
progression angles. We thereby classify walking patterns into normal,
supination, pronation and limp. We also illustrate how to run the purposed
machine learning models in low-resource environments such as a single
entry-level CPU. Experiments show that our single RGB camera method achieves
competitive performance compared to state-of-the-art methods based on depth
cameras or multi-camera motion capture system, at smaller hardware costs","A Single RGB Camera Based Gait Analysis with a Mobile Tele-Robot for
  Healthcare",http://arxiv.org/abs/2002.04700,,,,core
342060224,2020-01-01T00:00:00,"Tensegrity structures are an emergent type of soft-robotics that are compliant, lightweight, and impact-resilient. In collaboration with NASA Ames Research Center, research in the Berkeley Emergent Space Tensegrities Lab at UC Berkeley has largely focused on the design and control of these novel structures as potential surface exploration robots which could act as both landers and rovers. More recently, tensegrity robots have also been proposed for applications closer to home – working as disaster response and emergency co-robots to help first responders obtain situational awareness faster and safer. Constructed using isolated rigid bodies suspended in a tension network of elastic elements, tensegrity structures exhibit unique and advantageous mechanical properties for applications in uncertain and potentially hazardous environments, albeit at the cost of increased complexity for dynamic feedback control.  In addressing these challenges, this work explores possible approaches for feedback control and state estimation for ground-based rolling locomotion with six-bar spherical tensegrities. In this dissertation, we explore problems pertaining to practical implementation – state estimation, modeling, motion planning, and optimal control of tensegrity robots under uncertainty. Leveraging the well-structured dynamics of Class-1 tensegrity robots, we implement and evaluate model-based Model Predictive Control and iterative local quadratic methods for tensegrity motion planning. Additionally, we consider alternative tensegrity topologies and actuator schema which may enable improved performance for task-specific objectives. Due to the many degrees of freedom and compliant nature of tensegrity structures, however, excessive state estimate errors may propagate catastrophically. To evaluate these effects, Bayesian state estimators are applied to tensegrity ground mobility in simulation, evaluating their performance under the additional constraints of low-cost sensors and potentially scarce and noisy sensor data. An imitation learning approach is introduced to achieve directed rolling motion using a contextual neural network policy, combining deep learning and optimal control for real-time feedback control of highly nonlinear tensegrity systems. Finally, a robust minimax control approach is proposed in order to address challenges which arise at the intersection and interaction of state estimation and trajectory optimization for flexible tensegrity robotics.  Combined, these pragmatic research developments help advance the progression of this novel technology towards becoming a viable and more widely adopted robotics paradigm","Design, Control, and Motion Planning of Cable-Driven Flexible Tensegrity Robots",,"eScholarship, University of California",,,core
339157644,2020-07-11T10:13:18,"Since the past few decades, human trajectory forecasting has been a field of active research owing to its numerous real-world applications: evacuation situation analysis, traffic operations, deployment of social robots in crowded environments, to name a few. In this work, we cast the problem of human trajectory forecasting as learning a representation of human social interactions. Early works handcrafted this representation based on domain knowledge. However, social interactions in crowded environments are not only diverse but often subtle. Recently, deep learning methods have outperformed their handcrafted counterparts, as they learned about human-human interactions in a more generic data-driven fashion. In this work, we present an in-depth analysis of existing deep learning based methods for modelling social interactions. Based on our analysis, we propose a simple yet powerful method for effectively capturing these social interactions. To objectively compare the performance of these interaction-based forecasting models, we develop a large scale interaction-centric benchmark TrajNet++, a significant yet missing component in the field of human trajectory forecasting. We propose novel performance metrics that evaluate the ability of a model to output socially acceptable trajectories. Experiments on TrajNet++ validate the need for our proposed metrics, and our method outperforms competitive baselines on both real-world and synthetic datasets",Human Trajectory Forecasting in Crowds: A Deep Learning Perspective,,,,,core
287844227,2020-02-07T10:21:57,"Intelligent robotic systems are becoming essential for space applications, industries, nuclear plants and for harsh environments in general, such as the European Organization for Nuclear Research (CERN) particles accelerator complex and experiments. Robotics technology has huge potential benefits for people and its ultimate scope depends on the way this technology is  used. In order to increase safety and machine availability, robots can perform repetitive, unplanned and dangerous tasks, which humans either prefer to avoid or are unable to carry out due to hazards, size constraints, or the extreme environments in which they take place. Nowadays, mechatronic systems use mature technologies that allow their robust and safe use, even in collaboration with human workers. Over the past years, the progress of robots has been based on the development of smart sensors, artificial intelligence and modular mechanical systems. Due to the multiple challenges that hazardous and unstructured environments have for the application of autonomous industrial systems, there is still a high demand for intelligent and teleoperation systems that give the control of a robot (slave) to a human operator via haptic input devices (master), as well as using human-supervised telerobotic control techniques. Modern techniques like simulation and virtual reality systems can facilitate the preparation of ad-hoc mechatronic tools and robotic intervention including recovery scenarios and failure mode analysis.  The basic contribution of this thesis is the development of a novel robotic framework for autonomous inspections and supervised teleoperations in harsh environments. The proposed framework covers all aspects of a robotic intervention, from the specification and operator training, the choice of the robot and its material in accordance with possible radiological contamination risks, to the realization of the intervention, including procedures and recovery scenarios. In a second set of contributions, new methods for mutirobots maintenance operations are developed, including intervention preparation and best practices for remote handling and advanced tools. The third set of contributions is built on a novel multimodal user-friendly human-robot interface that allows  operator training using virtual reality systems and technicians not expert in robot operation to perform inspection/maintenance tasks. In this thesis, we exploit a robotic system able to navigate autonomously and to inspect unknown environments in a safe way. A new real-time control system has been implemented in order to guarantee a fast response to environmental changes and adaptation to different type of scenarios the robot may find in a semi-structured and hazardous environment. The proposed new robotic control system  has been integrated on different robots, tested and validated with several robotic interventions in the CERN hazardous particle accelerator complex",A Novel Robotic Framework for Safe Inspection and Telemanipulation in Hazardous and Unstructured Environments,,,,,core
395064085,2020-01-01T00:00:00,"Due to similarities in learning techniques, Reinforcement Learning (RL) is the closest alternative to human-level intelligence. Teleoperation systems using RL can adapt to new environmental conditions and deal with high uncertainty due to long-time delays. In this thesis, we propose a method that takes advantage of RL capabilities to extend the human reach in dangerous remote environments. The proposed method utilizes the Model Mediated Teleoperation (MMT) concept in which the teleoperator interacts with a simulated setup that resembles the real environment. The simulation can provide instant haptic feedback where the data from the real environment are delayed. The proposed approach enables haptic feedback teleoperation of high-DOF dexterous robots under long time delays in a time-varying environment with high uncertainty.



In existence of time delay, when the data is received by the remote system the environment may change drastically, therefore, the attempt for task execution will fail. To prevent failure, an intelligence system is realized in two layers, the first layer utilizes the Dynamic Movement Primitives (DMP) which accounts for certain changes in the environment. DMPs can adjust the shape of a trajectory based on given criteria, for example, a new target position or avoiding a new obstacle. But in an uncertain environment, DMPs fail, therefore, the second layer of intelligence makes use of different reinforcement learning methods based on expectation-maximization, stochastic optimal control and policy gradient to guarantee the successful completion of the task. 



Furthermore, To ensure the safety of the system, and speed up the learning process, each learning session for RL happens in multiple simulations of the remote system and environment, simultaneously.

 

The proposed approach was realized on DLR's haptic hand-arm user interface/exoskeleton, Exodex Adam. It has been used for the first time in this work as the master device to teleoperate a high-DOF dexterous robot. This slave device is an anthropomorphic hand-arm system combining a five-finger hand (FFH) attached to a custom configured DLR lightweight robot (LWR 4+) more closely fitting to the kinematics of the human arm. An augmented reality visualization implemented on the Microsoft Hololens fuses the slave device and virtual environment models to provide environment immersion for the teleoperator.



A preliminary user-study was carried out to help evaluate the human-robot interaction capabilities and performance of the system. Meanwhile, the RL approaches are evaluated separately in two different levels of difficulty; with and without uncertainty in perceived object position. 



The results from the unweighted NASA Task load Index (NASA TLX) and System Usability Score (SUS) questionnaires show a low workload (27) and above-average perceived usability (71). The learning results show all RL methods can find a solution for all challenges in a limited time. Meanwhile, the method based on stochastic optimal control has a better performance. The results also show DMPs to be effective at adapting to new conditions where there is no uncertainty involved",Adaptive Model Mediated Control Using Reinforcement Learning,https://core.ac.uk/download/395064085.pdf,,,,core
288394796,2020-02-25T00:00:00,"The autonomous landing of an Unmanned Aerial Vehicle (UAV) on a marker is one of the most challenging problems in robotics. Many solutions have been proposed, with the best results achieved via customized geometric features and external sensors. This paper discusses for the first time the use of deep reinforcement learning as an end-to-end learning paradigm to find a policy for UAVs autonomous landing. Our method is based on a divide-and-conquer paradigm that splits a task into sequential sub-tasks, each one assigned to a Deep Q-Network (DQN), hence the name Sequential Deep Q-Network (SDQN). Each DQN in an SDQN is activated by an internal trigger, and it represents a component of a high-level control policy, which can navigate the UAV towards the marker. Different technical solutions have been implemented, for example combining vanilla and double DQNs, and the introduction of a partitioned buffer replay to address the problem of sample efficiency. One of the main contributions of this work consists in showing how an SDQN trained in a simulator via domain randomization, can effectively generalize to real-world scenarios of increasing complexity. The performance of SDQNs is comparable with a state-of-the-art algorithm and human pilots while being quantitatively better in noisy conditions",Sim-to-Real Quadrotor Landing via Sequential Deep Q-Networks and Domain Randomization,https://core.ac.uk/download/288394796.pdf,'MDPI AG',10.3390/robotics9010008,,core
480167114,2020-09-28T00:00:00,"Deep Reinforcement Learning (DRL) has gained much attention for solving robotic hand-eye coordination tasks from raw pixel values. Despite promising results, training agents using images is hardware intensive often requiring millions of training steps to converge incurring long training times and increased risk of wear and tear on the robot. To speed up training, images are often cropped and downscaled resulting in a smaller field of view and loss of valuable high-frequency data. In this paper, we propose training the vision system using supervised learning prior to training robotic actuation using Deep Deterministic Policy Gradient (DDPG). The vision system uses a software retina, based on the mammalian retino-cortical transform, to preprocess full-size images to compress image data while preserving the full field of view and high-frequency visual information around the fixation point prior to processing by a Deep Convolutional Neural Network (DCNN) to extract visual state information. Using the vision system to preprocess the environment improves the agent's sample complexity and network update speed leading to significantly faster training with reduced image data loss. Our method is used to train a DRL system to control a real Baxter robot's arm, processing full-size images captured by an in-wrist camera to locate an object on a table and centre the camera over it by actuating the robot arm",Deep reinforcement learning control of hand-eye coordination with a software retina,,'Institute of Electrical and Electronics Engineers (IEEE)',10.1109/IJCNN48605.2020.9207332,,core
326495128,2020-06-28T00:00:00,"Increasing interest in integrating advanced robotics within manufacturing has
spurred a renewed concentration in developing real-time scheduling solutions to
coordinate human-robot collaboration in this environment. Traditionally, the
problem of scheduling agents to complete tasks with temporal and spatial
constraints has been approached either with exact algorithms, which are
computationally intractable for large-scale, dynamic coordination, or
approximate methods that require domain experts to craft heuristics for each
application. We seek to overcome the limitations of these conventional methods
by developing a novel graph attention network formulation to automatically
learn features of scheduling problems to allow their deployment. To learn
effective policies for combinatorial optimization problems via machine
learning, we combine imitation learning on smaller problems with deep
Q-learning on larger problems, in a non-parametric framework, to allow for
fast, near-optimal scheduling of robot teams. We show that our network-based
policy finds at least twice as many solutions over prior state-of-the-art
methods in all testing scenarios.Comment: This paper has been extended to an article in IEEE Robotics and
  Automation Letters (DOI: 10.1109/LRA.2020.3002198","Learning to Dynamically Coordinate Multi-Robot Teams in Graph Attention
  Networks",http://arxiv.org/abs/1912.02059,,,,core
344752398,2020-07-01T07:00:00,"All Too Well is a 2D animation graduate thesis film, with a length of 6 minutes and 49 seconds. The production phase went from September 2018 to January 2020.
A father who lost his son in a car accident recreates the family by bringing an artificial intelligence boy into the home to replace the lost son. Not only does their relationship fail to heal the father\u27s broken heart, but it also often brings back painful memories of his real boy. This only makes him realize that the robot son cannot replace his real son. So, he turns the power off of the robot son and deeply grieves. The mother witnesses her husband’s actions, approaches him, and then turns his power off. Themes of loss and healing in the age of artificial intelligence and the use of robots to fill our need for love and normal relationships are central to this abstracted narrative.
The major software used during the entire production process include: Adobe Photoshop, Adobe After Effects, Adobe Premiere, TVPaint and MOHO. The original music is a score by KaMan Chang. The final output format is 1080HD.
This paper will retrace the making of All Too Well and share the personal experience during its creation",All Too Well,https://core.ac.uk/download/344752398.pdf,RIT Scholar Works,,,core
357279592,2020-03-06T00:00:00,"AS ROBOTS INCREASINGLY BE- come part of our everyday lives, they will serve as caretakers for the elderly and disabled, assistants in surgery and rehabilitation, and educational toys. But for this to happen, programming and control must become simpler and human-robot interaction more natural. Both challenges are particularly relevant to humanoid robots, which are highly difficult to control yet most natural for interaction with people and operation in human environments. As this article shows, we have used biologically inspired notions of behavior-based control to address these challenges at the University of Southern California&apos;s Interaction Lab, part of the USC Robotics Research Labs. By endowing robots with the ability to imitate, we can program and interact with them through human demonstration, a natural human-humanoid interface. The human ability to imitate-to observe and repeat behaviors performed by a teacher-is a poorly understood but powerful form of skill learning. Two fundamental open problems in imitation involve interpreting and understanding the observed behavior and integrating the visual perception and movement control systems to reconstruct what was observed. Our research has a similarly twofold goal: we are developing methods for segmenting and classifying visual input for recognizing human behavior as well as methods for structuring the motor control system for general movement and imitation-learning capabilities. Our approach brings these two pursuits together much as the evolutionary process brought them together in biological systems. 1,2 We structure the motor system into a collection of movement primitives, which then serve both to generate the humanoid&apos;s movement repertoire and to provide prediction and classification capabilities for visual perception and interpretation of movement. This way, what the humanoid can do helps it understand what it sees and vice versa. The more it sees, the more it learns to do, and thus the better it gets at understanding what it sees for further learning; this is the imitation process. Behavior-based robotics Our work over the last 15 years has focused on developing distributed, behavior-based methods for controlling groups of mobile robots and, most recently, humanoids. Behavior-based control involves the design of control systems consisting of a collection of behaviors.  THIS BEHAVIOR-BASED APPROACH TO STRUCTURING IEEE INTELLIGENT SYSTEMS The inspiration for behavior-based control comes from biology, where natural systems are believed to be similarly organized, from spinal reflex movements up to more complex behaviors such as flocking and foraging.  Basis behaviors and primitives. Several methods for principled behavior design and coordination are possible.  Collections of behaviors are a natural representation for controlling collections of robots. But how can we use the same idea in the humanoid control domain, where the body&apos;s individual degrees of freedom are more coupled and constrained? For this, we have combined the notion of primitives with another line of evidence from neurosciencemirror neurons-to structure humanoid motor control into a general and robust system capable of a variety of skills and learning by imitation. 6 Humanoid control and imitation. Robot control is a complex problem, involving sensory and effector limitations and various forms of uncertainty. The more complex the system to be controlled, the more we must modularize the approach to make control viable and efficient. Humanoid agents and robots are highly complex; a human arm has seven degrees of freedom (DOF), the hand has 23, and the control of an actuated human spine is beyond current consideration. Yet humans display complex dynamic behaviors in real time and learn various motor skills throughout life, often through imitation. Methods for automating robot programming are in high demand. Reinforcement learning, which lets a robot improve its behavior based on trial-and-error feedback, is very popular. However, reinforcement learning is slow, as the robot must repeatedly try various behaviors in different situations. It can also jeopardize the robot. In contrast, learning by imitation is particularly appealing because it lets the designer specify entire behaviors by demonstration, instead of using low-level programming or trial and error by the robot. In biological systems, imitation appears to be a complex learning mechanism that involves an intricate interaction between visual perception and motor control, both of which are complex in themselves. Although various animals demonstrate simple mimicry, only a very few species, including humans, chimps, and dolphins, are capable of so-called true imitation, which involves the ability to learn arbitrary new skills by observation.  Neuroscience inspiration Evidence from neuroscience studies in animals points to two neural structures we find of key relevance to imitation: spinal fields and mirror neurons. Spinal fields, found in frogs and rats so far, code for complete primitive movements (or behaviors), such as reaching and wiping.  Investigators recently found neurons with so-called mirror properties in monkeys and humans. They appear to directly connect the visual and motor control systems by mapping observed behaviors, such as reaching and grasping, to motor structures that produce them.  We combine these two lines of evidence, spinal basis fields and mirror neurons, into a more sophisticated notion of behaviors, or perceptual-motor primitives. These let a complex system, such as a humanoid, recognize, reproduce, and learn motor skills. The primitives serve as the basis set for generating movements, but also as a vocabulary for classifying observed movements into executable ones. Thus, primitives can classify, predict, and act. In our approach to imitation, the vision system continually matches any observed human movements onto its own set of motor primitives. The primitive, or combination of primitives, that best approximates the observed input also provides the best predictor of what the robot expects to observe next. This expectation facilitates visual segmentation and interpretation of the observed movement. Imitation, then, is a process of matching, classification, and prediction. Learning by imitation, in turn, creates new skills as novel sequences and superpositions of the matched and classified primitives. The hierarchical structure of our imitation approach lets the robot initially observe and imitate a skill, then perfect it through repetition, so that the skill becomes a routine and itself a primitive. The set of primitives can thus adapt over time, to allow for learning arbitrary new skills-that is, for &quot;true&quot; imitation.  Choosing the primitives Movement primitives or behaviors are the unifying mechanism between visual perception and motor control in our approach, and choosing the right ones is a research challenge, driven by several constraints. On the one hand, the motor control system imposes physical bottom-up limitations, based on its kinematic and dynamic properties. It also provides topdown constraints from the type of movements the system is expected to perform, because the primitives must be sufficient for the robot&apos;s entire movement repertoire. On the other hand, the visual system&apos;s structure and inputs influence the choice of primitives for mapping the various observed movements onto its own executable repertoire. To serve as a general and parsimonious basis set, the primitives encode groups or classes of stereotypical movements, invariant to exact position, rate of motion, size, and perspective. Thus, they represent the generic building blocks of motion that can be implemented as parametric motor controllers. Consider a primitive for reaching. Its most important parameter is the goal position of the end point-that is, the hand or held object. It might be further parametrized by a default posture for the entire arm. Such a primitive lets a robot reach toward various goals within a multitude of tasks, from grasping objects and tools, to dancing, to writing and drawing. We used just such a reaching primitive in our experiments to reconstruct the popular dance, the Macarena.  What constitutes a good set of primitives? We have experimented with two types: innate and learned. Innate primitives are userselected and preprogrammed. We have demonstrated the effectiveness of a basis set consisting of three types: • discrete straight-line movements of subsets of degrees of freedom, accounting for reaching-type motions; • continuous oscillatory movements of subsets of DOFs, accounting for repetitive motions; 9 and • postures, accounting for large subsets of the body&apos;s DOFs. 2 Our approach computes the learned primitives directly from human movement data. We gather different types of such data, using the following methods: vision-based motion tracking of the human upper body (using our tracking system), magnetic markers on the arm (using the FastTrak system), and fullbody joint angle data (using the Sarcos Sensuit). We first reduce the dimensionality of the movement data by employing principal components analysis, wavelet compression, and correlation across multiple DOF. Next, we use clustering techniques to extract patterns of similar movements in the data. These clusters or patterns form the basis for the primitives; the movements in the clusters are generalized and parameterized, to result in primitives for producing a variety of similar movements. Visual classification into primitives. Visual perception is also an important constraint on the primitives and a key component of the imitation process. Because the human (and humanoid) visual attention is resource-limited, it must select the visual features that are most relevant to the given imitation task. Determining what those features are for a given demonstration is a challenging problem. Our previous work showed that people watching videos of arm movements displayed no difference in attention whether they were just watching or intending to subsequently imitate. In both cases, they fixated at the end point-the hand or a held object.  Consequently, we can effectively bias the visual perception mechanism toward recognizing movements that it can execute, especially those movements it performs most frequently. The motor control system&apos;s structure, and its underlying set of movement primitives, provides key constraints for visual movement recognition and classification. Our primitive classifier uses the primitives&apos; descriptions to segment a given motion based on the movement data. In the experiments described below, we used end-point data for both arms as input for the vector quantization-based classifier. 11 Again, a key issue in classification is representing the primitives such that they account for significant invariances, such as position, rotation, and scaling. Our classification approach forms the original motion into a vector of relative end-point movements between successive frames, then smoothes and normalizes it. At the classification level, we ignore all other information about the movement, such as global position and arm configuration, enabling a small set of high-level primitive representations instead of a potentially prohibitively large set of detailed ones. Other information necessary for correct imitation serves for parameterizing the selected primitives at the level of movement reconstruction and execution. To simplify matching, our approach describes primitives themselves in the same normalized form. For each time step of the observed motion, we compare a fixed- IEEE INTELLIGENT SYSTEMS WE CAN EFFECTIVELY BIAS THE VISUAL PERCEPTION MECHANISM TOWARD RECOGNIZING MOVEMENTS THAT IT CAN EXECUTE, ESPECIALLY THOSE MOVEMENTS IT PERFORMS MOST FREQUENTLY. horizon window to every primitive and select the one that best matches the input. Adjacent windows with identical classifications connect to form continuous segments. For any segments that fail to match the given primitives, our approach uses the reaching primitive to move the end point frame by frame. Because the horizon window is of fixed size, the perception of a distinct match of a primitive applies only for the given timescale. We are currently working on addressing classification at multiple timescales. To validate our approach, we implemented various examples of imitation, including reaching, ball throwing, aerobics moves, and dance, all on humanoid testbeds, taking human demonstrations as input.  We also used 3D magnetic marker data from the human arm, gathered from subjects imitating videos of arm movements while wearing FastTrak markers for position recording. (These data were gathered at the National Institutes of Health Resource for the Study of Neural Models of Behavior at the University of Rochester.) We used four markers: near the shoulder, the elbow, the wrist, and the start of the middle finger. The movement data resulting from this experiment serve as input into our imitation system, as well as for automatically learning the primitives. Finally, we used full-body joint angle data gathered with the Sarcos Sensuit, a wearable exoskeleton that simultaneously records the joint positions of 35 DOF: the shoulders, elbows, wrists, hips, knees, ankles, and waist. (These data are obtained through a collaboration with the ATR Dynamic Brain Project at the Human Information Processing Labs in Kyoto, Japan.) We are currently focusing on reproducing the upper-body movements from those data on our testbeds, described next. Evaluation testbeds To properly validate our approach to humanoid motor control and imitation, we use different experimental testbeds. Most of our work so far has been done on Adonis (developed at the Georgia Institute of Technology Animation Lab), a 3D rigid-body simulation of a human with full dynamics  As the project progresses, we plan to use physical humanoid robots as the ultimate testbeds for evaluating our imitation architecture. The NASA Robonaut is one candidate, through collaboration with the Johnson Space Center. The Sarcos full-body humanoid robot is another, through collaboration with the ATR Dynamic Brain Project. Both robots are highly complex, built to approximate human body structure as faithfully as practically possible, and feature binocular cameras for embodied visual perception critical for imitation. OUR APPROACH TO HUMANOID motor control and imitation relies on the use of a set of movement primitives. We have experimented with different types of such primitives on different humanoid simulation testbeds. Specifically, we have implemented two versions of the spinal fields found in animals. One closely modeled the frog data, and used a joint-space representation-it controlled individual joints of Adonis&apos;s arms.  We tested both types of primitives on a sequential motor task, dancing the Macarena. Both proved effective, but each had limitations for particular types of movements. This led us to propose and explore a combination approach, where multiple types of primitives can be sequenced and combined. For example, we constructed a basis behavior set consisting of three types of primitives: • discrete straight-line movements using impedance control; • continuous oscillatory movements using coupled oscillators (or a collection of piece-wise linear segments using impedance control); and • postures, using PD-servos to directly control the joints. We also added a forth type of primitive, for avoidance, implemented as a repulsive vector field. The continuously active fourth primitive combined with whatever other primitive was executing to prevent any collisions 22 IEEE INTELLIGENT SYSTEMS  between body parts. In the Macarena, for example, this is necessary for arm movements around and behind the head (  Our goal is not to achieve perfect, completely precise, high-fidelity imitation. While that might be possible, through the use of exact quantitative measurements of the observed movement using signal-processing techniques, it is not what happens in imitation in nature and it is neither necessary nor helpful for our main goals: natural interaction and programming of robots. For those purposes, we aim for an approximation of the observed behavior, one that allows any necessary freedom of interpretation by the humanoid robot, but achieves the task and effectively communicates and interacts with the human. Our goal is also distinct from task-level imitation, which only achieves the demonstration&apos;s goal, but does not imitate the behaviors involved. This problem has been studied in assembly robotics, where investigators used a robotic arm to record, segment, interpret, and then repeat a series of visual images of a human performing an object-stacking task","Getting Humanoids to Move and Imitate THIS BEHAVIOR-BASED APPROACH TO STRUCTURING AND CONTROLLING COMPLEX ROBOTIC SYSTEMS USES IMITATION FOR INTERACTION AND LEARNING. THE USE OF BASIS BEHAVIORS, OR PRIMITIVES, LETS THESE HUMANOID ROBOTS DANCE THE MACARENA",,,,,core
359940759,2020-12-01T00:00:00,"abstract: The use of Artificial Intelligence in assistive systems is growing in application and efficiency. From self-driving cars, to medical and surgical robots and industrial tasked unsupervised co-robots; the use of AI and robotics to eliminate human error in high-stress environments and perform automated tasks is something that is advancing society’s status quo. Not only has the understanding of co-robotics exploded in the industrial world, but in research as well. The National Science Foundation (NSF) defines co-robots as the following: “...a robot whose main purpose is to work with people or other robots to accomplish a goal” (NSF, 1). The latest iteration of their National Robotics Initiative, NRI-2.0, focuses on efforts of creating co-robots optimized for ‘scalability, customizability, lowering barriers to entry, and societal impact’(NSF, 1). While many avenues have been explored for the implementation of co-robotics to create more efficient processes and sustainable lifestyles, this project’s focus was on societal impact co-robotics in the field of human safety and well-being. Introducing a co-robotics and computer vision AI solution for first responder assistance would help bring awareness and efficiency to public safety. The use of real-time identification techniques would create a greater range of awareness for first responders in high-stress situations. A combination of environmental features collected through sensors (camera and radar) could be used to identify people and objects within certain environments where visual impairments and obstructions are high (eg. burning buildings, smoke-filled rooms, ect.). Information about situational conditions (environmental readings, locations of other occupants, etc.) could be transmitted to first responders in emergency situations, maximizing situational awareness. This would not only aid first responders in the evaluation of emergency situations, but it would provide useful data for the first responder that would help materialize the most effective course of action for said situation",The Investigation of Low Cost Computer Vision Application for First Responder Co-robotics,,,,,core
334907965,2020-11-12T00:00:00,"One of the crucial problems in robotic swarm-based operation is to search and
neutralize heterogeneous targets in an unknown and uncertain environment,
without any communication within the swarm. Here, some targets can be
neutralized by a single robot, while others need multiple robots in a
particular sequence to neutralize them. The complexity in the problem arises
due to the scalability and information uncertainty, which restricts the robot's
awareness of the swarm and the target distribution. In this paper, this problem
is addressed by proposing a novel Context-Aware Deep Q-Network (CA-DQN)
framework to obtain communication free cooperation between the robots in the
swarm. Each robot maintains an adaptive grid representation of the vicinity
with the context information embedded into it to keep the swarm intact while
searching and neutralizing the targets. The problem formulation uses a
reinforcement learning framework where two Deep Q-Networks (DQNs) handle
'conflict' and 'conflict-free' scenarios separately. The self-play-in-based
approach is used to determine the optimal policy for the DQNs. Monte-Carlo
simulations and comparison studies with a state-of-the-art coalition formation
algorithm are performed to verify the performance of CA-DQN with varying
environmental parameters. The results show that the approach is invariant to
the number of detected targets and the number of robots in the swarm. The paper
also presents the real-time implementation of CA-DQN for different scenarios
using ground robots in a laboratory environment to demonstrate the working of
CA-DQN with low-power computing devices.Comment: ""For associated video file, refer to http://bit.ly/cadqnvideo","Context-Aware Deep Q-Network for Decentralized Cooperative
  Reconnaissance by a Robotic Swarm",http://arxiv.org/abs/2001.11710,,,,core
387279955,2020-12-09T00:00:00,"Quadrupedal robots are skillful at locomotion tasks while lacking
manipulation skills, not to mention dexterous manipulation abilities. Inspired
by the animal behavior and the duality between multi-legged locomotion and
multi-fingered manipulation, we showcase a circus ball challenge on a
quadrupedal robot, ANYmal. We employ a model-free reinforcement learning
approach to train a deep policy that enables the robot to balance and
manipulate a light-weight ball robustly using its limbs without any contact
measurement sensor. The policy is trained in the simulation, in which we
randomize many physical properties with additive noise and inject random
disturbance force during manipulation, and achieves zero-shot deployment on the
real robot without any adjustment. In the hardware experiments, dynamic
performance is achieved with a maximum rotation speed of 15 deg/s, and robust
recovery is showcased under external poking. To our best knowledge, it is the
first work that demonstrates the dexterous dynamic manipulation on a real
quadrupedal robot","Circus ANYmal: A Quadruped Learning Dexterous Manipulation with Its
  Limbs",http://arxiv.org/abs/2011.08811,,,,core
327074089,2020-07-27T00:00:00,"Learning-based approaches often outperform hand-coded algorithmic solutions
for many problems in robotics. However, learning long-horizon tasks on real
robot hardware can be intractable, and transferring a learned policy from
simulation to reality is still extremely challenging. We present a novel
approach to model-free reinforcement learning that can leverage existing
sub-optimal solutions as an algorithmic prior during training and deployment.
During training, our gated fusion approach enables the prior to guide the
initial stages of exploration, increasing sample-efficiency and enabling
learning from sparse long-horizon reward signals. Importantly, the policy can
learn to improve beyond the performance of the sub-optimal prior since the
prior's influence is annealed gradually. During deployment, the policy's
uncertainty provides a reliable strategy for transferring a simulation-trained
policy to the real world by falling back to the prior controller in uncertain
states. We show the efficacy of our Multiplicative Controller Fusion approach
on the task of robot navigation and demonstrate safe transfer from simulation
to the real world without any fine-tuning. The code for this project is made
publicly available at https://sites.google.com/view/mcf-nav/homeComment: Accepted for presentation at IROS2020. Project site available at
  https://sites.google.com/view/mcf-nav/hom","Multiplicative Controller Fusion: Leveraging Algorithmic Priors for
  Sample-efficient Reinforcement Learning and Safe Sim-To-Real Transfer",http://arxiv.org/abs/2003.05117,,,,core
333938973,2020-01-01T00:00:00,"This paper provides an overview of the current and near-future applications of Artificial Intelligence
(AI) in Medicine and Health Care and presents a classification according to their ethical and societal
aspects, potential benefits and pitfalls, and issues that can be considered controversial and are not
deeply discussed in the literature.
This work is based on an analysis of the state of the art of research and technology, including existing
software, personal monitoring devices, genetic tests and editing tools, personalized digital models,
online platforms, augmented reality devices, and surgical and companion robotics.
Motivated by our review, we present and describe the notion of “extended personalized medicine”,
we then review existing applications of AI in medicine and healthcare and explore the public
perception of medical AI systems, and how they show, simultaneously, extraordinary opportunities
and drawbacks that even question fundamental medical concepts. Many of these topics coincide
with urgent priorities recently defined by the World Health Organization for the coming decade. In
addition, we study the transformations of the roles of doctors and patients in an age of ubiquitous
information, identify the risk of a division of Medicine into “fake-based”, “patient-generated”, and
“scientifically tailored”, and draw the attention of some aspects that need further thorough analysis
and public debate",Inteligencia Artificial en Medicina y Salud: revisión y clasificación de las aplicaciones actuales y del futuro cercano y su impacto ético y social,https://core.ac.uk/download/333938973.pdf,,,,core
359945621,2020-09-15T00:00:00,"In this work we focus on improving the efficiency and generalisation of learned navigation strategies when transferred from its training environment to previously unseen ones. We present an extension of the residual reinforcement learning framework from the robotic manipulation literature and adapt it to the vast and unstructured environments that mobile robots can operate in. The concept is based on learning a residual control effect to add to a typical sub-optimal classical controller in order to close the performance gap, whilst guiding the exploration process during training for improved data efficiency. We exploit this tight coupling and propose a novel deployment strategy, switching Residual Reactive Navigation (sRRN), which yields efficient trajectories whilst probabilistically switching to a classical controller in cases of high policy uncertainty. Our approach achieves improved performance over end-to-end alternatives and can be incorporated as part of a complete navigation stack for cluttered indoor navigation tasks in the real world. The code and training environment for this project is made publicly available at https://sites.google.com/view/srrn/home.</p",Residual Reactive Navigation:Combining Classical and Learned Navigation Strategies for Deployment in Unknown Environments,,'Institute of Electrical and Electronics Engineers (IEEE)',10.1109/ICRA40945.2020.9197386,,core
323307134,2020-04-13T00:00:00,"The emergence of Artificial Intelligence (AI) is creating new dimensions and redefining the concept and meaning of work in industrial settings. Documented success has been reported where AI is transforming industrial scenes such as scaling large operation processes, speed of execution, flexibility of processes where rigid manufacturing by dumb robots is replaced with smart individualized production following real-time customer choices, decision-making in which a huge amount of data can be quickly available at the fingertips of workers on the factory floor or even prevent problems before they happen, and personalization where AI uses data to deliver personalized user experience. According to the market research firm Trac tica, the global AI software market is expected to experience massive growth in the coming years, with revenues increasing from around US 9.5 billion in 2018 to an expected US 118.6 billion by 2025",Future trends in I&M: Human-machine co-creation in the rise of AI,https://core.ac.uk/download/323307134.pdf,'Institute of Electrical and Electronics Engineers (IEEE)',10.1109/MIM.2020.9062691,,core
402913130,2020-10-09T08:35:20,"Driver Less Vision examines the tension and reality of AI and humans merging and diverging as they negotiate Seoul's unique urban landscape—challenging us to consider how we can design cities for the future of autonomous vehicles. 

Driver Less Vision aims to generate empathy between humans and non-humans, to construct the trust required for negotiations that will settle how we will live together. By overlapping human and machine’s perceptions, the installation helps to identify the areas of the city that will need to be redesigned in the immediate future.

Driver Less Vision is the immersive experience of becoming an autonomous, self-driving vehicle. It explores the untapped conflicts and disruptive effects on the built environment caused by the deployment of technologies for autonomous mobility. Currently, the visual stimuli that organizes traffic is designed for human perception. The arrival of driverless cars entails the emergence of a omnidirectional gaze that is required to negotiate existing visual codes. To assume that driverless cars will fully adapt to future conditions of the city, however, neglects the history of transformations in urban streetscapes associated with changes in vehicular technologies. Driver Less Vision is an attempt to understand how driverless cars will change the city by immersing the audience in an urban journey through the car’s point of view, seeing the streets of Seoul through overlapping and dissonant perceptions. 

The project was produced for the Seoul Biennale of Architecture and Urbanism in 2017, utilizing an eight meter diameter dome with 360 visuals developed with the generous support of University of Technology Sydney, Rice University and Ocular Robotics",Driver Less Vision,http://hdl.handle.net/10453/143196,Mediabus,,,core
327060943,2020-07-01T00:00:00,"This chapter discusses two performative robot projects that share a core ethos about the nature of human- centred and creative AI: Embodied Robots for Music (2018 – present) by Craig Vear and Amigóide (2010 – present) by Fabrizio Augusto Poltronieri. Human-centred AI (HC-AI) focuses on the design, development and deployment of intelligent systems that co-operate with humans in real-time in a ‘deep and meaningful way’ (MIT 2019). HC-AI is ‘defined by two goals: (1) the AI system must continually improve by learning from humans while (2) creating an effective and fulfilling human-robot interaction experience’ (Ibid.). The two projects discussed in this chapter both apply these core goals as a central philosophy from which the concepts of Creative AI and Experiential Learning in the context of performative robots are developed. At the centre of this discussion is the articulation of a shift in thinking of what constitutes Creative AI and new HC-AI forms of machine learning from inside the flow of the shared experience between robot and human. The first case study is Embodied Robots for Music which investigates the technical solutions and artistic potential of AI driven robots co-creating with an improvising human musician in real-time. The second Amigóide performs a range of functions according to a predetermined set of coded instructions in search of humans to engage into friendships. These projects are ongoing and are part of the Creative AI ResearchGroup in the Institute of Creative Technologies at De Montfort University ",Human-Centred and Creative AI in Performative Robots,,,,,core
327163425,2020-07-31T00:00:00,"One of the great promises of robot learning systems is that they will be able
to learn from their mistakes and continuously adapt to ever-changing
environments. Despite this potential, most of the robot learning systems today
are deployed as a fixed policy and they are not being adapted after their
deployment. Can we efficiently adapt previously learned behaviors to new
environments, objects and percepts in the real world? In this paper, we present
a method and empirical evidence towards a robot learning framework that
facilitates continuous adaption. In particular, we demonstrate how to adapt
vision-based robotic manipulation policies to new variations by fine-tuning via
off-policy reinforcement learning, including changes in background, object
shape and appearance, lighting conditions, and robot morphology. Further, this
adaptation uses less than 0.2% of the data necessary to learn the task from
scratch. We find that our approach of adapting pre-trained policies leads to
substantial performance gains over the course of fine-tuning, and that
pre-training via RL is essential: training from scratch or adapting from
supervised ImageNet features are both unsuccessful with such small amounts of
data. We also find that these positive results hold in a limited continual
learning setting, in which we repeatedly fine-tune a single lineage of policies
using data from a succession of new tasks. Our empirical conclusions are
consistently supported by experiments on simulated manipulation tasks, and by
52 unique fine-tuning experiments on a real robotic grasping system pre-trained
on 580,000 grasps.Comment: 8.5 pages, 9 figures. See video overview and experiments at
  https://youtu.be/pPDVewcSpdc and project website at
  https://ryanjulian.me/continual-fine-tunin","Never Stop Learning: The Effectiveness of Fine-Tuning in Robotic
  Reinforcement Learning",http://arxiv.org/abs/2004.10190,,,,core
334916064,2020-02-25T00:00:00,"Learning Machines is developing a flexible, cross-industry, advanced
analytics platform, targeted during stealth-stage at a limited number of
specific vertical applications. In this paper, we aim to integrate a general
machine system to learn a variant of tasks from simulation to real world. In
such a machine system, it involves real-time robot vision, sensor fusion, and
learning algorithms (reinforcement learning). To this end, we demonstrate the
general machine system on three fundamental tasks including obstacle avoidance,
foraging, and predator-prey robot. The proposed solutions are implemented on
Robobo robots with mobile device (smartphone with camera) as interface and
built-in infrared (IR) sensors. The agent is trained in a virtual environment.
In order to assess its performance, the learned agent is tested in the virtual
environment and reproduce the same results in a real environment. The results
show that the reinforcement learning algorithm can be reliably used for a
variety of tasks in unknown environments",Learning Machines from Simulation to Real World,http://arxiv.org/abs/2002.10853,,,,core
357554258,2020-04-24T00:00:00,"ABSTRACT In robotics research, perception is one of the most challenging tasks. In contrast to existing approaches that rely only on computer vision, we propose an alternative method for improving perception by learning from human teammates. To evaluate, we apply this idea to a door detection problem. A set of preliminary experiments has been completed using software agents with real vision data. Our results demonstrate that information inferred from teammate observations significantly improves the perception precision. Categories and Subject Descriptors I.2.11 [Distributed Artificial Intelligence]: Intelligent agents General Terms Human Factors Keywords Robot perception, robot-human hybrid teams BACKGROUND Robot perception is generally formulated as a problem of analyzing and interpreting various sensory inputs, e.g., camera feeds. In this paper, we approach robot perception from a completely different direction. Our approach utilizes a team setting where a robot collaborates with human teammates. Motivated by the fact that humans possess superior perception skills relative to their robotic counterparts, we investigate how a robot can take advantage of its teammate&apos;s perfect vision. In general, an agent acquires new information through perception, and in turn, the agent chooses actions based on the information acquired. Let us suppose that a robot has a mental model of its human teammate such that a causal relationship is specified between information and actions. Then, by understanding the human mental model of such decision making (or planning), the robot can infer what the human teammate has seen based on the human&apos;s behavior. In other words, an observation of a human teammate can be * This work was conducted (in part) through collaborative participation in the Robotics Consortium sponsored by the U. used as evidence to infer the information perceived by the human. This, in turn, can be used to reduce uncertainty in robot perception. In this paper, we specifically focus on a motivating problem of door detection in the following scenario. Consider a team consisting of a robot and a human performing a military operation in a hostile environment. According to intelligence, armed insurgents are hiding in an urban street. The team is deployed to cover the buildings in the surrounding area, focusing on doors from which the insurgents may try to egress. This is a stealth operation. We make two specific assumptions that are reasonable in a team context. First, observing a teammate is generally more manageable than perceiving an unfamiliar environment. Second, team members share common objectives in reaching the team&apos;s goals. PERCEPTION USING VISION This section describes a purely camera-based approach. First, we find a likely semantic image segmentation using a computer vision technique called stacked hierarchical labeling  It is not constrained by shape grammars and can model a more general class of objects, but its method of constructing a hierarchical segmentation does not convey semantic meaning at a finer detail, as would be necessary to detect doors on a building. It is, however, reliable in detecting buildings as a whole, significantly reducing the search space for detecting doors in the next step. Once buildings are identified, we can apply a broad feature detector to detect likely openings on the façade of the building. As i",Enhancing Robot Perception Using Human Teammates * (Extended Abstract),https://core.ac.uk/download/357554258.pdf,,,,core
334873096,2020-02-06T00:00:00,"Key challenges for the deployment of reinforcement learning (RL) agents in
the real world are the discovery, representation and reuse of skills in the
absence of a reward function. To this end, we propose a novel approach to learn
a task-agnostic skill embedding space from unlabeled multi-view videos. Our
method learns a general skill embedding independently from the task context by
using an adversarial loss. We combine a metric learning loss, which utilizes
temporal video coherence to learn a state representation, with an entropy
regularized adversarial skill-transfer loss. The metric learning loss learns a
disentangled representation by attracting simultaneous viewpoints of the same
observations and repelling visually similar frames from temporal neighbors. The
adversarial skill-transfer loss enhances re-usability of learned skill
embeddings over multiple task domains. We show that the learned embedding
enables training of continuous control policies to solve novel tasks that
require the interpolation of previously seen skills. Our extensive evaluation
with both simulation and real world data demonstrates the effectiveness of our
method in learning transferable skills from unlabeled interaction videos and
composing them for new tasks. Code, pretrained models and dataset are available
at http://robotskills.cs.uni-freiburg.deComment: Accepted at the 2020 IEEE International Conference on Robotics and
  Automation (ICRA). Video at https://www.youtube.com/watch?v=z8gG1k9kSqA
  Project page at http://robotskills.cs.uni-freiburg.d",Adversarial Skill Networks: Unsupervised Robot Skill Learning from Video,http://arxiv.org/abs/1910.09430,,,,core
289188336,2020-03-27T00:00:00,"Η παρούσα διδακτορική διατριβή αφορά τη μελέτη, ανάπτυξη και εφαρμογή, μεθόδων
Μηχανικής Μάθησης μέσω Παρατήρησης (Learning from Demonstration) με στόχο την
ρομποτική αναπαραγωγή δράσεων χειρισμού. Η μεθοδολογία αυτή στηρίζεται στην
δημιουργία μιας αντιστοίχισης (mapping) μεταξύ της κινηματικής του ανθρώπινου χεριού και
ενός ρομποτικού βραχίονα, ή πιο συγκεκριμένα μεταξύ του πολυδιάστατου χώρου των
κινήσεων του ανθρώπου (human actor) με τον επίσης πολυδιάστατο χώρο δράσης του
ρομπότ. Η συσχέτιση των ανθρώπινων ενεργειών με αντίστοιχες ρομποτικές, επιτυγχάνεται
μέσω μιας άδηλης αναπαράστασης, που ονομάζεται λανθάνουσα απεικόνιση χώρου (latent
space). Πιο συγκεκριμένα, μελετάμε την αμοιβαία αλληλεπίδραση της αντίληψης και της 
δράσης, προκειμένου να διδάξουμε τα ρομπότ μια ποικιλία από νέες κινήσεις χειρός. Ως εκ
τούτου, υλοποιήθηκε ένα μεθοδολογικό πλαίσιο μάθησης μέσω παρατήρησης, το οποίο
ονομάζεται IMFO (Imitation Framework by Observation), που διευκολύνει την αναπαραγωγή
μαθημένων και νέων κινήσεων χειρισμού από ένα ρομπότ (manipulation tasks) και,
παράλληλα, έχει ευρεία εφαρμογή σε σενάρια αλληλεπίδρασης ανθρώπου-ρομπότ (HRI) σε
καθημερινά περιβάλλοντα.
Επιπλέον, σε αυτή τη διατριβή, εξετάζουμε το ρόλο της χρονικής διάρκειας εκτέλεσης μιας
κίνησης μέσα από τη διαδικασία μάθησης από παρατήρηση, ενισχύοντας το διαμορφωμένο
πλαίσιο IMFO με την δυνατότητα αναπαράστασης και αναπαραγωγής τόσο των χωρικών όσο
και των χρονικών χαρακτηριστικών των ανθρώπινων κινήσεων. Σε αντίθεση με άλλες
μεθόδους μάθησης μέσω παρατήρησης (LfD) που περιγράφουν την εκτελούμενη δράση μόνο
με βάση τα χωρικά χαρακτηριστικά της, η προτεινόμενη μεθοδολογία ενισχύει την
αναπαραγωγή των χωροχρονικών πτυχών μιας κίνησης επιτρέποντας την αποτελεσματική
εφαρμογή της σε πιο σύνθετα σενάρια HRI, όπου η χρονική αλληλουχία των δράσεων είναι
σημαντική. Επιπρόσθετα, εισάγεται ένα σύνολο καλά καθορισμένων μετρικών αξιολόγησης
(evaluation metrics) για να αποτιμηθεί η εγκυρότητα της προτεινόμενης προσέγγισης
λαμβάνοντας υπόψη τη χρονική και χωρική συνέπεια των αναπαραγόμενων συμπεριφορών.
Μια αξιοσημείωτη επέκταση του προαναφερθέντος πλαισίου αναφέρεται στην εκμάθηση
της δύναμης που επιβάλλεται από τον χρήστη για την επιτυχημένη εκτέλεση λεπτών
χειρισμών. Αυτή η διαδικασία παρουσιάζεται επίσης στην παρούσα διατριβή μέσω ενός
νέου πλαισίου εποπτευόμενης μάθησης, το οποίο ονομάζεται SLF (Supervised Learning
scheme for Force-based manipulation). Το SLF διατυπώνεται ως μία διαδικασία τριών
σταδίων: (α) επιβλεπόμενη διαδικασία εκτέλεσης κινήσεων χειρισμού σε προσομοίωση για
την απόκτηση επαρκών δεδομένων, (β) διαδικασία εκπαίδευσης (training) για τη
διευκόλυνση της μάθησης κινήσεων χειρισμού με την κατάλληλη προσαρμογή του καρπού
και της δύναμη πιασίματος και μεταφοράς και (γ) εκτέλεση της κίνησης από ρομποτικό
βραχίονα σε προσομοίωση. Στη συνέχεια, με τη χρήση της μεθόδου sim-to-real transfer,
επιτυγχάνεται αναπαραγωγή των μαθημένων δράσεων σε πραγματικά περιβάλλοντα
γενικεύοντας την εφαρμογή του πλαισίου μάθησης σε επιπλέον συνθήκες χειρισμού
εύθραυστων αντικειμένων. Τα αποτελέσματα με τη χρήση του ρομποτικού βραχίονα YuMi,
σε πειράματα με διαφορετικά αντικείμενα με παρόμοιους συντελεστές τριβής, και
εναλλακτικές πόζες πιασίματος, αποδεικνύουν ότι το ρομπότ είναι σε θέση να αναπαράγει
αποτελεσματικά απαιτητικές κινήσεις μεταφοράς και χειρισμού μετά την ολοκλήρωση της
διαδικασίας μάθησης.
Συνοπτικά, η παρούσα διατριβή μελετά την διαδικασία μάθησης μέσω παρατήρησης
συνεισφέροντας με μια νέα προσέγγιση που εισάγει την μελέτη δράσεων χειρισμού
αντικειμένων μέσα από έναν χώρο μειωμένων διαστάσεων, για την εύκολη και συμπαγή
κωδικοποίηση των επιμέρους χαρακτηριστικών των δράσεων. Ταυτόχρονα μελετώνται τα
χρονικά χαρακτηριστικά των κινήσεων ώστε να ενισχυθεί η εφαρμογή της μεθόδου σε
σύνθετες, πραγματικές συνθήκες που απαιτούν χρονική ακρίβεια αναπαραγωγής. Τέλος, η
διαμόρφωση μιας γενικευμένης διαδικασίας εποπτευόμενης μάθησης για τον χειρισμό
εύθραυστων αντικείμενων αναβαθμίζει περαιτέρω το αρχικό πλαίσιο μάθησης.The current PhD thesis addresses the formulation and implementation of a methodological
framework for robot Learning from Demonstration (LfD). The latter refers to methodologies
that develop behavioral policies from example state-to-action mappings. To this
end, we study the reciprocal interaction of perception and action, in order to teach robots
a repertoire of novel action behaviors. Based on that, we design, develop and implement
a robust imitation framework, termed IMFO (IMitation Framework by Observation), that
facilitates imitation learning and relevant applications in human-robot interaction (HRI)
tasks. IMFO can cope with the reproduction of learned (i.e. previously observed) actions,
aswell as novel ones. Mapping of human actions to the respective robotic ones is achieved
via an indeterminate depiction, termed latent space representation. The latter accomplishes
a compact, yet precise abstraction of action trajectories, effectively representing
high dimensional raw actions in a low dimensional space.
Moreover, throughout this thesis, we examine the role of time in LfD by enhancing
the aforementioned framework with the notion of learning both the spatial and temporal
characteristics of human motions. Accordingly, learned actions can be subsequently reproduced
in the context of more complex time-informed HRI scenarios. Unlike previous
LfD methods that cope only with the spatial traits of an action, the formulated scheme
effectively encompasses spatial and temporal aspects. Extensive experimentation with a
variety of real robotic platforms demonstrates the robustness and applicability of the introduced
integrated LfD scheme.
Learned actions are reproduced under the high level control of a time-informed task
planner. During the implementation of the studied scenarios, temporal and physical constraints
may impose speed adaptations in the performed actions. The employed latent
space representation readily supports such variations, giving rise to novel actions in the
temporal domain. Experimental results demonstrate the effectiveness of the proposed
enhanced imitation scheme in the implementation of HRI scenarios. Additionally, a set
of well defined evaluation metrics are introduced to assess the validity of the proposed
approach considering the temporal and spatial consistency of the reproduced behaviors.
A noteworthy extension of the above regards force-based object grasping for executing
sensitive manipulation tasks. This is also treated in the current thesis via a novel supervised
learning scheme, termed SLF (Supervised Learning for Force-based manipulation).
SLF is formulated as a three-stage process: (a) supervised trial-execution in simulation
to acquire sufficient training data; (b) training to facilitate grasp learning with suitable
robot-arm pose and lifting force; (c) grasp execution in simulation. Subsequently, following
sim-to-real transfer, operation in real environments is achieved in addition to simulated
ones, generalizing also for objects not included in the trial sessions. The proposed
learning scheme is demonstrated in object lifting tasks where the applied force varies for
different objects with similar contact friction coefficients, and likewise the grasping pose.
Experimental results on the manipulator YuMi show that the robot is able to effectively
reproduce demanding lifting and manipulation tasks after learning is accomplished.
In summary, our thesis has studied LfD and has contributed with a novel approach that
introduced latent space representations to encode the action characteristics. A framework
implementation (IMFO) of our approach allowed extensive experimentation and also conduction
of HRI scenarios. The inclusion of temporal aspects in our approach enhanced it
to cope with complex, real-life interactions. Finally, the extension of IMFO with forcebased
grasping facilitated manipulation tasks with sensitive objects",Μάθηση μέσω παρατήρησης για την επίτευξη ρομποτικών δράσεων χειρισμού,,,,,core
289188335,2020-03-27T00:00:00,"Η εκτίμηση του Κέντρου Μάζας (CoM) διαδραματίζει κρίσιμο ρόλο στη ρομποτική βάδιση. Οι περισσότεροι σχεδιαστές κίνησης και ελεγκτές βάδισης πραγματικού χρό¬νου υποθέτουν ότι η θέση και η ταχύτητα του CoM είναι διαθέσιμες για ανατροφοδό¬τηση ανά πάσα στιγμή. Σε αυτή τη διατριβή παρουσιάζουμε έναν από τους πρώτους τρισδιάστατους εκτιμητές κατάστασης CoM για το περπάτημα των ανθρωποειδών ρομπότ. Ο προτεινόμενος εκτιμητής συνδυάζει αποτελεσματικά τις μετρήσεις από αισθητήρες πίεσης στα πόδια, κωδικοποιητές στις αρθρώσεις και αδρανειακής μο¬νάδας (IMU) στο σώμα με ένα Εκτεταμένο Φίλτρο Κάλμαν (EKF) για την ακριβή εκτίμηση τόσο της θέσης και της ταχύτητας του CoM αλλά και των εξωτερικών δυ¬νάμεων που δρουν πάνω σε αυτό. Επιπλέον, λαμβάνει υπόψιν την ανωμαλότητα του εδάφους και την στροφορμή του σώματος με αποτέλεσμα να συνδυάζει το μετωπικό με το πλευρικό επίπεδο κίνησης, χωρίς να βασίζεται σε αισθητήρες δύναμης / ροπής (F/T) στα πόδια.
Ωστόσο, είναι κοινή πρακτική να επιχειρείται η μετατροπή των μετρήσεων σε ένα αδρανειακό σύστημα αναφοράς ώστε η εκτίμηση του CoM να γίνεται σε σχέση με αυτό. Κατά συνέπεια, για την επίτευξη του παραπάνω είναι υποχρεωτικό να συνε¬κτιμηθούν η βάση και το πόδι στήριξης του ρομπότ. Για το σκοπό αυτό, επεκτείνουμε έναν καθιερωμένο στη βιβλιογραφία εκτιμητή αιωρούμενης μάζας με τη δυναμική του ποδιού στήριξης χρησιμοποιώντας μετρήσεις κινηματικής και αδρανειακής μονάδας με το Φίλτρο Κάλμαν Σφάλματος Κατάστασης (ESKF) για την κατάλληλη διαχείριση της υπερ-παραμετροποίησης των περιστροφών. Με αυτό το τρόπο,δημιουργείται ένα σύστημα σειριακής εκτίμησης κατάστασης που αποτελείται από έναν εκτιμητή βάσης και έναν εκτιμητή CoM το οποίο ονομάζουμε State Estimation RObot Walking (SEROW). Επιπλέον, για να διορθώσουμε την κινηματική απόκλιση που προκαλείται από την ολίσθηση των ποδιών κατά το περπάτημα, χρησιμοποιούμε μετρήσεις Οπτι¬κής Οδομετρίας (VO) και/ή Οδομετρίας LIDAR (LO). Δυστυχώς, τέτοιες μετρήσεις υποφέρουν από ακραίες τιμές σε ένα δυναμικό περιβάλλον, αφού κατά τον υπολογι¬σμό τους χρησιμοποιείται η υπόθεση ότι μόνο το ρομπότ βρίσκεται σε κίνηση και ο κόσμος γύρω του είναι στατικός. Για αυτό το λόγο, εισάγουμε το Σθεναρό Γκαουσια-νό Φίλτρο Κάλμαν Σφάλματος Κατάστασης (RGESKF) για την αυτόματη ανίχνευση και απόρριψη των ακραίων μετρήσεων. Το προτεινόμενο φίλτρο δεν βασίζεται σε πρότερη γνώση σχετικά με τις κατανομές των μετρήσεων και δεν χρησιμοποιεί ειδι¬κά ρυθμισμένα κατώφλια. Ως εκ τούτου,το SEROW γίνεται ένα σθεναρό σύστημα εκτίμησης κατάστασης, κατάλληλο για δυναμικά ανθρώπινα περιβάλλοντα. Προ¬κειμένου να ενισχυθούν περαιτέρω οι ερευνητικές προσπάθειες, οο SEROW δίνεται ελεύθερα στη ρομποτική κοινότητα ως ένα πακέτο ROS/C++ ανοικτού κώδικα.
Τα σύγχρονα συστήματα ελέγχου και εκτίμησης κατάστασης ανθρωποειδών ρο¬μπότ υποθέτουν ότι η κατάσταση επαφής ποδιών-εδάφους είναι γνωστή εκ των προ¬τέρων. Η ανίχνευση τέτοιων επαφών είναι ένα σημαντικό και σε μεγάλο βαθμό ανεξερεύνητο θέμα στη σύγχρονη ρομποτική έρευνα. Σε αυτή τη διατριβή, διατυ¬πώνουμε μια ευρύτερη ερώτηση: σε ποια φάση βάδισης βρίσκεται το ρομπότ; Ποςς το σκοπό αυτό, προτείνουμε ένα ολιστικό πλαίσιο βασισμένο σε μη-επιβλεπόμενη μάθηση από δεδομένα ιδιοδεκτικής αίσθησης που αντιμετωπίζει με ακρίβεια και α¬ποτελεσματικότητα αυτό το πρόβλημα. Συγκεκριμένα, ανιχνεύουμε με ακρίβεια μια από τις τρεις φάσεις βάδισης, την Αριστερή Υποστήριξη (LSS), την Διπλή Υποστήριξη (DS) και τη Δεξιά Υποστήριξη (RSS), χρησιμοποιώντας μετρήσεις από κωδικοποιητές, IMU και F/T. Αρχικά, πραγματοποιείται μείωση των διαστάσεων με Ανάλυση Κύριων Στοιχείων (PCA) ή με αυτόματους κωδικοποιητές ώστε να εξαχθούν χρήσιμα χαρα¬κτηριστικά, μια συμπαγής αναπαράσταση και να μειωθεί ο θόρυβος στα δεδομένα. Στη συνέχεια, πραγματοποιείται μια ομαδοποίηση στον χώρο χαμηλών διαστάσεων με Γκαουσιανά Μοντέλα Μίγματος (GMMs). Ως αποτέλεσμα λαμβάνονται τρία πυ¬κνά συμπλέγματα που αντιστοιχούν στις φάσεις της βάδισης. Αυτό σημαίνει ότι η δυναμική της φάσης του βαδίσματος είναι χαμηλής διάστασης το οποίο λειτουργεί ως άλλη μια ένδειξη στο ότι ολόκληρη η διαδικασία της βάδισης είναι χαμηλής διά¬στασης. Επιπλέον, δεδομένου ότι το προτεινόμενο πλαίσιο χρησιμοποιεί μετρήσεις από αισθητήρες που είναι συνήθως διαθέσιμοι στα σημερινά ανθρωποειδή ρομπότ, προσφέρουμε στη ρομποτική κοινότητα το Gait-Phase Estimation Module (GEM), μια ανοικτού κώδικα εφαρμογή σε ROS/Python.
Το SEROW και το GEM έχουν αξιολογηθεί ποσοτικά και ποιοτικά αφορικά με την ακρίβεια και την αποδοτικότητα τους τόσο σε προσομοίωση όσο και σε πραγματικές συνθήκες.Αρχικά , χρησιμοποιήθηκε ένα προσομοιωμένο ρομπότ στο MATLAB και το ανθρωποειδές ρομπότ Valkyrie της NASA στο ROS/Gazebo για να τεκμηριωθούν τα προτεινόμενα σχήματα στο βάδισμα πάνω σε ανομοιόμορφο/ανώμαλο έδαφος. τηη συνέχεια, τα προτεινόμενα σχήματα ενσωματώθηκαν στο α) μικρού μεγέθους ανθρω-ποειδές ρομπότ NAO v4.0 και β) στο πλήρους μεγέθους ανθρωποειδές WALK-MAN v2.0 για περεταίρω πειραματική επικύρωση. Με το NAO, οο SEROW εφαρμόστηκε στο ρομπότ για να παράσχει την απαραίτητη ανατροφοδότηση στον σχεδιασμό της κίνησης και τη σταθεροποίηση του βηματισμού σε πραγματικό χρόνο. Με αυτό το τρόπο επιτεύχθηκε πολυκατευθυντική βάδιση ακόμη και σε εξωτερικά/ανομοιογενή εδάφη. Επιπλέον,το SEROW χρησιμοποιήθηκε στον σχεδιασμό βημάτων για την πλοήγηση και επίσης στο Visual SLAM με το ίδιο ρομπότ. Όσον αφορά το WALK¬MAN v2.0, το SEROW εφαρμόστηκε με δεδομένα κινηματικής, αδρανειακής μονάδας και F/T για να παρέχει ανατροφοδότηση βάσης και CoM σε πραγματικό χρόνο. Στην εκτίμηση λήφθηκε υπόψη και το VO για την διόρθωση της κινηματικής απόκλισης κατά το περπάτημα. Με αυτό το τρόπο διευκολύνεται σημαντικά ο πιθανός σχεδια¬σμός βημάτων. Τλοςς, το GEM χρησιμοποιήθηκε επίσης για την εκτίμηση της φάσης της βάδισης στο δυναμικό περπάτημα του WALK-MAN.
Συνοψίζοντας, σε αυτή τη διατριβή προτείνεται ένας σθεναρός μη-γραμμικός ε¬κτιμητής κατάστασης για το βάδισμα ανθρωποειδών ρομπότ. Παρόλα αυτά, το προ¬τεινόμενο σύστημα μπορεί εύκολα να επεκταθεί και σε άλλους τύπους ρομπότ με πόδια, όπως τα τετράποδα, μιας και διαθέτουν τις ίδιες βασικές αρχές κίνησης.Center of Mass (CoM) estimation realizes a crucial role in legged locomotion. Most walking
pattern generators and real-time gait stabilizers commonly assume that the CoM position
and velocity are available for feedback. In this thesis we present one of the first
3D-CoM state estimators for humanoid robot walking. The proposed estimation scheme
fuses effectively joint encoder, inertial, and feet pressure measurements with an Extended
Kalman Filter (EKF) to accurately estimate the 3D-CoM position, velocity, and external
forces acting on the CoM. Furthermore, it directly considers the presence of uneven terrain
and the body’s angular momentum rate and thus effectively couples the frontal with
the lateral plane dynamics, without relying on feet Force/Torque (F/T) sensing.
Nevertheless, it is common practice to transform the measurements to a world frame
of reference and estimate the CoM with respect to the world frame. Consequently, the
robot’s base and support foot pose are mandatory and need to be co-estimated. To this
end, we extend a well-established in literature floating mass estimator to account for the
support foot dynamics and fuse kinematic-inertial measurements with the Error State
Kalman Filter (ESKF) to appropriately handle the overparametrization of rotations. In
such a way, a cascade state estimation scheme consisting of a base and a CoM estimator
is formed and coined State Estimation RObot Walking (SEROW). Additionally, we employ
Visual Odometry (VO) and/or LIDAR Odometry (LO) measurements to correct the kinematic
drift caused by slippage during walking. Unfortunately, such measurements suffer
from outliers in a dynamic environment, since frequently it is assumed that only the
robot is inmotion and the world around is static. Thus, we introduce the Robust Gaussian
ESKF (RGESKF) to automatically detect and reject outliers without relying on any prior
knowledge on measurement distributions or finely tuned thresholds. Therefore, SEROW
is robustified and is suitable for dynamic human environments. In order to reinforce further
research endeavors, SEROW is released to the robotic community as an open-source
ROS/C++ package.
Up to date control and state estimation schemes readily assume that feet contact status
is known a priori. Contact detection is an important and largely unexplored topic in
contemporary humanoid robotics research. In this thesis, we elaborate on a broader question:
in which gait phase is the robot currently in? To this end, we propose a holistic framework
based on unsupervised learning from proprioceptive sensing that accurately and efficiently
addresses this problem. More specifically, we robustly detect one of the three gaitphases,
namely Left Single Support (LSS), Double Support (DS), and Right Single Support (RSS) utilizing joint encoder, IMU, and F/T measurements. Initially, dimensionality reduction
with Principal Components Analysis (PCA) or autoencoders is performed to extract
useful features, obtain a compact representation, and reduce the noise. Next, clustering
is performed on the low-dimensional latent space with GaussianMixtureModels (GMMs)
and three dense clusters corresponding to the gait-phases are obtained. Interestingly, it is
demonstrated that the gait phase dynamics are low-dimensional which is another indication
pointing towards locomotion being a low dimensional skill. Accordingly, given that
the proposed framework utilizes measurements fromsensors that are commonly available
on humanoids nowadays, we offer the Gait-phase Estimation Module (GEM), an opensource
ROS/Python implementation to the robotic community.
SEROW and GEM have been quantitatively and qualitatively assessed in terms of accuracy
and efficiency both in simulation and under real-world conditions. Initially, a simulated
robot in MATLAB and NASA’s Valkyrie humanoid robot in ROS/Gazebo were employed
to establish the proposed schemes with uneven/rough terrain gaits. Subsequently,
the proposed schemes were integrated on a) the small size NAO humanoid robot v4.0 and
b) the adult size WALK-MAN v2.0 for experimental validation. With NAO, SEROW was implemented
on the robot to provide the necessary feedback for motion planning and realtime
gait stabilization to achieve omni-directional locomotion even on outdoor/uneven
terrains. Additionally, SEROW was used in footstep planning and also in Visual SLAM
with the same robot. Regarding WALK-MAN v2.0, SEROW was executed onboard with
kinematic-inertial and F/T data to provide base and CoM feedback in real-time. Furthermore,
VO has also been considered to correct the kinematic drift while walking and facilitate
possible footstep planning. GEM was also employed to estimate the gait phase in
WALK-MAN’s dynamic gaits.
Summarizing, a robust nonlinear state estimator is proposed for humanoid robot walking.
Nevertheless, this scheme can be readily extended to other type of legged robots such as quadrupeds, since they share the same fundamental principles",Σθεναρή μη γραμμική εκτίμηση κατάστασης ανθρωποειδών ρομπότ,,,,,core
356660379,2020-11-24T00:00:00,"Most control methods deployed in lower extremity rehabilitation robots cannot automatically adjust to different gait cycle stages and different rehabilitation training modes for different impairment subjects. This article presents a continuous seamless assist-as-needed control method based on sliding mode adaptive control. A forgetting factor is introduced, and a small trajectory deviation from reference normal gait trajectory is used to learn the rehabilitation level of a human subject in real time. The assistance torque needed to complete the reference normal gait trajectory is learned through radial basis function neural networks, so that the rehabilitation robot can adaptively provide the assistance torque according to subject’s needs. The performance and efficiency of this adaptive seamless assist-as-needed control scheme are tested and validated by 12 volunteers on a rehabilitation robot prototype. The results show that the proposed control scheme could adaptively reduce the robotic assistance according to subject’s rehabilitation level, and the robotic assistance torque depends on the forgetting factor and the active participation level of subjects",An adaptive seamless assist-as-needed control scheme for lower extremity rehabilitation robots,https://core.ac.uk/download/356660379.pdf,'SAGE Publications',10.1177/0959651820970720,,core
146481787,2018-01-16T00:00:00,"Recent applications of deep learning to navigation have generated end-to-end
navigation solutions whereby visual sensor input is mapped to control signals
or to motion primitives. The resulting visual navigation strategies work very
well at collision avoidance and have performance that matches traditional
reactive navigation algorithms while operating in real-time. It is accepted
that these solutions cannot provide the same level of performance as a global
planner. However, it is less clear how such end-to-end systems should be
integrated into a full navigation pipeline. We evaluate the typical end-to-end
solution within a full navigation pipeline in order to expose its weaknesses.
Doing so illuminates how to better integrate deep learning methods into the
navigation pipeline. In particular, we show that they are an efficient means to
provide informed samples for sample-based planners. Controlled simulations with
comparison against traditional planners show that the number of samples can be
reduced by an order of magnitude while preserving navigation performance.
Implementation on a mobile robot matches the simulated performance outcomes.Comment: 7 pages, 6 figure","Learning to Navigate: Exploiting Deep Networks to Inform Sample-Based
  Planning During Vision-Based Navigation",http://arxiv.org/abs/1801.05132,,,,core
304935586,2018-06-30T22:22:06,"Advanced autonomous robotics space missions rely heavily on the flawless interaction of complex hardware, multiple sensors, and a mission-critical software system.  This software system consists of an operating system, device drivers, controllers, and executives; recently highly complex AI-based autonomy software have also been introduced. Prior to launch, this software has to undergo rigorous verification and validation (V&V).  Nevertheless, dormant software bugs, failing sensors, unexpected hardware-software interactions, and unanticipated environmental conditions—likely on a space exploration mission—can cause major software faults that can endanger the entire mission.

Our Integrated Software Health Management (ISWHM) system continuously monitors the hardware sensors and the software in real-time. The ISWHM uses Bayesian networks, compiled to arithmetic circuits, to model software and hardware interactions. Advanced reasoning algorithms using arithmetic circuits not only enable the ISWHM to handle large, hierarchical models that are necessary in the realm of complex autonomous systems, but also enable efficient execution on small embedded processors. The latter capability is of extreme importance for small (mobile) autonomous units with limited computational power and low telemetry bandwidth.  In this paper, we discuss the requirements of ISWHM.  As our initial demonstration platform, we use a primitive Lego rover. A Lego 
Mindstorms microcontroller is used to implement a highly simplified autonomous rover driving system, running on the OSEK real-time operating system. We demonstrate that our ISWHM, running on this small embedded microcontroller, can perform fault detection as well as on-board reasoning for advanced diagnosis and root-cause detection in real time",Software and System Health Management for Autonomous Robotics Missions,,10.1184/r1/6710654.v1,,,core
328259578,2018-09-03T00:00:00,"Real estate needs to improve its adoption of disruptive technologies to move from traditional to smart real estate (SRE). This study reviews the adoption of disruptive technologies in real estate. It covers the applications of nine such technologies, hereby referred to as the Big9. These are: drones, the internet of things (IoT), clouds, software as a service (SaaS), big data, 3D scanning, wearable technologies, virtual and augmented realities (VR and AR), and artificial intelligence (AI) and robotics. The Big9 are examined in terms of their application to real estate and how they can furnish consumers with the kind of information that can avert regrets. The review is based on 213 published articles. The compiled results show the state of each technology’s practice and usage in real estate. This review also surveys dissemination mechanisms, including smartphone technology, websites and social media-based online platforms, as well as the core components of SRE: sustainability, innovative technology and user centredness. It identifies four key real estate stakeholders—consumers, agents and associations, government and regulatory authorities, and complementary industries—and their needs, such as buying or selling property, profits, taxes, business and/or other factors. Interactions between these stakeholders are highlighted, and the specific needs that various technologies address are tabulated in the form of a what, who and how analysis to highlight the impact that the technologies have on key stakeholders. Finally, stakeholder needs as identified in the previous steps are matched theoretically with six extensions of the traditionally accepted technology adoption model (TAM), paving the way for a smoother transition to technology-based benefits for consumers. The findings pertinent to the Big9 technologies in the form of opportunities, potential losses and exploitation levels (OPLEL) analyses highlight the potential utilisation of each technology for addressing consumers’ needs and minimizing their regrets. Additionally, the tabulated findings in the form of what, how and who links the Big9 technologies to core consumers’ needs and provides a list of resources needed to ensure proper information dissemination to the stakeholders. Such high-quality information can bridge the gap between real estate consumers and other stakeholders and raise the state of the industry to a level where its consumers have fewer or no regrets. The study, being the first to explore real estate technologies, is limited by the number of research publications on the SRE technologies that has been compensated through incorporation of online reports","A Systematic Review of Smart Real Estate Technology: Drivers of, and Barriers to, the Use of Digital Disruptive Technologies and Online Platforms",http://www.mdpi.com/2071-1050/10/9/3142,10.3390/su10093142,'MDPI AG',,core
160782526,2018-08-11T00:00:00,"In this paper, we present a decentralized sensor-level collision avoidance
policy for multi-robot systems, which shows promising results in practical
applications. In particular, our policy directly maps raw sensor measurements
to an agent's steering commands in terms of the movement velocity. As a first
step toward reducing the performance gap between decentralized and centralized
methods, we present a multi-scenario multi-stage training framework to learn an
optimal policy. The policy is trained over a large number of robots in rich,
complex environments simultaneously using a policy gradient based reinforcement
learning algorithm. The learning algorithm is also integrated into a hybrid
control framework to further improve the policy's robustness and effectiveness.
  We validate the learned sensor-level collision avoidance policy in a variety
of simulated and real-world scenarios with thorough performance evaluations for
large-scale multi-robot systems. The generalization of the learned policy is
verified in a set of unseen scenarios including the navigation of a group of
heterogeneous robots and a large-scale scenario with 100 robots. Although the
policy is trained using simulation data only, we have successfully deployed it
on physical robots with shapes and dynamics characteristics that are different
from the simulated agents, in order to demonstrate the controller's robustness
against the sim-to-real modeling error. Finally, we show that the
collision-avoidance policy learned from multi-robot navigation tasks provides
an excellent solution to the safe and effective autonomous navigation for a
single robot working in a dense real human crowd. Our learned policy enables a
robot to make effective progress in a crowd without getting stuck. Videos are
available at https://sites.google.com/view/hybridmrc","Fully Distributed Multi-Robot Collision Avoidance via Deep Reinforcement
  Learning for Safe and Efficient Navigation in Complex Scenarios",http://arxiv.org/abs/1808.03841,,,,core
186300971,2018-12-18T00:00:00,"Conventional feedback control methods can solve various types of robot
control problems very efficiently by capturing the structure with explicit
models, such as rigid body equations of motion. However, many control problems
in modern manufacturing deal with contacts and friction, which are difficult to
capture with first-order physical modeling. Hence, applying control design
methodologies to these kinds of problems often results in brittle and
inaccurate controllers, which have to be manually tuned for deployment.
Reinforcement learning (RL) methods have been demonstrated to be capable of
learning continuous robot controllers from interactions with the environment,
even for problems that include friction and contacts. In this paper, we study
how we can solve difficult control problems in the real world by decomposing
them into a part that is solved efficiently by conventional feedback control
methods, and the residual which is solved with RL. The final control policy is
a superposition of both control signals. We demonstrate our approach by
training an agent to successfully perform a real-world block assembly task
involving contacts and unstable objects.Comment: 7 page",Residual Reinforcement Learning for Robot Control,http://arxiv.org/abs/1812.03201,,,,core
146478099,2018-10-15T00:00:00,"The rapid growth of IoT era is shaping the future of mobile services.
Advanced communication technology enables a heterogeneous connectivity where
mobile devices broadcast information to everything. Mobile applications such as
robotics and vehicles connecting to cloud and surroundings transfer the
short-range on-board sensor perception system to long-range mobile-sensing
perception system. However, the mobile sensing perception brings new challenges
for how to efficiently analyze and intelligently interpret the deluge of IoT
data in mission- critical services. In this article, we model the challenges as
latency, packet loss and measurement noise which severely deteriorate the
reliability and quality of IoT data. We integrate the artificial intelligence
into IoT to tackle these challenges. We propose a novel architecture that
leverages recurrent neural networks (RNN) and Kalman filtering to anticipate
motions and interac- tions between objects. The basic idea is to learn
environment dynamics by recurrent networks. To improve the robustness of IoT
communication, we use the idea of Kalman filtering and deploy a prediction and
correction step. In this way, the architecture learns to develop a biased
belief between prediction and measurement in the different situation. We
demonstrate our approach with synthetic and real-world datasets with noise that
mimics the challenges of IoT communications. Our method brings a new level of
IoT intelligence. It is also lightweight compared to other state-of-the-art
convolutional recurrent architecture and is ideally suitable for the
resource-limited mobile applications.Comment: 7 pages, 6 figures, 1 tabl","Deep Anticipation: Light Weight Intelligent Mobile Sensing in IoT by
  Recurrent Architecture",http://arxiv.org/abs/1801.01444,,,,core
186278550,2018-10-16T00:00:00,"A general-purpose intelligent robot must be able to learn autonomously and be
able to accomplish multiple tasks in order to be deployed in the real world.
However, standard reinforcement learning approaches learn separate
task-specific policies and assume the reward function for each task is known a
priori. We propose a framework that learns event cues from off-policy data, and
can flexibly combine these event cues at test time to accomplish different
tasks. These event cue labels are not assumed to be known a priori, but are
instead labeled using learned models, such as computer vision detectors, and
then `backed up' in time using an action-conditioned predictive model. We show
that a simulated robotic car and a real-world RC car can gather data and train
fully autonomously without any human-provided labels beyond those needed to
train the detectors, and then at test-time be able to accomplish a variety of
different tasks. Videos of the experiments and code can be found at
https://github.com/gkahn13/CAPsComment: Accepted to the Conference on Robot Learning (CoRL) 2018. Video at
  https://youtu.be/lOLT7zifEk","Composable Action-Conditioned Predictors: Flexible Off-Policy Learning
  for Robot Navigation",http://arxiv.org/abs/1810.07167,,,,core
200965017,2018-01-01T00:00:00,"We present an open-source accessory for the NAO robot, which enables to test computationally demanding algorithms in an external platform while preserving robot’s autonomy and mobility. The platform has the form of a backpack, which can be 3D printed and replicated, and holds an ODROID XU4 board to process algorithms externally with ROS compatibility. We provide also a software bridge between the B-Human’s framework and ROS to have access to the robot’s sensors close to real-time. We tested the platform in several robotics applications such as data logging, visual SLAM, and robot vision with deep learning techniques. The CAD model, hardware specifications and software are available online for the benefit of the community",The NAO Backpack: An open-hardware add-on for fast software development with the NAO robot,,10.1007/978-3-030-00308-1_25,'Springer Science and Business Media LLC',,core
189835451,2018-06-29T00:00:00,"Visual understanding of 3-D environments in real time, at low power, is a huge computational challenge. Often referred to as simultaneous localization and mapping (SLAM), it is central to applications spanning domestic and industrial robotics, autonomous vehicles, and virtual and augmented reality. This paper describes the results of a major research effort to assemble the algorithms, architectures, tools, and systems software needed to enable delivery of SLAM, by supporting applications specialists in selecting and configuring the appropriate algorithm and the appropriate hardware, and compilation pathway, to meet their performance, accuracy, and energy consumption goals. The major contributions we present are: 1) tools and methodology for systematic quantitative evaluation of SLAM algorithms; 2) automated, machine-learning-guided exploration of the algorithmic and implementation design space with respect to multiple objectives; 3) end-to-end simulation tools to enable optimization of heterogeneous, accelerated architectures for the specific algorithmic requirements of the various SLAM algorithmic approaches; and 4) tools for delivering, where appropriate, accelerated, adaptive SLAM solutions in a managed, JIT-compiled, adaptive runtime context","Navigating the landscape for real-time localisation and mapping for robotics, virtual and augmented reality",,10.1109/JPROC.2018.2856739,'Institute of Electrical and Electronics Engineers (IEEE)',"[{'title': 'Proceedings of the IEEE', 'identifiers': ['0018-9219', 'issn:0018-9219']}]",core
158414686,2018-04-09T17:56:06,"Robots that use learned  perceptual  models in the real world must be able to safely handle cases where they are forced to make decisions in scenarios that are unlike any of their training  examples. However,  state-of-the-art  deep  learning methods are known to produce erratic or unsafe predictions when faced with novel inputs. Furthermore, recent ensemble, bootstrap and dropout methods for quantifying neural network uncertainty may not efficiently provide accurate uncertainty estimates when queried  with  inputs  that  are  very  different  from  their  training data. Rather than unconditionally trusting the predictions of a neural network for unpredictable real-world data, we use an autoencoder  to recognize when a query is novel, and revert to a safe prior behavior. With this capability,  we can deploy an autonomous deep learning system in arbitrary environments, without concern for whether it has received the appropriate training. We demonstrate our method with a vision-guided robot that can leverage its deep neural network to navigate 50% faster than  a  safe  baseline  policy in familiar types of  environments, while  reverting  to  the prior behavior in novel environments so that it can safely collect additional training data and continually improve. A video illustrating our approach is available at: http://groups.csail.mit.edu/rrg/videos/safe visual navigation",Safe Visual Navigation via Deep Learning and Novelty Detection,,10.15607/RSS.2017.XIII.064,'Robotics: Science and Systems Foundation',,core
322371101,2018-01-01T00:00:00,"Neural networks have long been a promising model for creating high performance robotic systems, from robot navigation and SLAM to modern deep learning techniques for tasks like manipulation. Traditional neural network systems typically relied heavily on a large number of hand-tuned parameters, while many modern implementations perform end-to-end learning, often with extreme data and computational requirements. Past work has focused on achieving high performance in real world environments, but with extensive hand tuning. In this paper, we instead present a new framework for automatically calibrating and optimising the performance of a biologically inspired neural network SLAM system. This framework combines a preset network structure with learning procedures. We use simulations with realistic noise to demonstrate the system's ability to learn the basic components of SLAM: odometry integration, landmark learning and landmark-driven relocalisation. We also show the framework is able to calibrate a large range of network sizes, allowing rapid development and deployment of a bio-inspired SLAM system. Our work serves as a bridging contribution between traditional hand-crafted neural networks and modern end-to-end learning approaches.</p",Automatic calibration of a biologically inspired neural network for robot SLAM,,,Australian Robotics and Automation Association (ARAA),,core
211558043,2018-01-01T00:00:00,"Objective. The objective of this work is to present gumpy, a new free and open source Python toolbox designed for hybrid brain-computer interface (BCI). Approach. Gumpy provides state-of-the-art algorithms and includes a rich selection of signal processing methods that have been employed by the BCI community over the last 20 years. In addition, a wide range of classification methods that span from classical machine learning algorithms to deep neural network models are provided. Gumpy can be used for both EEG and EMG biosignal analysis, visualization, real-time streaming and decoding. Results. The usage of the toolbox was demonstrated through two different offline example studies, namely movement prediction from EEG motor imagery, and the decoding of natural grasp movements with the applied finger forces from surface EMG (sEMG) signals. Additionally, gumpy was used for real-time control of a robot arm using steady-state visually evoked potentials (SSVEP) as well as for real-time prosthetic hand control using sEMG. Overall, obtained results with the gumpy toolbox are comparable or better than previously reported results on the same datasets. Significance. Gumpy is a free and open source software, which allows end-users to perform online hybrid BCIs and provides different techniques for processing and decoding of EEG and EMG signals. More importantly, the achieved results reveal that gumpy’s deep learning toolbox can match or outperform the state-of-the-art in terms of accuracy. This can therefore enable BCI researchers to develop more robust decoding algorithms using novel techniques and hence chart a route ahead for new BCI improvements",Gumpy: a python toolbox suitable for hybrid brain-computer interfaces,https://core.ac.uk/download/211558043.pdf,10.1088/1741-2552/aae186,'IOP Publishing',,core
334987485,2018-09-15T00:00:00,"This dataset is available for download as a .zip file named Trash_ICRA19.zip. Within the compressed folder are both the dataset  and configurations for testing the datset with deep learning algorithms. There is a README in the top directory, and in most lower directories, explaining the files and directories in its directory.This data was sourced from the J-EDI dataset of marine debris. The videos that comprise that dataset vary greatly in quality, depth, objects in scenes, and the cameras used. They contain images of many different types of marine debris, captured from real-world environments, providing a variety of objects in different states of decay, occlusion, and overgrowth. Additionally, the clarity of the water and quality of the light vary significantly from video to video. These videos were processed to extract 5,700 images, which comprise this dataset, all labeled with bounding boxes on instances of trash, biological objects such as plants and animals, and ROVs. The eventual goal is to develop efficient and accurate trash detection methods suitable for onboard robot deployment. It is our hope that the release of this dataset will facilitate further research on this challenging problem, bringing the marine robotics community closer to a solution for the urgent problem of autonomous trash detection and removal",Trash-ICRA19: A Bounding Box Labeled Dataset of Underwater Trash,,10.13020/x0qn-y082,'British Editorial Society of Bone & Joint Surgery',,core
157801149,2018-04-06T00:00:00,"In the context of Human-Robot Interaction (HRI), face Re-Identification (face Re-ID) aims to verify if certain detected faces have already been observed by robots. The ability of distinguishing between different users is crucial in social robots as it will enable the robot to tailor the interaction strategy toward the users’ individual preferences. So far face recognition research has achieved great success, however little attention has been paid to the realistic applications of Face Re-ID in social robots. In this paper, we present an effective and unsupervised face Re-ID system which simultaneously re-identifies multiple faces for HRI. This Re-ID system employs Deep Convolutional Neural Networks to extract features, and an online clustering algorithm to determine the face's ID. Its performance is evaluated on two datasets: the TERESA video dataset collected by the TERESA robot, and the YouTube Face Dataset (YTF Dataset). We demonstrate that the optimised combination of techniques achieves an overall 93.55% accuracy on TERESA dataset and an overall 90.41% accuracy on YTF dataset. We have implemented the proposed method into a software module in the HCI^2 Framework [1] for it to be further integrated into the TERESA robot [2] , and has achieved real-time performance at 10–26 Frames per second",A real-time and unsupervised face re-identification system for human-robot interaction,,10.1016/j.patrec.2018.04.009,'Elsevier BV',"[{'title': 'Pattern Recognition Letters', 'identifiers': ['issn:0167-8655', '0167-8655']}]",core
203960489,15/10/2018,"2018-10-16Cognitive architectures model fixed structures underlying intelligence and seek to heed the original goal of AI—a working implementation of a full cognitive system in aid of creating synthetic agents with human capabilities. Sigma is a cognitive architecture, developed with the immediate aim of supporting real time needs of intelligent agents, robots and virtual humans. In Sigma, this requirement manifests as a system whose development heuristically is guided by knowledge about human cognition with the ultimate desire to explain human intelligence at an appropriate level of abstraction. ❧ Spoken language processing is an important cognitive capability and yet not addressed by existing cognitive architectures. This is indicative of the mixed—symbolic and probabilistic—nature of the speech problem. Sigma, guided in its development by a core set of desiderata that are an evolution of the desiderata implicit in Newell’s Unified Theories of Cognition, presents a unique opportunity to attempt the integration of spoken language understanding in a cognitive architecture. Such attempt is an exercise to push cognitive architectures beyond what they are capable of, taking a first step towards enabling an architecturally based theory of spoken language understanding—deconstructed in terms of the interplay between various cognitive and sub-cognitive capabilities that play an important role in the comprehension process. ❧ This dissertation investigates the issues involved in integration of incremental speech and language processing, with cognition, in aid of spoken language understanding, guided by the desiderata driving Sigma’s development. The space of possibilities this integration enables is explored and a suitable spoken language understanding task is chosen to evaluate the key properties of the theory of spoken language understanding developed in Sigma. Speech signal obtained from an external speech front end is combined with linguistic knowledge in the form of phonetic, lexical and semantic knowledge sources. The linguistic input is converted into meaning using a Natural Language Understanding (NLU) scheme implemented on top of the architecture. ❧ In addition to phonetic, lexical and semantic processing, language processing involves a syntactic component. Probabilistic context free grammar parsing is an important form of grammar processing that has not been possible to realize in cognitive architectures. Probabilistic context free grammar parsing poses a challenge to Sigma’s grounding in graphical models. Sigma is shown to be able to perform syntactic processing via Sum Product Networks (SPNs), a new kind of deep architecture that allows efficient, tractable and exact inference in a wide class of problems, including grammar parsing. It is shown that Sigma’s cognitive language is sufficient to specify any arbitrary valid SPN, with the tractability and exactness expected of them. This shows Sigma’s ability to efficiently specify a wide range of problems. The implications of this are discussed, along with Sigma mechanisms that allow for specifying SPNs. This leads to a novel relationship between neural networks and SPNs in the context of Sigma",Speech and language understanding in the Sigma cognitive architecture,,,University of Southern California. Libraries,,core
188391480,2018-01-01T00:00:00,"PreCrash problem of Intelligent Control of autonomous vehicles robot is a very complex problem, especially vehicle pre-crash scenarios and at points of intersections in real-time environments. The goal of this research is to develop a new artificial intelligent adaptive controller for autonomous vehicle Pre-Crash system along with vehicle recognition module and tested in MATLAB including some detailed modules. Following tasks were set: finding Objects in sensor Data (LiDAR. RADAR), Speed and Steering control, vehicle Recognition using convolution neural network and Alexnet. In this research paper, we implemented a real-time image/Lidar processing. At the beginning, we presented a real-time system which is composed of comprehensive modules, these modules are 3d object detection, object clustering and search, ground removal, deep learning using convolutional neural networks. Starting with nearest vehicle module our target is to find the nearest ahead car and consider it as our primary obstacle. This paper presents an Adaptive cruise pre-crash system and vehicle recognition. The Adaptive cruise pre-crash system module depends on Deep Learning and LiDAR sensor data, which meant to control the driver reckless behavior on the road by adjusting the vehicle speed to maintain a safe distance from objects ahead (such as cars, humans, bicycle or whatever the object) when the driver tries to raise speed. At the very moment the vehicle recognition module, detects and recognizes the vehicles surrounding to the car",A hybrid liar/radar-based deep learning and vehicle recognition engine for autonomous vehicle Precrash control,https://core.ac.uk/download/188391480.pdf,10.15587/1729-4061.2018.141298,'Private Company Technology Center',,core
201528782,2018-10-01T00:00:00,"It is crucial for robots to autonomously steer in complex environments safely without colliding with any obstacles. Compared to conventional methods, deep reinforcement learning-based methods are able to learn from past experiences automatically and enhance the generalization capability to cope with unseen circumstances. Therefore, we propose an end-to-end deep reinforcement learning algorithm in this paper to improve the performance of autonomous steering in complex environments. By embedding a branching noisy dueling architecture, the proposed model is capable of deriving steering commands directly from raw depth images with high efficiency. Specifically, our learning-based approach extracts the feature representation from depth inputs through convolutional neural networks and maps it to both linear and angular velocity commands simultaneously through different streams of the network. Moreover, the training framework is also meticulously designed to improve the learning efficiency and effectiveness. It is worth noting that the developed system is readily transferable from virtual training scenarios to real-world deployment without any fine-tuning by utilizing depth images. The proposed method is evaluated and compared with a series of baseline methods in various virtual environments. Experimental results demonstrate the superiority of the proposed model in terms of average reward, learning efficiency, success rate as well as computational time. Moreover, a variety of real-world experiments are also conducted which reveal the high adaptability of our model to both static and dynamic obstacle-cluttered environments",Learn to Steer through Deep Reinforcement Learning,,10.3390/s18113650,'MDPI AG',"[{'title': 'Sensors', 'identifiers': ['issn:1424-8220', '1424-8220']}]",core
286413436,2018-01-01T00:00:00,"Model-free reinforcement learning has recently been shown to be effective at learning navigation policies from complex image input. However, these algorithms tend to require large amounts of interaction with the environment, which can be prohibitively costly to obtain on robots in the real world. We present an approach for efficiently learning goal-directed navigation policies on a mobile robot, from only a single coverage traversal of recorded data. The navigation agent learns an effective policy over a diverse action space in a large heterogeneous environment consisting of more than 2km of travel, through buildings and outdoor regions that collectively exhibit large variations in visual appearance, self-similarity, and connectivity. We compare pretrained visual encoders that enable precomputation of visual embeddings to achieve a throughput of tens of thousands of transitions per second at training time on a commodity desktop computer, allowing agents to learn from millions of trajectories of experience in a matter of hours. We propose multi- ple forms of computationally efficient stochastic augmentation to enable the learned policy to generalise beyond these precomputed embeddings, and demonstrate successful deployment of the learned policy on the real robot without fine tuning, despite environmental appearance differences at test time. The dataset and code required to reproduce these results and apply the technique to other datasets and robots is made publicly available at rl-navigation.github.io/deployable ",Learning deployable navigation policies at kilometer scale from a single traversal,,,Proceedings of Machine Learning Research,,core
211826997,2018-01-01T00:00:00,"Sprowitz AT, Tuleu A, Ajallooeian M, et al. Oncilla Robot: A Versatile Open-Source Quadruped Research Robot With Compliant Pantograph Legs. FRONTIERS IN ROBOTICS AND AI. 2018;5: 18.We present Oncilla robot, a novel mobile, quadruped legged locomotion machine. This large-cat sized, 5.1 kg robot is one of a kind of a recent, bioinspired legged robot class designed with the capability of model-free locomotion control. Animal legged locomotion in rough terrain is clearly shaped by sensor feedback systems. Results with Oncilla robot show that agile and versatile locomotion is possible without sensory signals to some extend, and tracking becomes robust when feedback control is added (Ajallooeian, 2015). By incorporating mechanical and control blueprints inspired from animals, and by observing the resulting robot locomotion characteristics, we aim to understand the contribution of individual components. Legged robots have a wide mechanical and control design parameter space, and a unique potential as research tools to investigate principles of biomechanics and legged locomotion control. But the hardware and controller design can be a steep initial hurdle for academic research. To facilitate the easy start and development of legged robots, Oncilla-robot's blueprints are available through open-source. The robot's locomotion capabilities are shown in several scenarios. Specifically, its spring-loaded pantographic leg design compensates for overdetermined body and leg postures, i.e., during turning maneuvers, locomotion outdoors, or while going up and down slopes. The robot's active degree of freedom allow tight and swift direction changes, and turns on the spot. Presented hardware experiments are conducted in an open-loop manner, with little control and computational effort. For more versatile locomotion control, Oncilla-robot can sense leg joint rotations, and leg-trunk forces. Additional sensors can be included for feedback control with an open communication protocol interface. The robot's customized actuators are designed for robust actuation, and efficient locomotion. It trots with a cost of transport of 3.2 J/(Nm),at a speed of 0.63 m s(-1) (Froude number 0.25). The robot trots inclined slopes up to 10 degrees, at 0.25 m s(-1). The multi-body Webots model of Oncilla robot, and Oncilla robot's extensive software architecture enables users to design and test scenarios in simulation. Controllers can directly be transferred to the real robot. Oncilla robot's blueprints are open-source published (hardware GLP v3, software LGPL v3)",Oncilla Robot: A Versatile Open-Source Quadruped Research Robot With Compliant Pantograph Legs,,10.3389/frobt.2018.00067,'Frontiers Media SA',,core
427481811,2018-01-01T00:00:00,"Customers have rapidly changing behaviours and expectations and hence a company needs to rapidly adapt to the changing market needs. As a leading telecommunications infrastructure provider, CommScope engages in top-notch strategic measures in staying up to date with the evolving market needs. The evolution of the telecommunications industry is now growing towards the connectivity of IoT devices. Like in most industries, there is a value chain associated with IoT when it comes to the telecommunication services.  This research project not only looks into IoT but delves into the business operations of CommScope, exploring various Industry 4.0 technologies whilst aligning them with KPIs to determine the potential impact in the business operations of CommScope. CommScope is a multi-national telecommunications infrastructure provider headquartered in North Carolina, United States. It is located in over 130 countries with over 20,000 employees worldwide in NAR, EMEA, APAC and CALA geographical regions. Since the fourth industrial revolution is the move towards digitisation. The question is - What impact will this industrial revolution have on CommScope? Industry 4.0 can be described at the fourth industry revolution, the current mega trend impacting and shaping companies around the world today. It envisions the automation and connection between people, machines and products within and across an enterprise. This revolution promises to reshape the face of manufacturing by merging the traditional manufacturing techniques with industrial technology to aid data analysis across machines for faster, flexible and more efficient processes which will ultimately lead to the production of high quality products and/or services at reduced costs.  CommScope can be categorised as a product company which manufactures a diverse range of product such copper cables, fibre optic cables, connector panels, racking and metal. For a tangible implementation of Industry 4.0, a tailored approach into a particular business division and a focused into one product line was considered for optimum results. Following successful implementation as well as if desired objectives are met in this product line, it may then be replicated in other areas of the business. Industry 4.0 will enable automatic data collection that can be used by CommScope to further aid advancements in big data and powerful analytics which means that systems can sift through the huge sets of gathered data and produce insights that can be acted upon quickly. This will provide effective communications and right channelling of analysed and backed up data securely through systems to the relevant people, processes or machines. This Industry 4.0 network between people, processes and machines involves four main characteristics : vertical networking, horizontal integration, through-engineering across the entire value chain; as well as acceleration through exponential technologies. We conducted a strategic research conducted beginning with exploring the various Industry 4.0 technologies such as Big Data Analytics, Automation, Augmented Reality, Internet of Things (IoT), Artificial Intelligence, Blockchain, Shared Economies, eCommerce, 3D Printing, Robotics, Nano Technology, Fintech and aligning them with KPIs in order to determine their relevance and application. Following this, the technologies were clustered into the following segments for a more focused and interrelated use in 1) eCommerce, 2) Manufacturing, 3) Product development and customisation; as well as 4) Supply Chain and Logistics. Currently, CommScope has challenges in its business operations in terms of speed of deployment, production performance, supply chain performance which ultimately impacts customer experience, brand perception and costs. In order to tackle these challenges and ensure CommScope's future value creation, we performed an Industry 4.0 readiness assessment to determine CommScope's current readiness, its ambition and measure the gap between CommScope and other industry players through a benchmark. This assessment entailed interviews with internal stakeholders within CommScope in the EMEA and NAR region. Interpreting the result of CommScope's readiness assessment of Industry 4.0, CommScope is at an experienced level 3 in the legal considerations dimension with the rest of the other five dimensions namely products and services, manufacturing and operations, strategy and organisation and business model at an intermediate level 2. This was also similar to the overall results of the 53 benchmarked companies scoring an intermediate level 2 in all dimensions. Following these results, we shortlisted manufacturing operations and supply chain as the most challenging areas of CommScope's business operations. With this focus, we further identified gap areas in the manufacturing operations and supply chain divisions and recommended key capabilities that CommScope should develop to fill these gaps. In conclusion, we have outlined CommScope's vision and designed a roadmap to the stipulated goal of the ""Amazon one-click experience"" as a guideline with potential action steps which upon successful implementation can be replicated across all relevant product lines and/or business divisions. For this vision, there are key capabilities that also been identified to be developed to transform CommScope's business operations by moving towards digitisation. This Industry 4.0 journey calls for deliberate commitment in terms of investments of time and resources which will ultimately broaden CommScope's reach and capabilities and open up new potential market opportunities",Analysis of industry 4.0 and its impact in business operations,,,,,core
395835703,2018-01-01T00:00:00,"Real-time locomotion mode recognition can potentially be applied in the gait analysis as a diagnostic tool or a strategy to control the robotic motion. This research aimed the development of an automatic, accurate and time-effective tool to recognize, in real-time, the locomotion mode that is being performed by a humanoid robot. The proposed strategy should also be general to different walkers and walking conditions. For these purposes, we designed a strategy to identify, in an offline phase, the suitable features and classification models for the real-time recognition. We explored several classification models based on two machine learning approaches using the features previously selected by principal component analysis and genetic algorithm (GA). The validation was carried out for distinct walking directions and speeds of DARwIn-OP. The offline analysis suggests that the most skilled models are the ones created by weighted k-nearest neighbors (KNN), fine KNN, and cubic support vector machine using 2 features selected by GA. Results from the real-time implementation highlight that weighted KNN exhibits a higher recognition performance (accuracy > 99.15%) and a lower elapsed time in the recognition process (89 ms) comparatively to the state-of-the-art. The proposed recognition tool showed to be cost-effective, and highly accurate for the real-time gait analysis at different walking conditions.- (POCI",Automatic and real-time locomotion mode recognition of a humanoid robot,https://core.ac.uk/download/395835703.pdf,10.1142/9789813231047_0076,'World Scientific Pub Co Pte Lt',,core
160785022,2018-08-20T00:00:00,"Visual understanding of 3D environments in real-time, at low power, is a huge
computational challenge. Often referred to as SLAM (Simultaneous Localisation
and Mapping), it is central to applications spanning domestic and industrial
robotics, autonomous vehicles, virtual and augmented reality. This paper
describes the results of a major research effort to assemble the algorithms,
architectures, tools, and systems software needed to enable delivery of SLAM,
by supporting applications specialists in selecting and configuring the
appropriate algorithm and the appropriate hardware, and compilation pathway, to
meet their performance, accuracy, and energy consumption goals. The major
contributions we present are (1) tools and methodology for systematic
quantitative evaluation of SLAM algorithms, (2) automated,
machine-learning-guided exploration of the algorithmic and implementation
design space with respect to multiple objectives, (3) end-to-end simulation
tools to enable optimisation of heterogeneous, accelerated architectures for
the specific algorithmic requirements of the various SLAM algorithmic
approaches, and (4) tools for delivering, where appropriate, accelerated,
adaptive SLAM solutions in a managed, JIT-compiled, adaptive runtime context.Comment: Proceedings of the IEEE 201","Navigating the Landscape for Real-time Localisation and Mapping for
  Robotics and Virtual and Augmented Reality",http://arxiv.org/abs/1808.06352,10.1109/JPROC.2018.2856739,'Institute of Electrical and Electronics Engineers (IEEE)',,core
153555004,2018,"Humans have an innate tendency to anthropomorphize surrounding entities and have always been fascinated by the creation of machines endowed with human-inspired capabilities and traits. In the last few decades, this has become a reality with enormous advances in hardware performance, computer graphics, robotics technology, and artificial intelligence. New interdisciplinary research fields have brought forth cognitive robotics aimed at building a new generation of control systems and providing robots with social, empathetic and affective capabilities. This paper presents the design, implementation, and test of a human-inspired cognitive architecture for social robots. State-of-the-art design approaches and methods are thoroughly analyzed and discussed, cases where the developed system has been successfully used are reported. The tests demonstrated the system's ability to endow a social humanoid robot with human social behaviors and with in-silico robotic emotions",Designing the mind of a social robot,,10.3390/app8020302,'MDPI AG',,core
395008361,2018-01-01T00:00:00,"Human-robot collaboration could be advanced by facilitating the intuitive, gaze-based control of robots, and enabling robots to recognize human actions, infer human intent, and plan actions that support human goals. Traditionally, gaze tracking approaches to action recognition have relied upon computer vision-based analyses of two-dimensional egocentric camera videos. The objective of this study was to identify useful features that can be extracted from three-dimensional (3D) gaze behavior and used as inputs to machine learning algorithms for human action recognition. We investigated human gaze behavior and gaze-object interactions in 3D during the performance of a bimanual, instrumental activity of daily living: the preparation of a powdered drink. A marker-based motion capture system and binocular eye tracker were used to reconstruct 3D gaze vectors and their intersection with 3D point clouds of objects being manipulated. Statistical analyses of gaze fixation duration and saccade size suggested that some actions (pouring and stirring) may require more visual attention than other actions (reach, pick up, set down, and move). 3D gaze saliency maps, generated with high spatial resolution for six subtasks, appeared to encode action-relevant information. The ""gaze object sequence"" was used to capture information about the identity of objects in concert with the temporal sequence in which the objects were visually regarded. Dynamic time warping barycentric averaging was used to create a population-based set of characteristic gaze object sequences that accounted for intra- and inter-subject variability. The gaze object sequence was used to demonstrate the feasibility of a simple action recognition algorithm that utilized a dynamic time warping Euclidean distance metric. Averaged over the six subtasks, the action recognition algorithm yielded an accuracy of 96.4%, precision of 89.5%, and recall of 89.2%. This level of performance suggests that the gaze object sequence is a promising feature for action recognition whose impact could be enhanced through the use of sophisticated machine learning classifiers and algorithmic improvements for real-time implementation. Robots capable of robust, real-time recognition of human actions during manipulation tasks could be used to improve quality of life in the home and quality of work in industrial environments",Exploiting Three-Dimensional Gaze Tracking for Action Recognition During Bimanual Manipulation to Enhance Human-Robot Collaboration.,,,"eScholarship, University of California",,core
203998804,19/11/2018,"2018-11-20As robots increasingly perform tasks in a diverse set of real-world environments, they are expected to not only operate in close proximity to humans but interact with them as well. This has led to great interest in the communication challenges associated with the varying degrees of coordination and collaboration required between humans and robots for these tasks. Non-humanoid robots can benefit from the use of nonverbal signals as they often lack the communication modalities that humans intrinsically rely on to obtain important state information. ❧ The goal of this thesis is to enable non-humanoid robots to intelligently utilize nonverbal signals to communicate information about their internal state. As interaction is a complex process, we propose a computational framework that formalizes the robot’s communication behavior as a decision making problem under uncertainty. Building on prior work in notification systems, this framework takes into account information about the human and robot and attempts to balance their individual objectives to create more acceptable robot behavior. ❧ To inform the framework’s Markov Decision Process model, we explored the design space of light, sound, and motion for nonverbal signaling during a human-robot collaboration task. We present three user studies that identify underlying signal design principles based on human perceptions. We applied the findings of these studies to interaction scenarios in three different experiments. To increase the generalizability of this research, we employed several types of non-humanoid robot platform that vary in appearance and capabilities. ❧ Finally, we applied the communication framework to a simulated human-robot collaboration task. A policy for the robot’s nonverbal signaling behavior was generated using model-free reinforcement learning. This experiment evaluated the impact of the robot’s actions on participants’ perceptions of the robot as a teammate. Results showed that the use of this framework enables the robot to not only improve its own task-oriented outcomes but to act as a more thoughtful and considerate agent during interaction with humans. ❧ This research contributes to both the design and planning of nonverbal communication for non-humanoid robot platforms with both theoretically and empirically driven methodologies. Although the number of non-humanoid robots deployed in the world is growing, this field of research is still maturing. This work provides a foundation for future human-robot interaction research in these areas while promoting generalizability and standardization of robot behaviors across the diverse set of existing non-humanoid robots",Nonverbal communication for non-humanoid robots,,,University of Southern California. Libraries,,core
211982880,2018-05-09T08:36:06,"It has often been found that students appreciate hands-on work, and find that they learn more with courses that include a project than those relying solely on conventional lectures and tests. This type of project driven learning is a key component of “Inquiry-based learning” (IBL), which aims at teaching methodology as well as content by incorporating the student as an actor rather than a spectator. Robotics applications are especially well-suited for IBL due to the value of trial and error experience, the multiple possibilities for students to implement their own ideas, and the importance of programming, problem-solving and electro-mechanical skills in real world engineering and science jobs. Furthermore, robotics platforms can be useful teaching media and learning tools for a variety of topics. Here, we present RoboGen: an open-source, web-based, software and hardware platform for Robotics and Artificial Intelligence with a particular focus on Evolutionary Robotics. We describe the platform in detail, compare it to existing alternatives, and present results of its use as a platform for Inquiry-based learning within a master's level course at the Ecole Polytechnique Fédérale de Lausanne (EPFL)",Inquiry-Based Learning with RoboGen: An Open-Source Software and Hardware Platform for Robotics and Artificial Intelligence,,10.1109/TLT.2018.2833111,'Institute of Electrical and Electronics Engineers (IEEE)',,core
323103028,2019-08-30T00:00:00,"Dissertação (mestrado)—Universidade de Brasília, Faculdade de Tecnologia, Departamento de Engenharia Mecânica, 2019.A previsão e a evolução dos parâmetros climáticos (como radiação solar, índices de precipitações,
temperatura atmosférica e da umidade relativa) são informações importantes para a sociedade e
suas áreas de aplicação. As novas tecnologias (como estações meteorológicas inteligentes, colheitadeiras, robôs e drones) necessitam de informações atuais e futuras sobre as condições de
clima e tempo de sua localidade, para melhorar a eficiência no uso de recursos e permitir o funcionamento sustentável destes dispositivos. Adicionalmente os indicadores de clima e tempo para
a região amazônica apresentam baixa acurácia nas previsões com os modelos disponíveis, devido a fatores como a dinâmica tropical da região, instrumentação escassa e dificuldade logística
da região-acesso e energia elétrica. Este trabalho apresenta uma proposta de aplicação de algoritmos de Inteligência Artificial (IA), para dispositivos inteligentes com sistemas embarcados,
como: drones, robôs e estações meteorológicas, para previsão de classes de chuva (precipitação).
Como classificador foi utilizado o algoritmo SVM, combinados com o algoritmos bio-inspirados
- MOPSO e MODE. Para validação desta proposta foram utilizados dados reais de uma localidade da região amazônica, a cidade de Belém-PA. Essas técnicas foram propostas recentemente
na literatura e apresentaram boa capacidade de previsão de chuva na Europa, China e India. Os
resultados qualitativos e quantitativos, deste trabalho, demostraram que o desempenho destes algoritmos foram bons, e mostrou que seu desempenho depende das reais necessidades do problema
a ser aplicado. Os modelos apresentaram uma boa eficiência computacional, para aplicações em
sistemas embarcados, já que não requerem grandes recursos de hardware e software. Os modelos apresentaram, também, uma boa acurácia, boa precisão e recall eficientes, para as classes de
precipitação utilizadas, podendo ser implementado para previsão de curto prazo com baixo custo
computacional.Prediction and evolution of climate parameters (such as solar radiation, precipitation rates, atmospheric temperature and relative humidity) are important information for society and its application areas. New technologies (such as smart weather stations, harvesters, robots, and drones)
need current and future information on your local weather and weather conditions to improve
resource efficiency and enable these devices to function sustainably. Additionally, climate and
weather indicators for the Amazon region present low accuracy in the predictions with the available models, due to factors such as the region’s tropical dynamics, scarce instrumentation and
logistical difficulty of the access region and electricity. This work presents a proposal for the
application of Artificial Intelligence (AI) algorithms for intelligent devices in embedded systems,
such as: drones, robots and meteorological stations, to predict rainfall classes. For the classifier
the SVM algorithm was used, combined with the bio-inspired algorithms - MOPSO and MODE.
To validate this proposal, real data were used from a locality in the Amazon region, the city
of Belém-PA. These techniques were recently proposed in the literature and showed good rainfall prediction in Europe, China and India. The qualitative and quantitative results of this work
demonstrated that the performance of these algorithms were good and showed that their performance depends on the real needs of the problem to be applied. The models presented a good
computational efficiency, for applications in embedded systems, since they do not require great
hardware and software resources. The models also presented a good accuracy, good precision
and efficient recall for the precipitation classes used, being able to be implemented for short term
prediction with low computational cos",Previsão de precipitação usando máquinas de vetores de suporte visando sua implementação em sistemas embarcados,https://core.ac.uk/download/323103028.pdf,,,,core
286898266,2019-01-01T00:00:00,"Emergency personnel, such as firefighters, bomb technicians, and urban search and rescue specialists, can be exposed to a variety of extreme hazards during the response to natural and human-made disasters. In many of these scenarios, a risk factor is the presence of hazardous airborne chemicals. The recent and rapid advances in robotics and sensor technologies allow emergency responders to deal with such hazards from relatively safe distances. Mobile robots with gas-sensing capabilities allow to convey useful information such as the possible source positions of different chemicals in the emergency area. However, common gas sampling procedures for laboratory use are not applicable due to the complexity of the environment and the need for fast deployment and analysis. In addition, conventional gas identification approaches, based on supervised learning, cannot handle situations when the number and identities of the present chemicals are unknown. For the purpose of emergency response, all the information concluded from the gas detection events during the robot exploration should be delivered in real time. To address these challenges, we developed an online gas-sensing system using an electronic nose. Our system can automatically perform unsupervised learning and update the discrimination model as the robot is exploring a given environment. The online gas discrimination results are further integrated with geometrical information to derive a multi-compound gas spatial distribution map. The proposed system is deployed on a robot built to operate in harsh environments for supporting fire brigades, and is validated in several different real-world experiments of discriminating and mapping multiple chemical compounds in an indoor open environment. Our results show that the proposed system achieves high accuracy in gas discrimination in an online, unsupervised, and computationally efficient manner. The subsequently created gas distribution maps accurately indicate the presence of different chemicals in the environment, which is of practical significance for emergency response.Funding Agency:European Commission  645101</p",Towards Gas Discrimination and Mapping in Emergency Response Scenarios Using a Mobile Robot with an Electronic Nose,,10.3390/s19030685,'MDPI AG',,core
201491946,2019-01-01T00:00:00Z,"Ultrasonic sensors have been used in a variety of applications to measure ranges to objects. Hand gestures via ultrasonic sensors form unique motion patterns for controls. In this research, patterns formed by placing a set of objects in a grid of cells are used for control purposes. A neural network algorithm is implemented on a microcontroller which takes in range signals as inputs read from ultrasonic sensors and classifies them in one of four classes. The neural network is then trained to classify patterns based on objects’ locations in real-time. The testing of the neural network for pattern recognition is performed on a testbed consisting of Inter-Integrated Circuit (I2C) ultrasonic sensors and a microcontroller. The performance of the proposed model is presented and it is observed the model is highly scalable, accurate, robust and reliable for applications requiring high accuracy such as in robotics and artificial intelligence",Pattern Recognition using a Neural Network on a Microcontroller with I2C Ultrasonic Sensors,,,International Association for Educators and Researchers (IAER),"[{'title': None, 'identifiers': ['2516-0281', '2516-029x', 'issn:2516-0281', 'issn:2516-029X']}]",core
287621918,2019-01-01T00:00:00,"In order for autonomous systems like robots, drones, and self-driving cars to be reliably introduced into our society, they must have the ability to actively account for safety during their operation. While safety analysis has traditionally been conducted offline for controlled environments like cages on factory floors, the much higher complexity of open, human-populated spaces like our homes, cities, and roads makes it unviable to rely on common design-time assumptions, since these may be violated once the system is deployed. Instead, the next generation of robotic technologies will need to reason about safety online, constructing high-confidence assurances informed by ongoing observations of the environment and other agents, in spite of models of them being necessarily fallible.This dissertation aims to lay down the necessary foundations to enable autonomous systems to ensure their own safety in complex, changing, and uncertain environments, by explicitly reasoning about the gap between their models and the real world. It first introduces a suite of novel robust optimal control formulations and algorithmic tools that permit tractable safety analysis in time-varying, multi-agent systems, as well as safe real-time robotic navigation in partially unknown environments; these approaches are demonstrated on large-scale unmanned air traffic simulation and physical quadrotor platforms. After this, it draws on Bayesian machine learning methods to translate model-based guarantees into high-confidence assurances, monitoring the reliability of predictive models in light of changing evidence about the physical system and surrounding agents. This principle is first applied to a general safety framework allowing the use of learning-based control (e.g. reinforcement learning) for safety-critical robotic systems such as drones, and then combined with insights from cognitive science and dynamic game theory to enable safe human-centered navigation and interaction; these techniques are showcased on physical quadrotors—flying in unmodeled wind and among human pedestrians—and simulated highway driving. The dissertation ends with a discussion of challenges and opportunities ahead, including the bridging of safety analysis and reinforcement learning and the need to ``close the loop'' around learning and adaptation in order to deploy increasingly advanced autonomous systems with confidence",Game-Theoretic Safety Assurance for Human-Centered Robotic Systems,https://core.ac.uk/download/287621918.pdf,,"eScholarship, University of California",,core
214305732,2019-03-30T18:15:00,"Self-driving cars are of great interest in computer science and artificial intelligence due to their speed, efficiency, and affordability. Despite the attention, no one has been able to find the “perfect” solution: the implementation of completely predictable and reliable autonomous transportation. This research serves as an introduction to filling that gap by achieving total autonomy on a smaller scale through programming the Massachusetts Institute of Technology’s RACECAR (Rapid Autonomous Complex Environment Competing Ackermann-steering Robot), creating a safe and robust autonomous ground vehicle that will serve as an example for autonomy on passenger-carrying cars. Python, the advanced control systems on the car, ROS (Robot Operating System), and OpenCV were the main tools utilized to enable the car to drive on its own. My team created an autonomous race car capable of following walls, detecting and avoiding obstacles, and changing paths based on visual perception of its environment. These components were combined into a state machine that implemented different controllers and commands based on detection of AR (augmented reality) tags. The car proved able to navigate autonomously through a complex, dynamic course both safely and quickly. Autonomy was achieved, and, in the future, the implemented tactics can be translated to a passenger-carrying car to fulfill the ultimate goal of autonomous research: fully driverless transportation",Software Development for High-Speed Autonomous Ground Vehicles,,,Furman University Scholar Exchange,,core
343444666,2019-11-01T00:00:00,"The fuzzy Kalman filter (FKF), introduced some years ago, is revisited. In the initial version, trapezoidal possibility distributions functions instead of gaussian probability distributions were proposed, and trapezius were modeled by four representative points. Nevertheless, and although the algorithm works properly, an implementation problem occurs when propagating uncertainty through a non-linear function in multi-variable systems, something that was solved by linearization. In this work we propose an alternative method to represent uncertainty, still using trapezoidal distributions, which avoids the previous inconvenience and eases the Kalman filter steps computation. We reformulate the FKF algorithm, presenting a new theoretical approach as well as validation tests in both simulation and a real mobile robot.This work has been funded by Spanish Ministry of Science, Innovation and Universities (Artificial Intelligence Techniques and Assistance to Autonomous Navigation, reference DPI 2017-86915-C3-3-R). It has also received funding from the RoboCity2030-DIH-CM Madrid Robotics Digital Innovation Hub “Robótica aplicada a la mejora de la calidad de vida de los ciudadanos, fase IV”; S2018/NMT-4331), funded by “Programas de Actividades I+D en la Comunidad de Madrid” and cofunded by Structural Funds of the EU.Peer reviewe",The fuzzy Kalman filter: Improving its implementation by reformulating uncertainty representation,,10.1016/j.fss.2019.10.015,'Elsevier BV',"[{'title': 'Fuzzy Sets and Systems', 'identifiers': ['0165-0114', 'issn:0165-0114']}]",core
295497995,19/02/2019,"In the past, methods for hand sign recognition have been successfully tested in Human Robot Interaction (HRI) using traditional methodologies based on static image features and machine learning. However, the recognition of gestures in video sequences is a problem still open, because current detection methods achieve low scores when the background is undefined or in unstructured scenarios. Deep learning techniques are being applied to approach a solution for this problem in recent years. In this paper, we present a study in which we analyse the performance of a 3DCNN architecture for hand gesture recognition in an unstructured scenario. The system yields a score of 73% in both accuracy and F1. The aim of the work is the implementation of a system for commanding robots with gestures recorded by video in real scenarios.This work was funded by the Ministry of Economy, Industry and Competitiveness from the Spanish Government through the DPI2015-68087-R and the pre-doctoral grant BES-2016-078290, by the European Commission and FEDER funds through the project COMMANDIA (SOE2/P1/F0638), action supported by Interreg-V Sudoe",3DCNN Performance in Hand Gesture Recognition Applied to Robot Arm Interaction,,10.5220/0007570208020806,SciTePress,,core
288002542,2019-01-01T00:00:00,"Autonomous robots have become a very popular topic within the artificial
intelligence field. These systems are able to perform difficult
or risky tasks that could be dangerous when done by humans or
trained animals. Vision is commonly considered the most relevant
input sensor for autonomous robots and tracking systems. However,
auditory information is also important in some specific situations
where vision cannot provide any useful information when navigating.
In this work, a spike-based model of the medial superior olive
of the inner ear has been implemented in reconfigurable hardware
for performing sound source localization in real time. Future works
will focus on integrating this information with vision in order to
achieve a fully bio-inspired autonomous tracking system.Ministerio de Economía y Competitividad TEC2016-77785-",Work-in-Progress: A Neuromorphic Approach of the Sound Source Localization Task in Real-Time Embedded Systems,,10.1145/3349568.3351544,'Association for Computing Machinery (ACM)',,core
328148978,2019-12-15T08:00:00,"Data mining and predictive analytics in the sustainable-biomaterials industries is currently not feasible given the lack of organization and management of the database structures. The advent of artificial intelligence, data mining, robotics, etc., has become a standard for successful business endeavors and is known as the ‘Fourth Industrial Revolution’ or ‘Industry 4.0’ in Europe. Data quality improvement through real-time multi-layer data fusion across interconnected networks and statistical quality assessment may improve the usefulness of databases maintained by these industries. Relational databases with a high degree of quality may be the gateway for predictive modeling and enhanced business analytics.	Data quality is a key issue in the sustainable bio-materials industry. Untreated data from multiple databases (e.g., sensor data and destructive test data) are generally not in the right structure to perform advanced analytics. Some inherent problems of data from sensors that are stored in data warehouses at millisecond intervals include missing values, duplicate records, sensor failure data (data out of feasible range), outliers, etc. These inherent problems of the untreated data represent information loss and mute predictive analytics. The goal of this data science focused research was to create a continuous real-time software algorithm for data cleaning that automatically aligns, fuses, and assesses data quality for missing fields and potential outliers. The program automatically reduces the variable size, imputes missing values, and predicts the destructive test data for every record in a database. Improved data quality was assessed using 10-fold cross-validation and the normalized root mean square error of prediction (NRMSEP) statistic. 	The impact of outliers and missing data were tested on a simulated dataset with 201 variations of outlier percentages ranging from 0-90% and missing data percentages ranging from 0-90%.  The software program was also validated on a real dataset from the wood composites industry. One result of the research was that the number of sensors needed for accurate predictions are highly dependent on the correlation between independent variables and dependent variables. Overall, the data cleaning software program significantly decreased the NRMSEP ranging from 64% to 12% of quality control variables for key destructive test values (e.g., internal bond, water absorption and modulus of rupture)",Improving Manufacturing Data Quality with Data Fusion and Advanced Algorithms for Improved Total Data Quality Management,https://core.ac.uk/download/328148978.pdf,,TRACE: Tennessee Research and Creative Exchange,,core
237701782,2019-05-01T00:00:00,"Personalized stent graft is designed to treat Abdominal Aortic Aneurysms (AAA). Due to the individual difference in arterial structures, stent graft has to be custom made for each AAA patient. Robotic platforms for autonomous personalized stent graft manufacturing have been proposed in recently which rely upon stereo vision systems for coordinating multiple robots for fabricating customized stent grafts. This paper proposes a novel hybrid vision system for real-time visual-sevoing for personalized stent-graft manufacturing. To coordinate the robotic arms, this system is based on projecting a dynamic stereo microscope coordinate system onto a static wide angle view stereo webcam coordinate system. The multiple stereo camera configuration enables accurate localization of the needle in 3D during the sewing process. The scale-invariant feature transform (SIFT) method and color filtering are implemented for stereo matching and feature identifications for object localization. To maintain the clear view of the sewing process, a visual-servoing system is developed for guiding the stereo microscopes for tracking the needle movements. The deep deterministic policy gradient (DDPG) reinforcement learning algorithm is developed for real-time intelligent robotic control. Experimental results have shown that the robotic arm can learn to reach the desired targets autonomously",Visual guidance and automatic control for robotic personalized stent graft manufacturing,,10.1109/icra.2019.8794123,'Institute of Electrical and Electronics Engineers (IEEE)',,core
232211197,2019-02-01T00:00:00,"Genetic algorithms (GAs) are widely used in machine learning and optimization. This paper proposes a time-dependent genetic algorithm (TDGA) based on real-coded genetic algorithm (RCGA) to improve the convergence performance of functions over time such as a foot trajectory. TDGA has several distinguishing features when compared with traditional RCGA. First, individuals are arranged over time, and then the individuals are optimized in sequence. Second, search spaces of design variables are newly comprised of processes of reductions for search spaces. Third, the search space for crossover operations is expanded to avoid local minima traps that can occur in new search spaces up to the previous search space before performing any reduction of search space, and boundary mutation operation is performed to the new search spaces. Computer simulations are implemented to verify the convergence performance of the robot locomotion optimized by TDGA. Then, TDGA optimizes the desired feet trajectories of quadruped robots that climb up a slope and the impedance parameters of admittance control so that quadruped robots can trot stably over irregular terrains. Simulation results clearly represent that the convergence performance is improved by TDGA, which also shows that TDGA could be broadly used in robot locomotion research. (C) 2018 Elsevier B.V. All rights reserved",Time-dependent genetic algorithm and its application to quadruped&rsquo;s locomotion,,10.1016/j.robot.2018.10.015,'Elsevier BV',"[{'title': 'Robotics and Autonomous Systems', 'identifiers': ['1872-793x', 'issn:1872-793X', 'issn:0921-8890', '0921-8890']}]",core
334842213,2019-07-27T00:00:00,"Contact-rich manipulation tasks in unstructured environments often require
both haptic and visual feedback. It is non-trivial to manually design a robot
controller that combines these modalities which have very different
characteristics. While deep reinforcement learning has shown success in
learning control policies for high-dimensional inputs, these algorithms are
generally intractable to deploy on real robots due to sample complexity. In
this work, we use self-supervision to learn a compact and multimodal
representation of our sensory inputs, which can then be used to improve the
sample efficiency of our policy learning. Evaluating our method on a peg
insertion task, we show that it generalizes over varying geometries,
configurations, and clearances, while being robust to external perturbations.
We also systematically study different self-supervised learning objectives and
representation learning architectures. Results are presented in simulation and
on a physical robot.Comment: arXiv admin note: substantial text overlap with arXiv:1810.1019","Making Sense of Vision and Touch: Learning Multimodal Representations
  for Contact-Rich Tasks",http://arxiv.org/abs/1907.13098,,,,core
200333178,2019-01-01T00:00:00,"The need for tools to help guide decision making is growing within the manufacturing industry. The analysis performed by these tools will help operators and engineers to understand the behaviour of the manufacturing stations better and thereby take data-driven decisions to improve them. The tools use techniques borrowed from fields such as Data Analytics, BigData, Predictive Modelling, and Machine Learning. However, to be able to use these tools efficiently, data from the factory floor is required as input. This data needs to be extracted from two sources, the PLCs, and the robots. In practice, methods to extract usable data from robots are rather scarce. The present work describes an approach to capture data from robots, which can be applied to both legacy and current state-of-the-art manufacturing systems. The described approach is developed using Sequence Planner - a tool for modelling and analyzing production systems - and is currently implemented at an automotive company as a pilot project to visualize and examine the ongoing process. By exploiting the robot code structure, robot actions are converted to event streams that are abstracted into operations. We then demonstrate the applicability of the resulting operations, by visualizing the ongoing process in real-time as Gantt charts, that support the operators performing maintenance. And, the data is also analyzed off-line using process mining techniques to create a general model that describes the underlying behaviour existing in the manufacturing station. Such models are used to derive insights about relationships between different operations, and also between resources","From factory floor to process models: A data gathering approach to generate, transform, and visualize manufacturing processes",,10.1016/j.cirpj.2018.12.002,'Elsevier BV',,core
224837116,2019-01-01T00:00:00,"This work aims to increase the impact of computer vision on robotic positioning and grasping in industrial assembly lines. Real-time object detection and localization problem is addressed for robotic grasp-and-place operation using Selective Compliant Assembly Robot Arm (SCARA). The movement of SCARA robot is guided by deep learning-based object detection for grasp task and edge detection-based position measurement for place task. Deep Convolutional Neural Network (CNN) model, called KSSnet, is developed for object detection based on CNN Alexnet using transfer learning approach. SCARA training dataset with 4000 images of two object categories associated with 20 different positions is created and labeled to train KSSnet model. The position of the detected object is included in prediction result at the output classification layer. This method achieved the state-of-the-art results at 100% precision of object detection, 100% accuracy for robotic positioning and 100% successful real-time robotic grasping within 0.38 seconds as detection time. A combination of Zerocross and Canny edge detectors is implemented on a circular object to simplify the place task. For accurate position measurement, the distortion of camera lens is removed using camera calibration technique where the measured position represents the desired location to place the grasped object. The result showed that the robot successfully moved to the measured position with positioning Root Mean Square Error (0.361, 0.184) mm and 100% for successful place detection",Real-time robotic grasping and localization using deep learning-based object detection technique,https://core.ac.uk/download/224837116.pdf,10.1109/I2CACIS.2019.8825093,'Institute of Electrical and Electronics Engineers (IEEE)',,core
266988181,2019-12-04T00:00:00,"Building an efﬁcient and reliable collision perception visual system is a challenging problem for future robots and autonomous vehicles. The biological visual neural networks, which have evolved over millions of years in nature and are working perfectly in the real world, could be ideal models for designing artiﬁcial vision systems. In the locust’s visual pathways, a lobula giant movement detector (LGMD), that is, the LGMD2, has been identiﬁed as a looming perception neuron that responds most strongly to darker approaching objects relative to their backgrounds; similar situations which many ground vehicles and robots are often faced with. However, little has been done on modeling the LGMD2 and investigating its potential in robotics and vehicles. In this article, we build an LGMD2 visual neural network which possesses the similar collision selectivity of an LGMD2 neuron in locust via the modeling of biased-ON and -OFF pathways splitting visual signals into parallel ON/OFF channels. With stronger inhibition (bias) in the ON pathway, this model responds selectively to darker looming objects. The proposed model has been tested systematically with a range of stimuli including real-world scenarios. It has also been implemented in a micro-mobile robot and tested with real-time experiments. The experimental results have veriﬁed the effectiveness and robustness of the proposed model for detecting darker looming objects against various dynamic and cluttered backgrounds",A Robust Collision Perception Visual Neural Network with Specific Selectivity to Darker Objects,https://core.ac.uk/download/266988181.pdf,10.1109/TCYB.2019.2946090,'Institute of Electrical and Electronics Engineers (IEEE)',,core
334885889,2019-11-22T00:00:00,"The manual design of soft robots and their controllers is notoriously
challenging, but it could be augmented---or, in some cases, entirely
replaced---by automated design tools. Machine learning algorithms can
automatically propose, test, and refine designs in simulation, and the most
promising ones can then be manufactured in reality (sim2real). However, it is
currently not known how to guarantee that behavior generated in simulation can
be preserved when deployed in reality. Although many previous studies have
devised training protocols that facilitate sim2real transfer of control
polices, little to no work has investigated the simulation-reality gap as a
function of morphology. This is due in part to an overall lack of tools capable
of systematically designing and rapidly manufacturing robots. Here we introduce
a low cost, open source, and modular soft robot design and construction kit,
and use it to simulate, fabricate, and measure the simulation-reality gap of
minimally complex yet soft, locomoting machines. We prove the scalability of
this approach by transferring an order of magnitude more robot designs from
simulation to reality than any other method. The kit and its instructions can
be found here: https://github.com/skriegman/sim2real4design",Scalable sim-to-real transfer of soft robot designs,http://arxiv.org/abs/1911.10290,,,,core
334865183,2019-11-18T00:00:00,"With new advancements in interaction techniques, character animation also
requires new methods, to support fields such as robotics, and VR/AR.
Interactive characters in such fields are becoming driven by AI which opens up
the possibility of non-linear and open-ended narratives that may even include
interaction with the real, physical world. This paper presents and describes
ERIK, an expressive inverse kinematics technique aimed at such applications.
Our technique allows an arbitrary kinematic chain, such as an arm, snake, or
robotic manipulator, to exhibit an expressive posture while aiming its
end-point towards a given target orientation. The technique runs in
interactive-time and does not require any pre-processing step such as e.g.
training in machine learning techniques, in order to support new embodiments or
new postures. That allows it to be integrated in an artist-friendly workflow,
bringing artists closer to the development of such AI-driven expressive
characters, by allowing them to use their typical animation tools of choice,
and to properly pre-visualize the animation during design-time, even on a real
robot. The full algorithmic specification is presented and described so that it
can be implemented and used throughout the communities of the various fields we
address. We demonstrate ERIK on different virtual kinematic structures, and
also on a low-fidelity robot that was crafted using wood and hobby-grade
servos, to show how well the technique performs even on a low-grade robot. Our
evaluation shows how well the technique performs, i.e., how well the character
is able to point at the target orientation, while minimally disrupting its
target expressive posture, and respecting its mechanical rotation limits","Expressive Inverse Kinematics Solving in Real-time for Virtual and
  Robotic Interactive Characters",http://arxiv.org/abs/1909.13875,,,,core
157737587,2019-05-14T00:00:00,"Fully-autonomous miniaturized robots (e.g., drones), with artificial
intelligence (AI) based visual navigation capabilities are extremely
challenging drivers of Internet-of-Things edge intelligence capabilities.
Visual navigation based on AI approaches, such as deep neural networks (DNNs)
are becoming pervasive for standard-size drones, but are considered out of
reach for nanodrones with size of a few cm${}^\mathrm{2}$. In this work, we
present the first (to the best of our knowledge) demonstration of a navigation
engine for autonomous nano-drones capable of closed-loop end-to-end DNN-based
visual navigation. To achieve this goal we developed a complete methodology for
parallel execution of complex DNNs directly on-bard of resource-constrained
milliwatt-scale nodes. Our system is based on GAP8, a novel parallel
ultra-low-power computing platform, and a 27 g commercial, open-source
CrazyFlie 2.0 nano-quadrotor. As part of our general methodology we discuss the
software mapping techniques that enable the state-of-the-art deep convolutional
neural network presented in [1] to be fully executed on-board within a strict 6
fps real-time constraint with no compromise in terms of flight results, while
all processing is done with only 64 mW on average. Our navigation engine is
flexible and can be used to span a wide performance range: at its peak
performance corner it achieves 18 fps while still consuming on average just
3.5% of the power envelope of the deployed nano-aircraft.Comment: 15 pages, 13 figures, 5 tables, 2 listings, accepted for publication
  in the IEEE Internet of Things Journal (IEEE IOTJ",A 64mW DNN-based Visual Navigation Engine for Autonomous Nano-Drones,http://arxiv.org/abs/1805.01831,10.1109/JIOT.2019.2917066,'Institute of Electrical and Electronics Engineers (IEEE)',,core
428438066,2019-01-01T00:00:00,"Search and Rescue (SAR) missions represent an important challenge in the robotics research field as they usually involve exceedingly variable-nature scenarios which require a high-level of autonomy and versatile decision-making capabilities. This challenge becomes even more relevant in the case of aerial robotic platforms owing to their limited payload and computational capabilities. In this paper, we present a fully-autonomous aerial robotic solution, for executing complex SAR missions in unstructured indoor environments. The proposed system is based on the combination of a complete hardware configuration and a flexible system architecture which allows the execution of high-level missions in a fully unsupervised manner (i.e. without human intervention). In order to obtain flexible and versatile behaviors from the proposed aerial robot, several learning-based capabilities have been integrated for target recognition and interaction. The target recognition capability includes a supervised learning classifier based on a computationally-efficient Convolutional Neural Network (CNN) model trained for target/background classification, while the capability to interact with the target for rescue operations introduces a novel Image-Based Visual Servoing (IBVS) algorithm which integrates a recent deep reinforcement learning method named Deep Deterministic Policy Gradients (DDPG). In order to train the aerial robot for performing IBVS tasks, a reinforcement learning framework has been developed, which integrates a deep reinforcement learning agent (e.g. DDPG) with a Gazebo-based simulator for aerial robotics. The proposed system has been validated in a wide range of simulation flights, using Gazebo and PX4 Software-In-The-Loop, and real flights in cluttered indoor environments, demonstrating the versatility of the proposed system in complex SAR missions",A Fully-Autonomous Aerial Robot for Search and Rescue Applications in Indoor Environments using Learning-Based Techniques,https://core.ac.uk/download/428438066.pdf,,,,core
334880612,2019-11-09T00:00:00,"We present a library to automatically embed signal processing and neural
network predictions into the material robots are made of. Deep and shallow
neural network models are first trained offline using state-of-the-art machine
learning tools and then transferred onto general purpose microcontrollers that
are co-located with a robot's sensors and actuators. We validate this approach
using multiple examples: a smart robotic tire for terrain classification, a
robotic finger sensor for load classification and a smart composite capable of
regressing impact source localization. In each example, sensing and computation
are embedded inside the material, creating artifacts that serve as stand-in
replacement for otherwise inert conventional parts. The open source software
library takes as inputs trained model files from higher level learning
software, such as Tensorflow/Keras, and outputs code that is readable in a
microcontroller that supports C. We compare the performance of this approach
for various embedded platforms. In particular, we show that low-cost
off-the-shelf microcontrollers can match the accuracy of a desktop computer,
while being fast enough for real-time applications at different neural network
configurations. We provide means to estimate the maximum number of parameters
that the hardware will support based on the microcontroller's specifications.Comment: Accepted for publication in the proceedings of the International
  Symposium on Robotics Research (ISRR) 2019. 16 page",Embedded Neural Networks for Robot Autonomy,http://arxiv.org/abs/1911.03848,,,,core
200920878,2019-04-01T00:00:00,"Small series production with a high level of variability is not suitable for full automation. So, a manual assembly process must be used, which can be improved by cooperative robots and assisted by augmented reality devices. The assisted assembly process needs reliable object recognition implementation. Currently used technologies with markers do not work reliably with objects without distinctive texture, for example, screws, nuts, and washers (single colored parts). The methodology presented in the paper introduces a new approach to object detection using deep learning networks trained remotely by 3D virtual models. Remote web application generates training input datasets from virtual 3D models. This new approach was evaluated by two different neural network models (Faster RCNN Inception v2 with SSD, MobileNet V2 with SSD). The main advantage of this approach is the very fast preparation of the 2D sample training dataset from virtual 3D models. The whole process can run in Cloud. The experiments were conducted with standard parts (nuts, screws, washers) and the recognition precision achieved was comparable with training by real samples. The learned models were tested by two different embedded devices with an Android operating system: Virtual Reality (VR) glasses, Cardboard (Samsung S7), and Augmented Reality (AR) smart glasses (Epson Moverio M350). The recognition processing delays of the learned models running in embedded devices based on an ARM processor and standard x86 processing unit were also tested for performance comparison",An Automated Training of Deep Learning Networks by 3D Virtual Models for Object Recognition,,10.3390/sym11040496,'MDPI AG',"[{'title': 'Symmetry', 'identifiers': ['2073-8994', 'issn:2073-8994']}]",core
200918397,2019-04-01T00:00:00,"This paper presents and experimentally validates a concept of end-to-end imitation learning for autonomous systems by using a composite architecture of convolutional neural network (ConvNet) and Long Short Term Memory (LSTM) neural network. In particular, a spatio-temporal deep neural network is developed, which learns to imitate the policy used by a human supervisor to drive a car-like robot in a maze environment. The spatial and temporal components of the imitation model are learned by using deep convolutional network and recurrent neural network architectures, respectively. The imitation model learns the policy of a human supervisor as a function of laser light detection and ranging (LIDAR) data, which is then used in real time to drive a robot in an autonomous fashion in a laboratory setting. The performance of the proposed model for imitation learning is compared with that of several other state-of-the-art methods, reported in the machine learning literature, for spatial and temporal modeling. The learned policy is implemented on a robot using a Nvidia Jetson TX2 board which, in turn, is validated on test tracks. The proposed spatio-temporal model outperforms several other off-the-shelf machine learning techniques to learn the policy",Neural Network-Based Learning from Demonstration of an Autonomous Ground Robot,,10.3390/machines7020024,'MDPI AG',"[{'title': 'Machines', 'identifiers': ['2075-1702', 'issn:2075-1702']}]",core
186281574,2019-03-07T00:00:00,"Contact-rich manipulation tasks in unstructured environments often require
both haptic and visual feedback. However, it is non-trivial to manually design
a robot controller that combines modalities with very different
characteristics. While deep reinforcement learning has shown success in
learning control policies for high-dimensional inputs, these algorithms are
generally intractable to deploy on real robots due to sample complexity. We use
self-supervision to learn a compact and multimodal representation of our
sensory inputs, which can then be used to improve the sample efficiency of our
policy learning. We evaluate our method on a peg insertion task, generalizing
over different geometry, configurations, and clearances, while being robust to
external perturbations. Results for simulated and real robot experiments are
presented.Comment: ICRA 201","Making Sense of Vision and Touch: Self-Supervised Learning of Multimodal
  Representations for Contact-Rich Tasks",http://arxiv.org/abs/1810.10191,,,,core
362654704,2019-08-22T00:00:00,"Flexible subsea risers are highly complex structures with multiple layers and varying material types. They are being used worldwide for oil and gas extraction in deep water. The complexity of these flexible risers and the hostile conditions in a deep-water environment present major challenges for non-intrusive, in-service
inspection. This paper presents a novel automated radiography inspection system to detect defects from X-ray images of such flexible risers. The concept of a robotic digital X-ray scanning system which addresses the needs and challenges of deepwater flexible riser inspections has been studied. The proposed radiography inspection system is an ideal solution for flexible risers as it penetrates through all the layers in the riser structure, providing detailed information on any damage to the various layers. The system integrates the advanced technologies of image processing, machine learning, and robot crawlers to automatically detect abnormal textures such as erosion, corrosion and strand damage, foreign objects and other critical features
from flexible risers. The performance of feature extraction, feature normalization, image processing, and machine learning for the purpose of defect detection have been tested and analysed. With smart integration and enhancement of different techniques, the limitations of each algorithms have been mitigated while the overall performance has been improved significantly. The detailed stages of the implementation are also discussed. The key features of the subsea riser inspection system are: (i) Real-time radiographic inspection result, (ii) Robotic system adaptable to various pipe types and sizes, (iii) High accuracy and reliability.Innovate UK grant agreement No 10408",Robotic Digital X-ray Scanning System for Deep Water Flexible Riser Inspection,https://core.ac.uk/download/362654704.pdf,10.3233/ATDE190024,'IOS Press',,core
227454014,2019-01-01T00:00:00,"In this work, we present the initial progressive build of a prototype system, dubbed MyCompanion. MyCompanion to improve assisted living, ultimately targeting alleviating social exclusion and loneliness. The system, based on a Raspberry Pi, connects to a user’s television set and tracks data from sensors placed around the house. An inbuilt artificial intelligence chat-bot then listens to, as well as instantiates, conversation with the user based on self-imposed triggers. Data from the sensors inform MyCompanion in real time user behaviour and records these readings. We also port MyCompanion software to an anthropomorphic version inside a humanoid robot for us to be able to give a physical dimension to the digital character",MyCompanion: A digital social companion for assisted living,,10.1007/978-3-030-29390-1_55,'Springer Science and Business Media LLC',,core
287621866,2019-01-01T00:00:00,"We are in world where autonomous systems, such as self-driving cars, surgical robots, robotic manipulators are becoming a reality. Such systems are considered \textit{safety-critical} since they interact with humans on a regular basis. Hence, before such systems can be integrated into our day to day life, we need to guarantee their safety. Recent success in machine learning (ML) and artificial intelligence (AI) has led to an increase in their use in real world robotic systems. For example, complex perception modules in self-driving cars and deep reinforcement learning controllers in robotic manipulators. Although powerful, they introduce an additional level of complexity when it comes to the formal analysis of autonomous systems.  In this thesis, such systems are designated as Learning-Based Cyber-Physical Systems~(LB-CPS). In this thesis, we take inspiration from the Oracle-Guided Inductive Synthesis~(OGIS) paradigm to develop frameworks which can aid in achieving formal guarantees in different stages of an autonomous system design and analysis pipeline. Furthermore, we show that to guarantee the safety of LB-CPS, the design (synthesis) and analysis (verification) must consider feedback from the other.  We consider five important parts of the design and analysis process and show a strong coupling among them, namely (i) Robust Control Synthesis from High Level Safety Specifications; (ii) Diagnosis and Repair of Safety Requirements for Control Synthesis; (iii) Counter-example Guided Data Augmentation for training high-accuracy ML models; (iv) Simulation-Guided Falsification and Verification against Adversarial Environments; and (v) Bridging Model and Real-World Gap. Finally, we introduce a software toolkit \verifai{} for the design and analysis of AI based systems, which was developed to provide a common formal platform to implement design and analysis frameworks for LB-CPS",Oracle-Guided Design and Analysis of Learning-Based Cyber-Physical Systems,https://core.ac.uk/download/287621866.pdf,,"eScholarship, University of California",,core
224952327,2019-07-05T08:34:00,"This master thesis addresses the energy management system design for robots operating in harsh and hazardous environment. Intelligent mechatronic solutions aim at minimizing personnel exposition to radiation and maximizing machine uptime, therefore rational energy usage of on-board power supply is crucial to ensure the fulﬁllment of these goals. Main objectives of the energy management system include possibility of performing reliable forecasts of power consumption, opportunity of monitoring the actual energy usage through battery state of charge and development of tools for planning energy efﬁcient operations. The problem is tackled using sequential approach. First, the power consumption model of robotic arm based on machine learning techniques is developed. Second, different methods to build a power consumption model of the locomotion are studied and the polynomial model is chosen according to established criteria: accuracy, robustness, speed and complexity. Third, the solution of a constrained nonlinear optimization problem of constructing an optimal velocity proﬁle for predeﬁned trajectory length and desired traverse time is presented for trapezoidal and minimum energy velocity proﬁles. Finally, a method to compute the battery state of charge is proposed. Created algorithms are tested on the robot in the real environment to prove their validity and evaluation of the achieved results is presented. Developed energy management system is implemented in the robot’s hardware and is integrated into the graphical interface of the robotic framework",Design of Energy Management System for robots working in harsh and hazardous environment,,,,,core
345057372,2019-11-01T00:00:00,"Deep- and reinforcement-learning techniques have increasingly required large sets of real data to achieve stable convergence and generalization, in the context of image-recognition, object-detection or motion-control strategies. On this subject, the research community lacks robust approaches to overcome unavailable real-world extensive data by means of realistic synthetic-information and domain-adaptation techniques. In this work, synthetic-learning strategies have been used for the vision-based autonomous following of a noncooperative multirotor. The complete maneuver was learned with synthetic images and high-dimensional low-level continuous robot states, with deep- and reinforcement-learning techniques for object detection and motion control, respectively. A novel motion-control strategy for object following is introduced where the camera gimbal movement is coupled with the multirotor motion during the multirotor following. Results confirm that our present framework can be used to deploy a vision-based task in real flight using synthetic data. It was extensively validated in both simulated and real-flight scenarios, providing proper results (following a multirotor up to 1.3 m/s in simulation and 0.3 m/s in real flights)",Vision-Based Multirotor Following Using Synthetic Learning Techniques,,10.3390/s19214794,'MDPI AG',,core
200825699,2019-04-21T00:00:00,"Autonomous navigation is an essential capability of smart mobility for mobile
robots. Traditional methods must have the environment map to plan a
collision-free path in workspace. Deep reinforcement learning (DRL) is a
promising technique to realize the autonomous navigation task without a map,
with which deep neural network can fit the mapping from observation to
reasonable action through explorations. It should not only memorize the trained
target, but more importantly, the planner can reason out the unseen goal. We
proposed a new motion planner based on deep reinforcement learning that can
arrive at new targets that have not been trained before in the indoor
environment with RGB image and odometry only. The model has a structure of
stacked Long Short-Term memory (LSTM). Finally, experiments were implemented in
both simulated and real environments. The source code is available:
https://github.com/marooncn/navbot","Learning to Navigate in Indoor Environments: from Memorizing to
  Reasoning",http://arxiv.org/abs/1904.06933,,,,core
334889712,2019-12-02T00:00:00,"We present a robotic setup for real-world testing and evaluation of
human-robot and human-human collaborative learning. Leveraging the
sample-efficiency of the Soft Actor-Critic algorithm, we have implemented a
robotic platform able to learn a non-trivial collaborative task with a human
partner, without pre-training in simulation, and using only 30 minutes of
real-world interactions. This enables us to study Human-Robot and Human-Human
collaborative learning through real-world interactions. We present preliminary
results, showing that state-of-the-art deep learning methods can take
human-robot collaborative learning a step closer to that of humans interacting
with each other.Comment: Presented at NeurIPS'19 Workshop on Robot Learning: Control and
  Interaction in the Real Worl","Human-Robot Collaboration via Deep Reinforcement Learning of Real-World
  Interactions",http://arxiv.org/abs/1912.01715,,,,core
200830009,2019-04-25T00:00:00,"Initially, robots were developed with the aim of making our life easier,
carrying out repetitive or dangerous tasks for humans. Although they were able
to perform these tasks, the latest generation of robots are being designed to
take a step further, by performing more complex tasks that have been carried
out by smart animals or humans up to date. To this end, inspiration needs to be
taken from biological examples. For instance, insects are able to optimally
solve complex environment navigation problems, and many researchers have
started to mimic how these insects behave. Recent interest in neuromorphic
engineering has motivated us to present a real-time, neuromorphic, spike-based
Central Pattern Generator of application in neurorobotics, using an
arthropod-like robot. A Spiking Neural Network was designed and implemented on
SpiNNaker. The network models a complex, online-change capable Central Pattern
Generator which generates three gaits for a hexapod robot locomotion.
Reconfigurable hardware was used to manage both the motors of the robot and the
real-time communication interface with the Spiking Neural Networks. Real-time
measurements confirm the simulation results, and locomotion tests show that
NeuroPod can perform the gaits without any balance loss or added delay",NeuroPod: a real-time neuromorphic spiking CPG applied to robotics,http://arxiv.org/abs/1904.11243,10.1016/j.neucom.2019.11.007,'Elsevier BV',,core
322489943,2019-12-02T00:00:00,"We present a robotic setup for real-world testing and evaluation of human-robot and human-human collaborative learning. Leveraging the sample-efficiency of the Soft Actor-Critic algorithm, we have implemented a robotic platform able to learn a non-trivial collaborative task with a human partner, without pre-training in simulation, and using only 30 minutes of real-world interactions. This enables us to study Human-Robot and Human-Human collaborative learning through real-world interactions. We present preliminary results, showing that state-of-the-art deep learning methods can take human-robot collaborative learning a step closer to that of humans interacting with each other",Human-robot collaboration via deep reinforcement learning of real-world interactions,,,'Center for Open Science',,core
237082871,2019-07-13T00:00:00,"To design efficient and safe systems capable of interacting with people, many researchers and engineers are interested in the development of accurate human-machine interface models that could serve the better design of such systems. These models often include humans. Therefore it is important to have suitable tools to perform a biomechanical analysis of the human body so that the ergonomic risk associated to a task can be classified. Such an analysis can be tailored to specific applications that range from the human-robot interaction to the assessment of the ergonomic risk served by technologies and methodologies of robotics.
Recent advances in wearable technologies allow for accurate and fast motion tracking. This is an enabling technology that makes it possible to obtain a biomechanical analysis of the human based on an inverse dynamics approach. The remaining critical problems are the determination of external loads and the assessment of the kinematic loops, which add internal loads to be considered in the analysis.
This thesis proposes a method to analyze the biomechanics of a human based on the information gathered from wearable sensors to estimate both the kinematic variables and the external loads needed to solve the inverse dynamics of the human. In this thesis, the musculoskeletal system is considered as a tree in which bones are considered as rigid bodies, and the action of muscles is summarized by wrenches applied at the spherical joints which connect bodies.
Based on the data coming from a network of wearable inertial sensors that capture the human motion and the video stream of an egocentric camera, a procedure for computing the inverse dynamics of the human body was developed. This procedure takes into account four ground support cases, i.e. single, double and no support, and four typical load conditions, under the assumption that the applied external load is due to an object carried by the user.
For external loads recognition, a computer vision approach based on deep learning techniques was used. A camera with a sufficient field of view and resolution to perform image classification was selected. To recognize the carried object, a set of images was recorded and labeled, and a state-of-the-art convolutional neural network (Yolo) was trained on this set. A dictionary containing the inertial properties of each object class of this set was created to provide the necessary information to the inverse dynamics algorithm.
In addition to the information about the carried object, this algorithm takes as inputs the positions, velocities, and accelerations of the human bodies (links). These positions are obtained from the inertial sensors, whereas joint velocities and accelerations are obtained through filtering.
To facilitate the integration of the proposed procedure with robot models in a human-robot interaction task, a modular environment i.e. ROS was selected.
In the perspective of using this procedure in human-robot interaction tasks which could require real-time constraints, and after a few tests with a simple robotic arm, it was noted that Gazebo's dynamics simulator was a suitable choice to solve the inverse dynamics of the human body.
Therefore, an algorithm for the biomechanical analysis of the human was studied. This algorithm is implemented in C++ and it exploits the ROS/OROCOS KDL libraries (package). Since Orocos-KDL requires URDF files for the definition of the mechanism and the data coming from the inertial sensors are in a BVH format, MATLAB scripts to perform the porting from BVH to URDF were created.
Although the implemented algorithms are applied to the human under the assumption of no contacts with the external world, except for the feet, the proposed method applies to more complex situations.
Indeed this method allows for applying loads to any of the bodies composing the mechanism (not necessarily a human) while allowing the user to provide policies to solve the redundancies, that occur when multiple contact points with the environment are present.
For the development and validation of the method, three activities were carried out. In the first, images of the objects to be recognized were recorded and labeled to train and test the object detection algorithm. In the second, human motion was simulated by applying a known trajectory to test the inverse dynamics algorithm. In the last, a participant was equipped with the body sensor network and gathered data were used as input of the proposed method.
Object detection proved to be robust and solid in the (laboratory) experimental condition, with percentages of correct associations higher than 77% for all the classes and a false negative rate smaller than 6%. The human motion simulation test showed that the proposed method provides the correct wrenches in the different support conditions while taking into account the humans' dynamics. Finally, the final tests showed that the upper body wrenches are correctly computed, whereas the lower body wrenches suffer from a gait segmentation problem that was not addressed in this prototyping phase",Deep Learning assisted closed chain inverse dynamics for Biomechanical analysis during object manipulation,,,'Pisa University Press',,core
429076619,2019-01-01T00:00:00,"Automatic pain detection is an important challenge in health computing. In this paper we report on our efforts to develop a real-time, real-world pain detection system from human facial expressions. Although many studies addressed this challenge, most of them use the same dataset for training and testing. There is no cross-check with other datasets or implementation in real-time to check performance on new data. This is problematic, as evidenced in this paper, because the classifiers overtrain on dataset-specific features. This limits realtime, real-world usage. In this paper, we investigate different methods of real-time pain detection. The training data uses a combination of pain and emotion datasets, unlike other papers. The best model shows an accuracy of 88.4% on a dataset including pain and 7 non-pain emotional expressions. Results suggest that convolutional neural networks (CNN) are not the best methods in some cases as they easily overtrain if the dataset is biased. Finally we implemented our pain detection method on a humanoid robot for physiotherapy. Our work highlights the importance of cross-corpus evaluation & real-time testing, as well as the need for a well balanced and ecologically valid pain dataset",Real-time pain detection in facial expressions for health robotics,,10.1109/aciiw.2019.8925192,'Institute of Electrical and Electronics Engineers (IEEE)',,core
395096808,2019-12-30T00:00:00,"Robotics systems are now increasingly widespread in our day-life. For instance, robots have been successfully used in several fields, like, agriculture, construction, defense, aerospace, and hospitality. However, there are still several issues to be addressed for allowing the large scale deployment of robots. Issues related to security, and manufacturing and operating costs are particularly relevant. Indeed, differently from industrial applications, service robots should be cheap and capable of operating in unknown, or partially-unknown environments, possibly with minimal human intervention. To deal with these challenges, in the last years the research community focused on deriving learning algorithms capable of providing flexibility and adaptability to the robots. In this context, the application of Machine Learning and Reinforcement Learning techniques turns out to be especially useful. In this manuscript, we propose different learning algorithms for robotics systems. In Chapter 2, we propose a solution for learning the geometrical model of a robot directly from data, combining proprioceptive measures with data collected with a 2D camera. Besides testing the accuracy of the kinematic models derived with real experiments, we validate the possibility of deriving a kinematic controller based on the model identified. Instead, in Chapter 3, we address the robot inverse dynamics problem. Our strategy relies on the fact that the robot inverse dynamics is a polynomial function in a particular input space. Besides characterizing the input space, we propose a data-driven solution based on Gaussian Process Regression (GPR). Given the type of each joint, we define a kernel named Geometrically Inspired Polynomial (GIP) kernel, which is given by the product of several polynomial kernels. To cope with the dimensionality of the resulting polynomial, we use a variation of the standard polynomial kernel, named Multiplicative Polynomial kernel, further discussed in Chapter 6. Tests performed on simulated and real environments show that, compared to other data-driven solutions, the GIP kernel-based estimator is more accurate and data-efficient.

In Chapter 4, we propose a proprioceptive collision detection algorithm based on GPR. Compared to other proprioceptive approaches, we closely inspect the robot behaviors in quasi-static configurations, namely, configurations in which joint velocities are null or close to zero. Such configurations are particularly relevant in the Collaborative Robotics context, where humans and robots work side-by-side sharing the same environment. Experimental results performed with a UR10 robot confirm the relevance of the problem and the effectiveness of the proposed solution.

Finally, in Chapter 5, we present MC-PILCO, a model-based policy search algorithm inspired by the PILCO algorithm. As the original PILCO algorithm, MC-PILCO models the system evolution relying on GPR, and improves the control policy minimizing the expected value of a cost function. However, instead of approximating the expected cost by moment matching, MC-PILCO approximates the expected cost with a Monte Carlo particle-based approach; no assumption about the type of GPR model is necessary. Thus, MC-PILCO allows more freedom in designing the GPR models, possibly leading to better models of the system dynamics. Results obtained in a simulated environment show consistent improvements with respect to the original algorithm, both in terms of speed and success rate",Learning algorithms for robotics systems,,,,,core
226754253,2019,"Fully-autonomous miniaturized robots (e.g., drones), with artificial intelligence (AI) based visual navigation capabilities, are extremely challenging drivers of Internet-of-Things edge intelligence capabilities. Visual navigation based on AI approaches, such as deep neural networks (DNNs) are becoming pervasive for standard-size drones, but are considered out of reach for nano-drones with a size of a few . In this work, we present the first (to the best of our knowledge) demonstration of a navigation engine for autonomous nano-drones capable of closed-loop end-to-end DNN-based visual navigation. To achieve this goal we developed a complete methodology for parallel execution of complex DNNs directly on board resource-constrained milliwatt-scale nodes. Our system is based on GAP8, a novel parallel ultra-low-power computing platform, and a 27 commercial, open-source CrazyFlie 2.0 nano-quadrotor. As part of our general methodology, we discuss the software mapping techniques that enable the state-of-the-art deep convolutional neural network presented in dronet to be fully executed aboard within a strict real-time constraint with no compromise in terms of flight results, while all processing is done with only on average. Our navigation engine is flexible and can be used to span a wide performance range: at its peak performance corner, it achieves while still consuming on average just of the power envelope of the deployed nano-aircraft. To share our key findings with the embedded and robotics communities and foster further developments in autonomous nano-UAVs, we publicly release all our code, datasets, and trained networks",A 64mW DNN-based Visual Navigation Engine for Autonomous Nano-Drones,,10.1109/JIOT.2019.2917066,,,core
211061457,2019-01-01T00:00:00,"Business Process Management (BPM) is a central element of today’s organizations. Over the years, its main focus has been the support of business processes (BPs) in highly controlled domains. However—in the current era of Big Data and Internet-of-Things—several real-world domains are becoming cyber-physical (e.g., consider the shift from traditional manufacturing to Industry 4.0), characterized by ever-changing requirements, unpredictable environments and increasing amounts of data and events that influence the enactment of BPs. In such unconstrained settings, BPM professionals lack the needed knowledge to model all possible BP variants/contingencies at the outset. Consequently, BPM systems must increase their level of automation to provide the reactivity and flexibility necessary for process management. On the other hand, the Artificial Intelligence (AI) community has concentrated its efforts on investigating dynamic domains that involve active control of computational entities and physical devices (e.g., robots, software agents). In this context, automated planning, which is one of the oldest areas in AI, is conceived as a model-based approach to synthesize autonomous behaviors in automated way from a model. In this paper, we discuss how automated planning techniques can be leveraged to enable new levels of automation and support for solving concrete problems in the BPM field that were previously tackled with hard-coded solutions. To this aim, we first propose a methodology that shows how a researcher/practitioner should approach the task of encoding a concrete problem as an appropriate planning problem. Then, we discuss the required steps to integrate the planning technology in BPM environments. Finally, we show some concrete examples of the successful application of planning techniques to the different stages of the BPM life cycle",Automated planning for business process management,,10.1007/s13740-018-0096-0,'Springer Science and Business Media LLC',,core
334876638,2019-10-30T00:00:00,"Semantic understanding of scenes in three-dimensional space (3D) is a
quintessential part of robotics oriented applications such as autonomous
driving as it provides geometric cues such as size, orientation and true
distance of separation to objects which are crucial for taking mission critical
decisions. As a first step, in this work we investigate the possibility of
semantically classifying different parts of a given scene in 3D by learning the
underlying geometric context in addition to the texture cues BUT in the absence
of labelled real-world datasets. To this end we generate a large number of
synthetic scenes, their pixel-wise labels and corresponding 3D representations
using CARLA software framework. We then build a deep neural network that learns
underlying category specific 3D representation and texture cues from color
information of the rendered synthetic scenes. Further on we apply the learned
model on different real world datasets to evaluate its performance. Our
preliminary investigation of results show that the neural network is able to
learn the geometric context from synthetic scenes and effectively apply this
knowledge to classify each point of a 3D representation of a scene in
real-world.Comment: Accepted in 3rd Edition of Deep Learning for Automated Driving (DLAD)
  workshop, IEEE International Conference on Intelligent Transportation Systems
  (ITSC'19) [see
  https://sites.google.com/view/dlad-bp-itsc2019/schedule?authuser=0#h.p_gI84BCoB0_bJ",Multi Modal Semantic Segmentation using Synthetic Data,http://arxiv.org/abs/1910.13676,,,,core
334893461,2019-12-13T00:00:00,"Whenever we are addressing a specific object or refer to a certain spatial
location, we are using referential or deictic gestures usually accompanied by
some verbal description. Especially pointing gestures are necessary to dissolve
ambiguities in a scene and they are of crucial importance when verbal
communication may fail due to environmental conditions or when two persons
simply do not speak the same language. With the currently increasing advances
of humanoid robots and their future integration in domestic domains, the
development of gesture interfaces complementing human-robot interaction
scenarios is of substantial interest. The implementation of an intuitive
gesture scenario is still challenging because both the pointing intention and
the corresponding object have to be correctly recognized in real-time. The
demand increases when considering pointing gestures in a cluttered environment,
as is the case in households. Also, humans perform pointing in many different
ways and those variations have to be captured. Research in this field often
proposes a set of geometrical computations which do not scale well with the
number of gestures and objects, use specific markers or a predefined set of
pointing directions. In this paper, we propose an unsupervised learning
approach to model the distribution of pointing gestures using a
growing-when-required (GWR) network. We introduce an interaction scenario with
a humanoid robot and define so-called ambiguity classes. Our implementation for
the hand and object detection is independent of any markers or skeleton models,
thus it can be easily reproduced. Our evaluation comparing a baseline computer
vision approach with our GWR model shows that the pointing-object association
is well learned even in cases of ambiguities resulting from close object
proximity","Solving Visual Object Ambiguities when Pointing: An Unsupervised
  Learning Approach",http://arxiv.org/abs/1912.06449,,,,core
289965278,2019,"The goal of this thesis work is to implement a convolutional neural network on an FPGA device with the capability of recognising human sign language. The set of gestures that the neural network can identify has been taken from the Swedish sign language, and it consists of the signs used for representing the letters of the Swedish alphabet (a.k.a. fingerspelling). The motivation driving this project lies in the tremendous interest aroused by neural networks in recent years for its ability for solving complex problems and its capacity to learn by example. More specifically, convolutional neural networks are being extensively used for image classification, and this project aims to design a hardware accelerator to compute the convolutional layers of such type of network topology and test its accuracy and performance when dealing with human sign language. The network topology of choice is Zynqnet, proposed by Gschwend in 2016, which is a topology that has already been implemented successfully on an FPGA platform and it has been trained with the large picture dataset provided by ImageNet, for its popular image recognition contest. In this regard, the aim of this work is not to propose a new neural network topology but to re-use an existent one by introducing some improvements like the utilisation of an 8-bit dynamic fixed-point scheme and challenge it with a different but related task, like human sign language recognition. The methodology followed to carry out a successful hardware implementation has consisted, first, of the installation and setup of a reliable framework used for the training of the neural network. Different frameworks were tried out, like MATLAB or Caffe, but finally, DIGITS from NVIDIA was the more convenient due to its graphical environment and because it provides all the compatibility and drivers needed to run together with the GPU used in this project. Then, an image dataset of more than 13,000 pictures of hand gestures has been built up to grant enough input data for the framework to fine-tune ZynqNet for the new task, i.e. to provide the neural network with the ability to classify the different hand-signs into its corresponding alphabet letter. In parallel, the Register-Transfer Level (RTL) abstraction of the hardware architecture has been generated using a High-Level Synthesis tool chain, in which the algorithmic descriptions are written in C/C++. Finally, the validation of the design has been done by means of co-simulation techniques where the golden data obtained with the C test bench is compared with the output data of the RTL implementation, and all of it within the simulation environment provided by the Vivado Design Suite. As a result, the best-performing obtained solution achieved an accuracy of 80.1\% in the inference test and a frame rate of 6.4 FPS with a clock frequency of 250 MHz.Neural networks are becoming more and more ubiquitous in our everyday lives, many times in ways that we do not even realise. Artificial Intelligence (AI) is extensively used nowadays to improve the user's experience with digital technologies, for instance, the on-the-fly translation service provided by Skype and Google. In other fields like robotics, improvements in object detection from the hand of machine learning, allow robots and autonomous cars to take better decisions. And in medicine, automatic detection of blood diseases like leukaemia and lymphoma, powered by neural networks algorithms, have accelerated and improved diagnosis. So the list of applications found in many diverse fields goes on and on. Now, let's picture yourself in the hypothetical situation in which you have a friend or a relative who has been born with a hearing impairment. This person has been taught sign language from an early age on a specialised school, and you would like to learn sign language too so you both can have meaningful and pleasant communication. Furthermore, you want to be able to help this person in day-to-day situations where deaf people can be in clear disadvantage like a routine visit to the doctor or administrative processes. You learn from one of your classmates at the university about a mobile app which employs a deep neural network to recognise human sign language just by using the phone camera. The application translates the captured sequence of gestures from video to written text in real-time and automatically reproduces the message in the phone speaker. It also includes a sign language tutorial which can help you to rapidly learn to communicate by using your hands. The software records your gestures and improves your learning abilities by telling how accurate are your movements. Well, the situation just described is something that I believe it is not far to happen. The computational power found in embedded systems such as mobile phones is growing by the day. So far, at least, one can find mostly solutions that work one-way, that is, they convert speech into sign language, by mapping spoken language to signs and using a virtual animated human-like avatar. Fewer solutions can translate sign language into speech. Some of them use special gloves with position sensors not so pleasant to wear by the signer person, and others use 3D cameras that can pick the speaker's body gestures and then compare the obtained frame sequence with a reference frame stored in a dictionary. These solutions though, rely on mapping methods and are limited in terms of the number of gestures they can interpret. I really believe that neural networks are a game changer and they will bring powerful, elegant solutions to the kind of problems described above. This thesis work aims to provide proof-of-concept of a feasible implementation of a neural network trained for recognition of human sign language. The number of gestures to recognise is limited to the Swedish alphabet, and the network must show an acceptable level of prediction accuracy, throughput and area utilisation. The content of this thesis is addressed to a variety of public: from people interested in neural networks in general, and the significant development experienced in the field in recent years, to people interested in learning the basic concepts and the methodology for training neural networks, or practitioners who look to implement a deep learning model on an AI accelerator",Implementation of an 8-bit Dynamic Fixed-Point Convolutional Neural Network for Human Sign Language Recognition on a Xilinx FPGA Board,,,Lunds universitet/Institutionen för elektro- och informationsteknik,,core
200412086,2019-05-10T05:09:29Z,"<p>The control of soft continuum robots is challenging owing to their mechanical elasticity and complex dynamics. An additional challenge emerges when we want to apply Learning from Demonstration (LfD) and need to collect necessary demonstrations due to the inherent control difficulty. In this paper, we provide a multi-level architecture from low-level control to high-level motion planning for the Bionic Handling Assistant (BHA) robot. We deploy learning across all levels to enable the application of LfD for a real-world manipulation task. To record the demonstrations, an actively compliant controller is used. A variant of dynamical systems' application that are able to encode both position and orientation then maps the recorded 6D end-effector pose data into a virtual attractor space. A recent LfD method encodes the pose attractors within the same model for point-to-point motion planning. In the proposed architecture, hybrid models that combine an analytical approach and machine learning techniques are used to overcome the inherent slow dynamics and model imprecision of the BHA. The performance and generalization capability of the proposed multi-level approach are evaluated in simulation and with the real BHA robot in an apple-picking scenario which requires high accuracy to control the pose of the robot's end-effector.</p",Multi-level control architecture for Bionic Handling Assistant robot augmented by learning from demonstration for apple-picking,,10.6084/m9.figshare.8107610.v1,,,core
237080308,2019-01-01T00:00:00,"Intelligent robotic coworkers are considered a valuable addition in many application areas. This applies not only to terrestrial domains, but also to the exploration of our solar system. As humankind moves toward an ever increasing presence in space, infrastructure has to be constructed and maintained on distant planets such as Mars. AI-enabled robots will play a major role in this scenario. The space agencies envisage robotic co-workers to be deployed to set-up habitats, energy, and return vessels for future human scientists. By leveraging AI planning methods, this vision has already become one step closer to reality. In the METERON SUPVIS Justin experiment, the intelligent robotic coworker Rollin’ Justin was controlled from Astronauts aboard the International Space Station (ISS) in order to maintain a Martian mock-up solar panel farm located on Earth to demonstrate the technology readiness of the developed methods. For this work, the system is demonstrated at AAAI 2019, controlling Rollin’ Justin located in Munich, Germany from Honolulu, Hawaii",Global Remote Operation of Intelligent Space Robot Assistants,https://core.ac.uk/download/237080308.pdf,,,,core
304994832,2019-05-22T13:05:45,"A growing number of commercial and enterprise systems are increasingly relying on compute-intensive machine learning algorithms. While the demand for these apaplications is growing, the performance benefits from general-purpose platforms is diminishing. This challenge has coincided with the explosion of data where the rate of data generation has reached an overwhelming level that is beyond the capabilities of current computing systems. Therefore, applications such as machine learning and robotics can benefit from hardware acceleration. Traditionally, to accelerate a set of workloads, we pro- file the code optimized for CPUs and offload the hot functions on hardware compute units designed specially for that particular function, hence providing higher performance and energy efficiency. Instead in this work, we take a revolutionary approach where we delve into the algorithmic properties of applications to define domain-generic hardware acceleration solutions. We leverage the property that a wide range of machine learning algorithms can be modeled as stochastic optimization problems. Using this insight we devise compute stacks for hardware acceleration that are built independent of the CPU. These stacks expose a high-level mathematical programming interface and automatically generate accelerators for users who have limited knowledge about hardware design, but can benefit from large performance and efficiency gains for their programs. 
Keeping these ambitious goals in mind, our work (1) strikes a balance between generality and specialization by breaking the long-held traditional abstraction of the Instruction Set Architecture (ISA) in favor of a more algorithm-centric approach; (2) develops hard- ware acceleration frameworks by co-designing a language, compiler, runtime system, and hardware to provide high performance and efficiency, in addition to flexibility and programmability; (3) segregates algorithmic specification from implementation to shield the programmer from continual hardware/software modifications while allowing them to benefit from the emerging heterogeneity of modern compute platforms; and (4) develops real cross-stack prototypes to evaluate these innovative solutions in a real-world setting and make them open-source to maximize community engagement and industry impact. Our work TABLA (http://act-lab.org/artifacts/tabla/) is public, and defines the very first open-source hardware platform for machine learning and artificial intelligence.Ph.D",Balancing generality and specialization for machine learning in the post-ISA era,https://core.ac.uk/download/304994832.pdf,,Georgia Institute of Technology,,core
185531452,2019-01-01T00:00:00,"The book covers a variety of topics in Information and Communications Technology (ICT) and their impact on innovation and business. The authors discuss various innovations, business and industrial motivations, and impact on humans and the interplay between those factors in terms of finance, demand, and competition. Topics discussed include the convergence of Machine to Machine (M2M), Internet of Things (IoT), Social, and Big Data. They also discuss AI and its integration into technologies from machine learning, predictive analytics, security software, to intelligent agents, and many more. Contributions come from academics and professionals around the world.

Covers the most recent practices in ICT related topics pertaining to technological growth, innovation, and business; Presents a survey on the most recent technological areas revolutionizing how humans communicate and interact; Features four sections: IoT, Wireless Ad Hoc & Sensor Networks, Fog Computing, and Big Data Analytics.(Chapter) The recent advancements in robotic systems set new challenges for robotic simulation software, particularly for planning. It requires the realistic behavior of the robots and the objects in the simulation environment by incorporating their dynamics. Furthermore, it requires the capability of reasoning about the action effects. To cope with these challenges, this study proposes an open-source simulation tool for knowledge-oriented physics-based motion planning by extending The Kautham Project, a C++ based open-source simulation tool for motion planning. The proposed simulation tool provides a flexible way to incorporate the physics, knowledge and reasoning in planning process. Moreover, it provides ROS-based interface to handle the manipulation actions (such as push/pull) and an easy way to communicate with the real robotsPeer ReviewedPostprint (author's final draft",A tool for knowledge-oriented physics-based motion planning and simulation,https://core.ac.uk/download/185531452.pdf,10.1007/978-3-319-99966-1,'Springer Science and Business Media LLC',,core
294759151,2019-01-01T00:00:00,"The current PhD thesis addresses the formulation and implementation of a methodological framework for robot Learning from Demonstration (LfD). The latter refers to methodologies that develop behavioral policies from example state-to-action mappings. To this end, we study the reciprocal interaction of perception and action, in order to teach robots a repertoire of novel action behaviors. Based on that, we design, develop and implement a robust imitation framework, termed IMFO (IMitation Framework by Observation), that facilitates imitation learning and relevant applications in human-robot interaction (HRI) tasks. IMFO can cope with the reproduction of learned (i.e. previously observed) actions, as well as novel ones. Mapping of human actions to the respective robotic ones is achieved via an indeterminate depiction, termed latent space representation. The latter accomplishes a compact, yet precise abstraction of action trajectories, effectively representing high dimensional raw actions in a low dimensional space.Moreover, throughout this thesis, we examine the role of time in LfD by enhancing the aforementioned framework with the notion of learning both the spatial and temporal characteristics of human motions. Accordingly, learned actions can be subsequently reproduced in the context of more complex time-informed HRI scenarios. Unlike previous LfD methods that cope only with the spatial traits of an action, the formulated scheme effectively encompasses spatial and temporal aspects. Extensive experimentation with a variety of real robotic platforms demonstrates the robustness and applicability of the introduced integrated LfD scheme. Learned actions are reproduced under the high level control of a time-informed task planner. During the implementation of the studied scenarios, temporal and physical constraints may impose speed adaptations in the performed actions. The employed latent space representation readily supports such variations, giving rise to novel actions in the temporal domain. Experimental results demonstrate the effectiveness of the proposed enhanced imitation scheme in the implementation of HRI scenarios. Additionally, a set of well-defined evaluation metrics are introduced to assess the validity of the proposed approach considering the temporal and spatial consistency of the reproduced behaviors. A noteworthy extension of the above regards force-based object grasping for executing sensitive manipulation tasks. This is also treated in the current thesis via a novel supervised learning scheme, termed SLF (Supervised Learning for Force-based manipulation). SLF is formulated as a three-stage process: (a) supervised trial-execution in simulation to acquire sufficient training data; (b) training to facilitate grasp learning with suitable robot-arm pose and lifting force; (c) grasp execution in simulation. Subsequently, following sim-to-real transfer, operation in real environments is achieved in addition to simulated ones, generalizing also for objects not included in the trial sessions. The proposed learning scheme is demonstrated in object lifting tasks where the applied force varies for different objects with similar contact friction coefficients, and likewise the grasping pose. Experimental results on the manipulator YuMi show that the robot is able to effectively reproduce demanding lifting and manipulation tasks after learning is accomplished. In summary, our thesis has studied LfD and has contributed with a novel approach that introduced latent space representations to encode the action characteristics. A framework implementation (IMFO) of our approach allowed extensive experimentation and also conduction of HRI scenarios. The inclusion of temporal aspects in our approach enhanced it to cope with complex, real-life interactions. Finally, the extension of IMFO with force-based grasping facilitated manipulation tasks with sensitive objects.Η παρούσα διδακτορική διατριβή αφορά τη μελέτη, ανάπτυξη και εφαρμογή, μεθόδων Μηχανικής Μάθησης μέσω Παρατήρησης (Learning from Demonstration) με στόχο την ρομποτική αναπαραγωγή δράσεων χειρισμού. Η μεθοδολογία αυτή στηρίζεται στην δημιουργία μιας αντιστοίχισης (mapping) μεταξύ της κινηματικής του ανθρώπινου χεριού και ενός ρομποτικού βραχίονα, ή πιο συγκεκριμένα μεταξύ του πολυδιάστατου χώρου των κινήσεων του ανθρώπου (human actor) με τον επίσης πολυδιάστατο χώρο δράσης του ρομπότ. Η συσχέτιση των ανθρώπινων ενεργειών με αντίστοιχες ρομποτικές, επιτυγχάνεται μέσω μιας άδηλης αναπαράστασης, που ονομάζεται λανθάνουσα απεικόνιση χώρου (latent space). Πιο συγκεκριμένα, μελετάμε την αμοιβαία αλληλεπίδραση της αντίληψης και της δράσης, προκειμένου να διδάξουμε τα ρομπότ μια ποικιλία από νέες κινήσεις χειρός. Ως εκ τούτου, υλοποιήθηκε ένα μεθοδολογικό πλαίσιο μάθησης μέσω παρατήρησης, το οποίο ονομάζεται IMFO (Imitation Framework by Observation), που διευκολύνει την αναπαραγωγή μαθημένων και νέων κινήσεων χειρισμού από ένα ρομπότ (manipulation tasks) και, παράλληλα, έχει ευρεία εφαρμογή σε σενάρια αλληλεπίδρασης ανθρώπου-ρομπότ (HRI) σε καθημερινά περιβάλλοντα.Επιπλέον, σε αυτή τη διατριβή, εξετάζουμε το ρόλο της χρονικής διάρκειας εκτέλεσης μιας κίνησης μέσα από τη διαδικασία μάθησης από παρατήρηση, ενισχύοντας το διαμορφωμένο πλαίσιο IMFO με την δυνατότητα αναπαράστασης και αναπαραγωγής τόσο των χωρικών όσο και των χρονικών χαρακτηριστικών των ανθρώπινων κινήσεων. Σε αντίθεση με άλλες μεθόδους μάθησης μέσω παρατήρησης (LfD) που περιγράφουν την εκτελούμενη δράση μόνο με βάση τα χωρικά χαρακτηριστικά της, η προτεινόμενη μεθοδολογία ενισχύει την αναπαραγωγή των χωροχρονικών πτυχών μιας κίνησης επιτρέποντας την αποτελεσματική εφαρμογή της σε πιο σύνθετα σενάρια HRI, όπου η χρονική αλληλουχία των δράσεων είναι σημαντική. Επιπρόσθετα, εισάγεται ένα σύνολο καλά καθορισμένων μετρικών αξιολόγησης (evaluation metrics) για να αποτιμηθεί η εγκυρότητα της προτεινόμενης προσέγγισης λαμβάνοντας υπόψη τη χρονική και χωρική συνέπεια των αναπαραγόμενων συμπεριφορών. Μια αξιοσημείωτη επέκταση του προαναφερθέντος πλαισίου αναφέρεται στην εκμάθηση της δύναμης που επιβάλλεται από τον χρήστη για την επιτυχημένη εκτέλεση λεπτών χειρισμών. Αυτή η διαδικασία παρουσιάζεται επίσης στην παρούσα διατριβή μέσω ενός νέου πλαισίου εποπτευόμενης μάθησης, το οποίο ονομάζεται SLF (Supervised Learning scheme for Force-based manipulation). Το SLF διατυπώνεται ως μία διαδικασία τριών σταδίων: (α) επιβλεπόμενη διαδικασία εκτέλεσης κινήσεων χειρισμού σε προσομοίωση για την απόκτηση επαρκών δεδομένων, (β) διαδικασία εκπαίδευσης (training) για τη διευκόλυνση της μάθησης κινήσεων χειρισμού με την κατάλληλη προσαρμογή του καρπού και της δύναμη πιασίματος και μεταφοράς και (γ) εκτέλεση της κίνησης από ρομποτικό βραχίονα σε προσομοίωση. Στη συνέχεια, με τη χρήση της μεθόδου sim-to-real transfer,  επιτυγχάνεται αναπαραγωγή των μαθημένων δράσεων σε πραγματικά περιβάλλοντα γενικεύοντας την εφαρμογή του πλαισίου μάθησης σε επιπλέον συνθήκες χειρισμού εύθραυστων αντικειμένων. Τα αποτελέσματα με τη χρήση του ρομποτικού βραχίονα YuMi, σε πειράματα με διαφορετικά αντικείμενα με παρόμοιους συντελεστές τριβής, και εναλλακτικές πόζες πιασίματος, αποδεικνύουν ότι το ρομπότ είναι σε θέση να αναπαράγει αποτελεσματικά απαιτητικές κινήσεις μεταφοράς και χειρισμού μετά την ολοκλήρωση της διαδικασίας μάθησης. Συνοπτικά, η παρούσα διατριβή μελετά την διαδικασία μάθησης μέσω παρατήρησης συνεισφέροντας με μια νέα προσέγγιση που εισάγει την μελέτη δράσεων χειρισμού αντικειμένων μέσα από έναν χώρο μειωμένων διαστάσεων, για την εύκολη και συμπαγή  κωδικοποίηση των επιμέρους χαρακτηριστικών των δράσεων. Ταυτόχρονα μελετώνται τα χρονικά χαρακτηριστικά των κινήσεων ώστε να ενισχυθεί η εφαρμογή της μεθόδου σε σύνθετες, πραγματικές συνθήκες που απαιτούν χρονική ακρίβεια αναπαραγωγής. Τέλος, η διαμόρφωση μιας γενικευμένης διαδικασίας εποπτευόμενης μάθησης για τον χειρισμό εύθραυστων αντικείμενων αναβαθμίζει περαιτέρω το αρχικό πλαίσιο μάθησης",Μάθηση μέσω παρατήρησης για την επίτευξη ρομποτικών δράσεων χειρισμού,,,Πανεπιστήμιο Κρήτης,,core
294759144,2019-01-01T00:00:00,"Center of Mass (CoM) estimation realizes a crucial role in legged locomotion. Most walking pattern generators and real-time gait stabilizers commonly assume that the CoM position and velocity are available for feedback. In this thesis we present one of the first 3D-CoM state estimators for humanoid robot walking. The proposed estimation scheme fuses effectively joint encoder, inertial, and feet pressure measurements with an Extended Kalman Filter (EKF) to accurately estimate the 3D-CoM position, velocity, and external forces acting on the CoM. Furthermore, it directly considers the presence of uneven terrain and the body’s angular momentum rate and thus effectively couples the frontal with the lateral plane dynamics, without relying on feet Force/Torque (F/T) sensing.         Nevertheless, it is common practice to transform the measurements to a world frame of reference and estimate the CoM with respect to the world frame. Consequently, the robot’s base and support foot pose are mandatory and need to be co-estimated. To this end, we extend a well-established in literature floating mass estimator to account for the support foot dynamics and fuse kinematic-inertial measurements with the Error State Kalman Filter (ESKF) to appropriately handle the overparametrization of rotations. In such a way, a cascade state estimation scheme consisting of a base and a CoM estimator is formed and coined State Estimation RObot Walking (SEROW). Additionally, we employ Visual Odometry (VO) and/or LIDAR Odometry (LO) measurements to correct the kinematic drift caused by slippage during walking. Unfortunately, such measurements suffer from outliers in a dynamic environment, since frequently it is assumed that only the robot is in motion and the world around is static. Thus, we introduce the Robust Gaussian ESKF (RGESKF) to automatically detect and reject outliers without relying on any prior knowledge on measurement distributions or finely tuned thresholds. Therefore, SEROW is robustified and is suitable for dynamic human environments. In order to reinforce further research endeavors, SEROW is released to the robotic community as an open-source ROS/C++ package.Up to date control and state estimation schemes readily assume that feet contact status is known a priori. Contact detection is an important and largely unexplored topic in contemporary humanoid robotics research. In this thesis, we elaborate on a broader question: in which gait phase is the robot currently in? To this end, we propose a holistic framework based on unsupervised learning from proprioceptive sensing that accurately and efficiently addresses this problem. More specifically, we robustly detect one of the three gait- phases, namely Left Single Support (LSS), Double Support (DS), and Right Single Support(RSS) utilizing joint encoder, IMU, and F/T measurements. Initially, dimensionality reduction with Principal Components Analysis (PCA) or autoencoders is performed to extract useful features, obtain a compact representation, and reduce the noise. Next, clustering is performed on the low-dimensional latent space with Gaussian Mixture Models (GMMs) and three dense clusters corresponding to the gait-phases are obtained. Interestingly, it is demonstrated that the gait phase dynamics are low-dimensional which is another indication pointing towards locomotion being a low dimensional skill. Accordingly, given that the proposed framework utilizes measurements from sensors that are commonly available on humanoids nowadays, we offer the Gait-phase Estimation Module (GEM), an open-source ROS/Python implementation to the robotic community.         SEROW and GEM have been quantitatively and qualitatively assessed in terms of accuracy and efficiency both in simulation and under real-world conditions. Initially, a simulated robot in MATLAB and NASA’s Valkyrie humanoid robot in ROS/Gazebo were employed to establish the proposed schemes with uneven/rough terrain gaits. Subsequently, the proposed schemes were integrated on a) the small size NAO humanoid robot v4.0 and b) the adult size WALK-MAN v2.0 for experimental validation. With NAO, SEROW was implemented on the robot to provide the necessary feedback for motion planning and real-time gait stabilization to achieve omni-directional locomotion even on outdoor/uneven terrains. Additionally, SEROW was used in footstep planning and also in Visual SLAM with the same robot. Regarding WALK-MAN v2.0, SEROW was executed onboard with kinematic-inertial and F/T data to provide base and CoM feedback in real-time. Furthermore, VO has also been considered to correct the kinematic drift while walking and facilitate possible footstep planning. GEM was also employed to estimate the gait phase in WALK-MAN’s dynamic gaits.Summarizing, a robust nonlinear state estimator is proposed for humanoid robot walking. Nevertheless, this scheme can be readily extended to other type of legged robots such as quadrupeds, since they share the same fundamental principles.Η εκτίμηση του Κέντρου Μάζας (CoM) διαδραματίζει κρίσιμο ρόλο στη ρομποτική βάδιση. Οι περισσότεροι σχεδιαστές κίνησης και ελεγκτές βάδισης πραγματικού χρόνου υποθέτουν ότι η θέση και η ταχύτητα του CoM είναι διαθέσιμες για ανατροφοδότηση ανά πάσα στιγμή. Σε αυτή τη διατριβή παρουσιάζουμε έναν από τους πρώτους τρισδιάστατους εκτιμητές κατάστασης CoM για το περπάτημα των ανθρωποειδών ρομπότ. Ο προτεινόμενος εκτιμητής συνδυάζει αποτελεσματικά τις μετρήσεις από αισθητήρες πίεσης στα πόδια, κωδικοποιητές στις αρθρώσεις και αδρανειακής μονάδας (IMU) στο σώμα με ένα Εκτεταμένο Φίλτρο Κάλμαν (EKF) για την ακριβή εκτίμηση τόσο της θέσης και της ταχύτητας του CoM αλλά και των εξωτερικών δυνάμεων που δρουν πάνω σε αυτό. Επιπλέον, λαμβάνει υπόψιν την ανωμαλότητα του εδάφους και την στροφορμή του σώματος με αποτέλεσμα να συνδυάζει το μετωπικό με το πλευρικό επίπεδο κίνησης, χωρίς να βασίζεται σε αισθητήρες δύναμης / ροπής (F/T) στα πόδια. Ωστόσο, είναι κοινή πρακτική να επιχειρείται η μετατροπή των μετρήσεων σε ένα αδρανειακό σύστημα αναφοράς ώστε η εκτίμηση του CoM να γίνεται σε σχέση με αυτό. Κατά συνέπεια, για την επίτευξη του παραπάνω είναι υποχρεωτικό να συνεκτιμηθούν η βάση και το πόδι στήριξης του ρομπότ. Για το σκοπό αυτό, επεκτείνουμε έναν καθιερωμένο στη βιβλιογραφία εκτιμητή αιωρούμενης μάζας με τη δυναμική του ποδιού στήριξης χρησιμοποιώντας μετρήσεις κινηματικής και αδρανειακής μονάδας με το Φίλτρο Κάλμαν Σφάλματος Κατάστασης (ESKF) για την κατάλληλη διαχείριση της υπερ-παραμετροποίησης των περιστροφών. Με αυτό το τρόπο, δημιουργείται ένα σύστημα σειριακής εκτίμησης κατάστασης που αποτελείται από έναν εκτιμητή βάσης και έναν εκτιμητή CoM το οποίο ονομάζουμε State Estimation RObot Walking (SEROW). Επιπλέον, για να διορθώσουμε την κινηματική απόκλιση που προκαλείται από την ολίσθηση των ποδιών κατά το περπάτημα, χρησιμοποιούμε μετρήσεις Οπτικής Οδομετρίας (VO) και/ή Οδομετρίας LIDAR (LO). ∆υστυχώς, τέτοιες μετρήσεις υποφέρουν από ακραίες τιμές σε ένα δυναμικό περιβάλλον, αφού κατά τον υπολογισμό τους χρησιμοποιείται η υπόθεση ότι μόνο το ρομπότ βρίσκεται σε κίνηση και ο κόσμος γύρω του είναι στατικός. Για αυτό το λόγο, εισάγουμε το Σθεναρό Γκαουσιανό Φίλτρο Κάλμαν Σφάλματος Κατάστασης (RGESKF) για την αυτόματη ανίχνευση και απόρριψη των ακραίων μετρήσεων. Το προτεινόμενο φίλτρο δεν βασίζεται σε πρότερη γνώση σχετικά με τις κατανομές των μετρήσεων και δεν χρησιμοποιεί ειδικά ρυθμισμένα κατώφλια. Ως εκ τούτου, το SEROW γίνεται ένα σθεναρό σύστημα εκτίμησης κατάστασης, κατάλληλο για δυναμικά ανθρώπινα περιβάλλοντα. Προκειμένου να ενισχυθούν περαιτέρω οι ερευνητικές προσπάθειες, το SEROW δίνεται ελεύθερα στη ρομποτική κοινότητα ως ένα πακέτο ROS/C++ ανοικτού κώδικα. Τα σύγχρονα συστήματα ελέγχου και εκτίμησης κατάστασης ανθρωποειδών ρομπότ υποθέτουν ότι η κατάσταση επαφής ποδιών-εδάφους είναι γνωστή εκ των προτέρων. Η ανίχνευση τέτοιων επαφών είναι ένα σημαντικό και σε μεγάλο βαθμό ανεξερεύνητο θέμα στη σύγχρονη ρομποτική έρευνα. Σε αυτή τη διατριβή,  διατυπώνουμε μια ευρύτερη ερώτηση: σε ποια φάση βάδισης βρίσκεται το ρομπότ; Προς το σκοπό αυτό, προτείνουμε ένα ολιστικό πλαίσιο βασισμένο σε μη-επιβλεπόμενη μάθηση από δεδομένα ιδιοδεκτικής αίσθησης που αντιμετωπίζει με ακρίβεια και αποτελεσματικότητα αυτό το πρόβλημα. Συγκεκριμένα, ανιχνεύουμε με ακρίβεια μια από τις τρεις φάσεις βάδισης, την Αριστερή Υποστήριξη (LSS), την ∆ιπλή Υποστήριξη (DS) και τη ∆εξιά Υποστήριξη (RSS), χρησιμοποιώντας μετρήσεις από κωδικοποιητές, IMU και F/T. Αρχικά, πραγματοποιείται μείωση των διαστάσεων με Ανάλυση Κύριων Στοιχείων (PCA) ή με αυτόματους κωδικοποιητές ώστε να εξαχθούν χρήσιμα χαρακτηριστικά, μια συμπαγής αναπαράσταση και να μειωθεί ο θόρυβος στα δεδομένα. Στη συνέχεια, πραγματοποιείται μια ομαδοποίηση στον χώρο χαμηλών διαστάσεων με Γκαουσιανά Μοντέλα Μίγματος (GMMs). Ως αποτέλεσμα λαμβάνονται τρία πυκνά συμπλέγματα που αντιστοιχούν στις φάσεις της βάδισης. Αυτό σημαίνει ότι η δυναμική της φάσης του βαδίσματος είναι χαμηλής διάστασης το οποίο λειτουργεί ως άλλη μια ένδειξη στο ότι ολόκληρη η διαδικασία της βάδισης είναι χαμηλής διάστασης. Επιπλέον, δεδομένου ότι το προτεινόμενο πλαίσιο χρησιμοποιεί μετρήσεις από αισθητήρες που είναι συνήθως διαθέσιμοι στα σημερινά ανθρωποειδή ρομπότ, προσφέρουμε στη ρομποτική κοινότητα το Gait-Phase Estimation Module (GEM), μια ανοικτού κώδικα εφαρμογή σε ROS/Python. Το SEROW και το GEM έχουν αξιολογηθεί ποσοτικά και ποιοτικά αναφορικά με την ακρίβεια και την αποδοτικότητα τους τόσο σε προσομοίωση όσο και σε πραγματικές συνθήκες. Αρχικά, χρησιμοποιήθηκε ένα προσομοιωμένο ρομπότ στο MATLAB και το ανθρωποειδές ρομπότ Valkyrie της NASA στο ROS/Gazebo για να τεκμηριωθούν τα προτεινόμενα σχήματα στο βάδισμα πάνω σε ανομοιόμορφο/ανώμαλο έδαφος. Στη συνέχεια, τα προτεινόμενα σχήματα ενσωματώθηκαν στο α) μικρού μεγέθους ανθρωποειδές ρομπότ NAO v4.0 και β) στο πλήρους μεγέθους ανθρωποειδές WALK-MAN v2.0 για περεταίρω πειραματική επικύρωση. Με το NAO, το SEROW εφαρμόστηκε στο ρομπότ για να παράσχει την απαραίτητη ανατροφοδότηση στον σχεδιασμό της κίνησης και τη σταθεροποίηση του βηματισμού σε πραγματικό χρόνο. Με αυτό το τρόπο επιτεύχθηκε πολυκατευθυντική βάδιση ακόμη και σε εξωτερικά/ανομοιογενή εδάφη. Επιπλέον, το SEROW χρησιμοποιήθηκε στον σχεδιασμό βημάτων για την πλοήγηση και επίσης στο Visual SLAM με το ίδιο ρομπότ. Όσον αφορά το WALK-MAN v2.0, το SEROW εφαρμόστηκε με δεδομένα κινηματικής, αδρανειακής μονάδας και F/T για να παρέχει ανατροφοδότηση βάσης και CoM σε πραγματικό χρόνο. Στην εκτίμηση λήφθηκε υπόψη και το VO για την διόρθωση της κινηματικής απόκλισης κατά το περπάτημα. Με αυτό το τρόπο διευκολύνεται σημαντικά ο πιθανός σχεδιασμός βημάτων. Τέλος, το GEM χρησιμοποιήθηκε επίσης για την εκτίμηση της φάσης της βάδισης στο δυναμικό περπάτημα του WALK-MAN. Συνοψίζοντας, σε αυτή τη διατριβή προτείνεται ένας σθεναρός μη-γραμμικός εκτιμητής κατάστασης για το βάδισμα ανθρωποειδών ρομπότ. Παρόλα αυτά, το προτεινόμενο σύστημα μπορεί εύκολα να επεκταθεί και σε άλλους τύπους ρομπότ με πόδια, όπως τα τετράποδα, μιας και διαθέτουν τις ίδιες βασικές αρχές κίνησης",Σθεναρή μη γραμμική εκτίμηση κατάστασης ανθρωποειδών ρομπότ,,,Πανεπιστήμιο Κρήτης,,core
249373292,2019-07-01T00:00:00,"International audienceWe review a selection of recent results where applications of concepts that harness the “shape of light” in spatial or temporal domain have been applied widely to yield significant advances in areas such as computer-vision sensing with digital holography, spatial shaping of complex laser beams and photonic neural networks.Computer vision is a powerful contact-less measurement tools successfully applied in numerous domains of application, where depth of field and working distances are constrained by the imaging magnification chosen. The use of pseudo-periodic patterns on the target of interest overcomes these usual computer-vision limitations leading to sub-pixel resolutions and making the absolute measurement range independent of the field-of-observation of the imaging system [1]. The approach was also validated using digital holography as imaging method with a tremendous enlargement of the allowed working distance range [2], and seems very well suited to diverse application needs in the micro-robotic and biomedical domains.Principles of digital holography can be used also for spatial shaping of complex laser beams such as Bessel, Airy or arbitrary beams using liquid crystal spatial phase modulators (SLM). Applications in micro&nano-machining by non-diffracting ultrashort laser pulses in various materials have been proposed during recent years [3]. This will also open new perspectives for applications of complex beams for applications in microscopy, optical coherence tomography or ultrafast physics.Photonic systems have revolutionized the hardware implementation of Recurrent Neural Networks and Reservoir Computing, in particular [4]. The fundamental principles of ReservoirComputing strongly facilitate a realization in such complex analog systems. Especially delay systems, which potentially provide large numbers of degrees of freedom even in simple architectures, can efficiently be exploited for information processing. We also demonstrated learning in large-scale neural networks with numerous nonlinear nodes in an architecture using SLM [5]. This last scheme is fully parallel and the passive weights maximize energy efficiency and bandwidth.In high-tech areas such as micro-robotics and photonics, measurement requirements are increasing in terms of high resolution and their controls are based on multi-scale and complex parameters.  Increasingly real-time processing remains a big challenge for future applications, where next generation of systems will need to implement new hardware architectures, maybe based on photonic neural networks","The shape of light: how to measure, control and compute complexity?",,,HAL CCSD,,core
334855055,2019-09-04T00:00:00,"Precise robotic grasping is important for many industrial applications, such
as assembly and palletizing, where the location of the object needs to be
controlled and known. However, achieving precise grasps is challenging due to
noise in sensing and control, as well as unknown object properties. We propose
a method to plan robotic grasps that are both robust and precise by training
two convolutional neural networks - one to predict the robustness of a grasp
and another to predict a distribution of post-grasp object displacements. Our
networks are trained with depth images in simulation on a dataset of over 1000
industrial parts and were successfully deployed on a real robot without having
to be further fine-tuned. The proposed displacement estimator achieves a mean
prediction errors of 0.68cm and 3.42deg on novel objects in real world
experiments.Comment: Submitted and accepted to 12th Conference on Field and Service
  Robotics (FSR 2019","Towards Precise Robotic Grasping by Probabilistic Post-grasp
  Displacement Estimation",http://arxiv.org/abs/1909.02129,,,,core
334878860,2019-12-02T00:00:00,"This paper presents Gym-Ignition, a new framework to create reproducible
robotic environments for reinforcement learning research. It interfaces with
the new generation of Gazebo, part of the Ignition Robotics suite, which
provides three main improvements for reinforcement learning applications
compared to the alternatives: 1) the modular architecture enables using the
simulator as a C++ library, simplifying the interconnection with external
software; 2) multiple physics and rendering engines are supported as plugins,
simplifying their selection during the execution; 3) the new distributed
simulation capability allows simulating complex scenarios while sharing the
load on multiple workers and machines. The core of Gym-Ignition is a component
that contains the Ignition Gazebo simulator and exposes a simple interface for
its configuration and execution. We provide a Python package that allows
developers to create robotic environments simulated in Ignition Gazebo.
Environments expose the common OpenAI Gym interface, making them compatible
out-of-the-box with third-party frameworks containing reinforcement learning
algorithms. Simulations can be executed in both headless and GUI mode, the
physics engine can run in accelerated mode, and instances can be parallelized.
Furthermore, the Gym-Ignition software architecture provides abstraction of the
Robot and the Task, making environments agnostic on the specific runtime. This
abstraction allows their execution also in a real-time setting on actual
robotic platforms, even if driven by different middlewares.Comment: Accepted in SII202","Gym-Ignition: Reproducible Robotic Simulations for Reinforcement
  Learning",http://arxiv.org/abs/1911.01715,10.1109/SII46433.2020.9025951,'Institute of Electrical and Electronics Engineers (IEEE)',,core
200812390,2019-03-18T00:00:00,"This paper presents an upgraded, real world application oriented version of
gym-gazebo, the Robot Operating System (ROS) and Gazebo based Reinforcement
Learning (RL) toolkit, which complies with OpenAI Gym. The content discusses
the new ROS 2 based software architecture and summarizes the results obtained
using Proximal Policy Optimization (PPO). Ultimately, the output of this work
presents a benchmarking system for robotics that allows different techniques
and algorithms to be compared using the same virtual conditions. We have
evaluated environments with different levels of complexity of the Modular
Articulated Robotic Arm (MARA), reaching accuracies in the millimeter scale.
The converged results show the feasibility and usefulness of the gym-gazebo 2
toolkit, its potential and applicability in industrial use cases, using modular
robots","gym-gazebo2, a toolkit for reinforcement learning using ROS 2 and Gazebo",http://arxiv.org/abs/1903.06278,,,,core
186277109,2019-02-14T00:00:00,"Robotic weed control has seen increased research of late with its potential
for boosting productivity in agriculture. Majority of works focus on developing
robotics for croplands, ignoring the weed management problems facing rangeland
stock farmers. Perhaps the greatest obstacle to widespread uptake of robotic
weed control is the robust classification of weed species in their natural
environment. The unparalleled successes of deep learning make it an ideal
candidate for recognising various weed species in the complex rangeland
environment. This work contributes the first large, public, multiclass image
dataset of weed species from the Australian rangelands; allowing for the
development of robust classification methods to make robotic weed control
viable. The DeepWeeds dataset consists of 17,509 labelled images of eight
nationally significant weed species native to eight locations across northern
Australia. This paper presents a baseline for classification performance on the
dataset using the benchmark deep learning models, Inception-v3 and ResNet-50.
These models achieved an average classification accuracy of 95.1% and 95.7%,
respectively. We also demonstrate real time performance of the ResNet-50
architecture, with an average inference time of 53.4 ms per image. These strong
results bode well for future field implementation of robotic weed control
methods in the Australian rangelands.Comment: 14 pages, 8 figures, 4 table",DeepWeeds: A Multiclass Weed Species Image Dataset for Deep Learning,http://arxiv.org/abs/1810.05726,10.1038/s41598-018-38343-3,'Springer Science and Business Media LLC',,core
270088355,2019-10-28T00:00:00,"The human motor system is robust, adaptive and very flexible. The underlying principles of human motion provide inspiration for robotics. Pointing at different targets is a common robotics task, where insights about human motion can be applied. Traditionally in robotics, when a motion is generated it has to be validated so that the robot configurations involved are appropriate. The human brain, in contrast, uses the motor cortex to generate new motions reusing and combining existing knowledge before executing the motion. We propose a method to generate and control pointing motions for a robot using a biological inspired architecture implemented with spiking neural networks. We outline a simplified model of the human motor cortex that generates motions using motor primitives. The network learns a base motor primitive for pointing at a target in the center, and four correction primitives to point at targets up, down, left and right from the base primitive, respectively. The primitives are combined to reach different targets. We evaluate the performance of the network with a humanoid robot pointing at different targets marked on a plane. The network was able to combine one, two or three motor primitives at the same time to control the robot in real-time to reach a specific target. We work on extending this work from pointing to a given target to performing a grasping or tool manipulation task. This has many applications for engineering and industry involving real robots",Generating pointing motions for a humanoid robot by combining motor primitives,https://core.ac.uk/download/270088355.pdf,10.5445/IR/1000099401,Frontiers Media,,core
289166719,2019-08-01T00:00:00,"Industry 4.0 refers to the evolution in manufacturing from computerization to
fully cyberphysical systems that exploit rich sensor data, adaptive real-time safety-critical
control, and machine learning. An important aspect of this vision is the sensing and subsequent association of objects in the physical world with their cyber and virtual counterparts.
In this paper we propose Visible Light Positioning (VLP) as an enabler for these Industry 4.0
applications. We also explore sensing techniques, including cameras (and depth sensors), and
other light-based solutions for object positioning and detection along with their respective
limitations. We then demonstrate an application of positioning for real time robot control in
an interactive multiparty cyber-physical-virtual deployment. Lastly, based on our experience
with this cyber-physical-virtual application, we propose Ray-Surface Positioning (RSP), a
novel VLP technique, as a low cost positioning system for Industry 4.0.Accepted manuscrip",Visible light positioning for location-based services in Industry 4.0,https://open.bu.edu/bitstream/2144/39474/1/TR.pdf,10.1109/ISWCS.2019.8877305,'Institute of Electrical and Electronics Engineers (IEEE)',"[{'title': None, 'identifiers': ['issn:2154-0217', '2154-0217']}]",core
201227050,2019-02-01T00:00:00,"Emergency personnel, such as firefighters, bomb technicians, and urban search and rescue specialists, can be exposed to a variety of extreme hazards during the response to natural and human-made disasters. In many of these scenarios, a risk factor is the presence of hazardous airborne chemicals. The recent and rapid advances in robotics and sensor technologies allow emergency responders to deal with such hazards from relatively safe distances. Mobile robots with gas-sensing capabilities allow to convey useful information such as the possible source positions of different chemicals in the emergency area. However, common gas sampling procedures for laboratory use are not applicable due to the complexity of the environment and the need for fast deployment and analysis. In addition, conventional gas identification approaches, based on supervised learning, cannot handle situations when the number and identities of the present chemicals are unknown. For the purpose of emergency response, all the information concluded from the gas detection events during the robot exploration should be delivered in real time. To address these challenges, we developed an online gas-sensing system using an electronic nose. Our system can automatically perform unsupervised learning and update the discrimination model as the robot is exploring a given environment. The online gas discrimination results are further integrated with geometrical information to derive a multi-compound gas spatial distribution map. The proposed system is deployed on a robot built to operate in harsh environments for supporting fire brigades, and is validated in several different real-world experiments of discriminating and mapping multiple chemical compounds in an indoor open environment. Our results show that the proposed system achieves high accuracy in gas discrimination in an online, unsupervised, and computationally efficient manner. The subsequently created gas distribution maps accurately indicate the presence of different chemicals in the environment, which is of practical significance for emergency response",Towards Gas Discrimination and Mapping in Emergency Response Scenarios Using a Mobile Robot with an Electronic Nose,,10.3390/s19030685,'MDPI AG',"[{'title': 'Sensors', 'identifiers': ['issn:1424-8220', '1424-8220']}]",core
338815046,2019-08-15T00:00:00,"Search and Rescue (SAR) missions represent an important challenge in the robotics research field as they usually involve exceedingly variable-nature scenarios which require a high-level of autonomy and versatile decision-making capabilities. This challenge becomes even more relevant in the case of aerial robotic platforms owing to their limited payload and computational capabilities. In this paper, we present a fully-autonomous aerial robotic solution, for executing complex SAR missions in unstructured indoor environments. The proposed system is based on the combination of a complete hardware configuration and a flexible system architecture which allows the execution of high-level missions in a fully unsupervised manner (i.e. without human intervention). In order to obtain flexible and versatile behaviors from the proposed aerial robot, several learning-based capabilities have been integrated for target recognition and interaction. The target recognition capability includes a supervised learning classifier based on a computationally-efficient Convolutional Neural Network (CNN) model trained for target/background classification, while the capability to interact with the target for rescue operations introduces a novel Image-Based Visual Servoing (IBVS) algorithm which integrates a recent deep reinforcement learning method named Deep Deterministic Policy Gradients (DDPG). In order to train the aerial robot for performing IBVS tasks, a reinforcement learning framework has been developed, which integrates a deep reinforcement learning agent (e.g. DDPG) with a Gazebo-based simulator for aerial robotics. The proposed system has been validated in a wide range of simulation flights, using Gazebo and PX4 Software-In-The-Loop, and real flights in cluttered indoor environments, demonstrating the versatility of the proposed system in complex SAR mission",A Fully-Autonomous Aerial Robot for Search and Rescue Applications in Indoor Environments using Learning-Based Techniques,http://oa.upm.es/64148/,10.1007/s10846-018-0898-1,'Springer Science and Business Media LLC',,core
335413867,2019-11-01T00:00:00,"Deep- and reinforcement-learning techniques have increasingly required large sets of real data to achieve stable convergence and generalization, in the context of image-recognition, object-detection or motion-control strategies. On this subject, the research community lacks robust approaches to overcome unavailable real-world extensive data by means of realistic synthetic-information and domain-adaptation techniques. In this work, synthetic-learning strategies have been used for the vision-based autonomous following of a noncooperative multirotor. The complete maneuver was learned with synthetic images and high-dimensional low-level continuous robot states, with deep- and reinforcement-learning techniques for object detection and motion control, respectively. A novel motion-control strategy for object following is introduced where the camera gimbal movement is coupled with the multirotor motion during the multirotor following. Results confirm that our present framework can be used to deploy a vision-based task in real flight using synthetic data. It was extensively validated in both simulated and real-flight scenarios, providing proper results (following a multirotor up to 1.3 m/s in simulation and 0.3 m/s in real flights)",Vision-Based Multirotor Following Using Synthetic Learning Techniques,http://oa.upm.es/64118/,10.3390/s19214794,'MDPI AG',,core
265120533,13/12/2019,"International audienceThe use of Reinforcement Learning (RL) is still restricted to simulation or to enhance human-operated systems through recommendations. Real-world environments (e.g. industrial robots or power grids) are generally designed with safety constraints in mind implemented in the shape of valid actions masks or contingency controllers. For example, the range of motion and the angles of the motors of a robot can be limited to physical boundaries. Violating constraints thus results in rejected actions or entering in a safe mode driven by an external controller, making RL agents incapable of learning from their mistakes. In this paper, we propose a simple modification of a state-of-the-art deep RL algorithm (DQN), enabling learning from forbidden actions. To do so, the standard Q-learning update is enhanced with an extra safety loss inspired by structured classification. We empirically show that it reduces the number of hit constraints during the learning phase and accelerates convergence to near-optimal policies compared to using standard DQN. Experiments are done on a Visual Grid World Environment and Text-World domain","""I'm sorry Dave, I'm afraid I can't do that"" Deep Q-Learning From Forbidden Actions",,,HAL CCSD,,core
265310084,01/07/2019,"International audienceWe review a selection of recent results where applications of concepts that harness the “shape of light” in spatial or temporal domain have been applied widely to yield significant advances in areas such as computer-vision sensing with digital holography, spatial shaping of complex laser beams and photonic neural networks.Computer vision is a powerful contact-less measurement tools successfully applied in numerous domains of application, where depth of field and working distances are constrained by the imaging magnification chosen. The use of pseudo-periodic patterns on the target of interest overcomes these usual computer-vision limitations leading to sub-pixel resolutions and making the absolute measurement range independent of the field-of-observation of the imaging system [1]. The approach was also validated using digital holography as imaging method with a tremendous enlargement of the allowed working distance range [2], and seems very well suited to diverse application needs in the micro-robotic and biomedical domains.Principles of digital holography can be used also for spatial shaping of complex laser beams such as Bessel, Airy or arbitrary beams using liquid crystal spatial phase modulators (SLM). Applications in micro&nano-machining by non-diffracting ultrashort laser pulses in various materials have been proposed during recent years [3]. This will also open new perspectives for applications of complex beams for applications in microscopy, optical coherence tomography or ultrafast physics.Photonic systems have revolutionized the hardware implementation of Recurrent Neural Networks and Reservoir Computing, in particular [4]. The fundamental principles of ReservoirComputing strongly facilitate a realization in such complex analog systems. Especially delay systems, which potentially provide large numbers of degrees of freedom even in simple architectures, can efficiently be exploited for information processing. We also demonstrated learning in large-scale neural networks with numerous nonlinear nodes in an architecture using SLM [5]. This last scheme is fully parallel and the passive weights maximize energy efficiency and bandwidth.In high-tech areas such as micro-robotics and photonics, measurement requirements are increasing in terms of high resolution and their controls are based on multi-scale and complex parameters.  Increasingly real-time processing remains a big challenge for future applications, where next generation of systems will need to implement new hardware architectures, maybe based on photonic neural networks","The shape of light: how to measure, control and compute complexity?",,,HAL CCSD,,core
326830463,2019-01-01T00:00:00,"© . This manuscript version is made available under the CC-BY-NC-ND 4.0 license http://creativecommons.org/licenses/by-nc-nd/4.0/LEGION (Locally Excitatory, Globally Inhibitory Oscillator Network) topology has demonstrated good capabilities in scene segmentation applications. However, the implementation of LEGION algorithm requires machines with high performance to process a set of complex differential equations limiting its use in practical real-time applications. Recently, several authors have proposed alternative methods based on spiking neural networks (SNN) to create oscillatory neural networks with low computational complexity and highly feasible to be implemented on digital hardware to perform adaptive segmentation of images. Nevertheless, existing SNN with LEGION configuration focus on the membrane model leaving aside the behavior of the synapses although they play an important role in the synchronization of several segments by self-adapting their weights. In this work, we propose a SNN-LEGION configuration along with normalized weight of the synapses to self-adapt the SNN network to synchronize several segments of any size and shape at the same time. The proposed SNN-LEGION method involves a global inhibitor, which is in charge of performing the segmentation process between different objects with different sizes and shapes on time. To validate the proposal, the SNN-LEGION method is implemented on an optimized scalable neuromorphic architecture. Our preliminary results demonstrate that the proposed normalization process of the synaptic weights along with the SNN-LEGION configuration keep the capacity of the LEGION network to separate the segments on time, which can be useful in video processing applications such as vision processing systems for mobile robots, offering lower computational complexity and area consumption compared with previously reported solutions.The authors would like to thank the Consejo Nacional de Ciencia y Tecnologia (CONACyT) and the IPN for the financial support to realize this work under project SIP-20180251. This work was also supported in part by the Spanish Ministry of Science and Innovation and the European Social Fund (ESF) under Projects TEC2011-27047 and TEC2015-67278-R.Peer Reviewe",LEGION-based image segmentation by means of spiking neural networks using normalized synaptic weights implemented on a compact scalable neuromorphic architecture,,10.1016/j.neucom.2019.04.037,'Elsevier BV',"[{'title': 'Neurocomputing', 'identifiers': ['0925-2312', 'issn:0925-2312']}]",core
303790485,2019-01-01T00:00:00,"Robotic weed control has seen increased research of late with its potential for boosting productivity in agriculture. Majority of works focus on developing robotics for croplands, ignoring the weed management problems facing rangeland stock farmers. perhaps the greatest obstacle to widespread uptake of robotic weed control is the robust classification of weed species in their natural environment. the unparalleled successes of deep learning make it an ideal candidate for recognising various weed species in the complex rangeland environment. This work contributes the first large, public, multiclass image dataset of weed species from the Australian rangelands; allowing for the development of robust classification methods to make robotic weed control viable. The DeepWeeds dataset consists of 17,509 labelled images of eight nationally significant weed species native to eight locations across northern Australia. This paper presents a baseline for classification performance on the dataset using the benchmark deep learning models, Inception-v3 and ResNet-50. These models achieved an average classification accuracy of 95.1% and 95.7%, respectively. We also demonstrate real time performance of the ResNet-50 architecture, with an average inference time of 53.4 ms per image. These strong results bode well for future field implementation of robotic weed control methods in the Australian rangelands",DeepWeeds: a multiclass weed species image dataset for deep learning,https://core.ac.uk/download/303790485.pdf,10.1038/s41598-018-38343-3,'Springer Science and Business Media LLC',,core
334864337,2019-09-16T00:00:00,"Modern reinforcement learning methods suffer from low sample efficiency and
unsafe exploration, making it infeasible to train robotic policies entirely on
real hardware. In this work, we propose to address the problem of sim-to-real
domain transfer by using meta learning to train a policy that can adapt to a
variety of dynamic conditions, and using a task-specific trajectory generation
model to provide an action space that facilitates quick exploration. We
evaluate the method by performing domain adaptation in simulation and analyzing
the structure of the latent space during adaptation. We then deploy this policy
on a KUKA LBR 4+ robot and evaluate its performance on a task of hitting a
hockey puck to a target. Our method shows more consistent and stable domain
adaptation than the baseline, resulting in better overall performance.Comment: Submitted to ICRA 202",Meta Reinforcement Learning for Sim-to-real Domain Adaptation,http://arxiv.org/abs/1909.12906,,,,core
190268281,2019-06-01T00:00:00,"Autonomous navigation of robots in unknown environments from their current position to a desired target without colliding with obstacles represents an important aspect in the field of mobile robots. In literature, traditional methods do exist in case a complete knowledge of the environment is available. However, this is not the case in real-life applications where a complete knowledge about stochastic environments can hardly be obtained where the positions of the obstacles are unknown. Our main goal is to navigate a skid-steering mobile robot (SSMR) with non-holonomic constraints in an unknown environment to its desired target in minimum time while avoiding colliding with the obstacles. In the context of autonomous cart navigation, autonomous navigation of mobile robots in unstructured environments can be formulated as a Reinforcement learning (RL) problem. Reinforcement learning is a paradigm in which the agent (robot) learns its optimal path by interacting with the environment and receiving a reward based on its actions. Based on this action-reward scenario, the optimal action for each state can be discovered by maximizing a predefined accumulated reward that reflects the quality of the trajectory taken by the robot. Reinforcement learning can be categorized into two main methods (Bagnell J. Peters J. Kober, 2013), namely value based methods (Q-learning, Deep Q-learning) and policy based methods (REINFORCE with Policy Gradient). (i) Value based methods: In these methods, the value function that maps each state-action pair into a value is learned. Thanks to these methods, the best action to take for each state, the action with the biggest value, can be found. This works well in case of a finite set of actions. (ii) Policy based methods: In policy based methods, instead of learning value functions that give an indication of the expected total reward for each state-action pair, the policy function that maps the state into action is optimized directly (Brundage. M Arulkumaran. K and A., 2017). Policy search methods are more effective in high dimensional action spaces, or when using continuous actions like the case of mobile robot navigation (Neumann G. Peters J. Deisenroth, 2013). However, since both of these methods have their own drawbacks, there is hybrid method called Actor-Critic that employs both value functions and policy search (Brundage. M Arulkumaran. K and A., 2017). In this method, the actor adjust the policy parameters by a policy gradient ascent whereas the critic estimates the action value function using a policy evaluation algorithm such as temporal difference (TD) learning. This can be done through two neural networks running in parallel. Herein, we introduce a reinforcement learning based navigation system that enhance the navigation capabilities of a skid steering mobile robot. This goal will be achieved by using asynchronous deep deterministic policy gradient (ADDPG) through an actor-critic algorithm (Badia A. Mirza M. Graves A. Harley T. Lillicrap T. Silver D. Kavukcuoglu K. Mnih, 2016). Two deep neural-networks are introduced in this case. The first one (actornetwork) transfers the input vector that represents the states of the robot to linear and angular velocity commands that represent the actions. Whereas the second neural network (critic-network) is responsible for predicting the Q-value of the state and action pairs. Deterministic policies will be utilized since it was proposed that they outperform their stochastic counterparts in highdimensional action spaces (Lever G. Hess N. Degris T. Wierstra D. Riedmiller M. Silver, 2014). On the other hand, Simultaneous Localization and Mapping also known as SLAM techniques are to be integrated with reinforcement learning in an attempt to improve the learning rate by providing more accurate estimation of the robots states. SLAM has been an active topic for many years (C. Cadena, 2016) because it provides two fundamental components to robotics: where I am, and what I see. Herein, we introduce how the navigation problem of non-holonomic mobile robots can be formulated as a reinforcement learning problem that could be solved by using ADDPG actor-critic algorithm. Besides, we share the experimental results on how SLAM would help reinforcement learning at all by comparing the learning rate without SLAM (comparison when a partial map of the environment is given and when no map is available). For implementation of the parallel running neural networks, we do our initial experiments using the GPU cluster of University of Twente. However, when it comes to real-life implementations and demonstrations on real robots, the hardware becomes the key component to determine whether the goal can be achieved or not. Therefore, we implement the trained networks on NVIDIA Jetson TX2 in order to show real-life implementation possibilities when the algorithms need to run in a navigating robot (Jetson TX2 Module, online documentation, n.d.). In this sense, NVIDIA Jetson TX2 gives opportunity to run even multiple complex neural networks real-time on a credic card size processor unit which can easily be carried by a small (less than 20cm length) robot. We show our experiments on sensor selection and the OpenAI package in ROSGazebo platform as a possible simulation environment. Last but not least, we discuss the real-life implementation scenarios using our NVIDIA Jetson TX2 and how realistic the simulation results could be when the algorithms are implemented on the real robot. The early experiments indicate that, for an optimal design of a SLAM and reinforcement learning based system to truly work in real-world applications in order to reach a goal in an unknown environment, the hardware setup and the algorithms must be collaboratively designed",TOWARDS CONTINUOUS CONTROL FOR MOBILE ROBOT NAVIGATION: A REINFORCEMENT LEARNING AND SLAM BASED APPROACH,,,,,core
201423103,2019-01-01T00:00:00,"The cutting-edge technology Machine Learning (ML) is successfully applied for Business Intelligence. Among the various pre-processing steps of ML, Automatic Image Annotation (also known as automatic image tagging or linguistic indexing) is the process in which a computer system automatically assigns metadata in the form of captioning or keywords to a digital image. Automatic Image Annotation (AIA) methods (which have appeared during the last several years) make a large use of many ML approaches. Clustering and classification methods are most frequently applied to annotate images. In addition, these proposed solutions require a high computational infrastructure. However, certain real-time applications (small and ad-hoc intelligent applications) for example, autonomous small robots, gadgets, drone etc. have limited computational processing capacity. These small and ad-hoc applications demand a more dynamic and portable way to automatically annotate data and then perform ML tasks (Classification, clustering etc.) in real time using limited computational power and hardware resources. Through a comprehensive literature study we found that most image pre-processing algorithms and ML tasks are computationally intensive, and it can be challenging to run them on an embedded platform with acceptable frame rates. However, Raspberry Pi is sufficient for AIA and ML tasks that are relevant to small and ad-hoc intelligent applications. In addition, few critical intelligent applications (which require high computational resources, for example, Deep Learning using huge dataset) are only feasible to run on more powerful hardware resources. In this study, we present the framework of “Automatic Image Annotation for Small and Ad-hoc Intelligent Application using Raspberry Pi” and propose the low-cost infrastructures (single node and multi node using Raspberry Pi) and software module (for Raspberry Pi) to perform AIA and ML tasks in real time for small and ad-hoc intelligent applications. The integration of both AIA and ML tasks in a single software module (with in Raspberry Pi) is challenging. This study will helpful towards the improvement in various practical applications areas relevant to small intelligent autonomous systems",Automatic Image Annotation for Small and Ad hoc Intelligent Applications using Raspberry Pi,,10.1051/matecconf/201925501003,'EDP Sciences',"[{'title': 'MATEC Web of Conferences', 'identifiers': ['issn:2261-236X', '2261-236x']}]",core
395096792,2019-12-02T00:00:00,"The scene depth is an important information that can be used to retrieve the scene geometry, a missing element in standard color images. For this reason, the depth information is usually employed in many applications such as 3D reconstruction, autonomous driving and robotics.



The last decade has seen the spread of different commercial devices able to sense the scene depth. Among these, Time-of-Flight (ToF) cameras are becoming popular  because they are relatively cheap and they can be miniaturized and implemented on portable devices.  

Stereo vision systems are the most widespread 3D sensors and they are simply composed by two standard color cameras. However, they are not free from flaws, in particular they fail when the scene has no texture. Active stereo and structured light systems have been developed to overcome this issue by using external light projectors.

 

This thesis collects the findings of my Ph.D. research, which are mainly devoted to the denoising of depth data. First, some of the most widespread commercial 3D sensors  are introduced with their strengths and limitations. Then, some  techniques for the quality enhancement of ToF depth acquisition are presented and compared with other state-of-the-art methods. A first proposed method is based on a hardware modification of the standard ToF projector. A second approach instead uses multi-frequency ToF recordings as input of a deep learning network to improve the depth estimation. A particular focus will be given to how the denoising performance degrades, when the network is trained on synthetic data and tested on real data. Thus, a method to reduce the gap in performance will be proposed.

Since ToF and stereo vision systems have complementary characteristics, the possibility to fuse the information coming from these sensors is analysed and a method based on a locally consistent fusion, guided by a learning based reliability measure for the two sensors, is proposed. 

A part of this thesis is dedicated to the description of the data acquisition procedures and the related labeling required to collect the datasets we used for the training and evaluation of the proposed methods",Data Driven Approaches for Depth Data Denoising,,,,,core
200829848,2019-04-24T00:00:00,"Machine learning has been widely applied to various applications, some of
which involve training with privacy-sensitive data. A modest number of data
breaches have been studied, including credit card information in natural
language data and identities from face dataset. However, most of these studies
focus on supervised learning models. As deep reinforcement learning (DRL) has
been deployed in a number of real-world systems, such as indoor robot
navigation, whether trained DRL policies can leak private information requires
in-depth study. To explore such privacy breaches in general, we mainly propose
two methods: environment dynamics search via genetic algorithm and candidate
inference based on shadow policies. We conduct extensive experiments to
demonstrate such privacy vulnerabilities in DRL under various settings. We
leverage the proposed algorithms to infer floor plans from some trained Grid
World navigation DRL agents with LiDAR perception. The proposed algorithm can
correctly infer most of the floor plans and reaches an average recovery rate of
95.83% using policy gradient trained agents. In addition, we are able to
recover the robot configuration in continuous control environments and an
autonomous driving simulator with high accuracy. To the best of our knowledge,
this is the first work to investigate privacy leakage in DRL settings and we
show that DRL-based agents do potentially leak privacy-sensitive information
from the trained policies.Comment: The first three authors contributed equally. Accepted by AAMAS 201","How You Act Tells a Lot: Privacy-Leakage Attack on Deep Reinforcement
  Learning",http://arxiv.org/abs/1904.11082,,,,core
237402232,"November 7, 2019","Artificial Intelligence (AI) is a growing field of computa- tional science techniques designed to mimic functions per- formed by people. Advancements in autonomy will depend on a portfolio of AI technologies. Automated planning and scheduling is a venerable field of study in AI, and is needed for a variety of mission planning functions. Plan execution technology is less well studied, but important for auton- omy and robotics. Specialized forms of automated reason- ing and machine learning are key technologies to enable fault management. Over the past decade, the NASA Au- tonomous Systems and Operations (ASO) project has devel- oped and demonstrated numerous autonomy enabling tech- nologies employing AI techniques. Our work has employed AI in three distinct ways to enable autonomous mission op- erations capabilities. Crew Autonomy gives astronauts tools to assist in the performance of each of these mission oper-ations functions. Vehicle System Management uses AI tech- niques to turn the astronaut's spacecraft into a robot, allow- ing it to operate when astronauts are not present, or to reduce astronaut workload. AI technology also enables Autonomous Robots as crew assistants or proxies when the crew are not present. When these capabilities are used to enable astro- nauts to operate autonomously, they must be integrated with user interfaces, introducing numerous human factors con- siderations; when these capabilities are used to enable vehi- cle system management, they must be integrated with flight software, and run on embedded processors under the control of real-time operating systems.We first describe human spaceflight mission operations capabilities. The remainder of the paper will describe the ASO project, and the development and demonstration per- formed by ASO since 2011. We will describe the AI tech- niques behind each of these demonstrations, which include a variety of symbolic automated reasoning and machine learn- ing based approaches. Finally, we conclude with an assess- ment of future development needs for AI to enable NASA's future Exploration missions",Artificial Intelligence: Powering Human Exploration of the Moon and Mars,,,,,core
287585542,2019-12-03T00:00:00,"This chapter is a study of how automation and AI may impact employment and skills, probing the role of institutions and actors, and its policy implications. To the purpose, it critically assesses the National Strategy on AI recently devised by the NITI Aayog, Government of India. It undertakes a meta-analysis of data and recent evidence to look at how recent developments in AI and advanced robotics have become a disruptive technology insofar as Indian labour market and employment structures are concerned, and the issues that redressal mechanisms (viz. regulation and institutions) needs to address. The main argument is that the current National Strategy on AI does not take into account the logic of capital accumulation in India where a large informal economy interlocked with the formal economy is central to how work and employment is organised, skills are developed and deployed, and where a majority labour are locked in insecure, low pay and unprotected jobs. Any strategy has to start by acknowledging this reality in the Indian context, and then address it – something lacking in the current strategy",Work and Employment in the Times of Automation and Artificial Intelligence: The Indian Case,,,Palgrave Macmillan,,core
296958597,2019-01-16T00:00:00,"The cutting-edge technology Machine Learning (ML) is successfully applied for Business Intelligence. Among the various pre-processing steps of ML, Automatic Image Annotation (also known as automatic image tagging or linguistic indexing) is the process in which a computer system automatically assigns metadata in the form of captioning or keywords to a digital image. Automatic Image Annotation (AIA) methods (which have appeared during the last several years) make a large use of many ML approaches. Clustering and classification methods are most frequently applied to annotate images. In addition, these proposed solutions require a high computational infrastructure. However, certain real-time applications (small and ad-hoc intelligent applications) for example, autonomous small robots, gadgets, drone etc. have limited computational processing capacity. These small and ad-hoc applications demand a more dynamic and portable way to automatically annotate data and then perform ML tasks (Classification, clustering etc.) in real time using limited computational power and hardware resources. Through a comprehensive literature study we found that most image pre-processing algorithms and ML tasks are computationally intensive, and it can be challenging to run them on an embedded platform with acceptable frame rates. However, Raspberry Pi is sufficient for AIA and ML tasks that are relevant to small and ad-hoc intelligent applications. In addition, few critical intelligent applications (which require high computational resources, for example, Deep Learning using huge dataset) are only feasible to run on more powerful hardware resources. In this study, we present the framework of “Automatic Image Annotation for Small and Ad-hoc Intelligent Application using Raspberry Pi” and propose the low-cost infrastructures (single node and multi node using Raspberry Pi) and software module (for Raspberry Pi) to perform AIA and ML tasks in real time for small and ad-hoc intelligent applications. The integration of both AIA and ML tasks in a single software module (with in Raspberry Pi) is challenging. This study will helpful towards the improvement in various practical applications areas relevant to small intelligent autonomous systems",Automatic Image Annotation for Small and Ad hoc Intelligent Applications using Raspberry Pi,,10.1051/matecconf/201925501003,'EDP Sciences',,core
334857387,2019-09-19T00:00:00,"The past few years have seen rapid progress in the development of service
robots. Universities and companies alike have launched major research efforts
toward the deployment of ambitious systems designed to aid human operators
performing a variety of tasks. These robots are intended to make those who may
otherwise need to live in assisted care facilities more independent, to help
workers perform their jobs, or simply to make life more convenient. Service
robots provide a powerful platform on which to study Artificial Intelligence
(AI) and Human-Robot Interaction (HRI) in the real world. Research sitting at
the intersection of AI and HRI is crucial to the success of service robots if
they are to fulfill their mission.
  This symposium seeks to highlight research enabling robots to effectively
interact with people autonomously while modeling, planning, and reasoning about
the environment that the robot operates in and the tasks that it must perform.
AI-HRI deals with the challenge of interacting with humans in environments that
are relatively unstructured or which are structured around people rather than
machines, as well as the possibility that the robot may need to interact
naturally with people rather than through teach pendants, programming, or
similar interfaces.Comment: HTML file with clickable links to papers - All papers have been
  reviewed by at least two reviewers in a single blind fashion - Symposium
  website: https://ai-hri.github.io/2019",Proceedings of the AI-HRI Symposium at AAAI-FSS 2019,http://arxiv.org/abs/1909.04812,,,,core
345057400,2019-08-15T00:00:00,"Search and Rescue (SAR) missions represent an important challenge in the robotics research field as they usually involve exceedingly variable-nature scenarios which require a high-level of autonomy and versatile decision-making capabilities. This challenge becomes even more relevant in the case of aerial robotic platforms owing to their limited payload and computational capabilities. In this paper, we present a fully-autonomous aerial robotic solution, for executing complex SAR missions in unstructured indoor environments. The proposed system is based on the combination of a complete hardware configuration and a flexible system architecture which allows the execution of high-level missions in a fully unsupervised manner (i.e. without human intervention). In order to obtain flexible and versatile behaviors from the proposed aerial robot, several learning-based capabilities have been integrated for target recognition and interaction. The target recognition capability includes a supervised learning classifier based on a computationally-efficient Convolutional Neural Network (CNN) model trained for target/background classification, while the capability to interact with the target for rescue operations introduces a novel Image-Based Visual Servoing (IBVS) algorithm which integrates a recent deep reinforcement learning method named Deep Deterministic Policy Gradients (DDPG). In order to train the aerial robot for performing IBVS tasks, a reinforcement learning framework has been developed, which integrates a deep reinforcement learning agent (e.g. DDPG) with a Gazebo-based simulator for aerial robotics. The proposed system has been validated in a wide range of simulation flights, using Gazebo and PX4 Software-In-The-Loop, and real flights in cluttered indoor environments, demonstrating the versatility of the proposed system in complex SAR mission",A Fully-Autonomous Aerial Robot for Search and Rescue Applications in Indoor Environments using Learning-Based Techniques,,10.1007/s10846-018-0898-1,'Springer Science and Business Media LLC',,core
222881309,2019-06-24T00:00:00,"In recent years, Convolutional Neural Networks (CNNs) have repeatedly shown state-of-the-art performance for their accuracy in the task of object detection, but their heavy computational costs impede their ability for real-time detection when the supporting system is accelerating. At the same time, recent progress on visual inertial systems takes great advantage of movement information to robustly estimate the robot state and surrounding. This paper proposes to exploit the advantages of inertial odometry research for the purpose of real-time object detection system on mobile robots. We combine a CNN detector with VINS-Mono, a moving visual odometry system, and show reliable improvement in the detection process, especially as the robot starts to accelerate. Our system is ready-to-use in that it has very low deployment cost and requires no calibration. The resulting system allows for simultaneous robot state estimation and object detection, as well as object tracking. Lastly, the architecture is flexible in that it is not restrained to a specific object type or detector",Exploiter le compromis entre accélération et précision: un système en temps réel assisté par VINS sur système mobile,,,HAL CCSD,,core
186264978,2019-02-27T00:00:00,"Deployment of deep learning models in robotics as sensory information
extractors can be a daunting task to handle, even using generic GPU cards.
Here, we address three of its most prominent hurdles, namely, i) the adaptation
of a single model to perform multiple tasks at once (in this work, we consider
depth estimation and semantic segmentation crucial for acquiring geometric and
semantic understanding of the scene), while ii) doing it in real-time, and iii)
using asymmetric datasets with uneven numbers of annotations per each modality.
To overcome the first two issues, we adapt a recently proposed real-time
semantic segmentation network, making changes to further reduce the number of
floating point operations. To approach the third issue, we embrace a simple
solution based on hard knowledge distillation under the assumption of having
access to a powerful `teacher' network. We showcase how our system can be
easily extended to handle more tasks, and more datasets, all at once,
performing depth estimation and segmentation both indoors and outdoors with a
single model. Quantitatively, we achieve results equivalent to (or better than)
current state-of-the-art approaches with one forward pass costing just 13ms and
6.5 GFLOPs on 640x480 inputs. This efficiency allows us to directly incorporate
the raw predictions of our network into the SemanticFusion framework for dense
3D semantic reconstruction of the scene.Comment: The models are available here -
  https://github.com/drsleep/multi-task-refinenet; supplementary video here -
  https://youtu.be/qwShIBhaq8","Real-Time Joint Semantic Segmentation and Depth Estimation Using
  Asymmetric Annotations",http://arxiv.org/abs/1809.04766,,,,core
363914753,2019-12-15T00:00:00,"Many expert systems have been developed for self-adaptive PID controllers of mobile robots. However, the high computational requirements of the expert systems layers, developed for the tuning of the PID controllers, still require previous expert knowledge and high efficiency in algorithmic and software execution for real-time applications. To address these problems, in this paper we propose an expert agent-based system, based on a reinforcement learning agent, for self-adapting multiple low-level PID controllers in mobile robots. For the formulation of the artificial expert agent, we develop an incremental model-free algorithm version of the double Q-Learning algorithm for fast on-line adaptation of multiple low-level PID controllers. Fast learning and high on-line adaptability of the artificial expert agent is achieved by means of a proposed incremental active-learning exploration-exploitation procedure, for a non-uniform state space exploration, along with an experience replay mechanism for multiple value functions updates in the double Q-learning algorithm. A comprehensive comparative simulation study and experiments in a real mobile robot demonstrate the high performance of the proposed algorithm for a real-time simultaneous tuning of multiple adaptive low-level PID controllers of mobile robots in real world conditions.Fil: Carlucho, Ignacio. Universidad Nacional del Centro de la Provincia de Buenos Aires. Facultad de Ingeniería Olavarría. Departamento de Electromecánica. Grupo INTELYMEC; Argentina. Universidad Nacional del Centro de la Provincia de Buenos Aires. Centro de Investigaciones en Física e Ingeniería del Centro de la Provincia de Buenos Aires. - Consejo Nacional de Investigaciones Científicas y Técnicas. Centro Científico Tecnológico Conicet - Tandil. Centro de Investigaciones en Física e Ingeniería del Centro de la Provincia de Buenos Aires. - Provincia de Buenos Aires. Gobernación. Comisión de Investigaciones Científicas. Centro de Investigaciones en Física e Ingeniería del Centro de la Provincia de Buenos Aires; ArgentinaFil: de Paula, Mariano. Universidad Nacional del Centro de la Provincia de Buenos Aires. Facultad de Ingeniería Olavarría. Departamento de Electromecánica. Grupo INTELYMEC; Argentina. Universidad Nacional del Centro de la Provincia de Buenos Aires. Centro de Investigaciones en Física e Ingeniería del Centro de la Provincia de Buenos Aires. - Consejo Nacional de Investigaciones Científicas y Técnicas. Centro Científico Tecnológico Conicet - Tandil. Centro de Investigaciones en Física e Ingeniería del Centro de la Provincia de Buenos Aires. - Provincia de Buenos Aires. Gobernación. Comisión de Investigaciones Científicas. Centro de Investigaciones en Física e Ingeniería del Centro de la Provincia de Buenos Aires; ArgentinaFil: Acosta, Gerardo Gabriel. Universidad Nacional del Centro de la Provincia de Buenos Aires. Facultad de Ingeniería Olavarría. Departamento de Electromecánica. Grupo INTELYMEC; Argentina. Universidad Nacional del Centro de la Provincia de Buenos Aires. Centro de Investigaciones en Física e Ingeniería del Centro de la Provincia de Buenos Aires. - Consejo Nacional de Investigaciones Científicas y Técnicas. Centro Científico Tecnológico Conicet - Tandil. Centro de Investigaciones en Física e Ingeniería del Centro de la Provincia de Buenos Aires. - Provincia de Buenos Aires. Gobernación. Comisión de Investigaciones Científicas. Centro de Investigaciones en Física e Ingeniería del Centro de la Provincia de Buenos Aires; Argentin",Double Q-PID algorithm for mobile robot control,,10.1016/j.eswa.2019.06.066,'Elsevier BV',"[{'title': 'Expert Systems with Applications', 'identifiers': ['0957-4174', 'issn:1873-6793', '1873-6793', 'issn:0957-4174']}]",core
286528899,2019-01-01T00:00:00,"Video-based hand grasp analysis can support both robotics and prosthetics. Indeed, computational aspects represent a major issue, as hand grasp analysis is expected to support grasping systems that are hosted on low-power embedded systems. This paper proposes a framework for video-based grasping classification that is designed for implementation on resourceconstrained devices. The framework adopts a fully data-driven strategy and relies on deep learning to deal with advanced analysis of video signals. Nonetheless, the overall design takes advantage of CNN architectures that can cope with the constraints imposed by embedded systems. The experimental session involved a real-world dataset containing daily life activities collected using egocentric perspective. In addition, the complete inference system is implemented on a NVIDIA Jetson-TX2 obtaining real time performances. The results confirm that the proposed system can suitably balance the trade off between accuracy and computational costs",Data-Driven Video Grasping Classification for Low-Power Embedded System,,10.1109/ICECS46596.2019.8964645,'Institute of Electrical and Electronics Engineers (IEEE)',,core
224448556,2017-01-01T00:00:00,"Model predictive control is a very popular control scheme in a wide range of fields including driver assistance systems and autonomous robots. For example, in driver assistance systems, predictive control allows for improved safety and comfort. However, its implementation is a challenge in uncertain environments. Therefore, it is desirable to predict the evolution of the environment in which the controlled system operates. In other words, we pursue a highly accurate forecast of the environment so that we may achieve feasible and reliable action from the controller.This dissertation presents a systematic framework that uses predictive control and forecasts of the future environment to operate under uncertainties and constraints. In particular, we focus on enhancing the performance of a predictive control scheme based on an accurate trajectory prediction of any targets controlled by humans (e.g., vehicles driven by human or humans themselves).  We propose several motion-prediction-models using physics-based and data-driven approaches to improve the accuracy of the forecast. An interacting-multiple-model approach with Kalman filter techniques is useful in environments where it is difficult to have prior data sets such as disaster sites. Based on collected data from experimental vehicles, machine learning methods including hidden Markov models, convolution neural networks, and recurrent neural networks are used to enhance long-term predictions. Furthermore, we present predictive controls based on a probabilistic view of uncertain forecasts. The effectiveness of the proposed framework is demonstrated via applications such as human-companion robots, automotive adaptive cruise control, and autonomous lane change assist. The results of both simulation and real experimental data show the synergy between the motion prediction models and the predictive control designs","eScholarship, University of California",Predictive Control of Uncertain Systems based on Motion Prediction,,,,core
187143348,2017-01-01T00:00:00,"This paper presents a deep learning approach which evaluates accuracy and inference time speedups in

deep convolutional neural networks under various network quantizations. Quantized networks can result in

much faster inference time allowing them to be deployed in real time on an embedded system such as a robot.

We evaluate networks with activations quantized to 1, 2, 4, and 8-bits and binary weights. We found that

network quantization can yield a significant speedup for a small drop in classification accuracy. Specifically,

modifying one of our networks to use an 8-bit quantized input layer and 2-bit activations in hidden layers,

we calculate a theoretical 9.9x speedup in exchange for an F1 score decrease of just 3.4% relative to a full

precision implementation. Higher speedups are obtainable by designing a network architecture containing a

smaller proportion of the total multiplications within the input layer",Irish Pattern Recognition & Classification Society,Evaluating Quantized Convolutional Neural Networks for Embedded Systems,,,,core
300344122,2017-01-01T00:00:00,"This paper presents a deep learning approach which evaluates accuracy and inference time speedups in

deep convolutional neural networks under various network quantizations. Quantized networks can result in

much faster inference time allowing them to be deployed in real time on an embedded system such as a robot.

We evaluate networks with activations quantized to 1, 2, 4, and 8-bits and binary weights. We found that

network quantization can yield a significant speedup for a small drop in classification accuracy. Specifically,

modifying one of our networks to use an 8-bit quantized input layer and 2-bit activations in hidden layers,

we calculate a theoretical 9.9x speedup in exchange for an F1 score decrease of just 3.4% relative to a full

precision implementation. Higher speedups are obtainable by designing a network architecture containing a

smaller proportion of the total multiplications within the input layer",Irish Pattern Recognition & Classification Society,Evaluating Quantized Convolutional Neural Networks for Embedded Systems,,,,core
201560971,2017-04-01T00:00:00,"For centuries, humans’ capacity to capture and depict physical space has played a central role in industrial and societal development. However, the digital revolution and the emergence of networked devices and services accelerate geospatial capture, coordination, and intelligence in unprecedented ways. Underlying the digital transformation of industry and society is the fusion of the physical and digital worlds – ‘perceptality’ – where geospatial perception and reality merge. This paper analyzes the myriad forces that are driving perceptality and the future of geospatial intelligence and presents real-world implications and examples of its industrial application. Applications of sensors, robotics, cameras, machine learning, encryption, cloud computing and other software, and hardware intelligence are converging, enabling new ways for organizations and their equipment to perceive and capture reality. Meanwhile, demands for performance, reliability, and security are pushing compute ‘to the edge’ where real-time processing and coordination are vital. Big data place new restraints on economics, as pressures abound to actually use these data, both in real-time and for longer term strategic analysis and decision-making. These challenges require orchestration between information technology (IT) and operational technology (OT) and synchronization of diverse systems, data-sets, devices, environments, workflows, and people",'Informa UK Limited',The future of geospatial intelligence,,"[{'title': 'Geo-spatial Information Science', 'identifiers': ['issn:1993-5153', 'issn:1009-5020', '1993-5153', '1009-5020']}]",10.1080/10095020.2017.1337318,core
129054067,2017-01-01T00:00:00,"Background: In maize, silks are hundreds of filaments that simultaneously emerge from the ear for collecting pollen over a period of 1–7 days, which largely determines grain number especially under water deficit. Silk growth is a major trait for drought tolerance in maize, but its phenotyping is difficult at throughputs needed for genetic analyses. Results: We have developed a reproducible pipeline that follows ear and silk growths every day for hundreds of plants, based on an ear detection algorithm that drives a robotized camera for obtaining detailed images of ears and silks. We first select, among 12 whole-plant side views, those best suited for detecting ear position. Images are segmented, the stem pixels are labelled and the ear position is identified based on changes in width along the stem. A mobile camera is then automatically positioned in real time at 30 cm from the ear, for a detailed picture in which silks are identified based on texture and colour. This allows analysis of the time course of ear and silk growths of thousands of plants. The pipeline was tested on a panel of 60 maize hybrids in the PHENOARCH phenotyping platform. Over 360 plants, ear position was correctly estimated in 86% of cases, before it could be visually assessed. Silk growth rate, estimated on all plants, decreased with time consistent with literature. The pipeline allowed clear identification of the effects of genotypes and water deficit on the rate and duration of silk growth. Conclusions: The pipeline presented here, which combines computer vision, machine learning and robotics, provides a powerful tool for large-scale genetic analyses of the control of reproductive growth to changes in environmental conditions in a non-invasive and automatized way. It is available as Open Source software in the OpenAlea platform. (Résumé d'auteur",'Springer Science and Business Media LLC',A robot-assisted imaging pipeline for tracking the growths of maize ear and silks in a high-throughput phenotyping platform,http://agritrop.cirad.fr/585915/1/Brichet_et_al-2017-Plant_Methods.pdf,,10.1186/s13007-017-0246-7,core
84092094,2017-06-20T00:00:00,"We present an open-source accessory for the NAO robot, which enables to test
computationally demanding algorithms in an external platform while preserving
robot's autonomy and mobility. The platform has the form of a backpack, which
can be 3D printed and replicated, and holds an ODROID XU4 board to process
algorithms externally with ROS compatibility. We provide also a software bridge
between the B-Human's framework and ROS to have access to the robot's sensors
close to real-time. We tested the platform in several robotics applications
such as data logging, visual SLAM, and robot vision with deep learning
techniques. The CAD model, hardware specifications and software are available
online for the benefit of the community:
https://github.com/uchile-robotics/nao-backpackComment: Accepted in the RoboCup Symposium 2017. Final version will be
  published at Springe",'Springer Science and Business Media LLC',"The NAO Backpack: An Open-hardware Add-on for Fast Software Development
  with the NAO Robot",http://arxiv.org/abs/1706.06696,,10.1007/978-3-030-00308-1,core
299967783,2017-01-01T00:00:00,"The proliferation of artificial intelligence technologies and computerization has given rise to an era of automation. Human and robot interaction is now inevitable as robotic systems has become one of the pillars of industrial growth. From primordial tools like spears and stones in the past to the intricate robots used today, the relationship between man and tools is ever-changing. The robots, coupled with multiple customizations, must satisfy the needs of their respective industries to allow operators to utilise the robots to their fullest potential. As such, human-robot interaction should keep up with the roles of the robots as they evolve to suit industrial needs. Today, the human-robot systems are shifting their focus to the collaboration between the two, by using a more interactive human-robot interface to facilitate the system. However, the current keyboards and mouse interface is inadequate in meeting the requirements of the autonomous and semi-autonomous robots employed in the industrial applications. These industries applications demand swift and effective communication and exchange of data between human and robot. Therefore, the purpose of this project will be to improve human-robot systems through using augmented reality technology which is on the rise. This project employs the augmented reality technology to enhance the collaborative nature (2-way interaction) of human-robot systems by improving its system's interface, and aims to aid the control of robotic systems via a single-handed multi-modal interface for automatic robot taping. The taping is done autonomously by an agile robotic system with intuitive control. A unique framework was implemented for a collaborative human-robot interaction, incorporating a user-centered design to enhance feedback to users which improves robot usability.Bachelor of Engineering (Mechanical Engineering",,Single-handed multi-modal interface for robotic application in an industrial environment,,,,core
398616703,2017-11-08T00:00:00,"International audienceBackground:  In maize, silks are hundreds of filaments that simultaneously emerge from the ear for collecting pollen over a period of 1–7 days, which largely determines grain number especially under water deficit. Silk growth is a major trait for drought tolerance in maize, but its phenotyping is difficult at throughputs needed for genetic analyses.Results:  We have developed a reproducible pipeline that follows ear and silk growths every day for hundreds of plants, based on an ear detection algorithm that drives a robotized camera for obtaining detailed images of ears and silks. We first select, among 12 whole‑plant side views, those best suited for detecting ear position. Images are seg‑mented, the stem pixels are labelled and the ear position is identified based on changes in width along the stem. A mobile camera is then automatically positioned in real time at 30 cm from the ear, for a detailed picture in which silks are identified based on texture and colour. This allows analysis of the time course of ear and silk growths of thousands of plants. The pipeline was tested on a panel of 60 maize hybrids in the PHENOARCH phenotyping platform. Over 360 plants, ear position was correctly estimated in 86% of cases, before it could be visually assessed. Silk growth rate, estimated on all plants, decreased with time consistent with literature. The pipeline allowed clear identification of the effects of genotypes and water deficit on the rate and duration of silk growth.Conclusions:  The pipeline presented here, which combines computer vision, machine learning and robotics, provides a powerful tool for large‑scale genetic analyses of the control of reproductive growth to changes in environ‑mental conditions in a non‑invasive and automatized way. It is available as Open Source software in the OpenAlea platform",'Springer Science and Business Media LLC',A robot-assisted imaging pipeline for tracking the growths of maize ear and silks in a high-throughput phenotyping platform,,,10.1186/s13007-017-0246-7,core
144867413,16/10/2017,"Fast and accurate object detection is a desire of many vision-guided robotics based systems. Agriculture is an area where detection accuracy is often sacrificed for speed, especially in the pursuit of real time results. Pastoral landscapes are especially challenging with varying levels of complexity, as competing objects are rarely textually smooth or visibly different from surroundings. This study presents a machine learning algorithm designed for object detection called the Multiple Expert Colour Extreme Learning Machine (MEC-ELM). The MEC-ELM is a multiple expert implementation of a Colour Feature Extreme Learning Machine (CF-ELM). The CF-ELM is itself a modification of the Extreme Learning Machine (ELM) with a partially connected hidden layer and a fully connected output layer, taking 3 inputs. The inputs can be utilised by multiple colour systems, including, RGB, Y'UV and HSV. Colour inputs were chosen, as colour is not sensitive to adjustments in scale, size and location and provides information not available in the standard grey-scale ELM. In the MEC-ELM algorithm, feature extraction and classification techniques were implemented simultaneously making a fully functional object detection algorithm. The algorithm was tested on weed detection and cattle detection from a video feed, delivering 0.89 (cattle) to 0.98 (weeds) accuracy in tuning and a precision of 0.61 to 0.95 in testing, with classification times between 0.5s to 1s per frame. The algorithm has been designed with complex and unpredictable terrain in mind, making it an ideal application for agricultural or pastoral landscapes",,Fast object detection in pastoral landscapes using a multiple expert colour feature extreme learning machine,https://core.ac.uk/download/pdf/144867413.pdf,,10.5281/zenodo.897216,core
287607013,2017-02-01T00:00:00,"In this paper, we aim to present an adaptive position controller for multiple degree of freedom robotic manipulators. A decentralized approach is presented that utilizes Lyapunov function based artificial neural networks as inverse controllers of the robot’s nonlinear coupled dynamics. The proposed scheme is successfully implemented on the real time control of the TQ MA3000 robotic manipulator. Promising experimental results show the effectiveness of the proposed algorithm in the sense of fast convergence of adaptive tracking error and stability of the closed loop",'EJournal Publishing',Lyapunov function based neural networks for adaptive tracking of robotic arm,,,10.18178/ijmmm.2017.5.1.285,core
201801018,2018-09-01T00:00:00,"Real estate needs to improve its adoption of disruptive technologies to move from traditional to smart real estate (SRE). This study reviews the adoption of disruptive technologies in real estate. It covers the applications of nine such technologies, hereby referred to as the Big9. These are: drones, the internet of things (IoT), clouds, software as a service (SaaS), big data, 3D scanning, wearable technologies, virtual and augmented realities (VR and AR), and artificial intelligence (AI) and robotics. The Big9 are examined in terms of their application to real estate and how they can furnish consumers with the kind of information that can avert regrets. The review is based on 213 published articles. The compiled results show the state of each technology&rsquo;s practice and usage in real estate. This review also surveys dissemination mechanisms, including smartphone technology, websites and social media-based online platforms, as well as the core components of SRE: sustainability, innovative technology and user centredness. It identifies four key real estate stakeholders&mdash;consumers, agents and associations, government and regulatory authorities, and complementary industries&mdash;and their needs, such as buying or selling property, profits, taxes, business and/or other factors. Interactions between these stakeholders are highlighted, and the specific needs that various technologies address are tabulated in the form of a what, who and how analysis to highlight the impact that the technologies have on key stakeholders. Finally, stakeholder needs as identified in the previous steps are matched theoretically with six extensions of the traditionally accepted technology adoption model (TAM), paving the way for a smoother transition to technology-based benefits for consumers. The findings pertinent to the Big9 technologies in the form of opportunities, potential losses and exploitation levels (OPLEL) analyses highlight the potential utilisation of each technology for addressing consumers&rsquo; needs and minimizing their regrets. Additionally, the tabulated findings in the form of what, how and who links the Big9 technologies to core consumers&rsquo; needs and provides a list of resources needed to ensure proper information dissemination to the stakeholders. Such high-quality information can bridge the gap between real estate consumers and other stakeholders and raise the state of the industry to a level where its consumers have fewer or no regrets. The study, being the first to explore real estate technologies, is limited by the number of research publications on the SRE technologies that has been compensated through incorporation of online reports",'MDPI AG',"A Systematic Review of Smart Real Estate Technology: Drivers of, and Barriers to, the Use of Digital Disruptive Technologies and Online Platforms",,"[{'title': 'Sustainability', 'identifiers': ['2071-1050', 'issn:2071-1050']}]",10.3390/su10093142,core
161842230,2018-10-17T00:00:00,"This paper addresses an autonomous underwater vehicle (AUV) simulator with underwater image sonar and
optical vision emulation. Recently, various underwater missions have been automated through the great improvement in
underwater image sonar and optical vision technology along with utilization of artificial intelligence. Development of the
simulator that emulates underwater image sonar and optical vision can support intelligent underwater missions by
visualizing and simulating virtual scenarios and reproducing real missions. In order to benefit from existing software, the
presented simulator is based on ROS (Robot Operating System) environment integrated with our image sonar and optical
vision emulators. When an underwater virtual scenario of terrain, target objects, and AUVs with sensors is plotted using
standard 3-D modeling programs, the simulator configures the scenario and displays sonar and optical images. The
ultrasound and light beams are modeled as a set of rays each, and the image sonar and optical vision are modeled as
objects detecting collisions between the rays and target objects at certain positions and orientations. The sonar images
generated by the simulator are compared with real images to confirm the validity of the models.","Institute of Control, Robotics and Systems (ICROS)",Development of Simulator for Autonomous Underwater Vehicles utilizing Underwater Acoustic and Optical Sensing Emulators,,,,core
200684591,2018-09-26T10:46:13Z,"In this work, teams of small mobile robots are used to test hypotheses about cooperative transport by ants. This study attempts to explain a decrease in steady-state transport speed with increasing team size that was previously observed in the ant <i>Novomessor cockerelli</i>. Two models of one-dimensional collective towing are compared: one in which transporters with different maximum speeds pull the payload with continuous, variable forces, and another in which transporters with identical speeds pull with intermittent, unsynchronized forces. A statistical analysis of ant data supports the hypothesis that ants behave according to the first model, in which the steady-state transport speed is the maximum speed of the slowest teammate. By contrast, the ant data are not consistent with the second model, which predicts constant speed regardless of team size. To verify these predictions, the ant behaviours in each model are translated into decentralized controllers and implemented on teams of two to four robots. The controller for the first model incorporates a real-time reinforcement learning algorithm that successfully reproduces the observed relationship between ant team size and transport speed. The controller for the second model yields the predicted invariance of transport speed with team size. These results show the value of robotic swarms for testing mechanistic hypotheses about biological collectives",,Explanation of the assumption the ant data is normally distributed for proper order statistic analysis. from Multi-robot replication of ant collective towing behaviours,,,10.6084/m9.figshare.7133237.v1,core
78511725,2017-02-21T00:00:00,"Mobile robots are increasingly being employed for performing complex tasks in
dynamic environments. Reinforcement learning (RL) methods are recognized to be
promising for specifying such tasks in a relatively simple manner. However, the
strong dependency between the learning method and the task to learn is a
well-known problem that restricts practical implementations of RL in robotics,
often requiring major modifications of parameters and adding other techniques
for each particular task. In this paper we present a practical core
implementation of RL which enables the learning process for multiple robotic
tasks with minimal per-task tuning or none. Based on value iteration methods,
this implementation includes a novel approach for action selection, called
Q-biased softmax regression (QBIASSR), which avoids poor performance of the
learning process when the robot reaches new unexplored states. Our approach
takes advantage of the structure of the state space by attending the physical
variables involved (e.g., distances to obstacles, X,Y,{\theta} pose, etc.),
thus experienced sets of states may favor the decision-making process of
unexplored or rarely-explored states. This improvement has a relevant role in
reducing the tuning of the algorithm for particular tasks. Experiments with
real and simulated robots, performed with the software framework also
introduced here, show that our implementation is effectively able to learn
different robotic tasks without tuning the learning method. Results also
suggest that the combination of true online SARSA({\lambda}) with QBIASSR can
outperform the existing RL core algorithms in low-dimensional robotic tasks.Comment: 15 pages, 10 figures, 7 tables. To be published in a scientific
  journa",'Elsevier BV',"Towards a Common Implementation of Reinforcement Learning for Multiple
  Robotic Tasks",http://arxiv.org/abs/1702.06329,,10.1016/j.eswa.2017.11.011,core
132005917,2017-11-08T00:00:00,"International audienceBackground:  In maize, silks are hundreds of filaments that simultaneously emerge from the ear for collecting pollen over a period of 1–7 days, which largely determines grain number especially under water deficit. Silk growth is a major trait for drought tolerance in maize, but its phenotyping is difficult at throughputs needed for genetic analyses.Results:  We have developed a reproducible pipeline that follows ear and silk growths every day for hundreds of plants, based on an ear detection algorithm that drives a robotized camera for obtaining detailed images of ears and silks. We first select, among 12 whole‑plant side views, those best suited for detecting ear position. Images are seg‑mented, the stem pixels are labelled and the ear position is identified based on changes in width along the stem. A mobile camera is then automatically positioned in real time at 30 cm from the ear, for a detailed picture in which silks are identified based on texture and colour. This allows analysis of the time course of ear and silk growths of thousands of plants. The pipeline was tested on a panel of 60 maize hybrids in the PHENOARCH phenotyping platform. Over 360 plants, ear position was correctly estimated in 86% of cases, before it could be visually assessed. Silk growth rate, estimated on all plants, decreased with time consistent with literature. The pipeline allowed clear identification of the effects of genotypes and water deficit on the rate and duration of silk growth.Conclusions:  The pipeline presented here, which combines computer vision, machine learning and robotics, provides a powerful tool for large‑scale genetic analyses of the control of reproductive growth to changes in environ‑mental conditions in a non‑invasive and automatized way. It is available as Open Source software in the OpenAlea platform",'Springer Science and Business Media LLC',A robot-assisted imaging pipeline for tracking the growths of maize ear and silks in a high-throughput phenotyping platform,https://core.ac.uk/download/132005917.pdf,,10.1186/s13007-017-0246-7,core
196257734,2018-01-01T00:00:00,"The purpose of this paper is to review real-time control motion algorithms for wheeled mobile robot (WMR) when navigating in environment such as road. Its need a good controller to avoid collision with any disturbance and maintain a track error at zero level. The controllers are used with other aiding sensors to measure the WMR's velocities, posture, and interference to estimate the required torque to be applied on the wheels of mobile robot. Four main categories for wheeled mobile robot control systems have been found in literature which are namely: Kinematic based controller, Dynamic based controllers, artificial intelligence based control system, and Active Force control. A MATLAB/Simulink software is the main software to simulate and implement the control system. The real-time toolbox in MATLAB/SIMULINK are used to receive/send data from sensors/to actuator with presence of disturbances, however others software such C, C++ and visual basic are rare to be used",'IOP Publishing',An overview on real-time control schemes for wheeled mobile robot,https://core.ac.uk/download/196257734.pdf,,10.1088/1757-899X/342/1/012059,core
159195288,2018-03-01T00:00:00,"The purpose of this paper is to review real-time control motion algorithms for wheeled mobile robot (WMR) when navigating in environment such as road. Its need a good controller to avoid collision with any disturbance and maintain a track error with zero level. The controllers is used with and other aiding sensors to measure the WMR’s velocities, posture, and interference to estimate the needed torque of mobile robot due to wheel rotating. Four main categories for wheeled mobile robot control that have been found in literature which are namely: Kinematic based controller, Dynamic based controllers, artificial intelligence based control system, and Active Force control. A MATLAB/Simulink software is the main software to simulate and implement control system. The real-time toolbox in MATLAB/SIMULINK are used to receive/send data from sensors/to actuator with existing of real path disturbances",,Review on real-time control schemes for wheeled mobile robot,https://core.ac.uk/download/159195288.pdf,,,core
151245689,2018-10-28T00:00:00,"Collecting training data from the physical world is usually time-consuming
and even dangerous for fragile robots, and thus, recent advances in robot
learning advocate the use of simulators as the training platform.
Unfortunately, the reality gap between synthetic and real visual data prohibits
direct migration of the models trained in virtual worlds to the real world.
This paper proposes a modular architecture for tackling the virtual-to-real
problem. The proposed architecture separates the learning model into a
perception module and a control policy module, and uses semantic image
segmentation as the meta representation for relating these two modules. The
perception module translates the perceived RGB image to semantic image
segmentation. The control policy module is implemented as a deep reinforcement
learning agent, which performs actions based on the translated image
segmentation. Our architecture is evaluated in an obstacle avoidance task and a
target following task. Experimental results show that our architecture
significantly outperforms all of the baseline methods in both virtual and real
environments, and demonstrates a faster learning curve than them. We also
present a detailed analysis for a variety of variant configurations, and
validate the transferability of our modular architecture.Comment: 7 pages, accepted by IJCAI-1",,Virtual-to-Real: Learning to Control in Visual Semantic Segmentation,http://arxiv.org/abs/1802.00285,,,core
84495811,2017,"Combined efforts in the fields of neuroscience, computer science, and biology allowed to design biologically realistic models of the brain based on spiking neural networks. For a proper validation of these models, an embodiment in a dynamic and rich sensory environment, where the model is exposed to a realistic sensory-motor task, is needed. Due to the complexity of these brain models that, at the current stage, cannot deal with real-time constraints, it is not possible to embed them into a real-world task. Rather, the embodiment has to be simulated as well. While adequate tools exist to simulate either complex neural networks or robots and their environments, there is so far no tool that allows to easily establish a communication between brain and body models. The Neurorobotics Platform is a new web-based environment that aims to fill this gap by offering scientists and technology developers a software infrastructure allowing them to connect brain models to detailed simulations of robot bodies and environments and to use the resulting neurorobotic systems for in silico experimentation. In order to simplify the workflow and reduce the level of the required programming skills, the platform provides editors for the specification of experimental sequences and conditions, environments, robots, and brain-body connectors. In addition to that, a variety of existing robots and environments are provided. This work presents the architecture of the first release of the Neurorobotics Platform developed in subproject 10 ""Neurorobotics"" of the Human Brain Project (HBP).1At the current state, the Neurorobotics Platform allows researchers to design and run basic experiments in neurorobotics using simulated robots and simulated environments linked to simplified versions of brain models. We illustrate the capabilities of the platform with three example experiments: a Braitenberg task implemented on a mobile robot, a sensory-motor learning task based on a robotic controller, and a visual tracking embedding a retina model on the iCub humanoid robot. These use-cases allow to assess the applicability of the Neurorobotics Platform for robotic tasks as well as in neuroscientific experiments",'Frontiers Media SA',Connecting artificial brains to robots in a comprehensive simulation framework: The neurorobotics platform,,,10.3389/fnbot.2017.00002,core
87490052,2017-01-01T00:00:00Z,"The article considers the project of educational and research software and hardware kit designed for use in case-based educational project activities based on the theme of energetically autonomous robots. An important feature of the complex is its interdisciplinarity since robotics combines many fields of science and technology — mechanical design, electronics, programming, elements of artificial intelligence, energy science and others. Students can receive basic knowledge in these areas as well as gain practical skills in solving real problems that require a convergent approach. The kit serves as an experimental basis that allows to study intricacies of robot control and its hardware and software design while making structural changes, including using different power units: solar, fuel, thermoelectric and others. Power units (converters and the various sources of renewable energy) may be considered both from the theoretical point of view, i.e. from the principles of their functioning and from the practical aspect — studying their practical application in real systems. Another important aspect of the system is the development of software architecture for a robot and for a team of robots as well as studying the interactions between them. In addition to performing the target task, ancillary tasks such as maintaining the battery level, communication with other team members in a multi-agent system, should be considered in the control algorithm of the robot, which allows to study the distribution of priorities between these objectives, as shown in the example of multicriteria optimization. The prototype of the kit and its usage according to the described approach have been partially validated by conducting computational experiments with algorithms based on different search methods (random search, closest source and multi-criteria optimization methods), multi-agent paradigm and adaptive control which show a variety of possible approaches and illustrate the process of students' work. The hardware base has been validated by testing various energy modules and assembling robot using the modules of the proposed kit. The results showed the possibility and potential of studying a variety of interdisciplinary themes using the developed hardware and software complex",Plekhanov Russian University of Economics,Energetically self-sufficient robot group study kit,,"[{'title': None, 'identifiers': ['issn:1818-4243', '1818-4243', '2079-5939', 'issn:2079-5939']}]",10.21686/1818-4243-2017-2-68-77,core
195380051,2018-01-01T00:00:00,"abstract: Deep learning (DL) has proved itself be one of the most important developements till date with far reaching impacts in numerous fields like robotics, computer vision, surveillance, speech processing, machine translation, finance, etc. They are now widely used for countless applications because of their ability to generalize real world data, robustness to noise in previously unseen data and high inference accuracy. With the ability to learn useful features from raw sensor data, deep learning algorithms have out-performed tradinal AI algorithms and pushed the boundaries of what can be achieved with AI. In this work, we demonstrate the power of deep learning by developing a neural network to automatically detect cough instances from audio recorded in un-constrained environments. For this, 24 hours long recordings from 9 dierent patients is collected and carefully labeled by medical personel. A pre-processing algorithm is proposed to convert event based cough dataset to a more informative dataset with start and end of coughs and also introduce data augmentation for regularizing the training procedure. The proposed neural network achieves 92.3% leave-one-out accuracy on data captured in real world. 

Deep neural networks are composed of multiple layers that are compute/memory intensive. This makes it difficult to execute these algorithms real-time with low power consumption using existing general purpose computers. In this work, we propose hardware accelerators for a traditional AI algorithm based on random forest trees and two representative deep convolutional neural networks (AlexNet and VGG). With the proposed acceleration techniques, ~ 30x performance improvement was achieved compared to CPU for random forest trees. For deep CNNS, we demonstrate that much higher performance can be achieved with architecture space exploration using any optimization algorithms with system level performance and area models for hardware primitives as inputs and goal of minimizing latency with given resource constraints. With this method, ~30GOPs performance was achieved for Stratix V FPGA boards. 

Hardware acceleration of DL algorithms alone is not always the most ecient way and sucient to achieve desired performance. There is a huge headroom available for performance improvement provided the algorithms are designed keeping in mind the hardware limitations and bottlenecks. This work achieves hardware-software co-optimization for Non-Maximal Suppression (NMS) algorithm. Using the proposed algorithmic changes and hardware architecture 

With CMOS scaling coming to an end and increasing memory bandwidth bottlenecks, CMOS based system might not scale enough to accommodate requirements of more complicated and deeper neural networks in future. In this work, we explore RRAM crossbars and arrays as compact, high performing and energy efficient alternative to CMOS accelerators for deep learning training and inference. We propose and implement RRAM periphery read and write circuits and achieved ~3000x performance improvement in online dictionary learning compared to CPU. 

This work also examines the realistic RRAM devices and their non-idealities. We do an in-depth study of the effects of RRAM non-idealities on inference accuracy when a pretrained model is mapped to RRAM based accelerators. To mitigate this issue, we propose Random Sparse Adaptation (RSA), a novel scheme aimed at tuning the model to take care of the faults of the RRAM array on which it is mapped. Our proposed method can achieve inference accuracy much higher than what traditional Read-Verify-Write (R-V-W) method could achieve. RSA can also recover lost inference accuracy 100x ~ 1000x faster compared to R-V-W. Using 32-bit high precision RSA cells, we achieved  ~10% higher accuracy using fautly RRAM arrays compared to what can be achieved by mapping a deep network to an 32 level RRAM array with no variations.Dissertation/ThesisDoctoral Dissertation Electrical Engineering 201",,Algorithm and Hardware Design for Efficient Deep Learning Inference,https://core.ac.uk/download/195380051.pdf,,,core
293725865,2017-01-01T00:00:00,"Robotics has the potential to be one of the most revolutionary technologies in human history. The impact of cheap and
potentially limitless manpower could have a profound influence on our everyday life and overall onto our society. As
envisioned by Iain M. Banks, Asimov and many other science fictions writers, the effects of robotics on our society might
lead to the disappearance of physical labor and a generalized increase of the quality of life. However, the large-scale
deployment of robots in our society is still far from reality, except perhaps in a few niche markets such as manufacturing.
One reason for this limited deployment of robots is that, despite the tremendous advances in the capabilities of the
robotic hardware, a similar advance on the control software is still lacking. The use of robots in our everyday life is still
hindered by the necessary complexity to manually design and tune the controllers used to execute tasks. As a result,
the deployment of robots often requires lengthy and extensive validations based on human expert knowledge, which
limit their adaptation capabilities and their widespread diffusion. In the future, in order to truly achieve an ubiquitous
robotization of our society, it is necessary to reduce the complexity of deploying new robots in new environments and
tasks.
The goal of this dissertation is to provide automatic tools based on Machine Learning techniques to simplify and
streamline the design of controllers for new tasks. In particular, we here argue that Bayesian modeling is an important tool
for automatically learning models from raw data and properly capture the uncertainty of the such models. Automatically
learning models however requires the definition of appropriate features used as input for the model. Hence, we present
an approach that extend traditional Gaussian process models by jointly learning an appropriate feature representation
and the subsequent model. By doing so, we can strongly guide the features representation to be useful for the subsequent
prediction task.
A first robotics application where the use of Bayesian modeling is beneficial is the accurate learning of complex dynamics models. For highly non-linear robotic systems, such as in presence of contacts, the use of analytical system
identification techniques can be challenging and time-consuming, or even intractable. We introduce a new approach for
learning inverse dynamics models exploiting artificial tactile sensors. This approach allows to recognize and compensate
for the presence of unknown contacts, without requiring a spatial calibration of the tactile sensors. We demonstrate
on the humanoid robot iCub that our approach outperforms state-of-the-art analytical models, and when employed in
control tasks significantly improves the tracking accuracy.
A second robotics application of Bayesian modeling is automatic black-box optimization of the parameters of a controller. When the dynamics of a system cannot be modeled (either out of complexity or due to the lack of a full state
representation), it is still possible to solve a task by adapting an existing controller. The approach used in this thesis is
Bayesian optimization, which allows to automatically optimize the parameters of the controller for a specific task. We
evaluate and compare the performance of Bayesian optimization on a gait optimization task on the dynamic bipedal
walker Fox. Our experiments highlight the benefit of this approach by reducing the parameters tuning time from weeks
to a single day.
In many robotic application, it is however not possible to always define a single straightforward desired objective.
More often, multiple conflicting objectives are desirable at the same time, and thus the designer needs to take a decision
about the desired trade-off between such objectives (e.g., velocity vs. energy consumption). One framework that is
useful to assist in this decision making is the multi-objective optimization framework, and in particular the definition of
Pareto optimality. We propose a novel framework that leverages the use of Bayesian modeling to improve the quality
of traditional multi-objective optimization approaches, even in low-data regimes. By removing the misleading effects
of stochastic noise, the designer is presented with an accurate and continuous Pareto front from which to choose the
desired trade-off. Additionally, our framework allows the seamless introduction of multiple robustness metrics which can
be considered during the design phase. These contributions allow an unprecedented support to the design process of
complex robotic systems in presence of multiple objective, and in particular with regards to robustness.
The overall work in this thesis successfully demonstrates on real robots that the complexity of deploying robots to solve
new tasks can be greatly reduced trough automatic learning techniques. We believe this is a first step towards a future
where robots can be used outside of closely supervised environments, and where a newly deployed robot could quickly
and automatically adapt to accomplish the desired tasks",,Bayesian Modeling for Optimization and Control in Robotics,,,,core
231786118,2017-09-01T00:00:00,"Seoul, Republic of Korea, September, 2017 — Commissioned by the curators of the inaugural Seoul Biennale of Architecture and Urbanism, open September 2, 2017 through December ??, 2017 at the Donuimun Museum Village, Seoul, SK. Driver Less Vision presents the immersive experience of becoming an autonomous, self-driving vehicle. Created by Urtzi Grau (Fake Industries Architectural Agonism), Guillermo Fernandez-Abascal with Perlin Studios, the project was produced with virtual reality video and architectural design by the New York-based teams, and installed in a 25’ dome at the Seoul Biennale. Driver Less Vision examines the tension and reality of AI and humans merging and diverging as they negotiate Seoul's unique urban landscape—challenging us to consider how we can design cities for the future of autonomous vehicles. Driver Less Vision aims to generate empathy between humans and non-humans, to construct the trust required for negotiations that will settle how we will live together. By overlapping human and machine’s perceptions, the installation helps to identify the areas of the city that will need to be redesigned in the immediate future. Driver Less Vision is the immersive experience of becoming an autonomous, self-driving vehicle. It explores the untapped conflicts and disruptive effects on the built environment caused by the deployment of technologies for autonomous mobility. Currently, the visual stimuli that organizes traffic is designed for human perception. The arrival of driverless cars entails the emergence of a omnidirectional gaze that is required to negotiate existing visual codes. To assume that driverless cars will fully adapt to future conditions of the city, however, neglects the history of transformations in urban streetscapes associated with changes in vehicular technologies. Driver Less Vision is an attempt to understand how driverless cars will change the city by immersing the audience in an urban journey through the car’s point of view, seeing the streets of Seoul through overlapping and dissonant perceptions. The project was produced for the Seoul Biennale of Architecture and Urbanism in 2017, utilizing an eight meter diameter dome with 360 visuals developed with the generous support of University of Technology Sydney, Rice University and Ocular Robotics",Actar,Driver Less Vision,http://hdl.handle.net/10453/133482,,,core
149388185,2017-10-06T13:38:21Z,"<i>Abstract</i><div><br></div><div><br></div><div><div>Robocup is an international competition for multi agent research and related subject like: Artificial intelligence, Image processing, machine learning, robot path planning, control, and obstacle avoidance. In a soccer robot game, the environment is highly competitive and dynamic. In order to work in the dynamically changing environment, the decision-making system of a soccer robot system should have the features of flexibility and real-time adaptation. In this paper we will focus on the Middle Size Soccer Robot league (MSL) and new hierarchical hybrid fuzzy methods for decision making and action selection of a robot in Middle Size Soccer Robot league (MSL) are presented. First, the behaviors of an agent are introduced, implemented and classified in two layers, the Low_Level_Behaviors and the High_Level_Behaviors. In the second layer, a two phase mechanism for decision making is introduced. In phase one, some useful methods are implemented which check the robot’s situation for performing required behaviors. In the next phase, the team strategy, team formation, robot’s role and the robot’s positioning system are introduced. A fuzzy logical approach is employed to recognize the team strategy and further more to tell the player the best position to move. We believe that a Dynamic role engine is necessary for a successful team. Dynamic role engine and formation control during offensive or defensive play, help us to prevent collision avoidance among own players when attacking the ball and obstacle avoidance of the opponents. At last, we comprised our implemented algorithm in the Robocup 2007 and 2008 and results showed the efficiency of the introduced methodology. The results are satisfactory which has already been successfully implemented in ADRO RoboCup team. This project is still in progress and some new interesting methods are described in the current report.</div></div><div><br></div><div><br></div><div><b>Learn more at:</b></div><div><b>https://www.edusoft.ro/brain/index.php/brain/article/view/96</b><br></div",,BRAIN Journal - Design of an Action Selection Mechanism for Cooperative Soccer Robots Based on Fuzzy Decision Making Algorithm,,,10.6084/m9.figshare.5478223.v1,core
478159177,2018-01-01T00:00:00,"Real estate needs to improve its adoption of disruptive technologies to move from traditional to smart real estate (SRE). This study reviews the adoption of disruptive technologies in real estate. It covers the applications of nine such technologies, hereby referred to as the Big9. These are: drones, the internet of things (IoT), clouds, software as a service (SaaS), big data, 3D scanning, wearable technologies, virtual and augmented realities (VR and AR), and artificial intelligence (AI) and robotics. The Big9 are examined in terms of their application to real estate and how they can furnish consumers with the kind of information that can avert regrets. The review is based on 213 published articles. The compiled results show the state of each technology’s practice and usage in real estate. This review also surveys dissemination mechanisms, including smartphone technology, websites and social media-based online platforms, as well as the core components of SRE: sustainability, innovative technology and user centredness. It identifies four key real estate stakeholders—consumers, agents and associations, government and regulatory authorities, and complementary industries—and their needs, such as buying or selling property, profits, taxes, business and/or other factors. Interactions between these stakeholders are highlighted, and the specific needs that various technologies address are tabulated in the form of a what, who and how analysis to highlight the impact that the technologies have on key stakeholders. Finally, stakeholder needs as identified in the previous steps are matched theoretically with six extensions of the traditionally accepted technology adoption model (TAM), paving the way for a smoother transition to technology-based benefits for consumers. The findings pertinent to the Big9 technologies in the form of opportunities, potential losses and exploitation levels (OPLEL) analyses highlight the potential utilisation of each technology for addressing consumers’ needs and minimizing their regrets. Additionally, the tabulated findings in the form of what, how and who links the Big9 technologies to core consumers’ needs and provides a list of resources needed to ensure proper information dissemination to the stakeholders. Such high-quality information can bridge the gap between real estate consumers and other stakeholders and raise the state of the industry to a level where its consumers have fewer or no regrets. The study, being the first to explore real estate technologies, is limited by the number of research publications on the SRE technologies that has been compensated through incorporation of online reports",'MDPI AG',"A systematic review of smart real estate technology: drivers of, and barriers to, the use of digital disruptive technologies and online platforms",,,10.3390/su10093142,core
186279778,2018-10-19T00:00:00,"Humanoid robotics research depends on capable robot platforms, but recently
developed advanced platforms are often not available to other research groups,
expensive, dangerous to operate, or closed-source. The lack of available
platforms forces researchers to work with smaller robots, which have less
strict dynamic constraints or with simulations, which lack many real-world
effects. We developed NimbRo-OP2X to address this need. At a height of 135 cm
our robot is large enough to interact in a human environment. Its low weight of
only 19 kg makes the operation of the robot safe and easy, as no special
operational equipment is necessary. Our robot is equipped with a fast onboard
computer and a GPU to accelerate parallel computations. We extend our already
open-source software by a deep-learning based vision system and gait parameter
optimisation. The NimbRo-OP2X was evaluated during RoboCup 2018 in Montr\'eal,
Canada, where it won all possible awards in the Humanoid AdultSize class.Comment: International Conference on Humanoid Robots (Humanoids), Beijing,
  China, 201",,NimbRo-OP2X: Adult-sized Open-source 3D Printed Humanoid Robot,http://arxiv.org/abs/1810.08395,,,core
428380032,2017-05-01T07:00:00,"Report from a meeting held on the topic of disinformation, the Internet, and public diplomacy held at the Hoover Institution, Stanford University, in 2017.
Executive Summary
Scientific progress continues to accelerate, and while we’ve witnessed a revolution in communication technologies in the past ten years, what proceeds in the next ten years may be far more transformative. It may also be extremely disruptive, challenging long held conventions behind public diplomacy (PD) programs and strategies. In order to think carefully about PD in this ever and rapidly changing communications space, the Advisory Commission on Public Diplomacy (ACPD) convened a group of private sector, government, and academic experts at Stanford University’s Hoover Institution to discuss the latest trends in research on strategic communication in digital spaces. The results of that workshop, refined by a number of follow-on interviews and discussions, are included in this report. I encourage you to read each of the fourteen essays that follow, which are divided into three thematic sections: Digital’s Dark Side, Disinformation, and Narratives.
Digital’s Dark Side focuses on the emergence of social bots, artificial intelligence, and computational propaganda. Essays in this section aim to raise awareness regarding how technology is transforming the nature of digital communication, offer ideas for competing in this space, and raise a number of important policy and research questions needing immediate attention. The Disinformation section confronts Oxford English Dictionary’s 2016 word of the year – “post-truth” – with a series of compelling essays from practitioners, a social scientist, and philosopher on the essential roles that truth and facts play in a democratic society. Here, theory, research, and practice neatly align, suggesting it is both crucial and effective to double-down on fact-checking and evidence-based news and information programming in order to combat disinformation campaigns from our adversaries. The Narrative section concludes the report by focusing on how technology and facts are ultimately part of, and dependent on, strategic narratives. Better understanding how these narratives form, and what predicts their likely success, is necessary to think through precisely how PD can, indeed, survive the Internet. Below are some key takeaways from the report.
In Defense of Truth
• We are not living in a “post-truth” society. Every generation tends to think that the current generation is less honest than the previous generation. This is an old human concern, and should be seen today as a strategic narrative (see Hancock, p. 49; Roselle, p. 77). Defending the value and search for truth is crucial. As Jason Stanley notes (p. 71), “without truth, there is just power.”
• Humans are remarkably bad at detecting deception. Studies show that people tend to trust what others say, an effect called the truth bias. This bias is actually quite rational—most of the messages that a person encounters in a day are honest, so being biased toward the truth is almost always the correct response (see Hancock, p.49).
• At the same time people are also continuously evaluating the validity of their understanding of the world. This process is called “epistemic vigilance,” a continuous process checking that the information that a person believes they know about the world is accurate. While we have a difficult time detecting deception from interpersonal cues, people can detect lies when they have the time, resources, and motivation. Lies are often discovered through contradicting information from a third source, or evidence that challenges a deceptive account (see Hancock, p. 49).
• Fact checking can be effective, even in hyper-partisan settings (see Porter, p. 55), and is crucial for sustained democratic dialogue (Bennett, p. 61; Stanley, p. 71). Moreover, it is possible, using digital tools, to detect and effectively combat disinformation campaigns in real time (Henick and Walsh, p. 65).
Computational Propaganda
• Computational propaganda refers to the coordinated use of social media platforms, autonomous agents and big data directed towards the manipulation of public opinion.
• Social media bots (or “web robots”) are the primary tools used in the dissemination of computational propaganda. In their most basic form, bots provide basic answers to simple questions, publish content on a schedule or disseminate stories in response to triggers (e.g. breaking news). Bots can have a disproportionate impact because it is easy to create a lot of them and they can post a high-volume content at a high frequency (see Woolley, p.13).
• Political bots aim to automate political engagement in an attempt to manipulate public opinions. They allow for massive amplification of political views and can empower a small group of people to set conversation agenda’s online. Political bots are used over social media to manufacture trends, game hashtags, megaphone particular content, spam opposition and attack journalists. The noise, spam and manipulation inherent in many bot deployment techniques threaten to disrupt civic conversations and organization worldwide (see Chessen, p.19).
• Advances in artificial intelligence (AI) – an evolving constellation of technologies enabling computers to simulate cognitive processes – will soon enable highly persuasive machine-generated communications. Imagine an automated system that uses the mass of online data to infer your personality, political preferences, religious affiliation, demographic data and interests. It knows which news websites and social media platforms you frequent and it controls multiple user accounts on those platforms. The system dynamically creates content specifically designed to plug into your particular psychological frame and achieve a particular outcome (see Chessen, p. 39).
• Digital tools have tremendous advantages over humans. Once an organization creates and configures a sophisticated AI bot, the marginal cost of running it on thousands or millions of user accounts is relatively low. They can operate 24/7/365 and respond to events almost immediately. AI bots can be programmed to react to certain events and create content at machine speed, shaping the narrative almost immediately. This is critical in an information environment where the first story to circulate may be the only one that people recall, even if it is untrue (see Chessen, p. 39).
• PD practitioners need to consider the question of how they can create and sustain meaningful conversations and engagements with audiences if the mediums typically relied upon are becoming less trusted, compromised and dominated by intelligent machines.
• Challenging computational propaganda should include efforts to ensure the robustness and integrity of the marketplace of information online. Defensively, this strategy would focus on producing patterns of information exchange among groups that would make them difficult to sway using techniques of computational propaganda. Offensively, the strategy would seek to distribute the costs of counter-messaging broadly, shaping the social ecosystem to enable alternative voices to effectively challenge campaigns of misinformation (see Hwang, p. 27). In the persuasive landscape formed by social media and computational propaganda, it may be at times more effective to build tools, rather than construct a specific message.
• Practitioners are not alone in their concern about the escalating use of social bots by adversarial state actors. The private sector is, too. Social media platforms see this trend as a potentially existential threat to their business models, especially if the rise of bots and computational propaganda weakens users’ trust in the integrity of the platforms themselves. Coordination with private sector is key, as their policies governing autonomous bots will adapt and, thus, shape what is and isn’t feasible online.
Moving Past Folk Theories
• Folk theories, or how people think a particular process works, are driving far too many digital strategies. One example of a folk theory is in the prevalence of echo chambers online, or the idea that people are increasingly digitally walled off from one another, engaging only with content that fits cognitive predispositions and preferences.
• Research suggests that the more users rely on digital platforms (e.g. Twitter and Facebook) for their news and information, the more exposure they have to a multitude of sources and stories. This remains true even among partisans (though to a lesser extent than non-partisans). It turns out we haven’t digitally walled ourselves off after all (see Henick and Walsh, p. 65).
• Despite increased exposure to a pluralistic media ecosystem, we are becoming more and more ideological and partisan, and becoming more walled off at the interpersonal and physical layers. For example, marriages today are twice as likely to be between two people with similar political views than they were in 1960.
• Understanding this gap between a robustly diverse news environment and an increasingly “siloed” physical environment is crucial to more effectively engaging with target audiences around the world. Interpersonal and in-person engagement, including exchange programs, remain crucial for effective PD moving forward (see Wharton, p. 7).
• Despite this growing ideological divide, people are increasingly willing to trust one another, even complete strangers, when their goals are aligned (see the sharing economy, for example). This creates interesting opportunities for PD practitioners. Targeting strategies based on political attitudes or profiles may overshadow the possibility of aligned goals on important policy and social issues (see Hancock, p. 49).
Rethinking Our Digital Platforms and Metrics
Virality – the crown jewel in the social media realm – is overemphasized often at the expense of more important metrics like context and longevity. Many of the metrics used to measure the effectiveness of social media campaigns are vulnerable to manipulation, and more importantly, don’t measure engagement in any meaningful way. These metrics were built for an industry reliant on advertising for revenue generation, and as a result, may not be well-suited when applied to the context of PD (see Ford, p. 33; Woolley, p. 13).
• Overemphasizing certain metrics, such as reach or impressions, fails to account for the risks created by relaying on the same portals as other, less truthful and more nefarious actors. We need to be cautious and aware of the various ways in which the digital media business industries are shaping PD content, be aware of the risks, and think carefully about safeguarding the credibility U.S. Department of State PD programs operating in this space (see Wharton, p. 7; Ford, p. 33).
Strategic Narratives
• Strategic narratives—a means for political actors to construct a shared meaning of the past, present and future of politics in order to shape the behavior of other actors.” They provide the ideological backdrop for how audiences assess the meaning and significance of current events and breaking news. Put another way, they help people make sense of what would otherwise be a dizzying onslaught of news they are exposed to on a daily basis (see Roselle, p. 77; Kounalakis, p. 91).
• Crafting effective narratives require a genuine consensus--even if limited or temporary--on our policy priorities and their underlying values, as well as a detailed understanding and appreciation of local grievances and concerns about the related policy issue (see Wharton, p. 7; Roselle. P. 77). As such, effective strategic narratives must be mutually constructed.
• Rather than focusing on trending news topics and stories alone, we need to develop greater capacity to understand competing public narratives in foreign contexts and track how they adapt over time. Understanding distinctions between system (or governance), value, and identity narratives would allow PD practitioners to construct policy narratives that speak to, or at least acknowledge, the underlying pillars of belief in a given community (see Walker, p. 83; Roselle, p. 77).
• Every new administration creates new opportunities for foreign engagement. A shift towards a more transactional approach to PD, focused less on values but more on shared policy priorities, could allow for improved relations and cooperation with a number of countries previously hostile to American PD efforts and programs (see Kounalakis, p. 91)",DigitalCommons@University of Nebraska - Lincoln,"Can Public Diplomacy Survive the Internet? Bots, Echo Chambers, and Disinformation",https://core.ac.uk/download/428380032.pdf,,,core
289096303,2018-06-29T00:00:00,"We propose a multi-modal perceptual system that is inspired by the inner working of the human brain; in particular, the hierarchical structure of the sensory cortex and the spatial-temporal binding criteria. The system is context independent and can be applied to many on-going problems in social robotics, including but not limited to person recognition, emotion recognition, and multi-modal robot doctor to name a few. The system encapsulates the parallel distributed processing of real-world stimuli through different sensor modalities and encoding them into features vectors which in turn are processed via a number of dedicated processing units (DPUs) through hierarchical paths. DPUs are algorithmic realizations of the cell assemblies in neuroscience. A plausible and realistic perceptual system is presented via the integration of the outputs from these units by spiking neural networks. We will also discuss other components of the system including top-down influences and the integration of information through temporal binding with fading memory and suggest two alternatives to realize these criteria. Finally, we will demonstrate the implementation of this architecture on a hardware platform as a social robot and report experimental studies on the system",,A Brain-Inspired Multi-Modal Perceptual System for Social Robots: An Experimental Realization,https://core.ac.uk/download/289096303.pdf,,,core
154953649,2017-01-01T00:00:00,"Deep reinforcement learning is becoming increasingly popular for robot control algorithms, with the aim for a robot to self-learn useful feature representations from unstructured sensory input leading to the optimal actuation policy. In addition to sensors mounted on the robot, sensors might also be deployed in the environment, although these might need to be accessed via an unreliable wireless connection. In this paper, we demonstrate deep neural network architectures that are able to fuse information generated by multiple sensors and are robust to sensor failures at runtime. We evaluate our method on a search and pick task for a robot both in simulation and the real world",'Institute of Electrical and Electronics Engineers (IEEE)',Sensor fusion for robot control through deep reinforcement learning,,,,core
195562590,2018-06-28T11:02:40,"Social robots should be able to search and track people in order to help them. In this paper we present two different techniques for coordinated multi-robot teams for searching and tracking people. A probability map (belief) of a target person location is maintained, and to initialize and update it, two methods were implemented and tested: one based on a reinforcement learning algorithm and the other based on a particle filter. The person is tracked if visible, otherwise an exploration is done by making a balance, for each candidate location, between the belief, the distance, and whether close locations are explored by other robots of the team. The validation of the approach was accomplished throughout an extensive set of simulations using up to five agents and a large amount of dynamic obstacles; furthermore, over three hours of real-life experiments with two robots searching and tracking were recorded and analysed.Work partially supported by the Spanish Ministry of Science and Innovation under project Rob-In-Coop (DPI2013-42458-P) and EU FP7 project ARCAS (INFSO-ICT-287617).Peer Reviewe",'Springer Science and Business Media LLC',Searching and tracking people with cooperative mobile robots,,"[{'title': None, 'identifiers': [' 0929-5593', 'issn: 0929-5593']}]",10.1007/s10514-017-9681-6,core
160777913,2018-07-29T00:00:00,"To achieve scenario intelligence, humans must transfer knowledge to robots by
developing goal-oriented algorithms, which are sometimes insensitive to
dynamically changing environments. While deep reinforcement learning achieves
significant success recently, it is still extremely difficult to be deployed in
real robots directly. In this paper, we propose a hybrid structure named
Option-Interruption in which human knowledge is embedded into a hierarchical
reinforcement learning framework. Our architecture has two key components:
options, represented by existing human-designed methods, can significantly
speed up the training process and interruption mechanism, based on learnable
termination functions, enables our system to quickly respond to the external
environment. To implement this architecture, we derive a set of update rules
based on policy gradient methods and present a complete training process. In
the experiment part, our method is evaluated in Four-room navigation and
exploration task, which shows the efficiency and flexibility of our framework.Comment: 6 pages, 6 figure",,"Learning to Interrupt: A Hierarchical Deep Reinforcement Learning
  Framework for Efficient Exploration",http://arxiv.org/abs/1807.11150,,,core
157737712,2018-05-04T00:00:00,"Robots that navigate among pedestrians use collision avoidance algorithms to
enable safe and efficient operation. Recent works present deep reinforcement
learning as a framework to model the complex interactions and cooperation.
However, they are implemented using key assumptions about other agents'
behavior that deviate from reality as the number of agents in the environment
increases. This work extends our previous approach to develop an algorithm that
learns collision avoidance among a variety of types of dynamic agents without
assuming they follow any particular behavior rules. This work also introduces a
strategy using LSTM that enables the algorithm to use observations of an
arbitrary number of other agents, instead of previous methods that have a fixed
observation size. The proposed algorithm outperforms our previous approach in
simulation as the number of agents increases, and the algorithm is demonstrated
on a fully autonomous robotic vehicle traveling at human walking speed, without
the use of a 3D Lidar",,"Motion Planning Among Dynamic, Decision-Making Agents with Deep
  Reinforcement Learning",http://arxiv.org/abs/1805.01956,,,core
127611216,2017-11-08T00:00:00,"International audienceBackground:  In maize, silks are hundreds of filaments that simultaneously emerge from the ear for collecting pollen over a period of 1–7 days, which largely determines grain number especially under water deficit. Silk growth is a major trait for drought tolerance in maize, but its phenotyping is difficult at throughputs needed for genetic analyses.Results:  We have developed a reproducible pipeline that follows ear and silk growths every day for hundreds of plants, based on an ear detection algorithm that drives a robotized camera for obtaining detailed images of ears and silks. We first select, among 12 whole‑plant side views, those best suited for detecting ear position. Images are seg‑mented, the stem pixels are labelled and the ear position is identified based on changes in width along the stem. A mobile camera is then automatically positioned in real time at 30 cm from the ear, for a detailed picture in which silks are identified based on texture and colour. This allows analysis of the time course of ear and silk growths of thousands of plants. The pipeline was tested on a panel of 60 maize hybrids in the PHENOARCH phenotyping platform. Over 360 plants, ear position was correctly estimated in 86% of cases, before it could be visually assessed. Silk growth rate, estimated on all plants, decreased with time consistent with literature. The pipeline allowed clear identification of the effects of genotypes and water deficit on the rate and duration of silk growth.Conclusions:  The pipeline presented here, which combines computer vision, machine learning and robotics, provides a powerful tool for large‑scale genetic analyses of the control of reproductive growth to changes in environ‑mental conditions in a non‑invasive and automatized way. It is available as Open Source software in the OpenAlea platform",'Springer Science and Business Media LLC',A robot-assisted imaging pipeline for tracking the growths of maize ear and silks in a high-throughput phenotyping platform,,,10.1186/s13007-017-0246-7,core
300018968,2017-01-01T00:00:00,"Robust and accurate object detection are needed for the applications to mobile robots.
Unfortunately, most of the existing object detection approaches cannot satisfy with the real
application due to either slow speed or lower accuracy. Computer Vision has been
increasingly important in enabling smart technologies. In this project, the author aims to
develop an object detection system which can provide high accuracy while reducing missing
detection by using stereo camera and state-of-the-art machine learning. Stereo vision is one of
the options that can be implemented in order to provide more data and parameters besides
visuals provided by a single camera. This will involve the use of the disparity map in order to
obtain depth information of the scene. By combining the use of depth information with a deep
learning framework, fast, robust and accurate stereo object detection can be achieved.Bachelor of Engineerin",,Stereo object detection with the applications to mobile robot,,,,core
228554112,2017-07-07T00:00:00,"This paper describes and discusses a research work on ""DeliBOT – A Mobile Robot with Implementation of SLAM utilizing Computer Vision/Machine Learning Techniques"". The principle objective is to study about the utilization of Kinect in mobile robotics and use it to assemble an integrated system framework equipped for building a map of environment, and localizing mobile robot with respect to the map using visual cues. There were four principle work stages. The initial step was studying and testing solutions for mapping and navigation with a RGB-D sensor, the Kinect. The accompanying stage was implementing a system framework equipped for identifying and localizing objects from the point cloud given by the Kinect, permitting the execution of further errands on the system framework, i.e. considering the computational load. The third step was identifying the landmarks and the improvement they can present in the framework. At last, the joining of the previous modules was led and experimental evaluation and validation of the integrated system. The demand of substitution of human by a robot is winding up noticeably more probable eager these days because of the likelihood of less mistakes that the robot apparently makes. Amid the previous couple of years, the technology turn out to be more accurate and legitimate outcomes with less errors, and researches started to consolidate more sensors. By utilizing accessible sensors, robot will perceive and identify environment it is in and makes map. Additionally, robot will have element of itself locating inside environment. Robot fundamental operations are identification of objects and localization for conduction of the services. Robot conduct appropriate path planning and avoidance of object by setting a target or determining goal [1]. Because of the outstanding research and robotics applications in almost every segments of life of human's, from space surveillance to health-care, solution is created for autonomous mobile robots direct tasks excluding human intervention in indoor environment [2], a few applications like cleaning facilities and transportation fields. Robot navigation in environment that is safe that performs profoundly, require environment map. Since in the greater part of applications in real-life map is not given, exploration algorithm is used",International Journal of Innovative Technology and Research,DELIBOT WITH SLAM IMPLEMENTATION,https://core.ac.uk/download/228554112.pdf,,,core
212627020,2018-05-23T07:00:00,"In a world that increasingly relies on automation and intelligent robotics, there is a need for drones to expand their independence and adaptability in navigating their environments. One approach to this problem is the use of wireless communication between units in order to coordinate their sensor data and build real-time maps of the environments they are navigating. However, especially indoors, relying on a fixed transmission tower to provide data to the units faces connectivity challenges.
The purpose of this research was to determine the fitness of an on-drone assembly that uses the the NI B200mini software-defined radio board and Gnu Radio Companion (GRC) software in use for drone-to-drone communication. Using a Raspberry Pi computer with a linux-based operating system to link the assembly to a Matrice 100 quadcopter drone, we explore the ability to transmit and receive data packets between drones as a way for them to share the burden of identifying objects in their environment and communicate the location of safe paths of travel using machine learning algorithms",PDXScholar,An Exploration of Software Defined Radio and GNU Radio Companion for Use in Drone-to-Drone Communication,https://core.ac.uk/download/212627020.pdf,,,core
83837427,2017-03-13T00:00:00,"Deep reinforcement learning is becoming increasingly popular for robot
control algorithms, with the aim for a robot to self-learn useful feature
representations from unstructured sensory input leading to the optimal
actuation policy. In addition to sensors mounted on the robot, sensors might
also be deployed in the environment, although these might need to be accessed
via an unreliable wireless connection. In this paper, we demonstrate deep
neural network architectures that are able to fuse information coming from
multiple sensors and are robust to sensor failures at runtime. We evaluate our
method on a search and pick task for a robot both in simulation and the real
world.Comment: 6 pages, 6 figures, submitted to IROS 201",,Sensor Fusion for Robot Control through Deep Reinforcement Learning,http://arxiv.org/abs/1703.04550,,10.1109/iros.2017.8206048,core
132065972,2017-01-25T00:00:00,"This demonstration presents an open-source hardware and software platform which allows non-roboticists researchers to conduct machine learning experiments to benchmark algorithms for autonomous exploration and active learning. In particular, in addition to showing the general properties of the platform such as its modularity and usability, it demonstrates the online functioning of a particular algorithm which allows efficient learning of multiple forward and inverse models and can leverage information from human guidance. A first aspect of the demonstration is to illustrate the ease of use of the 3D printed low-cost Poppy humanoid robotic platform, that allows non-roboticists to quickly set up and program robotic experiments. A second aspect is to show how the Explauto library allows systematic comparison and evaluation of active learning and exploration algorithms in sensorimotor spaces, through a Python API to select already implemented exploration algorithms. The third idea is to showcase Active Model Babbling, an efficient exploration algorithm dynamically choosing which task/goal space to explore and particular goals to reach, and integrating social guidance from humans in real time to drive exploration towards particular objects or actions. Contact us for a CC-BY version of the video ![Forestier and Oudeyer, 2016] Forestier, S. and Oudeyer, P.-Y. (2016). Modular active curiosity-driven discovery of tool use. In 2016 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Daejeon, Korea. [Lapeyre et al., 2014] Lapeyre, M., Rouanet, P., Grizou, J., Nguyen, S., Depraetre, F., Le Falher, A., and Oudeyer, P.-Y. (2014). Poppy Project: Open-Source Fabrication of 3D Printed Humanoid Robot for Science, Education and Art. In Digital Intelligence 2014, page 6, Nantes, France. [Moulin-Frier et al., 2014] Moulin-Frier, C., Rouanet, P., Oudeyer, P.-Y., and others (2014). Explauto: an open- source Python library to study autonomous exploration in developmental robotics. In ICDL-Epirob-International Conference on Development and Learning, Epirob",HAL CCSD,Intrinsically Motivated Multi-Task Reinforcement LearningWith Open-Source Explauto Library and Poppy Humanoid Robot,,,,core
157734706,2018-05-16T00:00:00,"Designing agile locomotion for quadruped robots often requires extensive
expertise and tedious manual tuning. In this paper, we present a system to
automate this process by leveraging deep reinforcement learning techniques. Our
system can learn quadruped locomotion from scratch using simple reward signals.
In addition, users can provide an open loop reference to guide the learning
process when more control over the learned gait is needed. The control policies
are learned in a physics simulator and then deployed on real robots. In
robotics, policies trained in simulation often do not transfer to the real
world. We narrow this reality gap by improving the physics simulator and
learning robust policies. We improve the simulation using system
identification, developing an accurate actuator model and simulating latency.
We learn robust controllers by randomizing the physical environments, adding
perturbations and designing a compact observation space. We evaluate our system
on two agile locomotion gaits: trotting and galloping. After learning in
simulation, a quadruped robot can successfully perform both gaits in the real
world.Comment: Accompanying video: https://www.youtube.com/watch?v=lUZUr7jxoq",,Sim-to-Real: Learning Agile Locomotion For Quadruped Robots,http://arxiv.org/abs/1804.10332,,,core
286600336,2018-01-01T00:00:00,": Deep learning is the most recent approach to achieve artificial intelligence. Especially neural networks are used for solving many human problems - from repetitive operations to intelligent recognizing in image, sound and text processing. They are used in medicine, car industry, game industry and robotics. Business companies also try to find the way of exploitation of the latest technology despite the fact that it is the long way to the point where machines will be capable to replace the human intelligence. Authors of this paper explore possibilities of semi-supervised learning application in accounting. One of the latest deep learning algorithm is successfully used to reconstruct the journal entry key columns. The model was trained and tested on a real-world dataset so it could become base for developing the wide pallet of accounting and audit applications - as anomaly detection module of Enterprise Resource Planning (ERP) software or as a standalone application",,Journal entries with deep learning model,,,,core
200670767,2018-10-05T08:46:28Z,"In this work, teams of small mobile robots are used to test hypotheses about cooperative transport by ants. This study attempts to explain a decrease in steady-state transport speed with increasing team size that was previously observed in the ant <i>Novomessor cockerelli</i>. Two models of one-dimensional collective towing are compared: one in which transporters with different maximum speeds pull the payload with continuous, variable forces, and another in which transporters with identical speeds pull with intermittent, unsynchronized forces. A statistical analysis of ant data supports the hypothesis that ants behave according to the first model, in which the steady-state transport speed is the maximum speed of the slowest teammate. By contrast, the ant data are not consistent with the second model, which predicts constant speed regardless of team size. To verify these predictions, the ant behaviours in each model are translated into decentralized controllers and implemented on teams of two to four robots. The controller for the first model incorporates a real-time reinforcement learning algorithm that successfully reproduces the observed relationship between ant team size and transport speed. The controller for the second model yields the predicted invariance of transport speed with team size. These results show the value of robotic swarms for testing mechanistic hypotheses about biological collectives",,Explanation of the assumption the ant data is normally distributed for proper order statistic analysis. from Multi-robot replication of ant collective towing behaviours,,,10.6084/m9.figshare.7171844.v1,core
189837834,2018-09-02T00:00:00,"We have seen much recent progress in rigid object manipulation, but in- teraction with deformable objects has notably lagged behind. Due to the large con- figuration space of deformable objects, solutions using traditional modelling ap- proaches require significant engineering work. Perhaps then, bypassing the need for explicit modelling and instead learning the control in an end-to-end manner serves as a better approach? Despite the growing interest in the use of end-to-end robot learning approaches, only a small amount of work has focused on their ap- plicability to deformable object manipulation. Moreover, due to the large amount of data needed to learn these end-to-end solutions, an emerging trend is to learn control policies in simulation and then transfer them over to the real world. To- date, no work has explored whether it is possible to learn and transfer deformable object policies. We believe that if sim-to-real methods are to be employed fur- ther, then it should be possible to learn to interact with a wide variety of objects, and not only rigid objects. In this work, we use a combination of state-of-the-art deep reinforcement learning algorithms to solve the problem of manipulating de- formable objects (specifically cloth). We evaluate our approach on three tasks — folding a towel up to a mark, folding a face towel diagonally, and draping a piece of cloth over a hanger. Our agents are fully trained in simulation with domain randomisation, and then successfully deployed in the real world without having seen any real deformable objects",PMLR,Sim-to-real reinforcement learning for deformable object manipulation,,,,core
161100778,2018-09-12T00:00:00,"Despite of its long history, opera as an art form is constantly evolving. Composers have never lost their fascination about it and keep exploring with innovative aesthetics, techniques, and modes of expression. New technologies, such as Virtual Reality (VR), Robotics and Artificial Intelligence (AI) are steadily having an impact upon the world of opera. The evolving use of performance-based software such as Ableton Live and Max/MSP has created new and exciting compositional techniques that intertwine theatrical and musical performance. This paper presents some initial work on the development of an opera using such technologies that is being composed by Kallionpää and Chamberlain. Furthermore, it presents two composition case studies by Kallionpää: “She” (2017) and puppet opera “Croak” (2018), as well as their documentation within the world's first 360° 3D VR recordings with full spatial audio in third-order Ambisonics and the application of an unmixing paradigm for focusing and isolating individual voices",,"Under construction – contemporary opera in the crossroads between new aesthetics, techniques, and technologies",https://core.ac.uk/download/161100778.pdf,,,core
304993626,2018-05-31T18:12:19,"The objective of the proposed research is to develop methodologies, support algorithms and software-hardware infrastructure for detection and diagnosis of parametric failures, transient soft errors and security attacks in linear and nonlinear circuits and systems for sensing and control. This research is motivated by the proliferation of autonomous sense-and-control real-time systems, such as intelligent robots and self-driven cars, that must maintain a minimum level of performance in the presence of unavoidable electro-mechanical degradation of system-level components in the field as well as external security attacks. A key focus is on rapid recovery from the effects of such anomalies and impairments with minimal impact on system performance while maintaining low implementation overhead as opposed to traditional schemes for recovery that rely on duplication or triplication. Real-time detection and diagnosis techniques are investigated and rely on analysis of state-space encoding based check signatures. For on-line error detection and diagnosis in control systems, linear and nonlinear state space encodings of the system behavior are analyzed in real-time. Recovery is initiated using guided reinforcement learning algorithms that determine how best the system should be controlled in the presence of the diagnosed performance impairments. For cyber-physical systems, these state-space encodings are used to detect malicious security attacks and to diagnose the affected components swiftly. These checks are utilized for fast recovery from such attacks while avoiding catastrophic system failure. Further research in this area will pave the way for successful deployment of self-healing autonomous systems and resilient cyber-physical systems.Ph.D",Georgia Institute of Technology,State-space encoding driven error resilience in control systems and circuits,https://core.ac.uk/download/304993626.pdf,,,core
154988840,2018-04-11T00:00:00,"In the context of Human-Robot Interaction (HRI), face Re-Identification (face
Re-ID) aims to verify if certain detected faces have already been observed by
robots. The ability of distinguishing between different users is crucial in
social robots as it will enable the robot to tailor the interaction strategy
toward the users' individual preferences. So far face recognition research has
achieved great success, however little attention has been paid to the realistic
applications of Face Re-ID in social robots. In this paper, we present an
effective and unsupervised face Re-ID system which simultaneously re-identifies
multiple faces for HRI. This Re-ID system employs Deep Convolutional Neural
Networks to extract features, and an online clustering algorithm to determine
the face's ID. Its performance is evaluated on two datasets: the TERESA video
dataset collected by the TERESA robot, and the YouTube Face Dataset (YTF
Dataset). We demonstrate that the optimised combination of techniques achieves
an overall 93.55% accuracy on TERESA dataset and an overall 90.41% accuracy on
YTF dataset. We have implemented the proposed method into a software module in
the HCI^2 Framework for it to be further integrated into the TERESA robot, and
has achieved real-time performance at 10~26 Frames per second.Comment: Accepted to Pattern Recognition Letter",,"A real-time and unsupervised face Re-Identification system for
  Human-Robot Interaction",http://arxiv.org/abs/1804.03547,,,core
292925341,2017-03,"This paper investigates three different technologies for solving a planning and scheduling problem of deploying multiple robots in a retirement home environment to assist elderly residents. The models proposed make use of standard techniques and solvers developed in AI planning and scheduling, with two primary motivations. First, to find a planning and scheduling solution that we can deploy in our real-world application. Second, to evaluate planning and scheduling technology in terms of the ``model-and-solve'' functionality that forms a major research goal in both domain-independent planning and constraint programming. Seven variations of our application are studied using the following three technologies: PDDL-based planning, time-line planning and scheduling, and constraint-based scheduling. The variations address specific aspects of the problem that we believe can impact the performance of the technologies while also representing reasonable abstractions of the real world application. We evaluate the capabilities of each technology and conclude that a constraint-based scheduling approach, specifically a decomposition using constraint programming, provides the most promising results for our application. PDDL-based planning is able to find mostly low quality solutions while the timeline approach was unable to model the full problem without alterations to the solver code, thus moving away from the model-and-solve paradigm. It would be misleading to conclude that constraint programming is ``better'' than PDDL-based planning in a general sense, both because we have examined a single application and because the approaches make different assumptions about the knowledge one is allowed to embed in a model. Nonetheless, we believe our investigation is valuable for AI planning and scheduling researchers as it highlights these different modelling assumptions and provides insight into avenues for the application of AI planning and scheduling for similar robotics problems. In particular, as constraint programming has not been widely applied to robot planning and scheduling in the literature, our results suggest significant untapped potential in doing so.This research has been funded by the Natural Sciences and Engineering Council of Canada
(NSERC), Dr. Robot Inc., and the Canada Research Chairs (CRC) Program",AI Access Foundation,Robots in Retirement Homes: Applying Off-the-Shelf Planning and Scheduling to a Team of Assistive Robots,,,10.1613/jair.5306,core
73440570,2017-03-09T00:00:00,"Deep reinforcement learning (RL) can acquire complex behaviors from low-level
inputs, such as images. However, real-world applications of such methods
require generalizing to the vast variability of the real world. Deep networks
are known to achieve remarkable generalization when provided with massive
amounts of labeled data, but can we provide this breadth of experience to an RL
agent, such as a robot? The robot might continuously learn as it explores the
world around it, even while deployed. However, this learning requires access to
a reward function, which is often hard to measure in real-world domains, where
the reward could depend on, for example, unknown positions of objects or the
emotional state of the user. Conversely, it is often quite practical to provide
the agent with reward functions in a limited set of situations, such as when a
human supervisor is present or in a controlled setting. Can we make use of this
limited supervision, and still benefit from the breadth of experience an agent
might collect on its own? In this paper, we formalize this problem as
semisupervised reinforcement learning, where the reward function can only be
evaluated in a set of ""labeled"" MDPs, and the agent must generalize its
behavior to the wide range of states it might encounter in a set of ""unlabeled""
MDPs, by using experience from both settings. Our proposed method infers the
task objective in the unlabeled MDPs through an algorithm that resembles
inverse RL, using the agent's own prior experience in the labeled MDPs as a
kind of demonstration of optimal behavior. We evaluate our method on
challenging tasks that require control directly from images, and show that our
approach can improve the generalization of a learned deep neural network policy
by using experience for which no reward function is available. We also show
that our method outperforms direct supervised learning of the reward.Comment: ICLR 201",,Generalizing Skills with Semi-Supervised Reinforcement Learning,http://arxiv.org/abs/1612.00429,,,core
159635931,2018-01-01T00:00:00,"Enabling a robot to properly interact with users plays a key role in the effective deployment of robotic platforms in domestic environments.

Robots must be able to rely on interaction to improve their behaviour and adaptively understand their operational world.

Semantic mapping is the task of building a representation of the environment, that can be enhanced through interaction with the user. In this task, a proper and effective acquisition of semantic attributes of targeted entities is essential for the task accomplishment itself.

In this paper, we focus on the problem of learning dialogue policies to support semantic attribute acquisition, so that the effort required by humans in providing knowledge to the robot through dialogue is minimized.

To this end, we design our Dialogue Manager as a multi-objective Markov Decision Process, solving the optimisation problem through Reinforcement Learning. The Dialogue Manager interfaces with an online incremental visual classifier, based on a Load-Balancing Self-Organizing Incremental Neural Network (LB-SOINN).

Experiments in a simulated scenario show the effectiveness of the proposed solution, suggesting that perceptual information can be properly exploited to reduce human tutoring cost.

Moreover, a dialogue policy trained on a small amount of data generalises well to larger datasets, and so the proposed online scheme, as well as the real-time nature of the processing, are suited for an extensive deployment in real scenarios. To this end, this paper provides a demonstration of the complete system on a real robot",IFAAMAS,Incrementally learning semantic attributes through dialogue interaction,,,,core
346538510,2017-01-01T00:00:00,"Inverse Reinforcement Learning (IRL) for path planning enables robots to learn cost functions for difficult tasks from demonstration, instead of hard-coding them. However, IRL methods face practical limitations that stem from the need to repeat costly planning procedures. In this paper, we propose Rapidly Exploring Learning Trees (RLT∗ ), which learns the cost functions of Optimal Rapidly Exploring Random Trees (RRT∗ ) from demonstration, thereby making inverse learning methods applicable to more complex tasks. Our approach extends Maximum Margin Planning to work with RRT∗ cost functions. Furthermore, we propose a caching scheme that greatly reduces the computational cost of this approach. Experimental results on simulated and real-robot data from a social navigation scenario show that RLT∗ achieves better performance at lower computational cost than existing methods. We also successfully deploy control policies learned with RLT∗ on a real telepresence robot",'Institute of Electrical and Electronics Engineers (IEEE)',Rapidly exploring learning trees,,,,core
301262346,2018-01-01T00:00:00,"The final publication is available at link.springer.comSocial robots should be able to search and track people in order to help them. In this paper we present two different techniques for coordinated multi-robot teams for searching and tracking people. A probability map (belief) of a target person location is maintained, and to initialize and update it, two methods were implemented and tested: one based on a reinforcement learning algorithm and the other based on a particle filter. The person is tracked if visible, otherwise an exploration is done by making a balance, for each candidate location, between the belief, the distance, and whether close locations are explored by other robots of the team. The validation of the approach was accomplished throughout an extensive set of simulations using up to five agents and a large amount of dynamic obstacles; furthermore, over three hours of real-life experiments with two robots searching and tracking were recorded and analysed.Peer Reviewe",'Springer Science and Business Media LLC',Searching and tracking people with cooperative mobile robots,,"[{'title': 'Autonomous Robots', 'identifiers': ['0929-5593', 'issn:0929-5593']}]",10.1007/s10514-017-9681-6,core
153598490,2018-04-01T00:00:00Z,"Human–robot collaboration could be advanced by facilitating the intuitive, gaze-based control of robots, and enabling robots to recognize human actions, infer human intent, and plan actions that support human goals. Traditionally, gaze tracking approaches to action recognition have relied upon computer vision-based analyses of two-dimensional egocentric camera videos. The objective of this study was to identify useful features that can be extracted from three-dimensional (3D) gaze behavior and used as inputs to machine learning algorithms for human action recognition. We investigated human gaze behavior and gaze–object interactions in 3D during the performance of a bimanual, instrumental activity of daily living: the preparation of a powdered drink. A marker-based motion capture system and binocular eye tracker were used to reconstruct 3D gaze vectors and their intersection with 3D point clouds of objects being manipulated. Statistical analyses of gaze fixation duration and saccade size suggested that some actions (pouring and stirring) may require more visual attention than other actions (reach, pick up, set down, and move). 3D gaze saliency maps, generated with high spatial resolution for six subtasks, appeared to encode action-relevant information. The “gaze object sequence” was used to capture information about the identity of objects in concert with the temporal sequence in which the objects were visually regarded. Dynamic time warping barycentric averaging was used to create a population-based set of characteristic gaze object sequences that accounted for intra- and inter-subject variability. The gaze object sequence was used to demonstrate the feasibility of a simple action recognition algorithm that utilized a dynamic time warping Euclidean distance metric. Averaged over the six subtasks, the action recognition algorithm yielded an accuracy of 96.4%, precision of 89.5%, and recall of 89.2%. This level of performance suggests that the gaze object sequence is a promising feature for action recognition whose impact could be enhanced through the use of sophisticated machine learning classifiers and algorithmic improvements for real-time implementation. Robots capable of robust, real-time recognition of human actions during manipulation tasks could be used to improve quality of life in the home and quality of work in industrial environments",Frontiers Media S.A.,Exploiting Three-Dimensional Gaze Tracking for Action Recognition During Bimanual Manipulation to Enhance Human–Robot Collaboration,,"[{'title': None, 'identifiers': ['issn:2296-9144', '2296-9144']}]",10.3389/frobt.2018.00025/full,core
186266928,2018-09-16T00:00:00,"Cloud Robotics is a paradigm where distributed robots are connected to cloud
services via networks to access unlimited computation power, at the cost of
network communication. However, due to limitations such as network latency and
variability, it is difficult to control dynamic, human compliant service robots
directly from the cloud. In this work, by leveraging asynchronous protocol with
a heartbeat signal, we combine cloud robotics with a smart edge device to build
a Fog Robotic system. We use the system to enable robust teleoperation of a
dynamic self-balancing robot from the cloud. We first use the system to pick up
boxes from static locations, a task commonly performed in warehouse logistics.
To make cloud teleoperation more efficient, we deploy image based visual
servoing (IBVS) to perform box pickups automatically. Visual feedbacks,
including apriltag recognition and tracking, are performed in the cloud to
emulate a Fog Robotic object recognition system for IBVS. We demonstrate the
feasibility of real-time dynamic automation system using this cloud-edge
hybrid, which opens up possibilities of deploying dynamic robotic control with
deep-learning recognition systems in Fog Robotics. Finally, we show that Fog
Robotics enables the self-balancing service robot to pick up a box
automatically from a person under unstructured environments.Comment: 7 pages, 5 figures, ICRA 2019 (submitted, under review",,A Fog Robotic System for Dynamic Visual Servoing,http://arxiv.org/abs/1809.06716,,,core
197913932,2018-07-10T00:00:00,"International audienceHierarchical Task Network (HTN) planning is a proven approach to solving complex, real world planning problems more efficiently than planning from first principles when “standard operating procedures” (or “recipes”) can be supplied by the user. By planning for tasks in the same order that they are later executed, total-order HTN planners always know the complete state of the world at each planning step. This enables writing more expressive planning domains than what is possible in partial-order HTN planning, such as preconditions with calls to external procedures.Such features have facilitated the use of total-order HTN planners in agent systems and seen them excel in AI games.This paper describes the Hierarchical Agent-based Task Planner (HATP), a total-order HTN planner. Since its first implementation, HATP has had various extensions and integrations over the years, such as support for splitting a solution into multiple streams and assigning them to the agents in the domain; modelling their beliefs as distinct world states; allowing “social rules” to be included by the user to define what kind of agent behaviour is appropriate; allowing tasks to be planned by taking the human’s safety and comfort into account; and to interleave HTN and geometric planning.Since many of these implementations have remained prototypes, we have significantly enhanced them as well as HATP itself, and integrated them into a stand-alone HATP distribution, which is now available as open source software (under a BSD 2-Clause License).This paper presents some of our recent improvements to HATP, and gives an overview of its user-friendly language, which treats agents as distinct entities; its mechanisms for effective control over decomposition; and its integration into our robotics framewor",HAL CCSD,HATP: Hierarchical Agent-based Task Planner: Demonstration,,,,core
212884959,2018-10-01T07:00:00,"The field of urolithiasis has undergone many rapid changes in the last 3 decades. In this article, three eminent experts in various fields of urolithiasis research describe their respective visions for the future in stone research, stone treatment and surgical training. Many stone researchers have seen and regretted that there has not been a real breakthrough for decades now. Exceptions are the application of citrate prophylaxis and the abandonment of calcium-avoiding diet in stone formers. Certain areas of stone research have been exhausted and the body of literature available should suffice as background knowledge in those. Yet, to find meaningful mechanisms of clinically applicable stone prevention, the limited funds which are currently available should be used to research priority areas, of which crystal-cell interaction is envisioned by one of the present authors as being a crucial direction in future stone research. In the opinion of the second author, surgical stone treatment is very much technology-driven. This applies to the evolution of existing technologies and instruments. In addition, robotics, IT and communication software, and artificial intelligence are promising and are steadily making a meaningful impact in medicine in general, and endourology in particular. Finally, the third author believes that despite the exciting advances in technology, the role of the surgeon can never be replaced. The idea of a fully automated, artificially thinking and robotically performing system treating patients medically and surgically will not appeal to urologists or patients but may at least be a partial reality. His vision therefore is that surgical training will have to take on a new dimension, away from the patient and towards virtual reality, until the skill set is acceptably developed",eCommons@AKU,"Vision for the future on urolithiasis: Research, management, education and training-some personal views",,,,core
250573750,2018-07-10T00:00:00,"International audienceHierarchical Task Network (HTN) planning is a proven approach to solving complex, real world planning problems more efficiently than planning from first principles when “standard operating procedures” (or “recipes”) can be supplied by the user. By planning for tasks in the same order that they are later executed, total-order HTN planners always know the complete state of the world at each planning step. This enables writing more expressive planning domains than what is possible in partial-order HTN planning, such as preconditions with calls to external procedures.Such features have facilitated the use of total-order HTN planners in agent systems and seen them excel in AI games.This paper describes the Hierarchical Agent-based Task Planner (HATP), a total-order HTN planner. Since its first implementation, HATP has had various extensions and integrations over the years, such as support for splitting a solution into multiple streams and assigning them to the agents in the domain; modelling their beliefs as distinct world states; allowing “social rules” to be included by the user to define what kind of agent behaviour is appropriate; allowing tasks to be planned by taking the human’s safety and comfort into account; and to interleave HTN and geometric planning.Since many of these implementations have remained prototypes, we have significantly enhanced them as well as HATP itself, and integrated them into a stand-alone HATP distribution, which is now available as open source software (under a BSD 2-Clause License).This paper presents some of our recent improvements to HATP, and gives an overview of its user-friendly language, which treats agents as distinct entities; its mechanisms for effective control over decomposition; and its integration into our robotics framewor",HAL CCSD,HATP: Hierarchical Agent-based Task Planner: Demonstration,,,,core
297653331,,"Modern day-to-day operations, even in large-scale enterprise, still involve manual data entry, manual
verification of information, and manual handover of information from one party to the next party to be
processed. This current method of working very much leaves gaps in the process chain resulting in
incompleteness, inaccuracy and inconsistency of information. It also leaves loopholes for information
theft and misuse as well as involve a lot of time and effort in the form of manual repetitive non-valueadded work to verify and correct information before information can be used. In this study, the authors
present a novel idea and practical methods to automate the input and process the flow of enterprise
information by using robotic process automation (RPA). This platform is to be referred to as Cognitive
Automation Robots (CAR), developed by the National Applied R&D Centre of Malaysia, MIMOS
Berhad. Rule-based task software robots form a digital workforce to automate business processes in
interacting with systems and applications. The cognitive trait of these robots performs self-learning on
baseline information and continue to improve processes to optimal levels on their ownthe robots use
natural language processing, machine learning and reasoning for expert decision making. Business
insights such as statistics from the real-time digital trail of the robots\u27 activities are then generated to
measure effectiveness to be feedback into the system as operational analytics. CAR improves enterprise
operations that have historically required human intelligence, experience and situational reasoning","ICSSR e-Journal of the Social Science Research, WorldConferences.net Koperasi Kolej Universiti Islam Antarabangsa Selangor",Cognitive automation robots (CAR),,,,core
201869339,2018-06-01T00:00:00Z,"Recent studies suggest that some children with autism prefer robots as tutors for improving their social interaction and communication abilities which are impaired due to their disorder. Indeed, research has focused on developing a very promising form of intervention named Robot-Assisted Therapy. This area of intervention poses many challenges, including the necessary flexibility and adaptability to real unconstrained therapeutic settings, which are different from the constrained lab settings where most of the technology is typically tested. Among the most common impairments of children with autism and intellectual disability is social attention, which includes difficulties in establishing the correct visual focus of attention. This article presents an investigation on the use of novel deep learning neural network architectures for automatically estimating if the child is focusing their visual attention on the robot during a therapy session, which is an indicator of their engagement. To study the application, the authors gathered data from a clinical experiment in an unconstrained setting, which provided low-resolution videos recorded by the robot camera during the child&ndash;robot interaction. Two deep learning approaches are implemented in several variants and compared with a standard algorithm for face detection to verify the feasibility of estimating the status of the child directly from the robot sensors without relying on bulky external settings, which can distress the child with autism. One of the proposed approaches demonstrated a very high accuracy and it can be used for off-line continuous assessment during the therapy or for autonomously adapting the intervention in future robots with better computational capabilities",MDPI AG,Deep Learning Systems for Estimating Visual Attention in Robot-Assisted Therapy of Children with Autism and Intellectual Disability,,"[{'title': None, 'identifiers': ['2218-6581', 'issn:2218-6581']}]",10.3390/robotics7020025,core
201147855,2018-12-01T00:00:00,"The active implementation of digital technologies into all spheres of public life, as well as the rapid development of artificial intelligence, is assuming a serious dimension, thus requiring a special attention of the legislator. The article examines the current state of the legal regulation of the artificial intelligence. The author considers the Strategy of the Information Society Development in the Russian Federation for 2017-2030, as well as provides some clear examples of active implementation of artificial intelligence into social reality. The author also provides the McKinsey consulting group's research findings which reflect the prospects for replacing human labor by robots. It is pointed out that the issue of total computerization and the corresponding displacement of a human from the sphere of intellectual activity is rather controversial. The article also discusses the main possible problems related to the artificial intelligence technologies: the problems of responsibility that may arise in the operation of industrial robots; the continuity of digital activity can affect the psychoemotional state. The issue of a possibility for creating robots with intelligence and endowed with personality is being considered from the Philosophy perspective. The conclusion is drawn that the theoretical study of the intellect and the ""electronic person"" is one of the possible redirections of the Russian law development in modern conditions",'Peoples'' Friendship University of Russia',Artificial Intelligence in the Legal Space,,"[{'title': 'RUDN JOURNAL OF LAW', 'identifiers': ['2408-9001', 'issn:2408-9001', 'issn:2313-2337', '2313-2337']}]",10.22363/2313-2337-2018-22-3-315-328,core
153378042,2018-03-14T00:00:00,"Recent advances in artificial intelligence (AI), specifically in computer
vision (CV) and deep learning (DL), have created opportunities for novel
systems in many fields. In the last few years, deep learning applications have
demonstrated impressive results not only in fields such as autonomous driving
and robotics, but also in the field of medicine, where they have, in some
cases, even exceeded human-level performance. However, despite the huge
potential, adoption of deep learning-based methods is still slow in many areas,
especially in veterinary medicine, where we haven't been able to find any
research papers using modern convolutional neural networks (CNNs) in medical
image processing. We believe that using deep learning-based medical imaging can
enable more accurate, faster and less expensive diagnoses in veterinary
medicine. In order to do so, however, these methods have to be accessible to
everyone in this field, not just to computer scientists. To show the potential
of this technology, we present results on a real-world task in veterinary
medicine that is usually done manually: feline reticulocyte percentage. Using
an open source Keras implementation of the Single-Shot MultiBox Detector (SSD)
model architecture and training it on only 800 labeled images, we achieve an
accuracy of 98.7% at predicting the correct number of aggregate reticulocytes
in microscope images of cat blood smears. The main motivation behind this paper
is to show not only that deep learning can approach or even exceed human-level
performance on a task like this, but also that anyone in the field can
implement it, even without a background in computer science.Comment: 10 pages, 2 figure",,"Using Convolutional Neural Networks for Determining Reticulocyte
  Percentage in Cats",http://arxiv.org/abs/1803.04873,,,core
186263760,2018-09-14T00:00:00,"Reinforcement Learning methods are capable of solving complex problems, but
resulting policies might perform poorly in environments that are even slightly
different. In robotics especially, training and deployment conditions often
vary and data collection is expensive, making retraining undesirable.
Simulation training allows for feasible training times, but on the other hand
suffers from a reality-gap when applied in real-world settings. This raises the
need of efficient adaptation of policies acting in new environments. We
consider this as a problem of transferring knowledge within a family of similar
Markov decision processes.
  For this purpose we assume that Q-functions are generated by some
low-dimensional latent variable. Given such a Q-function, we can find a master
policy that can adapt given different values of this latent variable. Our
method learns both the generative mapping and an approximate posterior of the
latent variables, enabling identification of policies for new tasks by
searching only in the latent space, rather than the space of all policies. The
low-dimensional space, and master policy found by our method enables policies
to quickly adapt to new environments. We demonstrate the method on both a
pendulum swing-up task in simulation, and for simulation-to-real transfer on a
pushing task",,VPE: Variational Policy Embedding for Transfer Reinforcement Learning,http://arxiv.org/abs/1809.03548,,,core
389892212,2018-09-12T00:00:00,"PreCrash problem of Intelligent Control of autonomous vehicles robot is a very complex problem, especially vehicle pre-crash scenarios and at points of intersections in real-time environments.The goal of this research is to develop a new artificial intelligent adaptive controller for autonomous vehicle Pre-Crash system along with vehicle recognition module and tested in MATLAB including some detailed modules. Following tasks were set: finding Objects in sensor Data (LiDAR. RADAR), Speed and Steering control, vehicle Recognition using convolution neural network and Alexnet.In this research paper, we implemented a real-time image/Lidar processing. At the beginning, we presented a real-time system which is composed of comprehensive modules, these modules are 3d object detection, object clustering and search, ground removal, deep learning using convolutional neural networks. Starting with nearest vehicle  module our target is to find the nearest ahead car and consider it as our primary obstacle.This paper presents an Adaptive cruise pre-crash system and vehicle recognition. The Adaptive cruise pre-crash system module depends on Deep Learning and LiDAR sensor data, which meant to control the driver reckless behavior on the road by adjusting the vehicle speed to maintain a safe distance from objects ahead (such as cars, humans, bicycle or whatever the object) when the driver tries to raise speed. At the very moment the vehicle recognition module, detects and recognizes the vehicles surrounding to the car.Задача предаварийного интеллектуального управления робота автономных транспортных средств является очень сложной проблемой, особенно предаварийные условия транспортных средств и в точках пересечения в условиях реального времени.Целью данного исследования является разработка нового искусственного интеллектуального адаптивного регулятора для системы предаварийной безопасности автономных транспортных средств, а также модуля распознавания транспортных средств и тестирование в MATLAB, включая некоторые детализированные модули. Были поставлены следующие задачи: поиск объектов по данным датчиков (Лидар, Радар), контроль скорости и рулевого управления, распознавание транспортных средств с использованием сверточной нейронной сети и Alexnet.В данной исследовательской работе мы реализовали обработку изображений и лидарных данных в режиме реального времени. Вначале мы представили систему реального времени, которая состоит из комплексных модулей, а именно модули обнаружения трехмерных объектов, группирования и поиска объектов, удаления земли, глубинного обучения с использованием сверточных нейронных сетей. Начиная с модуля ближайшего транспортного средства, наша задача - найти ближайший впереди идущий автомобиль и считать его основным препятствием.В статье представлена адаптивная предаварийная система управления скоростью и распознавания транспортных средств. Модуль адаптивной предаварийной системы управления скоростью зависит от данных глубинного обучения и лидарного датчика, которые предназначены для управления безрассудным поведением водителя на дороге путем регулирования скорости транспортного средства для поддержания безопасного расстояния от объектов впереди (таких как автомобили, люди, велосипед или любой другой объект), когда водитель пытается повысить скорость. В настоящий момент модуль распознавания транспортных средств обнаруживает и распознает транспортные средства вокруг автомобиляЗавдання передаварійного інтелектуального керування робота автономних транспортних засобів є дуже складною проблемою, особливо передаварійні умови транспортних засобів і в точках перетину в умовах реального часу.Метою даного дослідження є розробка нового штучного інтелектуального адаптивного регулятора для системи передаварійної безпеки автономних транспортних засобів, а також модуля розпізнавання транспортних засобів та тестування в MATLAB, включаючи деякі деталізовані модулі. Були поставлені наступні завдання: пошук об'єктів за даними датчиків (Лiдар, Радар), контроль швидкості та рульового управління, розпізнавання транспортних засобів з використанням згорткової нейронної мережі та Alexnet.У даній дослідницькій роботі ми реалізували обробку зображень та лiдарних даних в режимі реального часу. Спочатку ми представили систему реального часу, яка складається з комплексних модулів, а саме модулі виявлення тривимірних об'єктів, групування та пошуку об'єктів, видалення землі, глибинного навчання з використанням згорткових нейронних мереж. Починаючи з модуля найближчого транспортного засобу, наше завдання - знайти найближчий попереду автомобіль і вважати його основною перешкодою.У статті представлена адаптивна передаварiйна система керування швидкістю та розпізнавання транспортних засобів. Модуль адаптивної передаварійної системи керування швидкістю залежить від даних глибинного навчання та лідарного датчика, які призначені для управління безрозсудною поведінкою водія на дорозі шляхом регулювання швидкості транспортного засобу для підтримки безпечної відстані від об'єктів попереду (таких як автомобілі, люди, велосипед або будь-який інший об'єкт), коли водій намагається підвищити швидкість. Наразi модуль розпізнавання транспортних засобів виявляє і розпізнає транспортні засоби навколо автомобіл",РС ТЕСHNOLOGY СЕNTЕR,Гібридний лiдарный/радарний механізм глибинного навчання та розпізнавання транспортних засобів для управління передаварійною безпекою автономних транспортних засобів,,,,core
157696490,2018-06-01T00:00:00,"© 2018 The Authors Recent advances in behavioural and computational neuroscience, cognitive robotics, and in the hardware implementation of large-scale neural networks, provide the opportunity for an accelerated understanding of brain functions and for the design of interactive robotic systems based on brain-inspired control systems. This is especially the case in the domain of action and language learning, given the significant scientific and technological developments in this field. In this work we describe how a neuroanatomically grounded spiking neural network for visual attention has been extended with a word learning capability and integrated with the iCub humanoid robot to demonstrate attention-led object naming. Experiments were carried out with both a simulated and a real iCub robot platform with successful results. The iCub robot is capable of associating a label to an object with a ‘preferred’ orientation when visual and word stimuli are presented concurrently in the scene, as well as attending to said object, thus naming it. After learning is complete, the name of the object can be recalled successfully when only the visual input is present, even when the object has been moved from its original position or when other objects are present as distractors",'Elsevier BV',Visual attention and object naming in humanoid robots using a bio-inspired spiking neural network,https://core.ac.uk/download/157696490.pdf,"[{'title': 'Robotics and Autonomous Systems', 'identifiers': ['issn:0921-8890', '0921-8890']}]",10.1016/j.robot.2018.02.010,core
162488734,2018-07-10T00:00:00,"International audienceHierarchical Task Network (HTN) planning is a proven approach to solving complex, real world planning problems more efficiently than planning from first principles when “standard operating procedures” (or “recipes”) can be supplied by the user. By planning for tasks in the same order that they are later executed, total-order HTN planners always know the complete state of the world at each planning step. This enables writing more expressive planning domains than what is possible in partial-order HTN planning, such as preconditions with calls to external procedures.Such features have facilitated the use of total-order HTN planners in agent systems and seen them excel in AI games.This paper describes the Hierarchical Agent-based Task Planner (HATP), a total-order HTN planner. Since its first implementation, HATP has had various extensions and integrations over the years, such as support for splitting a solution into multiple streams and assigning them to the agents in the domain; modelling their beliefs as distinct world states; allowing “social rules” to be included by the user to define what kind of agent behaviour is appropriate; allowing tasks to be planned by taking the human’s safety and comfort into account; and to interleave HTN and geometric planning.Since many of these implementations have remained prototypes, we have significantly enhanced them as well as HATP itself, and integrated them into a stand-alone HATP distribution, which is now available as open source software (under a BSD 2-Clause License).This paper presents some of our recent improvements to HATP, and gives an overview of its user-friendly language, which treats agents as distinct entities; its mechanisms for effective control over decomposition; and its integration into our robotics framewor",HAL CCSD,HATP: Hierarchical Agent-based Task Planner: Demonstration,,,,core
157734508,2018-07-28T00:00:00,"In the past decade many robots were deployed in the wild, and people
detection and tracking is an important component of such deployments. On top of
that, one often needs to run modules which analyze persons and extract higher
level attributes such as age and gender, or dynamic information like gaze and
pose. The latter ones are especially necessary for building a reactive, social
robot-person interaction.
  In this paper, we combine those components in a fully modular
detection-tracking-analysis pipeline, called DetTA. We investigate the benefits
of such an integration on the example of head and skeleton pose, by using the
consistent track ID for a temporal filtering of the analysis modules'
observations, showing a slight improvement in a challenging real-world
scenario. We also study the potential of a so-called ""free-flight"" mode, where
the analysis of a person attribute only relies on the filter's predictions for
certain frames. Here, our study shows that this boosts the runtime
dramatically, while the prediction quality remains stable. This insight is
especially important for reducing power consumption and sharing precious
(GPU-)memory when running many analysis components on a mobile platform,
especially so in the era of expensive deep learning methods.Comment: Code available at: https://github.com/sbreuers/dett",,Detection-Tracking for Efficient Person Analysis: The DetTA Pipeline,http://arxiv.org/abs/1804.10134,,,core
157797947,2018-05-01T00:00:00Z,"A novel approach is proposed for the path tracking of a Wheeled Mobile Robot (WMR) in the presence of an unknown lateral slip. Much of the existing work has assumed pure rolling conditions between the wheel and ground. Under the pure rolling conditions, the wheels of a WMR are supposed to roll without slipping. Complex wheel-ground interactions, acceleration and steering system noise are the factors which cause WMR wheel slip. A basic research problem in this context is localization and slip estimation of WMR from a stream of noisy sensors data when the robot is moving on a slippery surface, or moving at a high speed. DecaWave based ranging system and Particle Filter (PF) are good candidates to estimate the location of WMR indoors and outdoors. Unfortunately, wheel-slip of WMR limits the ultimate performance that can be achieved by real-world implementation of the PF, because location estimation systems typically partially rely on the robot heading. A small error in the WMR heading leads to a large error in location estimation of the PF because of its cumulative nature. In order to enhance the tracking and localization performance of the PF in the environments where the main reason for an error in the PF location estimation is angular noise, two methods were used for heading estimation of the WMR (1): Reinforcement Learning (RL) and (2): Location-based Heading Estimation (LHE). Trilateration is applied to DecaWave based ranging system for calculating the probable location of WMR, this noisy location along with PF current mean is used to estimate the WMR heading by using the above two methods. Beside the WMR location calculation, DecaWave based ranging system is also used to update the PF weights. The localization and tracking performance of the PF is significantly improved through incorporating heading error in localization by applying RL and LHE. Desired trajectory information is then used to develop an algorithm for extracting the lateral slip along X- and Y-axis from the PF estimated position of the WMR, the lateral slip along X- and Y-axis is then used to take some corrective measures. Lateral slip information is also used to find the direction along which WMR has to move to get back along the desired trajectory. Simulation results show that our proposed LHE and RL heading estimation methods significantly improve the PF localization and tracking performance on a slippery surface in both indoor and outdoor environments. The simulation results also show that the accurate locations of WMR and desired path information are used to estimate and compensate the lateral slip",MDPI AG,"A Robust Localization, Slip Estimation, and Compensation System for WMR in the Indoor Environments",,"[{'title': None, 'identifiers': ['2073-8994', 'issn:2073-8994']}]",10.3390/sym10050149,core
160745365,2018-11-01T00:00:00,"Robotic systems, which are controlled by artificial intelligent or tele-operation control interfaces, have been developed to be deployed instead of the human in extreme environments. However, insufficient artificial intelligence performance in unknown and unpredictable environments, and non-intuitive control interfaces with low immersive feedback have prevented wide spread of such robotic systems. In this paper, an intuitive and interactive control interface with inertial measurement units (IMUs), haptic gloves and a head mounted display (HMD) was developed to control a tele-operated robot in remote environments, which was abbreviated as AVATAR system. The tele-operated robot can be operated by a user&apos;s motions which are measured by the wearable interface. Through a kinematic analysis of the user and the tele-operated robot, desired robot joint angles are calculated to follow the user&apos;s motions in real time. Also, dual cameras on the robot head provide 3D visual information around the robot to the user. A grasping force of the robot hands, measured by motor current, is transmitted to the user as vibration feedback to fingertips of the haptic gloves. A long term evolution (LTE) was used as wireless communication between the user and the robot. The performance of the proposed AVATAR system has been verified by experiments. &amp;#169; 2018 Elsevier Ltd

SciVal Topic Prominence &amp;#61273;  










 
Topic: Units of measurement | Sensors | orientation estimation 


Prominence percentile: 95.750 


 

&amp;#61273;  




Robotic systems, which are controlled by artificial intelligent or tele-operation control interfaces, have been developed to be deployed instead of the human in extreme environments. However, insufficient artificial intelligence performance in unknown and unpredictable environments, and non-intuitive control interfaces with low immersive feedback have prevented wide spread of such robotic systems. In this paper, an intuitive and interactive control interface with inertial measurement units (IMUs), haptic gloves and a head mounted display (HMD) was developed to control a tele-operated robot in remote environments, which was abbreviated as AVATAR system. The tele-operated robot can be operated by a user&apos;s motions which are measured by the wearable interface. Through a kinematic analysis of the user and the tele-operated robot, desired robot joint angles are calculated to follow the user&apos;s motions in real time. Also, dual cameras on the robot head provide 3D visual information around the robot to the user. A grasping force of the robot hands, measured by motor current, is transmitted to the user as vibration feedback to fingertips of the haptic gloves. A long term evolution (LTE) was used as wireless communication between the user and the robot. The performance of the proposed AVATAR system has been verified by experiments. &amp;#169; 2018 Elsevier Lt",'Elsevier BV',An Interactive and Intuitive Control Interface for a Tele-operated Robot (AVATAR) System,,"[{'title': 'Mechatronics', 'identifiers': ['issn:0957-4158', '0957-4158']}]",10.1016/j.mechatronics.2018.08.011,core
478158895,2018-10-01T00:00:00,"The possibility of simulated trained artificial intelligence (AI) for robotic locomotive behaviour has been shown to be a possibility through work such done by others including work done by Schulman and his colleagues (Schulman, Wolski, Dhariwal, Radford, Klimov 2017). However, how accurate this AI is when implemented into an actual robot has not been fully looked at. An attempt to understand how accurate this is was attempted. The first component was to investigate and settle on the optimization algorithm and the AI to be used. The conclusion was to use a policy and value function neural network (NN) and a proximal policy optimization (PPO). This method was sourced from Github, it is titled TRPO (trust region policy optimization) and effectively develops and optimizes the NN (Coady, P, 2017). The benefit of this code was that it was built specifically to make use of a simulation software named Mujoco (Todorov, E, Erez, T, Tassa, Y 2012). It also used OpenAI Gym to develop the AI environment (Brockman, G, Cheung, V, Pettersson, L, Schneider, J, Schulman, J, Tang, J, Zaremba, W 2016). This gave access to pre-built Mujoco models for a bipedal and quadrupedal robot model. A robot was needed after this, the robot that was decided upon was a quadrupedal Arduino based robot it provided complexity that would likely increase any divergences in the operation of the NN when moved from the simulation to the real world. A Mujoco model was altered to match the design of the robot and its operation. Separate code needed to be written and interfaced with the TRPO code to allow for access to the robot with observation of the robot being formed from SHARP infrared height sensor and a 3- axis accelerometer on the robot and an OpenCV ball tracking program running on a raspberry pi with a camera positioned over the board. The ball tracking uses colour masking and contour detection to identify three balls of different colours which represent left, right, and a reference point. A virtual machine (VM) running ubuntu was set up to run the NN and optimization it is setup as a server for transmission control protocol (TCP) communication. The robot is setup as a client and communicates sensor data to the VM and receives servo angles. The raspberry pi is set up as a client and only transfers position and angle information to the VM. With this setup it would be possible to transfer and run the NN on the robot. Unfortunately, this was not achieved, time was a major factor with solvable issues not being addressed as there was not enough time to address them. The only completion that was possible was a 100,000-episode run of the simulation with optimization done at every 20 episodes. This was promising in showing signs that the model was optimizing. However, locomotive behaviour was not achieved. To achieve the locomotive behaviour a longer episode count may have been needed along with shorter batches before optimization. The test run of the robot, the main issue was the wooden board that the robot moved on made it impossible to identify the red ball. The solution was in painting the board black however time constraints meant this could not be attempted. The other issue is that the NN did not provide effective locomotive behaviour. It is a suggestion that any attempts to complete these tests would require an updating of the Mujoco model for the robot as well as an increase on the episode count and decrease of the episode batch size before optimization. Hopefully this paper provides enough inspiration that this issue will be looked at with closer detail",,Virtual Learning Environment for Emergent Behaviour AI and Transference into Real-World Application,,,,core
213635182,20/12/2018,"La « 5G » est une nouvelle génération des standards de la téléphonie mobile. Cette technologie de télécommunication sans fil promet de révolutionner la manière dont le monde communique. Absolument tout, des véhicules autonomes, aux robots chirurgicaux, en passant par les dispositifs de réalité virtuelle augmentée, les drones, l’« Internet des Objets » (Internet of Things ou IoT) et, plus généralement, toutes les communications mobiles seront couvertes par une nouvelle tranche de bande passante située entre 6 Ghz et 300 Ghz. Une partie de cette tranche sera sous licence, mais une grande partie libre. La 5G est différente des générations précédentes. Pour la première fois, l’infrastructure physique sera séparée de l’infrastructure logique ou « virtuelle ». Un réseau de type « SDN » (Software Dynamic Networks - réseaux définis par logiciel) sera selon, la demande, mis en place, supprimé, agrandi, ou réduit. La gestion complexe du réseau sera effectuée par apprentissage automatique (Machine Learning ou ML) et au moyen de l’Intelligence artificielle (AI). Mais pour que tout cela fonctionne, les principaux opérateurs devront accepter les normes internationales et ouvrir leurs interfaces à des acteurs extérieurs voire même à la concurrence. C’est à ce niveau que se jouera la bataille réglementaire et juridique. A moins d’une attention suffisante accordée dès à présent à ces problèmes émergents, les utilisateurs qui dépendent des télécommunications avancées en Amérique du Nord et dans l’Union européenne se verront empêchés de profiter de la large gamme d’avantages promis par 5G ; en conséquence, l’Innovation risquerait de prendre du retard par rapport à d’autres régions du monde.The “5G” is a new generation of wireless telecommunications technology that promises to revolutionize how the world communicates. Everything from autonomous vehicles, robots conducting delicate surgery, virtual and augmented reality devices, drones, the “Internet of Things” (IoT), and generally all mobile communications will be enabled by a new tranche of bandwidth between 6-Ghz and 300-Ghz, some of it licensed, but much of it not. But 5G is diﬀerent. For the ﬁrst time, the physical infrastructure will be separated from the logical or “virtual” infrastructure. Software Deﬁned Networks (SDN) will be set up and torn down, grown and lessened according to demand.Complex network management will be done by Machine Learning (ML) and Artiﬁcial Intelligence (AI). But for all of this to work properly, major carriers will need to accept international standards and open up their interfaces to outsiders, even to competitors. This is where the regulatory and legal ﬁght will take place. Unless suﬃcient attention is paid now to these emerging issues, users dependent on advanced telecommunications in both North American and the European Union will be prevented from enjoying the full range of beneﬁts promised by 5G, and as a consequence innovation will lag behind other regions of the world",Netcom,"La prochaine génération de téléphonie mobile (5G) et ses implications (Infrastructure, Réglementation)",,,10.4000/netcom.2869,core
293731653,2018-01-01T00:00:00,"Most people's imagination about robots has been shaped by Hollywood movies or novels, resulting in the dream of having robots as assistants or household helpers in our homes. However, there is still a large gap between this dream and the actual capabilities of robots. One underlying reason is that every home is unique and largely unstructured, making it impossible to pre-program a robot for all the challenges it might face in such an environment. For instance, floor plans and furniture differ from home to home. Humans and pets walk around, potentially getting in the robot's way and making the environment non-static. Hence, a pre-programmed robot deployed in such an environment will undoubtedly face problems that it cannot solve with its existing knowledge. In order to cope with this issue, researchers started to equip robots with learning capabilities. Ideally, such capabilities allow a robot to adapt skills to new or changing situations or even to learn completely new tasks. Also humans learn new skills over time and are able to adapt them if needed. Therefore, such learning capabilities seem natural to us. If we are not able to master a specific task, we usually would ask another person to demonstrate it or to give instructions on how to perform it. In robotics research, the field of ""Learning from Demonstration"" tries to mimic this behavior by learning new skills from demonstrations of a task. By applying machine learning techniques, the data perceived from a single or multiple demonstrations are exploited to learn a mapping from perception to the action of a robot.

In this thesis, we concentrate on important Learning from Demonstration aspects that have not gotten so much attention in the research community so far. In particular, we focus on learning methods for robot manipulation tasks. These tasks have two important characteristics. First, they can be naturally decomposed into a set of subtasks and, therefore, can be mastered by performing the individual subtasks in the correct sequential order. Second, they involve physical contact between the robot and objects in its environment. One aim of this thesis is developing methods which allow for learning skills for robot manipulation tasks that generalize well to unknown situations. For instance, a learned skill should also be applicable if positions and orientations of objects differ from those seen in a demonstration.

In the first part of the thesis, we focus on the ""sequential"" aspect of manipulation tasks. Many approaches assume that subtasks are executed in a purely sequential manner or that the human always
demonstrates the same sequence of subtasks. We propose an approach that does not have this assumption. Based on the demonstrations, a graph is generated which connects the subtasks with each other. Each subtask is associated with a movement primitive, a basic elementary movement necessary to perform the subtask. Depending on the environmental conditions, different sequences of movement primitives are executed, allowing the robot to perform tasks which for instance require an arbitrary number of repetitions (e.g., unscrewing a light bulb).

As we concentrate on the sequential aspects of a task in the first part of the thesis, we assume the demonstrations are labeled with the correct movement primitives over time. Additionally, the movement primitives are predefined. In the second part of the thesis, these two assumptions are relaxed. We first present an approach which decomposes the demonstrations into a set of meaningful movement primitives by inferring the underlying sequential structure of the task. The decomposition is based on a probability distribution we call Directional Normal Distribution. By utilizing the distribution, our method infers if a movement should be performed relative to an object in the scene and if a force should be applied in certain directions or not. Forces are especially important when interacting with the environment, for example if the robot has to manipulate objects. By defining movements relative to objects in the scene, the robot is likely to generalize better to new situations, for instance if the object positions differ from the demonstrations.
Our task-decomposition method allows for inferring the most likely movement primitives over time and replaces the process of manually labeling the demonstrations. By combining the method with the sequencing concept presented in the first part of the thesis, complex skills can be learned from scratch without further human supervision. Such a learning scheme is an essential requirement for domestic robots, as not every human teacher might be able or willing to do the tedious labeling of the data.

In both the decomposition and the sequencing part of the thesis, we assume that the teacher performs point-to-point movements and stops between two successive movements. While these assumptions lead to an approach which can learn skills for fairly complex tasks, it also restricts the class of tasks for which the approach can be used. In the third part of the thesis, we therefore introduce the Mixture of Attractors movement primitive representation. Here, a movement is modulated by continuously changing the activations of a set of simple attractors over time. We present a learning algorithm for the representation which learns both the attractors and their activations. An important property of the representation is that the attractors can be defined in different coordinate frames. The continuous activations and the attractors defined in different coordinate frames allow the system to learn movements of arbitrary shape and to generalize them to different object positions. In addition, the transitions between successive movements are smooth.
This property reflects an important behavior of humans who often tend to co-articulate between successive movements. In contrast to many existing approaches, movements are learned by solving a convex optimization problem that does not rely on a good initial estimate of parameters.

In summary, the contribution of this thesis to the state-of-the-art in Learning from Demonstration is two-fold. The first contribution is a framework which is able to learn sequential skills for robot manipulation tasks from a few demonstrations. In contrast to other approaches, our method incorporates object-relative movements and force information directly into the skill learning framework. The second contribution is the Mixture of Attractors movement primitive representation. The representation supports co-articulated movements represented in different coordinate frames and outperforms existing movement primitive representations in terms of accuracy and generalization capabilities. Both contributions are evaluated on a wide range of tasks in simulation and on a real single arm robot with seven degrees of freedom. Altogether, this thesis aims at bringing us closer to the dream of having autonomous robots in our homes",,Learning Sequential Skills for Robot Manipulation Tasks,,,,core
286412460,2018-01-01T00:00:00,"The rapid development and growth of unmanned aerial vehicles (UAVs) as a remote sensing platform, as well as advances in the miniaturization of instrumentation and data systems, have resulted in an increasing uptake of this technology in the environmental and remote sensing science communities. Although tough regulations across the globe may still limit the broader use of UAVs, their use in precision agriculture, ecology, atmospheric research, disaster response biosecurity, ecological and reef monitoring, forestry, fire monitoring, quick response measurements for emergency disaster, Earth science research,volcanic gas sampling, monitoring of gas pipelines, mining plumes, humanitarian observations and biological/chemo-sensing tasks continues to increase. This Special Issue provides a forum for high-quality peer-reviewed papers that broaden the awareness and understanding of UAV developments, applications of UAVs for remote sensing, and associated developments in sensor technology, data processing and communications, and UAV system design and sensing capabilities. This topic encompasses many algorithms and process flows and tools, including: robust vehicle detection in aerial images based on cascaded convolutional neural networks; a stereo dual-channel dynamic programming algorithm for UAV image stitching, as well as seamline determination based on PKGC segmentation for remote sensing image mosaicking; the implementation of an IMU-aided image stacking algorithm in digital cameras; the study of multispectral characteristics at different observation angles, rapid three-dimensional reconstruction for image sequence acquired from UAV cameras; comparisons of Riegl Ricopter UAV Lidar-derived canopy height and DBH with terrestrial Lidar; vision based target finding and inspection of a ground target using a multirotor UAV system; a localization framework for real-time UAV autonomous landing using an on-ground deployed visual approach; curvature continuous and bounded path planning for fixed-wing UAVs; the calculation and identification of the aerodynamic parameters for small-scaled fixed-wing UAVs Several wildfire and agricultural applications of UAVS including: deep learning-based wildfire identification in UAV imagery; postfire vegetation survey campaigns; secure utilization of beacons; and UAVS used in emergency response systems for building fire hazards; observing spring and fall phenology in a deciduous forest with aerial drone imagery; the design and testing of a UAV mapping system for agricultural field surveying; artificial neural network to predict vine water status spatial variability using multispectral information obtained from an unmanned aerial vehicle; automatic hotspot and sun glint detection in UAV multispectral images obtained via uncooled thermal camera calibration and optimization of the photogrammetry process for UAV applications in agriculture; olive yield forecast tool based on the tree canopy geometry using UAS imagery; spatial scale gap filling downscaling method for applications in precision agriculture; automatic co-registration algorithm to remove canopy shaded pixels in UAV-borne thermal images to improve the estimation of crop water stress on vineyards; methodologies for improving plant pest surveillance in vineyards and crops using UAV-based hyperspectral and spatial data; UAV-assisted dynamic clustering of wireless sensors and networks for crop health monitoring. Several applications of UAVS in the fields of environment and conservation including the following: the automatic detection of pre-existing termite mounds through UAS and hyperspectral imagery; aerial mapping of forests affected by pathogens using UAVs; hyperspectral sensors and artificial i ntelligence; coral reef and coral bleaching monitoring; invasive grass and vegetation surveys in remote arid lands. UAVs are also utilized in many other applications: vicarious calibration of SUAS microbolometer temperature imagery for the estimation of radiometric land surface temperature; the documentation of hiking trails in alpine areas; the detection of nuclear sources by UAV teleoperation using a visuo-haptic augmented reality interface; the design of a UAV-embedded microphone array system for sound source localization in outdoor environments; the monitoring of concentrated solar power plants, accuracy analysis of a dam model from drone surveys, mobile sensing and actuation infrastructure, UAV-based frameworks for river hydromorphological characterization; online aerial terrain mapping for ground robot navigation",'MDPI AG',UAV or Drones for Remote Sensing Applications (Volume 1),,,10.3390/books978-3-03897-092-7,core
160771987,2018-07-11T00:00:00,"Model-free reinforcement learning has recently been shown to be effective at
learning navigation policies from complex image input. However, these
algorithms tend to require large amounts of interaction with the environment,
which can be prohibitively costly to obtain on robots in the real world. We
present an approach for efficiently learning goal-directed navigation policies
on a mobile robot, from only a single coverage traversal of recorded data. The
navigation agent learns an effective policy over a diverse action space in a
large heterogeneous environment consisting of more than 2km of travel, through
buildings and outdoor regions that collectively exhibit large variations in
visual appearance, self-similarity, and connectivity. We compare pretrained
visual encoders that enable precomputation of visual embeddings to achieve a
throughput of tens of thousands of transitions per second at training time on a
commodity desktop computer, allowing agents to learn from millions of
trajectories of experience in a matter of hours. We propose multiple forms of
computationally efficient stochastic augmentation to enable the learned policy
to generalise beyond these precomputed embeddings, and demonstrate successful
deployment of the learned policy on the real robot without fine tuning, despite
environmental appearance differences at test time. The dataset and code
required to reproduce these results and apply the technique to other datasets
and robots is made publicly available at rl-navigation.github.io/deployable",,"Learning Deployable Navigation Policies at Kilometer Scale from a Single
  Traversal",http://arxiv.org/abs/1807.05211,,,core
162983474,2018-10-29T00:00:00,"International audienceDespite the recent successes of deep reinforcement learning, teaching complex motor skills to a physical robot remains a hard problem. While learning directly on a real system is usually impractical, doing so in simulation has proven to be fast and safe. Nevertheless, because of the ""reality gap,"" policies trained in simulation often perform poorly when deployed on a real system. In this work, we introduce a method for training a recurrent neural network on the differences between simulated and real robot trajectories and then using this model to augment the simulator. This Neural-Augmented Simulation (NAS) can be used to learn control policies that transfer significantly better to real environments than policies learned on existing simulators. We demonstrate the potential of our approach through a set of experiments on the Mujoco simulator with added backlash and the Poppy Ergo Jr robot. NAS allows us to learn policies that are competitive with ones that would have been learned directly on the real robot",HAL CCSD,Sim-to-Real Transfer with Neural-Augmented Robot Simulation,https://core.ac.uk/download/162983474.pdf,,,core
158377273,2018-05-20T00:00:00,"In many real-world reinforcement learning (RL) problems, besides optimizing
the main objective function, an agent must concurrently avoid violating a
number of constraints. In particular, besides optimizing performance it is
crucial to guarantee the safety of an agent during training as well as
deployment (e.g. a robot should avoid taking actions - exploratory or not -
which irrevocably harm its hardware). To incorporate safety in RL, we derive
algorithms under the framework of constrained Markov decision problems (CMDPs),
an extension of the standard Markov decision problems (MDPs) augmented with
constraints on expected cumulative costs. Our approach hinges on a novel
\emph{Lyapunov} method. We define and present a method for constructing
Lyapunov functions, which provide an effective way to guarantee the global
safety of a behavior policy during training via a set of local, linear
constraints. Leveraging these theoretical underpinnings, we show how to use the
Lyapunov approach to systematically transform dynamic programming (DP) and RL
algorithms into their safe counterparts. To illustrate their effectiveness, we
evaluate these algorithms in several CMDP planning and decision-making tasks on
a safety benchmark domain. Our results show that our proposed method
significantly outperforms existing baselines in balancing constraint
satisfaction and performance",,A Lyapunov-based Approach to Safe Reinforcement Learning,http://arxiv.org/abs/1805.07708,,,core
153716693,2018-04-04T09:08:28Z,"<p>Human–robot collaboration could be advanced by facilitating the intuitive, gaze-based control of robots, and enabling robots to recognize human actions, infer human intent, and plan actions that support human goals. Traditionally, gaze tracking approaches to action recognition have relied upon computer vision-based analyses of two-dimensional egocentric camera videos. The objective of this study was to identify useful features that can be extracted from three-dimensional (3D) gaze behavior and used as inputs to machine learning algorithms for human action recognition. We investigated human gaze behavior and gaze–object interactions in 3D during the performance of a bimanual, instrumental activity of daily living: the preparation of a powdered drink. A marker-based motion capture system and binocular eye tracker were used to reconstruct 3D gaze vectors and their intersection with 3D point clouds of objects being manipulated. Statistical analyses of gaze fixation duration and saccade size suggested that some actions (pouring and stirring) may require more visual attention than other actions (reach, pick up, set down, and move). 3D gaze saliency maps, generated with high spatial resolution for six subtasks, appeared to encode action-relevant information. The “gaze object sequence” was used to capture information about the identity of objects in concert with the temporal sequence in which the objects were visually regarded. Dynamic time warping barycentric averaging was used to create a population-based set of characteristic gaze object sequences that accounted for intra- and inter-subject variability. The gaze object sequence was used to demonstrate the feasibility of a simple action recognition algorithm that utilized a dynamic time warping Euclidean distance metric. Averaged over the six subtasks, the action recognition algorithm yielded an accuracy of 96.4%, precision of 89.5%, and recall of 89.2%. This level of performance suggests that the gaze object sequence is a promising feature for action recognition whose impact could be enhanced through the use of sophisticated machine learning classifiers and algorithmic improvements for real-time implementation. Robots capable of robust, real-time recognition of human actions during manipulation tasks could be used to improve quality of life in the home and quality of work in industrial environments.</p",,Video_1.MP4,,,10.3389/frobt.2018.00025.s001,core
189837365,2018-07-10T00:00:00,"Reinforcement Learning (RL) is an area of machine learning in which an agent interacts with the environment by making sequential decisions. The agent receives reward from the environment based on how good the decisions are and tries to find an optimal decision-making policy that maximises its longterm cumulative reward. This paper presents a novel approach which has showon promise in applying accelerated simulation of RL policy training to automating the control of a real robot arm for specific applications. The approach has two steps. First, design space exploration techniques are developed to enhance performance of an FPGA accelerator for RL policy training based on Trust Region Policy Optimisation (TRPO), which results in a 43% speed improvement over a previous FPGA implementation, while achieving 4.65 times speed up against deep learning libraries running on GPU and 19.29 times speed up against CPU. Second, the trained RL policy is transferred to a real robot arm. Our experiments show that the trained arm can successfully reach to and pick up predefined objects, demonstrating the feasibility of our approach",'Institute of Electrical and Electronics Engineers (IEEE)',Towards Hardware Accelerated Reinforcement Learning for Application-Specific Robotic Control,,"[{'title': None, 'identifiers': ['1063-6862', 'issn:1063-6862']}]",10.1109/ASAP.2018.8445099,core
158138052,2018-06-01T00:00:00,"Recent studies suggest that some children with autism prefer robots as tutors for improving their social interaction and communication abilities which are impaired due to their disorder. Indeed, research has focused on developing a very promising form of intervention named Robot-Assisted Therapy. This area of intervention poses many challenges, including the necessary flexibility and adaptability to real unconstrained therapeutic settings, which are different from the constrained lab settings where most of the technology is typically tested. Among the most common impairments of children with autism and intellectual disability is social attention, which includes difficulties in establishing the correct visual focus of attention. This article presents an investigation on the use of novel deep learning neural network architectures for automatically estimating if the child is focusing their visual attention on the robot during a therapy session, which is an indicator of their engagement. To study the application, the authors gathered data from a clinical experiment in an unconstrained setting, which provided low-resolution videos recorded by the robot camera during the child–robot interaction. Two deep learning approaches are implemented in several variants and compared with a standard algorithm for face detection to verify the feasibility of estimating the status of the child directly from the robot sensors without relying on bulky external settings, which can distress the child with autism. One of the proposed approaches demonstrated a very high accuracy and it can be used for off-line continuous assessment during the therapy or for autonomously adapting the intervention in future robots with better computational capabilities",'MDPI AG',Deep learning systems for estimating visual attention in robot-assisted therapy of children with autism and intellectual disability,https://core.ac.uk/download/158138052.pdf,,10.3390/robotics7020025,core
299941855,2018-01-01T00:00:00,"It is crucial for robots to autonomously steer in complex environments safely without colliding with any obstacles. Compared to conventional methods, deep reinforcement learning-based methods are able to learn from past experiences automatically and enhance the generalization capability to cope with unseen circumstances. Therefore, we propose an end-to-end deep reinforcement learning algorithm in this paper to improve the performance of autonomous steering in complex environments. By embedding a branching noisy dueling architecture, the proposed model is capable of deriving steering commands directly from raw depth images with high efficiency. Specifically, our learning-based approach extracts the feature representation from depth inputs through convolutional neural networks and maps it to both linear and angular velocity commands simultaneously through different streams of the network. Moreover, the training framework is also meticulously designed to improve the learning efficiency and effectiveness. It is worth noting that the developed system is readily transferable from virtual training scenarios to real-world deployment without any fine-tuning by utilizing depth images. The proposed method is evaluated and compared with a series of baseline methods in various virtual environments. Experimental results demonstrate the superiority of the proposed model in terms of average reward, learning efficiency, success rate as well as computational time. Moreover, a variety of real-world experiments are also conducted which reveal the high adaptability of our model to both static and dynamic obstacle-cluttered environments.Published versio",'MDPI AG',Learn to steer through deep reinforcement learning,,"[{'title': 'Sensors', 'identifiers': ['issn:1424-8220', '1424-8220']}]",10.3390/s18113650,core
288839487,2018-09-12T00:00:00,"PreCrash problem of Intelligent Control of autonomous vehicles robot is a very complex problem, especially vehicle pre-crash scenarios and at points of intersections in real-time environments.The goal of this research is to develop a new artificial intelligent adaptive controller for autonomous vehicle Pre-Crash system along with vehicle recognition module and tested in MATLAB including some detailed modules. Following tasks were set: finding Objects in sensor Data (LiDAR. RADAR), Speed and Steering control, vehicle Recognition using convolution neural network and Alexnet.In this research paper, we implemented a real-time image/Lidar processing. At the beginning, we presented a real-time system which is composed of comprehensive modules, these modules are 3d object detection, object clustering and search, ground removal, deep learning using convolutional neural networks. Starting with nearest vehicle  module our target is to find the nearest ahead car and consider it as our primary obstacle.This paper presents an Adaptive cruise pre-crash system and vehicle recognition. The Adaptive cruise pre-crash system module depends on Deep Learning and LiDAR sensor data, which meant to control the driver reckless behavior on the road by adjusting the vehicle speed to maintain a safe distance from objects ahead (such as cars, humans, bicycle or whatever the object) when the driver tries to raise speed. At the very moment the vehicle recognition module, detects and recognizes the vehicles surrounding to the car.Задача предаварийного интеллектуального управления робота автономных транспортных средств является очень сложной проблемой, особенно предаварийные условия транспортных средств и в точках пересечения в условиях реального времени.Целью данного исследования является разработка нового искусственного интеллектуального адаптивного регулятора для системы предаварийной безопасности автономных транспортных средств, а также модуля распознавания транспортных средств и тестирование в MATLAB, включая некоторые детализированные модули. Были поставлены следующие задачи: поиск объектов по данным датчиков (Лидар, Радар), контроль скорости и рулевого управления, распознавание транспортных средств с использованием сверточной нейронной сети и Alexnet.В данной исследовательской работе мы реализовали обработку изображений и лидарных данных в режиме реального времени. Вначале мы представили систему реального времени, которая состоит из комплексных модулей, а именно модули обнаружения трехмерных объектов, группирования и поиска объектов, удаления земли, глубинного обучения с использованием сверточных нейронных сетей. Начиная с модуля ближайшего транспортного средства, наша задача - найти ближайший впереди идущий автомобиль и считать его основным препятствием.В статье представлена адаптивная предаварийная система управления скоростью и распознавания транспортных средств. Модуль адаптивной предаварийной системы управления скоростью зависит от данных глубинного обучения и лидарного датчика, которые предназначены для управления безрассудным поведением водителя на дороге путем регулирования скорости транспортного средства для поддержания безопасного расстояния от объектов впереди (таких как автомобили, люди, велосипед или любой другой объект), когда водитель пытается повысить скорость. В настоящий момент модуль распознавания транспортных средств обнаруживает и распознает транспортные средства вокруг автомобиляЗавдання передаварійного інтелектуального керування робота автономних транспортних засобів є дуже складною проблемою, особливо передаварійні умови транспортних засобів і в точках перетину в умовах реального часу.Метою даного дослідження є розробка нового штучного інтелектуального адаптивного регулятора для системи передаварійної безпеки автономних транспортних засобів, а також модуля розпізнавання транспортних засобів та тестування в MATLAB, включаючи деякі деталізовані модулі. Були поставлені наступні завдання: пошук об'єктів за даними датчиків (Лiдар, Радар), контроль швидкості та рульового управління, розпізнавання транспортних засобів з використанням згорткової нейронної мережі та Alexnet.У даній дослідницькій роботі ми реалізували обробку зображень та лiдарних даних в режимі реального часу. Спочатку ми представили систему реального часу, яка складається з комплексних модулів, а саме модулі виявлення тривимірних об'єктів, групування та пошуку об'єктів, видалення землі, глибинного навчання з використанням згорткових нейронних мереж. Починаючи з модуля найближчого транспортного засобу, наше завдання - знайти найближчий попереду автомобіль і вважати його основною перешкодою.У статті представлена адаптивна передаварiйна система керування швидкістю та розпізнавання транспортних засобів. Модуль адаптивної передаварійної системи керування швидкістю залежить від даних глибинного навчання та лідарного датчика, які призначені для управління безрозсудною поведінкою водія на дорозі шляхом регулювання швидкості транспортного засобу для підтримки безпечної відстані від об'єктів попереду (таких як автомобілі, люди, велосипед або будь-який інший об'єкт), коли водій намагається підвищити швидкість. Наразi модуль розпізнавання транспортних засобів виявляє і розпізнає транспортні засоби навколо автомобіл",'Private Company Technology Center',Гібридний лiдарный/радарний механізм глибинного навчання та розпізнавання транспортних засобів для управління передаварійною безпекою автономних транспортних засобів,,,10.15587/1729-4061.2018.141298,core
151300082,2018-03-06T00:00:00,"Dentro da área da Robótica, Robôs Móveis têm recebido crescente atenção. Robôs móveis se propõem a realizar uma variedade de tarefas mais complexas que seus antecessores, os robôs industriais. Para tal, são necessárias técnicas que lhes permitam interagir de forma efetiva com o ambiente. A parte mais essencial desta interação é o Sistema de Navegação  que é um conjunto de métodos e procedimentos que o robô utiliza para se locomover e encontrar seu caminho no mundo. Infelizmente, as pesquisas até agora têm demonstrado pouco sucesso quando os robôs são submetidos a tarefas do mundo real. Métodos baseados em modelagem matemática são inadequados para os robôs móveis, porque seu ambiente é dinâmico e mutável. Já os métodos que rejeitam inteiramente os modelos do mundo e simplesmente reagem às contingências do ambiente, não conseguem ser escalados para problemas complexos. Apesar destas dificuldades, a natureza parece ter se saído particularmente bem ao dotar animais e seres humanos da capacidade de navegação. Uma abordagem recente é buscar inspiração nela. Esta abordagem é representada pelo estudo dos Mapas Cognitivos - estruturas mentais, encontradas em desde ratos até seres humanos, que permitem registrar fatos e raciocinar a respeito dos espaços. Os Mapas Cognitivos da natureza são implementados em Redes Neurais Naturais - os cérebros. Pesquisadores de computação e engenharia procuram imitá-lo com as Redes Neurais Artificiais. Este trabalho propõe criar um sistema de navegação para robô móvel, inspirado no mecanismo de mapa cognitivo, implementado através de Redes Neurais Artificiais. O objetivo é obter um sistema robusto, capaz de responder as exigências de desempenho presentes em tarefas do mundo real.In the arca of Robotics, Mobile Robots have been receiving increasing attention, mainly because they can perform a larger variety of tasks than their predecessors, the industrial robots. Mobile Robots need techniques that enable them to interact effectively with the environment. The most essential part of this interaction is the Navigation System - the set of methods and procedures they need to walk, and find their way in the world. Unfortunately, research has shown small success when robots are applied to real world tasks. Methods of mathematical modeling are inadequate to mobile robots, because their environment is dynamic and mutable. On the other hand, methods that completely reject world models, and just react to environment c,ontingencies, do not scale well. Despite these difficulties, Nature has been successful in providing animais and human beings with navigational capabilities. A recent approach is to get inspiration from Nature. This approach is represented by the study of Cognitive Maps, which are mental structures found from mice to human beings, that enables the registration of facts and the reasoning about the environment. Nature\'s cognitive maps are implemented in natural neural networks - the brains. Technology tries to imitate them with Artificial Neural Networks. This dissertation proposes to build a mobile robot navigation system, inspired by the cognitive map mechanism, and implemented by Artificial Neural Networks. The aim is to obtain a robust system able to respond to the performance demand of real world tasks","'Universidade de Sao Paulo, Agencia USP de Gestao da Informacao Academica (AGUIA)'",Not available,,,10.11606/D.55.2018.tde-06032018-105039,core
153686368,2018-02-01T00:00:00Z,"Humans have an innate tendency to anthropomorphize surrounding entities and have always been fascinated by the creation of machines endowed with human-inspired capabilities and traits. In the last few decades, this has become a reality with enormous advances in hardware performance, computer graphics, robotics technology, and artificial intelligence. New interdisciplinary research fields have brought forth cognitive robotics aimed at building a new generation of control systems and providing robots with social, empathetic and affective capabilities. This paper presents the design, implementation, and test of a human-inspired cognitive architecture for social robots. State-of-the-art design approaches and methods are thoroughly analyzed and discussed, cases where the developed system has been successfully used are reported. The tests demonstrated the system’s ability to endow a social humanoid robot with human social behaviors and with in-silico robotic emotions",MDPI AG,Designing the Mind of a Social Robot,,"[{'title': None, 'identifiers': ['2076-3417', 'issn:2076-3417']}]",10.3390/app8020302,core
186273805,2018-11-13T00:00:00,"Simulation-to-real transfer is an important strategy for making reinforcement
learning practical with real robots. Successful sim-to-real transfer systems
have difficulty producing policies which generalize across tasks, despite
training for thousands of hours equivalent real robot time. To address this
shortcoming, we present a novel approach to efficiently learning new robotic
skills directly on a real robot, based on model-predictive control (MPC) and an
algorithm for learning task representations. In short, we show how to reuse the
simulation from the pre-training step of sim-to-real methods as a tool for
foresight, allowing the sim-to-real policy adapt to unseen tasks. Rather than
end-to-end learning policies for single tasks and attempting to transfer them,
we first use simulation to simultaneously learn (1) a continuous
parameterization (i.e. a skill embedding or latent) of task-appropriate
primitive skills, and (2) a single policy for these skills which is conditioned
on this representation. We then directly transfer our multi-skill policy to a
real robot, and actuate the robot by choosing sequences of skill latents which
actuate the policy, with each latent corresponding to a pre-learned primitive
skill controller. We complete unseen tasks by choosing new sequences of skill
latents to control the robot using MPC, where our MPC model is composed of the
pre-trained skill policy executed in the simulation environment, run in
parallel with the real robot. We discuss the background and principles of our
method, detail its practical implementation, and evaluate its performance by
using our method to train a real Sawyer Robot to achieve motion tasks such as
drawing and block pushing.Comment: Submitted to ICRA 2019. See https://youtu.be/te4JWe7LPKw for
  supplemental vide",,"Zero-Shot Skill Composition and Simulation-to-Real Transfer by Learning
  Task Representations",http://arxiv.org/abs/1810.02422,,,core
160763033,2018-09-13T00:00:00,"Traffic light and sign detectors on autonomous cars are integral for road
scene perception. The literature is abundant with deep learning networks that
detect either lights or signs, not both, which makes them unsuitable for
real-life deployment due to the limited graphics processing unit (GPU) memory
and power available on embedded systems. The root cause of this issue is that
no public dataset contains both traffic light and sign labels, which leads to
difficulties in developing a joint detection framework. We present a deep
hierarchical architecture in conjunction with a mini-batch proposal selection
mechanism that allows a network to detect both traffic lights and signs from
training on separate traffic light and sign datasets. Our method solves the
overlapping issue where instances from one dataset are not labelled in the
other dataset. We are the first to present a network that performs joint
detection on traffic lights and signs. We measure our network on the
Tsinghua-Tencent 100K benchmark for traffic sign detection and the Bosch Small
Traffic Lights benchmark for traffic light detection and show it outperforms
the existing Bosch Small Traffic light state-of-the-art method. We focus on
autonomous car deployment and show our network is more suitable than others
because of its low memory footprint and real-time image processing time.
Qualitative results can be viewed at https://youtu.be/_YmogPzBXOwComment: Accepted in the IEEE 15th Conference on Computer and Robot Visio",,"A Hierarchical Deep Architecture and Mini-Batch Selection Method For
  Joint Traffic Sign and Light Detection",http://arxiv.org/abs/1806.07987,,,core
201845773,2018-07-01T00:00:00,"Biological intelligence processes information using impulses or spikes, which makes those living creatures able to perceive and act in the real world exceptionally well and outperform state-of-the-art robots in almost every aspect of life. To make up the deficit, emerging hardware technologies and software knowledge in the fields of neuroscience, electronics, and computer science have made it possible to design biologically realistic robots controlled by spiking neural networks (SNNs), inspired by the mechanism of brains. However, a comprehensive review on controlling robots based on SNNs is still missing. In this paper, we survey the developments of the past decade in the field of spiking neural networks for control tasks, with particular focus on the fast emerging robotics-related applications. We first highlight the primary impetuses of SNN-based robotics tasks in terms of speed, energy efficiency, and computation capabilities. We then classify those SNN-based robotic applications according to different learning rules and explicate those learning rules with their corresponding robotic applications. We also briefly present some existing platforms that offer an interaction between SNNs and robotics simulations for exploration and exploitation. Finally, we conclude our survey with a forecast of future challenges and some associated potential research topics in terms of controlling robots based on SNNs",'Frontiers Media SA',A Survey of Robotics Control Based on Learning-Inspired Spiking Neural Networks,https://core.ac.uk/download/201845773.pdf,"[{'title': 'Frontiers in Neurorobotics', 'identifiers': ['issn:1662-5218', '1662-5218']}]",10.3389/fnbot.2018.00035,core
201537294,2018-06-01T00:00:00,"Monitoring the status of the facilities and detecting any faults are considered an important technology in a smart factory. Although the faults of machine can be analyzed in real time using collected data, it requires a large amount of computing resources to handle the massive data. A cloud server can be used to analyze the collected data, but it is more efficient to adopt the edge computing concept that employs edge devices located close to the facilities. Edge devices can improve data processing and analysis speed and reduce network costs. In this paper, an edge device capable of collecting, processing, storing and analyzing data is constructed by using a single-board computer and a sensor. And, a fault detection model for machine is developed based on the long short-term memory (LSTM) recurrent neural networks. The proposed system called LiReD was implemented for an industrial robot manipulator and the LSTM-based fault detection model showed the best performance among six fault detection models",'MDPI AG',LiReD: A Light-Weight Real-Time Fault Detection System for Edge Computing Using LSTM Recurrent Neural Networks,,"[{'title': 'Sensors', 'identifiers': ['issn:1424-8220', '1424-8220']}]",10.3390/s18072110,core
154989865,2018-04-12T00:00:00,"Autonomous sorting is a crucial task in industrial robotics which can be very
challenging depending on the expected amount of automation. Usually, to decide
where to sort an object, the system needs to solve either an instance retrieval
(known object) or a supervised classification (predefined set of classes)
problem. In this paper, we introduce a new decision making module, where the
robotic system chooses how to sort the objects in an unsupervised way. We call
this problem Unsupervised Robotic Sorting (URS) and propose an implementation
on an industrial robotic system, using deep CNN feature extraction and standard
clustering algorithms. We carry out extensive experiments on various standard
datasets to demonstrate the efficiency of the proposed image clustering
pipeline. To evaluate the robustness of our URS implementation, we also
introduce a complex real world dataset containing images of objects under
various background and lighting conditions. This dataset is used to fine tune
the design choices (CNN and clustering algorithm) for URS. Finally, we propose
a method combining our pipeline with ensemble clustering to use multiple images
of each object. This redundancy of information about the objects is shown to
increase the clustering results.Comment: Paper published in International Journal of Artificial Intelligence
  and Applications (IJAIA), March 2018, Volume 9, Number 2 17 pages, 5 figures,
  7 tables. arXiv admin note: text overlap with arXiv:1707.0170",,Unsupervised robotic sorting: Towards autonomous decision making robots,http://arxiv.org/abs/1804.04572,,10.5121/ijaia.2018.9207,core
163029625,2018-01-01T00:00:00,"The papers in this special section focus on the use of artificial intelligence (AI) for long term autonomy. Autonomous systems have a long history in the fields of AI and robotics. However, only through recent advances in technology has it been possible to create autonomous systems capable of operating in long-term, real-world scenarios. Examples include autonomous robots that operate outdoors on land, in air, water, and space; and indoors in offices, care homes, and factories. Designing, developing, and maintaining intelligent autonomous systems that operate in real-world environments over long periods of time, i.e. weeks, months, or years, poses many challenges. This special issue focuses on such challenges and on ways to overcome them using methods from AI. Long-term autonomy can be viewed as both a challenge and an opportunity. The challenge of long-term autonomy requires system designers to ensure that an autonomous system can continue operating successfully according to its real-world application demands in unstructured and semi-structured environments. This means addressing issues related to hardware and software robustness (e.g., gluing in screws and profiling for memory leaks), as well as ensuring that all modules and functions of the system can deal with the variation in the environment and tasks that is expected to occur over its operating time",'Institute of Electrical and Electronics Engineers (IEEE)',Introduction to the Special Issue on AI for Long-Term Autonomy,https://core.ac.uk/download/163029625.pdf,,10.1109/LRA.2018.2870466,core
186287042,2018-11-06T00:00:00,"One of the more prominent trends within Industry 4.0 is the drive to employ
Robotic Process Automation (RPA), especially as one of the elements of the Lean
approach. The full implementation of RPA is riddled with challenges relating
both to the reality of everyday business operations, from SMEs to SSCs and
beyond, and the social effects of the changing job market. To successfully
address these points there is a need to develop a solution that would adjust to
the existing business operations and at the same time lower the negative social
impact of the automation process.
  To achieve these goals we propose a hybrid, human-centered approach to the
development of software robots. This design and implementation method combines
the Living Lab approach with empowerment through participatory design to
kick-start the co-development and co-maintenance of hybrid software robots
which, supported by variety of AI methods and tools, including interactive and
collaborative ML in the cloud, transform menial job posts into higher-skilled
positions, allowing former employees to stay on as robot co-designers and
maintainers, i.e. as co-programmers who supervise the machine learning
processes with the use of tailored high-level RPA Domain Specific Languages
(DSLs) to adjust the functioning of the robots and maintain operational
flexibility",,"Hybrid Approach to Automation, RPA and Machine Learning: a Method for
  the Human-centered Design of Software Robots",http://arxiv.org/abs/1811.02213,,,core
234653434,2018-03-02T00:00:00,"In our days a new type of journalism is been developed : the algorithmic journalism, also known as robot journalism or automated journalism. This kind of journalism is based on an artificial intelligence software (IA) and an advanced natural language generation (Advanced NLG), which automatically generate articles in near real time and in human readable ways. Algorithmic journalism was used in 2016 at the Olympic Games of Rio and at the present time is also used by international news providers. Keywords: algorithmic journalism, robot journalism, automated journalism, sport media, Greek media","'International Institute for Science, Technology and Education'",Potential Applications of Algorithmic (Robot) Journalism for the Greek Sport Media,https://core.ac.uk/download/234653434.pdf,,10.7176/NMMC.vol6727-30,core
211494626,2018-01-01T00:00:00,"Automatic speech recognition is extensively used in human-computer interaction mainly because communication via speech is natural, fast, and safe. Speech applications and voice-enabled, hands-free technologies are increasingly integrated to the modern area of multisensory smart environments (e.g., smart homes, assistive robotics), targeting life wellness and effectiveness at work. The recent advancements of deep learning have brought significant improvements, yielding word error rates below 10% in most cases of recognition with close-talk microphones. However, many challenges remain unsolved, especially in Distant Speech Recognition (DSR) applications where noise and reverberation effects degrade significantly the performance.In this dissertation, we examine a combination of multichannel methods for robust DSR using microphone arrays. More specifically, the proposed methodology includes 1) microphone selection using signal-based criteria of confidence, 2) microphone combination in the decision level following a hypothesis rescoring approach, 3) data augmentation and multi-style training by simulating domestic acoustic scenes with noisy far-field speech for applications in environments with limited amount of training data, 4) multi-microphone adaptation schemes, and 5) extraction of non-linear AM-FM features that are combined with traditional energy-based features. A multichannel demodulation approach is proposed for the extraction of improved frequency modulation features that capture the fine structure of speech formants and constitute beneficial and supplementary to the traditional energy-based cepstral features (e.g, MFCCs). Improvements have been demonstrated mainly in GMM-HMM systems for small and large vocabulary tasks. Yet, they have limited applications in DNN-HMM systems and DSR tasks. Herein, we elaborate on their integration within state-of-the-art front-end schemes that include tansformation of MFCCs resulting in discriminant and speaker-adapted features of large temporal contexts. We explore 1) multichannel demodulation schemes for multi-microphone setups, 2) richer descriptors of frequency modulations, and 3) feature transformation and combination via hierarchical deep bottleneck networks. We present results for tandem and hybrid recognition with GMM and DNN acoustic models, respectively. The improved modulation features are combined efficiently with MFCCs yielding modest and consistent improvements in widely known and challenging corpora for multi-microphone DSR benchmarking.Based on the proposed multichannel methods, a practical system pipeline is designed for always-listening, far-field spoken command recognition in every-day smart indoor environments of multiple rooms equipped with sparsely distributed microphone arrays. The system consists of a traditional cascade of far-field spoken command detection and recognition, the former based on the detection of activating key-phrases. A room-dependent speech activity detection module is used to enable parallel recognition pipelines in each room. The proposed approach is developed for the Greek language, exhibiting promising performance in real and challenging domestic acoustic scenes, reaching 76.6% command recognition accuracy, a result that represents 46% relative improvement over conventional beamforming.An online version of the proposed spoken command recognition system is implemented for four languages (Greek, German, Italian, English) and combined with gesture recognition, aiming at multimodal human-robot interaction. Within the context of assistive robotics, we develop a smart and natural interface that provides multimodal sensory processing capabilities. By fusing the complementary unimodal information streams, we obtain the optimum multimodal hypothesis.  Additionally, we explore new aspects on assistive living via smart social human-robot interaction (HRI). We provide a framework of resources, including datasets and tools that are demonstrated in two real-life use cases for elderly subjects: a multi-modal interface of an assistive robotic rollator and an assistive bathing robot, where following the proposed fusion scheme, the obtained command accuracy reaches 90% in real interaction scenarios with elderly users.Η αυτόματη αναγνώριση ομιλίας διαδραματίζει σημαντικό ρόλο στην αλληλεπίδραση ανθρώπου-υπολογιστή διότι η ομιλία αποτελεί ένα φυσικό, γρήγορο, και ασφαλές μέσο επικοινωνίας, με ευρύ πεδίο εφαρμογών στα σύγχρονα πολυαισθητηριακά ευφυή περιβάλλοντα (πχ. αυτοματοποιημένοι χώροι, ρομποτικά συστήματα) τα οποία σχεδιάζονται για τη βελτίωση της καθημερινότητας και της εργασίας των χρηστών τους. Τα τελευταία χρόνια, οι τεχνικές βαθιάς εκμάθησης με νευρωνικά δίκτυα έχουν επιφέρει σημαντικές βελτιώσεις, μειώνοντας το ποσοστιαίο λάθος αναγνώρισης λέξεων (word error rate) κάτω από 10% στις περισσότερες εφαρμογές με μικρόφωνα κοντά στον ομιλητή. Συνεπώς, τα φωνητικά συστήματα χρησιμοποιούνται όλο περισσότερο στην καθημερινότητα. Ωστόσο οι προκλήσεις παραμένουν αρκετές, ειδικά σε περιβάλλοντα με μικρόφωνα μακριά από τον ομιλητή, όπου ο θόρυβος και οι αντηχήσεις υποβαθμίζουν αισθητά την απόδοση της αναγνώρισης. Στην παρούσα διατριβή εξετάζουμε και συνδυάζουμε μεθόδους εύρωστης αναγνώρισης ομιλίας με πολλαπλούς αισθητήρες. Συγκεκριμένα, η προτεινόμενη μεθοδολογία περιλαμβάνει 1) επιλογή αξιόπιστων μικροφώνων βάσει κριτηρίων ποιότητας των σημάτων, 2) συνδυασμό των αποφάσεων των μικροφώνων με αναδιάταξη των υποθέσεων αναγνώρισης, 3) εκπαίδευση τύπου multi-style με τεχνητή αύξηση των δεδομένων εκπαίδευσης προσομοιώνοντας  ακουστικές σκηνές και  σήματα ομιλίας μακρινού πεδίου (far-field)  για εφαρμογές σε περιβάλλοντα με περιορισμένους πόρους, και 5) εξαγωγή μη-γραμμικών χαρακτηριστικών AM-FM.Προτείνεται μέθοδος πολυκαναλικής αποδιαμόρφωσης των σημάτων φωνής, για την εξαγωγή βελτιωμένων χαρακτηριστικών από τις στιγμιαίες διαμορφώσεις συχνότητας, οι οποίες μοντελοποιούν τις μικροδομές των συντονισμών της φωνής και αποτελούν χρήσιμη και συμπληρωματική πηγή πληροφορίας στα τυπικά χαρακτηριστικά ενέργειας (πχ. MFCC). Ενώ έχουν βρεθεί βελτιώσεις κυρίως σε συστήματα GMM-HMM μεγάλου λεξιλογίου, λίγες είναι οι εφαρμογές τους σε συστήματα DNN-HMM και στην αναγνώριση DSR γενικά. Εδώ, εργαζόμαστε για τη συνέργιά τους σε state-of-the-art σχήματα front-end στα οποία τα MFCCs ομαδοποιούνται σε μεγαλύτερα χρονικά πλαίσια (context) και μετασχηματίζονται έτσι ώστε να γίνουν πιο διακριτά και ανεξάρτητα του ομιλητή. Ερευνούμε 1) πολυκαναλικές μεθόδους αποδιαμόρφωσης, 2) νέους και πλουσιότερους περιγραφητές των συχνοτήτων διαμόρφωσης, και 3) μετασχηματισμούς και συνδυασμό χαρακτηριστικών μέσω ιεραρχικών βαθιών νευρωνικών δικτύων τύπου bottleneck. Παρουσιάζουμε αποτελέσματα στα σχήματα υβριδικής αναγνώρισης και αναγνώρισης tandem με μοντελοποίηση GMM και DNN αντίστοιχα, όπου τα βελτιωμένα χαρακτηριστικά διαμορφώσεων συνδυάζονται αποδοτικά με τα MFCC και οδηγούν σε βελτιωμένη απόδοση σε γνωστές πολυκαναλικές βάσεις αξιολόγησης της αναγνώρισης από απόσταση  (Distant Speech Recognition).Βασιζόμαστε στις προτεινόμενες πολυκαναλικές μεθόδους για να σχεδιάζουμε ένα πρακτικό σύστημα συνεχούς αναγνώρισης φωνητικών εντολών μακρινού πεδίου, σε ευφυή οικιακά περιβάλλοντα με διάσπαρτες συστοιχίες μικροφώνων σε ένα ή περισσότερα δωμάτια. Ακολουθούμε μία τυπική σειρά (cascade) διεργασιών για τον εντοπισμό και την αναγνώριση εντολών, με τον εντοπισμό να γίνεται βάσει φράσεων ενεργοποίησης. Στην αρχή της αλυσίδας επεξεργασίας, τα τμήματα ομιλίας εντοπίζονται και διαχωρίζονται σε κάθε δωμάτιο, επιτρέποντας την αναγνώριση ανά δωμάτιο. Με την προτεινόμενη προσέγγιση, η οποία αναπτύσσεται για τα Ελληνικά, επιτυγχάνεται ικανοποιητική απόδοση σε πραγματικές και δύσκολες οικιακές ακουστικές σκηνές, φτάνοντας έως 76.6% στην ακρίβεια αναγνώρισης εντολών, επιφέροντας σχετική βελτίωση  46% συγκριτικά με ένα τυπικό σύστημα στο οποίο χρησιμοποιείται beamforming για αποθορυβοποίηση.Υλοποιούμε την online εκδοχή του προτεινόμενου συστήματος αναγνώρισης φωνητικών εντολών για τέσσερις γλώσσες (Ελληνικά, Γερμανικά, Ιταλικά, Αγγλικά) και το συνδυάζουμε με την αναγνώριση χειρονομιών στοχεύοντας στην πολυτροπική αλληλεπίδραση ανθρώπου-ρομπότ. Στο πλαίσιο σχεδιασμού ρομποτικών βοηθών, αναπτύσσουμε ένα φυσικό περιβάλλον αλληλεπίδρασης εκμεταλλευόμενοι τις πολλαπλές ροές πληροφορίας από το πολυαισθητηριακό περιβάλλον του ρομπότ. Οι συμπληρωματικές ροές συνδυάζονται σε επίπεδο αποφάσεων για να  εξαχθεί μία πολυτροπική υπόθεση αναγνώρισης. Επιπλέον, μέσω της αλληλεπίδρασης με το ρομπότ, εξετάζουμε νέες πτυχές στο πεδίο της ρομποτικής για υποβοήθηση στην καθημερινότητα (assistive living),  αναπτύσσοντας  ένα σύνολο εργαλείων και δεδομένων για πολυτροπική αναγνώριση, τα οποία εφαρμόζουμε σε δύο πραγματικά παραδείγματα χρήσης (use cases)  για ηλικιωμένους χρήστες: κατά την υποβοήθηση στην κίνηση από ένα ρομποτικό τροχήλατο όχημα και κατά την υποβοήθηση σε εργασίες πλύσης από έναν ρομποτικό βοηθό στο μπάνιο. Ακολουθώντας την προτεινόμενη σύμμειξη των δύο μέσων επιτυγχάνουμε υψηλά ποσοστά πολυτροπικής αναγνώρισης (έως 90%) σε απαιτητικά σενάρια αλληλεπίδρασης με ηλικιωμένους και χρήστες με κινητικά  και ενδεχομένως νοητικά προβλήματα",Εθνικό Μετσόβιο Πολυτεχνείο (ΕΜΠ),Robust multichannel automatic speech recognition for smart environments,,,,core
156873021,2018-01-01T00:00:00,"On the occasion of the recent editions of the WSIS Forum MEDICI organised different workshops to showcase on the one side the richness of applications and services provided

by ICTs in the field of safety, security and disaster recovery and management and to

contribute to provide a reference point for all those working in these sectors and those

who may take advantage from their outcomes. This year we continued this path selecting additional international case study both to approach new sectors and enrich the platform

of skills and competences involved.

Safety and security are integral part of human rights; we must provide all the efforts in

order to guarantee such rights (as stated in art 3, 22, 25 - The Universal Declaration of

Human Rights). In addition, a number of SDGs are tightly connected or rely on safety and security: SDG 2, SDG3, SDG6, SGD6, SDG7, SDG8, SDG9, SDG11, SDG16, SDG17. Some of the specific fields are: food & water security, human security, safety, critical infrastructure resilience, drugs security and more.

Nowadays the demand for ""safety & security"" in all its forms has increased, especially

quantitatively and qualitatively, making clear the need for new approaches to enable the

entire sector to ensure better results.

Looking from a different perspective: we outline the role of ICTs in risks assessment and

management. They are playing key roles in a number of “risky” scenarios from health and

children abuse to homeland security and law enforcement, crimes, trafficking (humans,

drugs, weapons, artefacts, etc.) and even safety on working places and mobility.

Of course, technology it is not enough to solve problems, it is well known and

demonstrated that a holistic, interdisciplinary approach and a culture of ""safety & security"" taking adequately into account human factors are the basis in order to obtain

good results in this area.

We must promote an interdisciplinary approach and a “culture” of safety & security, they

are the basis in order to obtain good results in this area; foster the exchange of

experiences and best practices among countries and promote research thanks to the

WSIS.

On the occasion of previous editions of the WSIS Forum (e.g. 2014, 2015, 2016) some

eminent speakers underlined the key-role played by ICTs on the occasion of natural

disasters and other critical events, they said that cyber technologies have fuelled the

hope of people affected by the natural disaster. The availability of low price high

performance devices and the proactive activity of clever developers have boosted the

production of a number of smart solutions spread in different countries all-over the

world. Due to the actual “silos” segmenting these sectors it is quite difficult to have a

comprehensive vision on these resources and success stories, there is a need for a holistic

approach and best practice sharing.

Internet of things, machine learning, grids, network of sensors, remote sensing as well as

near field communication and, why not, unmanned vehicles glued by networking are

some of the building blocks of safety and security in different fields.

The nine case study presented by the distinguished speakers on the occasion of the ICT

for Safety and Security led to the following outcomes: there is a need to improve the

visibility of ICT applications devoted to safety and security raising the same level of

awareness actually limited to cyber security. The case study presented on the occasion

of the workshop this year and the one already presented in the last editions of the WSIS

have strengthened this need. Achievements in these fields positively impact the human

rights and must be shared among researchers and countries. The WSIS Forum is the key

forum for discussing the role of ICTs as a means of implementation of the Sustainable

Development Goals, if we consider ICTs as powerful means to implement SDGs we must

include and adequately take into account ICTs applied to safety and security in a broad

sense, they are relevant part of SDGs as outlined many times both within the UNGA

Overall WSIS Review and the UNDP 2030 Agenda for Sustainable Development SDGs.

An additional relevant issue emerged on the occasion of the workshop, as sometimes

happens after revolutions, revolutionaries wonder if what they achieved is actually what

they were hoping for. The original idea of computer scientists in the “hippies” counterculture era was aimed to empower citizens and provide them much freedom. Almost fifty years later, after the chimera of the “happy cyber-world”, some of us have started thinking that the foreseen Orwellian “1984” has simply come true ten, fifteen

years later: globalisation, always on devices, position tracking systems, CRMs and users’ profiles, CCTVs and IoT; are those technologies framing citizens? Thoughts for some time have circled around how the speed of the new information revolution renders us less capable develop a critical approach able to foresee the social, ethic, economic impact of such revolution in a long-term perspective. So, in recent times we started facing a wave

of criticism about the evolutionary path of the information and knowledge society, for quite a long time ICT gurus and humanists didn’t interact too much, the true power of cyber technology was largely unexpressed, there were some alerts as Artificial Intelligence, Virtual Reality, Robots often seen from humanists as potential danger for the mankind, but nothing concrete happened. The turning point was probably the exploitation of the Internet and the dissemination of information. Information is built on top of single or aggregation of data, for quite a long-time people use to think that cyberspace is a “black hole” without memory where you pour data without any side effect. Young generations shared on line sensitive information in order to access a videogame or chat with friends and more recently posted images and clips about their private life; does this mean “goodbye privacy?” As a consequence of a lack of “culture” in the use of emerging technologies now we have to deal with serious problems related to information ownership, use, abuse and misuse, not mentioning cybercrimes. An additional drawback is due to the deep technological intrusion affecting our daily life, we feel framed by cyber devices more than supported.

Some evident outcomes of this feeling are the “right to disconnect” - controversial reform of French labour law by the labour minister Myriam El Khomri back in May 2016 and the “right to obsolescence” or the “right to be forgotten” due to Viktor Mayer-Schonberger, the author of “Delete: The Virtue of Forgetting in the Digital Age”. All these to do not mention the cultural, social and economic impacts not always positive especially in a long-term perspective.

Technologies originally conceived by idealists to provide much more freedom and

wellness to humans took then a wrong path framing humans due to all the constraints

placed upon us with new technologies. For instance, as liberating as they are - by

providing flexibility and instant connectivity - we have become enslaved to our devices,

fearful of losing out information and access in an increasingly competitive and fast-paced

world. Consequently, our bodies have suffered, as have our minds (due to information overload), what of our work-life balance -- and this is just to begin with! Ranjit Makkuni’s

paper “Betrayed IT Revolution” and presentation outlines a vision for new design of devices, clutter-free access to web documents to create deeper learning experiences. At the implication level, the project rethinks implications for new design of web mark-up languages that support the creating of ‘privacy’ based secure browsing.

In conclusion we would like to stress the positive effects due to the WSIS process and its outcomes, panellists suggest to establish in the WSIS framework a global observatory on ICTs for safety, security and disaster recovery and to include and promote a wider range of “security” topics under the WSIS umbrella endorsing a holistic approach to the “Safety, Security, Disaster Recovery and Management” sector",International Telecommunication Union ITU,"Thematic Workshop: ICTs for Safety, Security and Disaster Recovery",,,,core
234970564,2018-04-11T00:00:00,"The Reporter is a publication produced by Western Carolina University featuring news, events, and campus community updates for faculty and staff. The publication began in August of 1970 and continues digitally today. Click on the link in the “Related Materials” field to access recent issues.The Reporter
News for the Faculty and Staff of Western Carolina University
June 24, 1998
Food Services to
Switch July 1
See you at Starbucks!
Cullowhee, North Carolina
Western's Learning Communities
Helping Freshmen Find a Niche
Jr Dining in takes on
a new dimension
July 1 at Western
with a changeover4
in food services. ARAMARK
Corp. has been selected to operate
the university's campus dining
services, including those at
Dodson and Brown cafeterias, the
food court areas at Hinds
University Center and Dodson,
concessions at the Ramsey
Regional Activity Center, and
catering services throughout
the campus.
Among the changes expected
under ARAMARKs management
are new menus and formats
providing a wide variety of student
dining options. These include addi­tional
nationally known franchises,
such as Starbucks Coffee and Little
Ceasars Pizza, as well as demon­stration
cooking areas where
customers can watch as their food
is prepared, and expansion of full
menu cafeteria-style service and
""grab-and-go"" items.
Clete Myers, operations
manager of food services at
Clemson University, will become
general manager of campus dining
services at Western. ARAMARK
hopes to retain personnel currently
employed in WClFs food service
when the management change
occurs.
Retention Services Director Susan Clarke Smith (far right) and Peer Mentor
Bryan Dodge (center), meet entering LC students.
""An institution's capacity to retain students is directly related to its ability to reach out and make
contact with students and integrate them into the social and intellectual fabric of institutional life.
It hinges on the establishment of a healthy, caring environment which enables individuals to find a
niche in the social and intellectual communities of the institution."" —VINCENT TINTO IN LEAVING COLLEGE
a ±JL good many of us came of age
when linking the notion of caring
with any institution would have been
greeted with skepticism—if not out­right
scorn. But Western, changing
with, and in many instances ahead of,
the times, will set the standard for
high tech and high touch this fall.
By embracing the idea of learning
communities as part of its freshman
year experience program, Western
makes use of a proven method for
meeting both the individual's need
to belong and the institution's need
to retain.
Randomly selected as participants
in the Learning Community (or LC
Pilot) project beginning this fall, some
170 of Western's estimated 1,200
first-year students will live together
and learn together in classes linked to
their common interests and conduct­ed
by a core group of instructors—all
in an effort to address the problems
presented by the transition from high
school to college. Meanwhile, the
institution will look closely to see if
the initiative addresses its own prob­lems
associated with an unusually
high rate of student attrition,
particularly after the first year.
Frank Prochaska, associate vice
chancellor for academic affairs and
chief architect of the LC Pilot project,
believes that establishing learning
communities at Western, even on a
limited basis initially, will deliver on
both counts. The numbers would
appear to bear him out, with
universities where LCs have become
a standard for first-year programs
reporting significantly higher GPAs
and retention rates among students
who participate.
Elizabeth Shelly coordinates the
Freshman Year Experience Program
through Student Affairs and is herself
a product of an undergraduate LC.
Shelly describes it as setting out in
college life with an ""instant group of
friends"" to go to at any time for
support. She explains that Western's
approach to the idea conforms to a
pretty standard model but incorporates
some new features made possible by
our unique environment.
The pilot group is divided into eight
communities of about twenty students
each, with several of the communities
grouped according to a declared major
or according to undergraduate college.
Three LCs are made up of freshmen
undecided upon majors. Each LC will
be housed in a suite-style layout in
Walker Hall with common areas set
aside for studying and group
activities. In addition to a resident
assistant on the floor, each commu­nity
will be assigned an upperclass
peer mentor who will live with the
group and work with the USI130
class designed for it.
A revamped USI course is key to
the LC Pilot, according to Prochaska.
The USI 130 course for the individual
communities will be co-taught by a
faculty member and a student affairs
professional, both trained in freshman
issues. Themes introduced for
discussion in this course will carry
over to the other General Education
courses linked for the purposes of each
community and taught by specially
selected instructors. For example,
Instructor Nory Prochaska's USI 130
section, and its linked English,
continued on page 2
Learning, continued
computer science, and math courses,
will examine the advantages and
disadvantages of the ""virtual
university"" idea, from the perspective
of LC students who have a particular
interest in technology and a close
comfort with using electronic media.
Susan Clarke Smith, director of
Retention Services, sees the linked
courses as an effective means of
addressing the ""intimidation factor,""
identified by most retention experts
such as Vincent Tinto as one of the
main reasons students leave college.
Smith asserts that by establishing
opportunities for interaction in and
out of the classroom, students and
faculty can begin to ""break down
barriers."" Smith says, ""Students can
see that their professors are human,
and faculty can feel less hesitation, as
part of a wider network of support, to
communicate concern on a more
personal level.""
To some extent, we need look no
farther than our own campus for an
example of a learning community
beginning to deliver on its promise.
Now entering its second year, The
Honors College combines an academic
emphasis with a residential and social
component. Brian Railsback, acting
dean of the college, calculates a 92
percent fall-spring retention rate for
honors freshmen for 1997-98. (The
university's rate for freshmen was
83 percent.) ""That figure,"" Railsback
says, ""obviously points to a good bond
and a real desire for these students to
stay together.""
Perhaps the most ambitious and
the most advantageous aspects of
initiating a learning community
program now come to a confluence
with the computer implementation
project, also spearheaded by
Prochaska. Student Affairs Vice
Chancellor Robert Caruso sees the
efforts as quite extraordinary. ""The
added component of new technologies
in the classroom and the residence
halls will revolutionize our whole
notion of community,"" Caruso says.
""With something on the order of only
forty higher education institutions
out of about 2,800 nationwide that
have a computer requirement, I think
everyone is going to be looking to
Western as a model for creating the
learning community of the twenty-first
century.""
Michael Dougherty
Named Dean of
Education, Allied
Professions
Carolina
Board of
Governors.
Associate
dean of the
College of
Education
and Allied
Professions
since 1996, Dougherty is a professor
in the Department of Human
Services and is a former head of the
department. He earned his bachelor's
degree from the University of Notre
Dame in 1968, master's degrees from
Oakland University in 1970 and
Notre Dame in 1971, and doctoral
degree from Indiana State University
in 1974. He has been a member of the
human services faculty since 1976.
The 1988 recipient of Western's
Paul A. Reid Distinguished Service
Award for Faculty, he also has been
nominated for the Chancellor's
Distinguished Teaching Award and
the Taft Botner Award for Superior
Teaching. Prior to coming to Western,
he was a teacher and counselor in
public schools in Detroit; Mattoon, 111.;
and Taylor County, Fla.
Dougherty is a member of several
professional organizations, including
the American Counseling Association
and the Association for Educational
and Psychological Consultation. His
research activities have focused on
study skills and locus of control, the
effects of counseling techniques on
incarcerates, and consultation styles.
Dougherty's appointment will be
effective July 1, upon the retirement
of Chambers, who served seventeen
years as the college's dean.
Highlighting the summer's cultural
events on campus are this weekend's
concerts by the Atlanta Ballet and the
Cassatt String Quartet.
Hailed as one of the nation's
outstanding young ensembles, the
Cassatt String Quartet will perform in
recital at 8 p.m. this Friday and will
also perform as part of the Atlanta
Ballet programs at 2 p.m. and 8 p.m.
on Saturday. All performances are in
Hoey Auditorium.
The oldest continually operating
ballet company in the United States,
the acclaimed Atlanta Ballet travels to
the mountains each year for a summer
residency. The two performances on
Saturday come in conjunction with
Western's hosting the second annual
Atlanta Ballet Centre for Dance Educa­tion
summer dance camp. Selected
participants in the June 14-July 4
camp, which attracts more than fifty of
the top ballet students in the Southeast,
will share the stage with the
professional dancers for one piece.
The ballet programs on Saturday
include ""Intermezzo,"" featuring three
couples in an intricate series of
dances set to music by Johannes
Brahms; ""Prisma,"" featuring music
by Charles Ives and Atlanta Ballet
executive/music director Robert
Chumbley performed by the Cassatt
String Quartet; and ""II Distrato,"" an
abstract ballet in five movements
demonstrating how the different
parts of the dancer's body work
separately and as a unit.
The Cassatt String Quartet's
Friday program features music for
strings by Beethoven and Ravel.
Admission for the quartet's recital
is $10 for adults, $5 for WCU
students and children. Admission for
the ballet's performances is $20 for
adults and $5 for WCU students and
children. For tickets, call 227-7397.
Architects Tapped for Construction Projects
A pair of major construction projects planned for Western moved closer to reality
recently with the board of trustees' selection of two Charlotte architecture firms
to design an expansion of the university center and a federally funded workforce
development facility.
The trustees named Lee Nichols Hepler Architecture of Charlotte to design
the expansion of the Hinds University Center and Jenkins-Peer Architects of
Charlotte to design the proposed new Western North Carolina laborforce high-technology
education and training center.
The Hinds University Center project is the second in a three-phased
expansion effort designed to enhance the quality of student life at WCU. Plans
call for the construction of approximately 31,000 additional square feet to add a
retail shopping area, a movie theater, increased meeting and office space for
student organizations, and a multicultural center. Preliminary estimates for the
expansion set the cost at about $4.5 million.
The regional high-tech workforce training center, announced last fall by U.S.
Rep. Charles Taylor, is designed to help raise the economic potential of the
region by improving the availability of high-technology education to the
mountains' workforce. The center could include such high-tech training tools as
an industrial laser lab, artificial intelligence lab, geographic information lab,
robotics training, and sound and video production facilities complete with digital
editing capabilities. The facility, pending funding from Congress and the federal
Economic Development Administration, is expected to be built adjacent to the
Belk Building.
Michael Dougherty, associate dean of
the College of Education and Allied
Professions, has been named by
WClFs board of trustees to succeed
Gurney E. Chambers as dean.
Appointment of Dougherty to the
dean's post was approved by the
board Wednesday, June 10, at its
quarterly meeting. The appointment
is subject to the approval of the
University
I of North
WCU Campus
Plays Host to
Weekend of CI
Music and
June 24,1998 • T he Reporter • p age 2
University Awards Top Honors for Teaching, Research, and Service
Western's top faculty and staff
awards for teaching, research, and
service for the 1997-98 academic
year were presented at the annual
spring General Faculty and Awards
Convocation in May.
Mary C. ""Katie"" Ray, assistant
professor of elementary and middle
grades education, won the
Chancellor's Distinguished Teaching
Award. The Paul A. Reid Distin­guished
Service Award for faculty
went to Gordon Mercer, professor
of political science and public
affairs, and the Paul A. Reid
Distinguished Service Award for
administrative staff went to
Stephen White, sports information
director. David J. Butcher,
associate professor of chemistry,
received the University Scholar
Award for distinguished scholarly
achievement.
The honors, presented by
Chancellor John Bardo, carry $1,000
cash awards and engraved plaques
for each recipient. Bardo also
presented the Academic Award of
Excellence to the Department of
English. The award provides
$10,000 for program and staff
development.
Chancellor's Distinguished
Teaching Award
Katie Ray joined WCU's faculty
in 1994 after seven years of teaching
in elementary and middle schools in
New York City. In presenting the
award, Bardo quoted from Ray's
comments to the awards selection
committee.
""For students to be great
teachers, they must be passionate
about living and learning,"" Ray said.
""Consequently, I must show
students by my own passion. The
challenge I face every day is not to
know about good teaching, but to
demonstrate it in every interaction I
have with my students.""
Paul A. Reid
Distinguished Service
Award—Faculty
Gordon Mercer has been
a member of the WCU
faculty since 1980. Among
his accomplishments are the
creation of the annual
Undergraduate Research
| Conference and the
organization of faculty
forum assemblies to foster
communication about
athletic director. He received twenty-six
publication awards from the
College Sports Information Directors of
America, and eight Football Writers
Association of America awards for
""Outstanding Press Box Service.""
Following his retirement on June 30,
integrate computers and new
technology with writing and research
in a way that reflects ""real-world""
practices. The department has become
a primary user of the electronic
classrooms, and during the 1997-98
academic year, every freshman
years of continuous service leave
from usual work commitments to
pursue concentrated scholarly work.
Recipients are chosen on a competi­tive
basis by a faculty committee.
important campus issues.
He has held leadership
positions in WCU's Faculty
Senate and the University of
North Carolina's Faculty Assembly.
Mercer also has been described as ""a
superior teacher"" by his students and
has been nominated frequently for
campus teaching awards.
Paul A. Reid Distinguished Service
Award—Staff
Steve White will retire at the end of
June as WCU's sports information
director. A 1967 graduate of WCU,
White has also served as associate
Distinguished Teacher Katie Ray, University Scholar David Butcher, and Reid Service honoree
Gordon Mercer receive their awards from Chancellor John Bardo.
White will head the Catamount Sports
Network, Inc., the radio broadcaster of
WCU's football and basketball games.
University Scholar Award
David Butcher joined WCU's faculty
in 1990. An analytical chemist, he has
received several external grants and
has published numerous articles on his
scientific research, which includes the
atomic absorption spectroscopy, atomic
fluorescence spectrometry, and the
search for potential
chemical causes for
Sports Information Director Steve White receives the Reid
Service Award for Staff from Chancellor John Bardo.
of Excellence, Bardo
praised the
Department of
English for its
innovative work to
studied composition in the electronic
environment.
Other major awards recognized at
the convocation were the Beyond the
Classroom Teaching Award and the
Scholarly Development Assignment
Program Awards.
The Beyond the Classroom
Teaching Award is given to an
academic teaching unit that excels
in enhancing students' learning
through such activities outside the
classroom as mentoring programs,
effective academic advising or
cooperative learning experience. The
1998 winner of the award, which is
funded by the UNC Board of
Governors, is the Department of
Health Sciences.
Recipients in the Scholarly
Development Assignment
Program are Richard Boyer
(English); Barbara Lovin (head,
Health Sciences); and Dan Pittillo
(Biology). The Scholarly Develop­ment
Assignment Program awards
provide full-time tenured faculty
members who have a minimum of six
the decline of Fraser
fir trees in the
Southern Appala­chian
Mountains.
Academic Award
of Exc ellence
In presenting
the Academic Award
June 24,1998 • The Reporter • page 3
Bruce Henderson Receives UNC System Teaching Award
Bruce B. Henderson, professor
of psychology, was among
sixteen recipients of the fourth
annual Awards for Excellence
in Teaching, presented by the
UNC Board of Governors.
Henderson accepted the award
at a special academic convoca­tion
held at N.C. Central
University in conjunction with
the inauguration of Molly
Corbett Broad as president of the
University of North Carolina.
Winners from
each campus
received a
bronze medallion
and a $7,500
cash prize.
Recipients were
nominated by
special commit­tees
from each of
the sixteen UNC
campuses and selected by the Board
of Governors Committee on Teach­ing
Awards.
Established by the Board of
Governors in 1994 to underscore
the importance of teaching and to
reward good teaching across the
university system, the awards are
given annually to a tenured faculty
member from each UNC campus.
Winners must have taught at their
present institutions at least seven
years, and no one may receive the
award more than once.
Henderson, on WCU's faculty
since 1978 and former head of the
psychology department, received
Western's Botner Superior
Teaching Award in 1988. He co-edited
the book Curiosity and
Exploration, focusing on how
intrinsic rewards affect behavior
in children. Henderson received
his bachelor's and master's degrees
from Bucknell University and
his doctorate from the University
of Minnesota.
WCU Colleges Present Awards
Awards for teaching, service, and scholarship were presented on a college-level
at the end of spring semester. The following is a listing of award
winners by college:
College of Arts and Sciences
• Curtis Wood (History) received
the Creighton Sossomon Professor­ship
for outstanding teacher-scholars
in American, English or European
history. Appointment to the
professorship is for a three-year term.
• Richard Bruce (Biology and
director, Highlands Biological
Station) received the H.F. and
Katherine P. Robinson Professorship.
• Robert Holquist (Music and
director of choral activities) received
the James Dooley Excellence in
Music Teaching Award, which carries
a $500 cash stipend.
• Betty Farmer (Communication
and Theatre Arts) received the Board
of Governors College of Arts and
Sciences Teaching Award, which
carries a $1,000 prize.
• Faculty members in the College of
Arts and Sciences also presented
acting dean J.C. Alexander Jr. with
a ""lifetime achievement award"" in
appreciation for service as acting
dean of the college.
The College of Business
• Roger Lirely (Accounting and
Information Systems) received the
Jay I. Kneedler Professor of
Excellence Award, which includes a
$1,000 cash prize and a plaque.
• Board of Governors Creative and
Innovative Teaching Awards went to
Julie Johnson (Business Adminis­tration,
Law and Marketing); Susan
Kask (Economics); Reagan
McLaurin (Business Administration,
Law and Marketing); and Max
Schreiber (Economics, Finance and
International Business). Each award
carries a $250 stipend.
The College of Education and
Allied Professions
• Carol Burton (director, Teaching
Fellows) received the annual Taft B.
Botner Award for Superior Teaching,
which includes a $750 cash prize and
a plaque.
• Board of Governors Awards for
Superior Teaching and $250 stipends
went to Barbara Bell (Elementary
and Middle Grades Education and
director, Reading Center); Cindy
Cavanaugh (Health and Human
Performance); Richard Haynes
(Administration, Curriculum and
Instruction and director of field
experiences and teacher education);
and Hedy White (Psychology).
The College of Applied Sciences
• The Board of Governors Innovation
in Teaching Award, which carries a
$1,000 stipend, went to Walter
Floreani (Health Sciences).
Trustees Approve Appointments and
Campus Name Changes
WCU's board of trustees approved a number of administrative appoint­ments
for the coming year at its quarterly meeting June 10.
• Terry L. Ballman, assistant professor of Hispanic studies at the
University of Northern Colorado, as associate professor and head of the
Department of Modern Foreign Languages.
• Paul F. Brandt as head of the Department of Chemistry and Physics.
• James A. Lewis as head of the Department of History.
• Carol C. Stephens as director of the Master of Science in Nursing ~
Program.
• Paul Wright as head of the Department of Biology.
• Kathleen S. Wright as head of the Department of Communication
and Theatre Arts.
• Jerry L. Kinard to continue as head of the Department of Manage­ment
through spring 1999.
• John A. Wade III to continue as head of the Department of Econom­ics,
Finance and International Business through spring 2000.
The trustees also approved several administrative and departmental
changes within the College of Business. The department ","Hunter Library Digital Collections, Western Carolina University, Cullowhee, NC 28723;","The Reporter, June 1998",,,,core
322434249,2018-07-04T00:00:00,"Long-term companionship, emotional attachment and realistic interaction with robots have always been the ultimate sign of technological advancement projected by sci-fi literature and entertainment industry. With the advent of artificial intelligence, we have indeed stepped into an era of socially believable robots or humanoids. Affective computing has enabled the deployment of emotional or social robots to a certain level in social settings like informatics, customer services and health care. Nevertheless, social believability of a robot is communicated through its physical embodiment and natural expressiveness. With each passing year, innovations in chemical and mechanical engineering have facilitated life-like embodiments of robotics; however, still much work is required for developing a “social intelligence” in a robot in order to maintain the illusion of dealing with a real human being. This chapter is a collection of research studies on the modeling of complex autonomous systems. It will further shed light on how different social settings require different levels of social intelligence and what are the implications of integrating a socially and emotionally believable machine in a society driven by behaviors and actions",'IntechOpen',Socially Believable Robots,https://core.ac.uk/download/322434249.pdf,,10.5772/intechopen.71375,core
160762897,2018-10-07T00:00:00,"We have seen much recent progress in rigid object manipulation, but
interaction with deformable objects has notably lagged behind. Due to the large
configuration space of deformable objects, solutions using traditional
modelling approaches require significant engineering work. Perhaps then,
bypassing the need for explicit modelling and instead learning the control in
an end-to-end manner serves as a better approach? Despite the growing interest
in the use of end-to-end robot learning approaches, only a small amount of work
has focused on their applicability to deformable object manipulation. Moreover,
due to the large amount of data needed to learn these end-to-end solutions, an
emerging trend is to learn control policies in simulation and then transfer
them over to the real world. To-date, no work has explored whether it is
possible to learn and transfer deformable object policies. We believe that if
sim-to-real methods are to be employed further, then it should be possible to
learn to interact with a wide variety of objects, and not only rigid objects.
In this work, we use a combination of state-of-the-art deep reinforcement
learning algorithms to solve the problem of manipulating deformable objects
(specifically cloth). We evaluate our approach on three tasks --- folding a
towel up to a mark, folding a face towel diagonally, and draping a piece of
cloth over a hanger. Our agents are fully trained in simulation with domain
randomisation, and then successfully deployed in the real world without having
seen any real deformable objects.Comment: Published at the Conference on Robot Learning (CoRL) 201",,Sim-to-Real Reinforcement Learning for Deformable Object Manipulation,http://arxiv.org/abs/1806.07851,,,core
224424713,2018-01-01T00:00:00,"Humans have a remarkable ability to learn new concepts from only a few examples and quickly adapt to unforeseen circumstances. To do so, they build upon their prior experience and prepare for the ability to adapt, allowing the combination of previous observations with small amounts of new evidence for fast learning. In most machine learning systems, however, there are distinct train and test phases: training consists of updating the model using data, and at test time, the model is deployed as a rigid decision-making engine. In this thesis, we discuss gradient-based algorithms for learning to learn, or meta-learning, which aim to endow machines with flexibility akin to that of humans. Instead of deploying a fixed, non-adaptable system, these meta-learning techniques explicitly train for the ability to quickly adapt so that, at test time, they can learn quickly when faced with new scenarios.To study the problem of learning to learn, we first develop a clear and formal definition of the meta-learning problem, its terminology, and desirable properties of meta-learning algorithms. Building upon these foundations, we present a class of model-agnostic meta-learning methods that embed gradient-based optimization into the learner. Unlike prior approaches to learning to learn, this class of methods focus on acquiring a transferable representation rather than a good learning rule. As a result, these methods inherit a number of desirable properties from using a fixed optimization as the learning rule, while still maintaining full expressivity, since the learned representations can control the update rule.We show how these methods can be extended for applications in motor control by combining elements of meta-learning with techniques for deep model-based reinforcement learning, imitation learning, and inverse reinforcement learning. By doing so, we build simulated agents that can adapt in dynamic environments, enable real robots to learn to manipulate new objects by watching a video of a human, and allow humans to convey goals to robots with only a few images. Finally, we conclude by discussing open questions and future directions in meta-learning, aiming to identify the key shortcomings and limiting assumptions of our existing approaches","eScholarship, University of California",Learning to Learn with Gradients,,,,core
201265468,2018-12-01T00:00:00,"The “5G” is a new generation of wireless telecommunications technology that promises to revolutionize how the world communicates. Everything from autonomous vehicles, robots conducting delicate surgery, virtual and augmented reality devices, drones, the “Internet of Things” (IoT), and generally all mobile communications will be enabled by a new tranche of bandwidth between 6-Ghz and 300-Ghz, some of it licensed, but much of it not. But 5G is diﬀerent. For the ﬁrst time, the physical infrastructure will be separated from the logical or “virtual” infrastructure. Software Deﬁned Networks (SDN) will be set up and torn down, grown and lessened according to demand.Complex network management will be done by Machine Learning (ML) and Artiﬁcial Intelligence (AI). But for all of this to work properly, major carriers will need to accept international standards and open up their interfaces to outsiders, even to competitors. This is where the regulatory and legal ﬁght will take place. Unless suﬃcient attention is paid now to these emerging issues, users dependent on advanced telecommunications in both North American and the European Union will be prevented from enjoying the full range of beneﬁts promised by 5G, and as a consequence innovation will lag behind other regions of the world",'OpenEdition',"La prochaine génération de téléphonie mobile (5G) et ses implications (Infrastructure, Réglementation)",,"[{'title': 'Netcom', 'identifiers': ['0987-6014', 'issn:0987-6014']}]",10.4000/netcom.2869,core
212471168,2018-01-01T00:00:00,"Autonomous cooperative driving systems require the integration of research activities in the field of embedded systems, robotics, communication, control and artificial intelligence in order to create a secure and intelligent autonomous drivers behaviour patterns in the traffic. Beside autonomous vehicle management, an important research focus is on the cooperation behaviour management. In this paper, we propose hybrid automaton modelling to emulate flexible vehicle Platoon and vehicles cooperation interactions. We introduce novel coding function for Platoon cooperation behaviour profile generation in time, which depends of vehicles number in Platoon and behaviour types. As the behaviour prediction of transportation systems, one of the primarily used methods of artificial intelligence in Intelligent Transport Systems, we propose an approach towards NARX neural network prediction of Platoon cooperation behaviour profile. With incorporation of Platoon manoeuvres dynamic prediction, which is capable of analysing traffic behaviour, this approach would be useful for secure implementation of real autonomous vehicles cooperation",'Mechanical Engineering Faculty in Slavonski Brod',Hybrid Automaton Based Vehicle Platoon Modelling and Cooperation Behaviour Profile Prediction,https://core.ac.uk/download/212471168.pdf,,10.17559/TV-20170308230100,core
219380447,2018-06-01T07:00:00,"Artificial neural networks (ANNs) are highly-capable alternatives to traditional problem solving schemes due to their ability to solve non-linear systems with a nonalgorithmic approach. The applications of ANNs range from process control to pattern recognition and, with increasing importance, robotics. This paper demonstrates continuous control of a robot using the deep deterministic policy gradients (DDPG) algorithm, an actor-critic reinforcement learning strategy, originally conceived by Google DeepMind. After training, the robot performs controlled locomotion within an enclosed area. The paper also details the robot design process and explores the challenges of implementation in a real-time system",DigitalCommons@CalPoly,Artificial Neural Network-Based Robotic Control,https://core.ac.uk/download/219380447.pdf,,,core
390020542,2018-02-27T00:00:00,"The economic-legal aspects of the state and trends of the Internet-based technologies (IP) technology, the place of intellectual property in it are considered. It is shown that the Internet of Things creates conditions for the emergence of a synergetic effect from the combination of possibilities of artificial intelligence, cloud computing, set of sensors, mathematical algorithms for processing large data (Big Data), robotic devices of various purposes, data transmission systems (Internet), which allows to provide various services and perform various work with or without the participation of people. The role of the state in promoting the development of IP, the existing problems and ways of their solution are shown. Many governments in recent years are taking measures to analyze the state of affairs with the introduction of IP technologies, the localization of problems and threats that may or may occur in the future in order to formulate a common strategy for the development of industry for the production of IP technologies and their application in various sectors of the economy and public life. The patent landscape of the IP is analyzed, the most productive companies and inventors of IP are discovered, the dynamics of patenting in the IP environment, the value of patents, patent research problems are shown. The problems of intellectual property protection in the sphere of IP, in particular, copyright, inventions, trademarks, commercial secrets, information security are considered. The intellectual potential and untapped potential of Ukraine in the development of IP technologies are considered. It is concluded that in the widespread use of IP technologies, there is a significant potential for increasing the efficiency of any type of human activity. It concerns the real economy, industry and agriculture, health care, public administration, education, financial turnover, etc. The development of IP technologies is the most powerful stimulating factor in the innovative development of nanotechnologies, microelectronics, semiconductor technologies, microiminating of executive devices, telecommunications, radio technologies, software computing, robotics, and more.Рассмотрены экономико-правовые аспекты состояния и тенденций развития технологий Интернета вещей (ИВ), места в нем интеллектуальной собственности. Показано, что Интернет вещей создает условия для появления синергетического эффекта от сочетания возможностей искусственного интеллекта, облачных вычислений, множества сенсоров, математических алгоритмов обработки больших данных (Big Data), роботизированных устройств различного назначения, систем передачи данных (сети Интернет), что позволяет предоставлять разнообразные услуги и осуществлять различные работы с участием или без участия людей. Показана роль государства в содействии развитию ИВ, существующие проблемы и пути их решения. Правительства многих стран в последнее время принимают меры по анализу состояния дел с внедрением ИВ-технологий, локализации проблем и угроз, имеющих место или могущих возникнуть в будущем, с целью формирования общей стратегии развития промышленности производства технологий ИВ и их применения в различных секторах экономики и общественной жизни. Проанализированы патентный ландшафт ИВ, выявлены наиболее продуктивные компании и изобретатели ИВ, показана динамика патентования в среде ИВ, ценность патентов, проблемы патентного поиска. Рассмотрены проблемы охраны интеллектуальной собственности в сфере ИВ, в частности, авторских прав, изобретений, торговых марок, коммерческой тайны, информационной безопасности. Рассмотрены интеллектуальный потенциал и неиспользованные возможности Украины в развитии технологий ИВ. Делается вывод, что в широком применении технологий ИВ заложен значительный потенциал повышения эффективности любого вида человеческой деятельности. Это касается сферы реальной экономики, промышленности и сельского хозяйства, системы здравоохранения, государственного управления, образования, финансового оборота и т. п. Развитие технологий ИВ является мощным стимулирующим фактором инновационного развития нанотехнологий, микроэлектроники, полупроводниковых технологий, микроминиатюризации исполнительных устройств, телекоммуникаций, радиотехнологий, программных вычислительных средств, робототехники и многого другого.Розглянуто економіко-правові аспекти стану та тенденцій розвитку технологій Інтернету речей (ІР), місця в ньому інтелектуальної власності. Показано роль дер- жави у сприянні розвитку ІР, проблеми та шляхи їх вирішення. Проаналізовано патентний ландшафт ІР, виявлені найбільш продуктивні компанії та винахідники ІР, показано динаміку патентування в середовищі ІР, цінність патентів, проблеми патентного пошуку. Визначено проблеми охорони інтелектуальної власності у сфері ІР, зокрема, авторських прав, винаходів, торгових марок, комерційної таємниці, інформаційної безпеки. Розглянуто інтелектуальний потенціал і невикористані мож- ливості України в розвитку технологій ІР. Обґрунтовано висновок, що в широкому застосуванні технологій ІР закладено значний потенціал підвищення ефективності економіки",Науково-дослідний інститут інтелектуальної власності НAПрН України,ІНТЕЛЕКТУАЛЬНА ВЛАСНІСТЬ В СИСТЕМІ ІНТЕРНЕТУ РЕЧЕЙ: ЕКОНОМІКО-ПРАВОВИЙ АСПЕКТ,,,,core
296894670,2018-09-03T22:34:18,"Orientadores: João Mauricio Rosario, Oscar Fernando AvilésTese (doutorado) - Universidade Estadual de Campinas, Faculdade de Engenharia MecânicaResumo: Esta tese contribui com a parametrização e caracterização dos sinais de eletromiografia de superfície usando a aprendizagem em profundidade (Deep Learning) como técnica avançada no reconhecimento de padrões para reproduzir os movimentos de preensão de uma mão robótica em ambientes industriais através da interação homem-máquina. A análise para reproduzir os movimentos da mão é realizada a partir da interação dos sinais de eletromiografia das ações, que geram uma resposta cognitiva com o objetivo de replica-lo para que o dispositivo robótico possa realizar o movimento de preensão de acordo com o movimento realizado pelo usuário. Nesta tese, parte-se da bancada experimental MUC-1 previamente desenvolvida no Laboratório de Automação Integrada e Robótica (LAIR) da Universidade Estadual de Campinas e acrescentam-se funções que aumentam o escopo e melhoram a exequibilidade dos testes. A técnica de obtenção dos valores experimentais dos dados é baseada na adaptação do sensor MYO armband® por meio dos oito bio-sensores de eletromiografia relacionando a cinemática e dinâmica da mão pela identificação dos músculos do braço correspondente aos métodos de preensão, os quais são aprimorados por médio do método baseado em redes neuronais convolucionais da aprendizagem em profundidade previamente investigado na literatura para o reconhecimento de padrões. Para validação do sistema proposto, foi construído três arquiteturas de redes convolucionais, viabilizando a execução do teste virtual por meio da mão implementada no Simmechanics de Matlab® e no modelo real MUC-1. Por fim, o procedimento experimental resultante é documentado e as etapas prévias de modelagem e filtragem são descritas de acordo com as condições de preensão de objetos de figuras geométricas preestabelecidas que são executadas no dispositivo robótico de forma naturalAbstract: This thesis contributes to the parametrization and characterization of surface electromyography signals using deep learning as an advanced technique in pattern recognition to reproduce the grip movements of a robotic hand in industrial environments through the man-machine interaction. The analysis to reproduce the movements of the hand is made from the interaction of the electromyography signals of the actions, which generate a cognitive response to replicate it so that the robotic device can perform the grip movement, in accordance with the movement made by the user. Some part of this thesis is experimental bench MUC-1 previously developed in the Laboratory of Automation and Robotics (LAIR) at the State University of Campinas and added functions that increase the scope and improve the feasibility of testing. The technique of obtaining the experimental values of the data is based on the adaptation of the MYO armband® sensor through the eight bio-sensors of electromyography relating the kinematics and dynamics of the hand by the identification of the muscles of the arm corresponding to the grasping methods, which are improved by the method based on convolutional neuronal networks of in-depth learning previously research in the literature for the recognition of patterns. For the validation of the proposed system, three convolutional network architectures were built, enabling the virtual test execution through the hand implemented in the Matlab® Simmechanics and in the real MUC-1 model. Finally, the resulting experimental process is documented and the previous stages of modeling and filtering are described according to the prehension conditions of objects of geometric figures that are executed in the robotic device in a natural wayDoutoradoMecatrônicaDoutor em Engenharia Mecânic",[s.n.],"Movement Identification from the intention of grasping, based on deep learning, with signals EMGs for use as HMI in robotic devices",https://core.ac.uk/download/296894670.pdf,,,core
323162494,2018,"This paper proposes a method for modeling and planning the grasping configuration of a robotic hand with underactuated finger mechanisms. The proposed modeling algorithm is based on analysis and mimicking of human grasping experience. Results of the analysis is preprocessed and stored in a database. The grasp configuration planning algorithm can be used within a real time online grasp control as based on artificial neural networks. Namely, shapes and sizes of task objects are described by taxonomy data, which are used to generate grasp configurations. Then, a robot hand grasp control system is designed as based on the proposed grasp planning with close-loop position and force feedback. Simulations and experiments are carried out to show the basic features of the proposed formulation for identifying the grasp configurations while dealing with target objects of different shapes and sizes. It is hoped that the well-trained underactuated robot hand can solve most of grasping tasks in our life. The research approach is aimed to research low-cost easy-operation solution for feasible and practical implementation",,Grasp configuration planning for a low-cost and easy-operation underactuated three-fingered robot hand,,,10.1016/j.mechmachtheory.2018.06.019,core
150127614,2016-01-01T00:00:00,"This paper is concerned with the coordination control of multiple biomimetic robotic fish in highly dynamic aquatic environments by building a hybrid centralized system. With the aid of the results of biorobotics and control techniques, a radio-controlled multijoint robotic fish and its locomotion control are developed. To enable a closed control loop, a visual subsystem that is responsible for tracking of multiple moving objects is constructed and implemented in real time. Furthermore, a behavior-based hierarchical architecture in conjunction with fuzzy reinforcement learning is proposed to accomplish effective coordination among multiple swimming robots. Finally, experiments on 2vs2 water polo game are carried out to verify the proposed coordination control scheme. Over the past eight years, this multirobot platform has been successfully applied to international underwater robot competitions to promote innovative research and education in underwater robotics",Coordination of Multiple Robotic Fish With Applications to Underwater Robot Competition,,,,,core
299962837,2016-01-01T00:00:00,"Cracking in plane turbine blades is the significant defect for the airplanes. Cracking usually occurs because of high pressure and temperature during manufacturing processes. In this project, automatic crack inspection system will be developed and implemented on real-time system. Inspection system is implemented on Jetson Tk1 embedded hardware and Robot Operating System (ROS). Many researched methods will be compared and inspection algorithm is developed based on the comparison results. Inspection algorithm includes a sequence of image processing methods and machine learning classifier to correctly output the defect location. For the final step, defects location coordinates will then return to ABB Industrial Robot for further executions and corrections. Accuracy rate of 89% was achieved at the final classification stage of the system with the average processing time less than 1 second.Bachelor of Engineering (Computer Science",Evaluation of low-power vision platform for robotic industrial application,,,,,core
212998283,2016-04-25T07:00:00,"Off-the-shelf Reinforcement Learning (RL) algorithms suffer from slow learning performance, partly because they are expected to learn a task from scratch merely through an agent\u27s own experience. In this thesis, we show that learning from scratch is a limiting factor for the learning performance, and that when prior knowledge is available RL agents can learn a task faster. We evaluate relevant previous work and our own algorithms in various experiments.  Our first contribution is the first implementation and evaluation of an existing interactive RL algorithm in a real-world domain with a humanoid robot. Interactive RL was evaluated in a simulated domain which motivated us for evaluating its practicality on a robot. Our evaluation shows that guidance reduces learning time, and that its positive effects increase with state space size.  A natural follow up question after our first evaluation was, how do some other previous works compare to interactive RL. Our second contribution is an analysis of a user study, where na ive human teachers demonstrated a real-world object catching with a humanoid robot. We present the first comparison of several previous works in a common real-world domain with a user study.  One conclusion of the user study was the high potential of RL despite poor usability due to slow learning rate. As an effort to improve the learning efficiency of RL learners, our third contribution is a novel human-agent knowledge transfer algorithm. Using demonstrations from three teachers with varying expertise in a simulated domain, we show that regardless of the skill level, human demonstrations can improve the asymptotic performance of an RL agent.  As an alternative approach for encoding human knowledge in RL, we investigated the use of reward shaping. Our final contributions are Static Inverse Reinforcement Learning Shaping and Dynamic Inverse Reinforcement Learning Shaping algorithms that use human demonstrations for recovering a shaping reward function. Our experiments in simulated domains show that our approach outperforms the state-of-the-art in cumulative reward, learning rate and asymptotic performance.  Overall we show that human demonstrators with varying skills can help RL agents to learn tasks more efficiently",Reinforcement Learning from Demonstration,https://core.ac.uk/download/212998283.pdf,Digital WPI,,,core
103772894,16/01/2016,"We are currently investigating models of the cerebellum as adaptive controllers[1] for soft, light-weight robots[2]. Literature has a number of cerebellar models of different detail and complexity. Only few of them are suitable to be implemented as an actual controller, however, mostly because their computational complexity prohibits their real-time execution. We are using SpiNNaker[3], a neuromorphic computing system tailor-made for the simulation of spiking neural networks, to lift this computational limit or at least increase it by orders of magnitude. Thus, many models which previously were slow to compute on traditional PCs can now be executed in real-time. We would like to develop a set of benchmarks to assess the performance of such models and compare them to more traditional adaptive controllers. The master candidate would need to provide a thourough understanding of control theory and adaptive control. The main tasks to tackle in this thesis are: 1. Get a functional understanding of the cerebellum, cerebellar networks and CMAC[4] controllers. 2. Get a detailed understanding of the biological and simulated cerebellum’s input and output mecha-nisms. 3. In comparison to more traditional controllers, single out relevant performance metrics",Benchmarking Cerebellar Models and Controllers Problem description:,,,,,core
142496142,2016-11-21T14:28:42Z,"O presente trabalho aborda e analisa o uso da Realidade Virtual (RV) em Sistemas Tutores Inteligentes (STI), buscando evidenciar a cooperação entre as duas áreas. Neste sentido, as teorias de Inteligência Artificial (IA) contribuíram para a estruturação de ambientes interativos de ensino com as melhores práticas de STI e RV, entre elas: facilidade de acesso, individualização do ensino, conteúdo lapidado, modelado e selecionado, um ambiente imersivo, interativo, intuitivo, destacando a integração do robô Phantom ao ambiente 3D e o seu relacionamento com a ontologia do projeto. Um estudo de caso foi realizado na área de anatomia ósseocraniana para cursos de graduação na área da saúde. O processo de validação do protótipo e da arquitetura proposta teve apoio de professores e um grupo de 120 estudantes da área da saúde da UCB e da UnB. Como os cursos eram de características distintas, dividiu-se a validação em dois grupos com propósitos qualificados. O grupo A ficou responsável pela validação da arquitetura/tecnologia formado pelos estudantes de medicina do 1° semestre da disciplina de anatomia de ambas as instituições. O grupo B foi utilizado para a validação do conceito/conteúdo e formado por estudantes de enfermagem e biomedicina também do 1° semestre da disciplina de anatomia, sendo que esses estavam sob a responsabilidade do mesmo professor e tiveram a mesma forma de avaliação. Os estudantes de biomedicina tiveram acesso ao sistema, enquanto que os de enfermagem tiveram apenas o ensino convencional, sem acesso ao ASM. O objetivo foi verificar se a utilização do ASM como foi implementado contribuiu para o ensino da anatomia humana. O processo de aprendizado foi alocado no trabalho como uma possibilidade de expansão da arquitetura e evolução desse trabalho, acrescentando uma rede neural artificial entre a interação do estudante e o tipo de conteúdo a ser solicitado da ontologia. Portanto, através de um questionário on-line realizado com o apoio de um especialista da área de saúde, o ASM teve resultados satisfatórios. _________________________________________________________________________________ ABSTRACTThe present study describes an analysis of the use of Virtual Reality (VR) in Intelligent Tutorial Systems (ITS), aiming to show the cooperation between the two areas. Consequently, Artificial Intelligence (AI) theories contributed to the structuring of interactive teaching environments with the best ITS and VR practices, among these: easy access, individualized teaching, refined, modeled and selected content, an intuitive, immersive, interactive environment, particularly the integration of the Phantom robot in a 3D environment and its relationship to the ontology of the project. A case study was completed for undergraduates majoring in health, in the area of cranial-skeletal anatomy. The prototype and proposed architecture validation process was supported by professors and a group of 120 students from UCB and UnB in the area of health. As the majors had distinct characteristics, the validation was divided into two groups with qualified proposals. Group A was responsible for the validation of the architecture/technology and consisted of the medical students in their 1st semester of anatomy from both of the institutions. Group B was used for the validation of concept/content and consisted of students in nursing, and biomedicine, also in their 1st semester of anatomy, under the supervision of the same professor, and undergoing the same evaluation process. The students in biomedicine had access to the system, while the nursing students had conventional teaching methods, without access to the Medical Simulation Platform (MSP). The objective was to verify if the use of the MSP, as it was implemented, contributed to the teaching of human anatomy. The learning process was allocated in the work as a possibility of expansion of the architecture and evolution of this work, adding an artificial neural network between the interaction of the student and the type of content to be solicited from the ontology. Through an on-line questionnaire, administered with the help of a health expert, the MSP presented satisfactory results",Integration of the phantom interface on intelligent tutorial system for the medical simulation environment,,Research on Biomedical Engineering,,,core
96884783,2016-09-16T00:00:00,"A key issue in designing robotics systems is the cost of an integrated camera sensor that meets the bandwidth/processing requirement for many advanced robotics applications, especially lightweight robotics applications, such as visual surveillance or SLAM in autonomous aerial vehicles. There is currently much work going on to adapt smartphones to provide complete robot vision systems, as the smartphone is so exquisitely integrated by having camera(s), inertial sensing, sound I/O and excellent wireless connectivity. Mass market production makes this a very low-cost platform and manufacturers from quadrotor drone suppliers to children’s toys, such as the Meccanoid robot [5], employ a smartphone to provide a vision system/control system [7,8].

 

Accordingly, many research groups are attempting to optimise image analysis, computer vision and machine learning libraries for the smartphone platform. However current approaches to robot vision remain highly demanding for mobile processors such as the ARM, and while a number of algorithms have been developed, these are very stripped down, i.e. highly compromised in function or performance. For example, the semi-dense visual odometry implementation of [1] operates on images of only 320x240pixels.

                

In our research we have been developing biologically motivated foveated vision algorithms based on a model of the mammalian retina [2], potentially 100 times more efficient than their conventional counterparts. Accordingly, vision systems based on the foveated architectures found in mammals have also the potential to reduce bandwidth and processing requirements by about x100 - it has been estimated that our brains would weigh ~60Kg if we were to process all our visual input at uniform high resolution. We have reported a foveated visual architecture [2,3,4] that implements a functional model of the retina-visual cortex to produce feature vectors that can be matched/classified using conventional methods, or indeed could be adapted to employ Deep Convolutional Neural Nets for the classification/interpretation stage. Given the above processing/bandwidth limitations, a viable way forward would be to perform off-line learning and implement the forward recognition path on the mobile platform, returning simple object labels, or sparse hierarchical feature symbols, and gaze control commands to the host robot vision system and controller. 

 

We are now at the early stages of investigating how best to port our foveated architecture onto an ARM-based smartphone platform. To achieve the required levels of performance we propose to port and optimise our retina model to the mobile ARM processor architecture in conjunction with their integrated GPUs. We will then be in the position to provide a foveated smart vision system on a smartphone with the advantage of processing speed gains and bandwidth optimisations. Our approach will be to develop efficient parallelising compilers and perhaps propose new processor architectural features to support this approach to computer vision, e.g. efficient processing of hexagonally sampled foveated images.

 

Our current goal is to have a foveated system running in real-time on at least a 1080p input video stream to serve as a front-end robot sensor for tasks such as general purpose object recognition and reliable dense SLAM using a commercial off-the-shelf smartphone. Initially this system would communicate a symbol stream to conventional hardware performing back-end visual classification/interpretation, although simple object detection and recognition tasks should be possible on-board the device. We propose that, as in Nature, foveated vision is the key to achieving the necessary data reduction to be able to implement complete visual recognition and learning processes on the smartphone itself",A Biologically Motivated Software Retina for Robotic Sensors for ARM-Based Mobile Platform Technology,https://core.ac.uk/download/96884783.pdf,,,,core
195551950,2016-12-05T00:00:00,"International audienceOur demonstration presents an open-source hardware and software platform which allows non-roboticistsresearchers to conduct machine learning experiments to benchmark algorithms for autonomous explorationand active learning. In particular, in addition to showing the general properties of the platform such asits modularity and usability, we will demonstrate the online functioning of a particular algorithm whichallows efficient learning of multiple forward and inverse models and can leverage information from humanguidance. A first aspect of our demonstration is to illustrate the ease of use of the 3D printed low-costPoppy humanoid robotic platform, that allows non-roboticists to quickly set up and program roboticexperiments. A second aspect is to show how the Explauto library allows systematic comparison andevaluation of active learning and exploration algorithms in sensorimotor spaces, through a Python API toselect already implemented exploration algorithms. The third idea is to showcase Active Model Babbling,an efficient exploration algorithm dynamically choosing which task/goal space to explore and particulargoals to reach, and integrating social guidance from humans in real time to drive exploration towardsparticular objects or actions.[Forestier and Oudeyer, 2016] Forestier, S. and Oudeyer, P.-Y. (2016). Modular active curiosity-driven discovery oftool use. In 2016 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Daejeon, Korea.[Lapeyre et al., 2014] Lapeyre, M., Rouanet, P., Grizou, J., Nguyen, S., Depraetre, F., Le Falher, A., and Oudeyer,P.-Y. (2014). Poppy Project: Open-Source Fabrication of 3D Printed Humanoid Robot for Science, Educationand Art. In Digital Intelligence 2014, page 6, Nantes, France.[Moulin-Frier et al., 2014] Moulin-Frier, C., Rouanet, P., Oudeyer, P.-Y., and others (2014). Explauto: an open-source Python library to study autonomous exploration in developmental robotics. In ICDL-Epirob-InternationalConference on Development and Learning, Epirob","Autonomous exploration, active learning and human guidance with open-source Poppy humanoid robot platform and Explauto library",,HAL CCSD,,,core
296199376,2016,"A key issue in designing robotics systems is the cost of an integrated camera sensor that meets the bandwidth/processing requirement for many advanced robotics applications, especially lightweight robotics applications, such as visual surveillance or SLAM in autonomous aerial vehicles. There is currently much work going on to adapt smartphones to provide complete robot vision systems, as the phone is so exquisitely integrated having camera(s), inertial sensing, sound I/O and excellent wireless connectivity. Mass market production makes this a very low-cost platform and manufacturers from quadrotor drone suppliers to children’s toys, such as the Meccanoid robot, employ a smartphone to provide a vision system/control system.



Accordingly, many research groups are attempting to optimise image analysis, computer vision and machine learning libraries for the smartphone platform. However current approaches to robot vision remain highly demanding for mobile processors such as the ARM, and while a number of algorithms have been developed, these are very stripped down, i.e. highly compromised in function or performance For example, the semi-dense visual odometry implementation of [1] operates on images of only 320x240pixels.



In our research we have been developing biologically motivated foveated vision algorithms, potentially some 100 times more efficient than their conventional counterparts, based on a model of the mammalian retina we have developed. Vision systems based on the foveated architectures found in mammals have the potential to reduce bandwidth and processing requirements by about x100 - it has been estimated that our brains would weigh ~60Kg if we were to process all our visual input at uniform high resolution. We have reported a foveated visual architecture that implements a functional model of the retina-visual cortex to produce feature vectors that can be matched/classified using conventional methods, or indeed could be adapted to employ Deep Convolutional Neural Nets for the classification/interpretation stage, [2,3,4]. 



We are now at the early stages of investigating how best to port our foveated architecture onto a smartphone platform. To achieve the required levels of performance we propose to optimise our retina model to the ARM processors utilised in smartphones, in conjunction with their integrated GPUs, to provide a foveated smart vision system on a smartphone. Our current goal is to have a foveated system running in real-time to serve as a front-end robot sensor for tasks such as general purpose object recognition and reliable dense SLAM using a commercial off-the-shelf smartphone which communicates with conventional hardware performing back-end visual classification/interpretation. We believe that, as in Nature, space-variance is the key to achieving the necessary data reduction to be able to implement the complete visual processing chain on the smartphone itself",A Biologically Motivated Software Retina for Robotic Sensors Based on Smartphone Technology,,,,,core
296199379,2016-09-16T00:00:00,"A key issue in designing robotics systems is the cost of an integrated camera sensor that meets the bandwidth/processing requirement for many advanced robotics applications, especially lightweight robotics applications, such as visual surveillance or SLAM in autonomous aerial vehicles. There is currently much work going on to adapt smartphones to provide complete robot vision systems, as the smartphone is so exquisitely integrated by having camera(s), inertial sensing, sound I/O and excellent wireless connectivity. Mass market production makes this a very low-cost platform and manufacturers from quadrotor drone suppliers to children’s toys, such as the Meccanoid robot [5], employ a smartphone to provide a vision system/control system [7,8].

 

Accordingly, many research groups are attempting to optimise image analysis, computer vision and machine learning libraries for the smartphone platform. However current approaches to robot vision remain highly demanding for mobile processors such as the ARM, and while a number of algorithms have been developed, these are very stripped down, i.e. highly compromised in function or performance. For example, the semi-dense visual odometry implementation of [1] operates on images of only 320x240pixels.

                

In our research we have been developing biologically motivated foveated vision algorithms based on a model of the mammalian retina [2], potentially 100 times more efficient than their conventional counterparts. Accordingly, vision systems based on the foveated architectures found in mammals have also the potential to reduce bandwidth and processing requirements by about x100 - it has been estimated that our brains would weigh ~60Kg if we were to process all our visual input at uniform high resolution. We have reported a foveated visual architecture [2,3,4] that implements a functional model of the retina-visual cortex to produce feature vectors that can be matched/classified using conventional methods, or indeed could be adapted to employ Deep Convolutional Neural Nets for the classification/interpretation stage. Given the above processing/bandwidth limitations, a viable way forward would be to perform off-line learning and implement the forward recognition path on the mobile platform, returning simple object labels, or sparse hierarchical feature symbols, and gaze control commands to the host robot vision system and controller. 

 

We are now at the early stages of investigating how best to port our foveated architecture onto an ARM-based smartphone platform. To achieve the required levels of performance we propose to port and optimise our retina model to the mobile ARM processor architecture in conjunction with their integrated GPUs. We will then be in the position to provide a foveated smart vision system on a smartphone with the advantage of processing speed gains and bandwidth optimisations. Our approach will be to develop efficient parallelising compilers and perhaps propose new processor architectural features to support this approach to computer vision, e.g. efficient processing of hexagonally sampled foveated images.

 

Our current goal is to have a foveated system running in real-time on at least a 1080p input video stream to serve as a front-end robot sensor for tasks such as general purpose object recognition and reliable dense SLAM using a commercial off-the-shelf smartphone. Initially this system would communicate a symbol stream to conventional hardware performing back-end visual classification/interpretation, although simple object detection and recognition tasks should be possible on-board the device. We propose that, as in Nature, foveated vision is the key to achieving the necessary data reduction to be able to implement complete visual recognition and learning processes on the smartphone itself",A Biologically Motivated Software Retina for Robotic Sensors for ARM-Based Mobile Platform Technology,https://core.ac.uk/download/pdf/296199379.pdf,,,,core
78940132,Jul-16,"By taking advantage of complementary communication technologies, distinct sensing functionalities and varied motion dynamics present in a heterogeneous multi-robotic network, it is possible to accomplish a main mission objective by assigning specialized sub-tasks to specific members of a robotic team. An adequate selection of the team members and an effective coordination are some of the challenges to fully exploit the unique capabilities that these types of systems can offer. Motivated by real world applications, we focus on a multi-robotic network consisting off aerial and ground agents which has the potential to provide critical support to humans in complex settings. For instance, aerial robotic relays are capable of transporting small ground mobile sensors to expand the communication range and the situational awareness of first responders in hazardous environments.

In the first part of this dissertation, we extend work on manipulation of cable-suspended loads using aerial robots by solving the problem of lifting the cable-suspended load from the ground before proceeding to transport it. Since the suspended load-quadrotor system experiences switching conditions during this critical maneuver, we define a hybrid system and show that it is differentially-flat. This property facilitates the design of a nonlinear controller which tracks a waypoint-based trajectory associated with the discrete states of the hybrid system. In addition, we address the case of unknown payload mass by combining a least-squares estimation method with the designed controller.

Second, we focus on the coordination of a heterogeneous team formed by a group of ground mobile sensors and a flying communication router which is deployed to sense areas of interest in a cluttered environment. Using potential field methods, we propose a controller for the coordinated mobility of the team to guarantee inter-robot and obstacle collision avoidance as well as connectivity maintenance among the ground agents while the main goal of sensing is carried out. For the case of the aerial communications relays, we combine antenna diversity with reinforcement learning to dynamically re-locate these relays so that the received signal strength is maintained above a desired threshold.

Motivated by the recent interest of combining radio frequency and optical wireless communications, we envision the implementation of an optical link between micro-scale aerial and ground robots. This type of link requires maintaining a sufficient relative transmitter-receiver position for reliable communications. In the third part of this thesis, we tackle this problem. Based on the link model, we define a connectivity cone where a minimum transmission rate is guaranteed. For example, the aerial robot has to track the ground vehicle to stay inside this cone. The control must be robust to noisy measurements. Thus, we use particle filters to obtain a better estimation of the receiver position and we design a control algorithm for the flying robot to enhance the transmission rate. Also, we consider the problem of pairing a ground sensor with an aerial vehicle, both equipped with a hybrid radio-frequency/optical wireless communication system. A challenge is positioning the flying robot within optical range when the sensor location is unknown. Thus, we take advantage of the hybrid communication scheme by developing a control strategy that uses the radio signal to guide the aerial platform to the ground sensor. Once the optical-based signal strength has achieved a certain threshold, the robot hovers within optical range.

Finally, we investigate the problem of building an alliance of agents with different skills in order to satisfy the requirements imposed by a given task. We find this alliance, known also as a coalition, by using a bipartite graph in which edges represent the relation between agent capabilities and required resources for task execution. Using this graph, we build a coalition whose total capability resources can satisfy the task resource requirements. Also, we study the heterogeneity of the formed coalition to analyze how it is affected for instance by the amount of capability resources present in the agents.Electrical EngineeringDoctoralUniversity of New Mexico.  Dept. of Electrical and Computer EngineeringFierro, RafaelOishi, MeekoTapia, LydiaSadler, Bria",Exploiting Heterogeneity in Networks of Aerial and Ground Robotic Agents,,,,,core
151089656,2016-01-01T00:00:00,"Telepresence Mobile Robots have prominent attributes in many fields as it provides virtual presence for human robot interaction. The deployment of this robot in healthcare sector has improved patient care and health. The vision system in a telepresence robot allows two way audiovisual communication between people at different location. In spite of such advancement, the manual way of controlling a robot to recognise and track people during an emergency is not favourable for a long duration. To circumvent this problem, biometric method using human face is proposed in this research which is implemented on Medical Telediagnosis Robot. This paper details the design of the face recognition and tracking system with four automated modules which are motion detection, face detection, face recognition and face tracking. The modules are developed with different algorithm and tested individually to ensure the stability of the system. Artificial Intelligence technique was applied at the face recognition stage while a two degree of freedom mechanism for actuator control was used at face tracking stage. A sequential mode operation is proposed to reduce the execution time in a real-time environment. To achieve this, only one module is operated at each time. A Graphical User Interface was developed to ease the users at the local and robot environment. The system is designed in LabVIEW platform. The biometric system proposed with hybrid algorithm at each module adapts for face images detected at different distances, poses and lighting condition. This system was tested in real-time and has an execution time of 55ms and 98% accuracy. The stand alone system designed for Medical Telediagnosis Robot can be will be very fruitful for various biometric system using facial technology",Towards real-time visual biometric authentication using human face for healthcare telepresence mobile robots,https://core.ac.uk/download/151089656.pdf,,,,core
228881183,2016-04-30T00:00:00,"Artificial Neural Networks (ANN) can be used to solve specific problems such as prediction, classification, processing data, and robotics. Based on the exposure, this study tried to develop a system by applying ANN models Fully Recurrent Neural Network (FRNN). Fully Recurrent Neural Network structures have been presence of feedback that can make faster iteration thus making the update parameters and convergence speed become faster. The learning method used is Backpropagation Through Time. The system is implemented using the C# program. Input vectors used consisted of 7 variables.The results showedt the developed system will rapidly converge and able to achieve the most optimal error value (minimum error) when using one hidden layer with 17 units of the number of neurons. The best accuracy can be obtained using the LR of 0.001, target of 0.1and momentum 0.95, with 30 the real data test, the system accuracy reaches 83.33%",KLASIFIKASI DATA MENGGUNAKAN JARINGAN SYARAF TIRUAN MODEL FRNN (FULLY RECURRENT NEURAL NETWORK),https://core.ac.uk/download/228881183.pdf,"Fakultas Sains dan Teknik, Universitas Nusa Cendana",,,core
71688756,2016-01-01T00:00:00,"This paper is concerned with the coordination control of multiple biomimetic robotic fish in highly dynamic aquatic environments by building a hybrid centralized system. With the aid of the results of biorobotics and control techniques, a radio-controlled multijoint robotic fish and its locomotion control are developed. To enable a closed control loop, a visual subsystem that is responsible for tracking of multiple moving objects is constructed and implemented in real time. Furthermore, a behavior-based hierarchical architecture in conjunction with fuzzy reinforcement learning is proposed to accomplish effective coordination among multiple swimming robots. Finally, experiments on 2vs2 water polo game are carried out to verify the proposed coordination control scheme. Over the past eight years, this multirobot platform has been successfully applied to international underwater robot competitions to promote innovative research and education in underwater robotics",Coordination of Multiple Robotic Fish With Applications to Underwater Robot Competition,,,,,core
42674090,2016-01-19T00:00:00,"Anthropomimetic robots are robots that sense, behave, interact and feel like
humans. By this definition, anthropomimetic robots require human-like physical
hardware and actuation, but also brain-like control and sensing. The most
self-evident realization to meet those requirements would be a human-like
musculoskeletal robot with a brain-like neural controller. While both
musculoskeletal robotic hardware and neural control software have existed for
decades, a scalable approach that could be used to build and control an
anthropomimetic human-scale robot has not been demonstrated yet. Combining
Myorobotics, a framework for musculoskeletal robot development, with SpiNNaker,
a neuromorphic computing platform, we present the proof-of-principle of a
system that can scale to dozens of neurally-controlled, physically compliant
joints. At its core, it implements a closed-loop cerebellar model which
provides real-time low-level neural control at minimal power consumption and
maximal extensibility: higher-order (e.g., cortical) neural networks and
neuromorphic sensors like silicon-retinae or -cochleae can naturally be
incorporated.Comment: Accepted at IEEE Robotics and Automation Magazine on 2015-12-3",Scalability in Neural Control of Musculoskeletal Robots,http://arxiv.org/abs/1601.04862,'Institute of Electrical and Electronics Engineers (IEEE)',10.1109/MRA.2016.2535081,,core
103838094,20/01/2016,"Abstract — This paper presents a probabilistic model of ul-trasonic range sensors using backpropagation neural networks trained on experimental data. The sensor model provides the probability of detecting mapped obstacles in the environment, given their position and orientation relative to the transducer. The detection probability can be used to compute the location of an autonomous vehicle from those obstacles that are more likely to be detected. The neural network model is more accurate than other existing approaches, since it captures the typical multilobal detection pattern of ultrasonic transducers. Since the network size is kept small, implementation of the model on a mobile robot can be efficient for real-time navigation. An example that demonstrates how the credence could be incorporated into the extended Kalman filter (EKF) and the numerical values of the final neural network weights are provided in the Appendixes. Index Terms—Kalman filtering, mobile robots, motion plan-ning, neural networks, sonar navigation. I",Modeling of Ultrasonic Range Sensors for Localization of Autonomous Mobile Robots,,,,,core
141671449,2016-01-01T08:00:00,"Deep learning consists of various machine learning algorithms that aim to learn multiple levels of abstraction from data in a hierarchical manner. It is a tool to construct models using the data that mimics a real world process without an exceedingly tedious modelling of the actual process. We show that deep learning is a viable solution to decision making in mechanical engineering problems and complex physical systems.
In this work, we demonstrated the application of this data-driven method in the design of microfluidic devices to serve as a map between the user-defined cross-sectional shape of the flow and the corresponding arrangement of micropillars in the flow channel that contributed to the flow deformation. We also present how deep learning can be used in the early detection of combustion instability for prognostics and health monitoring of a combustion engine, such that appropriate measures can be taken to prevent detrimental effects as a result of unstable combustion.
One of the applications in complex systems concerns robotic path planning via the systematic learning of policies and associated rewards. In this context, a deep architecture is implemented to infer the expected value of information gained by performing an action based on the states of the environment. We also applied deep learning-based methods to enhance natural low-light images in the context of a surveillance framework and autonomous robots. Further, we looked at how machine learning methods can be used to perform root-cause analysis in cyber-physical systems subjected to a wide variety of operation anomalies. In all studies, the proposed frameworks have been shown to demonstrate promising feasibility and provided credible results for large-scale implementation in the industry",Deep Learning for Decision Making and Autonomous Complex Systems,https://core.ac.uk/download/141671449.pdf,Iowa State University Digital Repository,,,core
151085194,2016-01-01T00:00:00,"The development of reliable and robust visual recognition systems is a main challenge towards the deployment of autonomous robotic agents in unconstrained environments. Learning to recognize objects requires image representations that are discriminative to relevant information while being
invariant to nuisances, such as scaling, rotations, light and background changes, and so forth. Deep Convolutional Neural Networks can learn such representations from large webcollected image datasets and a natural question is how these systems can be best adapted to the robotics context where little supervision is often available. In this work, we investigate different training strategies for deep architectures on a new dataset collected in a real-world
robotic setting. In particular we show how deep networks can be tuned to improve invariance and discriminability properties and perform object identification tasks with minimal supervision",Object identification from few examples by improving the invariance of a Deep Convolutional Neural Network,,country:USA,10.1109/iros.2016.7759720,,core
105398500,02/09/2016,"Mixed reality is an important classroom tool for managing complexity from both the students ’ and instructor’s stand-points. It can be used to provide important scaffolds when introducing robotics, by allowing elements of perception and control to be abstracted, and these abstractions removed as a course progresses (or left in place to introduce robotics to younger groups of students). In prior work, we have illus-trated the potential of this approach both in providing scaf-folding, building an inexpensive robotics laboratory, and also providing control of evaluation of robotics environments for student evaluation and scientific experimentation. In this pa-per, we explore integrating extensions and improvements to the mixed reality components themselves as part of a course in applied artificial intelligence and robotics. We present a set of assignments that in addition to exploring robotics con-cepts, actively integrate creating or improving mixed reality components. We find that this approach better leverages the advantages brought about by mixed reality in terms of stu-dent motivation, and also provides some very useful software engineering experience to the students",Leveraging Mixed Reality Infrastructure for Robotics and Applied AI Instruction,,,,,core
78372240,2016-08-16T00:00:00,"Parallel Discrete-Manipulators are a special kind of force regulated manipulators which can undergo continuous motions despite being commanded through a large but finite number of states only. Real-time control of such systems requires very fast and efficient methods for solving their inverse static analysis. In this paper, artificial intelligence techniques (AI) are investigated for addressing the inverse static analysis of a planar parallel array featuring ten three-state force actuators and two applications using 3D Massively Parallel Robots (MPRs) with one and two layers. In particular, the research method used simulation software and hardware testing with the case of parallel manipulator with two level discrete pneumatic actuators. Simulations with typical desired displacement inputs are presented and a good performance of the results compared to AI is obtained. The comparison showed that the parallel manipulator has the Root Mean Squared Error (RMSE) has less than 10% and can be used for controlling the ternary states of discrete manipulators via AI",Applications of Artificial Intelligence Control for Parallel Discrete-Manipulators,https://core.ac.uk/download/78372240.pdf,'Institute of Electrical and Electronics Engineers (IEEE)',10.1109/icaicta.2016.7803114,,core
131206311,2016-01-01T00:00:00,"This volume offers very selected papers from the 2014 conference of the “International Association for Computing and Philosophy” (IACAP) - a conference tradition of 28 years. - - -	Table of Contents - 0	Vincent C. Müller: - Editorial - 	1) Philosophy of computing - 1	Çem Bozsahin: - What is a computational constraint? - 2	Joe Dewhurst: - Computing Mechanisms and Autopoietic Systems - 3	Vincenzo Fano, Pierluigi Graziani, Roberto Macrelli and Gino Tarozzi: - Are Gandy Machines really local?  - 4	Doukas Kapantais: - A refutation of the Church-Turing thesis according to some interpretation of what the thesis says - 5	Paul Schweizer: - In What Sense Does the Brain Compute? - 	2) Philosophy of computer science & discovery - 6	Mark Addis, Peter Sozou, Peter C R Lane and Fernand Gobet: - Computational Scientific Discovery and Cognitive Science Theories - 7	Nicola Angius and Petros Stefaneas: - Discovering Empirical Theories of Modular Software Systems. An Algebraic Approach. - 8	Selmer Bringsjord, John Licato, Daniel Arista, Naveen Sundar Govindarajulu and Paul Bello: - Introducing the Doxastically Centered Approach to Formalizing Relevance Bonds in Conditionals - 9	Orly Stettiner: - From Silico to Vitro: - Computational Models of Complex Biological Systems Reveal Real-world Emergent Phenomena - 	3) Philosophy of cognition & intelligence - 10	Douglas Campbell: - Why We Shouldn’t Reason Classically, and the Implications for Artificial Intelligence - 11	Stefano Franchi: - Cognition as Higher Order Regulation - 12	Marcello Guarini: - Eliminativisms, Languages of Thought, & the Philosophy of Computational Cognitive Modeling - 13	Marcin Miłkowski: - A Mechanistic Account of Computational Explanation in Cognitive Science and Computational Neuroscience - 14	Alex Tillas: - Internal supervision & clustering: - A new lesson from ‘old’ findings? - 	4) Computing & society - 15	Vasileios Galanos: - Floridi/Flusser: - Parallel Lives in Hyper/Posthistory - 16	Paul Bello: - Machine Ethics and Modal Psychology - 17	Marty J. Wolf and Nir Fresco: - My Liver Is Broken, Can You Print Me a New One?  - 18	Marty J. Wolf, Frances Grodzinsky and Keith W. Miller: - Robots, Ethics and Software – FOSS vs. Proprietary License",Computing and philosophy: Selected papers from IACAP 2014,,,,,core
82439102,31/12/2016,"AbstractThe effort to develop an electronic skin is highly motivated by many application domains namely robotics, biomedical instrumentations, and replacement prosthetic devices. Several e-skin systems have been proposed recently and have demonstrated the need of an embedded electronic system for tactile data processing either to mimic the human skin or to respond to the application demands. Processing tactile data requires efficient methods to extract meaningful information from raw sensors data.In this framework, our goal is the development of a dedicated embedded electronic system for electronic skin. The embedded electronic system has to acquire the tactile data, process and extract structured information. Machine Learning (ML) represents an effective method for data analysis in many domains: it has recently demonstrated its effectiveness in processing tactile sensors data.This paper presents an embedded electronic system based on dedicated hardware implementation for electronic skin systems. It provides a Tensorial kernel function implementation for machine learning based on Tensorial kernel approach. Results assess the time latency and the hardware complexity for real time functionality. The implementation results highlight the high amount of power consumption needed for the input touch modalities classification task. Conclusions and future perspectives are also presented",Embedded Electronic System Based on Dedicated Hardware DSPs for Electronic Skin Implementation ,https://core.ac.uk/download/pdf/82439102.pdf,The Author(s). Published by Elsevier Ltd.,10.1016/j.protcy.2016.08.007,,core
198239554,2016-12-01T00:00:00,"This study presents a framework that recognizes and imitates human upper-body motions in real time. The framework consists of two parts. In the first part, a transformation algorithm is applied to 3D human motion data captured by a Kinect. The data are then converted into the robot’s joint angles by the algorithm. The human upper-body motions are successfully imitated by the NAO humanoid robot in real time.

In the second part, the human action recognition algorithm is implemented for upper-body gestures. A human action dataset is also created for the upper-body movements. Each action is performed 10 times by twenty-four users. The collected joint angles are divided into six action classes. Extreme Learning Machines (ELMs) are used to classify the human actions. Additionally, the Feed-Forward Neural Networks (FNNs) and K-Nearest Neighbor (K-NN) classifiers are used for comparison. According to the comparative results, ELMs produce a good human action recognition performance",Gesture imitation and recognition using Kinect sensor and extreme learning machines,,'Elsevier BV',10.1016/j.measurement.2016.09.026,,core
73391925,2016-10-11T00:00:00,"Developing control policies in simulation is often more practical and safer
than directly running experiments in the real world. This applies to policies
obtained from planning and optimization, and even more so to policies obtained
from reinforcement learning, which is often very data demanding. However, a
policy that succeeds in simulation often doesn't work when deployed on a real
robot. Nevertheless, often the overall gist of what the policy does in
simulation remains valid in the real world. In this paper we investigate such
settings, where the sequence of states traversed in simulation remains
reasonable for the real world, even if the details of the controls are not, as
could be the case when the key differences lie in detailed friction, contact,
mass and geometry properties. During execution, at each time step our approach
computes what the simulation-based control policy would do, but then, rather
than executing these controls on the real robot, our approach computes what the
simulation expects the resulting next state(s) will be, and then relies on a
learned deep inverse dynamics model to decide which real-world action is most
suitable to achieve those next states. Deep models are only as good as their
training data, and we also propose an approach for data collection to
(incrementally) learn the deep inverse dynamics model. Our experiments shows
our approach compares favorably with various baselines that have been developed
for dealing with simulation to real world model discrepancy, including output
error control and Gaussian dynamics adaptation","Transfer from Simulation to Real World through Learning Deep Inverse
  Dynamics Model",http://arxiv.org/abs/1610.03518,,,,core
103815429,18/01/2016,"Abstract — Many people suffer from the loss of a limb. Learning to get by without an arm or hand can be very challenging, and existing prostheses do not yet fulfil the needs of individuals with amputations. One promising solution is to provide greater communication between a prosthesis and its user. Towards this end, we present a simple machine learning interface to supplement the control of a robotic limb with feedback to the user about what the limb will be experiencing in the near future. A real-time prediction learner was implemented to predict impact-related electrical load experienced by a robot limb; the learning system’s predictions were then communicated to the device’s user to aid in their interactions with a workspace. We tested this system with five able-bodied subjects. Each subject manipulated the robot arm while receiving different forms of vibrotactile feedback regarding the arm’s contact with its workspace. Our trials showed that communicable predictions could be learned quickly during human control of the robot arm. Using these predictions as a basis for feedback led to a statistically significant improvement in task performance when compared to purely reactive feedback from the device. Our study therefore contributes initial evidence that prediction learning and machine intelligence can benefit not just control, but also feedback from an artificial limb. We expect that a greater level of acceptance and ownership can be achieved if the prosthesis itself takes an active role in transmitting learned knowledge about its state and its situation of use. I","Using Learned Predictions as Feedback to Improve Control and Communication with an Artificial Limb: Preliminary Findings,” arXiv:1408.1913 [cs.AI",,,,,core
96884779,2016-01-01T00:00:00,"A key issue in designing robotics systems is the cost of an integrated camera sensor that meets the bandwidth/processing requirement for many advanced robotics applications, especially lightweight robotics applications, such as visual surveillance or SLAM in autonomous aerial vehicles. There is currently much work going on to adapt smartphones to provide complete robot vision systems, as the phone is so exquisitely integrated having camera(s), inertial sensing, sound I/O and excellent wireless connectivity. Mass market production makes this a very low-cost platform and manufacturers from quadrotor drone suppliers to children’s toys, such as the Meccanoid robot, employ a smartphone to provide a vision system/control system.



Accordingly, many research groups are attempting to optimise image analysis, computer vision and machine learning libraries for the smartphone platform. However current approaches to robot vision remain highly demanding for mobile processors such as the ARM, and while a number of algorithms have been developed, these are very stripped down, i.e. highly compromised in function or performance For example, the semi-dense visual odometry implementation of [1] operates on images of only 320x240pixels.



In our research we have been developing biologically motivated foveated vision algorithms, potentially some 100 times more efficient than their conventional counterparts, based on a model of the mammalian retina we have developed. Vision systems based on the foveated architectures found in mammals have the potential to reduce bandwidth and processing requirements by about x100 - it has been estimated that our brains would weigh ~60Kg if we were to process all our visual input at uniform high resolution. We have reported a foveated visual architecture that implements a functional model of the retina-visual cortex to produce feature vectors that can be matched/classified using conventional methods, or indeed could be adapted to employ Deep Convolutional Neural Nets for the classification/interpretation stage, [2,3,4]. 



We are now at the early stages of investigating how best to port our foveated architecture onto a smartphone platform. To achieve the required levels of performance we propose to optimise our retina model to the ARM processors utilised in smartphones, in conjunction with their integrated GPUs, to provide a foveated smart vision system on a smartphone. Our current goal is to have a foveated system running in real-time to serve as a front-end robot sensor for tasks such as general purpose object recognition and reliable dense SLAM using a commercial off-the-shelf smartphone which communicates with conventional hardware performing back-end visual classification/interpretation. We believe that, as in Nature, space-variance is the key to achieving the necessary data reduction to be able to implement the complete visual processing chain on the smartphone itself",A Biologically Motivated Software Retina for Robotic Sensors Based on Smartphone Technology,https://core.ac.uk/download/96884779.pdf,,,,core
132585674,2016-09-06T00:00:00,"Unmanned Aerial Vehicles are currently investigated as an important sub-domain of robotics, a fast growing and truly multidisciplinary research field. UAVs are increasingly deployed in real-world settings for missions in dangerous environments or in environments which are challenging to access. Combined with autonomous flying capabilities, many new possibilities, but also challenges, open up. To overcome the challenge of early identification of degradation, machine learning based on flight features is a promising direction. Existing approaches build classifiers that consider their features to be correlated. This prevents a fine-grained detection of degradation for the different hardware components. This work presents an approach where the data is considered uncorrelated and, using machine learning
techniques, allows the precise identification of UAV’s damages",UAV degradation identification for pilot notification using machine learning techniques,https://core.ac.uk/download/132585674.pdf,,10.1109/etfa.2016.7733537,,core
293724665,2016-01-01T00:00:00,"The ability of robots to perform tasks in human environments has 
largely been limited to rather simple and specific tasks, such as lawn mowing 
and vacuum cleaning. As such, current robots are far away from the robot butlers, assistants, 
and housekeepers that are depicted in science fiction movies. Part of this gap can be 
explained by the fact that human environments are hugely varied, complex and unstructured. 
For example, the homes that a domestic robot might end up in are hugely varied. Since 
every home has a different layout with different objects and furniture, it is impossible for 
a human designer to anticipate all challenges a robot might 
face, and equip the robot a priori with all the necessary perceptual and manipulation skills.


Instead, robots could be programmed in a way that allows them to adapt to any 
environment that they are in. In that case, the robot designer would not 
need to precisely anticipate such environments. The ability to adapt can be provided by 
robot learning techniques, which can be applied to learn skills for perception and manipulation.
Many of the current
robot learning techniques,
however, rely on human supervisors to provide annotations or demonstrations, and to fine-tuning the methods parameters and heuristics. As such, 
it can require a significant amount of human time investment to 
make a robot perform a task in a novel environment, even if statistical learning techniques are used.

In this thesis, I focus on another way of obtaining the data a robot needs to 
learn about the environment and how to successfully 
perform skills in it. By exploring the environment using its own sensors and actuators, rather than 
passively waiting for annotations or demonstrations, a 
robot can obtain this data by itself. I investigate multiple approaches that allow a robot 
to explore its environment autonomously, while trying to minimize the design effort 
required to deploy such algorithms in different situations.

First, I consider an unsupervised robot with minimal prior knowledge
about its environment. It can only learn through observed
sensory feedback obtained though interactive exploration of its
environment. In a bottom-up, probabilistic approach, the robot tries to segment
the objects in its environment through clustering with minimal prior knowledge. This clustering is
based on static visual scene features and observed movement. Information theoretic principles are used to autonomously select actions that maximize
the expected information gain, and thus learning speed. Our evaluations 
on a real robot system equipped with an on-board camera show that the proposed 
method handles noisy inputs better than previous methods, and that 
action selection according to the information gain criterion does increase the learning speed.

Often, however, the goal of a robot is not just to learn the structure of the environment, but to learn 
how to perform a task encoded by a reward signal. 
 In addition to the weak feedback provided by reward signals, the robot has access to rich sensory data, that, even for
simple tasks, is often non-linear and high-dimensional. Sensory data can be
leveraged to learn a system model, but in high-dimensional sensory spaces this
step often requires manually designing features. I propose a robot
reinforcement learning algorithm with learned non-parametric models, value
functions, and policies that can deal with high-dimensional state representations.
As such, the proposed algorithm is well-suited to deal with high-dimensional signals 
such as camera images. To avoid that the robot converges prematurely  to a sub-optimal solution, 
the information loss of policy updates is limited. This constraint makes sure the robot keeps exploring the effects
of its behavior on the environment. The experiments show that the proposed non-parametric 
relative entropy policy search algorithm performs better than prior methods that either do not employ bounded updates, 
or that try to cover the state-space with general-purpose radial basis functions. Furthermore, 
the method is validated on a 
real-robot setup with high-dimensional camera image inputs.

One problem with typical exploration strategies is that the behavior is perturbed independently 
in each time step, for example through selecting a random action or random policy parameters. 
As such, the resulting exploration behavior might be incoherent. Incoherence causes 
inefficient random walk behavior, makes the system less robust, and causes wear and tear on the robot. 
A typical solution is to perturb the policy parameters directly, and use the same perturbation for an entire episode. However, this
strategy
tends to increase the number of episodes needed, since only a single perturbation can be evaluated per episode. I introduce a
strategy that can make a more balanced trade-off between the advantages of these two approaches.
The experiments show that intermediate trade-offs, rather than independent or episode-based exploration, 
is beneficial across different tasks and learning algorithms. 

This thesis thus addresses how robots can learn autonomously by exploring the world through
unsupervised learning and reinforcement learning. Throughout the thesis, new approaches 
and algorithms are introduced: a probabilistic interactive segmentation approach, the non-parametric 
relative entropy policy search algorithm, and a framework for generalized exploration. 
To allow the learning algorithms to be applied in different and unknown environments, 
the  design effort and supervision required from human designers or users is minimized. 
These approaches and algorithms contribute 
towards the capability of robots to autonomously learn useful skills in human environments in a practical manner",Machine Learning through Exploration for Perception-Driven Robotics,,,,,core
103796937,16/01/2016,"Abstract. Poses and gestures are an important part of the nonverbal inter-human communication. In the last years many different methods for estimating poses and gestures in the field of Human-Machine-Interfaces were developed. In this paper for the first time we present an exper-imental comparison of several re-implemented Neural Network based approaches for a demanding visual instruction task on a mobile sys-tem. For the comparison we used several Neural Networks (Neural Gas, SOM, LLM, PSOM and MLP) and a k-Nearest-Neighbourhood classifi-cator on a common data set of images, which we recorded on our mobile robot Horos under real world conditions. For feature extraction we use Gaborjets and the features of a special histogram on the image. We also compare the results of the different approaches with the results of human subjects who estimated the target point of a pointing pose. The results obtained demonstrate that a cascade of MLPs is best suited to cope with the task and achieves results equal to human subjects. ",Estimation of Pointing Poses on Monocular Images with Neural Techniques- An Experimental Comparison,,,,,core
104399210,17/08/2016,"†Joint first authors. Abstract — We introduce a dynamic neural algorithm called Dynamic Neural (DN) SARSA(λ) for learning a behavioral se-quence from delayed reward. DN-SARSA(λ) combines Dynamic Field Theory models of behavioral sequence representation, classical reinforcement learning, and a computational neuro-science model of working memory, called Item and Order work-ing memory, which serves as an eligibility trace. DN-SARSA(λ) is implemented on both a simulated and real robot that must learn a specific rewarding sequence of elementary behaviors from exploration. Results show DN-SARSA(λ) performs on the level of the discrete SARSA(λ), validating the feasibility of general reinforcement learning without compromising neural dynamics. I",Y.: Autonomous reinforcement of behavioral sequences in neural dynamics,,,,,core
80693500,2017-06-15T00:00:00,"Artificial intelligence that can change the way we live is less of a future possibility and more a present reality. Social Robots that help the elderly, play children’s games and learn from their environment in order to adapt and interact with humans are some of the latest breakthroughs (The Guardian, 2015). Such technological developments may make some people feel uneasy - perhaps a car’s cruise control function or robot vacuum cleaners are more familiar robotic technology advances that rest more comfortably with people. Even less extreme, but much more common, technological advances sees the use of digital devices and software applications being adopted by many and integrated into their everyday lives. For instance, wearable technology and fitness and weight loss apps that track performance, set goals and provide progress reports were amongst the most popular smartphone apps of 2015 (Techradar, 2015). GPS on smartphones is offered via Google Maps so there is little need to know where you are going or prepare a journey in advance or even be able to read a map accurately. Apps such as Timehop remind users of specific memories once posted on social networks, providing personalized material to reflect on and be nostalgic about. In relation to consumer practices, a wide variety of applications are routinely used via smartphones, tablets and laptop computers, and are consequently changing the way that people engage in practices and the ways that people consume more generally. For instance, 100 million monthly active users of Pinterest search, pin and share things they desire (Fortune, 2015) and can make purchases of these objects via the site. Nearly 50% of people reported using their smartphones while shopping for food and a third use their smartphones to find recipes as a matter of routine (Allrecipes.com, 2013). What these examples tell us is that there is a growing delegation of everyday practices to technology and it is clear that consumption has been altered and enhanced by such advances in digital technology. In this chapter we explore the growing digitalisation of consumer practices from a perspective of how human and non-human actors come together in configuring such practices. We draw on practice theory – an accepted and growing area of work in consumer research – to introduce the foundational concept of human-non-human hybrids. We then focus particularly on the ways in which consumers’ cognitive abilities are apparently extended by and externalised to digital technologies and use the concept of extended mind (Clark and Chalmers, 1998) to develop this line of thinking. In particular, we focus on consumers’ knowledge, imagination and memory related to a given practice or consumption object. To do this, we draw on data from a large, on-going study related to digital virtual consumption conducted over the last eight years, which enables us to consider how digital devices and the various platforms and software applications that are accessed through them are integrated in and consequently transform consumer practices. We identify the kinds of new work that is required from consumers in terms of using digital technology – i.e., developing skills, knowledge, competence and a commitment to their use. We also consider the implications of this for practice and for the consumption experience",Extending the mind: Digital devices and the transformation of consumer practices,https://core.ac.uk/download/80693500.pdf,'Informa UK Limited',,,core
148028242,2017-02-17T11:13:42,"Combined efforts in the fields of neuroscience, computer science, and biology allowed to design biologically realistic models of the brain based on spiking neural networks. For a proper validation of these models, an embodiment in a dynamic and rich sensory environment, where the model is exposed to a realistic sensory-motor task, is needed. Due to the complexity of these brain models that, at the current stage, cannot deal with real-time constraints, it is not possible to embed them into a real-world task. Rather, the embodiment has to be simulated as well. While adequate tools exist to simulate either complex neural networks or robots and their environments, there is so far no tool that allows to easily establish a communication between brain and body models. The Neurorobotics Platform is a new web-based environment that aims to fill this gap by offering scientists and technology developers a software infrastructure allowing them to connect brain models to detailed simulations of robot bodies and environments and to use the resulting neurorobotic systems for in silico experimentation. In order to simplify the workflow and reduce the level of the required programming skills, the platform provides editors for the specification of experimental sequences and conditions, environments, robots, and brain-body connectors. In addition to that, a variety of existing robots and environments are provided. This work presents the architecture of the first release of the Neurorobotics Platform developed in subproject 10 ""Neurorobotics"" of the Human Brain Project (HBP). 1 At the current state, the Neurorobotics Platform allows researchers to design and run basic experiments in neurorobotics using simulated robots and simulated environments linked to simplified versions of brain models. We illustrate the capabilities of the platform with three example experiments: a Braitenberg task implemented on a mobile robot, a sensory-motor learning task based on a robotic controller, and a visual tracking embedding a retina model on the iCub humanoid robot. These use-cases allow to assess the applicability of the Neurorobotics Platform for robotic tasks as well as in neuroscientific experiments",Connecting Artificial Brains to Robots in a Comprehensive Simulation Framework: The Neurorobotics Platform,,'Frontiers Media SA',10.3389/fnbot.2017.00002,,core
390020517,2017-12-25T00:00:00,"The economic-legal aspects of the state and trends of the Internet-based technologies (IP) technology, the place of intellectual property in it are considered. It is shown that the Internet of Things creates conditions for the emergence of a synergetic effect from the combination of possibilities of artificial intelligence, cloud computing, set of sensors, mathematical algorithms for processing large data (Big Data), robotic devices of various purposes, data transmission systems (Internet), which allows to provide various services and perform various work with or without the participation of people. The role of the state in promoting the development of IP, the existing problems and ways of their solution are shown. Many governments in recent years are taking measures to analyze the state of affairs with the introduction of IP technologies, the localization of problems and threats that may or may occur in the future in order to formulate a common strategy for the development of industry for the production of IP technologies and their application in various sectors of the economy and public life. The patent landscape of the IP is analyzed, the most productive companies and inventors of IP are discovered, the dynamics of patenting in the IP environment, the value of patents, patent research problems are shown. The problems of intellectual property protection in the sphere of IP, in particular, copyright, inventions, trademarks, commercial secrets, information security are considered. The intellectual potential and untapped potential of Ukraine in the development of IP technologies are considered. It is concluded that in the widespread use of IP technologies, there is a significant potential for increasing the efficiency of any type of human activity. It concerns the real economy, industry and agriculture, health care, public administration, education, financial turnover, etc. The development of IP technologies is the most powerful stimulating factor in the innovative development of nanotechnologies, microelectronics, semiconductor technologies, microiminating of executive devices, telecommunications, radio technologies, software computing, robotics, and moreРассмотрены экономико-правовые аспекты состояния и тенденций развития технологий Интернета вещей (ИВ), места в нем интеллектуальной собственности. Показано, что ИВ создает условия для появления синергетического эффекта от сочетания возможностей искусственного интеллекта, облачных вычислений, множества сенсоров, математических алгоритмов обработки больших данных (Big Data), роботизированных устройств различного назначения, систем передачи данных (сети Интернет), что позволяет предоставлять разнообразные услуги и осуществлять различные работы с участием или без участия людей. Показана роль государства в содействии развитию ИВ, существующие проблемы и пути их решения. Правительства многих стран в последнее время принимают меры по анализу состояния дел с внедрением ИВ-технологий, локализации проблем и угроз, имеющих место или могущих возникнуть в будущем, с целью формирования общей стратегии развития промышленности производства технологий ИВ и их применение в различных секторах экономики и общественной жизни. Проанализированы патентный ландшафт ИВ, выявлены наиболее продуктивные компании и изобретатели ИВ, показана динамика патентования в среде ИВ, ценность патентов, проблемы патентного поиска. Рассмотрены проблемы охраны интеллектуальной собственности в сфере ИВ, в частности, авторских прав, изобретений, торговых марок, коммерческой тайны, информационной безопасности. Рассмотрены интеллектуальный потенциал и неиспользованные возможности Украины в развитии технологий ИВ. Делается вывод, что в широком применении технологий ИВ заложен значительный потенциал повышения эффективности любого вида человеческой деятельности. Это касается сферы реальной экономики, промышленности и сельского хозяйства, системы здравоохранения, государственного управления, образования, финансового оборота и т. п. Развитие технологий ИВ является мощным стимулирующим фактором инновационного развития нанотехнологий, микроэлектроники, полупроводниковых технологий, микроминиатюризации исполнительных устройств, телекоммуникаций, радиотехнологий, программных вычислительных средств, робототехники и многого другого.Розглянуто економіко-правові аспекти стану та тенденцій розвитку технологій Інтернет речей (ІР), місця в ньому інтелектуальної власності. Показано роль держави у сприянні розвитку ІР, існуючі проблеми та шляхи їх вирішення. Проаналізовано патентний ландшафт ІР, виявлені найбільш продуктивні компанії та винахідники ІР, показано динаміку патентування в середовищі ІР, цінність патентів, проблеми патентного пошуку. Визначено проблеми охорони інтелектуальної власності у сфері ІР, зокрема, авторських прав, винаходів, торгових марок, комерційної таємниці, інформаційної безпеки. Розглянуто інтелектуальний потенціал та невикористані можливості України у розвитку технологій ІР.Робиться висновок, що у широкому застосуванні технологій ІР закладено значний потенціал підвищення ефективності економіки",ІНТЕЛЕКТУАЛЬНА ВЛАСНІСТЬ В СИСТЕМІ ІНТЕРНЕТ РЕЧЕЙ: ЕКОНОМІКО-ПРАВОВИЙ АСПЕКТ,,Науково-дослідний інститут інтелектуальної власності НAПрН України,,,core
29567940,2017-08-16T00:00:00,"One of the most fundamental problems when designing controllers for dynamic
systems is the tuning of the controller parameters. Typically, a model of the
system is used to obtain an initial controller, but ultimately the controller
parameters must be tuned manually on the real system to achieve the best
performance. To avoid this manual tuning step, methods from machine learning,
such as Bayesian optimization, have been used. However, as these methods
evaluate different controller parameters on the real system, safety-critical
system failures may happen. In this paper, we overcome this problem by
applying, for the first time, a recently developed safe optimization algorithm,
SafeOpt, to the problem of automatic controller parameter tuning. Given an
initial, low-performance controller, SafeOpt automatically optimizes the
parameters of a control law while guaranteeing safety. It models the underlying
performance measure as a Gaussian process and only explores new controller
parameters whose performance lies above a safe performance threshold with high
probability. Experimental results on a quadrotor vehicle indicate that the
proposed method enables fast, automatic, and safe optimization of controller
parameters without human intervention.Comment: IEEE International Conference on Robotics and Automation, 2016. 6
  pages, 4 figures. A video of the experiments can be found at
  http://tiny.cc/icra16_video . A Python implementation of the algorithm is
  available at https://github.com/befelix/SafeOp",Safe Controller Optimization for Quadrotors with Gaussian Processes,http://arxiv.org/abs/1509.01066,'Institute of Electrical and Electronics Engineers (IEEE)',10.1109/ICRA.2016.7487170,,core
199656793,31/12/2017,"본 연구의 목적은 인공지능을 비롯한 기술 변화, 그로 인하여 야기되는 노동시장 변화에 대응하기 위하여 고교 단계 직업교육이 변화해야할 방향을 모색하는 것이다.1. Research Purpose

Discussions about the rapid changes in society caused by  drsatic technological changes are vigorous. Currently, vocational education and training in Korea focuses on the development and application of national competence standards and the expansion of work-based learning. This study began with the question ""Is the direction of these high school vocational education policies compatible with the coming changes?"" The purpose of this study is to consider how vocational high school education should respond to technological change. 

2. Research Method

In order to find out the future direction of vocational high school education, we reviewed the existing discussions about technology change, job change, change in demand on competences and education change in the artificial intelligence era. 
In this study, 43 teachers engaged participated in a future workshop. The workshop was conducted three times. During the workshop, we recorded the discussions and conducted text mining and qualitative analysis.
Two surveys were conducted for teachers who participated in the workshop. We analyzed the teachers' perceptions about the direction and purpose of the ideal vocational high school education and analyzed the changes before and after the workshop. One month after the end of the workshop, teachers were asked to evaluate the future orientation of the current policy.

3. Technological Change, Job Change and Necessary Competences in the Era of Artificial Intelligence

We reviewed the development process of artificial intelligence technology and social changes due to technological change such as increasing dependence of task on machinery, platform-based industrial development, changes in production and consumption patterns, political change, and cultural change.
Several studies(WEF, 2016; Grace et al., 2017; Oh, Young et al., 2016; Kim, Seum-woo) differ in methodology and it is not appropriate to concretely synthesize the results of the research. There may be disagreements as to the magnitude and speed of the disappearance of jobs, but the vocational world will be largely reorganized in sooner or later. Jobs expected to have a relatively high possibility of replacement by machines were predicted as transportation, office and administrative support, service, sales, manufacturing, and construction. On the other hand, education, law, arts, social services, management and business, finance, health and computer related jobs are less likely to be automated.
Future Jobs changes involve not only the increase or decrease of jobs, but also the change in employment patterns. Flexible employment is expected to expand and short-term employment will increase. With the development of platform-based services, non-typical employment such as platform workers is expected to continue to expand.
As the future competences, problem solving ability, interpersonal ability (social intelligence), high level cognitive ability (agile thinking ability), creativity (design thinking) and communication ability were emphasized. These competencies are already emphasized as core competencies. Media competencies, virtual cooperation competencies, and digital competencies (data thinking) are added in recognition of technological change. We also considered Gardner's emphasis on the five minds for the future.
Given that there is more potential for future developments in jobs that require processes and strategies that can not be prescribed in advance, current vocational high school education policies that emphasize concrete and explicit skills can be seen as contrary to this trend.
We reviewed future prospects for education and examples of applying the latest technologies to education, and highlighted the following issues. First, the value of knowledge is still important in school education. Second, if artificial intelligence technology is actively used, it is more likely to realize the ideal of individualized customized learning. Third, we paid attention to the possibility that the application of advanced technology could be operated in a way that equality in education, respect for diversity, and social integration through it. Fourth, we need to pay attention to the advantages and limitations of hybrid education through MOOC. Finally, The teacher needs to develop a wide range of competencies, and changes in the curriculum of teacher training are needed.


4. Teachers' Perception on Future Society and Future Vocational high school Education

In order for teachers to visualize the future of vocational high school education, vision of future society should be premised. A total of 43 teachers participated in the future workshop, composed of 4 scenarios where teachers could experience one of them. 
The resuts of text mining on the teachers' narratives in the future workshop were presented for each scenarios. In all scenarios, ‘human’ is a major concept.  In blue society, there is a concept of ‘community’ and ‘environment friendly’, whereas in orange society, there is the concept of ‘individual’ and ‘polarization’. It is blue society and orange society that ‘life’ is closely related to ‘human’, an ‘work’ is especially emphasized only in orange society. In orange society, ‘work’ is closely connected with ‘human’and ‘robot’.
Teachers mentioned as contents of the future vocational high school education most frequently about liberal arts and personality education, reflection on human and life, emotional education, value and ethics education, and basic vocational compatences. There are teachers who have emphasized technical education, but it has also been predicted that the importance of technical education for specific functions will be lowered. Teachers envisioned that in future vocational education, individualized education reflecting the needs and characteristics of students will be carried out and high technology such as virtual reality, big data and robot technology will be applied to vocational high school education.
Teachers predict that future teachers will act as guiders, assistants, coaches, facilitators, coordinators, life designers, career guiders, and counselors. Teachers are predicting that the status of teachers will be lowered and their roles will be reduced in the future. 
There was a change in the awareness of the purpose of vocational high school education for teachers after the workshop. First, the tendency to respond to the needs of specific companies through specific skills acquisition and to emphasize employment after graduation has decreased. Instead, there has been a stronger agreement on broader skills, transferable capabilities, holistic development of students and access to broader career choices.
A month after the workshop, a questionnaire was sent to teachers to evaluate current vocational high school education policy. Teachers recognized that apprenticeship program, NCS based curriculum, and work-based training were not future oriented. 

5. Direction of 2030 Vocational High School Education

□ Moving focus from human resources to future competitiveness of students 
Instead of focusing too much on the need of the labor market, it is necessary to focus more on strengthening students' competencies in vocational high school education, so that they can adapt successfully to an uncertain future and live in a meaningful connection with society.

□ Turning in a direction to provide a wider experience than a specific skills
The NCS approach to break down job units into observable sub-elements emphasizes working in a predictable manner. Jobs to which this approach is applied is a job in which automation in the future can occur most efficiently. Competitive advantage in the current labor market, not only in the future, is to creatively and autonomously improve work and create new value. To help students become competitive in a changing future society, it is necessary to mitigate the current rigid curriculum and regime represented by NCS in  vocational high school education.

□ Transition from job preparation institutions to complex functional education institutions
It should be possible to set up the purpose and function of  vocational education more comprehensive and to provide flexible programs for various purposes. 

□ From basic vocational competences to conceptual thinking and higher mental functions
Emphasis on basic vocational competences in vocational high school education could imply a risk of accepting a standardized and fragmented approach, comforting that it is developing a very useful ability. Despite the fact that basic vocational competences are developed through various curricular activities, but they themselves can not be replaced with the purpose of the academic subjects. It is essential to learn conceptual and theoretical knowledge in school education so that they can understand the principles and apply them to newly developed situations.

□ Prioritization of cutting-edge teaching techniques 
Vocational high school policy has been changed in its direction whenever the regime changes. Policy was decided and implemented unilaterally without proper discussion. Now, it is necessary to look at vocational education as an object to be invested. It is necessary to push forward the high-technology of high school vocational education so that the most advanced technologies can be applied to the vocational high school education scene.

□ Meister and Makerspace
It is necessary to build makerspaces that provide the spaces where creativity and voluntary function can be demonstrated and to guide the path as a Meister.

□ From equality of access to equality of consequence and social integration
In high school vocational education, it is necessary to clearly set the minimum standards for individuals to live happily and healthily in future society, and to convert them into the direction of encouraging educational resources to reach all students. This emphasizes the equality of educational outcomes, not merely the equality of educational opportunities. Decisions should be made to prioritize resources so that students may be able to graduate from high school while meeting minimum standards by strengthening differentiated programs.

□ Change in teacher placement and teacher training
Teacher’s role should change drastically in order to lead future change and adapt to social change. Teachers should be proficient in technology-based teaching and learning methods for constructing class contents and supporting and evaluating learners using various technologies.

□ Build a platform for social consensus in vocational high school education
In the process of change of high school vocational education policy, a few experts or policy makers tried to make a large system change without sufficient consideration. I would like to propose a system for leading and implementing and seeking change and development of high school vocational education for the future. This proposal should be a critical review of the current situation and an active response to the future, and should be a process involving all stakeholders in vocational high school education.요약       ix

제1장 서 론       1
	제1절 연구 필요성과 목적       3
	제2절 연구 내용       7
	제3절 연구 방법       8

제2장 인공지능 기술 변화와 사회 변화       15
	제1절 인공지능 기술 변화       19
	제2절 기술 변화로 인한 사회 변화       38

제3장 인공지능 시대 직업 변화와 필요 역량       47
	제1절 자동화와 직업 변화       49
	제2절 미래 직업세계에서 요구되는 역량       57
	제3절 소결 및 시사점       69

제4장 기술 변화에 따른 교육 변화       79
	제1절 미래 학교교육에 대한 전망       82
	제2절 기술 변화가 반영된 교육 변화       87
	제3절 소결 및 시사점       90

제5장 미래사회와 직업교육에 대한 교사 인식       97
	제1절 미래 워크숍       99
	제2절 특성화고 교사들의 미래사회에 대한 인식       112
	제3절 미래사회의 고교 직업교육에 대한 교사 전망       125
	제4절 소결 및 시사점       133

제6장 고교 직업교육의 목적과 정책에 대한 교사 인식       139
	제1절 직업교육 목적에 대한 교사 인식       142
	제2절 주요 직업교육 정책 미래지향성에 대한 교사 평가       153
	제3절 소결 및 시사점       160

제7장 결론 및 제언       163
	제1절 주요 결론       165
	제2절 제언: 2030 고교 직업교육의 방향       168

SUMMARY       183

참고문헌       193

부록       207
	 미래 워크숍 진행자 역할 안내       209
	 미래 워크숍 1차 워크시트(미래사회)       212
	 미래 워크숍 2차 워크시트(미래 직업교육)       214
	 선호 미래사회/실현 가능성 설문조사       217
	 특성화고 교육의 목표에 대한 인식(사전, 사후)       219
	 미래 워크숍 만족도 조사       221
	 특성화고 정책의 미래지향성 설문조사       222
	 미래사회 시나리오별 텍스트마이닝 결과       22",Vocational educationin the Artificial Intelligence Era,,,,,core
88014109,2016-11-01T00:00:00Z,"Embodied artificial cognitive systems such as autonomous robots or intelligent observers connect cognitive processes to sensory and effector systems in real time. Prime candidates for such embodied intelligence are neurally inspired architectures. While components such as forward neural networks are well established, designing pervasively autonomous neural architectures remains a challenge. This includes the problem of tuning the parameters of such architectures so that they deliver specified functionality under variable environmental conditions and retain these functions as the architectures are expanded. The scaling and autonomy problems are solved, in part, by dynamic field theory (DFT), a theoretical framework for the neural grounding of sensorimotor and cognitive processes. In this paper, we address how to efficiently build DFT architectures that control embodied agents and how to tune their parameters so that the desired cognitive functions emerge while such agents are situated in real environments. In DFT architectures, dynamic neural fields or nodes are assigned dynamic regimes, that is, attractor states and their instabilities, from which cognitive function emerges. Tuning thus amounts to determining values of the dynamic parameters for which the components of a DFT architecture are in the specified dynamic regime under the appropriate environmental conditions. The process of tuning is facilitated by the software framework cedar, which provides a graphical interface to build and execute DFT architectures. It enables to change dynamic parameters online and visualize the activation states of any component while the agent is receiving sensory inputs in real-time. Using a simple example, we take the reader through the workflow of conceiving of DFT architectures, implementing them on embodied agents, tuning their parameters, and assessing performance while the system is coupled to real sensory inputs",Developing dynamic field theory architectures for embodied cognitive systems with cedar,,Frontiers Media S.A.,10.3389/fnbot.2016.00014/full,"[{'title': None, 'identifiers': ['issn:1662-5218', '1662-5218']}]",core
103643389,11/01/2016,"The automatic design of controllers for mobile robots usually requires two stages. In the first stage, sensorial data are preprocessed or transformed into high level and meaningful values of variables which are usually defined from expert knowledge. In the second stage, a machine learning technique is applied to obtain a controller that maps these high level variables to the control commands that are actually sent to the robot. This paper describes an algorithm that is able to embed the preprocessing stage into the learning stage in order to get controllers directly starting from sensorial raw data with no expert knowledge involved. Due to the high dimensionality of the sensorial data, this approach uses Quantified Fuzzy Rules (QFRs), that are able to transform low-level input variables into high-level input variables, reducing the dimensionality through summarization. The proposed learning algorithm, called Iterative Quantified Fuzzy Rule Learning (IQFRL), is based on genetic programming. IQFRL is able to learn rules with different structures, and can manage linguistic variables with multiple granularities. The algorithm has been tested with the implementation of the wall-following behavior both in several realistic simulated environments with different complexity and on a Pioneer 3-AT robot in two real environments. Results have been compared with several well-known learning algorithms combined with different data preprocessing techniques, showing that IQFRL exhibits a better and statistically significant performance. Moreover, three real world applications for which IQFRL plays a central role are also presented: path and object tracking with static and moving obstacles avoidance",Learning Fuzzy Controllers in Mobile Robotics with Embedded Preprocessing,,,,,core
103997624,13/08/2016,"A significant goal of the yearly progression of RoboCup robotics is the improvement of bipedal locomotion and stabil-isation. Such bipeds are expected to execute many different movement behaviours and switch between them seamlessly without falling over – the type of responsiveness expected in playing a game of soccer. We explore an approach to robot self-stabilisation using multi-goal Reinforcement Learning. This approach is applied to learn the side-to-side (lateral) movement of a bipedal robot in simulation. Applied to the Naos, this allows for self-stabilisation without the need for difficult hard-coded solutions in a variety of different behaviours, including: changing sup-port feet at different frequencies; standing upright; standing on either foot; and switching between these behaviours. These behaviours are then stable and able to respond optimally to dynamic disturbances and when switching. II. IMPLEMENTATION We begin by defining a robot model, including as much accuracy in the links, joints, masses, etc. as possible – an example of this is seen in Figure 1. We define the robot stabilisation state and establish cost functions in the form of goal behaviours (such as standing upright, for example), and state transitions as random action exploration, able to automatically identify the system model (i.e. we do not assume an inverted pendulum model). The reinforcement learning then learns the actions required for optimal stabilisation of given behaviours, as well as for the transition between behaviours. These actions are used in the creation of policies, which define the action required for each possible state of the robot. Some simplified example policies can be seen in Figure 2. The policy tables are then interpreted by the real-world Nao robots, allowing them to mimic the behaviour seen in the simulation by actuating joints to follow the optimal action sequence. III. DEMONSTRATIO",Stability Control through Machine Learned,,,,,core
159130268,2017-01-01T00:00:00,"In this presentation, we report the results of applying a binarised Convolutional Neural Network (CNN) and a Field Programmable Gate Array (FPGA) for image-based object recognition. While the demand rises for robots with robust object recognition implemented with Neural Networks, a trade-off between data processing rate and power consumption persists. Some applications utilise GPGPU (General Purpose computing on Graphics Processing Units), which results in high power consumption thus undesirable for embedded systems, while the others communicate with cloud computers to minimise computational resources at the clients’ side, i.e. robots, raising another concern that the robots are unable to perform object recognition without the servers and network connections. To overcome these difficulties, we propose an embedded object recognition system implemented with a binarised CNN and an FPGA. FPGAs consist of a matrix of reconfigurable logic gates allowing parallel computing which befits most image processing algorithms such as the CNN. We train the binarised CNN on one of our datasets that contain images of several kinds of food and beverages. The results of the experiments show that the binarised CNN with an FPGA maintains high accuracy as well as real-time computation, suggesting that the proposed system is suitable for robots to perform their tasks in a real-world environment without needing to communicate with a server",FPGA-enabled binarised convolutional neural networks toward real-time embedded object recognition system,https://core.ac.uk/download/159130268.pdf,,,,core
268448721,2016-12-01T08:00:00,"With foresight into the state of the wireless channel, a robot can make various optimization decisions with regards to routing packets, planning mobility paths, or switching between diverse radios. However, the process of predicting link quality (LQ) is nontrivial due to the streaming and dynamic nature of radio wave propagation, which is complicated by robot mobility. Due to robot movement, the wireless propagation environment can change considerably in terms of distance, obstacles, noise, and interference. Therefore, LQ must be learned and regularly updated while the robot is online. However, the existing fuzzy-based models for assessing LQ are non-adaptable due to the absence of any learning mechanism. To address this issue, we introduce a fuzzy-based prediction model designed for the efficient online and incremental learning of LQ. The unique approach uses fuzzy logic to infer LQ based on the collective output from a series of offset classifiers and their posterior probabilities. In essence, the proposed model leverages machine learning for extracting the underlying functional relationship between the input and output variables, but deeper inferences are made from the output of the learning algorithms using fuzzy logic. Wireless link data from a real-world robot network was used to compare the model with the traditional linear regression approach. The results show statistically significant improvements in three out of the six real-world indoor and outdoor environments where the robot operated. Additionally, the novel approach offers a number of other benefits, including the flexibility to use fuzzy logic for model tuning, as well as the ability to make implementation efficiencies in terms of parallelization and the conservation of labeling resources",A Fuzzy-Based Machine Learning Model for Robot Prediction of Link Quality,,USMA Digital Commons,,,core
163024812,2016-12-05T00:00:00,"International audienceOur demonstration presents an open-source hardware and software platform which allows non-roboticistsresearchers to conduct machine learning experiments to benchmark algorithms for autonomous explorationand active learning. In particular, in addition to showing the general properties of the platform such asits modularity and usability, we will demonstrate the online functioning of a particular algorithm whichallows efficient learning of multiple forward and inverse models and can leverage information from humanguidance. A first aspect of our demonstration is to illustrate the ease of use of the 3D printed low-costPoppy humanoid robotic platform, that allows non-roboticists to quickly set up and program roboticexperiments. A second aspect is to show how the Explauto library allows systematic comparison andevaluation of active learning and exploration algorithms in sensorimotor spaces, through a Python API toselect already implemented exploration algorithms. The third idea is to showcase Active Model Babbling,an efficient exploration algorithm dynamically choosing which task/goal space to explore and particulargoals to reach, and integrating social guidance from humans in real time to drive exploration towardsparticular objects or actions.[Forestier and Oudeyer, 2016] Forestier, S. and Oudeyer, P.-Y. (2016). Modular active curiosity-driven discovery oftool use. In 2016 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Daejeon, Korea.[Lapeyre et al., 2014] Lapeyre, M., Rouanet, P., Grizou, J., Nguyen, S., Depraetre, F., Le Falher, A., and Oudeyer,P.-Y. (2014). Poppy Project: Open-Source Fabrication of 3D Printed Humanoid Robot for Science, Educationand Art. In Digital Intelligence 2014, page 6, Nantes, France.[Moulin-Frier et al., 2014] Moulin-Frier, C., Rouanet, P., Oudeyer, P.-Y., and others (2014). Explauto: an open-source Python library to study autonomous exploration in developmental robotics. In ICDL-Epirob-InternationalConference on Development and Learning, Epirob","Autonomous exploration, active learning and human guidance with open-source Poppy humanoid robot platform and Explauto library",https://core.ac.uk/download/163024812.pdf,HAL CCSD,,,core
84092093,2017-06-20T00:00:00,"This paper addresses the design and implementation of complex Reinforcement
Learning (RL) behaviors where multi-dimensional action spaces are involved, as
well as the need to execute the behaviors in real-time using robotic platforms
with limited computational resources and training times. For this purpose, we
propose the use of decentralized RL, in combination with finite support basis
functions as alternatives to Gaussian RBF, in order to alleviate the effects of
the curse of dimensionality on the action and state spaces respectively, and to
reduce the computation time. As testbed, a RL based controller for the in-walk
kick in NAO robots, a challenging and critical problem for soccer robotics, is
used. The reported experiments show empirically that our solution saves up to
99.94% of execution time and 98.82% of memory consumption during execution,
without diminishing performance compared to classical approaches.Comment: Accepted in the RoboCup Symposium 2017. Final version will be
  published at Springe","Toward Real-Time Decentralized Reinforcement Learning using Finite
  Support Basis Functions",http://arxiv.org/abs/1706.06695,,,,core
103609730,08/01/2016,"Abstract — Planning is becoming increasingly prevalent as a tool for high-level reasoning in real-world robotics systems. This paper discusses the implementation of a high-level ‘mission planner ’ that utilises AI planning techniques to find initial plans for a fleet of robots acting in a manufacturing factory. The paper introduces the system architecture, and then focuses on the ROS-based mission planning component, which requires the translation of low-level robot ‘skills ’ and a world model to a high-level planning domain. This paper also introduces a new algorithm for decomposition-based planning that can find ‘balanced ’ plans in large multi-robot domains where current state-of-the-art techniques fail. I",Mission Planning for a Robot Factory Fleet,,,,,core
109241338,10/10/2016,"Abstract: Designing and developing software for autonomous robot control system is a challenging task. Issues related to real-time control, embedded system and artificial intelligence are involved in the software development process. This type of software must be developed with proper software methodology or well-define development process in order to increase the productivity and quality of the software design and software products. This paper presents a review of two real-time software development methodologies and compares their suitability for developing real-time control software for a wall-climbing robot under development at Universiti Teknologi Malaysia (UTM)",A Review of Real-Time Software Engineering Methodologies for Developing a Wall-Climbing Wobot Control Firmware,,,,,core
132172874,2017-11-01T00:00:00Z,"Abstract Background In maize, silks are hundreds of filaments that simultaneously emerge from the ear for collecting pollen over a period of 1–7 days, which largely determines grain number especially under water deficit. Silk growth is a major trait for drought tolerance in maize, but its phenotyping is difficult at throughputs needed for genetic analyses. Results We have developed a reproducible pipeline that follows ear and silk growths every day for hundreds of plants, based on an ear detection algorithm that drives a robotized camera for obtaining detailed images of ears and silks. We first select, among 12 whole-plant side views, those best suited for detecting ear position. Images are segmented, the stem pixels are labelled and the ear position is identified based on changes in width along the stem. A mobile camera is then automatically positioned in real time at 30 cm from the ear, for a detailed picture in which silks are identified based on texture and colour. This allows analysis of the time course of ear and silk growths of thousands of plants. The pipeline was tested on a panel of 60 maize hybrids in the PHENOARCH phenotyping platform. Over 360 plants, ear position was correctly estimated in 86% of cases, before it could be visually assessed. Silk growth rate, estimated on all plants, decreased with time consistent with literature. The pipeline allowed clear identification of the effects of genotypes and water deficit on the rate and duration of silk growth. Conclusions The pipeline presented here, which combines computer vision, machine learning and robotics, provides a powerful tool for large-scale genetic analyses of the control of reproductive growth to changes in environmental conditions in a non-invasive and automatized way. It is available as Open Source software in the OpenAlea platform",A robot-assisted imaging pipeline for tracking the growths of maize ear and silks in a high-throughput phenotyping platform,,BMC,10.1186/s13007-017-0246-7,"[{'title': None, 'identifiers': ['issn:1746-4811', '1746-4811']}]",core
302886582,2017-07-24T00:00:00,"Inverse Reinforcement Learning (IRL) for path
planning enables robots to learn cost functions for difficult tasks
from demonstration, instead of hard-coding them. However,
IRL methods face practical limitations that stem from the need
to repeat costly planning procedures. In this paper, we propose
Rapidly Exploring Learning Trees (RLT∗
), which learns the cost
functions of Optimal Rapidly Exploring Random Trees (RRT∗
)
from demonstration, thereby making inverse learning methods
applicable to more complex tasks. Our approach extends
Maximum Margin Planning to work with RRT∗
cost functions.
Furthermore, we propose a caching scheme that greatly reduces
the computational cost of this approach. Experimental results
on simulated and real-robot data from a social navigation
scenario show that RLT∗
achieves better performance at lower
computational cost than existing methods. We also successfully
deploy control policies learned with RLT∗
on a real telepresence
robot",Rapidly exploring learning trees,,'Institute of Electrical and Electronics Engineers (IEEE)',10.1109/ICRA.2017.7989184,,core
141489661,2017-08-01T00:00:00,"Carbon fiber reinforced plastic (CFRP) parts for airplane components can be so huge that a single industrial robot is no longer able to handle them, and cooperating robots are required. Manual programming of cooperating robots is difficult, but with large numbers of different sized and shaped cut-pieces, it is almost impossible. This paper presents an automated production system consisting of a camera for the precise detection of the position of each cut-piece and a collision-free path planner which can dynamically react to different positions for the transfer motions. The path is planned for multiple robots adhering to motion constrains, such as the requirement that the textile cut-piece must form a catenary which can change during transport. Different existing path planning algorithms are evaluated and compared. Additionally a technique based on machine learning has been implemented which correctly resolves redundancy for a linear axis during planning. Finally, all components are tested on a real robot system in industrial scale",Full Automatic Path Planning of Cooperating Robots in Industrial Applications,,,10.1109/coase.2017.8256157,,core
297028772,2017-01-01T00:00:00,"This paper presents a deep learning approach which evaluates accuracy and inference time speedups in

deep convolutional neural networks under various network quantizations. Quantized networks can result in

much faster inference time allowing them to be deployed in real time on an embedded system such as a robot.

We evaluate networks with activations quantized to 1, 2, 4, and 8-bits and binary weights. We found that

network quantization can yield a significant speedup for a small drop in classification accuracy. Specifically,

modifying one of our networks to use an 8-bit quantized input layer and 2-bit activations in hidden layers,

we calculate a theoretical 9.9x speedup in exchange for an F1 score decrease of just 3.4% relative to a full

precision implementation. Higher speedups are obtainable by designing a network architecture containing a

smaller proportion of the total multiplications within the input layer",Evaluating Quantized Convolutional Neural Networks for Embedded Systems,https://core.ac.uk/download/297028772.pdf,Irish Pattern Recognition & Classification Society,,,core
290312192,2016,"This paper presents control framework based
on multi-agent reinforcement approach for building
intellectual steering software for multi-wheeled mobile
robots. The framework uses modified reinforcement
learning approach based on special multi-agent structure
with virtual leader. The framework application example
will be shown with real multi-wheeled mobile platform.
The experiments were performed in simulation
environment with accurate virtual model",Multi-agent control framework for multi-wheeled mobile platforms,,Minsk: Publishing Center of BSU,,,core
80311420,2017-01-01T00:00:00,"The use of identical robots in the RoboCup Standard Platform League (SPL) made software development the key aspect to achieve good results in competitions. In particular, the visual detection process is crucial for extracting information about the environment. In this paper, we present a novel approach for object detection and classification based on Convolutional Neural Networks (CNN). The approach is designed to be used by NAO robots and is made of two stages: image region segmentation, for reducing the search space, and Deep Learning, for validation. The proposed method can be easily extended to deal with different objects and adapted to be used in other RoboCup leagues. Quantitative experiments have been conducted on a data set of annotated images captured in real conditions from NAO robots in action. The used data set is made available for the community. © 2017, Springer International Publishing AG",A Deep Learning Approach for Object Recognition with NAO Soccer Robots,https://core.ac.uk/download/80311420.pdf,'Springer Science and Business Media LLC',10.1007/978-3-319-68792-6_33,,core
299916105,07/07/2017,"Computational Intelligence: An Introduction, Second Edition offers an in-depth exploration into the adaptive mechanisms that enable intelligent behaviour in complex and changing environments. The main focus of this text is centred on the computational modelling of biological and natural intelligent systems, encompassing swarm intelligence, fuzzy systems, artificial neutral networks, artificial immune systems and evolutionary computation.
Engelbrecht provides readers with a wide knowledge of Computational Intelligence (CI) paradigms and algorithms; inviting readers to implement and problem solve real-world, complex problems within the CI development framework. This implementation framework will enable readers to tackle new problems without any difficulty through a single Java class as part of the CI library.

Key features of this second edition include:

A tutorial, hands-on based presentation of the material.
State-of-the-art coverage of the most recent developments in computational intelligence with more elaborate discussions on intelligence and artificial intelligence (AI).
New discussion of Darwinian evolution versus Lamarckian evolution, also including swarm robotics, hybrid systems and artificial immune systems.
A section on how to perform empirical studies; topics including statistical analysis of stochastic algorithms, and an open source library of CI algorithms.
Tables, illustrations, graphs, examples, assignments, Java code implementing the algorithms, and a complete CI implementation and experimental framework.
Computational Intelligence: An Introduction, Second Edition is essential reading for third and fourth year undergraduate and postgraduate students studying CI. The first edition has been prescribed by a number of overseas universities and is thus a valuable teaching tool. In addition, it will also be a useful resource for researchers in Computational Intelligence and Artificial Intelligence, as well as engineers, statisticians, operational researchers, and bioinformaticians with an interest in applying AI or CI to solve problems in their domains.

 Check out http://www.ci.cs.up.ac.za for examples, assignments and Java code implementing the algorithms",Computational Intelligence: An Introduction,,'Wiley',,,core
85123517,2016-08-01T00:00:00,"This paper investigates three different technologies for solving a planning and scheduling problem of deploying multiple robots in a retirement home environment to assist elderly residents. The models proposed make use of standard techniques and solvers developed in AI planning and scheduling, with two primary motivations. First, to find a planning and scheduling solution that we can deploy in our real-world application. Second, to evaluate planning and scheduling technology in terms of the ``model-and-solve'' functionality that forms a major research goal in both domain-independent planning and constraint programming. Seven variations of our application are studied using the following three technologies: PDDL-based planning, time-line planning and scheduling, and constraint-based scheduling. The variations address specific aspects of the problem that we believe can impact the performance of the technologies while also representing reasonable abstractions of the real world application. We evaluate the capabilities of each technology and conclude that a constraint-based scheduling approach, specifically a decomposition using constraint programming, provides the most promising results for our application. PDDL-based planning is able to find mostly low quality solutions while the timeline approach was unable to model the full problem without alterations to the solver code, thus moving away from the model-and-solve paradigm. It would be misleading to conclude that constraint programming is ``better'' than PDDL-based planning in a general sense, both because we have examined a single application and because the approaches make different assumptions about the knowledge one is allowed to embed in a model. Nonetheless, we believe our investigation is valuable for AI planning and scheduling researchers as it highlights these different modelling assumptions and provides insight into avenues for the application of AI planning and scheduling for similar robotics problems. In particular, as constraint programming has not been widely applied to robot planning and scheduling in the literature, our results suggest significant untapped potential in doing so.California Institute of Technology. Keck Institute for Space Studie",Robots in Retirement Homes: Applying Off-the-Shelf Planning and Scheduling to a Team of Assistive Robots,https://core.ac.uk/download/85123517.pdf,'AI Access Foundation',10.1613/jair.5306,"[{'title': 'Journal of Artificial Intelligence Research', 'identifiers': ['issn:1943-5037', 'issn:1076-9757', '1076-9757', '1943-5037']}]",core
141495302,2016-01-01T00:00:00,"Simultaneous localization and mapping (SLAM) is a technique applied in artificial intelligence mobile robot for a self-exploration in numerous geographical environment. SLAM becomes fundamental research area in recent days as it promising solution in solving most of problems which related to the self-exploratory oriented artificial intelligence mobile robot field. For example, the capability to explore without any prior knowledge on environment it explores and without any human interference. The unique feature in SLAM is that the process of mapping and localization is done concurrently and recursively. Since SLAM introduction, many SLAM algorithms have been proposed to apply SLAM technique in real practice. The aim of this paper is to provide an insightful review on information background, recent development, feature, implementation and recent issue in SLAM",Review on simultaneous localization and mapping (SLAM),,Institute of Electrical and Electronics Engineers Inc.,,,core
129712012,2016-07-01T00:00:00,"This research work presents a novel Cognitive Task Planning framework for Smart Industrial Robots. The framework makes an industrial mobile manipulator robot Cognitive by applying Semantic Web Technologies. It also introduces a novel Navigation Among Movable Obstacles algorithm for robots navigating and manipulating inside a ﬁrm. 



The objective of Industrie 4.0 is the creation of Smart Factories: modular ﬁrms provided with cyber-physical systems able to strong customize products under the condition of highly ﬂexible mass-production. Such systems should real-time communicate and cooperate with each other and with humans via the Internet of Things. They should intelligently adapt to the changing surroundings and autonomously navigate inside a ﬁrm while moving obstacles that occlude free paths, even if seen for the ﬁrst time. At the end, in order to accomplish all these tasks while being eﬃcient, they should learn from their actions and from that of other agents. 



Most of existing industrial mobile robots navigate along pre-generated trajectories. They follow ectriﬁed wires embedded in the ground or lines painted on th eﬂoor. When there is no expectation of environment changes and cycle times are critical, this planning is functional. When workspaces and tasks change frequently, it is better to plan dynamically: robots should autonomously navigate without relying on modiﬁcations of their environments. Consider the human behavior: humans reason about the environment and consider the possibility of moving obstacles if a certain goal cannot be reached or if moving objects may signiﬁcantly shorten the path to it. This problem is named Navigation Among Movable Obstacles and is mostly known in rescue robotics. This work transposes the problem on an industrial scenario and tries to deal with its two challenges: the high dimensionality of the state space and the treatment of uncertainty. 



The proposed NAMO algorithm aims to focus exploration on less explored areas. For this reason it extends the Kinodynamic Motion Planning by Interior-Exterior Cell Exploration algorithm. The extension does not impose obstacles avoidance: it assigns an importance to each cell by combining the eﬀorts necessary to reach it and that needed to free it from obstacles. The obtained algorithm is scalable because of its independence from the size of the map and from the number, shape, and pose of obstacles. It does not impose restrictions on actions to be performed: the robot can both push and grasp every object. Currently, the algorithm assumes full world knowledge but the environment is reconﬁgurable and the algorithm can be easily extended in order to solve NAMO problems in unknown environments. The algorithm handles sensor feedbacks and corrects uncertainties. 



Usually Robotics separates Motion Planning and Manipulation problems. NAMO forces their combined processing by introducing the need of manipulating multiple objects, often unknown, while navigating. Adopting standard precomputed grasps is not suﬃcient to deal with the big amount of existing diﬀerent objects. A Semantic Knowledge Framework is proposed in support of the proposed algorithm by giving robots the ability to learn to manipulate objects and disseminate the information gained during the fulﬁllment of tasks. The Framework is composed by an Ontology and an Engine. The Ontology extends the IEEE Standard Ontologies for Robotics and Automation and contains descriptions of learned manipulation tasks and detected objects. It is accessible from any robot connected to the Cloud. It can be considered a data store for the eﬃcient and reliable execution of repetitive tasks; and a Web-based repository for the exchange of information between robots and for the speed up of the learning phase. No other manipulation ontology exists respecting the IEEE Standard and, regardless the standard, the proposed ontology diﬀers from the existing ones because of the type of features saved and the eﬃcient way in which they can be accessed: through a super fast Cascade Hashing algorithm. The Engine lets compute and store the manipulation actions when not present in the Ontology. It is based on Reinforcement Learning techniques that avoid massive trainings on large-scale databases and favors human-robot interactions. 



The overall system is ﬂexible and easily adaptable to diﬀerent robots operating in diﬀerent industrial environments. It is characterized by a modular structure where each software block is completely reusable. Every block is based on the open-source Robot Operating System. Not all industrial robot controllers are designed to be ROS-compliant. This thesis presents the method adopted during this research in order to Open Industrial Robot Controllers and create a ROS-Industrial interface for them",Cognitive Task Planning for Smart Industrial Robots,https://core.ac.uk/download/129712012.pdf,,,,core
323893305,2017-11-01T00:00:00,"© 2017, The Author(s). A robot agent designed to engage in real-world human–robot joint action must be able to understand the social states of the human users it interacts with in order to behave appropriately. In particular, in a dynamic public space, a crucial task for the robot is to determine the needs and intentions of all of the people in the scene, so that it only interacts with people who intend to interact with it. We address the task of estimating the engagement state of customers for a robot bartender based on the data from audiovisual sensors. We begin with an offline experiment using hidden Markov models, confirming that the sensor data contains the information necessary to estimate user state. We then present two strategies for online state estimation: a rule-based classifier based on observed human behaviour in real bars, and a set of supervised classifiers trained on a labelled corpus. These strategies are compared in offline cross-validation, in an online user study, and through validation against a separate test corpus. These studies show that while the trained classifiers are best in a cross-validation setting, the rule-based classifier performs best with novel data; however, all classifiers also change their estimate too frequently for practical use. To address this issue, we present a final classifier based on Conditional Random Fields: this model has comparable performance on the test data, with increased stability. In summary, though, the rule-based classifier shows competitive performance with the trained classifiers, suggesting that for this task, such a simple model could actually be a preferred option, providing useful online performance while avoiding the implementation and data-scarcity issues involved in using machine learning for this task",Automatically Classifying User Engagement for Dynamic Multi-party Human–Robot Interaction,,'Springer Science and Business Media LLC',10.1007/s12369-017-0414-y,,core
25027369,2016-03-02T00:00:00,"This paper presents a solution to the problem of monitoring a region of
interest (RoI) using a set of nodes that is not sufficient to achieve the
required degree of monitoring coverage. In particular, sensing coverage of
wireless sensor networks (WSNs) is a crucial issue in projects due to failure
of sensors. The lack of sensor equipment resources hinders the traditional
method of using mobile robots to move around the RoI to collect readings.
Instead, our solution employs supervised neural networks to produce the values
of the uncovered locations by extracting the non-linear relation among randomly
deployed sensor nodes throughout the area. Moreover, we apply a hybrid
backpropagation method to accelerate the learning convergence speed to a local
minimum solution. We use a real-world data set from meteorological deployment
for experimental validation and analysis",Area Coverage Under Low Sensor Density,http://arxiv.org/abs/1405.4378,'Institute of Electrical and Electronics Engineers (IEEE)',10.1109/SAHCN.2014.6990347,,core
188165729,2016-01-01T00:00:00,"Artificial Intelligence without Using Ontological Data about a Presupposed Reality. This paper introduces an original model to provide software agents and robots with the capacity of learning by interpreting regularities in their stream of sensorimotor experience rather than by exploiting data that would give them ontological information about a predefined domain. Specifically, this model pulls inspiration from : a) the movement of embodied cognition, b) the philosophy of knowledge, c) constructivist epistemology, and d) the theory of enaction. Respectively to these four influences : a) Our agents discover their environment through their body’s active capacity of experimentation. b) They do not know their environment “ as such” but only “ as they can experience it”. c) They construct knowledge from regularities of sensorimotor experience. d) They have some level of constitutive autonomy. Technically, this model differs from the traditional perception / cognition/ action model in that it rests upon atomic sensorimotor experiences rather than separating percepts from actions. We present algorithms that implement this model, and we describe experiments to validate these algorithms. These experiments show that the agents exhibit a certain form of intelligence through their behaviors, as they construct proto-ontological knowledge of the phenomena that appear to them when they observe persistent possibilities of sensorimotor experiences in time and space. These results promote a theory of artificial intelligence without ontological data about a presupposed reality. An application includes a more robust way of creating robots capable of constructing their own knowledge and goals in the real world, which could be initially unknown to them and un-modeled by their designers.Cet article propose un modèle original pour doter des agents informatiques ou des robots de la capacité d’apprendre en interprétant des régularités dans leur flux d’expériences sensorimotrices plutôt qu’en exploitant des données qui leur apporteraient des informations ontologiques sur un domaine prédéfini. Ce modèle s’inspire en particulier de : a) le courant de la cognition incarnée, b) la philosophie de la connaissance, c) l’épistémologie constructiviste, et d) la théorie de l’énaction. Respectivement à ces quatre influences : a) Nos agents découvrent leur environnement à travers les capacités expérimentales actives de leur corps. b) Ils ne connaissent pas leur environnement «en soi » mais uniquement «en ce qu’ils peuvent en faire l’expérience » . c) Ils construisent leurs connaissances à partir de régularités d’expériences sensorimotrices. d) Ils disposent d’une certaine autonomie constitutive.
Techniquement, ce modèle se distingue du modèle perception/cognition/action classique par le fait qu’il considère des expériences sensorimotrices atomiques au lieu de séparer les percepts et les actions. Nous présentons des algorithmes qui implémentent ce modèle, et décrivons des expérimentations permettant de les valider. Les expérimentations montrent que les agents exhibent une certaine forme d’intelligence dans leurs comportements en construisant une connaissance protoontologique des phénomènes qui apparaissent à eux quand ils constatent des possibilités d’expériences sensorimotrices persistantes dans l’espace et le temps. Ces résultats promeuvent une théorie de l’intelligence artificielle sans données ontologiques sur une réalité présupposée, avec, comme perspectives applicatives, des robots capables de construire leurs propres connaissances et objectifs dans le monde réel, initialement inconnu d’eux et non modélisé par leur concepteur.Georgeon Olivier, Mille Alain, Gay Simon. Intelligence artificielle sans données ontologiques sur une réalité présupposée. In: Intellectica. Revue de l'Association pour la Recherche Cognitive, n°65, 2016/1. Nouvelles approches en Robotique Cognitive. pp. 143-168",Intelligence artificielle sans données ontologiques sur une réalité présupposée,,'PERSEE Program',10.3406/intel.2016.1793,,core
83859063,2017-07-18T00:00:00,"Developing and testing algorithms for autonomous vehicles in real world is an
expensive and time consuming process. Also, in order to utilize recent advances
in machine intelligence and deep learning we need to collect a large amount of
annotated training data in a variety of conditions and environments. We present
a new simulator built on Unreal Engine that offers physically and visually
realistic simulations for both of these goals. Our simulator includes a physics
engine that can operate at a high frequency for real-time hardware-in-the-loop
(HITL) simulations with support for popular protocols (e.g. MavLink). The
simulator is designed from the ground up to be extensible to accommodate new
types of vehicles, hardware platforms and software protocols. In addition, the
modular design enables various components to be easily usable independently in
other projects. We demonstrate the simulator by first implementing a quadrotor
as an autonomous vehicle and then experimentally comparing the software
components with real-world flights.Comment: Accepted for Field and Service Robotics conference 2017 (FSR 2017","AirSim: High-Fidelity Visual and Physical Simulation for Autonomous
  Vehicles",http://arxiv.org/abs/1705.05065,,,,core
83833712,2017-08-13T00:00:00,"Software testing is an important tool to ensure software quality. This is a
hard task in robotics due to dynamic environments and the expensive development
and time-consuming execution of test cases. Most testing approaches use
model-based and / or simulation-based testing to overcome these problems. We
propose model-free skill-centric testing in which a robot autonomously executes
skills in the real world and compares it to previous experiences. The skills
are selected by maximising the expected information gain on the distribution of
erroneous software functions. We use deep learning to model the sensor data
observed during previous successful skill executions and to detect
irregularities. Sensor data is connected to function call profiles such that
certain misbehaviour can be related to specific functions. We evaluate our
approach in simulation and in experiments with a KUKA LWR 4+ robot by
purposefully introducing bugs to the software. We demonstrate that these bugs
can be detected with high accuracy and without the need for the implementation
of specific tests or task-specific models",Autonomous Skill-centric Testing using Deep Learning,http://arxiv.org/abs/1703.00835,,10.1109/iros.2017.8202143,,core
84889551,2017-07-19T00:00:00,"Bled eConference, organized by University of Maribor, Faculty of Organizational Sciences, has been shaping electronic interactions since 1988 and is celebrating its 30th anniversary in 2017. Bled eConference is the oldest, most traditional and well renowned conference in the field. It attracts speakers and delegates from business, government, information technology providers and universities and is the major venue for researchers working in all aspects of “e”. The theme of this year’s conference is dedicated to “Digital Transformation – From Connecting Things to Transforming Our Lives”. 

The evolution of digital technologies and solutions (e.g. Internet of things, mobile technology, social media, cloud and high performance computing, artificial intelligence and advanced machine learning, virtual and augmented reality, big data & big data analytics, service architecture, digital technology platforms, 3D printing, robotics etc. …) has significantly impacted on the way how business is conducted and had big implications on our lives. The nowadays digital economy calls for transformation of businesses, governments, education and societies as whole. It also calls for enabling policies and politics for cross border and global digital business. 

Digital transformation is reflected in the organizations’ ability of comprehensive transformation of business activities, processes, competencies and business models in order to take advantage of digital technologies. Digital technologies require changes in our mind set, culture and functioning and have strategic impact on both the organization itself and its business ecosystems. 

In this year’s conference, we address various aspects of digital transformation and provide directions and guidelines for organizations to overcome challenges on their way of successful digital transformation. Themes covered in the papers of these proceedings are focused to digital transformation challenges, opportunities and cesses, furthermore to business model innovation, social media and big data analytics implementation, e-health, digital wellness and wellbeing experiences, new applications and organizational models, and novel approaches and cases in education in digital economy",30th Bled eConference: Digital transformation - from connecting things to transforming our lives,https://core.ac.uk/download/84889551.pdf,'University of Maribor',10.18690/978-961-286-043-1,,core
127503327,2017,"Background: In maize, silks are hundreds of filaments that simultaneously emerge from the ear for collecting pollen over a period of 1–7 days, which largely determines grain number especially under water deficit. Silk growth is a major trait for drought tolerance in maize, but its phenotyping is difficult at throughputs needed for genetic analyses.
Results: We have developed a reproducible pipeline that follows ear and silk growths every day for hundreds of plants, based on an ear detection algorithm that drives a robotized camera for obtaining detailed images of ears and silks. We first select, among 12 whole-plant side views, those best suited for detecting ear position. Images are segmented, the stem pixels are labelled and the ear position is identified based on changes in width along the stem. A mobile camera is then automatically positioned in real time at 30 cm from the ear, for a detailed picture in which silks are identified based on texture and colour. This allows analysis of the time course of ear and silk growths of thousands of plants. The pipeline was tested on a panel of 60 maize hybrids in the PHENOARCH phenotyping platform. Over 360 plants, ear position was correctly estimated in 86% of cases, before it could be visually assessed. Silk growth rate, estimated on all plants, decreased with time consistent with literature. The pipeline allowed clear identification of the effects of genotypes and water deficit on the rate and duration of silk growth.
Conclusions: The pipeline presented here, which combines computer vision, machine learning and robotics,
provides a powerful tool for large-scale genetic analyses of the control of reproductive growth to changes in environmental conditions in a non-invasive and automatized way. It is available as Open Source software in the OpenAlea platform",A robot-assisted imaging pipeline for tracking the growths of maize ear and silks in a high-throughput phenotyping platform,,,10.1186/s13007-017-0246-7,,core
145159196,2017-01-01T00:00:00,"18 pages, 16 figures, 3 tables, 6 pseudocodes/algorithms, video at https://youtu.be/IqtyHFrb3BUInternational audienceThe high probability of hardware failures prevents many advanced robots (e.g., legged robots) from being confidently deployed in real-world situations (e.g., post-disaster rescue). Instead of attempting to diagnose the failures, robots could adapt by trial-and-error in order to be able to complete their tasks. In this situation, damage recovery can be seen as a Reinforcement Learning (RL) problem. However, the best RL algorithms for robotics require the robot and the environment to be reset to an initial state after each episode, that is, the robot is not learning autonomously. In addition, most of the RL methods for robotics do not scale well with complex robots (e.g., walking robots) and either cannot be used at all or take too long to converge to a solution (e.g., hours of learning). In this paper, we introduce a novel learning algorithm called "" Reset-free Trial-and-Error "" (RTE) that (1) breaks the complexity by pre-generating hundreds of possible behaviors with a dynamics simulator of the intact robot, and (2) allows complex robots to quickly recover from damage while completing their tasks and taking the environment into account. We evaluate our algorithm on a simulated wheeled robot, a simulated six-legged robot, and a real six-legged walking robot that are damaged in several ways (e.g., a missing leg, a shortened leg, faulty motor, etc.) and whose objective is to reach a sequence of targets in an arena. Our experiments show that the robots can recover most of their locomotion abilities in an environment with obstacles, and without any human intervention",Reset-free Trial-and-Error Learning for Robot Damage Recovery,,'Elsevier BV',10.1016/j.robot.2017.11.010,,core
196789263,2017-01-01T00:00:00,"This paper presents a deep learning approach which evaluates accuracy and inference time speedups in

deep convolutional neural networks under various network quantizations. Quantized networks can result in

much faster inference time allowing them to be deployed in real time on an embedded system such as a robot.

We evaluate networks with activations quantized to 1, 2, 4, and 8-bits and binary weights. We found that

network quantization can yield a significant speedup for a small drop in classification accuracy. Specifically,

modifying one of our networks to use an 8-bit quantized input layer and 2-bit activations in hidden layers,

we calculate a theoretical 9.9x speedup in exchange for an F1 score decrease of just 3.4% relative to a full

precision implementation. Higher speedups are obtainable by designing a network architecture containing a

smaller proportion of the total multiplications within the input layer",Evaluating Quantized Convolutional Neural Networks for Embedded Systems,,Irish Pattern Recognition & Classification Society,,,core
78292650,2016,"This paper presents control framework based
on multi-agent reinforcement approach for building
intellectual steering software for multi-wheeled mobile
robots. The framework uses modified reinforcement
learning approach based on special multi-agent structure
with virtual leader. The framework application example
will be shown with real multi-wheeled mobile platform.
The experiments were performed in simulation
environment with accurate virtual model",Multi-agent control framework for multi-wheeled mobile platforms,,Minsk: Publishing Center of BSU,,,core
200965041,2017-01-01T00:00:00,"This paper addresses the design and implementation of complex Reinforcement Learning (RL) behaviors where multi-dimensional action spaces are involved, as well as the need to execute the behaviors in real-time using robotic platforms with limited computational resources and training times. For this purpose, we propose the use of decentralized RL, in combination with finite support basis functions as alternatives to Gaussian RBF, in order to alleviate the effects of the curse of dimensionality on the action and state spaces respectively, and to reduce the computation time. As testbed, a RL based controller for the in-walk kick in NAO robots, a challenging and critical problem for soccer robotics, is used. The reported experiments show empirically that our solution saves up to 99.94% of execution time and 98.82% of memory consumption during execution, without diminishing performance compared to classical approaches",Toward real-time decentralized reinforcement learning using finite support basis functions,,'Springer Science and Business Media LLC',10.1007/978-3-030-00308-1_8,,core
224990694,2016,"The effort to develop an electronic skin is highly motivated by many application domains namely robotics, biomedical instrumentations, and replacement prosthetic devices. Several e-skin systems have been proposed recently and have demonstrated the need of an embedded electronic system for tactile data processing either to mimic the human skin or to respond to the application demands. Processing tactile data requires efficient methods to extract meaningful information from raw sensors data.



In this framework, our goal is the development of a dedicated embedded electronic system for electronic skin. The embedded electronic system has to acquire the tactile data, process and extract structured information. Machine Learning (ML) represents an effective method for data analysis in many domains: it has recently demonstrated its effectiveness in processing tactile sensors data.



This paper presents an embedded electronic system based on dedicated hardware implementation for electronic skin systems. It provides a Tensorial kernel function implementation for machine learning based on Tensorial kernel approach. Results assess the time latency and the hardware complexity for real time functionality. The implementation results highlight the high amount of power consumption needed for the input touch modalities classification task. Conclusions and future perspectives are also presented",Embedded Electronic System Based on Dedicated Hardware DSPs for Electronic Skin Implementation,,'Elsevier BV',10.1016/j.protcy.2016.08.007,,core
83837245,2017-03-13T00:00:00,"A new model of symbol grounding is presented, in which the structures of
natural language, logical semantics, perception and action are represented
categorically, and symbol grounding is modeled via the composition of morphisms
between the relevant categories. This model gives conceptual insight into the
fundamentally systematic nature of symbol grounding, and also connects
naturally to practical real-world AI systems in current research and commercial
use. Specifically, it is argued that the structure of linguistic syntax can be
modeled as a certain asymmetric monoidal category, as e.g. implicit in the link
grammar formalism; the structure of spatiotemporal relationships and action
plans can be modeled similarly using ""image grammars"" and ""action grammars"";
and common-sense logical semantic structure can be modeled using
dependently-typed lambda calculus with uncertain truth values. Given these
formalisms, the grounding of linguistic descriptions in spatiotemporal
perceptions and coordinated actions consists of following morphisms from
language to logic through to spacetime and body (for comprehension), and vice
versa (for generation). The mapping is indicated between the spatial
relationships in the Region Connection Calculus and Allen Interval Algebra and
corresponding entries in the link grammar syntax parsing dictionary. Further,
the abstractions introduced here are shown to naturally model the structures
and systems currently being deployed in the context of using the OpenCog
cognitive architecture to control Hanson Robotics humanoid robots",Symbol Grounding via Chaining of Morphisms,http://arxiv.org/abs/1703.04368,,,,core
90851059,2017-03-01T00:00:00Z,"Enabling touch-sensing capability would help appliances understand interaction behaviors with their surroundings. Many recent studies are focusing on the development of electronic skin because of its necessity in various application domains, namely autonomous artificial intelligence (e.g., robots), biomedical instrumentation, and replacement prosthetic devices. An essential task of the electronic skin system is to locally process the tactile data and send structured information either to mimic human skin or to respond to the application demands. The electronic skin must be fabricated together with an embedded electronic system which has the role of acquiring the tactile data, processing, and extracting structured information. On the other hand, processing tactile data requires efficient methods to extract meaningful information from raw sensor data. Machine learning represents an effective method for data analysis in many domains: it has recently demonstrated its effectiveness in processing tactile sensor data. In this framework, this paper presents the implementation of digital signal processing based on FPGAs for tactile data processing. It provides the implementation of a tensorial kernel function for a machine learning approach. Implementation results are assessed by highlighting the FPGA resource utilization and power consumption. Results demonstrate the feasibility of the proposed implementation when real-time classification of input touch modalities are targeted",Real-Time Digital Signal Processing Based on FPGAs for Electronic Skin Implementation †,,MDPI AG,10.3390/s17030558,"[{'title': None, 'identifiers': ['issn:1424-8220', '1424-8220']}]",core
80699332,2017-02-23T00:00:00,"Artificial neural systems for computation were first proposed three quarters of a century ago and the concepts developed by the pioneers still shape the field today. The first generation of neural systems was developed in the nineteen forties in the context of analogue electronics and the theoretical research in logic and mathematics that led to the first digital computers in nineteen forties and fifties. The second generation of neural systems implemented on digital computers was born in the nineteen fifties and great progress was made in the subsequent half century with neural networks being applied to many problems in pattern recognition and machine learning. Through this history there has been an interplay between biologically inspired neural systems and their implementation by engineers on digital machines. This thesis concerns the third generation of neural networks, Spiking Neural Networks, which is making possible the creation of new kinds of brain inspired computing architectures that offer the potential to increase the level of realism and sophistication in terms of autonomous machine behaviour and cognitive computing. This thesis presents the development and demonstration of a new theoretical architecture for third generation neural systems, the Integrate-and-Fire based Spiking Neural Model with extended Neuro-modulated Spike Timing Dependent Plasticity capabilities. This proposed architecture overcomes the limitation of the homosynaptic architecture underlying existing implementations of spiking neural networks that it lacks a natural spike timing dependent plasticity regulation mechanism, and this results in ‘run away’ dynamics. To overcome this ad hoc procedures have been implemented to overcome the ‘run away’ dynamics that emerge from the use of spike timing dependent plasticity among other hebbian-based plasticity rules. The new heterosynaptic architecture presented, explicitly abstracts the modulation of complex biochemical mechanisms into a simplified mechanism that is suitable for the engineering of artificial systems with low computational complexity. Neurons work by receiving input signals from other neurons through synapses. The difference between homosynaptic and heterosynaptic plasticity is that, in the former the change in the properties of a synapse (e.g. synaptic efficacy) depends on the point to point activity in either of the sending and receiving neurons, in contrast for heterosynaptic plasticity the change in the properties of a synapse can be elicited by neurons that are not necessary presynaptic or postsynaptic to the synapse in question. The new architecture is tested by a number of implementations in simulated and real environments. This includes experiments with a simulation environment implemented in Netlogo, and an implementation using Lego Mindstorms as the physical robot platform. These experiments demonstrate the problems with the traditional Spike timing dependent plasticity homosynaptic architecture and how the new heterosynaptic approach can overcome them. It is concluded that the new theoretical architecture provides a natural, theoretically sound, and practical new direction for research into the role of modulatory neural systems applied to spiking neural networks",A Heterosynaptic Spiking Neural System for the Development of Autonomous Agents,https://core.ac.uk/download/80699332.pdf,'Fraunhofer-Institut fur Materialfluss und Logistik',10.21954/ou.ro.0000bef8,,core
212690902,2017-06-15T00:00:00,"This thesis examines methods enabling autonomous systems to make active sampling and planning decisions in real time. Gaussian Process (GP) regression is chosen as a framework for its non-parametric approach allowing flexibility in unknown environments.   The first part of the thesis focuses on depth constrained full coverage bathymetric surveys in unknown environments. Algorithms are developed to find and follow a depth contour, modelled with a GP, and produce a depth constrained boundary. An extension to the Boustrophedon Cellular Decomposition, Discrete Monotone Polygonal Partitioning is developed allowing efficient planning for coverage within this boundary. Efficient computational methods such as incremental Cholesky updates are implemented to allow online Hyper Parameter optimisation and fitting of the GP's. This is demonstrated in simulation and the field on a platform built for the purpose.  The second part of this thesis focuses on modelling the surface salinity profiles of estuarine tidal fronts. The standard GP model assumes evenly distributed noise, which does not always hold. This can be handled with Heteroscedastic noise. An efficient new method, Parametric Heteroscedastic Gaussian Process regression, is proposed. This is applied to active sample selection on stationary fronts and adaptive planning on moving fronts where a number of information theoretic methods are compared. The use of a mean function is shown to increase the accuracy of predictions whilst reducing optimisation time. These algorithms are validated in simulation.  Algorithmic development is focused on efficient methods allowing deployment on platforms with constrained computational resources. Whilst the application of this thesis is Autonomous Surface Vessels, it is hoped the issues discussed and solutions provided have relevance to other applications in robotics and wider fields such as spatial statistics and machine learning in general",Adaptive Sampling For Efficient Online Modelling,https://core.ac.uk/download/212690902.pdf,"School of Aerospace, Mechanical and Mechatronic Engineering",,,core
148925575,2016-12-01T00:00:00,"This study presents a framework that recognizes and imitates human upper-body motions in real time. The framework consists of two parts. In the first part, a transformation algorithm is applied to 3D human motion data captured by a Kinect. The data are then converted into the robot’s joint angles by the algorithm. The human upper-body motions are successfully imitated by the NAO humanoid robot in real time.

In the second part, the human action recognition algorithm is implemented for upper-body gestures. A human action dataset is also created for the upper-body movements. Each action is performed 10 times by twenty-four users. The collected joint angles are divided into six action classes. Extreme Learning Machines (ELMs) are used to classify the human actions. Additionally, the Feed-Forward Neural Networks (FNNs) and K-Nearest Neighbor (K-NN) classifiers are used for comparison. According to the comparative results, ELMs produce a good human action recognition performance",Gesture imitation and recognition using Kinect sensor and extreme learning machines,https://core.ac.uk/download/148925575.pdf,'Elsevier BV',10.1016/j.measurement.2016.09.026,,core
144821921,19/09/2017,"‘In faith, I do not love thee with mine eyes, For they in thee a thousand errors note; But ‘tis my heart that loves what they despise …’ 1 This sonnet and the ancient Japanese notion of wabi-sabi view aesthetics or beauty as imperfect, impermanent and incomplete. Rather than celebrating the human diversity created by our ‘imperfections’, today's society increasingly focuses on them as ‘areas for improvement’, often via a doctor’s scalpel or the latest gadget. Developments in science, technology, engineering, mathematics and medicine (STEMM) promise a tomorrow where ‘errors’ or ‘deficiencies’ in an organism’s genetic and/or phenotypic makeup can be modulated, enhanced, corrected, redefined or eradicated by, for instance, networks of biological nanomachines. Upgraded organisms will be convolutions of organic parts, electronic components, microchips, and biomechanotronic devices. Humans 1.0, Humans 2.0 and transhumans will live in new fully immersive worlds (virtual reality), inhabit a modified real world (augmented reality), and exist with an altered body schema (mixed-reality). This future world could be a place of total technological convergence, where it may not be possible to ensure privacy of an individual’s thoughts. It could also be a place where people can be subjected to Social Engineering and manipulation, including the potential for viruses and malware infecting the brain or body, as well as new forms of external control of individuals by third parties. In this discussion paper, we will explore the potential privacy, security, and ethical issues raised by humanmachine mergers. The focus is on research, development and products at the intersection of robotics, artificial intelligence, Big Data, and smart computing. We suggest that there is a need for a more holistic approach to the assessment of technology and its governance. Additionally, we suggest that in order to determine how the law will need to respond to this particular future space, it is necessary to understand the full impacts of human-machine mergers on societies and our planet – to go beyond these three issues. Since STEMM-related activities are promising a cornucopia of future spaces, we will propose that the problems of governance and assessment require a new conception of ‘responsible research and innovation’, one that is fulfilled by our recently proposed FLE5 SH framework.2 To some extent the FLE5 SH framework can be seen as allowing the formation of a social contract, whereby all stakeholders are required to engage in a review of this wider spectrum of the possible impacts of technologies. We suggest that a Precautionary Principle approach may be of assistance in considering the impacts of technologies, remembering that especially in the context of software based systems, it is always useful to think first and bugfix later",Governance and Assessment of Future Spaces: A Discussion of Some Issues Raised by the Possibilities of Human-Machine Mergers,https://core.ac.uk/download/pdf/144821921.pdf,,10.5281/zenodo.896109,,core
145170263,2017-01-01T00:00:00,"18 pages, 16 figures, 3 tables, 6 pseudocodes/algorithms, video at https://youtu.be/IqtyHFrb3BUInternational audienceThe high probability of hardware failures prevents many advanced robots (e.g., legged robots) from being confidently deployed in real-world situations (e.g., post-disaster rescue). Instead of attempting to diagnose the failures, robots could adapt by trial-and-error in order to be able to complete their tasks. In this situation, damage recovery can be seen as a Reinforcement Learning (RL) problem. However, the best RL algorithms for robotics require the robot and the environment to be reset to an initial state after each episode, that is, the robot is not learning autonomously. In addition, most of the RL methods for robotics do not scale well with complex robots (e.g., walking robots) and either cannot be used at all or take too long to converge to a solution (e.g., hours of learning). In this paper, we introduce a novel learning algorithm called "" Reset-free Trial-and-Error "" (RTE) that (1) breaks the complexity by pre-generating hundreds of possible behaviors with a dynamics simulator of the intact robot, and (2) allows complex robots to quickly recover from damage while completing their tasks and taking the environment into account. We evaluate our algorithm on a simulated wheeled robot, a simulated six-legged robot, and a real six-legged walking robot that are damaged in several ways (e.g., a missing leg, a shortened leg, faulty motor, etc.) and whose objective is to reach a sequence of targets in an arena. Our experiments show that the robots can recover most of their locomotion abilities in an environment with obstacles, and without any human intervention",Reset-free Trial-and-Error Learning for Robot Damage Recovery,https://core.ac.uk/download/145170263.pdf,'Elsevier BV',10.1016/j.robot.2017.11.010,,core
109112052,08/10/2016,"17. DIST RIUTION STATEMENT (of INe booec entered Iote&amp;l o 20, # I dIfferent, frem. fer E Ai lil I LLCTEI 10 SUPPLEMENTARY NOTES None CN IS. KEY WORDS (ConInue en rooe ee side If neeeeo md Identify 6? block so•b r) robotics architectures harware, software systems for control computational architecture for control 20. ABSTRACT (Cefnllnuo en rover.. aide It neceeow7 trd IdoneItt by block nmbier) Abstract: This paper describes a fully implemented computational architecture that controls the Utah-MIT dextrous hand and other complex robots. Robots like the Utah-MIT hand are characterized by large numbers of actuators and sensors, and require high servo rates. Consequently, powerful and flexible computer architectures are needed to control them The architecture described in thii paper derives its power from the highly efficient real-time environment provided for its control processors, coupled with a development hos",A Standard Architecture for Controlling,,,,,core
79499567,22/02/2016,"This paper on artificial intelligence in education (AIEd) has two aims. The first: to explain to a non-specialist, interested, reader what AIEd is: its goals, how it is built, and how it works. The second: to set out the argument for what AIEd can offer teaching and learning, both now and in the future, with an eye towards improving learning and life outcomes for all. Computer systems that are artificially intelligent interact with the world using capabilities (such as speech recognition) and intelligent behaviours (such as using available information to take the most sensible actions toward a stated goal) that we would think of as essentially human. At the heart of artificial intelligence in education is the scientific goal to make knowledge, which is often left implicit, computationally precise and explicit. In other words, in addition to being the engine behind much ‘smart’ ed tech, AIEd is also designed to be a powerful tool to open up what is sometimes called the ‘black box of learning,’ giving us more fine-grained understandings of how learning actually happens. Although some might find the concept of AIEd alienating, the algorithms and models that underpin ed tech powered by AIEd form the basis of an essentially human endeavor. Using AIEd, teachers will be able to offer learners educational experiences that are more personalised, flexible, inclusive and engaging. Crucially, we do not see a future in which AIEd replaces teachers. What we do see is a future in which the extraordinary expertise of teachers is better leveraged and augmented through the thoughtful deployment of well designed AIEd. We have available, right now, AIEd tools that could support student learning at a scale previously unimaginable by providing one-on-one tutoring to every student, in every subject. Existing technologies also have the capacity to provide intelligent support to learners working in a group, and to create authentic virtual learning environments where students have the right support, at the right time, to tackle real-life problems and puzzles. In the near future, we expect that teaching and learning will increasingly be supported by the thoughtful application of AIEd tools. For example, by lifelong learning companions powered by AI that can accompany and support individual learners throughout their studies - in and beyond school - and new forms of assessment that measure learning while it is taking place, shaping the learning experience in real time. If we are ultimately successful, we predict that AIEd will help us address some of the most intractable problems in education, including achievement gaps and teacher retention. AIEd will also help us respond to the most significant social challenge that AI has already brought - the steady replacement of jobs and occupations with clever algorithms and robots. It is our view that this provides a new innovation imperative in education, which can be expressed simply: as humans live and work alongside increasingly smart machines, our education systems will need to achieve at levels that none have managed to date. True progress will require the development of an AIEd infrastructure. This will not, however, be a single monolithic AIEd system. Instead, it will resemble the marketplace that has developed for smartphone apps: hundreds and then thousands of individual AIEd components, developed in collaboration with educators, conformed to uniform international data standards, and shared with researchers and developers worldwide. These standards will also enable system-level data collation and analysis that will help us to learn much more about learning itself – and how to improve it. Moving forward, we will need to pay close attention to three powerful forces as we map the future of artificial intelligence in education, namely pedagogy, technology, and system change. Paying attention to the pedagogy will mean that the design of new edtech should always start with what we know about learning. It also means that the system for funding this work must be simultaneously opened up and refocused, moving away from isolated pockets of R&D and toward collaborative enterprises that prioritise areas known to make a real difference to teaching and learning. Paying attention to the technology will mean creating smarter demand for commercial grade AIEd products that work. It also means the development of a robust, component-based AIEd infrastructure, similar to the smartphone app marketplace, where researchers and developers can access standardised components that have been developed in collaboration with educators. Paying attention to system change will mean involving teachers, students, and parents in co-designing new tools, so that AIEd will appropriately address the inherent “messiness” of real classroom, university, and workplace learning environments. It also means the development of data standards that promote the safe and ethical use of data. Said succinctly, we need intelligent technologies that embody what we know about great teaching and learning, embodied in enticing consumer grade products, which are then used effectively in real-life settings that combine the best of human and machine. We do not underestimate the new-thinking, inevitable wrong-turns, and effort required to realise these recommendations. However, if we are to properly unleash the intelligence of AIEd, we must do things differently - via new collaborations, sensible funding, and (always) a keen eye on the pedagogy. The potential prize is too great to act otherwise",Intelligence Unleashed: An argument for AI in Education,,UCL Knowledge Lab,,,core
296197060,2017-11,"A robot agent designed to engage in real-world human--robot joint action must be able to understand the social states of the human users it interacts with in order to behave appropriately. In particular, in a dynamic public space, a crucial task for the robot is to determine the needs and intentions of all of the people in the scene, so that it only interacts with people who intend to interact with it. We address the task of estimating the engagement state of customers for a robot bartender based on the data from audiovisual sensors. We begin with an offline experiment using hidden Markov models, confirming that the sensor data contains the information necessary to estimate user state. We then present two strategies for online state estimation: a rule-based classifier based on observed human behaviour in real bars, and a set of supervised classifiers trained on a labelled corpus. These strategies are compared in offline cross-validation, in an online user study, and through validation against a separate test corpus. These studies show that while the trained classifiers are best in a cross-validation setting, the rule-based classifier performs best with novel data; however, all classifiers also change their estimate too frequently for practical use. To address this issue, we present a final classifier based on Conditional Random Fields: this model has comparable performance on the test data, with increased stability. In summary, though, the rule-based classifier shows competitive performance with the trained classifiers, suggesting that for this task, such a simple model could actually be a preferred option, providing useful online performance while avoiding the implementation and data-scarcity issues involved in using machine learning for this task",Automatically classifying user engagement for dynamic multi-party human–robot interaction,,Springer,10.1007/s12369-017-0414-y,,core
202094075,2017-12-01T00:00:00Z,"One of the most notable trends associated with the Fourth industrial revolution is a significant strengthening of the role played by semantic methods. They are engaged in artificial intelligence means, knowledge mining in huge flows of big data, robotization, and in the internet of things. Smart contracts also can be mentioned here, although the ’intelligence’ of smart contracts still needs to be seriously elaborated. These trends should inevitably lead to an increased role of logical methods working with semantics, and significantly expand the scope of their application in practice. However, there are a number of problems that hinder this process.

We are developing an approach, which makes the application of logical modeling efficient in some important areas. The approach is based on the concept of locally simple models and is primarily focused on solving tasks in the management of enterprises, organizations, governing bodies. The most important feature of locally simple models is their ability to replace software systems. Replacement of programming by modeling gives huge advantages, for instance, it dramatically reduces development and support costs. Modeling, unlike programming, preserves the explicit semantics of models allowing integration with artificial intelligence and robots. In addition, models are much more understandable to general people than programs.

In this paper we propose the implementation of the concept of locally simple modeling on the basis of so-called document models, which has been developed by us earlier. It is shown that locally simple modeling is realized through document models with finite submodel coverages. In the second part of the paper an example of using document models for solving a management problem of real complexity is demonstrated",Locally Simple Models Construction: Methodology and Practice,,Irkutsk State University,,"[{'title': None, 'identifiers': ['1997-7670', '2541-8785', 'issn:1997-7670', 'issn:2541-8785']}]",core
214366808,2017-01-01T08:00:00,"Falls are common and often dangerous for groups with impaired mobility, like the elderly or people with lower limb amputations. Finding ways of minimizing the frequency or impact of a fall can improve quality of life dramatically. When someone does fall, real-time detection of the fall and a long-lie can trigger fast medical assistance. Such a system can also collect reliable data on the nature of real-world falls that can be used to better understand the circumstances, to aid in prevention efforts. This work has been to develop a real-time fall tracking system specifically for subjects with lower limb amputations.
In this study 17 subjects (10 healthy controls and 7 amputees) were asked to simulate 4 types of falls (trip, slip, right and left lateral) 3 times each with a mobile phone placed at 3 different locations on the body (pouch, pocket, and hand). Signals were collected from the accelerometer, gyroscope and barometer sensors using the Android mobile phone application Purple Robot. We compared 5 different machine learning classifiers for fall detection: logistic regression (L1 and L2 norm), support vector machines, K-nearest neighbors, decision trees, and random forest. Logistic regression (L1 regularized  lasso ) and random forest yielded the best results on the test set (98.8% and 98.4%, respectively). There was no significant difference between amputee and healthy control falls in terms of classifier accuracy. When testing on real world data with no recorded falls, the false positive rate was only 0.07%.
In addition to offline algorithmic development, the detection system was implemented for real-time application on a mobile platform. The previously-trained logistic regression model was implemented on the mobile platform for real-time detection. This platform will be used in an upcoming amputee population falls study. The completed system will gather data on the current conditions leading to the fall (weather, GPS location, etc.) and classify the type of the fall. The system will follow up with notifications requesting a response from the user, or automatically notify emergency contacts or 911 as needed. The steps taken in creating this system bring us closer to real-time intervention strategies to minimize the impact of falls, and enable us to collect accurate falls-related data to improve fall prevention strategies and prosthesis design",Real-Time Fall Detection and Response on Mobile Phones Using Machine Learning,https://core.ac.uk/download/214366808.pdf,Loyola eCommons,,,core
55260270,2016-01-01T00:00:00,"We propose Chaotic Neural Networks (CNN) as an alternative to other models of the Central Pattern Generation (CPG) circuits, which have been developed in the last years for robotic applications. We develop a new Matlab implementation of CNN and study their computational and functional performances. We show our results on walking humanoid robots, both in simulation and on real robots. We discuss our porting of the CNN to the on-board controller of the robot, where we verify the temporal and spatial performance. In a final comparison against CPG the CNN appear as a promising method to improve the adaptability of the robot to dynamic situations",Learning and executing rhythmic movements through chaotic neural networks: A new method for walking humanoid robots,,VDE Verlag GmbH,,,core
79586779,2016-01-01T00:00:00,"abstract: Robotic systems are outmatched by the abilities of the human hand to perceive and manipulate the world. Human hands are able to physically interact with the world to perceive, learn, and act to accomplish tasks. Limitations of robotic systems to interact with and manipulate the world diminish their usefulness. In order to advance robot end effectors, specifically artificial hands, rich multimodal tactile sensing is needed. In this work, a multi-articulating, anthropomorphic robot testbed was developed for investigating tactile sensory stimuli during finger-object interactions. The artificial finger is controlled by a tendon-driven remote actuation system that allows for modular control of any tendon-driven end effector and capabilities for both speed and strength. The artificial proprioception system enables direct measurement of joint angles and tendon tensions while temperature, vibration, and skin deformation are provided by a multimodal tactile sensor. Next, attention was focused on real-time artificial perception for decision-making. A robotic system needs to perceive its environment in order to make decisions. Specific actions such as “exploratory procedures” can be employed to classify and characterize object features. Prior work on offline perception was extended to develop an anytime predictive model that returns the probability of having touched a specific feature of an object based on minimally processed sensor data. Developing models for anytime classification of features facilitates real-time action-perception loops. Finally, by combining real-time action-perception with reinforcement learning, a policy was learned to complete a functional contour-following task: closing a deformable ziplock bag. The approach relies only on proprioceptive and localized tactile data. A Contextual Multi-Armed Bandit (C-MAB) reinforcement learning algorithm was implemented to maximize cumulative rewards within a finite time period by balancing exploration versus exploitation of the action space. Performance of the C-MAB learner was compared to a benchmark Q-learner that eventually returns the optimal policy. To assess robustness and generalizability, the learned policy was tested on variations of the original contour-following task. The work presented contributes to the full range of tools necessary to advance the abilities of artificial hands with respect to dexterity, perception, decision-making, and learning.Dissertation/ThesisDoctoral Dissertation Mechanical Engineering 201","Haptic Perception, Decision-making, and Learning  for Manipulation with Artificial Hands",https://core.ac.uk/download/79586779.pdf,,,,core
299424844,2017,"Combined efforts in the fields of neuroscience, computer science, and biology allowed to design biologically realistic models of the brain based on spiking neural networks. For a proper validation of these models, an embodiment in a dynamic and rich sensory environment, where the model is exposed to a realistic sensory-motor task, is needed. Due to the complexity of these brain models that, at the current stage, cannot deal with real-time constraints, it is not possible to embed them into a real-world task. Rather, the embodiment has to be simulated as well. While adequate tools exist to simulate either complex neural networks or robots and their environments, there is so far no tool that allows to easily establish a communication between brain and body models. The Neurorobotics Platform is a new web-based environment that aims to fill this gap by offering scientists and technology developers a software infrastructure allowing them to connect brain models to detailed simulations of robot bodies and environments and to use the resulting neurorobotic systems for in silico experimentation. In order to simplify the workflow and reduce the level of the required programming skills, the platform provides editors for the specification of experimental sequences and conditions, environments, robots, and brain–body connectors. In addition to that, a variety of existing robots and environments are provided. This work presents the architecture of the first release of the Neurorobotics Platform developed in subproject 10 “Neurorobotics” of the Human Brain Project (HBP).1 At the current state, the Neurorobotics Platform allows researchers to design and run basic experiments in neurorobotics using simulated robots and simulated environments linked to simplified versions of brain models. We illustrate the capabilities of the platform with three example experiments: a Braitenberg task implemented on a mobile robot, a sensory-motor learning task based on a robotic controller, and a visual tracking embedding a retina model on the iCub humanoid robot. These use-cases allow to assess the applicability of the Neurorobotics Platform for robotic tasks as well as in neuroscientific experiments.The research leading to these results has received funding from the European Union Seventh Framework Programme (FP7/2007-2013) under grant agreement no. 604102 (Human Brain Project) and from the European Unions Horizon 2020 Research and Innovation Programme under Grant Agreement No. 720270 (HBP SGA1)",Connecting Artificial Brains to Robots in a Comprehensive Simulation Framework: The Neurorobotics Platform,,Frontiers Media,10.3389/fnbot.2017.00002,"[{'title': None, 'identifiers': ['issn:1662-5218', '1662-5218']}]",core
83833604,2017-03-02T00:00:00,"Skilled robot task learning is best implemented by predictive action policies
due to the inherent latency of sensorimotor processes. However, training such
predictive policies is challenging as it involves finding a trajectory of motor
activations for the full duration of the action. We propose a data-efficient
deep predictive policy training (DPPT) framework with a deep neural network
policy architecture which maps an image observation to a sequence of motor
activations. The architecture consists of three sub-networks referred to as the
perception, policy and behavior super-layers. The perception and behavior
super-layers force an abstraction of visual and motor data trained with
synthetic and simulated training samples, respectively. The policy super-layer
is a small sub-network with fewer parameters that maps data in-between the
abstracted manifolds. It is trained for each task using methods for policy
search reinforcement learning. We demonstrate the suitability of the proposed
architecture and learning framework by training predictive policies for skilled
object grasping and ball throwing on a PR2 robot. The effectiveness of the
method is illustrated by the fact that these tasks are trained using only about
180 real robot attempts with qualitative terminal rewards.Comment: This work is submitted to IEEE/RSJ International Conference on
  Intelligent Robots and Systems 2017 (IROS2017",Deep Predictive Policy Training using Reinforcement Learning,http://arxiv.org/abs/1703.00727,,10.1109/iros.2017.8206046,,core
108171461,20/09/2016,"Major advancements in artificial intelligence have beenstimulated by well-designed competitions that tackleintriguing and often very complex problems. Chess, poker, stock trading, real-time strategy games, robot soccer, robot rescue or planning, and autonomous vehicles are among the most well known. Adaptability, proactiveness, and interop-erability are essential characteristics of these games. Many of these competitions focus on development of rational behavior that would allow dominance over competitors. Apart from game play, though, one of the main reasons for these competi-tions is to bridge the gap between AI and real-life domains. In many real-life domains, such as trading environments, self-interested entities need to operate subject to limited time and information. Additionally, the web has mediated an ever broad-er range of transactions, urging participants to concurrently trade across multiple markets. All these have generated the need for technologies that empower prompt investigation of large vol-umes of data and rapid evaluation of numerous alternative strategies in the face of constantly changing market conditions (Bichler, Gupta, and Ketter 2010). AI and machine-learning tech-niques, including neural networks and genetic algorithms, are continuously gaining ground in the support of such trading sce-narios. User modeling, price forecasting, market equilibrium pre-diction, and strategy optimization are typical cases where AI typ-ically provides reliable solutions. Yet, the adoption and deployment of AI practices in real trading environments remains limited, since the proprietary nature of markets precludes open benchmarking, which is critical for further scientific progress. T",Lessons Learned from the Trading Agent Competition,,,,,core
105909334,12/09/2016,"pulling his leg, playing him for a fool. But he knows that the Librarian, however convincingly rendered he may be, is just a piece of software and cannot actually do such things. (Stephenson, 1993) In Neal Stephenson’s novel SnowCrash, the protagonist uses a virtuallibrarian in “The Library ” located in his virtual home to answer reference questions and retrieve information for him. While this is in the realm of science fiction, technology is getting closer to such a reality in the form of bots. The word “bot” is derived from “robot. ” Whereas a robot is a physical machine, a bot is defined as “a computer application mimicking or em-bodying elements of human intellect.” (Smith, 2002) There are a number of different types of bots, such as search bots, tracking bots, shopping bots, etc. A chatterbot consists of a textbox for user input. The bot always gives a text response and sometimes a synthesized vocal re-sponse. Many of the chatterbots use animation to appear more life-like. Bots represent a branch of research in Artificial Intelligence (AI). An early attempt to mimic intelligence is ELIZA. (Weizenbaum, 1966) It was designed to act as a virtual therapist, and ELIZA is now considered an archaic “chatterbot. ” There are several Web-based versions of ELIZA and one such incarnation i",Reference Librarian,,,,,core
108739491,02/10/2016,"Technological and conceptual advances in fields such as artificial intelligence, robotics, and material science have enabled robotic building to be in the last decade prototypically implemented. In this context, robotic building implies both physically built robotic environments and robotically supported building processes, whereas reconfigurable, robotic environments incorporating sensor-actuator mechanisms that enable buildings to interact with their users and surroundings in real-time require design to production, assembly, and operation chains that may be (in part or as whole) implemented by robotic means. This paper presents and discusses research and experimental developments in robotic building implemented more recently at Hyperbody",DOI: 10.7564/14-NGBJ8 Robotic Building(s),,,,,core
144878273,02/11/2017,"This paper deals with the design and the
implementation of an automatic task planner for a robot, irrespective
of whether it is a stationary robot or a mobile robot. The aim of the
task planner nothing but, they are planning systems which are used to
plan a particular task and do the robotic manipulation. This planning
system is embedded into the system software in the computer, which
is interfaced to the computer. When the instructions are given using
the computer, this is transformed into real time application using the
robot. All the AI based algorithms are written and saved in the
control software, which acts as the intelligent task planning system",Design of an Artificial Intelligence Based Automatic Task Planner or a Robotic System,,,10.5281/zenodo.1132998,,core
151576746,2016-07-01T07:00:00,"By taking advantage of complementary communication technologies, distinct sensing functionalities and varied motion dynamics present in a heterogeneous multi-robotic network, it is possible to accomplish a main mission objective by assigning specialized sub-tasks to specific members of a robotic team. An adequate selection of the team members and an effective coordination are some of the challenges to fully exploit the unique capabilities that these types of systems can offer. Motivated by real world applications, we focus on a multi-robotic network consisting off aerial and ground agents which has the potential to provide critical support to humans in complex settings. For instance, aerial robotic relays are capable of transporting small ground mobile sensors to expand the communication range and the situational awareness of first responders in hazardous environments. In the first part of this dissertation, we extend work on manipulation of cable-suspended loads using aerial robots by solving the problem of lifting the cable-suspended load from the ground before proceeding to transport it. Since the suspended load-quadrotor system experiences switching conditions during this critical maneuver, we define a hybrid system and show that it is differentially-flat. This property facilitates the design of a nonlinear controller which tracks a waypoint-based trajectory associated with the discrete states of the hybrid system. In addition, we address the case of unknown payload mass by combining a least-squares estimation method with the designed controller. Second, we focus on the coordination of a heterogeneous team formed by a group of ground mobile sensors and a flying communication router which is deployed to sense areas of interest in a cluttered environment. Using potential field methods, we propose a controller for the coordinated mobility of the team to guarantee inter-robot and obstacle collision avoidance as well as connectivity maintenance among the ground agents while the main goal of sensing is carried out. For the case of the aerial communications relays, we combine antenna diversity with reinforcement learning to dynamically re-locate these relays so that the received signal strength is maintained above a desired threshold. Motivated by the recent interest of combining radio frequency and optical wireless communications, we envision the implementation of an optical link between micro-scale aerial and ground robots. This type of link requires maintaining a sufficient relative transmitter-receiver position for reliable communications. In the third part of this thesis, we tackle this problem. Based on the link model, we define a connectivity cone where a minimum transmission rate is guaranteed. For example, the aerial robot has to track the ground vehicle to stay inside this cone. The control must be robust to noisy measurements. Thus, we use particle filters to obtain a better estimation of the receiver position and we design a control algorithm for the flying robot to enhance the transmission rate. Also, we consider the problem of pairing a ground sensor with an aerial vehicle, both equipped with a hybrid radio-frequency/optical wireless communication system. A challenge is positioning the flying robot within optical range when the sensor location is unknown. Thus, we take advantage of the hybrid communication scheme by developing a control strategy that uses the radio signal to guide the aerial platform to the ground sensor. Once the optical-based signal strength has achieved a certain threshold, the robot hovers within optical range. Finally, we investigate the problem of building an alliance of agents with different skills in order to satisfy the requirements imposed by a given task. We find this alliance, known also as a coalition, by using a bipartite graph in which edges represent the relation between agent capabilities and required resources for task execution. Using this graph, we build a coalition whose total capability resources can satisfy the task resource requirements. Also, we study the heterogeneity of the formed coalition to analyze how it is affected for instance by the amount of capability resources present in the agents",Exploiting Heterogeneity in Networks of Aerial and Ground Robotic Agents,https://core.ac.uk/download/151576746.pdf,UNM Digital Repository,,,core
132846247,2017-12-19T00:00:00,"New technologies, such as Virtual Reality (VR), Robotics and Artificial Intelligence (AI) are steadily having an impact upon the world of opera. The evolving use of performance-based software such as Ableton Live and Max/MSP has created new and exciting compositional techniques that intertwine theatrical and musical performance. This poster presents some initial work on the development of an opera using such technologies that is being composed by Kallionpää and Chamberlain","The art and ‘science’ of opera: composing, staging & designing new forms of interactive theatrical performance",https://core.ac.uk/download/132846247.pdf,,,,core
144110905,2017-01-13T00:00:00,"O número de acidentes veiculares têm aumentado mundialmente e a principal causa associada a estes acidentes é a falha humana. O desenvolvimento de veículos autônomos é uma área que ganhou destaque em vários grupos de pesquisa do mundo, e um dos principais objetivos é proporcionar um meio de evitar estes acidentes. Os sistemas de navegação utilizados nestes veículos precisam ser extremamente confiáveis e robustos o que exige o desenvolvimento de soluções específicas para solucionar o problema. Devido ao baixo custo e a riqueza de informações, um dos sensores mais utilizados para executar navegação autônoma (e nos sistemas de auxílio ao motorista) são as câmeras. Informações sobre o ambiente são extraídas por meio do processamento das imagens obtidas pela câmera, e em seguida são utilizadas pelo sistema de navegação. O objetivo principal desta tese consiste do projeto, implementação, teste e otimização de um comitê de Redes Neurais Artificiais utilizadas em Sistemas de Visão Computacional para Veículos Autônomos (considerando em específico o modelo proposto e desenvolvido no Laboratório de Robótica Móvel (LRM)), em hardware, buscando acelerar seu tempo de execução, para utilização como classificadores de imagens nos veículos autônomos desenvolvidos pelo grupo de pesquisa do LRM. Dentre as contribuições deste trabalho, as principais são: um hardware configurado em um FPGA que executa a propagação do sinal em um comitê de redes neurais artificiais de forma rápida com baixo consumo de energia, comparado a um computador de propósito geral; resultados práticos avaliando precisão, consumo de hardware e temporização da estrutura para a classe de aplicações em questão que utiliza a representação de ponto-fixo; um gerador automático de look-up tables utilizadas para substituir o cálculo exato de funções de ativação em redes MLP; um co-projeto de hardware/software que obteve resultados relevantes para implementação do algoritmo de treinamento Backpropagation e, considerando todos os resultados, uma estrutura que permite uma grande diversidade de trabalhos futuros de hardware para robótica por implementar um sistema de processamento de imagens em hardware.The number of vehicular accidents have increased worldwide and the leading associated cause is the human failure. Autonomous vehicles design is gathering attention throughout the world in industry and universities. Several research groups in the world are designing autonomous vehicles or driving assistance systems with the main goal of providing means to avoid these accidents. Autonomous vehicles navigation systems need to be reliable with real-time performance which requires the design of specific solutions to solve the problem. Due to the low cost and high amount of collected information, one of the most used sensors to perform autonomous navigation (and driving assistance systems) are the cameras.Information from the environment is extracted through obtained images and then used by navigation systems. The main goal of this thesis is the design, implementation, testing and optimization of an Artificial Neural Network ensemble used in an autonomous vehicle navigation system (considering the navigation system proposed and designed in Mobile Robotics Lab (LRM)) in hardware, in order to increase its capabilites, to be used as image classifiers for robot visual navigation. The main contributions of this work are: a reconfigurable hardware that performs a fast signal propagation in a neural network ensemble consuming less energy when compared to a general purpose computer, due to the nature of the hardware device; practical results on the tradeoff between precision, hardware consumption and timing for the class of applications in question using the fixed-point representation; a automatic generator of look-up tables widely used in hardware neural networks to replace the exact calculation of activation functions; a hardware/software co-design that achieve significant results for backpropagation training algorithm implementation, and considering all presented results, a structure which allows a considerable number of future works on hardware image processing for robotics applications by implementing a functional image processing hardware system",Reconfigurable hardware system for autonomous vehicles visual navigation,,"'Universidade de Sao Paulo, Agencia USP de Gestao da Informacao Academica (AGUIA)'",10.11606/T.55.2017.tde-13012017-164142,,core
42675701,2016-01-25T00:00:00,"The motivation of this paper is to develop a smart system using multi-modal
vision for next-generation mechanical assembly. It includes two phases where in
the first phase human beings teach the assembly structure to a robot and in the
second phase the robot finds objects and grasps and assembles them using AI
planning. The crucial part of the system is the precision of 3D visual
detection and the paper presents multi-modal approaches to meet the
requirements: AR markers are used in the teaching phase since human beings can
actively control the process. Point cloud matching and geometric constraints
are used in the robot execution phase to avoid unexpected noises. Experiments
are performed to examine the precision and correctness of the approaches. The
study is practical: The developed approaches are integrated with graph
model-based motion planning, implemented on an industrial robots and applicable
to real-world scenarios",Teaching Robots to Do Object Assembly using Multi-modal 3D Vision,http://arxiv.org/abs/1601.06473,,,,core
103816561,18/01/2016,"Abstract. Humans have the subconscious ability to create simple ab-stractions from observations of our physical environment. The ability to consider the colour of an object in terms of “red ” or “blue”, rather than spatial distributions of reflected light wavelengths, is vital in processing and communicating information about important features within our lo-cal environment. The real-time identification of such features in image processing necessitates the software implementation of such a process; segmenting an image into regions of salient colour, and in doing so re-ducing the information stored and processed from 3-dimensional pixel values to a simple colour class label. This paper details a method by which colour segmentation may be performed offline and stored in a static data structure, allowing for constant time dimensionality reduction in an arbitrary environment of coloured features. The machine learning frame-work requires no human supervision, and its performance is evaluated in terms of feature classification performance within a RoboCup robot soc-cer environment. The developed system is demonstrated to yield an 8% improvement over slower traditional methods of manual colour mapping",Unsupervised Recognition of Salient Colour for Real-Time Image Processing,,,,,core
131956132,2017-11-08T00:00:00,"International audienceBackground:  In maize, silks are hundreds of filaments that simultaneously emerge from the ear for collecting pollen over a period of 1–7 days, which largely determines grain number especially under water deficit. Silk growth is a major trait for drought tolerance in maize, but its phenotyping is difficult at throughputs needed for genetic analyses.Results:  We have developed a reproducible pipeline that follows ear and silk growths every day for hundreds of plants, based on an ear detection algorithm that drives a robotized camera for obtaining detailed images of ears and silks. We first select, among 12 whole‑plant side views, those best suited for detecting ear position. Images are seg‑mented, the stem pixels are labelled and the ear position is identified based on changes in width along the stem. A mobile camera is then automatically positioned in real time at 30 cm from the ear, for a detailed picture in which silks are identified based on texture and colour. This allows analysis of the time course of ear and silk growths of thousands of plants. The pipeline was tested on a panel of 60 maize hybrids in the PHENOARCH phenotyping platform. Over 360 plants, ear position was correctly estimated in 86% of cases, before it could be visually assessed. Silk growth rate, estimated on all plants, decreased with time consistent with literature. The pipeline allowed clear identification of the effects of genotypes and water deficit on the rate and duration of silk growth.Conclusions:  The pipeline presented here, which combines computer vision, machine learning and robotics, provides a powerful tool for large‑scale genetic analyses of the control of reproductive growth to changes in environ‑mental conditions in a non‑invasive and automatized way. It is available as Open Source software in the OpenAlea platform",A robot-assisted imaging pipeline for tracking the growths of maize ear and silks in a high-throughput phenotyping platform,,'Springer Science and Business Media LLC',10.1186/s13007-017-0246-7,,core
155711533,2016,"This paper is concerned with the coordination control of multiple biomimetic robotic fish in highly dynamic aquatic environments by building a hybrid centralized system. With the aid of the results of biorobotics and control techniques, a radio-controlled multijoint robotic fish and its locomotion control are developed. To enable a closed control loop, a visual subsystem that is responsible for tracking of multiple moving objects is constructed and implemented in real time. Furthermore, a behavior-based hierarchical architecture in conjunction with fuzzy reinforcement learning is proposed to accomplish effective coordination among multiple swimming robots. Finally, experiments on 2vs2 water polo game are carried out to verify the proposed coordination control scheme. Over the past eight years, this multirobot platform has been successfully applied to international underwater robot competitions to promote innovative research and education in underwater robotics.National Natural Science Foundation of China [60774089, 10972003,   61304033, 61375102]; State Key Laboratory of RoboticsSCI(E)EIARTICLEjunzhi.yu@ia.ac.cn; wangchen@pku.edu.cn; xiegming@pku.edu.cn21280-12886",Coordination of Multiple Robotic Fish With Applications to Underwater   Robot Competition,,IEEE TRANSACTIONS ON INDUSTRIAL ELECTRONICS,10.1109/TIE.2015.2425359,"[{'title': None, 'identifiers': ['issn:0278-0046', '0278-0046', '1557-9948', 'issn:1557-9948']}]",core
154803302,2017-01-01T00:00:00,"Many tasks involve the fine manipulation of objects despite limited visual feedback. In such scenarios, tactile and proprioceptive feedback can be leveraged for task completion. We present an approach for real-time haptic perception and decision-making for a haptics-driven, functional contour-following task: the closure of a ziplock bag. This task is challenging for robots because the bag is deformable, transparent, and visually occluded by artificial fingertip sensors that are also compliant. A deep neural net classifier was trained to estimate the state of a zipper within a robot&#x0027;s pinch grasp. A Contextual Multi-Armed Bandit (C-MAB) reinforcement learning algorithm was implemented to maximize cumulative rewards by balancing exploration versus exploitation of the state-action space. The C-MAB learner outperformed a benchmark Q-learner by more efficiently exploring the state-action space while learning a hard-to-code task. The learned C-MAB policy was tested with novel ziplock bag scenarios and contours (wire, rope). Importantly, this work contributes to the development of reinforcement learning approaches that account for limited resources such as hardware life and researcher time. As robots are used to perform complex, physically interactive tasks in unstructured or unmodeled environments, it becomes important to develop methods that enable efficient and effective learning with physical testbeds. IEE",Functional Contour-following via Haptic Perception and Reinforcement Learning,,'Institute of Electrical and Electronics Engineers (IEEE)',10.1109/TOH.2017.2753233,,core
161538389,2017-01-01T00:00:00,"Combined efforts in the fields of neuroscience, computer science, and biology allowed to design biologically realistic models of the brain based on spiking neural networks. For a proper validation of these models, an embodiment in a dynamic and rich sensory environment, where the model is exposed to a realistic sensory-motor task, is needed. Due to the complexity of these brain models that, at the current stage, cannot deal with real-time constraints, it is not possible to embed them into a real-world task. Rather, the embodiment has to be simulated as well. While adequate tools exist to simulate either complex neural networks or robots and their environments, there is so far no tool that allows to easily establish a communication between brain and body models. The Neurorobotics Platform is a new web-based environment that aims to fill this gap by offering scientists and technology developers a software infrastructure allowing them to connect brain models to detailed simulations of robot bodies and environments and to use the resulting neurorobotic systems for in silico experimentation. In order to simplify the workflow and reduce the level of the required programming skills, the platform provides editors for the specification of experimental sequences and conditions, environments, robots, and brain–body connectors. In addition to that, a variety of existing robots and environments are provided. This work presents the architecture of the first release of the Neurorobotics Platform developed in subproject 10 “Neurorobotics” of the Human Brain Project (HBP).1 At the current state, the Neurorobotics Platform allows researchers to design and run basic experiments in neurorobotics using simulated robots and simulated environments linked to simplified versions of brain models. We illustrate the capabilities of the platform with three example experiments: a Braitenberg task implemented on a mobile robot, a sensory-motor learning task based on a robotic controller, and a visual tracking embedding a retina model on the iCub humanoid robot. These use-cases allow to assess the applicability of the Neurorobotics Platform for robotic tasks as well as in neuroscientific experiments.The research leading to these results has received funding from the European Union Seventh Framework Programme (FP7/2007-2013) under grant agreement no. 604102 (Human Brain Project) and from the European Unions Horizon 2020 Research and Innovation Programme under Grant Agreement No. 720270 (HBP SGA1)",Connecting Artificial Brains to Robots in a Comprehensive Simulation Framework: The Neurorobotics Platform,https://core.ac.uk/download/161538389.pdf,'Frontiers Media SA',10.3389/fnbot.2017.00002,"[{'title': 'Frontiers in Neurorobotics', 'identifiers': ['issn:1662-5218', '1662-5218']}]",core
148031086,2017-06-09T21:24:25,"In recent years, the emergence of artificial intelligence increases the demand of automatic and robust localization outdoors and indoors. While GPS provides enough accuracy in most outdoor cases, there is still a lack of robust and efficient indoor localization systems available on the market. In this report, an experimental framework for indoor localization is developed and tested. To operate an automatic robot owned by LCAV laboratory, two different operation modes have been successfully implemented, including controlling a robot in real-time or with extra input containing a list of commands. Also, a new visual fiducial system have been developed, and is able to capture the locations of Apriltags inside a room accurately. Multidimensional scaling (MDS) and Squared range least square (SRLS) algorithms containing distance information will also be introduced and the final localization result is within 2% error of tolerance compared with the ground truth results measured by a laser meter",Experimental indoors localization using distance measurements obtained from visual data,https://core.ac.uk/download/148031086.pdf,,,,core
78506578,2017-02-03T00:00:00,"Reinforcement learning can enable complex, adaptive behavior to be learned
automatically for autonomous robotic platforms. However, practical deployment
of reinforcement learning methods must contend with the fact that the training
process itself can be unsafe for the robot. In this paper, we consider the
specific case of a mobile robot learning to navigate an a priori unknown
environment while avoiding collisions. In order to learn collision avoidance,
the robot must experience collisions at training time. However, high-speed
collisions, even at training time, could damage the robot. A successful
learning method must therefore proceed cautiously, experiencing only low-speed
collisions until it gains confidence. To this end, we present an
uncertainty-aware model-based learning algorithm that estimates the probability
of collision together with a statistical estimate of uncertainty. By
formulating an uncertainty-dependent cost function, we show that the algorithm
naturally chooses to proceed cautiously in unfamiliar environments, and
increases the velocity of the robot in settings where it has high confidence.
Our predictive model is based on bootstrapped neural networks using dropout,
allowing it to process raw sensory inputs from high-bandwidth sensors such as
cameras. Our experimental evaluation demonstrates that our method effectively
minimizes dangerous collisions at training time in an obstacle avoidance task
for a simulated and real-world quadrotor, and a real-world RC car. Videos of
the experiments can be found at https://sites.google.com/site/probcoll",Uncertainty-Aware Reinforcement Learning for Collision Avoidance,http://arxiv.org/abs/1702.01182,,,,core
87950546,2017-01-25T00:00:00,"This demonstration presents an open-source hardware and software platform which allows non-roboticists researchers to conduct machine learning experiments to benchmark algorithms for autonomous exploration and active learning. In particular, in addition to showing the general properties of the platform such as its modularity and usability, it demonstrates the online functioning of a particular algorithm which allows efficient learning of multiple forward and inverse models and can leverage information from human guidance. A first aspect of the demonstration is to illustrate the ease of use of the 3D printed low-cost Poppy humanoid robotic platform, that allows non-roboticists to quickly set up and program robotic experiments. A second aspect is to show how the Explauto library allows systematic comparison and evaluation of active learning and exploration algorithms in sensorimotor spaces, through a Python API to select already implemented exploration algorithms. The third idea is to showcase Active Model Babbling, an efficient exploration algorithm dynamically choosing which task/goal space to explore and particular goals to reach, and integrating social guidance from humans in real time to drive exploration towards particular objects or actions. Contact us for a CC-BY version of the video ![Forestier and Oudeyer, 2016] Forestier, S. and Oudeyer, P.-Y. (2016). Modular active curiosity-driven discovery of tool use. In 2016 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Daejeon, Korea. [Lapeyre et al., 2014] Lapeyre, M., Rouanet, P., Grizou, J., Nguyen, S., Depraetre, F., Le Falher, A., and Oudeyer, P.-Y. (2014). Poppy Project: Open-Source Fabrication of 3D Printed Humanoid Robot for Science, Education and Art. In Digital Intelligence 2014, page 6, Nantes, France. [Moulin-Frier et al., 2014] Moulin-Frier, C., Rouanet, P., Oudeyer, P.-Y., and others (2014). Explauto: an open- source Python library to study autonomous exploration in developmental robotics. In ICDL-Epirob-International Conference on Development and Learning, Epirob",Intrinsically Motivated Multi-Task Reinforcement LearningWith Open-Source Explauto Library and Poppy Humanoid Robot,,HAL CCSD,,,core
201032520,2017-01-01T00:00:00,"The article is dedicated to the prospects for design of standalone smart robots based on MIVAR accumulation and logical data processing technologies. It is proposed to use MIVAR technologies for development of new generation standalone robots in such areas as text comprehension and natural language understanding, image recognition, expertise systems and behavior planning. The article also deals with the role of artificial intelligence and intelligent technologies in robotics. The key approaches to design of artificial intelligence (AI) have been reviewed. It is important to emphasize that MIVARbased technologies are used in robotics and have been implemented in UNIKUM project and in Murom-ISP project. The article also reviews Razumator KESMI software product (Wi! Mi), which is as well based on MIVR technologies and makes it possible to process more than 5 million of production rules per centisecond, which satisfies the real-time requirements for control systems of standalone smart robots",ON PROSPECTS FOR DESIGN OF STANDALONE SMART ROBOTS BASED ON MIVAR TECHNOLOGIES,https://core.ac.uk/download/201032520.pdf,'CRI Electronics',10.21778/2413-9599-2016-4-96-105,"[{'title': 'Radio Industry (Russia)', 'identifiers': ['issn:2413-9599', 'issn:2541-870X', '2541-870x', '2413-9599']}]",core
73393343,2017-12-12T00:00:00,"The high probability of hardware failures prevents many advanced robots
(e.g., legged robots) from being confidently deployed in real-world situations
(e.g., post-disaster rescue). Instead of attempting to diagnose the failures,
robots could adapt by trial-and-error in order to be able to complete their
tasks. In this situation, damage recovery can be seen as a Reinforcement
Learning (RL) problem. However, the best RL algorithms for robotics require the
robot and the environment to be reset to an initial state after each episode,
that is, the robot is not learning autonomously. In addition, most of the RL
methods for robotics do not scale well with complex robots (e.g., walking
robots) and either cannot be used at all or take too long to converge to a
solution (e.g., hours of learning). In this paper, we introduce a novel
learning algorithm called ""Reset-free Trial-and-Error"" (RTE) that (1) breaks
the complexity by pre-generating hundreds of possible behaviors with a dynamics
simulator of the intact robot, and (2) allows complex robots to quickly recover
from damage while completing their tasks and taking the environment into
account. We evaluate our algorithm on a simulated wheeled robot, a simulated
six-legged robot, and a real six-legged walking robot that are damaged in
several ways (e.g., a missing leg, a shortened leg, faulty motor, etc.) and
whose objective is to reach a sequence of targets in an arena. Our experiments
show that the robots can recover most of their locomotion abilities in an
environment with obstacles, and without any human intervention.Comment: 18 pages, 16 figures, 3 tables, 6 pseudocodes/algorithms, video at
  https://youtu.be/IqtyHFrb3BU, code at
  https://github.com/resibots/chatzilygeroudis_2018_rt",Reset-free Trial-and-Error Learning for Robot Damage Recovery,http://arxiv.org/abs/1610.04213,'Elsevier BV',10.1016/j.robot.2017.11.010,,core
84591553,2017-11-01T00:00:00,"A robot agent designed to engage in real-world human--robot joint action must be able to understand the social states of the human users it interacts with in order to behave appropriately. In particular, in a dynamic public space, a crucial task for the robot is to determine the needs and intentions of all of the people in the scene, so that it only interacts with people who intend to interact with it. We address the task of estimating the engagement state of customers for a robot bartender based on the data from audiovisual sensors. We begin with an offline experiment using hidden Markov models, confirming that the sensor data contains the information necessary to estimate user state. We then present two strategies for online state estimation: a rule-based classifier based on observed human behaviour in real bars, and a set of supervised classifiers trained on a labelled corpus. These strategies are compared in offline cross-validation, in an online user study, and through validation against a separate test corpus. These studies show that while the trained classifiers are best in a cross-validation setting, the rule-based classifier performs best with novel data; however, all classifiers also change their estimate too frequently for practical use. To address this issue, we present a final classifier based on Conditional Random Fields: this model has comparable performance on the test data, with increased stability. In summary, though, the rule-based classifier shows competitive performance with the trained classifiers, suggesting that for this task, such a simple model could actually be a preferred option, providing useful online performance while avoiding the implementation and data-scarcity issues involved in using machine learning for this task",Automatically classifying user engagement for dynamic multi-party human–robot interaction,https://core.ac.uk/download/84591553.pdf,'Springer Science and Business Media LLC',10.1007/s12369-017-0414-y,,core
296622026,2015-11-26T14:15:06Z,"The authors present a new approach using genetic algorithms, neural networks and nanorobotics concepts applied to the problem of control design for nanoassembly automation and its application in medicine. As a practical approach to validate the proposed design, we have elaborated and simulated a virtual environment focused on control automation for nanorobotics teams that exhibit collective behavior. This collective behavior is a suitable way to perform a large range of tasks and positional assembly manipulation in a complex 3D workspace. We emphasize the application of such techniques as a feasible approach for the investigation of nanorobotics system design in nanomedicine. Theoretical and practical analyses of control modelling is one important aspect that will enable rapid development in the emerging field of nanotechnology.2:00 AM95104Adleman, L.M., On constructing a molecular computer (1995) DNA Based Computers, , http://olymp.wuwien.ac.at/usr/ai/frisch/local.htmlBachand, G.D., Montemagno, C.D., Constructing organic/inorganic NEMS devices powered by biomolecular motors (2000) Biomedical Microdevices, 2, pp. 179-184Baraff, D., (1992) Dynamic Simulation of Non-penetrating Rigid Bodies, , PhD Thesis, Department of Computer Science, Cornell University, Ithaca, NYBerg, H.C., Dynamic properties of bacterial flagellar motors (1974) Nature, 249, pp. 77-79. , 3 MayBojinov, H., Casai, A., Hogg, T., Multiagent control of modular self-reconfigurable robots (2002) Artificial Intelligence, 142, pp. 99-120Brooks, R., (1992) Artificial Life and Real Robots, , MIT pressBrutzman, D.P., Kanayama, Y., Zyda, M.J., Integrated simulation for rapid development of autonomous underwater vehicles (1992) IEEE Autonomous Underwater Vehicle Conference, pp. 3-10. , IEEE Oceanic Engineering Society, Washington DC, JuneCasai, A., Hogg, T., Cavalcanti, A., Nanorobots as cellular assistants in inflammatory responses (2003) IEEE BCATS Biomedical Computation at Stanford 2003 Symposium, , IEEE Computer Society, Stanford CA, OctoberCavalcanti, A., Assembly automation with evolutionary nanorobots and sensor-based control applied to nanomedicine (2003) IEEE Transactions on Nanotechnology, 2 (2), pp. 82-87. , www.nanorobotdesign.com, JuneCavalcanti, A., Freitas Jr., R.A., Autonomous multi-robot sensor-based cooperation for nanomedicine (2002) Int'l J. Nonlinear Science Numerical Simulation, 3 (4), pp. 743-746. , www.rfreitas.com/NanoPubls.htm, AugustChen, W., Lewis, K., A robust design approach for achieving flexibility in multidisciplinary design (1999) AIAA Jounal, 37 (8), pp. 982-990Downing, H.A., Jeanne, R.L., Nest construction by the paperwasp, Plistes: A test of stigmergy theory (1988) Animal Behavior, 36, pp. 1729-1739Drexler, K.E., Forrest, D., Freitas Jr., R.A., Hall, J.S., Jacobstein, N., McKendree, T., Merkle, R., Peterson, C., (2001) A Debate about Assemblers, , www.imm.org/SciAmDebate2/whitesides.html, Institute for Molecular ManufacturingDrexler, K.E., (1992) Nanosystems: Molecular Machinery, Manufacturing, and Computation, , John Wiley & SonsDrucker, H.D., Wu, D., Vapnik, V., Support vector machines for spam categorization IEEE Transactions on Neural Networks, 10 (5), pp. 1048-1054Fishbine, G., (2001) The Investor's Guide to Nanotechnology & Micromachines, , John Wiley & SonsFreitas Jr., R.A., Phoenix, C.J., Vasculoid: A personal nanomedical appliance to replace human blood (2002) J. Evol. Technol., 11. , www.jetpress.org/volume11/vasculoid.htmlFreitas Jr., R.A., Nanomedicine (1999) Vol. I: Basic Capabilities, 1. , Landes BioscienceNanomedicine (2003) IIA: Biocompatibility, , www.nanomedicine.com, Landes BioscienceFukuda, T., Arai, T., Prototyping design and automation of micro/nano manipulation system (2000) Proc. of IEEE Int 'l Conf. on Robotics and Automation (ICRA '00), pp. 192-197Geppert, L., The amazing vanishing transistor act (2002) IEEE Spectrum Magazine, pp. 28-33. , Cover story, OctoberGrefenstette, J.J., Schultz, A., An evolutionary approach to learning in robots (1994) Machine Learning Workshop on Robot Learning, , www.citidel.org, New Brunswick, NJGrzeszczuk, R., Terzopoulos, D., Hinton, G., NeuroAnimator: Fast neural network emulation and control of physics-based models (1998) Proc. of ACM SIGGRAPH 98 Conf., pp. 142-148. , M. Cohen, edGuthold, M., Controlled manipulation of molecular samples with the nano-manipulator (2000) IEEE/ASME Transactions on Mechatronics, 5 (2), pp. 189-198Hagan, M.T., Demuth, H.B., Jesús, O.D., An introduction to the use of neural networks in control systems (2002) International Journal of Robust and Nonlinear Control, 12 (11), pp. 959-985. , John Wiley & Sons, SeptemberHagiya, M., From molecular computing to molecular programming (2000) Proc. 6th DIMACS Workshop on DNA Based Computers, pp. 198-204. , Leiden, NetherlandsHaykin, S., (1999) ""Neural Networks A Comprehensive Foundation"", 2nd Edition, , Prentice Hall, New Jersey, USAHellemans, A., German team creates new type of transistor-like device (2003) IEEE Spectrum Magazine, pp. 20-21. , News Analysis, JanuaryHelmer, G., Wong, J.S.K., Honavar, V., Miller, L., Intelligent agents for intrusion detection (1998) Proceedings, IEEE Information Technology Conference, pp. 121-124. , Syracuse, NY, SeptemberKatz, E., Riklin, A., Heleg-Shabtai, V., Willner, I., Bückmann, A.F., Glucose oxidase electrodes via reconstitution of the apo-enzyme: Tailoring of novel glucose biosensors (1999) Anal. Chim. Acta., 385, pp. 45-58Khatib, M., Bouilly, B., Simeon, T., Chatila, R., Indoor navigation with uncertainty using sensor based motions (1997) Proc. 1997 IEEE Int'l Conf. on Robotics and Automation, pp. 3379-3384. , Albuquerque, New Mexico, USAKretly, L.C., Almeida, A.F.L., Oliveira, R.S., Sasaki, J.M., Sombra, A.S., Electrical and optical properties of CaCu3 Ti4 O12 (CCTO) substrates for microwave devices and antennas (2003) Microwave and Optical Technology Letters, 39 (2), pp. 145-150. , New York, EU A, OctoberKrishnamurthy, B., Rosemblum, D.S., Yeast: A general purpose event-action system (1995) IEEE Transactions on Software Engineering, 21 (10), pp. 845-857. , OctoberKube, C.R., Zhang, H., Task modelling in collective robotics (1997) Autonomous Robots, 4 (1), pp. 53-72Kumar, M.N.V.R., Nano and microparticles as controlled drug delivery devices (2000) J. Pharmacy Parmaceutical Science, 3 (2), pp. 234-258Lehoczky, J., Sha, L., Ding, Y., (1989) The Rate Monotonic Scheduling Algorithm, pp. 166-171. , IEEE Computer Society Press, Santa Monica, California, USA, DecemberLewis, M.A., Bekey, G.A., The behavioral self-organization of nanorobots using local rules (1992) Proc. of IEEE Int'l Conf. on Intelligent Robots and Systems, , Raleigh, NCLyons, K., Wang, Y., An open architecture for virtual reality in nano-scale manipulation, measurement and manufacturing (M3) (2000) 8th Foresight Conference on Molecular Nanotechnology, , USAMakaliwe, J.H., Requicha, A.A.G., Automatic planning of nanoparticle assembly tasks (2001) Proc. IEEE Int'l Symp. on Assembly and Task Planning, pp. 288-293. , Fukuoka, JapanMartel, S., Madden, P., Sosnowski, L., Hunter, I., Lafontaine, S., NanoWalker: A fully autonomous highly integrated miniature robot for nano-scale measurements (1999) Proc. of the European Optical Society and SPIE Int'l Symposium on Envirosense, Microsystems Metrology and Inspection, 3825, pp. 64-76. , Munich, GermanyMatellan, V., Fernandez, C., Molina, J.M., Genetic learning of fuzzy reactive controllers (1998) Robotics and Autonomous Systems, 225 (2), pp. 33-41Menezes, A.J., Kapoor, V.J., Goel, V.K., Cameron, B.D., Lu, J.-Y., Within a nanometer of your Life (2001) Mechanical Engineering Magazine, , www.memagazine.org/backissues/aug01/features/nmeter/nmeter.html, AugustMerkle, R.C., Nanotechnology and medicine (1996) Advances in AntiAging Medicine, 1, pp. 277-286. , Mary Ann Liebert PressMok, A., Chen, D., A multiframe model for real-time tasks (1997) IEEE Transactions on Software Engineering, 23 (10), pp. 635-645Mokhoff, N., (2003) Education Overhaul Urged for Nanotech Revolution, , www.theworkcircuit.com/news/OEG20030206S0026, EE Times, FebruaryMoore, S.K., Just one word - Plastics (2002) IEEE Spectrum Magazine, pp. 55-59. , Special R&D Report, Organic Electronics, SeptemberOkamoto, T., Ishida, Y., A performance analysis of a mobile anti-virus system (2002) Proc. of AROB 02, 1, pp. 132-135www.eng.nsf.gov/sbirPetrie, C.J., (1996) Agent-based Engineering, the Web, and Intelligence, , www-cdr.stanford.edu/NextLink/Expert.html, IEEE Expert, decemberRamia, M., Tullock, D.L., Thien, N.P., The role of hydrodynamic interaction in the locomotion of microorganisms (1993) Biophys. J., 65, pp. 755-778Reppesgaard, L., Nanobiotechnologie: Die Feinmechaniker der Zukunft nutzen Biomaterial als Werkstoff (2002) Computer Zeitung, 36, p. 22. , 2 SeptemberRequicha, A.A.G., Nanorobots, NEMS and nanoassembly (2003) IEEE ICRA International Conference on Robotics and Automation, 91 (11), pp. 1922-1933. , special issue on Nanoelectronics and Nanoprocessing, Taipei, Taiwan, NovemberRequicha, A.A.G., Resch, R., Montoya, N., Koel, B.E., Madhukar, A., Will, P., Towards hierarchical nanoassembly (1999) IEEE/RSJ Int'l Conf. on Intelligent Robots & Systems, , Kyongju, KoreaRietman, E.A., (2001) Molecular Engineering of Nanosystems, , Biological Physics Series, Springer-Verlag, New York, USARoco, M.C., (2003) Government Nanotechnology Funding: An International Outlook, , www.nano.gov/html/res/IntlFundingRoco.htm, National Science Foundation, JuneSeely, T.D., Camazine, S., Sneyd, J., Collective Decision-making in honey bees: How colonies choose among nectar sources (1991) Behavioral Ecology and Sociobiology, 28, pp. 277-290Simon, J., Frank, R.K., McDevitt, M.R., Ma, D., Lai, L.T., Borchardt, P., Wu, K., Bander, N., Tumor therapy with targeted atomic nano-scale generators (2001) Science Magazine, , Nov. 16Sitti, M., Hashimoto, K., Teleoperated nano scale object manipulation (1999) Recent Advances on Mechatronics, pp. 172-178. , Springer Verlag Pub., Ed. O. KaynakStracke, R., Böhm, K.J., Burgold, J., Schacht, H., Unger, E., Physical and technical parameters determining the functioning of a kinesin-based cell-free motor system (2000) Nanotechnology, 11, pp. 52-56. , UKSun, J., Gao, M., Feldmann, J., Electric field directed layer-by-layer assembly of highly fluorescent CdTe nanoparticles (2001) Journal of Nanoscience and Nanotechnology, 1 (2), pp. 21-27. , American Scientific PublishersToth-Fejel, T., Agents, assemblers, and ANTS: Scheduling assembly with market and biological software mechanisms (2000) Nanotechnology, 11, pp. 133-137Voss, S., Meta-heuristics: Advances and trends in local search paradigms for optimization (1998) Meta-Heuristics International Conference, , Kluwer Academic PubWieland III, C.F., Is the US nanotechnology investiment paying off? (2004) Small Times Magazine, , www.burnsdoane.com/pubs/wieland2.pdf, Intellectual Property, January/FebruaryWhitcomb, L.L., Underwater Robotics: Out of the research laboratory and into the Field (2000) IEEE Int'l Conf. on Robotics and Automation, pp. 85-90. , San Francisco, CA, USAWurll, C., Henrich, D., Wörn, H., Parallel on-line motion planning for industrial robots (1998) 3rd ASCE Specialty Conf. on Robotics for Challenging Environments, Robotics, pp. 308-314. , New Mexico, US",Nanorobotics Control Design: A Practical Approach Tutorial,,,,,core
20381669,01/01/2014,"Domestic robotics has become a key part of present-day AI research, with clear real-world problems which robotic systems could solve. However, much of the research in this field still focuses on laboratory environments and the created systems generally cannot cope with the complex circumstances within the real world. There are several notable issues with operating in an unknown environment and three of them will be addressed here:

The first is that the creators of a robotic system cannot ve sure which way of executing a task will work best in the environment a specific robot will end up in.
Here, a system was implemented which allows the robot to learn which way to execute a task proves most effective by trying each option several times. This system was implemented as a core part of the robot control architecture used. The consistency in results and statistical basis of this system were tested. This was done by ececuting three ways to perform a navigationless search task a set amount of times. This was repeated several times, with consistent results. A statistical distinction between the versions used could also be seen in the data gathered most of the time. This demonstrates the viability of this system for certain types of tasks.

A second notable issue in domestic robotics lies in the quality of human-robot interaction using natural speech. A third issue is that robots cannot generally be taught new things once in the hands of an end user. A dialog system, which allows the user to give orders to and teach robotic systems, is implemented in the previously mentioned robot control architecture. This makes it one of the first domestic robotic systems which can actually be taught by its end users. This system is subsequently tested through interaction with human operators to determine the ease of use. Improvements are made based on the user feedback, followed by another round of testing. The final version appears to be well-liked by its users, although there are still issues, mainly with speech recognition.",Robots doing as they're told: a flexible task execution system taught through human-robot dialogue,,,,,core
23743474,16/01/2014,Machine Learning for real world applications is a complex task due to the huge state and action sets they deal with and the a priori unknown dynamics of the environment involved. Reinforcement Learning offers very efficient model-free methods which are often combined with approximation architectures to overcome these problems. We present a Q-learning implementation that uses a new adaptive clustering method to approximate state and actions sets. Experimental results for an obstacle avoidance behavior with the mobile robot Khepera are given,An Adaptive Clustering Method for Model-free Reinforcement Learning,,,,,core
323332613,2014-11-19T00:00:00,"Robotics has become a common subject in many engineering degrees and postgraduate programs. Although at undergraduate levels the students are introduced to basic theoretical concepts and tools, at postgraduate courses more complex topics have to be covered. One of those advanced subjects is Cognitive Robotics, which covers aspects like automatic symbolic reasoning, decision-making, task planning or machine learning. In particular, Reinforcement Learning (RL) is a machine learning and
decision-making methodology that does not require a model of the environment where the robot operates, overcoming this limitation by making observations. In order to get the greatest educational benefit, RL theory should be complemented with some hands-on RL task that uses a real robot, so students get a complete vision of the learning problem, as well as of the issues that arise when dealing with a physical robotic platform. There are several RL techniques that can be studied in such a subject; we have chosen Q-learning, since is a simple, effective and well-known RL algorithm.

In this paper we present a minimalist implementation of the Q-learning method for a Lego Mindstorms NXT mobile robot, focused on simplicity and applicability, and flexible enough to be adapted to several tasks. Starting from a simple wandering problem, we first design an off-line model of the learning process in which the Q-learning parameters are studied. After that, we implement the algorithm on the robot, gradually enlarging the number of states-actions of the problem. The final result of this work is a teaching framework for developing practical activities regarding Q-learning in our Robotics subjects, which will improve our teaching.Universidad de Málaga. Campus de Excelencia Internacional Andalucía Tech",LEGO© Mindstorms NXT and Q-Learning: a teaching approach for robotics in engineering,,,,,core
33705626,2014,"Humans have the subconscious ability to create simple abstractions from observations of their physical environment. The ability to consider the colour of an object in terms of ""red"" or ""blue"", rather than spatial distributions of reflected light wavelengths, is vital in processing and communicating information about important features within our local environment. The real-time identification of such features in image processing necessitates the software implementation of such a process; segmenting an image into regions of salient colour, and in doing so reducing the information stored and processed from 3-dimensional pixel values to a simple colour class label. This paper details a method by which colour segmentation may be performed offline and stored in a static look-up table, allowing for constant time dimensionality reduction in an arbitrary environment of coloured features. The machine learning framework requires no human supervision, and its performance is evaluated in terms of feature classification performance within a RoboCup robot soccer environment. The developed system is demonstrated to yield an 8% improvement over slower traditional methods of manual colour mapping",Unsupervised recognition of salient colour for real-time image processing,,Springer Verlag,10.1007/978-3-662-44468-9_33,,core
79462150,2015-01-01T00:00:00,"This report summarises and integrates two different tracks of research for the purpose of envisioning and preparing a joint research project proposal. Soft- and hardware systems have become increasingly complex and act ""concurrently"", both with respect to memory access (i.e. information flow) and computational resources (i.e. ""services""). The software development metaphor of cloud-storage, cloud-computing and service-oriented design has been anticipated by artificial intelligence (AI) research at least 30 years ago (parallel and distributed computation already dates back to the 1950’s and 1970s). What is known as a ""service"" today is what in AI is known as the capability of an agent; and the problem of information flow and consistency has been a headstone of information processing ever since. Based on a real-world robotics application we demonstrate how an increasingly abstract description of collaborating or competing agents correspond to a set of concurrent processes","On the needs for specification and verification of collaborative and concurrent robots, agents and processes",,,,,core
25049302,2014-08-08T00:00:00,"Many people suffer from the loss of a limb. Learning to get by without an arm
or hand can be very challenging, and existing prostheses do not yet fulfil the
needs of individuals with amputations. One promising solution is to provide
greater communication between a prosthesis and its user. Towards this end, we
present a simple machine learning interface to supplement the control of a
robotic limb with feedback to the user about what the limb will be experiencing
in the near future. A real-time prediction learner was implemented to predict
impact-related electrical load experienced by a robot limb; the learning
system's predictions were then communicated to the device's user to aid in
their interactions with a workspace. We tested this system with five
able-bodied subjects. Each subject manipulated the robot arm while receiving
different forms of vibrotactile feedback regarding the arm's contact with its
workspace. Our trials showed that communicable predictions could be learned
quickly during human control of the robot arm. Using these predictions as a
basis for feedback led to a statistically significant improvement in task
performance when compared to purely reactive feedback from the device. Our
study therefore contributes initial evidence that prediction learning and
machine intelligence can benefit not just control, but also feedback from an
artificial limb. We expect that a greater level of acceptance and ownership can
be achieved if the prosthesis itself takes an active role in transmitting
learned knowledge about its state and its situation of use.Comment: 7 pages, 5 figure","Using Learned Predictions as Feedback to Improve Control and
  Communication with an Artificial Limb: Preliminary Findings",http://arxiv.org/abs/1408.1913,,,,core
84962589,2014-01-01T00:00:00,"International audienceCompact Bionic Handling Assistant (CBHA) is a continuum manipulator, with pneumatic-based actuation and compliant gripper. This bionic arm is attached to a mobile robot named Robotino. Inspired by the elephant's trunk, it can reproduce biological behaviors of trunks, tentacles, or snakes. Unlike rigid link robot manipulators, the development of high performance control algorithm of continuum robot manipulators remains a challenge, particularly due to their complex mechanical design, hyper-redundancy and presence of uncertainties. Numerous studies have been investigated for modeling of such complex systems. Such continuum robots, like the CBHA present a set of nonlinearities and uncertainties, making difficult to build an accurate analytical model, which can be used for control strategies development. Hence, learning approach becomes a suitable tool in such scenarios in order to capture un-modeled nonlinear behaviors of the continuous robots. In this paper, we present a qualitative modeling approach, based on neuronal model of the inverse kinematic of CBHA. A penalty term constraint is added to the inverse objective function into Distal Supervised Learning (DSL) scheme to select one particular inverse model from the redundancy manifold. The inverse kinematic neuronal model is validated by conducting a real-time implementation on a CBHA trunk",Qualitative approach for inverse kinematic modeling of a Compact Bionic Handling Assistant trunk,,HAL CCSD,,,core
103275712,13/11/2015,"Abstract. The 1. RFC Stuttgart robot soccer team is used as a testbed for multi-agent software architecture principles in dynamic real time do-mains. The current research activities focus on a completely new design of a midsize robot, the enhancement of machine learning algorithms for strategies, new vision methods and a context-aware visualization. ",1. RFC Stuttgart Team Description 2009,,,,,core
211841412,2014-01-01T00:00:00,"Sandin F, Khan AI, Dyer AG, et al. Concept Learning in Neuromorphic Vision Systems: What Can We Learn from Insects? Journal of Software Engineering and Applications. 2014;7(5):387-395.Vision systems that enable collision avoidance, localization and navigation in complex and uncertain environments are common in biology, but are extremely challenging to mimic in artificial electronic systems, in particular when size and power limitations apply. The development of neuromorphic electronic systems implementing models of biological sensory-motor systems in silicon is one promising approach to addressing these challenges. Concept learning is a central part of animal cognition that enables appropriate motor response in novel situations by generalization of former experience, possibly from a few examples. These aspects make concept learning a challenging and important problem. Learning methods in computer vision are typically inspired by mammals, but recent studies of insects motivate an interesting complementary research direction. There are several remarkable results showing that honeybees can learn to master abstract concepts, providing a road map for future work to allow direct comparisons between bio-inspired computing architectures and information processing in miniaturized “real” brains. Considering that the brain of a bee has less than 0.01% as many neurons as a human brain, the task to infer a minimal architecture and mechanism of concept learning from studies of bees appears well motivated. The relatively low complexity of insect sensory-motor systems makes them an interesting model for the further development of bio-inspired computing architectures, in particular for resource-constrained applications such as miniature robots, wireless sensors and handheld or wearable devices. Work in that direction is a natural step towards understanding and making use of prototype circuits for concept learning, which eventually may also help us to understand the more complex learning circuits of the human brain. By adapting concept learning mechanisms to a polymorphic computing framework we could possibly create large-scale decentralized computer vision systems, for example in the form of wireless sensor networks",Concept Learning in Neuromorphic Vision Systems: What Can We Learn from Insects?,https://core.ac.uk/download/211841412.pdf,"'Scientific Research Publishing, Inc.'",10.4236/jsea.2014.75035,,core
160343943,2014-11-24T00:00:00Z,"<p>Cameras provide a rich source of information while being passive, cheap and lightweight for small and medium Unmanned Aerial Vehicles (UAVs). In this work we present the first implementation of receding horizon control, which is widely used in ground vehicles, with monocular vision as the only sensing mode for autonomous UAV flight in dense clutter. We make it feasible on UAVs via a number of contributions: novel coupling of perception and control via relevant and diverse, multiple interpretations of the scene around the robot, leveraging recent advances in machine learning to showcase anytime budgeted cost-sensitive feature selection, and fast non-linear regression for monocular depth prediction. We empirically demonstrate the efficacy of our novel pipeline via real world experiments of more than 2 kms through dense trees with a quadrotor built from off-the-shelf parts. Moreover our pipeline is designed to combine information from other modalities like stereo and lidar as well if available</p",Vision and Learning for Deliberative Monocular Cluttered Flight,,,10.1184/r1/6561554.v1,,core
29515281,2015-03-01T00:00:00,"We present an algorithm which combines recent advances in model based path
integral control with machine learning approaches to learning forward dynamics
models. We take advantage of the parallel computing power of a GPU to quickly
take a massive number of samples from a learned probabilistic dynamics model,
which we use to approximate the path integral form of the optimal control. The
resulting algorithm runs in a receding-horizon fashion in realtime, and is
subject to no restrictive assumptions about costs, constraints, or dynamics. A
simple change to the path integral control formulation allows the algorithm to
take model uncertainty into account during planning, and we demonstrate its
performance on a quadrotor navigation task. In addition to this novel
adaptation of path integral control, this is the first time that a
receding-horizon implementation of iterative path integral control has been run
on a real system.Comment: 6 pages, NIPS 2014 - Autonomously Learning Robots Worksho",GPU Based Path Integral Control with Learned Dynamics,http://arxiv.org/abs/1503.00330,,,,core
62902319,2014-11-19T00:00:00,"Robotics has become a common subject in many engineering degrees and postgraduate programs. Although at undergraduate levels the students are introduced to basic theoretical concepts and tools, at postgraduate courses more complex topics have to be covered. One of those advanced subjects is Cognitive Robotics, which covers aspects like automatic symbolic reasoning, decision-making, task planning or machine learning. In particular, Reinforcement Learning (RL) is a machine learning and
decision-making methodology that does not require a model of the environment where the robot operates, overcoming this limitation by making observations. In order to get the greatest educational benefit, RL theory should be complemented with some hands-on RL task that uses a real robot, so students get a complete vision of the learning problem, as well as of the issues that arise when dealing with a physical robotic platform. There are several RL techniques that can be studied in such a subject; we have chosen Q-learning, since is a simple, effective and well-known RL algorithm.

In this paper we present a minimalist implementation of the Q-learning method for a Lego Mindstorms NXT mobile robot, focused on simplicity and applicability, and flexible enough to be adapted to several tasks. Starting from a simple wandering problem, we first design an off-line model of the learning process in which the Q-learning parameters are studied. After that, we implement the algorithm on the robot, gradually enlarging the number of states-actions of the problem. The final result of this work is a teaching framework for developing practical activities regarding Q-learning in our Robotics subjects, which will improve our teaching.Universidad de Málaga. Campus de Excelencia Internacional Andalucía Tech",LEGO© Mindstorms NXT and Q-Learning: a teaching approach for robotics in engineering,https://core.ac.uk/download/62902319.pdf,,,,core
42727752,"April 9, 2014","The advanced inspection system is an autonomous control and analysis system that improves the inspection and remediation operations for ground and surface systems. It uses optical imaging technology with intelligent computer vision algorithms to analyze physical features of the real-world environment to make decisions and learn from experience. The advanced inspection system plans to control a robotic manipulator arm, an unmanned ground vehicle and cameras remotely, automatically and autonomously. There are many computer vision, image processing and machine learning techniques available as open source for using vision as a sensory feedback in decision-making and autonomous robotic movement. My responsibilities for the advanced inspection system are to create a software architecture that integrates and provides a framework for all the different subsystem components; identify open-source algorithms and techniques; and integrate robot hardware",Vision Based Autonomous Robotic Control for Advanced Inspection and Repair,https://core.ac.uk/download/pdf/42727752.pdf,,,,core
102609742,27/08/2015,"We have to prepare the evaluation (fitness) function to evaluate the performance of the robot when we apply the machine learning techniques to the robot application. In many cases, the fitness function is composed of several aspects. Simple implementation to cope with the multiple fitness functions is a weighted summation. This paper presents an adaptive fitness function for the evolutionary computation to obtain the purposive behaviors through changing the weights for the fitness function. As an example task, a basic behavior in a simplified soccer game (shooting a ball into the opponent goal) is selected to show the validity of the adaptive fitness function. Simulation results and real experiments are shown, and a discussion is given",Behavior generation for a mobile robot based on the adaptive fitness function,,,,,core
79153150,2015-01-01T00:00:00,"There is a long last tradition in Artificial Intelligence as use of Robots endowing human peculiarities, from a cognitive and emotional point of view, and not only in shape. Today Artificial Intelligence is more oriented to several form of collective intelligence, also building robot simulators (hardware or software) to deeply understand collective behaviors in human beings and society as a whole. Modeling has also been crucial in the social sciences, to understand how complex systems can arise from simple rules. However, while engineers' simulations can be performed in the physical world using robots, for social scientist this is impossible. For decades, researchers tried to improve simulations by endowing artificial agents with simple and complex rules that emulated human behavior also by using artificial intelligence (AI). To include human beings and their real intelligence within artificial societies is now the big challenge. We present an hybrid (human-artificial) platform where experiments can be performed by simulated artificial worlds in the following manner: 1) agents' behaviors are regulated by the behaviors shown in Virtual Reality involving real human beings exposed to specific situations to simulate, and 2) technology transfers these rules into the artificial world. These form a closed-loop of real behaviors inserted into artificial agents, which can be used to study real society",Virtual Reality for Artificial Intelligence: human-centered simulation for social science,,'IOS Press',10.3233/978-1-61499-595-1-177,,core
102564240,26/08/2015,"Abstract—This paper studies the impact of interfaces allowing non-expert users to efficiently and intuitively teach a robot to recognize new visual objects. We present challenges that need to be addressed for real-world deployment of robots capable of learning new visual ¡objects in interaction with everyday users. We argue that in addition to robust machine learning and computer vision methods, well-designed interfaces are crucial for learning efficiency. In particular, we argue that interfaces can be key in helping non-expert users to collect good learning examples and thus improve the performance of the overall learning system. Then, we present four alternative human-robot interfaces: three are based on the use of a mediating artifact (smartphone, wiimote, wiimote and laser), and one is based on natural human gestures (with a Wizard-of-Oz recognition system). These interfaces mainly vary in the kind of feedback provided to the user, allowing him to understand more or less easily what the robot is perceiving, and thus guide his way of providing training examples differently. We then evaluate the impact of these interfaces, in terms of learning efficiency, usability and user’s experience, through a real world and large scale user study. In this experiment, we asked participants to teach a robot twelve different new visual objects in the context of a robotic game. This game happens in a home-like environment and was designed to motivate and engage users in an interaction where using the system was meaningful. We then discuss results that show significant differences among interfaces. In particular, we show that interfaces such as the smartphone interface allows non-expert users to intuitively provide much better training examples to the robot, almost as good as expert users who are trained for this task and aware of the different visual perception and machine learning issues. We also show that artifact-mediated teaching is significantly more efficient for robot learning, and equally good in terms of usability and user’s experience, than teaching thanks to a gesture-based human-like interaction. Index Terms—Human-robot interaction, user interfaces, robot learning, object visual recognition, user study, personal robotics. I",1The Impact of Human-Robot Interfaces on the Learning of Visual Objects,,,,,core
23937776,04/02/2014,"Abstract — Data association is an essential problem in simultaneous localization and mapping. It is hard to solve correctly, especially in ambiguous environments. We consider a scenario where the robot can ease the data association problem by deploying a limited number of uniquely identifiable artificial landmarks along its path and use them afterwards as fixed anchors. Obviously, the choice of the positions where the robot should drop these markers is crucial as poor choices might prevent the robot from establishing accurate data associations. In this paper, we present a novel approach for learning when to drop the landmarks so as to optimize the data association performance. We use Monte Carlo reinforcement learning for computing an optimal policy and apply a statistical convergence test to decide if the policy is converged and the learning process can be stopped. Extensive experiments also carried out with a real robot demonstrate that the data association performance using landmarks deployed according to our learned policies is significantly higher compared to other strategies. I",Deploying Artificial Landmarks to Foster Data Association in Simultaneous Localization and Mapping,,,,,core
289949509,2014-01-01T00:00:00,"The purpose of my Master Thesis is to examine, in detail, how current international law relates to autonomous weapon systems. The background for this project is that the requirement of current international humanitarian law presupposes that the characters of war, namely soldiers and military officers, are human beings. Warfare has, however, changed increasingly since the Geneva Conventions were written in August 1949. War is now increasingly fought with machines or virtual networks that are somehow controlled by humans, and this method of warfare is here to stay. Artificial intelligence has made war-machines less dependent on human control and thus more autonomous. The use of autonomous weapon systems will cause difficulties in establishing responsibility for the implementation of humanitarian law when numerous individuals are involved, and when the actor is a robot. The question of accountability is therefore essential since this issue will arise in the framework of all fully autonomous and semiautonomous weapon systems. The concept of autonomous weapon systems (AWS) will be defined more precisely alongside three different forms of autonomy in order to demonstrate its compliance with current international law. The analysis will begin from the bottom of the decision-making process to gradually eliminate all candidates who do not have sufficient knowledge to assume accountability. The candidates that will be observed are the military personnel, the acquisition team, the programmer or manufacturer, corporations, and lastly, the robot. Each chapter will build upon the next and include a descriptive part in the beginning with an analysis toward the end of the thesis. Parallels will be drawn between the new legal phenomenon and existing legal systems. The thesis will later lead to the conclusion that States cannot absolve themselves of their obligations under international humanitarian law when deploying autonomous weapon systems on the field. The State will still remain responsible for ensuring that relevant and fundamental standards are met and that international law is respected. The thesis will also argue that there is no fully autonomous AWS existing at the present time, and AWSes can therefore not itself be held accountable for crimes under international law. Nevertheless, this new character of warfare will bring real challenges for military commanders, not least in how to manage the information to ensure proper conduct but also to maintain discipline amongst subordinates. An effective system of accountability for autonomous weapon systems is when the lines for responsibility are well defined. To the extent that the autonomy of semi-autonomous causes gaps in current accountability instruments, it will be argued that the gaps can be filled through establishment of a new framework of command responsibility with a technical expert as a vital subordinate.Syftet med mitt examensarbete är att undersöka i detalj hur gällande folkrätt förhåller sig till autonoma vapensystem. Bakgrunden till detta projekt är att internationell humanitär rätt förutsätter att aktörerna i kriget, soldater och officerare är människor. Krigföring har dock förändrats i allt högre grad sedan de första Genèvekonventionerna skrevs i augusti 1949. Krig kämpas numera med maskiner eller virtuella nätverk som på något sätt kontrolleras av människan, och denna metod av krigföring är här för att stanna. Artificiell intelligens har gjort krigsmaskiner mindre beroende av mänsklig kontroll och därmed mer självständiga. Den nya användningen av autonoma vapensystem kommer att orsaka svårigheter i fastställandet av ansvar för implementeringen av humanitär rätt när många enskilda personer är inblandade och den som agerar är en robot. Frågan om ansvar således är viktigt eftersom frågan kommer att uppstå inom ramen för alla hel- och halvautonoma vapen. Begreppet autonoma vapensystem kommer att preciseras genom tre olika former av självständighet för att se användandets överensstämmelse med gällande internationell rätt. Analysen kommer att göras i samband med bedömningen av den frågeställning som börjar längst ner i beslutsfattandet för att senare gradvis eliminera alla kandidater som inte hade tillräckliga kunskaper för att åta ansvar. De kandidater som kommer att observeras i uppsatsen är den militära personalen, personen/personer som anskaffar vapnet, programmeraren eller tillverkaren, företag och slutligen roboten. Varje kapitel kommer att innehålla en beskrivande del i början med en analys mot slutet av uppsatsen. Paralleller kommer även att dras mellan det nya rättsliga fenomenet och befintliga rättssystem. Uppsatsen utgår sedan till slutsatsen att staterna inte kan frita sig från ansvar och skyldigheter enligt internationell humanitär rätt genom upphandlandet av autonoma vapensystem på fältet. Staten kommer fortfarande att fortsätta att ansvara för att se till att de relevanta normerna uppfylls och att folkrätten respekteras. Uppsatsen kommer även hävda att det inte finns helt självständiga AWS existerande i dagsläget, vilket leder till dess oförmåga att ställas till svars för brott mot internationell rätt. Dock, kommer den nya karaktären av krigföring skapa verkliga utmaningar för militära befälhavare, inte minst i hur man ska hantera informationen från vapensystemet för att säkerställa operationer men också för att upprätthålla disciplinen bland underordnade. Ett effektivt system för ansvarsskyldighet för det autonoma vapensystemet är när linjerna för ansvar är väl definierade och definitiva. I den mån halvautonoma vapensystem orsakar ofullständigheter i nuvarande ansvarsinstrument, kommer uppsatsen hävda att dessa luckor kan fyllas genom inrättandet av ett nytt ramverk av befälsansvar med den tekniska experten som en avgörande underordnad",Semi-Autonomous Weapon Systems in International Humanitarian Law,https://core.ac.uk/download/pdf/289949509.pdf,Lunds universitet/Juridiska institutionen,,,core
33249923,2014-09-01T00:00:00,"Technological and conceptual advances in fields such as artificial intelligence, robotics, and material science have enabled robotic building to be in the last decade prototypically implemented. In this context, robotic building implies both physically built robotic environments and robotically supported building processes, whereas reconfigurable, robotic environments incorporating sensor-actuator mechanisms that enable buildings to interact with their users and surroundings in real-time require design to production, assembly, and operation chains that may be (in part or as whole) implemented by robotic means. This paper presents and discusses research and experimental developments in robotic building implemented more recently at Hyperbody.Architectural Engineering +TechnologyArchitecture and The Built Environmen",Robotic buildings(s),,'Baltzer Science Publishers',10.7564/14-NGBJ8,,core
82385915,31/12/2015,"AbstractIn this paper a revised reinforcement learning method is presented for stability control problems with real-value inputs and outputs. The revised eXtended Classifier System for Real-input and Real-output (XCSRR) controller is designed, which is capable of working at fully real-value environment such as stability control of robots. XCSRR is a novel approach to enhance the performance of classifier systems for more practical problems than systems with merely binary behaviour. As a case study, we use XCSRR to control the stability of a biped robot, which is subjected to unknown external forces that would disturb the robot equilibrium. The external forces and the dynamics of the upper body of the biped robot are modelled in MATLAB software to train the XCSRR controller. Theoretical and experimental results of the learning behaviour and the performance of stability control on the robot demonstrate the strength and efficiency of the proposed new approach",An Improved eXtended Classifier System for the Real-time-input Real-time-output (XCSRR) Stability Control of a Biped Robot ,https://core.ac.uk/download/pdf/82385915.pdf,Published by Elsevier B.V.,10.1016/j.procs.2015.09.198,,core
103096625,01/11/2015,"Artificial Intelligence is hardly a new idea. Human likenesses, with the ability to act as human, dates back to Geek mythology with Pygmalion’s ivory statue or the bronze robot of Hephaestus. However, with innovations in the technological world, AI is undergoing a renaissance that is giving way to new channels of creativity. The study and pursuit of creating artificial intelligence is more than designing a system that can beat grand masters at chess or win endless rounds of Jeopardy!. Instead, the journey of discovery has more real-life applications than could be expected. While it may seem like it is out of a science fiction novel, work in the field of AI can be used to perfect face recognition software or be used to design a fully functioning neural network. At the International Journal of Advanced Research in Artificial Intelligence, we strive to disseminate proposals for new ways of looking at problems related to AI. This includes being able to provide demonstrations of effectiveness in this field. We also look for papers that have real-life applications complete with descriptions of scenarios, solutions, and in-depth evaluations of the techniques being utilized. Our mission is to be one of the most respected publications in the field and engage in the ubiquitous spread of knowledge with effectiveness to a wide audience. It is why all of articles are open access and available view at any time",Editorial Preface From the Desk of Managing Editor…,,,,,core
103438962,04/12/2015,"Artificial Intelligence is hardly a new idea. Human likenesses, with the ability to act as human, dates back to Geek mythology with Pygmalion’s ivory statue or the bronze robot of Hephaestus. However, with innovations in the technological world, AI is undergoing a renaissance that is giving way to new channels of creativity. The study and pursuit of creating artificial intelligence is more than designing a system that can beat grand masters at chess or win endless rounds of Jeopardy!. Instead, the journey of discovery has more real-life applications than could be expected. While it may seem like it is out of a science fiction novel, work in the field of AI can be used to perfect face recognition software or be used to design a fully functioning neural network. At the International Journal of Advanced Research in Artificial Intelligence, we strive to disseminate proposals for new ways of looking at problems related to AI. This includes being able to provide demonstrations of effectiveness in this field. We also look for papers that have real-life applications complete with descriptions of scenarios, solutions, and in-depth evaluations of the techniques being utilized. Our mission is to be one of the most respected publications in the field and engage in the ubiquitous spread of knowledge with effectiveness to a wide audience. It is why all of articles are open access and available view at any time",Editorial Preface From the Desk of Managing Editor…,,,,,core
101037837,07/01/2015,"Abstract — In this paper, we demonstrate the design and implementation of a multi-agent algorithm including a set of strategies for robot soccer games in Fira [1] simulation league. Soccer games are pseudo matches performed on a simulator for testing real-timedecision-making schemes for cooperative multi-agent systems. One of most important issues in the soccer game is how to pass a ball among teammates. Many proposed strategies [2][3], based on reinforcement learning methods [21] [22][23][24] or heuristic schemes to infer the possible risks and costs, do not consider ball passing as a multi-criteria optimum problem. To determine whether passing the ball to a teammate in the goal area, a multi-criteria decision-making strategy is proposed by considering the multiple criteria, the angle of the ball to the two goalposts, the distance between the ball and the goal, and the position of the enemies. As to the formation of a team, we introduce an idea, called virtual forces [18], to suggest the optimum positions of team players. The optimum positions will depend on the real-time conditions such as the ball position and the positions of all enemy players. To evaluate the proposed strategies, we take the team UvA-Trilearn[4], the world champion in 2002, as an opponent. The results show that the proposed strategies attain better performance than UvA-Trilearn. I",A Multi-agent Algorithm for Robot Soccer Games in Fira Simulation League,,,,,core
229060871,2014-01-01T08:00:00,"In recent years, the advancement of neurobiologically plausible models and computer networking has resulted in new ways of implementing control systems on robotic platforms. The work presents a control approach based on vertebrate neuromodulation and its implementation on autonomous robots in the open-source, open-access environment of robot operating system (ROS). A spiking neural network (SNN) is used to model the neuromodulatory function for generating context based behavioral responses of the robots to sensory input signals. The neural network incorporates three types of neurons- cholinergic and noradrenergic (ACh/NE) neurons for attention focusing and action selection, dopaminergic (DA) neurons for rewards- and curiosity-seeking, and serotonergic (5-HT) neurons for risk aversion behaviors. This model depicts neuron activity that is biologically realistic but computationally efficient to allow for large-scale simulation of thousands of neurons. The model is implemented using graphics processing units (GPUs) for parallel computing in real-time using the ROS environment. The model is implemented to study the risk-taking, risk-aversive, and distracted behaviors of the neuromodulated robots in single- and multi-robot configurations. The entire process is implemented in a cloud computing environment using ROS where the robots communicate wirelessly with the computing nodes through the on-board laptops. However, unlike the traditional neural networks, the neuromodulatory models do not need any pre-training. Instead, the robots learn from the sensory inputs and follow the behavioral facets of living organisms. The details of algorithm development, the experimental setup and implementation results under different conditions, in both single- and multi-robot configurations, are presented along with a discussion on the scope of further work",Neuromodulation Based Control of Autonomous Robots on a Cloud Computing Platform,https://core.ac.uk/download/229060871.pdf,Digital Commons@Georgia Southern,,,core
35098120,2015-12-18T00:00:00,"This report summarises and integrates two different tracks of research for the purpose of envisioning and preparing a joint research project proposal.

Soft- and hardware systems have become increasingly complex and act ""concurrently"", both with respect to memory access (i.e. information flow) and computational resources (i.e. ""services""). The software development metaphor of cloud-storage, cloud-computing and service-oriented design has been anticipated by artificial intelligence (AI) research at least 30 years ago (parallel and distributed computation already dates back to the 1950’s and 1970s). What is known as a ""service"" today is what in AI is known as the capability of an agent; and the problem of information flow and consistency has been a headstone of information processing ever since. Based on a real-world robotics application we demonstrate how an increasingly abstract description of collaborating or competing agents correspond to a set of concurrent processes.

In the second part we review several approaches to the theory of concurrent systems. Based on the different kinds of program semantics we present corresponding logical and algebraic means for the description of parallel processes and memory access. It turns out that Concurrent Kleene Algebra (CKA) and its related graphlet metaphor appears to deliver a one-to-one matching formal description of the module structures developed in the first part. The problem of snapshotting system states in order to receive (partial) traces of a running system seems to be well describable by a Temporal Logic of Actions (TLA). Finally, the different types of subsystems and their mutual requirements such as exclusiveness etc. seem to be best describable in a separation-logic like approach.

We conclude with a list of research questions detailing some of the many promising issues raised in the report","On the needs for specification and verification of collaborative and concurrent robots, agents and processes",https://core.ac.uk/download/35098120.pdf,,,,core
198988799,2014-01-01T00:00:00,"International audienceCompact Bionic Handling Assistant (CBHA) is a continuum manipulator, with pneumatic-based actuation and compliant gripper. This bionic arm is attached to a mobile robot named Robotino. Inspired by the elephant's trunk, it can reproduce biological behaviors of trunks, tentacles, or snakes. Unlike rigid link robot manipulators, the development of high performance control algorithm of continuum robot manipulators remains a challenge, particularly due to their complex mechanical design, hyper-redundancy and presence of uncertainties. Numerous studies have been investigated for modeling of such complex systems. Such continuum robots, like the CBHA present a set of nonlinearities and uncertainties, making difficult to build an accurate analytical model, which can be used for control strategies development. Hence, learning approach becomes a suitable tool in such scenarios in order to capture un-modeled nonlinear behaviors of the continuous robots. In this paper, we present a qualitative modeling approach, based on neuronal model of the inverse kinematic of CBHA. A penalty term constraint is added to the inverse objective function into Distal Supervised Learning (DSL) scheme to select one particular inverse model from the redundancy manifold. The inverse kinematic neuronal model is validated by conducting a real-time implementation on a CBHA trunk",Qualitative approach for inverse kinematic modeling of a Compact Bionic Handling Assistant trunk,,HAL CCSD,,,core
79832360,2014-01-01T00:00:00,"模型預測控制是一種基於模型的先進控制策略，它通過反復優化一個有限時域内的約束優化問題實時求解最優控制信號。作爲一種有效的多變量控制方法，模型預測控制在過程控制、機械人、經濟學等方面取得了巨大的成功。模型預測控制研究與發展的一個關鍵問題在於如何實現高性能非綫性和魯棒預測控制算法。實時優化是一項具有挑戰性的任務，尤其在優化問題時非凸優化的情況下，實時優化變得更爲艱巨。在模型預測控制取得發展的同時，以建立仿腦計算模型為目標的神經網絡研究也取得一些重要突破，尤其是在系統辨識和實時優化方面。神經網絡為解決模型預測控制面臨的瓶頸問題提供了有力的工具。本篇論文重點討論基於神經動力學方法的模型預測控制的設計與分析。論文的主要目標在於設計高性能神經動力學算法進而提高模型預測控制的最優性與計算效率。論文包括兩大部分。第一部分討論如何在不需要求解非凸優化的前提下解決非綫性和魯棒模型預測控制。主要的解決方案是將非相信模型分解為帶有未知項的仿射模型，或將非綫性模型轉換為綫性變參數系統。仿射模型中的未知項通過極限學習機進行建模和數值補償。針對系統中的不塙定干擾，利用極小極大算法和擾動不變集方法獲取控制系統魯棒性。儅需要考慮多個評價指標是，採用目標規劃設計多目標優化算法。論文第一部分提出的設計方法可以將非綫性和魯邦模型預測控制設計為凸優化問題，進而採用神經動力學優化的方法進行實時求解。論文的第二部分設計了針對非凸優化的多神經網絡算法，並在此基礎上提出了模型預測控制算法。多神經網絡算法模型人類頭腦風暴的過程，同時應用多個神經網絡相互協作地進行全局搜索。神經網絡的動態方程指導其進行局部精確搜索，神經網絡之間的信息交換指導全局搜索。實驗結果表明該算法可以高效地獲得非凸優化的全局最優解。基於多神經網絡優化的的模型預測控制算法是一種創新性的高性能控制方法。論文的最後討論了應用模型預測控制解決海洋航行器的運動控制問題。Model predictive control (MPC) is an advanced model-based control strategy that generates control signals in real time by optimizing an objective function iteratively over a finite moving prediction horizon, subject to system constraints. As a very effective multivariable control technology, MPC has achieved enormous success in process industries, robotics, and economics. A major challenge of the MPC research and development lies in the realization of high-performance nonlinear and robust MPC algorithms. MPC requires to perform real time dynamic optimization, which is extremely demanding in terms of solution optimality and computational efficiency. The difficulty is significantly amplified when the optimization problem is nonconvex.In parallel to the development of MPC, research on neural networks has made significant progress, aiming at building brain-like models for modeling complex systems and computing optimal solutions. It is envisioned that the advances in neural network research will play a more important role in the MPC synthesis. This thesis is concentrated on analysis and design of neurodynamic approaches to nonlinear and robust MPC. The primary objective is to improve solution optimality by developing highly efficient neurodynamic optimization methods.The thesis is comprised of two coherent parts under a unified framework. The first part consists of several neurodynamics-based MPC approaches, aiming at solving nonlinear and robust MPC problems without confronting non-convexity. The nonlinear models are decomposed to input affine models with unknown terms, or transformed to linear parameter varying systems. The unknown terms are learned by using extreme learning machines via supervised learning. Minimax method and disturbance invariant tube method are used to achieve robustness against uncertainties. When multiobjective MPC is considered, goal programming technique is used to deal with multiple objectives. The presented techniques enable MPC to be reformulated as convex programs. Neurodynamic models with global convergence, guaranteed optimality, and low complexity are customized and applied for solving the convex programs in real time. Simulation results are presented to substantiate the effectiveness and to demonstrate the characteristics of proposed approaches. The second part consists of collective neurodynamic optimization approaches, aiming at directly solving the constrained nonconvex optimization problems in MPC. Multiple recurrent neural networks are exploited in framework of particle swarm optimization by emulating the paradigm of brainstorming. Each individual neural network carries out precise constrained local search, and the information exchange among neural networks guides the improvement of the solution quality. Implementation results on benchmark problems are included to show the superiority of the collective neurodynamic optimization approaches. The essence of the collective neurodynamic optimization lies in its global search capability and real time computational efficiency. By using collective neurodynamic optimization, high-performance nonlinear MPC methods can be realized. Finally, the thesis discusses applications of MPC on the motion control of marine vehicles.Detailed summary in vernacular field only.Detailed summary in vernacular field only.Yan, Zheng.Thesis (Ph.D.) Chinese University of Hong Kong, 2014.Includes bibliographical references (leaves 186-203).Abstracts also in Chinese",Analysis and design of neurodynamic approaches to nonlinear and robust model predictive control.,,,,,core
103115813,01/11/2015,"Abstract — This paper implements multi-layered ANFIS controller in PIC16F886 micro-controller as a supervisory control for a 6 DOF robotic arm. The complexity in mathematical modelling demands for machine-learning techniques, which rely less on precise mathematical analysis. ANFIS is one such machine learning technique which helps in decision making for the control of robotic arms. Standard PD controllers could be used as servos to guarantee precise tracking. Based on real physical parameters of Dexter ER-1, a model is developed in Sim –Mechanics to capture the actual dynamics of robot arm. The time dependent reachable set is generated out of which it gave nearly 40,000 data samples. This data is used as inverse training data for ANFIS network and is implemented as a supervisory controller in microcontroller. The controller is tested with predefined paths and random position targets and results are shown to act satisfactorily. Keywords- ANFIS, micro controller, PIC, fuzzy logic",Embedded ANFIS as a Supervisory Controller for a 6-DOF Robotic Arm,,,,,core
100551822,21/12/2014,"Abstract:- This paper refers to a virtual environment which represents the main support for experiments with mobile robots in the design and testing stage. This software environment is very useful because, compairing to the experiments with real robots, it allow the testing and evaluation of different types of interfaces and different working environments with diverse configurations. A very important facility of this interactive software environment is the fact that the designers of the robots sensors and interfaces are able to work in parallel to design test, optimize and realize different control devices for the robot. Key-Words:- artificial intelligence, mobile robots, virtual experiments, virtual environment, simulator, synchronization ",Virtual Experiments Environment For Mobile Robots Design And Testing,,,,,core
102634140,31/08/2015,"Efficient localisation is a highly desirable property for an autonomous navigation system. Weightless neural networks offer a real-time approach to robotics applications by reducing hardware and software requirements for pattern recognition techniques. Such networks offer the potential for objects, structures, routes and locations to be easily identified and maps constructed from fused limited sensor data as information becomes available. We show that in the absence of concise and complex information, localisation can be obtained using simple algorithms from data with inherent uncertainties using a combination of Genetic Algorithm techniques applied to a Weightless Neural Architecture. ",Highly efficient Localisation utilising Weightless neural systems.,,,,,core
19208033,2014-01-01T00:00:00,"We introduce AutoMoDe: a novel approach to the automatic design of control software for robot swarms. The core idea in AutoMoDe recalls the approach commonly adopted in machine learning for dealing with the bias-variance tradedoff: to obtain suitably general solutions with low variance, an appropriate design bias is injected. AutoMoDe produces robot control software by selecting, instantiating, and combining preexisting parametric modules-the injected bias. The resulting control software is a probabilistic finite state machine in which the topology, the transition rules and the values of the parameters are obtained automatically via an optimization process that maximizes a task-specific objective function. As a proof of concept, we define AutoMoDe-Vanilla, which is a specialization of AutoMoDe for the e-puck robot. We use AutoMoDe-Vanilla to design the robot control software for two different tasks: aggregation and foraging. The results show that the control software produced by AutoMoDe-Vanilla (i) yields good results, (ii) appears to be robust to the so called reality gap, and (iii) is naturally human-readable. © 2014 Springer Science+Business Media New York.SCOPUS: ar.jinfo:eu-repo/semantics/publishe",AutoMoDe: A novel approach to the automatic design of control software for robot swarms,,'Springer Science and Business Media LLC',10.1007/s11721-014-0092-4,,core
198985247,2014-01-01T00:00:00,"International audienceExisting object recognition techniques often rely on human labeled data conducting to severe limitations to design a fully autonomous machine vision system. In this work, we present an intelligent machine vision system able to learn autonomously individual objects present in real environment. This system relies on salient object detection. In its design, we were inspired by early processing stages of human visual system. In this context we suggest a novel fast algorithm for visually salient object detection, robust to real-world illumination conditions. Then we use it to extract salient objects which can be efficiently used for training the machine learning-based object detection and recognition unit of the proposed system. We provide results of our salient object detection algorithm on MSRA Salient Object Database benchmark comparing its quality with other state-of-the-art approaches. The proposed system has been implemented on a humanoid robot, increasing its autonomy in learning and interaction with humans. We report and discuss the obtained results, validating the proposed concepts",A machine learning based intelligent vision system for autonomous object detection and recognition,,'Springer Science and Business Media LLC',10.1007/s10489-013-0461-5,,core
102390601,23/08/2015,"Abstract. The paper mainly presents the developments of our middle-size league robot team “NuBot ” for RoboCup 2008 Suzhou. The improvements lie in robot hardware like new panoramic mirror and kicking device, and in robot software such as algorithms for panoramic image processing and robot’s self-localization, multi-robot cooperation, path planning and motion control. Our current research focuses on robust robot vision, multi-robot cooperation, new learning controller for DC motors, and reinforcement learning for real robots. ",NuBot Team Description Paper 2008,,,,,core
145650963,2015-05-01T00:00:00Z,"Although robotics has progressed to the extent that it has become relatively accessible with low-cost projects, there is still a need to create models that accurately represent the physical behavior of a robot. Creating a completely virtual platform allows us to test behavior algorithms such as those implemented using artificial intelligence, and additionally, it enables us to find potential problems in the physical design of the robot. The present work describes a methodology for the construction of a kinematic model and a simulation of the autonomous robot, specifically of an omni-directional wheeled robot. This paper presents the kinematic model development and its implementation using several tools. The result is a model that follows the kinematics of a triangular omni-directional mobile wheeled robot, which is then tested by using a 3D model imported from 3D Studio® and Matlab® for the simulation. The environment used for the experiment is very close to the real environment and reflects the kinematic characteristics of the robot",Kinematics modeling and simulation of an autonomous omni-directional mobile robot,,Universidad Nacional de Colombia,10.15446/ing.investig.v35n2.47763,"[{'title': None, 'identifiers': ['issn:0120-5609', '0120-5609', '2248-8723', 'issn:2248-8723']}]",core
101800193,19/03/2015,"Developing software for Autonomous Mobile Robot (AMR) is difficult and requires knowledge in embedded systems, real-time software issues, control theories and artificial intelligence aspects. To tackle the difficulty in developing software for AMR, many researchers have proposed the approach of reusable software component for mobile robot systems. Software pattern provides a way to reuse knowledge of expert across domain at all level of software development. In this paper component-based analysis patterns applicable to AMR software at high-level software development is proposed. Some important AMR component-based analysis patterns on AMR embedded software requirements are presented. How the analysis patterns can help in documenting two existing AMR software through pattern-based reverse engineering process is also illustrated",ENGINEERING OF MOBILE ROBOT SOFTWARE,,,,,core
296621333,2015-11-26T14:12:30Z,"The design of autonomous navigation systems for mobile robots, with simultaneous objectives to be satisfied such as garbage collection with integrity maintenance, requires refined coordination mechanisms to deal with modules of elementary behaviour. This paper shows the implementation on a real Khepera II robot of an immuno-genetic network for autonomous navigation that combines an evolutionary algorithm with a continuous immune network model. The proposed immuno-genetic system has the immune network implementing a dynamic process of decision-making, and the evolutionary algorithm defining the network structure. To be able to evaluate the controllers (immune networks) on the evolutionary process, a virtual environment was used for computer simulation, based on the characteristics of the navigation problem. The immune networks obtained by evolution were then analyzed and tested on new situations, presenting coordination capability in simple and more complex tasks. Some preliminary experiments on a real Khepera II robot demonstrate the feasibility of the evolved immune networks. © 2003 IEEE.1420426Dasgupta, D., (1999) Artificial Immune Systems and Their Applications, , Springer-VerlagDorigo, M., Colombetti, M., (1997) Robot Shaping: An Experiment in Behavior Engineering (Intelligent Robotics and Autonomous Agents), , MIT PressDe Castro, L.N., Timmis, J.I., (2002) Artificial Immune Systems: A New Computational Intelligence Approach, , Springer-Verlag: LondonFarmer, J.D., Packard, N.H., Perelson, A.S., The immune system, adaptation and machine learning (1986) Physica, 22 D, pp. 187-204Goldberg, D.E., (1989) Genetic Algorithms in Search Optimization, and Machine Learning, , Addison-Wesley, IncHolland, J.H., (1992) Adaptation in Natural and Artificial Systems: An Introductory Analysis with Applications to Biology, Control, and Artificial Intelligence, pp. MI91. , The MIT Press. Ann ArborIshiguro, A., Kondo, T., Watanabe, Y., Shirai, Y., Uchikawa, H., Immunoid: A robot with a decentralized consensus-making mechanism based on the immune system (1996) ICMAS Workshop on Inimumfy: Based Systems, pp. 82-92. , DecemberJeme, N.K., Towards a network theory of the immune system (1974) Ann. Immunol. (Int. Pasteur), 125 C, pp. 373-389Jeme, N.K., Idiotypic Networks and other preconceived ideas (1984) Immunological Rev, 79, pp. 5-24(2003), http//www.k-team.com, URL:, provides additional information about the Khepera II robotsMichelan, R., Von Zuben, F.J., Decentralized control system for autonomous navigation based on an evolved artificial immune network (2002) Proc. of the 2002 Congress on Evolutionary Computation (CEC2002), 2, pp. 1021-1026. , Workshop on Artificial Immune Systems in the 2002 WCCI'2002, Honolulu, Hawaii, May 12-17Nolfi, S., Floreano, D., (2000) Evolutionary Robotics: The Biology, Intelligence, and Technology of Self-organizing Machines, , The MIT PressVargas, P.A., De Castro, L.N., Von Zuben, F.J., Artificial immune systems as complex adaptive systems (2002) Proceedings of the First International Conference on Artificial Immune Systems-ICARIS-2002, pp. 115-123Vargas, P.A., De Castro, L.N., Von Zuben, F.J., Mapping artificial immune systems into learning classifier systems (2003) 1WLCS-2002. to Appear in Lecture Notes in Artificial Intelligence (LNAI), , Springer-VerlagVargas, P.A., De Castro, L.N., Michelan, R., Von Zuben, F.J., An immune learning classifier network for autonomous navigation (2003) Proceedings of the Second International Conference on Artificial Immune Systems (ICARIS'2003), 2787, pp. 69-80. , Timmis. J., Bentley, P. and Hart. E, eds., LNCS, Edinburgh, UKWatanabe, Y., Ishiguro, A., Uchikawa, H., Decentralized behaviour arbitration mechanism for autonomous mobile robot using immune network (1999) Artificial Immune Systems and Their Applications, , D. Dasgupta Editor, Springe",Implementation Of An Immuno-genetic Network On A Real Khepera Ii Robot,,IEEE Computer Society,10.1109/CEC.2003.1299606,,core
296633786,2015-11-26T14:45:09,"There are few contributions to robot autonomous navigation applying Learning Classifier Systems (LCS) to date. The primary objective of this work is to analyse the performance of the strength-based LCS and the accuracy-based LCS, named EXtended Learning Classifier System (XCS), when applied to two distinct robotic tasks. The first task is purely reactive, which means that the action to be performed can rely only on the current status of the sensors. The second one is non-reactive, which means that the robot might use some kind of memory to be able to deal with aliasing states. This work presents a rule evolution analysis, giving examples of evolved populations and their peculiarities for both systems. A review of LCS derivatives in robotics is provided together with a discussion of the main findings and an outline of future investigations. © 2008 Springer Berlin Heidelberg.4998 LNAI286305Pfeifer, R., Scheier, C., (1999) Understanding Intelligence, , MIT Press, CambridgeNolfi, S., Floreano, D., (2004) Evolutionary Robotics: The Biology, Intelligence, and Technology of Self-Organizing Machines, , Bradford BookStudley, M., Bull, L., X-TCS: Accuracy-based learning classifier system robotics (2005) Congress on Evolutionary Computation, pp. 2099-2106. , IEEE, Los AlamitosHolland, J.H.: Adaptation in Natural and Artificial Systems. University of Michigan Press (1975) 304 R.C. Moioli, P.A. Vargas, and F.J. Von ZubenHolland, J.H., (1995) Hidden Order, , Addison-Wesley, ReadingHolland, J.H., Holyoak, K.J., Nisbett, R.E., Thagard, P., (1986) Induction: Processes of Inference, Learning, and Discovery, , MIT Press, CambridgeHolmes, J.H., Lanzi, P.L., Stolzmann, W., Wilson, S., (2000) Learning classifier systems: New models, successful applicationsWilson, S.W., Classifier fitness based on accuracy (1995) Evolutionary Computation, 3, pp. 149-175Wilson, S.W., Classifiers that approximate functions. Natural Computing (2002) an international journal, 1 (2-3), pp. 211-234Lanzi, P.L., Wilson, S.W., Toward optimal classifier system performance in nonmarkov environments (2000) Evolutionary Computation, 8 (4), pp. 393-418Stolzmann, W., Learning classifier systems using the cognitive mechanism of anticipatory behavioural control (1996) Proceedings of the First European Workshop on Cognitive Modelling, pp. 82-89Hurst, J., Bull, L., Melhuish, C., TCS learning classifier system controller on a real robot (2002) Proceedings of the 7th International Conference on Parallel Problem Solving from NatureBull, L., (2004) Applications of Learning Classifier Systems, , Springer, HeidelbergVargas, P.A., Lyra Filho, C., von Zuben, F.J., Application of learning classifier systems to the on line reconfiguration of electric power distribution networks (2004) Applications of Learning Classifier Systems, 150, pp. 260-275Armano, G., (2004) NXCS experts for financial time series forecasting, , Bull, L, ed, Applications of Learning Classifier SystemsGoldberg, D.E., (1989) Genetic Algorithms in Search, Optimization, and Machine Learning, , Addison Wesley, Inc, ReadingVargas, P.A.: Classifier systems for loss reduction in electric power distribution networks (in Portuguese). Master's thesis, School of Electrical and Computer Engineering, Unicamp, Brazil (2000)Booker, L.B., Goldberg, D.E., Holland, J.H., Classifier systems and genetic algorithms (1989) Artificial Intelligence, 40, pp. 235-282Cazangi, R.R., Von Zuben, F.J., Figueiredo, M., A classifier system in real applications for robot navigation (2003) The IEEE Congress on Evolutionary Computation, 1, pp. 574-580. , Canberra, AustraliaButz, M.V., Wilson, S.W., An algorithmic description of XCS (2001) LNCS (LNAI, 1996, pp. 253-272. , Lanzi, P.L, Stolzmann, W, Wilson, S.W, eds, IWLCS 2000, Springer, HeidelbergCliff, D., Ross, S., (1995) Adding temporary memory to ZCS, , Adaptive BehaviorWilson, S.W., ZCS: A zeroth level classifier system (1994) Evolutionary Computation, 2 (1), pp. 1-18Stolzmann, W., Butz, M.V., (1999) Latent learning and action planning in robots with anticipatory classifier systems, , Learning Classifier SystemsDorigo, M., Colombetti, M., (1997) Robot Shaping: An Experiment in Behavior Engineering, , MIT Press, CambridgeKatagami, D., Yamada, S., Interactive classifier system for real robot learning (2000) Proceedings of the 2000 IEEE International Workshop on Robot and Human Interactive Communnication, , Osaka, JapanBonarini, A., An introduction to learning fuzzy classifier systems (2000) LNCS (LNAI, 1813, pp. 83-106. , Lanzi, P.L, Stolzmann, W, Wilson, S.W, eds, IWLCS 1999, Springer, HeidelbergBonarini, A.: Fuzzy modelling: Paradigms and practice. In: Pedrycz, W. (ed.) Fuzzy Modelling: Paradigms and Practice. Kluwer Academic Press, Norwell (1996) Analysing LCS in Reactive and Non-reactive Robotic Tasks 305Bonarini, A., Matteucc, M., Fixes: A fuzzy implementation of XCS (2007) IEEE International Fuzzy Systems Conference, FUZZ-IEEEGerard, P., Sigaud, O., YACS: Combining dynamic programming with generalization in classifier systems (2001) LNCS (LNAI, 1996, pp. 259-266. , Lanzi, P.L, Stolzmann, W, Wilson, S.W, eds, IWLCS 2000, Springer, HeidelbergHurst, J., Bull, L., A neural learning classifier system with self-adaptive constructivism for mobile robot control (2006) Artif. Life, 12 (3), pp. 353-380Webb, E., Hart, E., Ross, P., Lawson, A.: Controlling a simulated khepera with an XCS classifier system with memory. In: Banzhaf, W., Ziegler, J., Christaller, T., Dittrich, P., Kim, J.T. (eds.) ECAL 2003. LNCS (LNAI), 2801, pp. 885-892. Springer, Heidelberg (2003)Vargas, P.A., de Castro, L.N., Michelan, R., Von Zuben, F.J.: An immune learning classifier network for autonomous navigation. In: Timmis, J., Bentley, P.J., Hart, E. (eds.) ICARIS 2003. LNCS, 2787, pp. 69-80. Springer, Heidelberg (2003)Zatuchna, Z.V., (2005) AgentP: A Learning Classifier System with Associative Perception in Maze Environments, , PhD thesis, School of Computing Sciences, University of East AngliaKovacs, T., (2002) A learning classifier systems bibliography, , http://www.cs.bris.ac.uk/~kovacs/lcs/search.htmlKTEAM, S.A., (2007), http://www.k-team.comPerreta, S.J., Gallagher, J.C., (2004) The Java Khepera simulator from the wright, , state university, Ohio, USAKovacs, T., (2004) Strength or Accuracy: Credit Assignment in Learning Classifier Systems, , Springer, HeidelbergLanzi, P.L., An analysis of the memory mechanism of XCSM (1998) Proceedings of the Third Annual Conference on Genetic ProgrammingLanzi, P.L., Wilson, S.W., Optimal classifier system performance in non-markov environments (1999), Technical report, Politecnico de MilanoJakobi, N., (1998) Minimal Simulations for Evolutionary Robotics, , PhD thesis, University of SussexHusbands, P., Evolving robot behaviours with diffusing gas networks (1998) Evolutionary Robotics: First European Workshop, EvoRobotRiolo, R.L., The emergence of default hierarchies in learning classifier systems (1989) Proceedings of the Third Congress on Genetic AlgorithmsButz, M.V., Goldberg, D.E., Tharakunnel, K., Analysis and improvement of fitness exploitation in XCS: Bounding models, tournament selection, and bilateral accuracy (2003) Evolutionary Computation, 11 (3), pp. 239-27",Analysing Learning Classifier Systems In Reactive And Non-reactive Robotic Tasks,,'Springer Science and Business Media LLC',10.1007/978-3-540-88138-4-17,,core
196618218,2015-01-01T00:00:00,"Complex real problems increasingly require intelligent systems that combine knowledge, techniques and methodologies from various sources. Intelligent systems based on artificial intelligence techniques that are associated with the behavior of people can perform the processes of learning, reasoning and solving all kinds of problems. Such systems, which automatically can perform tasks set by the user or other software, today thankfully called intelligent agents. Independent, intelligent agents on the Internet can be very successful to perform some search work on behalf of and for the needs of different users. For efficient collection, manipulation and management of data, such software can be very interesting from the standpoint of intelligent data analysis in many areas the police. Analysis of the data collected by an intelligent agent (a software robot-bot) can be successfully utilized, among many jobs in the police, and in the field of crime and in particular manifestation of cyber­crime, traffic safety, emergencies, etc. To make the collection and analysis of data from criminal activities on the Internet effective, it is necessary to examine the existing artificial intelligence techniques to be used for the conclusion of the intelligent agents. On the other hand, using of methods of artificial intelligence in finding data along with intelligent data analysis (data mining) should be used, which has found wide use in the area of business, economics, mechanics, medicine, genetics, transport etc.Kompleksni realni problemi sve češće zahtevaju inteligentne sisteme koji kombinuju znanje, tehnike i metodologije iz različitih izvora. Inteligentni sistemi bazirani na tehnikama veštačke inteligencije koje asociraju na ponašanje ljudi mogu da obavljaju procese učenja, zaključivanja i rešavanje raznovrsnih problema. Ovakvi sistemi, koji automatski mogu da izvrše zadatke zadate od strane korisnika ili drugih softvera, danas se sreću pod imenom inteligentni agenti. Samostalno, inteligentni agenti na Internetu mogu veoma uspešno da izvode neki pretraživački posao u ime i za potrebe raznih korisnika. Zbog efikasnog sakupljanja, manipulisanja i upravljanja podacima, ovakvi softveri mogu biti veoma interesantni sa stanovišta inteligentne analize podataka u mnogim oblastima policije. Analiza podataka sakupljenih od strane inteligentnog agenta (softverskog robota - bota) može se uspešno iskoristiti, između mnogih poslova u policiji, i na polju kriminala i naročito pojavnog oblika sajber kriminala, bezbednosti saobraćaja, vanrednih situacija itd. Kako bi sakupljanje i analiza podataka iz kriminalnih aktivnosti na Internetu bila efikasna, neophodno je sagledati postojeće tehnike veštačke inteligencije koje se koriste za zaključivanje u inteligentnim agentima. S druge strane, treba iskoristiti metode veštačke inteligencije u pronalaženju podataka pri inteligentnoj analizi podataka (data mining-u) koja je našla široku primenu u oblasti poslovanja preduzeća, ekonomije, mehanike, medicine, genetike, saobraćaja i sl",Veštačka intelegencija u prikupljanju i analizi podataka u policiji,https://core.ac.uk/download/196618218.pdf,'Centre for Evaluation in Education and Science (CEON/CEES)',10.5937/NBP1503131K,"[{'title': 'Nauka bezbednost policija', 'identifiers': ['issn:0354-8872', '0354-8872']}]",core
89839154,2015-04-01T00:00:00Z,"The surface electromyography (sEMG) technique is proposed for muscle activation detection and intuitive control of prostheses or robot arms. Motion recognition is widely used to map sEMG signals to the target motions. One of the main factors preventing the implementation of this kind of method for real-time applications is the unsatisfactory motion recognition rate and time consumption. The purpose of this paper is to compare eight combinations of four feature extraction methods (Root Mean Square (RMS), Detrended Fluctuation Analysis (DFA), Weight Peaks (WP), and Muscular Model (MM)) and two classifiers (Neural Networks (NN) and Support Vector Machine (SVM)), for the task of mapping sEMG signals to eight upper-limb motions, to find out the relation between these methods and propose a proper combination to solve this issue. Seven subjects participated in the experiment and six muscles of the upper-limb were selected to record sEMG signals. The experimental results showed that NN classifier obtained the highest recognition accuracy rate (88.7%) during the training process while SVM performed better in real-time experiments (85.9%). For time consumption, SVM took less time than NN during the training process but needed more time for real-time computation. Among the four feature extraction methods, WP had the highest recognition rate for the training process (97.7%) while MM performed the best during real-time tests (94.3%). The combination of MM and NN is recommended for strict real-time applications while a combination of MM and SVM will be more suitable when time consumption is not a  key requirement",Comparison of sEMG-Based Feature Extraction and Motion Classification Methods for Upper-Limb Movement,,MDPI AG,10.3390/s150409022,"[{'title': None, 'identifiers': ['issn:1424-8220', '1424-8220']}]",core
296622416,2015-11-26T14:16:33Z,"[No abstract available]312563Arai, T., Pagello, E., Parker, L., Guest editorial, advances in multi-robot systems (2002) IEEE Transactions on Robotics and Automation, 18 (5), pp. 655-661Bonabeau, E., Dorigo, M., Theraulaz, G., (1999) Swarm Intelligence: From Natural to Artificial SystemsBalch, T., Arkin, R.C., Communication in reactive multiagent robotic systems (1994) Autonomous Robots, 1 (1), pp. 27-52Grasse, P., La reconstruction du nid et les coordinations inter-individuelle chez bellicoitermes natalenis et cubitermes sp la theorie de la stigmergie: Essai d'interpretation des termites constructeurs Insectes Sociaux, 6, p. 1959Camazine, S., Franks, N.R., Sneyd, J., Bonabeau, E., Deneubourg, J.-L., Theraula, G., (2001) Self-Organization in Biological Systems, , Princeton University PressHolland, O., Melhuish, C., Stimergy, self-organization, and sorting in collective robotics (1999) Artificial Life, 5 (2), pp. 173-202Cazangi, R.R., Von Zuben, F.J., Figueiredo, M.F., A classifier system in real applications for robot navigation (2003) Proceedings of the 2003 Congress on Evolutionary Computation, 1, pp. 574-580. , Canberra, Australia IEEE PressCazangi, R.R., Uma proposta evolutiva para controle inteligente em navegação autônoma de robôs (2004), Master's thesis, Faculdade de Engenharia Elétrica e de Computação, Universidade Estadual de CampinasHolland, J., Escaping brittleness: The possibilities of general purpose learning algorithms applied to parallel rule-based systems (1986) Machine Intelligence II, , In R. Michalsky, J. Carbonell, and T. Mitchell, editors Morgan KaufmannBrooks, R.A., Intelligence without reason (1991) Proceedings of the 1991 International Joint Conference on Artificial Intelligence, pp. 569-595Kube, C.R., Parker, C., Wang, T., Zhang, H., Biologically inspired collective robotics (2004) Recent Developments in Biologically In-spired Computing, , In L. N. de Castro and F. J. Von Zuben, editors, Idea Group IncNolfi, S., Floriano, D., (2000) Evolutionary Robotics, , The MIT PressEdelen, M., Swarm intelligence and stigmergy: Robotic implementation of foraging behavior (2003), Master's thesis, University of MarylandBonabeau, E., Theraulaz, G., Deneubourg, J.-L., Aron, S., Camazine, S., Self-organization in social insects (1997) Trends in Ecology and Evolution, 12, pp. 188-193de Castro, L.N., Timmis, J., (2002) Artificial Immune Systems: A New Computational Intelligence Paradigm, , SpringerVerlagResnick, M., Turtles, Termites, and Traffic Jams: Explorations in Massively Parallel Microworlds, , Bradford Books/MIT PressHolldobler, B., Wilson, E., (1990) The Ants, , Belknap Press of Harvard University PressCaetano, F.H., Klaus, J., Zara, F.J., (2002) Formigas: Biologia E Anatomia, , Editora da UNESPDeneubourg, J.L., Pasteels, J.M., Verhaeghe, J.C., Probabilistic behaviour in ants: A strategy of errors? (1983) Journal of Theoretical Biology, 105, pp. 259-271Pasteels, J., Deneubourg, J.L., Goss, S., Self-organization mechanisms in ant societies (i): Trail recruitment to newly discovered food sources (1987) Experientia Supplementum, 54, pp. 155-175Cao, Y., Fukunaga, A., Kahng, A., Cooperative mobile robotics: Antecedents and directions (1997) Autonomous Robots, 4 (1), pp. 7-27Tambe, M., Adabi, J., Al-Onaizan, Y., Erden, A., Kaminka, G., Marsella, S.C., Muslea, I., Building agent teams using an explicit teamwork model and learning (1999) Artificial Intelligence, 110 (2), pp. 215-239Wagner, I.A., Bruckstein, A.M., Cooperative cleaners: A study in ant-robotics (1997) Communications, Computation, Control, and Signal Processing, pp. 298-308Ding, Y., He, Y., Jiang, J., Multi-robot cooperation method based on the ant algorithm (2003) Proceedings of the 2003 IEEE Swarm Intelligence Symposium, pp. 14-18. , Indianapolis, USA IEEEDrogoul, A., Ferber, J., From tom thumb to the dockers: Some experiments with foraging robots (1992) Proceedings of the Second International Conference on Simulation of Adaptive Behavior, pp. 451-459. , Honolulu, USASugawara, K., Watanabe, T., Swarming robots - Foraging behavior of simple multirobot system (2002) Proceedings of the IEEE/RSJ International Conference on Intelligent Robots and Systems, pp. 2702-2707. , Lausanne, SwitzerlandNicolis, S.C., Deneubourg, J.L., Emerging patterns and food recruitment in ants: An analytical study (1999) Journal of Theoretical Biology, 198, pp. 575-592Deneubourg ans, J.L., Aron, S., Goss, S., Pasteels, J.M., The self-organizing exploratory pattern of the argentine ant (1990) Journal of Insect Behavior, 3, pp. 159-168Ermentrout, G., Edelstein-Keshet, L., Cellular automata approaches to biological modeling (1993) Journal of Theoretical Biology, 160, pp. 97-133Vaughan, R., Stoy, K., Sukhatme, G., Mataric, M., Lost: Localization-space trails for robot teams IEEE Transactions on Robotics and Automation, 18 (5)Sauter, J., Matthews, R., Parunak, H., Brueckner, S., Evolving adaptive pheromone path planning mechanisms (2002) Proceedings of the First International Joint Conference on Autonomous Agents and Multi-Agent Systems, pp. 434-440Deneubourg, J.L., Goss, S., Franks, N., Sendova-Franks, A., Detrain, C., Christien, L., The dynamics of collective sorting robot-like ants and ant-like robots (1991) Proceedings of the First International Conference on Simulation of Adaptive Behavior on From Animals to Animats, pp. 356-363. , Paris, France MIT PressBeckers, R., Holland, O.E., Deneubourg, J.L., From Local Actions to Global Tasks: Stigmergy and Collective Robotics (1994) Proceedings of the 4th International Workshop on the Synthesis and Simulation of Living Systems, pp. 181-189. , In R. A. Brooks and P. Maes, editors, Cambridge, USA MIT PressKube, C.R., Bonabeau, E., Cooperative transport by ants and robots (2000) Robotics and Autonomous Systems, 1, pp. 85-101Rybski, P., Larson, A., Veeraraghavan, H., LaPoint, M., Gini, M., Communication strategies in multi-robot search and retrieval (2004) Proceedings of the 7th International Symposium on Distributed Autonomous Robotic Systems, pp. 301-310. , Toulouse, FranceWurr, A., Robotic team navigation in complex environments using stigmergic clues (2003), Master's thesis, University of ManitobaCazangi, R.R., Von Zuben, F.J., Figueiredo, M.F., Autonomous navigation system applied to collective robotics with ant-inspired communication (2005) Proceedings of the 2005 Conference on Genetic and Evolutionary Computation, (1), pp. 121-128. , Washington DC, USA ACM PressSvennebring, J., Koenig, S., Towards building terrain-covering ant robots (2002) In Ant Algorithms, pp. 202-215Ziemke, T., On the role of robot simulations in embodied cognitive science (2003) Artificial Intelligence and Simulation of Behaviour, 1 (4), pp. 1-11Dorigo, M., Stützle, T., (2004) Ant Colony Optimization, , MIT Press/Bradford Book",Stigmergic Autonomous Navigation In Collective Robotics,,,10.1007/978-3-540-34690-6_2,,core
217329889,2015-09-30T07:00:00,"Ongoing research on multi-robot teams is focused on methods and systems to be utilized in dynamic and dangerous environments such as search and rescue missions, often with a human operator in the loop to supervise the system and make critical decisions. To increase the size of the team controlled by an operator, and to reduce the operator\u27s mental workload, the robots will have to be more autonomous and reliable so that tasks can be issued at a higher level. Typical in these domains, such high-level tasks are often composed of smaller tasks with dependencies and constraints. Assigning suitable robot platforms to execute these tasks is a combinatorial optimization problem. Operations Research and AI techniques can handle large numbers of robot allocations in real time, however most of these algorithms are opaque to humans; they provide no explanation or insight about how the solution is produced. Recent studies suggest that interaction between the human operator and robot team requires human-centric approaches for collaborative planning and task allocation, since black-box solutions are often too complex to examine under stressful conditions and are often discarded by experts.
The main contribution of this thesis is a methodology to help operators make decisions about complex task allocation in real time for high stress missions. First a novel, human-centric graphical model, TAG, is described to analyze and predict the complexity of task assignment and scheduling problem instances, taking into account the spatial distribution of resources and tasks. Then, the TAG model is extended for dynamic environments to the MAP model. Two user studies were conducted, first in static and then in dynamic environments, in order to identify and empirically verify the key factors, derived from the graphical model, which affect the decision making of human supervisors during task assignment for a team of robots. In these user studies, participants used software tools developed for this work. One of these software tools allows for two different levels of autonomy for the interaction scheme: manual control and collaborative control, with an option to invoke an automated assignment tool. Findings relating to the impact of decision support functionality on the mental workload and the performance of the supervisor are presented. Finally, steering of the common algorithms utilized by decision support tools, using the strategies employed by user study participants, related to the TAG and MAP model parameters, are discussed",Real-Time Supervision for Human Robot Teams in Complex Task Domains,https://academicworks.cuny.edu/cgi/viewcontent.cgi?article=2098&context=gc_etds,CUNY Academic Works,,,core
100773990,29/12/2014,Computers are often used to simulate real world activities and events. The usefulness of a software simulation is determined by the validity of the underlying model of the system. Software simulation often treats the real world in a less random and more predictable fashion than is the case in nature. The use of an actual physical controlled object creates a more realistic environment to carry out real-time or AI experiments. These physical objects exhibit a more chaotic behavior due to the mechanical issues independent of the controlling algorithm. The construction and use of robots to accomplish this behavior has been successfully done in the past. The research discussed in this paper established a semi-adversarial environment. In this type of an environment one of the parties neither hinders nor assists in the process undertaken. To demonstrate this behavior the most appropriate group of animals is a herd of sheep. To create this environment the basic behaviors and characteristics of sheep had to be understood and encoded. This paper describes those necessary attributes and the resulting sheep algorithms. Future use of these algorithms in semi-adversarial environments will also be discussed,and,,,,,core
78067765,2014-11-01T00:00:00,"We present a new method for planning footstep placements for a robot walking on uneven terrain with obstacles, using a mixed-integer quadratically-constrained quadratic program (MIQCQP). Our approach is unique in that it handles obstacle avoidance, kinematic reachability, and rotation of footstep placements, which typically have required non-convex constraints, in a single mixed-integer optimization that can be efficiently solved to its global optimum. Reachability is enforced through a convex inner approximation of the reachable space for the robot's feet. Rotation of the footsteps is handled by a piecewise linear approximation of sine and cosine, designed to ensure that the approximation never overestimates the robot's reachability. Obstacle avoidance is ensured by decomposing the environment into convex regions of obstacle-free configuration space and assigning each footstep to one such safe region. We demonstrate this technique in simple 2D and 3D environments and with real environments sensed by a humanoid robot. We also discuss computational performance of the algorithm, which is currently capable of planning short sequences of a few steps in under one second or longer sequences of 10-30 footsteps in tens of seconds to minutes on common laptop computer hardware. Our implementation is available within the Drake MATLAB toolbox [1].Hertz FoundationMIT Energy InitiativeMassachusetts Institute of Technology. Computer Science and Artificial Intelligence LaboratoryUnited States. Defense Advanced Research Projects Agency (Robotics Challenge",Footstep planning on uneven terrain with mixed-integer convex optimization,,'Institute of Electrical and Electronics Engineers (IEEE)',10.1109/HUMANOIDS.2014.7041373,,core
74388747,2015-10-13T15:14:19,"In this paper, a model based on Artificial Neural Networks (ANNs) extends the symbol grounding mechanism toabstract words for cognitive robots. The aim of this work is to obtain a semantic representation of abstract concepts through the grounding in sensorimotor experiences for a humanoid robotic platform. Simulation experiments have been developed on a software environment for the iCub robot. Words that express general actions with a sensorimotor component are first taught to the simulated robot. During the training stage the robot first learns to perform a set of basic action primitives through the mechanism of direct grounding. Subsequently, the grounding of action primitives, acquired via direct sensorimotor experience, is transferred to higher-order words via linguistic descriptions. The idea is that by combining words grounded in sensorimotor experience the simulated robot can acquire more abstract concepts. The experiments aim to teach the robot the meaning of abstract words by making it experience sensorimotor actions. The iCub humanoid robot will be used for testing experiments on a real robotic architecture",Towards the Grounding of Abstract Words: A Neural Network Model for Cognitive Robots,https://core.ac.uk/download/74388747.pdf,"San Jose, California",,,core
296631168,2015-11-26T14:42:17,"This paper presents an artificial homeostatic system (AHS) devoted to the autonomous navigation of mobile robots, with emphasis on neuro-endocrine interactions. The AHS is composed of two modules, each one associated with a particular reactive task and both implemented using an extended version of the GasNet neural model, denoted spatially unconstrained GasNet model or simply non-spatial GasNet (NS-GasNet). There is a coordination system, which is responsible for the specific role of each NSGasNet at a given operational condition. The switching among the NSGasNets is implemented as an artificial endocrine system (AES), which is based on a system of coupled nonlinear difference equations. The NSGasNets are synthesized by means of an evolutionary algorithm. The obtained neuro-endocrine controller is adopted in simulated and real benchmark applications, and the additional flexibility provided by the use of NSGasNet, together with the existence of an automatic coordination system, guides to convincing levels of performance. © 2008 IEEE.40234030Haykin, S., (1999) Neural Networks: A Comprehensive Foundation, , Prentice Hall, 2nd editionNeal, M., Timmis, J., Timidity: A useful mechanism for robot control (2003) Informatica, 7, pp. 197-203P. A. Vargas, R. C. Moioli, L, N. Castro, J. Timmis, M, Neal, and F.J. Von Zuben. Artificial homeostatic system: a novel approach. In Proceedings of the VIIIth European Conference on Artificial Life, 2005Besendovsky, H.O., Del Rey, A., Immune-neuro-endocrine interactions: Facts and hypotheses (1996) Endocrine Reviews, 17, pp. 64-102Levine, D.S., (1998) Explorations in Common Sense and Common Nonsense, , http://www.uta.edu/psychology/faculty/levine/EBOOK/index.htm, On-line bookCannon, W.B., Organization for physiological homeostasis (1929) Physiological Review, 9, pp. 399-431Pfeifer, R., Scheier, C., (1999) Understanding Intelligence, , MIT PressAshby, W.R., (1960) Design for a Brain: The Origin of Adaptive Behaviour, , London: Chapman and HallDi Paolo, E.A., Homeostatic adaptation to inversion of the visual field and other sensorimotor disruptions (2000) From Animals to Animals, Proc. of the Sixth International Conference on the Simulation of Adaptive Behavior, SAB'2000, pp. 440-449. , MIT PressHarvey, I., Homeostasis and rein control: From daisyworld to active perception (2004) Proceedings of the Ninth International Conference on the Simulation and Synthesis of Living Systems, ALIFE9, , MIT PressWelch, R.B., Research on adaptation to rearranged vision (1974) Perception, 3, pp. 367-392(1984) Sensory Experience, Adaptation and Perception: A Festschrift for ho Kohler, , L. Spillman and B. Wooten, editors, Lawrence ErlbaumHoinville, T., Hnaff, P., Comparative study of two homeostatic mechanisms in evolved neural controllers for legged locomotion (2004) Proccedings of 2004 IEEE/RSJ International Conference on Intelligent Robots and SystemsMcClintic, J.R., (1975) Basic Anatomy and Physiology of the Human Body, , J. Wiley & SonsPurves, W.K., Heller, H.C., Orians, G.H., Sadava, D., Life: The Science of Biology (2001) IE-Macmillan UK, , 6th editionTimmis, J., Neal, M., Once More Unto the Breach: Towards Artificial Homeostasis (2004) Recent Developments in Biologically Inspired Computing, pp. 340-366. , L. N. de Castro and F. J. Von Zuben editors, Idea Group Inc. Chapter XIVNolfi, S., Floreano, D., (2004) Evolutionary Robotics: The Biology, Intelligence, and Technology of Self-Organizing Machines, , MIT PressJakobi, N., Evolutionary robotics and the radical envelope of noise hypothesis (1998) Adaptive Behaviour, 6, pp. 325-368Jakobi, N., Running across the reality gap: Octopod locomotion evolved in a minimal simulation (1998) Evolutionary Robotics: First European Workshop, EvoRobot98, pp. 39-58. , Husbands, P. and Meyer, J.-A, editors, Springer-VerlagRumelhart, D.E., McClelland, J.L., Parallel Distributed Processing: Explorations in the Microstructure of Cognition (1986) Foundations, 1. , and The PDP Research Group, The MIT PressMcClelland, J.L., Rumelhart, D.E., Parallel Distributed Processing: Explorations in the Microsiructure of Cognition (1986) Psychological and Biological Models, 2. , and The PDF Research Group, The MTT PressChurchland, P., Sejnowski, T.J., (1994) The Computational Brain, , MTT PressO'Reilly, R.C., Munakata, Y., (2000) Computational Explorations in Cognitive Neuroscience: Understanding the Mind by Simulating the Brain, , MIT PressDayan, P., Abbot, L.F., (2001) Theoretical Neuroscience: Computational and Mathematical Modelling of Neural Systems, , MIT PressHusbands, P., Smith, T., Jakobi, N., OShea, M., Better living through chemistry: Evolving GasNets for robot control (1998) Connection Science, 10, pp. 185-210Vargas, P.A., Di Paolo, E.A., Husbands, P., Preliminary investigations on the evolvability of a non-spatial GasNet model (2007) Proceedings of the 9th European Conference on Artificial life ECAL 2007, , Springer-VerlagYao, X., Evolving artificial neural networks (1999) Proceedings of the IEEE, 87, pp. 1423-1447Yao, X., Liu, Y., Fast evolution strategies (1997) Control Cybern, 26, pp. 467-496Fogel, D.B., Fogel, L.J., Porto, V.W., Evolving neural networks (1990) Biological Cybern, 63, pp. 487-493Whitley, D., Starkweather, T., Bogart, C., Genetic algorithms and neural networks: Optimizing connections and connectivity (1990) Parallel Compute, 14, pp. 347-361Husbands, P., Evolving robot behaviours with diffusing gas networks (1998) Evolutionary Robotics: First European Workshop, EvoRobot98, pp. 71-86. , Springer-VerlagSmith, T.M.C., (2002) The Evolvability of Artificial Neural Networks for Robot Control, , PhD thesis, CCNR, Department of Informatics, University of Sussex, UKPhilippides, A., Husbands, P., Smith, T., O'Shea, M., Flexible couplings: Diffusing neuromodulators and adaptive robotics (2005) Artificial Life, 11, pp. 139-160KTEAM, S.A., (2007), http://www.k-team.comStorm, T., Kiks, a Khepera simulator for Matlab 5.3 and 6.0, , http://theodor.zoomin.se/index/2866.htmlCollins, R., Jefferson, D., Selection in massively parallel genetic algorithms (1991) Proccedings of the Fourth Intl Conf. on Genetic Algorithms, ICGA-91, pp. 249-256. , Morgan KaufmannHillis, W.D., Co-evolving parasites improve simulated evolution as an optimization procedure (1990) Physica D, 42, pp. 228-23",Towards The Evolution Of An Artificial Homeostatic System,,'Institute of Electrical and Electronics Engineers (IEEE)',10.1109/CEC.2008.4631346,,core
224448502,2015-01-01T00:00:00,"Beat tracking is a machine listening task which involves identifying both the time-varying tempo of an acoustic music signal and the location of beats. Ensemble learning methods, where the outputs of multiple classifiers or other models are combined to produce a single more robust output, have become increasingly popular in many machine learning problems but have not been extensively explored for beat tracking applications. Existing ensemble approaches for beat tracking have focused on non-real-time and non-causal beat tracking systems or have been agent- based approaches where agents must interact with the ensemble. However, real-time causal beat tracking is desirable for a number of interesting applications including foot-tapping robots and live musical machine improvisation or accompaniment. This dissertation introduces Beaker, a flexible framework for creating ensembles of arbitrary real-time causal beat trackers which do not require any knowledge of each other or of the ensemble. The Beaker framework uses clustering of ensemble member hypotheses to form discrete hypothesis classes, and reliability measures for each tracker are used to weight hypotheses to determine the ensemble's output. Simple TestTracker beat trackers are introduced as example ensemble members and used to demonstrate the performance of the Beaker ensemble framework. Diverse ensembles of TestTrackers can be created by varying parameters such as the input features and periodicity estimation approaches used by each tracker. Beaker ensembles using TestTrackers are shown to be competitive with a state-of-the-art real- time causal beat tracking system and can even outperform some non-causal beat tracking systems on certain evaluation metrics. Beaker's approach to combining the hypotheses of individual TestTrackers is shown to give superior results compared to recent developments in ensemble methods for non-causal beat tracking. The concept of ensemble diversity is discussed as it relates to Beaker ensembles, and a metric for predicting when the combination of two Beaker ensembles will give improved results over the individual ensembles is introduced. Finally, a real-time implementation and visualization of the Beaker ensemble framework in C++ is presented and areas for future work are discusse",An ensemble framework for real-time audio beat tracking,,"eScholarship, University of California",,,core
198966646,2015-01-01T00:00:00,"International audienceThis chapter is devoted to autonomous cognitive machines by mean of the design of an artificial curiosity based cognitive system for autonomous high-level knowledge acquisition from visual information. Playing a chief role as well in visual attention as in interactive high-level knowledge construction, the artificial curiosity is realized through combining visual saliency detection and Machine-Learning based approaches. Experimental results validating the deployment of the investigated system have been obtained using as well simulation facilities as a real humanoid robot acquiring visually knowledge about its surrounding environment interacting with a human tutor. As show the reported results and experiments, the proposed cognitive system allows the machine to discover autonomously the surrounding world in which it may evolve, to learn new knowledge about it and to describe it using human-like natural utterances",Artificial Curiosity Emerging Human-like Behavior: Toward Fully Autonomous Cognitive Robots,,'Springer Science and Business Media LLC',10.1007/978-3-319-23392-5_28,,core
211486899,2014-01-01T00:00:00,"Along the last decades persistent research endeavors in the areas of robotics andartificial intelligence revealed the great challenge of robot navigation, an areathat combines robot mobility with perception of the environment. Moreover,in modern human societies it is of great importance to build up machines thatcan be operated by non specialists or even by technologically illiterate people,such as youngsters or elderly. Therefore, the mobile robots to be released intothe market in the near future should possess, among others, the potential ofproducing meaningful internal perceptual representations of their own environment,capacitating them to cope a range of real-life situations and tasks. Tomake matters even more challenging, when it comes for mapping and navigation,robots should comprehend human concepts about places and objects,to skillfully deploy in human frequented environments. In response to thischallenge, intense research efforts, to build cognitive robots apt to competentlyperceive and understand their surroundings and to cooperate with humans,take place. With this goal in view, semantic mapping with mobile robots can constituteto a holistic solution in response to the aforementioned challenges. Thesemantic mapping is an augmented representation of the robot’s environmentthat -supplementary to the geometrical knowledge- encapsulates characteristicscompatible with human understanding. It provides several algorithmic opportunitiesfor innovative development of applications that will eventually lead tothe human robot interaction. The main objective of the PhD dissertation in handis the construction of accurate and consistent semantic maps facilitating amplerobot deployment in domestic environments.The motivation behind this PhD dissertation has been the observation thatthe plethora of the existing mapping and navigation algorithms are not ableto provide a sufficient representation of the environment in terms of humans’concepts. This is due to the fact that the mapping methods developed sofar focus on the construction of geometrical maps. Although some of thesesolutions proved to be capable of driving robots into specific target positions,they lack of high level cognition attributes, which would allow them to bring the human-robot interaction one step beyond. Aiming to remove this barrier, thisdissertation is oriented towards the direction of developing semantic mappingalgorithms for high level robot navigation. Therefore, this doctoral researchidentified the basic components of the semantic mapping and developed aninnovative solution for each one of them. Due to the fact that the semanticmapping requires an integrated system that comprises several subordinatemodules, a wide range of a algorithms that serve different tasks had to bedesigned and implemented. Within the context of this thesis several algorithmicmodules have been developed including a competent localization algorithm,novel simultaneously localization and mapping strategies, breakthrough place andobject recognition tactics, as well as the integration of all these methods undera time supervised framework able to produce consistent semantic maps. Dueto the fact that each subordinate module comprises an innovative solutionto the respective field, the resulting semantic mapping system constitutesa state of the art solution in the area of conceptual mapping with mobilerobots. The introduced algorithms exploit solely visual and depth sensors,while by combining basic tools from three different scientific areas such asrobotics, computer vision and machine learning the final objective is accomplished.Overall, the main contribution of this thesis to the advancement the stateof the art is the introduction of a stacked map hierarchy of four differenttype of maps, namely a metric, a topological, a labeled sparse topological andan augmented navigation one. Each of these representations accomplishes aunique purpose: (i) the metric is the physical (lower) layer; (ii) the topologicalone contains abstract geometrical information of the environment, i.e. pointclouds registered in a graph of nodes; (iii) the labeled sparse topological oneestablishes the spatiotemporal coherence by associating the respective nodes inthe topometric maps via place labels and geometrical transformations, enablingbidirectional exchange of information among the conceptual and metric mapsand, last, (iv) the augmented navigation map inheres the significance of thedetected places as well as their connectivity relationships, expressed in termsof their transition probability.The thesis in hand has been developed in a hierarchical fashion and can bedivided into four main chapters. The first one comprises a literature survey ofthe existing semantic mapping methods in which an explicit analysis of the sofar developed methods is sought. The insights of the semantic mapping arereviewed, the distinct components encompassing, to give a categorization of therelated literature, are studied the possible applications in mobile robotics areexamined and, lastly, the methods and databases available for benchmarkingare referred. Furthermore, a quality-based taxonomy of the existing semanticmapping methods highlights the dominant attributes such methods retain.More precisely, according to the scale, to which each method is expanded, the metric map could be either a single scene or a progressively created map.Another important attribute a typical semantic mapping method possessesis the existence of the respective topological map, that is an abstraction ofthe explored environment in terms of a graph. The nodes of such a graph areorganized in a geometrical manner, so as to simultaneously preserve conceptualknowledge about the explored places. Moreover, the modalities (single ormultiple visual cues) utilized to reason about the observed scene constitute anelement apt to distinguish the abundance of different methods. An additionalfeature in many recent semantic mapping techniques is the temporal coherencesuch a map reveals, which renders it useful for high-level activities, viz. taskplanning or human robot interaction.The second chapter refers to the description of the developed technologicalbackground required to build a consistent semantic map. Therefore, a majorcontribution of the first part is the development of an innovative visual odometryalgorithm able to operate in real time. This algorithm receives as input successivestereo pair of images from a stereoscopic camera mounted on a mobilerobot. It involves the detection of the salient landmarks between successiveimages. A depth estimation of these features is then obtained and a novelnon-iterative outlier detection and discarding methodology able to remove boththe mismatches between the features and the inserted errors due to the 3Dreconstruction procedure. A hierarchical motion estimation technique, whichproduces robust estimations for the movement of the robot is then adopted,thus providing refinements to the robot’s global position and orientation. Anadditional 3D reconstruction algorithm that operates on stereo images hasbeen developed providing accurate reconstruction of the area observed bythe robot. Moreover, the localization algorithm comprises the cornerstone forthe development of a simultaneously localization and mapping system suitablefor the 3D geometrical mapping of the explored environment. This 3D metricmapping system is based solely on an RGB-D sensor, where in course of robot’slocomotion 3D point clouds are merged with respect to the visual odometry.The resulted 3D map is refined by exploiting a random sample consensus planedetection algorithm accompanied by an iterative closest point registration step,among the dominant planes of the consecutive time instances, resulting thusin a very consistent geometrical 3D map. All the aforementioned developedalgorithms have been evaluated on a custom made robot platform bearing twostereoscopic cameras with different baselines and a RGB-D sensor.The third chapter encloses all the semantic mapping methods based onvisual cues that have been developed within this PhD dissertation. The firstone examines the overall traversability of the observed scene taking into considerationthe robot’s embodiment. This knowledge constitutes a cornerstone forthe autonomous robot navigation. The developed system utilizes an algorithm to retrieve specific characteristics of the environment using a stereo cameraand to produce a disparity map of the scene. Then, the v-disparity image iscalculated based on the disparity map. The v-disparity is then exploited by afeature extraction procedure to provide the system with respective conceptualvectors, which are used to train support vector machines in order to assess theoverall traversability of the scene. The traversable classified scenes are furtherprocessed and the likelihood distribution of the collision risk assessment, whenthe robot moves towards any direction, is calculated. The second part involvesthe description of a novel object recognition algorithm based on hierarchical temporalmemory networks. This constitutes a supervised learning method usedto recognize objects in different orientations. It introduces specific alternativerules for the design of each building block of a hierarchical temporal memorynetwork. These rules expand both the spatial and the temporal module ofthe network. Various type of input layers have been tested such as logppolarand saliency detection ones in order to find the solution that fits better in applicationsthat concern cluttered environments. The third part of this chaptercomprises the description of an innovative place classification algorithm. Withinthis work, in course of robot’s locomotion, salient visual features are detectedand they shape a bag-of-features problem, quantized by a neural gas to codethe spatial information for each scene. Each input image is transformed intoan appearance based histogram representation that abstracts the place in avery compatible and consistent manner. The learning procedure is performedby support vector machines able to accurately recognize multiple dissimilarplaces. The innovative appearance based algorithm produces semantic inferencessuitable for labeling unexplored environments. In the rest of this chaptera long range semantic mapping framework is described, where geometricalmapping, with place and object recognition is combined in order to construct ahuman oriented semasiological map. This framework features geometrical andsemasiological attributes capable to reveal relationships between objects andplaces in a real-life environment. The geometrical component consists of a 3Dmap, onto which a topological map is deployed, representing the explored areaas a graph of nodes. The semasiological part is realized by putting together aplace recognition algorithm and an object recognition one. The categorizationof the different places relies on the resolution of appearance-based consistencyhistograms, while for the recognition of objects in the scene we make use of thehierarchical temporal memory network boosted by a saliency attentional model.These semantical attributes are then deposited on each node of the topologicalmap, in order to augment it with the belief distributions regarding the visitedplaces, thus resulting in a 3D object map embodying geometrical and semanticalproperties.Finally, in the fourth chapter of this thesis a novel semantic mapping method suitable for robot navigation in a human compatible manner is illustrated. Themain objective of this chapter is to take into account the time proximity of therobot acquired frames within the semantic map. The goal of this method is; (i)to introduce a semasiological mapping method for robot exploration and (ii) tomake use of the constructed semantic map as a means to provide a hierarchicalnavigation solution. The semantic map is formed during the robot’s course,relying on the memorization of abstract place representations. It integrates thespace quantization, the time proximity of the acquired frames and the spatialcoherence in terms of a novel labeled sparse topological map. A time evolvingaugmented navigation graph is shaped determining the semantic topology ofthe explored environment and the physical connectivity among the recognizedplaces expressed by the inter-place transition probability. Concerning therobot navigation part, a human-robot interaction methodology is illustratedcapable of competently addressing go-to commands. As a product of thismodule, an augmented navigation graph is formed, which handles the high levelrobot navigation, forwarding the presumable sequence of places the robotshould traverse to reach its target location. Accordingly, for the low levellocal navigation the topometric data of the semantic map are made use of.Additionally, to assist the human robot interaction a graphical user interfacehas been developed providing an overall supervision of the mapping andnavigation procedure.The last chapter of this dissertation concludes the doctoral research conductedhereby. It discusses the achievements of this work, while it also highlightsthe open issues and the questions revealed during the developmentof the several algorithmic solutions. Additionally, some future trends of thesemantic mapping are outlined establishing the potential descendants of thisdissertation",Χωροχρονικά συναφείς σημασιολογικοί χάρτες με κινούμενα ρομπότ,,'National Documentation Centre (EKT)',10.12681/eadd/40861,,core
203489970,31/12/2015,"AbstractIn this paper a revised reinforcement learning method is presented for stability control problems with real-value inputs and outputs. The revised eXtended Classifier System for Real-input and Real-output (XCSRR) controller is designed, which is capable of working at fully real-value environment such as stability control of robots. XCSRR is a novel approach to enhance the performance of classifier systems for more practical problems than systems with merely binary behaviour. As a case study, we use XCSRR to control the stability of a biped robot, which is subjected to unknown external forces that would disturb the robot equilibrium. The external forces and the dynamics of the upper body of the biped robot are modelled in MATLAB software to train the XCSRR controller. Theoretical and experimental results of the learning behaviour and the performance of stability control on the robot demonstrate the strength and efficiency of the proposed new approach",An Improved eXtended Classifier System for the Real-time-input Real-time-output (XCSRR) Stability Control of a Biped Robot ,,Published by Elsevier B.V.,10.1016/j.procs.2015.09.198,,core
102554975,26/08/2015,"Many important developments in artificial intelligencehave been stimulated by organized competitions thattackle interesting, difficult challenge problems, such as chess, robot soccer, poker, robot navigation, stock trading, and others. Economics and artificial intelligence share a strong focus on rational behavior. Yet the real-time demands of many domains do not lend themselves to traditional assumptions of rationality (Simon 1979, Wellman 1996). This is the case in many trading environments, where self-interested entities need to operate subject to limited time and information. With the web mediating an ever broader range of transactions and open-ing the door for participants to concurrently trade across multi-ple markets, there is a growing need for technologies that empower participants to rapidly evaluate very large numbers of alternatives in the face of constantly changing market condi-tions. AI and machine-learning techniques, including neural networks and genetic algorithms, are already routinely used in support of automated trading scenarios. Yet, the deployment of these technologies remains limited, and their proprietary nature precludes the type of open benchmarking that is critical for fur-ther scientific progress",Agent Competition for,,,,,core
102900776,28/10/2015,"n The Robot World-Cup Soccer (RoboCup) is an attempt to foster AI and intelligent robotics research by providing a standard problem where a wide range of technologies can be integrated and examined. The first RoboCup competition will be held at the Fifteenth International Joint Conference on Artificial Intelligence in Nagoya, Japan. A robot team must actually perform a soc-cer game, incorporating various technologies, including design principles of autonomous agents, multiagent collaboration, strategy acquisi-tion, real-time reasoning, robotics, and sensor fusion. RoboCup is a task for a team of multiple fast-moving robots under a dynamic environ-ment. Although RoboCup’s final target is a world cup with real robots, RoboCup offers a software platform for research on the software aspects of RoboCup. This article describes technical chal-lenges involved in RoboCup, rules, and the simu-lation environment. The World Cup Robot Soccer (RoboCup)is an attempt to promote AI androbotics research by providing a com-mon task for evaluation of various theories, algorithms, and agent architectures. For the robot (physical robot and software agent) to play a soccer game reasonably well, a wide range of technologies need to be integrated, and numbers of technical breakthroughs must be accomplished. The range of tech-nologies spans both AI and robotics research, such as design principles of autonomous agents, multiagent collaboration, strategy acquisition, real-time reasoning and plan-ning, intelligent robotics, and sensor fusion. The first RoboCup, RoboCup-97, will be held during the Fifteenth International Joint Con-ference on Artificial Intelligence (IJCAI-97) i",,,,,,core
236372753,2015-11-11T00:00:00,"Cognitive Computing' has initiated a new era in computer science. Cognitive computers are not rigidly programmed computers anymore, but they learn from their interactions with humans, from the environment and from information. They are thus able to perform amazing tasks on their own, such as driving a car in dense traffic, piloting an aircraft in difficult conditions, taking complex financial investment decisions, analysing medical-imaging data, and assist medical doctors in diagnosis and therapy. Cognitive computing is based on artificial intelligence, image processing, pattern recognition, robotics, adaptive software, networks and other modern computer science areas, but also includes sensors and actuators to interact with the physical world.

Cognitive computers – also called 'intelligent machines' – are emulating the human cognitive, mental and intellectual capabilities. They aim to do for human mental power (the ability to use our brain in understanding and influencing our physical and information environment) what the steam engine and combustion motor did for muscle power. We can expect a massive impact of cognitive computing on life and work. Many modern complex infrastructures, such as the electricity distribution grid, railway networks, the road traffic structure, information analysis (big data), the health care system, and many more will rely on intelligent decisions taken by cognitive computers.

A drawback of cognitive computers will be a shift in employment opportunities: A raising number of tasks will be taken over by intelligent machines, thus erasing entire job categories (such as cashiers, mail clerks, call and customer assistance centres, taxi and bus drivers, pilots, grid operators, air traffic controllers, …).

A possibly dangerous risk of cognitive computing is the threat by “super intelligent machines” to mankind. As soon as they are sufficiently intelligent, deeply networked and have access to the physical world they may endanger many areas of human supremacy, even possibly eliminate humans.

Cognitive computing technology is based on new software architectures – the “cognitive computing architectures”. Cognitive architectures enable the development of systems that exhibit intelligent behaviour.:Introduction 5

1. Applying the Subsumption Architecture to the Genesis Story Understanding System – A Notion and Nexus of Cognition Hypotheses
(Felix Mai) 9

2. Benefits and Drawbacks of Hardware Architectures Developed Specifically for
Cognitive Computing (Philipp Schröppe)l 19

3. Language Workbench Technology For Cognitive Systems (Tobias Nett) 29

4. Networked Brain-based Architectures for more Efficient Learning (Tyler Butler) 41

5. Developing Better Pharmaceuticals – Using the Virtual Physiological Human (Ben Blau) 51

6. Management of existential Risks of Applications leveraged through Cognitive Computing (Robert Richter) 6",Cognitive Computing: Collected Papers,https://core.ac.uk/download/236372753.pdf,Technische Universität Dresden,,"[{'title': None, 'identifiers': ['issn:1430-211X', '1430-211x']}]",core
144097128,2015-11-25T00:00:00,"A pesquisa em redes neurais artificiais (RNAs) está atualmente experimentando um crescente interesse por modelos que utilizem a variável tempo como um grau de liberdade extra a ser explorado nas representações neurais. Esta ênfase na codificação temporal (temporal coding) tem provocado debates inflamados nas neurociências e áreas correlatas, mas nos últimos anos o surgimento de um grande volume de dados comportamentais e fisiológicos vêm dando suporte ao papel-chave desempenhado por este tipo de representação no cérebro [BALLARD et al. (1998)]. Contribuições ao estudo da representação temporal em redes neurais vêm sendo observadas nos mais diversos tópicos de pesquisa, tais como sistemas dinâmicos não-lineares, redes oscilatórias, redes caóticas, redes com neurônios pulsantes e redes acopladas por pulsos. Como conseqüência, várias tarefas de processamento da informação têm sido investigada via codificação temporal, a saber: classificação de padrões, aprendizagem, memória associativa, controle sensório-motor, identificação de sistemas dinâmicos e robótica. Freqüentemente, porém, não fica muito claro até que ponto a modelagem dos aspectos temporais de uma tarefa contribui para aumentar a capacidade de processamento da informação de modelos neurais. Esta tese busca apresentar, de uma maneira clara e abrangente, os principais conceitos e resultados referentes à proposição de dois modelos de redes neurais não-supervisionadas (RNATs), e como estas lançam mão da codificação temporal para desempenhar melhor a tarefa que lhes é confiada. O primeiro modelo, chamado rede competitiva Hebbiana temporal (competitive temporal Hebbian - CTH), é aplicado especificamente em tarefas de aprendizagem e reprodução de trajetórias do robô manipulador PUMA 560. A rede CTH é uma rede neural competitiva cuja a principal característica é o aprendizado rápido, em apenas uma época de treinamento, de várias trajetórias complexas contendo ) elementos repetidos. As relações temporais da tarefa, representadas pela ordem temporal da trajetória, são capturadas por pesos laterais treinados via aprendizagem hebbiana. As propriedades computacionais da rede CTH são avaliadas através de simulações, bem como através da implementação de um sistema de controle distribuído para o robô PUMA 560 real. O desempenho da rede CTH é superior ao de métodos tabulares (look-up table) tradicionais de aprendizagem de trajetórias robóticas e ao de outras técnicas baseadas em redes neurais, tais como redes recorrentes supervisionadas e modelos de memória associativa bidirecional (BAM). O segundo modelo, chamado rede Auto-Organizável NARX (Self-Organizing NARX-SONARX), é baseado no conhecido algoritmo SOM, proposto por KOHONEN (1997). Do ponto de vista computacional, as propriedades de rede SONARX são avaliadas em diferentes domínios de aplicação, tais como predição de séries temporais caóticas, identificação de um atuador hidráulico e no controle preditivo de uma planta não-linear. Do ponto de vista teórico, demonstra-se que a rede SONARX pode ser utilizada como aproximador assintótico de mapeamentos dinâmicos não-lineares, graças a uma nova técnica de modelagem neural, chamada Memória Associativa Temporal via Quantização Vetorial (MATQV). A MATQV, assim como a aprendizagem hebbiana da rede CTH, é uma técnica de aprendizado associativo temporal. A rede SONARX é comparada com modelos NARX supervisionados, implementados a partir das redes MLP e RBF. Em todos os testes realizados para cada uma das tarefas citadas no parágrafo anterior, a rede SONARX tem desempenho similar ou melhor do que o apresentado por modelos supervisionados tradicionais, com um custo computacional consideravelmente menor. A rede SONARX é também comparada com a rede CTH na parendizagem de trajetórias robóticas complexas, com o intuito de destacar as principais diferenças entre os dois ) tipos de aprendizado associativo. Esta tese também propõe uma taxonomia matemática, baseada na representação por espaço de estados da teoria de sistemas, que visa classificar redes neurais não-supervisionadas temporais com ênfase em suas propriedades computacionais. Esta taxonomia tem como principal objetivo unificar a descrição de modelos neurais dinâmicos, facilitando a análise e a comparação entre diferentes arquiteturas, contrastando suas características representacionais e operacionais. Como exemplo, as redes CTH e a SONARX serão descritas usando a taxonomia proposta.Neural network research is currently witnessing a significant shift of emphasis towards temporal coding, which uses time as an extra degree of freedom in neural representations. Temporal coding is passionately debated in neuroscience and related fields, but in the last few years a large volume of physiological and behavioral data has emerged that supports a key role for temporal coding in the brain [BALLARD et al. (1998)]. In neural networks, a great deal of research is undertaken under the topics of nonlinear dynamics, oscillatory and chaotic networks, spiking neurons, and pulse-coupled networks. Various information processing tasks are investigated using temporal coding, including pattern classification, learning, associative memory, inference, motor control, dynamical systems identification and control, and robotics. Progress has been made that substantially advances the state-of-the-art of neural computing. In many instances, however, it is unclear whether, and to what extent, the temporal aspects of the models contribute to information processing capabilities. This thesis seeks to present, in a clear and collective way, the main issues and results regarding the proposal of two unsupervised neural models, emphasizing how these networks make use of temporal coding to perform better in the task they are engaged in. The first model, called Competitive Temporal Hebbian (CTH) network, is applied specifically to learning and reproduction of trajectories of a PUMA 560 robot. The CTH model is a competitive neural network whose main characteristic is the fast learning, in just one training epoch, of multiple trajectories containing repeated elements. The temporal relationships within the task, represented by the temporal order of the elements of a given trajectory, are coded in lateral synaptic trained with hebbian learning. The computational properties of the CTH network are assessed through simulations, as well ) as through the practical implementation of a distributed control system for the real PUMA 560 robot. The CTH performs better than conventional look-up table methods for robot trajectory learning, and better than other neural-based techniques, such as supervised recurrent networks and bidirectional associative memory models. The second model, called Self-Organizing NARX (SONARX) network, is based on the well-known SOM algorithm by KOHONEN (1997). From the computational view-point, the properties of the SONARX model are evaluated in different application domains, such as prediction of chaotic time series, identification of an hydraulic actuator and predictive control of a non-linear plant. From the theoretic viewpoint, it is shown that the SONARX model can be seen as an asymptotic approximator for nonlinear dynamical mappings, thanks to a new neural modelling technique, called Vector-Quantized Temporal Associative Memory (VQTAM). This VQTAM, just like the hebbian learning rule of the CTH network, is a temporal associative memory techniques. The SONARX network is compared with supervised NARX models which based on the MLP and RBF networks. For all simulations, in each one of the forementioned application domains, the SONARX network had a similar and sometimes better performance than those observed for standard supervised models, with the additional advantage of a lower computational cost. The SONARX model is also compared with the CTH network in trajectory reproduction tasks, in order to contrast the main differences between these two types of temporal associative learning models. In this thesis, it is also proposed a mathematical taxonomy, based on the state-space representation of dynamical systems, for classification of unsupervised temporal neural networks with emphasis in their computational properties. The main goal of this taxonomy is to unify the description of dynamic neural models, ) facilitating the analysis and comparison of different architectures by constrasting their representational and operational characteristics. Is is shown how the CTH and SONARX models can be described using the proposed taxonomy",Temporal unsupervised neural networks for identification and control of dynamical systems,,"'Universidade de Sao Paulo, Agencia USP de Gestao da Informacao Academica (AGUIA)'",10.11606/T.18.2003.tde-25112015-115752,,core
101452808,09/02/2015,"Abstract: The Robot World Cup Initiative (RoboCup) is an international joint project to promote AI, robotics, and related field [1]. It provides a standard platform for robotic soccer game. The aim of the vision system for RoboCup small robot league is to track and predict the motion states of 10 agents and a ball, and send the states information to the Artificial Intelligence module. This paper presents a design and implementation of a real-time, robust global vision system. The work about the tracking including: the color-based segmentation is used to locate the interesting objects in the image; the blob analysis is adopted to calculate the positions of the objects in the image and filter out noise; the double-check data associate is used to associate the current time frame information with the previous time frame information; and the linear filter is adopted to track and predict the states information; the UDP socket is used to transmit the information from the vision system to the AI system. This paper also presents the K-Mean-clustering-based color calibration and Tsai’s model camera calibration. Experimental results show the effectiveness",THE VISION SYSTEM FOR ROBOCUP SMALL ROBOT LEAGUE,,,,,core
102647496,01/09/2015,"The aim of this thesis is to perform robot positioning, based on an odometry which is continuously corrected by different landmark detection systems demanding as less modifications as possible for the environment. Two independent correction systems (a supervised and an unsupervised) were imple-mented into two different experiences which represent the subject of this thesis. The supervised experiment uses grid lines painted on the floor which are detected by a single light sensor underneath the robot which cannot distinguish between horizontal and vertical lines. The robot knows the geometry of the grid lines and its estimated position which is calculated by odometry. A new position probability model calculates the assumed robot position and transforms this single sensor information into a reliable position and orientation indication of the robot. The intended trajectory is slightly modified in order to optimize the correction algorithm by guiding the robot more efficiently over close grid lines. The theory was implemented and tested on a real Khepera robot. Investigation and modeling of the odometry error is the main subject of this first experiment. The second experiment demonstrates continuous odometry correction by an unsupervised correction system. Different kind of unsupervised neural networks classify the robot’s rough sensor signals. A sta",1998Thesis: Robot Positioning by Supervised and Unsupervised Odometry Correction i,,,,,core
88496733,2015-01-01T00:00:00Z,"As a Computer scientist, a computer science students should have understanding about database theory as a concept of data maintenance. Database will be needed in every single human real life computer implementation such as information systems, information technology, internet, games, artificial intelligence, robot and so on.  Inevitably, the right data handling and managament will produce excellent technology implementation. Data warehouse as one of the specialization subject which is offered in computer science study program final semester, provide challenge for computer science students.A survey was conducted on 18 students of early year of computer science study program at Surya university and giving hypothesis that for those students who ever heard of a data warehouse would be interested to learn data warehouse and on other hand, students who had never heard of the data warehouse will not be interested to learn data warehouse. Therefore, it is important that delivery of the Data warehouse subject material should be understood by lecturers, so that students can well understoodwith the data warehouse",PEMAHAMAN TEORI DATA WAREHOUSE BAGI MAHASISWA TAHUN AWAL JENJANG STRATA SATU BIDANG ILMU KOMPUTER,,Petra Christian University,,"[{'title': None, 'identifiers': ['issn:1411-0105', '1411-0105', 'issn:2528-5823', '2528-5823']}]",core
296651874,2015-11-26T15:16:45Z,"Environment perception is a major research issue which is very important in the field of robotic system. In order to identify the horizon line and the drivable region, we have proposed a visual-perception system based on an automatic image discarding method as a simple solution to improve the system performance. In this paper, all these previous methods are organized in a visual-perception layer which also includes a method for estimating the risk-of-collision based on Pearson's Correlation Coefficient and an evolution of the Threshold and Horizon Finder (TH Finder). These methods were successfully evaluated from real data. © 2012 IEEE.16IEEE Latin American Robotics CouncilBonin-Font, F., Ortiz, A., Oliver, G., Visual Navigation for Mobile Robots: A Survey (2008) Journal of Intelligent and Robotic SystemsKim, B., Hubbard, P., Necsulescu, D., (2003) Swarming Unmanned Aerial Vehicles: Concept Development and Experimentation, A State of the Art Review on Flight and Mission Control, , DRDC-OTTAWATM-2003-176Technical MemorandumThrun, S., Stanley, the robot that won the DARPA Grand Challenge (2006) Journal of Robotic Systems, 23 (9), pp. 661-692. , DARPA Grand Challenge(2007) Spirit of Berlin: An Autonomous Car for the DARPA Urban Challenge Hardware and Software Architecture, , http://www.darpa.mil/grandchallenge/TechPapers/Team_Berlin.pdf, Team Berlin retrieved 02 Dec. 2010Gietelink, O., Ploeg, J., De Schutter, B., Verhaegen, M., Development of advanced driver assistance systems with vehicle hardware-in-the-loop simulations (2006) Vehicle System Dynamics, 44 (7), pp. 569-590Miranda Neto, A., Rittner, L., A Simple and Efficient Road Detection Algorithm for Real Time Autonomous Navigation based on Monocular Vision (2006) Proceedings of the 2006 IEEE 3rd Latin American Robotics SymposiumUlrich, I., Nourbakhsh, I., Appearance-Based Obstacle Detection with Monocular Color Vision (2000) Proceedings of the AAAI National Conference on Artificial Intelligence, July/August 2000, pp. 866-871Bertozzi, M., Broggi, A., Fascioli, A., Vision-based intelligent vehicles: State of the art and perspectives (2000) Robotics and Autonomous Systems, 32, pp. 1-16Miranda Neto, A., Rittner, L., Leite, N., Zampieri, D.E., Lotufo, R., Mendeleck, A., Pearson's Correlation Coefficient for Discarding Redundant Information in Real Time Autonomous Navigation System (2007) IEEE International Conference on Control Applications Part of IEEE Multi-conference on Systems and Control (CCA - MSC 2007), SingapuraMiranda Neto, A., Rittner, L., Leite, N., Zampieri, D.E., Victorino, A.C., Nondeterministic Criteria to Discard Redundant Information in Real Time Autonomous Navigation Systems based on Monocular Vision (2008) IEEE Multi-conference on Systems and Control (ISIC - MSC 2008), San Antonio, Texas, US, , ISIC Invited PaperMiranda Neto, A., Victorino, A.C., Fantoni, I., Zampieri, D.E., Real-Time Dynamic Power Management based on Pearson's Correlation Coefficient (2011) IEEE International Conference on Advanced Robotics (ICAR 2011), Tallinn, EstoniaRodgers, J.L., Nicewander, W.A., Thirteen Ways to Look at the Correlation Coefficient (1988) The American Statistician, 42, pp. 59-66Pearson, K., (1895) Royal Society Proceedings, 58, p. 241Eugene, Y.K., Johnston, R.G., (1996) The Ineffectiveness of the Correlation Coefficient for Image Comparisons, , Technical Report LA-UR-96-2474, Los Alamos(2005) DARPA Grand Challenge, , http://www.darpa.mil/grandchallenge05/, DARPA(2006) Stanford Racing Team's Entry in the 2005 DARPA Grand Challenge, , http://www.stanfordracing.org/, June 10Gonzalez, C.R., Woods, E.R., (2000) Digital Image Processing, , Ed. Edgard Blücher, S.Paulo, BrazilSahoo, P.K., Soltani, S., Wong, A.K.C., A survey of thresholding techniques (1988) Comput. Vision Graphics Image Processing, 41, pp. 233-260Otsu, N., A threshold selection method from gray-level histogram (1978) IEEETransactions on Systems, Man, and CyberneticsLee, U.S., Chung, Y.S., Park, H.R., A Comparative Performance Study of Several Global Thresholding Techniques for Segmentation (1990) Computer Vision, Graphics, and Image ProcessingLim, K.H., Vision-based Lane-Vehicle Detection and Tracking (2009) Chapter 13 of IAENG Transactions on Engineering Technologies, 3, pp. 157-171. , Special Edition', American Institute of PhysicsDahlkamp, H., Self-Supervised Monocular Road Detection in Desert Terrain (2006) Proceedings of the Robotics Science and Systems ConferenceEttinger, S., Vision-Guided Flight Stability and Control for Micro Air Vehicles (2003) Advanced Robotics, 17, pp. 617-640Sezgin, M., Sankur, B., Survey over image thresholding techniques and quantitative performance evaluation (2004) Journal of Electronic Imaging, 13, pp. 146-165Miranda Neto, A., Victorino, A.C., Fantoni, I., Zampieri, D.E., Robust Horizon Finding Algorithm for Real Time Autonomous Navigation based on Monocular Vision (2011) IEEE International Conference on Intelligent Transportation Systems (ITSC 2011), Washinton DC, USRauskolb, F.W., Caroline: An autonomously driving vehicle for urban environments (2008) Journal of Field Robotics, 25 (9), pp. 674-724http://www.youtube.com/user/kingdombr, April, 25 201",A Visual-perception Layer Applied To Reactive Navigation,,,10.1109/SBR-LARS.2012.8,,core
102942922,29/10/2015,"Abstract — Many research papers have reported studies on sports robots that realize giant-swing motion. However, almost all these robots were controlled using trajectory planning methods, and few robots realized giant-swing motion by learning. Consequently, in this study, we attempted to construct a humanoid robot that realizes giant-swing motion by Q-learning, a reinforcement learning technique. The significant aspect of our study is that few robotic models were constructed beforehand; the robot learns giant-swing motion only by interaction with the environment during simulations. Our implementation faced several problems such as imperfect perception of the velocity state and robot posture issues caused by using only the arm angle. However, our real robot realized giant-swing motion by averaging the Q value and by using rewards –the absolute angle of the foot angle and the angular velocity of the arm angle–in the simulated learning data; the sampling time was 250 ms. Furthermore, the feasibility of generalization of learning for realizing selective motion in the forward and backward rotational directions was investigated; it was revealed that the generalization of learning is feasible as long as it does not interfere with the robot&apos;s motions. I",,,,,,core
103020727,31/10/2015,"Artificial Intelligence is hardly a new idea. Human likenesses, with the ability to act as human, dates back to Geek mythology with Pygmalion’s ivory statue or the bronze robot of Hephaestus. However, with innovations in the technological world, AI is undergoing a renaissance that is giving way to new channels of creativity. The study and pursuit of creating artificial intelligence is more than designing a system that can beat grand masters at chess or win endless rounds of Jeopardy!. Instead, the journey of discovery has more real-life applications than could be expected. While it may seem like it is out of a science fiction novel, work in the field of AI can be used to perfect face recognition software or be used to design a fully functioning neural network. At the International Journal of Advanced Research in Artificial Intelligence, we strive to disseminate proposals for new ways of looking at problems related to AI. This includes being able to provide demonstrations of effectiveness in this field. We also look for papers that have real-life applications complete with descriptions of scenarios, solutions, and in-depth evaluations of the techniques being utilized. Our mission is to be one of the most respected publications in the field and engage in the ubiquitous spread of knowledge with effectiveness to a wide audience. It is why all of articles are open access and available view at any time",Editorial Preface From the Desk of Managing Editor…,,,,,core
296648423,2015-11-26T15:11:39Z,"The perception of the environment is a major issue in autonomous robots. In our previous works, we have proposed a visual perception system based on an automatic image discarding method as a simple solution to improve the performance of a real-time navigation system. In this paper, we take place in the obstacle avoidance context for vehicles in dynamic and unknown environments, and we propose a new method for Collision Risk Estimation based on Pearson's Correlation Coefficient (PCC). Applying the PCC to real-time CRE has not been done yet, making the concept unique. This paper provides a novel way of calculating collision risk and applying it for object avoidance using the PCC. This real-time perception system has been evaluated from real data obtained by our intelligent vehicle. © 2013 IEEE.4045ANR; French National Research AgencyThrun, S., Stanley, the robot that won the DARPA Grand Challenge (2006) Journal of Robotic Systems, 23 (9), pp. 661-692. , DARPA Grand Challenge(2007) Spirit of Berlin: An Autonomous Car for the DARPA Urban Challenge Hardware and Software Architecture, , http://www.darpa.mil/grandchallenge/TechPapers/Team_Berlin.pdf, Team Berlin retrieved 02 Dec. 2010Gietelink, O., Ploeg, J., De Schutter, B., Verhaegen, M., Development of advanced driver assistance systems with vehicle hardware-in-The-loop simulations (2006) Vehicle System Dynamics, 44 (7), pp. 569-590Ulrich, I., Nourbakhsh, I., Appearance-based obstacle detection with monocular color vision (2000) Proceedings of the AAAI National Conference on Artificial Intelligence, pp. 866-871. , July/AugustBonin-Font, F., Ortiz, A., Oliver, G., Visual navigation for mobile robots: A survey (2008) Journal of Intelligent and Robotic SystemsKim, B., Hubbard, P., Necsulescu, D., Swarming unmanned aerial vehicles: Concept development and experimentation, a state of the art review on flight and mission control (2003) Technical MemorandumDugarry, A., (2004) Advanced Driver Assistance Systems Information Management and Presentation, , PhD Thesis, Cranfield University School of Engineering Applied Mathematics and Computing GroupBeauchemin, S., Barron, J.L., The computation of optical flow (1995) ACM Computing Surveys, 27, pp. 433-467State of the art report and requirement specifications (2009) Hybrid Intelligent Virtual Actors, Integrating Research in Interactive Storytelling, , http://www.irisa.fr/, INRIA Report retrieved Feb. 03, 2011Pearson, K., (1895) Royal Society Proceedings, 58, p. 241Eugene, Y.K., Johnston, R.G., (1996) The Ineffectiveness of the Correlation Coefficient for Image Comparisons, , Technical Report LA-UR-96-2474, Los Alamos, 1996Miranda Neto, A., Rittner, L., Leite, N., Zampieri, D.E., Victorino, A.C., Nondeterministic criteria to discard redundant information in real time autonomous navigation systems based on monocular vision (2008) ISIC Invited Paper, 2008 IEEE Multi-conference on Systems and ControlMiranda Neto, A., Rittner, L., Leite, N., Zampieri, D.E., Lotufo, R., Mendeleck, A., Pearson's correlation coefficient for discarding redundant information in real time autonomous navigation systems (2007) Proceedings of the 2007 IEEE Multi-conference on Systems and ControlGreco, C.R., (2008) Real-Time Forward Urban Environment Perception for An Autonomous Ground Vehicle Using Computer Vision and Lidar, , Master of Science (thesis), Brigham Young UniversityBertozzi, M., Broggi, A., Fascioli, A., Vision-based intelligent vehicles: State of the art and perspectives (2000) Robotics and Autonomous Systems, 32, pp. 1-16Dahlkamp, H., Self-supervised monocular road detection in desert terrain (2006) Proceedings of the Robotics Science and Systems ConferenceRadio Spectrum Committee, European Commission, , http://ec.europa.eu/information_society/policy/ecomm/radio_spectrum /_document_storage/rsc/rsc32_public_docs/rscom10_35.pdf, Public Document, Brussels, 5 July 2010, RSCOM10-35 [retrieved Dec. 02, 2010]Gietelink, O.J., Ploeg, J., Schutter, B., Verhaegen, M., Development of a driver information and warningsystem with vehicle hardware-in-theloop simulations (2009) Mechatronics, 19, pp. 1091-1104Müller, D., Pauli, J., Nunn, C., Görmer, S., Müller-Schneiders, S., Time to contact estimation using interest points (2009) IEEE Proceedings of the International Conference on Intelligent Transportation Systems (ITSC 2009), , St.Louis, USAAlenya, G., Negre, A., Crowley, J.L., A comparison of three methods for measure of time to contact (2009) IEEE Proceedings of the International Conference on Intelligent Transportation Systems (ITSC 2009), , St.Louis, USADagan, E., Mano, O., Stein, G.P., Shashua, A., Forward collision warning with a single camera (2004) Intelligent Vehicles Symposium, IEEE, pp. 37-42Negre, A., Braillon, C., Crowley, J., Laugier, C., Real-time time-to-collision from variation of intrinsic scale (2006) INRIA Base, Proc. of the Int. Symp. on Experimental RoboticsHorn, B.K.P., (1986) Robot Vision, , The MIT PressWu, S., Decker, S., Chang, P., Camus, T., Eledath, J., Collision sensing by stereo vision and radar sensor fusion (2009) IEEE Transactions on Intelligent Transportation Systems, 10 (4)Beyeler, A., Zufferey, J.C., Floreano, D., Vision-based control of near-obstacle flight (2009) Autonomous Robots, 27 (3), pp. 201-219Ruffier, F., Franceschini, N., Optic flow regulation: The key to aircraft automatic guidance (2005) Robotics Autonomous Systems, 50, pp. 177-194Mesbah, M., Gradient-based optical flow: A critical review (1999) Proc. of the Fifth Int. Symp. on Signal Processing and Its Applications. ISSPA '99, 1, pp. 467-470. , 1999Otsu, N., A threshold selection method from graylevel histogram (1978) IEEE Transactions on Systems, Man, and CyberneticsRodgers, J.L., Nicewander, W.A., Thirteen ways to look at the correlation coefficient (1988) The American Statistician, 42, pp. 59-66Yanqing, W., Deyun, C., Chaoxia, S., Peidong, W., Vision-based road detection by monte carlo method (2010) Information Technology Journal, 9, pp. 481-487Sahoo, P.K., Soltani, S., Wong, A.K.C., A survey of thresholding techniques (1988) Comput. Vision Graphics Image Processing, 41, pp. 233-260Lee, U.S., Chung, Y.S., Park, H.R., A comparative performance study of several global thresholding techniques for segmentation (1990) Computer Vision, Graphics, and Image ProcessingSezgin, M., Sankur, B., Survey over image thresholding techniques and quantitative performance evaluation (2004) Journal of Electronic Imaging, 13, pp. 146-165Gonzalez, C.R., Woods, E.R., (1991) Digital Image Processing, , Addison-Wesley Publishing CompanyCanny, J.F., A computational approach to edge detection (1986) IEEE Trans. Pattern Anal. Machine Intell., 8 (6), pp. 679-698(2005) DARPA Grand Challenge Rulebook, , http://www.darpa.mil/grandchallenge05/, DARPA(2006) Stanford Racing Team's Entry in the 2005 DARPA Grand Challenge, , http://www.stanfordracing.org, June 10http://youtu.be/J8YuZlJFEx",Real-time Collision Risk Estimation Based On Pearson's Correlation Coefficient,,,10.1109/WORV.2013.6521911,,core
161682850,2015,"""Cognitive Computing"" has initiated a new era in computer science. Cognitive computers are not rigidly programmed computers anymore, but they learn from their interactions with humans, from the environment and from information. They are thus able to perform amazing tasks on their own, such as driving a car in dense traffic, piloting an aircraft in difficult conditions, taking complex financial investment decisions, analysing medical-imaging data, and assist medical doctors in diagnosis and therapy. Cognitive computing is based on artificial intelligence, image processing, pattern recognition, robotics, adaptive software, networks and other modern computer science areas, but also includes sensors and actuators to interact with the physical world.

Cognitive computers – also called ""intelligent machines"" – are emulating the human cognitive, mental and intellectual capabilities. They aim to do for human mental power (the ability to use our brain in understanding and influencing our physical and information environment) what the steam engine and combustion motor did for muscle power. We can expect a massive impact of cognitive computing on life and work. Many modern complex infrastructures, such as the electricity distribution grid, railway networks, the road traffic structure, information analysis (big data), the health care system, and many more will rely on intelligent decisions taken by cognitive computers.

A drawback of cognitive computers will be a shift in employment opportunities: A raising number of tasks will be taken over by intelligent machines, thus erasing entire job categories (such as cashiers, mail clerks, call and customer assistance centres, taxi and bus drivers, pilots, grid operators, air traffic controllers, …).

A possibly dangerous risk of cognitive computing is the threat by “super intelligent machines” to mankind. As soon as they are sufficiently intelligent, deeply networked and have access to the physical world they may endanger many areas of human supremacy, even possibly eliminate humans.

Cognitive computing technology is based on new software architectures – the “cognitive computing architectures”. Cognitive architectures enable the development of systems that exhibit intelligent behaviour.:Introduction 5

1. Applying the Subsumption Architecture to the Genesis Story Understanding System – A Notion and Nexus of Cognition Hypotheses
(Felix Mai) 9

2. Benefits and Drawbacks of Hardware Architectures Developed Specifically for
Cognitive Computing (Philipp Schröppe)l 19

3. Language Workbench Technology For Cognitive Systems (Tobias Nett) 29

4. Networked Brain-based Architectures for more Efficient Learning (Tyler Butler) 41

5. Developing Better Pharmaceuticals – Using the Virtual Physiological Human (Ben Blau) 51

6. Management of existential Risks of Applications leveraged through Cognitive Computing (Robert Richter) 6",Cognitive Computing: Collected Papers,,Technische Universität Dresden,,"[{'title': None, 'identifiers': ['issn:1430-211X', '1430-211x']}]",core
95305335,2015-05-01T00:00:00,"As a Computer scientist, a computer science students should have understanding about database theory as a concept of data maintenance. Database will be needed in every single human real life computer implementation such as information systems, information technology, internet, games, artificial intelligence, robot and so on.  Inevitably, the right data handling and managament will produce excellent technology implementation. Data warehouse as one of the specialization subject which is offered in computer science study program final semester, provide challenge for computer science students.A survey was conducted on 18 students of early year of computer science study program at Surya university and giving hypothesis that for those students who ever heard of a data warehouse would be interested to learn data warehouse and on other hand, students who had never heard of the data warehouse will not be interested to learn data warehouse. Therefore, it is important that delivery of the Data warehouse subject material should be understood by lecturers, so that students can well understoodwith the data warehouse",Pemahaman Teori Data Warehouse Bagi Mahasiswa Tahun Awal Jenjang Strata Satu Bidang Ilmu Komputer,https://media.neliti.com/media/publications/107167-ID-none.pdf,'Petra Christian University',,,core
103422510,04/12/2015,"Abstract—In this paper, a model based on Artificial Neural Networks (ANNs) extends the symbol grounding mechanism to abstract words for cognitive robots. The aim of this work is to obtain a semantic representation of abstract concepts through the grounding in sensorimotor experiences for a humanoid robotic platform. Simulation experiments have been developed on a software environment for the iCub robot. Words that express general actions with a sensorimotor component are first taught to the simulated robot. During the training stage the robot first learns to perform a set of basic action primitives through the mechanism of direct grounding. Subsequently, the grounding of action primitives, acquired via direct sensorimotor experience, is transferred to higher-order words via linguistic descriptions. The idea is that by combining words grounded in sensorimotor experience the simulated robot can acquire more abstract concepts. The experiments aim to teach the robot the meaning of abstract words by making it experience sensorimotor actions. The iCub humanoid robot will be used for testing experiments on a real robotic architecture. I",Towards the Grounding of Abstract Words: A Neural Network Model for Cognitive Robots,,,,,core
160635971,2015-11-11T00:00:00,"Cognitive Computing' has initiated a new era in computer science. Cognitive computers are not rigidly programmed computers anymore, but they learn from their interactions with humans, from the environment and from information. They are thus able to perform amazing tasks on their own, such as driving a car in dense traffic, piloting an aircraft in difficult conditions, taking complex financial investment decisions, analysing medical-imaging data, and assist medical doctors in diagnosis and therapy. Cognitive computing is based on artificial intelligence, image processing, pattern recognition, robotics, adaptive software, networks and other modern computer science areas, but also includes sensors and actuators to interact with the physical world.

Cognitive computers – also called 'intelligent machines' – are emulating the human cognitive, mental and intellectual capabilities. They aim to do for human mental power (the ability to use our brain in understanding and influencing our physical and information environment) what the steam engine and combustion motor did for muscle power. We can expect a massive impact of cognitive computing on life and work. Many modern complex infrastructures, such as the electricity distribution grid, railway networks, the road traffic structure, information analysis (big data), the health care system, and many more will rely on intelligent decisions taken by cognitive computers.

A drawback of cognitive computers will be a shift in employment opportunities: A raising number of tasks will be taken over by intelligent machines, thus erasing entire job categories (such as cashiers, mail clerks, call and customer assistance centres, taxi and bus drivers, pilots, grid operators, air traffic controllers, …).

A possibly dangerous risk of cognitive computing is the threat by “super intelligent machines” to mankind. As soon as they are sufficiently intelligent, deeply networked and have access to the physical world they may endanger many areas of human supremacy, even possibly eliminate humans.

Cognitive computing technology is based on new software architectures – the “cognitive computing architectures”. Cognitive architectures enable the development of systems that exhibit intelligent behaviour.:Introduction 5

1. Applying the Subsumption Architecture to the Genesis Story Understanding System – A Notion and Nexus of Cognition Hypotheses
(Felix Mai) 9

2. Benefits and Drawbacks of Hardware Architectures Developed Specifically for
Cognitive Computing (Philipp Schröppe)l 19

3. Language Workbench Technology For Cognitive Systems (Tobias Nett) 29

4. Networked Brain-based Architectures for more Efficient Learning (Tyler Butler) 41

5. Developing Better Pharmaceuticals – Using the Virtual Physiological Human (Ben Blau) 51

6. Management of existential Risks of Applications leveraged through Cognitive Computing (Robert Richter) 6",Cognitive Computing: Collected Papers,,Technische Universität Dresden,,"[{'title': None, 'identifiers': ['issn:1430-211X', '1430-211x']}]",core
296643354,2015-11-26T15:02:15,"Euclidean distance geometry is the study of Euclidean geometry based on the concept of distance. This is useful in several applications where the input data consist of an incomplete set of distances and the output is a set of points in Euclidean space realizing those given distances. We survey the theory of Euclidean distance geometry and its most important applications, with special emphasis on molecular conformation problems. © 2014 Society for Industrial and Applied Mathematics.561369Alexandrov, A., (1950) Convex Polyhedra, Gosudarstv. Izdat. Tekhn.-Theor. Lit., , MoscowAlfakih, A., Khandani, A., Wolkowicz, H., Solving Euclidean distance matrix completion problems via semidefinite programming (1999) Comput. Optim. Appl., 12, pp. 13-30Alves, R., Cassioli, A., Mucherino, A., Lavor, C., Liberti, L., Adaptive branching in iBP with Clifford algebra (2013) Proceedings of the Workshop on Distance Geometry and Applications, pp. 65-69. , A. Andrioni, C. Lavor, L. Liberti, A. Mucherino, N. Maculan, and R. Rodriguez, eds., Universidade Federal do Amazonas, ManausAnderson, B., Belhumeur, P., Eren, T., Goldenberg, D., Morse, S., Whiteley, W., Yang, R., Graphical properties of easily localizable sensor networks (2009) Wireless Networks, 15, pp. 177-191Arabie, P., Was Euclid an unnecessarily sophisticated psychologist? (1991) Psychometrika, 56, pp. 567-587Asimow, L., Roth, B., The rigidity of graphs (1978) Trans. Amer. Math. Soc., 245, pp. 279-289Asimow, L., Roth, B., The rigidity of graphs II (1979) J. Math. Anal. Appl., 68, pp. 171-190Aspnes, J., Eren, T., Goldenberg, D., Morse, S., Whiteley, W., Yang, R., Anderson, B., Belhumeur, P., A theory of network localization (2006) IEEE Trans. Mobile Comput., 5, pp. 1663-1678Aspnes, J., Goldenberg, D., Yang, R., On the computational complexity of sensor network localization (2004) Algorithmic Aspects of Wireless Sensor Networks, pp. 32-44. , S. Nikoletseas and J. Rolim, eds., Lecture Notes in Comput. Sci. 3121, Springer, BerlinAuslander, L., Mackenzie, R., (1977) Introduction to Differentiable Manifolds, , Dover, New YorkBahr, A., Leonard, J., Fallon, M., Cooperative localization for autonomous underwater vehicles (2009) Internat. J. Robotics Res., 28, pp. 714-728Barvinok, A., Problems of distance geometry and convex properties of quadratic maps (1995) Discrete Comput. Geom., 13, pp. 189-202Belkin, M., Niyogi, P., Laplacian eigenmaps for dimensionality reduction and data representation (2003) Neural Comput., 15, pp. 1373-1396Belotti, P., Lee, J., Liberti, L., Margot, F., Wächter, A., Branching and bounds tightening techniques for non-convex MINLP (2009) Optim. Methods Softw., 24, pp. 597-634Ben-Israel, A., Mond, B., What is invexity? (1986) J. Aust. Math. Soc. Ser. B, 28, pp. 1-9Benedetti, R., Risler, J.-J., (1990) Real Algebraic and Semi-algebraic Sets, , Hermann, ParisBerger, B., Kleinberg, J., Leighton, T., Reconstructing a three-dimensional model with arbitrary errors (1999) J. ACM, 46, pp. 212-235Berman, H., Westbrook, J., Feng, Z., Gilliland, G., Bhat, T., Weissig, H., Shindyalov, I., Bourne, P., The protein data bank (2000) Nucleic Acid Res., 28, pp. 235-242Biggs, N., (1974) Algebraic Graph Theory, , Cambridge University Press, Cambridge, UKBiggs, N., Lloyd, E., Wilson, R., (1976) Graph Theory 1736-1936, , Oxford University Press, OxfordBiswas, P., (2007) Semidefinite Programming Approaches to Distance Geometry Problems, , Ph.D. thesis, Stanford University, Stanford, CABiswas, P., Lian, T., Wang, T., Ye, Y., Semidefinite programming based algorithms for sensor network localization (2006) ACM Trans. Sensor Networks, 2, pp. 188-220Biswas, P., Liang, T.-C., Toh, K.-C., Wang, T.-C., Ye, Y., Semidefinite programming approaches for sensor network localization with noisy distance measurements (2006) IEEE Trans. Automation Sci. Engrg., 3, pp. 360-371Biswas, P., Toh, K.-C., Ye, Y., A distributed SDP approach for large-scale noisy anchorfree graph realization with applications to molecular conformation (2008) SIAM J. Sci. Comput., 30, pp. 1251-1277Biswas, P., Ye, Y., Semidefinite programming for ad hoc wireless sensor network localization (2004) Proceedings of the 3rd International Symposium on Information Processing in Sensor Networks (IPSN04), pp. 46-54. , ACM, New YorkBiswas, P., Ye, Y., A distributed method for solving semidefinite programs arising from ad hoc wireless sensor network localization (2006) Multiscale Optimization Methods and Applications, pp. 69-84. , W. Hager et al., eds., Nonconvex Optim. Appl. 82, Springer, New YorkBjörner, A., Las Vergnas, M., Sturmfels, B., White, N., Ziegler, G., (1993) Oriented Matroids, , Cambridge University Press, Cambridge, UKBlumenthal, L., (1953) Theory and Applications of Distance Geometry, , Oxford University Press, OxfordBohr, H., Brunak, S., (1996) Protein Folds: A Distance Based Approach, , CRC Press, Boca Raton, FLBokowski, J., Sturmfels, B., On the coordinatization of oriented matroids (1986) Discrete Comput. Geom., 1, pp. 293-306Borg, I., Groenen, P., (2010) Modern Multidimensional Scaling, , 2nd ed., Springer, New YorkBoyd, S., El Ghaoui, L., Feron, E., Balakrishnan, V., (1994) Linear Matrix Inequalities in System and Control Theory, , SIAM, PhiladelphiaBreu, H., Kirkpatrick, D., Unit disk graph recognition is NP-hard (1998) Comput. Geom., 9, pp. 3-24Canny, J., Emiris, I., A subdivision-based algorithm for the sparse resultant (2000) J. ACM, 47, pp. 417-451Carroll, J., Chang, J., Analysis of individual differences in multidimensional scaling via an n-way generalization of ""eckart-Young"" decomposition (1970) Psychometrika, 35, pp. 283-319Carvalho, R., Lavor, C., Protti, F., Extending the geometric build-up algorithm for the molecular distance geometry problem (2008) Inform. Process. Lett., 108, pp. 234-237Cauchy, A.-L., Sur les polygones et les polyèdres (1813) J. Ecole Polytechnique, 16, pp. 87-99Cayley, A., A theorem in the geometry of position (1841) Cambridge Math. J., 2, pp. 267-271Chen, H., (2012) Distance Geometry for Kissing Balls, , preprint, arXiv:1203.2131v2Chevalley, C., (1955) The Construction and Study of Certain Important Algebras, , The Mathematical Society of Japan, TokyoClark, B., Colburn, C., Johnson, D., Unit disk graphs (1990) Discrete Math., 86, pp. 165-177Clore, G., Gronenborn, A., Determination of three-dimensional structures of proteins and nucleic acids in solution by nuclear magnetic resonance spectroscopy (1989) Critical Reviews in Biochemistry and Molecular Biology, 24, pp. 479-564Connelly, R., A counterexample to the rigidity conjecture for polyhedra (1978) Inst. Hautes Études Sci. Publ. Math., 47, pp. 333-338Connelly, R., On generic global rigidity (1991) Applied Geometry and Discrete Mathematics, DIMACS Ser. Discrete Math. Theoret. Comput. Sci. 4, pp. 147-155. , AMS, Providence, RIConnelly, R., Generic global rigidity (2005) Discrete Comput. Geom., 33, pp. 549-563Conway, J., Sloane, N., (1993) Sphere Packings, , Lattices and Groups, Springer, BerlinCoope, I., Reliable computation of the points of intersection of n spheres in Rn (2000) Aust. N. Z. Indust. Appl. Math. J., 42, pp. C461-C477Costa, V., Lavor, C., Mucherino, A., Cassioli, A., Carvalho, L., Maculan, N., Discretization orders for protein side chains J. Global Optim., , to appearCremona, L., (1872) Le Figure Reciproche Nella Statica Grafica, , G. Bernardoni, MilanoCremona, L., (1874) Elementi di Calcolo Grafico, , Paravia, TorinoCrippen, G., Distance geometry for realistic molecular conformations (2013) Distance Geometry: Theory, Methods, and Applications, pp. 315-328. , A. Mucherino, C. Lavor, L. Liberti, and N. Maculan, eds., Springer, New YorkCrippen, G., Havel, T., (1988) Distance Geometry and Molecular Conformation, , Wiley, New YorkCrum Brown, A., On the theory of isomeric compounds (1864) Trans. Roy. Soc. Edinburgh, 23, pp. 707-719Cucuringu, M., Lipman, Y., Singer, A., Sensor network localization by eigenvector synchronization over the Euclidean group (2012) ACM Trans. Sensor Networks, 8, pp. 1-42Cucuringu, M., Singer, A., Cowburn, D., Eigenvector synchronization, graph rigidity and the molecule problem (2012) Inform. Inference, 1, pp. 21-67Dattorro, J., (2005) Convex Optimization and Euclidean Distance Geometry, , Mß oo, Palo AltoDattorro, J., Equality relating Euclidean distance cone to positive semidefinite cone (2008) Linear Algebra Appl., 428, pp. 2597-2600De Leeuw, J., Heiser, W., Theory of multidimensional scaling (1982) Classification Pattern Recognition and Reduction of Dimensionality, pp. 285-316. , P. Krishnaiah and L. Kanal, eds., Handbook of Statist. 2, ElsevierDemaine, E., Gomez-Martin, F., Meijer, H., Rappaport, D., Taslakian, P., Toussaint, G., Winograd, T., Wood, D., The distance geometry of music (2009) Comput. Geom., 42, pp. 429-454Deza, M., Deza, E., (2009) Encyclopedia of Distances, , Springer, BerlinDiestel, R., (2005) Graph Theory, , Springer, New YorkDirac, G., On rigid circuit graphs (1961) Abh. Math. Sem. Univ. Hamburg, 25, pp. 71-76Doherty, L., Pister, K., El Ghaoui, L., Convex position estimation in wireless sensor networks (2001) Twentieth Annual Joint Conference of the IEEE Computer and Communications Societies, Vol. 3 of INFOCOM, IEEE, pp. 1655-1663Donald, B., (2011) Algorithms in Structural Molecular Biology, , MIT Press, BostonDong, Q., Wu, Z., A linear-time algorithm for solving the molecular distance geometry problem with exact inter-atomic distances (2002) J. Global Optim., 22, pp. 365-375Dong, Q., Wu, Z., A geometric build-up algorithm for solving the molecular distance geometry problem with sparse distance data (2003) J. Global Optim., 26, pp. 321-333Dress, A., Havel, T., Distance geometry and geometric algebra (1993) Found. Phys., 23, pp. 1357-1374Dzemyda, G., Kurasova, O., Žilinskas, J., (2013) Multidimensional Data Visualiation: Methods and Applications, , Springer, New YorkDzhafarov, E., Colonius, H., Reconstructing distances among objects from their discriminability (2006) Psychometrika, 71, pp. 365-386Eaton, J., (2002) GNU Octave Manual, , Network Theory LimitedEckart, C., Young, G., The approximation of one matrix by another of lower rank (1936) Psychometrika, 1, pp. 211-218Emiris, I., Mourrain, B., Computer algebra methods for studying and computing molecular conformations (1999) Algorithmica, 25, pp. 372-402Eren, T., Goldenberg, D., Whiteley, W., Yang, Y., Morse, A., Anderson, B., Belhumeur, P., Rigidity, computation, and randomization in network localization (2004) IEEE Infocom Proc., 4, pp. 2673-2684Everitt, B., Rabe-Hesketh, S., (1997) The Analysis of Proximity Data, , Arnold, LondonFeferman, S., Dawson, J., Kleene, S., Moore, G., Solovay, R., Van Heijenoort, J., (1986) Kurt Gödel: Collected Works, 1. , Oxford University Press, OxfordFekete, Z., Jordán, T., Uniquely localizable networks with few anchors (2006) Algorithmic Aspects of Wireless Sensor Networks, pp. 176-183. , S. Nikoletseas and J. Rolim, eds., Lecture Notes in Comput. Sci. 4240, Springer, BerlinForman, G., Zahorjan, J., The challenges of mobile computing (1994) IEEE Comput., 27, pp. 38-47Fudos, I., Hoffmann, C., A graph-constructive approach to solving systems of geometric constraints (1997) ACM Trans. Graphics, 16, pp. 179-216Garey, M., Johnson, D., (1979) Computers and Intractability: A Guide to the Theory of NPCompleteness, , Freeman and Company, New YorkGibson, K., Scheraga, H., Energy minimization of rigid-geometry polypeptides with exactly closed disulfide loops (1997) J. Comput. Chem., 18, pp. 403-415Gluck, H., Almost all simply connected closed surfaces are rigid (1975) Geometric Topology, pp. 225-239. , A. Dold and B. Eckmann, eds., Lecture Notes in Math. 438, Springer, BerlinGlunt, W., Hayden, T.L., Hong, S., Wells, J., An alternating projection algorithm for computing the nearest Euclidean distance matrix (1990) SIAM J. Matrix Anal. Appl., 11, pp. 589-600Gortler, S., Healy, A., Thurston, D., Characterizing generic global rigidity (2010) Amer. J. Math., 132, pp. 897-939Gower, J., Some distance properties of latent root and vector methods in multivariate analysis (1966) Biometrika, 53, pp. 325-338Gower, J., Euclidean distance geometry (1982) Math. Sci., 7, pp. 1-14Gramacho, W., Mucherino, A., Lavor, C., Maculan, N., A parallel BP algorithm for the discretizable distance geometry problem (2012) Proceedings of the Workshop on Parallel Computing and Optimization, Shanghai, pp. 1756-1762. , IEEEGraver, J., Rigidity matroids (1991) SIAM J. Discrete Math., 4, pp. 355-368Graver, J., Servatius, B., Servatius, H., (1993) Combinatorial Rigidity, , AMS, Providence, RIGrippo, L., Sciandrone, M., On the convergence of the block nonlinear Gauss-Seidel method under convex constraints (2000) Oper. Res. Lett., 26, pp. 127-136Grone, R., Johnson, C., De Sá, E., Wolkowicz, H., Positive definite completions of partial Hermitian matrices (1984) Linear Algebra Appl., 58, pp. 109-124Grosso, A., Locatelli, M., Schoen, F., Solving molecular distance geometry problems by global optimization algorithms (2009) Comput. Optim. Appl., 43, pp. 23-27Havel, T., Metric matrix embedding in protein structure calculations (2003) Magnetic Resonance Chem., 41, pp. 537-550Havel, T., Kuntz, I., Crippen, G., The theory and practice of distance geometry (1983) Bull. Math. Biol., 45, pp. 665-720Hendrickson, B., Conditions for unique graph realizations (1992) SIAM J. Comput., 21, pp. 65-84Hendrickson, B., The molecule problem: Exploiting structure in global optimization (1995) SIAM J. Optim., 5, pp. 835-857Henneberg, L., (1886) Statik der Starren Systeme, , Bergstræsser, DarmstadtHenneberg, L., (1911) Die Graphische Statik der Starren Systeme, , Teubner, LeipzigHoai An, L., Solving large scale molecular distance geometry problems by a smoothing technique via the Gaussian transform and D.C. Programming (2003) J. Global Optim., 27, pp. 375-397Hoai An, L.T., Dinh Tao, P., Large-scale molecular optimization from distance matrices by a d.c. Optimization approach (2003) SIAM J. Optim., 14, pp. 77-114Huang, H.-X., Liang, Z.-A., Pardalos, P., Some properties for the Euclidean distance matrix and positive semidefinite matrix completion problems (2003) J. Global Optim., 25, pp. 3-21Hunt, K., (1990) Kinematic Geometry of Mechanisms, , Oxford University Press, OxfordIzrailev, S., Zhu, F., Agrafiotis, D., A distance geometry heuristic for expanding the range of geometries sampled during conformational search (2006) J. Comput. Chem., 26, pp. 1962-1969Jackson, B., Jordán, T., Connected rigidity matroids and unique realization of graphs (2005) J. Combin. Theory Ser. B, 94, pp. 1-29Jackson, B., Jordán, T., On the rigidity of molecular graphs (2008) Combinatorica, 28, pp. 645-658Jackson, B., Jordán, T., Graph theoretic techniques in the analysis of uniquely localizable sensor networks (2009) Localization Algorithms and Strategies for Wireless Sensor Networks: Monitoring and Surveillance Techniques for Target Tracking, pp. 146-173. , G. Mao and B. Fidan, eds., IGI GlobalJohnson, C., Kroschel, B., Wolkowicz, H., An interior-point method for approximate positive semidefinite completions (1998) Comput. Optim. Appl., 9, pp. 175-190Jolliffe, I., (2010) Principal Component Analysis, , 2nd ed., Springer, BerlinKostrowicki, J., Piela, L., Diffusion equation method of global minimization: Performance for standard test functions (1991) J. Optim. Theory Appl., 69, pp. 269-284Krishnaiah, P., Kanal, L., (1982) Theory of Multidimensional Scaling, 2. , North-HollandKrislock, N., (2010) Semidefinite Facial Reduction for Low-Rank Euclidean Distance Matrix Completion, , Ph.D. thesis, University of WaterlooKrislock, N., Wolkowicz, H., Explicit sensor network localization using semidefinite representations and facial reductions (2010) SIAM J. Optim., 20, pp. 2679-2708Kruskal, J., Multidimensional scaling by optimizing goodness of fit to a nonmetric hypothesis (1964) Psychometrika, 29, pp. 1-27Kruskal, J., Nonmetric multidimensional scaling: A numerical method (1964) Psychometrika, 29, pp. 115-129Kucherenko, S., Belotti, P., Liberti, L., Maculan, N., New formulations for the kissing number problem (2007) Discrete Appl. Math., 155, pp. 1837-1841Kucherenko, S., Sytsko, Y., Application of deterministic low-discrepancy sequences in global optimization (2004) Comput. Optim. Appl., 30, pp. 297-318Laman, G., On graphs and rigidity of plane skeletal structures (1970) J. Engrg. Math., 4, pp. 331-340Laurent, M., Cuts, matrix completions and graph rigidity (1997) Math. Program., 79, pp. 255-283Laurent, M., Polynomial instances of the positive semidefinite and Euclidean distance matrix completion problems (2000) SIAM J. Matrix Anal. Appl., 22, pp. 874-894Laurent, M., Matrix completion problems (2009) Encyclopedia of Optimization, pp. 1967-1975. , 2nd ed., C. Floudas and P. Pardalos, eds., Springer, New YorkLavor, C., On generating instances for the molecular distance geometry problem (2006) Global Optimization: From Theory to Implementation, pp. 405-414. , L. Liberti and N.Maculan, eds., Springer, BerlinLavor, C., Lee, J., Lee-St. John, A., Liberti, L., Mucherino, A., Sviridenko, M., Discretization orders for distance geometry problems (2012) Optim. Lett., 6, pp. 783-796Lavor, C., Liberti, L., Maculan, N., Grover's algorithm applied to the molecular distance geometry problem (2005) Proceedings of the 7th Brazilian Congress of Neural Networks, Natal, BrazilLavor, C., Liberti, L., Maculan, N., Computational experience with the molecular distance geometry problem (2006) Global Optimization: Scientific and Engineering Case Studies, pp. 213-225. , J. Pintér, ed., Springer, BerlinLavor, C., Liberti, L., Maculan, N., (2006) The Discretizable Molecular Distance Geometry Problem, , preprint, arXiv:q-bio/0608012Lavor, C., Liberti, L., Maculan, N., Molecular distance geometry problem (2009) Encyclopedia of Optimization, pp. 2305-2311. , 2nd ed., C. Floudas and P. Pardalos, eds., Springer, New YorkLavor, C., Liberti, L., Maculan, N., A note on ""a Branch-and-Prune Algorithm for the Molecular Distance Geometry Problem"" (2011) Internat. Trans. Oper. Res., 18, pp. 751-752Lavor, C., Liberti, L., Maculan, N., Mucherino, A., The discretizable molecular distance geometry problem (2012) Comput. Optim. Appl., 52, pp. 115-146Lavor, C., Liberti, L., Maculan, N., Mucherino, A., Recent advances on the discretizable molecular distance geometry problem (2012) European J. Oper. Res., 219, pp. 698-706Lavor, C., Liberti, L., Mucherino, A., The interval branch-and-prune algorithm for the discretizable molecular distance geometry problem with inexact distances (2013) J. Global Optim., 56, pp. 855-871Lavor, C., Liberti, L., Mucherino, A., Maculan, N., On a discretizable subclass of instances of the molecular distance geometry problem (2009) Proceedings of the 24th Annual ACM Symposium on Applied Computing, pp. 804-805. , D. Shin, ed., ACM, New YorkLavor, C., Mucherino, A., Liberti, L., Maculan, N., An artificial backbone of hydrogens for finding the conformation of protein molecules (2009) Proceedings of the Computational Structural Bioinformatics Workshop, pp. 152-155. , Washington D.C., IEEELavor, C., Mucherino, A., Liberti, L., Maculan, N., Computing artificial backbones of hydrogen atoms in order to discover protein backbones (2009) Proceedings of the International Multiconference on Computer Science and Information Technology, Mragowo, Poland, IEEE, pp. 751-756Lavor, C., Mucherino, A., Liberti, L., Maculan, N., Discrete approaches for solving molecular distance geometry problems using NMR data (2010) Internat. J. Comput. Biosci., 1, pp. 88-94Lavor, C., Mucherino, A., Liberti, L., Maculan, N., On the solution of molecular distance geometry problems with interval data (2010) Proceedings of the International Workshop on Computational Proteomics, Hong Kong, IEEE, pp. 77-82Lavor, C., Mucherino, A., Liberti, L., Maculan, N., On the computation of protein backbones by using artificial backbones of hydrogens (2011) J. Global Optim., 50, pp. 329-344Lavor, C., Mucherino, A., Liberti, L., Maculan, N., Finding low-energy homopolymer conformations by a discrete approach (2012) Proceedings of the Global Optimization Workshop, , D. Aloise, P. Hansen, and C. Rocha, eds., Universidade Federal do Rio Grande do Norte, NatalLe Grand, S., Elofsson, A., Eisenberg, D., The effect of distance-cutoff on the performance of the distance matrix error when used as a potential function to drive conformational search (1996) Protein Folds: A Distance Based Approach, pp. 105-113. , H. Bohr and S. Brunak, eds., CRC Press, Boca Raton, FLLee, J., Verleysen, M., (2010) Nonlinear Dimensionality Reduction, , Springer, BerlinLeung, N.-H.Z., Toh, K.-C., An SDP-based divide-and-conquer algorithm for large-scale noisy anchor-free graph realization (2009) SIAM J. Sci. Comput., 31, pp. 4351-4372Liberti, L., (2004) Reformulation and Convex Relaxation Techniques for Global Optimization, , Ph.D. thesis, Imperial College London, LondonLiberti, L., Reformulations in mathematical programming: Definitions and systematics (2009) RAIRO Oper. Res., 43, pp. 55-85Liberti, L., Dražic, M., Variable neighbourhood search for the global optimization of constrained NLPs (2005) Proceedings of GO Workshop, Almeria, SpainLiberti, L., Kucherenko, S., (2004) Comparison of Deterministic and Stochastic Approaches to Global Optimization, , Tech. Rep. 2004.25, DEI, Politecnico di MilanoLiberti, L., Lavor, C., On a relationship between graph realizability and distance matrix completion (2013) Optimization Theory, Decision Making, and Operational Research Applications, pp. 39-48. , A. Migdalas, A. Sifaleras, C. Georgiadis, J. Papathanaiou, and E. Stiakakis, eds., Proc. Math. Statist. 31, Springer, BerlinLiberti, L., Lavor, C., Maculan, N., A branch-and-prune algorithm for the molecular distance geometry problem (200",Euclidean Distance Geometry And Applications,,'Society for Industrial & Applied Mathematics (SIAM)',10.1137/120875909,,core
29432924,2015-08-01T00:00:00Z,"<p class=""Abstractandkeywordscontent""><span lang=""EN-US"">Although robotics has progressed to the extent that it has become relatively accessible with low-cost projects, there is still a need to create models that accurately represent the physical behavior of a robot. Creating a completely virtual platform allows us to test behavior algorithms such as those implemented using artificial intelligence, and additionally, it enables us to find potential problems in the physical design of the robot. The present work describes a methodology for the construction of a kinematic model and a simulation of the autonomous robot, specifically of an omni-directional wheeled robot. This paper presents the kinematic model development and its implementation using several tools. The result is a model that follows the kinematics of a triangular omni-directional mobile wheeled robot, which is then tested by using a 3D model imported from 3D Studio</span><span lang=""EN-US"">®</span><span lang=""EN-US""> and Matlab</span><span lang=""EN-US"">® for the simulation. The environment used for the experiment is very close to the real environment and reflects the kinematic characteristics of the robot.</span></p",Kinematics modeling and simulation of an autonomous omni-directional mobile robot,,Universidad Nacional de Colombia,,"[{'title': None, 'identifiers': ['issn:0120-5609', '0120-5609', '2248-8723', 'issn:2248-8723']}]",core
296648505,2015-11-26T15:11:44Z,"Camera-based estimation of drivable image areas is still in evolution. These systems have been developed for improved safety and convenience, without the need to adapt itself to the environment. Machine Vision is an important tool to identify the region that includes the road in images. Road detection is the major task of autonomous vehicle guidance. In this way, this work proposes a drivable region detection algorithm that generates the region of interest from a dynamic threshold search method and from a drag process (DP). Applying the DP to estimation of drivable image areas has not been done yet, making the concept unique. Our system was has been evaluated from real data obtained by intelligent platforms and tested in different types of image texture, which include occlusion case, obstacle detection and reactive navigation. © 2013 IEEE.6368Australian Dedicated Short Range Communication (AusDSRC),Griffith University Intelligent Control Systems Laboratory,Intelligent Transport Systems AustraliaBonin-Font, F., Ortiz, A., Oliver, G., Visual navigation for mobile robots: A survey (2008) Journal of Intelligent and Robotic Systems, 53 (3), pp. 263-296Rodríguez Flórez, S.A., (2010) Contributions by Vision Systems to Multi-sensor Object Localization and Tracking for Intelligent Vehicles, , Thesis, UTC, FranceThrun, S., Stanley, the robot that won the darpa grand challenge (2006) Journal of Robotic Systems, 23 (9), pp. 661-692. , 2006, ISSN: 0741-2223(2007) Spirit of Berlin: An Autonomous Car for the DARPA Urban Challenge Hardware and Software Architecture, , retrieved Jan 05, 2010]Gietelink, O., Ploeg, J., De Schutter, B., Verhaegen, M., Development of advanced driver assistance systems with vehicle hardware-in-The-loop simulations (2006) Vehicle System Dynamics, 44 (7)Ulrich, I., Nourbakhsh, I., Appearance-based obstacle detection with monocular color vision (2000) Proceedings of the AAAI National Conference on Artificial Intelligence, pp. 866-871Kim, B., Hubbard, P., Necsulescu, D., (2003) Swarming Unmanned Aerial Vehicles: Concept Development and Experimentation, A State of the Art Review on Flight and Mission Control, , Technical MemorandumAviña-Cervantes, G., Devy, M., Marín, A., Lane extraction and tracking for robot navigation in agricultural applications (2003) Proceedings of the IEEE ICAR 2003Dahlkamp, H., Self-supervised monocular road detection in desert terrain (2006) Proceedings of the Robotics Science and Systems ConferenceDiego, F., Álvarez, J.M., Serrat, J., López, A.M., Vision based road detection via on line video registration (2010) Proceedings of the IEEE ITSC 2010Chetan, J., Madhava, K., Jawahar, C.V., An adaptive outdoor terrain classification methodology using monocular camera (2010) Proceedings of the IEEE IROS 2010Yanqing, W., Deyun, C., Chaoxia, S., Peidong, W., Vision-based road detection by monte carlo method (2010) Information Technology Journal, 9, pp. 481-487Yamaguchi, K., Watanabe, A., Naito, T., Ninomiya, Y., Road region estimation using a sequence of monocular images (2008) Proceedings of the International Conference on Pattern Recognition, 2008Otsu, N., A threshold selection method from graylevel histogram (1978) IEEE Transactions on Systems, Man, and Cybernetics, 9, pp. 62-66Bertozzi Broggi, A.M., Fascioli, A., Vision-based intelligent vehicles: State of the art and perspectives (2000) Robotics and Autonomous Systems, 32, pp. 1-16Miranda Neto, A., Rittner, L., Leite, N., Zampieri, D.E., Lotufo, R., Mendeleck, A., Pearson's correlation coefficient for discarding redundant information in real time autonomous navigation systems (2007) Proceedings of the IEEE MSC 2007Benini, L., Bogliolo, A., Micheli, G.D., A survey of design techniques for system-level dynamic power management (2000) IEEE Transactions on Very Large Scale Integration Systems, 8 (3), pp. 299-316Miranda Neto, A., Victorino, A.C., Fantoni, I., Zampieri, D.E., Real-time dynamic power management based on pearson's correlation coefficient (2011) Proceedings of the IEEE ICAR 2011King, H.L., Vision-based lane-vehicle detection and tracking (2009) IAENG Transactions on Engineering Technologies Volume 3-Special Edition, pp. 157-171. , American Institute of PhysicsEttinger, S., Vision-guided flight stability and control for micro air vehicles (2003) Adv. Robotics, pp. 617-640Neto, A.M., Victorino, A.C., Fantoni, I., Zampieri, D.E., Robust horizon finding algorithm for real-time autonomous navigation based on monocular vision (2011) Proceedings of the IEEE ITSC 2011Miranda Neto, A., Rittner, L., A simple and efficient road detection algorithm for real time autonomous navigation based on monocular vision (2006) Proceedings of the IEEE 3rd LARS 2006Gonzalez, C.R., Woods, E.R., (1991) Digital Image Processing, , Addison-Wesley Publishing CompanySahoo, P.K., Soltani, S., Wong, A.K.C., A survey of thresholding techniques (1988) Comput. Vision Graphics Image Processing, 41, pp. 233-260Lee, U.S., Chung, Y.S., Park, H.R., A comparative performance study of several global thresholding techniques for segmentation (1990) Computer Vision, Graphics, and Image ProcessingSezgin, M., Sankur, B., Survey over image thresholding techniques and quantitative performance evaluation (2004) Journal of Electronic Imaging, 13, pp. 146-165Rauskolb, F.W., Caroline: An autonomously driving vehicle for urban environments (2008) Journal of Field Robotics, 25 (9), pp. 674-724Canny, J.F., (1986) A Computational Approach to Edge Detection, , IEEE Trans. Pattern Anal. Machine IntellBallard, D., Generalized hough transform to detect arbitrary shapes (1981) IEEE Trans. Pattern Anal. Machine Intell, 13 (2), pp. 111-122White, F.M., (1986) Fluid Mechanics, , 2nd Ed. McGraw HillDARPA 2005. DARPA Grand ChallengeNeto, A.M., Victorino, A.C., Fantoni, I., Zampieri, D.E., Ferreira, J.V., Visual-perception layer applied to reactive navigation (2012) Proceedings of the IEEE 9th LARS 2012http://youtu.be/ZpEbRo32pY8, retrieved Jan 31, 201",Real-time Estimation Of Drivable Image Area Based On Monocular Vision,,,10.1109/IVS.2013.6629448,,core
300347872,2015-01-01T00:00:00,"Complex real problems increasingly require intelligent systems that combine knowledge, techniques and methodologies from various sources. Intelligent systems based on artificial intelligence techniques that are associated with the behavior of people can perform the processes of learning, reasoning and solving all kinds of problems. Such systems, which automatically can perform tasks set by the user or other software, today thankfully called intelligent agents. Independent, intelligent agents on the Internet can be very successful to perform some search work on behalf of and for the needs of different users. For efficient collection, manipulation and management of data, such software can be very interesting from the standpoint of intelligent data analysis in many areas the police. Analysis of the data collected by an intelligent agent (a software robot-bot) can be successfully utilized, among many jobs in the police, and in the field of crime and in particular manifestation of cyber­crime, traffic safety, emergencies, etc. To make the collection and analysis of data from criminal activities on the Internet effective, it is necessary to examine the existing artificial intelligence techniques to be used for the conclusion of the intelligent agents. On the other hand, using of methods of artificial intelligence in finding data along with intelligent data analysis (data mining) should be used, which has found wide use in the area of business, economics, mechanics, medicine, genetics, transport etc.Kompleksni realni problemi sve češće zahtevaju inteligentne sisteme koji kombinuju znanje, tehnike i metodologije iz različitih izvora. Inteligentni sistemi bazirani na tehnikama veštačke inteligencije koje asociraju na ponašanje ljudi mogu da obavljaju procese učenja, zaključivanja i rešavanje raznovrsnih problema. Ovakvi sistemi, koji automatski mogu da izvrše zadatke zadate od strane korisnika ili drugih softvera, danas se sreću pod imenom inteligentni agenti. Samostalno, inteligentni agenti na Internetu mogu veoma uspešno da izvode neki pretraživački posao u ime i za potrebe raznih korisnika. Zbog efikasnog sakupljanja, manipulisanja i upravljanja podacima, ovakvi softveri mogu biti veoma interesantni sa stanovišta inteligentne analize podataka u mnogim oblastima policije. Analiza podataka sakupljenih od strane inteligentnog agenta (softverskog robota - bota) može se uspešno iskoristiti, između mnogih poslova u policiji, i na polju kriminala i naročito pojavnog oblika sajber kriminala, bezbednosti saobraćaja, vanrednih situacija itd. Kako bi sakupljanje i analiza podataka iz kriminalnih aktivnosti na Internetu bila efikasna, neophodno je sagledati postojeće tehnike veštačke inteligencije koje se koriste za zaključivanje u inteligentnim agentima. S druge strane, treba iskoristiti metode veštačke inteligencije u pronalaženju podataka pri inteligentnoj analizi podataka (data mining-u) koja je našla široku primenu u oblasti poslovanja preduzeća, ekonomije, mehanike, medicine, genetike, saobraćaja i sl",Veštačka intelegencija u prikupljanju i analizi podataka u policiji,https://core.ac.uk/download/pdf/300347872.pdf,'Centre for Evaluation in Education and Science (CEON/CEES)',10.5937/NBP1503131K,"[{'title': 'Nauka bezbednost policija', 'identifiers': ['issn:0354-8872', '0354-8872']}]",core
46920244,2015-01-01T00:00:00,"In this thesis, we introduce a novel architecture called  Intelligent Architecture for Legged Robot Terrain Classification Using Proprioceptive and Exteroceptive Data (iARTEC ) . The proposed architecture integrates different terrain characterization and classification with other robotic system components. Within iARTEC , we consider the problem of having a legged robot autonomously learn to identify different terrains. Robust terrain identification can be used to enhance the capabilities of legged robot systems, both in terms of locomotion and navigation. For example, a robot that has learned to differentiate sand from gravel can autonomously modify (or even select a different) path in favor of traversing over a better terrain. The same knowledge of the terrain type can also be used to guide a robot in order to avoid specific terrains. To tackle this problem, we developed four approaches for terrain characterization, classification, path planning, and control for a mobile legged robot. We developed a particle system inspired approach to estimate the robot footâ ground contact interaction forces. The approach is derived from the well known Bekkerâ s theory to estimate the contact forces based on its point contact model concepts. It is realistically model real-time 3-dimensional contact behaviors between rigid body objects and the soil. For a real-time capable implementation of this approach, its reformulated to use a lookup table generated from simple contact experiments of the robot foot with the terrain. Also, we introduced a short-range terrain classifier using the robot embodied data. The classifier is based on a supervised machine learning approach to optimize the classifier parameters and terrain it using proprioceptive sensor measurements. The learning framework preprocesses sensor data through channel reduction and filtering such that the classifier is trained on the feature vectors that are closely associated with terrain class. For the long-range terrain type prediction using the robot exteroceptive data, we present an online visual terrain classification system. It uses only a monocular camera with a feature-based terrain classification algorithm which is robust to changes in illumination and view points. For this algorithm, we extract local features of terrains using Speed Up Robust Feature (SURF). We encode the features using the Bag of Words (BoW) technique, and then classify the words using Support Vector Machines (SVMs). In addition, we described a terrain dependent navigation and path planning approach that is based on E* planer and employs a proposed metric that specifies the navigation costs associated terrain types. This generated path naturally avoids obstacles and favors terrains with lower values of the metric. At the low level, a proportional input-scaling controller is designed and implemented to autonomously steer the robot to follow the desired path in a stable manner. iARTEC performance was tested and validated experimentally using several different sensing modalities (proprioceptive and exteroceptive) and on the six legged robotic platform CREX. The results show that the proposed architecture integrating the aforementioned approaches with the robotic system allowed the robot to learn both robot-terrain interaction and remote terrain perception models, as well as the relations linking those models. This learning mechanism is performed according to the robot own embodied data. Based on the knowledge available, the approach makes use of the detected remote terrain classes to predict the most probable navigation behavior. With the assigned metric, the performance of the robot on a given terrain is predicted. This allows the navigation of the robot to be influenced by the learned models. Finally, we believe that iARTEC and the methods proposed in this thesis can likely also be implemented on other robot types (such as wheeled robots), although we did not test this option in our work",An Intelligent Architecture for Legged Robot Terrain Classification Using Proprioceptive and Exteroceptive Data,https://core.ac.uk/download/46920244.pdf,,,,core
55638595,2015-01-01T00:00:00,"""Lecture notes in computational vision and biomechanics series, ISSN 2212-9391, vol. 19""Hand gesture recognition is a natural way of human computer interaction and an area of very
active research in computer vision and machine learning. This is an area with many different possible applications, giving users a simpler and more natural way to communicate with robots/systems interfaces, without the need for extra devices. So, the primary goal of gesture recognition research applied to Human-Computer Interaction (HCI) is to create systems, which can identify specific human gestures and use them to convey information or controlling devices. For that, vision-based hand gesture interfaces require fast and extremely robust hand detection, and gesture recognition in real time. This paper presents a solution, generic enough, with the help of machine learning algorithms, allowing its application in a wide range of human-computer interfaces, for real-time gesture recognition. Experiments carried out showed that the system was able to achieve an accuracy of 99.4% in terms of hand posture recognition and an average accuracy of 93.72% in terms of dynamic gesture recognition. To validate the proposed framework, two applications were implemented. The first one is a real-time system able to help a robotic soccer referee judge a game in real time. The prototype combines a vision-based hand gesture recognition system with a formal language definition, the Referee CommLang, into what is called the Referee Command Language Interface System (ReCLIS). The second one is a real-time system able to interpret the Portuguese Sign Language. Sign languages are not standard and universal and the grammars differ from country to country. Although the implemented prototype was only trained to recognize the vowels, it is easily extended to recognize the rest of the alphabet, being a solid foundation for the development of any vision-based sign language recognition user interface system.(undefined",Hand gesture recognition system based in computer vision and machine learning,https://core.ac.uk/download/55638595.pdf,'Springer Science and Business Media LLC',10.1007/978-3-319-13407-9_21,"[{'title': None, 'identifiers': ['issn:2212-9391', '2212-9391']}]",core
101059112,07/01/2015,"A simple vacuum-cleaner agent is introduced in Russell and Norvig’s artificial intelligence (AI) text [Russell and Norvig, 2003] to illustrate different agent types to beginning AI students. Underlying the different agent types are several simple actions that take place in a vacuum cleaner world consisting of a grid of squares, some of which contain dirt. The agent’s actions include turning left or right, moving forward, and picking up dirt. Having students write a program to simulate the vacuum world is a useful way to provide them with a feeling for different agent types in a simplified environment. However, implementing a vacuum-cleaner agent using a low-cost robotics kit might teach students much more about agents in the real world and could serve to get them interested in and excited about AI in a way that working with purely simulated environments may not. This paper describes the design and implementation of “Dustbot”, a robot based on the Russell and Norvig vacuum-cleaner agent. The Dustbot project was carried out as part of an independent study by a student who had already taken the undergraduate AI course. The purpose of the project was to test and debug the vacuum-cleaner robot and to develop a set of instructions that could be used in subsequent offerings of our undergraduate AI course",Dustbot: Bringing a Vacuum-Cleaner Agent to Life,,,,,core
103073867,31/10/2015,"Robots are gradually entering into diverse ap-plication domains such as home, office, and playing field. This article presents advanced re-search activities related to these domains. First is RoboCup which is an attempt to promote AI and robotics research by providing a common task for evaluation of various performance, the-ories, algorithms, and robot architectures. In order for robots (both physical robots and soft agents) to play a soccer game reasonably well, a wide range of technologies need to be inte-grated and a number of technical breakthrough must be accomplished. The recent results from the last two RoboCups are reviewed and future leagues are introduced. Second, the richer do-main of service robotics has also received signif-icant interest recently. The task here is to serve as a human assistant in an office or domestic en-vironment, for tasks like cleaning and delivery. The human-robot interaction is a key issue to success, which poses new challenges in terms of integration of spoken dialogue, gestures, body language, etc. In addition mobile manipulation and safe navigation around humans is essential to success. These two areas integrates many different disciplines including control, percep-tion, natural language processing, hybrid sys-tems and handling of uncertainty, and applied to tour guiding, mail delivery, domestic ser-vices, and rescue activities. 1 In t roduct ion Robotics offers a fertile ground for demonstration of ar-tificial intelligence techniques. The domain provides a basis for real-world evaluation of techniques under real-istic assumptions. Construction of robotic systems re-quires at the same time integration of a diverse range of expertise in order to provide operational systems. Tradi-tionally robot systems have been used in manufacturing, in particular in the car industry. The majority of robots sold today are still deployed for spot-welding and ca","office, and playing field Robotics in the home,",,,,,core
103434678,04/12/2015,"Abstract Cameras provide a rich source of information while being passive, cheap and lightweight for small Unmanned Aerial Vehicles (UAVs). In this work we present the first implementation of receding horizon control, which is widely used in ground vehicles, with monocular vision as the only sensing mode for autonomous UAV flight in dense clutter. Two key contributions make this possible: novel cou-pling of perception and control via relevant and diverse, multiple interpretations of the scene around the robot, leveraging recent advances in machine learning to showcase anytime budgeted cost-sensitive feature selection, and fast non-linear re-gression for monocular depth prediction. We empirically demonstrate the efficacy of our novel pipeline via real world experiments of more than 2 kms through dense trees with an off-the-shelf quadrotor. Moreover our pipeline is designed to combine information from other modalities like stereo and lidar. ",Vision and Learning for Deliberative Monocular Cluttered Flight,,,,,core
287791827,2016-11-02T00:00:00,"Embodied artificial cognitive systems, such as autonomous robots or intelligent observers, connect cognitive processes to sensory and effector systems in real time. Prime candidates for such embodied intelligence are neurally inspired architectures. While components such as forward neural networks are well established, designing pervasively autonomous neural architectures remains a challenge. This includes the problem of tuning the parameters of such architectures so that they deliver specified functionality under variable environmental conditions and retain these functions as the architectures are expanded. The scaling and autonomy problems are solved, in part, by dynamic field theory (DFT), a theoretical framework for the neural grounding of sensorimotor and cognitive processes. In this paper, we address how to efficiently build DFT architectures that control embodied agents and how to tune their parameters so that the desired cognitive functions emerge while such agents are situated in real environments. In DFT architectures, dynamic neural fields or nodes are assigned dynamic regimes, that is, attractor states and their instabilities, from which cognitive function emerges. Tuning thus amounts to determining values of the dynamic parameters for which the components of a DFT architecture are in the specified dynamic regime under the appropriate environmental conditions. The process of tuning is facilitated by the software framework cedar, which provides a graphical interface to build and execute DFT architectures. It enables to change dynamic parameters online and visualize the activation states of any component while the agent is receiving sensory inputs in real time. Using a simple example, we take the reader through the workflow of conceiving of DFT architectures, implementing them on embodied agents, tuning their parameters, and assessing performance while the system is coupled to real sensory inputs",Developing dynamic field theory architectures for embodied cognitive systems with \(\it cedar\),,,,,core
42678687,2016-05-07T00:00:00,"In this paper, we propose a novel unsupervised learning method for the
lexical acquisition of words related to places visited by robots, from human
continuous speech signals. We address the problem of learning novel words by a
robot that has no prior knowledge of these words except for a primitive
acoustic model. Further, we propose a method that allows a robot to effectively
use the learned words and their meanings for self-localization tasks. The
proposed method is nonparametric Bayesian spatial concept acquisition method
(SpCoA) that integrates the generative model for self-localization and the
unsupervised word segmentation in uttered sentences via latent variables
related to the spatial concept. We implemented the proposed method SpCoA on
SIGVerse, which is a simulation environment, and TurtleBot2, which is a mobile
robot in a real environment. Further, we conducted experiments for evaluating
the performance of SpCoA. The experimental results showed that SpCoA enabled
the robot to acquire the names of places from speech sentences. They also
revealed that the robot could effectively utilize the acquired spatial concepts
and reduce the uncertainty in self-localization.Comment: This paper was accepted in the IEEE Transactions on Cognitive and
  Developmental Systems. (04-May-2016","Spatial Concept Acquisition for a Mobile Robot that Integrates
  Self-Localization and Unsupervised Word Discovery from Spoken Sentences",http://arxiv.org/abs/1602.01208,'Institute of Electrical and Electronics Engineers (IEEE)',10.1109/TCDS.2016.2565542,,core
44483801,2015-01-01T00:00:00,"Volume 1 : This book is focused on the recent advances in computer vision methodologies and technical solutions using conventional and intelligent paradigms. The Contributions include: ·         Morphological Image Analysis for Computer Vision Applications. ·         Methods for Detecting of Structural Changes in Computer Vision Systems. ·         Hierarchical Adaptive KL-based Transform: Algorithms and Applications. ·         Automatic Estimation for Parameters of Image Projective Transforms Based on Object-invariant Cores. ·         A Way of Energy Analysis for Image and Video Sequence Processing. ·         Optimal Measurement of Visual Motion Across Spatial and Temporal Scales. ·         Scene Analysis Using Morphological Mathematics and Fuzzy Logic. ·         Digital Video Stabilization in Static and Dynamic Scenes. ·         Implementation of Hadamard Matrices for Image Processing. ·         A Generalized Criterion of Efficiency for Telecommunication Systems. The book is directed to PhD students, professors, researchers and software developers working in the areas of digital video processing and computer vision technologies.Volume 2 : The research book is focused on the recent advances in computer vision methodologies and innovations in practice. The Contributions include: ·          Human Action Recognition: Contour-Based and Silhouette-based Approaches. ·         The Application of Machine Learning Techniques to Real Time Audience Analysis System. ·         Panorama Construction from Multi-view Cameras in Outdoor Scenes. ·         A New Real-Time Method of Contextual Image Description and Its Application in Robot Navigation and Intelligent Control. ·         Perception of Audio Visual Information for Mobile Robot Motion Control Systems. ·         Adaptive Surveillance Algorithms Based on the Situation Analysis. ·         Enhanced, Synthetic and Combined Vision Technologies for Civil Aviation. ·         Navigation of Autonomous Underwater Vehicles Using Acoustic and Visual Data Processing. ·         Efficient Denoising Algorithms for Intelligent Recognition Systems. ·         Image Segmentation Based on Two-dimensional Markov Chains. The book is directed to the PhD students, professors, researchers and software developers working in the areas of digital video processing and computer vision technologies",Computer vision in control systems,,'Springer Science and Business Media LLC',10.1007/978-3-319-10653-3,,core
297620750,,"Telepresence Mobile Robots have prominent attributes in many fields as it provides virtual presence for human robot interaction. The deployment of this robot in healthcare sector has improved patient care and health. The vision system in a telepresence robot allows two way audiovisual communication between people at different location. In spite of such advancement, the manual way of controlling a robot to recognise and track people during an emergency is not favourable for a long duration. To circumvent this problem, biometric method using human face is proposed in this research which is implemented on Medical Telediagnosis Robot. This paper details the design of the face recognition and tracking system with four automated modules which are motion detection, face detection, face recognition and face tracking. The modules are developed with different algorithm and tested individually to ensure the stability of the system. Artificial Intelligence technique was applied at the face recognition stage while a two degree of freedom mechanism for actuator control was used at face tracking stage. A sequential mode operation is proposed to reduce the execution time in a real-time environment. To achieve this, only one module is operated at each time. A Graphical User Interface was developed to ease the users at the local and robot environment. The system is designed in LabVIEW platform. The biometric system proposed with hybrid algorithm at each module adapts for face images detected at different distances, poses and lighting condition. This system was tested in real-time and has an execution time of 55ms and 98% accuracy. The stand alone system designed for Medical Telediagnosis Robot can be will be very fruitful for various biometric system using facial technology",Towards real-time visual biometric authentication using human face for healthcare Telepresence Mobile Robots,,"Journal of Telecommunication, Electronic and Computer Engineering, Universiti Teknikal Malaysia Melaka",,,core
304944408,2016-01-01T00:00:00,"Cameras provide a rich source of information while being passive, cheap and lightweight for small and medium Unmanned Aerial Vehicles (UAVs). In this work we present the first implementation of receding horizon control, which is widely used in ground vehicles, with monocular vision as the only sensing mode for autonomous UAV flight in dense clutter. We make it feasible on UAVs via a number of contributions: novel coupling of perception and control via relevant and diverse, multiple interpretations of the scene around the robot, leveraging recent advances in machine learning to showcase anytime budgeted cost-sensitive feature selection, and fast non-linear regression for monocular depth prediction. We empirically demonstrate the efficacy of our novel pipeline via real world experiments of more than 2 kms through dense trees with a quadrotor built from off-the-shelf parts. Moreover our pipeline is designed to combine information from other modalities like stereo and lidar as well if available</p",Vision and Learning for Deliberative Monocular Cluttered Flight,,,10.1184/r1/6561554.v1,,core
302457784,2015-12-18T00:00:00,"This report summarises and integrates two different tracks of research for the purpose of envisioning and preparing a joint research project proposal.

Soft- and hardware systems have become increasingly complex and act ""concurrently"", both with respect to memory access (i.e. information flow) and computational resources (i.e. ""services""). The software development metaphor of cloud-storage, cloud-computing and service-oriented design has been anticipated by artificial intelligence (AI) research at least 30 years ago (parallel and distributed computation already dates back to the 1950’s and 1970s). What is known as a ""service"" today is what in AI is known as the capability of an agent; and the problem of information flow and consistency has been a headstone of information processing ever since. Based on a real-world robotics application we demonstrate how an increasingly abstract description of collaborating or competing agents correspond to a set of concurrent processes.

In the second part we review several approaches to the theory of concurrent systems. Based on the different kinds of program semantics we present corresponding logical and algebraic means for the description of parallel processes and memory access. It turns out that Concurrent Kleene Algebra (CKA) and its related graphlet metaphor appears to deliver a one-to-one matching formal description of the module structures developed in the first part. The problem of snapshotting system states in order to receive (partial) traces of a running system seems to be well describable by a Temporal Logic of Actions (TLA). Finally, the different types of subsystems and their mutual requirements such as exclusiveness etc. seem to be best describable in a separation-logic like approach.

We conclude with a list of research questions detailing some of the many promising issues raised in the report","On the needs for specification and verification of collaborative and concurrent robots, agents and processes",,,,,core
103451302,04/12/2015,"This paper introduces an Intelligent agent for the vacuum cleaner named as VROBO. Objectives of this work are to prepare a pedagogical device for Artificial Intelligence students and to practically implement the Artificial Intelligent Technology in real world problems to enhance the physical capabilities of human being. Most of the significant Intelligent Agent’s attributes like; Goals, Perception, Autonomy and Action, may be found in this agent. Two options are given for the implementation of proposed setup i.e. Screen oriented simulation developed in java and Java API for “real ” robotic simulation using LEGO Mindstorms robots developed by Frank and Scott. Home location is represented by blue color, while yellow color is used to represent dirt. When robot sees dirt it emits beep. This Intelligent Agent is just like a prototype system and conceptual in nature, which can be extended for enhancements to be implemented physically",An Intelligent Agent for a Vacuum Cleaner,,,,,core
103612251,08/01/2016,"Experience has shown that a major obstacle towards real au-tonomy of mobile robots is the occurrence of unexpected faults at runtime. While past research has mainly focused on the hardware, we have developed methods for the local-ization and repair of faulty software components at runtime. However, as it is often not possible to autonomously repair failed components, the deliberative layer of the control sys-tem should be aware of the lost capabilities of the system and adapt its decision-making. In this paper we present an AI-planning system for an autonomous soccer robot. We model the abstract capabilities of the control system, and we show how the planning system infers the available capabilities from the results obtained by the runtime diagnosis. We augment action preconditions with capability requirements, and we propose a method which allows to dynamically determine the sensing capabilities required for monitoring of plans",AI-Planning in a Mobile Autonomous Robot with Degraded Software Capabilities ∗,,,,,core
290011561,2016-02-01T00:00:00,"As a Computer scientist, a computer science students should have understanding about database theory as a concept of data maintenance. Database will be needed in every single human real life computer implementation such as information systems, information technology, internet, games, artificial intelligence, robot and so on.  Inevitably, the right data handling and managament will produce excellent technology implementation. Data warehouse as one of the specialization subject which is offered in computer science study program final semester, provide challenge for computer science students.A survey was conducted on 18 students of early year of computer science study program at Surya university and giving hypothesis that for those students who ever heard of a data warehouse would be interested to learn data warehouse and on other hand, students who had never heard of the data warehouse will not be interested to learn data warehouse. Therefore, it is important that delivery of the Data warehouse subject material should be understood by lecturers, so that students can well understoodwith the data warehouse",PEMAHAMAN TEORI DATA WAREHOUSE BAGI MAHASISWA TAHUN AWAL JENJANG STRATA SATU BIDANG ILMU KOMPUTER,https://core.ac.uk/download/290011561.pdf,'Petra Christian University',10.9744/informatika.13.1.20-24,,core
31021561,2015-01-01T00:00:00,"This paper presents a novel failure-tolerant architecture for future robotic spacecraft. It is based on the Time and Space Partitioning (TSP) principle as well as a combination of Artificial Intelligence (AI) and traditional concepts for system failure detection, isolation and recovery (FDIR). Contrary to classic payload that is separated from the platform, robotic devices attached onto a satellite become an integral part of the spacecraft itself. Hence, the robot needs to be integrated into the overall satellite FDIR concept in order to prevent fatal damage upon hardware or software failure. In addition, complex dexterous manipulators as required for onorbit servicing (OOS) tasks may reach unexpected failure states, where classic FDIR methods reach the edge of their capabilities with respect to successfully detecting and resolving them. Combining, and partly replacing traditional methods with flexible AI approaches aims to yield a control environment that features increased robustness, safety and reliability for space robots. The developed architecture is based on a modular on-board operational framework that features deterministic partition scheduling, an OS abstraction layer and a middleware for standardized inter-component and external communication. The supervisor (SUV) concept is utilized for exception and health management as well as deterministic system control and error management. In addition, a Kohonen self-organizing map (SOM) approach was implemented yielding a real-time robot sensor confidence analysis and failure detection. The SOM features nonsupervized training given a typical set of defined world states. By compiling a set of reviewable three-dimensional maps, alternative strategies in case of a failure can be found, increasing operational robustness. As demonstrator, a satellite simulator was set up featuring a client satellite that is to be captured by a servicing satellite with a 7-DoF dexterous manipulator. The avionics and robot control were - ntegrated on an embedded, space-qualified Airbus e.Cube on-board computer. The experiments showed that the integration of SOM for robot failure detection positively complemented the capabilities of traditional FDIR methods",Utilizing Artificial Intelligence for Achieving a Robust Architecture for Future Robotic Spacecraft,https://core.ac.uk/download/31021561.pdf,,,,core
302396931,2015-12-18T00:00:00,"This report summarises and integrates two different tracks of research for the purpose of envisioning and preparing a joint research project proposal.

Soft- and hardware systems have become increasingly complex and act ""concurrently"", both with respect to memory access (i.e. information flow) and computational resources (i.e. ""services""). The software development metaphor of cloud-storage, cloud-computing and service-oriented design has been anticipated by artificial intelligence (AI) research at least 30 years ago (parallel and distributed computation already dates back to the 1950’s and 1970s). What is known as a ""service"" today is what in AI is known as the capability of an agent; and the problem of information flow and consistency has been a headstone of information processing ever since. Based on a real-world robotics application we demonstrate how an increasingly abstract description of collaborating or competing agents correspond to a set of concurrent processes.

In the second part we review several approaches to the theory of concurrent systems. Based on the different kinds of program semantics we present corresponding logical and algebraic means for the description of parallel processes and memory access. It turns out that Concurrent Kleene Algebra (CKA) and its related graphlet metaphor appears to deliver a one-to-one matching formal description of the module structures developed in the first part. The problem of snapshotting system states in order to receive (partial) traces of a running system seems to be well describable by a Temporal Logic of Actions (TLA). Finally, the different types of subsystems and their mutual requirements such as exclusiveness etc. seem to be best describable in a separation-logic like approach.

We conclude with a list of research questions detailing some of the many promising issues raised in the report","On the needs for specification and verification of collaborative and concurrent robots, agents and processes",,,,,core
296620606,2015-11-26T14:09:38,"Computational Semiotics is a quite new field of research, which emerged from advanced studies on artificial intelligence and intelligent systems. It is an alternative to the mainstream approaches - those based either on cognitivism or on biologically inspired techniques. Instead, Computational Semiotics seeks its inspiration on semiotics, a tradition on the philosophy of mind dealing with the concepts of representation and communication from a more technical perspective. The whole story besides this new field of research is usually unknown from many researchers working on the field of intelligent systems. This story started during the 1970's, where two main precursors first published works relating semiotics to intelligent systems development, almost at the same time in Russia and in the U.S. These pioneering studies were performed by Dmitri Pospelov in Russia and Eugene Pendergraft in U.S. Despite promising research results, for diverse reasons they stayed ignored from the mainstream in Artificial Intelligence for about 25 years. However, around 1995 they were rediscovered, motivating the development of this new paradigm of research. The goal of this paper is to briefly review the main contributions in this area. © 2005 IEEE.2005393398Noth, W., (1995) Handbook of Semiotics (Advances in Semiotics), , Indiana University PressPospelov, D.A., Principles of situational control (1971) Engineering Cybernetics, (2), pp. 216-222. , MoscowPospelov, D.A., (1986) Situational Control: Theory and Practice, , unpublished translation from the original in russian, edited by Nauka Publishers, MoscowPendergraft, E.P., (1964) Self-organizing Linguistic Sytems Linguistic Research Seminar, , Santa Monica, CaliforniaPendergraft, E.P., The future's voice - Intelligence based on pragmatic logic (1993) Technical Report, , Creative Intelligence Incorporated, DRAFT (FATE), Jasper, EUA, JunePospelov, D.A., Zhelezov, Zh.Y., On a class of large systems (1970) Engineering Cybernetics, (2), pp. 243-246. , MoscowPospelov, D.A., Yeimov, Ye.I., Semiotic models in planning problems of artificial intellect systems (1977) Engineering Cybernetics, (5), pp. 37-43. , MoscowOsipov, G.S., Origins of applied semiotics (2000) ECAI 2000, 14th European Conference on Artificial IntelligenceSulosky, M.F., Semiotics situational control - JSM-type reasoning and Q-analysis, intelligent systems: A semiotic perspective (1996) Proceedings of the 1996 International Multidisciplinary Conference, Vol. II: Applied Semiotics, 2. , Gaithersburg, MarylandWaterman, D.A., (1986) A Guide to Expert Systems, , Addison Wesley Publishing Company(1997) White Paper, , A Skeleton Autognome, Autognomics Corp., EUA(2001) Autognomics Technology - Brief Overview of Autognomics, , Autognomics Corp., EUAAlbus, J.S., Outline for a theory of intelligence (1991) IEEE Transactions on System, Man and Cybernetics, 21 (3). , May/JuneAlbus, J.S., Meystel, A.M., (2001) Engineering of Mind: An Introduction to the Science of Intelligent Systems, , John Wiley & SonsISBN: 0471438545Meystel, A.M., Albus, J.S., (2001) Intelligent Systems: Architecture, Design, Control, , Wiley-InterscienceISBN: 0471193747Albus, J.S., The NIST Real-time Control System (RCS): An approach to intelligent system research (1997) J. Expt. Theor. Artif. Intell, pp. 157-162Meystel, A.M., (1995) Semiotic Modeling and Situation Analysis : An Introduction, , AdRem Inc. - Bala Cynwyd, PA, USALiu, K., (2000) Semiotics in Information Systems Engineering, , Cambridge University Press, ISBN: 0521593352Liu, K., Clarke, R.J., Andersen, P.B., Stamper, R.K., (2001) Information, Organisation and Technology: Studies in Organisational Semiotics, , 2001 Kluwer Academic Publishers, ISBN: 0792372581Van Heusden, B., Jorna, R.J., Reconsidering the standard: A semiotic model of organisations Coordination and Communication Using Signs: Studies in Organisational Semiotics, 2, pp. 153-167. , Kecheng Liu, R.J. Clarke, P.B. Andersen & R.K. Stamper (Eds.). Dordrecht: Kluwer Academic PublishersGazendam, H.W.M., Semiotics, virtual organizations, and information systems (2001) Information, Organisation and Technology: Studies in Organisational Semiotics, pp. 1-48. , (2001). Kecheng Liu, Rodney J. Clarke, Peter Bøgh Andersen and Ronald K. Stamper (eds.). Boston: Kluwer Academic PublishersGazendam, H.W.M., Jorna, R.J., Semiotics, multi-agent systems and organizations (1998) Proceedings of the Joint Conference on the Science and Technology of Intelligent Systems, pp. 84-89. , (1998). September 14-17, Gaithersburg, MD, USA. Piscataway, NJ: IEEE. ISBN 0-7803-4423-3Rieger, B.B., Situations, language games, and SCIPS. Modeling semiotic cognitive information processing systems (1995) Architectures for Semiotic Modeling and Situation Analysis in Large Complex Systems (Proceedings of the 1995 ISIC-workshop, Monterey: the 10th International IEEE-symposium on Intelligent Control), pp. 130-138. , Albus, J./ Meystel, A./ Pospelov, D./ Reader, T. (eds.), Bala Cynwyd, PA. (AdRem Inc.)Rieger, B.B., Semiotics and computational linguistics. on semiotic cognitive information processing (1999) Computing with Words in Information/ Intelligent Systems I. Foundations [Studies in Fuzziness and Soft Computing], 33, pp. 93-118. , Zadeh, Lotfi A. / Kacprzyk, Janusz (eds.): Heidelberg, (Physica Verlag)Rieger, B.B., A systems theoretical view on computational semiotics. Modeling text understanding as meaning constitution by SCIPS (1998) Proceedings of the Joint IEEE Conference on the Science and Technology of Intelligent Systems (ISIC/CIRA/ISAS-98), pp. 840-845. , Piscataway, NJ (IEEE/Omnipress)Pattee, H.H., Cell psychology: An evolutionary approach to the symbol-matter problem (1982) Cognition and Brain Theory, 5 (4), pp. 325-341Joslyn, C., The semiotics of control and modeling relations in complex systems (2001) BioSystems, 60, pp. 131-148Joslyn, C., Rocha, L.M., Towards semiotic agent-based models of socio-technical organizations (2000) AI, Simulation and Planning (AIS 2000) Conference, , Tucson, Arizona, USARocha, L.M., Models of embodied, evolving, semiosis in artificial environments (1998) Proceeding of the Virtual World and Simulation Conference, pp. 233-238. , Landauer, C., Bellman, K.L. The society for Computer SimulationPerlovsky, L., Semiotics, mind and architecture of target tracker (1998) Proceedings of the IEEE ISIC/CIRA/ISAS Joint Conference, , Gaithersburg, MD. SepPerlovsky, L., (2000) Neural Networks and Intellect, , Oxford Press - ISBN 0195111621Prueitt, P.S., Is computation something new?, Intelligent systems: A semiotic perspective (1996) Proceedings of the 1996 International Multidisciplinary Conference, Vol. II: Applied Semiotics, 2. , Gaithersburg, MarylandPrueitt, P., (1999) Foundations of Knowledge Management for the 21st Century, , BCN Group Inc., EUADöben-Henisch, G., Semiotic machines - Theory, implementation, semiotic relevance (1996) 8th International Semiotic Congress of the German and the Netherlands Semiotic Societies, , Amsterdam, AugustDöben-Henisch, G., Erasmus, L., Hasebrook, J., Knowledge robots for knowledge workers: Self-learning agents connecting information and skills (2002) Intelligent Agents and Their Applications (Studies in Fuzziness and Soft Computing), 98, pp. 59-79. , L. C. Jain, Zhengxin Chen, Nikhil Ichalkaranje (eds.), Springer, New YorkDöben-Henisch, G., Klöckner, M., Agents with consciousness - Knowbotic-interface project (1995) KIP - Technical Report, , INM - Institut für Neue Medien - Daimlerstr. 32 Frankfurt, Germany - Augus",Towards An Introduction To Computational Semiotics,,'Institute of Electrical and Electronics Engineers (IEEE)',10.1109/KIMAS.2005.1427113,,core
225647572,2016-01-01T00:00:00,"Artificial Intelligence without Using Ontological Data about a Presupposed Reality. This paper introduces an original model to provide software agents and robots with the capacity of learning by interpreting regularities in their stream of sensorimotor experience rather than by exploiting data that would give them ontological information about a predefined domain. Specifically, this model pulls inspiration from : a) the movement of embodied cognition, b) the philosophy of knowledge, c) constructivist epistemology, and d) the theory of enaction. Respectively to these four influences : a) Our agents discover their environment through their body’s active capacity of experimentation. b) They do not know their environment “ as such” but only “ as they can experience it”. c) They construct knowledge from regularities of sensorimotor experience. d) They have some level of constitutive autonomy. Technically, this model differs from the traditional perception / cognition/ action model in that it rests upon atomic sensorimotor experiences rather than separating percepts from actions. We present algorithms that implement this model, and we describe experiments to validate these algorithms. These experiments show that the agents exhibit a certain form of intelligence through their behaviors, as they construct proto-ontological knowledge of the phenomena that appear to them when they observe persistent possibilities of sensorimotor experiences in time and space. These results promote a theory of artificial intelligence without ontological data about a presupposed reality. An application includes a more robust way of creating robots capable of constructing their own knowledge and goals in the real world, which could be initially unknown to them and un-modeled by their designers.Cet article propose un modèle original pour doter des agents informatiques ou des robots de la capacité d’apprendre en interprétant des régularités dans leur flux d’expériences sensorimotrices plutôt qu’en exploitant des données qui leur apporteraient des informations ontologiques sur un domaine prédéfini. Ce modèle s’inspire en particulier de : a) le courant de la cognition incarnée, b) la philosophie de la connaissance, c) l’épistémologie constructiviste, et d) la théorie de l’énaction. Respectivement à ces quatre influences : a) Nos agents découvrent leur environnement à travers les capacités expérimentales actives de leur corps. b) Ils ne connaissent pas leur environnement «en soi » mais uniquement «en ce qu’ils peuvent en faire l’expérience » . c) Ils construisent leurs connaissances à partir de régularités d’expériences sensorimotrices. d) Ils disposent d’une certaine autonomie constitutive.
Techniquement, ce modèle se distingue du modèle perception/cognition/action classique par le fait qu’il considère des expériences sensorimotrices atomiques au lieu de séparer les percepts et les actions. Nous présentons des algorithmes qui implémentent ce modèle, et décrivons des expérimentations permettant de les valider. Les expérimentations montrent que les agents exhibent une certaine forme d’intelligence dans leurs comportements en construisant une connaissance protoontologique des phénomènes qui apparaissent à eux quand ils constatent des possibilités d’expériences sensorimotrices persistantes dans l’espace et le temps. Ces résultats promeuvent une théorie de l’intelligence artificielle sans données ontologiques sur une réalité présupposée, avec, comme perspectives applicatives, des robots capables de construire leurs propres connaissances et objectifs dans le monde réel, initialement inconnu d’eux et non modélisé par leur concepteur.Georgeon Olivier, Mille Alain, Gay Simon. Intelligence artificielle sans données ontologiques sur une réalité présupposée. In: Intellectica. Revue de l'Association pour la Recherche Cognitive, n°65, 2016/1. Nouvelles approches en Robotique Cognitive. pp. 143-168",Intelligence artificielle sans données ontologiques sur une réalité présupposée,,'PERSEE Program',10.3406/intel.2016.1793,,core
203171141,31/12/2016,"AbstractThe effort to develop an electronic skin is highly motivated by many application domains namely robotics, biomedical instrumentations, and replacement prosthetic devices. Several e-skin systems have been proposed recently and have demonstrated the need of an embedded electronic system for tactile data processing either to mimic the human skin or to respond to the application demands. Processing tactile data requires efficient methods to extract meaningful information from raw sensors data.In this framework, our goal is the development of a dedicated embedded electronic system for electronic skin. The embedded electronic system has to acquire the tactile data, process and extract structured information. Machine Learning (ML) represents an effective method for data analysis in many domains: it has recently demonstrated its effectiveness in processing tactile sensors data.This paper presents an embedded electronic system based on dedicated hardware implementation for electronic skin systems. It provides a Tensorial kernel function implementation for machine learning based on Tensorial kernel approach. Results assess the time latency and the hardware complexity for real time functionality. The implementation results highlight the high amount of power consumption needed for the input touch modalities classification task. Conclusions and future perspectives are also presented",Embedded Electronic System Based on Dedicated Hardware DSPs for Electronic Skin Implementation ,,The Author(s). Published by Elsevier Ltd.,10.1016/j.protcy.2016.08.007,,core
74313005,2016-01-01T00:00:00,"Convolutional Neural Networks (CNNs) are a variation of feed-forward Neural Networks inspired by the biological process in the visual cortex of animals. The interest in this supervised learning algorithm has rapidly grown in many fields like image and video recognition and natural language processing. Nowadays they have become the state of the art in various applications like mobile robot vision, video surveillance and Big Data analytics. The specific computation pattern of CNNs results to be highly suitable for hardware acceleration, in fact different types of accelerators have been proposed based on GPU, Field Programmable Gate Array (FPGA) and ASIC. In particular, in the embedded systems context, due to real time and power consumption challenges, it is crucial to find the right tradeoff between performance, energy efficiency, fast development round and cost. This work proposes a framework meant as a tool for the user to accelerate and simplify the design and the implementation of CNNs on FPGAS by leveraging High Level Synthesis, still providing a certain level of customization of the hardware design",Hardware design automation of convolutional neural networks,,'Institute of Electrical and Electronics Engineers (IEEE)',10.1109/ISVLSI.2016.101,,core
104419710,17/08/2016,"The automatic design of controllers for mobile robots usually requires two stages. In the first stage, sensorial data are preprocessed or transformed into high level and meaningful values of variables which are usually defined from expert knowledge. In the second stage, a machine learning technique is applied to obtain a controller that maps these high level variables to the control commands that are actually sent to the robot. This paper describes an algorithm that is able to embed the preprocessing stage into the learning stage in order to get controllers directly starting from sensorial raw data with no expert knowledge involved. Due to the high dimensionality of the sensorial data, this approach uses Quantified Fuzzy Rules (QFRs), that are able to transform low-level input variables into high-level input variables, reducing the dimensionality through summarization. The proposed learning algorithm, called Iterative Quantified Fuzzy Rule Learning (IQFRL), is based on genetic programming. IQFRL is able to learn rules with different structures, and can manage linguistic variables with multiple granularities. The algorithm has been tested with the implementation of the wall-following behavior both in several realistic simulated environments with different complexity and on a Pioneer 3-AT robot in two real environments. Results have been compared with several well-known learning algorithms combined with different data preprocessing techniques, showing that IQFRL exhibits a better and statistically significant performance. Moreover, three real world applications for which IQFRL plays a central role are also presented: path and object tracking with static and moving obstacles avoidance",Learning Fuzzy Controllers in Mobile Robotics with Embedded Preprocessing,,,,,core
43661485,2015-02-02T13:13:38,"The next generation of intelligent robots will need to be able to plan reaches. Not just  ballistic point to point reaches, but reaches around things such as the edge of a table,  a nearby human, or any other known object in the robot’s workspace. Planning  reaches may seem easy to us humans, because we do it so intuitively, but it has  proven to be a challenging problem, which continues to limit the versatility of what  robots can do today. In this document, I propose a novel intrinsically motivated RL  system that draws on both Path/Motion Planning and Reactive Control. Through  Reinforcement Learning, it tightly integrates these two previously disparate  approaches to robotics. The RL system is evaluated on a task, which is as yet  unsolved by roboticists in practice. That is to put the palm of the iCub humanoid robot  on arbitrary target objects in its workspace, start- ing from arbitrary initial  configurations. Such motions can be generated by planning, or searching the  configuration space, but this typically results in some kind of trajectory, which must  then be tracked by a separate controller, and such an approach offers a brit- tle  runtime solution because it is inflexible. Purely reactive systems are robust to many  problems that render a planned trajectory infeasible, but lacking the capacity to search,  they tend to get stuck behind constraints, and therefore do not replace motion  planners. The planner/controller proposed here is novel in that it deliberately plans  reaches without the need to track trajectories. Instead, reaches are composed of  sequences of reactive motion primitives, implemented by my Modular Behavioral  Environment (MoBeE), which provides (fictitious) force control with reactive collision  avoidance by way of a realtime kinematic/geometric model of the robot and its  workspace. Thus, to the best of my knowledge, mine is the first reach planning  approach to simultaneously offer the best of both the Path/Motion Planning and  Reactive Control approaches. By controlling the real, physical robot directly, and  feeling the influence of the con- straints imposed by MoBeE, the proposed system  learns a stochastic model of the iCub’s configuration space. Then, the model is  exploited as a multiple query path planner to find sensible pre-reach poses, from which  to initiate reaching actions. Experiments show that the system can autonomously find  practical reaches to target objects in workspace and offers excellent robustness to  changes in the workspace configuration as well as noise in the robot’s sensory-motor  apparatus",Learning to reach and reaching to learn: a unified approach to path planning and reactive control through reinforcement learning,https://core.ac.uk/download/43661485.pdf,,,,core
346361208,2016-01-01T08:00:00,"While robots are still absent from our homes, they have started to spread over battlefields. However, the military robots of today are mostly remotely controlled platforms, with no real autonomy. This paper will disclose the obstacles in implementing autonomy for such systems by answering a technical question: What level of autonomy is needed in military robots and how and when might it be achieved, followed by a techno-legal one: How to implement the rules of humanitarian law within autonomous fighting robots, in order to allow their legal deployment? The first chapter scrutinizes the significance of autonomy in robots and the metrics used to quantify it, which were developed by the US Department of Defense. The second chapter focuses on the autonomy of  state-of-the-art” robots (e.g.; Google’s self-driving car, DARPA’s projects, etc.) for navigation, ISR or lethal missions. Based on public information, we will get a hint of the architectures, the functioning, the thresholds and technical limitations of such systems. The bottleneck to a higher autonomy of robots seems to be their poor “perceptive intelligence.” The last chapter looks to the requirements of humanitarian law (rules of “jus in bello”/rules of engagement) to the legal deployment of autonomous lethal robots on the battlefields. The legal and moral reasoning of human soldiers, complying with humanitarian law, is a complex cognitive process which must be emulated by autonomous robots that could make lethal decisions. However, autonomous completion of such “moral” tasks by artificial agents is much more challenging than the autonomous implementation of other tasks, such as navigation, ISR or kinetic attacks. Given the limits of current Artificial Intelligence, it is highly unlikely that robots will acquire such moral capabilities anytime soon. Therefore, for the time being, the autonomous weapon systems might be legally deployed, but only in very particular circumstances, where the requirements of humanitarian law happen to be irrelevant","Autonomy of Military Robots: Assessing the Technical and Legal (“Jus In Bello”) Thresholds, 32 J. Marshall J. Info. Tech. & Privacy L. 57 (2016)",,UIC Law Open Access Repository,,,core
42701139,2016-03-16T00:00:00,"Despite the advances made in artificial intelligence, software agents, and
robotics, there is little we see today that we can truly call a fully
autonomous system. We conjecture that the main inhibitor for advancing autonomy
is lack of trust. Trusted autonomy is the scientific and engineering field to
establish the foundations and ground work for developing trusted autonomous
systems (robotics and software agents) that can be used in our daily life, and
can be integrated with humans seamlessly, naturally and efficiently.
  In this paper, we review this literature to reveal opportunities for
researchers and practitioners to work on topics that can create a leap forward
in advancing the field of trusted autonomy. We focus the paper on the `trust'
component as the uniting technology between humans and machines. Our inquiry
into this topic revolves around three sub-topics: (1) reviewing and positioning
the trust modelling literature for the purpose of trusted autonomy; (2)
reviewing a critical subset of sensor technologies that allow a machine to
sense human states; and (3) distilling some critical questions for advancing
the field of trusted autonomy. The inquiry is augmented with conceptual models
that we propose along the way by recompiling and reshaping the literature into
forms that enables trusted autonomous systems to become a reality. The paper
offers a vision for a Trusted Cyborg Swarm, an extension of our previous
Cognitive Cyber Symbiosis concept, whereby humans and machines meld together in
a harmonious, seamless, and coordinated manner","A Review of Theoretical and Practical Challenges of Trusted Autonomy in
  Big Data",http://arxiv.org/abs/1604.00921,'Institute of Electrical and Electronics Engineers (IEEE)',10.1109/ACCESS.2016.2571058,,core
102390751,23/08/2015,"Abstract—This study attempts to make a compact humanoid robot acquire a giant-swing motion without any robotic models by using reinforcement learning; only the interaction with environment is available. Generally, it is widely said that this type of learning method is not appropriated to obtain dynamic motions because Markov property is not necessarily guaranteed during the dynamic task. However, in this study, we try to avoid this problem by embedding the dynamic information in the robotic state space; the applicability of the proposed method is considered using both the real robot and dynamic simulator. This paper, in particular, discusses how the robot with 5-DOF, in which the Q-Learning algorithm is implemented, acquires a giant-swing motion. Further, we describe the reward effects on the Q-Learning. Finally, this paper demonstrates that the application of the Q-Learning enable the robot to perform a very attractive giant-swing motion. I",,,,,,core
103828497,20/01/2016,"This paper presents an educational experience developed in the fourth year of Computer Science degree at Huelva University (Spain). Tomake Artificial Intelligent (AI) learning processes more captivating, a new educational project was incorporated into classical teaching ofArtificial Intelligence andKnowledge Engineering subject. In this paper, we present the experience fulfilled with a group of college students. Here it is related how they changed for some days their classroom lessons for the robotic competition arena. With this project we have extended regular classroom lessons with additional work that could be useful and cannot be provided by traditional practical lessons, the real life experience.As a real example about how theworkwas accomplishedwe describe themechanical construction of themobile robots aswell as the software development process",From Classroom to Mobile Robots Competition Arena: An Experience on Artificial Intelligence Teaching*,,,,,core
104343683,17/08/2016,"Abstract — Many people suffer from the loss of a limb. Learning to get by without an arm or hand can be very challenging, and existing prostheses do not yet fulfil the needs of individuals with amputations. One promising solution is to provide greater communication between a prosthesis and its user. Towards this end, we present a simple machine learning interface to supplement the control of a robotic limb with feedback to the user about what the limb will be experiencing in the near future. A real-time prediction learner was implemented to predict impact-related electrical load experienced by a robot limb; the learning system’s predictions were then communicated to the device’s user to aid in their interactions with a workspace. We tested this system with five able-bodied subjects. Each subject manipulated the robot arm while receiving different forms of vibrotactile feedback regarding the arm’s contact with its workspace. Our trials showed that communicable predictions could be learned quickly during human control of the robot arm. Using these predictions as a basis for feedback led to a statistically significant improvement in task performance when compared to purely reactive feedback from the device. Our study therefore contributes initial evidence that prediction learning and machine intelligence can benefit not just control, but also feedback from an artificial limb. We expect that a greater level of acceptance and ownership can be achieved if the prosthesis itself takes an active role in transmitting learned knowledge about its state and its situation of use. I","Using Learned Predictions as Feedback to Improve Control and Communication with an Artificial Limb: Preliminary Findings,” arXiv:1408.1913 [cs.AI",,,,,core
104398759,17/08/2016,"Abstract — Cameras provide a rich source of information while being passive, cheap and lightweight for small and medium Unmanned Aerial Vehicles (UAVs). In this work we present the first implementation of receding horizon control, which is widely used in ground vehicles, with monocular vision as the only sensing mode for autonomous UAV flight in dense clutter. We make it feasible on UAVs via a number of contributions: novel coupling of perception and control via rel-evant and diverse, multiple interpretations of the scene around the robot, leveraging recent advances in machine learning to showcase anytime budgeted cost-sensitive feature selection, and fast non-linear regression for monocular depth prediction. We empirically demonstrate the efficacy of our novel pipeline via real world experiments of more than 2 kms through dense trees with a quadrotor built from off-the-shelf parts. Moreover our pipeline is designed to combine information from other modalities like stereo and lidar as well if available. I",Vision and Learning for Deliberative Monocular Cluttered Flight,,,10.1007/978-3-319-27702-8_26,,core
105823348,09/09/2016,"Hybrid systems are important in applications in CAD, real-time software, robotics and automation, mechatronics, aero-nautics, air and ground transportation systems, process control, and have recently been at the center of intense research activity in the control theory, computer-aided verification, and artificial intelligence communities. In the past several years, methodologies have been developed to model hybrid systems, to analyze their behavior, and to synthesize con-trollers that guarantee closed-loop safety and performance specifications. These advances have been complemented by computational tools for the automatic verification and simulation of hybrid systems. Modern technologies of computer simulation tools include preparing, debugging, analysis and calculation of effective program models, meaningful inter-pretation of research results",Specification and Analysis of Discrete Behavior of Hybrid Systems in the Workbench ISMA,,,,,core
297280421,2015-01-01T00:00:00,"In this thesis, we introduce a novel architecture called  Intelligent Architecture for Legged Robot Terrain Classification Using Proprioceptive and Exteroceptive Data (iARTEC ) . The proposed architecture integrates different terrain characterization and classification with other robotic system components. Within iARTEC , we consider the problem of having a legged robot autonomously learn to identify different terrains. Robust terrain identification can be used to enhance the capabilities of legged robot systems, both in terms of locomotion and navigation. For example, a robot that has learned to differentiate sand from gravel can autonomously modify (or even select a different) path in favor of traversing over a better terrain. The same knowledge of the terrain type can also be used to guide a robot in order to avoid specific terrains. To tackle this problem, we developed four approaches for terrain characterization, classification, path planning, and control for a mobile legged robot. We developed a particle system inspired approach to estimate the robot footâ ground contact interaction forces. The approach is derived from the well known Bekkerâ s theory to estimate the contact forces based on its point contact model concepts. It is realistically model real-time 3-dimensional contact behaviors between rigid body objects and the soil. For a real-time capable implementation of this approach, its reformulated to use a lookup table generated from simple contact experiments of the robot foot with the terrain. Also, we introduced a short-range terrain classifier using the robot embodied data. The classifier is based on a supervised machine learning approach to optimize the classifier parameters and terrain it using proprioceptive sensor measurements. The learning framework preprocesses sensor data through channel reduction and filtering such that the classifier is trained on the feature vectors that are closely associated with terrain class. For the long-range terrain type prediction using the robot exteroceptive data, we present an online visual terrain classification system. It uses only a monocular camera with a feature-based terrain classification algorithm which is robust to changes in illumination and view points. For this algorithm, we extract local features of terrains using Speed Up Robust Feature (SURF). We encode the features using the Bag of Words (BoW) technique, and then classify the words using Support Vector Machines (SVMs). In addition, we described a terrain dependent navigation and path planning approach that is based on E* planer and employs a proposed metric that specifies the navigation costs associated terrain types. This generated path naturally avoids obstacles and favors terrains with lower values of the metric. At the low level, a proportional input-scaling controller is designed and implemented to autonomously steer the robot to follow the desired path in a stable manner. iARTEC performance was tested and validated experimentally using several different sensing modalities (proprioceptive and exteroceptive) and on the six legged robotic platform CREX. The results show that the proposed architecture integrating the aforementioned approaches with the robotic system allowed the robot to learn both robot-terrain interaction and remote terrain perception models, as well as the relations linking those models. This learning mechanism is performed according to the robot own embodied data. Based on the knowledge available, the approach makes use of the detected remote terrain classes to predict the most probable navigation behavior. With the assigned metric, the performance of the robot on a given terrain is predicted. This allows the navigation of the robot to be influenced by the learned models. Finally, we believe that iARTEC and the methods proposed in this thesis can likely also be implemented on other robot types (such as wheeled robots), although we did not test this option in our work",An Intelligent Architecture for Legged Robot Terrain Classification Using Proprioceptive and Exteroceptive Data,https://core.ac.uk/download/pdf/297280421.pdf,,,,core
229268022,2016-12-01T00:00:00,"Telepresence Mobile Robots have prominent attributes in many fields as it provides virtual presence for human robot interaction. The deployment of this robot in healthcare sector has improved patient care and health. The vision system in a telepresence robot allows two way audiovisual communication between people at different location. In spite of such advancement, the manual way of controlling a robot to recognise and track people during an emergency is not favourable for a long duration. To circumvent this problem, biometric method using human face is proposed in this research which is implemented on Medical Telediagnosis Robot. This paper details the design of the face recognition and tracking system with four automated modules which are motion detection, face detection, face recognition and face tracking. The modules are developed with different algorithm and tested individually to ensure the stability of the system. Artificial Intelligence technique was applied at the face recognition stage while a two degree of freedom mechanism for actuator control was used at face tracking stage. A sequential mode operation is proposed to reduce the execution time in a real-time environment. To achieve this, only one module is operated at each time. A Graphical User Interface was developed to ease the users at the local and robot environment. The system is designed in LabVIEW platform. The biometric system proposed with hybrid algorithm at each module adapts for face images detected at different distances, poses and lighting condition. This system was tested in real-time and has an execution time of 55ms and 98% accuracy. The stand alone system designed for Medical Telediagnosis Robot can be will be very fruitful for various biometric system using facial technology",Towards Real-Time Visual Biometric Authentication Using Human Face for Healthcare Telepresence Mobile Robots,https://core.ac.uk/download/229268022.pdf,"Journal of Telecommunication, Electronic and Computer Engineering (JTEC)",,,core
78921839,11/03/2011,"In current literature, the definition of a cognitive radio (CR) seems to be different from one research community
to another: For communication theorists it is about dynamic spectrum sharing (DSS), for hardware/RF engineers
it is an upgrade of Software-defined Radio (SDR), for networking/IT researchers CR is a device capable of crosslayer
optimizations and for computer scientists it is a device capable of machine learning. We put-forth a new
unified vision for a future CR by defining it to be a radio that is capable of self-managing and self-reconfiguring
in real-time to match its RF environment while continuously self-learning from its past experience. To avoid
confusion, we call our futuristic radios as Radiobots: They are the radio devices equivalence of robots in mechanical
engineering or Autobots in sci-fi movies. While every Radiobot will necessarily be a CR, none of the CRs
found in current literature can be considered as a Radiobot. We emphasize that our proposed Radiobots are not
aimed just at achieving DSS. Indeed, they are expected to have all of the following capabilities: (1.) autonomous
operation (2.) spectrum coexistence/efficiency including DSS, (3.) inter-operability in heterogeneous RF network
environments (4.) multi-mode operability (i.e. simultaneous operation over multiple modes/networks), and (5.)
power efficient green communications. The Radiobots are supposed to autonomously find and join/avoid any
radio networks in their vicinity to achieve their performance objectives. In this technical report we present a
system level architecture of a future Radiobot device, cognitive algorithms critical for its operation and the need
for real-time reconfigurable hardware and antennas","Radiobots: Architecture, Algorithms and Realtime Reconfigurable Antenna Designs for Autonomous, Self-learning Future Cognitive Radios",,,,,core
100119937,2011,"The primary focus of this project is the creation of an experimental platform for controlling avatars in a game environment using spiking neural networks as well as the development of suitable networks that make these avatars exhibit human-like behaviour. The platform that has been used is the computer game Unreal Tournament 2004 (UT2004), which provides an efficient environment for comparing human and embodied AI behaviour without the cost and difficulty of real humanoid robots. The first stage was the development of a wrapper for UT2004 that transforms this video game into a artificial intelligent (AI) agent simulator, that can be used to test AI techniques, such as spiking neural networks, to control avatars within its environment. The second stage was the creation and advancement of a system that uses an architecture based on global workspace the-ory and spiking neurons and the evaluation of this system using the developed software. This biologically-inspired approach is the first neural implementation of a global workspace architecture that is embodied in a dynamic real time environment. The conclusion to the project was the participation in the 2K BotPrize human-like bot com",Spiking Neural Networks for Human-like Avatar Control in a Simulated Environment,,,,,core
82974290,2012-01-01T00:00:00,"The field of behaviour-based artificial intelligence (AI), with its roots in the robotics research of Rodney Brooks, is not predominantly tied to linguistic interaction in the sense of the classic Turing test (or, ""imitation game""). Yet, it is worth noting, both are centred on a behavioural model of intelligence. Similarly, there is no intrinsic connection between musical AI and the language-based Turing test, though there have been many attempts to forge connections between them. Nonetheless, there are aspects of musical AI and the Turing test that can be considered in the context of non-language-based interactive environments–-in particular, when dealing with real-time musical AI, especially interactive improvisation software. This paper draws out the threads of intentional agency and human indistinguishability from Turing’s original 1950 characterisation of AI. On the basis of this distinction, it considers different approaches to musical AI. In doing so, it highlights possibilities for non-hierarchical interplay between human and computer agents","Interactive intelligence: behaviour-based AI, musical HCI and the Turing Test",https://core.ac.uk/download/82974290.pdf,,,,core
158741163,2013,"The need to understand and model human-like behavior and intelligence has been embraced by a multidisciplinary community for several decades. The success so far has been shown in solutions for a concrete task or a competence, and these solutions are seldom a truly multidisciplinary effort. In this paper we analyze the needs and the opportunities for combining artificial intelligence and bio-inspired computation within an application domain that provides a cluster of solutions instead of searching for a solution to a single task. We analyze applications of training children with autism spectrum disorder (ASD) with a humanoid robot, because it must include multidisciplinary effort and at the same time there is a clear need for better models of human-like behavior which will be tested in real life scenarios through these robots. We designed, implemented, and carried out three applied behavior analysis (ABA) based robot interventions. All interventions aim to promote self initiated social behavior in children with ASD. We found out that the standardization of the robot training scenarios and using unified robot platforms can be an enabler for integrating multiple intelligent and bio-inspired algorithms for creation of tailored, but domain specific robot skills and competencies. This approach might set a new trend to how artificial and bio-inspired robot applications develop. We suggest that social computing techniques are a pragmatic solution to creation of standardized training scenarios and therefore enable the replacement of perceivably intelligent robot behaviors with truly intelligent ones",Interplay between natural and artificial intelligence in training autistic children with robots,,Springer,10.1007/978-3-642-38637-4_17,,core
200027491,2011-06-27T00:00:00,"We present results and research projects about the computational aspects of classical problems in Artificial Intelligence. We are interested in the setting of agents able to describe their environment through a possibly huge number of Boolean descriptors, and to act upon this environment. The typical applications of this kind of studies are to the design of autonomous robots (for exploring unknown zones, for instance) or of software assistants (for scheduling, for instance). The ultimate goal of research in this domain is the design of agents able to learn autonomously, by learning and interacting with their environment (including human users), also able to reason for producing new pieces of knowledge, for explaining observed phenomena, and finally, able to decide on which action to take at any moment, in a rational fashion. Ideally, such agents will be fast, efficient as soon as they start to interact with their environment, they will improve their behavior as time goes by, and they will be able to communicate naturally with humans. Among the numerous research questions raised by these objectives, we are especially interested in concept and preference learning, in reinforcement learning, in planning, and in some underlying problems in complexity theory. A particular attention is paid to interaction with humans and to huge numbers of descriptors of the environment, as are necessary in real-world applications","Computational Aspects of Learning, Reasoning, and Deciding",,HAL CCSD,,,core
250416067,2013-01-01T00:00:00,"\u3cp\u3eThe need to understand and model human-like behavior and intelligence has been embraced by a multidisciplinary community for several decades. The success so far has been shown in solutions for a concrete task or a competence, and these solutions are seldom a truly multidisciplinary effort. In this paper we analyze the needs and the opportunities for combining artificial intelligence and bio-inspired computation within an application domain that provides a cluster of solutions instead of searching for a solution to a single task. We analyze applications of training children with autism spectrum disorder (ASD) with a humanoid robot, because it must include multidisciplinary effort and at the same time there is a clear need for better models of human-like behavior which will be tested in real life scenarios through these robots. We designed, implemented, and carried out three applied behavior analysis (ABA) based robot interventions. All interventions aim to promote self initiated social behavior in children with ASD. We found out that the standardization of the robot training scenarios and using unified robot platforms can be an enabler for integrating multiple intelligent and bio-inspired algorithms for creation of tailored, but domain specific robot skills and competencies. This approach might set a new trend to how artificial and bio-inspired robot applications develop. We suggest that social computing techniques are a pragmatic solution to creation of standardized training scenarios and therefore enable the replacement of perceivably intelligent robot behaviors with truly intelligent ones.\u3c/p\u3",Interplay between natural and artificial intelligence in training autistic children with robots,,'Springer Fachmedien Wiesbaden GmbH',,,core
160335033,2012-09-04T00:00:00Z,"Advanced autonomous robotics space missions rely heavily on the flawless interaction of complex hardware, multiple sensors, and a mission-critical software system.  This software system consists of an operating system, device drivers, controllers, and executives; recently highly complex AI-based autonomy software have also been introduced. Prior to launch, this software has to undergo rigorous verification and validation (V&V).  Nevertheless, dormant software bugs, failing sensors, unexpected hardware-software interactions, and unanticipated environmental conditions—likely on a space exploration mission—can cause major software faults that can endanger the entire mission.

Our Integrated Software Health Management (ISWHM) system continuously monitors the hardware sensors and the software in real-time. The ISWHM uses Bayesian networks, compiled to arithmetic circuits, to model software and hardware interactions. Advanced reasoning algorithms using arithmetic circuits not only enable the ISWHM to handle large, hierarchical models that are necessary in the realm of complex autonomous systems, but also enable efficient execution on small embedded processors. The latter capability is of extreme importance for small (mobile) autonomous units with limited computational power and low telemetry bandwidth.  In this paper, we discuss the requirements of ISWHM.  As our initial demonstration platform, we use a primitive Lego rover. A Lego 
Mindstorms microcontroller is used to implement a highly simplified autonomous rover driving system, running on the OSEK real-time operating system. We demonstrate that our ISWHM, running on this small embedded microcontroller, can perform fault detection as well as on-board reasoning for advanced diagnosis and root-cause detection in real time",Software and System Health Management for Autonomous Robotics Missions,,,10.1184/r1/6710654.v1,,core
22963311,20/08/2013,"Abstract. Teleoperation of remote robotic systems over time delays in the range of 2-10 seconds poses a unique set of challenges. In the context of a supervisory control system for the JSC Robonaut humanoid robot, we have developed an “intelligent assistant”, called the Task Level Assistant (TLA), that integrates an AI planner (JSHOP2) with execution monitoring of the state of both the human supervisor and the remote robot. The TLA reasons simultaneously about the world state on both sides of the time delay, which represents a novel application of this technology. The purpose of the assistant is to provide near real-time advice to the human supervisor about current and future activities, derived from a sequence of high-level goals to be achieved. To do this, the assistant must simultaneously monitor and react to various data sources, including actions taken by the supervisor who is issuing commands to the robot (e.g. with a data glove), actions taken by the robot as reported over the time delay, the environment of the robot, as currently perceived over the time delay, and the current sequence of goals. We have developed a “leader/follower ” software architecture to handle the dual time-shifted streams of feedback from executing tasks. In this paper we describe the integrated planner and its executive, and how it operates in normal and anomaly situations. 1",Integrating AI Planning for Telepresence with Time Delays,,,,,core
21357738,01/01/2011,"Online model learning in real-time is required by many applications such as in robot tracking control. It poses a difficult problem, as fast and incremental online regression with large data sets is the essential component which cannot be achieved by straightforward usage of off-the-shelf machine learning methods (such as Gaussian process regression or support vector regression). In this paper, we propose a framework for online, incremental sparsification with a fixed budget designed for large scale real-time model learning. The proposed approach combines a sparsification method based on an independence measure with a large scale database. In combination with an incremental learning approach such as sequential support vector regression, we obtain a regression method which is applicable in real-time online learning. It exhibits competitive learning accuracy when compared with standard regression techniques. Implementation on a real robot emphasizes the applicability of the proposed approach in real-time online model learning for real world systems. ",Incremental Sparsification for Real-time Online Model Learning,,,,,core
21672092,2011,"Since the emergence of the field of Evolutionary Robotics, new breakthroughs have been made to further its development and prove its effectiveness. While from a purely engineering standpoint the modeling of biological evolutionary phenomena may not seem to be the most efficient means of implementation, it has proven to be able to provide solutions to interesting problems within the robotics world. The approach of evolving autonomous controllers for robots has had proven benefits when compared to a more traditional hand-coded approach.\ud
\ud
Like most fields of research, Evolutionary Robotics contains its own set of problems. One such problem involves the use of simulators to speed up the evolutionary processes. When transferring the robotic controller from the simulation to the physical robot it tends to perform poorly on a given task. This issue is referred to as the reality gap problem. In this paper, a new approach to bridging the reality gap is presented and explored. The goal is to evolve a robotic controller that gains correct desires based on its current state and uses reinforcement learning to perform actions that achieve such desires. By doing so, the goal is to have a robotic controller adapt to uncertainties and perturbations within the real world once transferred from simulation",Evolving Robotic Desires: A New Approach to Bridging the Reality Gap,,,,,core
323895706,2013-09-09T00:00:00,"This study builds on the previous work of urine utilisation and uses small-scale microbial fuel cells (MFCs), working both as individual units in cascade or collectively as a stack, to utilise artificial urine. Artificial urine was prepared at concentrations typically found in real human urine with peptone employed as a surrogate proteinacious component. MFCs were constructed from Nanocure® polymer using rapid prototype technology. The anode and cathode electrodes were made of 15 cm2 carbon veil, folded down to fit in the 1 mL chambers. Eight MFCs were inoculated using activated anaerobic sludge; after 17 days of fed batch mode they were switched to continuous flow, initially at 0.09 mL/h and subsequently at 0.43 mL/h, resulting in HRT of 12.69 minutes/MFC. MFCs showed stable performance following the maturing period and produced, under polarisation experiments, peak power levels of 117 μW, corresponding to 962.94 W/m3. Continuous flow experiments data showed higher power production, increasing with the concentration of the carbon/energy source within artificial urine. The work demonstrates that artificial urine of varying composition can be successfully utilised for the production of energy and concomitant cleanup of organic waste. Finally, in line with the practical implementation and robotics work in our group, the small-scale MFCs were configured into a stack and directly energised electronic devices. © IWA Publishing 2013",Energy production and sanitation improvement using microbial fuel cells,,'IWA Publishing',10.2166/washdev.2013.117,,core
26654623,2013-01-01T00:00:00Z,"This paper presents an intelligent motion controller for four-wheeled holonomic mobile robots with four driving omnidirectional wheels equally spaced at 90 degrees from one another by using field-programmable gate array (FPGA)-based artificial immune system (AIS) algorithm. Both the nature-inspired AIS computational approach and motion controller are implemented in one FPGA chip to address the optimal control problem of real-world mobile robotics application. The proposed FPGA-based AIS method takes the advantages of artificial intelligence and FPGA technology by using system-on-a-programmable chip (SoPC) methodology. Experimental results are conducted to show the effectiveness and merit of the proposed FPGA-based AIS intelligent motion controller for four-wheeled omnidirectional mobile robots. This FPGA-based AIS autotuning intelligent controller outperforms the conventional nonoptimal controllers, the genetic algorithm (GA) controller, and the particle swarm optimization (PSO) controller",Intelligent Motion Control for Four-Wheeled Holonomic Mobile Robots Using FPGA-Based Artificial Immune System Algorithm,,SAGE Publishing,10.1155/2013/589510,"[{'title': None, 'identifiers': ['issn:1687-8132', '1687-8132']}]",core
21710956,15/02/2012,"The revolution in digital technology has fueled a need for design techniques that can guarantee safety and performance specifications of embedded systems, or systems that couple discrete logic with the analog physical environment. Hybrid systems are dynamical systems with interacting continuous-time dynamics (modeled by differential equations) and discrete-event dynamics (modeled by automata). They are important in applications in CAD, real-time software, robotics and automation, mechatronics, aeronautics, air and ground transportation systems, process control, and have recently been at the center of intense research activity in the control theory, computer-aided verification, and artificial intelligence communities. In the past several years, methodologies have been developed to model hybrid systems, to analyze their behavior, and to synthesize controllers that guarantee closed-loop safety and performance specifications. These advances have been complemented by computational tools for the automatic verification and simulation of hybrid systems. This course will present the recent advances in modeling, analysis, control, and verification of hybrid systems. Topics will include: • continuous-time and discrete-event models • safety specifications and model checking • optimal control theory and differential games • Lyapunov stability analysis and verification tools 1 • numerical simulation • a range of engineering example",Lecture Information,,,,,core
23350799,21/10/2013,"Mobile robots deployed in real-world domains frequently find it difficult to process all sensor inputs, or to operate without human input and domain knowledge. At the same time, complex domains make it difficult to provide robots all relevant domain knowledge in advance, and humans are unlikely to have the time and expertise to provide elaborate and accurate feedback. This paper presents an integrated framework that creates novel opportunities for addressing these learning, adaptation and collaboration challenges associated with human-robot collaboration. The framework consists of hierarchical planning, bootstrap learning and online reinforcement learning algorithms that inform and guide each other. As a result, robots are able to make best use of sensor inputs, soliciting high-level feedback from non-expert humans when such feedback is necessary and available. All algorithms are evaluated in simulation and on wheeled robots in dynamic indoor domains. ",Integrating Visual Learning and Hierarchical Planning for Autonomy in Human-Robot Collaboration,,,,,core
49843221,2013-04-29T00:00:00,"International audienceThis paper studies the impact of interfaces allowing non-expert users to efficiently and intuitively teach a robot to recognize new visual objects. We present challenges that need to be addressed for real-world deployment of robots capable of learning new visual ¡objects in interaction with everyday users. We argue that in addition to robust machine learning and computer vision methods, well-designed interfaces are crucial for learning efficiency. In particular, we argue that interfaces can be key in helping non-expert users to collect good learning examples and thus improve the performance of the overall learning system. Then, we present four alternative human-robot interfaces: three are based on the use of a mediating artifact (smartphone, wiimote, wiimote and laser), and one is based on natural human gestures (with a Wizard-of-Oz recognition system). These interfaces mainly vary in the kind of feedback provided to the user, allowing him to understand more or less easily what the robot is perceiving, and thus guide his way of providing training examples differently. We then evaluate the impact of these interfaces, in terms of learning efficiency, usability and user's experience, through a real world and large scale user study. In this experiment, we asked participants to teach a robot twelve different new visual objects in the context of a robotic game. This game happens in a home-like environment and was designed to motivate and engage users in an interaction where using the system was meaningful. We then discuss results that show significant differences among interfaces. In particular, we show that interfaces such as the smartphone interface allows non-expert users to intuitively provide much better training examples to the robot, almost as good as expert users who are trained for this task and aware of the different visual perception and machine learning issues. We also show that artifact-mediated teaching is significantly more efficient for robot learning, and equally good in terms of usability and user's experience, than teaching thanks to a gesture-based human-like interaction",The Impact of Human-Robot Interfaces on the Learning of Visual Objects,https://core.ac.uk/download/49843221.pdf,'Institute of Electrical and Electronics Engineers (IEEE)',10.1109/TRO.2012.2228134,,core
8823171,2012-07-11T00:00:00,"Probabilistic inference problems arise naturally in distributed systems such
as sensor networks and teams of mobile robots. Inference algorithms that use
message passing are a natural fit for distributed systems, but they must be
robust to the failure situations that arise in real-world settings, such as
unreliable communication and node failures. Unfortunately, the popular
sum-product algorithm can yield very poor estimates in these settings because
the nodes' beliefs before convergence can be arbitrarily different from the
correct posteriors. In this paper, we present a new message passing algorithm
for probabilistic inference which provides several crucial guarantees that the
standard sum-product algorithm does not. Not only does it converge to the
correct posteriors, but it is also guaranteed to yield a principled
approximation at any point before convergence. In addition, the computational
complexity of the message passing updates depends only upon the model, and is
dependent of the network topology of the distributed system. We demonstrate the
approach with detailed experimental results on a distributed sensor calibration
task using data from an actual sensor network deployment.Comment: Appears in Proceedings of the Twentieth Conference on Uncertainty in
  Artificial Intelligence (UAI2004",Robust Probabilistic Inference in Distributed Systems,http://arxiv.org/abs/1207.4174,,,,core
210917101,2011-05-01T00:00:00,"For many applications such as compliant, accurate robot tracking control, dynamics models learned from data can help to achieve both compliant control performance as well as high tracking quality. Online learning of these dynamics models allows the robot controller to adapt itself to changes in the dynamics (e.g., due to time-variant nonlinearities or unforeseen loads). However, online learning in real-time applications -- as required in control -- cannot be realized by straightforward usage of off-the-shelf machine learning methods such as Gaussian process regression or support vector regression. In this paper, we propose a framework for online, incremental sparsification with a fixed budget designed for fast real-time model learning. The proposed approach employs a sparsification method based on an independence measure. In combination with an incremental learning approach such as incremental Gaussian process regression, we obtain a model approximation method which is applicable in real-time online learning. It exhibits competitive learning accuracy when compared with standard regression techniques. Implementation on a real Barrett WAM robot demonstrates the applicability of the approach in real-time online model learning for real world systems",Incremental online sparsification for model learning in real-time robot control,,'Elsevier BV',10.1016/j.neucom.2010.06.033,,core
293709756,2013-07-01T00:00:00,"Most tasks that humans need to accomplished in their everyday life require certain motor skills. Although most motor skills seem to rely on the same elementary movements, humans are able to accomplish
many different tasks. Robots, on the other hand, are still limited to a small number of skills and depend on well-defined environments. Modeling new motor behaviors is therefore an important research area
in robotics. Computational models of human motor control are an essential step to construct robotic systems that are able to solve complex tasks in a human inhabited environment. These models can be
the key for robust, efficient, and human-like movement plans. In turn, the reproduction of human-like behavior on a robotic system can be also beneficial for computational neuroscientists to verify their
hypotheses. Although biomimetic models can be of great help in order to close the gap between human and robot motor abilities, these models are usually limited to the scenarios considered. However, one
important property of human motor behavior is the ability to adapt skills to new situations and to learn new motor skills with relatively few trials. Domain-appropriate machine learning techniques, such as supervised and reinforcement learning, have a great potential to enable robotic systems to autonomously
learn motor skills. In this thesis, we attempt to model and subsequently learn a complex motor task. As a test case
for a complex motor task, we chose robot table tennis throughout this thesis. Table tennis requires a series of time critical movements which have to be selected and adapted according to environmental
stimuli as well as the desired targets. We first analyze how humans play table tennis and create a computational model that results in human-like hitting motions on a robot arm. Our focus lies on
generating motor behavior capable of adapting to variations and uncertainties in the environmental conditions. We evaluate the resulting biomimetic model both in a physically realistic simulation and on a real anthropomorphic seven degrees of freedom Barrett WAM robot arm. This biomimetic model based purely on analytical methods produces successful hitting motions, but does not feature the flexibility found in human motor behavior. We therefore suggest a new framework that allows a robot to learn cooperative table tennis from and with a human. Here, the robot first learns a set of elementary hitting movements from a human teacher by kinesthetic teach-in, which is compiled into a set of motor primitives. To generalize these movements to a wider range of situations we introduce the mixture of motor primitives algorithm. The resulting motor policy enables the robot to select appropriate motor primitives as well as to generalize between them. Furthermore, it also allows to adapt the selection process of the hitting movements based on the outcome of previous trials. The framework is evaluated both in simulation and on a real Barrett WAM robot. In consecutive experiments, we show that our approach allows the robot to return balls from a ball launcher and furthermore to play table tennis with a human partner.
Executing robot movements using a biomimetic or learned approach enables the robot to return balls successfully. However, in motor tasks with a competitive goal such as table tennis, the robot not
only needs to return the balls successfully in order to accomplish the task, it also needs an adaptive strategy. Such a higher-level strategy cannot be programed manually as it depends on the opponent and the abilities of the robot. We therefore make a first step towards the goal of acquiring such a strategy and investigate the possibility of inferring strategic information from observing humans playing table tennis. We model table tennis as a Markov decision problem, where the reward function captures the goal of the task as well as knowledge on effective elements of a basic strategy. We show how this reward function, and therefore the strategic information can be discovered with model-free inverse reinforcement learning from human table tennis matches. The approach is evaluated on data collected from players with different playing styles and skill levels. We show that the resulting reward functions are able to capture expert-specific strategic information that allow to distinguish the expert among players with different playing skills as well as different playing styles. To summarize, in this thesis, we have derived a computational model for table tennis that was
successfully implemented on a Barrett WAM robot arm and that has proven to produce human-like hitting motions. We also introduced a framework for learning a complex motor task based on a library
of demonstrated hitting primitives. To select and generalize these hitting movements we developed the mixture of motor primitives algorithm where the selection process can be adapted online based
on the success of the synthesized hitting movements. The setup was tested on a real robot, which showed that the resulting robot table tennis player is able to play a cooperative game against an human
opponent. Finally, we could show that it is possible to infer basic strategic information in table tennis from observing matches of human players using model-free inverse reinforcement learning",Modeling and Learning of Complex Motor Tasks: A Case Study with Robot Table Tennis,,,,,core
16497915,2011-06-10T00:00:00,"Fast software performance is often the focus when developing real-time vision-based control applications for robot simulators. In this paper we have developed a thin, high performance middleware for USARSim and other simulators designed for real-time vision-based control applications. It includes a fast image server providing images in OpenCV, Matlab or web formats and a simple command/sensor processor. The interface has been tested in USARSim with an Unmanned Aerial Vehicle using two control applications; landing using a reinforcement learning algorithm and altitude control using elementary motion detection. The middleware has been found to be fast enough to control the flying robot as well as very easy to set up and use",AltURI: a thin middleware for simulated robot vision applications,https://core.ac.uk/download/16497915.pdf,'Institute of Electrical and Electronics Engineers (IEEE)',,,core
24936789,2013-05-31T00:00:00,"In mobile robotics, a solid test for adaptation is the ability of a control
system to function not only in a diverse number of physical environments, but
also on a number of different robotic platforms. This paper demonstrates that a
set of behaviours evolved in simulation on a miniature robot (epuck) can be
transferred to a much larger-scale platform (Pioneer), both in simulation and
in the real world. The chosen architecture uses artificial evolution of epuck
behaviours to obtain a genetic sequence, which is then employed to seed an
idiotypic, artificial immune system (AIS) on the Pioneers. Despite numerous
hardware and software differences between the platforms, navigation and
target-finding experiments show that the evolved behaviours transfer very well
to the larger robot when the idiotypic AIS technique is used. In contrast,
transferability is poor when reinforcement learning alone is used, which
validates the adaptability of the chosen architecture.Comment: Evolutionary Intelligence 3 (3), 123-136, 2010. arXiv admin note:
  text overlap with arXiv:1007.037","Real-world Transfer of Evolved Artificial Immune System Behaviours
  between Small and Large Scale Robotic Platforms",http://arxiv.org/abs/1305.7432,,,,core
147977176,2011-12-16T12:31:33,"This study attempts to make a compact humanoid robot acquire a giant-swing motion without any robotic models by using reinforcement learning; only the interaction with environment is available. Generally, it is widely said that this type of learning method is not appropriated to obtain dynamic motions because Markov property is not necessarily guaranteed during the dynamic task. However, in this study, we try to avoid this problem by embedding the dynamic information in the robotic state space; the applicability of the proposed method is considered using both the real robot and dynamic simulator. This paper, in particular, discusses how the robot with 5-DOF, in which the Q-Learning algorithm is implemented, acquires a giant-swing motion. Further, we describe the reward effects on the Q-Learning. Finally, this paper demonstrates that the application of the Q-Learning enable the robot to perform a very attractive giant-swing motion",Consideration on Robotic Giant-swing Motion Generated by Reinforcement Learning,,"Ieee Service Center, 445 Hoes Lane, Po Box 1331, Piscataway, Nj 08855-1331 Usa",,,core
22956230,19/08/2013,"In this paper, we present a new approach for automatic synthesis of fault detection modules for autonomous mobile robots. The method relies on the fact that hardware faults typically change the flow of sensory perceptions received by the robot and the subsequent behavior of the control program. We collect data from three experiments with real robots. In each experiment, we record all sensory inputs from the robots while they are operating normally and after software-simulated faults have been injected. We use backpropagation neural networks to synthesize task-dependent fault detection modules. The performance of the modules is evaluated in terms of false positives and latency. ",Automatic Synthesis of Fault Detection Modules for Mobile Robots,,,,,core
45810141,2011,"For many applications such as compliant, accurate robot tracking control, dynamics models learned from data can help to achieve both compliant control performance as well as high tracking quality. Online learning of these dynamics models allows the robot controller to adapt itself to changes in the dynamics (e.g., due to time-variant nonlinearities or unforeseen loads). However, online learning in real-time applications -- as required in control -- cannot be realized by straightforward usage of off-the-shelf machine learning methods such as Gaussian process regression or support vector regression. In this paper, we propose a framework for online, incremental sparsification with a fixed budget designed for fast real-time model learning. The proposed approach employs a sparsification method based on an independence measure. In combination with an incremental learning approach such as incremental Gaussian process regression, we obtain a model approximation method which is applicable in real-time online learning.
It exhibits competitive learning accuracy when compared with standard regression techniques. Implementation on a real Barrett WAM robot demonstrates the applicability of the approach in real-time online model learning for real world systems",Incremental online sparsification for model learning in real-time robot control,,,10.1016/j.neucom.2010.06.033,,core
210801660,2011-05-01T00:00:00,"For many applications such as compliant, accurate robot tracking control, dynamics models learned from data can help to achieve both compliant control performance as well as high tracking quality. Online learning of these dynamics models allows the robot controller to adapt itself to changes in the dynamics (e.g., due to time-variant nonlinearities or unforeseen loads). However, online learning in real-time applications -- as required in control -- cannot be realized by straightforward usage of off-the-shelf machine learning methods such as Gaussian process regression or support vector regression. In this paper, we propose a framework for online, incremental sparsification with a fixed budget designed for fast real-time model learning. The proposed approach employs a sparsification method based on an independence measure. In combination with an incremental learning approach such as incremental Gaussian process regression, we obtain a model approximation method which is applicable in real-time online learning. It exhibits competitive learning accuracy when compared with standard regression techniques. Implementation on a real Barrett WAM robot demonstrates the applicability of the approach in real-time online model learning for real world systems",Incremental online sparsification for model learning in real-time robot control,,'Elsevier BV',10.1016/j.neucom.2010.06.033,,core
226221809,01/01/2013,"This paper deals with a decentralized inverse optimal neural controller for MIMO discrete-time unknown nonlinear systems, in a presence of external disturbances and parameter uncertainties. It uses two techniques: first, an identifier based on a discrete-time recurrent high order neural network (RHONN) trained with an extended Kalman filter (EKF) algorithm; second, on the basis of the real identifier a controller which uses inverse optimal control, is designed to avoid solving the Hamilton Jacobi Bellman (HJB) equation. The proposed scheme is implemented in real-time to control a Shrimp robot. "" 2013 IEEE."",,,,,,""10.1109/IJCNN.2013.6706785"",,,""http://hdl.handle.net/20.500.12104/44084"",""http://www.scopus.com/inward/record.url?eid=2-s2.0-84893556601&partnerID=40&md5=8905f32f0549a0374daf8d381e395ceb"",,,,,,,,""Proceedings of the International Joint Conference on Neural Networks"",,,,,,""Scopus"",,,,,,""Decentralized Inverse Optimal Neural Control; Mobile Robots; Neural Control; Neural identifier; Recurrent High Order Neural Networks"",,,,,,""Real-time decentralized inverse optimal neural control for a Shrimp robot"",,""Conference Paper""
""45885"",""123456789/35008"",,""Hernández-Vázquez, S., Departamento de Estudios para el Desarrollo Sustentable de Zona Costera, Universidad de Guadalajara, Gómez Farías # 82, Municipio de Cihuatlán, 48980 Jalisco, Mexico; Rodríguez-Estrella, R., Centro de Investigaciones Biológicas del Noroeste, Mar Bermejo 195, 23090 La Paz, Baja California Sur, Mexico; Ramírez-Ortega, F., Fundación de la Costa de Jalisco A.C., Av. Chapultepec # 206. Col Roma Norte, Delegacion Cuauhtemoc, 06700 Mexico D.F., Mexico; Loera, J., Fundación de la Costa de Jalisco A.C., Av. Chapultepec # 206. Col Roma Norte, Delegacion Cuauhtemoc, 06700 Mexico D.F., Mexico; Ortega, M., Fundación de la Costa de Jalisco A.C., Av. Chapultepec # 206. Col Roma Norte, Delegacion Cuauhtemoc, 06700 Mexico D.F., Mexico"",,""Hernandez-Vazquez,  S",Real time implementation,,'Springer Science and Business Media LLC',10.1007/978-3-540-78289-6_7,,core
51877228,2011-01-01T00:00:00,"Methods for intelligent mobile robots control which are based on principles of hierarchical control systems will be reviewed in this article. Hierarchical intelligent mobile robots are new direction for development of robotics, which have wide application perspectives. Despite increasing progress in technologies, the main problem of autonomous mobile robots development is that, they are ineffective in their control. In each of the hierarchical control levels (movement in space, problems solving and signal processing sets) will define by specific management of objectives, goals and rules. Communication and management between hierarchies are implemented by higher level of hierarchy using obtained information about the environment and lover level of hierarchy. Studies have shown that artificial neural networks, fuzzy logic are widely used for the development of the hierarchical systems. The main focus of the work is on communications in hierarchy levels, since the robot must be controlled in real time",Hierarchiniai autonominių mobiliųjų robotų valdymo metodai,,'Kaunas University of Technology (KTU)',10.5755/j01.eee.110.4.298,,core
82248545,30/11/2013,"AbstractThe cerebellum plays an essential role in adaptive motor control. Once we are able to build a cerebellar model that runs in realtime, which means that a computer simulation of 1 s in the simulated world completes within 1 s in the real world, the cerebellar model could be used as a realtime adaptive neural controller for physical hardware such as humanoid robots. In this paper, we introduce “Realtime Cerebellum (RC)”, a new implementation of our large-scale spiking network model of the cerebellum, which was originally built to study cerebellar mechanisms for simultaneous gain and timing control and acted as a general-purpose supervised learning machine of spatiotemporal information known as reservoir computing, on a graphics processing unit (GPU). Owing to the massive parallel computing capability of a GPU, RC runs in realtime, while reproducing qualitatively the same simulation results of the Pavlovian delay eyeblink conditioning with the previous version. RC is adopted as a realtime adaptive controller of a humanoid robot, which is instructed to learn a proper timing to swing a bat to hit a flying ball online. These results suggest that RC provides a means to apply the computational power of the cerebellum as a versatile supervised learning machine towards engineering applications",Realtime cerebellum: A large-scale spiking network model of the cerebellum that runs in realtime using a graphics processing unit ,https://core.ac.uk/download/pdf/82248545.pdf,Elsevier Ltd.,10.1016/j.neunet.2013.01.019,,core
22640122,23/07/2013,"Abstract — A new paradigm of intelligent navigation system for mobile robot has been enriched with some common features like: criteria for optimal performance and ways to optimize design, structure and control of robot. With the growing need for the deployment of intelligent and highly autonomous systems, it would be beneficial to flawlessly combine robust learning capabilities of artificial neural networks with a high level of knowledge interpretability provided by fuzzy-logic. Fuzzy-neural network is able to build comprehensive knowledge bases considering sensor-rich system with real time constraints by adaptive learning, rule extraction and insertion, and neural/fuzzy reasoning. This technique is simulated and also compared with other simulation studies by previous researcher. The training for back propagation algorithm and its navigational performances analysis has been done in real experimental setup. As experimental result matches well with the simulation result, the realism of method is verified",Fuzzy-Neuro based Navigational Strategy for Mobile Robot,,,,,core
26339085,2012-03-01T00:00:00Z,"<p>Biomimetic autonomous group manipulation of mobile robots has great potential in artificial intelligence, smart life, and automation related applications. In addition, it is also possible to use it as a tool for exploring the behavior of biological group motion in groups such as geese and fish. In order to fulfill these goals, fundamental capabilities of identification, navigation, and communication between robots must be established. In this work, key schemes are proposed for carrying out subsequent navigation tasks. By integrating omni-wheel mobile robots with X-Bee communication protocols, Arduino controls, IR range finders, and CMOS cameras, as well as with wiimote multi-zone localization, tasks such as obstacle and collision avoidance, object following, autonomous movement, and the indoor localization of group robots are implemented as the first step toward the autonomous control of group robots for subsequent biomimetic and smart life applications. With the resolution of these key issues, more realistic scenarios can be designed to achieve real group robot applications for indoor service in the future.</p",Development of Navigation Schemes for Grouped Mobile Robots Leading to Biomimetic Applications,,Chinese Institute of Automation Engineers (CIAE) & Taiwan Smart Living Space Association (SMART LISA),10.5875/ausmt.v2i1.113,"[{'title': None, 'identifiers': ['issn:2223-9766', '2223-9766']}]",core
400071107,2011-07-12T00:00:00,"International audienceA major challenge in modern robotics is to liberate robots from controlled industrial settings, and allow them to interact with humans and changing environments in the real world. The current research attempts to determine if a neurophysiologically motivated model of cortical function in the primate can help to address this challenge. Primates are endowed with cognitive systems that allow them to maximize the feedback from their environment by learning the values of actions in diverse situations and by adjusting their behavioral parameters (i.e. cognitive control) to accommodate unexpected events. In such contexts uncertainty can arise from at least two distinct sources - expected uncertainty resulting from noise during sensory-motor interaction in a known context, and unexpected uncertainty resulting from the changing probabilistic structure of the environment. However, it is not clear how neurophysiological mechanisms of reinforcement learning and cognitive control integrate in the brain to produce efficient behavior. Based on primate neuroanatomy and neurophysiology, we propose a novel computational model for the interaction between lateral prefrontal and anterior cingulate cortex (LPFC and ACC) reconciling previous models dedicated to these two functions. We deployed the model in two robots and demonstrate that, based on adaptive regulation of a meta-parameter β that controls the exploration rate, the model can robustly deal with the two kinds of uncertainties in the real world. In addition the model could reproduce monkey behavioral performance and neurophysiological data in two problem-solving tasks. A last experiment extends this to human-robot interaction with the iCub humanoid, and novel sources of uncertainty corresponding to ""cheating"" by the human. The combined results provide concrete evidence for the ability of neurophysiologically inspired cognitive systems to control advanced robots in the real world",Robot cognitive control with a neurophysiologically inspired reinforcement learning model,,'Frontiers Media SA',10.3389/fnbot.2011.00001,,core
79177829,2011-06-01T00:00:00,"O presente trabalho aborda e analisa o uso da Realidade Virtual (RV) em Sistemas Tutores Inteligentes (STI), buscando evidenciar a cooperação entre as duas áreas. Neste sentido, as teorias de Inteligência Artificial (IA) contribuíram para a estruturação de ambientes interativos de ensino com as melhores práticas de STI e RV, entre elas: facilidade de acesso, individualização do ensino, conteúdo lapidado, modelado e selecionado, um ambiente imersivo, interativo, intuitivo, destacando a integração do robô Phantom ao ambiente 3D e o seu relacionamento com a ontologia do projeto. Um estudo de caso foi realizado na área de anatomia ósseocraniana para cursos de graduação na área da saúde. O processo de validação do protótipo e da arquitetura proposta teve apoio de professores e um grupo de 120 estudantes da área da saúde da UCB e da UnB. Como os cursos eram de características distintas, dividiu-se a validação em dois grupos com propósitos qualificados. O grupo A ficou responsável pela validação da arquitetura/tecnologia formado pelos estudantes de medicina do 1° semestre da disciplina de anatomia de ambas as instituições. O grupo B foi utilizado para a validação do conceito/conteúdo e formado por estudantes de enfermagem e biomedicina também do 1° semestre da disciplina de anatomia, sendo que esses estavam sob a responsabilidade do mesmo professor e tiveram a mesma forma de avaliação. Os estudantes de biomedicina tiveram acesso ao sistema, enquanto que os de enfermagem tiveram apenas o ensino convencional, sem acesso ao ASM. O objetivo foi verificar se a utilização do ASM como foi implementado contribuiu para o ensino da anatomia humana. O processo de aprendizado foi alocado no trabalho como uma possibilidade de expansão da arquitetura e evolução desse trabalho, acrescentando uma rede neural artificial entre a interação do estudante e o tipo de conteúdo a ser solicitado da ontologia. Portanto, através de um questionário on-line realizado com o apoio de um especialista da área de saúde, o ASM teve resultados satisfatórios. _________________________________________________________________________________ ABSTRACTThe present study describes an analysis of the use of Virtual Reality (VR) in Intelligent Tutorial Systems (ITS), aiming to show the cooperation between the two areas. Consequently, Artificial Intelligence (AI) theories contributed to the structuring of interactive teaching environments with the best ITS and VR practices, among these: easy access, individualized teaching, refined, modeled and selected content, an intuitive, immersive, interactive environment, particularly the integration of the Phantom robot in a 3D environment and its relationship to the ontology of the project. A case study was completed for undergraduates majoring in health, in the area of cranial-skeletal anatomy. The prototype and proposed architecture validation process was supported by professors and a group of 120 students from UCB and UnB in the area of health. As the majors had distinct characteristics, the validation was divided into two groups with qualified proposals. Group A was responsible for the validation of the architecture/technology and consisted of the medical students in their 1st semester of anatomy from both of the institutions. Group B was used for the validation of concept/content and consisted of students in nursing, and biomedicine, also in their 1st semester of anatomy, under the supervision of the same professor, and undergoing the same evaluation process. The students in biomedicine had access to the system, while the nursing students had conventional teaching methods, without access to the Medical Simulation Platform (MSP). The objective was to verify if the use of the MSP, as it was implemented, contributed to the teaching of human anatomy. The learning process was allocated in the work as a possibility of expansion of the architecture and evolution of this work, adding an artificial neural network between the interaction of the student and the type of content to be solicited from the ontology. Through an on-line questionnaire, administered with the help of a health expert, the MSP presented satisfactory results",Integration of the phantom interface on intelligent tutorial system for the medical simulation environment,https://core.ac.uk/download/79177829.pdf,'Editora Cubo',10.4322/rbeb.2011.009,,core
153548995,2011,"This thesis focuses on the development of an artificial intelligence system for a heterogeneous ensemble of mobile robots. Many robots in the ensemble may have limited processing, communication, sensing, and/or actuation capabilities. This means that each robot may not be able to execute all tasks that are input to the system. A hierarchical system is proposed to permit robots with superior processing and communication abilities to assign tasks and coordinate the less computationally able robots. The limited processing robots may also utilise the resources of superior robots during task execution. Effective task allocation and coordination should result in efficient execution of a global task. Many existing approaches to robot task allocation assume expert knowledge for task specification. This is not ideal if a non-expert human user wants to modify the task requirements.
A novel reduced human user input task allocation and feedback coordination technique for limited capability mobile robots is developed and implemented. Unlike existing approaches, the presented method focuses on expressing tasks and robots in terms of processing, communication, sensing, and actuation physical resources. This has the potential to allow non-expert human users to specify tasks to the team of robots. Fuzzy inference systems are utilised to simplify detailed robot information for comparison with simple human user inputs that represent task resource requirements. Like many existing task allocation methods, a greedy algorithm is employed to select robots. This can result in suboptimal task allocation. In addition to this, the non-expert user’s task specifications might be erroneous in some instances. Hence, a feedback coordination component monitors robot performance during task execution.
In this thesis, a customised multi-robot mapping and exploration task is utilised as a model task to test the effectiveness of the developed task allocation and feedback coordination strategy. Extensive simulation experiments with various robot team configurations are executed in environments of varying sizes and obstacle densities to assess the performance of the technique. Task allocation is able to identify suitable robots and is robust to selection weight variation. The task allocation process is subjective to fuzzy membership function parameters which may vary for different This thesis focuses on the development of an artificial intelligence system for a heterogeneous ensemble of mobile robots. Many robots in the ensemble may have limited processing, communication, sensing, and/or actuation capabilities. This means that each robot may not be able to execute all tasks that are input to the system. A hierarchical system is proposed to permit robots with superior processing and communication abilities to assign tasks and coordinate the less computationally able robots. The limited processing robots may also utilise the resources of superior robots during task execution. Effective task allocation and coordination should result in efficient execution of a global task. Many existing approaches to robot task allocation assume expert knowledge for task specification. This is not ideal if a non-expert human user wants to modify the task requirements.
A novel reduced human user input task allocation and feedback coordination technique for limited capability mobile robots is developed and implemented. Unlike existing approaches, the presented method focuses on expressing tasks and robots in terms of processing, communication, sensing, and actuation physical resources. This has the potential to allow non-expert human users to specify tasks to the team of robots. Fuzzy inference systems are utilised to simplify detailed robot information for comparison with simple human user inputs that represent task resource requirements. Like many existing task allocation methods, a greedy algorithm is employed to select robots. This can result in suboptimal task allocation. In addition to this, the non-expert user’s task specifications might be erroneous in some instances. Hence, a feedback coordination component monitors robot performance during task execution.
In this thesis, a customised multi-robot mapping and exploration task is utilised as a model task to test the effectiveness of the developed task allocation and feedback coordination strategy. Extensive simulation experiments with various robot team configurations are executed in environments of varying sizes and obstacle densities to assess the performance of the technique. Task allocation is able to identify suitable robots and is robust to selection weight variation. The task allocation process is subjective to fuzzy membership function parameters which may vary for different users. Feedback coordination is robust to variation in weights and thresholds for failure detection. This permits the correction of suboptimal allocations arising from greedy task allocation, incorrect initial task specifications or unexpected failures. By being robust within the tested limits, weights and thresholds can be intuitively selected. However, other parameters such as ideal achievement data can be difficult to accurately characterise in some instances.
A hierarchical hybrid deliberative-reactive navigation system for memory constrained heterogeneous robots to navigate obstructed environments is developed. Deliberative control is developed using a modified version of the A* algorithm and a rectangular occupancy grid map. A novel two-tiered path planner executes on limited memory mobile robots utilising the memory of a computationally powerful robot to enable navigation beyond localised regions of a large environment. Reactive control is developed using a modified dynamic window approach and a polar histogram technique to remove the need for periodic path planning.
A range of simulation experiments in different sized environments is conducted to assess the performance of the two-tiered path planning strategy. The path planner is able to achieve superior or comparable execution times to non-memory constrained path planning when small sized local maps are employed in large global environments. Performance of hybrid deliberative-reactive navigation is assessed in a range of simulated environments and is also validated on a real robot. The developed reactive control system outperforms the dynamic window method",Development of an Artificial Intelligence System for the Instruction and Control of Cooperating Mobile Robots,,Victoria University of Wellington,,,core
297882385,2012,"This paper presents a discrete-time sliding mode control based on neural networks designed for robotic manipulators. Radial basis function neural networks are used to learn about uncertainties affecting the system. The on-line learning algorithm combines the growing criterion and the pruning strategy of the minimal resource allocating network technique with an adaptive extended Kalman filter to update all the parameters of the networks. A method to improve the run-time performance for the real-time implementation of the learning algorithm has been considered. The analysis of the control stability is given and the controller is evaluated on the ERICC robot arm. Experiments show that the proposed controller produces good trajectory tracking performance and it is robust in the presence of model inaccuracies, disturbances and payload perturbations",Minimal resource allocating networks for discrete time sliding mode control of robotic manipulators,,,10.1109/TII.2012.2205395,,core
143918919,2013-08-14T12:11:10,"The 18th International Joint Conference on Artificial Intelligence (IJCAI-03), Acapulco, Mexico, 9-15 August 2003In many real-world environments, Automatic Speech Recognition (ASR) technologies fail to provide adequate performance for applications such as human robot dialog. Despite substantial evidence that speech recognition in humans is performed in a top-down as well as bottom-up manner, ASR systems typically fail to capitalize on this, instead relying on a purely statistical, bottom up methodology. In this paper we advocate the use of a knowledge based approach to improving ASR in domains such as mobile robotics. A simple implementation is presented, which uses the visual recognition of objects in a robot\u27s environment to increase the probability that words and sentences related to these objects will be recognized.TS 13.08.1",Improving speech recognition on a mobile robot platform through the use of top-down visual queues,,IJCAI Workshop,,,core
148452645,2011-01-01T00:00:00,"In critical domains such as urban search and rescue (USAR), and bomb disposal, the deployment of teleoperated robots is essential to reduce the risk of first responder personnel. Teleoperation is a difficult task, particularly when controlling robots from an isolated safety zone. In general, the operator has to solve simultaneously the problems of mission planning, target identification, robot navigation, and robot control. We introduce a system to support teleoperated navigation with real-time mapping consisting of a two-step scan matching method that re-considers data associations during the search. The algorithm processes data from laser range finder and gyroscope only, thereby it is independent from the robot platform. Furthermore, we introduce a user-guided procedure for improving the global consistency of maps generated by the scan matcher. Globally consistent maps are computed by a graph-based maximum likelihood method that is biased by localizing crucial parts of the scan matcher trajectory on a prior given geo-tiff image. The approach has been implemented as an embedded system and extensively tested on robot platforms designed for teleoperation in critical situations, such as bomb disposal. Furthermore, the system was evaluated in a test maze by first responders during the Disaster City event in Texas 2008.Artificial Intelligence & Integrated Computer System",Mapping for the Support of First Responders in Critical Domains,,'Springer Science and Business Media LLC',10.1007/s10846-010-9520-x,,core
79814016,2012-10-29T00:00:00,"International audienceThe Artificial Intelligence entities, capable to communicate with their vicinity, such as PDAs, Smartphones, sensors, robots, software, middleware, etc. are increasingly introduced in our environment. The conventional topography of our everyday space introduces Distributed Artificial Intelligence, Ambient Intelligence and Ubiquitous Computing concepts. For example, when traveling to a new city, information about sights in the city, hotels, or even bus timetable are needed. Thanks to ubiquitous computing, citizens as well as tourists can communicate with their environment and get all these information in real time right to their terminals. In this paper, we present a large package of ubiquitous services that might be requested in a ubiquitous space (in this case, the ubiquitous space is the Grand Stade Lille Métropole) and we have designed a multi-agents architecture which will support these services and optimize their quality as response time and information cost.Les entités de l'Intelligence Artificielle, communiquantes avec leur entourage, telles que les PDAs, les téléphones, les capteurs, les robots, les logiciels, les middlewares, etc. sont de plus en plus présentes dans notre environnement. La nouvelle topographie de nos espaces quotidiens introduit les notions de l'Intelligence Artificielle Distribuée, l'Intelligence Ambiante et l'Informatique Ubiquitaire. Notre objectif est d'avoir un accès en temps réel, via un support mobile, à différents types d'information issus d'un environnement ubiquitaire, tels que les lieux touristiques et les horaires de bus. Dans ce papier, nous proposons une architecture efficiente d'un système ubiquitaire ciblé basé sur une approche multi-agent. Nous nous focalisons dans notre étude sur le Grand Stade Lille Métropole et les services qui peuvent y être demandés. L'architecture proposée va devoir supporter les services et optimiser leur qualité par rapport au temps de réponse et au coût de l'information",Architecture à base d'agents pour optimiser les services d'aide à la mobilité : vers une conception d'un espace ubiquitaire,,HAL CCSD,,,core
23486612,03/12/2013,"the Czech robota (forced labor). Limited to work too tedious or dangerous for humans, today s robots weld parts on assembly lines, inspect nuclear plants, and explore other planets. Generally, robots are still far from achieving their fictional counterparts intelligence and flexibility. Humanoid robotics labs worldwide are working on creating robots that are one step closer to science fiction s androids. Building a humanlike robot is a formidable engineering task requiring a combination of mechanical, electrical, and software engineering; computer architecture; and realtime control. In 1993, we began a project aimed at constructing a humanoid robot for use in exploring theories of human intelligence. 1,2 In addition to the relevant engineering, computer architecture, and real-time-control issues, we ve had to address issues particular to integrated systems: What types of sensors should we use, and how should the robot interpret the data? How can the robot act deliberately to achieve a task and remain responsive to the environment? How can the system adapt to changing conditions and learn new tasks? Each humanoid robotics lab must address many of the same motor-control, perception, and machine-learning problems. The principles behind our methodology The real divergence between groups stems from radically different research agendas an","ASIDE FROM THEIR TRADITIONAL ROLES, HUMANOID ROBOTS CAN BE USED TO EXPLORE THEORIES OF HUMAN INTELLIGENCE. THE AUTHORS DISCUSS THEIR PROJECT AIMED AT DEVELOPING ROBOTS THAT CAN BEHAVE LIKE AND INTERACT WITH HUMANS.",,,,,core
360547026,2011-04-07T00:00:00,"Before deploying a software system we need to assure ourselves (and stake- holders) that the system will behave correctly. This assurance is usually done by testing the system. However, it is intuitively obvious that adaptive systems, including agent-based systems, can exhibit complex behaviour, and are thus harder to test. In this paper we examine this “obvious intuition” in the case of Belief-Desire-Intention (BDI) agents. We analyse the size of the behaviour space of BDI agents and show that although the intuition is correct, the factors that influence the size are not what we expected them to be; specifically, we found that the introduction of failure handling had a much larger effect on the size of the behaviour space than we expected. We also discuss the implications of these findings on the testability of BDI agents.Unpublished1. Wooldridge, M.: An Introduction to MultiAgent Systems. John Wiley & Sons, Chichester, England (2002). ISBN 0 47149691X

2. Munroe, S., Miller, T., Belecheanu, R., Pechoucek, M., McBurney, P., Luck, M.: Crossing the agent technology chasm: Experiences and challenges in commercial applications of agents. Knowledge Engineering Review 21(4), 345–392 (2006)

3. Benfield, S.S., Hendrickson, J., Galanti, D.: Making a strong business case for multiagent technology. In: P. Stone, G. Weiss (eds.) Proceedings of the Fifth International Joint Conference on Autonomous Agents and Multiagent Systems (AAMAS), pp. 10–15. ACM Press (2006)

4. Rao, A.S., Georgeff, M.P.: Modeling rational agents within a BDI-architecture. In: J. Allen, R. Fikes, E. Sandewall (eds.) Principles of Knowledge Representation and Reasoning, Proceedings of the Second International Conference, pp. 473–484. Morgan Kaufmann (1991)

5. Bratman, M.E.: Intentions, Plans, and Practical Reason. Harvard University Press, Cambridge, MA (1987)

6. Zhang, Z., Thangarajah, J., Padgham, L.: Model based testing for agent systems. In: J. Filipe, B. Shishkov, M. Helfert, L. Maciaszek (eds.) Software and Data Technologies, Communications in Computer and Information Science, vol. 22, pp. 399–413. Springer, Berlin/Heidelberg (2009)

7. Ekinci, E.E., Tiryaki, A.M., Çetin, Ö., Dikenelli: Goal-oriented agent testing revisited. In: M. Luck, J.J. Gomez-Sanz (eds.) Agent-Oriented Software Engineering IX, Lecture Notes in Computer Science, vol. 5386, pp. 173–186. Springer, Berlin/Heidelberg (2009)

8. Gomez-Sanz, J.J., Botía, J., Serrano, E., Pavón, J.: Testing and debugging of MAS interactions with INGENIAS. In: M. Luck, J.J. Gomez-Sanz (eds.) Agent-Oriented Software Engineering IX, Lecture Notes in Computer Science, vol. 5386, pp. 199–212. Springer, Berlin/Heidelberg (2009)

9. Nguyen, C.D., Perini, A., Tonella, P.: Experimental evaluation of ontology-based test generation for multi-agent systems. In: M. Luck, J.J. Gomez-Sanz (eds.) Agent-Oriented Software Engineering IX, Lecture Notes in Computer Science, vol. 5386, pp. 187–198. Springer, Berlin/Heidelberg (2009)

10. Padgham, L., Winikoff, M.: Developing Intelligent Agent Systems: A Practical Guide. John Wiley and Sons (2004). ISBN 0-470-86120-7

11. Shaw, P., Farwer, B., Bordini, R.: Theoretical and experimental results on the goal-plan tree problem. In: Proceedings of the Seventh International Conference on Autonomous Agents and Multiagent Systems (AAMAS), pp. 1379–1382. IFAAMAS (2008)

12. Erol, K., Hendler, J.A., Nau, D.S.: HTN planning: Complexity and expressivity. In: Proceedings of the 12th National Conference on Artificial Intelligence (AAAI), pp. 1123–1128. AAAI Press (1994)

13. de Silva, L., Padgham, L.: A comparison of BDI based real-time reasoning and HTN based planning. In: G. Webb, X. Yu (eds.) AI 2004: Advances in Artificial Intelligence, Lecture Notes in Computer Science, vol. 3339, pp. 1167–1173. Springer, Berlin/Heidelberg (2004)

14. Erol, K., Hendler, J., Nau, D.: Complexity results for HTN planning. Annals of Mathematics and Artificial Intelligence 18(1), 69–93 (1996)

15. Paolucci, M., Shehory, O., Sycara, K.P., Kalp, D., Pannu, A.: A planning component for RETSINA agents. In: N.R. Jennings, Y. Lespérance (eds.) Intelligent Agents VI, Agent Theories, Architectures, and Languages (ATAL), 6th International Workshop, ATAL ’99, Orlando, Florida, USA, July 15-17, 1999, Proceedings, Lecture Notes in Computer Science, vol. 1757, pp. 147–161. Springer, Berlin/Heidelberg (2000)

16. Busetta, P., Rönnquist, R., Hodgson, A., Lucas, A.: JACK Intelligent Agents - Components for Intelligent Agents in Java. AgentLink News (2) (1999). URL http://www.agentlink.org/newsletter/2/newsletter2.pdf

17. Huber, M.J.: JAM: A BDI-theoretic mobile agent architecture. In: Proceedings of the Third International Conference on Autonomous Agents (Agents’99), pp. 236–243. ACM Press (1999)

18. d’Inverno, M., Kinny, D., Luck, M., Wooldridge, M.: A formal specification of dMARS. In: M. Singh, A. Rao, M. Wooldridge (eds.) Intelligent Agents IV: Proceedings of the Fourth International Workshop on Agent Theories, Architectures, and Languages, Lecture Notes in Artificial Intelligence, vol. 1365, pp. 155–176. Springer, Berlin/Heidelberg (1998)

19. Georgeff, M.P., Lansky, A.L.: Procedural knowledge. Proceedings of the IEEE, Special Issue on Knowledge Representation 74(10), 1383–1398 (1986)

20. Ingrand, F.F., Georgeff, M.P., Rao, A.S.: An architecture for real-time reasoning and system control. IEEE Expert 7(6), 33–44 (1992)

21. Lee, J., Huber, M.J., Kenny, P.G., Durfee, E.H.: UM-PRS: An implementation of the procedural reasoning system for multirobot applications. In: Proceedings of the Conference on Intelligent Robotics in Field, Factory, Service, and Space (CIRFFSS’94), pp. 842–849 (1994)

22. Bordini, R.H., Hübner, J.F., Wooldridge, M.: Programming multi-agent systems in AgentSpeak using Jason. Wiley (2007). ISBN 0470029005

23. Morley, D., Myers, K.: The SPARK agent framework. In: Proceedings of the Third International Joint Conference on Autonomous Agents and Multiagent Systems (AAMAS), pp. 714–721. IEEE Computer Society, Washington, DC, USA (2004)

24. Pokahr, A., Braubach, L., Lamersdorf, W.: Jadex: A BDI reasoning engine. In: R.H. Bordini, M. Dastani, J. Dix, A. El Fallah Seghrouchni (eds.) Multi-Agent Programming: Languages, Platforms and Applications, pp. 149–174. Springer (2005)

25. Bratman, M.E., Israel, D.J., Pollack, M.E.: Plans and resource-bounded practical reasoning. Computational Intelligence 4, 349–355 (1988)

26. Rao, A.S.: AgentSpeak(L): BDI agents speak out in a logical computable language. In: W.V. de Velde, J. Perrame (eds.) Agents Breaking Away: Proceedings of the Seventh European Workshop on Modelling Autonomous Agents in a Multi-Agent World (MAAMAW’96), Lecture Notes in Artificial Intelligence, vol. 1038, pp. 42–55. Springer, Berlin/Heidelberg (1996)

27. Winikoff, M., Padgham, L., Harland, J., Thangarajah, J.: Declarative & procedural goals in intelligent agent systems. In: Proceedings of the Eighth International Conference on Principles of Knowledge Representation and Reasoning (KR2002), pp. 470–481. Morgan Kaufmann, Toulouse, France (2002)

28. Georgeff, M.: Service orchestration: The next big challenge. DM Review Special Report (2006). URL http://www.dmreview.com/specialreports/20060613/1056195-1.html. (2006)

29. Dastani, M.: 2APL: a practical agent programming language. Autonomous Agents and Multi-Agent Systems 16(3), 214–248 (2008)

30. Naish, L.: Resource-oriented deadlock analysis. In: V. Dahl, I. Niemelä (eds.) Logic Programming, Lecture Notes in Computer Science, vol. 4670, pp. 302–316. Springer, Berlin/Heidelberg (2007)

31. Wilf, H.S.: generatingfunctionology, second edn. Academic Press Inc., Boston, MA (1994). URL http: //www.math.upenn.edu/∼wilf/gfology2.pdf

32. Sloane, N.J.A.: The on-line encyclopedia of integer sequences. http://www.research.att.com/∼njas/sequences/ (2007)

33. Burmeister, B., Arnold, M., Copaciu, F., Rimassa, G.: BDI-agents for agile goal-oriented business processes. In: Proceedings of the Seventh International Conference on Autonomous Agents and Multiagent Systems (AAMAS), pp. 37–44. IFAAMAS (2008)

34. Dorigo, M., Stützle, T.: Ant Colony Optimization. MIT Press (2004). ISBN 0-262-04219-3

35. van Riemsdijk, M.B., Dastani, M., Winikoff, M.: Goals in agent systems: A unifying framework. In: Proceedings of the Seventh Conference on Autonomous Agents and Multiagent Systems (AAMAS), pp. 713–720. IFAAMAS (2008)

36. Thangarajah, J., Winikoff, M., Padgham, L., Fischer, K.: Avoiding resource conflicts in intelligent agents. In: F. van Harmelen (ed.) Proceedings of the 15th European Conference on Artificial Intelligence (ECAI), pp. 18–22. IOS Press (2002)

37. Nguyen, C.D., Perinirini, A., Tonella, P.: Automated continuous testing of multi-agent systems. In: Proceedings of the Fifth European Workshop on Multi-Agent Systems (EUMAS) (2007)

38. Dwyer, M.B., Hatcliff, J., Pasareanu, C., Robby, Visser, W.: Formal software analysis: Emerging trends in software model checking. In: Future of Software Engineering 2007, pp. 120–136. IEEE Computer Society, Los Alamitos, CA (2007)

39. Wooldridge, M., Fisher, M., Huget, M.P., Parsons, S.: Model checking multi-agent systems with MABLE. In: Proceedings of the First International Joint Conference on Autonomous Agents and Multi-Agent Systems (AAMAS), pp. 952–959. ACM Press (2002)

40. Bordini, R.H., Fisher, M., Pardavila, C., Wooldridge, M.: Model checking AgentSpeak. In: Proceedings of the Second International Joint Conference on Autonomous Agents and Multiagent Systems (AAMAS), pp. 409–416. ACM Press (2003)

41. Raimondi, F., Lomuscio, A.: Automatic verification of multi-agent systems by model checking via ordered binary decision diagrams. J. Applied Logic 5(2), 235–251 (2007)

42. Burch, J., Clarke, E., McMillan, K., Dill, D., Hwang, J.: Symbolic model checking: 1020 states and beyond. Information and Computation 98(2), 142–170 (1992)

43. Fix, L., Grumberg, O., Heyman, A., Heyman, T., Schuster, A.: Verifying very large industrial circuits using 100 processes and beyond. In: D. Peled, Y.K. Tsay (eds.) Automated Technology for Verification and Analysis, Lecture Notes in Computer Science, vol. 3707, pp. 11–25. Springer, Berlin/Heidelberg (2005",On the testability of BDI agent systems,,'University of Otago Library',,,core
26046753,2011-07-01T00:00:00Z,"A major challenge in modern robotics is to liberate robots from controlled industrial settings, and allow them to interact with humans and changing environments in the real world.  The current research attempts to determine if a neurophysiologically motivated model of cortical function in the primate can help to address this challenge. Primates are endowed with cognitive systems that allow them to maximize the feedback from their environment by learning the values of actions in diverse situations and by adjusting their behavioral parameters (i.e. cognitive control) to accommodate unexpected events. In such contexts uncertainty can arise from at least two distinct sources – expected uncertainty resulting from noise during sensory-motor interaction in a known context, and unexpected uncertainty resulting from the changing probabilistic structure of the environment. However, it is not clear how neurophysiological mechanisms of reinforcement learning and cognitive control integrate in the brain to produce efficient behavior. Based on primate neuroanatomy and neurophysiology, we propose a novel computational model for the interaction between lateral prefrontal and anterior cingulate cortex (LPFC and ACC) reconciling previous models dedicated to these two functions. We deployed the model in two robots and demonstrate that, based on adaptive regulation of a meta-parameter β that controls the exploration rate, the model can robustly deal with the two kinds of uncertainties in the real world. In addition the model could reproduce monkey behavioral performance and neurophysiological data in two problem-solving tasks. A last experiment extends this to human-robot interaction with the iCub humanoid, and novel sources of uncertainty corresponding to cheating by the human. The combined results provide concrete evidence for the ability of neurophysiologically inspired cognitive systems to control advanced robots in the real world",Robot cognitive control with a neurophysiologically inspired reinforcement learning model,,Frontiers Media S.A.,10.3389/fnbot.2011.00001/full,"[{'title': None, 'identifiers': ['issn:1662-5218', '1662-5218']}]",core
9840187,2011-01-01T00:00:00,"It is expected that in a near future, personal robots will be endowed with enough autonomy to function and live in an individual's home. This is while commercial robots are designed with default configuration and factory settings which may often be different to an individual's operating preferences. This paper presents how reinforcement learning is applied and utilised towards personalisation of a robot's behaviour. Two-level reinforcement learning has been implemented: first level is in charge of energy autonomy, i.e. how to survive, and second level is involved in adapting robot's behaviour to user's preferences. In both levels Q-learning algorithm has been applied. First level actions have been learnt in a simulated environment and then the results have been transferred to the real robot. Second level has been fully implemented in the real robot and learnt by human-robot interaction. Finally, experiments showing the performance of the system are presente",Robot self-preservation and adaptation to user preferences in game play : a preliminary study,,'Institute of Electrical and Electronics Engineers (IEEE)',10.1109/ROBIO.2011.6181679,,core
79312149,31/07/2011,"Technological and conceptual advances in fields such as artificial intelligence, robotics, and material science have enabled robotic architectural environments to be implemented and tested in the last decade in virtual and physical prototypes. These prototypes are incorporating sensing-actuating mechanisms that enable interaction with their users and surroundings in real-time. While these prototypes obviously point towards a paradigm shift from inanimate towards animate architecture, they do not operate at building but at building component scale and do not address socio-economical or environmental aspects that affect architecture and society at large. This paper, on the one hand, critically discusses robotic prototypes built in the last decade at Delft University of Technology, on the other hand, it proposes a framework for future research envisioning robotic environments, as resizable, able to spatially expand or contract as well as move or be moved as needed. Such reconfigurable environments aim to validate the assumption that robotics incorporated in architecture improve efficiency of use due to multiple use of built space in condensed timeframes, while at the same time they advance technology for distributed autonomous robotic systems exhibiting collective behavior as well as test their application to sustainable architecture",Robotic environments,,"IAARC, International Association for Automation and Robotics in Construction",,,core
22632877,23/07/2013,"Abstract—This paper presents insight to ideas and the current state of the project SyRoTek- System for a robotic e-learning that aims to create a platform for students ’ practical verification of gained knowledge in the fields of Robotics and Artificial Intelligence. A set of real mobile robots is being developed in order to provide remote access to real hardware for enrolled students. The advantage of the real system over a pure virtual simulated environment is in realistic confrontation with noise and uncertainty that is an indivisible part of the real world. In such a system, students can acquire in deep understanding of main studied principles in an attractive form, as students (especially future engineers) like to control real things. On the other side, this can be a potential issue if an accessibility to the system have to be guaranteed in 24/7 mode. In SyRoTek, robots are designed with special attention to long-term and heavy duty usage. Moreover, safety mechanisms are realized in several layers of the proposed software architecture that provide access to robot control and sensors. In addition, support for semi-autonomous evaluation of students ’ solution of their assignments is a part of the system. Index Terms—artificial intelligence, robotics, e-learning I",SyRoTek- A Robotic System for Education,,,,,core
16341615,2013-07-31T15:59:16,"The 18th International Joint Conference on Artificial Intelligence (IJCAI-03), Acapulco, Mexico, 9-15 August 2003In many real-world environments, Automatic Speech Recognition (ASR) technologies fail to provide adequate performance for applications such as human robot dialog. Despite substantial evidence that speech recognition in humans is performed in a top-down as well as bottom-up manner, ASR systems typically fail to capitalize on this, instead relying on a purely statistical, bottom up methodology. In this paper we advocate the use of a knowledge based approach to improving ASR in domains such as mobile robotics. A simple implementation is presented, which uses the visual recognition of objects in a robot's environment to increase the probability that words and sentences related to these objects will be recognized.TS 13.08.1",Improving speech recognition on a mobile robot platform through the use of top-down visual queues,,IJCAI Workshop,,,core
22692265,31/07/2013,"Abstract—This paper deals with the design and the implementation of an automatic task planner for a robot, irrespective of whether it is a stationary robot or a mobile robot. The aim of the task planner nothing but, they are planning systems which are used to plan a particular task and do the robotic manipulation. This planning system is embedded into the system software in the computer, which is interfaced to the computer. When the instructions are given using the computer, this is transformed into real time application using the robot. All the AI based algorithms are written and saved in the control software, which acts as the intelligent task planning system",Design of an Artificial Intelligence Based Automatic Task Planner or a Robotic System,,,,,core
23420249,2012,"Bridging the gap between symbolic and subsymbolic representations is a – perhaps the – key obstacle along the path from the present state of AI achievement to human-level artificial general intelligence. One approach to bridging this gap is hybridization – for instance, incorporation of a subsymbolic system and a symbolic system into a integrative cognitive architecture. Here we present a detailed design for an implementation of this approach, via integrating a version of the DeSTIN deep learning system into OpenCog, an integrative cognitive architecture including rich symbolic capabilities. This is a ”tight” integration, in which the symbolic and subsymbolic aspects exert detailed real-time influence on each others ’ operations. Part I of this paper has described in detail the revisions to DeSTIN needed to support this integration, which are mainly along the lines of making it more ”representationally transparent, ” so that its internal states are easier for OpenCog to understand. We describe an extension of the approach beyond vision to include multi-sensory integration, and perception-action integration. We discuss the potential use of this integrated system to control mobile robots, as exemplified via a thought-experiment involving eye-hand coordination. ",Perception processing for general intelligence: Bridging the symbolic/subsymbolic gap,,Springer,10.1007/978-3-642-35506-6_9,,core
23350976,21/10/2013,"Developments in sensor technology and sensory input processing algorithms have enabled the use of mobile robots in real-world domains. As they are increasingly deployed to interact with humans in our homes and offices, robots need the ability to operate autonomously based on sensory cues and high-level feedback from non-expert human participants. Towards this objective, this chapter describes an integrated framework that jointly addresses the learning, adaptation and interaction challenges associated with robust human-robot interaction in real-world application domains. The novel probabilistic framework consists of: (a) a bootstrap learning algorithm that enables a robot to learn layered graphical models of environmental objects and adapt to unforeseen dynamic changes; (b) a hierarchical planning algorithm based on partially observable Markov decision processes (POMDPs) that enables the robot to reliably and efficiently tailor learning, sensing and processing to the task at hand; and (c) an augmented reinforcement learning algorithm that enables the robot to acquire limited high-level feedback from non-expert human participants, and merge human feedback with the information extracted from sensory cues. Instances of these algorithms are implemented and fully evaluated on mobile robots and in simulated domains using vision as the primary source of information in conjunction with range data and simplistic verbal inputs. Furthermore, a strategy is outlined to integrate these components to achieve robust human-robot interaction in real-world application domains",An Integrated Framework for Robust Human-Robot Interaction,,,,,core
54650951,2012,"This paper presents a discrete-time sliding mode control

based on neural networks designed for robotic manipulators.

Radial basis function neural networks are used to learn about

uncertainties affecting the system. The online learning algorithm

combines the growing criterion and the pruning strategy of the

minimal resource allocating network technique with an adaptive

extended Kalman filter to update all the parameters of the

networks. A method to improve the run-time performance for

the real-time implementation of the learning algorithm has been

considered. The analysis of the control stability is given and the

controller is evaluated on the ERICC robot arm. Experiments

show that the proposed controller produces good trajectory

tracking performance and it is robust in the presence of model

inaccuracies, disturbances and payload perturbations",Minimal Resource Allocating Networks for Discrete Time Sliding Mode Control of Robotic Manipulators,,,10.1109/tii.2012.2205395,,core
77003593,2014-09-15T00:00:00,"This paper studies the effect of passive and active impedance for protecting jumping robots from landing impacts. The theory of force transmissibility is used for selecting the passive impedance of the system to minimize the shock propagation. The active impedance is regulated online by a joint-level controller. On top of this controller, a reflex-based leg retraction scheme is implemented which is optimized using direct policy search reinforcement learning based on particle filtering. Experiments are conducted both in simulation and on a real-world hopping leg. We show that although the impact dynamics is fast, the addition of passive impedance provides enough time for the active impedance controller to react to the impact and protect the robot from damage",Can active impedance protect robots from landing impact?,,'Institute of Electrical and Electronics Engineers (IEEE)',10.1109/HUMANOIDS.2014.7041490,,core
22963097,20/08/2013,"Second Life is a 3D virtual environment that is created by Linden Labs. It is currently continuing to grow and is widely used all over the world. It has already been adopted by organisations such as IBM, Sun Microsystems, ABC, Sony, and many others. The Sony AIBO (short for Artificial Intelligence roBOt) is an artificial intelligence robotic pet dog designed and manufactured by Sony. The AIBO is considered to be an autonomous robot, which is able to gain information about the environment and be able to function without human intervention. The main aim of this project is to use the Second Life online virtual space (virtual world) to simulate and control the movements of the Sony AIBO robot (real world) in a wireless environment using sound software engineering principles. This paper details the design of the teleoperation system and the rationale behind the design as well as proving that the aim of the project can be successfully met. The outcomes of the experiments performed to measure the success of the system are detailed in this paper. These outcomes include how successful the system was in terms of teleoperating the robot using the Second Life virtual world as well as some of the issues faced while performing the experiments and a dicussion of the possible solutions on how to solve the problems. This project could lead to future work in various application domains such as the smart home concept or in the mining industry",Connecting the Real World with the Virtual World,,,,,core
148530906,2012,"In teaching software engineering, it is often interesting to introduce real life scenarios for students to experience and to learn how to collect information from respective clients. The ideal arrangement is to have some real clients willing to spend time to provide their ideas of a target system through interviews. However, this arrangement cannot be scaled up as it demands too much resource. Starting from 2008, we have used Second Life (SL) to create a virtual company, named SVG Corporation, which has multiple departments so as to simulate the real-world business environment. The development of this fictitious company not only provides a new experience in requirement collection to students, but also lowers the working effort of our colleagues in acting as external business clients. Students can practice their communication and fact finding skills during visits in the departments and interviews with the virtual ""staff"". The company has been used to support 2 subjects, Human Computer Interface and Foundations of Database Systems. The presence of SL acts as an online platform for students to access and acquire user requirements from staff (AI robots) of a virtual company, through a series of interviews, for system development. The roles of SL are twofold: to reduce the operational overheads in the project administration and to allow students to gain more hands-on experiences through working on a simulated real-life business cases. Hence, student could learn how to apply their knowledge and understand the software development process in the real business world. In this paper, we would like to report our experience and results of using SL in the software engineering student projects. Furthermore, the problems and the difficulties encountered during project period will be discussed for future enhancement.Department of Computin",Acquiring software project specifications in a virtual world,,,,"[{'title': None, 'identifiers': ['1479-4403', 'issn:1479-4403']}]",core
79326814,01/09/2014,"Technological and conceptual advances in fields such as artificial intelligence, robotics, and material science have enabled robotic building to be in the last decade prototypically implemented. In this context, robotic building implies both physically built robotic environments and robotically supported building processes, whereas reconfigurable, robotic environments incorporating sensor-actuator mechanisms that enable buildings to interact with their users and surroundings in real-time require design to production, assembly, and operation chains that may be (in part or as whole) implemented by robotic means. This paper presents and discusses research and experimental developments in robotic building implemented more recently at Hyperbody",Robotic buildings(s),,Baltzer Science Publishers,,,core
211472495,2013-01-01T00:00:00,"As robots’ working environments become more and more complex, new pattern recognition challenges emerge. Apart from the conventional question ”What is the object?"", nowadays, scholars should answer to other appealing queries such as ""Where is it?"" or ""How to manipulate it?"". State of the art vision methods consist of advanced recognition subroutines that are fed into machine learning algorithms so as to reach assumptions, regarding the current world state, akin to the ones of a human mind. One of the most challenging tasks in machine vision is the estimation of the 3D pose of an object, due to its practical implication and its augmentative importance in scene interpretation processes. A plethora of diverse applications impose upon the accurate assessment of objects’ geometrical configuration, relative to a given coordinate system, in order to achieve their goals. In robotics, autonomous object manipulation can efficiently be accomplished only in cases where the full pose (six Degrees of Freedom (DoF)) of the testing target is known. Despite the affluent research endeavors and the achievements reached so far, an advanced vision system characterized by low complexity and sufficient generalization capabilities has yet to be built. This PhD thesis is inspired by the remarkable skills of humans in the particular task of estimating the relative pose of objects given an initial hypothesis. Regardless the lighting conditions and common visual disturbances, such as partial occlusions, humans excel in interpreting the 3D geometrical configuration of arbitrary placed objects. Among all operations realized by human beings, the majority is directly related to object manipulation. On the other hand, the realization of target manipulation by a robotic arm puts strict constraints regarding the exact speed of execution and the ""nature"" of the testing objects. Contemporary solutions to this problem require high storage capabilities whilst failing to generalize to unknown testing targets, i.e. objects that are not included into the training set. The main objective of this PhD thesis is the development of innovative vision algorithms tackling the 3D object pose estimation problem and that are to be realized on robotic platforms. It is apparent that, the designed architectures should initially, deal with the position estimation problem -i.e. mainly the depth estimation one- and gradually solve for the remainder three orientation DoFs. The relevant literature was surveyed to reveal the current state of the art and to highlight open research issues that are to be addressed. Afterwards, novel manifold modeling approaches, resulting in low dimensional and high discriminative feature vectors, were designed and implemented. Lastly, the developed algorithms were adopted by robotic platforms to carry out autonomous object grasping. In the first research endeavor of this thesis, the construction of a simple and easy-to-build framework for location assignment of an object in a scene, is investigated. The developed method is based on the observation that features extracted from any two-part (detector-descriptor) algorithm correspond to spots on the object’s surface and their center of mass is related to the one of the objects. Thus, by extracting these features at known positions of the sought object, one can estimate its distance from the camera. Comparing to the contemporary solutions for depth estimation, the employed approach is computationally inexpressive whilst requiring only a single sensor opposed to stereo vision architectures. Moreover, a neural network-based framework that is able to calculate the three rotational DoFs of any object is presented. The network is trained with numerous targets contained in several available datasets with the training process been guided by a fuzzy extraction of the centers of the abstracted features through the Fuzzy c-means algorithm. The 3D pose of an object in a training instance is considered as the distances of the fuzzy centers from one particular cluster. The proposed method involves a new input-output mapping that reduces the dimensionality of the input vectors with good performance. The contribution of this study entails the formalization of this new input-output method that outperforms the conventional dimensionality reduction techniques widely used in image processing applications. Following the intuition that (a) one object viewed under varying perspectives lays on a well-projected subspace and (b) different objects captured under similar viewpoints share identical poses, a sophisticated framework capable of both recognizing objects and estimating their pose in the 3D working space was coined. It encompasses a manifold modeling procedure that depends on the attributes of mutual information and a constellation-based structure, respectively. The employed recognition module incorporates a modified approach of a known dimensionality reduction technique that constructs a similarity matrix based on mutual information among objects and then seek a low-dimensional representation that preserves the local structure of the objects in the initial high-dimensional space. Once the data are projected into the sub-space a Support Vector Machine (SVM) classifier provides accurate recognition. The 3D pose estimation module is based on a manifold modeling algorithm that constructs feature vectors of low dimensionality and high discriminative capabilities. Then a Sparse Grasping Manifolds (SGM) method is proposed in this thesis, which aims at solving the problem of manipulation of unknown objects by unifying 3D pose manifolds and grasping points into a cohesive framework for robot grasping. Unlike contemporary systems that crave extensive supervision and large repositories of images of objects, meticulous emphasis is given on providing a ground solution with large generalization capacities founded on unsupervised learning. Additionally, the visual data available are processed in a way that their projection onto the corresponding subspaces is sparse, compact and highly representative. Grasping manifolds depend on a novel bunch-based architecture, here introduced for the first time that, unlike previous works, bypasses the part selection process using unsupervised clustering, while by extracting local patches encapsulates both appearance and geometrical attributes of the objects. Contrary to earlier works the presented method offers higher generalization capacities mainly due to the efficient learning of the employed function that is based on a large a priori training set containing numerous examples of real and artificial data. Moreover, through the minimization of the l₁ norm, the presented approach builds sparse and compact manifolds that are highly representative, tolerant to common imaging disturbances (e.g. noise) and possess superior discrimination capabilities. In the last study of this thesis, the goal was to design a unified architecture for autonomous manipulation of unknown objects, which is capable of answering and addressing the constraints of all the following questions: ""What is the object?"", ""Where is it?"" and ""How to grasp it?"". The recognition problem is tackled from a shape-based perspective, whilst obtaining accurate detection decisions via a Bag-of-Visual Words classification scheme. Grasping points are found through an ontology-based knowledge acquisition where recognized objects inherit the grasping points assigned to the respective class. The presented ontologies include: a) object-class related data, b) pose-manifolds assigned to each instance of the object-class conceptual model and c) information about the grasping points of every trained instance. This work represents the first integrated research endeavor in concept ontologization focusing on the liaison between image understanding algorithms and the corresponding motor commands in the particular task of unknown object grasping.Καθώς το περιβάλλον εργασίας των ρομπότ γίνεται όλο και πιο πολύπλοκο, αναδύονται νέες προκλήσεις αναγνώρισης προτύπων. Εκτός από τη συμβατική ερώτηση «Ποιο είναι το αντικείμενο», στις μέρες μας, οι επιστήμονες καλούνται να απαντήσουν και σε άλλες εξίσου ενδιαφέρουσες ερωτήσεις, όπως «Πού είναι αυτό;» ή «Πώς να το πιάσω;». Οι τεχνικές αιχμής περιλαμβάνουν προηγμένες υπορουτίνες αναγνώρισης προτύπων που συνεργάζονται με αλγόριθμους μηχανικής μάθησης, έτσι ώστε να παρέχουν πορίσματα, σχετικά με την τρέχουσα κατάσταση του περιβάλλοντος, παρόμοια με εκείνα που εξάγει ο ανθρώπινος νους. Ένα ενδιαφέρον πρόβλημα στον τομέα της υπολογιστικής όρασης είναι η εκτίμηση του τρισδιάστατου διανύσματος θέσης ενός αντικειμένου, λόγω της πρακτικής ιδιαιτερότητας και της θεμελιώδους σημασίας του, σε διαδικασίες κατανόησης σκηνικών. Μια πληθώρα διαφορετικών εφαρμογών απαιτούν την ακριβή εκτίμηση της γεωμετρικής τοποθέτησης των αντικειμένων, ως προς ένα δεδομένο σύστημα αναφοράς, για την επίτευξη των στόχων τους. Στη ρομποτική, ο αυτόνομος χειρισμός αντικειμένων είναι   εφικτός μόνο σε περιπτώσεις, κατά τις οποίες οι έξι βαθμοί ελευθερίας του στόχου είναι γνωστοί. Παρά τις άφθονες ερευνητικές προσπάθειες και τα μέχρι στιγμής επιτεύγματα, δεν έχει ακόμη κατασκευαστεί ένα προηγμένο σύστημα όρασης, το οποίο να χαρακτηρίζεται από χαμηλή πολυπλοκότητα και επαρκείς δυνατότητες γενίκευσης. Η παρούσα διδακτορική διατριβή είναι εμπνευσμένη από τις αξιοσημείωτες έμφυτες ικανότητες των ανθρώπων, που εντοπίζονται στην εκτίμηση της σχετικής θέσης των αντικειμένων, δεδομένης μιας αρχικής υπόθεσης. Ανεξάρτητα από τις συνθήκες φωτισμού κα τις πιθανές κοινές  διαταραχές της όρασης, όπως η μερική επικάλυψη, η υπεροχή των ανθρώπων σε ότι αφορά στην ερμηνεία της τρισδιάστατης γεωμετρικής τοποθέτησης των αυθαίρετα τοποθετημένων αντικειμένων είναι αδιαμφισβήτητη. Από όλες τις εργασίες που πραγματοποιούνται από ανθρώπινα όντα, η πλειοψηφία έχει άμεση σχέση με το χειρισμό αντικειμένων, είτε κατά τη διάρκεια της βρώσης και της πόσης, είτε για τον χειρισμό ενός εργαλείου. Από την άλλη πλευρά, η υλοποίηση του χειρισμού στόχου από ένα ρομποτικό βραχίονα τοποθετεί αυστηρούς περιορισμούς ως προς την ακριβή ταχύτητα εκτέλεσης και την «φύση» των υπό εξέταση αντικειμένων. Σύγχρονες λύσεις για το πρόβλημα αυτό προϋποθέτουν υψηλές δυνατότητες αποθήκευσης ενώ παράλληλα αποτυγχάνουν να γενικεύσουν σε άγνωστους στόχους, δηλαδή σε αντικείμενα που δεν περιλαμβάνονται στο σύνολο εκπαίδευσης. Ο κύριος στόχος αυτής της διδακτορικής διατριβής είναι η ανάπτυξη καινοτόμων αλγόριθμων όρασης, για την επίλυση του προβλήματος εύρεσης του διανύσματος θέσης αντικειμένων, οι οποίοι θα μπορούν να υιοθετηθούν από ρομποτικές πλατφόρμες. Είναι προφανές ότι, οι αρχιτεκτονικές που θα σχεδιαστούν θα πρέπει σε πρώτο στάδιο, να αντιμετωπίσουν το πρόβλημα εκτίμησης της θέσης (δηλαδή κυρίως του υπολογισμού του βάθους) και σταδιακά, να υπολογίζουν τους υπόλοιπους τρεις περιστροφικούς βαθμούς ελευθερίας. Αρχικά, ερευνήθηκε η σχετική βιβλιογραφία προκειμένου να αποκαλυφθούν οι τρέχουσες τεχνικές στην αιχμή της τεχνολογίας και να αναδειχθούν τα ανοικτά ζητήματα έρευνας που πρέπει να αντιμετωπιστούν. Στη συνέχεια σχεδιάστηκαν και υλοποιήθηκαν, καινοτόμες προσεγγίσεις μοντελοποίησης πολύπτυχων μορφωμάτων, που αντιστοιχούν σε διανύσματα χαμηλών διαστάσεων και υψηλών διακριτικών ικανοτήτων. Τέλος, οι κατασκευασμένοι αλγόριθμοι τοποθετήθηκαν σε προηγμένες ρομποτικές πλατφόρμες για αυτόνομο χειρισμό αντικειμένων. Αρχικά αυτή η διατριβή, ερευνά το σχεδιασμό ενός απλού και εύκολου στην υλοποίηση συστήματος, για την εύρεση της τοποθεσίας ενός αντικειμένου σε ένα σκηνικό. Η μέθοδος που αναπτύχθηκε εδράζεται στην παρατήρηση ότι, τα χαρακτηριστικά γνωρίσματα, που εξάγονται από οποιοδήποτε αλγόριθμο δύο μερών (ανιχνευτή-περιγραφέα), αντιστοιχούν σε σημεία στην επιφάνεια του αντικειμένου, το κέντρο μάζας των οποίων σχετίζεται με εκείνο του αντικειμένου. Επομένως, εξάγοντας χαρακτηριστικά γνωρίσματα σε γνωστές θέσεις του αναζητούμενου αντικειμένου, είναι εφικτός ο υπολογισμός της απόστασής του από την κάμερα. Σε σύγκριση με τις σύγχρονες λύσεις στο πρόβλημα της εκτίμησης βάθους, η προσέγγιση που προτείνεται απαιτεί μικρό υπολογιστικό κόστος, ενώ χρησιμοποιεί μόνο ένα αισθητήρα όρασης, σε αντίθεση με αρχιτεκτονικές που βασίζουν τη λειτουργία τους σε στερεοσκοπικές κάμερες. Στη συνέχεια παρουσιάζεται ένα πλαίσιο το οποίο, βασισμένο στις ιδιότητες των νευρωνικών δικτύων, είναι σε θέση να υπολογίζει τους 3 περιστροφικούς βαθμούς ελευθερίας οποιουδήποτε αντικειμένου. Το δίκτυο χρησιμοποιεί πολυάριθμα δεδομένα εκπαίδευσης, που περιέχονται σε διαθέσιμες βάσεις δεδομένων, ενώ τα χαρακτηριστικά γνωρίσματα εξάγονται μέσω του αλγορίθμου Fuzzy c-means. Λαμβάνοντας υπόψη ότι (α) ένα αντικείμενο φωτογραφισμένο υπό διαφορετικές γωνίες μπορεί να προβληθεί σε ένα συγκεκριμένο υποχώρο και (β) εντελώς διαφορετικά αντικείμενα φωτογραφισμένα υπό παρόμοιες οπτικές γωνίες μοιράζονται ίδια διανύσματα θέσης, υλοποιήθηκε ένα εξελιγμένο πλαίσιο, ικανό τόσο να αναγνωρίζει στόχους όσο και να υπολογίζει τη γεωμετρική τους τοποθέτηση. Αυτό περιλαμβάνει μια διαδικασία μοντελοποίησης πολύπτυχων μορφωμάτων βασισμένη στις ιδιότητες της αμοιβαίας πληροφορίας και σε μια αρχιτεκτονική μερών, αντίστοιχα. Η προτεινόμενη μέθοδος αναγνώρισης προτύπων ενσωματώνει μια τροποποιημένη προσέγγιση μιας γνωστής τεχνικής μείωσης διαστάσεων, που κατασκευάζει έναν πίνακα ομοιότητας με βάση την αμοιβαία πληροφορία μεταξύ των αντικειμένων και στη συνέχεια αναζητεί ένα αντιπροσωπευτικό διάνυσμα χαμηλών διαστάσεων, που να διατηρεί την τοπική δομή των αντικειμένων. Μόλις τα δεδομένα προβληθούν στον υποχώρο, ένας ταξινομητής βασισμένος σε μηχανές υποστήριξης διανυσμάτων παρέχει ακριβή πορίσματα αναγνώρισης. Όσον αφορά στο υποσύστημα τρισδιάστατου εντοπισμού θέσης, δημιουργούνται πολύπτυχα μορφώματα τα οποία, παρόλο που είναι χαμηλής διάστασης, παρέχουν αξιοσημείωτες διακριτικές ικανότητες. Η προτεινόμενη μέθοδος συνιστά μια εναλλακτική προσέγγιση στο πρόβλημα του υπολογισμού θέσης αντικειμένων καθώς είναι ικανή να προσφέρει αυξημένη γενίκευση, εν αντιθέσει με άλλες τεχνικές του τομέα που περιορίζονται στην εκτίμηση των γεωμετρικών τοποθετήσεων αυτοκινήτων. Η μέθοδος Αραιών Πολύπτυχων Μορφωμάτων Λαβής στοχεύει στην επίλυση του προβλήματος του χειρισμού άγνωστων αντικειμένων, με την ενοποίηση μορφωμάτων θέσης και σημείων λαβής σε ένα συνεκτικό πλαίσιο. Αντίθετα με τα σύγχρονα συστήματα που απαιτούν εκτεταμένη εποπτεία και αποθήκευση πολυάριθμων εικόνων των αντικειμένων, δίνεται ιδιαίτερη έμφαση στην παροχή μιας λύσης με μεγάλες δυνατότητες γενίκευσης, βασισμένη στις ιδιότητες της εκμάθησης χωρίς επίβλεψη. Επιπλέον, τα οπτικά στοιχεία, που είναι διαθέσιμα, υφίστανται επεξεργασία, τέτοια ώστε η προβολή τους στους αντίστοιχους υποχώρους να είναι αραιή, συμπαγής και επαρκώς αντιπροσωπευτική. Τα πολύπτυχα μορφώματα χειρισμού εξαρτώνται από μια αρχιτεκτονική μερών, η οποία παρουσιάζεται για πρώτη φορά και σε αντίθεση με   προηγούμενες τεχνικές, παρακάμπτει τη διαδικασία επιλογής χαρακτηριστικών. Συγκεκριμένα, χρησιμοποιώντας τεχνικές εκμάθησης χωρίς επίβλεψη, η αρχιτεκτονική μερών είναι σε θέση να εξάγει νέα γνωρίσματα, που φέρουν τοπικές πληροφορίες γειτνίασης και υφής. Συγκρινόμενη με αλγορίθμους, που ανήκουν στην αιχμή της τεχνολογίας, η μέθοδος προσφέρει μεγαλύτερη ικανότητα γενίκευσης, κυρίως λόγω του ακριβούς υπολογισμού των παραμέτρων της προτεινόμενης συνάρτησης και της εκπαίδευσής της με μεγάλο αριθμό δεδομένων. Επιπροσθέτως, μέσω της ελαχιστοποίησης της L₁ νόρμας, δημιουργεί αραιά και συμπαγή πολύπτυχα μορφώματα, τα οποία είναι ανεκτικά σε κοινές διαταραχές απεικόνισης (π.χ. θόρυβος) και κατέχουν ανώτερες διακριτικές ικανότητες. Στην τελευταία μέθοδο της παρούσας διατριβής, δίνεται ιδιαίτερη έμφαση στο σχεδιασμό μιας ενοποιημένης αρχιτεκτονικής για αυτόνομο χειρισμό άγνωστων αντικειμένων, η οποία είναι σε θέση να απαντήσει στα ακόλουθα ερωτήματα: «Ποιο είναι το αντικείμενο;», «Πού είναι;» και «Πώς να το πιάσω;». Το πρόβλημα της αναγνώρισης ερευνάται υπό το πρίσμα των μορφολογικών ιδιοτήτων των στόχων, ενώ ταυτόχρονα, εξάγονται ακριβή πορίσματα για την ταυτότητα των αντικειμένων με τη χρήση του ταξινομητή Bag-of-Visual Words. Τα σημεία λαβής υπολογίζονται με τη χρήση οντολογιών, σύμφωνα με τις οποίες τα αντικείμενα που αναγνωρίζονται, κληρονομούν σημεία λαβής παρόμοια εκείνων της κλάσης στην οποία ανήκουν. Οι εν λόγω οντολογίες περιλαμβάνουν: α) δεδομένα που σχετίζονται με την κλάση του αντικειμένου, β) πολύπτυχα μορφώματα γεωμετρικής τοποθέτησης για κάθε στιγμιότυπο του αντικειμένου και γ) πληροφορίες σχετικά με τα σημεία λαβής του κάθε παραδείγματος εκπαίδευσης. Ο προτεινόμενος αλγόριθμος συνιστά την πρώτη ολοκληρωμένη ερευνητική προσπάθεια αξιοποίησης των δυνατοτήτων των οντολογιών και επικεντρώνεται στη δημιουργία σχέσεων μεταξύ τεχνικών κατανόησης εικόνας και μεθόδων αυτόνομου χειρισμού αντικειμένων",Νέες τεχνικές αναγνώρισης προτύπων για συστήματα τεχνητής όρασης,,'National Documentation Centre (EKT)',10.12681/eadd/29283,,core
33078043,2014-01-01T00:00:00,"BACKGROUND:

Robotics-assisted tilt table technology was introduced for early rehabilitation of neurological patients. It provides cyclical stepping movement and physiological loading of the legs. The aim of the present study was to assess the feasibility of this type of device for peak cardiopulmonary performance testing using able-bodied subjects.

METHODS:

A robotics-assisted tilt table was augmented with force sensors in the thigh cuffs and a work rate estimation algorithm. A custom visual feedback system was employed to guide the subjects' work rate and to provide real time feedback of actual work rate. Feasibility assessment focused on: (i) implementation (technical feasibility), and (ii) responsiveness (was there a measurable, high-level cardiopulmonary reaction?). For responsiveness testing, each subject carried out an incremental exercise test to the limit of functional capacity with a work rate increment of 5 W/min in female subjects and 8 W/min in males.

RESULTS:

11 able-bodied subjects were included (9 male, 2 female; age 29.6 ± 7.1 years: mean ± SD). Resting oxygen uptake (O_{2}) was 4.6 ± 0.7 mL/min/kg and O_{2}peak was 32.4 ± 5.1 mL/min/kg; this mean O_{2}peak was 81.1% of the predicted peak value for cycle ergometry. Peak heart rate (HRpeak) was 177.5 ± 9.7 beats/min; all subjects reached at least 85% of their predicted HRpeak value. Respiratory exchange ratio (RER) at O_{2}peak was 1.02 ± 0.07. Peak work rate) was 61.3 ± 15.1 W. All subjects reported a Borg CR10 value for exertion and leg fatigue of 7 or more.

CONCLUSIONS:

The robotics-assisted tilt table is deemed feasible for peak cardiopulmonary performance testing: the approach was found to be technically implementable and substantial cardiopulmonary responses were observed. Further testing in neurologically-impaired subjects is warranted",Cardiopulmonary performance testing using a robotics-assisted tilt table: Feasibility assessment in able-bodied subjects,,'IOS Press',10.3233/THC-140783,,core
41069910,2014-09-03T00:00:00,"This research focuses on the development of a realtime intelligent facial emotion recognition system for a humanoid robot. In our system, Facial Action Coding System is used to guide the automatic analysis of emotional facial behaviours. The work includes both an upper and a lower facial Action Units (AU) analyser. The upper facial analyser is able to recognise six AUs including Inner and Outer Brow Raiser, Upper Lid Raiser etc, while the lower facial analyser is able to detect eleven AUs including Upper Lip Raiser, Lip Corner Puller, Chin Raiser, etc. Both of the upper and lower analysers are implemented using feedforward Neural Networks (NN). The work also further decodes six basic emotions from the recognised AUs. Two types of facial emotion recognisers are implemented, NN-based and multi-class Support Vector Machine (SVM) based. The NN-based facial emotion recogniser with the above recognised AUs as inputs performs robustly and efficiently. The Multi-class SVM with the radial basis function kernel enables the robot to outperform the NN-based emotion recogniser in real-time posed facial emotion detection tasks for diverse testing subjects",Intelligent Facial Action and emotion recognition for humanoid robots,,'Institute of Electrical and Electronics Engineers (IEEE)',10.1109/IJCNN.2014.6889647,,core
23907164,03/02/2014,"Abstract — In this paper we show how simple laterally interacting computational entities, i.e. neurons, can be guided by a selectionist process into spatial patterns that show interesting and purposeful dynamics with regard to a particular utility measure. In other words, if a suitable population of laterally interacting mobile entities exist, it is possible to gradually arrange the entities into a spatial pattern that exhibits the desired dynamics. In this paper, the selectionist process is implemented with the Stochastic Evolutionary Neuron Migration Process (SENMP) and approach is used to evolve Dynamic Recurrent Neural Networks (DRNNs) for controlling complex dynamic systems such as autonomous mobile robots, for example. The feasibility and advantages of the approach are demonstrated by evolving neural controllers for solving a non-Markovian double pole balancing problem. In addition, we have earlier used SENMP to evolve navigation behaviors for mobile robots in complex simulated and real environments. I",Dynamics from patterns: Creating neural controllers with SENMP,,,,,core
21901992,15/06/2012,"Career Objectives and Research Interests My long term career objective is to work at the intersection of basic research and real-world applications, where I believe I can have the largest impact. I am interested in a variety of topics regarding the application of AI planning and knowledge representation techniques to real-world problems. These include planning in highly dynamic, “open-world ” environments, integrated planning and execution, execution monitoring, diagnosis, the design of robust plans, the integration of domain-expert knowledge into planning and control, and the application of relevance-based, symbolic reasoning techniques to verification and control of deployed model-based systems. Applications I have worked on or am interested in include autonomous robots, workflows, semantic web, ubiquitous computing, design automation, and manufacturing analysis. Appointments Research Scientist",,,,,,core
22549884,12/07/2013,"Simulation is often used in research and industry as a low cost, high efficiency alternative to real model testing. Simulation has also been used to develop and test powerful learning algorithms. However, parameters learned in simulation often do not translate directly to the application, especially because heavy optimization in simulation has been observed to exploit the inevitable simulator simplifications, thus creating a gap between simulation and application that reduces the utility of learning in simulation. This paper introduces Grounded Simulation Learning (GSL), an iterative optimization framework for speeding up robot learning using an imperfect simulator. In GSL, a behavior is developed on a robot and then repeatedly: 1) the behavior is optimized in simulation; 2) the resulting behavior is tested on the real robot and compared to the expected results from simulation, and 3) the simulator is modified, using a machine-learning approach to come closer in line with reality. This approach is fully implemented and validated on the task of learning to walk using an Aldebaran Nao humanoid robot. Starting from a set of stable, hand-coded walk parameters, four iterations of this three-step optimization loop led to more than a 25 % increase in the robot’s walking speed",Humanoid Robots Learning to Walk Faster: From the Real World to Simulation and Back,,,,,core
22662157,25/07/2013,"Abstract. Robot games have been proposed as a way to motivate people to do physical exercises while playing. Although this area is very new, both commercial and scientific robot games have been developed mainly based on interaction with a single user and a robot. The goal of this paper is to describe a generic software framework which can be used to create games where multiple players can play against a mobile robot. The paper shows how an adaptive AI system (D2) developed for real-time strategy (RTS) computer games can be successfully applied in a robotics context using the robotics control framework Player/Stage. D2 is based on Case-Based Planning which learns from demonstration. Using the proposed framework, the paper shows how a robot learns a strategy for an implementation of a simple game",A Software Framework for Multi Player Robot Games,,,,,core
22966348,20/08/2013,"Abstract. Teleoperation of remote robotic systems over time delays in the range of 2-10 seconds poses a unique set of challenges. In the context of a supervisory control system for the JSC Robonaut humanoid robot, we have developed an “intelligent assistant”, called the Task Level Assistant (TLA), that integrates an AI planner (JSHOP2) with execution monitoring of the state of both the human supervisor and the remote robot. The TLA reasons simultaneously about the world state on both sides of the time delay, which represents a novel application of this technology. The purpose of the assistant is to provide near real-time advice to the human supervisor about current and future activities, derived from a sequence of high-level goals to be achieved. To do this, the assistant must simultaneously monitor and react to various data sources, including actions taken by the supervisor who is issuing commands to the robot (e.g. with a data glove), actions taken by the robot as reported over the time delay, the environment of the robot, as currently perceived over the time delay, and the current sequence of goals. We have developed a “leader/follower ” software architecture to handle the dual time-shifted streams of feedback from executing tasks. In this paper we describe the integrated planner and its executive, and how it operates in normal and anomaly situations. 1",Integrating AI Planning for Telepresence with Time Delays,,,,,core
19509902,2012-07-13T00:00:00,"Efficient localisation is a highly desirable property for an autonomous navigation system. Weightless neural networks offer a real-time approach to robotics applications by reducing hardware and software requirements for pattern recognition techniques. Such networks offer the potential for objects, structures, routes and locations to be easily identified and maps constructed from fused limited sensor data as information becomes available. We show that in the absence of concise and complex information, localisation can be obtained using simple algorithms from data with inherent uncertainties using a combination of Genetic Algorithm techniques applied to a Weightless Neural Architecture",Highly efficient Localisation utilising Weightless neural systems,https://core.ac.uk/download/19509902.pdf,ESANN,,,core
250576871,2014-10-20T00:00:00,"International audienceMany robotic projects use simulation as a faster and easier way to develop, evaluate and validate software components compared with on-board real world settings. In the human-robot interaction field, some recent works have attempted to integrate humans in the simulation loop. In this paper we investigate how such kind of robotic simulation software can be used to provide a dynamic and interactive environment to both collect a multimodal situated dialogue corpus and to perform an efficient reinforcement learning-based dialogue management optimisation procedure. Our proposition is illustrated by a preliminary experiment involving real users in a Pick-Place-Carry task for which encouraging results are obtained",Simulating Human-Robot Interactions for Dialogue Strategy Learning,,'Springer Science and Business Media LLC',10.1007/978-3-319-11900-7_6,,core
23318698,10/10/2013,"Abstract — Data association is an essential problem in simultaneous localization and mapping. It is hard to solve correctly, especially in ambiguous environments. We consider a scenario where the robot can ease the data association problem by deploying a limited number of uniquely identifiable artificial landmarks along its path and use them afterwards as fixed anchors. Obviously, the choice of the positions where the robot should drop these markers is crucial as poor choices might prevent the robot from establishing accurate data associations. In this paper, we present a novel approach for learning when to drop the landmarks so as to optimize the data association performance. We use Monte Carlo reinforcement learning for computing an optimal policy and apply a statistical convergence test to decide if the policy is converged and the learning process can be stopped. Extensive experiments also carried out with a real robot demonstrate that the data association performance using landmarks deployed according to our learned policies is significantly higher compared to other strategies. I",Deploying Artificial Landmarks to Foster Data Association in Simultaneous Localization and Mapping,,,10.1109/icra.2013.6631325,,core
44366987,2013-01-01T00:00:00,"This book presents and develops new reinforcement learning methods that enable fast and robust learning on robots in real-time. Robots have the potential to solve many problems in society, because of their ability to work in dangerous places doing necessary jobs that no one wants or is able to do. One barrier to their widespread deployment is that they are mainly limited to tasks where it is possible to hand-program behaviors for every situation that may be encountered. For robots to meet their potential, they need methods that enable them to learn and adapt to novel situations that they were not programmed for. Reinforcement learning (RL) is a paradigm for learning sequential decision making processes and could solve the problems of learning and adaptation on robots. This book identifies four key challenges that must be addressed for an RL algorithm to be practical for robotic control tasks. These RL for Robotics Challenges are: 1) it must learn in very few samples; 2) it must learn in domains with continuous state features; 3) it must handle sensor and/or actuator delays; and 4) it should continually select actions in real time. This book focuses on addressing all four of these challenges. In particular, this book is focused on time-constrained domains where the first challenge is critically important. In these domains, the agent’s lifetime is not long enough for it to explore the domains thoroughly, and it must learn in very few samples",TEXPLORE: temporal difference reinforcement learning for robots and time-constrained domains,,'Springer Science and Business Media LLC',10.1007/978-3-319-01168-4,,core
9660956,2012-12-01T00:00:00,"Autonomous Learning Systems is the result of over a decade of focused research and studies in this emerging area which spans a number of well-known and well-established disciplines that include machine learning, system identification, data mining, fuzzy logic, neural networks, neuro-fuzzy systems, control theory and pattern recognition. The evolution of these systems has been both industry-driven with an increasing demand from sectors such as defence and security, aerospace and advanced process industries, bio-medicine and intelligent transportation, as well as research-driven – there is a strong trend of innovation of all of the above well-established research disciplines that is linked to their on-line and real-time application; their adaptability and flexibility. Providing an introduction to the key technologies, detailed technical explanations of the methodology, and an illustration of the practical relevance of the approach with a wide range of applications, this book addresses the challenges of autonomous learning systems with a systematic approach that lays the foundations for a fast growing area of research that will underpin a range of technological applications vital to both industry and society. Key features: • Presents the subject systematically from explaining the fundamentals to illustrating the proposed approach with numerous applications. • Covers a wide range of applications in fields including unmanned vehicles/robotics, oil refineries, chemical industry, evolving user behaviour and activity recognition. • Reviews traditional fields including clustering, classification, control, fault detection and anomaly detection, filtering and estimation through the prism of evolving and autonomously learning mechanisms • Accompanied by a website hosting additional material, including the software toolbox and lecture notes Autonomous Learning Systems provides a ‘one-stop shop’ on the subject for academics, students, researchers and practicing engineers. It is also a valuable reference for Government agencies and software developers",Autonomous Learning Systems:From Data to Knowledge in Real Time,,John Willey and Sons,,,core
22175078,17/01/2013,"Abstract—In 2010,we proposed CRFNFP[1] algorithm to enhance long-range terrain perception for outdoor robots through the integration of both appearance features and spatial contexts. And our preliminary simulation results indicated the superiority of CRFNFP over other existing approaches in terms of accuracy, robustness and adaptability to dynamic unstructured outdoor environments. In this paper, we further study on the comparison experiments for navigation behaviors of robotic systems with different scene perception algorithms in real outdoor scenes. We implemented 3 robotic systems and repeated the running jobs under various conditions. We also defined 3 criterion to facilitate comparison for all systems: Obstacle Response Distance (ORD), Time to Finish Job (TFJ), and Distance of the Whole Run (DWR). The comparative experiments indicate that, the CRFNFP-based navigating system outperforms traditional localmap-based navigating systems in terms of all criterions. And the results also show that the CRFNFP algorithm does enhance the long-range perception for mobile robots and helps planning more efficient paths for the navigation. Keywords- autonomous nagivation; stereo vision; machine learning; conditional random fields; scene analysis. I",Experimental Validation for CRFNFP Algorithm,,,,,core
76044404,2012-01-01T00:00:00,"Abstract. To design robots or embodied conversational agents that can
accurately display facial expressions indicating an emotional state, we need
technology to produce those facial expressions, and research that investigates
the relationship between those technologies and human social perception of
those artificial faces. Our starting point is assessing human perception of core
facial information: Moving dots representing the facial landmarks, i.e., the
locations and movements of the crucial parts of a face. Earlier research
suggested that participants can relatively accurately identity facial expressions
when all they can see of a real human full face are moving white painted dots
representing the facial landmarks (although less accurate than recognizing full
faces). In the current study we investigated the accuracy of recognition of
emotions expressed by comparable facial landmarks (compared to accuracy of
recognition of emotions expressed by full faces), but now used face-tracking
software to produce the facial landmarks. In line with earlier findings, results
suggested that participants could accurately identify emotions expressed by the
facial landmarks (though less accurately than those expressed by full faces).
Thereby, these results provide a starting point for further research on the
fundamental characteristics of technology (AI methods) producing facial
emotional expressions and their evaluation by human users",How to make a robot smile? Perception of emotional expressions from digitally-extracted facial landmark configurations,,'Springer Fachmedien Wiesbaden GmbH',10.1007/978-3-642-34103-8_3,,core
100217941,08/12/2014,"ABSTRACT: Technological and conceptual advances in fields such as artificial intelligence, robotics, and material science have enabled robotic architectural environments to be implemented and tested in the last decade in virtual and physical prototypes. These prototypes are incorporating sensing-actuating mechanisms that enable interaction with their users and surroundings in real-time. While these prototypes obviously point towards a paradigm shift from inanimate towards animate architecture, they do not operate at building but at building component scale and do not address socio-economical or environmental aspects that affect architecture and society at large. This paper, on the one hand, critically discusses robotic prototypes built in the last decade at Delft University of Technology, on the other hand, it proposes a framework for future research envisioning robotic environments, as resizable, able to spatially expand or contract as well as move or be moved as needed. Such reconfigurable environments aim to validate the assumption that robotics incorporated in architecture improve efficiency of use due to multiple use of built space in condensed timeframes, while at the same time they advance technology for distributed autonomous robotic systems exhibiting collective behavior as well as test their application to sustainable architecture",ROBOTIC ENVIRONMENTS,,,,,core
100265088,10/12/2014,"The relatively new field of artificial life attempts to study and understand biological life by synthesizing artificial life forms. To paraphrase Chris Langton, the founder of the field, the goal of artificial life is to “model life as it could be so as to understand life as we know it. ” Arti-ficial life is a very broad discipline which spans such diverse topics as artificial evolution, artificial ecosys-tems, artificial morphogenesis, molecular evolution, and many more. Langton offers a nice overview of the different research questions studied by the discipline [6]. Artificial life shares with artificial intelligence (AI) its interest in synthesizing adaptive autonomous agents. Autonomous agents are computational systems that inhabit some complex, dynamic environment, sense and act autonomously in this environment, and by doing so realize a set of goals or tasks for which they are designed. The goal of building an autonomous agent is as old as the field of AI itself. The artificial life community has initiated a radically different approach to this goal, which focuses on fast, reactive behavior, rather than on knowledge and reasoning, as well as on adaptation and learning. Its approach is largely inspired by biology, and more specifically the field of ethology, which attempts to understand the mechanisms animals use to demonstrate adaptive and successful behavior. Autonomous agents can take many different forms, depending on the nature of the environment they inhabit. If the environment is the real phys-ical environment, then the agent takes the form of an autonomous robot. Alternatively, one can build 2D or 3D animated agents that inhabit simulat-ed physical environments. Finally, so-called knowbots, software agents or interface agents are disembodied entities that inhabit the digital world of computers and computer networks [8]. There are obvious applications for all these types of agents. For example, autonomous robots have been built for surveillance, exploration, and other tasks in environments that are unac-cessible or dangerous for human beings. There is a long tradition of build",Entertainment: Lifelike Autonomous Agents,,,,,core
158709724,2012,"Abstract. To design robots or embodied conversational agents that can accurately display facial expressions indicating an emotional state, we need technology to produce those facial expressions, and research that investigates the relationship between those technologies and human social perception of those artificial faces. Our starting point is assessing human perception of core facial information: Moving dots representing the facial landmarks, i.e., the locations and movements of the crucial parts of a face. Earlier research suggested that participants can relatively accurately identity facial expressions when all they can see of a real human full face are moving white painted dots representing the facial landmarks (although less accurate than recognizing full faces). In the current study we investigated the accuracy of recognition of emotions expressed by comparable facial landmarks (compared to accuracy of recognition of emotions expressed by full faces), but now used face-tracking software to produce the facial landmarks. In line with earlier findings, results suggested that participants could accurately identify emotions expressed by the facial landmarks (though less accurately than those expressed by full faces). Thereby, these results provide a starting point for further research on the fundamental characteristics of technology (AI methods) producing facial emotional expressions and their evaluation by human users",How to make a robot smile? Perception of emotional expressions from digitally-extracted facial landmark configurations,,Springer,,,core
23185457,21/09/2013,"the date of receipt and acceptance should be inserted later Abstract We present a framework for the machine learning of denotational concept semantics using a simple form of symbolic interaction of machines with human users. The capability of software agents and robots to learn how to communicate verbally with human users would obviously be highly useful in several real-world applications, and our framework is meant to provide a further step towards this goal. Whereas the large majority of existing approaches to the machine learning of word sense and other language aspects focuses on learning using text corpora, our framework allows for the interactive learning of concepts in a dialog of human and agent, using an approach in the area of Relational Reinforcement Learning. Such an approach has a wide range of possible applications, e.g., the interactive acquisition of semantic categories for the Semantic Web, Human-Computer Interaction, (interactive",Noname manuscript No. (will be inserted by the editor) Interactive Relational Reinforcement Learning of Concept Semantics,,,,,core
21839507,14/05/2012,"In recent years, the advances in robotics have allowed for robots to venture into places too dangerous for humans. Unfortunately, the terrain in which these robots are being deployed may not be known by humans in advance, making it difficult to create motion programs robust enough to handle all scenarios that the robot may encounter. For this reason, research is being done to add learning capabilities to improve the robot’s ability to adapt to its environment. Reinforcement learning is well suited for these robot domains because often the desired outcome is known, but the best way to achieve this outcome is unknown. In a real world domain, a reinforcement-learning agent has to learn a great deal from experience. Therefore, it must be sample-size efficient. To do so, it must balance the amount of exploration that is needed to properly model the environment with the need to use the information that it has already obtained to complete its original task. In robot domains, the exploration process is especially costly in both time and energy. Therefore, it is important to make the bes",Perception-Based Generalization in Model-Based Reinforcement Learning,,,,,core
100600196,23/12/2014,"Reinforcement learning is an important family of algo-rithms that have been extremely effective in fields such as robotics, economics, and artificial intelligence. Current al-gorithms become increasingly expensive as the state space of the problem increases in size. Additionally, computer ar-chitectures are becoming increasingly parallelized, and ex-isting algorithms need to be reworked to fit within this par-allelization paradigm. We will present a general framework for parallelizing such algorithms. Many reinforcement learning problems (such as robot navigation) have state-spaces that correspond to real, phys-ical spaces, in which states are physical locations and ac-tions transition between neighboring locations. We will demonstrate how problems of this nature can be decom-posed into smaller subproblems that can solved in parallel. We built two implementations of our framework, a MATLAB implementation for rapid prototyping (in which all paral-lelism is simulated) and a Java multicore implementation for which our reported runtimes are actual parallel runtime. Our framework improves the runtime of policy iteration by up to 3 × on an simulated 8 core processor (MATLAB), and up to 203 × on an actual 8 core processor (Java). 1",Parallelizing Reinforcement Learning,,,,,core
100103883,01/12/2014,"Abstract—This paper presents insight to ideas and the current state of the project SyRoTek- System for a robotic e-learning that aims to create a platform for students ’ practical verification of gained knowledge in the fields of Robotics and Artificial Intelligence. A set of real mobile robots is being developed in order to provide remote access to real hardware for enrolled students. The advantage of the real system over a pure virtual simulated environment is in realistic confrontation with noise and uncertainty that is an indivisible part of the real world. In such a system, students can acquire in deep understanding of main studied principles in an attractive form, as students (especially future engineers) like to control real things. On the other side, this can be a potential issue if an accessibility to the system have to be guaranteed in 24/7 mode. In SyRoTek, robots are designed with special attention to long-term and heavy duty usage. Moreover, safety mechanisms are realized in several layers of the proposed software architecture that provide access to robot control and sensors. In addition, support for semi-autonomous evaluation of students ’ solution of their assignments is a part of the system. Index Terms—artificial intelligence, robotics, e-learning I",SyRoTek- A Robotic System for Education,,,,,core
55802668,2012-01-01T00:00:00,"A biologically inspired navigation system for the mobile rat-like robot named Psikharpax is presented, allowing for self-localization and autonomous navigation in an initially unknown environment. The ability of parts of the model (e. g. the strategy selection mechanism) to reproduce rat behavioral data in various maze tasks has been validated before in simulations. But the capacity of the model to work on a real robot platform had not been tested. This paper presents our work on the implementation on the Psikharpax robot of two independent navigation strategies (a place-based planning strategy and a cue-guided taxon strategy) and a strategy selection meta-controller. We show how our robot can memorize which was the optimal strategy in each situation, by means of a reinforcement learning algorithm. Moreover, a context detector enables the controller to quickly adapt to changes in the environment-recognized as new contexts-and to restore previously acquired strategy preferences when a previously experienced context is recognized. This produces adaptivity closer to rat behavioral performance and constitutes a computational proposition of the role of the rat prefrontal cortex in strategy shifting. Moreover, such a brain-inspired meta-controller may provide an advancement for learning architectures in robotics",A biologically inspired meta-control navigation system for the Psikharpax rat robot,https://core.ac.uk/download/55802668.pdf,'IOP Publishing',10.1088/1748-3182/7/2/025009,,core
23453394,28/11/2013,"Abstract—This paper studies the impact of interfaces, allowing nonexpert users to efficiently and intuitively teach a robot to recognize new visual objects. We present challenges that need to be addressed for real-world deployment of robots capable of learning new visual objects in interaction with everyday users. We argue that in addition to robust machine learning and computer vision methods, well-designed interfaces are crucial for learning efficiency. In particular, we argue that interfaces can be key in helping nonexpert users to collect good learning examples and, thus, improve the performance of the overall learning system. Then, we present four alternative human–robot interfaces: Three are based on the use of a mediating artifact (smartphone, wiimote, wiimote and laser), and one is based on natural human gestures (with a Wizard-of-Oz recognition system). These interfaces mainly vary in the kind of feedback provided to the user, allowing him to understand mor","This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination. IEEE TRANSACTIONS ON ROBOTICS 1 The Impact of Human–Robot Interfaces on the Learning of Visual Objects",,,,,core
211824079,2014-01-01T00:00:00,"Lemaignan S, Hanheide M, Karg M, et al. Simulation and HRI Recent Perspectives with the MORSE Simulator. In: Brugali D, Broenink JF, Kroeger T, MacDonald B, eds. Simulation, Modeling, and Programming for Autonomous Robots. 4th International Conference, proceedings. Lecture Notes in Artificial Intelligence (LNAI). Vol 8810. Cham: Springer International Publishing;  2014.Simulation in robotics is often a love-hate relationship: while simulators do save us a lot of time and effort compared to regular deployment of complex software architectures on complex hardware, simulators are also known to evade many (if not most) of the real issues that robots need to manage when they enter the real world. Because humans are the paragon of dynamic, unpredictable, complex, real world entities, simulation of human-robot interactions may look condemn to fail, or, in the best case, to be mostly useless. This collective article reports on five independent applications of the MORSE simulator in the field of human-robot interaction: It appears that simulation is already useful, if not essential, to successfully carry out research in the field of HRI, and sometimes in scenarios we do not anticipate",Simulation and HRI Recent Perspectives with the MORSE Simulator,https://core.ac.uk/download/211824079.pdf,Springer International Publishing,,,core
225230726,,"碩士[[abstract]]機器人技術與產業已是世界各國列為前瞻優先發展的新科技產業，全世界在機器人各項領域研發中均投入了相當多的人力與成本，尤其互動式娛樂機器人因對人們的食、衣、住、行、娛樂等方面生活更是密切，其顯得更是重要的研究主題。本論文旨在設計與實現一互動式娛樂機器人可以陪伴使用者玩四子棋遊戲，其具有遊戲人工智慧演算法可以針對使用者所下之棋局演算出最合適之下棋位置，使己方的棋局可以保持優勢，同時讓使用者的棋局處於劣勢，藉此達到獲勝目的。整個機器人包含機構主體、伺服馬達模組、光感測模組、觸碰開關模組、伺服介面卡、核心伺服控制卡等部分，其中本論文選擇具有ARM Cortex-M3內核之LM3S1138為控制核心，撰寫伺服馬達定位控制演算法，A/D轉換與GPIO等介面韌體程式和遊戲人工智慧演算法，最後，經由實際系統整合測試後可發現所提出之四子棋互動式娛樂機器人可以陪伴使用者達到娛樂之效果。[[abstract]]Robotic industries and techniques have become a new primary development in the information technology around the world. Many counties spend lots of manpower and cost founding on the robotic research and development, especially in the topic of the interactive entertainment robot design. The goal of this thesis is to design and implementation an interactive connect-four robot which the user can play together. The proposed robot is composed of a main part, a servo motor module, a light sensor module, a touch button module, and a servo card. It has a game artificial intelligence which can detect the player''s moves and can find the best strategy to defeat the player. In the servo card, a 32-bits microcontroller (LM3S1138) which has a ARM cortex-M3 kernel is used to design the automation controller algorithm and game artificial intelligence. Finally, the proposed connect-four robot shows that it brings lots of entertainment to player throughout some real-time system tests.[[tableofcontents]]第一章 緒論 1
1.1 研究目的 1
1.2 伺服控制 5
1.3 論文成果 7
第二章 系統架構介紹 8
2.1 ARM背景或發展 8
2.2 四子棋互動式娛樂機器人 10
2.2.1 伺服卡介紹 12
2.2.2 模糊控制 17
2.2.3 其他模組介紹 24
第三章 四子棋遊戲人工智慧 27
3.1 人工智慧 27
3.2 四子棋介紹 29
3.3 四子棋演算法 30
第四章 實驗結果 37
4.1 玩家先下棋之實驗結果 37
4.2 人工智慧先下棋之結果 46
第五章 結論 52
5.1 結論	52
5.2 未來研究 53
參考文獻 54

圖目錄
圖1.1 AIBO小型機器人 3
圖1.2 機器海豹PARO 3
圖1.3 Robii娛樂機器人 4
圖1.4 ASIMO 5
圖2.1 系統方塊 11
圖2.2 實驗室所設計的伺服卡 12
圖2.3 HCTL2032解碼IC[18] 14
圖2.4 暫存器簡介[18] 14
圖2.5 角度計算程式碼 14
圖2.6 馬達驅動IC LB1836[19] 15
圖2.7 控制馬達原理圖 16
圖2.8 一維模糊控制器 18
圖2.9 一維模糊控制器歸屬函數 19
圖2.10 二維模糊控制器 20
圖2.11 二維模糊控制器歸屬函數 21
圖2.12 模糊滑動模式控制器 22
圖2.13 模糊滑動模式控制器歸屬函數 23
圖2.14 四子棋整體架構 25
圖2.15 光感測AD模組 25
圖3.1 屏風式四子棋 29
圖3.2 四子棋橫直線、豎直線、棋斜線 30
圖3.3 四子棋棋盤 31
圖3.4 四子棋棋盤情況一 32
圖3.5 四子棋棋盤情況二 32
圖3.6 四子棋棋盤情況三 33
圖3.7 軟體流程圖 34
圖3.8 偵測觸碰開關模組 35
圖4.1 使用者按下觸碰開關模組	47

表目錄
表2.1 一維模糊控制器模糊控制表 19
表2.2 二維模糊控制器模糊控制表 21
表2.3 模糊滑動模式控制表 23[[note]]學號: 600470065, 學年度: 10",[[alternative]]Design and implementation of an interactive robot for a connect four game,,,,,core
297623281,,"Artificial intelligence is a branch of technology that is related to the development of
intelligent machine and software. A system that is able to perceive its environment and take actions
according to the situation maximizes its chances of success to perform the action. There are many
researches and studies that have been done to create this non-possible dream, which is to create a
machine that is able to replicate real life. This project aims to create an interactive robot by creating a
robot that is able to perceive the command from people and act according to the command by using the
foundation of artificial intelligence and image processing, and kept as a physical pet",Development of an interactive robotic pet dog,,"Journal of Advanced Research in Applied Mechanics, Penerbit Akademia Baru",,,core
211485914,2012-01-01T00:00:00,"This Special Issue is an outgrowth of the HAIS'10, the 5th International Conference on Hybrid Artificial Intelligence Systems, which was held in San Sebastián, Spain, 23–25 June 2010. The HAIS conference series is devoted to the presentation of innovative techniques involving the hybridization of emerging and active topics in data mining and decision support systems, information fusion, evolutionary computation, visualization techniques, ensemble models, intelligent agent-based systems (complex systems), cognitive and reactive distributed AI systems, case base reasoning, nature-inspired smart hybrid systems, bio- and neuro-informatics and their wide range of applications. It is dedicated to promote novel and advanced hybrid techniques as well as interdisciplinary applications and practice. HAIS'10 received over 269 submissions worldwide. After careful peer-review, only 132 papers were accepted for presentation at the conference and for inclusion in the proceedings, published as Springer's Lecture Notes in Artificial Intelligence series. Authors of the most innovative papers within the scope of the NEUROCOMPUTING Journal were invited to submit their substantially extended and updated papers with additional original materials based on their most recent research findings. Each submitted paper was subsequently reviewed by 3–5 experts and leading researchers in the field. Finally eighteen papers passed the journal's rigorous review process and were included in this Special Issue. They present an exclusive sample of the conference and its recent topics. In the area of artificial vision and image processing, Segovia et al. present a comparison between two methods for analyzing PET data in order to develop more accurate CAD systems for the diagnosis of Alzheimer's disease. One of them is based on the Gaussian Mixture Model (GMM) and models the Regions Of Interest (ROIs) defined as differences between controls and AD subject. After GMM estimation using the EM algorithm, feature vectors are extracted for each image depending on the positions of the resulting Gaussians. The other method under study computes score vectors through a Partial Least Squares (PLS) algorithm based estimation and those vectors are used as features. Before extracting the score vectors, a binary mask based dimensional reduction of the input space is performed in order to remove low-intensity voxels. The validity of both methods is tested on the ADNI database by implementing several CAD systems with linear and nonlinear classifiers and comparing them with previous approaches such as VAF and PCA. The contribution of Chyzhyk et al. entitled “Hybrid Dendritic Computing with Kernel-LICA applied to Alzheimer's disease detection in MRI” presents the issue of enhancing the generalization classification power of the Dendritic Computing approach to classifier training. The paper contributes a hybrid of the Lattice Independent Component Analysis (LICA) and the Kernel transformation that provides an enhanced generalization on cross-validation experiments performed on a dataset of magnetic resonance imaging (MRI) features for Alzheimer's disease computer aided diagnosis design. The contribution by Cilla et al. presents a human action recognition system combining multiple camera observations. The proposal is centered on how to efficiently combine the observations from different viewpoints without the explicit usage of 3D models or camera calibration. To achieve this goal, a local action prediction is defined using the features extracted at each camera. These local predictions take the form of posterior probability distributions to capture the uncertainty on the classification. These distributions are combined to obtain a final posterior distribution on the performed action. Experiments show that the proposed scheme achieves successful results in the human recognition task. In bioinformatics and medical applications, the contribution by López et al. presents the theoretical and practical results of a novel data mining process that combines hybrid techniques of association analysis and classical sequentiation algorithms of genomics to generate grammatical structures of a specific language. The authors used an application of a compilers generator system that allows the development of a practical application within the area of grammarware, where the concepts of the language analysis are applied to other disciplines, such as bioinformatic. The tool allows the complexity of the obtained grammar to be measured automatically from textual data. A technique of incremental discovery of sequential patterns is presented to obtain simplified production rules, and compacted with bioinformatics criteria to make up a grammar. The work by Cuadra et al. entitled “Response Calibration in Neuroblastoma Cultures over Multielectrode Array” assesses the statistical relevance of neuroblastoma culture responses to excitation when they are grown in a Multielectrode Array (MEA) setup in order to provide an appropriate calibration of the systems response to excitation. The MAE are characterized by very low signal-to-noise ratio, which is improved by the proposed calibration, opening the possibility to employ neuroblastoma cultures for information processing in hybrid systems. In Evolutionary Computation, Maravall et al. present the application of evolutionary strategies to the self-emergence of a common lexicon in a population of agents. By modeling the vocabulary or lexicon of each agent as an association matrix or look-up table that maps the meanings (i.e. the objects encountered by the agents or the states of the environment itself) into symbols or signals it is checked whether it is possible for the population to converge in an autonomous, decentralized way to a common lexicon, so that the communication efficiency of the entire population is optimal. In the contribution by García-Gutiérrez et al. a novel hybrid classifier applied to remote sensing data fusion is presented. The classifier, called EVOR-STACK, is based on the use of ensemble techniques and evolutionary computation. The latter allows to calculate a set of weights for every feature depending on the candidate label and therefore, extending the classical evolutive feature weighting concept. The former improves the final results due to the introduction of contextual data previously weighted with the results of the label-dependent evolutionary algorithm. The results confirmed that EVOR-STACK outperforms SVM and R-STACK (its predecessor, presented in HAIS-2010) when they are applied to LIDAR and images data fusion. In terms of Neural-based models and applications, Fernández-Navarro et al. propose an alternative to the standard Gaussian Radial Basis Function for binary classification problems. The authors present q-Gaussian Radial Basis Function Neural Networks, where the basis functions include a supplementary degree of freedom in order to adapt the model to the distribution of the data. A Hybrid Algorithm (HA) is used to search for a suitable architecture and parameters for these models. In the contribution by Lei and Ghorbani, two new clustering algorithms are presented, the Improved Competitive Learning Network (ICLN) and the Supervised Improved Competitive Learning Network (SICLN), for the applications in the area of fraud detection and network intrusion detection. The ICLN is an unsupervised clustering algorithm applying new rules to the Standard Competitive Learning Neural Network (SCLN). In the ICLN, network neurons are trained to represent the center of the data by a new reward-punishment update rule. The authors claim that the new update rule overcomes the instability of the SCLN. The SICLN is a supervised clustering algorithm further developed from the ICLN by introducing supervised mechanism. According to the authors, in the SICLN, the new supervised update rule utilizes data labels to guide the training process in order to achieve a better clustering result, and can be applied to both labeled and unlabeled data. They claim that the SICLN algorithm is highly tolerant to missing or delayed labels. Furthermore, the SICLN is completely independent from the initial number of clusters because it is able to reconstruct itself according to the labels of the cluster members. The authors successfully demonstrate the SICLN's high performance through experimental analysis using both academic research data and practical real-world data for fraud detection and network intrusion detection. They also show that the SICLN outperforms traditional unsupervised clustering algorithms. Then Prieto et al. present a hybrid intelligent system to provide autonomous robots with the ability to classify the motion behavior patterns of a group of robots present in their surroundings. It is a first step in the development of a cognitive model that can detect and understand the events occurring in the environment, events that are not the robot's own actions. The hybrid system is called ANPAC (Automatic Neural-based Pattern Classifier). It uses a variable size ANN to perform pattern classification and an advisor module to adjust the preprocessing parameters and, consequently, the size of the ANN, depending on the learning results of the network. The components and operations of ANPAC are described in depth and illustrated using an example related to the recognition of behavior patterns in the flocks motion. The next contribution by Heras et al. presents an abstract argumentation framework for the support of agreement processes in agent societies. It takes into account some arguments, among them attacks arguments, and the social context of the agents that put forward arguments. The framework is implemented as a neural network that computes the set of acceptable arguments and can be tuned to give more or less importance to argument attacks. The proposal is illustrated with an example in a real domain of a water-rights transfer market. Two contributions based on ensembles techniques are included in this special issue. The contribution by Buza et al. proposes a model-selection framework for stacking regressors. Due to the differences in their underlying assumptions, principles and settings, various regression models (such as neural and Bayesian networks, support vector machines, regression trees, etc.) err differently on instances, i.e., compared to the true value of the target, some of the models predict lower values, while other models predict higher ones. Therefore, different models are potentially able to compensate for each other's errors. This idea of error compensation is explored and exploited for making stacking-based ensembles more accurate: the proposed framework focuses on the principled selection of a well-performing set of models, which may be substantially different from the individually best models due to the inter-correlations between them. Then Corchado and Baruque present a novel model for multi-dimensional data visualization in 2 dimensions. It can be considered an extension of a previously developed Visualization Induced Self-Organizing Map by the use of the ensemble meta-algorithm in an unsupervised context. The contribution shows, with a comparative study, how the calculation of slightly different ViSOM maps and its subsequent fusion into a final map can give as a result a more truthful visual 2D display of the analyzed data. This algorithm proves to be a useful tool for data visualization where the dataset under analysis is especially complex, as the enhanced representation compensates for the extra complexity of its calculations. Regarding issues of Classification methods & Information Processing, Wilk and Wozniak present a possibility of generalizing the two-class classification into multiclass classification by means of a fuzzy inference system. Fuzzy combiner harnesses the support values from classifiers to provide final response having no other restrictions on their structure. Authors compare the combination methods with ECOC and two variations of decision templates, based on Euclidean and symmetric distance. The quality of the proposed method was evaluated via computer experiment. Then, in the contribution by Chmielnicki and Stapor, the authors present a method of combining a support vector machine (SVM)—discriminative classifier with regularized discriminant analysis (RDA)—generative classifier. The hybrid SVM–RDA classifier is used in the protein fold prediction. It is a very challenging multiclass problem with high data dimensionality and a very small number of samples for each class. The authors show how to deal with these difficulties using advantages of generative and discriminative classifiers. In the contribution by Kazienko and Kajdanowicz entitled “Label-dependent Node Classification in the Network”, are presented an original extension and application of sampling algorithms to node classification in the networked data. In their new approach, they make use of new input variables, calculated based on structural network measures. Additionally, these measures, called label-dependent features, are computed separately for sub-networks of nodes with a given label-class. As a result, two new approaches of sampling algorithms—LDBootstrapping and LDGibbs have been developed for the purpose of collective classification. According to experimental studies carried out, the novel approaches provide more accurate results comparing to competitive ones in generalization for sparse networked datasets. In the contribution by Zafra et al., is presented a filter-based feature selection method for working in the multiple-instance learning scenario called ReliefF-MI. This method based on the principles of the well-known ReliefF algorithm is applied as a preprocessing step that is completely independent from the multi-instance classifier learning process and therefore is more efficient and generic than wrapper approaches proposed in this area. Different extensions are designed and implemented and their performance checked in multiple-instance learning. Experimental results on five benchmark real-world datasets and seventeen classification algorithms confirm the utility and efficiency of this method, both statistically and from the point of view of the execution time. Finally Villar et al. propose a novel approach to represent the uncertainty in the data in order to learn white box models. This representation includes; introducing fuzzy evaluation of the imprecise variables, a genetic programming approach and the learning algorithms to deal with the data. Two main multi-objective algorithms are used: Multi-objective Simulated Annealing and NSGA-II. The obtained results show this approach as a promising path for developing new controller design techniques. We would like to thank our peer-reviewers for their diligent work and efficient efforts. We are also grateful to the Editor-in-Chief, Prof. Tom Heskes, for his continued support for the HAIS conference and for this Special Issue on this prestigious journal",Special Issue: New trends and applications on hybrid artificial intelligence systems,,'Elsevier BV',,,core
203458113,30/11/2013,"AbstractThe cerebellum plays an essential role in adaptive motor control. Once we are able to build a cerebellar model that runs in realtime, which means that a computer simulation of 1 s in the simulated world completes within 1 s in the real world, the cerebellar model could be used as a realtime adaptive neural controller for physical hardware such as humanoid robots. In this paper, we introduce “Realtime Cerebellum (RC)”, a new implementation of our large-scale spiking network model of the cerebellum, which was originally built to study cerebellar mechanisms for simultaneous gain and timing control and acted as a general-purpose supervised learning machine of spatiotemporal information known as reservoir computing, on a graphics processing unit (GPU). Owing to the massive parallel computing capability of a GPU, RC runs in realtime, while reproducing qualitatively the same simulation results of the Pavlovian delay eyeblink conditioning with the previous version. RC is adopted as a realtime adaptive controller of a humanoid robot, which is instructed to learn a proper timing to swing a bat to hit a flying ball online. These results suggest that RC provides a means to apply the computational power of the cerebellum as a versatile supervised learning machine towards engineering applications",Realtime cerebellum: A large-scale spiking network model of the cerebellum that runs in realtime using a graphics processing unit ,,Elsevier Ltd.,10.1016/j.neunet.2013.01.019,,core
21869091,24/05/2012,"We present an implementation and analysis of a real-time, on-line, supervised learning system for non-parametrically learning behaviors from a human trainer on a mobile robot in outdoor environments. This approach enables a human operator to train and tune robot behaviors simply by driving the robot with a remote control. Hand-designed behaviors for outdoor environments often require many parameters, and complicated behaviors can be difficult or impossible to specify with a manageable number of parameters. Furthermore, their design requires knowledge of the robot’s internal models, and knowledge of the environment in which the behaviors will be used. In real-world scenarios, we can design new behaviors using our learning system much more quickly than we can write hand-crafted behaviors. We present the results of training the robot to execute several specialized and general-purpose behaviors, including traversing a slalom, staying near “cover”, navigating on paths, navigating in an obstacle field, and general-purpose navigation. Our system learns and executes most of these behaviors well after 1-4 hours of operator training time. In quantitative tests, the learned behavior is not as robust as a hand-crafted behavior, but often completes obstacle courses more quickly. Additionally, we identify the factors that influence the effectiveness of this approach and investigate the properties of the training data provided by the human trainer. Based on our analyses, we suggest future work to ensure sufficient training, handle conflicting training examples, model robot dynamics, and further investigate dimensionality reduction of perception features. ",Learning outdoor mobile robot behaviors by example,,,,,core
29486105,2014-11-14T00:00:00,"The automatic design of controllers for mobile robots usually requires two
stages. In the first stage,sensorial data are preprocessed or transformed into
high level and meaningful values of variables whichare usually defined from
expert knowledge. In the second stage, a machine learning technique is applied
toobtain a controller that maps these high level variables to the control
commands that are actually sent tothe robot. This paper describes an algorithm
that is able to embed the preprocessing stage into the learningstage in order
to get controllers directly starting from sensorial raw data with no expert
knowledgeinvolved. Due to the high dimensionality of the sensorial data, this
approach uses Quantified Fuzzy Rules(QFRs), that are able to transform
low-level input variables into high-level input variables, reducingthe
dimensionality through summarization. The proposed learning algorithm, called
Iterative QuantifiedFuzzy Rule Learning (IQFRL), is based on genetic
programming. IQFRL is able to learn rules with differentstructures, and can
manage linguistic variables with multiple granularities. The algorithm has been
testedwith the implementation of the wall-following behavior both in several
realistic simulated environmentswith different complexity and on a Pioneer 3-AT
robot in two real environments. Results have beencompared with several
well-known learning algorithms combined with different data
preprocessingtechniques, showing that IQFRL exhibits a better and statistically
significant performance. Moreover,three real world applications for which IQFRL
plays a central role are also presented: path and objecttracking with static
and moving obstacles avoidance","Learning Fuzzy Controllers in Mobile Robotics with Embedded
  Preprocessing",http://arxiv.org/abs/1411.3895,'Elsevier BV',10.1016/j.asoc.2014.09.021,,core
102953038,2013,"A robot agent existing in the physical world must be able to under-stand the social states of the human users it interacts with in order to respond appropriately. We compared two implemented methods for estimating the engagement state of customers for a robot bartender based on low-level sensor data: a rule-based version derived from the analysis of human behaviour in real bars, and a trained version using supervised learning on a labelled multimodal corpus. We first compared the two implementations using cross-validation on real sensor data and found that nearly all classifier types significantly outperformed the rule-based classifier. We also carried out feature selection to see which sensor features were the most informative for the classification task, and found that the position of the head and hands were relevant, but that the torso orientation was not. Finally, we performed a user study comparing the ability of the two clas-sifiers to detect the intended user engagement of actual customers of the robot bartender; this study found that the trained classifier was faster at detecting initial intended user engagement, but that the rule-based classifier was more stable",How can I help you? Comparing engagement classification strategies for a robot bartender,,,,,core
103773983,2013,"Abstract. The need to understand and model human-like behavior and intelligence has been embraced by a multidisciplinary community for sev-eral decades. The success so far has been shown in solutions for a concrete task or a competence, and these solutions are seldom a truly multidis-ciplinary effort. In this paper we analyze the needs and the opportu-nities for combining artificial intelligence and bio-inspired computation within an application domain that provides a cluster of solutions instead of searching for a solution to a single task. We analyze applications of training children with autism spectrum disorder (ASD) with a humanoid robot, because it must include multidisciplinary effort and at the same time there is a clear need for better models of human-like behavior which will be tested in real life scenarios through these robots. We designed, implemented, and carried out three applied behavior analysis (ABA) based robot interventions. All interventions aim to promote self initiated social behavior in children with ASD. We found out that the standard-ization of the robot training scenarios and using unified robot platforms can be an enabler for integrating multiple intelligent and bio-inspired algorithms for creation of tailored, but domain specific robot skills and competencies. This approach might set a new trend to how artificial and bio-inspired robot applications develop. We suggest that social com-puting techniques are a pragmatic solution to creation of standardized training scenarios and therefore enable the replacement of perceivably intelligent robot behaviors with truly intelligent ones",Interplay between natural and artificial intelligence in training autistic children with robots,,Springer,,,core
35168175,2012-03-15T00:00:00,"The ability of detecting people has become a crucial subtask, especially in robotic systems which aim an application in public or domestic environments. Robots already provide their services e.g. in real home improvement markets and guide people to a desired product. In such a scenario many robot internal tasks would benefit from the knowledge of knowing the number and positions of people in the vicinity. The navigation for example could treat them as dynamical moving objects and also predict their next motion directions in order to compute a much safer path. Or the robot could specifically approach customers and offer its services. This requires to detect a person or even a group of people in a reasonable range in front of the robot. Challenges of such a real-world task are e.g. changing lightning conditions, a dynamic environment and different people shapes. In this thesis a 3D people detection approach based on point cloud data provided by the Microsoft Kinect is implemented and integrated on mobile service robot. A Top-Down/Bottom-Up segmentation is applied to increase the systems flexibility and provided the capability to the detect people even if they are partially occluded. A feature set is proposed to detect people in various pose configurations and motions using a machine learning technique. The system can detect people up to a distance of 5 meters. The experimental evaluation compared different machine learning techniques and showed that standing people can be detected with a rate of 87.29% and sitting people with 74.94% using a Random Forest classifier. Certain objects caused several false detections. To elimante those a verification is proposed which further evaluates the persons shape in the 2D space. The detection component has been implemented as s sequential (frame rate of 10 Hz) and a parallel application (frame rate of 16 Hz). Finally, the component has been embedded into complete people search task which explorates the environment, find all people and approach each detected person",3D people detection in domestic environments,https://core.ac.uk/download/35168175.pdf,Hochschule Bonn-Rhein-Sieg,10.18418/978-3-96043-008-7,,core
144064376,2012-03-09T00:00:00,"Robôs manipuladores espaciais serão aplicados, em um futuro próximo, em serviços de resgate e manutenção de naves e satélites em órbita. O estudo e o desenvolvimento de controladores para esse tipo de sistema é fundamental para que essas aplicações se tornem realidade. Nesta tese, uma plataforma experimental é construída para possibilitar a avaliação comportamental desse tipo de sistema. Baseada em um módulo de flutuação por colchões de ar, é composta por uma base livre, elos conectados por juntas e efetuadores. Duas possibilidades de flutuação foram definidas para tornar a estrutura mais versátil, a primeira utiliza uma câmara de ar na mesa de apoio e a segunda utiliza câmaras de ar na base e em cada junta do robô. Sua estrutura mecânica modular permite diversas configurações, com um ou dois braços compostos por elos rígidos ou flexíveis. Toda a eletrônica de comando e a alimentação dos componentes do robô são alocadas em sua base flutuante, baseando a comunicação do sistema com o computador remoto em um padrão de comunicação sem fio. O software de controle, desenvolvido em Matlab e residente no computador remoto, apresenta uma interface amigável e intuitiva, possibilitando a utilização tanto do UARM como do robô de base livre flutuante para testes simulados e experimentais de sistemas de controle. A principal característica dos manipuladores espaciais é o acoplamento dinâmico entre a base e o braço robótico. A fim de evitar as complicações envolvidas no mapeamento cinemático desses sistemas, o problema de acompanhamento de trajetória é formulado diretamente no espaço da tarefa. Assim as posições do efetuador do manipulador são diretamente controladas. O equacionamento dinâmico do manipulador de base livre flutuante é descrito a partir do conceito do Manipulador Dinamicamente Equivalente. Propõe-se uma solução de controle adaptativo robusto baseado no critério H Infinito para lidar com o problema de acompanhamento de trajetória sujeito a incertezas no modelo e distúrbios externos. A adaptabilidade das redes neurais é aliada à robustez definida por um controlador H Infinito  não linear, compondo diferentes técnicas desenvolvidas de acordo com o conhecimento e a disponibilidade do modelo do robô para o controlador. A análise de resultados de simulação e de experimentos realizados no UARM mostraram a aplicabilidade dos métodos, assim como sua capacidade de robustez. Gráficos ilustraram o procedimento do acompanhamento de trajetória realizado pelo efetuador do manipulador espacial identificando a ação das leis de controle propostas. Uma comparação numérica entre as estratégias foi estabelecida por índices de desempenho relacionados ao consumo de energia e ao erro de acompanhamentoSpace manipulators robots will be applied, in a near future, in rescue services and maintenance of spacecraft and satellites in orbit. The study and development of controllers for this type of system is crucial to ensure that those applications become reality. At this thesis, an experimental platform is built to enable behavioral assessment of this type of system. Based on a floating module by air bearings, it is composed by a free base, links connected by joints and end-effectors. Two possibilities of fluctuation were set to make the structure more versatile. The first uses an air chamber in the support desk and the second uses air chambers at the base and in each joint of the robot. Its modular mechanical structure allows a variety of configurations, with one or two arms which may be composed of flexible or rigid links. The entire command electronics and the power of the robots components are allocated in its floating base, basing the system communication with the remote computer in a wireless communication standard. The control software, developed in Matlab and residing on the remote computer, presents a friendly and intuitive interface, enabling the use of both the UARM and the free-floating base robot for simulated and experimental testing of control systems. The main characteristic of space manipulators is the dynamic coupling between the base and the robotic arm. In order to avoid the complications involved in kinematic mapping of these systems, the problem of trajectory tracking is formulated directly in task space. So the positions of the manipulator end-effector are directly controlled. The dynamic equation of the free-floating manipulator is described from the concept of Dynamically Equivalent Manipulator. A solution of adaptive robust control is proposed, based on H¥ criterion to deal with the problem of trajectory tracking subject to uncertainties in the model and external disturbances. The adaptability of neural networks is combined with robustness defined by a nonlinear H Infinite controller composing different techniques developed in accordance with the knowledge and the availability of the robots model to the controller. The analysis of results of simulation and experiments performed in UARM showed the applicability of the methods, as well as its capacity for robustness. Graphs have illustrated the trajectory tracking procedure conducted by the end-effector of the space manipulator identifying the action of control laws proposed. A numerical comparison between the strategies was provided by performance indices related to energy consumption and the tracking erro",Assembly and Nonlinear H Infinitye Control of Free-Floating Base Space Manipulators.,,"'Universidade de Sao Paulo, Agencia USP de Gestao da Informacao Academica (AGUIA)'",10.11606/T.18.2012.tde-14022012-101404,,core
23429592,10/11/2013,"Lifelike means convincing the audience an animated character has intelligence, personality, and emotion while inhabiting a physical world. Toy Story, the first fulllength computer-generated animated feature film (released 1995) established itself as a visual benchmark for computer graphics hardware and software development. Soon after the film’s debut, graphics chip makers wanted to know how they could compute Toy-Storyquality imagery on a PC; game developers wanted to know how they could deliver Toy-Story-quality animation on game consoles; and robotics researchers wanted to know how they could build artificial intelligence into their machines to achieve Toy-Storyquality lifelike characters. As we at Pixar tried to answer, we also sought to create scenes even more complex, images more wondrous, and characters more fluid. For A Bug’s Life (released 1998), we extended our lighting and shading methodology to depict the transparency and back-lighting of an insect world. We developed new methods for modeling and animating large crowds of characters. And we embraced the use of subdivision surfaces to provide more flexible and organic characters",On Site MARTIN MAYO (TOP); DISNEY/PIXAR (BOTTOM) Creating Lifelike Characters in,,,,,core
24771683,2013-05-14T00:00:00,"We introduce a dynamic neural algorithm called Dynamic Neural (DN)
SARSA(\lambda) for learning a behavioral sequence from delayed reward.
DN-SARSA(\lambda) combines Dynamic Field Theory models of behavioral sequence
representation, classical reinforcement learning, and a computational
neuroscience model of working memory, called Item and Order working memory,
which serves as an eligibility trace. DN-SARSA(\lambda) is implemented on both
a simulated and real robot that must learn a specific rewarding sequence of
elementary behaviors from exploration. Results show DN-SARSA(\lambda) performs
on the level of the discrete SARSA(\lambda), validating the feasibility of
general reinforcement learning without compromising neural dynamics.Comment: Sohrob Kazerounian, Matthew Luciw are Joint first author",Autonomous Reinforcement of Behavioral Sequences in Neural Dynamics,http://arxiv.org/abs/1210.3569,,10.1109/ijcnn.2013.6706877,,core
225232087,,"[[abstract]]With recent advances in the humanoid robots technology, the communication of robots has been the state of Ubiquitous Computing (said Mark Weiser). For the robot soccer game to say, once robots can share the information retrieved from different sensors, they will collaborate and have the opportunities to win the game. Moreover, the humanoid robots have more kinds of sensor such as motion control, environment sensing, motion detection, high-speed visual processing, artificial intelligence, knowledge management and intelligence control system than general robots. Therefore, competitive robots with a common operation platform will help integrate the requirements of different control systems. In this paper, a real-time distributed platform with REAL TIME MODULE in robot software framework was proposed to meet the real-time processing and distributed computing requirements.[[sponsorship]]Chinese Automatic Control Society (CACS)[[conferencetype]]國際[[conferencedate]]20081121~20081123[[booktype]]紙本[[iscallforpapers]]Y[[conferencelocation]]臺南, 臺",A Real-Time Distributed Embedded Platform for Humanoid Robots Motion Control and Data Exchange,,,,,core
198967451,2012-01-01T00:00:00,"International audiencePurpose– Different machines are already present in the human environment, easing human beings' daily life. In the future, this tendency will be accentuated by integration of numerous robots (e.g. wheeled robots, legged robots, humanoid robots, network sensors, etc.) in the human environment. A wide range of applications, such as those dealing with warehouse management, industrial assembling, military applications, daily‐life tasks, can benefit from multi‐robot systems. The purpose of this paper is to propose an intelligent system for industrial robotics in the logistic field, based on collaboration between heterogeneous robots.Design/methodology/approach– The proposed infrastructure for this multi‐robot system is composed of a robots' network including one humanoid robot, wheeled robots, cameras, and remote computer. All devices can communicate between them by using wireless network. The goal of the humanoid robot is to lead the wheeled robots according to the environment and wheeled robots are used to carry a load. The camera allows providing complementary information about the environment; and thanks to machine learning, this control strategy allows complex tasks to be perormed for these logistic applications.Findings– This concept is implemented on real robots within the frame of a demonstrator including the above‐mentioned kind of robots. The preliminary results, obtained during experimentations, prove the feasibility of the presented strategy for real applications.Originality/value– The main originalities of this work are, on the one hand, the use of an heterogeneous multi‐robots system for logistic tasks, and on the other hand, the proposed machine learning allows a collaboration task between heterogeneous robots in an autonomous manner",Intelligent systems for Industrial Robotics: Application in Logistic Field,,'Emerald',10.1108/01439911211217071,,core
11533626,2012-01-01T00:00:00,"This thesis focuses on the development of an artificial intelligence system for a heterogeneous ensemble of mobile robots. Many robots in the ensemble may have limited processing, communication, sensing, and/or actuation capabilities. This means that each robot may not be able to execute all tasks that are input to the system. A hierarchical system is proposed to permit robots with superior processing and communication abilities to assign tasks and coordinate the less computationally able robots. The limited processing robots may also utilise the resources of superior robots during task execution. Effective task allocation and coordination should result in efficient execution of a global task. Many existing approaches to robot task allocation assume expert knowledge for task specification. This is not ideal if a non-expert human user wants to modify the task requirements. 



A novel reduced human user input task allocation and feedback coordination technique for limited capability mobile robots is developed and implemented. Unlike existing approaches, the presented method focuses on expressing tasks and robots in terms of processing, communication, sensing and actuation physical resources. This has the potential to allow non-expert human users to specify tasks to the team of robots. Fuzzy inference systems are utilised to simplify detailed robot information for comparison with simple human user inputs that represent task resource requirements. Like many existing task allocation methods, a greedy algorithm is employed to select robots. This can result in suboptimal task allocation. In addition to this, the non-expert user's task specifications might be erroneous in some instances. Hence, a feedback coordination component monitors robot performance during task execution. 



In this thesis, a customised multi-robot mapping and exploration task is utilised as a model task to test the effectiveness of the developed task allocation and feedback coordination strategy. Extensive simulation experiments with various robot team configurations are executed in environments of varying sizes and obstacle densities to assess the performance of the technique. Task allocation is able to identify suitable robots and is robust to selection weight variation. The task allocation process is subjective to fuzzy membership function parameters which may vary for different users. Feedback coordination is robust to variation in weights and thresholds for failure detection. This permits the correction of suboptimal allocations arising from greedy task allocation, incorrect initial task specifications or unexpected failures. By being robust within the tested limits, weights and thresholds can be intuitively selected. However, other parameters such as ideal achievement data can be difficult to accurately characterise in some instances.



A hierarchical hybrid deliberative-reactive navigation system for memory constrained heterogeneous robots to navigate obstructed environments is developed. Deliberative control is developed using a modified version of the A* algorithm and a rectangular occupancy grid map. A novel two-tiered path planner executes on limited memory mobile robots utilising the memory of a computationally powerful robot to enable navigation beyond localised regions of a large environment. Reactive control is developed using a modified dynamic window approach and a polar histogram technique to remove the need for periodic path planning. 



A range of simulation experiments in different sized environments is conducted to assess the performance of the two-tiered path planning strategy. The path planner is able to achieve superior or comparable execution times to non-memory constrained path planning when small sized local maps are employed in large global environments. Performance of hybrid deliberative-reactive navigation is assessed in a range of simulated environments and is also validated on a real robot. The developed reactive control system outperforms the dynamic window method",Development of an artificial intelligence system for the instruction and control of cooperating mobile robots,,Dissertation.com – Universal Publishers,,,core
10647067,2013-02-25T16:55:59,"textA cyber-physical system (CPS) is a system featuring a tight combination of, and coordination between, the system’s computational and physical elements. A large-scale CPS usually
consists of several subsystems which are formed by networked sensors and actuators, and deployed in different locations. These subsystems interact with the physical world and execute specific monitoring and control functions. How to organize the sensors and actuators inside each subsystem and interconnect these physically separated subsystems together to achieve secure, reliable and real-time communication is a big challenge. In this thesis, we first present a TDMA-based low-power and secure real-time wireless protocol. This
protocol can serve as an ideal communication infrastructure for CPS subsystems which require flexible topology control, secure and reliable communication and adjustable real-time service support. We then describe the network management techniques designed for ensuring the reliable routing and real-time services inside the subsystems and data management techniques for maintaining the quality of the sampled data from the physical world. To evaluate these proposed techniques, we built a prototype system and deployed it in different
environments for performance measurement. We also present a light-weighted and scalable solution for interconnecting heterogeneous CPS subsystems together through a slim IP adaptation layer and a constrained application protocol layer. This approach makes the underlying connectivity technologies transparent to the application developers thus enables rapid application development and efficient migration among different CPS platforms. At the end of this thesis, we present a semi-autonomous robotic system called cyberphysical avatar. The cyberphysical avatar is built based on our proposed network infrastructure and data management techniques. By integrating recent advance in body-compliant control in robotics, and neuroevolution in machine learning, the cyberphysical avatar can adjust to an unstructured environment and perform physical tasks subject to critical timing constraints while under human supervision.Computer Science",Networking infrastructure and data management for large-scale cyber-physical systems,https://core.ac.uk/download/10647067.pdf,,,,core
44365975,2013-01-01T00:00:00,"There isn’t a facet of human life that has not been touched and influenced by robots and automation. What makes robots and machines versatile is their computational intelligence. While modern intelligent sensors and powerful hardware capabilities have given a huge fillip to the growth of intelligent machines, the progress in the development of algorithms for smart interaction, collaboration and pro-activeness will result in the next quantum jump. This book deals with the recent advancements in design methodologies, algorithms and implementation techniques to incorporate intelligence in robots and automation systems. Several articles deal with navigation, localization and mapping of mobile robots, a problem that engineers and researchers are grappling with all the time. Fuzzy logic, neural networks and neuro-fuzzy based techniques for real world applications have been detailed in a few articles. This edited volume is targeted to present the latest state-of-the-art computational intelligence techniques in Robotics and Automation. It is a compilation of the extended versions of the very best papers selected from the many that were presented at the 5th International Conference on Automation, Robotics and Applications (ICARA 2011) which was held in Wellington, New Zealand from 6-8 December, 2011. Scientists and engineers who work with robots and automation systems will find this book very useful and stimulating","5th  International Conference on Automation, Robotics and Applications (ICARA 2011)",,'Springer Science and Business Media LLC',10.1007/978-3-642-37387-9,,core
23871217,30/01/2014,"In recent years, the advances in robotics have allowed for robots to venture into places too dangerous for humans. Unfortunately, the terrain in which these robots are being deployed may not be known by humans in advance, making it difficult to create motion programs robust enough to handle all scenarios that the robot may encounter. For this reason, research is being done to add learning capabilities to improve the robot’s ability to adapt to its environment. Reinforcement learning is well suited for these robot domains because often the desired outcome is known, but the best way to achieve this outcome is unknown. In a real world domain, a reinforcement-learning agent has to learn a great deal from experience. Therefore, it must be sample-size efficient. To do so, it must balance the amount of exploration that is needed to properly model the environment with the need to use the information that it has already obtained to complete its original task. In robot domains, the exploration process is especially costly in both time and energy. Therefore, it is important to make the bes",Perception-Based Generalization in Model-Based Reinforcement Learning,,,,,core
23779788,21/01/2014,"Abstract- This paper introduces the real time implementation of a vehicle in the changing environment, changes in the environment is notified through video motion transmitter and receiver. The receiver takes snapshot of an image to detect threatening objects in the environment. Those images are trained and tested by using Associative memory. The runtime image has been trained, tested and quickly retrieved for fast decision making. This robot detects threatening objects in the environment with HAM network. After making decision the remote control access is done through Zigbee device to monitor and control the speed of a Roving robot in an accurate path. Supervised learning method has been applied to detect the object correctly. Delta learning rule is applied in HAM network to test and train the image",Intelligent Vehicle Tracking For Detection of Objects,,,,,core
197924372,2014-10-20T00:00:00,"International audienceMany robotic projects use simulation as a faster and easier way to develop, evaluate and validate software components compared with on-board real world settings. In the human-robot interaction field, some recent works have attempted to integrate humans in the simulation loop. In this paper we investigate how such kind of robotic simulation software can be used to provide a dynamic and interactive environment to both collect a multimodal situated dialogue corpus and to perform an efficient reinforcement learning-based dialogue management optimisation procedure. Our proposition is illustrated by a preliminary experiment involving real users in a Pick-Place-Carry task for which encouraging results are obtained",Simulating Human-Robot Interactions for Dialogue Strategy Learning,,'Springer Science and Business Media LLC',10.1007/978-3-319-11900-7_6,,core
23439546,2013,"Machine Learning methods have found increasing applicability and relevance to the real world, finding applications in a broad range of fields in robotics, data mining, physics and biology, among many others. However, with the growth of the World Wide Web, and with improvements in data collection technology, real world datasets have been rapidly increasing in size and complexity, necessitating comparable scaling of Machine Learning algorithms. However, designing and implementing efficient parallel Machine Learning algorithms is challenging. Existing high-level parallel abstractions like MapReduce are insufficiently expressive while low-level tools such as MPI are difficult to use, and leave Machine Learning experts repeatedly solving the same design challenges. In this thesis, we trace the development of a framework called GraphLab which aims to provide an expressive and efficient high level abstraction to satisfy the needs of a broad range of Machine Learning algorithms. We discuss the initial GraphLab design, including details of a shared memory and distributed memory implementation. Next, we discuss the scaling limitations of GraphLab on real-world power-law graphs and how that informed the design of PowerGraph. By placing restrictions on the abstraction, we are able to improve scalability",GraphLab: A Distributed Abstraction for Large Scale Machine Learning,,,,,core
103520479,2012,"†Joint first authors. Abstract — We introduce a dynamic neural algorithm called Dynamic Neural (DN) SARSA(λ) for learning a behavioral se-quence from delayed reward. DN-SARSA(λ) combines Dynamic Field Theory models of behavioral sequence representation, classical reinforcement learning, and a computational neuro-science model of working memory, called Item and Order work-ing memory, which serves as an eligibility trace. DN-SARSA(λ) is implemented on both a simulated and real robot that must learn a specific rewarding sequence of elementary behaviors from exploration. Results show DN-SARSA(λ) performs on the level of the discrete SARSA(λ), validating the feasibility of general reinforcement learning without compromising neural dynamics. I",Autonomous reinforcement of behavioral sequences in neural dynamics,,,10.1109/devlrn.2012.6400831,,core
100322136,12/12/2014,"1Abstract—The need for intelligent unmanned vehicles has been steadily increasing. These vehicles could be air-, ground-, space-, or sea-based. This paper will review some of the most common software systems and methods that could be used for controlling such vehicles. Early attempts at mobile robots were confined to simple laboratory environments. For vehicles to operate in real-world noisy and uncertain environments, they need to include numerous sensors and they need to include both reactive and deliberative features. The most effective software systems have been hierarchical or multi-layered. Many of these systems mimic biological systems. This paper reviews several software approaches for autonomous vehicles. While there are similarities, there are differences as well. Most of these software systems are very difficult to use, and few of them have the ability to learn. Autonomous vehicles promise remarkable capabilities for both civilian and military applications, but much work remains to develop intelligent systems software which can be used for a wide range of applications. In particular there is a need for reliable open-source software that can be used on inexpensive autonomous vehicles. Index Terms—Mobile robots, autonomous vehicles, intelligent agents, software, and artificial intelligence",,,,,,core
231838794,2013-01-31T00:00:00,"Gain tuning is a crucial part of controller design and depends not only on an accurate understanding of the system in question, but also on the designer's ability to predict what disturbances and other perturbations the system will encounter throughout its operation. This letter presents ANUBIS (artificial neuromodulation using a Bayesian inference system), a novel biologically inspired technique for automatically tuning controller parameters in real time. ANUBIS is based on the Bayesian brain concept and modifies it by incorporating a model of the neuromodulatory system comprising four artificial neuromodulators. It has been applied to the controller of EchinoBot, a prototype walking rover for Martian exploration. ANUBIS has been implemented at three levels of the controller; gait generation, foot trajectory planning using Bézier curves, and foot trajectory tracking using a terminal sliding mode controller. We compare the results to a similar system that has been tuned using a multilayer perceptron. The use of Bayesian inference means that the system retains mathematical interpretability, unlike other intelligent tuning techniques, which use neural networks, fuzzy logic, or evolutionary algorithms. The simulation results show that ANUBIS provides significant improvements in efficiency and adaptability of the three controller components; it allows the robot to react to obstacles and uncertainties faster than the system tuned with the MLP, while maintaining stability and accuracy. As well as advancing rover autonomy, ANUBIS could also be applied to other situations where operating conditions are likely to change or cannot be accurately modeled in advance, such as process control. In addition, it demonstrates one way in which neuromodulation could fit into the Bayesian brain framework",ANUBIS: artificial neuromodulation using a Bayesian inference system,,'MIT Press - Journals',10.1162/NECO_a_00376,,core
23912141,2008,"Abstract — In order for an autonomous agent to behave robustly in a variety of environments, it must have the ability to learn approximations to many different functions. The function approximator used by such an agent is subject to a number of constraints that may not apply in a traditional supervised learning setting. Many different function approximators exist and are appropriate for different problems. This paper proposes a set of criteria for function approximators for autonomous agents. Additionally, for those problems on which polynomial regression is a candidate technique, the paper presents an enhancement that meets these criteria. In particular, using polynomial regression typically requires a manual choice of the polynomial’s degree, trading off between function accuracy and computational and memory efficiency. Polynomial Regression with Automated Degree (PRAD) is a novel function approximation method that uses training data to automatically identify an appropriate degree for the polynomial. PRAD is fully implemented. Empirical tests demonstrate its ability to efficiently and accurately approximate both a wide variety of synthetic functions and real-world data gathered by a mobile robot. I",Polynomial regression with automated degree: A function approximator for autonomous agents,10.1142/s0218213008003820,,,,core
76908452,2010-01-01T08:00:00,"As technology has advanced, many have wondered whether (or simply when) artificial intelligent devices will replace the humans who perform complex, interactive, interpersonal tasks such as dispute resolution. Has science now progressed to the point that artificial intelligence devices can replace human mediators, arbitrators, dispute resolvers and problem solvers? Can humanoid robots, attractive avatars and other relational agents create the requisite level of trust and elicit the truthful, perhaps intimate or painful, disclosures often necessary to resolve a dispute or solve a problem? This article will explore these questions. Regardless of whether the reader is convinced that the demise of the human mediator or arbitrator is imminent, one cannot deny that artificial intelligence now has the capability to assume many of the responsibilities currently being performed by alternative dispute resolution (ADR) practitioners. It is fascinating (and perhaps unsettling) to realize the complexity and seriousness of tasks currently delegated to avatars and robots. This article will review some of those delegations and suggest how the artificial intelligence developed to complete those assignments may be relevant to dispute resolution and problem solving. “Relational Agents,” which can have a physical presence such as a robot, be embodied in an avatar, or have no detectable form whatsoever and exist only as software, are able to create long term socio-economic relationships with users built on trust, rapport and therapeutic goals. Relational agents are interacting with humans in circumstances that have significant consequences in the physical world. These interactions provide insights as to how robots and avatars can participate productively in dispute resolution processes. Can human mediators and arbitrators be replaced by robots and avatars that not only physically resemble humans, but also act, think, and reason like humans? And to raise a particularly interesting question, can robots, avatars and other relational agents look, move, act, think, and reason even “better” than humans","Artificial Intelligence: Robots, Avatars and the Demise of the Human Mediator",,,Mitchell Hamline Open Access,,core
267164087,2010-01-01T08:00:00,"As technology has advanced, many have wondered whether (or simply when) artificial intelligent devices will replace the humans who perform complex, interactive, interpersonal tasks such as dispute resolution. Has science now progressed to the point that artificial intelligence devices can replace human mediators, arbitrators, dispute resolvers and problem solvers? Can humanoid robots, attractive avatars and other relational agents create the requisite level of trust and elicit the truthful, perhaps intimate or painful, disclosures often necessary to resolve a dispute or solve a problem? This article will explore these questions. Regardless of whether the reader is convinced that the demise of the human mediator or arbitrator is imminent, one cannot deny that artificial intelligence now has the capability to assume many of the responsibilities currently being performed by alternative dispute resolution (ADR) practitioners. It is fascinating (and perhaps unsettling) to realize the complexity and seriousness of tasks currently delegated to avatars and robots. This article will review some of those delegations and suggest how the artificial intelligence developed to complete those assignments may be relevant to dispute resolution and problem solving. “Relational Agents,” which can have a physical presence such as a robot, be embodied in an avatar, or have no detectable form whatsoever and exist only as software, are able to create long term socio-economic relationships with users built on trust, rapport and therapeutic goals. Relational agents are interacting with humans in circumstances that have significant consequences in the physical world. These interactions provide insights as to how robots and avatars can participate productively in dispute resolution processes. Can human mediators and arbitrators be replaced by robots and avatars that not only physically resemble humans, but also act, think, and reason like humans? And to raise a particularly interesting question, can robots, avatars and other relational agents look, move, act, think, and reason even “better” than humans","Artificial Intelligence: Robots, Avatars and the Demise of the Human Mediator",,https://core.ac.uk/download/267164087.pdf,Mitchell Hamline Open Access,,core
25951596,2008-11-01T00:00:00Z,"In this work, several localization algorithms that are designed and implemented for Cerberus'05 Robot Soccer Team are analyzed and compared. These algorithms are used for global localization of autonomous mobile agents in the robotic soccer domain, to overcome the uncertainty in the sensors, environment and the motion model. The algorithms are Reverse Monte Carlo Localization (R-MCL), Simple Localization (S-Loc) and Sensor Resetting Localization (SRL). R-MCL is a hybrid method based on both Markov Localization (ML) and Monte Carlo Localization (MCL) where the ML module finds the region where the robot should be and MCL predicts the geometrical location with high precision by selecting samples in this region. S-Loc is another localization method where just one sample per percept is drawn, for global localization. Within this method another novel method My Environment (ME) is designed to hold the history and overcome the lack of information due to the drastically decrease in the number of samples in S-Loc. ME together with S-Loc is used in the Technical Challenges in Robocup 2005 and play an important role in ranking the First Place in the Challenges. In this work, these methods together with SRL, which is a widely used successful localization algorithm, are tested with both offline and real-time tests. First they are tested on a challenging data set that is used by many researches and compared in terms of error rate against different levels of noise, and sparsity. Besides time required recovering from kidnapping and speed of the methods are tested and compared. Then their performances are tested with real-time tests with scenarios like the ones in the Technical Challenges in ROBOCUP. The main aim is to find the best method which is very robust and fast and requires less computational power and memory compared to similar approaches and is accurate enough for high level decision making which is vital for robot soccer",Comparison of Localization Methods for a Robot Soccer Team,,,SAGE Publishing,"[{'title': None, 'identifiers': ['issn:1729-8806', '1729-8814', '1729-8806', 'issn:1729-8814']}]",core
21242040,31/07/2010,"For many applications such as compliant, accurate robot tracking control, dynamics models learned from data can help to achieve both compliant control performance as well as high tracking quality. Online learning of these dynamics models allows the robot controller to adapt itself to changes in the dynamics (e.g., due to time-variant nonlinearities or unforeseen loads). However, online learning in real-time applications – as required in control – cannot be realized by straightforward usage of off-the-shelf machine learning methods such as Gaussian process regression or support vector regression. In this paper, we propose a framework for online, incremental sparsification with a fixed budget designed for fast real-time model learning. The proposed approach employs a sparsification method based on an independence measure. In combination with an incremental learning approach such as incremental Gaussian process regression, we obtain a model approximation method which is applicable in real-time online learning. It exhibits competitive learning accuracy when compared with standard regression techniques. Implementation on a real Barrett WAM robot demonstrates the applicability of the approach in real-time online model learning for real world systems",Incremental Online Sparsification for Model Learning in Real-time Robot Control,,,,,core
20841031,03/12/2008,"Simulated robotic soccer is a frequently used as a test method for contemporary artificial intelligence research. It provides a real-time environment with complex dynamics and sensor information that is both noisy and limited. Team coordination between the robots is essential for success. Genetic programming enables machines to learn skills and it is developed from the principle of survival of the fittest. A population of computer programs is generated and each program is tested against a fitness function. The best programs according to the fitness function are cloned, mutated, and recombined to create a new generation of programs. This process continues until the evolved programs satisfy a user defined criterion. In this study, genetic programming is used to teach software robots to play soccer. The robots quickly learn to chase and kick the ball towards the goal. With time, a number of players in each team develop defensive abilities and recognize that team coordination is necessary for further development",Supervisors,,,,,core
53415069,2008-01-01T00:00:00,"For understanding a real-world environment on a conceptual level, any agent requires the capability for autonomous, open-ended learning. One of the main challenges in Artificial Intelligence is to bias the learning phase sufficiently in order to obviate complexity issues, while at the same time not restricting the agent to a certain environment or to a particular task. In this paper we describe a framework for autonomous design of experiments for a robotic agent, which enables the robot to improve and increase its conceptual knowledge about the environment through open-ended learning by experimentation. We specify our implementation of this framework and describe how its modules can recognize situations in which learning is useful or necessary, gather target-oriented data and provide it to machine learning algorithms, thus reducing the search space
for the learning target significantly. We describe the integration of these modules and the real world scenarios in which we tested them",Towards autonomous design of experiments for robots,,,,,core
20704999,02/04/2008,"Abstract. In order for an autonomous agent to behave robustly in a variety of environments, it must have the ability to learn approximations to many different functions. The function approximator used by such an agent is subject to a number of constraints that may not apply in a traditional supervised learning setting. Many different function approximators exist and are appropriate for different problems. This paper proposes a set of criteria for function approximators for autonomous agents. Additionally, for those problems on which polynomial regression is a candidate technique, the paper presents an enhancement that meets these criteria. In particular, using polynomial regression typically requires a manual choice of the polynomial’s degree, trading off between function accuracy and computational and memory efficiency. Polynomial Regression with Automated Degree (PRAD) is a novel function approximation method that uses training data to automatically identify an appropriate degree for the polynomial. PRAD is fully implemented. Empirical tests demonstrate its ability to efficiently and accurately approximate both a wide variety of synthetic functions and real-world data gathered by a mobile robot. ",Polynomial Regression with Automated Degree: A Function Approximator for Autonomous Agents,,,,,core
20947903,30/12/2008,"Abstract — Humans frequently engage in activities for their own sake rather than as a step towards solving a specific task. During such behavior, which psychologists refer to as being intrinsically motivated, we often develop skills that allow us to exercise mastery over our environment. Reference [7] have recently proposed an algorithm for intrinsically motivated reinforcement learning (IMRL) aimed at constructing hierarchies of skills through self-motivated interaction of an agent with its environment. While they were able to successfully demonstrate the utility of IMRL in simulation, we present the first realization of this approach on a real robot. To this end, we implemented a control architecture for the Sony-AIBO robot that extends the IMRL algorithm to this platform. Through experiments, we examine whether the Aibo is indeed able to learn useful skill hierarchies","Index Terms — Reinforcement Learning, Self-Motivated",,,,,core
24507288,06/02/2008,"Physical environments are so complex that it is hard to hand-tune all of the domain knowledge, especially to model the dynamics of the environment. The work presented in this paper explores machine learning techniques to autonomously identify situations in the environment that a ect plan quality. Weintroduce the concept of situation-dependent costs, where situational features can be attached to the costs used by the path planner. These costs e ectively diagnose and predict situations the robot encounters so that the planner can generate paths that are appropriate for each situation. We present an implementation of our situationdependent learning approach in a real robotic system, Rogue. Rogue learns situation-dependent costs for arcs in a topological map of the environment; these costs are then used by the path planner to predict and avoid failures. In this article, we present the representation of the path planner and the navigation modules, and describe the execution trace. We show how training data is extracted from the execution trace. We present experimental results from a simulated, controlled environment aswell as from data collected from the actual robot. Our approach e ectively re nes models of dynamic systems and improves the efciency of generated plans. ",Learning Situation-Dependent Costs: Using Execution to Re ne Planning Models,,,,,core
89152501,2010-12-01T00:00:00Z,"RoboCup is an international competition to prompted robotics and related subject like: Artificial intelligence, Image processing, control, devise design and etc. One of the subjects in RoboCup competitions is Soccer. Naturally robotic soccer is an interactive and complex procedure. it might be so idealistic, but some consider a challenge with a real human football team in 2050, as the final goal of robotic soccer. There are several classes in robotic football matches such as: Middle size, Small size, simulation and so on. One of the most essential parts of a soccer robot in Middle size and Small size classes in the kicking system, this system is in charge of kicking the ball upon the command issued by the processor of robot. Almost every team develops their own unique shooting device. There are three main approaches to design and implement the robot kicking system. In this paper we designed and developed multi power kicking system that enables loop and vary shooting power. To design a good solenoid and to obtain maximum velocity of ball some parameters like: inductance, response time, resistance, force, dimensions and core-material should be balanced carefully. We used a DC-DC converter (Boost regulator) for getting different currents to have different power of shooting. We are going to review the advantages of all of those approaches. Next we are purposes a novel Solenoid-based kicking system which has already been successfully implemented in Adro RoboCup team",Design and Implementation Solenoid Based Kicking Mechanism for Soccer Robot Applied in Robocup-MSL,10.5772/10490,,SAGE Publishing,"[{'title': None, 'identifiers': ['1729-8814', 'issn:1729-8814']}]",core
17354074,2009-03-01T00:00:00,"In this paper we present a sound-source model for localising and tracking an acoustic source of interest along the azimuth plane in acoustically cluttered environments, for a mobile service robot. The model we present is a hybrid architecture using cross-correlation and recurrent neural networks to develop a robotic model accurate and robust enough to perform within an acoustically cluttered environment. This model has been developed with considerations of both processing power and physical robot size, allowing for this model to be deployed on to a wide variety of robotic systems where power consumption and size is a limitation. The development of the system we present has its inspiration taken from the central auditory system (CAS) of the mammalian brain. In this paper we describe experimental results of the proposed model including the experimental methodology for testing sound-source localisation systems. The results of the system are shown in both restricted test environments and in real-world conditions. This paper shows how a hybrid architecture using band pass filtering, cross-correlation and recurrent neural networks can be used to develop a robust, accurate and fast sound-source localisation model for a mobile robot",Robotic sound-source localisation architecture using cross-correlation and recurrent neural networks,10.1016/j.neunet.2009.01.013,,'Elsevier BV',,core
162670640,2010-01-01T00:00:00,"In mobile robotics, a solid test for adaptation isthe ability of a control system to function not only in adiverse number of physical environments, but also on anumber of different robotic platforms. This paper demonstrates that a set of behaviours evolved in simulation on a miniature robot (epuck) can be transferred to a much larger-scale platform (Pioneer), both in simulation and in the real world. The chosen architecture uses artificial evolution of epuck behaviours to obtain a genetic sequence, which is then employed to seed an idiotypic, artificial immune system (AIS) on the Pioneers. Despite numerous hardware and software differences between the platforms, navigation and target-finding experiments show that the evolved behaviours transfer very well to the larger robot when the idiotypic AIS technique is used. In contrast, transferability is poor when reinforcement learning alone is used, which validates the adaptability of the chosen architecture",Real-world transfer of evolved artificial immune system behaviours between small and large scale robotic platforms,10.1007/s12065-010-0039-7,https://core.ac.uk/download/162670640.pdf,'Springer Science and Business Media LLC',,core
33538911,2009-12-10T00:00:00,"Dissertação (mestrado)—Universidade de Brasília, Faculdade de Tecnologia, Departamento de Engenharia Mecânica, 2009.Este trabalho apresenta a modelagem e controle de um robô em bicicleta utilizando para isto o modelo de um Acrobot. Qualquer um de duas aproximações são possíveis para o robô: (a) uma configuração dedicada onde o robô e a bicicleta mesmo, e (b) o robô tem características humanas. No primeiro caso, a estabilidade do robô pode ser obtida por meio do movimento do guidão da bicicleta, com limitações de velocidade. A segunda opção permite que o robô seja controlado em baixas velocidades, inclusive quando fica parado (velocidade igual a zero). Neste caso, a modelagem do robô ciclista tem similaridade com o problema do pendulo duplo interaturado (Acrobot). Neste trabalho foram desenvolvidos controladores para a segunda aproximação. A primeira implementação foi desenvolvida baseada na teoria de controle moderno, envolvendo: (a) um controle de realimentação de estados baseado na alocação de pólos apropriada, garantindo estabilidade e (b) o projeto de um controlador de LQR (Linear Quadratic Regulator) que minimiza um critério de custo quadrático. O projeto destes dois tipos de controladores foram obtidos mediante a linearização das equações dinâmicas do sistema devido as não-linearidades implícitas. A segunda implementação foi desenvolvida baseada em alguns métodos de controle inteligente, envolvendo: (a) lógica difusa, (b) redes neurais artificiais e (c) um sistema de otimização neuro-difuso o qual mistura a capacidade de aprendizado das redes neurais com o poder de interpretação lingüística dos sistemas de inferência nebulosos. E feita uma comparação entre as técnicas de controle baseadas em especificações de desempenho obtidas da resposta do sistema, e também em alguns índices de desempenho. Para testar o desempenho dos controladores foram calculadas variáveis de estado do sistema e uma solução numérica foi aplicada com o intuito de obter a simulação do sistema. O projeto dos controladores e realizado em Matlab da empresa Mathworks, e um ambiente virtual foi desenvolvido usando a ferramenta Virtual Reality Toolbox, a qual permite obter os testes dos controladores sobre um modelo gráfico do robo ciclista. Finalmente foi realizada a implementação do controlador nebuloso sobre arquiteturas reconfiguráveis mediante a utilização da ferramenta xfuzzy e utilizando as ferramentas de projeto de processadores embarcados sobre FPGAs (Field Programmable Gate Arrays) da Xilinx. _______________________________________________________________________________ ABSTRACTThis works presents the modeling and control system design for a robot that rides a bicycle using the well-known Acrobot model for slow speeds. In this case, either two approaches are possible for the robot: (a) a dedicated configuration where the robot is the bicycle itself, and (b) the robot having human characteristics. In the first one, the stability of the robot can be achieved by means of the movement of the handlebar of the bicycle, with limitation of speeds. Therefore, in low speeds the centrifugal force generated by the circular or elliptical movement as a response of the handlebar movement is not enough to keep the robot balanced. On the other hand, the second option allows the robot to be controlled for low speed, even for null speed. In this case, the modeling of the robot cyclist has similarity to the problem of the underactuated inverted double pendulum (Acrobot). In this work the implementation of the controller was achieved for the second option following two ways. The first one implementation was developed based on modern control theories, involving: (a) the states feedback controller issues based on the appropriated poles allocation, guarantying stability and (b) the designs of a LQR (Linear Quadratic Regulator) type controller that minimizes the criterion of quadratic cost. The design flow of both controllers are based on the linear dynamic equations of the system due to their implicit nonlinearities. The second one implementation was achieve by means of some intelligent control methods, involving: (a) fuzzy logic, (b) artificial neural networks and (c) tuning neuro-fuzzy systems, which merge the capacity of learning of the neural networks with the power of linguistic interpretation of the fuzzy inference systems (neuro-fuzzy system ANFIS, Adaptive _euro Fuzzy Systems). A comparison between the proposed techniques of control is done based on the performance specifications of the obtained system response, as well as the performance index, and the system response due to parameters variations. To test the performance of the implemented controllers, the state variables of the system were calculated and a numerical solution was applied in order to obtain the system simulation. The controllers design were developed in Matlab and a three-dimensional virtual environment was developed using the Virtual Reality Toolbox, which allow the designer to test all the controllers over an virtual graphic model of the cyclist robot. Finally, an implementation of the fuzzy controller has been achieved on reconfigurable architectures through of the Xfuzzy tool and the Xilinx embedded processor design tools for FPGAs (Field Programmable Gate Arrays)",Implementação de controle difuso de um Acrobot em FPGA,,https://core.ac.uk/download/33538911.pdf,,,core
21088598,21/12/2009,"This paper describes the work-in-progress of creating an artificial 3D environment and robot, suitable for educational simulation. A visual 3D vehicle robot, equipped with a monocular camera navigates in a physics based 3D environment, with some artificial intelligence capabilities. Students can interact with the robot, add new objects and set the robot various tasks. This multimedia tool is designed for students with very little experience with robotics, and aims at giving students unlimited access to a relatively sophisticated robotic system, incorporating artificial intelligence, with an extremely low cost compared to using real robot systems. Our current version of the simulation software has been designed to perform three main tasks, play soccer, avoid object, and wander. The simulation is being designed to closely resemble its real-world counterparts, and we hope, will ultimately become a powerful research and development tool",A 3D Robot Simulation for Education,,,,,core
20874515,03/12/2008,"Abstract. Constructing soccer robots is an attempt in development of AI researches, done by defining a standard problem and solving it by many researchers all over the world. In this field, every year a formal federation holds international competitions, called RoboCup [1]. The Simulation League is one of the branches of the RoboCup. We have designed and implemented an online coach for a soccer simulation team, which is able to analyze the simulated match similar to a coach in a real football match, and sends commands to the players to improve their behaviors and get a better result from the match process. This coach is able to exchange the roles between players during the match. Also, it has the capability of recognizing the opponent formation and improving the playing style of the team. This coach got the 1 st place of Seattle’2001 RoboCup world championship in the field of online coaches. ",Coaching a Soccer Simulation Team in RoboCup Environment,,,,,core
20844901,03/12/2008,"Path planning for mobile robots in stochastic, dynamic environments is a difficult problem and the subject of much research in the field of robotics. While many approaches to solving this problem put the computational burden of path planning on the robot, physical path planning methods place this burden on a set of sensor nodes distributed throughout the environment that can communicate information to each other about path costs. Previous approaches to physical path planning have looked at the performance of such networks in regular environments (e.g., office buildings) using highly structured, uniform deployments of networks (e.g., grids). Additionally, these networks do not make use of real experience obtained from the robots they assist in guiding. We extend previous work in this area by incorporating reinforcement learning techniques into these methods and show improved performance in simulated, rough terrain environments. We also show that these networks, which we term SWIRLs (Swarms of Interacting Reinforcement Learners), can perform well with deployment distributions that are not as highly structured as in previous approaches",Distributed Path Planning for Mobile Robots using a Swarm of Interacting Reinforcement Learners,,,,,core
20863982,03/12/2008,"This report addresses the problem of making a humanoid robot learn a human partner’s preferences regarding personal space and adapt to these in real-time. An adaptive system using policy gradient reinforcement learning (PGRL) is proposed, implemented and evaluated in an experiment using human subjects. The experiment shows that this is a viable solution to the problem, but that there are some issues that remain to be resolved. Beteendeanpassning för en socialt interaktiv robo",Supervisor at Nada was Henrik Christensen,,,,,core
21326645,19/12/2010,"Robocup is an international competition for multi agent research and related subject like: Artificial intelligence, Image processing, machine learning, Robot path planning, control, and obstacle avoidance. In a soccer robot game, the environment is highly competitive and dynamic. In order to work in the dynamically changing environment, the decision making system of a soccer robot system should have the features of flexibility and real time adaptation. In this paper we will focus on the Middle Size Soccer Robot league (MSL) and new hierarchical hybrid fuzzy methods for decision making and action selection of a robot in Middle Size Soccer Robot league (MSL) are presented. First, the behaviors of an agent are introduced, implemented and classified in two layers, the Low_Level_Behaviors and the High_Level_Behaviors. In the second layer, a two phase mechanism for decision making is introduced. In phase one, some useful methods are implemented which check the robot’s situation for performing required behaviors. In the next phase, the team strategy, team formation, robot’","S.Hamidreza Kasaei, S.Mohammadreza Kasaei, S.Alireza Kasaei Development a Real Time Cooperative Behavior Approach for Autonomous Soccer Robots Applied in Robocup- MSL",,,,,core
11615378,2010-07-23T00:00:00,"E’ stato progettato il controllo di un manipolatore planare a tre gradi di libertà tramite il software Simulink. Il controllo avviene in tempo reale utilizzando Real-Time Windows Target per migliorare le prestazioni, la simulazione gira ad 1kHz. 

Il programma è incentrato sulla realizzazione di una macchina a stati finiti che permette l' elaborazione dei riferimenti di posizione e la gestione delle emergenze.

Lo scambio degli Input e Output con il sistema reale avviene tramite la scheda di acquisizione dati Sensoray 626.

Le traiettorie desiderate per movimentare il robot sono pianificate da un altro calcolatore e comunicate al controllo tramite l' apertura di un socket, che utilizza il protocollo di trasmissione UDP. La frequenza di invio è 100 Hz. Il programma si occupa quindi di interpolare le posizioni ricevute e di mandarle ai motori.

Viene implementato un controllo sia di tipo decentralizzato sia centralizzat",Controllo di manipolatore a tre gradi di libertà,,https://core.ac.uk/download/11615378.pdf,,,core
41213248,2009-01-01T00:00:00,"Teleoperation is a difficult task, particularly when controlling robots from an isolated operator station. In general, the operator has to solve nearly blindly the problems of mission planning, target identification, robot navigation, and robot control at the same time. The goal of the proposed system is to support teleoperated navigation with real-time mapping. We present a novel scan matching technique that re-considers data associations during the search, enabling robust pose estimation even under varying roll and pitch angle of the robot enabling mapping on rough terrain. The approach has been implemented as an embedded system and extensively tested on robot platforms designed for teleoperation in critical situations, such as bomb disposal. Furthermore, the system has been evaluated in a test maze by first responders during the Disaster City event in Texas 2008. Finally, experiments conducted within different environments show that the system yields comparably accurate maps in real-time when compared to higher sophisticated offline methods, such as Rao-Blackwellized SLAM.(Best Paper Award Finalist)Artificial Intelligence & Integrated Computer System",Operator-Assistive Mapping in Harsh Environments,10.1109/SSRR.2009.5424159,,'Institute of Electrical and Electronics Engineers (IEEE)',,core
44254124,2010-01-01T00:00:00,"Electronics is under development in this country in an organized and institutional way since the beginning of 30-ties of the previous century. It grew up from electrical engineering of weak currents and its first name used popularly was communications. It was time when television was born and the radio was maturing. Electronics is a branch of research and technology which deals with generation and processing of electrical and electromagnetic signals. A subject of telecommunications is signal transmission for a distance. Electronics and telecommunications (ET) includes or is combined with other branches like: microelectronics, radioelectronics, optoelectronics, photonics, acoustoelectronics, magnetronics, bioelectronics, energoelectronics, material engineering, semiconductor physics, automation and robotics, mechatronics and microsystems, informatics, teleinformatics, software engineering and other. Devices and functional systems of ET such as computers, data warehouses, cell phones, TV sets, Internet, GPS are build of electronic components and circuits. ET is a branch which belongs to hi-tech area, where the products gather a large load of knowledge of value overcoming frequently the price of work and material. ET has recently turned to an active participant of the processes of generation, storing, processing, transportation, distribution and usage of knowledge in the society. ET started to create artificial intelligence, co-creates intellectual property, searches for knowledge in big data sets, aids medicine, extends virtual/augmented reality, builds Internet of persons and things, strengthens security, protects natural environment, facilitates our life, aids our decisions, activates individuals, equalizes chances, provides convenient personal communications and access to data, starts building a penetrating ubiquitous infrastructure, ceases to be only a branch of technology, grows into the social space, touches culture, sociology, psychology and art. Such an important role of ET is combined with the existence in the society of an adequate infrastructure which recreates the full development cycle of high technology embracing: people, institutions, finances and logistics, in this also science, higher education, education, continuous training, dissemination and outreach, professional social environment, legal basis, political support and lobbying, innovation structures, applications, industry and economy. The digest of chosen development tendencies in ET was made here from the academic perspective, in a wider scale and on this background the national one, trying to situate this branch in the society, determine its changing role to build a new technical infrastructure of a society based on knowledge, a role of builder of many practical gadgets facilitating life, a role of a big future integrator of today’s single bricks into certain more useful unity. This digest does not have a character of a systematic analysis of ET. It is a kind of an arbitrary utterance of the authors inside their field of competence. The aim of this paper is to take an active part in the discussion of the academic community in this country on the development strategy of ET, choice of priorities for cyclically rebuilding economy, in competitive environments. The review paper was initiated by the Committee of Electronics and Telecommunications of Polish Academy of Sciences and was published in Polish as introductory chapter of a dedicated expertise, printed in a book format. This version makes the included opinions available for a wider community",Electronics and telecommunications in Poland - issues and perspectives,,,Proc. SPIE,,core
230196658,2010-08-05T00:00:00,"&Ccedil;ok serbestlik dereceli robotların doğrusal olmayan dinamik modelinin detaylı bir şekilde elde edilmesi ve bu modelin denetim algoritması i&ccedil;inde ger&ccedil;ek zamanda uygulanması zordur. Bu &ccedil;alışma kapsamında, yaklaşık robot modelini kullanan, uygulamaya y&ouml;nelik, &ouml;ğrenebilen ve &ouml;z uyarlamalı gelişmiş bir adaptif robot denetim yapısı elde edilmesi ama&ccedil;lanmıştır. Bu yapıya ulaşabilmek i&ccedil;in bulanık mantıkla denetim, yapay sinir ağları ve bunların bileşimi olan yapay sinir ağı temelli bulanık denetleyiciler incelenmiş ve model referans adaptif izleme sistemi yapısında, robot denetimi ama&ccedil;lı bir bulanık yapay sinir ağı denetleyici (BUYSA) &ouml;nerilmiştir. Bulanık yapay sinir ağı denetleyicinin temelini oluşturan ger&ccedil;ek zamanda yapı geliştirme ve parametre &ouml;ğrenme algoritmaları tanıtılmıştır. Anahtar Kelimeler: Robotik, doğrusal olmayan sistemler, bulanık yapay sinir ağıyla denetim, geriye yayılma, &ouml;ğrenen manip&uuml;lat&ouml;r denetimi, &ouml;z yapılanmalı denetleyici.Multi degree of freedom robot manipulators have nonlinear and complex dynamic characteristics. The use of computed torque etc. conventional type controllers require detailed manipulator dynamics model. Because of coupling effects between links, external disturbances, variation of payloads, frictions, saturation of actuators etc. obtaining the complicated nonlinear manipulator model and its real time implementation in control algorithm are difficult. In this study, fuzzy logic and artificial neural network have been combined together under model reference adaptive control structure to obtain a self-learning and self-tuning sophisticated adaptive robot controller (BUYSA). To achieve this controller structure, fuzzy logic control, neural networks, neuro-fuzzy control subjects have been investigated and a new neuro-fuzzy controller has been developed under the model reference adaptive system structure. Real time structure development and parameter learning algorithms, forming the basis of BUYSA, have been introduced. In the simulations implemented by proposed BUYSA, membership functions and number of fuzzy rules changed in real time in relation to dynamic characteristics of system, variation of system parameters and external disturbances, saturation of actuators, complexity of motion tasks and expected control quality. If degree of freedom of robot and/or complexity of motion tasks increase then huge calculations caused by necessity of bigger fuzzy neural network with bigger number of neurons have to be avoided. To achieve this, a separated structure that formed by fuzzy neural networks has been proposed.Keywords: Nonlinear robotic systems, neuro-fuzzy control, back propagation, learning manipulator control, self-structuring controller",Model reference robot control using fuzzy neural network,,https://core.ac.uk/download/230196658.pdf,İTÜDERGİSİ/d,,core
148440386,2009-01-01T00:00:00,"Urban Search And Rescue (USAR) is a time critical task since all survivors have to be rescued within the first 72 hours. One goal in Rescue Robotics is to support emergency response by mixed-initiative teams consisting of humans and robots. Their task is to explore the disaster area rapidly while reporting victim locations and hazardous areas to a central station, which then can be utilized for planning rescue missions. To fulfill this task efficiently, humans and robots have to map disaster areas jointly while co- ordinating their search at the same time. Additionally, robots have to perform subproblems, such as victim detection and navigation, autonomously. In disaster areas these problems are extraordinarily challenging due to the unstructured environment and rough terrain. Furthermore, when communication fails, methods that are deployed under such conditions have to be decentralized, i.e. operational without a central station. In this thesis a unified approach joining human and robot resources for solving these problems is contributed. Following the vision of combined multi-robot and multi-human teamwork, core problems, such as position tracking on rough terrain, mapping by mixed teams, and decentralized team coordination with limited radio communication, are directly addressed. More specific, RFID-SLAM, a novel method for robust and efficient loop closure in large-scale environments that utilizes RFID technology for data association, is contributed. The method is capable of jointly improving multiple maps from humans and robots in a centralized and decentralized manner without requiring team members to perform loops on their routes. Thereby positions of humans are tracked by PDR (Pedestrian Dead Reckoning), and robot positions by slippage- sensitive odometry, respectively. The joint-graph emerging from these trajectories serves as an input for an iterative map optimization procedure. The introduced map representation is further utilized for solving the centralized and decentralized coordination of large rescue teams. On the one hand, a deliberate method for combined task assignment and multi-agent path planning, and on the other hand, a local search method using the memory of RFIDs for coordination, are proposed. For autonomous robot navigation on rough terrain and real-time victim detection in disaster areas an efficient method for elevation map building and a novel approach to genetic MRF (Markov Random Field) model optimization are contributed. Finally, a human in the loop architecture is presented that integrates data collected by first responders into a multi-agent system via wearable computing. In this context, the support and coordination of disaster mitigation in large-scale environments from a central-command-post-perspective are described. Methods introduced in this thesis were extensively evaluated in outdoor environments and official USAR testing arenas designed by the National Institute of Standards and Technology (NIST). Furthermore, they were an integral part of systems that won in total more than 10 times the first prize at international competitions, such as the RoboCup world championships.This is a Ph.D. thesis originally defended at University of Freiburg.Artificial Intelligence & Integrated Computer System",Mapping and Exploration for Search and Rescue with Humans and Mobile Robots,,,Freiburg : University of Freiburg,,core
35314993,28/03/2008,"This PhD Thesis is focussed in the study of spiking neural networks. In this framework the presented work presents different hardware architectures that are implemented in reconfigurable devices (FPGAs). Different approaches are proposed adopting time¬driven or alternatively event-driven processing schemes. The work presents alternative control approaches in the field of robotics and studies computing architectures for the simulation of massive spiking neural networks of millions of neurons processing sensorimotor information in real-time. These proposed approaches have been implemented in two hybrid Hardware/Software platforms with different levels of autonomy of the hardware (stand-alone and co-processing strategy) with respect to the software modules (in a PC as a host computer) that simulated in real-time these large scale networks. In a second stage, this Thesis focuses on experiments with real-robots, as validation methodology of the control neural networks under study. The choice of working with real robots instead of simulated ones in motivated by the difficulty of describing in a realistic way the interaction with the real-world in a simulated framework. Therefore, the work here also adopts the ""Embodiment concept"" which stresses the necessity of having a physical body as learning mechanism for the knowledge emergence generation. In this field, the Thesis describes two robotic platforms built and adapted for being controlled by spiking neural systems. The obtained results show that imitating in more or less detail the biology is feasible building neural circuits which represent valid alternatives to be considered for control of biomorphic robots with complex physical structures",Hardware architectures for neural processing systems for bio-inspired robot control,,https://core.ac.uk/download/pdf/35314993.pdf,,,core
210960336,2008-01-01T00:00:00,"In this thesis, computational models of adaptive motor control and visuomotor coordination are explored and developed. These models relate to hypotheses on how sensorimotor processing in biological organisms might be organized at an abstract level; furthermore, these models and their specific implementations offer solutions for technical problems in the domain of adaptive robotics. For this reason, both biological and technical aspects are addressed. On the one hand, this thesis focuses on the learning of so-called internal models (Miall et al., 1993; Kawato, 1999): ""forward models"", which predict the sensory consequences of the agent's own actions, and ""inverse models"", which act like motor controllers and generate motor commands. In this area, new strategies and algorithms for learning are suggested and tested on both simulated and real-world robot setups. This work contributes to the understanding of the ""building blocks"" of integrated sensorimotor processing. On the other hand, this thesis suggests complex models of sensorimotor coordination: In a study on the grasping to extrafoveal targets with a robot arm, it is explored how forward and inverse models may interact, and a second study addresses the question how visual perception of space might arise from the learning of sensorimotor relationships. The theoretical part of the thesis starts with a close view on sensorimotor processing. The cognitivist approach and the embodied approach to sensorimotor processing are contrasted with each other, providing evidence from psychological and neurophysiological studies in favor of the latter. It is outlined how the application of robots fits into the embodied approach as research method. Furthermore, internal models are defined in a formal way, and an overview of their role in models of perception and cognition is provided, with a special emphasis on anticipation and predictive forward models. Afterwards, a thorough overview of internal models in adaptive motor control (covering both kinematics and dynamics) and a novel learning strategy for kinematic control problems (""learning by averaging"") are presented. The experimental work comprises four different studies. First, a detailed comparison study of various motor learning strategies for kinematic problems is presented. The performance of ""feedback error learning"" (Kawato et al., 1987), ""distal supervised learning"" (Jordan and Rumelhart, 1992), and ""direct inverse modeling"" (e.g., Kuperstein, 1987) is directly compared on several learning tasks from the domain of eye and arm control (on simulated setups). Moreover, an improved version of direct inverse modeling on the basis of abstract recurrent networks and learning by averaging are included in the comparison. The second study is dedicated to the learning of a visual forward model for a robot camera head. This forward model predicts the visual consequences of camera movements for all pixels of the camera image. The presented learning algorithm is able to overcome the two main difficulties of visual prediction: first, the high dimensionality of the input and output space, and second, the need to detect which part of the visual output is non-predictable. To demonstrate the robustness of the presented learning algorithm, the work is not carried out on plain camera images, but on distorted ""retinal images"" with a decreasing resolution towards the corners. In the third experimental chapter, a model for grasping to extrafoveal (non-fixated) targets is presented. It is implemented on a robot setup, consisting of a camera head and a robot arm. This model is based on the premotor theory of attention (Rizzolatti et al., 1994) and adds one specific hypothesis: Attention shifts caused by saccade programming imply a prediction of the retinal foveal images after the saccade. For this purpose, the visual forward model from the preceding study is used. Based on this model, several grasping modes are compared; the obtained results are qualitatively congruent with the performance that can be expected from human subjects. The fourth study is based on the theory that visual perception of space and shape is based on an internal simulation process which relies on forward models (Moeller, 1999). This theory is tested by synthetic modeling in the task domain of block pushing with a robot arm",Adaptive Internal Models for Motor Control and Visual Prediction,,,'Logos Verlag Berlin',,core
71922296,2009-01-01T00:00:00,"In the past few years, the field of autonomous robot has been rigorously studied and non-industrial applications of robotics are rapidly emerging. One of the most interesting aspects of this field is the development of the learning ability which enables robots to autonomously adapt to given environments without human guidance. As opposed to the conventional methods of robots' control, where human logically design the behavior of a robot, the ability to acquire action strategies through some learning processes will not only significantly reduce the production costs of robots but also improves the applicability of robots in wider tasks and environments. However, learning algorithms usually require large calculation cost, which make them unsuitable for robots with limited resources. In this study, we propose a simple two-layered neural network that implements a novel and fast Reinforcement Learning. The proposed learning method requires significantly less calculation resources, hence applicable to small physical robots running in the real world environments.For this study, we built several simple robots and implemented the proposed learning mechanism to them. In the experiments, to evaluate the efficacy of the proposed learning mechanism, several robots were simultaneously trained to acquire obstacle avoidance strategies in a same environment, thus, forming a dynamic environment where the learning task is substantially harder than in the case of learning in a static environment",Fast reinforcement learning for simple physical robots,10.1007/s12293-009-0015-x,,'Springer Fachmedien Wiesbaden GmbH',,core
105997839,2010,"This research compares the behavior of three robot navigation controllers namely: PID, Artificial Neural Networks (ANN), and Fuzzy Logic (FL), that are used to control the same autonomous mobile robot platform navigating a real unknown indoor environment that contains simple geometric-shaped static objects to reach a goal in an unspecified location. In particular, the study presents and compares the design, simulation, hardware implementation, and testing of these controllers. The first controller is a traditional linear PID controller, and the other two are intelligent non-linear controllers, one using Artificial Neural Networks and the other using Fuzzy Logic Techniques. Each controller is simulated first in MATLAB ® using the Simulink Toolbox. Later the controllers are implemented using Quartus ll ® software and finally the hardware design of each controller is implemented and downloaded to a Field-Programmable Gate Array (FPGA) card which is mounted onto the mobile robot platform. The response of eac",DESIGN AND DEVELOPMENT OF INTELLIGENT NAVIGATION CONTROL SYSTEMS FOR AUTONOMOUS ROBOTS THAT USES NEURAL NETWORKS AND FUZZY LOGIC TECHNIQUES AND FPGA FOR ITS IMPLEMENTATION,,,,,core
232133379,2009-08-01T07:00:00,"Real-time simulations of biological neural networks (BNNs) provide a natural platform for applications in a variety of fields: data classification and pattern recognition, prediction and estimation, signal processing, control and robotics, prosthetics, neurological and neuroscientific modeling. BNNs possess inherently parallel architecture and operate in continuous signal domain. Spiking neural networks (SNNs) are type of BNNs with reduced signal dynamic range: communication between neurons occurs by means of time-stamped events (spikes). SNNs allow reduction of algorithmic complexity and communication data size at a price of little loss in accuracy. Simulation of SNNs using traditional sequential computer architectures results in significant time penalty. This penalty prohibits application of SNNs in real-time systems. Graphical processing units (GPUs) are cost effective devices specifically designed to exploit parallel shared memory-based floating point operations applied not only to computer graphics, but also to scientific computations. This makes them an attractive solution for SNN simulation compared to that of FPGA, ASIC and cluster message passing computing systems. Successful implementations of GPU-based SNN simulations have been already reported. The contribution of this thesis is the development of a scalable GPU-based realtime system that provides initial framework for design and application of SNNs in various domains. The system delivers an interface that establishes communication with neurons in the network as well as visualizes the outcome produced by the network. Accuracy of the simulation is emphasized due to its importance in the systems that exploit spike time dependent plasticity, classical conditioning and learning. As a result, a small network of 3840 Izhikevich neurons implemented as a hybrid system with Parker-Sochacki numerical integration method achieves real time operation on GTX260 device. An application case study of the system modeling receptor layer of retina is reviewed",GPU-based implementation of real-time system for spiking neural networks,,https://core.ac.uk/download/232133379.pdf,RIT Scholar Works,,core
36204603,2009-01-01T00:00:00,"Teleoperation is a difficult task, particularly when controlling robots from an isolated operator station. In general, the operator has to solve nearly blindly the problems of mission planning, target identification, robot navigation, and robot control at the same time. The goal of the proposed system is to support teleoperated navigation with real-time mapping. We present a novel scan matching technique that re-considers data associations during the search, enabling robust pose estimation even under varying roll and pitch angle of the robot enabling mapping on rough terrain. The approach has been implemented as an embedded system and extensively tested on robot platforms designed for teleoperation in critical situations, such as bomb disposal. Furthermore, the system has been evaluated in a test maze by first responders during the Disaster City event in Texas 2008. Finally, experiments conducted within different environments show that the system yields comparably accurate maps in real-time when compared to higher sophisticated offline methods, such as Rao-Blackwellized SLAM.(Best Paper Award Finalist)Artificial Intelligence & Integrated Computer System",Operator-Assistive Mapping in Harsh Environments,10.1109/SSRR.2009.5424159,,'Institute of Electrical and Electronics Engineers (IEEE)',,core
46061565,01/05/2011,"For many applications such as compliant, accurate robot tracking control, dynamics models learned from data can help to achieve both compliant control performance as well as high tracking quality. Online learning of these dynamics models allows the robot controller to adapt itself to changes in the dynamics (e.g., due to time-variant nonlinearities or unforeseen loads). However, online learning in real-time applications -- as required in control -- cannot be realized by straightforward usage of off-the-shelf machine learning methods such as Gaussian process regression or support vector regression. In this paper, we propose a framework for online, incremental sparsification with a fixed budget designed for fast real-time model learning. The proposed approach employs a sparsification method based on an independence measure. In combination with an incremental learning approach such as incremental Gaussian process regression, we obtain a model approximation method which is applicable in real-time online learning. It exhibits competitive learning accuracy when compared with standard regression techniques. Implementation on a real Barrett WAM robot demonstrates the applicability of the approach in real-time online model learning for real world systems",Incremental online sparsification for model learning in real-time robot control,10.1016/j.neucom.2010.06.033,,,,core
154907505,2011-01-01T00:00:00,"One of the main and recent problem in developing countries like Malaysia is lack of surgeon or specialists,

especially in rural areas. Insufficient specialized surgeons in such regions particularly in the niche of orthopedic, causes more fatalities and loss of limbs due to time and distance constrain in attending the patients. A mobile robotic system known as OTOROB (Orthopedic Robot) is designed and developed to aid surgeons to virtually present at such areas for attending patients in order to make life saving decisions. The developed mobile robotic platform is integrated with a flexible robotic arm vision system to be controlled remotely by the remote surgeon to obtain visual inspection on the patients. Fuzzy logic control is implemented in the control system as Artificial intelligence (AI) to provide safety features for the robotic arm articulation. The safety system of the robotic arm consists of Danger Monitoring System (DMS) and Obstacle Avoidance System (OAS). The experiments conducted on DMS shows that the DMS capable of conveying danger level surrounding the robotic arm to the user through GUI with warning indication and obstacle position. While, OAS developed, responded to the mobile and static obstacle around the robotic arm. The robotic arm is capable of avoiding approaching obstacle autonomously via fuzzy control. The smooth control of robotic arm coupled with safety routines improved the overall articulation of the robotic arm. The safety oriented flexible robotic arm system of OTOROB able to deliver reliable and convenient for both remote doctor and patient in real time emergency circumstances",Safety system for non-interventional flexible robotic arm of  Orthopaedic Robot (OTOROB) using fuzzy logic,10.5923/j.ajis.20110101.04,https://core.ac.uk/download/154907505.pdf,'Scientific and Academic Publishing',,core
46823014,2011-06-27T00:00:00,"We present results and research projects about the computational aspects of classical problems in Artificial Intelligence. We are interested in the setting of agents able to describe their environment through a possibly huge number of Boolean descriptors, and to act upon this environment. The typical applications of this kind of studies are to the design of autonomous robots (for exploring unknown zones, for instance) or of software assistants (for scheduling, for instance). The ultimate goal of research in this domain is the design of agents able to learn autonomously, by learning and interacting with their environment (including human users), also able to reason for producing new pieces of knowledge, for explaining observed phenomena, and finally, able to decide on which action to take at any moment, in a rational fashion. Ideally, such agents will be fast, efficient as soon as they start to interact with their environment, they will improve their behavior as time goes by, and they will be able to communicate naturally with humans. Among the numerous research questions raised by these objectives, we are especially interested in concept and preference learning, in reinforcement learning, in planning, and in some underlying problems in complexity theory. A particular attention is paid to interaction with humans and to huge numbers of descriptors of the environment, as are necessary in real-world applications","Computational Aspects of Learning, Reasoning, and Deciding",,,HAL CCSD,,core
21042443,11/07/2009,"While Reinforcement Learning (RL) is not traditionally designed for interactive supervisory input from a human teacher, several works in both robot and software agents have adapted it for human input by letting a human trainer control the reward signal. In this work, we experimentally examine the assumption underlying these works, namely that the human-given reward is compatible with the traditional RL reward signal. We describe an experimental platform with a simulated RL robot and present an analysis of real-time human teaching behavior found in a study in which untrained subjects taught the robot to perform a new task. We report three main observations on how people administer feedback when teaching a robot a task through Reinforcement Learning: (a) they use the reward channel not only for feedback, but also for future-directed guidance; (b) they have a positive bias to their feedback — possibly using the signal as a motivational channel; and (c) they change their behavior as they develop a mental model of the robotic learner. Given this, we made specific modifications to the simulated RL robot, and analyzed and evaluated its learning behavior in four additional experiments with human trainers. We report significant improvements on several learning measures. This work demonstrates the importance of understanding the human-teacher/robot-learner partnership in order to design algorithms that support how people want to teach while simultaneously improving the robot’s learning behavior",Teachable Robots: Understanding Human Teaching Behavior to Build More Effective Robot Learners,,,,,core
188718041,2009-02-04T00:00:00,"In this paper we present a sound-source model for localising and tracking an acoustic source of interest along the azimuth plane in acoustically cluttered environments, for a mobile service robot. The model we present is a hybrid architecture using cross-correlation and recurrent neural networks to develop a robotic model accurate and robust enough to perform within an acoustically cluttered environment. This model has been developed with considerations of both processing power and physical robot size, allowing for this model to be deployed on to a wide variety of robotic systems where power consumption and size is a limitation. The development of the system we present has its inspiration taken from the central auditory system (CAS) of the mammalian brain. In this paper we describe experimental results of the proposed model including the experimental methodology for testing sound-source localisation systems. The results of the system are shown in both restricted test environments and in real-world conditions. This paper shows how a hybrid architecture using band pass filtering, cross-correlation and recurrent neural networks can be used to develop a robust, accurate and fast sound-source localisation model for a mobile robot",Robotic sound-source localisation architecture using cross-correlation and recurrent neural networks,10.1016/j.neunet.2009.01.013,,'Elsevier BV',,core
147947345,2009-01-15T14:17:42,"The work of this thesis centers around the research subject of distributed robotic search. Within the field of distributed robotic systems, a task of particular interest is attempting to locate one or more targets in a possibly unknown environment. While numerous studies have proposed and analyzed methods to accomplish this, there is currently a lack of a strong foundation of tools and techniques that can be used to facilitate the development and evaluation of different approaches to distributed robotic search. In this work, we aim to provide such a foundation through tools, methods, models, and analysis of experimentation. An often overlooked aspect of the distributed robotic research process is the development and analysis of tools and modules to be used with robotic systems. These may include plug-ins for realistic robotic simulators, software/hardware systems to track multiple mobile robots in real-time, extension boards for robotic platforms, and the robots themselves. Along with other tools developed and used for this work, we focus particularly on the development, characterization, and validation of a fast, accurate on-board system for relative positioning and communication between robots, a capability which is critical for effective distributed robotic search. Designing individual robot controllers to generate a specific group behavior is a difficult and often counter-intuitive process. A possible alternative to hand-crafting distributed search controllers is automatic synthesis using machine-learning techniques. We explore the effectiveness of using a noise-resistant version of the Particle Swarm Optimization algorithm to optimize the weights of an embedded Artificial Neural Network, allowing the robot to learn obstacle avoidance behavior (a common benchmark for robotic learning techniques); we find that this technique appears to offer superior performance as compared to the canonical approach of using Genetic Algorithms for this type of learning. A method for faster learning using distributed evaluation in a robot team is tested and is found to offer comparable performance using only a fraction of the original learning time. This technique can be used for fast, effective learning and adaptation in a distributed robotic system performing search. The process of designing and analyzing algorithms for distributed robotic systems can be greatly facilitated if models are available to describe the dynamics of the algorithm at an abstract level. Inspired by previous examples in the distributed robotic field, we work to design a model of robotic search that captures the system at different levels of abstraction, ranging from accurately recreating the details of individual robots to describing the entire system as an indivisible whole. To capture the entire search process, we model both the exploration phase, where robots cover an environment in an effort to detect traces of targets, and the localization phase, where robots use target emission sensing to navigate towards the target. The utility of our models is demonstrated by using them to develop an effective technique for the declaration phase of search, where robots decide that a target has been accurately localized and announce its position. In distributed robotics research, it is important that techniques developed with abstracted simulations and models are ultimately validated using real robotic platforms in order to verify their correctness. In that spirit, we run systematic sound search experiments using teams of up to ten real robots. These experiments utilize the tools developed throughout the research process, demonstrate the utility of our learning technique for fast search adaptation, and serve to validate our models of distributed robotic localization. In addition, they allow us to analyze and better understand the subtle dynamics of the search process, providing information which should be useful for any future work on distributed robotic search","Synthesis, modeling, and experimental validation of distributed robotic search",10.5075/epfl-thesis-4256,,"Lausanne, EPFL",,core
45796066,2010,"Online model learning in real-time is required
by many applications such as in robot tracking
control. It poses a difficult problem, as
fast and incremental online regression with
large data sets is the essential component
which cannot be achieved by straightforward
usage of off-the-shelf machine learning methods
(such as Gaussian process regression or
support vector regression). In this paper,
we propose a framework for online, incremental
sparsification with a fixed budget designed
for large scale real-time model learning.
The proposed approach combines a
sparsification method based on an independence
measure with a large scale database.
In combination with an incremental learning
approach such as sequential support vector
regression, we obtain a regression method
which is applicable in real-time online learning.
It exhibits competitive learning accuracy
when compared with standard regression
techniques. Implementation on a real
robot emphasizes the applicability of the proposed
approach in real-time online model
learning for real world systems",Incremental Sparsification for Real-time Online Model Learning,,,,,core
26148108,2011-01-01T00:00:00Z,"RoboCup is an international competition to prompted robotics and related subject like: Artificial intelligence, Image processing, control, devise design and etc. One of the subjects in RoboCup competitions is Soccer. Naturally robotic soccer is an interactive and complex procedure. it might be so idealistic, but some consider a challenge with a real human football team in 2050 , as the final goal of robotic soccer. There are several classes in robotic football matches such as: Middle size, Small size, simulation and so on. One of the most essential parts of a soccer robot in Middle size and Small size classes in the kicking system, this system is in charge of kicking the ball upon the command issued by the processor of robot. Almost every team develops their own unique shooting device. There are three main approaches to design and implement the robot kicking system. In this paper we designed and developed multi power kicking system that enables loop and vary shooting power. To design a good solenoid and to obtain maximum velocity of ball some parameters like: inductance, response time, resistance, force, dimensions and core-material should be balanced carefully. We used a DC-DC converter (Boost regulator) for getting different currents to have different power of shooting. We are going to review the advantages of all of those approaches. Next we are purposes a novel Solenoid-based kicking system which has already been successfully implemented in Adro RoboCup team",Design and Implementation Solenoid Based Kicking Mechanism for Soccer Robot Applied in Robocup-MSL,,,SAGE Publishing,"[{'title': None, 'identifiers': ['issn:1729-8806', '1729-8814', '1729-8806', 'issn:1729-8814']}]",core
196689363,2011-11,"Autonomy is a prime issue on robotics field and it is closely related to decision making. Last researches on decision making for social robots are focused on biologically inspired mechanisms for taking decisions. Following this approach, we propose a motivational system for decision making, using internal (drives) and external stimuli for learning to choose the right action. Actions are selected from a finite set of skills in order to keep robot's needs within an acceptable range. The robot uses reinforcement learning in order to calculate the suitability of every action in each state. The state of the robot is determined by the dominant motivation and its relation to the objects presents in its environment. The used reinforcement learning method exploits a new algorithm called Object Q-Learning. The proposed reduction of the state space and the new algorithm considering the collateral effects (relationship between different objects) results in a suitable algorithm to be applied to robots living in real environments. In this paper, a first implementation of the decision making system and the learning process is implemented on a social robot showing an improvement in robot's performance. The quality of its performance will be determined by observing the evolution of the robot's wellbeing.The funds provided by the Spanish Government through the project called “Peer
to Peer Robot-Human Interaction” (R2H), of MEC (Ministry of Science and Education), the project “A new approach to social robotics” (AROS), of MICINN (Ministry of Science and Innovation), and the RoboCity2030-II-CM project (S2009/DPI-1559), funded by Programas de Actividades I+D en la Comunidad de Madrid and cofunded by Structural Funds of the EU",Learning the selection of actions for an autonomous social robot by reinforcement learning based on motivations,10.1007/s12369-011-0113-z,,Springer,,core
21090383,22/12/2009,"Abstract. The current paper explores some of the cognitive abilities of an embodied agent based on the DUAL/AMBR architecture. The model has been extended with several capabilities like continuous operation and learning based on encoding of episodes and general knowledge. A new mechanism of analogical transfer has been implemented and demonstrated on a simulated interaction with a user. A focus of interest discussed throughout the paper is how a cognitive model can be embodied in a virtual environment and what are the benefits of combining soft cognitive capabilites with hard AI based platform. The latter is based on a Mind-Body-Environment metaphor which positions the cognitive agent in a situation similar to the one of a robot in a real environment. In the paper, results from simulations of simple interactions with a hypothetical user are presented and the internal cognitive mechanisms are discussed",The Embodiment of a DUAL/AMBR Based Cognitive Model in the RASCALLI Multi-Agent Platform,,,,,core
54278558,2010-01-01T00:00:00,"In real-world robotic applications, many factors, both at low level (e.g., vision, motion control and behaviors) and at high level (e.g., plans and strategies) determine the quality of the robot performance. Consequently, fine tuning of the parameters, in the implementation of the basic functionalities, as well as in the strategic decisions, is a key issue in robot software development. In recent years, machine learning techniques have been successfully used to find optimal parameters for typical robotic functionalities. However, one major drawback of learning techniques is time consumption: in practical applications, methods designed for physical robots must be effective with small amounts of data. In this paper, we present a method for concurrent learning of best strategy and optimal parameters using policy gradient reinforcement learning algorithm. The results of our experimental work in a simulated environment and on a real robot show a very high convergence rate. (C) 2010 Elsevier B.V. All rights reserved",Policy gradient learning for quadruped soccer robots,10.1016/j.robot.2010.03.008,,'Elsevier BV',,core
21094627,2009,"In several agent-oriented scenarios in the real world, an autonomous agent that is situated in an unknown environment must learn through a process of trial and error to take actions that result in long-term benefit. Reinforcement Learning (or sequential decision making) is a paradigm well-suited to this requirement. Value function-based methods and policy search methods are contrasting approaches to solve reinforcement learning tasks. While both classes of methods benefit from independent theoretical analyses, these often fail to extend to the practical situations in which the methods are deployed. We conduct an empirical study to examine the strengths and weaknesses of these approaches by introducing a suite of test domains that can be varied for problem size, stochasticity, function approximation, and partial observability. Our results indicate clear patterns in the domain characteristics for which each class of methods excels. We investigate whether their strengths can be combined, and develop an approach to achieve that purpose. The effectiveness of this approach is also demonstrated on the challenging benchmark task of robot soccer Keepaway. We highlight several lines of inquiry that emanate from this study",An empirical analysis of value function-based and policy search reinforcement learning,,,,,core
45870151,2010-05,"Online model learning in real-time is required by many applications such as in robot tracking control. It poses a difficult problem, as fast and incremental online regression with large data sets is the essential component which cannot be achieved by straightforward usage of off-the-shelf machine learning methods (such as Gaussian process regression or support vector regression). In this paper, we propose a framework for online, incremental sparsification with a fixed budget designed for large scale real-time model learning. The proposed approach combines a sparsification method based on an independence measure with a large scale database. In combination with an incremental learning approach such as sequential support vector regression, we obtain a regression method which is applicable in real-time online learning. It exhibits competitive learning accuracy when compared with standard regression techniques. Implementation on a real robot emphasizes the applicability of the proposed approach in real-time online model learning for real world systems",Incremental Sparsification for Real-time Online Model Learning,,,,,core
11667735,2011-12-12T00:00:00,"Humanoid robotics offers a unique research tool for understanding the human brain and body. The synthesis of human motion is a complex procedure that involves accurate reconstruction of movement sequences, modeling of musculoskeletal kinematics, dynamics and actuation, and characterization of reliable performance criteria. Many of these processes have much in common with the problems found in robotics research, with the recent advent of complex humanoid systems. This work presents the design and development of a new-generation bipedal robot. Its modeling and simulation has been realized by using an open-source software to create and analyze dynamic simulation of movement: OpenSim. Starting from a study by Fuben He, our model aims to be used as an innovative approach to the study of a such type of robot in which there are series elastic actuators represented by active and passive spring components in series with motors. It has provided of monoarticular and biarticular joint in a very similar manner to human musculoskeletal model.  

This thesis is only the starting point of a wide range of other possible future works: from the control structure completion and whole-body control application, to imitation learning and reinforcement learning for human locomotion, from motion test on at ground to motion test on rough ground, and obviously the transition from simulation to practice with a real elastic bipedal robot biologically-inspired that can move like a human bein",Simulating a Flexible Robotic System based on Musculoskeletal Modeling,,https://core.ac.uk/download/11667735.pdf,,,core
62915068,2010-01-01T00:00:00,"High performance robot arms are faster, more accurate, and stronger
than humans.

Yet many manipulation tasks that are easily performed by humans as
part of their daily life are well beyond the capabilities of such
robots.  The main reason for this superiority is that humans can rely
upon neural information processing and control mechanisms which are
tailored for performing complex motor skills, adapting to uncertain
environments and to not imposing a danger to surrounding humans.  As
we are working towards autonomous service robots operating and
performing manipulation in the presence of humans and in human living
and working environments, the robots must exhibit similar levels of
flexibility, compliance, and adaptivity.

The goal of this Dagstuhl seminar is to make a big step towards
pushing robot manipulation forward such that robot assisted living can
become a concrete vision for the future.

In order to achieve this goal, the computational aspects of everyday
manipulation tasks need to be well-understood, and requires the
thorough study of the interaction of 
perceptual, learning, reasoning, planning, and control mechanisms.
The challenges to be met include cooperation with humans, uncertainty
in both task and environments, real-time action requirements, and the
use of tools. The challenges cannot be met by merely improving the
software engineering and programming techniques.  Rather the systems
need built-in capabilities to deal with these challenges. Looking at
natural intelligent systems, the most promising approach for handling
them is to equip the systems with more powerful cognitive mechanisms.

The potential impact of bringing cognition, control and learning
methods together for robotic manipulation can be enormous. This urge
for such concerted approaches is reflected by a large number of
national and international research initiatives including the DARPA
cognitive systems initiative of the Information Processing Technoloy
Office, various integrated projects funded by the European Community,
the British Foresight program for cognitive systems, huge Japanese
research efforts, to name only a few.

As a result, many researchers all over the world engage in cognitive
system research and there is need for and value in discussion. These
discussions become particularly promising because of the growing
readiness of researchers of different disciplines to talk to each
other.

Early results of such interdisciplinary crossfertilization can already
be observed and we only intend to give a few examples: Cognitive
psychologists have presented empirical evidence for the use of
Bayesian estimation and discovered the cost functions possibly
underlying human motor control. Neuroscientists have shown that
reinforcement learning algorithms can be used to explain the role of
Dopamine in the human basal ganglia as well as the functioning of the
bea brain. Computer scientists and engineers have shown that the
understanding of brain mechanisms can result into realiable learning
algorithms as well as control setups. Insights from artificial
intelligence such as Bayesian networks and the associated reasoning
and learning mechanisms have inspired research in cognitive
psychology, in particular the formation of causal theory in young
children.

These examples suggest that (1)~successful computational mechanisms in
artificial cognitive systems tend to have counterparts with similar
functionality in natural cognitive systems; and (2)~new consolidated
findings about the structure and functional organization of perception
and motion control in natural cognitive systems indicate in a number
of cases much better ways of organizing and specifying computational
tasks in artificial cognitive systems","09341 Summary -- Cognition, Control and Learning for Robot Manipulation in Human Environments",,,"Dagstuhl Seminar Proceedings. 09341 - Cognition, Control and Learning for Robot Manipulation in Human Environments",,core
13470390,2010-05-01T00:00:00,"Recently mechanical engineering has extended from a component to a system oriented approach. The structural analysis of components is now completed by the whole mechanical system simulation using multibody system analysis. This evolution aims at capturing better the real loading conditions accounting for the component interaction and couplings in the system. Structural optimization is continuing along the same tracks. Recent works in structural optimization have tried to optimize components with respect to loadings conditions defined through dynamic loading coming from multibody system dynamic analysis. Generally, optimization techniques consider that the structural component is isolated from the rest
of the mechanism and use simplified quasi-static load cases to mimic the complex loadings in service. In contrast, we have shown in previous works devoted to topology optimization that an optimization directly based on the dynamic response of the flexible multibody system leads to a more integrated approach. In order to overcome the limitations of some previous approaches, a more integrated optimization technique is proposed here, based on the nonlinear finite element approach for flexible multibody systems. The non linear finite element formalism accounts for both large rigid-body motions and elastic deflection of the structural components. In previous work, the optimal design of components was realized as an optimal truss / beam layout. The present communication investigates the optimal design of components considered as a continuum medium. We study first sizing optimal design of structural components and we later extend the method to optimal material distribution approach to address their lay out optimization. The continuum domain is discretized into finite elements. For sizing optimization, the design variables are wall-thickness and lumped element variables. For topology optimization the design variables are classically density-like parameters associated to a power law interpolation of effective material properties for intermediate densities, also known as Simply Isotropic
Material with Penalization (SIMP). This study assesses the feasibility of this approach, which extends optimization techniques to continuum flexible bodies included in MBS. The numerical implementation is conducted in SAMCEF MECANO for the flexible MBS analysis and BOSS Quattro for the optimization shell. The nonlinear equations of motion are solved using a generalized-a time integration scheme while the sensitivity analysis of mechanical responses is based on a direct differentiation method or finite differences.
For sizing and parametric optimization the paper investigates and compares several optimization approaches methods such as classical gradient-based methods (SQP, Augmented lagrangian), sequential convex programming methods (CONLIN, MMA), but also surrogate-based optimization method (with Neural Networks) combined with genetic algorithms. The formulation of the problem is also discussed, and its influence on the convergence history is illustrated. Optimal sizing, shape and topology optimization of a simple model of a robot are addressed. The optimization approach is illustrated on numerical applications of sizing optimization of robot
arms during trajectory tracking and lightweight layout optimal design of automotive components",Recent developments in optimization of flexible components of multibody systems,,http://orbi.ulg.ac.be/bitstream/2268/66534/2/Duysinx_ECCM2010_V3.pdf,,,core
211464860,2010-01-01T00:00:00,"The thesis proposes pattern recognition techniques based on machine learning methods and automated data mining; the application domain is systems that evolve over time. It describes algorithms and resolution strategies that combine and exploit features from established methodologies developed in the framework of computational and statistical learning. The attributes of the proposed identification and recognition techniques are: 1) low time-space complexity, 2) incremental and fast learning, 3) real-time response, 4) application independence, 5) simplicity, 6) automated operation, 7) scalability, and 8) distributed computing. After the application of these techniques, the produced solutions are not proved to be the optimal ones; nevertheless they are sufficient and functional. The empirical verification is implemented in real-world problems of different complexity. These problems can be represented by time evolving systems and emerge from the fields of environmental parameters forecasting, voice/speech and image recognition, and video-based content extraction such as human pose recovery and human action recognition. It is known that time evolution occurs in a variety of problems encountered in physics, chemistry, biology, economy and more; thus, the proposed recognition schemes present a wide range of applications. Additionally, the main trait of incremental learning algorithms is the integration of new information into the knowledge base without the need for the model’s retraining. This attribute is applicable to domains where the data flow is continuous such as autonomous robots learning, information retrieval on the Web, human-machine interaction, medical monitoring devices, error diagnosis, security systems, predictive models for natural phenomena and more. The difficulty in defining the analytical form of functions that describe the aforementioned problems, the high dimensionality of data sets, the intervals of intense discontinuity and the presence of high noise make impossible the application of the differential calculus and classical linear or nonlinear analysis. This research work demonstrates experimentally that the adoption of techniques which combine the regression with continuous clustering and classification, allows the handling of non- well defined problems, especially in those cases where the conditions that lead to the convergence to the optimal solution cannot be verified in real world.Η παρούσα διατριβή προτείνει τεχνικές αναγνώρισης προτύπων/μορφωμάτων που βασίζονται σε μεθόδους μηχανικής μάθησης και αυτόματης εξόρυξης δεδομένων, με αντικείμενο εφαρμογής τα συστήματα που εξελίσσονται στο χρόνο. Περιγράφονται αλγόριθμοι και στρατηγικές επίλυσης που συνδυάζουν και αξιοποιούν χαρακτηριστικά από καθιερωμένες μεθοδολογίες που αναπτύχθηκαν στα πλαίσια της υπολογιστικής και στατιστικής μάθησης. Οι ιδιότητες με τις οποίες εφοδιάζονται οι προτεινόμενες τεχνικές αναγνώρισης είναι: 1) χαμηλή χωρο-χρονική πολυπλοκότητα, 2) σταδιακή και ταχεία μάθηση, 3) απόκριση σε πραγματικό χρόνο, 4) ανεξαρτησία από το συγκεκριμένο πεδίο εφαρμογής τους, 5) απλότητα στη χρήση, 6) αυτοματοποιημένη λειτουργία, 7) επεκτασιμότητα, και 8) δυνατότητα κατανεμημένης επεξεργασίας. Οι λύσεις που προκύπτουν μετά την εφαρμογή των εν λόγω τεχνικών δε βεβαιώνεται ότι είναι οι βέλτιστες, ωστόσο είναι επαρκείς και λειτουργικές. Η εμπειρική επαλήθευση υλοποιείται σε διαφορετικής πολυπλοκότητας προβλήματα του πραγματικού κόσμου, τα οποία δύναται ν' αναπαρασταθούν από χρονικώς εξελισσόμενα συστήματα. Τα προβλήματα που αντιμετωπίστηκαν προέρχονται από το χώρο της πρόβλεψης περιβαλλοντικών παραμέτρων, από τους χώρους αναγνώρισης φωνής/ομιλίας και εικόνας, καθώς και από το πεδίο αναγνώρισης περιεχομένου μέσω οπτικών εγγραφών (video), όπως είναι η ανάκτηση του ανθρώπινου σώματος και η αναγνώριση ανθρώπινης δραστηριότητας. Όπως είναι γνωστό, η χρονική εξέλιξη συναντάται σε μια πληθώρα προβλημάτων που εμφανίζονται στη φυσική, στη χημεία, στη βιολογία, στην οικονομία κα., με αποτέλεσμα οι διατάξεις αναγνώρισης που προτείνονται να παρουσιάζουν ένα ευρύ πεδίο εφαρμογών. Επιπρόσθετα, το κύριο χαρακτηριστικό των αλγορίθμων προοδευτικής μάθησης και αναγνώρισης είναι η ενσωμάτωση καινούργιας πληροφορίας στην υπάρχουσα γνωσιακή δεξαμενή επεξεργασμένων δεδομένων όποτε αυτή είναι διαθέσιμη, αποφεύγοντας έτσι το σκόπελο της επανεκπαίδευσης. Αυτή η ιδιότητα βρίσκει εφαρμογή σε προβλήματα όπου υπάρχει συνεχής ροή πληροφορίας, όπως στην εκμάθηση των αυτόνομων ρομπότ, στην ανάκτηση πληροφοριών στον Παγκόσμιο Ιστό, στην αλληλεπίδραση ανθρώπου-μηχανής, σε συσκευές ιατρικής παρακολούθησης, στη διάγνωση σφαλμάτων, σε συστήματα ασφαλείας, σε προγνωστικά μοντέλα φυσικών φαινομένων κα. Η δυσκολία ορισμού της αναλυτικής μορφής των συναρτήσεων που περιγράφουν τα εν λόγω προβλήματα, η υψηλή διαστατικότητα των συνόλων δεδομένων, τα διαστήματα έντονης ασυνέχειας, καθώς και η ύπαρξη υψηλού θορύβου καθιστούν ανέφικτη την εφαρμογή του διαφορικού λογισμού και της κλασικής γραμμικής ή μη-γραμμικής ανάλυσης. Στην παρούσα ερευνητική εργασία αποδεικνύεται πειραματικά ότι η υιοθέτηση τεχνικών που συνδυάζουν την παλινδρόμηση με τη συνεχή ομαδοποίηση και ταξινόμηση, επιτρέπει το χειρισμό μη-καλώς ορισμένων προβλημάτων, ειδικά εκείνων των περιπτώσεων όπου οι συνθήκες που διαμορφώνουν τη σύγκλιση στη βέλτιστη λύση δεν μπορούν να επαληθευτούν στο φυσικό κόσμο","Incremental machine learning methods in time-dependent problems: pattern, time-series and system recognition applications in real-time decision-making",10.12681/eadd/24658,,'National Documentation Centre (EKT)',,core
21652812,2009,"In this paper we describe simulation of autonomous robots controlled by recurrent neural networks, which are evolved through indirect encoding using HyperNEAT algorithm. The robots utilize 180 degree wide sensor array. Thanks to the scalability of the neural network generated by HyperNEAT, the sensor array can have various resolution. This would allow to use camera as an input for neural network controller used in real robot. The robots were simulated using software simulation environment. In the experiments the robots were trained to drive with imaximum average speed. Such fitness forces them to learn how to drive on roads and avoid collisions. Evolved neural networks show excellent scalability. Scaling of the sensory input breaks performance of the robots, which should be gained back with re-training of the robot with a different sensory input resolution",HyperNEAT controlled robots learn to drive on roads in . . .,,,,,core
20326398,2011-01-01T00:00:00,"Új algoritmuscsaládot fejlesztettünk ki, a bakteriális memetikus algoritmusokat, a bakteriális evolúciós algoritmust globális, a Levenberg-Marquard algoritmust pedig lokális keresésként alkalmazva. Ez az eljárás jobb algoritmusokat eredményezett az ismert hasonló módszereknél a pontosság és a ciklusszám összefüggésében; ezt különböző referencia alkalmazások és más példák segítségével bizonyítottuk.  Az alkalmazások másik csoportját a logisztika adta. Kiterjesztettük Kano minőségi modelljét fuzzy exponensekre, melyet BMA-val optimalizáltunk és megkezdtük az utazó ügynök probléma közelítő megoldásának vizsgálatát is. Megmutattuk, hogy a fuzzy szabályinterpoláció számos valós probléma megoldására alkalmas. Sikeresen foglalkoztunk komplex forgalomirányítási alkalmazásokkal, továbbá vasúti menetrend és késés miatti átütemezés kérdéskörével. Szoftverrendszert implementáltunk, mely nagyszámú fuzzy következtetési és irányítási algoritmus összehasonlítására alkalmas. Kiterjesztettük a fuzzy szignatúrákat hierarchikus struktúrákra is és a Mamdani algoritmusra is. A fuzzy szignatúrákat robotok mozgásirányítására és kommunikációjára alkalmaztuk. E robotokat szimulációs technikával és saját fejlesztésű hardver segítségével is vizsgáltuk. Új kutatási részterületet indítottunk el a fuzzy műveletek és a rajtuk alapuló fuzzy flip-floppok vizsgálatával, melyekből konnekcionista rendszereket hoztunk létre és e fuzzy neurális hálózatokat modellkonstrukcióra és approximációra alkalmaztuk.  |  We developed a new family of algorithms, the Bacterial Memetic Algorithms by combining the Bacterial Evolutionary Algorithm as a global search and the Levenberg-Marquard algorithm as a local search method. This approach provided better algorithms in terms of approximation accuracy and population cycles than other similar approaches in the literature, as it was evidenced by various benchmark and real life applications. Another group of successful applications is in the logistics area. We extended Kano’s quality model to fuzzy exponents, optimized by BMA, and we started to research for the approximate solution of the Traveling Salesman Problem.We showed that fuzzy rule interpolation could be deployed for a number of real application areas. We dealt with complex traffic control applications as well as with railway time table and delay triggered rescheduling problems successfully. We implemented a software for the comparison of a large number of fuzzy reasoning and control algorithms. We extended Fuzzy Signatures to both hierarchical structures and Mamdani’s algorithm. We applied Fuzzy Signatures for motion control and fuzzy communication of robots. Such robots were investigated both in simulation and hardware construction developed by ourselves. We started a new research sub-direction by analyzing fuzzy operators and fuzzy flip-flops based on them. We built connectionist systems from them and we used these fuzzy neural networks for model construction and approximation",Fuzzy rendszerek és modellek elemzése és identifikációja  =  Analysis and identification of fuzzy systems and models,,https://core.ac.uk/download/20326398.pdf,OTKA,,core
45869587,2011-05,"For many applications such as compliant, accurate robot tracking control, dynamics models learned from data can help to achieve both compliant control performance as well as high tracking quality. Online learning of these dynamics models allows the robot controller to adapt itself to changes in the dynamics (e.g., due to time-variant nonlinearities or unforeseen loads). However, online learning in real-time applications -- as required in control -- cannot be realized by straightforward usage of off-the-shelf machine learning methods such as Gaussian process regression or support vector regression. In this paper, we propose a framework for online, incremental sparsification with a fixed budget designed for fast real-time model learning. The proposed approach employs a sparsification method based on an independence measure. In combination with an incremental learning approach such as incremental Gaussian process regression, we obtain a model approximation method which is applicable in real-time online learning. It exhibits competitive learning accuracy when compared with standard regression techniques. Implementation on a real Barrett WAM robot demonstrates the applicability of the approach in real-time online model learning for real world systems",Incremental online sparsification for model learning in real-time robot control,10.1016/j.neucom.2010.06.033,,,,core
145023543,2010-01-01T08:00:00Z,"Analyzing and understanding human biosignals have been important research areas that have many practical applications in everyday life.  For example, Brain Computer Interface is a research area that studies the connection between the human brain and external systems by processing and learning the brain signals called Electroencephalography (EEG) signals.  Similarly, various assistive robotics applications are being developed to interpret eye or muscle signals in humans in order to provide control inputs for external devices. The efficiency for all of these applications depends heavily on being able to process and classify human biosignals. Therefore many techniques from Signal Processing and Machine Learning fields are applied in order to understand human biosignals better and increase the efficiency and success of these applications. This thesis proposes a new classifier for biosignal data classification utilizing Particle Swarm Optimization Clustering and Radial Basis Function Networks (RBFN). The performance of the proposed classifier together with several variations in the technique is analyzed by utilizing comparisons with the state of the art classifiers such as Fuzzy Functions Support Vector Machines (FFSVM), Improved Fuzzy Functions Support Vector Machines (IFFSVM). These classifiers are implemented on the classification of same biological signals in order to evaluate the proposed technique. Several clustering algorithms, which are used in these classifiers, such as K-means, Fuzzy c-means, and Particle Swarm Optimization (PSO), are studied and compared with each other based on clustering abilities. The effects of the analyzed clustering algorithms in the performance of Radial Basis Functions Networks classifier are investigated. Strengths and weaknesses are analyzed on various standard and EEG datasets. Results show that the proposed classifier that combines PSO clustering with RBFN classifier can reach or exceed the performance of these state of the art classifiers. Finally, the proposed classification technique is applied to a real-time system application where a mobile robot is controlled based on person\u27s EEG signal",A Study of recent classification algorithms and a novel approach for biosignal data classification,,,RIT Scholar Works,,core
144212904,2009-01-01T08:00:00Z,"Automatically navigated cars that drive on their own and robots that can perform any manual work science fiction authors predicted that it would all be possible in the 20th Century (see Asimov, 1951 \u22I, Robot\u22).  In the dawn of artificial intelligence, the main obstacle that has to be overcome was creating robots that are able to solve logical problems.  Humans, as far as we know, are unique in being able to solve abstract logical problems.  Thus, creating a robot to mimic this ability is an extremely complex task.
What researchers initially perceived as a much easier task - having a robot navigate and interact with the physical environment, however, proved equally difficult.  Robot vision and localization are just two of the tasks that need to be solved effectively before robots can truly do what Asimov predicted.
Robot localization specifically is at the heart of any artificial intelligence system that needs to navigate in the physical world.  It is not possible for a robot to carry out tasks that involve moving in the environment if every time it moves, it gets lost.
In this paper we examine an implementation of the Monte Carlo localization algorithm for the Pioneer 2 robots.  Our implementation of the algorithm utilizes the robot motion sensors, sonar sensors and camera to gain as detailed a picture as possible of its environment to allow it to navigate successfully",Solving the Maze:  Robot Localization Using the Monte Carlo Localization Algorithm and Shape Context,,,DigitalCommons@Macalester College,,core
148745821,2011-10-31T00:00:00,"The adoption of applicable artificial intelligence technologies to library real-time virtual reference services is an innovative experimentation in one of the key areas of library services. Based on the open source software Artificial Linguistic Internet Computer Entity (A.L.I.C.E.) and a combined application of several other relevant supporting technologies for facilitating the use of the current existing library resources, Tsinghua University Library has recently developed a real-time smart talking robot, named Xiaotu, for the enhancement of its various service functions, such as reference services, book searching, Baidu Baike searching, self-directed learning, etc. The operation of Xiaotu is programmed into Renren website (a social networking website), which adds significantly an innovative feature to the modus operandi of the real-time virtual reference service at Tsinghua University Library.</p",Real-time virtual reference service based on applicable artificial intelligence technologies: The début of the robot Xiaotu at Tsinghua University Library,,,,,core
9703890,2010-01-01T00:00:00,"In mobile robotics, a solid test for adaptation is

the ability of a control system to function not only in a

diverse number of physical environments, but also on a

number of different robotic platforms. This paper demonstrates that a set of behaviours evolved in simulation on a miniature robot (epuck) can be transferred to a much larger-scale platform (Pioneer), both in simulation and in the real world. The chosen architecture uses artificial evolution of epuck behaviours to obtain a genetic sequence, which is then employed to seed an idiotypic, artificial immune system (AIS) on the Pioneers. Despite numerous hardware and software differences between the platforms, navigation and target-finding experiments show that the evolved behaviours transfer very well to the larger robot when the idiotypic AIS technique is used. In contrast, transferability is poor when reinforcement learning alone is used, which validates the adaptability of the chosen architecture",Real-world transfer of evolved artificial immune system behaviours between small and large scale robotic platforms,10.1007/s12065-010-0039-7,https://core.ac.uk/download/9703890.pdf,'Springer Science and Business Media LLC',,core
11983069,2011-10-01T00:00:00,"Conjugate Gradient BackPropagation (CGBP) is one backpropagation method wich have complexity higher than Gradient Descent BackPropagation. We tried implemented on our wheel robot to avoid obstacles because there are still no many researcher using this method to their artificial intelligence systems. They told us about the complexity of building this method. We thought that one control systems which have more complexity than a famous one, would be have a better performance. We hoped this method could handle a blank spot of training of Neural Network which most of the people got the same problem with blank spot training. This wheel robot have 3 sonar sensors, and 2 motors. This robot realized in webots simulator environment which complected all physics condition so we can build many obstacles on it like a real world implemented. We train the robot without a cylindric shape obstacle. For running process, we provided with 2 conditions, without an obstacle and with some  obstacles. The result is successful about 100% to get wall follower action without any an obstacle. And without got a training with some an obstacles before, we got the results about 50% success. This conclude that CGBP still can not handle the blank spot of training and just it must to train with an obstacles to get the better successful",Conjugate Gradient Backpropagation Implemented On Obstacle Avoidance Wheel Robot In Webots,,https://core.ac.uk/download/11983069.pdf,,,core
41213247,2011-01-01T00:00:00,"In critical domains such as urban search and rescue (USAR), and bomb disposal, the deployment of teleoperated robots is essential to reduce the risk of first responder personnel. Teleoperation is a difficult task, particularly when controlling robots from an isolated safety zone. In general, the operator has to solve simultaneously the problems of mission planning, target identification, robot navigation, and robot control. We introduce a system to support teleoperated navigation with real-time mapping consisting of a two-step scan matching method that re-considers data associations during the search. The algorithm processes data from laser range finder and gyroscope only, thereby it is independent from the robot platform. Furthermore, we introduce a user-guided procedure for improving the global consistency of maps generated by the scan matcher. Globally consistent maps are computed by a graph-based maximum likelihood method that is biased by localizing crucial parts of the scan matcher trajectory on a prior given geo-tiff image. The approach has been implemented as an embedded system and extensively tested on robot platforms designed for teleoperation in critical situations, such as bomb disposal. Furthermore, the system was evaluated in a test maze by first responders during the Disaster City event in Texas 2008.Artificial Intelligence & Integrated Computer System",Mapping for the Support of First Responders in Critical Domains,10.1007/s10846-010-9520-x,,'Springer Science and Business Media LLC',,core
21171822,25/03/2010,"“Intelligence without Reason, ” Brooks argues for mobile robots as the foundation of AI research. This article argues that even if we seek to investigate complete agents in real-world environments, robotics is neither necessary nor sufficient as a basis for AI research. The article proposes real-world software environments, such as operating systems or databases, as a complementary substrate for intelligent-agent research and considers the relative advantages of software environments as test beds for AI. First, the cost, effort, and expertise necessary to develop and systematicall",without Representation ” and,,,,,core
21057500,24/08/2009,"ARCHON ™ (ARchitecture for Cooperative Heterogeneous ON-line systems) was Europe’s largest project in the area of Distributed Artificial Intelligence (DAI). It devised a general-purpose architecture, software framework, and methodology which has been used to support the development of DAI systems in a number of real world industrial domains. Some examples of the applications to which it has been successfully applied include: electricity distribution and supply, electricity transmission and distribution, control of a cement kiln complex, control of a particle accelerator, and control of a robotics application. The type of cooperating community that it supports has a decentralised control regime and individual problem solving agents which are large grain, loosely coupled, and semi-autonomous. This paper will tackle a broad range of issues related to the application of ARCHON technology to industrial applications. Firstly, it gives the rationale for a DAI approach to industrial applications and highlights the characteristics which typify this important domain. Secondly, the ARCHON framework is detailed- with a special emphasis being placed upon the implementation architecture. Thirdly, a brief resumee and status report of the main applications is presented. Finally, the lessons learned and the future plans are presented. 1","Mile End Road,",,,,,core
213389987,2009-01-01T00:00:00,"This dissertation focuses on the collaboration of multiple heterogeneous, intelligent agents (hardware or software) which collaborate to learn a task and are capable of sharing knowledge. The concept of collaborative learning in multi-agent and multi-robot systems is largely under studied, and represents an area where further research is needed to gain a deeper understanding of team learning. This work presents experimental results which illustrate the importance of heterogeneous teams of collaborative learning agents, as well as outlines heuristics which govern successful construction of teams of classifiers. A number of application domains are studied in this dissertation. One approach is focused on the effects of sharing knowledge and collaboration of multiple heterogeneous, intelligent agents (hardware or software) which work together to learn a task. As each agent employs a different machine learning technique, the system consists of multiple knowledge sources and their respective heterogeneous knowledge representations. Collaboration between agents involves sharing knowledge to both speed up team learning, as well as to refine the team's overall performance and group behavior. Experiments have been performed that vary the team composition in terms of machine learning algorithms, learning strategies employed by the agents, and sharing frequency for a predator-prey cooperative pursuit task. For lifelong learning, heterogeneous learning teams were more successful compared to homogeneous learning counterparts. Interestingly, sharing increased the learning rate, but sharing with higher frequency showed diminishing results. Lastly, knowledge conflicts are reduced over time, as more sharing takes place. These results support further investigation of the merits of heterogeneous learning. This dissertation also focuses on discovering heuristics for constructing successful teams of heterogeneous classifiers, including many aspects of team learning and collaboration. In one application, multi-agent machine learning and classifier combination are utilized to learn rock facies sequences from wireline well log data. Gas and oil reservoirs have been the focus of modeling efforts for many years as an attempt to locate zones with high volumes. Certain subsurface layers and layer sequences, such as those containing shale, are known to be impermeable to gas and/or liquid. Oil and natural gas then become trapped by these layers, making it possible to drill wells to reach the supply, and extract for use. The drilling of these wells, however, is costly. Here, the focus is on how to construct a successful set of classifiers, which periodically collaborate, to increase the classification accuracy. Utilizing multiple, heterogeneous collaborative learning agents is shown to be successful for this classification problem. We were able to obtain 84.5% absolute accuracy using the Multi-Agent Collaborative Learning Architecture, an improvement of about 6.5% over the best results achieved by Kansas Geological Survey with the same data set. Several heuristics are presented for constructing teams of multiple collaborative classifiers for predicting rock facies. Another application utilizes multi-agent machine learning and classifier combination to learn water presence using airborne polar radar data acquired from Greenland in 1999 and 2007. Ground and airborne depth-soundings of the Greenland and Antarctic ice sheets have been used for many years to determine characteristics such as ice thickness, subglacial topography, and mass balance of large bodies of ice. Ice coring efforts have supported these radar data to provide ground truth for validation of the state (wet or frozen) of the interface between the bottom of the ice sheet and the underlying bedrock. Subglacial state governs the friction, flow speed, transport of material, and overall change of the ice sheet. In this dissertation, we focus on how to construct a successful set of classifiers which periodically collaborate to increase classification accuracy. The underlying method results in radar independence, allowing model transfer from 1999 to 2007 to produce water presence maps of the Greenland ice sheet with differing radars. We were able to obtain 86% accuracy using the Multi-Agent Collaborative Learning Architecture with this data set. Utilizing multiple, heterogeneous collaborative learning agents is shown to be successful for this classification problem as well. Several heuristics, some of which agree with those found in the other applications, are presented for constructing teams of multiple collaborative classifiers for predicting subglacial water presence. General findings from these different experiments suggest that constructing a team of classifiers using a heterogeneous mixture of homogeneous teams is preferred. Larger teams generally perform better, as decisions from multiple learners can be combined to arrive at a consensus decision. Employing heterogeneous learning algorithms integrates different error models to arrive at higher accuracy classification from complementary knowledge bases. Collaboration, although not found to be universally useful, offers certain team configurations an advantage. Collaboration with low to medium frequency was found to be beneficial, while high frequency collaboration was found to be detrimental to team classification accuracy. Full mode learning, where each learner receives the entire training set for the learning phase, consistently outperforms independent mode learning, where the training set is distributed to all learners in a team in a non-overlapping fashion. Results presented in this dissertation support the application of multi-agent machine learning and collaboration to current challenging, real-world classification problems",Collective Machine Learning: Team Learning and Classification in Multi-Agent Systems,,https://core.ac.uk/download/213389987.pdf,'Paleontological Institute at The University of Kansas',,core
13939734,2011-07-31T00:00:00,"Technological and conceptual advances in fields such as artificial intelligence, robotics, and material science have enabled robotic architectural environments to be implemented and tested in the last decade in virtual and physical prototypes. These prototypes are incorporating sensing-actuating mechanisms that enable interaction with their users and surroundings in real-time. While these prototypes obviously point towards a paradigm shift from inanimate towards animate architecture, they do not operate at building but at building component scale and do not address socio-economical or environmental aspects that affect architecture and society at large. This paper, on the one hand, critically discusses robotic prototypes built in the last decade at Delft University of Technology, on the other hand, it proposes a framework for future research envisioning robotic environments, as resizable, able to spatially expand or contract as well as move or be moved as needed. Such reconfigurable environments aim to validate the assumption that robotics incorporated in architecture improve efficiency of use due to multiple use of built space in condensed timeframes, while at the same time they advance technology for distributed autonomous robotic systems exhibiting collective behavior as well as test their application to sustainable architecture.ArchitectureArchitectur",Robotic environments,10.22260/isarc2011/0160,,"IAARC, International Association for Automation and Robotics in Construction",,core
35168826,2009-01-01T00:00:00,"In Artificial Intelligence, numerous learning paradigms have been developed over the past decades. In most cases of embodied and situated agents, the learning goal for the artificial agent is to „map“ or classify the environment and the objects therein [1, 2], in order to improve navigation or the execution of some other domain-specific task. Dynamic environments and changing tasks still pose a major challenge for robotic learning in real-world domains. In order to intelligently adapt its task strategies, the agent needs cognitive abilities to more deeply understand its environment and the effects of its actions. In order to approach this challenge within an open-ended learning loop, the XPERO project (http://www.xpero.org) explores the paradigm of Learning by Experimentation to increase the robot's conceptual world knowledge autonomously. In this setting, tasks which are selected by an actionselection mechanism are interrupted by a learning loop in those cases where the robot identifies learning as necessary for solving a task or for explaining observations. It is important to note that our approach targets unsupervised learning, since there is no oracle available to the agent, nor does it have access to a reward function providing direct feedback on the quality of its learned model, as e.g. in reinforcement learning approaches. In the following sections we present our framework for integrating autonomous robotic experimentation into such a learning loop. In section 1 we explain the different modules for stimulation and design of experiments and their interaction. In section 2 we describe our implementation of these modules and how we applied them to a real world scenario to gather target-oriented data for learning conceptual knowledge. There we also indicate how the goaloriented data generation enables machine learning algorithms to revise the failed prediction model",Autonomous Design of Experiments for Learning by Experimentation,,,,,core
21273745,14/09/2010,"Computational concepts of cognition, their implementation in complex autonomous systems, and their empirical evaluation are key techniques to understand and validate concepts of cognition and intelligence. In this paper we want to describe computational concepts of cognition that were successfully implemented in the domain of soccer playing robots and show the interactions between cognitive concepts, software engineering and real time application development. Beside a description of the general concepts we will focus on aspects of perception, behavior architecture, and reinforcement learning. Key words: autonomous robots, reinforcement learning, behavior architecture, robot soccer 1",Cognitive Concepts in Autonomous Soccer Playing Robots,10.1016/j.cogsys.2009.12.003,,,,core
21098948,2009,"In several agent-oriented scenarios in the real world, an autonomous agent that is situated in an unknown environment must learn through a process of trial and error to take actions that result in long-term benefit. Reinforcement Learning (or sequential decision making) is a paradigm well-suited to this requirement. Value function-based methods and policy search methods are contrasting approaches to solve reinforcement learning tasks. While both classes of methods benefit from independent theoretical analyses, these often fail to extend to the practical situations in which the methods are deployed. We conduct an empirical study to examine the strengths and weaknesses of these approaches by introducing a suite of test domains that can be varied for problem size, stochasticity, function approximation, and partial observability. Our results indicate clear patterns in the domain characteristics for which each class of methods excels. We investigate whether their strengths can be combined, and develop an approach to achieve that purpose. The effectiveness of this approach is also demonstrated on the challenging benchmark task of robot soccer Keepaway. We highlight several lines of inquiry that emanate from this study",An empirical analysis of value function-based and policy search reinforcement learning,,,,,core
21216987,13/05/2010,"Theorists agree that real-time algorithms are an interesting new topic in the field of cyberinformatics, and electrical engineers concur [2,4,16,16,23,32,49, 73,87,97]. After yearsofprivateresearchintoMarkov models, we disconfirm the deployment of neural networks, which embodies the theoretical principles of robotics. In order to achieve this ambition, we use low-energy communication to demonstrate that the infamous Bayesian algorithm for the essential unification ofthe Internet and Internet QoS by O. L. Bose runs in Ω(2 n) time [13,16,29,29,37,39,67,73,93,97]",IPv4 Considered Harmful,,,,,core
210948176,2010-05-01T00:00:00,"Online model learning in real-time is required by many applications such as in robot tracking control. It poses a difficult problem, as fast and incremental online regression with large data sets is the essential component which cannot be achieved by straightforward usage of off-the-shelf machine learning methods (such as Gaussian process regression or support vector regression). In this paper, we propose a framework for online, incremental sparsification with a fixed budget designed for large scale real-time model learning. The proposed approach combines a sparsification method based on an independence measure with a large scale database. In combination with an incremental learning approach such as sequential support vector regression, we obtain a regression method which is applicable in real-time online learning. It exhibits competitive learning accuracy when compared with standard regression techniques. Implementation on a real robot emphasizes the applicability of the proposed approach in real-time online model learning for real world systems",Incremental Sparsification for Real-time Online Model Learning,,,,,core
36204601,2011-01-01T00:00:00,"In critical domains such as urban search and rescue (USAR), and bomb disposal, the deployment of teleoperated robots is essential to reduce the risk of first responder personnel. Teleoperation is a difficult task, particularly when controlling robots from an isolated safety zone. In general, the operator has to solve simultaneously the problems of mission planning, target identification, robot navigation, and robot control. We introduce a system to support teleoperated navigation with real-time mapping consisting of a two-step scan matching method that re-considers data associations during the search. The algorithm processes data from laser range finder and gyroscope only, thereby it is independent from the robot platform. Furthermore, we introduce a user-guided procedure for improving the global consistency of maps generated by the scan matcher. Globally consistent maps are computed by a graph-based maximum likelihood method that is biased by localizing crucial parts of the scan matcher trajectory on a prior given geo-tiff image. The approach has been implemented as an embedded system and extensively tested on robot platforms designed for teleoperation in critical situations, such as bomb disposal. Furthermore, the system was evaluated in a test maze by first responders during the Disaster City event in Texas 2008.Artificial Intelligence & Integrated Computer System",Mapping for the Support of First Responders in Critical Domains,10.1007/s10846-010-9520-x,,'Springer Science and Business Media LLC',,core
13631690,2011-01-01T08:00:00,"This thesis develops robotic skills for manipulating novel articulated objects. The degrees of freedom of an articulated object describe the relationship among its rigid bodies, and are often relevant to the object\u27s intended function. Examples of everyday articulated objects include scissors, pliers, doors, door handles, books, and drawers. Autonomous manipulation of articulated objects is therefore a prerequisite for many robotic applications in our everyday environments. Already today, robots perform complex manipulation tasks, with impressive accuracy and speed, in controlled environments such as factory floors. An important characteristic of these environments is that they can be engineered to reduce or even eliminate perception. In contrast, in unstructured environments such as our homes and offices, perception is typically much more challenging. Indeed, manipulation in these unstructured environments remains largely unsolved. We therefore assume that to enable autonomous manipulation of objects in our everyday environments, robots must be able to acquire information about these objects, making as few assumption about the environment as possible. Acquiring information about the world from sensor data is a challenging problem. Because there is so much information that could be measured about the environment, considering all of it is impractical given current computational speeds. Instead, we propose to leverage our understanding of the task, in order to determine the relevant information. In our case, this information consists of the object\u27s shape and kinematic structure. Perceiving this task-specific information is still challenging. This is because in order to understand the object\u27s degrees of freedom, we must observe relative motion between its rigid bodies. And, as relative motion is not guaranteed to occur, this information may not be included in the sensor stream. The main contribution of this thesis is the design and implementation of a robotic system capable of perceiving and manipulating articulated objects. This system relies on Interactive Perception, an approach which exploits the synergies that arise when crossing the boundary between action and perception. In interactive perception, the emphasis of perception shifts from object appearance to object function. To enable the perception and manipulation of articulated objects, this thesis develops algorithms for perceiving the kinematic structure and shape of objects. The resulting perceptual capabilities are used within a relational reinforcement learning framework, enabling a robot to obtain general domain knowledge for manipulation. This composition enables our robot to reliably and efficiently manipulate novel articulated objects. To verify the effectiveness of the proposed robotic system, simulated and real-world experiments were conducted with a variety of everyday objects",Interactive perception of articulated objects for autonomous manipulation,,,ScholarWorks@UMass Amherst,,core
41213264,2009-01-01T00:00:00,"Urban Search And Rescue (USAR) is a time critical task since all survivors have to be rescued within the first 72 hours. One goal in Rescue Robotics is to support emergency response by mixed-initiative teams consisting of humans and robots. Their task is to explore the disaster area rapidly while reporting victim locations and hazardous areas to a central station, which then can be utilized for planning rescue missions. To fulfill this task efficiently, humans and robots have to map disaster areas jointly while co- ordinating their search at the same time. Additionally, robots have to perform subproblems, such as victim detection and navigation, autonomously. In disaster areas these problems are extraordinarily challenging due to the unstructured environment and rough terrain. Furthermore, when communication fails, methods that are deployed under such conditions have to be decentralized, i.e. operational without a central station. In this thesis a unified approach joining human and robot resources for solving these problems is contributed. Following the vision of combined multi-robot and multi-human teamwork, core problems, such as position tracking on rough terrain, mapping by mixed teams, and decentralized team coordination with limited radio communication, are directly addressed. More specific, RFID-SLAM, a novel method for robust and efficient loop closure in large-scale environments that utilizes RFID technology for data association, is contributed. The method is capable of jointly improving multiple maps from humans and robots in a centralized and decentralized manner without requiring team members to perform loops on their routes. Thereby positions of humans are tracked by PDR (Pedestrian Dead Reckoning), and robot positions by slippage- sensitive odometry, respectively. The joint-graph emerging from these trajectories serves as an input for an iterative map optimization procedure. The introduced map representation is further utilized for solving the centralized and decentralized coordination of large rescue teams. On the one hand, a deliberate method for combined task assignment and multi-agent path planning, and on the other hand, a local search method using the memory of RFIDs for coordination, are proposed. For autonomous robot navigation on rough terrain and real-time victim detection in disaster areas an efficient method for elevation map building and a novel approach to genetic MRF (Markov Random Field) model optimization are contributed. Finally, a human in the loop architecture is presented that integrates data collected by first responders into a multi-agent system via wearable computing. In this context, the support and coordination of disaster mitigation in large-scale environments from a central-command-post-perspective are described. Methods introduced in this thesis were extensively evaluated in outdoor environments and official USAR testing arenas designed by the National Institute of Standards and Technology (NIST). Furthermore, they were an integral part of systems that won in total more than 10 times the first prize at international competitions, such as the RoboCup world championships.This is a Ph.D. thesis originally defended at University of Freiburg.Artificial Intelligence & Integrated Computer System",Mapping and Exploration for Search and Rescue with Humans and Mobile Robots,,,Freiburg : University of Freiburg,,core
270341124,2010-10-30T00:00:00,International audienceThis paper describes experimental results regarding the real time implementation of continuous time recurrent neural networks (CTRNN) and the dynamic back-propagation through time (BPTT) algorithm for the on-line learning control laws. Experiments are carried out to control the balance of a biped robot prototype in its standing posture. The neural controller is trained to compensate for external perturbations by controlling the torso's joint motions. Algorithms are embedded in the real time electronic unit of the robot. On-line learning implementations are presented in detail. The results on learning behavior and control performance demonstrate the strength and the efficiency of the proposed approach,Real time implementation of CTRNN and BPTT algorithm to learn on-line biped robot balance: Experiments on the standing posture,10.1016/j.conengprac.2010.10.002,,'Elsevier BV',,core
148440413,2009-01-01T00:00:00,"Teleoperation is a difficult task, particularly when controlling robots from an isolated operator station. In general, the operator has to solve nearly blindly the problems of mission planning, target identification, robot navigation, and robot control at the same time. The goal of the proposed system is to support teleoperated navigation with real-time mapping. We present a novel scan matching technique that re-considers data associations during the search, enabling robust pose estimation even under varying roll and pitch angle of the robot enabling mapping on rough terrain. The approach has been implemented as an embedded system and extensively tested on robot platforms designed for teleoperation in critical situations, such as bomb disposal. Furthermore, the system has been evaluated in a test maze by first responders during the Disaster City event in Texas 2008. Finally, experiments conducted within different environments show that the system yields comparably accurate maps in real-time when compared to higher sophisticated offline methods, such as Rao-Blackwellized SLAM.(Best Paper Award Finalist)Artificial Intelligence & Integrated Computer System",Operator-Assistive Mapping in Harsh Environments,10.1109/SSRR.2009.5424159,,'Institute of Electrical and Electronics Engineers (IEEE)',,core
145022591,2009-08-01T07:00:00Z,"Real-time simulations of biological neural networks (BNNs) provide a natural platform for applications in a variety of fields: data classification and pattern recognition, prediction and estimation, signal processing, control and robotics, prosthetics, neurological and neuroscientific modeling. BNNs possess inherently parallel architecture and operate in continuous signal domain. Spiking neural networks (SNNs) are type of BNNs with reduced signal dynamic range: communication between neurons occurs by means of time-stamped events (spikes). SNNs allow reduction of algorithmic complexity and communication data size at a price of little loss in accuracy. Simulation of SNNs using traditional sequential computer architectures results in significant time penalty. This penalty prohibits application of SNNs in real-time systems. Graphical processing units (GPUs) are cost effective devices specifically designed to exploit parallel shared memory-based floating point operations applied not only to computer graphics, but also to scientific computations. This makes them an attractive solution for SNN simulation compared to that of FPGA, ASIC and cluster message passing computing systems. Successful implementations of GPU-based SNN simulations have been already reported. The contribution of this thesis is the development of a scalable GPU-based realtime system that provides initial framework for design and application of SNNs in various domains. The system delivers an interface that establishes communication with neurons in the network as well as visualizes the outcome produced by the network. Accuracy of the simulation is emphasized due to its importance in the systems that exploit spike time dependent plasticity, classical conditioning and learning. As a result, a small network of 3840 Izhikevich neurons implemented as a hybrid system with Parker-Sochacki numerical integration method achieves real time operation on GTX260 device. An application case study of the system modeling receptor layer of retina is reviewed",GPU-based implementation of real-time system for spiking neural networks,,,RIT Scholar Works,,core
229063559,2010-12-01T08:00:00,"This research compares the behavior of three robot navigation controllers namely: PID, Artificial Neural Networks (ANN), and Fuzzy Logic (FL), that are used to control the same autonomous mobile robot platform navigating a real unknown indoor environment that contains simple geometric-shaped static objects to reach a goal in an unspecified location. In particular, the study presents and compares the design, simulation, hardware implementation, and testing of these controllers. The first controller is a traditional linear PID controller, and the other two are intelligent non-linear controllers, one using Artificial Neural Networks and the other using Fuzzy Logic Techniques. Each controller is simulated first in MATLAB® using the Simulink Toolbox. Later the controllers are implemented using Quartus ll® software and finally the hardware design of each controller is implemented and downloaded to a Field-Programmable Gate Array (FPGA) card which is mounted onto the mobile robot platform. The response of each controller was tested in the same physical testing environment using a maze that the robot should navigate avoiding obstacles and reaching the desired goal. To evaluate the controllers\u27 behavior each trial run is graded with a standardized rubric based on the controllers\u27 ability to react to situations presented within the trial run. The results of both the MATLAB® simulation and FPGA implementation show the two intelligent controllers, ANN and FL, outperformed the PID controller. The ANN controller was marginally superior to the FL controller in overall navigation and intelligence",Design and Development of Intelligent Navigation Control Systems for Autonomous Robots that Uses Neural Networks and Fuzzy Logic Techniques and Fpga For Its Implementation,,https://core.ac.uk/download/229063559.pdf,Digital Commons@Georgia Southern,,core
13621323,2011-09-01T07:00:00,"This thesis develops robotic skills for manipulating novel articulated objects. The degrees of freedom of an articulated object describe the relationship among its rigid bodies, and are often relevant to the object\u27s intended function. Examples of everyday articulated objects include scissors, pliers, doors, door handles, books, and drawers. Autonomous manipulation of articulated objects is therefore a prerequisite for many robotic applications in our everyday environments. Already today, robots perform complex manipulation tasks, with impressive accuracy and speed, in controlled environments such as factory floors. An important characteristic of these environments is that they can be engineered to reduce or even eliminate perception. In contrast, in unstructured environments such as our homes and offices, perception is typically much more challenging. Indeed, manipulation in these unstructured environments remains largely unsolved. We therefore assume that to enable autonomous manipulation of objects in our everyday environments, robots must be able to acquire information about these objects, making as few assumption about the environment as possible. Acquiring information about the world from sensor data is a challenging problem. Because there is so much information that could be measured about the environment, considering all of it is impractical given current computational speeds. Instead, we propose to leverage our understanding of the task, in order to determine the relevant information. In our case, this information consists of the object\u27s shape and kinematic structure. Perceiving this task-specific information is still challenging. This is because in order to understand the object\u27s degrees of freedom, we must observe relative motion between its rigid bodies. And, as relative motion is not guaranteed to occur, this information may not be included in the sensor stream. The main contribution of this thesis is the design and implementation of a robotic system capable of perceiving and manipulating articulated objects. This system relies on Interactive Perception, an approach which exploits the synergies that arise when crossing the boundary between action and perception. In interactive perception, the emphasis of perception shifts from object appearance to object function. To enable the perception and manipulation of articulated objects, this thesis develops algorithms for perceiving the kinematic structure and shape of objects. The resulting perceptual capabilities are used within a relational reinforcement learning framework, enabling a robot to obtain general domain knowledge for manipulation. This composition enables our robot to reliably and efficiently manipulate novel articulated objects. To verify the effectiveness of the proposed robotic system, simulated and real-world experiments were conducted with a variety of everyday objects",Interactive Perception of Articulated Objects for Autonomous Manipulation,,https://core.ac.uk/download/13621323.pdf,ScholarWorks@UMass Amherst,,core
23857772,2009,"which permits unrestricted use, distribution, and reproduction in any medium, provided the original work is properly cited. A central goal of robotics and AI is to enable a team of robots to operate autonomously in the real world and collaborate with humans over an extended period of time. Though developments in sensor technology have resulted in the deployment of robots in specific applications the ability to accurately sense and interact with the environment is still missing. Key challenges to the widespread deployment of robots include the ability to learn models of environmental features based on sensory inputs, bootstrap off of the learned models to detect and adapt to environmental changes, and autonomously tailor the sensory processing to the task at hand. This paper summarizes a comprehensive effort towards such bootstrap learning, adaptation, and processing management using visual input. We describe probabilistic algorithms that enable a mobile robot to autonomously plan its actions to learn models of color distributions and illuminations. The learned models are used to detect and adapt to illumination changes. Furthermore, we describe a probabilistic sequential decision-making approach that autonomously tailors the visual processing to the task at hand. All algorithms are fully implemented and tested on robot platforms in dynamic environments. 1",Research Article Bootstrap Learning and Visual Processing Management on Mobile Robots,,,,,core
46725214,2009-01-01T08:00:00,"Automatically navigated cars that drive on their own and robots that can perform any manual work science fiction authors predicted that it would all be possible in the 20th Century (see Asimov, 1951  I, Robot ).  In the dawn of artificial intelligence, the main obstacle that has to be overcome was creating robots that are able to solve logical problems.  Humans, as far as we know, are unique in being able to solve abstract logical problems.  Thus, creating a robot to mimic this ability is an extremely complex task.
What researchers initially perceived as a much easier task - having a robot navigate and interact with the physical environment, however, proved equally difficult.  Robot vision and localization are just two of the tasks that need to be solved effectively before robots can truly do what Asimov predicted.
Robot localization specifically is at the heart of any artificial intelligence system that needs to navigate in the physical world.  It is not possible for a robot to carry out tasks that involve moving in the environment if every time it moves, it gets lost.
In this paper we examine an implementation of the Monte Carlo localization algorithm for the Pioneer 2 robots.  Our implementation of the algorithm utilizes the robot motion sensors, sonar sensors and camera to gain as detailed a picture as possible of its environment to allow it to navigate successfully",Solving the Maze:  Robot Localization Using the Monte Carlo Localization Algorithm and Shape Context,,https://core.ac.uk/download/46725214.pdf,DigitalCommons@Macalester College,,core
102191137,2010,"Abstract—The multilevel Darwinist brain (MDB) is a cognitive architecture that follows an evolutionary approach to provide au-tonomous robots with lifelong adaptation. It has been tested in real robot on-line learning scenarios obtaining successful results that reinforce the evolutionary principles that constitute the main orig-inal contribution of the MDB. This preliminary work has lead to a series of improvements in the computational implementation of the architecture so as to achieve realistic operation in real time, which was the biggest problem of the approach due to the high compu-tational cost induced by the evolutionary algorithms that make up the MDB core. The current implementation of the architecture is able to provide an autonomous robot with real time learning ca-pabilities and the capability for continuously adapting to changing circumstances in its world, both internal and external, with min-imal intervention of the designer. This paper aims at providing an overview or the architecture and its operation and defining what is required in the path towards a real cognitive robot following a developmental strategy. The design, implementation and basic op-eration of the MDB cognitive architecture are presented through some successful real robot learning examples to illustrate the va-lidity of this evolutionary approach. Index Terms—Adaptive systems, artificial neural networks, au-tonomous robotics, cognitive architecture, developmental robotics, evolutionary computation. I",D.: Multilevel Darwinist Brain (MDB): Artificial Evolution in a Cognitive Architecture for Real Robots,,,,,core
153819968,2009-10-01T00:00:00,"This paper reviews the state of the art of field programmable gate array (FPGA) with the focus on FPGA-based systems. The paper starts with an overview of FPGA in the previous literature, after that starts to get an idea about FPGA programming. FPGA-based neural networks also provided in this paper in order to highlight the best advantage by using FPGA with this type of intelligent systems, and a survey of FPGA-based control systems design with different applications. In this paper, we focus on the main differences between software-based systems with respect to FPGA-based systems, and the main features for FPGA technology and its real-time applications. FPGA-based robotics systems design also provided in this review, finally, the most popular simulation results with FPGA design and implementations are highlighted",Design and implementation of FPGA-based systems - a review,,https://core.ac.uk/download/153819968.pdf,'American-Eurasian Network for Scientific Information (AENSI)',,core
196892467,2011-03-01T00:00:00,"Monografia (graduação)—Universidade de Brasília, Faculdade de Tecnologia, 2011.Há tempos o Laboratório de Automação e Robótica da Universidade de Brasília vem desenvolvendo projetos na área de ambientes inteligentes visando a racionalização de energia e o conforto térmico por meio automação predial. Inserido nesse projeto está o presente trabalho de graduação, seu objetivo é acionar aparelhos de ar condicionado por meio de um sistema supervisório, decisão essa de ligar ou desligar os aparelhos tomada de acordo com a localização de usuários em um ambiente fechado fazendo-se uso da tecnologia Radio Frequency Identi cator (RFID) e de redes neurais implementadas e disponibilizadas pelo laboratório. O foco deste trabalho é a comunicação entre o sistema supervisório e o sistema de localização, a comunicação entre o sistema supervisório e os aparelhos de ar condicionado e por  m uma tentativa de melhoramento da localização de usuários quando estes estiverem em movimento, dado que o sistema previamente implementado funcionava apenas para os casos em que o usuário estivesse parado. Vários softwares e equipamentos foram utilizados, RFID ativos em três ambientes distintos do laboratório, o software supervisório ActionView, o módulo de comunicação ModBus ERS 1050, scripts em MatLab para aquisição de dados das leitoras RFID e para treinamento e uso das redes neurais arti ciais Perceptron Multicamadas. O Acionamento foi realizado com êxito, quando o supervisório envia o sinal de liga, ou de desliga, os comandos são executados imediatamente e logo são mostrados na tela do software, a comunicação com o sistema de localização está funcionando sem erros através do protocolo OLE for Process Control (OPC), e as redes neurais estão realizando localizações com uma precisão aceitável para ambientes fechados. Contudo faz-se necessário um desenvolvimento melhor na parte de localização para obter-se uma robustez maior para o projeto.For a long time the Automation and Robotics Laboratory of the University of Brasilia is developing works in Ambient Intelligence, studding ways to save energy while ensureing thermal comfort to the users, through building automation. This graduation work is inserted in this context, its goal is to actuate air conditioning equipments through a supervisory system, this decision of turning on or turning o  the equipments is taken according to the localization of the users in teh environment, using the Radio Frequency Identi cator technology, and neural networks implemented and available at the Laboratory. This work focuses on communication between the supervisory and the localization system, the communication between the supervisory and the air conditioning equipments and a try of improvement of the localization of users when these are moving around, because the localization system that exists works only if the user is static. Many software and equipments were used in this work: active RFID in three rooms of the laboratory, the supervisory software ActionView, the ModBus communication module ERS 1050 [SPIN Engenharia de automação], Matlab scripts for data acquisition of the RFID readers and for the Multilayer Perceptron neural network train and use. The actuation of the air conditioning equipments is considered successfully when the supervisory send the command to turn on and to turn o , the command is executed in real time and immediately after it has been sent, it appears on the software screen. The communication between the locallization system and the supervisory is working without errors using the OLE for Process Control Protocol (OPC), and the neural networks are locating the user with a acceptable precision for indoor localization applications, nervertheless it is still necessary to improve the localization to get into a system that is reliable",Ambientes inteligentes para racionalização de energia utilizando localização RFID,,,,,core
55463,2009-01-01T00:00:00,"A major issue for reinforcement learning (RL) applied to robotics is the time required to learn a new skill. While RL has been used to learn mobile robot control in many simulated domains, applications involving learning on real

robots are still relatively rare. In this paper, the Least-Squares Policy Iteration (LSPI) reinforcement learning algorithm and a new model-based algorithm Least-Squares Policy Iteration with Prioritized Sweeping (LSPI+), are implemented on a mobile robot to acquire new skills quickly and efficiently. LSPI+ combines the benefits of LSPI and prioritized sweeping, which uses all previous experience to focus the computational effort on the most “interesting” or dynamic parts of the state space. 

The proposed algorithms are tested on a household vacuum

cleaner robot for learning a docking task using vision as the only sensor modality. In experiments these algorithms are compared to other model-based and model-free RL algorithms. The results show that the number of trials required to learn the docking task is significantly reduced using LSPI compared to the other RL algorithms investigated, and that LSPI+ further improves on the performance of LSPI","Vision-based reinforcement learning using approximate policy

iteration",,https://core.ac.uk/download/55463.pdf,,,core
21135366,17/02/2010,"In this paper we present the LIDA architecture as a working model of cognition. We argue that such working models are broad in scope and address real world problems in comparison to experimentally based models which focus on specific pieces of cognition. While experimentally based models are useful, we need a working model of cognition that integrates what we know from neuroscience, cognitive science and AI. The LIDA architecture provides such a working model. A LIDA based cognitive robot or software agent will be capable of multiple learning mechanisms. With artificial feelings and emotions as primary motivators and learning facilitators, such systems will ‘live ’ through a developmental period during which they will learn in multiple ways to act in an effective, human-like manner in complex, dynamic, and unpredictable environments. We discuss the integration of the learning mechanisms into the existing IDA architecture as a working model of cognition",LIDA: A Working Model of Cognition,,,,,core
232134939,2010-01-01T08:00:00,"Analyzing and understanding human biosignals have been important research areas that have many practical applications in everyday life.  For example, Brain Computer Interface is a research area that studies the connection between the human brain and external systems by processing and learning the brain signals called Electroencephalography (EEG) signals.  Similarly, various assistive robotics applications are being developed to interpret eye or muscle signals in humans in order to provide control inputs for external devices. The efficiency for all of these applications depends heavily on being able to process and classify human biosignals. Therefore many techniques from Signal Processing and Machine Learning fields are applied in order to understand human biosignals better and increase the efficiency and success of these applications. This thesis proposes a new classifier for biosignal data classification utilizing Particle Swarm Optimization Clustering and Radial Basis Function Networks (RBFN). The performance of the proposed classifier together with several variations in the technique is analyzed by utilizing comparisons with the state of the art classifiers such as Fuzzy Functions Support Vector Machines (FFSVM), Improved Fuzzy Functions Support Vector Machines (IFFSVM). These classifiers are implemented on the classification of same biological signals in order to evaluate the proposed technique. Several clustering algorithms, which are used in these classifiers, such as K-means, Fuzzy c-means, and Particle Swarm Optimization (PSO), are studied and compared with each other based on clustering abilities. The effects of the analyzed clustering algorithms in the performance of Radial Basis Functions Networks classifier are investigated. Strengths and weaknesses are analyzed on various standard and EEG datasets. Results show that the proposed classifier that combines PSO clustering with RBFN classifier can reach or exceed the performance of these state of the art classifiers. Finally, the proposed classification technique is applied to a real-time system application where a mobile robot is controlled based on person\u27s EEG signal",A Study of recent classification algorithms and a novel approach for biosignal data classification,,https://core.ac.uk/download/232134939.pdf,RIT Scholar Works,,core
53457984,2009-01-01T00:00:00,"In Artificial Intelligence, numerous learning paradigms have been developed over the past decades. In most cases of embodied and situated agents, the learning goal for the artificial agent is to \u201emap\u201c or classify the environment and the objects therein [1, 2], in order to improve navigation or the execution of some other domain-specific task. Dynamic environments and changing tasks still pose a major challenge for robotic learning in real-world domains. In order to intelligently adapt its task strategies, the agent needs cognitive abilities to more deeply understand its environment and the effects of its actions. In order to approach this challenge within an open-ended learning loop, the XPERO project (http://www.xpero.org) explores the paradigm of Learning by Experimentation to increase the robot's conceptual world knowledge autonomously. In this setting, tasks which are selected by an actionselection mechanism are interrupted by a learning loop in those cases where the robot identifies learning as necessary for solving a task or for explaining observations. It is important to note that our approach targets unsupervised learning, since there is no oracle available to the agent, nor does it have access to a reward function providing direct feedback on the quality of its learned model, as e.g. in reinforcement learning approaches. In the following sections we present our framework for integrating autonomous robotic experimentation into such a learning loop. In section 1 we explain the different modules for stimulation and design of experiments and their interaction. In section 2 we describe our implementation of these modules and how we applied them to a real world scenario to gather target-oriented data for learning conceptual knowledge. There we also indicate how the goaloriented data generation enables machine learning algorithms to revise the failed prediction model",Autonomous design of experiments for learning by experimentation,10.1007/978-3-7908-2127-7_13,,'Springer Science and Business Media LLC',,core
153410494,2011-01-01T00:00:00,"Another two years have passed from the last HoloMAS conference held in Linz
in 2009. It is a pleasure to say that the R&D activities around holonic and
multi-agent systems for industrial applications have not faded during this period.
On the contrary, the number of scientific events aimed at the subject field
is growing steadily. Besides HoloMAS, which has been the pioneering event in
this field, there are multiple conferences such as the IEEE Conference on Industrial
Informatics (INDIN), the IEEE Conference on Emergent Technologies
and Factory Automation (ETFA) or the IFAC Symposium on Information Control
Problems in Manufacturing (INCOM) that aim their attention at advanced
industrial automation systems based on intelligent agents.
This year’s conference was the eighth in the sequence of HoloMAS events. The
first three (HoloMAS 2000 in Greenwich, HoloMAS 2001 in Munich and Holo-
MAS 2002 in Aix-en-Provence) were organized as workshops under the umbrella
of DEXA association. Starting with 2003, HoloMAS became an independent
conference organized biyearly on the odd years, still under the DEXA patronage
(HoloMAS 2003 in Prague, HoloMAS 2005 in Copenhagen, HoloMAS 2007 in Regensburg
and HoloMAS 2009 in Linz). On the even years the attention is focused
on specific events: the IEEE Workshop on Distributed Intelligent Systems (DIS
2006) with a special track covering the “obvious”HoloMAS topics was organized
in Prague in June 2006. Similarly, the IEEE Conference on Distributed Human–
Machine Systems (DHMS 2008), which has absorbed the HoloMAS field, was
held in Athens, Greece, in March 2008, and the IFAC Workshop on Intelligent
Manufacturing Systems (IMS 2010) in Lisbon, Portugal, in 2010. This approach
allows the HoloMAS community to be better integrated with both the information
society-oriented DEXA community as well as the IEEE Society aimed at
human–machine systems, cybernetics, and industrial informatics.
The research of holonic and agent-based systems receives constant support
from both the public sector and private institutions. There is an increased interest
from the IEEE System, Man, and Cybernetics (SMC) Society, namely,
from its Technical Committee on Distributed Intelligent Systems, which leverages
the experience gained in the former Holonic Manufacturing Systems consortium.
Another IEEE body - the Industrial Electronics Society - supports
the related R&D field through its Technical Committee on Industrial Agents
(http://tcia.ieee-ies.org/). Its mission is to provide a base for researchers and
application practitioners, to share their experiences with applications of holonic
and agent technologies in the industrial sector, especially in assembly and process
control, planning and scheduling, and supply chain managements. There
are number of impacted journals that provide space for articles dealing with industrial
agents such as the IEEE Transactions on SMC, Part C: Applications and Reviews, Journal of Engineering of Artificial Intelligence Applications (EAAI),
IEEE Transactions on Industrial Informatics, Computers in Industry or the
Journal of Autonomous Agents and Multi-Agent Systems (JAAMAS). Let us
recall that the extended versions of selected best HoloMAS 2009 papers were
published in the special issue of the International Journal of Production
Research.
It is our pleasure to inform you that for HoloMAS 2011 there were 36 papers
submitted, from which the ProgramCommittee selected 25 papers to be included
in this volume, by authors from 13 countries all over the world. Among the key
trends that the accepted papers report on is the effort to shift the holonic/agentbased
control system from the personal computer (the prevalent hosting platform
in the past) closer to the “hardware.” The obvious reason is to increase even
more the distributiveness and thus the robustness and flexibility of the control
system. In this model the control application is designed as embedded software
running on a dedicated microcontroller. This brings new challenges related to
limited resources in terms of memory, processing power, battery life, etc. Related
topics discussed in this volume are the employment of simulation techniques for
modeling, designing and validating the control system prior to its deployment
on the real hardware. The boom in Web applications and smart mobile devices
like smart phones and tablets have recently drawn the attention of the industrial
sector as it brings new challenges and possibilities for building next–generation
user interfaces for SCADA and operator panels. Looking at the applications of
holonic and agent systems we collected quite an interesting portfolio this year,
including smart grids, supply chain and logistics, healthcare, mobile robots and
unmanned aerial vehicles.
There were two invited talks specifically targeted toward HoloMAS 2011:
• Peter Skobelev (Magenta Solutions): “Multi-agent Systems for Real-Time
Resource Allocation, Scheduling, Optimization and Controlling”
• Alois Zoitl (Vienna University of Technology): “A Control Architecture for
Self-Reconfigurable Production Systems”
Also, for the first time in the HoloMAS history, there was a special session
organized covering the topic of smart industrial systems.
The HoloMAS 2011 conference was a highly motivating environment, challenging
the future research and fostering integration in the subject field. It has
always served as a showcase of the holonic and agent-based manufacturing research
offering information on the state of the art to specialists in neighboring,
knowledge-processing research fields covered by the DEXA multi-conference
event. We are very grateful to the DEXA Association for providing us with this
excellent opportunity. We would like to express our many thanks to Gabriela
Wagner for all her organizational efforts which were of key importance for the
success of this event. We would like to thank the IEEE SMC Society, and especially the Technical
Committee on Distributed Intelligent Systems of this Society, for its technical
co-sponsorship",Holonic and multi-agent systems for manufacturing: proceedings of the 5th International Conference on Industrial Applications of Holonic and Multi-Agent Systems,10.1007/978-3-642-23181-0,,'Springer Science and Business Media LLC',,core
214602749,2011-04-07T00:00:00,"Before deploying a software system we need to assure ourselves (and stake- holders) that the system will behave correctly. This assurance is usually done by testing the system. However, it is intuitively obvious that adaptive systems, including agent-based systems, can exhibit complex behaviour, and are thus harder to test. In this paper we examine this “obvious intuition” in the case of Belief-Desire-Intention (BDI) agents. We analyse the size of the behaviour space of BDI agents and show that although the intuition is correct, the factors that influence the size are not what we expected them to be; specifically, we found that the introduction of failure handling had a much larger effect on the size of the behaviour space than we expected. We also discuss the implications of these findings on the testability of BDI agents.Unpublished1. Wooldridge, M.: An Introduction to MultiAgent Systems. John Wiley & Sons, Chichester, England (2002). ISBN 0 47149691X

2. Munroe, S., Miller, T., Belecheanu, R., Pechoucek, M., McBurney, P., Luck, M.: Crossing the agent technology chasm: Experiences and challenges in commercial applications of agents. Knowledge Engineering Review 21(4), 345–392 (2006)

3. Benfield, S.S., Hendrickson, J., Galanti, D.: Making a strong business case for multiagent technology. In: P. Stone, G. Weiss (eds.) Proceedings of the Fifth International Joint Conference on Autonomous Agents and Multiagent Systems (AAMAS), pp. 10–15. ACM Press (2006)

4. Rao, A.S., Georgeff, M.P.: Modeling rational agents within a BDI-architecture. In: J. Allen, R. Fikes, E. Sandewall (eds.) Principles of Knowledge Representation and Reasoning, Proceedings of the Second International Conference, pp. 473–484. Morgan Kaufmann (1991)

5. Bratman, M.E.: Intentions, Plans, and Practical Reason. Harvard University Press, Cambridge, MA (1987)

6. Zhang, Z., Thangarajah, J., Padgham, L.: Model based testing for agent systems. In: J. Filipe, B. Shishkov, M. Helfert, L. Maciaszek (eds.) Software and Data Technologies, Communications in Computer and Information Science, vol. 22, pp. 399–413. Springer, Berlin/Heidelberg (2009)

7. Ekinci, E.E., Tiryaki, A.M., Çetin, Ö., Dikenelli: Goal-oriented agent testing revisited. In: M. Luck, J.J. Gomez-Sanz (eds.) Agent-Oriented Software Engineering IX, Lecture Notes in Computer Science, vol. 5386, pp. 173–186. Springer, Berlin/Heidelberg (2009)

8. Gomez-Sanz, J.J., Botía, J., Serrano, E., Pavón, J.: Testing and debugging of MAS interactions with INGENIAS. In: M. Luck, J.J. Gomez-Sanz (eds.) Agent-Oriented Software Engineering IX, Lecture Notes in Computer Science, vol. 5386, pp. 199–212. Springer, Berlin/Heidelberg (2009)

9. Nguyen, C.D., Perini, A., Tonella, P.: Experimental evaluation of ontology-based test generation for multi-agent systems. In: M. Luck, J.J. Gomez-Sanz (eds.) Agent-Oriented Software Engineering IX, Lecture Notes in Computer Science, vol. 5386, pp. 187–198. Springer, Berlin/Heidelberg (2009)

10. Padgham, L., Winikoff, M.: Developing Intelligent Agent Systems: A Practical Guide. John Wiley and Sons (2004). ISBN 0-470-86120-7

11. Shaw, P., Farwer, B., Bordini, R.: Theoretical and experimental results on the goal-plan tree problem. In: Proceedings of the Seventh International Conference on Autonomous Agents and Multiagent Systems (AAMAS), pp. 1379–1382. IFAAMAS (2008)

12. Erol, K., Hendler, J.A., Nau, D.S.: HTN planning: Complexity and expressivity. In: Proceedings of the 12th National Conference on Artificial Intelligence (AAAI), pp. 1123–1128. AAAI Press (1994)

13. de Silva, L., Padgham, L.: A comparison of BDI based real-time reasoning and HTN based planning. In: G. Webb, X. Yu (eds.) AI 2004: Advances in Artificial Intelligence, Lecture Notes in Computer Science, vol. 3339, pp. 1167–1173. Springer, Berlin/Heidelberg (2004)

14. Erol, K., Hendler, J., Nau, D.: Complexity results for HTN planning. Annals of Mathematics and Artificial Intelligence 18(1), 69–93 (1996)

15. Paolucci, M., Shehory, O., Sycara, K.P., Kalp, D., Pannu, A.: A planning component for RETSINA agents. In: N.R. Jennings, Y. Lespérance (eds.) Intelligent Agents VI, Agent Theories, Architectures, and Languages (ATAL), 6th International Workshop, ATAL ’99, Orlando, Florida, USA, July 15-17, 1999, Proceedings, Lecture Notes in Computer Science, vol. 1757, pp. 147–161. Springer, Berlin/Heidelberg (2000)

16. Busetta, P., Rönnquist, R., Hodgson, A., Lucas, A.: JACK Intelligent Agents - Components for Intelligent Agents in Java. AgentLink News (2) (1999). URL http://www.agentlink.org/newsletter/2/newsletter2.pdf

17. Huber, M.J.: JAM: A BDI-theoretic mobile agent architecture. In: Proceedings of the Third International Conference on Autonomous Agents (Agents’99), pp. 236–243. ACM Press (1999)

18. d’Inverno, M., Kinny, D., Luck, M., Wooldridge, M.: A formal specification of dMARS. In: M. Singh, A. Rao, M. Wooldridge (eds.) Intelligent Agents IV: Proceedings of the Fourth International Workshop on Agent Theories, Architectures, and Languages, Lecture Notes in Artificial Intelligence, vol. 1365, pp. 155–176. Springer, Berlin/Heidelberg (1998)

19. Georgeff, M.P., Lansky, A.L.: Procedural knowledge. Proceedings of the IEEE, Special Issue on Knowledge Representation 74(10), 1383–1398 (1986)

20. Ingrand, F.F., Georgeff, M.P., Rao, A.S.: An architecture for real-time reasoning and system control. IEEE Expert 7(6), 33–44 (1992)

21. Lee, J., Huber, M.J., Kenny, P.G., Durfee, E.H.: UM-PRS: An implementation of the procedural reasoning system for multirobot applications. In: Proceedings of the Conference on Intelligent Robotics in Field, Factory, Service, and Space (CIRFFSS’94), pp. 842–849 (1994)

22. Bordini, R.H., Hübner, J.F., Wooldridge, M.: Programming multi-agent systems in AgentSpeak using Jason. Wiley (2007). ISBN 0470029005

23. Morley, D., Myers, K.: The SPARK agent framework. In: Proceedings of the Third International Joint Conference on Autonomous Agents and Multiagent Systems (AAMAS), pp. 714–721. IEEE Computer Society, Washington, DC, USA (2004)

24. Pokahr, A., Braubach, L., Lamersdorf, W.: Jadex: A BDI reasoning engine. In: R.H. Bordini, M. Dastani, J. Dix, A. El Fallah Seghrouchni (eds.) Multi-Agent Programming: Languages, Platforms and Applications, pp. 149–174. Springer (2005)

25. Bratman, M.E., Israel, D.J., Pollack, M.E.: Plans and resource-bounded practical reasoning. Computational Intelligence 4, 349–355 (1988)

26. Rao, A.S.: AgentSpeak(L): BDI agents speak out in a logical computable language. In: W.V. de Velde, J. Perrame (eds.) Agents Breaking Away: Proceedings of the Seventh European Workshop on Modelling Autonomous Agents in a Multi-Agent World (MAAMAW’96), Lecture Notes in Artificial Intelligence, vol. 1038, pp. 42–55. Springer, Berlin/Heidelberg (1996)

27. Winikoff, M., Padgham, L., Harland, J., Thangarajah, J.: Declarative & procedural goals in intelligent agent systems. In: Proceedings of the Eighth International Conference on Principles of Knowledge Representation and Reasoning (KR2002), pp. 470–481. Morgan Kaufmann, Toulouse, France (2002)

28. Georgeff, M.: Service orchestration: The next big challenge. DM Review Special Report (2006). URL http://www.dmreview.com/specialreports/20060613/1056195-1.html. (2006)

29. Dastani, M.: 2APL: a practical agent programming language. Autonomous Agents and Multi-Agent Systems 16(3), 214–248 (2008)

30. Naish, L.: Resource-oriented deadlock analysis. In: V. Dahl, I. Niemelä (eds.) Logic Programming, Lecture Notes in Computer Science, vol. 4670, pp. 302–316. Springer, Berlin/Heidelberg (2007)

31. Wilf, H.S.: generatingfunctionology, second edn. Academic Press Inc., Boston, MA (1994). URL http: //www.math.upenn.edu/∼wilf/gfology2.pdf

32. Sloane, N.J.A.: The on-line encyclopedia of integer sequences. http://www.research.att.com/∼njas/sequences/ (2007)

33. Burmeister, B., Arnold, M., Copaciu, F., Rimassa, G.: BDI-agents for agile goal-oriented business processes. In: Proceedings of the Seventh International Conference on Autonomous Agents and Multiagent Systems (AAMAS), pp. 37–44. IFAAMAS (2008)

34. Dorigo, M., Stützle, T.: Ant Colony Optimization. MIT Press (2004). ISBN 0-262-04219-3

35. van Riemsdijk, M.B., Dastani, M., Winikoff, M.: Goals in agent systems: A unifying framework. In: Proceedings of the Seventh Conference on Autonomous Agents and Multiagent Systems (AAMAS), pp. 713–720. IFAAMAS (2008)

36. Thangarajah, J., Winikoff, M., Padgham, L., Fischer, K.: Avoiding resource conflicts in intelligent agents. In: F. van Harmelen (ed.) Proceedings of the 15th European Conference on Artificial Intelligence (ECAI), pp. 18–22. IOS Press (2002)

37. Nguyen, C.D., Perinirini, A., Tonella, P.: Automated continuous testing of multi-agent systems. In: Proceedings of the Fifth European Workshop on Multi-Agent Systems (EUMAS) (2007)

38. Dwyer, M.B., Hatcliff, J., Pasareanu, C., Robby, Visser, W.: Formal software analysis: Emerging trends in software model checking. In: Future of Software Engineering 2007, pp. 120–136. IEEE Computer Society, Los Alamitos, CA (2007)

39. Wooldridge, M., Fisher, M., Huget, M.P., Parsons, S.: Model checking multi-agent systems with MABLE. In: Proceedings of the First International Joint Conference on Autonomous Agents and Multi-Agent Systems (AAMAS), pp. 952–959. ACM Press (2002)

40. Bordini, R.H., Fisher, M., Pardavila, C., Wooldridge, M.: Model checking AgentSpeak. In: Proceedings of the Second International Joint Conference on Autonomous Agents and Multiagent Systems (AAMAS), pp. 409–416. ACM Press (2003)

41. Raimondi, F., Lomuscio, A.: Automatic verification of multi-agent systems by model checking via ordered binary decision diagrams. J. Applied Logic 5(2), 235–251 (2007)

42. Burch, J., Clarke, E., McMillan, K., Dill, D., Hwang, J.: Symbolic model checking: 1020 states and beyond. Information and Computation 98(2), 142–170 (1992)

43. Fix, L., Grumberg, O., Heyman, A., Heyman, T., Schuster, A.: Verifying very large industrial circuits using 100 processes and beyond. In: D. Peled, Y.K. Tsay (eds.) Automated Technology for Verification and Analysis, Lecture Notes in Computer Science, vol. 3707, pp. 11–25. Springer, Berlin/Heidelberg (2005",On the testability of BDI agent systems,,,'University of Otago Library',,core
21152389,06/03/2010,"Abstract — A major issue for reinforcement learning (RL) applied to robotics is the time required to learn a new skill. While RL has been used to learn mobile robot control in many simulated domains, applications involving learning on real robots are still relatively rare. In this paper, the Least-Squares Policy Iteration (LSPI) reinforcement learning algorithm and a new model-based algorithm Least-Squares Policy Iteration with Prioritized Sweeping (LSPI+), are implemented on a mobile robot to acquire new skills quickly and efficiently. LSPI+ combines the benefits of LSPI and prioritized sweeping, which uses all previous experience to focus the computational effort on the most “interesting ” or dynamic parts of the state space. The proposed algorithms are tested on a household vacuum cleaner robot for learning a docking task using vision as the only sensor modality. In experiments these algorithms are compared to other model-based and model-free RL algorithms. The results show that the number of trials required to learn the docking task is significantly reduced using LSPI compared to the other RL algorithms investigated, and that LSPI+ further improves on the performance of LSPI. I",Vision-Based Reinforcement Learning using Approximate Policy Iteration,,,,,core
21042461,11/07/2009,"While Reinforcement Learning (RL) is not traditionally designed for interactive supervisory input from a human teacher, several works in both robot and software agents have adapted it for human input by letting a human trainer control the reward signal. In this work, we experimentally examine the assumption underlying these works, namely that the human-given reward is compatible with the traditional RL reward signal. We describe an experimental platform with a simulated RL robot and present an analysis of real-time human teaching behavior found in a study in which untrained subjects taught the robot to perform a new task. We report three main observations on how people administer feedback when teaching a Reinforcement Learning agent: (a) they use the reward channel not only for feedback, but also for future-directed guidance; (b) they have a positive bias to their feedback, possibly using the signal as a motivational channel; and (c) they change their behavior as they develop a mental model of the robotic learner. Given this, we made specific modifications to the simulated RL robot, and analyzed and evaluated its learning behavior in four follow-up experiments with human trainers. We report significant improvements on several learning measures. This work demonstrates the importance of understanding the human-teacher/robot-learner partnership in order to design algorithms that support how people want to teach and simultaneously improve the robot’s learning behavior",Teachable Robots: Understanding Human Teaching Behavior to Build More Effective Robot Learners,,,,,core
27080989,2010-04-01T00:00:00Z,"Robocup is an international competition for multi agent research and related subject like: Artificial intelligence, Image processing, machine learning, robot path planning, control, and&lt;br /&gt;obstacle avoidance. In a soccer robot game, the environment is highly competitive and dynamic. In order to work in the dynamically changing environment, the decision-making system of a soccer robot system should have the features of flexibility and real-time adaptation. In this paper we will&lt;br /&gt;focus on the Middle Size Soccer Robot league (MSL) and new hierarchical hybrid fuzzy methods for decision making and action selection of a robot in Middle Size Soccer Robot league (MSL) are presented. First, the behaviors of an agent are introduced, implemented and classified in two layers,&lt;br /&gt;the Low_Level_Behaviors and the High_Level_Behaviors. In the second layer, a two phase mechanism for decision making is introduced. In phase one, some useful methods are implemented which check the robot&amp;rsquo;s situation for performing required behaviors. In the next phase, the team strategy, team formation, robot&amp;rsquo;s role and the robot&amp;rsquo;s positioning system are introduced. A fuzzy logical approach is employed to recognize the team strategy and further more to tell the player the&lt;br /&gt;best position to move. We believe that a Dynamic role engine is necessary for a successful team. Dynamic role engine and formation control during offensive or defensive play, help us to prevent collision avoidance among own players when attacking the ball and obstacle avoidance of the opponents. At last, we comprised our implemented algorithm in the Robocup 2007 and 2008 and results showed the efficiency of the introduced methodology. The results are satisfactory which has already been successfully implemented in ADRO RoboCup team. This project is still in progress and some new interesting methods are described in the current report",Design of an Action Selection Mechanism for Cooperative Soccer Robots Based on Fuzzy Decision Making Algorithm,,,EduSoft publishing,"[{'title': None, 'identifiers': ['2067-3957', 'issn:2067-3957']}]",core
25912986,2010-01-01T00:00:00Z,"A central goal of robotics and AI is to enable a team of robots to operate autonomously in the real world and collaborate with humans over an extended period of time. Though developments in sensor technology have resulted in the deployment of robots in specific applications the ability to accurately sense and interact with the environment is still missing. Key challenges to the widespread deployment of robots include the ability to learn models of environmental features based on sensory inputs, bootstrap off of the learned models to detect and adapt to environmental changes, and autonomously tailor the sensory processing to the task at hand. This paper summarizes a comprehensive effort towards such bootstrap learning, adaptation, and processing management using visual input. We describe probabilistic algorithms that enable a mobile robot to autonomously plan its actions to learn models of color distributions and illuminations. The learned models are used to detect and adapt to illumination changes. Furthermore, we describe a probabilistic sequential decision-making approach that autonomously tailors the visual processing to the task at hand. All algorithms are fully implemented and tested on robot platforms in dynamic environments",Bootstrap Learning and Visual Processing Management on Mobile Robots,10.1155/2010/765876,,Hindawi Limited,"[{'title': None, 'identifiers': ['1687-7489', 'issn:1687-7470', '1687-7470', 'issn:1687-7489']}]",core
58795057,2011-11,"Autonomy is a prime issue on robotics field and it is closely related to decision making. Last researches on decision making for social robots are focused on biologically inspired mechanisms for taking decisions. Following this approach, we propose a motivational system for decision making, using internal (drives) and external stimuli for learning to choose the right action. Actions are selected from a finite set of skills in order to keep robot's needs within an acceptable range. The robot uses reinforcement learning in order to calculate the suitability of every action in each state. The state of the robot is determined by the dominant motivation and its relation to the objects presents in its environment. The used reinforcement learning method exploits a new algorithm called Object Q-Learning. The proposed reduction of the state space and the new algorithm considering the collateral effects (relationship between different objects) results in a suitable algorithm to be applied to robots living in real environments. In this paper, a first implementation of the decision making system and the learning process is implemented on a social robot showing an improvement in robot's performance. The quality of its performance will be determined by observing the evolution of the robot's wellbeing.The funds provided by the Spanish Government through the project called “Peer
to Peer Robot-Human Interaction” (R2H), of MEC (Ministry of Science and Education), the project “A new approach to social robotics” (AROS), of MICINN (Ministry of Science and Innovation), and the RoboCity2030-II-CM project (S2009/DPI-1559), funded by Programas de Actividades I+D en la Comunidad de Madrid and cofunded by Structural Funds of the EU",Learning the selection of actions for an autonomous social robot by reinforcement learning based on motivations,10.1007/s12369-011-0113-z,,Springer,,core
79996950,2009,"15 years of industry experience developing large-scale, multi-agent information and control systems for diverse applications including manufacturing, combat pilot decision support and mission management, robotics, and surveillance. In these areas, he developed and applied technologies including distributed, component-based software architectures, software and systems engineering process models, intelligent control, the semantic web, and real-time artificial intelligence. In 1999, Dr. Hawker joined the Computer Science Department at the University of Alabama as an Assistant Professor focusing on software engineering, and in 2004 he moved t",AC 2009-2001: A SOFTWARE PROCESS ENGINEERING COURSE,,,,,core
102246345,2010,"Abstract—Primates often perform coordinated eye and arm movements, contextually fixating and reaching towards nearby objects. This combination of looking and reaching to the same target is used by infants to establish an implicit visuomotor representation of the peripersonal space, useful for both oculo-motor and arm motor control. In this work, taking inspiration from such behavior and from primate visuomotor mechanisms, a shared sensorimotor map of the environment, built on a ra-dial basis function framework, is configured and trained by the coordinated control of eye and arm movements. Computational results confirm that the approach seems especially suitable for the problem at hand, and for its implementation on a real humanoid robot. By exploratory gazing and reaching actions, either free or goal-based, the artificial agent learns to perform direct and inverse transformations between stereo vision, oculomotor, and joint-space representations. The integrated sensorimotor map that allows to contextually represent the peripersonal space through different vision and motor parameters is never made explicit, but rather emerges thanks to the interaction of the agent with the environment. Index Terms — Eye–arm coordination, humanoid robots, ra-dial basis function networks, self-supervised learning, spatial awareness. I",Implicit sensorimotor mapping of the peripersonal space by gazing and reaching (submitted,,,,,core
60721767,2011-07-12T00:00:00,"International audienceA major challenge in modern robotics is to liberate robots from controlled industrial settings, and allow them to interact with humans and changing environments in the real world. The current research attempts to determine if a neurophysiologically motivated model of cortical function in the primate can help to address this challenge. Primates are endowed with cognitive systems that allow them to maximize the feedback from their environment by learning the values of actions in diverse situations and by adjusting their behavioral parameters (i.e. cognitive control) to accommodate unexpected events. In such contexts uncertainty can arise from at least two distinct sources - expected uncertainty resulting from noise during sensory-motor interaction in a known context, and unexpected uncertainty resulting from the changing probabilistic structure of the environment. However, it is not clear how neurophysiological mechanisms of reinforcement learning and cognitive control integrate in the brain to produce efficient behavior. Based on primate neuroanatomy and neurophysiology, we propose a novel computational model for the interaction between lateral prefrontal and anterior cingulate cortex (LPFC and ACC) reconciling previous models dedicated to these two functions. We deployed the model in two robots and demonstrate that, based on adaptive regulation of a meta-parameter β that controls the exploration rate, the model can robustly deal with the two kinds of uncertainties in the real world. In addition the model could reproduce monkey behavioral performance and neurophysiological data in two problem-solving tasks. A last experiment extends this to human-robot interaction with the iCub humanoid, and novel sources of uncertainty corresponding to ""cheating"" by the human. The combined results provide concrete evidence for the ability of neurophysiologically inspired cognitive systems to control advanced robots in the real world",Robot cognitive control with a neurophysiologically inspired reinforcement learning model,10.3389/fnbot.2011.00001,,'Frontiers Media SA',,core
53327826,2011-01-01T00:00:00,"In this paper a joint application of Artificial Intelligence (AI), robotics and Web services is described. The aim of the work presented here was to create a new integrated framework that keeps advantage on one side of the sensing and exploring capabilities of the robotic systems that work in the real world and, on the other side, of the information available via Web. Robots are conceived like (semi-)autonomous systems able to explore and manipulate a portion of their environment in order to find and collect information and data. On the other hand, theWeb, that in a robotic domain is usually considered like a channel of communication (e.g. tele-operation, tele-manipulation), here is conceived also like a source of knowledge. This allows to define a new framework able to manage robotic agents in order to get precise, real-time information from the real world. Besides, software agents may search for and get additional information from the Web logical world. The intelligent administration of these services can be applied in different environments and leads to optimize procedures and solve practical problems. To this end a traffic control application has been defined and a simplified test-case implemented",An intelligent framework to manage robotic autonomous agents,10.1016/j.eswa.2010.12.080,,'Elsevier BV',,core
225196685,,"碩士[[abstract]]競賽型的人型機器人在比賽過程中，往往伴隨著大量的資料傳輸與計算，為了能有效的處理這些相關資料以及有很好的效能展現，在這篇論文中，跟據小型人形機器人硬體系統整合設計之需求，設計出一個嵌入式系統平台為人行機器人所用，除了整合周邊傳輸硬體之外，也在軟體上建置了各功能的API程式功給各系統使用，藉由ARM平台的優點讓所完成的人形機器人之控制平台成為一個高效能且低功的系統。

CPU Scheduling(排班)是一項基本的操作系統功能，幾乎所有的計算機資源排程之前都會使用它。CPU當然是一個主要計算機資源，因此，排班是控制做作業系統設計的因素。CPU排班是當有很多可以執行的Processes時，去決定由哪一個Process去執行。之所以非常重要，因為它對整體性能全部系統可以有很大的影響和對資源利用(Sabrina et al.,2005.)。所以在本論文中，是在探討人行機器人控制理，行程處理程序(process)對系統效能的影響，而在這些行程處理程序之中，針對佔用CPU系統資源較久的行程處理程序提出來做討論:馬達行程理程序，它是一個不可搶先式之的行程處理程序 (Nonp-preemtive proces)。在人型機器人中，馬達運動控制是一比非常大量的負擔，因此在CPU排班過程中，佔用較長時間資源的行程處理程序，將會造成其他行程理程序及系統的影響。
所以在本論文中，藉由國科會計劃編NSC-(96~98)-2218-E-032-004 具有教育及競賽功能的人形機器人系統之設計 與開發-子計畫三：機器人即時分散嵌入式系統之研究(3/3)之下，，開發設計了嵌入式系統平台，，不但整合其他子計劃的硬體傳輸裝置介面，並且在即時模組(REAL TIME MODULE)上，設計API(Application Programming Interface)內容郭其他子計畫所使用。最後將會使用所設計好的系統，針對馬達行程處理程序做探討，希望在加入CPU排班之前，可以有效的減少佔用系統資源的時間，提升系統執行的效率。而在未來在加入其他行程處理程序於CPU排班之中做管理。[[abstract]]Abstract—With recent advances in the humanoid robots technology, the communication of robots has been the state of Ubiquitous Computing (said Mark Weiser). For the robot soccer game to say, once robots can share the information retrieved from different sensors, they will collaborate and have the opportunities to win the game. Moreover, the humanoid robots have more kinds of sensor such as motion control, environment sensing, motion detection, high-speed visual processing, artificial intelligence, knowledge management and intelligence control system than general robots. Therefore, competitive robots with a common operation platform will help integrate the requirements of different control systems. In this paper, a real-time distributed plat-form with REAL TIME MODULE in robot software framework was proposed to meet the real-time processing and distributed computing requirements[[tableofcontents]]目    錄
第1章  緒論	- 1 -
1.1	前言	- 1 -
1.2	動機與目的	- 4 -
1.3	論文章節架構	- 5 -
第2章  背景知識與相關研究	- 6 -
2.1	作業系統(OS Operating System)	- 6 -
2.1.1	即時系統	- 7 -
2.1.2	即時系統任務分類	- 9 -
2.1.3	排班執行分類	- 11 -
2.1.4	任務行程處理程序定義	- 11 -
2.1.5	排班原則	- 14 -
2.2	POSIX(Portable Operating System Interface)	- 16 -
2.2.1	行程處理程序與信號	- 16 -
2.2.2	多重執行緒的觀念(Multi-Threads)	- 20 -
2.3	嵌入式系統平台製作	- 20 -
2.3.1	系統平台上阻抗的評估	- 21 -
2.3.2	系統平台上分層與走線	- 23 -
2.3.3	系統平台上佈局與LAYOUT	- 24 -
2.3.4	系統平台上PCB製作	- 27 -
2.3.5	系統SMT(Surface-mount techmologt)	- 29 -
2.4	環境介紹	- 34 -
2.4.1	系統平台上PCB製作	- 35 -
第3章  問題描述與解決方法	- 42 -
3.1	問題描述	- 42 -
3.2	解決方法	- 44 -
第4章  研究成果展現	- 48 -
4.1	作業系統安裝	- 48 -
4.1.1	U-Boot安裝	- 48 -
4.1.2	Linux kernel安裝	- 52 -
4.2	Application Programming Interface(API)	- 52 -
4.2.1	Motor API制定	- 53 -
4.2.2	Sensor API制定	- 55 -
4.3	實驗分析與模擬	- 57 -
4.3.1	實驗環境	- 58 -
4.3.2	人型機器人控制平台	- 62 -
4.3.3	人型機器人系統實驗模擬與探討	- 63 -
第5章	結論與未來展望	- 68 -
參考文獻	- 70 -
圖目錄
圖1.1 Honda ASIMO	- 2 -
圖2.1 行程處理程序定義圖	- 8 -
圖2.2 硬即時與軟即時對Deadline的響應圖	- 9 -
圖2.3 行程(Process)執行過程內狀態圖	- 12 -
圖2.4 行程處理程序與多重執行緒結構圖	- 17 -
圖2.5 行程處理程序示意圖	- 18 -
圖2.6 Microstrip-Line	- 22 -
圖2.7 Stripline	- 22 -
圖2.8 特性阻抗計算軟體	- 23 -
圖2.10 PCB設計示意圖	- 26 -
圖2.11 PCB印電路板製作流程圖與系統PCB成品	- 29 -
圖2.12 錫膏機機板錫膏印圖與機板錫膏印製完成	- 30 -
圖2.13 SMT高速機RLC打件圖與泛用機IC元件打件	- 30 -
圖2.14 PCB迴焊作業與機板進行光學檢測	- 30 -
圖2.15 SMT 作業流程	- 31 -
圖2.16 BGA rework station	- 33 -
圖2.17 機器人系統架構圖	- 34 -
圖2.18 嵌入式平台軟體架構圖	- 36 -
圖2.19 ARM920T 核心架構圖	- 37 -
圖2.20 AMBA BUS	- 38 -
圖2.21 SC32442 核心架構圖	- 39 -
圖2.22系統方塊圖	- 40 -
圖3.1 RX-28 伺服馬達實體圖	- 44 -
圖3.2 馬達傳輸格式圖	- 44 -
圖3.3 RS485傳輸格式圖	- 45 -
圖3.4 單執行緒與多重執行緒示意圖	- 46 -
圖3.5 comport建置完成示意圖	- 47 -
圖3.6 RS232 硬體電路圖	- 47 -
圖4.1 OS porting流程圖	- 49 -
圖4.2 CPU、記憶體 ID讀取	- 49 -
圖4.3 U-BOOT程式燒錄	- 50 -
圖4.4NANDFLASH讀取控制時序	- 51 -
圖4.5 Linux作業系統完成架設	- 52 -
圖4.6 壓力感測器控制流程圖	- 55 -
圖4.7 人形機器人架構與自由度示意圖	- 58 -
圖4.8 人形機器人的運動控制分析圖	- 59 -
圖4.9 人形機器人左右腳運動控制分析圖	- 61 -
圖4.10 人型機器人系統架構圖	- 63 -
圖4.11 人型機器人系統程式流程圖	- 65 -
表目錄
表2.1 PCB製作規格表	- 28 -
表4.1 馬達API	- 54 -
表4.2  壓力感測器控制相關函式	- 56 -
表4.3 壓力感測器傳送與接收封包格式	- 57 -
表4.4 單執行緒與多重執行緒分析表	- 66 -[[note]]學號: 696470078, 學年度: 9",[[alternative]]Research of multi-threads applications for real-time control systems on humanoid robot embedded platforms,,,,,core
144062887,2011-10-25T00:00:00,"Os métodos de Aprendizagem por Reforço (AR) se mostram adequados para problemas de tomadas de decisões em diversos domínios por sua estrutura flexível e adaptável. Apesar de promissores, os métodos AR frequentemente tem seu campo de atuação prático restrito a problemas com espaço de estados de pequeno ou médio porte devido em muito à forma com que realizam a estimativa da função de avaliação. Nesta tese, uma nova abordagem de AR, denominada de Agente Topológico de Aprendizagem por Reforço (ATAR), inspirada em aprendizagem latente, é proposta para acelerar a aprendizagem por reforço através de um mecanismo alternativo de seleção dos pares estado-ação para atualização da estimativa da função de avaliação. A aprendizagem latente refere-se à aprendizagem animal que ocorre na ausência de reforço e que não é aparente até que um sinal de reforço seja percebido pelo agente. Este aprendizado faz com que um agente aprenda parcialmente uma tarefa mesmo antes que este receba qualquer sinal de reforço. Mapas Cognitivos são usualmente empregados para codificar a informação do ambiente em que o agente está imerso. Desta forma, o ATAR usa um mapa topológico, baseado em Mapas Auto-Organizáveis, para realizar as funções do mapa cognitivo e permitir um mecanismo simples de propagação das atualizações. O ATAR foi testado, em simulação, para planejamento de navegação de um robô móvel em  ambientes inicialmente desconhecidos e não-estruturados. Comparações com outros seis algoritmos AR avaliaram comparativamente o desempenho do agente proposto na navegação. Os resultados obtidos são promissores e comparáveis com os algoritmos AR mais rápidos testados, alcançando em alguns ensaios desempenho superior aos dos demais algoritmos - principalmente nas simulações que consideram situações observadas em ambientes não-estruturados. Três características do ATAR original foram alteradas para tornar ainda mais viável sua aplicação prática: (i) mudanças no mapa topológico para reduzir o número de vértices, (ii) mudança  na heurística usada na seleção das ações do agente e (iii) variações na estratégia de exploração do ATAR. Do ponto (i), foi proposto e implementado um novo mapa topológico, o Mapa Topológico Incremental Classificador  MTIC, que a partir da classificação dos estados do ambiente gera os vértices de uma triangularização de Watson. O ponto (ii) criou um método aplicável a outros problemas de planejamento de trajetória em grafos denominado de Melhoria das trajetórias por detecção de ponto interior. O terceiro estudou estratégias direcionadas de exploração como uma opção para acelerar o aprendizado do ATAR.Reinforcement Learning (RL) methods have shown to be a good choice for decision-making problems due to their flexible and adaptive characteristics. Despite such promising features, RL methods often have their practical application restricted to small or medium size (at state, or state-action, space) problems mainly because of their standard strategies for value function estimation. In this thesis, a new RL approach, called \""Topological Reinforcement Learning Agent\"" - TRLA, is proposed  to accelerate learning through an alternative mechanism to update the state-action value function. TRLA is inspired in latent learning, which refers to animal learning that occurs in the absence of reinforcements and that is not visible until an environmental reinforcement is perceived. This concept considers that part of a task can be learned even before the agent receives any indication of how to perform such a task. Cognitive Maps are usually used to encode  information about the environment where the agent is immersed. Thus, the TRLA uses a topological map, based on  Self-Organized Maps, to implement cognitive map functions and permit a new simple mechanism to execute the propagation of state-action updates. The chosen problem to test TRLA is the simulation of a mobile robot navigation in some initially unknown and unstructured environments. Performance comparisons of the TRLA with six other RL algorithms were carried out to the execution of the navigation task. The obtained results are very promising and comparable with some of faster RL algorithms simulated. In some experiments, the TRLA\'s performance overcomes the others  especially in simulations with unstructured environments. Three characteristics of the original TRLA were modified to make it more suitable for real implementations: (i) changes in the topological map construction to reduce the vertices number, (ii) changes in the agents heuristic for action selection, and (iii) variations on the TRLAs strategy for exploration of the state-action space. In (i), a new procedure to construct topological maps was proposed and implemented, the Incremental Classifier Topological Map  ICTM, which generates the vertices for a Watsons triangulation from the classification of the input states. In (ii), it was proposed a method to optimize trajectory planning problems based on graphs, denoted \""trajectory improvement from inner point detection\"". The third point considers directed exploration strategies as an option for TRLA\'s learning acceleration",Topological reinforcement learning agent,10.11606/T.18.2004.tde-21102011-081848,,"'Universidade de Sao Paulo, Agencia USP de Gestao da Informacao Academica (AGUIA)'",,core
268809713,2011-12-01T08:00:00,"This thesis addresses an expansion of the control programs for the Cyton Alpha 7D 1G arm. The original control system made use of configurable software which exploited the arm’s seven degrees of freedom and kinematic redundancy to control the arm based on desired behaviors that were configured off-line. The inclusions of the GraspIt! grasp planning simulator and toolkit enables the Cyton Alpha to be used in more proactive on-line grasping problems, as well as, presenting many additional tools for on-line learning applications. In short, GraspIt! expands what is possible with the Cyton Alpha to include many machine learning tools and opportunities for future research. Noteworthy features of GraspIt!:
• A 3D user interface allowing the user to see and interact virtual objects, obstacles, and robots, in addition to a 3D representation of the Cyton Alpha
• A collision detection and contact determination system within simulation • On-line grasp analysis routines
• Visualization methods for determining the weak points within a grasp, as well as, creating projections of grasp quality and ability to resist dynamic forces.
• Computation of numerical grasp quality metrics and visualization methods for proposed grasps
• Dynamics engine
• Support for lower-dimensional hand posture subspaces
• Interaction with sensors (Flock of Birds tracker) and hardware (Pioneer robot) within simulation
• GraspIt! can generate huge databases of labeled grasp data, which can be used for data-driven grasp-planning algorithms and has built in support for the Columbia Grasp Database.
By making use of the GraspIt! simulator, it is possible to test algorithms for grasp manipulation, grasp planning, or grasp synthesis more quickly and with greater repeatability than would be possible on the real robot. Contributions of this system include:
1. A joint based 3D rendering of the Cyton Alpha 7D 1G arm
2. Simulated bodies for several objects in the DI Lab
3. Support for multiple representations of joint data within three-dimensional space
• Euler Angles
• Quaternions
• Denavit-Hartenberg Parameters
4. Framework for future work in grasp-planning, grasp synthesis, cooperative grasping tasks, and transfer learning applications with the Cyton Alpha arm",cytonGrasp: Cyton Alpha Controller via GraspIt! Simulation,,https://core.ac.uk/download/268809713.pdf,TRACE: Tennessee Research and Creative Exchange,,core
30857249,2011-01-01T00:00:00,"Editors: Alexander Refsum Jensenius, Anders Tveit, Rolf Inge Godøy, Dan Overholt
Table of Contents

-Tellef Kvifte: Keynote Lecture 1: Musical Instrument User Interfaces: the Digital Background of the Analog Revolution - page 1
-David Rokeby: Keynote Lecture 2: Adventures in Phy-gital Space - page 2 
-Sergi Jordà: Keynote Lecture 3: Digital Lutherie and Multithreaded Musical Performance: Artistic, Scientific and Commercial Perspectives - page 3

Paper session A — Monday 30 May 11:00–12:30  
-Dan Overholt: The Overtone Fiddle: an Actuated Acoustic Instrument - page 4  
-Colby Leider, Matthew Montag, Stefan Sullivan and Scott Dickey: A Low-Cost, Low-Latency Multi-Touch Table with Haptic Feedback for Musical Applications - page 8 
-Greg Shear and Matthew Wright: The Electromagnetically Sustained Rhodes Piano - page 14
-Laurel Pardue, Christine Southworth, Andrew Boch, Matt Boch and Alex Rigopulos: Gamelan Elektrika: An Electronic Balinese Gamelan - page 18
-Jeong-Seob Lee and Woon Seung Yeo: Sonicstrument: A Musical Interface with Stereotypical Acoustic Transducers - page 24

Poster session B— Monday 30 May 13:30–14:30 
-Scott Smallwood: Solar Sound Arts: Creating Instruments and Devices Powered by Photovoltaic Technologies - page 28
-Niklas Klügel, Marc René Frieß and Georg Groh: An Approach to Collaborative Music Composition - page 32
-Nicolas Gold and Roger Dannenberg: A Reference Architecture and Score Representation for Popular Music Human-Computer Music Performance Systems - page 36
-Mark Bokowiec: V’OCT (Ritual): An Interactive Vocal Work for Bodycoder System and 8 Channel Spatialization - page 40
-Florent Berthaut, Haruhiro Katayose, Hironori Wakama, Naoyuki Totani and Yuichi Sato: First Person Shooters as Collaborative Multiprocess Instruments - page 44
-Tilo Hähnel and Axel Berndt: Studying Interdependencies in Music Performance: An Interactive Tool - page 48
-Sinan Bokesoy and Patrick Adler: 1city 1001vibrations: development of a interactive sound installation with robotic instrument performance - page 52
-Tim Murray-Browne, Di Mainstone, Nick Bryan-Kinns and Mark D. Plumbley:The medium is the message: Composing instruments and performing mappings - page 56
-Seunghun Kim, Luke Keunhyung Kim, Songhee Jeong and Woon Seung Yeo: Clothesline as a Metaphor for a Musical Interface - page 60
-Pietro Polotti and Maurizio Goina: EGGS in action - page 64
-Berit Janssen: A Reverberation Instrument Based on Perceptual Mapping - page 68
-Lauren Hayes: Vibrotactile Feedback-Assisted Performance - page 72
-Daichi Ando: Improving User-Interface of Interactive EC for Composition-Aid by means of Shopping Basket Procedure - page 76
-Ryan McGee, Yuan-Yi Fan and Reza Ali: BioRhythm: a Biologically-inspired Audio-Visual Installation - page 80
-Jon Pigott: Vibration, Volts and Sonic Art: A practice and theory of electromechanical sound - page 84
-George Sioros and Carlos Guedes: Automatic Rhythmic Performance in Max/MSP: the kin.rhythmicator - page 88
-Andre Goncalves: Towards a Voltage-Controlled Computer — Control and Interaction Beyond an Embedded System - page 92
-Tae Hun Kim, Satoru Fukayama, Takuya Nishimoto and Shigeki Sagayama: Polyhymnia: An automatic piano performance system with statistical modeling of polyphonic expression and musical symbol interpretation - page 96
-Juan Pablo Carrascal and Sergi Jorda: Multitouch Interface for Audio Mixing - page 100
-Nate Derbinsky and Georg Essl: Cognitive Architecture in Mobile Music Interactions - page 104
-Benjamin D. Smith and Guy E. Garnett: The Self-Supervising Machine - page 108
-Aaron Albin, Sertan Senturk, Akito Van Troyer, Brian Blosser, Oliver Jan and Gil Weinberg: Beatscape, a mixed virtual-physical environment for musical ensembles - page 112
-Marco Fabiani, Gaël Dubus and Roberto Bresin: MoodifierLive: Interactive and collaborative expressive music performance on mobile devices - page 116
-Benjamin Schroeder, Marc Ainger and Richard Parent: A Physically Based Sound Space for Procedural Agents - page 120
-Francisco Garcia, Leny Vinceslas, Esteban Maestre and Josep Tubau Acquisition and study of blowing pressure profiles in recorder playing - page 124
-Anders Friberg and Anna Källblad:Experiences from video-controlled sound installations - page 128
-Nicolas d’Alessandro, Roberto Calderon and Stefanie Müller: ROOM#81 —Agent-Based Instrument for Experiencing Architectural and Vocal Cues - page 132

Demo session C — Monday 30 May 13:30–14:30 
-Yasuo Kuhara and Daiki Kobayashi: Kinetic Particles Synthesizer Using Multi-Touch Screen Interface of Mobile Devices - page 136
-Christopher Carlson, Eli Marschner and Hunter Mccurry: The Sound Flinger: A Haptic Spatializer - page 138
-Ravi Kondapalli and Benzhen Sung: Daft Datum – an Interface for Producing Music Through Foot-Based Interaction - page 140
-Charles Martin and Chi-Hsia Lai: Strike on Stage: a percussion and media performance - page 142

Paper session D — Monday 30 May 14:30–15:30 
-Baptiste Caramiaux, Patrick Susini, Tommaso Bianco, Frédéric Bevilacqua, Olivier Houix, Norbert Schnell and Nicolas Misdariis: Gestural Embodiment of Environmental Sounds: an Experimental Study - page 144
-Sebastian Mealla, Aleksander Valjamae, Mathieu Bosi and Sergi Jorda: Listening to Your Brain: Implicit Interaction in Collaborative Music Performances - page 149
-Dan Newton and Mark Marshall: Examining How Musicians Create Augmented Musical Instruments - page 155
 
Paper session E — Monday 30 May 16:00–17:00 
-Zachary Seldess and Toshiro Yamada: Tahakum: A Multi-Purpose Audio Control Framework - page 161
-Dawen Liang, Guangyu Xia and Roger Dannenberg: A Framework for Coordination and Synchronization of Media - page 167
-Edgar Berdahl and Wendy Ju: Satellite CCRMA: A Musical Interaction and Sound Synthesis Platform  - page 173
 
Paper session F — Tuesday 31 May 09:00–10:50 
-Nicholas J. Bryan and Ge Wang: Two Turntables and a Mobile Phone - page 179
-Nick Kruge and Ge Wang: MadPad: A Crowdsourcing System for Audiovisual Sampling - page 185
-Patrick O’Keefe and Georg Essl: The Visual in Mobile Music Performance - page 191
-Ge Wang, Jieun Oh and Tom Lieber: Designing for the iPad: Magic Fiddle - page 197
-Benjamin Knapp and Brennon Bortz: MobileMuse: Integral Music Control Goes Mobile - page 203
-Stephen Beck, Chris Branton, Sharath Maddineni, Brygg Ullmer and Shantenu Jha: Tangible Performance Management of Grid-based Laptop Orchestras - page 207
 
Poster session G— Tuesday 31 May 13:30–14:30
-Smilen Dimitrov and Stefania Serafin: Audio Arduino—an ALSA (Advanced Linux Sound Architecture) audio driver for FTDI-based Arduinos - page 211
-Seunghun Kim and Woon Seung Yeo: Musical control of a pipe based on acoustic resonance - page 217
-Anne-Marie Hansen, Hans Jørgen Andersen and Pirkko Raudaskoski: Play Fluency in Music Improvisation Games for Novices - page 220
-Izzi Ramkissoon: The Bass Sleeve: A Real-time Multimedia Gestural Controller for Augmented Electric Bass Performance - page 224
-Ajay Kapur, Michael Darling, James Murphy, Jordan Hochenbaum, Dimitri Diakopoulos and Trimpin: The KarmetiK NotomotoN: A New Breed of Musical Robot for Teaching and Performance - page 228
-Adrian Barenca Aliaga and Giuseppe Torre:  The Manipuller: Strings Manipulation and Multi-Dimensional Force Sensing - page 232
-Alain Crevoisier and Cécile Picard-Limpens: Mapping Objects with the Surface Editor - page 236
-Jordan Hochenbaum and Ajay Kapur: Adding Z-Depth and Pressure Expressivity to Tangible Tabletop Surfaces - page 240
-Andrew Milne, Anna Xambó, Robin Laney, David B. Sharp, Anthony Prechtl and Simon Holland: Hex Player—A Virtual Musical Controller - page 244
-Carl Haakon Waadeland: Rhythm Performance from a Spectral Point of View - page 248
-Josep M Comajuncosas, Enric Guaus, Alex Barrachina and John O’Connell: Nuvolet : 3D gesture-driven collaborative audio mosaicing - page 252
-Erwin Schoonderwaldt and Alexander Refsum Jensenius: Effective and expressive movements in a French-Canadian fiddler’s performance - page 256
-Daniel Bisig, Jan Schacher and Martin Neukom: Flowspace – A Hybrid Ecosystem - page 260
-Marc Sosnick and William Hsu: Implementing a Finite Difference-Based Real-time Sound Synthesizer using GPUs - page 264
-Axel Tidemann: An Artificial Intelligence Architecture for Musical Expressiveness that Learns by Imitation - page 268
-Luke Dahl, Jorge Herrera and Carr Wilkerson: TweetDreams: Making music with the audience and the world using real-time Twitter data - page 272
-Lawrence Fyfe, Adam Tindale and Sheelagh Carpendale: JunctionBox: A Toolkit for Creating Multi-touch Sound Control Interfaces - page 276
-Andrew Johnston: Beyond Evaluation: Linking Practice and Theory in New Musical Interface Design - page 280
-Phillip Popp and Matthew Wright: Intuitive Real-Time Control of Spectral Model Synthesis - page 284
-Pablo Molina, Martin Haro and Sergi Jordà: BeatJockey: A new tool for enhancing DJ skills - page 288
-Jan Schacher and Angela Stoecklin: Traces – Body, Motion and Sound - page 292
-Grace Leslie and Tim Mullen: MoodMixer: EEG-based Collaborative Sonification - page 296
-Ståle A. Skogstad, Kristian Nymoen, Yago de Quay and Alexander Refsum Jensenius: OSC Implementation and Evaluation of the Xsens MVN suit - page 300
-Lonce Wyse, Norikazu Mitani and Suranga Nanayakkara: The effect of visualizing audio targets in a musical listening and performance task - page 304
-Freed Adrian, John Maccallum and Andrew Schmeder: Composability for Musical Gesture Signal Processing using new OSC-based Object and Functional Programming Extensions to Max/MSP - page 308
-Kristian Nymoen, Ståle A. Skogstad and Alexander Refsum Jensenius: SoundSaber —A Motion Capture Instrument - page 312
-Øyvind Brandtsegg, Sigurd Saue and Thom Johansen: A modulation matrix for complex parameter sets - page 316

Demo session H— Tuesday 31 May 13:30–14:30 
-Yu-Chung Tseng, Che-Wei Liu, Tzu-Heng Chi and Hui-Yu Wang: Sound Low Fun- page 320
-Edgar Berdahl and Chris Chafe: Autonomous New Media Artefacts (AutoNMA) - page 322
-Min-Joon Yoo, Jin-Wook Beak and In-Kwon Lee: Creating Musical Expression using Kinect - page 324
-Staas de Jong: Making grains tangible: microtouch for microsound - page 326
Baptiste Caramiaux, Frederic Bevilacqua and Norbert Schnell: Sound Selection by Gestures - page 329

Paper session I — Tuesday 31 May 14:30–15:30 
-Hernán KerlleÃevich, Manuel Eguia and Pablo Riera: An Open Source Interface based on Biological Neural Networks for Interactive Music Performance - page 331
-Nicholas Gillian, R. Benjamin Knapp and Sile O’Modhrain: Recognition Of Multivariate Temporal Musical Gestures Using N-Dimensional Dynamic Time Warping - page 337
-Nicholas Gillian, R. Benjamin Knapp and Sile O’Modhrain: A Machine Learning Toolbox For Musician Computer Interaction - page 343
 
Paper session J — Tuesday 31 May 16:00–17:00 
-Elena Jessop, Peter Torpey and Benjamin Bloomberg: Music and Technology in Death and the Powers - page 349
-Victor Zappi, Dario Mazzanti, Andrea Brogni and Darwin Caldwell: Design and Evaluation of a Hybrid Reality Performance - page 355
-Jérémie Garcia, Theophanis Tsandilas, Carlos Agon and Wendy Mackay: InkSplorer : Exploring Musical Ideas on Paper and Computer - page 361

Paper session K — Wednesday 1 June 09:00–10:30 
-Pedro Lopes, Alfredo Ferreira and Joao Madeiras Pereira: Battle of the DJs: an HCI perspective of Traditional, Virtual, Hybrid and Multitouch DJing - page 367
-Adnan Marquez-Borbon, Michael Gurevich, A. Cavan Fyans and Paul Stapleton: Designing Digital Musical Interactions in Experimental Contexts - page 373
-Jonathan Reus: Crackle: A mobile multitouch topology for exploratory sound interaction - page 377
-Samuel Aaron, Alan F. Blackwell, Richard Hoadley and Tim Regan: A principled approach to developing new languages for live coding - page 381
-Jamie Bullock, Daniel Beattie and Jerome Turner: Integra Live: a new graphical user interface for live electronic music - page 387

Paper session L — Wednesday 1 June 11:00–12:30 
-Jung-Sim Roh, Yotam Mann, Adrian Freed and David Wessel: Robust and Reliable Fabric, Piezoresistive Multitouch Sensing Surfaces for Musical Controllers - page 393
-Mark Marshall and Marcelo Wanderley: Examining the Effects of Embedded Vibrotactile Feedback on the Feel of a Digital Musical Instrument - page 399
-Dimitri Diakopoulos and Ajay Kapur: HIDUINO: A firmware for building driverless USB-MIDI devices using the Arduino microcontroller - page 405
-Emmanuel Flety and Côme Maestracci: Latency improvement in sensor wireless transmission using IEEE 802.15.4 - page 409
-Jeff Snyder: The Snyderphonics Manta, a Novel USB Touch Controller - page 413
 
Poster session M — Wednesday 1 June 13:30–14:30
-William Hsu: On Movement, Structure and Abstraction in Generative Audiovisual Improvisation - page 417
-Claudia Robles Angel: Creating Interactive Multimedia Works with Bio-data - page 421
-Paula Ustarroz: TresnaNet: musical generation based on network protocols - page 425
-Matti Luhtala, Tiina Kymäläinen and Johan Plomp: Designing a Music Performance Space for Persons with Intellectual Learning Disabilities - page 429
-Tom Ahola, Teemu Ahmaniemi, Koray Tahiroglu, Fabio Belloni and Ville Ranki: Raja —A Multidisciplinary Artistic Performance - page 433
-Emmanuelle Gallin and Marc Sirguy: Eobody3: A ready-to-use pre-mapped & multi-protocol sensor interface- page 437
-Rasmus Bååth, Thomas Strandberg and Christian Balkenius: Eye Tapping: How to Beat Out an Accurate Rhythm using Eye Movements - page 441
-Eric Rosenbaum: MelodyMorph: A Reconfigurable Musical Instrument - page 445
-Karmen Franinovic: Flo)(ps: Between Habitual and Explorative Action-Sound Relationships - page 448
-Margaret Schedel, Rebecca Fiebrink and Phoenix Perry: Wekinating 000000Swan: Using Machine Learning to Create and Control Complex Artistic Systems - page 453
-Carles F. Julià, Daniel Gallardo and Sergi Jordà: MTCF: A framework for designing and coding musical tabletop applications directly in Pure Data - page 457
-David Pirrò and Gerhard Eckel: Physical modelling enabling enaction: an example - page 461
-Thomas Mitchell and Imogen Heap: SoundGrasp: A Gestural Interface for the Performance of Live Music - page 465
-Tim Mullen, Richard Warp and Adam Jansch: Minding the (Transatlantic) Gap: An Internet-Enabled Acoustic Brain-Computer Music Interface - page 469
-Stefano Papetti, Marco Civolani and Federico Fontana: Rhythm’n’Shoes: a wearable foot tapping interface with audio-tactile feedback - page 473
-Cumhur Erkut, Antti Jylhä and Reha Di¸sçio˘glu: A structured design and evaluation model with application to rhythmic interaction displays - page 477
-Marco Marchini, Panos Papiotis, Alfonso Perez and Esteban Maestre: A Hair Ribbon Deflection Model for Low-Intrusiveness Measurement of Bow Force in Violin Performance - page 481
-Jonathan Forsyth, Aron Glennon and Juan Bello: Random Access Remixing on the iPad - page 487
-Erika Donald, Ben Duinker and Eliot Britton: Designing the EP trio: Instrument identities, control and performance practice in an electronic chamber music ensemble - page 491
-Cavan Fyans and Michael Gurevich: Perceptions of Skill in Performances with Acoustic and Electronic Instruments - page 495
-Hiroki Nishino: Cognitive Issues in Computer Music Programming - page 499
-Roland Lamb and Andrew Robertson: Seaboard: a new piano keyboard-related interface combining discrete and continuous control - page 503
-Gilbert Beyer and Max Meier: Music Interfaces for Novice Users: Composing Music on a Public Display with Hand Gestures - page 507
-Birgitta Cappelen and Anders-Petter Andersson: Expanding the role of the instrument - page 511
-Todor Todoroff: Wireless Digital/Analog Sensors for Music and Dance Performances - page 515
-Trond Engum: Real-time control and creative convolution— exchanging techniques between distinct genres - page 519
-Andreas Bergsland: The Six Fantasies Machine – an instrument modelling phrases from Paul Lansky’s Six Fantasies - page 523
 
Demo session N — Wednesday 1 June 13:30–14:30
-Jan Trützschler von Falkenstein: Gliss: An Intuitive Sequencer for the iPhone and iPad - page 527
-Jiffer Harriman, Locky Casey, Linden Melvin and Mike Repper: Quadrofeelia — A New Instrument for Sliding into Notes - page 529
-Johnty Wang, Nicolas D’Alessandro, Sidney Fels and Bob Pritchard: SQUEEZY: Extending a Multi-touch Screen with Force Sensing Objects for Controlling Articulatory Synthesis - page 531
-Souhwan Choe and Kyogu Lee: SWAF: Towards a Web Application Framework for Composition and Documentation of Soundscape - page 533
-Norbert Schnell, Frederic Bevilacqua, Nicolas Rasamimana, Julien Blois, Fabrice Guedy and Emmanuel Flety: Playing the ""MO"" —Gestural Control and Re-Embodiment of Recorded Sound and Music - page 535
-Bruno Zamborlin, Marco Liuni and Giorgio Partesana: (LAND)MOVES - page 537
-Bill Verplank and Francesco Georg: Can Haptics make New Music? —Fader and Plank Demos - page 53",Proceedings of the International Conference on New Interfaces for Musical Expression,,https://core.ac.uk/download/30857249.pdf,,,core
41337136,2011-01-01T00:00:00,"This thesis focuses on the development of an artificial intelligence system for a heterogeneous ensemble of mobile robots. Many robots in the ensemble may have limited processing, communication, sensing, and/or actuation capabilities. This means that each robot may not be able to execute all tasks that are input to the system. A hierarchical system is proposed to permit robots with superior processing and communication abilities to assign tasks and coordinate the less computationally able robots. The limited processing robots may also utilise the resources of superior robots during task execution. Effective task allocation and coordination should result in efficient execution of a global task. Many existing approaches to robot task allocation assume expert knowledge for task specification. This is not ideal if a non-expert human user wants to modify the task requirements.
A novel reduced human user input task allocation and feedback coordination technique for limited capability mobile robots is developed and implemented. Unlike existing approaches, the presented method focuses on expressing tasks and robots in terms of processing, communication, sensing, and actuation physical resources. This has the potential to allow non-expert human users to specify tasks to the team of robots. Fuzzy inference systems are utilised to simplify detailed robot information for comparison with simple human user inputs that represent task resource requirements. Like many existing task allocation methods, a greedy algorithm is employed to select robots. This can result in suboptimal task allocation. In addition to this, the non-expert user’s task specifications might be erroneous in some instances. Hence, a feedback coordination component monitors robot performance during task execution.
In this thesis, a customised multi-robot mapping and exploration task is utilised as a model task to test the effectiveness of the developed task allocation and feedback coordination strategy. Extensive simulation experiments with various robot team configurations are executed in environments of varying sizes and obstacle densities to assess the performance of the technique. Task allocation is able to identify suitable robots and is robust to selection weight variation. The task allocation process is subjective to fuzzy membership function parameters which may vary for different This thesis focuses on the development of an artificial intelligence system for a heterogeneous ensemble of mobile robots. Many robots in the ensemble may have limited processing, communication, sensing, and/or actuation capabilities. This means that each robot may not be able to execute all tasks that are input to the system. A hierarchical system is proposed to permit robots with superior processing and communication abilities to assign tasks and coordinate the less computationally able robots. The limited processing robots may also utilise the resources of superior robots during task execution. Effective task allocation and coordination should result in efficient execution of a global task. Many existing approaches to robot task allocation assume expert knowledge for task specification. This is not ideal if a non-expert human user wants to modify the task requirements.
A novel reduced human user input task allocation and feedback coordination technique for limited capability mobile robots is developed and implemented. Unlike existing approaches, the presented method focuses on expressing tasks and robots in terms of processing, communication, sensing, and actuation physical resources. This has the potential to allow non-expert human users to specify tasks to the team of robots. Fuzzy inference systems are utilised to simplify detailed robot information for comparison with simple human user inputs that represent task resource requirements. Like many existing task allocation methods, a greedy algorithm is employed to select robots. This can result in suboptimal task allocation. In addition to this, the non-expert user’s task specifications might be erroneous in some instances. Hence, a feedback coordination component monitors robot performance during task execution.
In this thesis, a customised multi-robot mapping and exploration task is utilised as a model task to test the effectiveness of the developed task allocation and feedback coordination strategy. Extensive simulation experiments with various robot team configurations are executed in environments of varying sizes and obstacle densities to assess the performance of the technique. Task allocation is able to identify suitable robots and is robust to selection weight variation. The task allocation process is subjective to fuzzy membership function parameters which may vary for different users. Feedback coordination is robust to variation in weights and thresholds for failure detection. This permits the correction of suboptimal allocations arising from greedy task allocation, incorrect initial task specifications or unexpected failures. By being robust within the tested limits, weights and thresholds can be intuitively selected. However, other parameters such as ideal achievement data can be difficult to accurately characterise in some instances.
A hierarchical hybrid deliberative-reactive navigation system for memory constrained heterogeneous robots to navigate obstructed environments is developed. Deliberative control is developed using a modified version of the A* algorithm and a rectangular occupancy grid map. A novel two-tiered path planner executes on limited memory mobile robots utilising the memory of a computationally powerful robot to enable navigation beyond localised regions of a large environment. Reactive control is developed using a modified dynamic window approach and a polar histogram technique to remove the need for periodic path planning.
A range of simulation experiments in different sized environments is conducted to assess the performance of the two-tiered path planning strategy. The path planner is able to achieve superior or comparable execution times to non-memory constrained path planning when small sized local maps are employed in large global environments. Performance of hybrid deliberative-reactive navigation is assessed in a range of simulated environments and is also validated on a real robot. The developed reactive control system outperforms the dynamic window method",Development of an Artificial Intelligence System for the Instruction and Control of Cooperating Mobile Robots,,,'Victoria University of Wellington Library',,core
36204674,2009-01-01T00:00:00,"Urban Search And Rescue (USAR) is a time critical task since all survivors have to be rescued within the first 72 hours. One goal in Rescue Robotics is to support emergency response by mixed-initiative teams consisting of humans and robots. Their task is to explore the disaster area rapidly while reporting victim locations and hazardous areas to a central station, which then can be utilized for planning rescue missions. To fulfill this task efficiently, humans and robots have to map disaster areas jointly while co- ordinating their search at the same time. Additionally, robots have to perform subproblems, such as victim detection and navigation, autonomously. In disaster areas these problems are extraordinarily challenging due to the unstructured environment and rough terrain. Furthermore, when communication fails, methods that are deployed under such conditions have to be decentralized, i.e. operational without a central station. In this thesis a unified approach joining human and robot resources for solving these problems is contributed. Following the vision of combined multi-robot and multi-human teamwork, core problems, such as position tracking on rough terrain, mapping by mixed teams, and decentralized team coordination with limited radio communication, are directly addressed. More specific, RFID-SLAM, a novel method for robust and efficient loop closure in large-scale environments that utilizes RFID technology for data association, is contributed. The method is capable of jointly improving multiple maps from humans and robots in a centralized and decentralized manner without requiring team members to perform loops on their routes. Thereby positions of humans are tracked by PDR (Pedestrian Dead Reckoning), and robot positions by slippage- sensitive odometry, respectively. The joint-graph emerging from these trajectories serves as an input for an iterative map optimization procedure. The introduced map representation is further utilized for solving the centralized and decentralized coordination of large rescue teams. On the one hand, a deliberate method for combined task assignment and multi-agent path planning, and on the other hand, a local search method using the memory of RFIDs for coordination, are proposed. For autonomous robot navigation on rough terrain and real-time victim detection in disaster areas an efficient method for elevation map building and a novel approach to genetic MRF (Markov Random Field) model optimization are contributed. Finally, a human in the loop architecture is presented that integrates data collected by first responders into a multi-agent system via wearable computing. In this context, the support and coordination of disaster mitigation in large-scale environments from a central-command-post-perspective are described. Methods introduced in this thesis were extensively evaluated in outdoor environments and official USAR testing arenas designed by the National Institute of Standards and Technology (NIST). Furthermore, they were an integral part of systems that won in total more than 10 times the first prize at international competitions, such as the RoboCup world championships.This is a Ph.D. thesis originally defended at University of Freiburg.Artificial Intelligence & Integrated Computer System",Mapping and Exploration for Search and Rescue with Humans and Mobile Robots,,,Freiburg : University of Freiburg,,core
21131321,17/02/2010,"The process of developing genetic algorithms, genetic programs or training neural networks is a time consuming task. When the target device is an autonomous mobile robot, this development is often performed using software simulation. Software simulations are a cost effective tool and provide researchers with the ability to test out multiple algorithms quickly and efficiently. However, the end result is that the optimized algorithm(s) must be implemented and tested on an actual robot to evaluate performance in the real world. Significant cost can be associated with this final step. In this paper we propose to leverage Radio Frequency Identification (RFID) and a low-cost RFID capable mobile robot with the intent of creating basic foraging behavior. Additionally, we will present experimental results that demonstrate the effectiveness of using Genetic Programming (GP) and a low-cost RFID capable robot t",Using RFID and a Low Cost Robot to Evolve Foraging Behavior,,,,,core
147909108,2006-01-12T12:30:26,"Perception is needed for action, not for the pure sake of the construction of abstract representations, although it does not exclude the role of internal representations for mediating complex behaviors. We think that, for the purpose of building autonomous robots, active perception requires specific recipes for three related aspects: the design of the physical sensory system, the modality and type of information extracted, and the structure and functioning of the control system. We outline a set of solutions for these three aspects and describe their implementation on a real mobile robot through a set of three different experiments using a combination of neural networks and genetic algorithms. The results show that active perception is a useful feature that is exploited by autonomous agents. The experiments show that the combination of genetic algorithms and neural networks is a feasible and fruitful technique for the development of active perception in autonomous agents","Active Perception, Navigation, Homing, and Grasping: An autonomous Perspective",https://core.ac.uk/download/147909108.pdf,,,,core
21147322,2006,"Abstract: In this work, several localization algorithms that are designed and implemented for Cerberus&apos;05 Robot Soccer Team are analyzed and compared. These algorithms are used for global localization of autonomous mobile agents in the robotic soccer domain, to overcome the uncertainty in the sensors, environment and the motion model. The algorithms are Reverse Monte Carlo Localization (R-MCL), Simple Localization (S-Loc) and Sensor Resetting Localization (SRL). R-MCL is a hybrid method based on both Markov Localization (ML) and Monte Carlo Localization (MCL) where the ML module finds the region where the robot should be and MCL predicts the geometrical location with high precision by selecting samples in this region. S-Loc is another localization method where just one sample per percept is drawn, for global localization. Within this method another novel method My Environment (ME) is designed to hold the history and overcome the lack of information due to the drastically decrease in the number of samples in S-Loc. ME together with S-Loc is used in the Technical Challenges in Robocup 2005 and play an important role in ranking the First Place in the Challenges. In this work, these methods together with SRL, which is a widely used successful localization algorithm, are tested with both offline and real-time tests. First they are tested on a challenging data set that is used by many researches and compared in terms of error rate against different levels of noise, and sparsity. Besides time required recovering from kidnapping and speed of the methods are tested and compared. Then their performances are tested with real-time tests with scenarios like the ones in the Technical Challenges in ROBOCUP. The main aim is to find the best method which is very robust and fast and requires less computational power and memory compared to similar approaches and is accurate enough for high level decision making which is vital for robot soccer",Comparison of localization methods for a robot soccer team,,10.5772/5727,,,core
37835704,2007,"Deploying AI, and specifically P&S, technology into the real world entails many stimulating problems for researchers and system designers. Various smart solutions have been produced both for specialized problems and, in fewer cases, for more general purpose domains. An important aspect which is fundamental for any successful application, and more specifically for those which address a broad audience, concerns the users\u27 perception and acceptance of technology. This issue is often either neglected or underestimated. We have been working for two years on the issue of importing experimental techniques from HCI and experimental psychology into smart system development. This approach has both pros - interesting features from the user perspective are discovered and can be used to bias design and research activities - and cons - experimenting with humans adds additional difficulty to the project, and applying a correct methodology is very time consuming. This paper describes a fielded experimental investigation of a fully implemented AI system named ROBO CARE . The system uses constraint-based scheduling technology to actively monitor a pattern of activities executed by an assisted person and uses detected temporal constraint violations to trigger meaningful and contextualized proactive interaction. Dialogue with the users is managed by a robotic mediator who acts as the main communication channel between the users and the intelligent domestic environment. The paper presents an evaluation of elderly people\u27s perception of the intelligent system, focusing on aspects related to the robot\u27s aspect, interaction modalities, content and timing of interaction, providing suggestions and hints for system designers",Caring About the User\u27s View: The Joys and Sorrows of Experiments with People,https://core.ac.uk/download/pdf/37835704.pdf,,,,core
24393253,2005,"Indoor environments can typically be divided into  places with different functionalities like kitchens, offices,  or seminar rooms. We believe that such semantic  information enables a mobile robot to more efficiently  accomplish a variety of tasks such as human-robot interaction,  path-planning, or localization. This paper  presents a supervised learning approach to label different  locations using boosting. We train a classifier using  features extracted from vision and laser range data. Furthermore,  we apply a Hidden Markov Model to increase  the robustness of the final classification. Our technique  has been implemented and tested on real robots as well  as in simulation. The experiments demonstrate that our  approach can be utilized to robustly classify places into  semantic categories. We also present an example of localization  using semantic labeling",Semantic Place Classification of Indoor Environments With Mobile Robots using Boosting,,,,,core
429704714,2007-08-31T00:00:00,"The University of Edinburgh and research sponsors are authorised to reproduce and distribute reprints and on-line copies for their purposes notwithstanding any copyright annotation hereon.  The views and conclusions contained herein are the author’s and shouldn’t be interpreted as necessarily representing the official policies or endorsements, either expressed or implied, of other parties.The Collaborative Operations for Personnel Recovery (Co-OPR) project sought to
provide collaborative task support for a Search and Rescue coordination center. The
project aimed to create a prototype “Personnel Recovery (Experimental) Pack” (PREP) and to demonstrate its use.

A number of requirements capture, knowledge gathering and transition workshops and
meetings were held. This included an initial requirements setting workshop early in 2005,
meetings at the USJFCOM Joint Personnel Recovery Agency’s (JPRA) Personnel
Recovery Education and Training Center (PRETC) in Fredericksburg, Virginia in June
2005, a review meeting in Edinburgh in August 2005, and attendance at a Command Post
Exercise (CPX) at the PRETC in November 2005. These initially established the potential
areas for use of Co-OPR and I-X tools in training exercises. In the second project year such tools were developed and tested using a training exercise as held at the PRETC, using observers from PRETC and USJFCOM and an evaluation expert from USJFCOM/J9.

The project was provided with a rich set of urban and rural scenarios by JPRA/PRETC
which together are unclassified versions of scenarios used within the PRETC training
courses and Command Post Exercises. At the time, these stretched the capabilities of the
current and envisaged technologies within Co-OPR/I-X. Refinement of the scenarios
alongside PRETC, and knowledge engineering to capture information on standard operating procedures and responses were a key part of making the work relevant to the potential real use of Co-OPR/I-X for Personnel Recovery.

The core I-X technology was packaged into a number of checkpoint releases to make
available the features required to meet the application needs. I-X version 4.3 released in November 2005 to checkpoint the results achieved on the first 12 months of work with
the PRETC. It formed a basis for the work on really using the technology at the PRETC.
New “white cell” aids for training were made available in an initial version. A new I-Sim
simulation capability, and advanced option exploration tools have all been improved
significantly to make them more usable, including the features of the I-Plan AI planner and its capability for plan repair after failures. The features of the Domain Editor (I-DE) and its ability to browse and update or augment standard operating procedure knowledge dynamically during missions were enhanced. The final release of I-X that includes all the new developments achieved in the Co-OPR project is version 4.5.

The results of the work were packaged, along with Personnel Recovery domain specific
models, as a web site and/or CD which could be considered as a prototype “Personnel
Recovery (Experimental) Pack” of tools to assist a Joint Personnel Recovery Center
(JPRC) and associated operational staff in performing their operations. The versions of
PREP produced were used in one workshop or Command Post Exercise at the PRETC
under guidance from Dr. Jeff Hansberger at training related workshops already organized
by USJFCOM/J9 Expt. and Fred Kleibacker, the (now former) Director of the PRETC in
Fredericksburg, Virginia. Co-OPR team members were engaged with these workshops to
to show the tools in realistic settings, to assist with training where possible, and to gather experimental feedback.

Realistic use of tools for Personnel Recovery requires that the systems can work with
emerging technology for geo-positioning, survival radios, evasion aids, robotic or semiautomated rescue aids or robots, and doctrine or tactics, techniques and procedures for Personnel Recovery. A number of short studies of these “complementary technologies” were made which explored deployment and inter-working aspects of these with the Co-OPR/I-X technology",Collaborative Operations for Personnel Recovery Final Report on DARPA/AFRL,,,Artificial Intelligence Applications Institute,,core
235648671,2007-01-01T00:00:00,"This paper describes an integrated system based on open-domain and domain-specific knowledge for the purpose of providing query-based intelligent web interaction. It is understood that general purpose conversational agents are not able to answer questions on specific domain subject. On the other hand, domain specific systems lack the flexibility to handle common sense questions. To overcome the above limitations, this paper proposed an integrated system comprises of an artificial intelligent conversation software robot or chatterbot, called Artificial Intelligence Natural-language Identity (hereafter, AINI), and an Automated Knowledge Extraction Agent (AKEA) for the acquisition of real world knowledge from the Internet. The objective of AKEA is to retrieve real world knowledge or information from trustworthy websites. AINI is the mechanism used to manage the knowledge and to provide appropriate answer to the user. In this paper, we compare the performance of the proposed system against two popular search engines, two question answering systems and two other conversational systems",Query based intelligent web interaction with real world knowledge,https://core.ac.uk/download/235648671.pdf,,"Ohmsha, Ltd.",,core
54376977,2007-01-01T00:00:00,"In real-world robotic applications, many factors, both at low-level (e.g., vision and motion control parameters) and at high-level (e.g., the behaviors) determine the quality of the robot performance. Thus, for many tasks, robots require fine tuning of the parameters, in the implementation of behaviors and basic control actions, as well as in strategic decisional processes. In recent years, machine learning techniques have been used to find optimal parameter sets for different behaviors. However, a drawback of learning techniques is time consumption: in practical applications, methods designed for physical robots must be effective with small amounts of data. In this paper, we present a method for concurrent learning of best strategy and optimal parameters, by extending the policy gradient reinforcement learning algorithm. The results of our experimental work in a simulated environment and on a real robot show a very high convergence rate. ©2007 IEEE",An extended policy gradient algorithm for robot task learning,,10.1109/IROS.2007.4399219,'Institute of Electrical and Electronics Engineers (IEEE)',,core
20820357,2005,"(Under the direction of Walter D. Potter) Autonomous robot design is a challenging field in artificial intelligence. It is not easy to design autonomous robots to perform useful tasks in real environments. There are two reasons why designing a control system for an autonomous robot is a difficult task. The first reason is that it is difficult to coordinate different parts of the robot to work together and to operate as the control system requested. The second reason is that autonomous robots interact with an environment, which means they may face unpredictable environment situations so that their control system may be misled. A novel approach using Genetic Algorithm simulation is discussed in this thesis and the results of the simulation are verified physically by real robot implementation",An Area Exploration Strategy,,,,,core
100678425,2007,"Abstract—In real-world robotic applications, many factors, both at low-level (e.g., vision and motion control parameters) and at high-level (e.g., the behaviors) determine the quality of the robot performance. Thus, for many tasks, robots re-quire fine tuning of the parameters, in the implementation of behaviors and basic control actions, as well as in strategic deci-sional processes. In recent years, machine learning techniques have been used to find optimal parameter sets for different behaviors. However, a drawback of learning techniques is time consumption: in practical applications, methods designed for physical robots must be effective with small amounts of data. In this paper, we present a method for concurrent learning of best strategy and optimal parameters, by extending the policy gradient reinforcement learning algorithm. The results of our experimental work in a simulated environment and on a real robot show a very high convergence rate. I",An extended policy gradient algorithm for robot task learning,,10.1109/iros.2007.4399219,Morgan Kaufmann,,core
103041451,2007,"Deploying AI, and specifically P&amp;S, technology into the real world entails many stimulating problems for researchers and system designers. Various smart solutions have been pro-duced both for specialized problems and, in fewer cases, for more general purpose domains. An important aspect which is fundamental for any successful application, and more specif-ically for those which address a broad audience, concerns the users ’ perception and acceptance of technology. This issue is often either neglected or underestimated. We have been working for two years on the issue of import-ing experimental techniques from HCI and experimental psy-chology into smart system development. This approach has both pros – interesting features from the user perspective are discovered and can be used to bias design and research activ-ities – and cons – experimenting with humans adds additional difficulty to the project, and applying a correct methodology is very time consuming. This paper describes a fielded experimental investigation of a fully implemented AI system named ROBOCARE. The sys-tem uses constraint-based scheduling technology to actively monitor a pattern of activities executed by an assisted per-son and uses detected temporal constraint violations to trig-ger meaningful and contextualized proactive interaction. Di-alogue with the users is managed by a robotic mediator who acts as the main communication channel between the users and the intelligent domestic environment. The paper presents an evaluation of elderly people’s perception of the intelligent system, focusing on aspects related to the robot’s aspect, in-teraction modalities, content and timing of interaction, pro-viding suggestions and hints for system designers",Caring About the User’s View: The Joys and Sorrows of Experiments with People,,,,,core
147921699,2006-11-24T14:05:59,"Over the last decades, calibration techniques have been widely used in robotics since they represent a cost-effective solution for improving the accuracy of robots and machine-tools. They only involve software modification without the necessity of revising the robot design or tightening the manufacturing tolerances. The goal of this thesis is to propose a procedure that guides the engineer through the calibration of a given multi-DOF flexure parallel robot within sub-µm accuracy. Two robots having 3 and 6 degrees of freedom have been considered as a case-study throughout the work. As in any calibration procedure, the work has been conducted on three different fronts: measurement, data processing and validation. The originality of this thesis in respect to published material lies in these three points. Measurements were carried out in a chamber inside which the measuring environment was protected against mechanical and thermal perturbations. In particular, the temperature variations experienced by the different parts of the measuring loop during a typical measurement session were stabilized within less than ± 0.1 °C. Proposed procedures allow the collection of reliable sets of data on the two robots. Delicate aspects of practical implementation are discussed. In particular, the problem of collecting a complete set of 6D data within accuracies in the nanometre range, for which there is still a lack of standard equipment, is solved using a procedure comprising several steps and making use of existing instrumentation. Suggestions for future investigations are given, regarding either long-term research problems or short-term industrial implementation issues. Data processing was performed using two different techniques in order to reach absolute accuracies after calibration better than ± 100 nm for translations and ± 3 arcsec for rotations (± 0.3 arcsec inside a more restricted range of ± 0.11°). The first method is called the ""model-based approach"" and requires the use of a known analytical relationship between the motor and operational coordinates of the robot. This relationship involves a certain number of parameters that can be related to the geometry of the robot (physical models) or simply mathematical coefficients of an approximating mathematical function (behavioural models). In the case of high-precision multi-DOF flexure parallel robots, we show that polynomial-based behavioural models are preferable to physical models in terms of accuracy for data processing tasks. In the second method, called the ""model-free approach"", the user does not need to model explicitly the main error sources (or their effect) affecting the robot accuracy. A model-free approach has been implemented using Artificial Neural Networks. We show that, using a heuristic search based on a decision-tree, the architecture of a network with satisfactory prediction capability can be found systematically. In particular, this algorithm can find a network able to predict the direct correspondence between the motor and operational coordinates (within the desired accuracy) without the help of the Inverse Geometric Model of the robot, i.e. even if the nominal geometry of the robot being calibrated remains unknown. This result contradicts conclusions reported by previous researchers. It is claimed that any robot (not necessarily a high-precision flexure parallel mechanism) can be calibrated by means of a ""neural approach"" in which the architecture of an appropriate network is determined with the help of our algorithm. Two examples (other than the robots measured in this thesis) are given to illustrate this universality. In the last part of this work, we provide a feasibility study on the use of indentation, a technique traditionally used for material testing, as a validation procedure to assess the accuracy of the calibrated degrees of freedom. The industrial interest of this technique lies in the fact that the robot is asked to execute similar motions to those involved in a real micro-machining operation",Calibration of high-precision flexure parallel robots,https://core.ac.uk/download/147921699.pdf,10.5075/epfl-thesis-3712,"Lausanne, EPFL",,core
22559811,2007,"We expand on previous work with the Hybrid Introspective Robot Modeling (HIRM) algorithm, which is an algorithm that develops introspective models of robots with real-world experience. These introspective models that are developed by this algorithm can then be used to accurately plan behaviors in simulation. HIRM is important because using current machine learning techniques to autonomously develop optimal robot controllers is difficult. Machine learning can be applied in the same way it is with software: develop a policy that optimally maps a set of perceptions to a set of actions. This is inefficient with robots because reinforcement learning uses physical trial-and-error. Another approach is to learn a robot controller in simulation. This technique does not have the same efficiency problems as applying to machine learning to robots but has its own deficiencies. Simulating the real world and is painstaking and in some cases impossible. Therefore, most real world simulations are simplifications of the true environment, which can lead to severe faults in the robot controller. HIRM strives to avoid the pitfalls of real-world interaction and simulation to learn complex behaviors in an efficient and accurate manner",HIRM: An Algorithm for Developing Introspective Models of Robots,,,,,core
401730316,2006-01-01T00:00:00,"Práce se zabývá vývojem nového algoritmu pro zpětnovazební učení (reinforcement learning), nazvaného StimulusActionReward Network (krátce SARN). Cílem je vyvinout algoritmus pro nasazení v reálném prostředí. To klade na použité techniky dvě hlavní omezení: řídící algoritmus musí pracovat se vstupy v oboru reálných čísel a učení musí probíhat za pochodu, bez předchozích trénovacích běhů. Dalším cílem je minimalizovat zásahy učitele (člověka) nutné pro úspěšné nasazení algoritmu pro daný problém. Architektura SARN kombinuje konekcionistickou síť a skalární zpětnou vazbu použitím hebbovských principů. Postupnou změnou vah v síti se tvoří vazby mezi relevantními vstupy (stimuly) a akcemi, které vedou k pozitivní zpětné vazbě. Protože použitý algoritmus je schopen rychle vybrat důležité vstupy, je možné použít vstupní prostor poměrně velké dimenze. To vede k myšlence použití náhodné rekurentní sítě pro před-zpracování vstupu. Prototyp byl testován ve virtuálním prostředí Unreal 2004. V porovnání s Q-learning vykazuje SARN v časové skále desítek sekund až jednotek minut typicky lepší výsledky. Zejména po spojení s Echo State Network vyžaduje SARN narozdíl od většiny srovnatelných algoritmů velmi málo zásahů od učitele nad rámec skalární zpětné vazby. Díky těmto vlastnostem je algoritmus použitelný například pro...In this work, a novel reinforcement learning algorithm, Stimulus Action Reward Network (SARN), is developed. It is targeted for application in real-time domainswhere the inputs are usually continuous and adaptation must proceed on-line, without separate training periods. Another objective is to minimise the amount of problem-specific teacher(human) input needed for successful application of the algorithm. The SARN architecture combines a connectionist network and scalar reinforcement feedback by employing Hebbian principles. By adapting the network weights, connections are established between stimuli and actions that lead to positive feedback. Since the links between the input stimuli and the actions are formed quite rapidly, it is possible to use a large number of stimuli. This leads to the idea of using recurrent random network (Echo State Network) as a pre-processing layer. Prototype implementation is tested in Unreal 2004 game environment. The comparison with Q-learning shows that on the time scale of tens of seconds to minutes, SARN typically achieves better performance. When coupled with an Echo State Network, SARN requires a uniquely low amount of problem-specific information supplied by the teacher. These features make SARN useful for domains such as autonomous robot control and game AI.Department of Software EngineeringKatedra softwarového inženýrstvíFaculty of Mathematics and PhysicsMatematicko-fyzikální fakult",On-line learning in real-time environments,https://core.ac.uk/download/401730316.pdf,,"Univerzita Karlova, Matematicko-fyzikální fakulta",,core
17354015,2005-07-01T00:00:00,"Indoor environments can typically be divided into places with different functionalities like kitchens, offices, or seminar rooms. We believe that such semantic information enables a mobile robot to more efficiently accomplish a variety of tasks such as human-robot interaction, path-planning, or localization. This paper presents a supervised learning approach to label different locations using boosting. We train a classifier using features extracted from vision and laser range data. Furthermore, we apply a Hidden Markov Model to increase the robustness of the final classification. Our technique has been implemented and tested on real robots as well as in simulation. The experiments demonstrate that our approach can be utilized to robustly classify places into semantic categories. We also present an example of localization using semantic labeling",Semantic place classification of indoor environments with mobile robots using boosting,,,American Association for Artificial Intelligence,,core
24561246,2007,"A central goal of robotics and AI is to be able to deploy an agent to act autonomously in the real world over an extended period of time. It is commonly asserted that in order to do so, the agent must be able to learn to deal with unexpected environmental conditions. However an ability to learn is not sufficient. For true extended autonomy, an agent must also be able to recognize when to abandon its current model in favor of learning a new one; and how to learn in its current situation. This paper presents a fully implemented example of such autonomy in the context of color map learning on a vision-based mobile robot for the purpose of image segmentation. Past research established the ability of a robot to learn a color map in a single fixed lighting condition when manually given a “curriculum, ” an action sequence designed to facilitate learning. This paper introduces algorithms that enable a robot to i) devise its own curriculum; and ii) recognize when the lighting conditions have changed sufficiently to warrant learning a new color map. ",Color learning on a mobile robot: Towards full autonomy under changing illumination,,,,,core
148440389,2007-01-01T00:00:00,"One primary goal in rescue robotics is to deploy a team of robots for coordinated victim search after a disaster. This requires robots to perform subtasks, such as victim detection, in real-time. Human detection by computationally cheap techniques, such as color thresholding, turn out to produce a large number of false-positives. Markov Random Fields (MRFs) can be utilized to combine the local evidence of multiple weak classifiers in order to improve the detection rate. However, inference in MRFs is computational expensive. In this paper we present a novel approach for the genetic optimizing of the building process of MRF models. The genetic algorithm determines offline relevant neighborhood relations with respect to the data, which are then utilized for generating efficient MRF models from video streams during runtime. Experimental results clearly show that compared to a Support Vector Machine (SVM) based classifier, the optimized MRF models significantly reduce the false-positive rate. Furthermore, the optimized models turned out to be up to five times faster then the non-optimized ones at nearly the same detection rate.Artificial Intelligence & Integrated Computer System",Genetic MRF Model Optimization for Real-Time Victim Detection in Search and Rescue,,10.1109/IROS.2007.4399006,'Institute of Electrical and Electronics Engineers (IEEE)',,core
147909160,2006-01-12T12:30:31,"Evolutionary robotics is a new technique for the automatic creation of autonomous robots. Inspired by the Darwinian principle of selective reproduction of the fittest, it views robots as autonomous artificial organisms that develop their own skills in close interaction with the environment and without human intervention. Drawing heavily on biology and ethology, it uses the tools of neural networks, genetic algorithms, dynamic systems, and biomorphic engineering. The resulting robots share with simple biological systems the characteristics of robustness, simplicity, small size, flexibility, and modularity. In evolutionary robotics, an initial population of artificial chromosomes, each encoding the control system of a robot, is randomly created and put into the environment. Each robot is then free to act (move, look around, manipulate) according to its genetically specified controller while its performance on various tasks is automatically evaluated. The fittest robots then ""reproduce"" by swapping parts of their genetic material with small random mutations. The process is repeated until the ""birth"" of a robot that satisfies the performance criteria. This book describes the basic concepts and methodologies of evolutionary robotics and the results achieved so far. An important feature is the clear presentation of a set of empirical experiments of increasing complexity. Software with a graphic interface, freely available on a Web page, will allow the reader to replicate and vary (in simulation and on real robots) most of the experiments","Evolutionary Robotics. The Biology, Intelligence, and Technology of Self-organizing Machines",,,"Cambridge, MA, MIT Press",,core
11787904,2005-01-01T00:00:00,"Developing software for Autonomous Mobile Robot (AMR) is difficult and requires knowledge in embedded systems, real-time software issues, control theories and artificial intelligence aspects. To tackle the difficulty in developing software for AMR, many researchers have proposed the approach of reusable software component for mobile robot systems. Software pattern provides a way to reuse knowledge of expert across domain at all level of software development. In this paper component-based analysis patterns applicable to AMR software at high-level software development is proposed. Some important AMR component-based analysis patterns on AMR embedded software requirements are presented. How the analysis patterns can help in documenting two existing AMR software through pattern-based reverse engineering process is also illustrated",Application of component-based analysis patterns for pattern-based reverse engineering of mobile robot software,,,,,core
24462913,2005,"Many knowledge-based systems suffer from structural problems such as inefficiency rooted in overinformedness and inability to cope with the unexpected or exceptional nature of real-world data. Behaviour-based architectures are better suited for such problems, but are not yet widely applied, possibly because design strategies are not yet well established. In robotics, subsumption architecture has proven an effective framework for developing such systems. In this paper we suggest the techniques of subsumption architecture can be transferred to other areas of Artificial Intelligence, and present a project implemented in this fashion. The development strategies used and the types of problems approachable by this method are also discussed",Edinburgh,,,,,core
334031,2005-01-01T00:00:00,"This paper describes the framework of a real-time simulation system to model human behavior and reactions in dangerous environments. The system utilizes the latest 3D computer animation techniques, combined with artificial intelligence, robotics and psychology, to model human behavior, reactions and decision making under expected/unexpected dangers in real-time in virtual environments. The development of the system includes: classification on the conscious/subconscious behaviors and reactions of different people; capturing different motion postures by the Eagle Digital System; establishing 3D character animation models; establishing 3D models for the scene; planning the scenario and the contents; and programming within Virtools (TM) Dev. Programming within Virtools (TM) Dev is subdivided into modeling dangerous events, modeling character's perceptions, modeling character's decision making, modeling character's movements, modeling character's interaction with environment and setting up the virtual cameras. The real-time simulation of human reactions in hazardous environments is invaluable in military defense, fire escape, rescue operation planning, traffic safety studies, and safety planning in chemical factories, the design of buildings, airplanes, ships and trains. Currently, human motion modeling can be realized through established technology, whereas to integrate perception and intelligence into virtual human's motion is still a huge undertaking. The challenges here are the synchronization of motion and intelligence, the accurate modeling of human's vision, smell, touch and hearing, the diversity and effects of emotion and personality in decision making. There are three types of software platforms which could be employed to realize the motion and intelligence within one system, and their advantages and disadvantages are discussed",Modelling human behaviours and reactions under dangerous environment,https://core.ac.uk/download/334031.pdf,,International Society of Automation,"[{'title': None, 'identifiers': ['0067-8856', 'issn:0067-8856']}]",core
86953,2006,"In this paper we present the LIDA architecture as a working model of cognition. We argue that such working models are broad in scope and address real world problems in comparison to experimentally based models which focus on specific pieces of cognition. While experimentally based models are useful, we need a working model of cognition that integrates what we know from neuroscience, cognitive science and AI. The LIDA architecture provides such a working model. A LIDA based cognitive robot or software agent will be capable of multiple learning mechanisms. With artificial feelings and emotions as primary motivators and learning facilitators, such systems will ‘live’ through a developmental period during which they will learn in multiple ways to act in an effective, human-like manner in complex, dynamic, and unpredictable environments. We discuss the integration of the learning mechanisms into the existing IDA architecture as a working model of cognition",LIDA: A Working Model of Cognition,https://core.ac.uk/download/pdf/86953.pdf,,,,core
297575572,,"Developing software for Autonomous Mobile Robot (AMR) is difficult and requires knowledge in embedded systems, real-time software issues, control theories and artificial intelligence aspects. To tackle the difficulty in developing software for AMR, many researchers have proposed the approach of reusable software component for mobile robot systems. Software pattern provides a way to reuse knowledge of expert across domain at all level of software development. In this paper component-based analysis patterns applicable to AMR software at high-level software development is proposed. Some important AMR component-based analysis patterns on AMR embedded software requirements are presented. How the analysis patterns can help in documenting two existing AMR software through pattern-based reverse engineering process is also illustrated",Application of component-based analysis patterns for pattern-based reverse engineering of mobile robot software,,,"Malaysian Journal of Computer Science , University of Malaya",,core
24336796,22/11/2007,"MINNI is a system whereby a back propagation neural network is used to control the steering of a micromouse (small robot) in following a straight path. The neural network must be minimised to run on the hardware/software platform available on the Macquarie Micromouse. The development of the network follows intensive trials of algorithm and architecture variations in MATLAB. The implementation is programmed in ADA.  Keywords Neural networks, robotics.  1 Introduction  The IEEE International Micromouse Competition runs events pitting robots from many universities at state, national, and international levels to promote the application of real time control in university computing and engineering departments. The Rodentronics team from Macquarie has known some success in the past. The micromouse itself has undergone many changes over the years. Starting with the simple kit used as the basis of most competition entrants, controlled by C code, the group has designed and developed a more sophi..",MINNI: Micromouse Incorporating Neural Network Intelligence,,,,,core
24363194,22/11/2007,"Sutton&apos;s Dyna algorithm for integrated learning, planning and reacting is applied to a real mobile platform (Robosoft&apos;s Robuter). The mobile robot uses sonar to scan for obstacles and odometry for selflocalization. Practical problems associated with the implementation of the algorithm on a real setup and results from real experiments are presented and discussed.   Keywords: Reinforcement Learning, Planning, Reacting, Mobile Robots, Dyna algorithm.  1 Introduction  Many researchers have applied learning to Robotics. A robot can learn from the data provided by its external sensors (e.g. cameras, ultrasound transducers, proximity detectors) or internal alarms (e.g. battery failure, timeout while running a process). Reinforcement  learning, where limited information is available about the algorithm instantaneous performance, typically in the form of success or failure signals, is particularly interesting for Robotics as it involves the exchange of small bandwidth information (failure and s..","An Integrated Learning, Planning and Reacting Algorithm Applied to a Real Mobile Robot",,,,,core
89153085,2006-12-01T00:00:00Z,"In this work, several localization algorithms that are designed and implemented for Cerberus'05 Robot Soccer Team are analyzed and compared. These algorithms are used for global localization of autonomous mobile agents in the robotic soccer domain, to overcome the uncertainty in the sensors, environment and the motion model. The algorithms are Reverse Monte Carlo Localization (R-MCL), Simple Localization (S-Loc) and Sensor Resetting Localization (SRL). R-MCL is a hybrid method based on both Markov Localization (ML) and Monte Carlo Localization (MCL) where the ML module finds the region where the robot should be and MCL predicts the geometrical location with high precision by selecting samples in this region. S-Loc is another localization method where just one sample per percept is drawn, for global localization. Within this method another novel method My Environment (ME) is designed to hold the history and overcome the lack of information due to the drastically decrease in the number of samples in S-Loc. ME together with S-Loc is used in the Technical Challenges in Robocup 2005 and play an important role in ranking the First Place in the Challenges. In this work, these methods together with SRL, which is a widely used successful localization algorithm, are tested with both offline and real-time tests. First they are tested on a challenging data set that is used by many researches and compared in terms of error rate against different levels of noise, and sparsity. Besides time required recovering from kidnapping and speed of the methods are tested and compared. Then their performances are tested with real-time tests with scenarios like the ones in the Technical Challenges in ROBOCUP. The main aim is to find the best method which is very robust and fast and requires less computational power and memory compared to similar approaches and is accurate enough for high level decision making which is vital for robot soccer",Comparison of Localization Methods for a Robot Soccer Team,,10.5772/5727,SAGE Publishing,"[{'title': None, 'identifiers': ['1729-8814', 'issn:1729-8814']}]",core
102315651,2007,"This thesis concerns the development of an autonomous mobile robot intended for use in a human environment. From a primary focus on HRI, a novel robotic behaviour al-gorithm has been developed and validated through simulation as well as real world ex-periments. Furthermore, the behaviour of the robot is continuously adjusted to that of encountered people, by incorporating CBR based artificial intelligence, implemented by use of aMySQL database. Besides interacting with people, the robot is capable of navigating and localizing itself in a given environment while avoiding un-known obstacles. All robot software, including the implemen-tation of the above algorithm, has been de-veloped for use with the Player robot soft-ware framework, and implemented on a FESTO Robotino®. Consequently, due to the Player framework offering a wide range of robot features, the developed system can form the basis of future robotic research. Experiments performed on the robot show promising results, in relation to both the behavioural capabilities and the developed system in general",PROJECT GROUP: 1030 GROUPMEMBERS:,,,,,core
36204612,2007-01-01T00:00:00,"One primary goal in rescue robotics is to deploy a team of robots for coordinated victim search after a disaster. This requires robots to perform subtasks, such as victim detection, in real-time. Human detection by computationally cheap techniques, such as color thresholding, turn out to produce a large number of false-positives. Markov Random Fields (MRFs) can be utilized to combine the local evidence of multiple weak classifiers in order to improve the detection rate. However, inference in MRFs is computational expensive. In this paper we present a novel approach for the genetic optimizing of the building process of MRF models. The genetic algorithm determines offline relevant neighborhood relations with respect to the data, which are then utilized for generating efficient MRF models from video streams during runtime. Experimental results clearly show that compared to a Support Vector Machine (SVM) based classifier, the optimized MRF models significantly reduce the false-positive rate. Furthermore, the optimized models turned out to be up to five times faster then the non-optimized ones at nearly the same detection rate.Artificial Intelligence & Integrated Computer System",Genetic MRF Model Optimization for Real-Time Victim Detection in Search and Rescue,,10.1109/IROS.2007.4399006,'Institute of Electrical and Electronics Engineers (IEEE)',,core
61389021,2008-01-01T00:00:00,"One of the areas that needs further improvement
within E-Learning environments via Internet (A big effort is
required in this area if progress is to be made) is allowing students
to access and practice real experiments in a real laboratory,
instead of using simulations [1]. Real laboratories allow students
to acquire methods, skills and experience related to real
equipment, in a manner that is very close to the way they are
being used in industry. The purpose of the project is the study,
development and implementation of an E-Learning environment
to allow undergraduate students to practice subjects related to
Robotics and Artificial Intelligence. The system, which is now at a
preliminary stage, will allow the remote experimentation with real
robotic devices (i.e. robots, cameras, etc.). It will enable the
student to learn in a collaborative manner (remote participation
with other students) where it will be possible to combine the onsite
activities (performed “in-situ” within the real lab during the
normal practical sessions), with the “on-line” one (performed
remotely from home via the Internet). Moreover, the remote
experiments within the E-Laboratory to control the real robots
can be performed by both, students and even scientist. This
project is under development and it is carried out jointly by two
Universities (UPC and UJI). In this article we present the system
architecture and the way students and researchers have been able
to perform a Remote Programming of Multirobot Systems via web",Remote Programming of Multirobot Systems within the UPC-UJI Telelaboratories: System Architecture and Agent-Based Multirobot Control,https://core.ac.uk/download/61389021.pdf,,Westing Publishing Co.,,core
20726519,03/04/2008,"Abstract. The question of implementing emotions in robots is twofold: on the on hand it should be verified whether such an effort is valuable, and on the other it should be determined whether the implementation is feasible. The answer to the first question seems easy: besides and beyond the reasons of pure intellectual curiosity and scientific research, emotions should be studied and implemented if the overall behavior of such robots is better than their unemotional counterparts with respect to behaving efficiently in a real world environment. Two diverse opinions have emerged in the previous discussion. One, due to McCarthy, asserts that emotions will introduce obstacles in the communication among robots and human beings [6]. On the other hand, Minsky sustains the opinion that it is impossible to implement intelligence without emotions [7]. In this paper we analyze these perspectives, discuss a possible way to approach the topic, and provide an architecture to implement emotions, which has shown some very interesting characteristics. We sustain that the research on emotions — from the Artificial Intelligence point of view — is valuable and worth pursuing. 1 The need for emotions You [humans] are, after all, essentially irrational",Artificial Emotions,,,,,core
24604826,07/02/2008,"Abstract — In real-world robotic applications, many factors, both at low-level (e.g., vision and motion control parameters) and at high-level (e.g., the behaviors) determine the quality of the robot performance. Thus, for many tasks, robots require fine tuning of the parameters, in the implementation of behaviors and basic control actions, as well as in strategic decisional processes. In recent years, machine learning techniques have been used to find optimal parameter sets for different behaviors. However, a drawback of learning techniques is time consumption: in practical applications, methods designed for physical robots must be effective with small amounts of data. In this paper, we present a method for concurrent learning of best strategy and optimal parameters, by extending the policy gradient reinforcement learning algorithm. The results of our experimental work in a simulated environment and on a real robot show a very high convergence rate. I",An extended policy gradient algorithm for robot task learning,,,,,core
216383565,2005-08-17T07:00:00,"In the Robotics industry, it is a frequent requirement that robots operate in real-time. The usual approach to this issue involves creating robots driven entirely by direct environmental input rather than complicated planning and decision-making AI. This approach means that the current state of the robot in relation to its environment exclusively determines the actions of the robot. In the simplest terms, this approach creates a Finite State Machine (FSM). Clearly, a standard FSM is completely pre-deterministic upon its creation. This is a drawback which immediately disallows the robot to cope with dynamic environments in an autonomous manner. This research suggests a solution to this problem, while still maintaining real-time performance of the FSM structure, through the development of a Self-Adjusting FSM (SA-FSM). A SA-FSM is a FSM with an additional module which adds, removes, and adjusts specific states of its FSM structure. By adjusting its FSM the SA-FSM will have the basis for autonomous attributes. It will be capable of coping with drastic changes in its environment by making necessary fundamental adjustments to its behavior. Through this mechanism, the process of learning can be implemented. In this regard, only the inherent learning/inference algorithms the SA-FSM employs to adjust its FSM determine the complexity of the behavior produced by a SA-FSM based robot",Self-Adjusting Finite State Machines: an approach to Real-Time Autonomous Behavior in Robots,https://core.ac.uk/download/216383565.pdf,,Digital Commons @ Trinity,,core
20641120,2007-05-03T12:37:11,"Over the last decades, calibration techniques have been widely used in robotics since they represent a cost-effective solution for improving the accuracy of robots and machine-tools. They only involve software modification without the necessity of revising the robot design or tightening the manufacturing tolerances.  The goal of this thesis is to propose a procedure that guides the engineer through the calibration of a given multi-DOF flexure parallel robot within sub-µm accuracy. Two robots having 3 and 6 degrees of freedom have been considered as a case-study throughout the work. As in any calibration procedure, the work has been conducted on three different fronts: measurement, data processing and validation. The originality of this thesis in respect to published material lies in these three points.  Measurements were carried out in a chamber inside which the measuring environment was protected against mechanical and thermal perturbations. In particular, the temperature variations experienced by the different parts of the measuring loop during a typical measurement session were stabilized within less than ± 0.1 °C.  Proposed procedures allow the collection of reliable sets of data on the two robots. Delicate aspects of practical implementation are discussed. In particular, the problem of collecting a complete set of 6D data within accuracies in the nanometre range, for which there is still a lack of standard equipment, is solved using a procedure comprising several steps and making use of existing instrumentation.  Suggestions for future investigations are given, regarding either long-term research problems or short-term industrial implementation issues.  Data processing was performed using two different techniques in order to reach absolute accuracies after calibration better than ± 100 nm for translations and ± 3 arcsec for rotations (± 0.3 arcsec inside a more restricted range of ± 0.11°).  The first method is called the ""model-based approach"" and requires the use of a known analytical relationship between the motor and operational coordinates of the robot. This relationship involves a certain number of parameters that can be related to the geometry of the robot (physical models) or simply mathematical coefficients of an approximating mathematical function (behavioural models). In the case of high-precision multi-DOF flexure parallel robots, we show that polynomial-based behavioural models are preferable to physical models in terms of accuracy for data processing tasks.  In the second method, called the ""model-free approach"", the user does not need to model explicitly the main error sources (or their effect) affecting the robot accuracy. A model-free approach has been implemented using Artificial Neural Networks. We show that, using a heuristic search based on a decision-tree, the architecture of a network with satisfactory prediction capability can be found systematically. In particular, this algorithm can find a network able to predict the direct correspondence between the motor and operational coordinates (within the desired accuracy) without the help of the Inverse Geometric Model of the robot, i.e. even if the nominal geometry of the robot being calibrated remains unknown. This result contradicts conclusions reported by previous researchers.  It is claimed that any robot (not necessarily a high-precision flexure parallel mechanism) can be calibrated by means of a ""neural approach"" in which the architecture of an appropriate network is determined with the help of our algorithm. Two examples (other than the robots measured in this thesis) are given to illustrate this universality.  In the last part of this work, we provide a feasibility study on the use of indentation, a technique traditionally used for material testing, as a validation procedure to assess the accuracy of the calibrated degrees of freedom.  The industrial interest of this technique lies in the fact that the robot is asked to execute similar motions to those involved in a real micro-machining operation.Dans les dernières décennies, les techniques d'étalonnage ont connu un succès fulgurant en robotique. Elles sont en effet une solution attractive pour améliorer la précision d'un manipulateur industriel donné puisqu'elles ne demandent qu'une modification au niveau du logiciel. Le but de cette thèse est de proposer une procédure visant à guider l'ingénieur dans l'étalonnage des robots parallèles de précisions sub-µm à articulations flexibles et ayant plusieurs degrés de liberté. Deux robots à 3 et 6 degrés de liberté sont pris comme cas d'étude. Comme dans n'importe quelle procédure d'étalonnage, le travail a été conduit sur plusieurs fronts: procédés de mesure, traitement de données et validation. L'originalité de cette thèse réside dans ces trois aspects. Les mesures ont eu lieu à l'intérieur d'une chambre visant à garantir une bonne isolation mécanique et thermique de l'environnement métrologique. En particulier, les variations de température subies par les différentes parties de la boucle de mesure pendant la durée d'une mesure typique, ont été stabilisées avec une tolérance meilleure que ± 0.1 °C. Des procédures ont été proposées permettant l'acquisition de données fiables sur les deux robots. Les aspects délicats liés à l'implémentation pratique de ces procédures sont discutés. En particulier, le problème de l'acquisition de mesures 6D avec des précisions nanométriques (pour lequel il n'y a toujours pas d'équipement standard) est résolu par l'intermédiaire d'une procédure en plusieurs étapes et faisant intervenir des instruments existants. Des suggestions sont formulées pour la suite des travaux soit sur le long terme, au niveau de la recherche fondamentale, soit dans une perspective d'implémentation industrielle à court terme. Deux techniques ont été proposées pour le traitement des données permettant d'atteindre des précisions absolues après étalonnage meilleures que ± 100 nm pour les translations et ± 3 arcsec pour les rotations (± 0.3 arcsec à l'intérieur d'une plage plus restreinte de ± 0.11°). La première méthode requiert un modèle analytique décrivant la correspondance entre les coordonnées articulaires et les coordonnées opérationnelles du robot. Ce modèle fait intervenir un certain nombre de paramètres pouvant être liés à la géométrie du robot (modèles à représentation physique) ou simplement des coefficients d'une fonction mathématique (modèles de comportement). Dans le cas des robots parallèles à articulations flexibles, il a été démontré que des modèles de comportement basés sur des fonctions polynomiales priment sur des modèles à représentation physique en termes de précision dans le traitement de données. Dans une deuxième approche, l'utilisateur n'a pas besoin de modéliser explicitement les différentes sources d'erreur (ou leur effet). Une approche de ce type a été implémentée en utilisant des réseaux de neurones artificiels. On démontre qu'une heuristique de type ""arbre de décision"" permet de déterminer systématiquement l'architecture d'un réseau pouvant fournir une précision satisfaisante au problème. En particulier, il est possible d'obtenir un réseau pouvant prédire la correspondance directe entre coordonnées opérationnelles et articulaires avec la précision escomptée sans l'aide du modèle géométrique inverse du robot, c'est-à-dire même en ne connaissant pas a priori la géométrie nominale du robot à étalonner. Ce dernier résultat contredit les résultats obtenus par d'autres chercheurs. Il est dit que tout robot (pas nécessairement un robot parallèle de haute précision à articulations flexibles) peut être étalonné par le biais d'une ""approche neuronale"" dans laquelle l'architecture d'un réseau approprié est déterminée par l'algorithme proposé. Cette universalité est illustrée sur 2 exemples autres que les robots mesurés dans ce travail. Dans la dernière partie de cette thèse, on fait une étude de faisabilité sur l'utilisation de l'indentation, une technique employée traditionnellement dans la caractérisation des propriétés mécaniques des matériaux, en tant que procédé de validation pour contrôler la qualité de l'étalonnage effectué. L'intérêt industriel de cette technique réside dans le fait que le robot y exécute des mouvements semblables à ceux qu'il serait amené à faire lors d'un procédé typique de micro-usinage",Calibration of high-precision flexure parallel robots,,,,,core
54263737,2007-01-01T00:00:00,"Cellular neural networks (CNNs) are well suited for image processing due to the possibility of a parallel computation. In this paper, we present two algorithms for tracking and obstacle avoidance using CNNs. Furthermore, we show the implementation of an autonomous robot guided using only real-time visual feedback; the image processing is performed entirely by a CNN system embedded in a digital signal processor (DSP). We successfully tested the two algorithms on this robot. Copyright (c) 2006 John Wiley & Sons, Ltd",Robot vision with cellular neural networks: A practical implementation of new algorithms,,10.1002/cta.395,'Wiley',,core
24681418,21/11/2007,"To date there has only been one implementation of Holland&apos;s Learning Classifier System (LCS) on real robots. In  this paper the use of Wilson&apos;s ZCS system is described for an obstacle avoidance task. Although the task is simple it  does present some advances and change of emphasis over the previous LCS robotic implementation. The controller  model is &quot;event&quot; based. Instead of the robot being assigned fixed length actions, continuous actions are taken. These  actions are taken until an &quot;event&quot; occurs. An event can be thought of as a change of state. This division of the world  into states is usually part of the problem description, and to do this automatically is currently one of the challenges  facing machine learning. The paper then introduces TCS, a form of ZCS that attempts to address this issue. LCS  have the ability to generalise over the state action-space. In TCS this generalisation ability can also be used to  determine the extent of this space. TCS also implements components from SMDP reinforcement learning theory to  weight the influence of time on the reward functions of the LCS. A simple light-seeking task on the robot platform  using TCS is presented which demonstrates desirable adaptive characteristics for the use of LCS on real robots",ZCS and TCS Learning Classifier System,,,,,core
22933394,2005,"Indoor environments can typically be divided into places with different functionalities like kitchens, offices, or seminar rooms. We believe that such semantic information enables a mobile robot to more efficiently accomplish a variety of tasks such as human-robot interaction, path-planning, or localization. This paper presents a supervised learning approach to label different locations using boosting. We train a classifier using features extracted from vision and laser range data. Furthermore, we apply a Hidden Markov Model to increase the robustness of the final classification. Our technique has been implemented and tested on real robots as well as in simulation. The experiments demonstrate that our approach can be utilized to robustly classify places into semantic categories. We also present an example of localization using semantic labeling",Place classification of indoor environments with mobile robots using boosting,,,,,core
22498795,21/11/2007,"This paper presents an approach to learning  how to control a dynamical system that requires  several real-valued control actions. The  reinforcement theory has been implemented  and applied to the simulation of an autonomous  flying robot. Experiments have shown good  learning results.  1 Introduction  Traditionally control problems have been solved for dynamical systems with discrete control actions, and several solutions to the well-known cart-pole system have been presented in the past, notably the Boxes algorithm [ 5 ] and the ASE-ACE approach [ 2 ] . A number of new methods of reinforcement learning have been presented since [ 7 ] , the most popular of which is Q-learning [ 6 ] . In the cart-pole system however the learned control actions are simple: either the cart is pushed to the left, or it is pushed to the right; the cart-pole algorithms provide no facility for controlling real-valued actions. Gullappalli [ 4 ] presented a stochastical reinforcement learning algorithm that h..",Reinforcement Learning of Multiple Real-Valued Control Actions in a Dynamical System,,,,,core
24449396,06/02/2008,"Abstract—This paper presents the design and implementation of a coordination architecture for quadruped walking robots to learn and execute soccer-playing behaviors. A typical hybrid architecture combing reactive behaviors with deliberative reasoning is developed. The reactive behaviors directly map spatial information extracted from sensors into actions. The deliberative reasoning represents temporal constraints of a robot’s strategy in terms of finite state machines. In order to achieve real-time and robust control performance in reactive behaviors, fuzzy logic controllers (FLCs) are used to encode the behaviors, and a two-stage learning scheme is adopted to make these FLCs adaptive to complex situations. The experimental results are provided to show the suitability of the architecture and effectiveness of the proposed learning scheme. Index Terms—Behavior-based control, fuzzy logic controller (FLC), reinforcement learning (RL). I",Integration of Coordination Architecture and Behavior Fuzzy Learning in Quadruped Walking Robots,,,,,core
24530409,2005,"Abstract — Designing robots that are capable of interacting with humans in real life settings is a challenging task. One key issue is the integration of multiple modalities (e.g., mobility, physical structure, navigation, vision, audition, dialogue, reasoning) into a coherent framework. Taking the AAAI Mobile Robot Challenge (making a robot attend the National conference on Artificial Intelligence) as the experimental context, we are currently addressing hardware, software and computation integration issues involved in designing a robot capable of sophisticated interaction with humans. This paper reports on our design solutions and the current status of the work, along with the potential impacts this design will have on human-robot interaction research. Index Terms — Socially interactive mobile robot, Embodied interaction and communication, Multi-modal communication",Modularity and integration in the design of a socially interactive robot,,,,,core
24709674,01/04/2008,"This paper presents a heterogeneous, asynchronous architecture for controlling autonomous mobile robots which is capable of controlling a robot performing multiple tasks in real time in noisy, unpredictable environments. The architecture produces behavior which is reliable, task-directed (and taskable), and reactive to contingencies. Experiments on real and simulated realworld robots are described. The architecture smoothly integrates planning and reacting by performing these two functions asynchronously using heterogeneous architectural elements, and using the results of planning to guide the robot&apos;s actions but not to control them directly. The architecture can thus be viewed as a concrete implementation of Agre and Chapman&apos;s plans-ascommunications theory. The central result of this work is to show that completely unmodified classical AI programming methodologies using centralized world models can be usefully incorporated into real-world embedded reactive systems. 1",Integrating Planning and Reacting in a Heterogeneous Asynchronous Architecture for Controlling Real-World Mobile Robots,,,,,core
24724033,02/04/2008,"Abstract. The development of speech tools suitable for use in real world environments requires collaboration between computational linguistics and new implementation fields e.g. robotics, and the incorporation of new AI techniques to improve overall system performance. In this paper we present the core development concepts of SAID (Speaking Autonomous Intelligent Devices). The work presented centres around four key strands of research, namely the recasting of the Time Map model as a Multi-Agent System (MAS), the development of a MAS based audio feature extraction system, the deployment of a BDI based dialog agent and the design of a MAS based social robot architecture",Speaking autonomous intelligent devices,,,,,core
24638320,01/04/2008,"Abstract: In this work, several localization algorithms that are designed and implemented for Cerberus&apos;05 Robot Soccer Team are analyzed and compared. These algorithms are used for global localization of autonomous mobile agents in the robotic soccer domain, to overcome the uncertainty in the sensors, environment and the motion model. The algorithms are Reverse Monte Carlo Localization (R-MCL), Simple Localization (S-Loc) and Sensor Resetting Localization (SRL). R-MCL is a hybrid method based on both Markov Localization (ML) and Monte Carlo Localization (MCL) where the ML module finds the region where the robot should be and MCL predicts the geometrical location with high precision by selecting samples in this region. S-Loc is another localization method where just one sample per percept is drawn, for global localization. Within this method another novel method My Environment (ME) is designed to hold the history and overcome the lack of information due to the drastically decrease in the number of samples in S-Loc. ME together with S-Loc is used in the Technical Challenges in Robocup 2005 and play an important role in ranking the First Place in the Challenges. In this work, these methods together with SRL, which is a widely used successful localization algorithm, are tested with both offline and real-time tests. First they are tested on a challenging data set that is used by many researches and compared in terms of error rate against different levels of noise, and sparsity. Besides time required recovering from kidnapping and speed of the methods are tested and compared. Then their performances are tested with real-time tests with scenarios like the ones in the Technical Challenges in ROBOCUP. The main aim is to find the best method which is very robust and fast and requires less computational power and memory compared to similar approaches and is accurate enough for high level decision making which is vital for robot soccer",Comparison of Localization Methods for a Robot Soccer Team,,,,,core
24668770,01/04/2008,"the Czech robota (forced labor). Limited to work too tedious or dangerous for humans, today s robots weld parts on assembly lines, inspect nuclear plants, and explore other planets. Generally, robots are still far from achieving their fictional counterparts intelligence and flexibility. Humanoid robotics labs worldwide are working on creating robots that are one step closer to science fiction s androids. Building a humanlike robot is a formidable engineering task requiring a combination of mechanical, electrical, and software engineering; computer architecture; and realtime control. In 1993, we began a project aimed at constructing a humanoid robot for use in exploring theories of human intelligence. 1,2 In addition to the relevant engineering, computer architecture, and real-time-control issues, we ve had to address issues particular to integrated systems: What types of sensors should we use, and how should the robot interpret the data? How can the robot act deliberately to achieve a task and remain responsive to the environment? How can the system adapt to changing conditions and learn new tasks? Each humanoid robotics lab must address many of the same motor-control, perception, and machine-learning problems. The principles behind our methodology The real divergence between groups stems from radically different research agendas an","ASIDE FROM THEIR TRADITIONAL ROLES, HUMANOID ROBOTS CAN BE USED TO EXPLORE THEORIES OF HUMAN INTELLIGENCE. THE AUTHORS DISCUSS THEIR PROJECT AIMED AT DEVELOPING ROBOTS THAT CAN BEHAVE LIKE AND INTERACT WITH HUMANS.",,,,,core
101021544,2007,"Since the computational complexity of the learning task depends on the size of the state space, it is necessary to simplify either the environment, the task, or both. Abstract – This paper describes two experiments with supervised reinforcement learning (RL) on a real, mobile robot. Two types of experiments were preformed. One tests the robot’s reliability in implementing a navigation task it has been taught by a supervisor. The other, in which new obstacles are placed along the previously learned path to the goal, measures the robot’s robustness to changes in environment. Supervision consisted of human-guided, remote-controlled runs through a navigation task during the initial stages of reinforcement learning. The RL algorithms deployed enabled the robot to learn a path to a goal yet retain the ability to explore different solutions when confronted with a new obstacle. Experimental analysis was based on measurements of average time to reach the goal, the number of failed states encountered during an episode, and how closely the RL learner matched the supervisor’s actions. Index Terms – Reinforcement learning, mobile robots, Q",Reinforcement learning with a supervisor for a mobile robot in a real-world environment,,10.1109/cira.2007.382878,,,core
401842708,2008-01-01T00:00:00,"This paper deals with design and implementation of an evolutionary system for control of an autonomous mobile robot. This system should make possible an adaptation to a group of tasks, that can be similarly defined for a living being. We use results of real experiments with laboratory rats and tasks, which these rats are able to learn. The robot control system is a combination of several methods of mobile robotics and artificial intelligence. The adaptable part of the control system is based on genetic algorithms and neural networks. This work covers a wide range of problems related with this subject - various elements of the control system, the robot control and implementation, and also components of the test environment, which can be used to execute evolutionary experiments with real robots. The final part of this work includes results of our practical experiments with the robot and their comparison with real testing of rats on similar tasks.Práce se zabývá návrhem a implementací evolučního systému pro řízení autonomního mobilního robota. Tento systém by mu měl umožňovat adaptaci na třídu úloh, kterou lze srovnatelně definovat i pro skutečného živočicha, který je rovněž schopen se úlohy z této třídy naučit. Pro tyto účely jsou využity výsledky reálných pokusů s učením laboratorních potkanů. Systém pro řízení robota je kombinací metod mobilní robotiky a umělé inteligence. Adaptabilní část jeho řízení je založena na principu genetických algoritmů a neuronových sítí. Kromě různých aspektů jednotlivých prvků systému, vlastního řízení robota a jeho realizace, jsou zde rozebrány i další nutné komponenty testovacího prostředí, ve kterém lze provádět evoluční experimenty s reálnými roboty. Součástí práce jsou i výsledky praktických experimentů s vytvořeným systémem a jejich porovnání s výsledky reálných testů s potkany na podobných úlohách.Department of Theoretical Computer Science and Mathematical LogicKatedra teoretické informatiky a matematické logikyFaculty of Mathematics and PhysicsMatematicko-fyzikální fakult",A comparison between natural and artificial learning,,,"Univerzita Karlova, Matematicko-fyzikální fakulta",,core
146912032,2007,"Robot soccer pits teams of fast-moving robots in a dynamic environment (Sng et al., 2002).\ud
Robot soccer fosters AI and intelligent robotics research by providing a standard problem\ud
where a wide range of technologies can be integrated and examined (Asada & Kitano, 1999).\ud
Today two international robot soccer federations, RoboCup (RoboCup, 2007) and FIRA\ud
(FIRA, 2007), organize competitions in an eclectic range of categories. Those competitions are accompanied with technical conferences. The first international robot soccer tournament MiroSot'96 was held at Korea Advanced Institute of Science and Technology (KAIST), in November, 1996. At the time of writing, we can count more than ten different robot soccer leagues from RoboCup and FIRA.\ud
Taxonomy of the robot soccer leagues could start with the vision system used. The global\ud
vision group contains all the leagues that allow a global vision system (camera that gives an eye-bird view of the playing field). The image processing is done on a PC that controls the robots via a radio link. Whereas the local vision group contains all the leagues that require the vision processing to be done on the robots themselves. In this second group, the robots achieve a higher level of autonomy. Only wheeled robots are used in the global vision group. Whereas, the local vision group can be subdivided into wheeled robots and legged robots. Finally there are simulation leagues that provide a test bed for multi-agent research for those who do not have access to real robots.\ud
Robot soccer not only stimulates robotic research, but also provides a platform for\ud
computational intelligence education that allows the development of engaging\ud
undergraduate level assignments. However, there are several limiting factors for the\ud
widespread use of robot soccer as a research platform or a teaching tool. Most robot soccer leagues like the popular RoboCup Small Size and FIRA Mirosot leagues require a large playing field and a team of several postgraduate students to build the hardware and\ud
develop the complex software. The least resource-demanding robot soccer league is the\ud
simulation league. Unfortunately, by its very nature, this league does not provide the\ud
invaluable experience of real robots. With the constraint of using real robots, the least\ud
resource-demanding robot soccer league is arguably the FIRA KheperaSot league. This\ud
league represents Desktop Robot Soccer, in the sense that the playing field fits on a desktop or a computer laboratory bench.\ud
The regulations of the KheperaSot league impose size restrictions on the robots. The size\ud
limitation lowers the entry barrier for participants in relation to other robot soccer\ud
tournaments, making it more accessible to individuals and small teams with modest\ud
funding and infrastructure support. The size limitation also poses challenge for hardware\ud
technology. It pushes the limits of how much processing and sensing can be put into the\ud
small package at a reasonable cost. However the size is not as small as to requiring\ud
miniaturisation technology beyond the reach of standard electronics and construction\ud
techniques. The KheperaSot league was the first fully autonomous robot soccer league of\ud
FIRA",Desktop Robot Soccer,,10.5772/5131,I-Tech Education and Publishing,,core
24583648,07/02/2008,"central goal of robotics and AI is to be able to deploy an agent to act autonomously in the real world over an extended period of time. To operate in the real world, autonomous robots rely on sensory information. Despite the potential richness of visual information from on-board cameras, many mobile robots continue to rely on non-visual sensors such as tactile sensors, sonar, and laser. This preference for relatively lowfidelity sensors can be attributed to, among other things, the characteristic requirement of real-time operation under limited computational resources. Illumination changes pose another big challenge. For true extended autonomy, an agent must be able to recognize for itself when to abandon its current model in favor of learning a new one; and how to learn in its current situation. We describe a self-contained vision system that works on-board a vision-based autonomous robot under varying illumination conditions. First, we present a baseline system capable of color segmentation and object recognition within the computational and memory constraints of the robot. This relies on manually labeled data and operates under constant and reasonably uniform illumination conditions. We then relax these limitations by introducing algorithms for i) Autonomous planned color learning, where the robot uses the knowledge of its environment (position, size and shape of objects) to automatically generate a suitable motion sequence and learn the desired colors, and ii) Illumination change detection and adaptation, where the robot recognizes for itself when the illumination conditions have changed sufficiently to warrant revising its knowledge of colors. Our algorithms are fully implemented and tested on the Sony ERS-7 Aibo robots",Structure-Based Color Learning on a Mobile Robot under Changing Illumination,,,,,core
23796062,2005,"This paper introduces an integration of reinforcement learning and behavior-based control designed to produce real-time learning in situated agents. The model layers a distributed and asynchronous reinforcement learning algorithm over a learned topological map and standard behavioral substrate to create a reinforcement learning complex. The topological map creates a small and task-relevant state space that aims to make learning feasible, while the distributed and asynchronous nature of the model make it compatible with behavior-based design principles. We present the design, implementation and results of an experiment that requires a mobile robot to perform puck foraging in three artificial arenas using the new model, a random decision making model, and a layered standard reinforcement learning model. The results show that our model is able to learn rapidly on a real robot in a real environment, learning and adapting to change more quickly than both alternative models. We show that the robot is able to make the best choices it can given its drives and experiences using only local decisions and therefore displays planning behavior without the use of classical planning techniques. ","An architecture for BehaviorBased reinforcement learning,” Adaptive Behavior",,,,,core
24745888,02/04/2008,"Since the beginning of space exploration, the space community had the common belief that in the near future, the Automation &amp; Robotics will be an important element for such missions. Today, this has become a reality. The implementation of this technology will play an important role in future missions, as robotic systems and “Artificial Intelligence ” keeps developing further. The development of robotics arms as Canadarm and the ERA are citated as examples of contribution this kind of technology has made and how it will influence in the future. The main discussion in this paper is how this technology has been implemented in space exploration and how it will continuously keep contributing in such missions. A brief state of the art of how this technology has contributed to the space missions and a future approach for the coming years is proposed. ________________________________________________________________________________________________ 1",DEVELOPMENT OF AUTOMATION &amp; ROBOTICS IN SPACE EXPLORATION,,,,,core
290044901,2008,"One of the areas that needs further improvement
within E-Learning environments via Internet (A big effort is
required in this area if progress is to be made) is allowing students
to access and practice real experiments in a real laboratory,
instead of using simulations [1]. Real laboratories allow students
to acquire methods, skills and experience related to real
equipment, in a manner that is very close to the way they are
being used in industry. The purpose of the project is the study,
development and implementation of an E-Learning environment
to allow undergraduate students to practice subjects related to
Robotics and Artificial Intelligence. The system, which is now at a
preliminary stage, will allow the remote experimentation with real
robotic devices (i.e. robots, cameras, etc.). It will enable the
student to learn in a collaborative manner (remote participation
with other students) where it will be possible to combine the onsite
activities (performed “in-situ” within the real lab during the
normal practical sessions), with the “on-line” one (performed
remotely from home via the Internet). Moreover, the remote
experiments within the E-Laboratory to control the real robots
can be performed by both, students and even scientist. This
project is under development and it is carried out jointly by two
Universities (UPC and UJI). In this article we present the system
architecture and the way students and researchers have been able
to perform a Remote Programming of Multirobot Systems via web",Remote Programming of Multirobot Systems within the UPC-UJI Telelaboratories: System Architecture and Agent-Based Multirobot Control,,,Westing Publishing Co.,,core
204663590,30/04/2008,"AbstractWhile Reinforcement Learning (RL) is not traditionally designed for interactive supervisory input from a human teacher, several works in both robot and software agents have adapted it for human input by letting a human trainer control the reward signal. In this work, we experimentally examine the assumption underlying these works, namely that the human-given reward is compatible with the traditional RL reward signal. We describe an experimental platform with a simulated RL robot and present an analysis of real-time human teaching behavior found in a study in which untrained subjects taught the robot to perform a new task. We report three main observations on how people administer feedback when teaching a Reinforcement Learning agent: (a) they use the reward channel not only for feedback, but also for future-directed guidance; (b) they have a positive bias to their feedback, possibly using the signal as a motivational channel; and (c) they change their behavior as they develop a mental model of the robotic learner. Given this, we made specific modifications to the simulated RL robot, and analyzed and evaluated its learning behavior in four follow-up experiments with human trainers. We report significant improvements on several learning measures. This work demonstrates the importance of understanding the human-teacher/robot-learner partnership in order to design algorithms that support how people want to teach and simultaneously improve the robot's learning behavior",Teachable robots: Understanding human teaching behavior to build more effective robot learners ,,10.1016/j.artint.2007.09.009,Elsevier B.V.,,core
24479060,06/02/2008,"In this paper, we present a new approach for automatic synthesis of fault detection modules for autonomous mobile robots. The method relies on the fact that hardware faults typically change the flow of sensory perceptions received by the robot and the subsequent behavior of the control program. We collect data from three experiments with real robots. In each experiment, we record all sensory inputs from the robots while they are operating normally and after software-simulated faults have been injected. We use backpropagation neural networks to synthesize task-dependent fault detection modules. The performance of the modules is evaluated in terms of false positives and latency. ",Automatic Synthesis of Fault Detection Modules for Mobile Robots £,,,,,core
56365897,2005-05-31T00:00:00,"Realistic and intelligent agent movement remains one of the greatest challenges for games developers. Path-finding strategies are usually employed as a means of allowing an agent to navigate from one part of the game world to another. Typically the game world is stored in a pre-processed structure called a map which contains all of the relevant geometry. In order to cut down the search space for the path-finder, this map is broken down and simplified. The path-finder then uses this simplified representation to determine the best path from the starting point to the desired destination. These simplified representations correspond to graphs, and algorithms such as Dijkstra and A* [6] can then be employed to quickly find paths between the nodes in the graph. The graph used is based on a pre-processed static representation of the game world. However, the assumption that the geometry of the game remains static during the course of play is not necessarily valid anymore. This difficulty is then compounded by the fact that the agent typically has no real-time awareness of the environment around it. This situation results in a number of problems for path-finders, each of which we now outline. The increasing use of physics engines opens up the possibility of completely dynamic game geometry, where the players and agents can physically alter the structure of the game world as play progresses, by knocking over walls for example [2]. Dynamic obstacles can therefore be introduced that block previously accessible nodes on the graph. When this happens the agent will still believe it can walk along this path due to its reliance on the preprocessed static graph. Techniques have been developed that improve the agents’ reactive abilities when dynamic objects obstruct a path. These work well in some situations, but generally the agent will not react until it has collided with an obstacle, as it has no sense of awareness until a trigger is set when a collision occurs. Another problem is the rigid and unrealistic movement that occurs when the agent walks in a straight line between nodes. This is caused by the dilemma which arises in the trade off between speed (the less number of nodes to search the better) and realistic movement (the more nodes, the more realistic the movement). This has been improved in some games by applying spline curves for smoothing out paths along nodes. A further problem is implementing tactical path-finding. This involves not just finding the shortest route but also the route that offers the most cover, or avoids unnecessary encounters with undesirable game entities. One approach is to modify the cost heuristic of A* to take line of fire from other enemy agents into account [7]. This has the benefits of adding realism to the game and also presents a less predictable opponent for the human player. The drawback is that due to the added cost, the search space becomes much larger for A* to process. This approach also assumes that the threat remains static during the paths duration, which is seldom the case. Generally game developers add in special case code to deal with these problems but typically this is only applicable to that particular game [1]. This paper examines these problems and introduces the concept of applying learning techniques to solve them in a new novel way [4]. Our solution to this problem is to provide the agent with a means of navigating its own way around the world, rather than simply relying on routes provided by the game engine. In order to accomplish this the agent requires two important abilities. Firstly it needs to be able to examine its environment in some way in order to know what is in front of it and around it, thus giving it real-time awareness. Secondly it needs some way of processing this information to accomplish tasks such as steering around obstacles that have been placed in its path. The first ability is achieved by embedding sensors in the agent. This is a concept borrowed from robotics where ultrasound or infrared sensors are common. We adapt this idea for our agents by casting rays which test for intersections with the game geometry. In this way information can be provided to the agent pertaining to the proximity of objects within its field of vision. The second ability is being able to process this information in some way. Our solution to this problem is to furnish each agent with an Artificial Neural Network (ANN) [3] which takes the sensor information as input. The ANN is a learning algorithm that we have trained to exhibit the behaviour we want – namely that the agent has the ability to steer around objects. We describe how this provides robust steering behaviour that is tolerant of noisy data. Another advantage of this approach is that the processing required is minimal and hence multiple agents can be imbued with this behaviour without causing a major strain on the CPU. This is used in conjunction with a traditional path-finding algorithm. The algorithm works out a path for the agent but the sensors and the ANN are responsible for moving the agent along that path, and are capable of adapting the path to steer around obstacles or other dynamically introduced geometric changes. Our system is implemented using the Quake 2 [5] game engine and we have extensively tested these ideas against more traditional approaches to path-finding. The game engine gives us a test bed whereby a genetic algorithm [6] is used in real-time to evolve the weights of the neural network. Since the sensors are influencing the agents movement in real-time, as it walks from node to node, it tends to gradually veer away from obstacles thus resulting in less rigid movement. Giving the agent this real-time awareness also compliments the tactical elements of pathfinding as the agent can be alerted in real-time to imminent threats. Our results indicate that this approach is extremely useful in the dynamic environments that are becoming the norm in modern computer games. References [1] Cain, Timothy, ""Practical Optimizations for A*"", AI Game Programming Wisdom, Charles River Media, 2002 [2] Eberly,David,H, ""Game Physics"", Elsevier, Inc, 2004 [3] Fausett, Laurene, ""Fundamentals of Neural Networks Architectures, Algorithms, and Applications"", Prentice-Hall, Inc, 1994. [4] Graham, Ross,""Neural Networks for Real-time Pathfinding in Computer Games"", In proceedings of ITB journal, Issue 9 (2004) [5] www.idsoftware.com/games/quake/quake2/ [6] Russel, Stuart., Norvig, Peter., ""Artificial Intelligence A Modern Approach"", Prentice-Hall, Inc, 1995 [8] Van der Sterren, William,. ""Tactical Path-Finding with A*"", Game Programming Gems 3, Charles River Media, 200",Realistic Agent Movement in Dynamic Game Environments,https://core.ac.uk/download/56365897.doc,,,,core
141210965,2006,"In this paper we present the LIDA architecture as a working model of cognition. We argue that such working models are broad in scope and address real world problems in comparison to experimentally based models which focus on specific pieces of cognition. While experimentally based models are useful, we need a working model of cognition that integrates what we know from neuroscience, cognitive science and AI. The LIDA architecture provides such a working model. A LIDA based cognitive robot or software agent will be capable of multiple learning mechanisms. With artificial feelings and emotions as primary motivators and learning facilitators, such systems will ‘live’ through a developmental period during which they will learn in multiple ways to act in an effective, human-like manner in complex, dynamic, and unpredictable environments. We discuss the integration of the learning mechanisms into the existing IDA architecture as a working model of cognition",LIDA: A Working Model of Cognition,,,,,core
20643496,2008-12-11T14:06:11,"There is a large number of possible applications in the field of mobile robotics: Mail delivery robots, domestic or industrial vacuum cleaners, surveillance robots, demining robots and many others could be very interesting products. Despite this potential market and the actual technology, only few simple systems are commercially available. This proves that there are several important and problematic issues in this field, mainly at the intelligence level. As a reaction to the failure of the classical artificial intelligence applied to the field of mobile robotics, several new approaches have been proposed. Artificial neural networks are one of these, and genetic algorithms, supported by the Artificial Life trend, are also getting more and more consideration. These two techniques have already been applied to mobile robotics, but mainly in simulation, and without a final test on a real mobile robot. The use of physical robots for this research seems to be still problematic due to the lack of efficient tools. Several neural structures for the control of mobile robots have been analysed in this work. All experiences have been carried out on physical robots. To reach this goal, an important effort has been made in order to design new efficient robotic tools. Together with Edo Franzi, André Guignard and Yves Cheneval, we have developed and built hardware and software tools that make an efficient research work possible. Along with several analysis software tools, the mobile robot Khepera has been a result of this development. Using this equipment, six experiences have been carried out, covering a large spectrum of the possible ways neural networks can be used for the control of mobile robots. These experiments have nevertheless been restricted to simple behaviours and small neural networks. The first two experiments show, with a very simple and manually adjusted behaviour, the important role of the interaction of the robot with its environment. The first experiment is based on a collective behaviour, the second on a collaborative one. The adaptation of the robot to the environment is introduced in the third experiment, in which a learning technique is applied. The result is a robot able to learn how to use visual stimuli to avoid particular obstacles. Despite its interesting results, this approach has turned out to be very limited, due to the rigid structure needed. The last three experiments demonstrate the possibilities of the use of genetic algorithms, which proved to be a very flexible adaptation mechanism. The first of these three experiments tests the feasibility of this approach. The second one takes advantage of the characteristics of genetic algorithms to achieve more complex behaviours. Finally, genetic algorithms and learning techniques are associated in the last experiment, showing a high adaptive structure. An important effort has been made to show both advantages and disadvantages of each technique, in order to provide the necessary elements for the continuation of this research activity.Les applications de la robotique mobile autonome pourraient être nombreuses: le robot facteur, le robot aspirateur domestique ou nettoyeur industriel, le robot surveillant, le robot démineur et bien d'autres applications ont un marché considérable. Malgré le marché potentiel et les possibilités technologiques actuelles, seules quelques applications très limitées sont exploitées commercialement, ce qui prouve la difficulté de ces développements du point de vue intelligence. Face à l'échec des méthodes de conception classiques liées à l'intelligence artificielle, plusieurs nouvelles approches ont été proposées. Parmi celles-ci on trouve les réseaux de neurones et, plus récemment, les algorithmes génétiques, qui ont émergé de l'intérêt suscité par la vie artificielle (Artificial Life). L'application de ces techniques aux robots mobiles a été étudiée principalement en simulation, et dans la plupart des cas sans validation sur des robots réels. L'implémentation sur un robot mobile physique pose en effet souvent des problèmes pratiques qui limitent fortement leur utilisation. Ce travail a permis d'analyser plusieurs structures neuronales utilisées pour le contrôle d'un robot mobile, et ceci entièrement sur un robot physique. Pour atteindre ce but, une partie de l'effort a été consacrée à la mise au point d'outils matériels et logiciels. La collaboration avec Edo Franzi, André Guignard et Yves Cheneval a permis de développer le robot mobile Khepera et ses accessoires, ainsi que le logiciel de contrôle associé. Ces outils ont permis de réaliser six expériences qui couvrent le spectre des possibilités d'utilisation des réseaux de neurones pour le contrôle d'un robot mobile autonome. Ces expériences se limitent toutefois à la réalisation de comportements simples réalisés avec des réseaux de petite taille. Les deux premières expériences permettent, par un comportement conçu manuellement et finement ajusté, de mettre en évidence le rôle des interactions entre le robot et son environnement. La tâche exécutée est collective dans la première expérience, et coopérative dans la deuxième. Afin de permettre une adaptation du robot à son environnement, la troisième expérience introduit l'utilisation d'algorithmes d'apprentissage. Avec cette technique, un système capable d'apprendre à utiliser de la vision artificielle pour effectuer de l'évitement d'obstacles a été réalisé. Cette approche se montre toutefois très limitée à cause des structures très rigides nécessaires à son fonctionnement. Les dernières trois expériences mettent en oeuvre les algorithmes génétiques, qui se révèlent être un mécanisme d'adaptation plus flexible. La première de ces trois expériences prouve la faisabilité de l'approche, tandis que la deuxième va plus en profondeur, en essayant de mieux exploiter les caractéristiques des algorithmes génétiques. Finalement algorithmes génétiques et apprentissage sont combinés dans la dernière expérience, donnant lieu à un système de contrôle d'un degré d'adaptation élevé. L'analyse des résultats a permis de mettre en évidence avantages et inconvénients de chaque approche dans l'optique d'une utilisation concrète, et ceci afin de permettre une meilleure orientation de la recherche dans ce domaine",Conception de structures neuronales pour le contrôle de robots mobiles autonomes,,,,,core
82112916,30/04/2008,"AbstractWhile Reinforcement Learning (RL) is not traditionally designed for interactive supervisory input from a human teacher, several works in both robot and software agents have adapted it for human input by letting a human trainer control the reward signal. In this work, we experimentally examine the assumption underlying these works, namely that the human-given reward is compatible with the traditional RL reward signal. We describe an experimental platform with a simulated RL robot and present an analysis of real-time human teaching behavior found in a study in which untrained subjects taught the robot to perform a new task. We report three main observations on how people administer feedback when teaching a Reinforcement Learning agent: (a) they use the reward channel not only for feedback, but also for future-directed guidance; (b) they have a positive bias to their feedback, possibly using the signal as a motivational channel; and (c) they change their behavior as they develop a mental model of the robotic learner. Given this, we made specific modifications to the simulated RL robot, and analyzed and evaluated its learning behavior in four follow-up experiments with human trainers. We report significant improvements on several learning measures. This work demonstrates the importance of understanding the human-teacher/robot-learner partnership in order to design algorithms that support how people want to teach and simultaneously improve the robot's learning behavior",Teachable robots: Understanding human teaching behavior to build more effective robot learners ,https://core.ac.uk/download/pdf/82112916.pdf,10.1016/j.artint.2007.09.009,Elsevier B.V.,,core
4719415,2008-10-21T00:00:00,"Recently in robotics, substantial efforts have been invested on critical applications such as military, nursing, and search-and-rescue. These applications are critical in a sense that the robots may directly deal with human lives in life-or-death situations, and they are therefore required to make highly intelligent decisions as rapidly as possible. The intelligence we are looking for in this type of situations is proactiveness: the ability to anticipate as well as improvise.

Anticipation here means that the robot can assess the current situation, predict the future consequence of the situation, and execute an action to have desired outcome based on the determined assessment and prediction. On the other hand, improvisation is performed when the consequence of the situation is not fully known. In other words, it is the ability to deal with a novel situation based on knowledge or skill being acquired before.

In this presentation, we introduce a biologically inspired computational model of proactive intelligent behavior for robots. Integrating multiple levels of machine learning techniques such as temporal difference learning, instance-based learning, and partially observable Markov decision process, aggregated episodic memories are processed in order to accomplish anticipation as well as improvisation. How this model can be implemented within a software architectural framework and integrated into a physically realized robotic system is also explained. The experimental results using a real robot and high fidelity 3D simulators are then presented in order to help us understand how extended experience of a robot influences its ability to behave proactively.Ph.D.Committee Chair: Arkin, Ronald; Committee Member: Balch, Tucker; Committee Member: Dellaert, Frank; Committee Member: Potter, Steve; Committee Member: Ram, Ashwi",Countering murphys law: the use of anticipation and improvisation via an episodic memory in support of intelligent robot behavior,https://core.ac.uk/download/4719415.pdf,,Georgia Institute of Technology,,core
100338589,2005,"This tutorial aims to present new trends, methods and applications of Virtual Environments endowed with information/knowledge in order to provide to the user more interesting immersive interaction. We present concepts related to environment modelling, visualization and user interaction, mainly focused on some important proprieties: dynamic entities, adaptive environment, intelligent agents and behavioural models. Intelligent Virtual Reality Environments (IVRE) integrate Virtual Reality, Artificial Intelligence and Simulation tools and techniques in order to provide more realism, more dynamic environments and to improve interaction with users. This tutorial aims to present an overview of some important techniques related to IVRE implementation, for example: intelligent objects; autonomous agents control architectures; knowledge acquisition, representation and manipulation techniques; behavioural models (physically based and/or human based); etc. The IVRE have been employed in many applications, described in this tutorial, such as: e-commerce, e-learning, games, simulation of real situations (from crowd simulation to robotics) and also visualization of animations and study of specific behaviours. We conclude this tutorial with some examples of IVRE practica","Intelligent Virtual Reality Environments (IVRE): Principles, Implementation, Interaction, Examples and Practical Applications.",,,,,core
24662810,01/04/2008,"This paper explores how qualitative information can be used to improve the performance of global optimization procedures. Specifically, we have constructed a nonlinear parameter estimation reasoner (NPER) for finding parameter values that match an ordinary differential equation (ODE) model to observed data. Qualitative reasoning (QR) is used within the NPER, for instance, to intelligently choose starting values for the unknown parameters and to empirically determine when the system appears to be chaotic. This enables odrpack, the nonlinear least-squares solver that lies at the heart of this NPER, to avoid terminating at local extrema in the regression landscape. odrpack is uniquely suited to this task because of its efficiency and stability. The NPER&apos;s robustness is demonstrated via a Monte Carlo analysis ofsimulated examples drawn from across the domain of dynamics, including systems that are nonlinear, chaotic, and noisy. It is shown to locate solutions for noisy, incomplete real-world sensor datafrom radio-controlled carsused in the University of British Columbia&apos;s soccer-playing robot project. The parameter estimation scheme described in this paper is a component of pret, an implemented computer program that uses a variety of artificial intelligence techniques to automate system identification-the process ofinferring an internal ODE model from external ob",Global Solutions for Nonlinear Systems Using Qualitative Reasoning*,,,,,core
23584503,22/11/2007,"Abstract  In this paper we show how symbolic Artificial Intelligence (AI) techniques can be used to develop intelligent Virtual Reality (VR) environments. We have developed an expert system architecture for building intelligent agents that respond to voice and gesture commands in virtual environments. To demonstrate the utility of this we present a simple application that allows a user to drive a virtual robot (the VirBot) around a virtual environment with multimodal commands. This software agent is able to accept spoken commands from the user, interpret them using natural language understanding and interact with and modify the virtual environment. Command recognition is further improved by the use of contextual information. Context is encoded using scripts, making it easy to compose inferences about events for which there is incomplete or fuzzy information. This reduces command recognition error rates by 31%.  Introduction  Although humans communicate with each other through a range o..",VirBot: A Virtual Reality Robot Driven With Multimodal Commands,,,,,core
23932296,2007,"Models of the environment are needed for a wide range of robotic applications, from search and rescue to automated vacuum cleaning. Learning maps has therefore been a major research focus in the robotics community over the last decades. In general, one distinguishes between metric and topological maps. Metric maps model the environment based on grids or geometric representations whereas topological maps model the structure of the environment using a graph. The contribution of this paper is an approach that learns a metric as well as a topological map based on laser range data obtained with a mobile robot. Our approach consists of two steps. First, the robots solves the simultaneous localization and mapping problem using an efficient probabilistic filtering technique. In a second step, it acquires semantic information about the environment using machine learning techniques. This semantic information allows the robot to distinguish between different types of places like, e.g., corridors or rooms. This enables the robot to construct annotated metric as well as topological maps of the environment. All techniques have been implemented and thoroughly tested using real mobile robot in a variety of environments",Efficiently Learning Metric and Topological Maps with Autonomous Service Robots,,,,,core
323141410,2006-04-01T00:00:00,"An autonomous mobil robot has been implemented on a Digital Signal Processor (for real time operation) using neural networks as the main part of the program that runs on the processor. The neural network was based on a single layer perceptron (SLP) with two neurons, four inputs coming from four sonar sensors, and two outputs to control the direction of two CD motors. The goal of the mobil robot is to avoid obstacles while it runs randomly in a given room. Two training sets were tested to provide two di®erent reactive behaviors, the first one with forward direction, left turn and right turn, and the second which includes reverse direction. Both
behaviors were compared",Implementation of a neuron-based autonomous Mobil Robot on a DSP,https://core.ac.uk/download/323141410.pdf,,'Universidad Autonoma de Zacatecas - Francisco Garcia Salinas',,core
20915488,29/12/2008,"www.robotics.lu.se The kernels of Matlab and Envision Telerobotics (TR) were merged on the SGI platform, taking advantage of Matlab Engine and the open architecture of Envision TR, designing a new robot simulation environment called the Flexible Unified Simulation Environment (FUSE). Envision TR is a time-continuous graphical robot simulation program from DELMIA Inc. In FUSE, Matlab is able to take advantage of geometrical libraries for most commercial robots on the market and the optimized graphical kernel for real-time 3D applications offered by Envision TR. Envision TR is gained by full access to Matlabs vectorized calculation environment accelerating either the speed of execution at run-time, in some cases above the speed that compiled C-code offers, or at debugging and software simulation, offering the capability to modify the software at run-time, altering the program flow instantly in the middle of program execution. In addition, development of new robot systems are optimized by the ability of using specialized Matlab toolboxes in diverse application areas, such as statistics, automatic control and artificial neural networks, minimizing the software development time, effort and risks taken in major robot projects. FUSE was used in the Rower-2 project, a European Union project aimed to develop a robot system for automatic arc-welding of hulls in tanker ships. The seam-tracking and sensor control software developed in FUSE were implemented in the target QNX embedded system",The Unified Simulation Environment- Envision Telerobotics and Matlab Merged into One Application,,,,,core
24630669,01/04/2008,"designed for interactive supervisory input from a human teacher, several works in both robot and software agents have adapted it for human input by letting a human trainer control the reward signal. In this work, we experimentally examine the assumption underlying these works, namely that the human-given reward is compatible with the traditional RL reward signal. We describe an experimental platform with a simulated RL robot and present an analysis of real-time human teaching behavior found in a study in which untrained subjects taught the robot to perform a new task. We report three main observations on how people administer feedback when teaching a robot a task through Reinforcement Learning: (a) they use the reward channel not only for feedback, but also for future-directed guidance; (b) they have a positive bias to their feedback — possibly using the signal as a motivational channel; and (c) they change their behavior as they develop a mental model of the robotic learner. In conclusion, we discuss future extensions to RL to accommodate these lessons. I",Reinforcement Learning with Human Teachers: Understanding How People Want to Teach Robots Abstract — While Reinforcement Learning (RL) is not traditionally,,,,,core
23470892,22/11/2007,"We describe a variety of projects developed as part of a course in Artificial Intelligence at the University of Minnesota. The projects cover navigation of small mobile robots and learning to accomplish simple tasks, and require a variety of approaches from neural networks to genetic programming to reactive behaviors. The projects have all been implemented on real robots. We discuss how the combination of robotics with Artificial Intelligence adds value to the learning of AI concepts and how the fun of building and programming a robot is a highly motivating force for the learning process. 1 Introduction  The major goal of this paper is to describe examples of integration of real robotics projects in a course in Artificial Intelligence. The examples presented here are some of the class projects done by students taking a course in Artificial Intelligence at the University of Minnesota. The course is intended for senior undergraduate and first year graduate students. The textbook we use i..",Interacting with the real world: a way of teaching Artificial Intelligence concepts,,,,,core
20842697,2005,"In recent years, research has progressed steadily in regard to the use of computers to recognize and render sign language. This paper reviews significant projects in the field beginning with finger-spelling hands such as &quot;Ralph&quot; (robotics), Cyber-Gloves (virtual reality sensors to capture isolated and continuous signs), camera-based projects such as the CopyCat interactive American Sign Language game (computer vision), and sign recognition software (Hidden Markov Modeling and neural network systems). Avatars such as &quot;Tessa&quot; (Text and Sign Support Assistant; three-dimensional imaging) and spoken language to sign language translation systems such as Poland’s project entitled &quot;THETOS&quot; (Text into Sign Language Automatic Translator, which operates in Polish; natural language processing) are addressed. The application of this research to education is also explored. The &quot;ICICLE&quot; (Interactive Computer Identification and Correction of Language Errors) project, for example, uses intelligent computer-aided instruction to build a tutorial system for deaf or hard-of-hearing children that analyzes their English writing and makes tailored lessons and recommendations. Finally, the article considers synthesized sign, which is being added to educational material and has the potential to be developed by students themselves. Technology is rapidly changing and improving the way the world operates. Barriers for people who are deaf are diminishing as projects of the past two decades have unfolded. Through the use of artificial intelligence, researchers are striving to develop hardware and software that will impact the way deaf individuals communicate and learn. This paper takes the reade",Sign Language Recognition and Translation: A Multidisciplined Approach From the Field of ARTIFICIAL INTELLIGENCE,,10.1093/deafed/enj003,,,core
147898898,2005-03-16T13:28:13,"There is a large number of possible applications in the field of mobile robotics: Mail delivery robots, domestic or industrial vacuum cleaners, surveillance robots, demining robots and many others could be very interesting products. Despite this potential market and the actual technology, only few simple systems are commercially available. This proves that there are several important and problematic issues in this field, mainly at the intelligence level. As a reaction to the failure of the classical artificial intelligence applied to the field of mobile robotics, several new approaches have been proposed. Artificial neural networks are one of these, and genetic algorithms, supported by the Artificial Life trend, are also getting more and more consideration. These two techniques have already been applied to mobile robotics, but mainly in simulation, and without a final test on a real mobile robot. The use of physical robots for this research seems to be still problematic due to the lack of efficient tools. Several neural structures for the control of mobile robots have been analysed in this work. All experiences have been carried out on physical robots. To reach this goal, an important effort has been made in order to design new efficient robotic tools. Together with Edo Franzi, André Guignard and Yves Cheneval, we have developed and built hardware and software tools that make an efficient research work possible. Along with several analysis software tools, the mobile robot Khepera has been a result of this development. Using this equipment, six experiences have been carried out, covering a large spectrum of the possible ways neural networks can be used for the control of mobile robots. These experiments have nevertheless been restricted to simple behaviours and small neural networks. The first two experiments show, with a very simple and manually adjusted behaviour, the important role of the interaction of the robot with its environment. The first experiment is based on a collective behaviour, the second on a collaborative one. The adaptation of the robot to the environment is introduced in the third experiment, in which a learning technique is applied. The result is a robot able to learn how to use visual stimuli to avoid particular obstacles. Despite its interesting results, this approach has turned out to be very limited, due to the rigid structure needed. The last three experiments demonstrate the possibilities of the use of genetic algorithms, which proved to be a very flexible adaptation mechanism. The first of these three experiments tests the feasibility of this approach. The second one takes advantage of the characteristics of genetic algorithms to achieve more complex behaviours. Finally, genetic algorithms and learning techniques are associated in the last experiment, showing a high adaptive structure. An important effort has been made to show both advantages and disadvantages of each technique, in order to provide the necessary elements for the continuation of this research activity",Conception de structures neuronales pour le contrôle de robots mobiles autonomes,https://core.ac.uk/download/147898898.pdf,10.5075/epfl-thesis-1598,"Lausanne, EPFL",,core
24645751,01/04/2008,"Evolutionary Robotics (ER) is a field of research that applies evolutionary computing methods to the automated design and synthesis of behavioral robotics controllers. In the general case, reinforcement learning (RL) using high-level task performance feedback is applied to the evolution of controllers for autonomous mobile robots. This form of RL learning is required for the evolution of complex and non-trivial behaviors because a direct error-feedback signal is generally not available. Only the high-level behavior or task is known, not the complex sensor-motor signal mappings that will generate that behavior. Most work in the field has used evolutionary neural computing methods. Over the course of the preceding decade, ER research has been largely focused on proof-of-concept experiments. Such work has demonstrated both the evolvablility of neural network controllers and the feasibility of implementation of those evolved controllers on real robots. However, these proof-of-concept results leave important questions unanswered. In particular, no ER work to date has shown that it is possible to evolve complex controllers in the general case. The research described in this work addresses issues relevant to the extension of ER to generalize","Abstract Nelson, Andrew Lincoln. Competitive Relative Performance and Fitness Selection",,,,,core
20939278,21/11/2007,"The development of speech tools suitable for use in real world  environments requires collaboration between computational linguistics  and new implementation fields e.g. robotics, and the incorporation of new  AI techniques to improve overall system performance. In this paper we  present the core development concepts of SAID (Speaking Autonomous  Intelligent Devices). The work presented centres around four key strands  of research, namely the recasting of the Time Map model as a MultiAgent  System (MAS), the development of a MAS based audio feature  extraction system, the deployment of a BDI based dialog agent and the  design of a MAS based social robot architecture",Speaking Autonomous Intelligent Devices,,,,,core
147933442,2007-12-13T13:42:30,"Embedded systems play an increasing role in our society. From consumer electronics to driving assistance in cars, from medical devices to space exploration, they are ubiquitous. To better assist human beings, embedded systems become more complex, and more intelligent or even autonomous behaviors are expected. To handle their interaction with the real-world, those systems rely on artificial intelligence methods but are also subject to real-time constraints imposed by their environment. Taking advantage of increasingly powerful embedded processors, most of the new intelligent functionalities are implemented in software. As a result, the complexity of embedded software increases dramatically and can no longer be managed by domain specialists that are neither computer scientists nor software engineers. Unfortunately, the computer science community has left aside embedded systems for many years, and most modern software engineering paradigms cannot be applied directly to develop embedded applications. In this work, we present a hybrid framework based on software components that embed value-adding contributions from domain specialists. The framework relies on a dual-kernel approach that executes both a real-time and a best-effort operating system on a single processor. It is centered on dual-portability of software components across hardware platforms and across execution modes. The hybrid framework empowers domain specialists and promotes reuse of valuable software assets. It also supports the generation of hybrid embedded applications that combine real-time and best-effort software components. Such applications can take advantage of both time determinism and high computational throughput to face uncertain and dynamic real-world environments in an intelligent manner. The application of the hybrid framework is illustrated through real-world case studies with autonomous mobile robots from ASL (Autonomous Systems Laboratory) and NASA (National Aeronautics and Space Administration). It is also adapted to support the development of integrated satellite payload controllers for ESA (European Space Agency)",A component-based software framework for hybrid real-time and best-effort embedded applications,,10.5075/epfl-thesis-4023,"Lausanne, EPFL",,core
20777912,13/08/2008,"designed for interactive supervisory input from a human teacher, several works in both robot and software agents have adapted it for human input by letting a human trainer control the reward signal. In this work, we experimentally examine the assumption underlying these works, namely that the human-given reward is compatible with the traditional RL reward signal. We describe an experimental platform with a simulated RL robot and present an analysis of real-time human teaching behavior found in a study in which untrained subjects taught the robot to perform a new task. We report three main observations on how people administer feedback when teaching a robot a task through Reinforcement Learning: (a) they use the reward channel not only for feedback, but also for future-directed guidance; (b) they have a positive bias to their feedback — possibly using the signal as a motivational channel; and (c) they change their behavior as they develop a mental model of the robotic learner. In conclusion, we discuss future extensions to RL to accommodate these lessons. I",Reinforcement Learning with Human Teachers: Understanding How People Want to Teach Robots Abstract — While Reinforcement Learning (RL) is not traditionally,,,,,core
24508191,06/02/2008,"Traditional AI has not concerned itself extensively with sociology nor with what emotional reactions might be produced in its users. On the other hand, entertainment is very concerned indeed with these issues. AI and ALife programs which are to be used in entertainment must therefore be viewed both as AI/ALife endeavors and as psychological and sociological endeavors. This paper presents a brief description of Julia [Mauldin 94], an implemented software agent, and then examines the sociology of those who encounter her, using both transcripts of interactions with Julia, and direct interviews with users. Julia is designed to pass as human in restricted environments while being both entertaining and informative, and often elicits surprisingly intense emotional reactions in those who encounter her. An introduction to MUDs and Julia Julia [Mauldin 94] is a MUD [Curtis 92] [Bruckman 93] [Evard 93] robot. A MUD is a text-only, multiperson, virtual reality. [Mauldin 94], while describing Julia’s internal structure, gives very little ‘feel ’ for what it like to interact with her outside of the strictures of a formal Turing test; hence, transcripts of many interactions with her appear below as examples. (Since Julia adamantly insists that she is female, I refer to the program here as ‘she’.",Entertaining Agents: A Sociological Case Study,,,,,core
11231194,2007-01-01T00:00:00,"This paper describes an integrated system based on open-domain and domain-specific knowledge for the purpose of providing query-based intelligent web interaction. It is understood that general purpose conversational agents are not able to answer questions on specific domain subject. On the other hand, domain specific systems lack the flexibility to handle common sense questions. To overcome the above limitations, this paper proposed an integrated system comprises of an artificial intelligent conversation software robot or chatterbot, called Artificial Intelligence Natural-language Identity (hereafter, AINI), and an Automated Knowledge Extraction Agent (AKEA) for the acquisition of real world knowledge from the Internet. The objective of AKEA is to retrieve real world knowledge or information from trustworthy websites. AINI is the mechanism used to manage the knowledge and to provide appropriate answer to the user. In this paper, we compare the performance of the proposed system against two popular search engines, two question answering systems and two other conversational systems",Query based intelligent web interaction with real world knowledge,,10.1007/s00354-007-0031-7,"Ohmsha, Ltd.",,core
236055346,2006-04-01T00:00:00,"In this work it presents the improvement and extension of the virtual reality software created by the Robotics, Artificial Intelligence and Automatization Outpost Laboratory in the school of electrical engineering of the Pontificia Universidad Católica de Valparaíso",Improvement and extension of Virtual Reality for flexible systems of manufacture,https://core.ac.uk/download/236055346.pdf,10.15837/ijccc.2006.2.2289,'Agora University of Oradea',,core
41213251,2007-01-01T00:00:00,"One primary goal in rescue robotics is to deploy a team of robots for coordinated victim search after a disaster. This requires robots to perform subtasks, such as victim detection, in real-time. Human detection by computationally cheap techniques, such as color thresholding, turn out to produce a large number of false-positives. Markov Random Fields (MRFs) can be utilized to combine the local evidence of multiple weak classifiers in order to improve the detection rate. However, inference in MRFs is computational expensive. In this paper we present a novel approach for the genetic optimizing of the building process of MRF models. The genetic algorithm determines offline relevant neighborhood relations with respect to the data, which are then utilized for generating efficient MRF models from video streams during runtime. Experimental results clearly show that compared to a Support Vector Machine (SVM) based classifier, the optimized MRF models significantly reduce the false-positive rate. Furthermore, the optimized models turned out to be up to five times faster then the non-optimized ones at nearly the same detection rate.Artificial Intelligence & Integrated Computer System",Genetic MRF Model Optimization for Real-Time Victim Detection in Search and Rescue,,10.1109/IROS.2007.4399006,'Institute of Electrical and Electronics Engineers (IEEE)',,core
22279903,21/11/2007,"In this paper we describe a neural network for reactive and adaptive robot navigation. The network is based on a model of classical and operant conditioning first proposed by Grossberg [3]. The network has been successfully implemented on the real Khepera robot. This work shows the potential of applying self-organizing neural networks to the area of intelligent robotics.  Introduction  In order to survive, an autonomous agent must interact successfully with its environment. Since the environment changes constantly, the agent must learn about the causes and consequences of the events that take place around it. Moreover, it must learn to predict the consequences of its own actions, in order to promote favorable events and to prevent unfavorable events. In this respect, one impressive aspect of animal behavior is the facility with which organisms learn to interact with their environment as they gather food, seek mates, and avoid predators and other natural hazards. At our Lab, we believe ..",A Neural Network Model of Avoidance and Approach Behaviors for Mobile Robots,,,,,core
24346321,22/11/2007,". For many reasons, it is desirable to use robots in courses such as introductory computer science, artificial intelligence, and cognitive science, yet the knowledge normally required by students to make effective use of these tools is often prohibitive in such courses with well established curricula. We have developed a user interface that allows students with no prior experience or training in robotics to experiment with behavior networks in real robots, and then brings them down through the software and then the hardware involved. The interface is still in the early stages of development, and has been tested somewhat in a Cognitive Science course, but more widespread use is expected.  Subject Areas: AI Education, Cognitive Science Education, Robotics. 1 Introduction  Robots are very useful tools in education. In computer science, they can be used to provide immediate feedback on performance, and force students to understand resource limitations and efficiency, among other things [Me..",Teaching Bottom-Up AI From the Top Down,,,,,core
24567202,2007,"A central goal of robotics and AI is to be able to deploy an agent to act autonomously in the real world over an extended period of time. It is commonly asserted that in order to do so, the agent must be able to learn to deal with unexpected environmental conditions. However an ability to learn is not sufficient. For true extended autonomy, an agent must also be able to recognize when to abandon its current model in favor of learning a new one; and how to learn in its current situation. This paper presents a fully implemented example of such autonomy in the context of color map learning on a vision-based mobile robot for the purpose of image segmentation. Past research established the ability of a robot to learn a color map in a single fixed lighting condition when manually given a “curriculum, ” an action sequence designed to facilitate learning. This paper introduces algorithms that enable a robot to i) devise its own curriculum; and ii) recognize when the lighting conditions have changed sufficiently to warrant learning a new color map. ",Color learning on a mobile robot: Towards full autonomy under changing illumination,,,,,core
20952041,30/12/2008,"the Czech robota (forced labor). Limited to work too tedious or dangerous for humans, today s robots weld parts on assembly lines, inspect nuclear plants, and explore other planets. Generally, robots are still far from achieving their fictional counterparts intelligence and flexibility. Humanoid robotics labs worldwide are working on creating robots that are one step closer to science fiction s androids. Building a humanlike robot is a formidable engineering task requiring a combination of mechanical, electrical, and software engineering; computer architecture; and realtime control. In 1993, we began a project aimed at constructing a humanoid robot for use in exploring theories of human intelligence. 1,2 In addition to the relevant engineering, computer architecture, and real-time-control issues, we ve had to address issues particular to integrated systems: What types of sensors should we use, and how should the robot interpret the data? How can the robot act deliberately to achieve a task and remain responsive to the environment? How can the system adapt to changing conditions and learn new tasks? Each humanoid robotics lab must address many of the same motor-control, perception, and machine-learning problems. The principles behind our methodology The real divergence between groups stems from radically different research agendas an","ASIDE FROM THEIR TRADITIONAL ROLES, HUMANOID ROBOTS CAN BE USED TO EXPLORE THEORIES OF HUMAN INTELLIGENCE. THE AUTHORS DISCUSS THEIR PROJECT AIMED AT DEVELOPING ROBOTS THAT CAN BEHAVE LIKE AND INTERACT WITH HUMANS.",,,,,core
24476334,06/02/2008,"As autonomous robots become more complex in their behavior, more sophisticated software architectures are required to support the ever more sophisticated robotics software. These software architectures must support complex behaviors involving adaptation and learning, implemented, in particular, by neural networks. We present in this paper a neural based schema [2] software architecture for the development and execution of autonomous robots in both simulated and real worlds. This architecture has been developed in the context of adaptive robotic agents, ecological robots [6], cooperating and competing with each other in adapting to their environment. The architecture is the result of integrating a number of development an",Francisco Cervantes,,,,,core
20691033,02/04/2008,"Two search algorithms (the wall-following search and the “most-open-area” search) have been designed, tested, compared and analysed both in simulations and hardware experiments. Dangerous search tasks such as bomb and chemical searches put human lives seriously at risk and it is imperative to utilise artificial intelligence and robotics to help us in these real-life situations. The search methods presented here have shown that search ideas can be incorporated into robotics and lay down the validations and foundations necessary for practical usage. In these two algorithms, an autonomous mobile robot is programmed using C++ object-oriented programming language and relevant software interfaces to search for a light target source (a fluorescent light bulb) in a dark room. The search is done in a flat or office-like layout, which has rooms and door openings and it ends when the robot comes to a complete stop within a stipulated distance away from the light source. Simulations are done with a robot control device software, Player &amp; Stage and they provide the necessary testing platforms of the algorithms before they are fully implemented on the actual robot hardware. The hardware experiments are conducted using a simple servo-driven robot, equipped with only sonar and photoelectric light sensors and a magnetic compass. The factors used to compare the efficiencies of the two algorithms are the time taken to complete the search tasks as well as their success rates. Final Year Project- Search Algorithms Summary Final results show that the wall-following search is more efficient in terms of success rates compared to the “most-open-area ” search. The latter search method though, provide a faster search method to complete the search in the majority of the cases",Summary,,,,,core
20811999,14/08/2008,"In recent years, research has progressed steadily in regard to the use of computers to recognize and render sign language. This paper reviews significant projects in the field beginning with finger-spelling hands such as ‘‘Ralph’ ’ (robotics), Cyber-Gloves (virtual reality sensors to capture isolated and continuous signs), camera-based projects such as the CopyCat interactive American Sign Language game (computer vision), and sign recognition software (Hidden Markov Modeling and neural network systems). Avatars such as ‘‘Tessa’ ’ (Text and Sign Support Assistant; three-dimensional imaging) and spoken language to sign language translation systems such as Poland’s project entitled ‘‘THETOS’ ’ (Text into Sign Language Automatic Translator, which operates in Polish; natural language processing) are addressed. The application of this research to education is also explored. The ‘‘ICICLE’’ (Interactive Computer Identification and Correction of Language Errors) project, for example, uses intelligent computer-aided instruction to build a tutorial system for deaf or hard-of-hearing children that analyzes their English writing and makes tailored lessons and recommendations. Finally, the article considers synthesized sign, which is being added to educational material and has the potential to be developed by students themselves. Technology is rapidly changing and improving the way the world operates. Barriers for people who are deaf are diminishing as projects of the past two decades have unfolded. Through the use of artificial intelligence, researchers are striving to develop hardware and software that will impact the way deaf individuals communicate and learn. This paper takes the reade","Journal of Deaf Studies and Deaf Education Advance Access published September 28, 2005 Sign Language Recognition and Translation: A Multidisciplined Approach From the Field of Artificial Intelligence",,,,,core
20724198,03/04/2008,"central goal of robotics and AI is to be able to deploy an agent to act autonomously in the real world over an extended period of time. To operate in the real world, autonomous robots rely on sensory information. Despite the potential richness of visual information from on-board cameras, many mobile robots continue to rely on non-visual sensors such as tactile sensors, sonar, and laser. This preference for relatively lowfidelity sensors can be attributed to, among other things, the characteristic requirement of real-time operation under limited computational resources. Illumination changes pose another big challenge. For true extended autonomy, an agent must be able to recognize for itself when to abandon its current model in favor of learning a new one; and how to learn in its current situation. We describe a self-contained vision system that works on-board a vision-based autonomous robot under varying illumination conditions. First, we present a baseline system capable of color segmentation and object recognition within the computational and memory constraints of the robot. This relies on manually labeled data and operates under constant and reasonably uniform illumination conditions. We then relax these limitations by introducing algorithms for i) Autonomous planned color learning, where the robot uses the knowledge of its environment (position, size and shape of objects) to automatically generate a suitable motion sequence and learn the desired colors, and ii) Illumination change detection and adaptation, where the robot recognizes for itself when the illumination conditions have changed sufficiently to warrant revising its knowledge of colors. Our algorithms are fully implemented and tested on the Sony ERS-7 Aibo robots",Structure-Based Color Learning on a Mobile Robot under Changing Illumination,,,,,core
21786625,21/11/2007,"Reinforcement learning algorithms without an internal world model often suer from overly long time to converge. Mostly the agent has to be successful a several hundred times before it could learn how to behave in even simple environments. In this case, a world model could be useful to reduce the number of real world trials by performing the action virtually in the world model. This may help to propagate the Reinforcement Q- or V- values much faster through the state (action) space and could be interpreted as a simple form of planning. In the following investigation we introduce a self organizing deterministic world model &quot;DIVA&quot; (&quot;Discretization Improvement by VAriancereduction&quot;) with an adaptive discretization, which can speed up learning by using common methods like Suttons Dyna-Q. Proposed in this article, the &quot;DIVA&quot;-model is implemented in a six legged walking robot, which learns how to walk in a minimum of time and with a minimum of real world moving trials.  ",DIVA: A Self Organizing Adaptive World Model for Reinforcement Learning,,,,,core
24569531,07/02/2008,"central goal of robotics and AI is to be able to deploy an agent to act autonomously in the real world over an extended period of time. To operate in the real world, autonomous robots rely on sensory information. Despite the potential richness of visual information from on-board cameras, many mobile robots continue to rely on non-visual sensors such as tactile sensors, sonar, and laser. This preference for relatively lowfidelity sensors can be attributed to, among other things, the characteristic requirement of real-time operation under limited computational resources. Illumination changes pose another big challenge. For true extended autonomy, an agent must be able to recognize for itself when to abandon its current model in favor of learning a new one; and how to learn in its current situation. We describe a self-contained vision system that works on-board a vision-based autonomous robot under varying illumination conditions. First, we present a baseline system capable of color segmentation and object recognition within the computational and memory constraints of the robot. This relies on manually labeled data and operates under constant and reasonably uniform illumination conditions. We then relax these limitations by introducing algorithms for i) Autonomous planned color learning, where the robot uses the knowledge of its environment (position, size and shape of objects) to automatically generate a suitable motion sequence and learn the desired colors, and ii) Illumination change detection and adaptation, where the robot recognizes for itself when the illumination conditions have changed sufficiently to warrant revising its knowledge of colors. Our algorithms are fully implemented and tested on the Sony ERS-7 Aibo robots",Structure-Based Color Learning on a Mobile Robot under Changing Illumination,,,,,core
24662542,01/04/2008,"Abstract. The research in autonomous systems has been influencing the improvement in navigation of mobile vehicles and robots using artificial intelligence techniques. Agriculture and industry are instances of economical segments that may be benefit from the application of those scientific efforts. However, those systems demand control architectures that, most of the time, have a high degree of complexity when having tested by physical implementations. On the other hand, recent works confirm the eficacy of Petri nets for modeling real systems with concurrent activities as well as in planning and controlling of mobile robot tasks. The aim of this paper is to apply Petri nets for modeling and analysing concurrent behaviors present in a simple robotic navigation architecture, which was successfully tested in a mini-robot (Khepera), to apply as reference for implementation in a Autonomous Agricultural Vehicle (VAA). Available software for academic purposes were used for models development and proprieties analysis. The results point that using the formalism proposed for modeling, one is capable of determining control policies, analysing and identifying conflicts in concorrent robotic behaviors with ease in comparison to real implementations",MODELING OF A CONTROL ARCHITECTURE FOR A MINI-ROBOT NAVIGATION USING PETRI NETS,,,,,core
268806248,2007-12-01T08:00:00,"The inverse kinematic problem is crucial for robotics. In this paper, a solution algorithm is presented using artificial intelligence to improve the pseudo-inverse Jacobian calculation for the 7-DOF Whole Arm Manipulator (WAM) and 6-DOF Titan II teleoperation system. An investigation of the inverse kinematics based on fuzzy logic and artificial neural networks for the teleoperation system was undertaken. Various methods such as Adaptive Neural-Fuzzy Inference System (ANFIS), Genetic Algorithms (GA), Multilayer Perceptrons (MLP) Feedforward Networks, Radial Basis Function Networks (RBF) and Generalized Regression Neural Networks (GRNN) were tested and simulated using MATLAB. Each method for identification of the pseudo-inverse problem was tested, and the best method was selected from the simulation results and the error analysis.
From the results, the Multilayer Perceptrons with Levenberg-Marquardt (MLP-LM) method had the smallest error and the fastest computation among the other methods. For the WAM-Titan II teleoperation system, the new inverse kinematics calculations for the Titan II were simulated and analyzed using MATLAB. Finally, extensive C code for the alternative algorithm was developed, and the inverse kinematics based on the artificial neural network with LM method is implemented in the real system. The maximum error of Cartesian position was 1.3 inches, and from several trajectories, 75 % of time implementation was achieved compared to the conventional method. Because fast performance of a real time system in the teleoperation is vital, these results show that the new inverse kinematics method based on the MLP-LM is very successful with the acceptable error",Inverse Kinematics Based on Fuzzy Logic and Neural Networks for the WAM-Titan II Teleoperation System,https://core.ac.uk/download/268806248.pdf,,TRACE: Tennessee Research and Creative Exchange,,core
144020151,2007-06-15T00:00:00,"Este projeto consiste em um sistema de navegação autônomo baseado em redes neurais nebulosas modulares capacitando o robô a alcançar alvos, ou pontos metas, em ambientes desconhecidos. Inicialmente, o sistema não tem habilidade para a navegação, após uma fase de experimentos com algumas colisões, o mecanismo de navegação aprimora-se guiando o robô ao alvo de forma eficiente. Uma arquitetura híbrida inteligente é apresentada para este sistema de navegação, baseada em redes neurais artificiais e lógica nebulosa. A arquitetura é hierárquica e costitiui-se de dois módulos responsáveis por gerar comportamentos inatos de desvio de obstáculos e de busca ao alvo. Um mecanismo de aprendizagem por reforço, baseada em uma extensão da lei de Hebb, pondera os comportamentos inatos conflitantes ajustando os pesos sinápticos das redes neurais nos instantes de captura do alvo e de colisão contra obstáculos. A abordagem consolidada em simulação é validada em ambientes reais neste trabalho. Para tanto, este sistema foi implementado e testado no simulador Saphira, ambiente de simulação que acompanha o robô Pioneer I e que denota um estágio anterior aos testes em ambientes reais por apresentar comportamentos do robô similares aos comportamentos do robô móvel. Modificações na arquitetura híbrida foram necessárias para adaptar o sistema de navegação simulado ao sistema incorporado no Pioneer I. Experimentos em ambientes reais demonstraram a eficiência e a capacidade de aprendizagem do sistema de navegação, validando a arquitetura híbrida inteligente para aplicação em robôs móveisThis project consists in a autonomous navigation system based on modular neuro-fuzzy networks that is able to guide the robot in unknown environments from a initial point to the goal. Initially, the system is not able to navigate, but after a trial and error period and some collisions, it improves in guiding the robot to the goal efficiently. A intelligent hybrid architecture is presented for this naviga tion system based on artificial neural networks and fuzzy logic. This architecture is hierarquical and consists in two modules that generate innate behaviors, like obstacles avoiding and target reaching. A reinforcement learning mecanism, based on the extended Hebb law, balances this conflicting innate behaviors adjusting the neural network synaptic weights as obstacle and collision avoidance and target reaching takes place. In this project, the approach is consolidated in simulation and validated in real environments. To this end, this system has been implemented by using Saphira simulator and Pioneer I simulation environment. This simulated evironment is a previous stage of tests performed real time and presents simulated robot behaviors similar to real mobile robot behaviors. The hybrid architecture was modified to adapt the simulated navigation system into Pioneer I software. Experiments in a real environments show the efficiency and learning capabilities of the navigation system, validating the intelligent hybrid architecture for mobile robots application",Intelligent hybrid architecture for robot autonomous navigation,,10.11606/D.55.2007.tde-15062007-161911,"'Universidade de Sao Paulo, Agencia USP de Gestao da Informacao Academica (AGUIA)'",,core
10881976,2007-01-01T00:00:00,"Robot soccer pits teams of fast-moving robots in a dynamic environment (Sng et al., 2002). Robot soccer fosters AI and intelligent robotics research by providing a standard problem where a wide range of technologies can be integrated and examined (Asada & Kitano, 1999). Today two international robot soccer federations, RoboCup (RoboCup, 2007) and FIRA (FIRA, 2007), organize competitions in an eclectic range of categories. Those competitions are accompanied with technical conferences. The first international robot soccer tournament MiroSot'96 was held at Korea Advanced Institute of Science and Technology (KAIST), in November, 1996. At the time of writing, we can count more than ten different robot soccer leagues from RoboCup and FIRA. Taxonomy of the robot soccer leagues could start with the vision system used. The global vision group contains all the leagues that allow a global vision system (camera that gives an eye-bird view of the playing field). The image processing is done on a PC that controls the robots via a radio link. Whereas the local vision group contains all the leagues that require the vision processing to be done on the robots themselves. In this second group, the robots achieve a higher level of autonomy. Only wheeled robots are used in the global vision group. Whereas, the local vision group can be subdivided into wheeled robots and legged robots. Finally there are simulation leagues that provide a test bed for multi-agent research for those who do not have access to real robots. Robot soccer not only stimulates robotic research, but also provides a platform for computational intelligence education that allows the development of engaging undergraduate level assignments. However, there are several limiting factors for the widespread use of robot soccer as a research platform or a teaching tool. Most robot soccer leagues like the popular RoboCup Small Size and FIRA Mirosot leagues require a large playing field and a team of several postgraduate students to build the hardware and develop the complex software. The least resource-demanding robot soccer league is the simulation league. Unfortunately, by its very nature, this league does not provide the invaluable experience of real robots. With the constraint of using real robots, the least resource-demanding robot soccer league is arguably the FIRA KheperaSot league. This league represents Desktop Robot Soccer, in the sense that the playing field fits on a desktop or a computer laboratory bench. The regulations of the KheperaSot league impose size restrictions on the robots. The size limitation lowers the entry barrier for participants in relation to other robot soccer tournaments, making it more accessible to individuals and small teams with modest funding and infrastructure support. The size limitation also poses challenge for hardware technology. It pushes the limits of how much processing and sensing can be put into the small package at a reasonable cost. However the size is not as small as to requiring miniaturisation technology beyond the reach of standard electronics and construction techniques. The KheperaSot league was the first fully autonomous robot soccer league of FIRA",Desktop Robot Soccer,https://core.ac.uk/download/10881976.pdf,,I-Tech Education and Publishing,,core
20778584,13/08/2008,"ARCHON ™ (ARchitecture for Cooperative Heterogeneous ON-line systems) was Europe’s largest project in the area of Distributed Artificial Intelligence (DAI). It devised a general-purpose architecture, software framework, and methodology which has been used to support the development of DAI systems in a number of real world industrial domains. Some examples of the applications to which it has been successfully applied include: electricity distribution and supply, electricity transmission and distribution, control of a cement kiln complex, control of a particle accelerator, and control of a robotics application. The type of cooperating community that it supports has a decentralised control regime and individual problem solving agents which are large grain, loosely coupled, and semi-autonomous. This paper will tackle a broad range of issues related to the application of ARCHON technology to industrial applications. Firstly, it gives the rationale for a DAI approach to industrial applications and highlights the characteristics which typify this important domain. Secondly, the ARCHON framework is detailed- with a special emphasis being placed upon the implementation architecture. Thirdly, a brief resumee and status report of the main applications is presented. Finally, the lessons learned and the future plans are presented. 1","Mile End Road,",,,,,core
20691363,02/04/2008,"Communication among participants (agents, robots) is central to an appearance of Collective AI. In this work we deal with the development of local communication mechanisms for real microrobotic swarms. We demonstrate that despite of very limited capabilities of the microrobot, the specific construction of communication hardware and software allows very extended collective capabilities of the whole swarm. We propose mechanisms providing information content and context for collective navigation, coordination and spatial perception in a group of microrobots. ",Collective AI: context awareness via communication,,,,,core
20865100,03/12/2008,"Abstract: The hardware implementation of neural networks is a new step in the evolution and use of neural networks in practical applications. The CMAC cerebellar model articulation controller is intended especially for hardware implementation, and this type of network is used successfully in the areas of robotics and control, where the real time capabilities of the network are of particular importance. The implementation of neural networks on FPGA’s has several benefits, with emphasis on parallelism and the real time capabilities.This paper discusses the hardware implementation of the CMAC type neural network, the architecture and parameters and the functional modules of the hardware implemented neuro-processor",Neural Network on FPGA for Command Surface Approximation,,,,,core
24586579,2006,"1 Spartacus is the name of our robot entry at the 2005 AAAI Mobile Robot Challenge, which consists of making a robot attend the National Conference on Artificial Intelligence. Designing robots that are capable of interacting with humans in real life settings can be considered the ultimate challenge when it comes to intelligent autonomous systems. One key issue is the integration of multiple modalities (e.g., mobility, physical structure, navigation, vision, audition, dialogue, reasoning) into a coherent implementation. Such integration increases the complexity and the diversity of interactions the robot can have, as of analysis and monitoring of such increased capabilities. This paper reports on our solutions and findings resulting from the hardware, software and computation integration work on Spartacus, along with future perspectives regarding this initiative",Manuscript Click here to download Manuscript: AR2006.tex Spartacus Attending the 2005 AAAI Conference,,,,,core
147909127,2006-01-12T12:30:27,"Evolutionary robotics - as other adaptive methods, such as reinforcement learning and learning classifier systems - can take considerable time and resources which require a careful evaluation of the hardware tools and methodologies employed. We outline a set of hardware solutions and working methodologies that can be used for successfully implementing and extending the evolutionary approach to complex environments, robots, and real-world applications. The issues discussed include the integration of simulation and real robots, design issues of evolvable robots, hardware requirements for incremental evolution, and hardware and software tools for monitoring and analysis",Hardware Solutions for Evolutionary Robotics,,10.1007/3-540-64957-3_69,'Springer Science and Business Media LLC',,core
10543849,"October 12, 2006","Electroactive polymers (EAP) are human made actuators that are the closest to mimic biological muscles. Technology was advanced to the level that biologically inspired robots are taking increasing roles in the world around us and making science fiction ideas a closer engineering reality. Artificial technologies (AI, AM, and others) are increasingly becoming practical tools for making biologically inspired devices and instruments with enormous potential for space applications. Polymer materials are used to produce figures that resemble human and animals. These materials are widely employed by the movie industry for making acting figures and by the orthopedic industry to construct cyborg components. There are still many challenges ahead that are critical to making such possibilities practical. The annual armwrestling contest is providing an exciting measure of how well advances in EAP are implemented to address the field challenges. There is a need to document natures inventions in an engineering form to possibly inspire new capabilities",Biomimetics as a Model for Inspiring Human Innovation,,,,,core
24549251,2007,"A central goal of robotics and AI is to be able to deploy an agent to act autonomously in the real world over an extended period of time. It is commonly asserted that in order to do so, the agent must be able to learn to deal with unexpected environmental conditions. However an ability to learn is not sufficient. For true extended autonomy, an agent must also be able to recognize when to abandon its current model in favor of learning a new one; and how to learn in its current situation. This paper presents a fully implemented example of such autonomy in the context of color map learning on a vision-based mobile robot for the purpose of image segmentation. Past research established the ability of a robot to learn a color map in a single fixed lighting condition when manually given a “curriculum, ” an action sequence designed to facilitate learning. This paper introduces algorithms that enable a robot to i) devise its own curriculum; and ii) recognize when the lighting conditions have changed sufficiently to warrant learning a new color map. ",Color learning on a mobile robot: Towards full autonomy under changing illumination,,,,,core
24715544,02/04/2008,"Q-Learning is a Reinforcement Learning method for solving sequential decision problems, where the utility of actions depends on a sequence of decisions and there exists uncertainty about the dynamics of the environment the agent is situated on. This general framework has allowed that Q-Learning and other Reinforcement Learning methods to be applied to a broad spectrum of complex real world problems such as robotics, industrial manufacturing, games and others. Despite its interesting properties, Q-learning is a very slow method that requires a long period of training for learning an acceptable policy. In order to solve or at least reduce this problem, we propose a parallel implementation model of Q-learning using a tabular representation and via a communication scheme based on cache. This model is applied to a particular problem and the results obtained with different processor configurations are reported. A brief discussion about the properties and current limitations of our approach is finally presented",A parallel implementation of Q-Learning based on communication with cache,,,,,core
20937925,21/11/2007,"Describes the mechanical, electronic and software design of a 10-DOF  bipedal robot which has been constructed to study control, parameterisation  and automatic expansion of the stability envelope of a complex real-time  behaviour, namely, dynamically-balanced two-legged walking. The machine  is physically complete and demonstrates reasonable reliability in movement  control including dynamically-balanced standing. High-level reinforcement  learning code is being developed to extend this to walking. The machine  offers a challenging problem domain to the flourishing machine learning  community and represents a shift in emphasis, away from learning  algorithms that work on simplified, preprocessed, artificial and static data  sets to learning heuristics which deal with noisy, real-time data collected  from sensors on a dynamic, real-world system",A dynamically-Balanced Walking Biped,,,,,core
220558616,2007-01-01T00:00:00,"Using the Distributed Artificial Intelligence and the Distributed Robotics as a frame of reference, particularly
the RoboCup World Championship, in this work an architecture for multiple mobile, autonomous, rational and
coordinated agents is proposed. This architecture is composed of a mechanism of reasoning or automatic decision
making, a functional and control structure, an artificial vision system, a navigation system, and a robotic architecture
for a team of small minibots with capacity of acting at the Small Size League. In the design, development and
construction of these components are coupled methods and techniques of Emergent Computation with models based
on the traditional knowledge representation of Artificial Intelligence, producing algorithms that combine properties
as adaptability, robustness, uniformity with representation, inference and universality. The Agents are conformed by
mechanisms that integrate sensors out of the body of the robot with effectors embedded in the robot and processes
that are executed jointly over different computing devices, providing them with cooperative and competitive
behaviors which are exhibited by the minibots while they operate in real time. The execution and performance of
the architecture proposed had been experimented and evaluated through the implementation of a Multiagent System
that allows the operation of one or more robots on a testbed conformed by a football field made to comply with
RoboCup rules.Tomando como marco de referencia a la Inteligencia Artificial Distribuida y la Robótica Distribuida, particularmente
a la Copa Mundial de Fútbol de Robots RoboCup, en este trabajo se propone una arquitectura para
múltiples agentes móviles, autónomos, racionales, coordinados, la cual comprende un modelo de organización para
los agentes, una estructura funcional y de control para cada uno de los agentes, un mecanismo de razonamiento o
toma de decisiones automáticas, un sistema de visión artificial, un sistema de navegación y una arquitectura para un
equipo de pequeños minibots con capacidad de actuación en la Liga de Pequeños Robots. En el diseño, desarrollo y
construcción de estos componentes se acoplan métodos y técnicas clasificadas dentro de Computación Emergente con
modelos basados en la representación tradicional de la Inteligencia Artificial, dando lugar a algoritmos que combinan
las propiedades de adaptabilidad, robustez y uniformidad con la representación, inferencia y la universalidad. Los
Agentes están conformados por mecanismos que integran sensores ubicados fuera del cuerpo del robot con efectores
instalados en el robot y procesos que se ejecutan en forma conjunta sobre diferentes dispositivos de cómputo,
dotándolos de comportamientos competitivos y cooperativos los cuales son exhibidos por los minibots al operar en
tiempo real. La ejecución y rendimiento de la arquitectura propuesta ha sido experimentada y evaluada mediante
la implementación de un Sistema Multiagentes que permite la operación de uno o más robots en un ambiente de
trabajo constituido por un campo de fútbol construido de acuerdo a las nor:u¡ativas de Robocup",Arquitectura multiagentes para un equipo de pequeños robots,,,,,core
147936115,2008-02-27T07:21:29,"A key challenge for neural modeling is to explain how a continuous stream of multimodal input from a rapidly changing environment can be processed by stereotypical recurrent circuits of integrate-and-fire neurons in real time. We propose a new computational model for real-time computing on time-varying input that provides an alternative to paradigms based on Turing machines or attractor neural networks. It does not require a task-dependent construction of neural circuits. Instead, it is based on principles of high-dimensional dynamical systems in combination with statistical learning theory and can be implemented on generic evolved or found recurrent circuitry. It is shown that the inherent transient dynamics of the high-dimensional dynamical system formed by a sufficiently large and heterogeneous neural circuit may serve as universal analog fading memory. Readout neurons can learn to extract in real time from the current state of such recurrent neural circuit information about current and past inputs that may be needed for diverse tasks. Stable internal states are not required for giving a stable output, since transient internal states can be transformed by readout neurons into stable target outputs due to the high dimensionality of the dynamical system. Our approach is based on a rigorous computational model, the liquid state machine, that, unlike Turing machines, does not require sequential transitions between well-defined discrete internal states. It is supported, as the Turing machine is, by rigorous mathematical results that predict universal computational power under idealized conditions, but for the biologically more realistic scenario of real-time processing of time-varying inputs. Our approach provides new perspectives for the interpretation of neural coding, the design of experiments and data analysis in neurophysiology, and the solution of problems in robotics and neurotechnology",Real-time computing without stable states: a new framework for neural computation based on perturbations,,10.1162/089976602760407955,'MIT Press - Journals',,core
147909112,2006-01-12T12:30:27,"From perception to action and form action to perception, all elements of an autonomous agent are interdependent and need to be strongly coherent. The final behavior of the agent is the result of the global activity of this loop and every weakness of incoherence of a single element has strong consequences on the performances of the agent. We think that, for the purpose of building autonomous robots, all these elements need to be developed together in continuous interaction with the environment. We describe the implementation of a possible solution (artificial neural networks and genetic algorithms) on a real mobile robot through a set of three different experiments. We focus our attention on three different aspects of the control structure: perception, internal representation and action. In all the experiments these aspects are not considered as single processing elements, but as part of an agent. For every experiment, the advantages and disadvantages of this approach are presented and discussed. The results show that the combination of genetic algorithms and neural networks is a very interesting technique for the development of control structures in autonomous agents. The time necessary for evolution, on the other hand, is very important limitation of the evolutionary approach",Evolution of neural control structures: Some experiments on mobile robots,https://core.ac.uk/download/147909112.pdf,10.1016/0921-8890(96)81008-6,'Elsevier BV',,core
21193226,1997,"Introduction  Mobile robotic systems offer an ideal platform for testing and implementing many of the concepts developed in more abstract artificial intelligence. Robotic systems embody a complex interaction of computation, perception and actuation that depend upon such familiar tasks as recognition and reaction. In order for robots to perform real-world tasks such as navigation, localization and exploration, the subsystems of motion, sensing and computation must be merged into a single, realizable unit that uses the different techniques together.  Our group is investigating problems in the domain of computational perception, in the context of mobile robotics. In particular, we are concerned with environment exploration, and map construction. We are using the AAAI 1997 Mobile Robot competition as an opportunity to test a number of implementations of systems in navigation, spatial reasoning and perception.  Methodology  We are using a modular, distributed software a",Autonomous Exploration: An Integrated Systems Approach,,,,,core
4384163,2004-06-16T00:00:00,"Most Artificial Intelligence (AI) work can be characterized as either ``high-level'' (e.g., logical, symbolic)  or ``low-level'' (e.g., connectionist networks, behavior-based robotics). Each approach suffers from  particular drawbacks. High-level AI uses abstractions that often have no relation to the way real,  biological brains work. Low-level AI, on the other hand, tends to lack the powerful abstractions that are  needed to express complex structures and relationships. I have tried to combine the best features of  both approaches, by building a set of programming abstractions defined in terms of simple, biologically  plausible components. At the ``ground level'', I define a primitive, perceptron-like computational unit.  I then show how more abstract computational units may be implemented in terms of the primitive  units, and show the utility of the abstract units in sample networks. The new units make it possible to  build networks using concepts such as long-term memories, short-term memories, and frames. As a  demonstration of these abstractions, I have implemented a simulator for ``creatures'' controlled by a  network of abstract units. The creatures exist in a simple 2D world, and exhibit behaviors such as  catching mobile prey and sorting colored blocks into matching boxes. This program demonstrates that  it is possible to build systems that can interact effectively with a dynamic physical environment, yet use  symbolic representations to control aspects of their behavior",Building Grounded Abstractions for Artificial Intelligence Programming,,,,,core
105887771,2004,"Abstract. The activities of search and rescue of victims in large-scale disasters are not only highly relevant social problems, but pose several challenges from a scientific standpoint. In this context, the RoboCup-Rescue project focused on the problems of bringing aids immediately after a large disaster, and aims at creating system based on AI and Robotics technologies, where heterogeneous agents (software, robots, hu-man beings) interact in a cooperative manner. In this paper we present the achievements of a research project, based on the RoboCup Rescue simulator, carried out in Italy in collaboration with the Italian Fire Department. The overall project goal is to devise tools to allow monitoring and supporting decisions which are needed in a real-time rescue operation in a large scale disaster, and to provide a methodology for evaluation of multi-agent system which considers not only the efficiency of a system, but also its robustness when conditions in the environment change, as well as other features, such as the ability to acquire a precise and coherent representation of the disaster scenario. ",RoboCup Rescue Simulation: Methodologies Tools and Evaluation for Practical Applications,,,,,core
4752017,2002-01-01T00:00:00,"©2002. American Association for Artificial Intelligence. The original publication is available at: www.aaai.orgPresented at the AAAI Mobile Robot Competition Workshop (AAAI-02), 28 July-1 August 2002 in Edmonton, Alberta, Canada.We describe our entry in the AAAI 2002 Urban Search and
Rescue (USAR) competition, a marsupial team consisting of
a larger wheeled robot and several small legged robots, carried
around by the larger robot. This setup exploits complimentary
strengths of each robot type in a challenging domain.
We describe both the hardware and software architecture, and
the on-board real-time mapping which forms the basis of accurate
victim-localization crucial to the USAR domain. We
also evaluate what challenges remain to be resolved in order
to deploy search and rescue robots in realistic scenarios",The Georgia Tech Yellow Jackets: A Marsupial Team for Urban Search and Rescue,https://core.ac.uk/download/4752017.pdf,AAAI Press,,,core
22552321,2002,"Research Objective and Interests Create and apply new artificial intelligence (AI) techniques and software that leverage models of human domain knowledge to coordinate software, robotic, or human agents in performing real‐time planning, scheduling, execution, diagnosis, and state estimation tasks in uncertain or partially known environments. My interests include computer security, AI, multiagent systems, robotics, control architectures, simulation, operations research, distributed and real‐time systems. Educatio",NASA Space Act Awards,,,,,core
22938237,1998,".  This paper presents an implementation of the sins multi-strategy learning controller for  mobile robot navigation. This controller uses low-level reactive control that is modulated  on-line by a learning system based on case-based reasoning and reinforcement learning.  The case-based reasoning part captures regularities in the environment. The reinforcement  learning part gradually improves the acquired knowledge. Evaluation of the controller is  presented in a real and in a simulated mobile robot.  1 Introduction  How to specify behaviour in a robot has come a long way since the low-level languages of assembly robotics (Lozano-Perez, 1982). The classical AI approach to control  1  proved too slow and too fragile for the real world but showed that representations of the environment, however difficult to maintain, produce interesting behaviour. In nouvelle AI, e.g. (Brooks, 1985; Brooks, 1991a; Brooks, 1991b), agents merely react to the current environmental situation posed, limited ..","A Learning Mobile Robot: Theory, Simulation and Practice",,,10.1007/3-540-49240-2_10,,core
24435344,2004,"This report introduces a general formulation of relational behaviours for cooperative real robots and an example of its implementation using the pass between soccer robots of the Middle-Sized League of RoboCup. The framework supports explicit teamwork between two team mates. This implies that both participants know from each other that they are committed to the relational behaviour and that they will not quit without informing the other team members first. The formulation is based on the Joint-Commitment Theory by Cohen and Levesque [2]. The implementation of the pass concerns furthermore the development of two primitive behaviours, the intercept and aimAndPass behaviour, and the introduction of heuristics to support coordinated execution. This implementation is supported by past work on soccer robots navigation. Results of experiments with real robots under controlled situations (i.e., not during a game) are presented to illustrate the described concepts. The framework provides an easy way for implementing relational behaviours and takes care of synchronized execution. Acknowledgement A short word of thanks for those who have been helping me during this graduation research. The main part of this project has been done at the Institute for Systems and Robotics, of the Instituto Superior Técnico in Lisbon Portugal. I would like to thank my supervisor overthere, prof. dr. Pedro Lima, for all his advice and efforts. It is too much to mention all members of the SocRob-group personally, so I thank you all in general for the cooperation in both professional and friendly way. Then, from the Department of Artificial Intelligence of the University of Groningen, I got good advice and guidance from prof. dr. Lambert Schomaker, my internal supervisor. I also want to thank dr. Rineke Verbrugge for revising this report. And I should mention the other students in room 155 and the ping-pong table overthere. Of course I have family and friends who are absolutely valuable at all moments",Artificial Intelligence,,,,,core
24638848,2000,"This report describes the design and implementation of the agent-based EVMs system. A prototype is available at www.sims.berkeley.edu/research/metadata/demo.html. 1.0 Intelligent Agents Since 1990, intelligent agents have been broadly used in complex, dynamic, and open applications such as production planning, robotics, and in searching the Internet. Though theoretical research as well as real implementation of agent technology are common, there is no commonly agreed-upon definition of intelligent agents. From an Artificial Intelligence point of view, an intelligent agent is a hardware or (more usually) softwarebased computer system that has the following properties: (Wooldridge &amp; Jennings 1995) • Autonomy: agents work on their own without direct interventions of humans or others. • Social ability: agents interact with each other using an agent communication language (ACL, e.g., Telescript, Safe-Tcl, KQML, etc.). • Reactivity: agents perceive their environment and respond in a timely manner. • Pro-activeness: agents are goal-oriented",Design and implementation of the agent-based EVMs System,,,,,core
21097665,2003,"In the robotics area, visual tracking is an important and difficult problem therefore is necessary to have a robust and efficient control algorithm which presents immunity characteristics to stochastic direction and speed changes of the object to be tracked. Also is important count with a segmentation algorithm which be able to tolerate changes in the intensity of light. We describe in this report the implementation of fuzzy controllers based on the fuzzy condensed algorithm and also the developed of a LVQ neural network to segment the image. For this work we used two fuzzy condensed algorithms running in a PC to control a robot’s head which tracks a human face. We describe the main lines of the fuzzy condensed algorithm as well as the LVQ neural networks architecture employed and the implementation, the fuzzy condensed controller performance in comparison to a PID controller and real time results. ",Intelligent Tracking,,,,,core
23046160,1998,"The control of mobile robots acting autonomously in the real world is one of the long-term goals of the field of artificial intelligence. So far the field lacks methods bridging the gap between the sophisticated symbolic techniques to represent and reason about action and more and more reliable low-level robot control and navigation systems. In this paper we present GOLEX, an execution and monitoring system for the logic-based action language GOLOG and the complex and distributed RHINO control software which operates on RWI B21 and B14 mobile robots. GOLEX provides the following features: it maps abstract primitive actions into low-level commands of the robot control system, thus allowing the user to concentrate on the application rather than the inner workings of the robot; it monitors the execution of the primitive GOLOG actions, making it possible to detect simple execution failures and timeouts; and it includes means to deal with sensing and user input and to continue the operati..",GOLEX --- Bridging the Gap between Logic (GOLOG) and a Real Robot,,,,,core
10563749,Nov-04,"Topics include: Multifunction Imaging and Spectroscopic Instrument; Position-Finding Instrument Built Around a Magnetometer; Improved Measurement of Dispersion in an Optical Fiber; Probe for Sampling of Interstitial Fluid From Bone; Neuropsychological Testing of Astronauts; Method of Calibration for a Large Cathetometer System; Four-Channel PC/104 MIL-STD-1553 Circuit Board; Improved Method of Locating Defects in Wiring Insulation; Strobe Traffic Lights Warn of Approaching Emergency Vehicles; Improved Timing Scheme for Spaceborne Precipitation Radar; Concept for Multiple-Access Free-Space Laser Communications; Variable Shadow Screens for Imaging Optical Devices; Verifying Diagnostic Software; Initial Processing of Infrared Spectral Data; Activity-Centric Approach to Distributed Programming; Controlling Distributed Planning; New Material for Surface-Enhanced Raman Spectroscopy; Treated Carbon Nanofibers for Storing Energy in Aqueous KOH; Advanced Infant Car Seat Would Increase Highway Safety; Development of Biomorphic Flyers; Second-Generation Six-Limbed Experimental Robot; Miniature Linear Actuator for Small Spacecraft; Process for Making Single-Domain Magnetite Crystals; A New Process for Fabricating Random Silicon Nanotips; Resin-Transfer-Molding of a Tool Face; Improved Phase-Mask Fabrication of Fiber Bragg Gratings; Tool for Insertion of a Fiber-Optic Terminus in a Connector; Nanofluidic Size-Exclusion Chromatograph; Lightweight, Low-CTE Tubes Made From Biaxially Oriented LCPs; Using Redundancy To Reduce Errors in Magnetometer Readings; Compact Instrument for Measuring Profile of a Light Beam; Multilayer Dielectric Transmissive Optical Phase Modulator; Second-Generation Multi-Angle Imaging Spectroradiometer; Real-Time Adaptive Color Segmentation by Neural Networks; Research and Development in Optical Communications; Tests of Multibeam Scintillation Mitigation on Laser Uplinks; and Spaceborne Infrared Atmospheric Sounder","NASA Tech Briefs, November 2004",https://core.ac.uk/download/pdf/10563749.pdf,,,,core
71146495,01/10/1997,"'The first year of this effort emphasized independent development and refinement of each of the three major subsystems (imaging/AI, robotics and virtual reality). Two of the three efforts emphasized the critical task of site. virtualization, prior to telepresence-guided D and D. Substantial algorithm refinement and software and hardware development occurred in each area. Relevant publications resulting from this work are cited below.","Advanced sensing and control techniques to facilitate semi-autonomous decommissioning of hazardous sites. 1997 annual progress report, September 15, 1996--September 14, 1997",,"Clemson Univ., SC (United States)",10.2172/13497,,core
4705833,1997-01-01T00:00:00,"Reactive controllers are widely used in mobile robots because they are able to achieve successful performance in real-time. However, the configuration of a reactive controller depends highly on the operating conditions of the robot and the environment; thus, a reactive controller configured for one class of environments may not perform adequately in another. This paper presents a formulation of parameter-adaptive reactive controllers. Parameter-adaptive reactive controllers inherit all the advantages of traditional reactive controllers, but in addition they are able to adjust themselves to the current operating conditions of the robot and the environment in order to improve task performance. Additionally, the paper describes a multistrategy learning algorithm that combines ideas from case-based reasoning and reinforcement learning to construct a mapping between the operating conditions of the mobile robot and the appropriate controller configuration; this mapping is in turn used to adapt the controller configuration dynamically. The algorithm is implemented and evaluated in a robotic navigation system that controls a Denning MRV-III mobile robot",Learning of Parameter-Adaptive Reactive Controllers for Robotic Navigation,,Georgia Institute of Technology,,,core
21694446,1997,"Traditional AI has not concerned itself extensively with sociology nor with what emotional reactions might be produced in its users. On the other hand, entertainment is very concerned indeed with these issues. AI and ALife programs which are to be used in entertainment must therefore be viewed both as AI/ALife endeavors and as psychological and sociological endeavors. This paper presents a brief description of Julia [Mauldin 94], an implemented software agent, and then examines the sociology of those who encounter her, using both transcripts of interactions with Julia, and direct interviews with users. Julia is designed to pass as human in restricted environments while being both entertaining and informative, and often elicits surprisingly intense emotional reactions in those who encounter her. An introduction to MUDs and Julia Julia [Mauldin 94] is a MUD [Curtis 92] [Bruckman 93] [Evard 93] robot. A MUD is a text-only, multiperson, virtual reality. [Mauldin 94], while describing Julia’s internal structure, gives very little ‘feel ’ for what it like to interact with her outside of the strictures of a formal Turing test; hence, transcripts of many interactions with her appear below as examples. (Since Julia adamantly insists that she is female, I refer to the program here as ‘she’.",Entertaining Agents: a sociological case study,,Press,,,core
40542519,1998-01-01T00:00:00,"Hybrid agent architectures comprise the radical change of paradigms in AI over the past decades by reconciling the different styles of reactive, deliberative, even social systems. They have been successfully applied to a range of complex real-world domains. Due to their originally informal background, a verification of design goals in derived implementations, theoretical foundations, and a detailed comparison with other agent models have not yet been obvious. The present work proposes a formal methodology to bridge the gap between theoretical and practical aspects especially of hybrid designs, such as the layered INTERRAP. The employed, connected stages of specification, i.e., architecture, computational model, theory, proof calculus, and implementation, also provide a yet unique framework for comparing heterogeneous agent models including unified and logic-based ones. Based on recent work on INTERRAP, we demonstrate that this methodology allows to compare state-of-the-art designs from robotics, AI, computer science, and cognitive science with respect to a spectrum of inherent properties along the two dimensions of abstraction and declarativity. This supports our claim that INTERRAP is a coherent and advanced account of layered agency including goal-oriented abstraction planning in on-line interaction with reactive skills and social reasoning. We also derive particular research issues to guide the future development of INTERRAP. (orig.)Available from TIB Hannover: RR 1812(98-01) / FIZ - Fachinformationszzentrum Karlsruhe / TIB - Technische InformationsbibliothekSIGLEBundesministerium fuer Bildung, Wissenschaft, Forschung und Technologie, Bonn (Germany)DEGerman",Methodological comparison of agent models,,,,,core
20700348,2002,"This paper presents the design, development and implementation of a Dynamic Fuzzy Neural Networks (D-FNNs) Controller suitable for real-time industrial applications. The unique feature of the D-FNNs controller is that it has dynamic self-organising structure, fast learning speed, good generalisation and flexibility in learning. The approach of rapid prototyping is employed to implement the D-FNNs controller with a view of controlling a Selectively Compliance Assembly Robot Arm (SCARA) in real time. Simulink, iterative software for simulating dynamic systems, is used for modelling, simulation and analysis of the dynamic system. The D-FNNs controller was implemented through Real-Time Workshop (RTW). RTW generates C-codes from the Simulink block diagrams and in turn, the generated codes (object codes) are downloaded to the dSPACE DS1102 floating-point processor, together with the supporting files, for execution. The performance of the D-FNNs controller was found to be superior and it matches favourably with the simulation results",Real-time implementation of a dynamic fuzzy neural networks controller for a SCARA,,,10.1016/s0141-9331(02)00069-8,,core
143105982,01/01/2002,"Q-Learning is a Reinforcement Learning method for solving sequential decision problems, where the utility of actions depends on a sequence of decisions and there exists uncertainty about the dynamics of the environment the agent is situated on. This general framework has allowed that Q-Learning and other Reinforcement Learning methods to be applied to a broad spectrum of complex real world problems such as robotics, industrial manufacturing, games and others. Despite its interesting properties, Q-learning is a very slow method that requires a long period of training for learning an acceptable policy. In order to solve or at least reduce this problem, we propose a parallel implementation model of Q-learning using a tabular representation and via a communication scheme based on cache. This model is applied to a particular problem and the results obtained with different processor configurations are reported. A brief discussion about the properties and current limitations of our approach is finally presented.Facultad de Informátic",A parallel implementation of Q-learning based on communication with cache,,,,,core
100892750,1997,"Reactive controllers has been widely used in mobile robots since they are able to achieve suc-cessful performance in real-time. However, the con®guration of a reactive controller depends highly on the operating conditions of the robot and the environment; thus, a reactive controller con®gured for one class of environments may not perform adequately in another. This paper presents a formulation of learning adaptive reactive controllers. Adaptive reactive controllers inherit all the advantages of traditional reactive controllers, but in addition they are able to ad-just themselves to the current operating conditions of the robot and the environment in order to improve task performance. Furthermore, learning adaptive reactive controllers can learn when and how to adapt the reactive controller so as to achieve effective performance under different conditions. The paper presents an algorithm for a learning adaptive reactive controller that combines ideas from case-based reasoning and reinforcement learning to construct a mapping between the operating conditions of a controller and the appropriate controller con®guration; this mapping is in turn used to adapt the controller con®guration dynamically. As a case study, the algorithm is implemented in a robotic navigation system that controls a Dennin",Learning adaptive reactive controllers,,,,,core
327070322,2002-01-01T00:00:00,"To date there have been few implementation of Holland’s Learning Classifier System (LCS) on real robots. The paper introduces a Temporal Classifier System (TCS), an LCS derived from Wilson’s ZCS. Traditional LCS have the ability to generalise over the state action-space of a reinforcement learning problem using evolutionary techniques. In TCS this generalisation ability can also be used to determine the state divisions in the state space considered by the LCS. TCS also implements components from Semi-Mark-Decision Process (SMDP) theory to weight the influence of time on the reward functions of the LCS. A simple light-seeking task on a real robot platform using TCS is presented which demonstrates desirable adaptive characteristics for the use of LCS on real robots",TCS learning classifier system controller on a real robot,,'Springer Science and Business Media LLC',10.1007/3-540-45712-7_57,,core
20898187,1998,"The aim of this thesis is to perform robot positioning, based on an odometry which is continuously corrected by different landmark detection systems demanding as less modifications as possible for the environment. Two independent correction systems (a supervised and an unsupervised) were implemented into two different experiences which represent the subject of this thesis. The supervised experiment uses grid lines painted on the floor which are detected by a single light sensor underneath the robot which cannot distinguish between horizontal and vertical lines. The robot knows the geometry of the grid lines and its estimated position which is calculated by odometry. A new position probability model calculates the assumed robot position and transforms this single sensor information into a reliable position and orientation indication of the robot. The intended trajectory is slightly modified in order to optimize the correction algorithm by guiding the robot more efficiently over close grid lines. The theory was implemented and tested on a real Khepera robot. Investigation and modeling of the odometry error is the main subject of this first experiment. The second experiment demonstrates continuous odometry correction by an unsupervised correction system. Different kind of unsupervised neural networks classify the robot’s rough sensor signals. A statistica",Robot Positioning by Supervised and Unsupervised Odometry Correction,,,,,core
24553423,2000,"Two types of neural networks were trained and tested on a real robot for a natural landmark recognition task. The neural networks investigated were the multilayer perceptron (MLP) and learning vector quantisation (LVQ). The intended application is for autonomous vacuuming robots in completely unknown indoor environments, using a novel topological world model and region filling algorithm. A topological world model based on natural landmarks is built incrementally while the robot systematically cleans the environment. The implementation of this world model depends on robust and accurate recognition of natural landmarks. Both types of neural network were found to be able to successfully recognise the natural landmarks selected",Natural landmark recognition using neural networks for autonomous vacuuming robots,,,,,core
24599737,2003,"It is difficult to apply traditional reinforcement learning algorithms to robots, due to problems with large and continuous domains, partial observability, and limited numbers of learning experiences. This paper deals with these problems by combining: 1. reinforcement learning with memory, implemented using an LSTM recurrent neural network whose inputs are discrete events extracted from raw inputs; 2. online exploration and offline policy learning. An experiment with a real robot demonstrates the methodology’s feasibility",A robot that reinforcement-learns to identify and memorize important previous observations,,,,,core
22382787,1998,"As autonomous robots become more complex in their behavior, more sophisticated software architectures are required to support the ever more sophisticated robotics software. These software architectures must support complex behaviors involving adaptation and learning, implemented, in particular, by neural networks. We present in this paper a neural based schema [2] software architecture for the development and execution of autonomous robots in both simulated and real worlds. This architecture has been developed in the context of adaptive robotic agents, ecological robots [6], cooperating and competing with each other in adapting to their environment. The architecture is the result of integrating a number of development and execution systems: NSL, a neural simulation language; ASL, an abstract schema language; and MissionLab, a schema-based mission-oriented simulation and robot system. This work contributes to modeling in Brain Theory (BT) and Cognitive Psychology, with applications in D..",A Neural Schema Architecture for Autonomous Robots,,,,,core
24021849,2000,"Believable agents usually depend upon explicit, model-based simulations of human emotions.  This work appeals instead to the sensibilities of dramatic acting to create agents that are  believable. The chosen task is that of comedy improvisation as it provides a solid demonstration  of the agents believability in the context of a high-level deliberate goal. Furthermore, this work  employs physical robots as the actors, employing the real-time sensor values from the robots as  inputs into the acting process. This paper describes the dramatic approach to acting that we  used and describes the Java-based implementation on two Nomad Scout robots. Actual,  improvised scripts created by the robots are included and analyzed.  Introduction  The field of Artificial Intelligence has spawned research on creating believable agents that can respond to situations with emotions that mimic our own. An obvious application of this ability and a good testing ground for it is in storytelling, particularly ..",Robot Improv: Using Drama to Create Believable Agents,,,,,core
209828433,31/10/2002,"AbstractOver recent years, the notion of agency has claimed a major role in defining the trends of modern research. Influencing a broad spectrum of disciplines such as Sociology, Psychology, among others, the agent paradigm virtually invaded every sub-field of Computer Science, not least because of the Internet and Robotics.Multi-agent Systems (MAS) are communities of problem-solving entities that can perceive and act upon their environments to achieve their individual goals as well as joint goals. The work on such systems integrates many technologies and concepts in artificial intelligence and other areas of computing. There is a full spectrum of MAS applications that have been and are being developed; from search engines to educational aids to electronic commerce and trade.Although commonly implemented by means of imperative languages, mainly for reasons of efficiency, the agent concept has recently increased its influence in the research and development of computational logic based systems.Computational Logic, by virtue of its nature both in substance and method, provides a well-defined, general, and rigorous framework for systematically studying computation, be it syntax, semantics, procedures, or attending implementations, environments, tools, and standards. Computational Logic approaches problems, and provides solutions, at a sufficient level of abstraction so that they generalise from problem domain to problem domain, afforded by the nature of its very foundation in logic, both in substance and method, which constitutes one of its major assets.The purpose of this workshop is to discuss techniques, based on computational logic, for representing, programming and reasoning about multi-agent systems in a formal way. This is clearly a major challenge for computational logic, to deal with real world issues and applications.The first workshop in this series took place in Las Cruces, New Mexico, USA, in 1999, under the designation Multi-Agent Systems in Logic Programming (MASLP'99), and affiliated with ICLP'99. In the following year, the name of the workshop changed to Computational Logic in Multi-Agent Systems (CLIMA'00), taking place in London, UK, and affiliated with CL'2000. The subsequent edition, CLIMA'01, took place in Paphos, Cyprus, affiliated with ICLP'01. The present edition, CLIMA'02, takes place in Copenhagen, Denmark, on August the1st of 2002, and is affiliated with ICLP'02 and part of FLOC'02.We would like to thank the authors of the submitted papers, the members of the program committee and the additional reviewers for their contribution to both the meeting and this volume. We would also like to thank Michael Mislove for his help with the editing of the proceedings.Programme Committee
				Jürgen Dix (The University of Manchester, UK)Thomas Eiter (Vienna University of Technology, Austria)Klaus Fischer (DFKI, Germany)Michael Fisher (University of Liverpool, UK)James Harland (Royal Melbourne Institute of Technology, Australia)Wiebe van der Hoek (Utrecht University, The Netherlands)Katsumi Inoue (Kobe University, Japan)João Alexandre Leite (New University of Lisbon, Portugal)Luís Moniz Pereira (New University of Lisbon, Portugal)Ken Satoh (National Institute of Informatics, Japan)V. S. Subrahmanian (University of Maryland, USA)Francesca Toni (Imperial College, UK)Paolo Torroni (University of Bologna, Italy)Additional Reviewers
				José AlferesKoji IwanumaAndrea SchalkTadashi AraragiGerhard LakemeyerMichael SchroederAlastair BurtWei LiuKenji TaguchiAnna CiampoliniSeng LokeHans TompitsPierangelo Dell'AcquaHidetomo NabeshimaMirosaw TruszczynskiUwe EglyNaoyuki NideMathijs de WeerdtMichael FinkInna PivkinnaMichael WinikoffChiara GhidiniFabrizio RiguzziCees WitteveenHisashi HayashiChiaki SakamaFor this edition of CLIMA, we have received 25 submissions of which 12 were selected for presentation, after a careful review process where each paper was independently reviewed by three members of the Program Committee.The workshop consisted of five sessions: four devoted to the oral presentation of the selected papers and subsequent discussion; and one devoted to a panel discussion, Paolo Torroni being the invited moderator. There follows a brief overview of the workshop.Session 1 - Agents: Arguments and UpdatesSchroeder and Schweimeier present a framework based on logic programming with 3-valued multi-agent argumentation and fuzzy unification, for knowledge representation and reasoning in agents, to accommodate arguments for negotiating agents when agent communication is subject to uncertainty.Leite et al. extend the language LUPS introducing MLUPS, an update command language designed for specifying the flexible evolution of hierarchically related groups of agents, based on logic programming, thus assigning them declarative semantics.Kakas and Moraïtis present a modular argumentation framework for modelling agent deliberation, where object level arguments can be made conditional on agents' roles and the priority relation amongst such roles can, in turn, be made conditional on contents, on top of which a simple form of abduction allows dealing with incomplete knowledge.Session 2 - Logics for AgentsToyama et al. introduce a translation of multi-agent autoepistemic logic (MAEL), a logic for multi-agent systems based on Moore's autoepistemic logic, into logic programming, and show the correspondence between MAEL extensions and the stable models of the corresponding logic program.Dell'Acqua et al. extend their previous work on abductive logic programming based multi-agent systems, in which agents can update themselves and each other, eliminate contradictory update rules, abduce hypotheses to explain observations, and use them to generate actions, with asynchronous based communication through the use of buffers.Harland and Winikoff discuss the formalisation and implementation issues of BDI-type agents, using a Linear Logic based calculus that allows a mixture of forward- and backward-chaining techniques.Session 3 - BDI Agent SystemsBordini and Moreira investigate how far the Asymmetry Thesis Principles formulated by Rao and Georgeff are actually met by the abstract agent specification language AgentSpeak(L), hence contributing to the reconciliationBetween practice and theory of BDI-based agents.Araragi et al. formalise and propose a method to solve a verification problem that arises in implementing a commitment strategy for the BDI architecture, namely the verification of the suitability and/or feasibility of the intentions of an agent.Nide et al. extend with mental state consistency features their previously presented deduction system for CTL-based propositional BDI Logics using sequent calculus, as a step towards the use of the expressive power of BDI Logics as executable specification languages of rational agents.Session 4 - Agents: Speculative Computation and IntrospectionHayashi et al. address the issue of integrating speculative computation and action execution through logic programming, namely by devising a method for plan modification when speculative computation fails or actions are executed.Iwanuma and Inoue refine the first-order consequence-finding procedure based on clausal tableaux SOL, with conditional answer computation and skip-preference, to formalise speculative computation in a master-slave multi-agent system.Bolander investigates on finding consistent classes of formulas under the syntactical treatment of knowledge and belief, identifying three maximal sets of introspective beliefs that strong introspective agents can consistently maintain so as to avoid the paradoxes of self-reference.Session 5 - Panel DiscussionTorroni moderates a panel discussion entitled ""Logics and Multi-agents: towards a new symbolic model of cognition"".This volume constitutes the proceedings of CLIMA'02.September 2002, Jürgen Dix, João Alexandre Leite and Ken Satoh (Guest Editors",Preface ,,Published by Elsevier B.V.,10.1016/S1571-0661(05)80598-5,,core
196650468,1998-01-01T00:00:00,"Hybrid agent architectures comprise the radical change of paradigms in AI over the past decades by reconciling the different styles of reactive, deliberative, even social systems. They have been successfully applied to a range of complex real-world domains. Due to their originally informal background, a verification of design goals in derived implementations, theoretical foundations, and a detailed comparison with other agent models have not yet been obvious. The present work proposes a formal methodology to bridge the gap between theoretical and practical aspects especially of hybrid designs, such as the layered INTERRAP. The employed, connected stages of specification, i.e., architecture, computational model, theory, proof calculus, and implementation, also provide a yet unique framework for comparing heterogeneous agent models including unified and logic-based ones. Based on recent work on INTERRAP, we demonstrate that this methodology allows to compare state-of-the-art designs from robotics, AI, computer science, and cognitive science with respect to a spectrum of inherent properties along the two dimensions of abstraction and declarativity. This supports our claim that INTERRAP is a coherent and advanced account of layered agency including goal-oriented abstraction planning in on-line interaction with reactive skills and social reasoning. We also derive particular research issues to guide the future development of INTERRAP",Methodological comparison of agent models,https://core.ac.uk/download/196650468.pdf,'Walter de Gruyter GmbH',10.22028/D291-25009,,core
323904252,2002-01-01T00:00:00,"To date there has only been one implementation of Holland's Learning Classifier System (LCS) on real robots. In this paper the use of Wilson's ZCS system is described for an obstacle avoidance task. Although the task is simple it does present some advances and change of emphasis over the previous LCS robotic implementation. The controller model is ""event"" based. Instead of the robot being assigned fixed length actions, continuous actions are taken. These actions are taken until an ""event"" occurs. An event can be thought of as a change of state. This division of the world into states is usually part of the problem description, and to do this automatically is currently one of the challenges facing machine learning. The paper then introduces TCS, a form of ZCS that attempts to address this issue. LCS have the ability to generalise over the state action-space. In TCS this generalisation ability can also be used to determine the extent of this space. TCS also implements components from SMDP reinforcement learning theory to weight the influence of time on the reward functions of the LCS. A simple light-seeking task on the robot platform using TCS is presented which demonstrates desirable adaptive characteristics for the use of LCS on real robots",ZCS and TCS learning classifier system controllers on real robots,,,,,core
57031841,2002,"Reinforcement learning algorithms without an internal world model often suffer from overly long time to converge. Mostly the agent has to be successful a several hundred times before it could learn how to behave in even simple environments. In this case, a world model could be useful to reduce the number of real world trials by performing the action virtually in the world model. This may help to propagate the Reinforcement Q- or V- values much faster through the state (action) space and could be interpreted as a simple form of planning. In the following investigation we introduce a self organizing deterministic world model “DIVA” (“Discretization Improvement by Variance reduction”) with an adaptive discretization, which can speed up learning by using common methods like Suttons Dyna-Q. Proposed in this article, the “DIVA”-model is implemented in a six legged walking robot, which learns how to walk in a minimum of time and with a minimum of real world moving trials",DIVA: A self organizing adaptive world model for reinforcement learning,,,,,core
4676412,1997-01-01T00:00:00,"Reactive controllers has been widely used in mobile robots since they are
able to achieve successful performance in real-time. However, the
configuration of a reactive controller depends highly on the operating
conditions of the robot and the environment; thus, a reactive controller
configured for one class of environments may not perform adequately in
another. This paper presents a formulation of ""learning adaptive
reactive controllers"". Adaptive reactive controllers inherit all the
advantages of traditional reactive controllers, but in addition they are
able to adjust themselves to the current operating conditions of the robot 
and the environment in order to improve task performance. Furthermore, 
learning adaptive reactive controllers can learn when and how to adapt the
reactive controller so as to achieve effective performance under different
conditions. The paper presents an algorithm for a learning adaptive
reactive controller that combines ideas from case-based reasoning and
reinforcement learning to construct a mapping between the operating
conditions of a controller and the appropriate controller configuration;
this mapping is in turn used to adapt the controller configuration
dynamically.  As a case study, the algorithm is implemented in a robotic
navigation system that controls a Denning MRV-III mobile robot. The system 
is extensively evaluated using statistical methods to verify its learning 
performance and to understand the relevance of different design parameters 
on the performance of the system",Multistrategy Learning of Adaptive Reactive Controllers,,Georgia Institute of Technology,,,core
4751372,2001-06-01T00:00:00,"Presented at the 6th International Symposium on Artificial Intelligence and Robotics and Automation in Space: i-SAIRAS 2001, Canadian Space Agency, St-Hubert, Quebec, Canada, June 18-22, 2001.This paper presents a new strategy for autonomous navigation of eld mobile robots on hazardous natural terrain using a fuzzy logic approach and a novel mea- sure of terrain traversability. The navigation strategy is comprised of three simple, independent behaviors: seek-goal, traverse-terrain, and avoid-obstacle. The recommendations from these three behaviors are com- bined through appropriate weighting factors to gen- erate the nal steering and speed commands that are executed by the robot. The weighting factors are pro- duced by fuzzy logic rules that take into account the current status of the robot. This navigation strategy requires no a priori information about the environ- ment, and uses the on-board traversability analysis to enable the robot to select relatively easy-to-traverse paths autonomously. Field test results obtained from implementation of the proposed algorithms on the commercial Pioneer AT rover are presented. These results demonstrate the real-time capabilities of the terrain assessment and fuzzy logic navigation algorithms",Terrain-Based Navigation of Planetary Rovers:  A Fuzzy Logic Approach,,Canadian Space Agency,,,core
54643084,2003,The paper proposes a neural networks approach to the solution of the tracking problem for mobile robots. Neural networks based controllers are investigated in order to exploit the nonlinear approximation capabilities of the nets for modeling the kinematic behavior of the vehicle and for reducing unmodeled tracking errors contributions. The training of the nets and the control performances analysis have been done in a real experimental setup. The proposed solutions are implemented on a PC-based control architecture for the real-time control of the LabMate mobile base and are compared with classical kinematic control schemes. Experimental results are satisfactory in terms of tracking errors and computational efforts,Neural Networks Based Control of Mobile Robots: Development and  Experimental Validation.,,,10.1002/rob.10110,,core
71312485,2001-04-01T00:00:00,"Advances in Artificial Intelligence (AI) and micro-technologies will soon give rise to production of large-scale forces of autonomous micro-robots with systems of innate behaviors and with capabilities of self-organization and real world tasking. Such organizations have been compared to schools of fish, flocks of birds, herds of animals, swarms of insects, and military squadrons. While these systems are envisioned as maintaining a high degree of autonomy, it is important to understand the relationship of man with such machines. In moving from research studies to the practical deployment of large-scale numbers of robots, one of critical pieces that must be explored is the command and control architecture for humans to re-task and also inject global knowledge, experience, and intuition into the force. Tele-operation should not be the goal, but rather a level of adjustable autonomy and high-level control. If a herd of sheep is comparable to the collective of robots, then the human element is comparable to the shepherd pulling in strays and guiding the herd in the direction of greener pastures. This report addresses the issues and development of command and control for largescale numbers of autonomous robots deployed as a collective force",Fiscal Year 2000,,'Office of Scientific and Technical Information  (OSTI)',10.2172/911031,,core
24337302,1997,"At the MIT Artificial Intelligence Laboratory we have been working on technologies for an Intelligent Room. Rather than pull people into the virtual world of the computer we are trying to pull the computer out into the real world of people. To do this we are combining robotics and vision technology with speech understanding systems, and agent based architectures to provide ready at hand computation and information services for people engaged in day to day activities, both on their own and in conjunction with others. We have built a layered architecture where at the bottom level vision systems track people and identify their activities and gestures, and through word spotting decide whether people in the room are talking to each other or to the room itself. At the next level an agent architecture provides a uniform interface to such specially built systems, and to other off the shelf software, such as web browsers, etc. At the highest level we are able to build application systems that p..",The Intelligent Room Project,,,,,core
21010267,2000,"as a derivative of the Czech robota (forced labor). Limited to work too tedious or dangerous for humans, today’s robots weld parts on assembly lines, inspect nuclear plants, and explore other planets. Generally, robots are still far from achieving their fictional counterparts’ intelligence and flexibility. Humanoid robotics labs worldwide are working on creating robots that are one step closer to science fiction’s androids. Building a humanlike robot is a formidable engineering task requiring a combination of mechanical, electrical, and software engineering; computer architecture; and real-time control. In 1993, we began a project aimed at constructing a humanoid robot for use in exploring theories of human intelligence. 1,2 In addition to the relevant engineering, computer architecture, and real-time-control issues, we’ve had to address issues particular to integrated systems: What types of sensors should we use, and how should the robot interpret the data? How can the robot act deliberately to achieve a task and remain responsive to the environment? How can the system adapt to changing conditions and learn new tasks? Each humanoid robotics lab must address many of the same motor-control, perception, and machine-learning problems",Humanoid robots: a new kind of tool,,,10.1109/5254.867909,,core
20968761,2001,"Most Artificial Intelligence (AI) work can be characterized as either “high-level” (e.g., logical, symbolic) or “low-level” (e.g., connectionist networks, behavior-based robotics). Each approach suffers from particular drawbacks. High-level AI uses abstractions that often have no relation to the way real, biological brains work. Low-level AI, on the other hand, tends to lack the powerful abstractions that are needed to express complex structures and relationships. I have tried to combine the best features of both approaches, by building a set of programming abstractions defined in terms of simple, biologically plausible components. At the “ground level”, I define a primitive, perceptron-like computational unit. I then show how more abstract computational units may be implemented in terms of the primitive units, and show the utility of the abstract units in sample networks. The new units make it possible to build networks using concepts such as long-term memories",Building Grounded Abstraction for   Artificial   Intelligence Programming,,,,,core
198328378,01/09/2004,"Analisamos a variação de desempenho de algoritmos de aprendizagem por reforço em situações de ambigüidade de estados comumente produzidas pela baixa capacidade sensorial de robôs móveis. Esta variação é produzida pela violação da condição de Markov, importante para garantir a convergência destes algoritmos. As conseqüências práticas desta violação em sistemas reais não estão avaliadas de maneira definitiva na literatura. São estudados neste artigo os algoritmos Q-learning, Sarsa e Q(lambda), em experimentos realizados em um robô móvel Magellan Pro™. De modo a definir um verificador de desempenho para os algoritmos testados, foi implementado um método para criar mapas cognitivos de resolução variável. Os resultados mostram um desempenho satisfatório dos algoritmos, com uma degradação suave em função da ambigüidade sensorial. O algoritmo Q-learning teve o melhor desempenho, seguido do algoritmo Sarsa. O algoritmo Q(lambda) teve seu desempenho limitado pelos parâmetros experimentais. O método de criação de mapas se mostrou bastante eficiente, permitindo uma análise adequada dos algoritmos.We analyzed the performance variation of reinforcement learning algorithms in ambiguous state situations commonly caused by the low sensing capability of mobile robots. This variation is caused by violation of the Markov condition, which is important to guarantee convergence of these algorithms. Practical consequences of this violation in real systems are not firmly established in the literature. The algorithms assessed in this study were Q-learning, Sarsa and Q(lambda), and the experiments were performed on a Magellan Pro™robot. A method to build variable resolution cognitive maps of the environment was implemented in order to provide realistic data for the experiments. The implemented learning algorithms presented satisfactory performance on real systems, with a graceful degradation of efficiency due to state ambiguity. The Q-learning algorithm accomplished the best performance, followed by the Sarsa algorithm. The Q(lambda) algorithm had its performance restrained by experimental parameters. The cognitive map learning method revealed to be quite efficient, allowing adequate algorithms assessment",Desempenho de algoritmos de aprendizagem por reforço sob condições de ambiguidade sensorial em robótica móvel,,Sociedade Brasileira de Automática,,,core
56097525,1998-10-15T00:00:00,"The Cooperative Intelligent Real-time Control Architecture (CIRCA)
is a novel architecture for intelligent real-time control that can
guarantee to meet hard deadlines while still using unpredictable,
unrestricted AI methods. CIRCA includes a real-time subsystem used to
execute reactive control plans that are guaranteed to meet the domain's
real-time deadlines, keeping the system safe. At the same time, CIRCA's AI
subsystem performs higher-level reasoning about the domain and the
system's goals and capabilities, to develop future reactive control plans.
CIRCA thus aims to be intelligent about real-time: rather than requiring
the system's AI methods to meet deadlines, CIRCA isolates its reasoning
about which time-critical reactions to guarantee from the actual execution
of the se ected reactions. 

	The formal basis for CIRCA's performance guarantees is a
state-based world model of agent/environment interactions. Borrowing
approaches from real-time systems research, the world model provides the
information required to make real-time performance guarantees, but avoids
unnecessary complexity. Using the world model, the AI subsystem develops
reactive control plans that restrict the world to a limited set of safe
and desirable states, by guaranteeing the timely performance of actions to
preempt transitions that lead out of the set of states. By executing such
""safe"" and ""stable"" plans, CIRCA's real-time subsystem is able to keep the
system safe (in the world as modeled) for an indeterminate amount of time,
while the parallel AI subsystem develops the next appropriate plan. 

	We have applied a prototype CIRCA implementation to a simulated
Puma robot arm performing multiple tasks with real-time deadlines, such as
packing parts off a conveyor belt and responding to asynchronous
interrupts. Our experimental results show how the system can guarantee to
accomplish these tasks under a given set of domain conditions (e.g.,
conveyor belt speed) and resource limitations (e.g., robot arm speed). 
Furthermore, because CIRCA reasons explicitly about its own predictable,
guaranteed behaviors, the system can recognize when its resources are
insufficient for the desired behaviors (e.g., parts are arriving too
quickly to be packed carefully), and can then make principled
modifications to its performance (e.g., temporarily stacking parts on a
table) to maintain system safety. 
(Also cross-referenced as UMIACS-TR-93-104",Circa: The Cooperatice Intelligent Real-Time Control Architecture,,,,,core
297871873,2003,The paper proposes a neural networks approach to the solution of the tracking problem for mobile robots. Neural networks based controllers are investigated in order to exploit the nonlinear approximation capabilities of the nets for modeling the kinematic behavior of the vehicle and for reducing unmodeled tracking errors contributions. The training of the nets and the control performances analysis have been done in a real experimental setup. The proposed solutions are implemented on a PC-based control architecture for the real-time control of the LabMate mobile base and are compared with classical kinematic control schemes. Experimental results are satisfactory in terms of tracking errors and computational efforts,Neural networks based control of mobile robots: Development and experimental validation,,,10.1002/rob.10110,,core
23698623,1997,"This paper explores a newly developing direction of machine learning called &quot;socially embedded learning&quot;. In this research we have been building an office-conversant mobile robot which autonomously moves around in an office environment, actively gathers information through close interaction with this environment including sensing multi-modal data and making dialog with people in the office, and acquires knowledge about the environment with which it ultimately becomes conversant. Here our major concerns are in how the close interaction between the learning system and its social environment can help or accelerate the system&apos;s learning process, and what kinds of prepared mechanisms are necessary for the emergence of such interactions. The office-conversant robot is a platform on which we implement our ideas and test their feasibility in a real-world setting. An overview of the system is given and two examples of implemented ideas, i.e. dialog-based map acquisition and route acquisition by..",Socially Embedded Learning of the Office-Conversant Mobile Robot,,,,,core
20851133,1997,"this article, we describe and develop methodologies for mod- eling and transferring human control strategy (HCS). This research has potential application in a variety of areas such as the Intelligent Vehicle Highway System (IVHS), human-machine interfacing, real-time training, space telerobotics, and agile manufacturing. We specifically address the following issues: (1) how to efficiently model human control strategy through learning cascade neural networks, (2) how to select state inputs in order to generate reliable models, (3) how to validate the computed models through an independent, Hidden Markov Model-based procedure, and (4) how to effectively transfer human control strategy. We have implemented this approach experimentally in the real-time control of a human driving simulator, and are working to transfer these methodologies for the control of an autonomous vehicle and a mobile robot. In providing a framework for abstracting computational models of human skill, we expect to facilitate analysis of human control, the development of humanlike intelligent machines, improved human-robot coordination, and the transfer of skill from one  human to anothe","Human Control Strategy: Abstraction, Verification, and Replication",,,10.1109/37.621469,,core
23922667,1997,"The quest for real autonomous robots leads us to discuss the problem about the best possible control architecture enabling that important characteristic. It has been broadly accepted that an hybrid architecture, i.e. putting together both reactive and deliberative paradigmes is needed to efficiently execute tasks in realistic dynamic environments. Our proposal, which is being implemented to control a RobuterII (from robotsoft) mobile platform, involves the use of a two-layers architecture, using symbolic representation for knowledge and goals at the deliberative level and sub-symbolic neural networks for implementing the behaviors at the reactive level. One of the main problems we are now addressing is how to make these two levels to communicate, to interat without being completly depending from each other. The Multi-agent system framework gives a flexible strategy for single agents cooperation and enables a set of behaviours to have a certain degree of autonomy. This reactive layer wo..",Robots as responsible Agents,,,,,core
100454615,2000,"This paper describes the application of reactive planning techniques for controlling the motion of a two-dimensional robot arm, given the objective of finding and touching a light source. I compare reactive planning techniques with “traditional” artificial intelligence planning techniques, which historically have been unacceptable for operation in real-world environments. I present examples of systems that are essentially a hybrid mixture of traditional and reactive techniques. Finally, I describe the implementation of pure reactive planning on a robot arm c ntrolled by a single Handy Board 68HC11-based controller. The results show that reactive planning is a viable alternative to traditional planning. Acknowledgements This project has been a long and somewhat arduous journey for me. And, considering that it will likely be sometime before I have the opportunity to make a statement like this again, assuming that I ever do, I want to recognize the people who, whether realizing it or not, have helped me along the way. First and foremost, I want to thank my dviser, Susan Fox, for her patience i",Reactive Planning in a Fully Embodied Robot Arm,,,,,core
82455252,31/10/2002,"AbstractOver recent years, the notion of agency has claimed a major role in defining the trends of modern research. Influencing a broad spectrum of disciplines such as Sociology, Psychology, among others, the agent paradigm virtually invaded every sub-field of Computer Science, not least because of the Internet and Robotics.Multi-agent Systems (MAS) are communities of problem-solving entities that can perceive and act upon their environments to achieve their individual goals as well as joint goals. The work on such systems integrates many technologies and concepts in artificial intelligence and other areas of computing. There is a full spectrum of MAS applications that have been and are being developed; from search engines to educational aids to electronic commerce and trade.Although commonly implemented by means of imperative languages, mainly for reasons of efficiency, the agent concept has recently increased its influence in the research and development of computational logic based systems.Computational Logic, by virtue of its nature both in substance and method, provides a well-defined, general, and rigorous framework for systematically studying computation, be it syntax, semantics, procedures, or attending implementations, environments, tools, and standards. Computational Logic approaches problems, and provides solutions, at a sufficient level of abstraction so that they generalise from problem domain to problem domain, afforded by the nature of its very foundation in logic, both in substance and method, which constitutes one of its major assets.The purpose of this workshop is to discuss techniques, based on computational logic, for representing, programming and reasoning about multi-agent systems in a formal way. This is clearly a major challenge for computational logic, to deal with real world issues and applications.The first workshop in this series took place in Las Cruces, New Mexico, USA, in 1999, under the designation Multi-Agent Systems in Logic Programming (MASLP'99), and affiliated with ICLP'99. In the following year, the name of the workshop changed to Computational Logic in Multi-Agent Systems (CLIMA'00), taking place in London, UK, and affiliated with CL'2000. The subsequent edition, CLIMA'01, took place in Paphos, Cyprus, affiliated with ICLP'01. The present edition, CLIMA'02, takes place in Copenhagen, Denmark, on August the1st of 2002, and is affiliated with ICLP'02 and part of FLOC'02.We would like to thank the authors of the submitted papers, the members of the program committee and the additional reviewers for their contribution to both the meeting and this volume. We would also like to thank Michael Mislove for his help with the editing of the proceedings.Programme Committee
				Jürgen Dix (The University of Manchester, UK)Thomas Eiter (Vienna University of Technology, Austria)Klaus Fischer (DFKI, Germany)Michael Fisher (University of Liverpool, UK)James Harland (Royal Melbourne Institute of Technology, Australia)Wiebe van der Hoek (Utrecht University, The Netherlands)Katsumi Inoue (Kobe University, Japan)João Alexandre Leite (New University of Lisbon, Portugal)Luís Moniz Pereira (New University of Lisbon, Portugal)Ken Satoh (National Institute of Informatics, Japan)V. S. Subrahmanian (University of Maryland, USA)Francesca Toni (Imperial College, UK)Paolo Torroni (University of Bologna, Italy)Additional Reviewers
				José AlferesKoji IwanumaAndrea SchalkTadashi AraragiGerhard LakemeyerMichael SchroederAlastair BurtWei LiuKenji TaguchiAnna CiampoliniSeng LokeHans TompitsPierangelo Dell'AcquaHidetomo NabeshimaMirosaw TruszczynskiUwe EglyNaoyuki NideMathijs de WeerdtMichael FinkInna PivkinnaMichael WinikoffChiara GhidiniFabrizio RiguzziCees WitteveenHisashi HayashiChiaki SakamaFor this edition of CLIMA, we have received 25 submissions of which 12 were selected for presentation, after a careful review process where each paper was independently reviewed by three members of the Program Committee.The workshop consisted of five sessions: four devoted to the oral presentation of the selected papers and subsequent discussion; and one devoted to a panel discussion, Paolo Torroni being the invited moderator. There follows a brief overview of the workshop.Session 1 - Agents: Arguments and UpdatesSchroeder and Schweimeier present a framework based on logic programming with 3-valued multi-agent argumentation and fuzzy unification, for knowledge representation and reasoning in agents, to accommodate arguments for negotiating agents when agent communication is subject to uncertainty.Leite et al. extend the language LUPS introducing MLUPS, an update command language designed for specifying the flexible evolution of hierarchically related groups of agents, based on logic programming, thus assigning them declarative semantics.Kakas and Moraïtis present a modular argumentation framework for modelling agent deliberation, where object level arguments can be made conditional on agents' roles and the priority relation amongst such roles can, in turn, be made conditional on contents, on top of which a simple form of abduction allows dealing with incomplete knowledge.Session 2 - Logics for AgentsToyama et al. introduce a translation of multi-agent autoepistemic logic (MAEL), a logic for multi-agent systems based on Moore's autoepistemic logic, into logic programming, and show the correspondence between MAEL extensions and the stable models of the corresponding logic program.Dell'Acqua et al. extend their previous work on abductive logic programming based multi-agent systems, in which agents can update themselves and each other, eliminate contradictory update rules, abduce hypotheses to explain observations, and use them to generate actions, with asynchronous based communication through the use of buffers.Harland and Winikoff discuss the formalisation and implementation issues of BDI-type agents, using a Linear Logic based calculus that allows a mixture of forward- and backward-chaining techniques.Session 3 - BDI Agent SystemsBordini and Moreira investigate how far the Asymmetry Thesis Principles formulated by Rao and Georgeff are actually met by the abstract agent specification language AgentSpeak(L), hence contributing to the reconciliationBetween practice and theory of BDI-based agents.Araragi et al. formalise and propose a method to solve a verification problem that arises in implementing a commitment strategy for the BDI architecture, namely the verification of the suitability and/or feasibility of the intentions of an agent.Nide et al. extend with mental state consistency features their previously presented deduction system for CTL-based propositional BDI Logics using sequent calculus, as a step towards the use of the expressive power of BDI Logics as executable specification languages of rational agents.Session 4 - Agents: Speculative Computation and IntrospectionHayashi et al. address the issue of integrating speculative computation and action execution through logic programming, namely by devising a method for plan modification when speculative computation fails or actions are executed.Iwanuma and Inoue refine the first-order consequence-finding procedure based on clausal tableaux SOL, with conditional answer computation and skip-preference, to formalise speculative computation in a master-slave multi-agent system.Bolander investigates on finding consistent classes of formulas under the syntactical treatment of knowledge and belief, identifying three maximal sets of introspective beliefs that strong introspective agents can consistently maintain so as to avoid the paradoxes of self-reference.Session 5 - Panel DiscussionTorroni moderates a panel discussion entitled ""Logics and Multi-agents: towards a new symbolic model of cognition"".This volume constitutes the proceedings of CLIMA'02.September 2002, Jürgen Dix, João Alexandre Leite and Ken Satoh (Guest Editors",Preface ,https://core.ac.uk/download/pdf/82455252.pdf,Published by Elsevier B.V.,10.1016/S1571-0661(05)80598-5,,core
100073453,2000,"This article describes a methodology for programming robots known as probabilistic robotics. The probabilistic paradigm pays tribute to the inherent uncertainty in robot perception, relying on explicit representations of uncertainty when determining what to do. This article surveys some of the progress in the field, using in-depth examples to illustrate some of the nuts and bolts of the basic approach. My central conjecture is that the probabilistic approach to robotics scales better to complex real-world applications than approaches that ignore a robot’s uncertainty. Building autonomous robots is a centralobjective of research in AI. Over the pastdecades, researchers in AI have developed a range of methodologies for developing robotic software, ranging from model-based to pure",Probabilistic algorithms in robotics,,,,,core
23704733,1998,"A simulated hockey environmentisintroduced as a  test bed for studying adaptive behavior and evolution  of robot controllers. A near-frictionless playing surface  is employed, partially mimicking zero gravity conditions.  We showhow a neural network using a simple  evolutionary algorithm can develop nimble strategies  for moving about the rink and scoring goals quickly  and e#ectively.  1. Introduction  In recentyears, there has been growing interest in using machine learning and evolutionary techniques for developing software controllers that play competitive or cooperative games in some kind of physical environment, either real or simulated. These have included: avoiding obstacles #Xiao et al., 1997; Lee et al., 1996; Brooks, 1986#, foraging for #food&quot; #Werger and Mataric, 1996#, playing pursuit#evasion games #Miller and Cli#, 1994#, discriminating objects #Beer, 1996#, #ghting for control of a cube #Sims, 1995# and RoboCup soccer #Kitano et al., 1995#. Following in these traditions,..",The Evolution of Subtle Manoeuvres in Simulated Hockey,,,,,core
20682132,2003,"It is difficult to apply traditional reinforcement learning algorithms to robots, due to problems with large and continuous domains, partial observability, and limited numbers of learning experiences. This paper deals with these problems by combining: 1. reinforcement learning with memory, implemented using an LSTM recurrent neural network whose inputs are discrete events extracted from raw inputs; 2. online exploration and offline policy learning. An experiment with a real robot demonstrates the methodology&apos;s feasibility",A Robot that Reinforcement-Learns to Identify and Memorize Important Previous Observations,,,,,core
20712115,1997,"This paper explores a newly developing direction of machine learning called &apos;&apos;socially embedded learning&amp;quot;. In this research we have been building an office-conversant mobile robot which autonomously moves around in an office environment, actively gathers information through close interaction with this environment including sensing multi-modal data and making dialog with people in the office, and acquires knowledge about the environment with which it ultimately becomes conversant. Here our major concerns are in how the close interaction between the learning system and its social environment can help or accelerate the systems learning process, and what kinds of prepared mechanisms are necessary for the emergence of such interactions. The office-conversant robot is a platform on which we implement our ideas and test their feasibility in a real-world setting. An overview of the system is given and two examples of implemented ideas, i.e. dialog-based map acquisition and route acquisition by following, are described in detail",Socially embedded learning of the office-conversant mobile robot jijo-2,,,,,core
24292610,1998,". Evolutionary robotics--- as other adaptive methods, such as reinforcement learning and learning classifier systems---can take considerable time and resources which require a careful evaluation of the hardware tools and methodologies employed. We outline a set of hardware solutions and working methodologies that can be used for successfully implementing and extending the evolutionary approach to complex environments, robots, and real-world applications. The issues discussed include the integration of simulation and real robots, design issues of evolvable robots, hardware requirements for incremental evolution, and hardware and software tools for monitoring and analysis. 1 Introduction  Evolutionary techniques applied to robot control can generate efficient, smart, and creative solutions which match the constraints imposed by the environment and the selection criterion. The power, flexibility, and generality of artificial evolution has often been exploited both for finding engineering ..",Hardware Solutions for Evolutionary Robotics,,Springer Verlag,10.1007/3-540-64957-3_69,,core
24293782,1997,"In this paper we describe the benefits of vision for autonomous vehicles in a concrete real-world setup. The autonomous vehicles, implemented in the form of small robots, have to face two basic tasks. First, they have to do autonomous recharging. Second, they are required to do some &quot;work&quot; which is &quot;paid&quot; in energy. We present a way to let the robots solve these tasks with basic sensors. In doing so, we focus on navigation as crucial problem. Then, vision is introduced. We argue for using the active vision framework and present an implementation on our robots.  INTRODUCTION  At the VUB AI-lab we are working with several autonomous robotic vehicles in a special experimental set-up. This so-called ecosystem is inspired by biology [McFarland, 1994] and has been successfully implemented and used in Artificial Intelligence research [Steels, 1994; McFarland and Steels, 1995; Birk, 1996; Steels, 1996a; Steels, 1996b]. Apart from the basic research issues involved in this previous and ongoing ..",On The Watch,,ISATA Press,,,core
101988193,2002,"The Problem: Reinforcement-learning (RL) [1] techniques have elegant theoretical foundations and have proven useful in learning a variety of off-line tasks, such as playing Backgammon. However, they are still much too inefficient to be of use in the majority of robot-learning domains. In this project, we seek to make reinforcement learning effective for real robots. We are particularly interested in domains with continuous sensory inputs and continuous actions, and require that learning take place online from a relatively small amount of experience. Motivation: In order to deploy robots in a wide variety of applications, from household to military, we must find a way to “program ” them efficiently. Direct programming by humans is a tedious process, requiring a large amount of trial-and-error debugging on the part of the human. In addition, such hand-built programs are only suited for a single domain, and must be re-engineered for new houses or military situations. Thus, behavior learning must play a major role in the wide deployment of robots. Previous Work: There has been work on RL for real robots that takes good advantage of particular properties of the application domain. Schaal and Atkeson [3] built a juggling robot that assumed continuous deterministic world dynamics. Moore developed a system that learned to control a factory packaging machine that took advantage of very slow variation in the process [1]. Approach: Our first step will be to use nearest neighbor (and locally weighted regression) [2] as a function approx",Making Reinforcement Learning Work on Real Robots,,,,,core
11778430,2000-09-24T00:00:00,"Designing and developing software for autonomous robot control system is a challenging task. Issues related to real-time control, embedded system and artificial intelligence are involved in the software development process. This type of software must be developed with proper software methodology or well-defined development process in order to increase the productivity and quality of the software design and software products. This paper presents a review of two real-time software development methodologies and compares their suitability for developing real-time control software for a wall-climbing robot under development at Universiti Teknologi Malaysia (UTM",A review of real-time software engineering methodologies for developing a wall-climbing robot control firmware,https://core.ac.uk/download/11778430.pdf,,,,core
21688919,2001,":  Automation of the search for and classification of Antarctic  meteorites offers a unique case for early demonstration of  robotics in a scenario analogous to geological exploratory  missions to other planets and to the Earth&apos;s extremes. Moreover,  the discovery of new meteorite samples is of great value because  meteorites are the only significant source of extraterrestrial  material available to scientists. In this paper we focus on the  primary outcomes and technical lessons learned from the first  field demonstration of autonomous search and in situ  classification of Antarctic meteorites by a robot. Using a novel  autonomous control architecture, specialized science sensing,  combined manipulation and visual servoing, and Bayesian  classification, the Nomad robot classified five indigenous  meteorites during an expedition to the remote site of Elephant  Moraine in January 2000. Nomad&apos;s expedition proved the  rudiments of science autonomy and exemplified the merits of  machine learning techniques for autonomous geological  classification in real-world settings. On the other hand, the  expedition showcased the difficulty in executing reliable robotic  deployment of science sensors and a limited performance in the  speed and coverage of autonomous search.  Keywords: Robotic Meteorite Search, Science Autonomy.  ",Robotic Antarctic Meteorite Search: Outcomes,,,,,core
100964389,1997,"Reactive controllers has been widely used in mobile robots since they are able to achieve suc-cessful performance in real-time. However, the configuration of a reactive controller depends highly on the operating conditions of the robot and the environment; thus, a reactive controller configured for one class of environments may not perform adequately in another. This paper presents a formulation of learning adaptive reactive controllers. Adaptive reactive controllers inherit all the advantages of traditional reactive controllers, but in addition they are able to ad-just themselves to the current operating conditions of the robot and the environment in order to improve task performance. Furthermore, learning adaptive reactive controllers can learn when and how to adapt the reactive controller so as to achieve effective performance under different conditions. The paper presents an algorithm for a learning adaptive reactive controller that combines ideas from case-based reasoning and reinforcement learning to construct a mapping between the operating conditions of a controller and the appropriate controller configuration; this mapping is in turn used to adapt the controller configuration dynamically. As a case study, the algorithm is implemented in a robotic navigation system that controls a Dennin",Learning adaptive reactive controllers,,,,,core
11778841,2000-09-24T00:00:00,"Designing and developing software for autonomous robot control system is a challenging task. Issues related to real-time control, embedded system and artificial intelligence are involved in the software development process. This type of software must be developed with proper software methodology or well-defined development process in order to increase the productivity and quality of the software design and software products. This paper presents a review of two real-time software development methodologies and compares their suitability for developing real-time control software for a wall-climbing robot under development at Universiti Teknologi Malaysia (UTM",A review of real-time software engineering methodologies for developing a wall-climbing robot control firmware,https://core.ac.uk/download/11778841.pdf,'Institute of Electrical and Electronics Engineers (IEEE)',,,core
54355579,2004-01-01T00:00:00,"The activities of search and rescue of victims in large-scale disasters are not only highly relevant social Problems, but pose several challenges from a scientific standpoint. In this context, the RoboCup-Rescue project focused on the problems of bringing aids immediately after a large disaster, and aims at creating system based on AI and Robotics technologies, where heterogeneous agents (software, robots, human beings) interact in a cooperative manner. In this paper we present the achievements of a research project, based on the RoboCup Rescue simulator, carried out in Italy in collaboration with the Italian Fire Department. The overall project goal is to devise tools to allow monitoring and supporting decisions which are needed in a real-time rescue operation in a large scale disaster, and to provide a methodology for evaluation of multi-agent system which considers not only the efficiency of a system, but also its robustness when conditions in the environment change, as well as other features, such as the ability to acquire a precise and coherent representation of the disaster scenario",RoboCup Rescue simulation: Methodologies tools and evaluation for practical applications,,'Springer Science and Business Media LLC',10.1007/978-3-540-25940-4_62,,core
10698277,1998,"Hybrid agent architectures comprise the radical change of paradigms in AI over the past decades by reconciling the different styles of reactive, deliberative, even social systems. They have been successfully applied to a range of complex real-world domains. Due to their originally informal background, a verification of design goals in derived implementations, theoretical foundations, and a detailed comparison with other agent models have not yet been obvious. The present work proposes a formal methodology to bridge the gap between theoretical and practical aspects especially of hybrid designs, such as the layered INTERRAP. The employed, connected stages of specification, i.e., architecture, computational model, theory, proof calculus, and implementation, also provide a yet unique framework for comparing heterogeneous agent models including unified and logic-based ones. Based on recent work on INTERRAP, we demonstrate that this methodology allows to compare state-of-the-art designs from robotics, AI, computer science, and cognitive science with respect to a spectrum of inherent properties along the two dimensions of abstraction and declarativity. This supports our claim that INTERRAP is a coherent and advanced account of layered agency including goal-oriented abstraction planning in on-line interaction with reactive skills and social reasoning. We also derive particular research issues to guide the future development of INTERRAP",Methodological comparison of agent models,https://core.ac.uk/download/pdf/10698277.pdf,Sonstige Einrichtungen. DFKI Deutsches Forschungszentrum für Künstliche Intelligenz,,,core
22284296,1997,"We have recently introduced a neural network for reactive obstacle avoidance based on a model of classical and operant conditioning. In this article we describe the success of this model when implemented on two real autonomous robots. Our results show the promise of self-organizing neural networks in the domain of intelligent robotics.  Introduction  The study of autonomous behavior has become an active research area in the field of robotics. Even the simplest organisms are capable of behavioral feats unimaginable for the most sophisticated machines. One especially impressive aspect of animal behavior is the facility with which simple and complex organisms learn to interact successfully with their environment as they gather food, seek mates, and avoid predators and other natural hazards. When an animal has to operate in an unknown environment it must somehow learn to predict the consequences of its own actions. By learning the causality of environmental events, it becomes possible for ..",Neural Competitive Maps for Reactive and Adaptive Navigation,,,,,core
24717775,2003,"The idea of artificially intelligent agents hearkens back to the first science fiction stories of androids, robotic people who could interact with humans and with each other. Today we have separated the tasks of building physical robots from designing their software, but the basic idea remains the same. A synthetic agent is an embodied character who acts independently and interactively in a believable fashion. Of course, what is believable depends on the observer and the agent. For a synthetic human agent, a human observer would be expected to impose his or her own models of human personality and emotion. The believability of such an agent then depends on how closely its behaviour and expression match with the observer’s expectations. Given the goal of “build a believable synthetic agent, ” how might one go about it? The answer for many researchers has been to design emotional agent systems that give synthetic agents human-like, emotional behaviour. In a discussion of emotional computing, Gratch notes that emotions “have received the most attention for their strength in creating engaging and believable characters... emotional reasoning can play a key role in educational tools” [Gra00]. Emotional, believable synthetic agents have applications in education, entertainment (both passive and active), virtual reality training scenarios, and user interfaces. This research field follows the accepted current AI methodology of situated agency. Under this view, a computational agent perceives and acts within some environment. Its decisions for action are based on its perception and a computational model [RN95, 31–49]. The complexity of the agent computational model depends on the complexity of the environment and desired agent performance within that environment. Emotional agent systems are based on a more complex computational model than other agents, since the environment can be complex and the performance metric (“believability ” according to a human observer) is very demanding. Most emotional agent systems include four components: a method for interpreting stimuli (input), be it internal or external; a computational model of emotions that determines how emotions are generated and managed; a way to direct agent behaviour and actions, informed by emotional state; and a method for expressing emotional state to the world (output). Som",Summary Emotional Modeling with Synthetic Agents,,,,,core
71166550,11/03/1999,"Recent attention has been given to the deployment of an adaptable sensor array realized by multi-robotic systems. Our group has been studying the collective behavior of autonomous, multi-agent systems and their applications in the area of remote-sensing and emerging threats. To accomplish such tasks, an interdisciplinary research effort at Sandia National Laboratories are conducting tests in the fields of sensor technology, robotics, and multi-robotic and multi-agents architectures. Our goal is to coordinate a constellation of point sensors that optimizes spatial coverage and multivariate signal analysis using unmanned robotic vehicles (e.g., RATLERs, Robotic All-ten-sin Lunar Exploration Rover-class vehicles). Overall design methodology is to evolve complex collective behaviors realized through simple interaction (kinetic) physics and artificial intelligence to enable real-time operational responses to emerging threats. This paper focuses on our recent work understanding the dynamics of many-body systems using the physics-based hydrodynamic model of lattice gas automata. Three design features are investigated. One, for single-speed robots, a hexagonal nearest-neighbor interaction topology is necessary to preserve standard hydrodynamic flow. Two, adaptability, defined by the swarm's deformation rate, can be controlled through the hydrodynamic viscosity term, which, in turn, is defined by the local robotic interaction rules. Three, due to the inherent non-linearity of the dynamical equations describing large ensembles, development of stability criteria ensuring convergence to equilibrium states is developed by scaling information flow rates relative to a swarm's hydrodynamic flow rate. An initial test case simulates a swarm of twenty-five robots that maneuvers past an obstacle while following a moving target. A genetic algorithm optimizes applied nearest-neighbor forces in each of five spatial regions distributed over the simulation domain. Armed with knowledge, the swarm adapts by changing state in order to avoid the obstacle. Simulation results are qualitatively similar to lattice gas",Dynamical Behavior of Multi-Robot Systems Using Lattice Gas Automata,,Sandia National Laboratories,,,core
23843980,1998,"Soccer meets the requirements of the Situated Agent approach and as a task domain is suflciently rich to support research integrating many branches of robotics and AI. A robot is an integrated system, with a controller embedded in its plant. A robotic system is the coupling of a robot to its environment. Robotic systems are, in general, hybrid dynamic systems, consisting of continuous, discrete and event-driven components. Constraint Nets provide a semantic model for modeling hybrid dynamic systems. Controllers are embedded constraint solvers that solve constraints in real-time. A controller for our new softbot soccer team, UBC Dynamo98, has been modeled in Constraint Nets, and implemented in Java, using the Java Beans architecture. The paper demonstrates that the formal Constraint Net approach is a practical tool for designing and implementing controllers for robots in multi-agent real-time environments",A constraint-based controller for soccer-playing robots,,,,,core
144001529,2003-08-13T00:00:00,"Este trabalho trata do estudo de concepções de arquitetura do controle aplicadas aos robôs móveis autônomos e da proposição de um delas à instrumentação e controle em tempo real de um modelo de embarcação naval de alto desempenho. Tal veículo remotamente operado foi desenvolvido como parte das atividades do projeto temático ""Comportamento em Ondas de Embarcações de Alto Desempenho"" (proc.Fapesp 1997/13090-3). Realizou-se uma investigação dos diversos paradigmas de inteligência artificial que orientaram a evolução dos robôs móveis autônomos até o presente momento e, em particular, as concepções baseadas em modelos sócio-antropológicos e computacionais (teoria de agentes e orientação a objetos) através de sua aplicação à implementação de um sistema de aquisição e controle orientado a objetos, modelado através da UML (Unified Modeling Language), para o veículo mencionado. Testes de validação da arquitetura do controle foram realizados, sendo obtidos resultados experimentais que permitiram análises a respeito da dinâmica, manobrabilidade e navegação do veículo, as quais sugerem vários aperfeiçoamentos para o sistema de hardware e software em trabalhos futuros.This work deals with the study of control architecture approaches applied to autonomous mobile robots, and proposes one of them for the control system of a self-propelled high speed ship model. Such unmanned vehicle was developed for the research project Comportamento em Ondas de Embarcações de Alto Desempenho"" (proc. FAPESP 1997/13090-3). A number of artificial intelligence paradigms, related to the autonomous robot evolution up to now, were investigated. Models based on the socio-anthropological paradigm and the corresponding computer science approaches, i.e. agent theory and object-oriented modeling, were emphasized. Object-oriented control software based on the UML (Unified Modeling Language) was designed for the real-time embedded system of the ship model. Validation tests of the control architecture were carried out. Experimental results, related to vehicle dynamics, maneuverability and navigation were acquired by the embedded system and analyzed in this work. These results suggest a number of improvements for future works on the software and hardware systems",Development of a control architecture based on objects for an aquatic mobile robot.,,"'Universidade de Sao Paulo, Agencia USP de Gestao da Informacao Academica (AGUIA)'",10.11606/D.3.2003.tde-31072003-153011,,core
23704893,1997,"This paper reviews key concepts of the Autonomous Robot Architecture (AuRA). Its structure, strengths, and roots in biology are presented. AuRA is a hybrid deliberative/reactive robotic architecture that has been developed and refined over the past decade. In this article, particular focus is placed on the reactive behavioral component of this hybrid architecture. Various real world robots that have been implemented using this architectural paradigm are discussed, including a case study of a multiagent robotic team that competed and won the 1994 AAAI Mobile Robot Competition. 1 Introduction The Autonomous Robot Architecture (AuRA) was developed in the mid-1980&apos;s as a hybrid approach to robotic navigation [6]. Hybridization arises from the presence of two distinct components: a deliberative or hierarchical planner, based on traditional artificial intelligence techniques; and a reactive controller, based upon schema theory [2]. It was the first robot navigational system to be presented ..",AuRA: Principles and Practice in Review,,,,,core
24026188,2000,". This paper deals with the the subject of learning and planning for  real mobile robots, using Sutton&apos;s Dyna algorithm. The Dyna algorithm integrates  reinforcement learning, planning and reactive execution. In this paper we present  an extension of the Dyna algorithm which includes symmetric and cooperative  learning with multiple robots. We applied the extended version of the algorithm to a  population of two real robots. Practical problems associated with the implementation  of the algorithm on a real setup are solved. Results obtained from simulations and  real experiments are presented and discussed.  Key words: Reinforcement Learning, Cooperative Robotics, Visual Tracking, Dyna.  1 INTRODUCTION  The control of a robotic system, especially the planning of tasks to be performed by the robot to achieve a goal, is in most cases a very complex problem. Several methods have been developed to address this problem, one of which is reinforcement learning [5]. This method reveals great si..",Cooperative Learning And Planning For Multiple Robots,,,,,core
21674548,2004,"learning and controller adaptation will be an essential component for legged robots in the next few years as they begin to leave the laboratory setting and join our world. I present the first example of a learning system which is able to quickly and reliably acquire a robust feedback control policy for 3D dynamic bipedal walking from a blank slate using only trials implemented on the physical robot. The robot begins walking within a minute and learning converges in approximately 20 minutes. The learning works quickly enough that the robot is able to continually adapt to the terrain as it walks. This success can be attributed in part to the mechanics of our robot, which is capable of stable walking down a small ramp even when the computer is turned off. In this thesis, I analyze the dynamics of passive dynamic walking, starting with reduced planar models and working up to experiments on our real robot. I describe, in detail, the actor-critic reinforcement learning algorithm that is implemented on the return map dynamics of the biped. Finally, I address issues of scaling and controller augmentatio",Dynamically Stable Legged Locomotion,,,,,core
15979411,1999-01-01T00:00:00,"Loffler A, Klahold J, Rückert U. Artificial neural networks for autonomous robot control: reflective navigation and adaptive sensor calibration. In:  Neural Information Processing, 1999. Proceedings. ICONIP '99. 6th International Conference on. Vol 2. IEEE;  1999: 667-672.In this paper, we present the application of artificial
neural networks to the control of a mobile, autonomous
robot, which is acting in a totally unknown and
- most importantly - dynamically changing environment.
In particular, the employment of interacting
'simple', i.e. hand-designed, neural networks for navigation
purposes is investigated as well as a variation
of self-organizing maps for adaptive sensor calibration.
We insofar take a pragmatic point of view as the
minimal condition imposed on the developed algorithms
is that they do well on a real system acting in a
real environment. Hence, the design of all of the
implemented neural networks is clearly motivated by
their applicability. In this context, special considerations
are dedicated to ensure robustness, real-time
capability and memory resourcefulness. In order to
practically demonstrate the obtained results, the minirobot
Khepera is utilized as an experimentatory platform,
which is - due to its small size - a versatile tool
for scientific investigation",Artificial neural networks for autonomous robot control: reflective navigation and adaptive sensor calibration,,'Institute of Electrical and Electronics Engineers (IEEE)',10.1109/ICONIP.1999.845675,,core
24267836,1999,"Robotic soccer is a challenging research domain because problems in robotics, artificial intelligence, multi-agent systems and real-time reasoning have to be solved in order to create a successful team of robotic soccer players. In this paper, we describe the key components of the CS Freiburg team. We focus on the self-localization and object recognition method based on using laser range finders and the integration of all this information into a global world model. Using the explicit model of the environment built by these components, we have implemented path planning, simple ball handling skills and basic multi-agent cooperation. The resulting system is a very successful robotic soccer team, which has not lost any official game yet","Reliable Self-Localization, Multirobot Sensor Integration, Accurate Path-Planning and Basic Soccer Skills: Playing an Effective Game of Robotic Soccer",,,,,core
23918061,1998,"Physical environments are so complex that it  is hard to hand-tune all of the domain knowledge,  especially to model the dynamics of the  environment. The work presented in this paper  explores machine learning techniques to autonomously  identify situations in the environment  that affect plan quality. We introduce the  concept of situation-dependent costs, where situational  features can be attached to the costs used  by the path planner. These costs effectively diagnose  and predict situations the robot encounters  so that the planner can generate paths that are  appropriate for each situation.  We present an implementation of our situationdependent  learning approach in a real robotic system,   Rogue. Rogue learns situation-dependent  costs for arcs in a topological map of the environment;  these costs are then used by the path  planner to predict and avoid failures. In this article,  we present the representation of the path  planner and the navigation modules, and describe  the e..",Learning Situation-Dependent Costs: Using Execution to Refine Planning Models,,,,,core
20901058,2005,"This paper describes the framework of a real-time simulation system to model human behaviour and reactions in dangerous environments. The system utilizes the latest 3D computer animation techniques, combined with artificial intelligence, robotics and psychology, to model human behaviour, reactions and decision making under expected/unexpected dangers in real-time in virtual environments. The development of the system includes: classification on the conscious/subconscious behaviours and reactions of different people; capturing different motion postures by the Eagle Digital System; establishing 3D character animation models; establishing 3D models for the scene; planning the scenario and the contents; and programming within Virtools TM Dev. Programming within Virtools TM Dev. is subdivided into modelling dangerous events, modelling character’s perceptions, modelling character’s decision making, modelling character’s movements, modelling character’s interaction with environment and setting up the virtual cameras. The real-time simulation of human reactions in hazardous environments is invaluable in military defense, fire escape, rescue operation planning, traffic safety studies, and safety planning in chemical factories, the design of buildings, airplanes, ships and trains. Currently, human motion modelling can be realized through established technology, whereas to integrate perception and intelligence into virtual human’s motion is still a huge undertaking. The challenges here are the synchronization of motion and intelligence, the accurate modelling of human’s vision, smell, touch and hearing, the diversity and effects of emotion and personality in decision making. There are three types of software platforms which could be employed to realize the motion and intelligence within one system, and their advantages and disadvantages are discussed","Y.Zhao, “Modeling Human Behaviours and Reactions Under Dangerous Environment",,,,,core
71191582,01/06/1998,"The main paradigm in sub-symbolic learning robot domain is the reinforcement learning method. Various techniques have been developed to deal with the memorization/generalization problem, demonstrating the superior ability of artificial neural network implementations. In this paper, the authors address the issue of designing the reinforcement so as to optimize the exploration part of the learning. They also present and summarize works relative to the use of bias intended to achieve the effective synthesis of the desired behavior. Demonstrative experiments involving a self-organizing map implementation of the Q-learning and real mobile robots (Nomad 200 and Khepera) in a task of obstacle avoidance behavior synthesis are described. 3 figs., 5 tabs",Reinforcement function design and bias for efficient learning in mobile robots,,Oak Ridge National Laboratory,,,core
210554045,2005-01-01T08:00:00,"Techniques of realizing CMOS continuous-time analog recurrent neural networks are studied in this paper. We discuss the application of an analogue recurrent neural network to learn and track the dynamics of an industrial robot. The observations made from this study suggest that RNNs (similar to those in Fig. 1) can be applied to the control of real systems that manifest complex properties - specifically, high-dimensionality, non-linearity and requiring continuous action. Examples of these real systems include aircraft control, satellite stabilization, and robot manipulator control. We conclude that robust controllers of partially observable (non-Markov). systems require real-time electronic systems that can be designed as single-chip Integrated Circuits (CMOS IC). This paper explored such techniques and identified suitable circuits. The synaptic weights are modeled as variable gain cells that can be implemented with a few MOS transistors. For the specific purpose of demonstrating the trajectory learning capabilities, a periodic signal with varying characteristics is used. The developed architecture, however, allows for more general learning tasks typical in applications of identification and control. On-line versions of the synaptic update can be formulated using simple CMOS circuits. The simulated network contains interconnected recurrent neurons ·with continuous-time dynamics. The system emulates random-direction descent of the error as a multidimensional extension to the stochastic approximation. To achieve unsupervised learning in recurrent dynamical systems we propose a synapse circuit which has a very simple structure and is suitable for implementation in VLSI",Analog Recurrent networks for signal tracking and control applications,,"Edith Cowan University, Research Online, Perth, Western Australia",,,core
357282770,1999-01-01T00:00:00,"ABSTRACT Measurement and signal intelligence (MASINT) of the battlespace has created new requirements in information management, communication and interoperability as they effect surveillance and situational awareness. In many situations, stand-off remote sensing and hazard-interdiction techniques over realistic operational areas are often impractical and difficult to characterize. An alternative approach is to implement adaptive remote-sensing techniques with swarms of mobile agents employing collective behavior for optimization of mapping signatures and positional orientation (registration). We have expanded intelligent control theory using physics-based collective behavior models and genetic algorithms to produce a uniquely powerfid implementation of distributed ground-based measurement incorporating both local collective behavior, and inter-operative global optimization for sensor fusion and mission oversight. By using a layered hierarchical control architecture to orchestrate adaptive reconilguration of semiautonomous robotic agents, we can improve overall robustness and functionality in dynamic tactical environments without information bottlenecking. In our concept, each sensor is equipped with a miniaturized optical reflectance modulator which is interactively monitored as a remote transponder using a laser communication protocol from a remotemothership or operative. Robotdata-sharing at the groundlevelcanbe leveragedwithglobalevaluation criteria, including terrain overlays and remote imaging data. Information sharing and distributed intelligence opens up a new class of remote sensing applications in which small single-function autonomous observers at the local level can collectively optimize and measure large scale ground-level signatures. As the need for coverage and the number of agents grows to improve spatial resolution, cooperative behavior orchestrated by a global situational awareness umbrella will be an essential ingredient to offset increasing bandwidth requirements within the net. A system of this type is being developed which will be capable of sensitively detecting, tracking, and mapping spatial distributions of measurement signatures, which are nonstationary or obscured by clutter or interfering obstacles by virtue of adaptive reconfiguration. This methodology is being used to field an adaptive ground-penetrating impulse radar from a superposition of small radiating dipoles for detection of underground structures and to detecthemediate hazardous biological or chemical species in migrating plumes. This paper focuses on our recent work at Sandia National Laboratories toward engineering a physics-based swarm of mobile vehicles for distributed sensing applications. Our goal is to coordinate a sensor array that optimizes sensor coverage and multivariate signal analysis by implementing artificial intelligence and evolutionary computational techniques. These intelligent control systems integrate both globally operating decision-making systems and locally cooperative information-sharing modes using genetically-trained neural nehvorks. Once trained, neural networks have the ability to enhance real-time operational responses to dynamical environments, such as obstacle avoidance, responding to prevailing wind patterns, and overcoming other natural obscurants or interferences (jammers). The swarm realizes a collective set of sensor neurons with simple properties incorporating interactions based on basic community rules (potential fields) and complex interconnecting functions based on various neural network architectures, Therefore, the swarm is capable of redundant heterogeneous measurements which furnishes an additional degreeof robustnessand fault tolerancenot affordedby conventional systems,whileaccomplishing such cognitive tasks as generalization, error correction, pattern recognition, and sensor fision. The robotic platforms could be equipped with specialized sensor devices including transmitkeceive dipole antennas, chemical or biological &quot;sniffers&quot; in combination with recognition analysis tools, communication modulators, and laser diodes. Our group has been studying the collective behavior of an autonomous, multi-agent system applied to emerging threat applications. To accomplish such tasks, research in the fields of robotics, sensor technology, and swarms are being conducted within an integrated program. Mission scenarios under consideration include ground penetrating impulse radar (GPR) for detection of under-ground structures, airborne systems, and plume detectionh-emediation. We will describe our research in these areas and give a status report on our progress, including simulations and laboratory-based sensor experiments",Adaptive Remote-Sensing Techniques Implementing Swarms of Mobile Agents,,,,,core
21545849,2001,"This paper presents the design, implementation and testing of a real-time system using computer vision and machine learning techniques to demonstrate learning behavior in a miniature mobile robot. The miniature robot, through environmental sensing, learns to navigate a maze choosing the optimum route. Several reinforcement learning based algorithms, such as Q-learning, Q()-learning , fast online Q()-learning and DYNA structure, are considered",Real-Time Robot Learning,,,,,core
21686484,2001,"Symbols are used everywhere to help us nd our way and they provide useful information about our world. Autonomous  robots that would operate in real life settings could surely benet from these indications. Researchoncharacter recognition have been going on for quite some time now, and demonstrations have been made of machines that can read printed and handwritten characters. To give an autonomous robot the ability to read symbols, we need to integrate character recognition techniques with methods to position the robot in front of the symbol, to capture the image that will be used in the identication process, and to validate the overall system on a robot. Our goal is not to develop new character recognition methods, but to address the dierent aspects required in making a mobile robotic platform, using current hardware and software capabilities, recognize symbols placed in real world environment. Validated on a Pioneer 2 robot, the approach described in this paper uses colors to detect symbols, a PID controller to position the camera, simple heuristics to select image regions that could contain symbols, and nally a neural system for symbol identication. Results in dierent lighting conditions are described, along with the use of our approach by our robot entry to the AAAI&apos;2000 Mobile Robot Challenge, making the robot attend the National Conference on AI.  1",Teaching a Robot How to Read Symbols,,,10.1145/375735.376096,,core
20676422,2002,"This paper presents an investigation of flocking, the formation and maintenance of coherent group movement, by teams of autonomous mobile robots using principles of Swarm Intelligence. First, we present a simple flocking task, and we describe a leaderless distributed flocking algorithm (LD) that is more conducive to implementation on embodied agents than the established algorithms used in computer animation. Next, we use an embodied simulator and reinforcement learning techniques to optimize LD performance under different conditions, showing that this method can be used not only to improve performance but also to gain insight into which algorithm components contribute most to system behavior. Finally, we demonstrate that a group of real robots executing LD with emulated sensors can successfully flock (even in the presence of individual agent failure) and that systematic characterization (and therefore optimization) of real robot flocking parameters is achievable",Self-Organized Flocking with Agent Failure: Off-Line Optimization and Demonstration with Real Robots,,,,,core
23579133,1998,". The control of mobile robots acting autonomously in the real world is one of the long-term goals of the field of artificial intelligence. So far the field lacks methods bridging the gap between the sophisticated symbolic techniques to represent and reason about action and more and more reliable low-level robot control and navigation systems. In this paper we present GOLEX, an execution and monitoring system for the logic-based action language GOLOG and the complex and distributed RHINO control software which operates on RWI B21 and B14 mobile robots. GOLEX provides the following features: it maps abstract primitive actions into low-level commands of the robot control system, thus allowing the user to concentrate on the application rather than the inner workings of the robot; it monitors the execution of the primitive GOLOG actions, making it possible to detect simple execution failures and timeouts; and it includes means to deal with sensing and user input and to continue the operati..",GOLEX - Bridging the Gap between Logic (GOLOG) and a Real Robot,,,10.1007/bfb0095437,,core
20868026,2004,"The localisation is not a trivial problem of mobile robotics. The robot must localise itself, having merely a map of the environment and sensor readings. This is a base knowledge that robot must possess in order to be able to complete higher level tasks like path planning. The Monte Carlo Localisation (MCL) is a probabilistic algorithm successfully applied in many mobile robot systems. The core of the system that implements the MCL algorithm is the “T1000 ” Java-written robot application platform which cooperates with the “Saphira ” robot control system developed at SRI International’s Artificial Intelligence Center. T1000 is a client application communicating via TCP/IP network with a Windows/Linux server for Saphira, what enables running and testing several robot applications at the same time. Thanks to affine transformation of robot’s sensor measurements, the algorithm does not have to be synchronised with incoming sensor data, and allows processing several measurements at a time. Moreover, they can be processed at any convenient time, what simplifies using any preselection algorithm. In addition, the improved particles ’ initialisation can also reduce the number of particles without decreasing the effectiveness of the basic MCL algorithm. Even though the system was not tested yet on real robots, the experiments with the “Pioneer” robot simulator are promising, and it seems that the system is comparable to the original MCL software attached to Saphira",Mini-Project II Monte Carlo Localisation,,,,,core
24324529,1998,"A simulated hockey environment is introduced as a test bed for studying adaptive behavior and evolution of robot controllers. A near-frictionless playing surface is employed, partially mimicking zero gravity conditions. We show how a neural network using a simple evolutionary algorithm can develop nimble strategies for moving about the rink and scoring goals quickly and effectively.  1. Introduction  In recent years, there has been growing interest in using machine learning and evolutionary techniques for developing software controllers that play competitive or cooperative games in some kind of physical environment, either real or simulated. These have included: avoiding obstacles (Xiao et al., 1997; Lee et al., 1996; Brooks, 1986), foraging for &quot;food&quot; (Werger &amp; Mataric, 1996), playing pursuit/evasion games (Miller &amp; Cliff, 1994), discriminating objects (Beer, 1996), fighting for control of a cube (Sims, 1995) and RoboCup soccer (Kitano et al., 1995). Following in these traditions, we ..",The Evolution of Subtle Manoeuvres in Simulated Hockey,,,,,core
24545325,2003,"This thesis advocates the usefulness and practicality of a logic-based approach to AI and in particular to high-level control of mobile robots. The contribution of the research work reported here is twofold: 1) the development of theoretical frameworks that account for uncertainty and unmodeled dynamics in an environment where an acting agent has to achieve certain goals and 2) the implementation of the developed ideas on a mobile robot. We have elaborated the approach to designing efficient and reliable controllers in Golog following two different perspectives on the environment where the control program is supposed to operate. According to one perspective, investigated in Chapter 4, the agent has a logical model of the world, but there is no probabilistic information about the environment where the agent is planning to act, and the agent is not capable or has no time for acquiring probabilities of different effects of its actions. In this case, the uncertainty and dynamics of the environment can be accounted only by observing the real outcomes of actions executed by the agent, by determining possible discrepancies between the observed outcomes and the effects expected according to the logical model of the world and then by recovering, if necessary, from th",High-Level Robot Programming in Dynamic and Incompletely Known Environments,,,,,core
4396235,2004-06-16T00:00:00,"Most Artificial Intelligence (AI) work can be characterized as either ``high-level'' (e.g., logical, symbolic) or ``low-level'' (e.g., connectionist networks, behavior-based robotics). Each approach suffers from particular drawbacks. High-level AI uses abstractions that often have no relation to the way real, biological brains work. Low-level AI, on the other hand, tends to lack the powerful abstractions that are needed to express complex structures and relationships. I have tried to combine the best features of both approaches, by building a set of programming abstractions defined in terms of simple, biologically plausible components. At the ``ground level'', I define a primitive, perceptron-like computational unit. I then show how more abstract computational units may be implemented in terms of the primitive units, and show the utility of the abstract units in sample networks. The new units make it possible to build networks using concepts such as long-term memories, short-term memories, and frames. As a demonstration of these abstractions, I have implemented a simulator for ``creatures'' controlled by a network of abstract units. The creatures exist in a simple 2D world, and exhibit behaviors such as catching mobile prey and sorting colored blocks into matching boxes. This program demonstrates that it is possible to build systems that can interact effectively with a dynamic physical environment, yet use symbolic representations to control aspects of their behavior",Building Grounded Abstractions for Artificial Intelligence Programming,,,,,core
4964599,2002-01-01T08:00:00,"It is now commonly believed that intelligent agents are best implemented via multiple behaviors. For example, in the context of mobile agents, the various behaviors may correspond to “follow hallway”, “avoid obstacles”, “seek goal”, etc. It is not difficult to see that in any tangible application, the various behaviors will produce a discordant set of commands for the robot. The problem then becomes one of arbitrating between the commands dictated by the various behaviors and choosing one of the commands or some combination of them. This problem is known as the problem of arbitration. In the past, various solutions have been suggested for solving this problem such as the vector field approach by Arkin [1], the subsumption approach by Brooks [2], and the task based approach by Simmons [3]. In our research, we present a different solution. We think of behaviors as mapping functions from the domain that consists of sensory readings to the co-domain that consists of multiple sets of command decisions. In our approach, the behaviors are combined by taking a weighted sum of these functions. The individual weights are initially specified, but dynamically modified and learned by a reinforcement learning scheme. All these behaviors are programmed within a framework which is another aspect of our research. The results, both from simulation and real world situations, support our claim of solving the problem of arbitration while providing the system with the ability to dynamically adapt to hitherto unknown environments","TLAB, task learning architecture using behaviors, and its application to a mobile agent",,'Purdue University (bepress)',,,core
15979850,2002-01-01T00:00:00,"Iske B, Löffler A, Rückert U. A Direction Sensitive Network Based on a Biophysical Neurone Model. In: Dorronsoro JR, ed. Artificial Neural Networks- ICANN 2002. Lecture notes in computer science. Vol 2415. Madrid, Spain: Springer-Verlag;  2002: 153-159.To our understanding, modelling the dynamics of brain functions on cell level is essential to develop both a deeper understanding and classification of the experimental data as well as a guideline for further research. This paper now presents the implementation and training of a direction sensitive network on the basis of a biophisical neurone model including synaptic excitation, dendritic propagation and action-potential generation. The underlying model not only describes the functional aspects of neural signal processing, but also provides insight into their underlying energy consumption. Moreover, the training data set has been recorded by means of a real robotics system, thus bridging the gap to technical applications",A Direction Sensitive Network Based on a Biophysical Neurone Model,,'Springer Science and Business Media LLC',10.1007/3-540-46084-5_26,,core
24603497,2003,"This thesis advocates the usefulness and practicality of a logic-based approach to AI and in particular to high-level control of mobile robots. The contribution of the research work reported here is twofold: 1) the development of theoretical frameworks that account for uncertainty and unmodeled dynamics in an environment where an acting agent has to achieve certain goals and 2) the implementation of the developed ideas on a mobile robot. We have elaborated the approach to designing efficient and reliable controllers in Golog following two different perspectives on the environment where the control program is supposed to operate. According to one perspective, investigated in Chapter 4, the agent has a logical model of the world, but there is no probabilistic information about the environment where the agent is planning to act, and the agent is not capable or has no time for acquiring probabilities of different effects of its actions. In this case, the uncertainty and dynamics of the environment can be accounted only by observing the real outcomes of actions executed by the agent, by determining possible discrepancies between the observed outcomes and the effects expected according to the logical model of the world and then by recovering, if necessary, from th",High-Level Robot Programming in Dynamic and Incompletely Known Environments,,,,,core
24630728,1998,"Abstract. Evolutionary robotics | as other adaptive methods, such as reinforcement learning and learning classi er systems|can take considerable time and resources which require a careful evaluation of the hardware tools and methodologies employed. We outline a set of hardware solutions and working methodologies that can be used for successfully implementing and extending the evolutionary approach to complex environments, robots, and real-world applications. The issues discussed include the integration of simulation and real robots, design issues of evolvable robots, hardware requirements for incremental evolution, and hardware and software tools for monitoring and analysis. ",Hardware Solutions for Evolutionary Robotics,,Springer Verlag,,,core
21000458,2000,"Software agents are very different from mobile robots. For instance, they exist in a world quite unlike the world robots exist in. Also, software agents are not embodied. We investigate how these two differences reflect on agent design principles like situatedness and embodiment. We find that the insights gained from designing mobile robots cannot be applied directly to software agent design. Similarly, in the other direction, insights gained from designing software agents cannot be readily applied to mobile robots. This leads us to a notion of situatedness that is different from the notion of being in the world. The Feynman problem solving Algorithm: • Write down the problem • Think real hard • Write down the answer But that’s Feynman. For an ordinary Agent, the algorithm has a lot more lines, and a lot of the extra lines read the same: ask others. This social nature of human problemsolving has been rediscovered by Artificial Intelligence, and there’s a new AI developing, Distributed Artificial Intelligence, or DAI. Distributed AI is an &amp;quot;inherently social study of artificial agents existing in some society or world&amp;quot; (Marsh, 1994). Most of the work in DAI tends to be engineering-oriented. Very little has been done in linking work in DAI to higherlevel models of cognition. In this paper, we try to relate results from DAI to one high–level model of cognition— the &amp;quot;intelligence-without-representation &amp;quot; model proposed by Rodney Brooks (1991). We explore the relationship between real-world agents and software agents, and try to find out how the fields relate to one other. First, we offer a brief overview of the Rodney Brooks model. The Brooks model is largely based on the robot Herbert, developed at the MIT Artificial Intelligence lab",Software Agents and Situatedness: Being Where,,Press,,,core
19948893,2005-01-01T00:00:00,"This paper describes the framework of a real-time simulation system to model human behavior and reactions in dangerous environments. The system utilizes the latest 3D computer animation techniques, combined with artificial intelligence, robotics and psychology, to model human behavior, reactions and decision making under expected/unexpected dangers in real-time in virtual environments. The development of the system includes: classification on the conscious/subconscious behaviors and reactions of different people; capturing different motion postures by the Eagle Digital System; establishing 3D character animation models; establishing 3D models for the scene; planning the scenario and the contents; and programming within Virtools Dev. Programming within Virtools Dev is subdivided into modeling dangerous events, modeling character's perceptions, modeling character's decision making, modeling character's movements, modeling character's interaction with environment and setting up the virtual cameras. The real-time simulation of human reactions in hazardous environments is invaluable in military defense, fire escape, rescue operation planning, traffic safety studies, and safety planning in chemical factories, the design of buildings, airplanes, ships and trains. Currently, human motion modeling can be realized through established technology, whereas to integrate perception and intelligence into virtual human's motion is still a huge undertaking. The challenges here are the synchronization of motion and intelligence, the accurate modeling of human's vision, smell, touch and hearing, the diversity and effects of emotion and personality in decision making. There are three types of software platforms which could be employed to realize the motion and intelligence within one system, and their advantages and disadvantages are discussed",Modeling human behaviors and reactions under dangerous environment,,"Rocky Mountain Bioengineering Symposium, Inc.",,,core
25879200,2005-12-01T00:00:00Z,"The paper considers a general model of real biological creatures' antennae, which is practically implemented and tested, over a real element of a mobile modular robotic system - the robot MR1. The last could be utilized in solving of the most classical problem in Robotics - Object Localization. The functionality of the represented sensor system is described in a new and original manner by utilizing the tool of Generalized Nets - a new likelihood for description, modelling and simulation of different objects from the Artificial Intelligence area including Robotics",Biologically Inspired Object Localization for a Modular Mobile Robotic System,https://core.ac.uk/download/pdf/25879200.pdf,Academic Publishing House,,"[{'title': None, 'identifiers': ['issn:1313-261X', 'issn:1312-451X', '1312-451x', '1313-261x']}]",core
4705698,1998-01-01T00:00:00,"As autonomous robots become more complex in their behavior, more sophisticated software
architectures are required to support the ever more sophisticated robotics software. These
software architectures must support complex behaviors involving adaptation and learning,
implemented, in particular, by neural networks. We present in this paper a neural based schema
[2] software architecture for the development and execution of autonomous robots in both
simulated and real worlds. This architecture has been developed in the context of adaptive
robotic agents, ecological robots [6], cooperating and competing with each other in adapting to
their environment. The architecture is the result of integrating a number of development and
execution systems: NSL, a neural simulation language; ASL, an abstract schema language; and
MissionLab, a schema-based mission-oriented simulation and robot system. This work
contributes to modeling in Brain Theory (BT) and Cognitive Psychology, with applications in
Distributed Artificial Intelligence (DAI), Autonomous Agents and Robotics",A Neural Schema Architecture for Autonomous Robots,https://core.ac.uk/download/4705698.pdf,Georgia Institute of Technology,,,core
41534236,2005-01-01T08:00:00,"A real-time analogue recurrent neural network (RNN) can extract and learn the unknown dynamics (and features) of a typical control system such as a robot manipulator. The task at hand is a tracking problem in the presence of disturbances. With reference to the tasks assigned to an industrial robot, one important issue is to determine the motion of the joints and the effector of the robot. In order to model robot dynamics we use a neural network that can be implemented in hardware. The synaptic weights are modelled as variable gain cells that can be implemented with a few MOS transistors. The network output signals portray the periodicity and other characteristics of the input signal in unsupervised mode. For the specific purpose of demonstrating the trajectory learning capabilities, a periodic signal with varying characteristics is used. The developed architecture, however, allows for more general learning tasks typical in applications of identification and control. The periodicity of the input signal ensures convergence of the output to a limit cycle. Online versions of the synaptic update can be formulated using simple CMOS circuits. Because the architecture depends on the network generating a stable limit cycle, and consequently a periodic solution which is robust over an interval of parameter uncertainties, we currently place the restriction of a periodic format for the input signals. The simulated network contains interconnected recurrent neurons with continuous-time dynamics. The system emulates random-direction descent of the error as a multidimensional extension to the stochastic approximation. To achieve unsupervised learning in recurrent dynamical systems we propose a synapse circuit which has a very simple structure and is suitable for implementation in VLSI",An analogue recurrent neural networks for trajectory learning and other industrial applications,https://core.ac.uk/download/41534236.pdf,"Edith Cowan University, Research Online, Perth, Western Australia",,,core
23050104,2000,"This paper presents biologically inspired neural controllers for generating motor patterns in a quadruped  robot. Sets of artificial neural networks are presented which provide 1) pattern generation and gait control,  allowing continuous passage from walking to trotting to galloping, 2) control of sitting and lying  down behaviors, and 3) control of scratching. The neural controllers consist of sets of oscillators composed  of leaky-integrator neurons, which control pairs of flexor-extensor muscles attached to each joint.  The networks receive sensory feedback proportional to the contraction of simulated muscles and to joint  flexion. Similarly to what is observed in cats, locomotion can be initiated by either applying tonic (i.e.  non-oscillating) input to the locomotion network or by sensory feedback from extending the legs. The  networks are implemented in a quadruped robot. It is shown that computation can be carried out in real  time and that the networks can generate the above me..",Biologically Inspired Neural Controllers for Motor Control in a Quadruped Robot.,,IEEE Computer Society,10.1109/ijcnn.2000.859467,,core
20835184,2002,"I would like to express my appreciation to Dah-Yoh Lim for his advice on machine learning and reinforce-ment learning techniques, and for kindly reviewing this report. I would also like to thank Ben Leong for his moral support during the project presentation. Last but not least, I would also like to thank Indraneel Chakraborty and Ji Li for their companionship and stimulating challenges that spurred me to have a better understanding of building a real robot. i Autonomous learning robots have the advantage over manually programmed robots in that they are able to adapt to varying conditions, both internal to the robot (e.g., energy levels) as well as external environmen-tal conditions (e.g., light, friction). In this project, we implemented a robot that learns how avoid obstacle using online self-adaptation. We further enhanced the robot’s learning ability by incorporating a light seek-ing behavior in the robot when its internal energy level is low. This light seeking behavior uses a tree-like data structure that is based on the concept of eligibility traces used in Q-learning algorithms (reinforcement learning). Finally we implemented a genetic algorithm on the robot and experimented with using genetic algorithm as a form of robot learning. The robot was built using the Lego RCX microcomputer and was designed with the subsumption architecture",Acknowledgements,,,,,core
24315003,1998,". Robotic soccer is a challenging research domain because problems in robotics, artificial intelligence, multi-agent systems and real-time reasoning have to be solved in order to create a successful team of robotic soccer players. In this paper, we describe the key components of the CS Freiburg team. We focus on the self-localization and object recognition method based on using laser range finders and the integration of all this information into a global world model. Using the explicit model of the environment built by these components, we have implemented path planning, simple ball handling skills and basic multi-agent cooperation. The resulting system is a very successful robotic soccer team, which has not lost any game yet. 1 Introduction  Robotic soccer is a challenging research domain because problems in robotics, artificial intelligence, multi-agent systems and real-time reasoning have to be solved in order to create a successful team of robotic soccer players [11]. We took up th..","The CS Freiburg Robotic Soccer Team: Reliable Self-Localization, Multirobot Sensor Integration, and Basic Soccer Skills",,Springer-Verlag,,,core
4751811,2004-03-01T00:00:00,"Presented at the AAAI Symposium on Accessible, Hands-on AI/Robotics Education, San Jose, CA, March 2004.The focus of Jet Propulsion Laboratory’s Office of Communication and Education is to engage the public through the implementation of innovative approaches such as informal and formal education methods, science outreach, and research development efforts [ 13. One of the direct outcomes of this emphasis is to create a bridge to provide students access to the research and development activities involved in space exploration, as well as to support and develop a pipeline program to encourage the next generation of engineers and scientists. Of special interest is to transfer robotics research knowledge derived from JPL efforts to the educational arena [2]. The robotics field represents the integration of multiple facets of engineering and science - from mechanical construction to intelligence programming to science data analysis. It is an ideal opportunity to showcase the relationship math and science have on tangible real-world applications. In addition, the robotics field incorporates non-traditional areas of study such as the social aspects of teamwork, system engineering, and hands-on experimentation methods. These represent only a small subset of the elements necessary for developing an integrated, intelligent robotic system for space exploration. To promote robotics education, there is a concrete process needed for transferring the knowledge derived from space robotics research to platforms capable of teaching individual artificial intelligence topics, providing step-by-step instructions on robotics techniques, or integrating hands-on learning into the curriculum. In this paper, we will discuss the AI Toolkit, a task carried out at NASA’s Jet Propulsion Laboratory (JPL) that focuses on bridging the gap between robotics research efforts and the classroom environment. The AI Toolkit enables training on artificial intelligence techniques through step-by-step instruction and hands-on exercises applied to the robotics arena",Bridging the Gap between Space Robotics Research and Robotics Education,,'American Institute of Aeronautics and Astronautics (AIAA)',,,core
10564967,Oct-03,"Topics covered include: Cryogenic Temperature-Gradient Foam/Substrate Tensile Tester; Flight Test of an Intelligent Flight-Control System; Slat Heater Boxes for Thermal Vacuum Testing; System for Testing Thermal Insulation of Pipes; Electrical-Impedance-Based Ice-Thickness Gauges; Simulation System for Training in Laparoscopic Surgery; Flasher Powered by Photovoltaic Cells and Ultracapacitors; Improved Autoassociative Neural Networks; Toroidal-Core Microinductors Biased by Permanent Magnets; Using Correlated Photons to Suppress Background Noise; Atmospheric-Fade-Tolerant Tracking and Pointing in Wireless Optical Communication; Curved Focal-Plane Arrays Using Back-Illuminated High-Purity Photodetectors; Software for Displaying Data from Planetary Rovers; Software for Refining or Coarsening Computational Grids; Software for Diagnosis of Multiple Coordinated Spacecraft; Software Helps Retrieve Information Relevant to the User; Software for Simulating a Complex Robot; Software for Planning Scientific Activities on Mars; Software for Training in Pre-College Mathematics; Switching and Rectification in Carbon-Nanotube Junctions; Scandia-and-Yttria-Stabilized Zirconia for Thermal Barriers; Environmentally Safer, Less Toxic Fire-Extinguishing Agents; Multiaxial Temperature- and Time-Dependent Failure Model; Cloverleaf Vibratory Microgyroscope with Integrated Post; Single-Vector Calibration of Wind-Tunnel Force Balances; Microgyroscope with Vibrating Post as Rotation Transducer; Continuous Tuning and Calibration of Vibratory Gyroscopes; Compact, Pneumatically Actuated Filter Shuttle; Improved Bearingless Switched-Reluctance Motor; Fluorescent Quantum Dots for Biological Labeling; Growing Three-Dimensional Corneal Tissue in a Bioreactor; Scanning Tunneling Optical Resonance Microscopy; The Micro-Arcsecond Metrology Testbed; Detecting Moving Targets by Use of Soliton Resonances; and Finite-Element Methods for Real-Time Simulation of Surgery","NASA Tech Briefs, October 2003",https://core.ac.uk/download/pdf/10564967.pdf,,,,core
20771821,2001,"Most Artificial Intelligence (AI) work can be characterized as either &quot;high-level&quot; (e.g., logical, symbolic) or &quot;low-level&quot; (e.g., connectionist, behavior-based robotics). Each approach suffers from particular drawbacks. High-level AI uses abstractions that often have no relation to the way real, biological brains work. Low-level AI, on the other hand, tends to lack the powerful abstractions that are needed to express complex structures and relationships. I have tried to combine the best features of both approaches, by building a set of programming abstractions defined in terms of simple, biologically plausible components. At the &quot;ground level&quot;, I define a primitive, perceptron-like computational unit. I then show how more abstract computational units may be implemented in terms of the primitive units, and show the utility of the abstract units in sample networks. The new units make it possible to build networks using concepts such as long-term memories, short-term memories, and frames. As a demonstration of these abstractions, I have implemented a simulator for &quot;creatures&quot; controlled by a network of abstract units. The creatures exist in a simple 2D world, and exhibit behaviors such as catching mobile prey and sorting colored blocks into matching boxes. This program demonstrates that it is possible to build systems that can interact effectively with a dynamic physical environment, yet use symbolic representations to control aspects of their behavior. Thesis Supervisor: Gerald J. Sussman Title: Matsushita Professor of Electrical Engineering, MIT 2 Acknowledgments  .Iwould like to thank my advisor Gerry Sussman for his support and valuable suggestions. .Iwould like to thank Joel Moses for additional support. .Iwould like to thank Marvin Minsky, Jake Beal, and Push Singh ..",Building Grounded Abstractions for Artificial Intelligence Programming,,,,,core
21233236,2005,"Abstract: Software analysis pattern is an approach of software reuse which provides a way to reuse expertise that can be used across domains at early level of development. Developing software for a mobile robot system involves multi-disciplines expert knowledge which includes embedded systems, real-time software issues, control theories and artificial intelligence aspects. This paper focuses on analysis patterns as a means to facilitate mobile robot software knowledge reuse by capturing conceptual models in those domains in order to allow reuse across applications. The use of software analysis patterns as a means to facilitate Autonomous Mobile Robots (AMR) software knowledge reuse through component-based software engineering is proposed. The software analysis patterns for AMR were obtained through a pattern mining process, and documented using a standard catalogue template. These analysis patterns are categorized according to hybrid deliberate layered architecture of robot software: Reactive layer, supervisor layer and deliberative layer. Particularly, the analysis patterns in the reactive layer are highlighted and presented. The deployment of the analysis patterns are illustrated and discussed using an AMR software case study. To verify the existence of the pattern in AMR systems, pattern-based reverse engineering was performed on two existing AMR systems. The reuse potential of these patterns is evaluated by measuring the reusability of components in the analysis patterns",Software Reuse for Mobile Robot Applications Through Analysis Patterns,,,,,core
22610706,2002,"In this paper we examine the underlying similarities and differences between two major computational formalisms in developing intelligent robots; namely, artificial potential fields, which are often implemented for real-time robot planning and control, and artificial neural networks, which are usually considered as one of the biologically-inspired powerful learning techniques. Such comparisons will offer us new insights into how the two can complement each other in learning and control during robot-environment interaction. (This paper is based on an article by Liu and Khatib for the forthcoming The Handbook of Brain Theory and Neural Networks, Michael Arbib, Editor, MIT Press, 2000.)  1 INTRODUCTION  The problem of robot motion planning was traditionally treated as an optimization problem, in which the configuration of a robot is represented in a parameter space, and a solution to this problem is computed by searching the parameter space in an attempt to satisfy a predefined cost funct..",Practical Connection between Potential Fields and Neural Networks,,The MIT Press,,,core
22603066,2004,"This paper presents results generated with a new evolutionary robotics (ER) simulation environment and its complementary real mobile robot colony research test-bed. Neural controllers producing mobile robot maze searching and exploration behaviors using binary tactile sensors as inputs were evolved in a simulated environment and subsequently transferred to and tested on real robots in a physical environment. There has been a considerable amount of proof-of-concept and demonstration research done in the field of ER control in recent years, most of which has focused on elementary behaviors such as object avoidance and homing. Artificial neural networks (ANN) are the most commonly used evolvable controller paradigm found in current ER literature. Much of the research reported to date has been restricted to the implementation of very simple behaviors using small ANN controllers. In order to move beyond the proof-of-concept stage our ER research was designed to train larger more complicated ANN controllers, and to implement those controllers on real robots quickly and efficiently. To achieve this a physical robot test-bed that includes a colony of eight real robots with advanced computing and communication abilities was designed and built. The real robot platform has been coupled to a simulation environment that facilitates the direct wireless transfer of evolved neural controllers from simulation to real robots (and vice versa). We believe that it is the simultaneous development of ER computing systems in both the simulated and the physical worlds that will produce advances in mobile robot colony research. Our simulation and training environment development focuses on the definition and training of our new class of ANNs, networks that include multiple hidden layers, and tim..",Maze Exploration Behaviors Using An Integrated Evolutionary Robotics Environment,,,10.1016/j.robot.2003.11.002,,core
23698462,1998,"A simulated hockey environment is introduced as a test bed for studying adaptive behavior and evolution of robot controllers. A near-frictionless playing surface is employed, partially mimicking zero gravity conditions. We show how a neural network using a simple evolutionary algorithm can develop nimble strategies for moving about the rink and scoring goals quickly and effectively.  1. Introduction  In recent years, there has been growing interest in using machine learning and evolutionary techniques for developing software controllers that play competitive or cooperative games in some kind of physical environment, either real or simulated. These have included: avoiding obstacles (Xiao et al., 1997; Lee et al., 1996; Brooks, 1986), foraging for &quot;food&quot; (Werger and Mataric, 1996), playing pursuit/evasion games (Miller and Cliff, 1994), discriminating objects (Beer, 1996), fighting for control of a cube (Sims, 1995) and RoboCup soccer (Kitano et al., 1995). Following in these traditions,..",The Evolution of Subtle Manoeuvres in Simulated Hockey,,,,,core
199427094,2003-01-01T00:00:00,"In the robotics area, visual tracking is an important and difficult problem
therefore is necessary to have a robust and efficient control algorithm which
presents immunity characteristics to stochastic direction and speed changes of
the object to be tracked. Also is important count with a segmentation
algorithm which be able to tolerate changes in the intensity of light. We
describe in this report the implementation of fuzzy controllers based on the
fuzzy condensed algorithm and also the developed of a LVQ neural network to
segment the image. For this work we used two fuzzy condensed algorithms
running in a PC to control a robot’s head which tracks a human face. We
describe the main lines of the fuzzy condensed algorithm as well as the LVQ
neural networks architecture employed and the implementation, the fuzzy
condensed controller performance in comparison to a PID controller and real
time results",Intelligent tracking,https://core.ac.uk/download/199427094.pdf,,10.17169/refubium-22498,,core
22926363,1999,"Abstract. A major challenge in mobile robotics is integration of methods into operational autonomous systems. Construction of such systems requires use of methods from perception, control engineering, software engineering, mathematical modelling, and artificial intelligence. In this paper it is described how such a variety of methods have been integrated to provide an autonomous service robot system that can carry out fetch-and-carry type missions. The system integrates sensory information from sonars, laser scanner, and computer vision to allow navigation and human-computer interaction through the use of a hybrid deliberative architecture. The paper presents the underlying objectives, the underlying architecture, and the needed behaviours. Throughout the paper, examples from real-world evaluation of the system are presented. ",Isr: An intelligent service robot,,,10.1007/10705474_16,,core
24337211,1991,"The Constraint Satisfaction Problem (CSP) has been a useful model for various industrial and engineering applications. These include image processing and pattern recognition, VLSI engineering, robotics manipulation, and computer hardware design automation. In this paper, we give a novel AI architecture for discrete relaxation that effectively prunes a backtracking search tree in CSP. This algorithm has been implemented on a finegrained, massively parallel hardware computer architecture. For practical application problems, many orders of magnitude of efficiency improvement can be reached on this hardware architecture. This enables real-time processing of a large class of practical problems. Keywords: Artificial Intelligence, backtrack search, constraint satisfaction problem (CSP), Discrete Relaxation Algorithm (DRA), AI architectures. 1 Introduction  Constraint-based search problems have three components: variables, values, and constraints. The goal is to find an assignment of values to..",A Parallel Architecture for Constraint Satisfaction,,,,core,
24277257,1995,"Operation of an autonomous vehicle in a real-world environment requires capabilities for real-time processing of uncertain, incomplete, and approximate information. We report progress in the development of methods for robust autonomous navigation of autonomous vehicles in unstructured environments. We focus in particular on those issues where the use of techniques based on fuzzy logic has proved to be particularly helpful with emphasis on the representation and execution of complex navigation plans. We illustrate our discussion with examples taken from experimental implementation of these techniques on Flakey, the mobile robot of the Artificial Intelligence Center of SRI International.    On leave from Iridia, Universit&apos;e Libre de Bruxelles, Brussels, Belgium.  1 Introduction  The development of techniques to plan and execute autonomous-vehicle actions in a real-world unstructured environment requires consideration of multiple issues. First, knowledge about the environment is, in gene..",Progress in Research on Autonomous Vehicle Motion Planning,,,,core,
24357433,1996,"The paper describes an approach to reasoning about actions and planning that starting from a logical formalization arrives at the realization of an actual agent, the mobile robot &quot;Tino&quot;. The reasoning tools allow the robot to derive plans from the knowledge about the environment and the action specification, while its reactive capabilities allow it to execute its plans in the real world. The formalization is based on the propositional dynamic logics framework, but exploits the correspondence that exists between propositional dynamic logics and description logics, to carefully weaken the logical inference process in order to keep the reasoning tools of the robot both effective and efficient. Such reasoning tools are then implemented by making use of a general knowledge representation system based on description logics, namely the system CLASSIC. Introduction  In Artificial Intelligence there has always been a great interest in the design of agents that can exhibit different forms of int..",Moving a Robot Starting From a Theory of Actions,,AAAI press,,core,
235574475,1993-07-07T00:00:00,"An archive of the Magrath Trading Store News.The University of Lethbridge Library received permission from the Wes Balderson to digitize and display this content.MAGRATH
NEWS
Published Weekly since 1932 by
The Magrath Trading Company
30 cents
° 1993 MAGRATH CELEBRATIONS
« a 93 YEARS YOUNG!
""HAPPINESS IS HOMESPUN""
to
p
6:00 p.m.*.
D
THURSDAY, JULY 22
..............Softball Tournament
Contact: Bill Alston B ,
* p FRIDAY. JULY 23 *
4 - 7:00 p.m........Bake Sale at the Arena
Contact: Lorraine Balderson
...... Softball Tournament
......¡Community Dance - Tom
Karren Gym - Pyramid
Entertainment
Contact: Tyler Mandin
Cost: $2 /person or $10 /family
or a donation to the Library Fund.
a
SATURDAY, JULY 24
6:00 p.m......
9 - 1:00 a.m.
TSUd a.m.............. Pancake Breakfast - Lions
Club. Contact: John Bourne
11:00 a.m.............PARADE!
t, Meet in front of the School at 10:00
0 for Line-up and Judging.
g Childrens entries meet at
„ "" d Grusendorf s comer.
PARADE MARSHALL - DON JOHNSON
a
12 noon................Luncheon at Ice Arena
Magrath 4th Scouts
1:00 p.m................Children’s Races at the Track
Field. Contact: Dave Clark
1:00 p.m............... Free Swim at the Pool
2:00 p.m...............Softball Tournament
£
a 4
a
o
0
I
o &
**NOTE: ALL PROCEEDS FROM THE
BAKE SALE AND THE COMMUNITY
DANCE WILL GO TO THE LIBRARY
MUSEUM FUND** * o
Wednesday July 7, 1993
3:00 p.m............... Lions Club Bottle Race at Main
Canal by Allan Owens. Q
a p Contact: Norm Robinson ,
AFTER THE SOFTBALL TOURNAMENT
Program and free Beef on a Bun at the School
Auditorium - Co-ordinators: Shirley Perry,
Marilyn Henderson, and Marie Stevenson. °
o
♦FIREWORKS*FIREWORKS*FIREWORKS*
A*
PARADE ROUTE: Meet at the school, go #
NORTH to the Hospital, WEST to Highway 62,
then SOUTH to the Trading Company comer, a
then EAST to the LDS Church, loop back to Art
Grusendorf's comer - 1 St. West and 1 Ave North
o
<
WE HATE TO KEEP HARRASSING YOU
ABOUT THIS, BUT IT WOULD BE
GREATLY APPRECIATED IF YOU WOULD
LET US KNOW IF YOU WOULD LIKE TO
RENEW YOUR SUBSCRIPTION TO THE
STORE NEWS. A SIMPLE PHONE CALL
WILL GET YOU OFF OUR HIT LIST AND
YOU CAN IGNORE THESE ANNOYING
REMINDERS. THE FEE IS $12.00 PER
YEAR WHICH CAN BE CHARGED (OVER
THE PHONE) OR PAID AT THE NEWS
OFFICE. THANKS.
COMING EVENTS
St, Johns Lutheran Church
""2nd ANNUAL
VOLLEYBALL
TOURNAMENT"" will be held
on Saturday, July 10th, with
orientation at
8:00 a.m. and ,( \r—j
play starting at Vy ¡J
9:00 a.m.
Registration will be $5.00 per
team member with a maximum
of 7 members. Registration
forms are available at the Post
Office, The Store, Blue Goose,
The Trading Company,
Gurneys Garage, Treasury
Branch, and the Bank of
Montreal. Registration and
entry fee to be returned to Bob
Weing (758-6601) no later than
July 5, 1993.Get your entry in
early- we are expecting a
bigger and better event than
last year with more prizes and
fun for young and old alike.
See you all on July 10th.
afc □!<:*****
REMINDER: Spirit of
Alberta Mini Guard Camp will
be held July 19-24. There is a
late registration fee and
registration forms are available
from Mr. Chatwin at 758-3765
or Elizabeth Strong at 758-
3166, or send to Box 222,
Magrath.
*******
ANNOUNCEMENT: Dave
and Lisa Sannes invite friends
to an Open House at the
Lounge and Patio Marigold
Manor in Raymond on
Saturday, July 17, 1993 from
2:00 - 5:00 p.m. They W||j frg
married in Edmonton on July
10th. See you there!
SELECTED CHILDRENS
POL¥/C0TTON PRINTS
75% OFF!
ALE FABRIC PENS & 1
1/2 PRICE!
*****
ALL LACE
DRY GOODS SPECIALS
*****
ALL BABY QUILT PANELS
1/2 PRICE!
NOTICE: Chinook Health
Unit is offering a Senior's
Wellness Clinic in Cardston
July 8th from 1:00 - 3:30 p.m.
All senior's in the area are
welcome. The clinic offers
foot care, blood pressure
checks, health counselling and
information. To make an
appointment call the Health
Unit office at 653-4981.
*******
WEDDING
ANNOUNCEMENT:
Bonnie Bolokoski to Rhett
Mandin Saturday, July 17th.
A calling reception will be
held in their honor at the
Magrath Seniors Centre from
7:00- 9:00 that evening.
Everyone welcome.
Me******
James and Charlotte
Anderson are pleased to
announce the forthcoming
marriage of their daughter
Joanne to Kevin Tidball, son
of Donna and Dennis Tidball
of Edmonton. The wedding
will take place Saturday, July
10th in the Cardston, Alberta
Temple. A calling reception
will be held at the Magrath
Stake Centre from 7:00 - 9:00
that evening.
*******
COED RECREATIONAL
SOFTBALL SUMMER
LEAGUE - 18 years and
older. To begin July 7th and
will continue throughout the
summer on Thursday nights.
Phone and register with Laurel
Bennett (758-6222).
FRIENDS OF THE
PRODUCE BUYS
Tomatoes .591b $1.30 kg
Cantaloupe .49 1b $1.08 kg
Honeydew Melons .891b $1.96 kg
Long English Cucumbers .99 each
Valencia Oranges .491b $1.08 kg
Lettuce .491b $1108 kg
MAGRATH PUBLIC
LIBRARY FOUNDATION
QUILT RAFFLE
Come and see this beautiful
quilt on display in the front
window of the Magrath
Trading Company. Draw will
be made July 24th.
TICKETS: $1.00 available at
the Library or call:
Ann Pilling 758-3088
Ann Fazikos 758-6425
I have just heard that
Jean Butlin was presented this
week with the highest award a
Lions Club can give, she has
been presented with the
""Melvin Jones"" Fellow Award,
for dedicated humanitarian
services from the Lethbridge
Coulee Lions Club and the
Lions International Foundation.
Congratulations Jean!
*******
Marie Stevenson
Bernice Sillito
Hazel Dudley
758-3540
758-3618
758-3213
Nicole and John
Feddock of San Carlos,
California
are the
proud
parents of
a baby
girl, JANAE NYCHELLE.
Grandparents are Shirley &
Joe Paulson of Cardston, and
great grandparents are Joe &
Inez Gibb of Magrath and
Mrs. Mildred Paulson of Lake
Cowichan, B.C.
* 3|e 3#C 3fe 5|c ifc
THANK YOU to those that
have supported Russell and
Robert with their tilling. We
do not have a tiller but have
mowers that will travel. Any
size lawn in or out of town.
Call 758-3107.
ATTENTION JR. HIGH
KIDS:
Plan to create an ""Alien"" for
display and
judging during
our summer
reading
program
(androids,
mutants, and
robots also welcome). Make
them any size up to foot high,
and use any items and material
you want. Bring them to us
any time during regular library
hours. Enter our weekly
prize draw and watch for other
projects. Must have a library
card.
Magrath Public Library
*******
PUBLIC NOTICE:
Wire will not be accepted at
the Transfer Station unless it is
rolled into TIGHT TWO
FOOT ROLLS. Anyone
showing up with wire other
than that will be turned away.
Station hours are Wednesday
& Saturday from 10:00 a.m. -
6:00 p.m. (By order of Chief
Mountain Solid Waste
Authority) *******
/ctJteAznlo/v .
SPECTACULAR
SUMMER SALE
r niivrFRS ROBES, NIGHTWEAR,
1/2 PRICE
M RACK OF C«ll.OKFm CLOTHE
1/2 PRICE
---------stSOCT? SELECTED EARRINGS 3 PAIRS FOR $100
Cl^e
..........................
MANY new BLOUSES HAVE, ADDED to THE $19.95 RA
COME IN AND CHECK OUT
NEW .50 BASKET FHLF.D» miscellaneous item. MANY NEW I^H^ EE^ TO THE SALES RACKSBest Decorated
MORE GROCERY SPECIALS
KooLAid
3 for .78
Ultra Tide
6 litre
$9.98 each
Tide Original
12 litre
$8.98 each
Cascade Original (1.8 kg) or Cascade Gel (1.3 L)
$3.88 each
Bounce
50 pack
$4.98 each
Scottowels Super Mega Roll
2 pack
$2.58 each
Scotties
150 pack
.88 each
Purex
24 pack
$7.98 each
SATURDAY CELEBRATION PARADE
PRIZE MONEY
FLOATS:
First
Second
Third
Family
$75.00
$50.00
$25.00
Commercial
$75.00
$50.00
$25.00
Fraternal
$75.00
$50.00
$25.00
Club
$75.00
$50.00
$25.00
Decorated Car
$50.00
$30.00
$20.00
Novelty Floats:
Humorous
$40.00
$20.00
$10.00
Motorized
$40.00
$20.00
$10.00
**ALL KIDS IN THIS SECTION WILL
Child
$6.00
$4.00
$2.00
Best Decorated
Group of Two
$10.00
$8.00
$6.00
Best Decorated
Family^ __
$15.00
$10.Q0
$5. Q0
Novelty
$6.00
$4.00
$2.00
CHILDREN'S SECTION
Best Decorated
Tricycle & Child
$6.00
$4.00
$2.00
Best Decorated
Bicycle & Child
$6.00
$4.00
$2.00
RECEIVE $1.00 (LOONIE)**
HORSES:
First
Second
Best Male
Rider & Horse
$25.00
$15.00
Best Female
Rider & Horse
$25.00
$15.00
Best Couple
$25.00
$15.00
Best Horse Drawn
$25.00
$15.00
FRIENDS OF THE
MAGRATH PUBLIC LIBRARY / MUSEUM FOUNDATION
FUND RAISING COMMITTEE
1 ji B
REMEMBERCLASSIFIED ADS
DEADLINE: TUESDAY 12 NOON PHONE 758-6377
Less than 30 words—$1.07
1 /4 page----
----------------$7.49
Full Page—Copy Ready—$25.00
Small ad (2.5""X3.5"")-$5.35
1/3 page-—
------- ----$8.56
Full Page-—We do----------$37.45
1/2 page—
...................$10.70
Flyer insertion (your paper)$21.40
FOUND:
LEFT AT THE DAHL CLINIC: Several items of clothing and a pillow. Claim at the Dahl Clinic.
FOUND: Mens watch. Phone 758-6817.
*******
FOUND: Bicycle helmet at the school playground.
Identify to claim. Phone 758- 6840.
GARAGE SALES:
Garage Sale to be held in the PANTREE FOODS location on Main Street on Saturday, July 10th from 8:00 a.m. to 1:00 p.m.
GARAGE SALE: Moving to go back to school - variety of items, big and small at Cosgrove's - 108 E. 1st Ave S. Magrath, Saturday, July 10, from 8:30 a.m. - 3:30 p.m.
LOST:
LOST: Pink and White ""Lil"" Princess bike. If you have seen it, please call Jacie Chipman at 758-3675. Thank you.
LOST: GIZMO TIMEX WATCH (Blue and black with squiggley lines) elasticized wristband. Phone 758-3427 . (Aaron).
MERCHANDISE:
FOR SALE: Large quantities of hay, square and round bales. Reasonable. 758-3598.
*******
FOR SALE: Various waterbed mattresses & liners, also one fiberglass shell for full size truck - $100 o.b.o. Phone D. Hatch 758-6652.
*******
FOR SALE: 20 poles 17* long, 8 - 9"" bottoms. $15.00 each. Call Cam Jordan at 758- 6532. 144 N 2nd Ave.
*******
FOR SALE: 1. One Culligan water softener complete.
2.
One iron remover - complete.
3.
One set of wooden stock racks to fit L.W.B. Narrow Box Chev.
4.
One Bell & Howell slide projector with 38 cubes.
5.
500 gallon stock watering trough.
For any of these items, call 758-6271.
*******
FOR SALE: Large log doghouse. Can be seen at 81 E. 2nd Ave N. For more information, call 329-4938.
*******
FOR SALE: 2.5 - 20 lb Fire Extinguishers through the fire department to raise money for equipment. Firefighters will be going door to door or phone 758-3167 or 758-6804.
FOR SALE: New Singer Knitting Machine with ribber attachment, complete with instruction books and custom built table. Asking $1100 cash. Phone 758-3545.
FOR SALE: 1976 Olds engine 350 V8. Phone Koji 758-6896 *******
FOR SALE: Old garage door. Give offers. Phone 758-3605.
*******
SEASONAL CRAFTS FOR SALE - by Lisa Jensen at Sue's On First.
*******
ADORABLE KITTENS FOR SALE - $5.00 each. Call Kris at 758-6896.
FOR SALE: 1. 7 1/2 truck camper with 4 hydrolic jacks $1000.
2.
Two mens 10 speed bikes - rusty and in need of repair.
3.
Green electric range - oven not working properly.
4.
Silver Bullet ride-on mower needs work.
5.
Childs Tricycle (3 or 4 year old)
Phone 758-3637.
REAL ESTATE:
FOR RENT: 2 bedroom suite in the fourplex. $390/month + electrical. Abstainers only. Phone Robert at 758-3107 (home) or 758-6866 (work):REAL ESTATE cont...
FOR SALE: 1288 sq. ft.
bungalow in Magrath. Priced
to sell at 39,900. Call Sharon
Bacon at 382-7696 or Royal
Lepage at 327-2 111. MLS.
*******
WANTED TO RENT: 3
bedroom home in Magrath
required for August 1st.
Phone the Pillings at 758-
3057.
******* '-<■
REAL ESTATE NEEDED: I
need mid-market priced
residential listings for qualified
buyers wishing to relocate in
Magrath. Prices must support
bank appraisals for financing.
Phone Jack Elliott, 758-3551
or 327-1166, Coldwell Banker
First Lethbridge Realty.
SERVICES:
CAKES BY RITA - Will do
cakes for almost any occasion.
Advance notice greatly
appreciated. To order, please
phone 758-6315.
*******
TED GREGSON STUDIOS -
Weddings, family groups,
general photography.
Reminder to 3rd & 4th Ward
members - the building
dedication memorial ward
photos are available at $10 for
an 8x10. Call 758-3110.
WAXWORKS - Small,
medium, and large cars inside
and out. Phone 758-6840.
*******
CUSTOM HAYING - Cutting
and round baling. Phone Rick
Strate at 758-6749.
*******
FOR ALL YOUR
LASER PRINTER NEEDS
/ labels I flyers I resumes /
/ type setting for books /
Call Bonny 758-6309
*******
DOG GROOMING - Small
dogs only. Call Chris at 758-
6896.
*******
WORK WANTED: Lawns,
cleaning, etc. Call Chad at
758-6840.
*******
If you would like a nice
young man to come mow
your lawn, call 758-6249
and ask for Dano Gurney.
VEHICLES:
FOR SALE: 1992 TOYOTA
COROLLA LSX. 4 doors, 5
speed, AM/FM cassette, tilt,
cruise, in excellent condition.
Asking 13,600. Phone 758-
6530.
*******
FOR SALE: 1986 Honda
Spree Scooter. Red in color,
2000 km. You get 50 km to a
tank of gas. Phone 758-3209.
*******
FOR SALE: 1981 GMC
Cabelero Pickup. Very good
shape. Phone 758-6596.
5jc * 3fC 3fc
FOR SALE: 1984 Chev 1/2
ton 4x4, Very good
condition. New paint, rebuilt
4 speed transmission, new
shocks, new stereo radio,
10,000 km on new motor.
Phone 758-6596.
*******
FOR SALE: 1980 Chrysler
LeBaron. Also a 1972 Chev
Van- $1000 O.B.O. Phone
758-6652.
CARRIAGE HOUSE
THEATRE
-Presents-
""DAVE”
Showing through July 8
Nightly at 9:00 p.m.
AAAAAAAAAAA
""DENNIS THE
MENACE”
Showing July 9-15
Nightly at 9:00 p.m.
WANTED:
WANTED: 2 single
mattresses. If you have any
please give me a call. Phone
758-3209.
*******
WANTED: A pitchfork. Do
you have one just hanging
around your place doing
nothing? Please give me a call
if you want to part with it.
Phone 758-3209.
*******
WANTED: Used 4 burner gas
stove. Phone 758-3418.
*******
NOTICE: The Swimming
Pool is holding ""AQUAFIT""
nightly from 9:00 - 10:00 p.m.
Anyone interested is welcome.
HYPERMEDIA COMPUTER CAMP
will be held from Tuesday, August 3th to
Saturday, August 7th at the Magrath School
Computer Room. There will.be two sessions
offered: Grades 4-5 from 9:00 a.m. - 12 noon
of Magrath, was helping with the catering and
his son, Garry Nelson, owns the building and
service. They sent their card and greetings to the
Store News and friends in Magrath.
and Grades 6-8 from 1:00 - 4:00 p.m. The
camp will be limited to 15 students per session.
The camp will focus on enhancing students
computer skills in the area of multimedia.
Students will work with a variety of software
and hardware. The Instructor is Bonny West
and the cost is $50.00. Please phone Mrs.
West at 758-3072 for more information and
registration forms.
SENIORS NEWS
Our special July supper will be held
next Wednesday, July 14th at 6:00 p.m. at the
Seniors Centre. Come have an enjoyable
evening of eating and visiting.
Those who are going to the Calgary
Stampede next Tuesday, July 13th, should be
at the Seniors Centre, ready to leave at 8:00
a.m. Watch for more details of the day trip we
will be taking on August 14th.
3|C3iC9(C3jC3|C3|C9|C
To announce the death of Jay Dickson,
after a lengthy illness July 4th at Scarboro,
Ontario. Jay was bom and raised in Magrath,
the second son of Earl and Della Dickson. Jay
married Marjory James and they have three
children: Spence, Donna, and Denise.
Jay developed Canada's first automatic
weather station, which won him the top award
in meteorology. It took 10 years to develop,
and now a computer makes all the readings. It
has been called the biggest improvement in
weather instruments in the century. Jay will be
greatly missed.
3|ej|e9(c3|ej|c9|e9jc9|c
On June 23rd Freda and Nyal Fletcher
were in Lehi, Utah for Freda's 60th school
reunion. Out of her starting class of 75, 40
students were in attendance. The reunion was
held at the newly restored Colonial House
Catering and Reception Centre in I^ehi, Utah.
To our surprise Presty Nelson, formerly
GRAHAM
WILLIAM RONALDA
•.‘14 - vÇC .km? — -
),-i ' ■-. ■ t Wil
is lypu isoifSS iovt
Planning.
Before You
Need It.
An important part of planning your estate
is selecting a cemetery monument. Like
a will, life insurance or arranging for a
professional funeral home director, a
decision on a memorial to your life
finalizes the important things for those
left behind - with love.
Heather Thomson
Memorial Counsellor 758-6386
REMCO
REMCO MEMORIALS LTD
The Monument Professionals Since 1924
I am now your Magrath area Remco Counsellor.
If I can help you with your memorial monument needs,
piease give me a call at 758-6386.
R.C.M.P. NEWS RELEASE
MEAT SPECIALS
Sirloin Steak $4.29 lb $9.46 kg
Bulk Wieners .99 lb $2.18 kg
Olympic Bacon 500 g $2.69 each
Olympic Party Sticks 500 g $2.89 each
Resers Deli Sticks 397 g $1.69 each
Olympic Wieners 450 g $1.99 each
Recently the local
R.C.M. Police and Magrath
Community Advisory
Committee came up with a
plan to promote Bicycle and
Helmet Safety in Magrath.
Cst. McRoberts attended the
Magrath Elementary School
and spoke to grade 1-6
students showing them the
video ""Gearing Up”. We
would like to thank all the
students who participated in
the poster contest. Due to the
excellent response, we had a
very difficult time in picking
the winners. Thanks also to
Coca Cola Bottling Ltd. who
donated 10 Wayne Gretzky
Sports Bottles for the
honorable mention prizes.
CHRISTIE VORNBROCK
was the winner of a Bicycle
Helmet in the grades 4-6
category. ASHLEY ZOBELL
was the winner of the Bicycle
Helmet in the grades 1-3
category. Congratulations
Ashley and Christie! This
artwork is now decorating the
walls of the local R.C.M.P.
detachments. Thanks again
everyone and remember: USE
YOUR HEAD AND CYCLE SAFE.
!
Oate: July 26 - July 36
Place: Tom Karren Gym
Cost: $68.88 Per Rthlete
Eligibility:
Camp *R*~ Grades 4-6 (Co-ed)
Camp *B‘- Grades 7-9 (Co-ed)
Time: Camp *fl*- 9:88am-12:88pm
Camp *B'~ 1:8Bpih-4:88pm
instructor Philip Tollestrup
Note: -This camp will operate on a first come first serve basis.
- Rthletes must provide their own sneakers and gym clothes.
- Each camp will include fundamental drills on Shooting. Defence,
-Individual moves. Simple Team Offences, and Rebounding.
-There will also be scrimmages each day and a tournament on
the final day of each camp.
-Camp T-Shirts will be given to each participant.
iBiffliisir®oo®Ki
!
Name:.
Grade:. .Phone:_______________________________
Camp'H'-Upper Elementary $68.68
Camp *8‘-Junior High $66.88
Send this form with tuition fees to: Philip Tollestrup
Boh 398
Magrath RIberta
T8K 1J8
758-6716
OR: Bring the form with tuition to Mr. Tollestrup at schoo
Please Check one:
Once again the four Magrath Wards Relief
Societies, the Magrath United Church, Catholic
Church, and Lutheran Church ladies are
combining their efforts this Friday, July 23rd
and Saturday, July 24th in a ""GIANT
COMMUNITY BAKE SALE"" to be held in the
Magrath Ice Arena. Please have your baking at
the Arena by 2:00 p.m. on Friday, July 23rd for
pricing. ""Thanks"". Selling will begin Friday,
July 23rd at 4:00 - 7:00 p.m. Selling Saturday,
July 24th will be from noon until baking is all
gone. Please bring your baking wrapped and
ready to sell. All proceeds will go to the
""LIBRARY AND MUSEUM BUILDING
FUND"" Best sellers are meat pies, butterhorns,
bread, fruit pies, tarts, donuts, popcorn balls,
Rice Krispie squares, and various rhubarb treats.
Very slow sellers are fruit loaves, squares, and
muffins, or any kind of cupcakes and cookies.
Bake ahead and freeze if you won't have time
later. VOLUNTEERS ARE NEEDED. Save
your bags for bake sale counter. PLEASE
SUPPORT THIS VERY WORTHWHILE
COMMUNITY PROJECT. For more information
please contact Irene Ririe 758-3456, Lorraine
Balderson 758-6380, or Alice Stevenson 758-3190
*******
The Public Library is again sponsoring an exciting
SUMMER READING PROGRAM - ""Book To
The Future"" for all children who have completed
Grades 1 - 6. Come to the library every day for
fun, games, activities, and prizes, too.
Registration has started and will continue all
summer until August 26th. Even if you are out of
town a lot, you can get your name in the draw
when you come in. This year the ""Green Team""
club meetings will be held every Thursday
afternoon between 1 - 2 p.m. Drop in for fun and
surprises each week before going to Public Swim.
Attention all JUNIOR HIGH readers - we are
having a separate contest for you this year with
prizes and fun. Summer Reading Programs
prevent the loss of reading and vocabulary skills
that commonly occurs among children over the
summer vacation. They are also fun and
challenging activities which students will enjoy.
We hope that parents will encourage their children
to visit the library.
REAL ESTATE
FOR SALE
RETIRE ON THE WAY TO
WATERTON!
* Modern bungalow with 5
bedrooms, well-located on large lot
in Cardston. Asking $85,000.
Finished & ready to go. No GST.
*****
PRICE SLASHED!
$12^00 $117,500
* 4.84 acres, subdivided, with
custom built 3 bedroom home, fully
serviced. Quality inside and out.
*****
* New subdivision underway.
1/2 acre lots @ only $4,600 each.
*****
REAL ESTATE NEEDED
I need mid-market priced
residential listings for qualified
buyers wishing to relocate in
Magrath. Prices must support
bank appraisals for financing.
JACK ELLIOTT
Phone 758-3551 or 327-1166
Coldwell Banker
I First Lethbridge Realty
HARDWARE SPECIALS
COLEMAN 19 Litre
WATER CARRIER
______S_a_le_ _$_7_._9_7_ ___
PAINT THINNER
4L $2.99
WEATHER SHIELD WATER SEAL
Repels water, prevents moisture damage.
Reg. $17.99 Sale $12.99
BEACH TOWELS 30"" X 60"" $4.77
WHITE STACKING CHAIRS
Regular $6.99 Sale $3.97
FURNITURE & DECK STAIN
Reg. $27.99 Sale $20.99
SOLAR GARDEN LIGHTS
Regular $44.99 Sale $37.99
RUBBERMAID GARBAGE CAN
121 litre with wheels - Sale $17.99
TERRY
329-0661
NEON SIGNS
TRUCK SIGNS
BACKLIGHT SIGNS
BANNERS AND PLACARDS
PORTABLE SIGN SALES AND RENTALS
T & C SIGNS
QUALITY PERSONALIZED SIGN SERVICE
GROCERY SPECIALS
Alpha Yogurt
175 g
.78 each
Beatrice Fruit Bottom Yogurt
175 g
3 for $1.59
Beatrice Sour Cream
500 ml
$1.49 each
Beatrice Popsicles
12 pack
$2.29 each
Beatrice Creamsicles
12 pack
$2.69 each
1 McGavins 100% Whole Wheat Country Bread
567 g
$1.18 each
Bistro Dutch Bread
570 g
$1.18 each
| W.F. Ketchup
1 litre
$1.98 each
French's Mustard
500 ml
$1.88 each
Bicks Relis","Magrath Store News (July 7, 1993)",,J. A. Ririe,,core,
341985689,,,"Volume 2, Issue 3, Special issue on Recent Advances in Engineering Systems (Published Papers) Articles Transmit / Received Beamforming for Frequency Diverse Array with Symmetrical frequency offsets  Shaddrack Yaw Nusenu  Adv. Sci. Technol. Eng. Syst. J. 2(3), 1-6 (2017); View Description Detailed Analysis of Amplitude and Slope Diffraction Coefficients for knife-edge structure in S-UTD-CH Model  Eray Arik, Mehmet Baris Tabakcioglu  Adv. Sci. Technol. Eng. Syst. J. 2(3), 7-11 (2017); View Description Applications of Case Based Organizational Memory Supported by the PAbMM Architecture  Martín, María de los Ángeles, Diván, Mario José  Adv. Sci. Technol. Eng. Syst. J. 2(3), 12-23 (2017); View Description Low Probability of Interception Beampattern Using Frequency Diverse Array Antenna  Shaddrack Yaw Nusenu  Adv. Sci. Technol. Eng. Syst. J. 2(3), 24-29 (2017); View Description Zero Trust Cloud Networks using Transport Access Control and High Availability Optical Bypass Switching  Casimer DeCusatis, Piradon Liengtiraphan, Anthony Sager  Adv. Sci. Technol. Eng. Syst. J. 2(3), 30-35 (2017); View Description A Derived Metrics as a Measurement to Support Efficient Requirements Analysis and Release Management  Indranil Nath  Adv. Sci. Technol. Eng. Syst. J. 2(3), 36-40 (2017); View Description Feedback device of temperature sensation for a myoelectric prosthetic hand  Yuki Ueda, Chiharu Ishii  Adv. Sci. Technol. Eng. Syst. J. 2(3), 41-40 (2017); View Description Deep venous thrombus characterization: ultrasonography, elastography and scattering operator  Thibaud Berthomier, Ali Mansour, Luc Bressollette, Frédéric Le Roy, Dominique Mottier  Adv. Sci. Technol. Eng. Syst. J. 2(3), 48-59 (2017); View Description Improving customs’ border control by creating a reference database of cargo inspection X-ray images  Selina Kolokytha, Alexander Flisch, Thomas Lüthi, Mathieu Plamondon, Adrian Schwaninger, Wicher Vasser, Diana Hardmeier, Marius Costin, Caroline Vienne, Frank Sukowski, Ulf Hassler, Irène Dorion, Najib Gadi, Serge Maitrejean, Abraham Marciano, Andrea Canonica, Eric Rochat, Ger Koomen, Micha Slegt  Adv. Sci. Technol. Eng. Syst. J. 2(3), 60-66 (2017); View Description Aviation Navigation with Use of Polarimetric Technologies  Arsen Klochan, Ali Al-Ammouri, Viktor Romanenko, Vladimir Tronko  Adv. Sci. Technol. Eng. Syst. J. 2(3), 67-72 (2017); View Description Optimization of Multi-standard Transmitter Architecture Using Single-Double Conversion Technique Used for Rescue Operations  Riadh Essaadali, Said Aliouane, Chokri Jebali and Ammar Kouki  Adv. Sci. Technol. Eng. Syst. J. 2(3), 73-81 (2017); View Description Singular Integral Equations in Electromagnetic Waves Reflection Modeling  A. S. Ilinskiy, T. N. Galishnikova  Adv. Sci. Technol. Eng. Syst. J. 2(3), 82-87 (2017); View Description Methodology for Management of Information Security in Industrial Control Systems: A Proof of Concept aligned with Enterprise Objectives.  Fabian Bustamante, Walter Fuertes, Paul Diaz, Theofilos Toulqueridis  Adv. Sci. Technol. Eng. Syst. J. 2(3), 88-99 (2017); View Description Dependence-Based Segmentation Approach for Detecting Morpheme Boundaries  Ahmed Khorsi, Abeer Alsheddi  Adv. Sci. Technol. Eng. Syst. J. 2(3), 100-110 (2017); View Description Paper Improving Rule Based Stemmers to Solve Some Special Cases of Arabic Language  Soufiane Farrah, Hanane El Manssouri, Ziyati Elhoussaine, Mohamed Ouzzif  Adv. Sci. Technol. Eng. Syst. J. 2(3), 111-115 (2017); View Description Medical imbalanced data classification  Sara Belarouci, Mohammed Amine Chikh  Adv. Sci. Technol. Eng. Syst. J. 2(3), 116-124 (2017); View Description ADOxx Modelling Method Conceptualization Environment  Nesat Efendioglu, Robert Woitsch, Wilfrid Utz, Damiano Falcioni  Adv. Sci. Technol. Eng. Syst. J. 2(3), 125-136 (2017); View Description GPSR+Predict: An Enhancement for GPSR to Make Smart Routing Decision by Anticipating Movement of Vehicles in VANETs  Zineb Squalli Houssaini, Imane Zaimi, Mohammed Oumsis, Saïd El Alaoui Ouatik  Adv. Sci. Technol. Eng. Syst. J. 2(3), 137-146 (2017); View Description Optimal Synthesis of Universal Space Vector Digital Algorithm for Matrix Converters  Adrian Popovici, Mircea Băbăiţă, Petru Papazian  Adv. Sci. Technol. Eng. Syst. J. 2(3), 147-152 (2017); View Description Control design for axial flux permanent magnet synchronous motor which operates above the nominal speed  Xuan Minh Tran, Nhu Hien Nguyen, Quoc Tuan Duong  Adv. Sci. Technol. Eng. Syst. J. 2(3), 153-159 (2017); View Description A synchronizing second order sliding mode control applied to decentralized time delayed multi−agent robotic systems: Stability Proof  Marwa Fathallah, Fatma Abdelhedi, Nabil Derbel  Adv. Sci. Technol. Eng. Syst. J. 2(3), 160-170 (2017); View Description Fault Diagnosis and Tolerant Control Using Observer Banks Applied to Continuous Stirred Tank Reactor  Martin F. Pico, Eduardo J. Adam  Adv. Sci. Technol. Eng. Syst. J. 2(3), 171-181 (2017); View Description Development and Validation of a Heat Pump System Model Using Artificial Neural Network  Nabil Nassif, Jordan Gooden  Adv. Sci. Technol. Eng. Syst. J. 2(3), 182-185 (2017); View Description Assessment of the usefulness and appeal of stigma-stop by psychology students: a serious game designed to reduce the stigma of mental illness  Adolfo J. Cangas, Noelia Navarro, Juan J. Ojeda, Diego Cangas, Jose A. Piedra, José Gallego  Adv. Sci. Technol. Eng. Syst. J. 2(3), 186-190 (2017); View Description Kinect-Based Moving Human Tracking System with Obstacle Avoidance  Abdel Mehsen Ahmad, Zouhair Bazzal, Hiba Al Youssef  Adv. Sci. Technol. Eng. Syst. J. 2(3), 191-197 (2017); View Description A security approach based on honeypots: Protecting Online Social network from malicious profiles  Fatna Elmendili, Nisrine Maqran, Younes El Bouzekri El Idrissi, Habiba Chaoui  Adv. Sci. Technol. Eng. Syst. J. 2(3), 198-204 (2017); View Description Pulse Generator for Ultrasonic Piezoelectric Transducer Arrays Based on a Programmable System-on-Chip (PSoC)  Pedro Acevedo, Martín Fuentes, Joel Durán, Mónica Vázquez, Carlos Díaz  Adv. Sci. Technol. Eng. Syst. J. 2(3), 205-209 (2017); View Description Enabling Toy Vehicles Interaction With Visible Light Communication (VLC)  M. A. Ilyas, M. B. Othman, S. M. Shah, Mas Fawzi  Adv. Sci. Technol. Eng. Syst. J. 2(3), 210-216 (2017); View Description Analysis of Fractional-Order 2xn RLC Networks by Transmission Matrices  Mahmut Ün, Manolya Ün  Adv. Sci. Technol. Eng. Syst. J. 2(3), 217-220 (2017); View Description Fire extinguishing system in large underground garages  Ivan Antonov, Rositsa Velichkova, Svetlin Antonov, Kamen Grozdanov, Milka Uzunova, Ikram El Abbassi  Adv. Sci. Technol. Eng. Syst. J. 2(3), 221-226 (2017); View Description Directional Antenna Modulation Technique using A Two-Element Frequency Diverse Array  Shaddrack Yaw Nusenu  Adv. Sci. Technol. Eng. Syst. J. 2(3), 227-232 (2017); View Description Classifying region of interests from mammograms with breast cancer into BIRADS using Artificial Neural Networks  Estefanía D. Avalos-Rivera, Alberto de J. Pastrana-Palma  Adv. Sci. Technol. Eng. Syst. J. 2(3), 233-240 (2017); View Description Magnetically Levitated and Guided Systems  Florian Puci, Miroslav Husak  Adv. Sci. Technol. Eng. Syst. J. 2(3), 241-244 (2017); View Description Energy-Efficient Mobile Sensing in Distributed Multi-Agent Sensor Networks  Minh T. Nguyen  Adv. Sci. Technol. Eng. Syst. J. 2(3), 245-253 (2017); View Description Validity and efficiency of conformal anomaly detection on big distributed data  Ilia Nouretdinov  Adv. Sci. Technol. Eng. Syst. J. 2(3), 254-267 (2017); View Description S-Parameters Optimization in both Segmented and Unsegmented Insulated TSV upto 40GHz Frequency  Juma Mary Atieno, Xuliang Zhang, HE Song Bai  Adv. Sci. Technol. Eng. Syst. J. 2(3), 268-276 (2017); View Description Synthesis of Important Design Criteria for Future Vehicle Electric System  Lisa Braun, Eric Sax  Adv. Sci. Technol. Eng. Syst. J. 2(3), 277-283 (2017); View Description Gestural Interaction for Virtual Reality Environments through Data Gloves  G. Rodriguez, N. Jofre, Y. Alvarado, J. Fernández, R. Guerrero  Adv. Sci. Technol. Eng. Syst. J. 2(3), 284-290 (2017); View Description Solving the Capacitated Network Design Problem in Two Steps  Meriem Khelifi, Mohand Yazid Saidi, Saadi Boudjit  Adv. Sci. Technol. Eng. Syst. J. 2(3), 291-301 (2017); View Description A Computationally Intelligent Approach to the Detection of Wormhole Attacks in Wireless Sensor Networks  Mohammad Nurul Afsar Shaon, Ken Ferens  Adv. Sci. Technol. Eng. Syst. J. 2(3), 302-320 (2017); View Description Real Time Advanced Clustering System  Giuseppe Spampinato, Arcangelo Ranieri Bruna, Salvatore Curti, Viviana D’Alto  Adv. Sci. Technol. Eng. Syst. J. 2(3), 321-326 (2017); View Description Indoor Mobile Robot Navigation in Unknown Environment Using Fuzzy Logic Based Behaviors  Khalid Al-Mutib, Foudil Abdessemed  Adv. Sci. Technol. Eng. Syst. J. 2(3), 327-337 (2017); View Description Validity of Mind Monitoring System as a Mental Health Indicator using Voice  Naoki Hagiwara, Yasuhiro Omiya, Shuji Shinohara, Mitsuteru Nakamura, Masakazu Higuchi, Shunji Mitsuyoshi, Hideo Yasunaga, Shinichi Tokuno  Adv. Sci. Technol. Eng. Syst. J. 2(3), 338-344 (2017); View Description The Model of Adaptive Learning Objects for virtual environments instanced by the competencies  Carlos Guevara, Jose Aguilar, Alexandra González-Eras  Adv. Sci. Technol. Eng. Syst. J. 2(3), 345-355 (2017); View Description An Overview of Traceability: Towards a general multi-domain model  Kamal Souali, Othmane Rahmaoui, Mohammed Ouzzif  Adv. Sci. Technol. Eng. Syst. J. 2(3), 356-361 (2017); View Description L-Band SiGe HBT Active Differential Equalizers with Variable, Positive or Negative Gain Slopes Using Dual-Resonant RLC Circuits  Yasushi Itoh, Hiroaki Takagi  Adv. Sci. Technol. Eng. Syst. J. 2(3), 362-368 (2017); View Description Moving Towards Reliability-Centred Management of Energy, Power and Transportation Assets  Kang Seng Seow, Loc K. Nguyen, Kelvin Tan, Kees-Jan Van Oeveren  Adv. Sci. Technol. Eng. Syst. J. 2(3), 369-375 (2017); View Description Secure Path Selection under Random Fading  Furqan Jameel, Faisal, M Asif Ali Haider, Amir Aziz Butt  Adv. Sci. Technol. Eng. Syst. J. 2(3), 376-383 (2017); View Description Security in SWIPT with Power Splitting Eavesdropper  Furqan Jameel, Faisal, M Asif Ali Haider, Amir Aziz Butt  Adv. Sci. Technol. Eng. Syst. J. 2(3), 384-388 (2017); View Description Performance Analysis of Phased Array and Frequency Diverse Array Radar Ambiguity Functions  Shaddrack Yaw Nusenu  Adv. Sci. Technol. Eng. Syst. J. 2(3), 389-394 (2017); View Description Adaptive Discrete-time Fuzzy Sliding Mode Control For a Class of Chaotic Systems  Hanene Medhaffar, Moez Feki, Nabil Derbel  Adv. Sci. Technol. Eng. Syst. J. 2(3), 395-400 (2017); View Description Fault Tolerant Inverter Topology for the Sustainable Drive of an Electrical Helicopter  Igor Bolvashenkov, Jörg Kammermann, Taha Lahlou, Hans-Georg Herzog  Adv. Sci. Technol. Eng. Syst. J. 2(3), 401-411 (2017); View Description Computational Intelligence Methods for Identifying Voltage Sag in Smart Grid  Turgay Yalcin, Muammer Ozdemir  Adv. Sci. Technol. Eng. Syst. J. 2(3), 412-419 (2017); View Description A Highly-Secured Arithmetic Hiding cum Look-Up Table (AHLUT) based S-Box for AES-128 Implementation  Ali Akbar Pammu, Kwen-Siong Chong, Bah-Hwee Gwee  Adv. Sci. Technol. Eng. Syst. J. 2(3), 420-426 (2017); View Description Service Productivity and Complexity in Medical Rescue Services  Markus Harlacher, Andreas Petz, Philipp Przybysz, Olivia Chaillié, Susanne Mütze-Niewöhner  Adv. Sci. Technol. Eng. Syst. J. 2(3), 427-434 (2017); View Description Principal Component Analysis Application on Flavonoids Characterization  Che Hafizah Che Noh, Nor Fadhillah Mohamed Azmin, Azura Amid  Adv. Sci. Technol. Eng. Syst. J. 2(3), 435-440 (2017); View Description A Reconfigurable Metal-Plasma Yagi-Yuda Antenna for Microwave Applications  Giulia Mansutti, Davide Melazzi, Antonio-Daniele Capobianco  Adv. Sci. Technol. Eng. Syst. J. 2(3), 441-448 (2017); View Description Verifying the Detection Results of Impersonation Attacks in Service Clouds  Sarra Alqahtani, Rose Gamble  Adv. Sci. Technol. Eng. Syst. J. 2(3), 449-459 (2017); View Description Image Segmentation Using Fuzzy Inference System on YCbCr Color Model  Alvaro Anzueto-Rios, Jose Antonio Moreno-Cadenas, Felipe Gómez-Castañeda, Sergio Garduza-Gonzalez  Adv. Sci. Technol. Eng. Syst. J. 2(3), 460-468 (2017); View Description Segmented and Detailed Visualization of Anatomical Structures based on Augmented Reality for Health Education and Knowledge Discovery  Isabel Cristina Siqueira da Silva, Gerson Klein, Denise Munchen Brandão  Adv. Sci. Technol. Eng. Syst. J. 2(3), 469-478 (2017); View Description Intrusion detection in cloud computing based attack patterns and risk assessment  Ben Charhi Youssef, Mannane Nada, Bendriss Elmehdi, Regragui Boubker  Adv. Sci. Technol. Eng. Syst. J. 2(3), 479-484 (2017); View Description Optimal Sizing and Control Strategy of renewable hybrid systems PV-Diesel Generator-Battery: application to the case of Djanet city of Algeria  Adel Yahiaoui, Khelifa Benmansour, Mohamed Tadjine  Adv. Sci. Technol. Eng. Syst. J. 2(3), 485-491 (2017); View Description RFID Antenna Near-field Characterization Using a New 3D Magnetic Field Probe  Kassem Jomaa, Fabien Ndagijimana, Hussam Ayad, Majida Fadlallah, Jalal Jomaah  Adv. Sci. Technol. Eng. Syst. J. 2(3), 492-497 (2017); View Description Design, Fabrication and Testing of a Dual-Range XY Micro-Motion Stage Driven by Voice Coil Actuators  Xavier Herpe, Matthew Dunnigan, Xianwen Kong  Adv. Sci. Technol. Eng. Syst. J. 2(3), 498-504 (2017); View Description Self-Organizing Map based Feature Learning in Bio-Signal Processing  Marwa Farouk Ibrahim Ibrahim, Adel Ali Al-Jumaily  Adv. Sci. Technol. Eng. Syst. J. 2(3), 505-512 (2017); View Description A delay-dependent distributed SMC for stabilization of a networked robotic system exposed to external disturbances",,'ASTES Journal',10.25046/aj020366,core,
42703169,Oct-95,"Proceedings from symposia of the Technology 2004 Conference, November 8-10, 1994, Washington, DC. Volume 2 features papers on computers and software, virtual reality simulation, environmental technology, video and imaging, medical technology and life sciences, robotics and artificial intelligence, and electronics","Technology 2004, Vol. 2",https://core.ac.uk/download/pdf/42703169.pdf,,,core,
106097620,1989,"Next generation real-time systems will require greater flexibility and predictability than is commonly found in today&apos;s ystems. These future systems include the space sta-tion, integrated vision/robotics/AI systems, collections of humans/robots coordinating to achieve common objectives (usually in hazardous environments such as undersea explo-ration or chemical plants), and various command and control applications. The Spring kernel is a research oriented kernel designed to form the basis of a flexible, hard real-time operating system for such applications. Our approach challenges everal basic assump-tions upon which most current real-time operating systems are built and subsequently advocates a new paradigm based on the notion of predictability and a method for on-line dynamic guarantees ofdeadlines. The Spring kernel is being implemented on a network of (68020 based) multiprocessors called SpringNet. 1 In t roduct ion Real-t ime computing is that type of computing where the correctness of the system depends not only oll the logical result of the computation, but also on the time at which the results are produced. Real-t ime computing systems play a vital role in our society and the spectru",A New Paradigm for Real-Time Operating Systems*,,,,core,
22499114,1996,"A robot control architecture called SPOTT is proposed and implemented as a real-time and parallel system of concurrently executing and co-operating modules. SPOTT provides a bridge for linking behavioral (i.e., reactive) and symbolic control. The control system is a real-time AI system which is responsible for dynamically adapting to changing environmental circumstances in order to successfully execute and complete a set of navigational tasks for an autonomous mobile robot. SPOTT consists of a behavioral controller, a local dynamic path planner, and a global path planner, as well as a map and a graphical user interface. The behavioral control formalism is called TR+ and is based on an adaptation and extension of the TeleoReactive (TR) formalism introduced by Nils Nilsson. TR+ rules make decisions which affect actuator control and map database maintenance. A dynamic local path planner continually polls the map database in order to navigate around newly encountered obstacles. The loc..",SPOTT: A Mobile Robot Control Architecture for Unknown or Partially Known Environments,,AAAI Press,,core,
23581667,1995,". The work presented in this paper investigates one possible methodology for training an agent by means of an automatic, qualitative teacher monitoring the behaviour of the learner and continuously interacting with it. It is a hybrid methodology integrating teaching and reinforcement learning. Moreover it encompasses a novelty in the field, the implementation of a Shaping procedure to be used for training. Preliminary , interesting results are reported at the end of the paper. Key Words. teaching, reinforcement learning, real-world applications, control, robotics 1 INTRODUCTION Recently, there has been a growing interest in applying Machine Learning (ML) to robotics. One of the main motivations is the need for adaptive, flexible agents, which is not fulfilled by traditional control theory. Classical PI(D) controllers, in fact, cannot face any situation which has not been foreseen by the designers. Moreover, control theory primarily supports the production of linear controllers, while n..",Teaching by Shaping,,,,core,
23915232,1994,"this paper, our aim is to show that such behavior, including switching between goals, can be simulated by simple artificial Neural Networks (NN) where no complex computation is performed. We will present a real development and simulations about a Khepera^TM robot (fig. 1) and a simulated system named Prometheus. Figure 1: The Khepera^TM robot developed at the LAMI [Mon93]. We use a novel neural architecture named PerAc (Perception-Action) which is a systematic way to decompose the control of an autonomous robot in perception and action flows [Gau94b]. We show that action simplifies the interpretation of perception: each action is a choice and conditions entirely the future of the robot. That way, acting in the world is necessary to the categorization and hence the interpretation of perceived signal, i.e., to the emergence of an elementary &quot;cognition&quot;. The greatest advantage of this type of approach is that it makes cognition sequential, thereby avoiding the possible large duplications and relaxation mechanisms needed by massively parallel systems. We also focus on the interest to perform an autonomous on line learning of the relevant places to the robot in its environment. Furthermore, we compel ourselves not to touch modify the internal structures of the artificial robot &quot;brain&quot; by hand, while it operates. Thus we have to: - design a self modifiable connection diagram. - pay attention to the self adaptability of each simple block to data variations. - allow the robot to use the signals correlations which are really relevant to it. - introduce a limbic system to control the robot&apos;s learning, motivation and behaviors. We emphasize the interest of a constructivist approach [Mat87], [Ste91] as implemented for instance by the subsumption architecture [Bro86]. A special stre..",Navigating With an Animal Brain: A Neural Network for Landmark Identification and Navigation.,,,,core,
42786583,"Feb 1, 1994","Proceedings from symposia of the Technology 2003 Conference and Exposition, Dec. 7-9, 1993, Anaheim, CA, are presented. Volume 2 features papers on artificial intelligence, CAD&E, computer hardware, computer software, information management, photonics, robotics, test and measurement, video and imaging, and virtual reality/simulation","Technology 2003: The Fourth National Technology Transfer Conference and Exposition, volume 2",https://core.ac.uk/download/pdf/42786583.pdf,,,core,
323896730,,"Safety is an important requirement for human-robot interaction. Compliance control can often help to ensure safety in human-robot interaction (HRI). The aim of this work is to develop a compliance control strategy for safe HRI. Compliance can be achieved through passive means (mechanical structure or passive actuation) or through active compliance methods, employing force/torque feedback. This thesis deals with the compliance control of Bristol-Elumotion Robot Torso (BERT) II robot arm which is inherently rigid  and heavy. As the dynamic model of the arm is difficult to obtain and prone to inaccuracies, parametric uncertainties and un-modelled  nonlinearities, a model-free adaptive compliance controller is employed. The control scheme is using a mass-spring-damper system as reference model to produce compliant behaviour. The adaptive control scheme may cause actuator saturations, which could lead to instabilities and eventually windup. Hence, an anti-windup compensator is employed to address actuator saturation issues.  The control scheme is a Cartesian one (tracking x, y and z coordinates) and employing four joints (namely, shoulder flexion, shoulder abduction, humeral rotation and elbow flexion joints) of the BERT II arm. Although, this needs three degrees of freedom (DOF), the fourth redundant DOF is employed to generate human-like motion, minimising a gravitational function. The adaptive compliance control scheme works efficiently for the application and produces good tracking and compliance results.It is often the case that adaptive control schemes are not necessarily (control) optimal, which may create difficulties in the controller design. Furthermore, it is difficult to incorporate constraints or any other desired behaviour. Therefore,  bio-inspired reinforcement learning (RL) schemes are explored. A recently formulated RL based optimal adaptive controller scheme is employed and modified for real time testing on our robot arm. The RL based scheme is implemented for non-constrained and constrained cases in the joint space. Particularly, the results produced with the constrained case are encouraging, where the controller learns to deal with the constraints in the form of joint limits.  An RL based Cartesian model reference  compliance controller is also tested for two links of the BERT II arm. Generally, the results with this scheme are very good. However, there are limitations on the representation of the RL cost functions and the control scheme using neural networks (NNs).To a large extent these limitations have been overcome through a novel practical approach of representing the cost function and the control via a simple neural network. Nevertheless, available computational power permitted only two link experimental implementation.Integration of these new control approaches into practical HRI system is important. A final achievement is an initial HRI experiment for passing of objects between human and robot employing the model reference adaptive compliance control scheme mentioned in the beginning. This experimental scenario is implemented using also separate hand controller and speech interface",Adaptive and reinforcement learning control methods for active compliance control of a humanoid robot arm,,,,core,
161101822,,"Traffic congestion is a major issue on many urban road networks around the world. The distributed and stochastic nature of traffic has attracted the multi-agent and market mechanism community to the traffic domain which has resulted in many novel approaches to both traffic control and traffic assignment. However, the real-world application of many market-based traffic control systems remains in question because they require technology that has not yet been developed, e.g., autonomous cars. This thesis focuses on the use of market mechanisms for traffic control, more specifically, the application of market principles set forth in market-based multi-robot systems to the traffic domain. Thus, the primary goal of this thesis is the design, implementation and evaluation of a multi-agent market-based traffic control system which does not rely on vehicle agents and other major changes to vehicles or transportation infrastructure. Evaluation of the traffic control system is conducted on two grid-based maps using six different traffic scenarios. The traffic scenarios represent various traffic patterns which include changes in traffic intensity and direction. The traffic scenarios are simulated in SUMO, an open source, macro traffic simulator. Additionally, performance is measured using three metrics: travel time, traffic density, and number of stops. This thesis makes five contributions: (i) demonstration of the efficacy of a novel multi-agent market-based traffic control methodology; (ii) demonstration of the efficacy of a market-based technique for dynamic coalition formation; (iii) analysis of three key traffic control parameters used by SCOOT, a popular urban adaptive traffic control mechanism used in over a dozen countries; (iv ) development of a Python implementation of SCOOT for use on SUMO and (v) a thorough evaluation of the novel market-based mechanisms introduced here, along with SCOOT and a reinforcement-learning traffic controller, over a variety of road traffic conditions. This thesis provides a unique insight into the behaviour of three key traffic control parameters and results show that the novel market-based mechanism has the potential to improve traffic performance in traffic conditions that are less than ideal for SCOOT",An Exploration of Traffic Signal Control using Multi-agent Market-based Mechanisms,https://core.ac.uk/download/161101822.pdf,,10.17638/03021623,core,
23809036,1996,"In this paper, we describe the application of a self-organizing network to the robot which  learns to recognize rooms (enclosures) using behavior sequences. In robotics research, most  studies on recognizing environments have tried to build the precise geometric map with high  sensitive sensors. However many natural agents like animals recognize the environments with  low sensitive sensors, and a geometric map may not be necessary. Thus we attempt to build a  mobile robot using a self-organizing network to recognize the enclosures, in which it acts, with  low sensitive and local sensors. The mobile robot is behavior-based and does wall-following in  an enclosure. Then the sequences of behaviors executed in each enclosure are obtained. The  sequences are transformed into real-value vectors, and inputted to the Kohonen&apos;s self-organizing  network. Unsupervised-learning is done and a mobile robot becomes able to distinguish and  identify enclosures. We fully implemented the system using a ..",Applying Self-Organizing Networks to Recognizing Rooms with Behavior Sequences of a Mobile Robot,,,,core,
24290956,1993,"In his recent papers, entitled &quot;Intelligence without Representation and &quot;Intelligence without Reason,&quot; Brooks argues for studying complete agents in real-world environments and for mobile robots as the foundation for AI research. This article argues that, even if we seek to investigate complete agents in real-world environments, robotics is neither necessary nor sufficient as a basis for AI research. The article proposes real-world software environments, such as operating systems or databases, as a complementary substrate for intelligent-agents research, and considers the relative advantages of software environments as testbeds for AI. First, the cost, effort, and expertise necessary to develop and systematically experiment with software artifacts are relatively low. Second, software environments circumvent many thorny, but peripheral, research issues that are inescapable in physical environments. Brooks&apos;s mobile robots tug AI towards a bottom-up focus in which the mechanics of percept..",Intelligence without Robots (A Reply to Brooks),,,,core,
20733084,1995,"Sensor and motor systems are not separable for autonomous agents to accomplish tasks in a dynamic environment. This paper proposes a method to represent the interaction between a vision-based learning agent and its environment. The method is called “motion sketch ” by which a one-eyed mobile robot can learn several behaviors such as obstacle avoidance and target pursuit. A motion sketch is a collection of visual motion cues detected by a group of visual tracking routines of which visual behaviors are determined by individual tasks, and is tightly coupled with motor behaviors which are obtained by Q-learning, a most widely used reinforcement learning method, based on the visual motion cues. In order for the motion sketch to work, first the fundamental relationship between visual motions and motor commands is obtained, and then the Q-learning is applied to obtain the set of motor commands tightly coupled with the motion cues. Finally, the experimental results of real robot implementation with real-time motion tracker are shown. ",Motion sketch: Acquisition of visual motion guided behaviors,,Morgan Kaufmann,,core,
197952730,1992-01-01T00:00:00,"In this thesis the application of artificial intelligence to monitoring and diagnosis of robotic spacecraft is discussed. Several software prototype systems were developed to serve as testbeds for the research and to evaluate the effectiveness of the approach against real problems and current techniques used in NASA\u27s planetary exploration program.  Software prototypes were used to investigate the verification of robot plan execution. New artificial intelligence algorithms for monitoring and diagnosis of robot systems were designed, programmed, and tested. These included plan analysis for monitoring, sensor planning, generation of expected sensor values, and diagnosis of execution failures caused by hardware, environmental or plan anomalies. Testing was performed on a laboratory telerobotic hardware testbed for satellite servicing and on a mobile planetary rover robot operating in natural terrain.  Artificial intelligence algorithms, software prototypes, and more advanced, operationally capable systems for monitoring ground support systems and actual spacecraft in flight were designed, programmed, and tested. A ground support system that served as one test domain was the mirror cooling circuit of the 25-foot Space Simulator at the Jet Propulsion Laboratory (JPL) in Pasadena, California. A prototype monitoring system for this device based on a theory of ""predictive monitoring"" was developed and tested. Mission operations for the Voyager II spacecraft served as another test domain for an intelligent spacecraft health-monitoring and diagnosis system. This system was successfully tested in support of telecommunications operations during Voyager II\u27s encounter with the planet Neptune in 1989. This was the one of the first artificial intelligence systems to be used in planetary spacecraft operations at NASA/JPL. Subsequently, this system was adapted and tested in support of operations of the Magellan spacecraft telecommunications subsystem and the Galileo spacecraft power and pyro subsystem.  Some of the specific artificial intelligence algorithms that were developed for monitoring and diagnosis included the use of heuristic and causal model-based reasoning techniques for predictive generation of sensor values, sensor selection planning, dynamic alarm limit checking, hierarchical procedure specialists for fault diagnosis, and integration of Al with conventional systems in full-scale monitoring and diagnosis applications.  In support of this overall program of research, novel software engineering tools for artificial intelligence research and application development were also developed and will be discussed in the thesis.  The application of artificial intelligence techniques to the monitoring and diagnosis of robotic space systems was shown to be very effective with specific benefits in the areas of systems autonomy, spacecraft safety, ground operations productivity and automation. As a result of this work in part, artificial intelligence is now considered by senior mission designers to be an enabling technology for on-board automation of planetary rovers and for automation in mission operations at the Jet Propulsion Laboratory",Artificial Intelligence for Monitoring and Diagnosis of Robotic Spacecraft,,,,core,
199611538,1996-01-01T00:00:00,"Earthmoving is a common activity at mines, construction sites, hazardous waste cleanup locations, and road works. Expensive and sophisticated machines such as front-end-loaders (FEL), backhoe loaders, LHD loaders and front shovels are used for these excavation tasks. Autonomous excavation control for these machines has gained considerable attention in order to remove human operators from hazardous environments, improve productivity and utilization, reduce machine abuse, as well as decrease machine operating costs. However, automatic control of excavation tasks for many sites that require digging in rock cannot be implemented using existing factory-based automation techniques. For example, control of bucket motions by simply partitioning the terrain into a set of volumes where each equals the bucket capacity often does not work. Planning in this way is possible only when digging in the materials such as loose soils where bucket motion resistance through the media can be predicted. Resistance predictions are impossible and/or infeasible to generate for excavation in the environments which consists mainly of irregular rigid objects such as rock piles with oversized particles, since no means exists to predetermine subsurface bucket/material interactions that are required to preplan the bucket trajectory. As a result, bucket actions must be determined through on-line decision making based on sensory feedback of the current excavation status in the unpredictable, unstructured and dynamic rock excavation environment. This research proposes a control method for autonomous rock excavation. The control architecture is designed following the behavior-based control concept. That is, the rock excavation control problem is solved by decomposition of the complicated task into a variety of simple elements that can be implemented by excavation behaviors. However, this control approach presents a new structure and operational paradigm that is developed based on, but different from the traditional behavior control method. Here, the behaviors are chosen using fuzzy excavation situation assessment with guidance of excavation task planning which embodies excavation heuristics and human strategies. Task plans are formulated using finite state machines which integrate neural networks for decision making. This organizational structure has the capability to include more excavation goals and to adapt to different environments via learning. Excavation behaviors are performed by primitive and machine executable actions or action sequences structured using finite state machines and simple action arbitration rules. The actions of human FEL operators were observed and analyzed to extract basic bucket actions and define rules of arbitration for different actions or action sequences under particular excavation environments. Fuzzy logic is applied to implement each excavation action where fuzzy rules represent the human experience and heuristics that are intrinsically linguistic, and bucket excavation motions are evaluated based on insufficient and inaccurate input sensory data. A variety of experiments were performed to test the ability of the proposed control algorithm. The laboratory-based experimental autonomous excavation system consists of a robotic arm, an excavation testbed, a force/torque sensor mounted between the robot arm wrist and the excavation bucket, and a control computer. Various rock piles to simulate realistic excavation environments and conditions were generated in the testbed. With these experiments, the control algorithm has demonstrated the ability to execute real-time automated loading cycles effectively and efficiently in complex excavation environments and under difficult digging conditions, through the use of the flexible excavation behaviors",Intelligent control of autonomous rock excavation: Theory and experimentation,,The University of Arizona.,,core,
236529994,,",/
/,r""""M
t:-/~, /-
'I, f tZ~....< (;
/~;-'G.I
£eL c. N'I~ ""'Tk N~ (}p»~ ;""a..7/U..u-'-ti"" '7P- 3(;.3 ~/'!f'V~2<?r:-.i/O;- / - r 79S
his view, is the denial of explanation itself.
Dennett attempts to persuade his read-ers
that the problem of consciousness is
just a childishly tenacious attachment to
Descartes"" version of consciousness. We
must abandon, he says, this ""Cartesian
Theatre"": the notion- tbat the ""1"":of the ego
responds to an observer of events which
are ""present to consciousness"".
Dennett's starting-point is ""parallel dis-tributed
processing"". the idea that the
brain is not so much like a single-sequence
digital computer but, rather. a vast array of
processing units simultaneously at work
which relate to each other through myriad
connections. It is this new technology of
computing, a system which does not re-quire
a single controlling processer, he
argues, that ""blazes the first remotely
plausible trails of unification in the huge
terra incognita lying between the mind
sciences and the brain sciences"". Con-sciousness
for him is an illusion which
emerges from the sheer complexity of
parallel processing units.
At the heart of his ""explanation"" is a
metaphor taken from the art of narrative
---:'the concept of ""multiple drafts"", as in
the many drafts of a text that will eventual-ly
take on the ""semblance"" of a completed
version with a compelling internal integri-ty.
The self, then, the knowing I,"" exists
only in tenns of multiple drafts of stories
that we spin about ourselves on the ""virtual
machine"" of the brain.
TIIE TABLET 25 June-l994
I '-tJ fV~
.c.
From soul to software
n j . • John Cornwell-f/.
'""1~' ..d~.""'-1 tr '"" '~;.. '/ ,-..-,.
The advance of neuroscience has brougbt new.aa.pts to explain consciousness
- or to explain it away, according to tbe critics. 'I1Iisweek end next, the director
or the Science and Human Dimension projod at Jesus CoUegeCambridge
examinesIbe work of two or Ibe leading e~ f1l eliminativemateria6sm.
No human phenomenon has so resisted
scientific explanation as the idea of the
self, traditionally known as the soul;
human higher-order self-consciousness,
that unique inner observation point from
which we view the world.
Consciousness, wrote the Princeton
psychologist, Julian Jaynes, somewhat
lushly, is ""a secret theatre of speechless
monologue end prevenient counsel, an
invisible mansion of all moods, musings
and mysteries, an infinite resort of
i:lisappointments and discoveries"".
The notion of consciousness seems to
stand at the frontier of mind and matter,
subject and object, soul and body, deter-minism
and free will, life and death. Not
only is it the great, final mystery of human
experience; there is something within
many of us that wishes to preserve that
mystery at all costs. Some unconscious
instinct wants to resist the temptation
to explain it, or explain it away; as if a
final explanation, or elucidation, would
threaten the very core of one's being.
Julian Jaynes carved a niche for himself
in the history of psychology with his book
The Origin of Consciousness in the Break-down
of the Bicameral Mind, published in
1976. He came down firmly on the side of
the reductionists, arguing that conscious-ness
is an illusion evolved 3,(0) years ago
as a result of hallucinated internal dia-logues
in times of stress. Curiously, he
argued in conclusion that contemporary
scepticism about consciousness would pro-voke
a fragmentation of authority, values,
the destruction of the cohesion of com-munities
and societies. From this distance
his prophecy seems prescient.
By 1976 deconstruction of the self was
already fashionable under the influence of.
the French philosopher Jacques Derrida
and was soon to spread by a process of
adaption and imitation throughout depart-ments
of literary studies in Europe and
North America. In his novel Nice Work,
published in 1988, David Lodge satirised a
fashionable version familiar in depart-ments
of English literature throughout
Britain in the Eighties:
There is no such thing as the ""self"" on which
capitalism and the classic novel are founded
- that is to say. a finite, unique soul or
essence that constitutes a person's identity;
there is only a subject position in an infinite
web of discourses - the discourses of power,
sex, family, science. religion, poetry, etc.
David Lodge was perhaps unaware that
the object of his satire would soon flourish
in the realms of popularised philosophy of
mind ia the 1990s. Two recent books,
Daniel C. Dennett's Consciousness
ExplaiJooll'(published in 1991). and Francis
~~~!~~~i:,~~n~~';;~~~S~~d~:
May, exemplify a trend in popular exposi-tion
. towards a self-confident denial of
traditional ideas of personhood. Dennett,
a philosopber. and Crick, a bioJogist, have
published new, and related, hypotheses
about 1he self. accessible to. wide
readership!. and representing a formidable
challenge to Christianaccounts of human
identity. :.'
Dennett"" an American in his late fifties,
a former pupil of Gilbert Ryle, and now a
De1Ulett had a way of
delivering left hooks into the
air, as if there were spooky
Cartesians out there to be
thumped into submission.
professor at Tufts University in Boston,
came to Cambridge in the autumn of 1991
as part of a whistle-stop tour to publicise
his book ..Lady Mitchell Hall was fined to
capacity. For an hour he assailed them in a
rapid, combative voice. He had a way of
delivering left hooks into the air, as if there
were spooky Cartesians out there, still
believing in mind-body dualism, to be
thumped into submission.
Dennett is no mere ""promissory mater-ialist""
(Sir Jobn Eccles's phrase for those
cognitive scientists who believe that the
mind-body problem could be solved at
some point in the future). He believes that
we have fOlmd the explanation of mind-body
relationship, and he is convinced that
his solution 8 on the side of human dignity.
He portrays his deconstructionist view of
the self DOt only as a moment of new
intellectual maturity, but as an affirmation
rather than a denial of personhood. Con-sciousness
Explained is an important mile-stone
of contemporary philosophy, pre-cisely
because it denies a version of the
self, of conscious experience, that is clearly
respected and held dear by so many who
ponder these matters. Yet Dennett - the
eliminative materialist par excellence r:
manages to make his opponents, who
stress the mystery, the imponderable
nature of oonsciousness, sound pessimistic,
nihilistic, reductionist, negative. Denial, in
We are almost constantly engaged in pre-senting
ourselves to others, and to ourselves,
and hence representing ourselves - in Jan-
:~,guage and gesture, external and internal.
The most obvious difference in our environ-
. ..' ment that would explain this difference in
our behaviour is the behaviour itself. Our
human environment contains not just food
and shelter, enemies to fight or flee, and
conspecifics with whom to mate, but words,
words, words ... protective strings of narra-ti,,~.
Perhaps the most remarkable aspect of
Dennett's theory is his admission that his
materialist explanation involves the sub-stitution
of one set of metaphors for,
another. This comes in the startling two .
sentences with which he concludes his
book:
I haven't replaced metaphorical theory, the
Cartesian Theatre. with a non-metaphorical
(""literal, scientific"") theory. All I have.done,
really, is to replace one family of metaphors
and images with another, trading in the
Theatre, the Witness. the Central Meaner.
the Figment, for Software, Virtual
Machines. Multiple Drafts, a Pandemonium
ot Homunculi!
This Jlublication
Is available
in microform
from University_
Microfilms
International.
796
TO RUSSIA WITH LO·YE .' ;'~'£""_-' r-,.,/:.,_t:.... t,.
We can't Just talk about unity:' we have'to:llvelt.'
Many of the 6,0.0.0.Russian Orthodox 'priests are. d'esp~~tely
poor, earning £20. a month or less. They have to work to feed
their families, instead of looking after their faithful. ,....-
In co-operanon with the Orthodox bishops Aid to the Church
In Need is offering support to all the Orthodox priests in
Russia - and to all the Catholic priests as well. .
This year, we want to give £70.0. to every priest in Russia.
Please help us in this ecumenical initiative -:- responding with
love to the Pope's call for unity with our Orthodox sister-churches.
Al\ Aid to the Church in Need
•• 124 carshalton Road, Sutton, Surrey SM1 4RL
Tel: 081-642 8888 Reglst_ Charily No.285582
Name .•.•.•.•.•...•.........•.•.•....•.•.•.•..•.•.....•...•.•.•.•..•.......•.......•........•.•.•.. ,..•.•.•..
Address .•.•.•.•..•.•...•..•..•...•.•.•.•.•........•.•.•.•.•.•.•.•.•.•. :•.•..•.•.•.•.•.•.•.•.. :.•.•.•.•.•.•..
I enclose 0 £150 £500 £1000 £2500£7000
o Other £ .
o for Russian Orthodox priests
o for Calholic priests in Russia and Eastern Europe
I enclose a cheque OR
Please debit my VISN ACCESs/CAF CharityCard account no.
00000000 00000000
Expiry date--.1_ Tab25/6
THE TABLET 25 June 1994
.imet Daniel Dennett at the launch party
in London for his book. With reason, I
could congratulate him on. his success.
Consciousness Explained had been greeted
with ecstatic reviews on both sides of the
Atlantic, and declared one of the New
York Times ten best books of 1991. And
yet not all his populist press reviewers were
so adulatory. so uncritical. Sitting in the
comer of the library of the Travellers""
Oub, the venue for the launch party. was
the lugubrious figure ui Professor Stuart
Sutherland •. director of the centre for
research on perception and cognition at
Sussex University. perhaps pondering the
article he had written on Dennett's book
for the Boston Glooe, the principal daily
paper in Dennett's home town.
Sutherland's overall verdict in the review
calls in question the curious hubris of
eliminative materialism in going well
beyond what can be inferred from the logic
of its position. His criticisms a~e an exam-ple
of clear thinking - on a topic that
professional philosophers all too often tend
to obfuscate among themselves:
Dennett believes that the brain is made up of
a great many small unintelligent specialist
centres, whose combined activity is responsi-
... ble for intelligent behaviour and constitutes
consciousness: there is no controlling centre
where all the information is put together and
which is the seat of consciousness. Even if he
were tight -and he may be wrong since only
, a very small number of ideas can be enter-tained
in consciousness at anyone time -
.this would.not prove that consciousness and
brain processes were identical. Moreover, an
army, a. factory or an ant-hill are also
, organised in separate units working for the
most part independently, yet we are under
no temptation to ascribe to them some form
of collective consciousness.
Philosophers are of course the supreme
intellectuals and Dennett believes that if a
-computer could think intelligently it would
be conscious. But would we really consider
something conscious that evinced no sign of
emotion? A computer. could never bave.,
'genuine human emotions for the simple
:.reason tbat it does not have the same
biological constraints. How could an entity
.. ,that does not reproduce sexually feel real
«sexual jealousy?
And what of Dennett's assurance that
his brand of eliminative materialism offers
no insult to human dignity? In the final
chapter of Consciousness Explained he
attempts to make good his promise to
restore a maturer human dignity. He refers
there to ""that delicate part of our belief
environment concerned, with the disposi-tionof
our bodies after death"".
Few of us, he says, would approve of the
idea of disposing of our dead in plastic bags
as trash, not because we believe ""that
corpses can actually suffer some indignity"".
but because that corpse is the ""body of
dear old Jones, a Centre of Narrative
Gravity that owes its reality as much to our
collaborative efforts of mutual heterophe-nomenologicaJ
interpretation as to the
body that is now lifeless"".
The term ""heterophencmenological"" is
interesting in this context. It is used by
THETABLET 25 June 1994
anthropologists in investigating unfamiliar
cultures. The heterophenomenological in-vestigator
neither challenges nor accepts as
entirely true the assertions of his subjects,
but rather maintains a constructive and
sympathetic neutrality in the hopes of
compiling a definitive description of their
world.
Adopting this perspective, Dennett goes
on:
Treating a corpse ""badly"" may not directly
harm any dying person, and certainly doesn't
harm the corpse, but if it became common
practice and this became widely known (as it
would), this wou1d significantly· change the
belief environment that surrounds dying.
People would imagine tbe events that were
due 10 follow their demise differently from
the way they now imagine them, and in ways
that would be particularly depressing. Maybe
not for any good reason, but so what? If
people are going to be depressed, that in
itself is a good reason for not adopting a
policy.
It is interesting that Dennett should
choose a dead body rather than a living, or
a dying, body, for his thought experiment.
For presumably he would invoke the same
rationale of respect with the living as with
the dead. Why should we not treat a living
body as a piece of garbage? Because,
Dennett would say, tbis might change the
""belief environment that surrounds the
living""; and that would be wrong because
they might be depressed.
So what if two groups of people - say,
Bosnians and Serbs - find the ""belief
environment"" of their neighbours objec-tionable
to the point where their geog-raphical
co-existence becomes ""depress-ing""?
Would this justify mutual attempts at
ethnic cleansing? .
I do not wish to suggest that Dr Dennett
would condone any such tbing; but it is
surely crucial that we test an account of
human respect to see if it answers -every
case.
Our reluctance to depress people, De-nnett
continues, does not mean that we
should not open the Pandora's Box wbich
may lead to the exposure of their myths:
myths, in bis view, including such nonsense
as individual personhood, and certainly the
traditional idea of the soul. ""Those who
are worried"", he says, ""about the costs
threatened by this unasked-for enlighten-ment
should take a hard look at the costs of
the current myths. Do we really tbink what
we are currently confronted with is worth
protecting with some creative obscurant-ism?""
Currently confronted with what? And
now, at last, he conjures up a few scenarios
more to the point than the fate of corpses:
Do we think, for instance, that vast resources
should be seraside to preserve the imaginary
prospects of a renewed mental life for deeply
comatose people, while there are no re-sources
to spare to enhance the desperate,
but far from imaginary, expectations of the
poor? Myths about the sanctity of life, or of
consciousness, cut both ways. They may be
useful in erecting barriers (against euthana-sia,
against capital punishment, against abor-tion,
against eating meat) to impress the
unimaginative, but at the price of offensive
hypocrisy or ridiculous self-deception among
the more enlightened ..
The moral relativism inf~rred by placing
meat-eating in the same sentence as abor-tion
speaks for itself; bllt what can he
possibly mean?
He is saying that the ""sanctity of life"" is a
myth which the enlightened should patro-
'ruse with a heterophenomenoiogical in-dulgence
until such time as it conflicts with
mature principles as understood by the
imaginative; at which point it will be just
too bad if the unimaginitive are depressed
by an offence against their ""belief environ-ment"",
So what, in_the ~alamuYSis',is Dennett
The contemporary crisis over
human identity lfnddignity is
every hit as grave and far-reaching
as one could have
supposed it to he.
putting forward as the basis of human
dignity? How- does, he propose that we
should transcend all these 'degenerate
myths? What is a human being? His answer
is as follows: '
The campaign that used to be waged against
materialism has already succumbed .to
797
embarrassment, and the campaign against
""strong AI"" [the theory that human beings
and robots are equivalent), while equally
well-intentioned, can offer only the most
threadbare alternative models of the mind.
Surely it would be better to try to foster an
appreciation for the non-absolutist, non-intrinsic,
non-dichotomised grounds for mor-
'at concern that can co-exist with our increas-ing
knowledge of the inner workings of that
most amazing machine, the brain.
So this is bis challenge. If all the argu-ments
that can be brought against the
proposition of an equivalence between
minds and macbines are threadbare, our
account of what a human being is should
proceed from that proposition.
It is interesting that Dennett should see
the issue of consciousness not SO much as
an enquiry or a debate but as a ""cam-paign"",
to be won or lost. Interesting too
that he should deem materialism's success
a consequence of the ""threadbare"" altema-tives
put forward by the ""opposition"". The
fact that his campaign had been assisted by
the widely adulatory reception of Con-sciousness
Explained in newspapers and
periodicals not especially committed to
discussing issues in moral philosophy, sure-ly
demonstrates that the contemporary
crisis over human identity and dignity is
every bit as grave and far- reaching as one
could have supposed it to be.
1. Penguin edition. (1993) £8.99.
2. Simon & Schuster, £16.99.
••• In a second article nut week, John Cornwell
appraises Francis Crick's new book. ' .
WITHAM WELD
solicitors
For legal advice and assistance:
• Churches • Charities & Trusts
• Schools & ColIeges • Employers and Employees
• Companies • Private Individuals
• Clubs and Licensees • Hospitals & Nursing Homes
. • Local Councils and Societies
Can we help you? We are happy to let our reputation
speak for itself. For over 200 years we have been closely
associated not only with the Catholic Church but with
every kind of client and, today, our aim remains the same
- to provide the best quality legal service founded on
expertise and experience: this is ensured by a programme
of continuous improvement and innovation which aims at
providing the highest quality professional service whilst
adhering to traditional values.
For advice and assistance on legal matters or to request a
copy of our brochure please contact:
Mrs Alexa Beale or Patrick Herschan
70 ST GEORGE'S SQUARE, LONDON SWl V 3RD
TEL: 071-8218211 FAX: 071·630 648",Item 0004,,Saint Louis University Libraries Special Collections,,core,
105549260,1991,"We propose that some aspects of task based learning in robotics can be approached using nativist and constructivist views on human sensorimotor development as a metaphor. We use findings in developmental psychology, neurophysiology, and machine perception to guide a robotic learning system&apos;s level of representation both for actions and for percepts. Visually driven grasping is chosen as the experimental task since it has general applicability and it has been extensively researched from several perspectives. An implementation of a robotic system with a dexterous three fingered hand, compliant instrumented wrist, arm and vision is used to test these ideas. Several sensorimotor primitives (vision segmentation and manipulatory reflexes) are implemented in this system and may be though of as the &quot;innate &quot; perceptual and motor abilities of the system. Applying empirical learning techniques to real situations brings up some important issues such as observation sparsity in high dimensional spaces, arbitrary underlying functional forms of the reinforcement distribution and robustness to noise in exemplars. The well established technique of non-parametric projection pursuit regression (PPR) is used to accomplish reinforcement learning by searching for generalization directions determining projections of high dimensional data sets which capture task invariants. Additionally, the learnin",Sensorimotor learning using active perception in continuous domains,,,,core,
4404903,1987-07-01T00:00:00,"This is the CONDOR programmer's manual, that describes the hardware and software that form the basis of the real-time computational architecture built originally for the Utah-MIT hand. The architecture has been used successfully to control the hand and the MIT-Serial Link Direct Drive Arm in the past. A number of such systems are being built to address the computational needs of other robotics research efforts in and around the lab. This manual, which is intended primarily for programmers/users of the CONDOR system, represents our effort at documenting the system so that it can be a generally useful research tool.MIT Artificial Intelligence Laborator",The Condor Programmer's Manual - Version II,https://core.ac.uk/download/4404903.pdf,MIT Artificial Intelligence Laboratory,,core,
1496955,1994-01-01T00:00:00,"The issue of Command and Control (C2) is generally associated with the management infrastructure of large scale systems for warfare, public utilities and public transportation, and is concerned with ensuring that the distributed human elements of command and control can be fully integrated into a coherent, total system. Intelligent Autonomous Systems (IASs) are a class of complex systems that perform tasks autonomously in uncertain, dynamic environments, the management of which can be viewed from the perspective of embedded command and control systems. This thesis establishes a vision for the modular construction of intelligent autonomous embedded C2 systems, which defines a complex integration problem characterised by distributed intelligence, world knowledge and control, concurrent processing on heterogeneous platforms, and real-time performance requirements. It concludes that by adopting an appropriate systems infrastructure model, based on Object Technology, it is possible to view the construction of embedded C2 systems as the integration of a temporally assembled collection of reusable components. To support this metaphor it is necessary to construct a common reference model, or standards framework, for the representation and specification of modular C2 systems. This framework must support the coherent long term development and evolution in system capability, ensuring that systems are extensible, robust and perform correctly. In this research, which draws together the themes of other published research in object oriented systems and robotics, classical AI models for intelligent systems architectures are used to specify the overall system structure, with open systems technologies supporting the interoperation of elements within the architecture. All elements of this system are modelled in terms of objects, with well defined, implementation independent interfaces. This approach enables the system to be specified in terms of an object model, and the development process to be framed in terms of object technology, defining a new approach to IAS development. The implementation of an On-board Command and Control System for an Autonomous Underwater Vehicle is used to validate these concepts. The further application of emergent industrial standards in distributed object oriented systems means that this kind of component-based integration is scaleable, providing a near-term solution to generic command and control problems, including Computer Integrated Manufacturing and large scale autonomous systems, where individual autonomous systems, such as robots, form elements of a complete, total intelligent system, for application to areas such as fully automated factories and cooperating intelligent autonomous vehicles for construction sites",Embedded command and control infrastructures for Intelligent Autonomous Systems,,'University of Southampton',,core,
23473705,1992,"AI is moving away from &quot;toy tasks&quot; such as block stacking towards real-world problems. This trend is positive, but the amount of preliminary groundwork required to tackle a real-world task can be staggering, particularly when developing an integrated agent architecture. To address this problem, we advicate real-world software environments, such as operating systems or databases, as domains for agent research. The cost, effort, and expertise required to develop and systematically experiment with software agents is relatively low. Furthermore, software environments circumvent many thorny, but peripheral, research issues that are inescapable in other environments. Thus, software environments enable us to test agents ina real world yet focus on core AI research issues. To support this claim, we describe our project to develop UNIX softbots (software robots) -- intelligent agnets that interact with UNIX. Existing softbots accept a diverse set of high-level goals, generate and execute plans to achieve these goals in real time, and recover from errors when necessary",Building Softbots for UNIX (Preliminary Report),,,,core,
24329260,1996,". This paper describes a novel qualitative navigation method for indoor environments, implemented on top of a commercial power wheelchair. Our approach is based on qualitative representations of variations in sensor behavior between adjacent regions in space. We use these representations to localize and guide planning and reaction. Off-line, the system accepts as input a topological diagram of the environment and generates a map based on a simple qualitative model of sensor behavior. During execution, the robot controller integrates this map into a reaction module. We have tested this architecture both in simulation and in a real wheelchair. Our experimental results show that the proposed method can safely navigate a real robot in indoor environments. 1 INTRODUCTION  Qualitative Reasoning seeks to develop qualitative models of physical systems [9]. Furthermore, recent AI research aims to develop computational theories of autonomous agents&apos; involvement in their environments [1]. This pa..",Qualitative Autonomous Navigation for Wheelchair Robots,,,,core,
23696504,1995,"This paper presents a view--based approach to map learning and navigation in mazes. By means of graph theory we have shown that the view--graph is a sufficient representation for map behaviour such as path planning. A neural network for unsupervised learning of the view--graph from sequences of views is constructed. We use a modified Kohonen (1988) learning rule that transforms temporal sequence (rather than featural similarity) into connectedness. In the main part of the paper, we present a robot implementation of the scheme. The results show that the proposed network is able to support map behaviour in simple environments.  1 Introduction: The view--graph representation  A cognitive map is a neural mechanism supporting navigation and orientation tasks much as a real map of the environment. Scholkopf and Mallot (1995) presented a mechanism for the learning of a cognitive map of a maze from the sequence of local views encountered when exploring the maze. In this approach, the topologic..",View-Based Cognitive Map Learning By an Autonomous Robot.,,,,core,
132514837,,"The collaboration between humans and robots is one of the most disruptive and challenging research areas. Even considering advances in design and artificial intelligence, humans and robots could soon ally to perform together a number of different tasks. Robots could also became new playmates. In fact, an emerging trend is associated with the so-called phygital gaming, which builds upon the idea of merging the physical world with a virtual one in order to let physical and virtual entities, such as players, robots, animated characters and other game objects interact seamlessly as if they were all part of the same reality. This paper specifically focuses on mixed reality gaming environments that can be created by using floor projection, and tackles the issue of enabling accurate and robust tracking of off-the-shelf robots endowed with limited sensing capabilities. The proposed solution is implemented by fusing visual tracking data gathered via a fixed camera in a smart environment with odometry data obtained from robot's on-board sensors. The solution has been tested within a phygital gaming platform in a real usage scenario, by experimenting with a robotic game that exhibits many challenging situations which would be hard to manage using conventional tracking techniques",Robust robot tracking for next-generation collaborative robotics-based gaming environments,,IEEE,,core,
217406971,1990-01-01T08:00:00,"This dissertation addresses a fundamental problem in computational AI--developing a class of massively parallel, neural algorithms for learning robustly, and in real-time, complex nonlinear transformations from representative exemplars. Provision of such a capability is at the core of many real-life problems in robotics, signal processing and control. The concepts of terminal attractors in dynamical systems theory and adjoint operators in nonlinear sensitivity theory are exploited to provide a firm mathematical foundation for learning such mappings with dynamical neural networks, while achieving a dramatic reduction in the overall computational costs. Further, we derive an efficient methodology for handling a multiplicity of application-specific constraints during run-time, that precludes additional retraining or disturbing the synaptic structure of the  learned  network. The scalability of proposed theoretical models to large-scale embodiments in neural hardware is analyzed. Neurodynamical parameters, e.g., decay constants, response gains, etc., are systematically analyzed to understand their implications on network scalability, convergence, throughput and fault tolerance, during both concurrent simulations and implementation in concurrently asynchronous VLSI, optical and opto-electronic hardware. Dynamical diagnostics, e.g., Lyapunov exponents, are used to formally characterize the widely observed dynamical instability in neural networks as  emergent computational chaos . Using contracting operators and nonconstructive theorems from fixed point theory, we rigorously derive necessary and sufficient conditions for eliminating all oscillatory and chaotic behavior in additive-type networks. Extensive benchmarking experiments are conducted with arbitrarily large neural networks (over 100 million interconnects) to verify the methodological robustness of our network  conditioning  formalisms. Finally, we provide insight for exploiting our proposed repertoire of neural learning formalisms in addressing a fundamental problem in robotics--manipulation controller design for robots operating in unpredictable environments. Using some recent results in task analysis and dynamic modeling we develop the  Perceptual Manipulation Architecture . The architecture, conceptualized within a perceptual framework, is shown to be well beyond the state-of-the-art model-directed robotics. For a stronger physical interpretation of its implications, our discussions are embedded in context of a novel systems\u27 concept for automated space operations",Computational Neural Learning Formalisms for Perceptual Manipulation: Singularity Interaction Dynamics Model.,https://core.ac.uk/download/217406971.pdf,LSU Digital Commons,,core,
291401898,,"The book covers a variety of topics in Information and Communications Technology (ICT) and their impact on innovation and business. The authors discuss various innovations, business and industrial motivations, and impact on humans and the interplay between those factors in terms of finance, demand, and competition. Topics discussed include the convergence of Machine to Machine (M2M), Internet of Things (IoT), Social, and Big Data. They also discuss AI and its integration into technologies from machine learning, predictive analytics, security software, to intelligent agents, and many more. Contributions come from academics and professionals around the world.

Covers the most recent practices in ICT related topics pertaining to technological growth, innovation, and business; Presents a survey on the most recent technological areas revolutionizing how humans communicate and interact; Features four sections: IoT, Wireless Ad Hoc & Sensor Networks, Fog Computing, and Big Data Analytics.(Chapter) The recent advancements in robotic systems set new challenges for robotic simulation software, particularly for planning. It requires the realistic behavior of the robots and the objects in the simulation environment by incorporating their dynamics. Furthermore, it requires the capability of reasoning about the action effects. To cope with these challenges, this study proposes an open-source simulation tool for knowledge-oriented physics-based motion planning by extending The Kautham Project, a C++ based open-source simulation tool for motion planning. The proposed simulation tool provides a flexible way to incorporate the physics, knowledge and reasoning in planning process. Moreover, it provides ROS-based interface to handle the manipulation actions (such as push/pull) and an easy way to communicate with the real robotsPeer Reviewe",A tool for knowledge-oriented physics-based motion planning and simulation,,,,core,
23471122,1993,"Learning plays a vital role in the development of situated agents. In this paper, we explore the use of reinforcement learning to &quot;shape&quot; a robot to perform a predefined target behavior. We connect both simulated and real robots to ALECSYS, a parallel implementation of a learning classifier system with an extended genetic algorithm. After classifying different kinds of Animatlike behaviors, we explore the effects on learning of different types of agent&apos;s architecture (monolithic, flat and hierarchical) and of training strategies. In particular, hierarchical architecture requires the agent to learn how to coordinate basic learned responses. We show that the best results are achieved when both the agent&apos;s architecture and the training strategy match the structure of the behavior pattern to be learned. We report the results of a number of experiments carried out both in simulated and in real environments, and show that the results of simulations carry smoothly to real robots. While most o..",Robot Shaping: Developing Situated Agents through Learning,,,,core,
24319181,1994,"Perception is needed for action, not for the pure sake of the construction of abstract representations, although it does not exclude the role of internal representations for mediating complex behaviours. We think that, for the purpose of building autonomous robots, active perception requires specific recipes for three related aspects: the design of the physical sensory system, the modality and type of information extracted, and the structure and functioning of the control system. We outline a set of solutions for these three aspects and describe their implementation on a real mobile robot through a set of three different experiments using a combination of neural networks and genetic algorithms. The results show that active perception is a useful feature that is exploited by autonomous agents.The experiments show that the combination of genetic algorithms and neural networks is a feasible and fruitful technique for the development of active perception in autonomous agents.  Keywords  Ge..","Active Perception, Navigation, Homing, and Grasping: An Autonomous Perspective",,Society Press,,core,
24349702,1995,"Most real-time scheduling has focused on a relatively small set of independent tasks directly invoked periodically or via interrupts. In many real-time applications such as flexible manufacturing, this system model is too simplistic. In flexible manufacturing two entirely different sets of resources must be scheduled and &quot;connected.&quot; At the highest level, there are raw materials, robot arms, platform space, etc., and at the lowest level there are computational resources. Upon ordering products, high level resources must be scheduled, and the associated computational resources to achieve the manufacturing of those products must also be scheduled. This gives rise to the need for high level Real-Time Artificial Intelligence (RTAI) planners, low level schedulers that can handle large numbers of precedence- and resource-constrained tasks, and a suitable interface between the two schedulers. This paper presents the initial stages of the design and implementation of a flexible manufacturing t..",Multi-Level Scheduling for Flexible Manufacturing,,,,core,
24281666,1996,"This paper presents an automatic design method for fuzzy systems using genetic algorithms. A flexible, compact coding scheme for the genetic representation of the fuzzy rule base is suggested. The method is applied to adapt the behaviour of a mobile robot implemented by means of a fuzzy logic controller. The mobile robot is tested on real world situations.  Keywords: Fuzzy control, genetic algorithms, mobile robot, autonomous agent 1 Introduction  The guiding principle of soft computing is the combination of neural networks, fuzzy logic (FL) and genetic algorithms (GAs) in hybrid systems in order to benefit from the advantages of each methodology. Thirty years ago Lotfi Zadeh [6] founded the theory of fuzzy sets, by extending the classical concept of a set. Unlike classical logic, in which elements either do or do not belong to a set, the degree of membership for elements of a fuzzy set can take on any value of the interval [0; 1]. Fuzzy logic offers a framework for representing imprec..",Evolutionary Algorithms for Learning of Mobile Robot Controllers,,,,core,
80640932,"Dec 1, 1971","Research in the field of artificial intelligence is discussed. The focus of recent work has been the design, implementation, and integration of a completely new system for the control of a robot that plans, learns, and carries out tasks autonomously in a real laboratory environment. The computer implementation of low-level and intermediate-level actions; routines for automated vision; and the planning, generalization, and execution mechanisms are reported. A scenario that demonstrates the approximate capabilities of the current version of the entire robot system is presented",Research and applications:  Artificial intelligence,https://core.ac.uk/download/pdf/80640932.pdf,,,core,
20767457,1994,". The Cooperative Intelligent Real-time Control Architecture (CIRCA) automates the process of designing, scheduling, and executing real-time reactive monitoring and control systems. This paper provides an overview of CIRCA from a design-automation perspective, and illustrates the architecture&apos;s ability to dynamically alter its control system design based on resource limitations or environmental constraints.  Keywords. Artificial intelligence; real-time computer systems; robots; control system design; self-adapting systems. 1. INTRODUCTION The Cooperative Intelligent Real-time Control Architecture (CIRCA) (Musliner et al. 1993, Musliner et al. 1995) is designed to automate the entire process of building a real-time reactive monitoring and control system, from planning tasks, to deriving their constraints, to scheduling them, and finally to executing them predictably. By automating this design and implementation process, CIRCA is &quot;intelligent about real-time.&quot; That is, CIRCA uses AI meth..",Automating The Design Of Real-Time Reactive Systems,,,,core,
22364360,1995,"analytical systems in theform of standard computing software has recently been advanced by the introduction of artificial intelligence (AI), both as expert systems and as neural networks. This paper considers the role of software in system operation, control and automation, and attempts to define intelligence. AI is characterized by its ability to deal with incomplete and imprecise injbrmation and to accumulate knowledge. Expert systems, building on standard computing techniques, depend heavily on the domain experts and knowledge engineers that have programmed them to represent the real world. Neural networks are intended to emulate the pattern-recognition and parallel processing capabilities of the human brain and are taught rather than programmed. The future may lie in a combination of the recognition ability of the neural network and the rationalization capability of the expert system. In the second part of the paper, examples are given of applications ofAI in stand-alone systemsfor knowledge engineering and medical diagnosis and in embedded systems for failure detection, image analysis, user interfacing, natural language processing, robotics and machine learning, as related to clinical laboratories. II is concluded that AI constitutes a collective form of intellectual propery, and that there is a need for better documentation, evaluation and regulation of the systems already being used in clinical laboratories. THEOR",Use of Artificial Intelligence in Analytical Systems for Clinical Laboratory.” Clinical Biochemistry 28,,,,core,
24239041,1996,". One of the most significant cost factors in robotics applications is the design and development of real-time robot control software. Control theory helps when linear controllers have to be developed, but it doesn&apos;t sufficiently support the generation of non-linear controllers, although in many cases (such as in compliance control), nonlinear control is essential for achieving high performance. This paper discusses how Machine Learning has been applied to the design of (non-)linear controllers. Several alternative function approximators, including Multilayer Perceptrons (MLP), Radial Basis Function Networks (RBFNs), and Fuzzy Controllers are analyzed and compared, leading to the definition of two major families: Open Field Function Function Approximators and Locally Receptive Field Function Approximators. It is shown that RBFNs and Fuzzy Controllers bear strong similarities, and that both have a symbolic interpretation. This characteristics allows for applying both symbolic and statis..",Learning Controllers for Industrial Robots,,,,core,
214169740,1991-09-18T07:00:00,"We propose that some aspects of task based learning in robotics can be approached using nativist and constructivist views on human sensorimotor development as a metaphor. We use findings in developmental psychology, neurophysiology, and machine perception to guide a robotic learning system\u27s level of representation both for actions and for percepts. Visually driven grasping is chosen as the experimental task since it has general applicability and it has been extensively researched from several perspectives. An implementation of a robotic system with a dexterous three fingered hand, compliant instrumented wrist, arm and vision is used to test these ideas. Several sensorimotor primitives (vision segmentation and manipulatory reflexes) are implemented in this system and may be thought of as the  innate  perceptual and motor abilities of the system.
Applying empirical learning techniques to real situations brings up some important issues such as observation sparsity in high dimensional spaces, arbitrary underlying functional forms of the reinforcement distribution and robustness to noise in exemplars. The well established technique of non-parametric projection pursuit regression (PPR) is used to accomplish reinforcement learning by searching for generalization directions determining projections of high dimensional data sets which capture task invariants. Additionally, the learning process generally implies failures along the way. Therefore, the mechanics of the untrained robotic system must be able to tolerate grave mistakes during learning and not damage itself. We address this by the use of an instrumented compliant robot wrist which controls impact forces",Robotic Sensorimotor Learning in Continuous Domains,,ScholarlyCommons,,core,
214170588,1992-03-22T08:00:00,"We use findings in machine learning, developmental psychology, and neurophysiology to guide a robotic learning system\u27s level of representation both for actions and for percepts. Visually-driven grasping is chosen as the experimental task since it has general applicability and it has been extensively researched from several perspectives. An implementation of a robotic system with a gripper, compliant instrumented wrist, arm and vision is used to test these ideas. Several sensorimotor primitives (vision segmentation and manipulatory reflexes) are implemented in this system and may be thought of as the  innate  perceptual and motor abilities of the system.
Applying empirical learning techniques to real situations brings up such important issues as observation sparsity in high-dimensional spaces, arbitrary underlying functional forms of the reinforcement distribution and robustness to noise in exemplars. The well-established technique of non-parametric projection pursuit regression (PPR) is used to accomplish reinforcement learning by searching for projections of high-dimensional data sets that capture task invariants.
We also pursue the following problem: how can we use human expertise and insight into grasping to train a system to select both appropriate hand preshapes and approaches for a wide variety of objects, and then have it verify and refine its skills through trial and error. To accomplish this learning we propose a new class of Density Adaptive reinforcement learning algorithms. These algorithms use statistical tests to identify possibly  interesting  regions of the attribute space in which the dynamics of the task change. They automatically concentrate the building of high resolution descriptions of the reinforcement in those areas, and build low resolution representations in regions that are either not populated in the given task or are highly uniform in outcome.
Additionally, the use of any learning process generally implies failures along the way. Therefore, the mechanics of the untrained robotic system must be able to tolerate mistakes during learning and not damage itself. We address this by the use of an instrumented, compliant robot wrist that controls impact forces",A Robotic System for Learning Visually-Driven Grasp Planning (Dissertation Proposal),https://core.ac.uk/download/214170588.pdf,ScholarlyCommons,,core,
55177515,1994-01-01T00:00:00,"Learning plays a vital role in the development of situated agents. In this paper, we explore the use of reinforcement learning to ""shape"" a robot to perform a predefined target behavior. We connect both simulated and real robots to Alecsys, a parallel implementation of a learning classifier system with an extended genetic algorithm. After classifying different kinds of Animat-like behaviors, we explore the effects on learning of different types of agent's architecture (monolithic, flat and hierarchical) and of training strategies. In particular, hierarchical architecture requires the agent to learn how to coordinate basic learned responses. We show that the best results are achieved when both the agent's architecture and the training strategy match the structure of the behavior pattern to be learned. We report the results of a number of experiments carried out both in simulated and in real environments, and show that the results of simulations carry smoothly to real robots. While most of our experiments deal with simple reactive behavior, in one of them we demonstrate the use of a simple and general memory mechanism. As a whole, our experimental activity demonstrates that classifier systems with genetic algorithms can be practically employed to develop autonomous agent",Robot shaping: Developing autonomous agents through learning,,"Elsevier BV:PO Box 211, 1000 AE Amsterdam Netherlands:011 31 20 4853757, 011 31 20 4853642, 011 31 20 4853641, EMAIL: nlinfo-f@elsevier.nl, INTERNET: http://www.elsevier.nl, Fax: 011 31 20 4853598",10.1016/0004-3702(94)90047-7,core,
100000219,1992,"Contract DE-AC07-99ID13727 iii Advances in Artificial Intelligence (AI) and micro-technologies will soon give rise to production of large-scale forces of autonomous micro-robots with systems of innate behaviors and with capabilities of self-organization and real world tasking. Such organizations have been compared to schools of fish, flocks of birds, herds of animals, swarms of insects, and military squadrons. While these systems are envisioned as maintaining a high degree of autonomy, it is important to understand the relationship of man with such machines. In moving from research studies to the practical deployment of large-scale numbers of robots, one of critical pieces that must be explored is the command and control architecture for humans to re-task and also inject global knowledge, experience, and intuition into the force. Tele-operation should not be the goal, but rather a level of adjustable autonomy and high-level control. If a herd of sheep is comparable to the collective of robots, then the human element is comparable to the shepherd pulling in strays and guiding the herd in the direction of greener pastures. This report addresses the issues and development of command and control for large-scale numbers of autonomous robots deployed as a collective force. iv vACKNOWLEDGMENTS A special thanks is extended to the following people who provide","Command and control architectures for autonomous micro-robotic forces,” Idaho",,,,core,
100263910,1991,"We propose that some aspects of task based learning in robotics can be approached using nativist and constructivist views on human sensorimotor development as a metaphor. We use findings in developmental psychology, neurophysiology, and machine perception to guide a robotic learning system&apos;s level of representation both for actions and for percepts. Visually driven grasping is chosen as the experimental task since it has general applicability and it has been extensively researched from several perspectives. An implementation of a robotic system with a dexterous three fingered hand, compliant instrumented wrist, arm and vision is used to test these ideas. Several sensorimotor primitives (vision segmentation and manipulatory reflexes) are implemented in this system and may be though of as the &quot;innate &quot; perceptual and motor abilities of the system. Applying empirical learning techniques to real situations brings up some important issues such as observation sparsity in high dimensional spaces, arbitrary underlying functional forms of the reinforcement distribution and robustness to noise in exemplars. The well established technique of non-parametric projection pursuit regression (PPR) is used to accomplish reinforcement learning by searching for generalization directions determining projections of high dimensional data sets which capture task invariants. Additionally, the learnin",Sensorimotor learning using active perception in continuous domains. AAAI Fall Symposium Series: Sensory Aspects of Robotic Intelligence,,,,core,
24349862,1988,"Many potential uses of qualitative physics, such as robot planning and intelligent computer-aided engineering, require integrating physics with actions taken by agents. This paper proposes to augment qualitative simulation to include the effects of actions to form  action-augmented envisionments. The action-augmented envisionment incorporates both the effects of an agent&apos;s actions and what will happen in the physical world whether or not the agent does something. Consequently, it should provide a richer basis for planning and procedure generation than any previous representation. This paper defines actionaugmented envisionments and an algorithm for directly computing them, along with an analysis of its complexity and suitability for different kinds of problems. We describe our initial implementation and discuss potential extensions, including incremental algorithms.  Keywords: Qualitative reasoning, planning, artificial intelligence. Presented at the 2nd Qualitative Physics Workshop Pa..",Introducing Actions into Qualitative Simulation,,,10.21236/ada466199,core,
76363613,1991-09-18T07:00:00,"We propose that some aspects of task based learning in robotics can be approached using nativist and constructivist views on human sensorimotor development as a metaphor. We use findings in developmental psychology, neurophysiology, and machine perception to guide a robotic learning system\u27s level of representation both for actions and for percepts. Visually driven grasping is chosen as the experimental task since it has general applicability and it has been extensively researched from several perspectives. An implementation of a robotic system with a dexterous three fingered hand, compliant instrumented wrist, arm and vision is used to test these ideas. Several sensorimotor primitives (vision segmentation and manipulatory reflexes) are implemented in this system and may be thought of as the  innate  perceptual and motor abilities of the system.
Applying empirical learning techniques to real situations brings up some important issues such as observation sparsity in high dimensional spaces, arbitrary underlying functional forms of the reinforcement distribution and robustness to noise in exemplars. The well established technique of non-parametric projection pursuit regression (PPR) is used to accomplish reinforcement learning by searching for generalization directions determining projections of high dimensional data sets which capture task invariants. Additionally, the learning process generally implies failures along the way. Therefore, the mechanics of the untrained robotic system must be able to tolerate grave mistakes during learning and not damage itself. We address this by the use of an instrumented compliant robot wrist which controls impact forces",Robotic Sensorimotor Learning in Continuous Domains,https://core.ac.uk/download/76363613.pdf,ScholarlyCommons,,core,
24276048,1994,"Researchers at the University of Michigan&apos;s AI Lab are working to produce mobile robot systems capable of intelligently reacting to events and objects in a real world environment. A system has been built which integrates a reactive planner with perception and navigation capabilities in order to generate flexible, responsive vehicle behavior.  The system architecture consists of four layers; The vehicle control layer, the behavior control layer, the manager layer and the planner layer.  This system design was implemented on a outdoor mobile robot. Results have shown that the robot, using this architecture, is able to perform reactive autonomous navigation in an outdoor environment.  Introduction  As part of the work for the Unmanned Ground Vehicles (UGV) project  9, 10  The University of Michigan AI Lab built an outdoor mobile robot named MAVERIC (see Figure 2).  This mobile robot is being used as a testbed for work in planning, navigation, and information assimilation for single agent ..",Implementation Of A Reactive Autonomous Navigation,,,,core,
100067320,1996,"In this paper, we proposed a method by which a stereo vision-based mobile robot learns to reach a tar-get by detecting and avoiding occlusions. We call the internal representation that describes the learned be-havior “stereo sketch. ” First, an input scene is segmented into homogeneous regions by the enhanced ISODATA algorithm with MDL principle in terms of image coordinates and disparity information obtained from the fast stereo matcher based on the coarse-to-fine control method. Then, in terms of the segmented regions including the target area and their occlusion status identified during the stereo and motion dispari-ty estimation process, we construct a state space for a reinforcement learning method to obtain target reach-ing behavior. As a result, the robot can avoid obstacles without explicitly describing them. We give the com-puter simulation results and real robot implementation to show the validity of our method. ",Stereo sketch: Stereo vision-based target reaching behavior acquisition with occlusion detection and avoidance,,,,core,
23920862,1992,"In this paper, a peg-in-hole insertion task is used as an example to illustrate the utility of direct associative reinforcement learning methods for learning control under real-world conditions of uncertainty and noise. An associative reinforcement learning system has to learn appropriate actions in various situations through search guided by evaluative performance feedback. We used such a learning system, implemented as a connectionist network, to learn active compliant control for peg-in-hole insertion. Our results indicate that direct reinforcement learning can be used to learn a reactive control strategy that works well even in the presence of a high degree of noise and uncertainty.  1 Introduction  The peg-in-hole insertion task has been widely used by roboticists for testing various approaches to robot control. Peg-in-hole insertion is also studied as a canonical robot assembly operation by researchers in industrial robotics. Both two-dimensional [15] and three-dimensional [8] pe..",Learning Reactive Admittance Control,,IEEE,,core,
24653700,1991,"Next generation real-time systems will require greater flexibility and predictability than is commonly found in today&apos;s systems. These future systems include the space sta-tion, integrated vision/robotics/AI systems, collections of humans/robots coordinating to achieve common objectives (usually in hazardous environments such as undersea explo-ration or chemical plants), and various command and control applications. The Spring kernel is a research oriented kernel designed to form the basis of a flexible, hard real-time operating system for such applications. Our approach challenges several basic assump-tions upon which most current real-time operating systems are built and subsequently advocates a new paradigm based on the notion of predictability and a method for on-line dynamic guarantees of deadlines. The Spring kernel is being implemented on a network of (68020 based) multiprocessors called SpringNet. ",The Spring Kernel: A New Paradigm for Real-Time Systems,,,10.1109/52.88945,core,
24292044,1996,"A robot control architecture called SPOTT  1  is proposed and implemented as a real-time and parallel system of concurrently executing and co-operating modules. SPOTT provides a bridge for linking behavioral (i.e., reactive) and symbolic control. The control system is a real-time AI system which is responsible for dynamically adapting to changing environmental circumstances in order to successfully execute and complete a set of navigational tasks for an autonomous mobile robot. SPOTT consists of a behavioral controller, a local dynamic path planner, and a global path planner, as well as a map and a graphical user interface. The behavioral control formalism is called TR+ and is based on an adaptation and extension of the TeleoReactive (TR) formalism introduced by Nils Nilsson. TR+ rules make decisions which affect actuator control and map database maintenance. A dynamic local path planner continually polls the map database in order to navigate around newly encountered obstacles. The loc..",SPOTT: A Mobile Robot Control Architecture for Unknown or Partially Known Environments,,AAAI Press,,core,
24279249,1992,"Seven di#erent Radial Basis Functions have been applied in a Feedforward Neural Network and tested on five di#erent real or simulated multivariate modelling problems. A short theory of Radial Basis Functions is presented as well as the particular implementation of the Radial Basis Function Network (RBFN). The real world data modelling problems are; identifying the dynamic actuator characteristics of a hydraulic industrial robot, modelling carbon consumption in a metallurgic industrial process and estimation of the water content in fish food products based on NIRspectroscopy. In addition the RBFNs have been applied for modelling data generated from a simulated chemical reactor and to identify a 10-dimensional test function. Key-words : Artificial Neural Networks, Radial Basis Functions, Nonlinear data modelling, Applications. This paper has been published in Proceedings of Neuro-Nimes&apos;92,  Neural Networks and their Applications, EC2, France, 1992, pp.623-633.  1 A short theory of Radia..",Radial Basis Function Networks and Nonlinear Data Modelling.,,,,core,
42772710,Mar. 1995,"Four important generic issues are identified and addressed in some depth in this thesis as part of the development of an adaptive neural network based control system for an experimental free flying space robot prototype. The first issue concerns the importance of true system level design of the control system. A new hybrid strategy is developed here, in depth, for the beneficial integration of neural networks into the total control system. A second important issue in neural network control concerns incorporating a priori knowledge into the neural network. In many applications, it is possible to get a reasonably accurate controller using conventional means. If this prior information is used purposefully to provide a starting point for the optimizing capabilities of the neural network, it can provide much faster initial learning. In a step towards addressing this issue, a new generic Fully Connected Architecture (FCA) is developed for use with backpropagation. A third issue is that neural networks are commonly trained using a gradient based optimization method such as backpropagation; but many real world systems have Discrete Valued Functions (DVFs) that do not permit gradient based optimization. One example is the on-off thrusters that are common on spacecraft. A new technique is developed here that now extends backpropagation learning for use with DVFs. The fourth issue is that the speed of adaptation is often a limiting factor in the implementation of a neural network control system. This issue has been strongly resolved in the research by drawing on the above new contributions",Experiments in Neural-Network Control of a Free-Flying Space Robot,https://core.ac.uk/download/pdf/42772710.pdf,,,core,
71172437,01/04/1997,"Current robotic systems are unable to recognize most unexpected changes in the work environment, such as tool breakage, workpiece motion, or sensor failure. Unless halted by a human operator, they are likely to continue actions that are at best inappropriate, and at worst may cause damage to the workpiece or robot. This project investigated use of Artificial Neural Networks (ANNs) to learn the expected characteristics of sensor data during normal operations, recognize when data no longer is consistent with normal operation, suspend operations and alert a human operator. Data on force and torque applied at the robot tool tip were collected from two workcells: a robotic deburring system and a robot material-handling system. Data were collected for normal operations and for operations in which a fault condition was introduced. Data simulating sensor failure and excessive sensor noise were generated. Artificial Neural Networks (ANN) were trained to classify operating conditions; several ANN architectures were tested. The selected ANNs were able to correctly classify all valid operating conditions and the majority of fault conditions over the entire range of operating conditions, having {open_quotes}learned{close_quotes} the expected force/torque data. Most faults introduced appreciable error in the data; these were correctly classified. However, a small minority of faults did not give rise to a detectable difference in force and torque data. It is believed that these faults could be detected using other sensors. The computational workload varies with the implementation, but is moderate: up to 2.3 megaflops. This makes implementation of a real-time workcell monitor feasible",Artificial awareness for robots using artificial neural nets to monitor robotic workcells,,Sandia National Laboratories,10.2172/469142,core,
8595117,,"A major challenge in modern robotics is to liberate robots from controlled industrial settings, and allow them to interact with humans and changing environments in the real-world. The current research attempts to determine if a neurophysiologically motivated model of cortical function in the primate can help to address this challenge. Primates are endowed with cognitive systems that allow them to maximize the feedback from their environment by learning the values of actions in diverse situations and by adjusting their behavioral parameters (i.e., cognitive control) to accommodate unexpected events. In such contexts uncertainty can arise from at least two distinct sources – expected uncertainty resulting from noise during sensory-motor interaction in a known context, and unexpected uncertainty resulting from the changing probabilistic structure of the environment. However, it is not clear how neurophysiological mechanisms of reinforcement learning and cognitive control integrate in the brain to produce efficient behavior. Based on primate neuroanatomy and neurophysiology, we propose a novel computational model for the interaction between lateral prefrontal and anterior cingulate cortex reconciling previous models dedicated to these two functions. We deployed the model in two robots and demonstrate that, based on adaptive regulation of a meta-parameter β that controls the exploration rate, the model can robustly deal with the two kinds of uncertainties in the real-world. In addition the model could reproduce monkey behavioral performance and neurophysiological data in two problem-solving tasks. A last experiment extends this to human–robot interaction with the iCub humanoid, and novel sources of uncertainty corresponding to “cheating” by the human. The combined results provide concrete evidence for the ability of neurophysiologically inspired cognitive systems to control advanced robots in the real-world",Robot Cognitive Control with a Neurophysiologically Inspired Reinforcement Learning Model,https://core.ac.uk/download/pdf/8595117.pdf,Frontiers Research Foundation,,core,
21393500,1971,"This is the final report for the most recent year of a program research in the field of artificial intelligence. The focus of recent work has been the design, implementation, and integration of a completely new system for the control of a robot that plans, learns, and carries out tasks autonomously in a real laboratory environment. The report includes sections that describe the computer implementation of low-level and intermediate-level actions; routines for automated vision; and the planning, generalization, and execution mechanisms. Section II of the report contains a scenario that demonstrates the approximate capabi Ii ties of the current version of the entire robot system. iii CONTENT",Approved by:,,,,core,
22617796,1996,"A robot control architecture called SPOTT  1  is proposed and implemented as a real-time and parallel system of concurrently executing and cooperating modules. SPOTT provides a bridge for linking behavioral (i.e., reactive) and symbolic control. The control system is a realtime AI system which is responsible for dynamically adapting to changing environmental circumstances in order to successfully execute and complete a set of navigational tasks for an autonomous mobile robot. SPOTT consists of a behavioral controller, a local dynamic path planner, and a global path planner, as well as a map and a graphical user interface. The behavioral control formalism is called TR+ and is based on an adaptation and extension of the Teleo-Reactive (TR) formalism introduced by Nils Nilsson. TR+ rules make decisions which affect actuator control and map database maintenance. A dynamic local path planner continually polls the map database in order to navigate around newly encountered obstacles. The loca..",SPOTT: A Mobile Robot Control Architecture for Unknown or Partially Known Environments,,AAAI Press,,core,
24375193,1992,"&quot;If you want to think about thinking, you have to think about thinking about something.&quot;  --- Seymour Papert A softbot (software robot) is a program that interacts with a software environment by issuing commands and interpreting the environment&apos;s feedback. Because softbots are much easier to build than physical robots, softbots are an attractive substrate for machine-learning and agentarchitecture research. To illustrate this point, we consider Rodney, a UNIX  1  softbot under development at the University of Washington. Rodney uses prodigy as its planner and the theo frame system to represent its world model. Custom-built execution and sensing modules allow Rodney to interact with a UNIX shell in real time.  Motivation  Much of the recent work in machine learning and planning has been influenced by two themes:  ffl integrated architectures: focusing on programs that combine planning, learning, and execution capabilities (e.g. [ Laird and Rosenbloom, 1990, Mitchell, 1990, Sutton, 1990 ..",Softbots as Testbeds for Machine Learning,,AAAI Press,,core,
23053377,1997,". This paper describes the implementation of Hebbian learning in electronic hardware and its application in  real world sensory-motor adaptation tasks. Networks based on the principles of classical and operant conditioning that  exploit the neuromimetic circuit&apos;s long and short-term dynamics are shown. We report results from different setups  including a robot&apos;s adaptation of maneuvers and timed responses. Stability and robustness are demonstrated in a  variety of situations.  1 Introduction  A local learning rule is attractive for both, neuroscientists, who aim to explain the emergent properties of natural nervous systems, and for engineers, who are interested in an efficient hardware implementation of neural networks. The principle of Hebbian learning offers the locality that is appreciated by electrical engineers, since it reduces the required wiring compared to learning algorithms like Backpropagation. Hebbian learning as employed here only needs local signals that are directly ava..",Application of a Local Learning Rule in a Wheelchair Robot,,,,core,
20848900,1992,"This dissertation describes an attempt to apply a behaviour-based robotics design methodology, subsumption architecture, to a software project which emulates a human competence. The project produces a chord structure for a song when presented with its melody as audio input, with the goal of providing real time accompaniment without the benefit of a score. The implications of behaviour-based distributed intelligence paradigms for both the engineering and the explanatory aspects of artificial intelligence are discussed",The Subsumption Strategy Development of a Music Modelling System,,,,core,
42808056,"JAN 1, 1991","Significant advances have occurred during the last decade in intelligent systems technologies (a.k.a. knowledge-based systems, KBS) including research, feasibility demonstrations, and technology implementations in operational environments. Evaluation and simulation data obtained to date in real-time operational environments suggest that cost-effective utilization of intelligent systems technologies can be realized for Automated Rendezvous and Capture applications. The successful implementation of these technologies involve a complex system infrastructure integrating the requirements of transportation, vehicle checkout and health management, and communication systems without compromise to systems reliability and performance. The resources that must be invoked to accomplish these tasks include remote ground operations and control, built-in system fault management and control, and intelligent robotics. To ensure long-term evolution and integration of new validated technologies over the lifetime of the vehicle, system interfaces must also be addressed and integrated into the overall system interface requirements. An approach for defining and evaluating the system infrastructures including the testbed currently being used to support the on-going evaluations for the evolutionary Space Station Freedom Data Management System is presented and discussed. Intelligent system technologies discussed include artificial intelligence (real-time replanning and scheduling), high performance computational elements (parallel processors, photonic processors, and neural networks), real-time fault management and control, and system software development tools for rapid prototyping capabilities",Intelligent systems technology infrastructure for integrated systems,https://core.ac.uk/download/pdf/42808056.pdf,,,core,
234919232,9999,"The collaboration between humans and robots is one of the most disruptive and challenging research areas. Even considering advances in design and artificial intelligence, humans and robots could soon ally to perform together a number of different tasks. Robots could also became new playmates. In fact, an emerging trend is associated with the so-called phygital gaming, which builds upon the idea of merging the physical world with a virtual one in order to let physical and virtual entities, such as players, robots, animated characters and other game objects interact seamlessly as if they were all part of the same reality. This paper specifically focuses on mixed reality gaming environments that can be created by using floor projection, and tackles the issue of enabling accurate and robust tracking of off-the-shelf robots endowed with limited sensing capabilities. The proposed solution is implemented by fusing visual tracking data gathered via a fixed camera in a smart environment with odometry data obtained from robot's on-board sensors. The solution has been tested within a phygital gaming platform in a real usage scenario, by experimenting with a robotic game that exhibits many challenging situations which would be hard to manage using conventional tracking techniques",Robust robot tracking for next-generation collaborative robotics-based gaming environments,,'Institute of Electrical and Electronics Engineers (IEEE)',10.1109/TETC.2017.2769705,core,
474477411,,,Visual Navigation of Wheeled Mobile Robots Using Deep Reinforcement Learning: Simulation to Real-Time Implementation,,'ASME International',10.1115/DSCC2020-3279,core,
276277838,1988-01-01T00:00:00,"technical reportA variety of problems in artificial intelligence, operations research, symbolic logic, pattern recognition and computer vision, and robot manipulation are special cases of the Consistent Labeling Problem (CLP). The Discrete Relaxation Algorithm (DRA) is an efficient  computational technique to enforce arc consistency (AC) in a CLP problem. The original sequential AC-1 algorithm suffers from 0 ( n 3m3 ) time complexity, for an n-object and mlabel problem. Sample problem runs show all these algorithms are too slow to meet the need for any useful real-time CLP applications. In this paper, we give an optimal, parallel DRA5 algorithm which reaches the optimal lower bound, O(nm), for parallel AC algorithms  (where the number of processors is polynomial in the problem size). This algorithm has been implemented on a fine-grained, massively parallel hardware computer architecture. For problems of practical interest, four to ten orders of magnitude of efficiency improvement can be reached on this hardware architecture","An optimal, parallel discrete relaxation algorithm and architecture (Revised January 1988 and August 1989)",,University of Utah,,core,
341985688,,,"Volume 2, Issue 3, Special issue on Recent Advances in Engineering Systems (Published Papers) Articles Transmit / Received Beamforming for Frequency Diverse Array with Symmetrical frequency offsets  Shaddrack Yaw Nusenu  Adv. Sci. Technol. Eng. Syst. J. 2(3), 1-6 (2017); View Description Detailed Analysis of Amplitude and Slope Diffraction Coefficients for knife-edge structure in S-UTD-CH Model  Eray Arik, Mehmet Baris Tabakcioglu  Adv. Sci. Technol. Eng. Syst. J. 2(3), 7-11 (2017); View Description Applications of Case Based Organizational Memory Supported by the PAbMM Architecture  Martín, María de los Ángeles, Diván, Mario José  Adv. Sci. Technol. Eng. Syst. J. 2(3), 12-23 (2017); View Description Low Probability of Interception Beampattern Using Frequency Diverse Array Antenna  Shaddrack Yaw Nusenu  Adv. Sci. Technol. Eng. Syst. J. 2(3), 24-29 (2017); View Description Zero Trust Cloud Networks using Transport Access Control and High Availability Optical Bypass Switching  Casimer DeCusatis, Piradon Liengtiraphan, Anthony Sager  Adv. Sci. Technol. Eng. Syst. J. 2(3), 30-35 (2017); View Description A Derived Metrics as a Measurement to Support Efficient Requirements Analysis and Release Management  Indranil Nath  Adv. Sci. Technol. Eng. Syst. J. 2(3), 36-40 (2017); View Description Feedback device of temperature sensation for a myoelectric prosthetic hand  Yuki Ueda, Chiharu Ishii  Adv. Sci. Technol. Eng. Syst. J. 2(3), 41-40 (2017); View Description Deep venous thrombus characterization: ultrasonography, elastography and scattering operator  Thibaud Berthomier, Ali Mansour, Luc Bressollette, Frédéric Le Roy, Dominique Mottier  Adv. Sci. Technol. Eng. Syst. J. 2(3), 48-59 (2017); View Description Improving customs’ border control by creating a reference database of cargo inspection X-ray images  Selina Kolokytha, Alexander Flisch, Thomas Lüthi, Mathieu Plamondon, Adrian Schwaninger, Wicher Vasser, Diana Hardmeier, Marius Costin, Caroline Vienne, Frank Sukowski, Ulf Hassler, Irène Dorion, Najib Gadi, Serge Maitrejean, Abraham Marciano, Andrea Canonica, Eric Rochat, Ger Koomen, Micha Slegt  Adv. Sci. Technol. Eng. Syst. J. 2(3), 60-66 (2017); View Description Aviation Navigation with Use of Polarimetric Technologies  Arsen Klochan, Ali Al-Ammouri, Viktor Romanenko, Vladimir Tronko  Adv. Sci. Technol. Eng. Syst. J. 2(3), 67-72 (2017); View Description Optimization of Multi-standard Transmitter Architecture Using Single-Double Conversion Technique Used for Rescue Operations  Riadh Essaadali, Said Aliouane, Chokri Jebali and Ammar Kouki  Adv. Sci. Technol. Eng. Syst. J. 2(3), 73-81 (2017); View Description Singular Integral Equations in Electromagnetic Waves Reflection Modeling  A. S. Ilinskiy, T. N. Galishnikova  Adv. Sci. Technol. Eng. Syst. J. 2(3), 82-87 (2017); View Description Methodology for Management of Information Security in Industrial Control Systems: A Proof of Concept aligned with Enterprise Objectives.  Fabian Bustamante, Walter Fuertes, Paul Diaz, Theofilos Toulqueridis  Adv. Sci. Technol. Eng. Syst. J. 2(3), 88-99 (2017); View Description Dependence-Based Segmentation Approach for Detecting Morpheme Boundaries  Ahmed Khorsi, Abeer Alsheddi  Adv. Sci. Technol. Eng. Syst. J. 2(3), 100-110 (2017); View Description Paper Improving Rule Based Stemmers to Solve Some Special Cases of Arabic Language  Soufiane Farrah, Hanane El Manssouri, Ziyati Elhoussaine, Mohamed Ouzzif  Adv. Sci. Technol. Eng. Syst. J. 2(3), 111-115 (2017); View Description Medical imbalanced data classification  Sara Belarouci, Mohammed Amine Chikh  Adv. Sci. Technol. Eng. Syst. J. 2(3), 116-124 (2017); View Description ADOxx Modelling Method Conceptualization Environment  Nesat Efendioglu, Robert Woitsch, Wilfrid Utz, Damiano Falcioni  Adv. Sci. Technol. Eng. Syst. J. 2(3), 125-136 (2017); View Description GPSR+Predict: An Enhancement for GPSR to Make Smart Routing Decision by Anticipating Movement of Vehicles in VANETs  Zineb Squalli Houssaini, Imane Zaimi, Mohammed Oumsis, Saïd El Alaoui Ouatik  Adv. Sci. Technol. Eng. Syst. J. 2(3), 137-146 (2017); View Description Optimal Synthesis of Universal Space Vector Digital Algorithm for Matrix Converters  Adrian Popovici, Mircea Băbăiţă, Petru Papazian  Adv. Sci. Technol. Eng. Syst. J. 2(3), 147-152 (2017); View Description Control design for axial flux permanent magnet synchronous motor which operates above the nominal speed  Xuan Minh Tran, Nhu Hien Nguyen, Quoc Tuan Duong  Adv. Sci. Technol. Eng. Syst. J. 2(3), 153-159 (2017); View Description A synchronizing second order sliding mode control applied to decentralized time delayed multi−agent robotic systems: Stability Proof  Marwa Fathallah, Fatma Abdelhedi, Nabil Derbel  Adv. Sci. Technol. Eng. Syst. J. 2(3), 160-170 (2017); View Description Fault Diagnosis and Tolerant Control Using Observer Banks Applied to Continuous Stirred Tank Reactor  Martin F. Pico, Eduardo J. Adam  Adv. Sci. Technol. Eng. Syst. J. 2(3), 171-181 (2017); View Description Development and Validation of a Heat Pump System Model Using Artificial Neural Network  Nabil Nassif, Jordan Gooden  Adv. Sci. Technol. Eng. Syst. J. 2(3), 182-185 (2017); View Description Assessment of the usefulness and appeal of stigma-stop by psychology students: a serious game designed to reduce the stigma of mental illness  Adolfo J. Cangas, Noelia Navarro, Juan J. Ojeda, Diego Cangas, Jose A. Piedra, José Gallego  Adv. Sci. Technol. Eng. Syst. J. 2(3), 186-190 (2017); View Description Kinect-Based Moving Human Tracking System with Obstacle Avoidance  Abdel Mehsen Ahmad, Zouhair Bazzal, Hiba Al Youssef  Adv. Sci. Technol. Eng. Syst. J. 2(3), 191-197 (2017); View Description A security approach based on honeypots: Protecting Online Social network from malicious profiles  Fatna Elmendili, Nisrine Maqran, Younes El Bouzekri El Idrissi, Habiba Chaoui  Adv. Sci. Technol. Eng. Syst. J. 2(3), 198-204 (2017); View Description Pulse Generator for Ultrasonic Piezoelectric Transducer Arrays Based on a Programmable System-on-Chip (PSoC)  Pedro Acevedo, Martín Fuentes, Joel Durán, Mónica Vázquez, Carlos Díaz  Adv. Sci. Technol. Eng. Syst. J. 2(3), 205-209 (2017); View Description Enabling Toy Vehicles Interaction With Visible Light Communication (VLC)  M. A. Ilyas, M. B. Othman, S. M. Shah, Mas Fawzi  Adv. Sci. Technol. Eng. Syst. J. 2(3), 210-216 (2017); View Description Analysis of Fractional-Order 2xn RLC Networks by Transmission Matrices  Mahmut Ün, Manolya Ün  Adv. Sci. Technol. Eng. Syst. J. 2(3), 217-220 (2017); View Description Fire extinguishing system in large underground garages  Ivan Antonov, Rositsa Velichkova, Svetlin Antonov, Kamen Grozdanov, Milka Uzunova, Ikram El Abbassi  Adv. Sci. Technol. Eng. Syst. J. 2(3), 221-226 (2017); View Description Directional Antenna Modulation Technique using A Two-Element Frequency Diverse Array  Shaddrack Yaw Nusenu  Adv. Sci. Technol. Eng. Syst. J. 2(3), 227-232 (2017); View Description Classifying region of interests from mammograms with breast cancer into BIRADS using Artificial Neural Networks  Estefanía D. Avalos-Rivera, Alberto de J. Pastrana-Palma  Adv. Sci. Technol. Eng. Syst. J. 2(3), 233-240 (2017); View Description Magnetically Levitated and Guided Systems  Florian Puci, Miroslav Husak  Adv. Sci. Technol. Eng. Syst. J. 2(3), 241-244 (2017); View Description Energy-Efficient Mobile Sensing in Distributed Multi-Agent Sensor Networks  Minh T. Nguyen  Adv. Sci. Technol. Eng. Syst. J. 2(3), 245-253 (2017); View Description Validity and efficiency of conformal anomaly detection on big distributed data  Ilia Nouretdinov  Adv. Sci. Technol. Eng. Syst. J. 2(3), 254-267 (2017); View Description S-Parameters Optimization in both Segmented and Unsegmented Insulated TSV upto 40GHz Frequency  Juma Mary Atieno, Xuliang Zhang, HE Song Bai  Adv. Sci. Technol. Eng. Syst. J. 2(3), 268-276 (2017); View Description Synthesis of Important Design Criteria for Future Vehicle Electric System  Lisa Braun, Eric Sax  Adv. Sci. Technol. Eng. Syst. J. 2(3), 277-283 (2017); View Description Gestural Interaction for Virtual Reality Environments through Data Gloves  G. Rodriguez, N. Jofre, Y. Alvarado, J. Fernández, R. Guerrero  Adv. Sci. Technol. Eng. Syst. J. 2(3), 284-290 (2017); View Description Solving the Capacitated Network Design Problem in Two Steps  Meriem Khelifi, Mohand Yazid Saidi, Saadi Boudjit  Adv. Sci. Technol. Eng. Syst. J. 2(3), 291-301 (2017); View Description A Computationally Intelligent Approach to the Detection of Wormhole Attacks in Wireless Sensor Networks  Mohammad Nurul Afsar Shaon, Ken Ferens  Adv. Sci. Technol. Eng. Syst. J. 2(3), 302-320 (2017); View Description Real Time Advanced Clustering System  Giuseppe Spampinato, Arcangelo Ranieri Bruna, Salvatore Curti, Viviana D’Alto  Adv. Sci. Technol. Eng. Syst. J. 2(3), 321-326 (2017); View Description Indoor Mobile Robot Navigation in Unknown Environment Using Fuzzy Logic Based Behaviors  Khalid Al-Mutib, Foudil Abdessemed  Adv. Sci. Technol. Eng. Syst. J. 2(3), 327-337 (2017); View Description Validity of Mind Monitoring System as a Mental Health Indicator using Voice  Naoki Hagiwara, Yasuhiro Omiya, Shuji Shinohara, Mitsuteru Nakamura, Masakazu Higuchi, Shunji Mitsuyoshi, Hideo Yasunaga, Shinichi Tokuno  Adv. Sci. Technol. Eng. Syst. J. 2(3), 338-344 (2017); View Description The Model of Adaptive Learning Objects for virtual environments instanced by the competencies  Carlos Guevara, Jose Aguilar, Alexandra González-Eras  Adv. Sci. Technol. Eng. Syst. J. 2(3), 345-355 (2017); View Description An Overview of Traceability: Towards a general multi-domain model  Kamal Souali, Othmane Rahmaoui, Mohammed Ouzzif  Adv. Sci. Technol. Eng. Syst. J. 2(3), 356-361 (2017); View Description L-Band SiGe HBT Active Differential Equalizers with Variable, Positive or Negative Gain Slopes Using Dual-Resonant RLC Circuits  Yasushi Itoh, Hiroaki Takagi  Adv. Sci. Technol. Eng. Syst. J. 2(3), 362-368 (2017); View Description Moving Towards Reliability-Centred Management of Energy, Power and Transportation Assets  Kang Seng Seow, Loc K. Nguyen, Kelvin Tan, Kees-Jan Van Oeveren  Adv. Sci. Technol. Eng. Syst. J. 2(3), 369-375 (2017); View Description Secure Path Selection under Random Fading  Furqan Jameel, Faisal, M Asif Ali Haider, Amir Aziz Butt  Adv. Sci. Technol. Eng. Syst. J. 2(3), 376-383 (2017); View Description Security in SWIPT with Power Splitting Eavesdropper  Furqan Jameel, Faisal, M Asif Ali Haider, Amir Aziz Butt  Adv. Sci. Technol. Eng. Syst. J. 2(3), 384-388 (2017); View Description Performance Analysis of Phased Array and Frequency Diverse Array Radar Ambiguity Functions  Shaddrack Yaw Nusenu  Adv. Sci. Technol. Eng. Syst. J. 2(3), 389-394 (2017); View Description Adaptive Discrete-time Fuzzy Sliding Mode Control For a Class of Chaotic Systems  Hanene Medhaffar, Moez Feki, Nabil Derbel  Adv. Sci. Technol. Eng. Syst. J. 2(3), 395-400 (2017); View Description Fault Tolerant Inverter Topology for the Sustainable Drive of an Electrical Helicopter  Igor Bolvashenkov, Jörg Kammermann, Taha Lahlou, Hans-Georg Herzog  Adv. Sci. Technol. Eng. Syst. J. 2(3), 401-411 (2017); View Description Computational Intelligence Methods for Identifying Voltage Sag in Smart Grid  Turgay Yalcin, Muammer Ozdemir  Adv. Sci. Technol. Eng. Syst. J. 2(3), 412-419 (2017); View Description A Highly-Secured Arithmetic Hiding cum Look-Up Table (AHLUT) based S-Box for AES-128 Implementation  Ali Akbar Pammu, Kwen-Siong Chong, Bah-Hwee Gwee  Adv. Sci. Technol. Eng. Syst. J. 2(3), 420-426 (2017); View Description Service Productivity and Complexity in Medical Rescue Services  Markus Harlacher, Andreas Petz, Philipp Przybysz, Olivia Chaillié, Susanne Mütze-Niewöhner  Adv. Sci. Technol. Eng. Syst. J. 2(3), 427-434 (2017); View Description Principal Component Analysis Application on Flavonoids Characterization  Che Hafizah Che Noh, Nor Fadhillah Mohamed Azmin, Azura Amid  Adv. Sci. Technol. Eng. Syst. J. 2(3), 435-440 (2017); View Description A Reconfigurable Metal-Plasma Yagi-Yuda Antenna for Microwave Applications  Giulia Mansutti, Davide Melazzi, Antonio-Daniele Capobianco  Adv. Sci. Technol. Eng. Syst. J. 2(3), 441-448 (2017); View Description Verifying the Detection Results of Impersonation Attacks in Service Clouds",,'ASTES Journal',10.25046/aj020358,core,
10483845,Feb-97,"NASA is planning to send numerous unmanned planetary missions to explore the space. This requires autonomous robotic vehicles which can navigate in an unstructured, unknown, and uncertain environment. Landmark based navigation is a new area of research which differs from the traditional goal-oriented navigation, where a mobile robot starts from an initial point and reaches a destination in accordance with a pre-planned path. The landmark based navigation has the advantage of allowing the robot to find its way without communication with the mission control station and without exact knowledge of its coordinates. Current algorithms based on landmark navigation however pose several constraints. First, they require large memories to store the images. Second, the task of comparing the images using traditional methods is computationally intensive and consequently real-time implementation is difficult. The method proposed here consists of three stages, First stage utilizes a heuristic-based algorithm to identify significant objects. The second stage utilizes a neural network (NN) to efficiently classify images of the identified objects. The third stage combines distance information with the classification results of neural networks for efficient and intelligent navigation",Neural Network Based Sensory Fusion for Landmark Detection,,,,core,
22945452,1994,"Abstract. This paper presents a method of vision-based reinforcement learning by which a robot learns to shoot a ball into a goal. We discuss several issues in applying the reinforcement learning method to a real robot with vision sensor by which the robot can obtain information about the changes in an environment. First, we construct a state space in terms of size, position, and orientation of a ball and a goal in an image, and an action space is designed in terms of the action commands to be sent to the left and right motors of a mobile robot. This causes a “state-action deviation ” problem in constructing the state and action spaces that reflect the outputs from physical sensors and actuators, respectively. To deal with this issue, an action set is constructed in a way that one action consists of a series of the same action primitive which is successively executed until the current state changes. Next, to speed up the learning time, a mechanism of Learning from Easy Missions (or LEM) is implemented. LEM reduces the learning time from exponential to almost linear order in the size of the state space. The results of computer simulations and real robot experiments are given",purposive behavior acquisition on a real robot by vision-based reinforcement learning,,,,core,
23270923,1994,"ARCHON  ^TM  (ARchitecture for Cooperative Heterogeneous ON-line systems) was Europe&apos;s largest project in the area of Distributed Artificial Intelligence (DAI). It devised a general-purpose architecture, software framework, and methodology which has been used to support the development of DAI systems in a number of real world industrial domains. Some examples of the applications to which it has been successfully applied include: electricity distribution and supply, electricity transmission and distribution, control of a cement kiln complex, control of a particle accelerator, and control of a robotics application. The type of cooperating community that it supports has a decentralised control regime and individual problem solving agents which are large grain, loosely coupled, and semi-autonomous. This paper will tackle a broad range of issues related to the application of ARCHON technology to industrial applications. Firstly, it gives the rationale for a DAI approach to industrial applic..",The Archon System And Its Applications,,,,core,
20816609,1994,"representations, although it does not exclude the role of internal representations for mediating complex behaviours. We think that, for the purpose of building autonomous robots, active perception requires speci c recipes for three related aspects: the design of the physical sensory system, the modality and type of information extracted, and the structure and functioning of the control system. We outline a set of solutions for these three aspects and describe their implementation on a real mobile robot through a set of three di erent experiments using a combination of neural networks and genetic algorithms. The results show that active perception is a useful feature that is exploited by autonomous agents.The experiments show that the combination of genetic algorithms and neural networks is a feasible and fruitful technique for the development of active perception in autonomous agents","Active perception, navigation, homing, and grasping: an autonomous perspective",,Society Press,,core,
24283292,1995,"The Robot World Cup Initiative (RoboCup) is an attempt to foster AI and intelligent robotics research by providing a standard problem where wide range of technologies can be integrated and examined. In order for a robot team to actually perform a soccer game, various technologies must be incorporated including: design principles of autonomous agents, multiagent collaboration, strategy acquisition, realtime reasoning, robotics, and sensor-fusion. Unlike AAAI robot competition, which is tuned for a single heavy-duty slow-moving robot, RoboCup is a task for a team of multiple fastmoving robots under a dynamic environment. Although RoboCup&apos;s final target is a world cup with real robots, RoboCup offers a software platform for research on the software aspects of RoboCup. This paper describes technical challenges involved in RoboCup, rules, and simulation environment.  1 Introduction: RoboCup as a Standard AI Problem  We propose a Robot World Cup (RoboCup), as a new standard problem for AI an..",RoboCup: The Robot World Cup Initiative,,,,core,
24299224,1995,"The Robot World Cup Initiative (RoboCup) is an attempt to foster AI and intelligent robotics research by providing a standard problem where wide range of technologies can be integrated and examined. The first RoboCup  competition will be held at IJCAI-97, Nagoya. In order for a robot team to actually perform a soccer game, various technologies must be incorporated including: design principles of autonomous agents, multi-agent collaboration, strategy acquisition, real-time reasoning, robotics, and sensor-fusion. Unlike AAAI robot competition, which is tuned for a single heavyduty slow-moving robot, RoboCup is a task for a team of multiple fast-moving robots under a dynamic environment. Although RoboCup&apos;s final target is a world cup with real robots, RoboCup offers a software platform for research on the software aspects of RoboCup. This paper describes technical challenges involved in RoboCup, rules, and simulation environment.  1 Introduction: RoboCup as a Standard AI Problem  We propo..",RoboCup: The Robot World Cup Initiative,,,,core,
24371280,1993,"There is a need to empirically validate theoretical Machine Learning research. This can be difficult because the algorithms being tested often require large amounts of training data. Softbots provide a unique solution to this problem. Softbots or software robots are intelligent agents that interact with software environments. By attaching the agent directly to the training environment, it is possible for softbots to gather their own training data. St. Bernard is a softbot that locates files within the UNIX  1  file system. It is based on the theory of satisficing search. The training data used by satisficing search is gathered directly from its environment. St. Bernard demonstrates the usefulness of satisficing search and addresses some of the practical issues of implementing this algorithm. 1  UNIX is a trademark of AT&amp;T Bell Labs.  1 Introduction  There is always the danger when doing theoretical Machine Learning research that the results will not prove to be useful in the real worl..",St. Bernard: The File Retrieving Softbot,,,,core,
24315958,1995,"Sensor and motor systems are not separable for autonomous agents to accomplish tasks in a dynamic environment. This paper proposes a method to represent the interaction between a vision-based learning agent and its environment. The method is called &quot;motion sketch&quot; by which a one-eyed mobile robot can learn several behaviors such as obstacle avoidance and target pursuit. A motion sketch is a collection of visual motion cues detected by a group of visual tracking routines of which visual behaviors are determined by individual tasks, and is tightly coupled with motor behaviors which are obtained by Q-learning, a most widely used reinforcement learning method, based on the visual motion cues. In order for the motion sketch to work, first the fundamental relationship between visual motions and motor commands is obtained, and then the Q-learning is applied to obtain the set of motor commands tightly coupled with the motion cues. Finally, the experimental results of real robot implementation w..",Motion Sketch: Acquisition of Visual Motion Guided Behaviors,,Morgan Kaufmann,,core,
235571268,1993-07-14T00:00:00,"An archive of the Magrath Trading Store News.The University of Lethbridge Library received permission from the Wes Balderson to digitize and display this content.MAGRATH
NEWS
Published Weekly since 1932 by
The Magrath Trading Company
30 cents Wednesday July 14, 1993
The Chinnok Health Unit would like to
congratulate the following winners in the
*93 FITNESS CHALLENGE*.
First Place Winners:
Awful/Lawful R.C.M.P,
Donna Warnock
Reyko Nishiyama
Lori McRoberts
Greg Squire
Ron McGeachy
Darrin Turnbull
Mac Nishiyama
Bob Muskovich
Greg Perrett
Gavin Warnock
Second Place Winners:
""Chinooks"" Health Unit
Juliann Sommerfeldt
Dalan Sommerfeldt
Marie Sabey
Donna Kearl
Lydia Brunner
Melodie Johnston
Mary Sallenback
Jean Bolokoski
Charlotte Webster
Janet Squire
Third Place Winners:
""The Germinators""
Dahl Clinic
Bev Biddlecome
Sharon Jarvas
Cathy Gruninger
DeNai Harker
Dennis Dahl
Mark Dahl
Ken Dahl
Brenda Stringam
Lance Harker
Marilyn Dahl
BEST TEAM NAME; ""Kalorie Killers""
S.A.S.H. Team Members: Drena Harker,
Bonnie Soltys, Ted Clifton, Sybil George, Grace
Lonnerts, Karin Sabey, Phil Sillito, Lorna Harker,
Zola Wolff, and Karyn Healy.
Prizes were awarded to all winning participants.
We would like to acknowledge the other teams
entered and compliment them on their good work:
Diamond Willow Diamonds - Magrath Lodge
Fat Fighters - S.A.S.H.
Mighty Jocks - S.A.S.H.
David & The Goliaths - Town of Magrath
Bulging Money Bags - Bank of Montreal
Grandmas & Grandpa + 3 - TOPS Club
Tanco Woodchucks - Tanco
*******
The children of Carol Beswick and Ed
Ririe wish to announce the marriage of their
parents. The unpredictable couple were married
in the Magrath United Church, June 29, by Rev.
Doug Cowan (B.B.). Sharing the occasion were
JZd's brother Rod and Carol's daughter Anne. A
musical solo was provided by grandson Evan.
The couple will reside in Australia for five
months, spend Christmas in Florida, and return to
Magrath in January when Ms. Beswick will
complete the school year.
*******
DO YOU LIKE TO LIP SYNC?
DO YOU LIKE FUN TIMES?
DO YOU LIKE GREAT PRIZES?
If you answered yes to any of these questions then
you need to enter the First Annual Magrath
Celebrations Lip Sync Contest! This exciting
and entertaining event will be held in conjunction
with the town dance on Friday, July 23rd starting
at 9:00 p.m. in the Tom Karren Gym. This
contest is for all ages to participate. For more
information call Brad or Shannon Sabey or you
can register at the dance.
COMING EVENTS
You are all invited to
come and say hello to
Courtney & Ilena, the new
baby, and Steele & Laree
Brewerton. Hope to see you
July 18th from 4 - 6 p.m. in
the Senior Citizens Centre.
*******
WEDDING
ANNOUNCEMENT:
Bonnie Bolokoski to Rhett
Mandin Saturday, July 17th.
A calling reception will be
held in their honor at the
Magrath Seniors Centre from
7:00 - 9:00 that evening.
Everyone welcome.
*******
NOTICE: The Open House
for Dave and Lisa Sannes has
been cancelled due to illness.
Call 752-4594.
*******
SENIORS NEWS
Our dinner for this month will
be held Wednesday, July 14th
(TODAY) at 6:00 p.m. at the
Seniors Centre. Come and
enjoy the good food and
pleasant company.
On Saturday, August
14th we are planning for a day
trip that will include a visit to
the Old Man River Dam, lunch
at noon in Fincher Creek (not
included in the price of the
trip) and on to Bellevue for a
tour of the coal mine there.
Then off to Waterton for
supper and entertainment at
Olsen's. The price is $30 each.
If you are interested
phone at once as spaces are
going fast. Call Bakers at
758-3207, Margaret Leishman
at 758-3241, and Doreen
DRY GOODS SPECIALS f
c
J
ALL FABRIC
Including Tricot, Brushed Nylon,
Nvlon Sheer, broadcloth, flannelette, etc. 30% OFF
ALL LACE
20% OFF -**•>. F-------- <—n • ~ W r-M W
j—_jj—
ALL BABY QUILT PANELS
50% OFF
FABRIC PENS & PAINT
50% OFF
Alston at 758-3281, or Hazel
Rasmussen at 758-3545.
REMINDER: Spirit of
Alberta Mini Guard Camp will
be held July 19-24. There is a
late registration fee and
registration forms are available
from Mr. Chatwin at 758-3765
or Elizabeth Strong at 758-
3166, or send to Box 222.
COED RECREATIONAL
SOFTBALL SUMMER
LEAGUE - 18 years and
older. To begin July 7th and
will continue throughout the
summer on Thursday nights.
Phone and register with Laurel
Bennett (758-6222).
FRIENDS OF THE
MAGRATH PUBLIC
LIBRARY FOUNDATION
QUILT RAFFLE
Come and see this beautiful
quilt on display
in the front
window of the
Magrath
Trading
Company.
Draw will be
made July 24th.
TICKETS: $1.00 available at
the Library or call:
Ann Pilling
Ann Fazikos
Marie Stevenson
Bernice Sillito
758-3088
758-6425
758-3540
758-3618
Hazel Dudley 758-3213
t LADIES WEAR SPECIALS J
/•K.Z.
&&&ONE RACK OF CO-ORDINATES
skirts, slacks, blouses
1/2 PRICE
/come in and check out our great
Si SELECTION OF ""MAGRATH"" T-SHIRTS.
(ft
ALL NIGHTWEAR, DUSTERS, ROBES, ■
SLIPS, CAMISOLES, & BRUNCHCOATS [
1/2 PRICE
ONE RACK OF CHILDRENS WEAR
1/2 PRICE
~ SELECTED EARRINGS
3 PAIRS FOR $1.00
ALL JEWELLERY
1/2 PRICE
HYPERMEDIA COMPUTER CAMP
dairy specials
1 Alpha Sour Cream 500 ml $1.78 each
1-------------------------- - —
j 1886 Ice Cream 1 litre $2.68 each
j Beatrice Diet Yogurt 175 g 2 for $1.09
1 Beatrice Cottage Cheese 500 g $1.49 each
Western Family Cheese Slices 500 g $3.38 each
Western Family Margarine 1.36 kg $1.98 each
Western Family Soft Margarine 454 g .78 each
will be held from Tuesday, August 3th to
Once again the four Magrath Wards Relief
Societies, the Magrath United Church, Catholic
Church, and Lutheran Church ladies are
combining their efforts this Friday, July 23rd
and Saturday, July 24th in a ’’GIANT
COMMUNITY BAKE SALE"" to be held in the
Magrath Ice Arena. Please have your baking at
the Arena by 2:00 p.m. on Friday, July 23rd for
pricing. ""Thanks"". Selling will begin Friday; ;
July 23rd at 4:00 - 7:00 p.m. Selling Saturday,
July 24th will be
from noon until
baking is all gone.
Please bring your
baking wrapped and
ready to sell. All
proceeds will go to
the ""LIBRARY AND BUILDING
FUND"" Best sellers are meat pies, butterhorns,
bread, fruit pies, tarts, donuts, iced or un-iced
cakes, popcorn balls, Rice Krispie squares, and
various rhubarb treats. Very slow sellers are fruit
loaves, squares, and muffins, or any kind of
cupcakes and cookies. Bake ahead and freeze if
you won't have time later. VOLUNTEERS ARE
NEEDED. Save your bags for bake sale counter,
PLEASE SUPPORT THIS VERY
WORTHWHILE COMMUNITY PROJECT. For
more information please contact Irene Ririe 758-
3456, Lorraine Balderson 758-6380, or Alice
Stevenson 758-3190
Saturday, August 7th at the Magrath
H School Computer Room. There will
. be two sessions offered: Grades 4-5
from 9:00 a.m. - 12 noon and Grades
? 6 - 8: from 1:00 4:00 p.m. The camp will be
limited to 15 students per session. The camp will
focus on enhancing students computer skills in the
area of multimedia. Students will work with a
variety of software and hardware. The Instructor
is Bonny West and the cost is $50.00. Please
phone Mrs. West at 758-3072 for more
information and registration forms.
*******
THANK -YOU: We wish to express our
appreciation to all those who helped in any way to
make our wedding such a special day. A very
special thank you to all of our friends and family
who helped with the decorations and family
dinner.
Joanne & Kevin Tidball
James & Charlotte Anderson
Dennis & Donna Tidball
*******
MAGRATH GOLF CLUB RESTAURANT
SPECIALS July 19 - 23""
Monday - Baked Virginia Ham
Tuesday - Roast Breast of Chicken
Wednesday - Swiss Steak
Thursday - Grilled Pork Cutlet
Friday - Roast Beef
SOUTHCHIEF BASEBALL
The Southchief baseball
All-Star teams began exhibition
games and tournaments this
past week.
The Seniors (14-15)
participated in a tournament in
Kalispell and placed fourth
winning two
and losing
two. Darren
Baker was the
winning
pitcher in one
of the games.
The
Juniors (13)
have played in
two
tournaments
and a few
exhibition
games.
Although they have lost some
games, they are improving
with each game.
The Majors (11-12)
participated in a tournament in
Lethbridge last weekend and
were 1-2. They lost their
first game to Lethbridge
Norcrest, their second game 3
- 1 to Regina, and won their
3rd game by defeating
Edmonton 12-2. Casey
Christensen was the winning
pitcher in the 3rd game while
Darren Balderson had a double
and single to lead the
Southchief hitters.
The Triple A's (9-10)
A Team won their first district
playoff game and then went
undefeated in a tournament in
Fort Macleod. Winning
pitchers were Brooks
Blackmer, Ben Wood, and
Jimmy Balderson. The Triple
A - B Team won one game
and lost two in the same
tournament.
District Playoffs
continue this week with the
following home (Magrath)
games for the teams.
SENIORS: Wednesday, July
14th at 6:30 p.m.
MAJORS: Tuesday, July 13th
at 6:30 p.m. & Wednesday,
July 14th at 6:30 p.m.
TRIPLE A: Tuesday, July
20th at 6:30 p.m. & Saturday,
July 24th at 12:00 noon - final
tournament.
£ 1
WHi «s» wi
<r»
’*JAr '
Oate: July 26 - July 30
Place: Tom Karren Gym
Cost: $60.80 Per Rthlete
Eligibility:
Camp *0*- Grades 4-6 (Co-ed)
Camp ”8""- Grades 1-9 (Cs-etij
Time: Camp ’fl’- 9:00am-12:00pm
Camp ""B’- t :80pm-4:08pm
Instructor Philip Tollestrup
Note: -This camp will operate on a first come first serve basis.
- Rthletes must proulde their own sneakers and gym clothes.
- Each camp will include fundamental drills on Shooting, Defence,
-Individual moves. Simple Team Offences, and Rebounding.
-There will also be scrimmages each day and a tournament on
the final day of each camp.
-Camp T-Shirts will be given to each participant.
EOEE OSTO DOT DDDN (F(TOM
Name:__________
Grade:___________
Please Check one:
.Phone:_____________________________
Camp “R'-Upper Elementary $68.00
Camp “B“-Junior High $68.00
Send this form with tuition fees to: Philip Tollestrup
Boh 398
Magrath Rlberta
TBK 1J8
758-6716
OH: Bring the form with tuition to Mr. Tollestrup at schoo
MAGRATH RECREATION
NEWS
Registration for the Third
Session of Swimming Lessons
will be held July 17th at 9:00
a.m. Lessons begin the
following Monday, July 19-30.
* # sje
Special thanks to the
Magrath Ambulance Service
and Dr. Dennis Dahl for the
quick response and medical
attention they gave my dad last
Friday. He's doing well now.
Thanks ever so much.
Kirk, Judy, & family
CLASSIFIED ADS
DEADLINE: TUESDAY 12 NOON PHONE 758-6377
Less than 30 words—$1.07
Small ad (2.5""X3.5"")-$5.35
1/4 page---------------------$7.49 ! Full Page—Copy Ready—$25.00
1/3 page---------------------$8.56 j Full Page—We do----------$37.45
1/2 page--------------------$10.70 jj Flyer insertion (your paper)$21.40
FOUND:
FOUND: Bicycle helmet at the school playground.
Identify to claim. Phone 758- 6840.
GARAGE SALES: MULTI-FAMILY GARAGE SALE: Friday, July 16th from 9:00 a.m. - 2:00 p.m. at the PanTree Foods Location
*******
If you are interested in having a Garage Sale, or just have items you want us to sell for you, call Lynne at 758-3107 or 758-6212 for more information.
*******
MOVING SALE: To be held July 31st. Variety of items - big and small. All must sell! Don't miss it! Watch for location and times in next weeks paper. Yoshihara's.
LOST:
LOST: Pink and White ""Lil"" Princess bike. If you have / seen it, please call Jacie
Chipman at 758-3675. Thank you.
MERCHANDISE:
FOR SALE: Used bathtub, sink, and toilet. Phone 758- 3724.
4: * * ** *
TO GIVE AWAY: Free hamster, food, and shavings. Call 758-3615.
FOR SALE: Various waterbed mattresses & liners, also one fiberglass shell for full size truck - $100 o.b.o. Phone D. Hatch 758-6652.
* * *****
FOR SALE: 20 poles 17' long, 8 - 9"" bottoms. $15.00 each. Call Cam Jordan at 758- 6532. 144 N 2nd Ave.
FOR SALE: 1. One Culligan water softener complete.
2.
One iron remover - complete.
3.
One set of wooden stock racks to fit L.W.B. Narrow Box Chev.
4.
One Bell & Howell slide projector with 38 cubes.
5.
500 gallon stock watering trough.
For any of these items, call 758-6271.
*******
FOR SALE: Large log doghouse. Can be seen at 81 E. 2nd AveN. For more information ' call 329-4938.
*******
FOR SALE: 2.5 - 20 lb Fire Extinguishers through the fire department to raise money for equipment. Firefighters will be going door to door or phone 758-3167 or 758-6804.
*******
FOR SALE: Nintendo - $75.00 or best offer. Games for sale - individually marked. Phone 758-3615.
FOR SALE: New Singer Knitting Machine with ribber attachment, complete with instruction books and custom built table. Asking $1100 cash. Phone 758-3545.
3k 3k He* 3k 3k 9k
FOR SALE: Fresh raspberries. Call Marie at 758-3527.
*******
FOR SALE: Old garage door. Give offers. Phone 758-3605.
*******
SEASONAL CRAFTS FOR SALE - by Lisa Jensen at Sue's On First.
*******
FOR SALE: 35 used golf balls including 7 Titleist, 11 Topflight - $16.00. Call Ryan at 758-6894.
*******
FOR SALE: 2 childrens ""Big Wheels"" - $8 each. 758-6378.
REAL ESTATE:
FOR SALE: 1288 sq. ft. bungalow in Magrath. Priced to sell at 39,900. Call Sharon Bacon at 382-7696 or Royal Lepage at 327-2111. MLS.
SERVICES:
Have mowers will travel. Call Russel or Robert at 758-3107.
*******
CAKES BY RITA - Will do cakes for almost any occasion. Advance notice greatly appreciated. To order, please phone 758-6315.SERVICES:
ATTENTION ALL MTM CUSTOMERS:
Due to the Magrath Celebrations we will have
pick up on Friday, July 23 instead of the 24th.
TED GREGSON STUDIOS -
Weddings, family groups, general
photography. Reminder to 3rd & 4th Ward
members - the building dedication memorial
ward photos are available at $10 for an 8x10.
Call 758-3110.
*******
NOTICE: Joy Christensen is doing hair on
Wednesday and Friday at Sue’s On First, 128
N. 1st St. W. Call for an appointment 758-
6851 or 758-6711 on Wednesday or Friday.
*******
CUSTOM HAYING - Cutting and round
baling. Phone Rick Strate at 758-6749.
*******
FOR ALL YOUR
LASER PRINTER NEEDS
! labels 1 flyers / resumes
I type setting for books I
Call Bonny 758-6309
VEHICLES;
FOR SALE: 1986 Honda Spree Scooter.
Red in color, 2000 km. You get 50 km to a
tank of gas. Please phone 758-3209.
*******
FOR SALE: 1980 Chrysler LeBaron. Also a
1972 Chev Van- $1000 obo. Phone 758-6652.
WANTED:
WANTED: Two single or double beds. If
you have any, please give us a call. Lise or
LaDean Thompson 758-3209.
*******
WANTED: A pitchfork. Do you have one
just hanging around your place just doing
nothing? Please give me a call if you want
to part with it.
NOTICE: The Swimming Pool is holding
""AQUAFIT"" nightly from 9:00 - 10:00 p.m.
Anyone interested is welcome.
COALHURST MINERS DAYS CRAFT SALE:
August 21, 1993 from 10:00 a.m. - 4:00 p.m. at
the Coaihurst Community Center. Tables sold at
$25 each. To book tables call Ruth at 381-4528.
The Monument Professionals Since 1924
I am now your Magrath area Remco Counsellor.
If I can help you with your memorial monument needs
please give me a call at 758-6386.
An important part of planning your estate
is selecting a cemetery monument. Like
a will, life insurance or arranging for a
professional funeral home director, a
decision on a memorial to your life
finalizes the important things for those
left behind - with love.
HEATHER THOMSON
MEMORIAL COUNSELLOR
758-6386 RgMCO
REMCO MEMORIALS LTD
». ....
I WILLIAM RON ALDA
I 1■■ 91•' --1 —■ v1 9.3 ■6■■■ ■ 1917—. • 19. 3■ 6
CARL Vtxi UNOIRty CAVt
RfllRhiD te.rXR LXtXLSS iOVf
WWW MEAT SPEOAI.S
I ........ 1 1 ---- T
i Outside Round Roast
$2.29 lb
$5.05 kg
Top Round Steak
$3.29 each
$7.25 kg
Seasoned Chicken Legs
$1.49 lb
$3.28 kg
Seasoned Chicken Breast
$2.69 lb
$5.93 kg
Devon Sliced Cooked Ham
175 g
$1.29 each
-------—-
Country Cottage Bacon
500 g
$1.39 each
Maple Leaf Deli Style Meats
125 g
$2.29 each
Schneiders Wieners
450 g
$1.89 each
Fletchers Deli Sticks
500 g
$2.69 each
LIBRARY NEWS
All children in grades 1-6 who are registered in the Summer Reading Program: Drop in at the library this Thursday and each Thursday all summer between 1:00 - 2:00 p.m for the special ""Green Team Club"" activities. Each week we will be recycling different items. This week bring your empty pop bottles and cans to the club. The public is welcome to donate their empty cans and bottles too. Please drop them off at the library during the Town or Library hours. Thanks for your support.
All Junior High students who register for hte Summer Reading Program will be eligible for a daily draw for a free Slurpee from the Blue Goose Gas Bar and a weekly draw for a free video from The Store. We thank these businesses for their support.
New books at the library: ""The Firm"" by John Grisham, ""Legacy of Secrets"" by Elizabeth Adler, ""Cauldron"" by Larry Bond, ""Virgins in Paradise"" by Barbara Wook, ""Missing Joseph"" by Elizabeth George, and ""A Woman's Choice"" by Marianne Williamson.
For the youth: ""Maxine's Tree"", ""The Legend of Indian Paintbrush"", The Ladybird Green Book"", The Cry of the Crow"", ""Seal Child"", and King Emmett the Second"". These books were purchased by the Chinook System with a grant provided by the Alberta Federation
for the Arts to promote the Summer Reading Theme ""Book to the Future"".
*******
The Public Library is again sponsoring an exciting SUMMER READING PROGRAM - ""Book To The Future"" for all children who have completed Grades 1-6. Come to the library every day for fun, games, activities, and prizes, too. Registration has started and will continue all summer until August 26th. Even if you are out of town a lot, you can get your name in the draw when you come in. Drop in for fun and surprises each week before going to Public Swim. Attention all Junior High readers - we are having a separate contest for you this year with prizes and fun. Summer Reading Programs prevent the loss of reading and vocabulary skills that commonly occurs among children over the summer vacation. They are also fun and challenging activities which students will enjoy. We hope that parents will encourage their children to visit the library.
ATTENTION JR. HIGH KIDS:
Plan to create an ""Alien"" for display and judging during our summer reading program (androids, mutants, and robots also welcome). Make them any size up to foot high, and use any items and material you want. Bring them to us any time during regular library hours. Enter our weekly prize draw and watch for other projects. All entrants must have a library card.Natural and Herbal Cosmetics
r ■■ ■- ■■■>■■■ ■ . .. ■ ■■ 1 ■—■■ ■■■■■■ ------------------------;
»PRODUCE SPECIALS JULY 12 - 17»
Bing Cherries .99 lb , $2.18 kg
Nectarines $1.59 lb $3.51
Navel Oranges .79 lb $1.74 kg
Green Leaf Lettuce .69 each
New White Potatoes 5 lb bag $1.79 each
Celery 3 lbs ! .99 .73 kg
nqevr Money continues to pour in for
our new Library/Museum. We
have received donations from:
* James & Shelby Anderson
* Pat Chipman
* Herb & Dianna McKelvey
* Dan & Alma Davies
* Dale Davies
* Norma & Gaelynd Pilling
* George & Joan Taylor
* William Wocknitz
* Vivian Avery
* Lorelle Gurney
* Roger & Sybil Ferguson in
honor of Jill & Blaine Harker
Donations have been
made to the Library Building
Fund in memory of Mae
Dudley by Cecelia Foote,
Marilyn & Blaine Neilson,
Lucille & DeVere Dudley, and
Marjorie & Eldon Coleman.
Anonymous donations have
been made in memory of
Lydia Anderson. The amount
from donations is now $1,940.
*******
MAGRATH STAKE FAMILY
HISTORY CENTRE
Hours:
Tues. & Thurs. 1 - 9:00 p.m.
Wed. 9:00 a.m. - 9:00 p.m.
Sunday 3 - 6:00 p.m.
Director: Betty Clifton 758-
6772
WHY IS NOEVIR UNIQUE?
□□□□
□
□
□□
□
□□□□
□
Natural to the skin
Ingredients are in full compliance with FDA Regulations
Manufacture their own products (no middle man)
Natural Bases (They come closest to the moisturizers our bodies
produce naturally.)
• Stearic Acid and HS Lipids from Soy Beans of China
• Squalene from Shark Liver Oil of Australia
• Beeswax from Honeycombs of Africa
• Jojoba Oil from Jojoba Plants of Arizona
9-24 herbs and herbal extracts from the Black Forest of West
Germany and France
No mineral oil (Most skincare companies use mineral oil as the main
base ingredient — not found in human sebum and
not natural to our skin.)
No artificial colors or fragrances
No artificial preservatives
No chemical binders or fillers
No drying alcohol
No animal testing
No human placenta
003 Line - made without animal ingredients or testing and packaged
in recyclable containers
100% money back guarantee within 30 days
A PHENOMENON IN SKINCARE!
HEATHER THOMSON
758-6386
0 “ 1993 MAGRATH CELEBRATIONS
M s ’ 94 YEARS YOUNG! $
.^WHAPPINESS IS HOMESPUN"" °
V"" U/ » * . , 0
I IX THURSDAY. JULY 22 -
¿6:00 p.m
xk»« « -, y
19-
» 4
Softball Tournament &
o Contact: Bill Alston a > L?
jU” ° & 1 V' a FRIDAY, JULY 23 n
......Bake Sale at the Arena ’
Contact: Lorraine Balderson
......Softball Tournament p
......Community Dance - Tom
Karren Gym - Pyramid o /
Entertainment a Ar I
Contact: Tyler Mandin F*
Cost: $2 /person or $10 /family \
or a donation to the Library Fund.&
° /
SATURDAY. JULY 24 V* /
........... Pancake Breakfast - Lions I
7 ! Club. Contact: John Bourne °
a.m...’..........PARADE!
fe!
a.m
□ \
7:00 a.m..
o D
11:00 m fl T
Meet in front of the School at 10:00
for Line-up and Judging.
Childrens entries meet at
Grusendorfs corner. a
n
o PARADE MARSHALL - DON JOHNSON 1
a a
12 noon Luncheon at Ice Arena
p. o Magrath 4th Scouts v *
1:00 p.m................Children's Races at the Track
a o Field. Contact: Dave Clark
1:00 p.m............... Free Swim at the Pool |
2:00 p.m...............Softball Tournament 0 <
3:00 p.m...............Lions Club Bottle Race at 1
JL, * Canal by Allan Owens, p /
a Contact: Norm Robinson ( p
FEtf. f ■£$>- a P x.
OaFTER THE SOFTBALL TOURNAMENT
Program and free Beef on a Bun at the School
Auditorium - Co-ordinators: Shirley Perry, 0
Marilyn Henderson, and Marie Stevenson.
p a e
PARADE ROUTE: Meet aCthe school, go
NORTH to the Hospital, WEST to Highway 62, °
j ° o
r then SOUTH to the Trading Company W
corn er, then EAST to the LDS Church, I
loop back t","Magrath Store News (July 14, 1993)",,J. A. Ririe,,core,
46611182,,"The recent phenomenon of ‘Deep Learning,’ which has given us such science-fiction-like innovations as search tools in photographic applications and the growing reality of self-driving cars, is a new form, and subset, of ‘Machine Learning’ made possible by very recent innovations in computing.   Machine Learning itself has been around for some decades – essentially pattern-recognition software that requires very substantial computing resources, which were, until very recently, mostly theoretical and hard to come by.  Machine Learning was one avenue of the field of Artificial Intelligence known as Narrow A.I. – the kind of ‘artificial intelligence’ that was strictly limited in scope as a first-steps starting point of what came, as a result, to be known as General A.I.  General A.I., known then as simply, ‘Artificial Intelligence’, was the 1950s dream that brought us such things as Robbie the Robot, and more recently C3PO, and The Terminator: the kind of science fiction characters that remain the only manifestations of General Artificial Intelligence.



‘Deep Learning’ also continues engineering’s 1940s trend of using language in a way that I will contest in this paper: a co-opting of words that have been used, in the past, to describe human activities, using them instead to describe what engineers have managed to make machines do.  These co-optations reduce the richness of the word, making its referent an algorithm: a flow diagram that represents the bare essentials of what an engineer can understand and reproduce of a human activity; not the human activity itself.  This diagram of the ‘engineering possible’ over-simplifies the human activity it tries to depict.   With continued usage, the meaning of the word for us today has all-too-often become reduced to what the engineer has newly defined it to mean: something much less than it once was.  



In this paper I propose to attempt to roll back some of these co-optations, and to re-introduce some of the richness of the words that have been taken by engineering.  I shall examine Turing’s seminal paper on the notion of a thinking machine.  I shall be using the philosophical insights of Henri Bergson, especially in his book, Matter and Memory, and the discoveries of neuroscience and complexity scientists. I will try to show that the answer to Turing’s question, ‘Can machines think?’ remains a resounding, ‘No!’, and that notions such as ‘deep learning’ are in fact not only an inaccurate use of the very human experience of learning, but degrade the latter in using such a term",Matter and memory and deep learning,,,,core,
21047466,1997,"By now it is widely accepted that learning a task from scratch, i.e., without any prior knowledge, is a daunting undertaking. Humans, however, rarely attempt to learn from scratch. They extract initial biases as well as strategies how to approach a learning problem from instructions and/or demonstrations of other humans. For learning control, this paper investigates how learning from demonstration can be applied in the context of reinforcement learning. We consider priming the Q-function, the value function, the policy, and the model of the task dynamics as possible areas where demonstrations can speed up learning. In general nonlinear learning problems, only model-based reinforcement learning shows significant speed-up after a demonstration, while in the special case of linear quadratic regulator (LQR) problems, all methods profit from the demonstration. In an implementation of pole balancing on a complex anthropomorphic robot arm, we demonstrate that, when facing the complexities of real signal processing, model-based reinforcement learning offers the most robustness for LQR problems. Using the suggested methods, the robot learns pole balancing in just a single trial after a 30 second long demonstration of the human instructor. 1",Learning from demonstration,,MIT Press,,core,
21875092,1996,"The paper describes an approach to reasoning about actions and planning that starting from a logical formalization arrives at the realization of an actual agent, the mobile robot &quot;Tino&quot;. The reasoning tools allow the robot to derive plans from the knowledge about the environment and the action specification, while its reactive capabilities allow it to execute its plans in the real world. The formalization is based on the propositional dynamic logics framework, but exploits the correspondence that exists between propositional dynamic logics and description logics, to carefully weaken the logical inference process in order to keep the reasoning tools of the robot both effective and efficient. Such reasoning tools are then implemented by making use of a general knowledge representation system based on description logics, namely the system CLASSIC. Introduction  In Artificial Intelligence there has always been a great interest in the design of agents that can exhibit different forms of int..",Moving a Robot Starting from a Theory of Actions,,AAAI press,,core,
23698309,0,"At the MIT Artificial Intelligence Laboratory we have been working on technologies for an Intelligent Room. Rather than pull people into the virtual world of the computer we are trying to pull the computer out into the real world of people. To do this we are combining robotics and vision technology with speech understanding systems, and agent based architectures to provide ready at hand computation and information services for people engaged in day to day activities, both on their own and in conjunction with others.  We have built a layered architecture where at the bottom level vision systems track people and identify their activities and gestures, and through word spotting decide whether people in the room are talking to each other or to the room itself. At the next level an agent architecture provides a uniform interface to such specially built systems, and to other off the shelf software, such as web browsers, etc. At the highest level we are able to build application systems that ..",The Intelligent Room Project,,,,core,
291397273,,"The final publication is available at link.springer.comSocial robots should be able to search and track people in order to help them. In this paper we present two different techniques for coordinated multi-robot teams for searching and tracking people. A probability map (belief) of a target person location is maintained, and to initialize and update it, two methods were implemented and tested: one based on a reinforcement learning algorithm and the other based on a particle filter. The person is tracked if visible, otherwise an exploration is done by making a balance, for each candidate location, between the belief, the distance, and whether close locations are explored by other robots of the team. The validation of the approach was accomplished throughout an extensive set of simulations using up to five agents and a large amount of dynamic obstacles; furthermore, over three hours of real-life experiments with two robots searching and tracking were recorded and analysed.Peer Reviewe",Searching and tracking people with cooperative mobile robots,,,,core,
4404874,1985-02-01T00:00:00,"We are building a mobile robot which will roam around the AI lab observing and later perhaps doing. Our approach to building the robot and its controlling software differs from that used in many other projects in a number of ways. (1) We model the world as three dimensional rather than two. (2) We build no special environment for our robot and insist that it must operate in the same real world that we inhabit. (3) In order to adequately deal with uncertainty of perception and control we build relational maps rather than maps embedded in a coordinate system, and we maintain explicit models of all uncertainties. (4) We explicitly monitor the computational performance of the components of the control system, in order to refine the design of a real time control system for mobile robots based on a special purpose distributed computation engine. (5) We use vision as our primary sense and relegate acoustic sensors to local obstacle detection. (6) We use a new architecture for an intelligent system designed to provide integration of many early vision processes, and robust real-time performance even in cases of sensory overload, failure of certain early vision processes to deliver much information in particular situations, and computation module failure.MIT Artificial Intelligence Laborator",A Mobile Robot Project,https://core.ac.uk/download/4404874.pdf,MIT Artificial Intelligence Laboratory,,core,
287598982,1993-12-22T00:00:00,"This thesis studies the problem of robot navigation in the presence of unexpected environmental changes, which include unknown static obstacles and moving objects with unknown trajectories. Throughout this work, neural networks, as a new technique, are used to develop the functional components, which constitute the proposed navigation strategy. The neural network based navigation strategy we propose follows a two-level hierarchy and operates by integrating three network components (planner, navigator and predictor). At the higher level, the planner generates a nominal path from the initial position to the goal among the fixed known obstacles. At the lower level, the navigator incorporates the predictor to refine the coarse path by taking into account unexpected environment changes to achieve on line real-time guidance.



During this research, three neural network components were developed. The path planner was developed first by using a three-layer feedforward network to optimize the cost (collision penalty) function of a path. The first version of the navigator - Navigator-1 - was then implemented using a multilayer feedforward network in which steering commands for static obstacle avoidance were generated by directly converting sensor reading through the network. To enable the navigator to handle moving objects with unknown trajectories, on-line motion prediction was introduced. The predictor was developed using an Elman recurrent net. Following that, an enhanced version of the Navigator-1 - Navigator-2 - was developed using a structured network in which three sub-nets were used - two of the sub-nets were used to realise dynamic obstacle avoidance and static obstacle avoidance respectively, and the third sub-net was used to make final steering decision by reconciling the results from those two sub-nets. Finally, the overall navigation strategy was implemented in a simulation system. Simulations showed encouraging results. It demonstrates that the neural network based strategy is capable of achieving adaptive navigation in the presence of unexpected environmental changes",A Neural Network Based Strategy for Robot Navigation in Dynamic Environments,https://core.ac.uk/download/287598982.pdf,'Fraunhofer-Institut fur Materialfluss und Logistik',10.21954/ou.ro.0000ff15,core,
341985691,,,"Volume 2, Issue 3, Special issue on Recent Advances in Engineering Systems (Published Papers) Articles Transmit / Received Beamforming for Frequency Diverse Array with Symmetrical frequency offsets  Shaddrack Yaw Nusenu  Adv. Sci. Technol. Eng. Syst. J. 2(3), 1-6 (2017); View Description Detailed Analysis of Amplitude and Slope Diffraction Coefficients for knife-edge structure in S-UTD-CH Model  Eray Arik, Mehmet Baris Tabakcioglu  Adv. Sci. Technol. Eng. Syst. J. 2(3), 7-11 (2017); View Description Applications of Case Based Organizational Memory Supported by the PAbMM Architecture  Martín, María de los Ángeles, Diván, Mario José  Adv. Sci. Technol. Eng. Syst. J. 2(3), 12-23 (2017); View Description Low Probability of Interception Beampattern Using Frequency Diverse Array Antenna  Shaddrack Yaw Nusenu  Adv. Sci. Technol. Eng. Syst. J. 2(3), 24-29 (2017); View Description Zero Trust Cloud Networks using Transport Access Control and High Availability Optical Bypass Switching  Casimer DeCusatis, Piradon Liengtiraphan, Anthony Sager  Adv. Sci. Technol. Eng. Syst. J. 2(3), 30-35 (2017); View Description A Derived Metrics as a Measurement to Support Efficient Requirements Analysis and Release Management  Indranil Nath  Adv. Sci. Technol. Eng. Syst. J. 2(3), 36-40 (2017); View Description Feedback device of temperature sensation for a myoelectric prosthetic hand  Yuki Ueda, Chiharu Ishii  Adv. Sci. Technol. Eng. Syst. J. 2(3), 41-40 (2017); View Description Deep venous thrombus characterization: ultrasonography, elastography and scattering operator  Thibaud Berthomier, Ali Mansour, Luc Bressollette, Frédéric Le Roy, Dominique Mottier  Adv. Sci. Technol. Eng. Syst. J. 2(3), 48-59 (2017); View Description Improving customs’ border control by creating a reference database of cargo inspection X-ray images  Selina Kolokytha, Alexander Flisch, Thomas Lüthi, Mathieu Plamondon, Adrian Schwaninger, Wicher Vasser, Diana Hardmeier, Marius Costin, Caroline Vienne, Frank Sukowski, Ulf Hassler, Irène Dorion, Najib Gadi, Serge Maitrejean, Abraham Marciano, Andrea Canonica, Eric Rochat, Ger Koomen, Micha Slegt  Adv. Sci. Technol. Eng. Syst. J. 2(3), 60-66 (2017); View Description Aviation Navigation with Use of Polarimetric Technologies  Arsen Klochan, Ali Al-Ammouri, Viktor Romanenko, Vladimir Tronko  Adv. Sci. Technol. Eng. Syst. J. 2(3), 67-72 (2017); View Description Optimization of Multi-standard Transmitter Architecture Using Single-Double Conversion Technique Used for Rescue Operations  Riadh Essaadali, Said Aliouane, Chokri Jebali and Ammar Kouki  Adv. Sci. Technol. Eng. Syst. J. 2(3), 73-81 (2017); View Description Singular Integral Equations in Electromagnetic Waves Reflection Modeling  A. S. Ilinskiy, T. N. Galishnikova  Adv. Sci. Technol. Eng. Syst. J. 2(3), 82-87 (2017); View Description Methodology for Management of Information Security in Industrial Control Systems: A Proof of Concept aligned with Enterprise Objectives.  Fabian Bustamante, Walter Fuertes, Paul Diaz, Theofilos Toulqueridis  Adv. Sci. Technol. Eng. Syst. J. 2(3), 88-99 (2017); View Description Dependence-Based Segmentation Approach for Detecting Morpheme Boundaries  Ahmed Khorsi, Abeer Alsheddi  Adv. Sci. Technol. Eng. Syst. J. 2(3), 100-110 (2017); View Description Paper Improving Rule Based Stemmers to Solve Some Special Cases of Arabic Language  Soufiane Farrah, Hanane El Manssouri, Ziyati Elhoussaine, Mohamed Ouzzif  Adv. Sci. Technol. Eng. Syst. J. 2(3), 111-115 (2017); View Description Medical imbalanced data classification  Sara Belarouci, Mohammed Amine Chikh  Adv. Sci. Technol. Eng. Syst. J. 2(3), 116-124 (2017); View Description ADOxx Modelling Method Conceptualization Environment  Nesat Efendioglu, Robert Woitsch, Wilfrid Utz, Damiano Falcioni  Adv. Sci. Technol. Eng. Syst. J. 2(3), 125-136 (2017); View Description GPSR+Predict: An Enhancement for GPSR to Make Smart Routing Decision by Anticipating Movement of Vehicles in VANETs  Zineb Squalli Houssaini, Imane Zaimi, Mohammed Oumsis, Saïd El Alaoui Ouatik  Adv. Sci. Technol. Eng. Syst. J. 2(3), 137-146 (2017); View Description Optimal Synthesis of Universal Space Vector Digital Algorithm for Matrix Converters  Adrian Popovici, Mircea Băbăiţă, Petru Papazian  Adv. Sci. Technol. Eng. Syst. J. 2(3), 147-152 (2017); View Description Control design for axial flux permanent magnet synchronous motor which operates above the nominal speed  Xuan Minh Tran, Nhu Hien Nguyen, Quoc Tuan Duong  Adv. Sci. Technol. Eng. Syst. J. 2(3), 153-159 (2017); View Description A synchronizing second order sliding mode control applied to decentralized time delayed multi−agent robotic systems: Stability Proof  Marwa Fathallah, Fatma Abdelhedi, Nabil Derbel  Adv. Sci. Technol. Eng. Syst. J. 2(3), 160-170 (2017); View Description Fault Diagnosis and Tolerant Control Using Observer Banks Applied to Continuous Stirred Tank Reactor  Martin F. Pico, Eduardo J. Adam  Adv. Sci. Technol. Eng. Syst. J. 2(3), 171-181 (2017); View Description Development and Validation of a Heat Pump System Model Using Artificial Neural Network  Nabil Nassif, Jordan Gooden  Adv. Sci. Technol. Eng. Syst. J. 2(3), 182-185 (2017); View Description Assessment of the usefulness and appeal of stigma-stop by psychology students: a serious game designed to reduce the stigma of mental illness  Adolfo J. Cangas, Noelia Navarro, Juan J. Ojeda, Diego Cangas, Jose A. Piedra, José Gallego  Adv. Sci. Technol. Eng. Syst. J. 2(3), 186-190 (2017); View Description Kinect-Based Moving Human Tracking System with Obstacle Avoidance  Abdel Mehsen Ahmad, Zouhair Bazzal, Hiba Al Youssef  Adv. Sci. Technol. Eng. Syst. J. 2(3), 191-197 (2017); View Description A security approach based on honeypots: Protecting Online Social network from malicious profiles  Fatna Elmendili, Nisrine Maqran, Younes El Bouzekri El Idrissi, Habiba Chaoui  Adv. Sci. Technol. Eng. Syst. J. 2(3), 198-204 (2017); View Description Pulse Generator for Ultrasonic Piezoelectric Transducer Arrays Based on a Programmable System-on-Chip (PSoC)  Pedro Acevedo, Martín Fuentes, Joel Durán, Mónica Vázquez, Carlos Díaz  Adv. Sci. Technol. Eng. Syst. J. 2(3), 205-209 (2017); View Description Enabling Toy Vehicles Interaction With Visible Light Communication (VLC)  M. A. Ilyas, M. B. Othman, S. M. Shah, Mas Fawzi  Adv. Sci. Technol. Eng. Syst. J. 2(3), 210-216 (2017); View Description Analysis of Fractional-Order 2xn RLC Networks by Transmission Matrices  Mahmut Ün, Manolya Ün  Adv. Sci. Technol. Eng. Syst. J. 2(3), 217-220 (2017); View Description Fire extinguishing system in large underground garages  Ivan Antonov, Rositsa Velichkova, Svetlin Antonov, Kamen Grozdanov, Milka Uzunova, Ikram El Abbassi  Adv. Sci. Technol. Eng. Syst. J. 2(3), 221-226 (2017); View Description Directional Antenna Modulation Technique using A Two-Element Frequency Diverse Array  Shaddrack Yaw Nusenu  Adv. Sci. Technol. Eng. Syst. J. 2(3), 227-232 (2017); View Description Classifying region of interests from mammograms with breast cancer into BIRADS using Artificial Neural Networks  Estefanía D. Avalos-Rivera, Alberto de J. Pastrana-Palma  Adv. Sci. Technol. Eng. Syst. J. 2(3), 233-240 (2017); View Description Magnetically Levitated and Guided Systems  Florian Puci, Miroslav Husak  Adv. Sci. Technol. Eng. Syst. J. 2(3), 241-244 (2017); View Description Energy-Efficient Mobile Sensing in Distributed Multi-Agent Sensor Networks  Minh T. Nguyen  Adv. Sci. Technol. Eng. Syst. J. 2(3), 245-253 (2017); View Description Validity and efficiency of conformal anomaly detection on big distributed data  Ilia Nouretdinov  Adv. Sci. Technol. Eng. Syst. J. 2(3), 254-267 (2017); View Description S-Parameters Optimization in both Segmented and Unsegmented Insulated TSV upto 40GHz Frequency  Juma Mary Atieno, Xuliang Zhang, HE Song Bai  Adv. Sci. Technol. Eng. Syst. J. 2(3), 268-276 (2017); View Description Synthesis of Important Design Criteria for Future Vehicle Electric System  Lisa Braun, Eric Sax  Adv. Sci. Technol. Eng. Syst. J. 2(3), 277-283 (2017); View Description Gestural Interaction for Virtual Reality Environments through Data Gloves  G. Rodriguez, N. Jofre, Y. Alvarado, J. Fernández, R. Guerrero  Adv. Sci. Technol. Eng. Syst. J. 2(3), 284-290 (2017); View Description Solving the Capacitated Network Design Problem in Two Steps  Meriem Khelifi, Mohand Yazid Saidi, Saadi Boudjit  Adv. Sci. Technol. Eng. Syst. J. 2(3), 291-301 (2017); View Description A Computationally Intelligent Approach to the Detection of Wormhole Attacks in Wireless Sensor Networks  Mohammad Nurul Afsar Shaon, Ken Ferens  Adv. Sci. Technol. Eng. Syst. J. 2(3), 302-320 (2017); View Description Real Time Advanced Clustering System  Giuseppe Spampinato, Arcangelo Ranieri Bruna, Salvatore Curti, Viviana D’Alto  Adv. Sci. Technol. Eng. Syst. J. 2(3), 321-326 (2017); View Description Indoor Mobile Robot Navigation in Unknown Environment Using Fuzzy Logic Based Behaviors  Khalid Al-Mutib, Foudil Abdessemed  Adv. Sci. Technol. Eng. Syst. J. 2(3), 327-337 (2017); View Description Validity of Mind Monitoring System as a Mental Health Indicator using Voice  Naoki Hagiwara, Yasuhiro Omiya, Shuji Shinohara, Mitsuteru Nakamura, Masakazu Higuchi, Shunji Mitsuyoshi, Hideo Yasunaga, Shinichi Tokuno  Adv. Sci. Technol. Eng. Syst. J. 2(3), 338-344 (2017); View Description The Model of Adaptive Learning Objects for virtual environments instanced by the competencies  Carlos Guevara, Jose Aguilar, Alexandra González-Eras  Adv. Sci. Technol. Eng. Syst. J. 2(3), 345-355 (2017); View Description An Overview of Traceability: Towards a general multi-domain model  Kamal Souali, Othmane Rahmaoui, Mohammed Ouzzif  Adv. Sci. Technol. Eng. Syst. J. 2(3), 356-361 (2017); View Description L-Band SiGe HBT Active Differential Equalizers with Variable, Positive or Negative Gain Slopes Using Dual-Resonant RLC Circuits  Yasushi Itoh, Hiroaki Takagi  Adv. Sci. Technol. Eng. Syst. J. 2(3), 362-368 (2017); View Description Moving Towards Reliability-Centred Management of Energy, Power and Transportation Assets  Kang Seng Seow, Loc K. Nguyen, Kelvin Tan, Kees-Jan Van Oeveren  Adv. Sci. Technol. Eng. Syst. J. 2(3), 369-375 (2017); View Description Secure Path Selection under Random Fading  Furqan Jameel, Faisal, M Asif Ali Haider, Amir Aziz Butt  Adv. Sci. Technol. Eng. Syst. J. 2(3), 376-383 (2017); View Description Security in SWIPT with Power Splitting Eavesdropper  Furqan Jameel, Faisal, M Asif Ali Haider, Amir Aziz Butt  Adv. Sci. Technol. Eng. Syst. J. 2(3), 384-388 (2017); View Description Performance Analysis of Phased Array and Frequency Diverse Array Radar Ambiguity Functions  Shaddrack Yaw Nusenu  Adv. Sci. Technol. Eng. Syst. J. 2(3), 389-394 (2017); View Description Adaptive Discrete-time Fuzzy Sliding Mode Control For a Class of Chaotic Systems  Hanene Medhaffar, Moez Feki, Nabil Derbel  Adv. Sci. Technol. Eng. Syst. J. 2(3), 395-400 (2017); View Description Fault Tolerant Inverter Topology for the Sustainable Drive of an Electrical Helicopter  Igor Bolvashenkov, Jörg Kammermann, Taha Lahlou, Hans-Georg Herzog  Adv. Sci. Technol. Eng. Syst. J. 2(3), 401-411 (2017); View Description Computational Intelligence Methods for Identifying Voltage Sag in Smart Grid  Turgay Yalcin, Muammer Ozdemir  Adv. Sci. Technol. Eng. Syst. J. 2(3), 412-419 (2017); View Description A Highly-Secured Arithmetic Hiding cum Look-Up Table (AHLUT) based S-Box for AES-128 Implementation  Ali Akbar Pammu, Kwen-Siong Chong, Bah-Hwee Gwee  Adv. Sci. Technol. Eng. Syst. J. 2(3), 420-426 (2017); View Description Service Productivity and Complexity in Medical Rescue Services  Markus Harlacher, Andreas Petz, Philipp Przybysz, Olivia Chaillié, Susanne Mütze-Niewöhner  Adv. Sci. Technol. Eng. Syst. J. 2(3), 427-434 (2017); View Description Principal Component Analysis Application on Flavonoids Characterization  Che Hafizah Che Noh, Nor Fadhillah Mohamed Azmin, Azura Amid  Adv. Sci. Technol. Eng. Syst. J. 2(3), 435-440 (2017); View Description A Reconfigurable Metal-Plasma Yagi-Yuda Antenna for Microwave Applications  Giulia Mansutti, Davide Melazzi, Antonio-Daniele Capobianco  Adv. Sci. Technol. Eng. Syst. J. 2(3), 441-448 (2017); View Description Verifying the Detection Results of Impersonation Attacks in Service Clouds  Sarra Alqahtani, Rose Gamble  Adv. Sci. Technol. Eng. Syst. J. 2(3), 449-459 (2017); View Description Image Segmentation Using Fuzzy Inference System on YCbCr Color Model  Alvaro Anzueto-Rios, Jose Antonio Moreno-Cadenas, Felipe Gómez-Castañeda, Sergio Garduza-Gonzalez  Adv. Sci. Technol. Eng. Syst. J. 2(3), 460-468 (2017); View Description Segmented and Detailed Visualization of Anatomical Structures based on Augmented Reality for Health Education and Knowledge Discovery  Isabel Cristina Siqueira da Silva, Gerson Klein, Denise Munchen Brandão  Adv. Sci. Technol. Eng. Syst. J. 2(3), 469-478 (2017); View Description Intrusion detection in cloud computing based attack patterns and risk assessment  Ben Charhi Youssef, Mannane Nada, Bendriss Elmehdi, Regragui Boubker  Adv. Sci. Technol. Eng. Syst. J. 2(3), 479-484 (2017); View Description Optimal Sizing and Control Strategy of renewable hybrid systems PV-Diesel Generator-Battery: application to the case of Djanet city of Algeria  Adel Yahiaoui, Khelifa Benmansour, Mohamed Tadjine  Adv. Sci. Technol. Eng. Syst. J. 2(3), 485-491 (2017); View Description RFID Antenna Near-field Characterization Using a New 3D Magnetic Field Probe  Kassem Jomaa, Fabien Ndagijimana, Hussam Ayad, Majida Fadlallah, Jalal Jomaah  Adv. Sci. Technol. Eng. Syst. J. 2(3), 492-497 (2017); View Description Design, Fabrication and Testing of a Dual-Range XY Micro-Motion Stage Driven by Voice Coil Actuators  Xavier Herpe, Matthew Dunnigan, Xianwen Kong  Adv. Sci. Technol. Eng. Syst. J. 2(3), 498-504 (2017); View Description Self-Organizing Map based Feature Learning in Bio-Signal Processing  Marwa Farouk Ibrahim Ibrahim, Adel Ali Al-Jumaily  Adv. Sci. Technol. Eng. Syst. J. 2(3), 505-512 (2017); View Description A delay-dependent distributed SMC for stabilization of a networked robotic system exposed to external disturbances  Fatma Abdelhedi, Nabil Derbel  Adv. Sci. Technol. Eng. Syst. J. 2(3), 513-519 (2017); View Description Modelization of cognition, activity and motivation as indicators for Interactive Learning Environment  Asmaa Darouich, Faddoul Khoukhi, Khadija Douzi  Adv. Sci. Technol. Eng. Syst. J. 2(3), 520-531 (2017); View Description Homemade array of surface coils implementation for small animal magnetic resonance imaging  Fernando Yepes-Calderon, Olivier Beuf  Adv. Sci. Technol. Eng. Syst. J. 2(3), 532-539 (2017); View Description An Encryption Key for Secure Authentication: The Dynamic Solution  Zubayr Khalid, Pritam Paul, Khabbab Zakaria, Himadri Nath Saha  Adv. Sci. Technol. Eng. Syst. J. 2(3), 540-544 (2017); View Description Multi-Domain Virtual Network Embedding with Coordinated Link Mapping  Shuopeng Li, Mohand Yazid Saidi, Ken Chen  Adv. Sci. Technol. Eng. Syst. J. 2(3), 545-552 (2017); View Description Semantic-less Breach Detection of Polymorphic Malware in Federated Cloud",,'ASTES Journal',10.25046/aj020371,core,
76360605,1992-03-22T08:00:00,"We use findings in machine learning, developmental psychology, and neurophysiology to guide a robotic learning system\u27s level of representation both for actions and for percepts. Visually-driven grasping is chosen as the experimental task since it has general applicability and it has been extensively researched from several perspectives. An implementation of a robotic system with a gripper, compliant instrumented wrist, arm and vision is used to test these ideas. Several sensorimotor primitives (vision segmentation and manipulatory reflexes) are implemented in this system and may be thought of as the  innate  perceptual and motor abilities of the system.
Applying empirical learning techniques to real situations brings up such important issues as observation sparsity in high-dimensional spaces, arbitrary underlying functional forms of the reinforcement distribution and robustness to noise in exemplars. The well-established technique of non-parametric projection pursuit regression (PPR) is used to accomplish reinforcement learning by searching for projections of high-dimensional data sets that capture task invariants.
We also pursue the following problem: how can we use human expertise and insight into grasping to train a system to select both appropriate hand preshapes and approaches for a wide variety of objects, and then have it verify and refine its skills through trial and error. To accomplish this learning we propose a new class of Density Adaptive reinforcement learning algorithms. These algorithms use statistical tests to identify possibly  interesting  regions of the attribute space in which the dynamics of the task change. They automatically concentrate the building of high resolution descriptions of the reinforcement in those areas, and build low resolution representations in regions that are either not populated in the given task or are highly uniform in outcome.
Additionally, the use of any learning process generally implies failures along the way. Therefore, the mechanics of the untrained robotic system must be able to tolerate mistakes during learning and not damage itself. We address this by the use of an instrumented, compliant robot wrist that controls impact forces",A Robotic System for Learning Visually-Driven Grasp Planning (Dissertation Proposal),https://core.ac.uk/download/76360605.pdf,ScholarlyCommons,,core,
357528414,1997-01-01T00:00:00,"Abstract By now it is widely accepted that learning a task from scratch, i.e., without any prior knowledge, is a daunting undertaking. Humans, however, rarely attempt to learn from scratch. They extract initial biases as well as strategies how to approach a learning problem from instructions and/or demonstrations of other humans. For learning control, this paper investigates how learning from demonstration can be applied in the context of reinforcement learning. We consider priming the Q-function, the value function, the policy, and the model of the task dynamics as possible areas where demonstrations can speed up learning. In general nonlinear learning problems, only model-based reinforcement learning shows significant speed-up after a demonstration, while in the special case of linear quadratic regulator (LQR) problems, all methods profit from the demonstration. In an implementation of pole balancing on a complex anthropomorphic robot arm, we demonstrate that, when facing the complexities of real signal processing, modelbased reinforcement learning offers the most robustness for LQR problems. Using the suggested methods, the robot learns pole balancing in just a single trial after a 30 second long demonstration of the human instructor",Learning from demonstration”.,,'MIT Press - Journals',,core,
214169325,1991-10-01T07:00:00,"We propose that some aspects of task based learning in robotics can be approached using nativist and constructivist views on human sensorimotor development as a metaphor. We use findings in developmental psychology, neurophysiology, and machine perception to guide a robotic learning system\u27s level of representation both for actions and for percepts. Visually driven grasping is chosen as the experimental task since it has general applicability and it has been extensively researched from several perspectives. An implementation of a robotic system with a dexterous three fingered hand, compliant instrumented wrist, arm and vision is used to test these ideas. Several sensorimotor primitives (vision segmentation and manipulatory reflexes) are implemented in this system and may be though of as the  innate  perceptual and motor abilities of the system.
Applying empirical learning techniques to real situations brings up some important issues such as observation sparsity in high dimensional spaces, arbitrary underlying functional forms of the reinforcement distribution and robustness to noise in exemplars. The well established technique of non-parametric projection pursuit regression (PPR) is used to accomplish reinforcement learning by searching for generalization directions determining projections of high dimensional data sets which capture task invariants. Additionally, the learning process generally implies failures along the way. Therefore, the mechanics of the untrained robotic system must be able to tolerate grave mistakes during learning and not damage itself. We address this by the use of an instrumented compliant robot wrist which controls impact forces",Sensorimotor Learning Using Active Perception in Continuous Domains,,ScholarlyCommons,,core,
24625417,1995,"From perception to action and from action to perception, all elements of an autonomous agent are interdependent and need to be strongly coherent. The nal behavior of the agent is the result of the global activity ofthisloopandevery weakeness or incoherence of a single element has strong consequences on the performances of the agent. We think that, for the purpose of building autonomous robots, all these elements need to be developed together in continuous interaction with the environment. We describe the implementation of a possible solution (arti cial neural networks and genetic algorithms) on a real mobile robot through a set of three di erent experiments. We focus our attention on three di erent aspects of the control structure: perception, internal representation and action. In all the experiments these aspects are not considered as single processing elements, but as part of an agent. For every experiment, the advantages and disadvantages of this approach are presented and discussed. The results show that the combination of genetic algorithms and neural networks is a very interesting technique for the development ofcontrol structures in autonomous agents. The time necessary for evolution, on the other hand, is a very important limitation oftheevolutionary approach",Evolution of Neural Control Structures: Some Experiments on Mobile Robots,,,,core,
24339212,1993,". To get a better understanding of the application of artificial neural networks to the robotics field, the first part of this 3-year project supported by the NFP - PNR23 project was devoted to the study of some applications of neural networks to motor control, sensor fusion and image processing and to the development of sensor interfaces and neural network dedicated accelerators.  The second part of the project, described in this paper, takes advantage from the experience gained in the first part and studies the application of artificial neural networks to a particular sub-field of robotics: mobile robotics. Most of the problems of the domain seem to be particularly adapted to neural network solutions. To illustrate this, we present in the first part of the article an example implementation of a very simple behaviour in a real robot using several design methodologies. The robot used for these experiments is described. In the second part of the document we present some more complex exp..",Biologically Inspired Mobile Robot Control Algorithms,,,,core,
24296461,1996,". In this paper we present an example of the application of a technique, which we call robot shaping, to designing and building learning autonomous robots. Our autonomous robot (called HAMSTER  1  ) is a multi-sensor mobile robot that performs the task of collecting &quot;food&quot; and bringing it to its &quot;nest&quot;. Its control architecture is based on the behavioral paradigm. The behavioral modules are implemented as classifier systems and are learned by a reinforcement learning technique exploiting the Bucket Brigade and an extended version of the Genetic Algorithm. The chief features of HAMSTER are that it combines innate (i.e., prewired) and learned behaviors, and that training was carried out in a simulated environment and then transferred to the real robot. Published in Proceedings of ISRAM&apos;96, Sixth International Symposium on Robotics and Manufacturing, M.Jamshidi et al. (Eds.), May 28--30, 1996, Montpellier, France. ^  Currently Ph.D. student at the University of Padua, Italy.  1  HAMSTER :..",Robot Shaping: The Hamster Experiment,,,,core,
24366768,1997,". While Artificial Intelligence techniques have been applied to a variety of software engineering applications, the area of automated software testing remains largely unexplored. Yet, test cases for certain types of systems (e.g., those with command language interfaces and transaction based systems) are similar to plans. We have exploited this similarity by constructing an automated test case generator with an AI planning system at its core. We compared the functionality and output of two systems, one based on Software Engineering techniques and the other on planning, for a real application: the StorageTek robot tape library command language. From this, we showed that AI planning is a viable technique for test case generation and that the two approaches are complementary in their capabilities.  Keywords: System testing, AI planning, blackbox testing 1. Automated Test Case Generation  Testing consumes a large amount of time and effort in software development. Although critical for ensur..",Test Case Generation as an AI Planning Problem,,,10.1007/978-0-585-34714-1_5,core,
100228653,1992,"We propose that some aspects of task based learning in robotics can be approached using nativist and constructivist views on human sensorimotor development as a metaphor. We use findings in developmental psychology, neurophysiology, and machine perception to guide a robotic learning system&apos;s level of representation both for actions and for percepts. Visually driven grasping is chosen as the experimental task since it has general applicability and it has been extensively researched from several perspectives. An implementation of a robotic system with a dexterous three fingered hand, compliant instrumented wrist, arm and vision is used to test these ideas. Several sensorimotor primitives (vision segmentation and manipulatory reflexes) are implemented in this system and may be thought of as the &quot;innate &quot; perceptual and motor abilities of the system. Applying empirical learning techniques to real situations brings up some important issues such as observation sparsity in high dimensional spaces, arbitrary underlying functional forms of the reinforcement distribution and robustness to noise in exemplars. The well established technique of non-parametric projection pursuit regression (PPR) is used to accomplish reinforcement learning by searching for generalization directions determining projections of high dimensional data sets which capture task invariants. Additionally, the learnin",Robotic sensorimotor learning in continuous domains,,,,core,
100333342,1993,"Learning plays a vital role in the development of situated agents. In this paper, we explore the use of reinforcement learning to &quot;shape &quot; a robot to perform a predefined target behavior. We connect both simulated and real robots to ALECSYS, a parallel implementation of a learning classifier system with an extended genetic algorithm. After classifying different kinds of Animat-like behaviors, we explore the effects on learning of different types of agent&apos;s architecture (monolithic, flat and hierarchical) and of training strategies. In particular, hierarchical architecture requires the agent to learn how to coordinate basic learned responses. We show that the best results are achieved when both the agent&apos;s architecture and the training strategy match the structure of the behavior pattern to be learned. We report the results of a number of experiments carried out both in simulated and in real environments, and show that the results of simulations carry smoothly to real robots. While most of our experiments deal with simple reactive behavior, in one of them we demonstrate the use of a simple and general memory mechanism. As a whole, our experimental activity demonstrates that classifier systems with genetic algorithms can be practically employed to develop autonomous agents",Robot Shaping: Developing Situated Agents through Learning,,,,core,
229089491,1994-01-01T08:00:00,"As research fields in AI accelerate and a high number of experts are demanded by industry, Expert Systems play an important role in meeting the technological sophistication required in today’s competitive world. Industries are demanding the assistance of human experts to solve complicated problems. However, there is a shortage of experts due to this demand. Expert Systems are rapidly becoming one of the major approaches to solve engineering and manufacturing problems. They have been implemented for several practical applications in many decision making problems. Expert Systems are helping major companies to diagnose processes in real time, schedule operations, maintain machinery and design services and production facilities.
The area of robotics is a fertile one for application of Expert Systems. Robots are an integral part of today’s manufacturing environment. New tasks are being defined for robots in order to meet the challenges of Flexible Manufacturing Systems. Robots are entering every fact of manufacturing. Along with this grown there is an increasing variety of robots to choose from. One of the major problems facing the potential robot user will be his/her choice of an optimum robot for a particular task. Various parameters should be considered and the user should choose an industrial robot whose characteristics satisfy the requirements of the intended task.
This work will present a viable solution to the problem of selecting an optimum robot by building a Knowledge-Based Expert System using the LEVEL5 shell. LEVEL5 is an Expert System software created by Information Builders Inc., which runs on the IBM Personal Computer, XT and AT. The system asks the user several questions regarding the usage and requirements of the desired robot. It uses the knowledge base and the rules to determine an optimum robot --Abstract, page iii",Application of knowledge-based expert systems for selection of industrial robots,,Scholars\u27 Mine,,core,
323907578,,"This thesis proposes a parsimonious approach to localization, mapping and object recog- nition for a pseudo-mobile robot equipped with a biomimetic array of tactile whiskers to autonomously interact, explore and represent a real-world environment. Tactile whisker sensors enable the robotic platform to perceive unique environmental properties and can operate in extreme conditions that preclude the use of conventional sensors, however, such sensors are disadvantaged by their limited range and sample sparsity. To address the sparsity, the information contained in each contact should be fully exploited, whilst the limited range of the array can be addressed through appropriate movement and placement of the whiskers and the array.An existing Simultaneous Localization and Mapping (SLAM) algorithm called Rat- SLAM was adopted as the basis for the inference of location and demonstrated as suitable for correcting odometry errors using whisker tactile sensing. The adoption of a closed loop contact induced whisker placement strategy, directly inspired by rat whisking be- havior, improved the performance of the algorithm in further reducing odometry error. The fidelity of object shape reconstruction through the forward kinematic projection of whisker contact locations was analyzed and a number of machine learning approaches compared to assess their ecacy at discerning radial distance to contact and thus im- prove object shape reconstruction. A support vector regression technique was found to reliably improve estimates of radial distance to contact along the whisker shaft following natural, unconstrained whisker contacts. A framework for combining the 3D pose esti- mation from RatSLAM with a 6D pose estimation system suitable for object recognition is proposed with the 6D system implemented and demonstrated correctly identifying household objects through tactile whisker exploration. The adoption of whisker array placement strategies inspired by cutaneous-tactile research improved the robustness of object identification and two regional search strategies were investigated for the purpose of reducing the time taken to correctly classify objects",Toward navigating complex terrains using a biomimetic whisker sensor array,,,,core,
24301065,1996,"In this paper we present an example of the application of a technique, which we call robot shaping, to designing and building learning autonomous robots. Our autonomous robot (called HAMSTER  1  ) is a multi-sensor mobile robot that performs the task of collecting &quot;food&quot; and bringing it to its &quot;nest&quot;. Its control architecture is based on the behavioral paradigm. The behavioral modules are implemented as classifier systems and are learned by a reinforcement learning technique exploiting the Bucket Brigade and an extended version of the Genetic Algorithm. The chief features of HAMSTER are that it combines innate (i.e., prewired) and learned behaviors, and that training was carried out in a simulated environment and then transferred to the real robot.  KEYWORDS: Learning; classifier systems; autonomous robots 1.INTRODUCTION  Our work is concerned with designing and building learning autonomous robots. Programming an autonomous robot so that it reliably acts in a dynamic environment is a d..",Robot Shaping: The Hamster Experiment,,,,core,
42810688,"Jul 18, 1991","The use of eddy current techniques for characterizing flaws in graphite-based filament-wound cylindrical structures is described. A major emphasis was also placed upon incorporating artificial intelligence techniques into the signal analysis portion of the inspection process. Developing an eddy current scanning system using a commercial robot for inspecting graphite structures (and others) was a goal in the overall concept and is essential for the final implementation for the expert systems interpretation. Manual scans, as performed in the preliminary work here, do not provide sufficiently reproducible eddy current signatures to be easily built into a real time expert system. The expert systems approach to eddy current signal analysis requires that a suitable knowledge base exist in which correct decisions as to the nature of a flaw can be performed. A robotic workcell using eddy current transducers for the inspection of carbon filament materials with improved sensitivity was developed. Improved coupling efficiencies achieved with the E-probes and horseshoe probes are exceptional for graphite fibers. The eddy current supervisory system and expert system was partially developed on a MacIvory system. Continued utilization of finite element models for predetermining eddy current signals was shown to be useful in this work, both for understanding how electromagnetic fields interact with graphite fibers, and also for use in determining how to develop the knowledge base. Sufficient data was taken to indicate that the E-probe and the horseshoe probe can be useful eddy current transducers for inspecting graphite fiber components. The lacking component at this time is a large enough probe to have sensitivity in both the far and near field of a thick graphite epoxy component",Automated eddy current analysis of materials,https://core.ac.uk/download/pdf/42810688.pdf,,,core,
40582445,1995-01-01T00:00:00,"The project Foundations, Applications and Implementation of Distributed AI Systems: Modelling Cooperating Agents (AKA-MOD) contributed to research in the fields of Distributed AI and Multiagent Systems. The objective of the project AKA-MOD was to analyse the applicability of existing techniques from the area of multiagent Systems to real-world scenarios. The investigations concentrated on the areas Distributed Scheduling and Robotics. The goal was to enhance existing and to develop new methodologies applicable to these application areas, and to design a domain-independent testbed for the development of multiagent system applications, which incorporates the results from the investigation of the application scenarios. This domain-independent testbed for multiagent system applications, which was used as an implementation platform in the project, forms the basis for the concepts developed in the AKA-MOD project. On this basis an architecture for the description of individual agents was defined. For the description of communication processes among these agents, a communication and negotiation framework was defined. To prove the power of the developed concepts, prototypical multiagent systems modelling specific application examples were implemented. (orig.)SIGLEAvailable from TIB Hannover: F95B1331+a / FIZ - Fachinformationszzentrum Karlsruhe / TIB - Technische InformationsbibliothekBundesministerium fuer Forschung und Technologie (BMFT), Bonn (Germany)DEGerman","Grundlagen, Anwendungen und Implementierung von verteilten KI-Systemen  Modellierung kooperierender Agenten. Abschlussbericht",,,,core,
1498818,1994-01-01T00:00:00,"ARCHON™ (ARchitecture for Cooperative Heterogeneous ON-line systems) was Europe’s largest project in the area of Distributed Artificial Intelligence (DAI). It devised a general-purpose architecture, software framework, and methodology which has been used to support the development of DAI systems in a number of real world industrial domains. Some examples of the applications to which it has been successfully applied include: electricity distribution and supply, electricity transmission and distribution, control of a cement kiln complex, control of a particle accelerator, and control of a robotics application. The type of cooperating community that it supports has a decentralised control regime and individual problem solving agents which are large grain, loosely coupled, and semi-autonomous. This paper will tackle a broad range of issues related to the application of ARCHON technology to industrial applications. Firstly, it gives the rationale for a DAI approach to industrial applications and highlights the characteristics which typify this important domain. Secondly, the ARCHON framework is detailed - with a special emphasis being placed upon the implementation architecture. Thirdly, a brief resumee and status report of the main applications is presented. Finally, the lessons learned and the future plans are presented",The ARCHON System and its Applications,,,,core,
24278121,1996,"A continuous-time, continuous-state version of the temporal difference (TD) algorithm is derived in order to facilitate the application of reinforcement learning to real-world control tasks and neurobiological modeling. An optimal nonlinear feedback control law was also derived using the derivatives of the value function. The performance of the algorithms was tested in a task of swinging up a pendulum with limited torque. Both the &quot;critic&quot; that specifies the paths to the upright position and the &quot;actor&quot; that works as a nonlinear feedback controller were successfully implemented by radial basis function (RBF) networks. 1 INTRODUCTION  The temporal-difference (TD) algorithm (Sutton, 1988) for delayed reinforcement learning has been applied to a variety of tasks, such as robot navigation, board games, and biological modeling (Houk et al., 1994). Elucidation of the relationship between TD learning and dynamic programming (DP) has provided good theoretical insights (Barto et al., 1995). How..",Temporal Difference Learning in Continuous Time and Space,,MIT Press,,core,
24314719,1995,"The Robot World Cup Initiative (RoboCup) is an attempt to foster AI and intelligent robotics research by providing a standard problem where wide range of technologies can be integrated and examined. In order for a robot team to actually perform a soccer game, various technologies must be incorporated including: design principles of autonomous agents, multiagent collaboration, strategy acquisition, realtime reasoning, robotics, and sensor-fusion. Unlike AAAI robot competition, which is tuned for a single heavy-duty slow-moving robot, RoboCup is a task for a team of multiple fastmoving robots under a dynamic environment. Although RoboCup&apos;s final target is a world cup with real robots, RoboCup offers a software platform for research on the software aspects of RoboCup. This paper describes technical challenges involved in RoboCup, rules, and simulation environment. 1 Introduction: RoboCup as a Standard AI Problem We propose a Robot World Cup (RoboCup), as a new standard problem for AI an..",RoboCup: The Robot World Cup Initiative,,,,core,
24295028,1996,"In this paper, we proposed a method by which a stereo vision-based mobile robot learns to reach a target by detecting and avoiding occlusions. We call the internal representation that describes the learned behavior  \stereo sketch.&quot; First, an input scene is segmented into homogeneous regions by the enhanced ISODATA algorithm with MDL principle in terms of image coordinates and disparity information obtained from the fast stereo matcher based on the coarse-to- ne control method. Then, in terms of the segmented regions including the target area and their occlusion status identied during the stereo and motion disparity estimation process, we construct a state space for a reinforcement learning method to obtain target reaching behavior. As a result, the robot can avoid obstacles without explicitly describing them. We give the computer simulation results and real robot implementation to show the validity of our method. 1 Introduction Realization o# a#tono#o## a#ent# t#at o##anize t#ei# o#..",Stereo Sketch: Stereo Vision-Based Target Reaching Behavior Acquisition with Occlusion Detection and Avoidance,,IEEE,,core,
24369943,1995,"From perception to action and from action to perception, all elements of an autonomous agent are interdependent and need to be strongly coherent. The final behavior of the agent is the result of the global activity of this loop and every weakeness or incoherence of a single element has strong consequences on the performances of the agent. We think that, for the purpose of building autonomous robots, all these elements need to be developed together in continuous interaction with the environment. We describe the implementation of a possible solution (artificial neural networks and genetic algorithms) on a real mobile robot through a set of three different experiments. We focus our attention on three different aspects of the control structure: perception, internal representation and action. In all the experiments these aspects are not considered as single processing elements, but as part of an agent. For every experiment, the advantages and disadvantages of this approach are presented and..",Evolution of Neural Control Structures: Some Experiments on Mobile Robots,,,10.1016/0921-8890(96)81008-6,core,
42820555,"Dec 1, 1990","New analytical development in kinematics planning is reported. The INtelligent KInematics Planner (INKIP) consists of the kinematics spline theory and the adaptive logic annealing process. Also, a novel framework of robot learning mechanism is introduced. The FUzzy LOgic Self Organized Neural Networks (FULOSONN) integrates fuzzy logic in commands, control, searching, and reasoning, the embedded expert system for nominal robotics knowledge implementation, and the self organized neural networks for the dynamic knowledge evolutionary process. Progress on the mechanical construction of SRA Advanced Robotic System (SRAARS) and the real time robot vision system is also reported. A decision was made to incorporate the Local Area Network (LAN) technology in the overall communication system",Intelligent manipulation technique for multi-branch robotic systems,https://core.ac.uk/download/pdf/42820555.pdf,,,core,
42816049,"Jun 24, 1990","This research effort focused on the use of eddy current techniques for characterizing flaws in graphite-based filament-wound cylindrical structures. A major emphasis was on incorporating artificial intelligence techniques into the signal analysis portion of the inspection process. Developing an eddy current scanning system using a commercial robot for inspecting graphite structures (and others) has been a goal in the overall concept and is essential for the final implementation for expert system interpretation. Manual scans, as performed in the preliminary work here, do not provide sufficiently reproducible eddy current signatures to be easily built into a real time expert system. The expert systems approach to eddy current signal analysis requires that a suitable knowledge base exist in which correct decisions as to the nature of the flaw can be performed. In eddy current or any other expert systems used to analyze signals in real time in a production environment, it is important to simplify computational procedures as much as possible. For that reason, we have chosen to use the measured resistance and reactance values for the preliminary aspects of this work. A simple computation, such as phase angle of the signal, is certainly within the real time processing capability of the computer system. In the work described here, there is a balance between physical measurements and finite element calculations of those measurements. The goal is to evolve into the most cost effective procedures for maintaining the correctness of the knowledge base",Automated eddy current analysis of materials,https://core.ac.uk/download/pdf/42816049.pdf,,,core,
