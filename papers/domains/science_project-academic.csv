doi,publication_date,publication,publisher,title,abstract,database
10.1007/978-0-387-85820-3,2010-10-28,b,Springer,recommender systems handbook," The explosive growth of e-commerce and online environments has made the issue of information search and selection increasingly serious; users are overloaded by options to consider and they may not have the time or knowledge to personally evaluate these options. Recommender systems have proven to be a valuable way for online users to cope with the information overload and have become one of the most powerful and popular tools in electronic commerce. Correspondingly, various techniques for recommendation generation have been proposed. During the last decade, many of them have also been successfully deployed in commercial environments. Recommender Systems Handbook, an edited volume, is a multi-disciplinary effort that involves world-wide experts from diverse fields, such as artificial intelligence, human computer interaction, information technology, data mining, statistics, adaptive user interfaces, decision support systems, marketing, and consumer behavior. Theoreticians and practitioners from these fields continually seek techniques for more efficient, cost-effective and accurate recommender systems. This handbook aims to impose a degree of order on this diversity, by presenting a coherent and unified repository of recommender systems major concepts, theories, methodologies, trends, challenges and applications. Extensive artificial applications, a variety of real-world applications, and detailed case studies are included. Recommender Systems Handbook illustrates how this technology can support the user in decision-making, planning and purchasing processes. It works for well known corporations such as Amazon, Google, Microsoft and AT&T. This handbook is suitable for researchers and advanced-level students in computer science as a reference.",project-academic
10.1126/SCIENCE.AAC6076,2015-07-17,a,American Association for the Advancement of Science,computational rationality a converging paradigm for intelligence in brains minds and machines," After growing up together, and mostly growing apart in the second half of the 20th century, the fields of artificial intelligence (AI), cognitive science, and neuroscience are reconverging on a shared view of the computational foundations of intelligence that promotes valuable cross-disciplinary exchanges on questions, methods, and results. We chart advances over the past several decades that address challenges of perception and action under uncertainty through the lens of computation. Advances include the development of representations and inferential procedures for large-scale probabilistic inference and machinery for enabling reflection and decisions about tradeoffs in effort, precision, and timeliness of computations. These tools are deployed toward the goal of computational rationality: identifying decisions with highest expected utility, while taking into consideration the costs of computation in complex real-world problems in which most relevant calculations can only be approximated. We highlight key concepts with examples that show the potential for interchange between computer science, cognitive science, and neuroscience.",project-academic
10.1007/S10115-015-0830-Y,2016-02-01,a,Springer London,a survey on indexing techniques for big data taxonomy and performance evaluation," The explosive growth in volume, velocity, and diversity of data produced by mobile devices and cloud applications has contributed to the abundance of data or `big data.' Available solutions for efficient data storage and management cannot fulfill the needs of such heterogeneous data where the amount of data is continuously increasing. For efficient retrieval and management, existing indexing solutions become inefficient with the rapidly growing index size and seek time and an optimized index scheme is required for big data. Regarding real-world applications, the indexing issue with big data in cloud computing is widespread in healthcare, enterprises, scientific experiments, and social networks. To date, diverse soft computing, machine learning, and other techniques in terms of artificial intelligence have been utilized to satisfy the indexing requirements, yet in the literature, there is no reported state-of-the-art survey investigating the performance and consequences of techniques for solving indexing in big data issues as they enter cloud computing. The objective of this paper is to investigate and examine the existing indexing techniques for big data. Taxonomy of indexing techniques is developed to provide insight to enable researchers understand and select a technique as a basis to design an indexing mechanism with reduced time and space consumption for BD-MCC. In this study, 48 indexing techniques have been studied and compared based on 60 articles related to the topic. The indexing techniques' performance is analyzed based on their characteristics and big data indexing requirements. The main contribution of this study is taxonomy of categorized indexing techniques based on their method. The categories are non-artificial intelligence, artificial intelligence, and collaborative artificial intelligence indexing methods. In addition, the significance of different procedures and performance is analyzed, besides limitations of each technique. In conclusion, several key future research topics with potential to accelerate the progress and deployment of artificial intelligence-based cooperative indexing in BD-MCC are elaborated on.",project-academic
,2020-07-07,a,,a vision based social distancing and critical density detection system for covid 19," Social distancing has been proven as an effective measure against the spread of the infectious COronaVIrus Disease 2019 (COVID-19). However, individuals are not used to tracking the required 6-feet (2-meters) distance between themselves and their surroundings. An active surveillance system capable of detecting distances between individuals and warning them can slow down the spread of the deadly disease. Furthermore, measuring social density in a region of interest (ROI) and modulating inflow can decrease social distancing violation occurrence chance. 
On the other hand, recording data and labeling individuals who do not follow the measures will breach individuals' rights in free-societies. Here we propose an Artificial Intelligence (AI) based real-time social distancing detection and warning system considering four important ethical factors: (1) the system should never record/cache data, (2) the warnings should not target the individuals, (3) no human supervisor should be in the detection/warning loop, and (4) the code should be open-source and accessible to the public. Against this backdrop, we propose using a monocular camera and deep learning-based real-time object detectors to measure social distancing. If a violation is detected, a non-intrusive audio-visual warning signal is emitted without targeting the individual who breached the social distancing measure. Also, if the social density is over a critical value, the system sends a control signal to modulate inflow into the ROI. We tested the proposed method across real-world datasets to measure its generality and performance. The proposed method is ready for deployment, and our code is open-sourced.",project-academic
10.2139/SSRN.3547322,2020-01-01,a,,politics of adversarial machine learning," In addition to their security properties, adversarial machine-learning attacks and defenses have political dimensions. They enable or foreclose certain options for both the subjects of the machine learning systems and for those who deploy them, creating risks for civil liberties and human rights. In this paper, we draw on insights from science and technology studies, anthropology, and human rights literature, to inform how defenses against adversarial attacks can be used to suppress dissent and limit attempts to investigate machine learning systems. To make this concrete, we use real-world examples of how attacks such as perturbation, model inversion, or membership inference can be used for socially desirable ends. Although the predictions of this analysis may seem dire, there is hope. Efforts to address human rights concerns in the commercial spyware industry provide guidance for similar measures to ensure ML systems serve democratic, not authoritarian ends.",project-academic
,2020-02-01,a,,politics of adversarial machine learning," In addition to their security properties, adversarial machine-learning attacks and defenses have political dimensions. They enable or foreclose certain options for both the subjects of the machine learning systems and for those who deploy them, creating risks for civil liberties and human rights. In this paper, we draw on insights from science and technology studies, anthropology, and human rights literature, to inform how defenses against adversarial attacks can be used to suppress dissent and limit attempts to investigate machine learning systems. To make this concrete, we use real-world examples of how attacks such as perturbation, model inversion, or membership inference can be used for socially desirable ends. Although the predictions of this analysis may seem dire, there is hope. Efforts to address human rights concerns in the commercial spyware industry provide guidance for similar measures to ensure ML systems serve democratic, not authoritarian ends",project-academic
10.1126/SCIROBOTICS.AAZ9239,2020-04-22,a,Science Robotics,electronic skins and machine learning for intelligent soft robots," Soft robots have garnered interest for real-world applications because of their intrinsic safety embedded at the material level. These robots use deformable materials capable of shape and behavioral changes and allow conformable physical contact for manipulation. Yet, with the introduction of soft and stretchable materials to robotic systems comes a myriad of challenges for sensor integration, including multimodal sensing capable of stretching, embedment of high-resolution but large-area sensor arrays, and sensor fusion with an increasing volume of data. This Review explores the emerging confluence of e-skins and machine learning, with a focus on how roboticists can combine recent developments from the two fields to build autonomous, deployable soft robots, integrated with capabilities for informative touch and proprioception to stand up to the challenges of real-world environments.",project-academic
10.1109/ECRIME.2012.6489521,2012-10-01,p,IEEE,phishari automatic realtime phishing detection on twitter," With the advent of online social media, phishers have started using social networks like Twitter, Facebook, and Foursquare to spread phishing scams. Twitter is an immensely popular micro-blogging network where people post short messages of 140 characters called tweets. It has over 100 million active users who post about 200 million tweets everyday. Phishers have started using Twitter as a medium to spread phishing because of this vast information dissemination. Further, it is difficult to detect phishing on Twitter unlike emails because of the quick spread of phishing links in the network, short size of the content, and use of URL obfuscation to shorten the URL. Our technique, PhishAri, detects phishing on Twitter in realtime. We use Twitter specific features along with URL features to detect whether a tweet posted with a URL is phishing or not. Some of the Twitter specific features we use are tweet content and its characteristics like length, hashtags, and mentions. Other Twitter features used are the characteristics of the Twitter user posting the tweet such as age of the account, number of tweets, and the follower-followee ratio. These twitter specific features coupled with URL based features prove to be a strong mechanism to detect phishing tweets. We use machine learning classification techniques and detect phishing tweets with an accuracy of 92.52%. We have deployed our system for end-users by providing an easy to use Chrome browser extension. The extension works in realtime and classifies a tweet as phishing or safe. In this research, we show that we are able to detect phishing tweets at zero hour with high accuracy which is much faster than public blacklists and as well as Twitter's own defense mechanism to detect malicious content. We also performed a quick user evaluation of PhishAri in a laboratory study to evaluate the usability and effectiveness of PhishAri and showed that users like and find it convenient to use PhishAri in real-world. To the best of our knowledge, this is the first realtime, comprehensive and usable system to detect phishing on Twitter.",project-academic
10.1109/TASE.2017.2731371,2017-08-16,a,IEEE,toward socially aware robot navigation in dynamic and crowded environments a proactive social motion model," Safe and social navigation is the key to deploying a mobile service robot in a human-centered environment. Widespread acceptability of mobile service robots in daily life is hindered by robot’s inability to navigate in crowded and dynamic human environments in a socially acceptable way that would guarantee human safety and comfort. In this paper, we propose an effective proactive social motion model (PSMM) that enables a mobile service robot to navigate safely and socially in crowded and dynamic environments. The proposed method considers not only human states (position, orientation, motion, field of view, and hand poses) relative to the robot but also social interactive information about human–object and human group interactions. This allows development of the PSMM that consists of elements of an extended social force model and a hybrid reciprocal velocity obstacle technique. The PSMM is then combined with a path planning technique to generate a motion planning system that drives a mobile robot in a socially acceptable manner and produces respectful and polite behaviors akin to human movements. None Note to Practitioners —In this paper, we validated the effectiveness and feasibility of the proposed proactive social motion model (PSMM) through both simulation and real-world experiments under the newly proposed human comfortable safety indices. To do that, we first implemented the entire navigation system using the open-source robot operating system. We then installed it in a simulated robot model and conducted experiments in a simulated shopping mall-like environment to verify its effectiveness. We also installed the proposed algorithm on our mobile robot platform and conducted experiments in our office-like laboratory environment. Our results show that the developed socially aware navigation framework allows a mobile robot to navigate safely, socially, and proactively while guaranteeing human safety and comfort in crowded and dynamic environments. In this paper, we examined the proposed PSMM with a set of predefined parameters selected based on our empirical experiences about the robot mechanism and selected social environment. However, in fact a mobile robot might need to adapt to various contextual and cultural situations in different social environments. Thus, it should be equipped with an online adaptive interactive learning mechanism allowing the robot to learn to auto-adjust their parameters according to such embedded environments. Using machine learning techniques, e.g., inverse reinforcement learning None [1] None to optimize the parameter set for the PSMM could be a promising research direction to improve adaptability of mobile service robots in different social environments. In the future, we will evaluate the proposed framework based on a wider variety of scenarios, particularly those with different social interaction situations and dynamic environments. Furthermore, various kinds of social cues and signals introduced in None [2] None and None [3] None will be applied to extend the proposed framework in more complicated social situations and contexts. Last but not least, we will investigate different machine learning techniques and incorporate them in the PSMM in order to allow the robot to automatically adapt to diverse social environments.",project-academic
,2020-10-16,a,,robot navigation in constrained pedestrian environments using reinforcement learning," Navigating fluently around pedestrians is a necessary capability for mobile robots deployed in human environments, such as buildings and homes. While research on social navigation has focused mainly on the scalability with the number of pedestrians in open spaces, typical indoor environments present the additional challenge of constrained spaces such as corridors and doorways that limit maneuverability and influence patterns of pedestrian interaction. We present an approach based on reinforcement learning (RL) to learn policies capable of dynamic adaptation to the presence of moving pedestrians while navigating between desired locations in constrained environments. The policy network receives guidance from a motion planner that provides waypoints to follow a globally planned trajectory, whereas RL handles the local interactions. We explore a compositional principle for multi-layout training and find that policies trained in a small set of geometrically simple layouts successfully generalize to more complex unseen layouts that exhibit composition of the structural elements available during training. Going beyond walls-world like domains, we show transfer of the learned policy to unseen 3D reconstructions of two real environments. These results support the applicability of the compositional principle to navigation in real-world buildings and indicate promising usage of multi-agent simulation within reconstructed environments for tasks that involve interaction.",project-academic
10.1109/ICRA48506.2021.9560893,2021-05-30,p,IEEE,robot navigation in constrained pedestrian environments using reinforcement learning," Navigating fluently around pedestrians is a necessary capability for mobile robots deployed in human environments, such as buildings and homes. While research on social navigation has focused mainly on the scalability with the number of pedestrians in open spaces, typical indoor environments present the additional challenge of constrained spaces such as corridors and doorways that limit maneuverability and influence patterns of pedestrian interaction. We present an approach based on reinforcement learning (RL) to learn policies capable of dynamic adaptation to the presence of moving pedestrians while navigating between desired locations in constrained environments. The policy network receives guidance from a motion planner that provides waypoints to follow a globally planned trajectory, whereas RL handles the local interactions. We explore a compositional principle for multi-layout training and find that policies trained in a small set of geometrically simple layouts successfully generalize to more complex unseen layouts that exhibit composition of the structural elements available during training. Going beyond walls-world like domains, we show transfer of the learned policy to unseen 3D reconstructions of two real environments. These results support the applicability of the compositional principle to navigation in real-world buildings and indicate promising usage of multi-agent simulation within reconstructed environments for tasks that involve interaction. https://ai.stanford.edu/∼cdarpino/socialnavconstrained/",project-academic
,2021-06-24,a,,towards understanding and mitigating social biases in language models," As machine learning methods are deployed in real-world settings such as healthcare, legal systems, and social science, it is crucial to recognize how they shape social biases and stereotypes in these sensitive decision-making processes. Among such real-world deployments are large-scale pretrained language models (LMs) that can be potentially dangerous in manifesting undesirable representational biases - harmful biases resulting from stereotyping that propagate negative generalizations involving gender, race, religion, and other social constructs. As a step towards improving the fairness of LMs, we carefully define several sources of representational biases before proposing new benchmarks and metrics to measure them. With these tools, we propose steps towards mitigating social biases during text generation. Our empirical results and human evaluation demonstrate effectiveness in mitigating bias while retaining crucial contextual information for high-fidelity text generation, thereby pushing forward the performance-fairness Pareto frontier.",project-academic
,2020-07-07,a,,human trajectory forecasting in crowds a deep learning perspective," Since the past few decades, human trajectory forecasting has been a field of active research owing to its numerous real-world applications: evacuation situation analysis, traffic operations, deployment of social robots in crowded environments, to name a few. In this work, we cast the problem of human trajectory forecasting as learning a representation of human social interactions. Early works handcrafted this representation based on domain knowledge. However, social interactions in crowded environments are not only diverse but often subtle. Recently, deep learning methods have outperformed their handcrafted counterparts, as they learned about human-human interactions in a more generic data-driven fashion. In this work, we present an in-depth analysis of existing deep learning based methods for modelling social interactions. Based on our analysis, we propose a simple yet powerful method for effectively capturing these social interactions. To objectively compare the performance of these interaction-based forecasting models, we develop a large scale interaction-centric benchmark TrajNet++, a significant yet missing component in the field of human trajectory forecasting. We propose novel performance metrics that evaluate the ability of a model to output socially acceptable trajectories. Experiments on TrajNet++ validate the need for our proposed metrics, and our method outperforms competitive baselines on both real-world and synthetic datasets.",project-academic
10.1109/TITS.2021.3069362,2021-04-19,a,IEEE,human trajectory forecasting in crowds a deep learning perspective," Since the past few decades, human trajectory forecasting has been a field of active research owing to its numerous real-world applications: evacuation situation analysis, deployment of intelligent transport systems, traffic operations, to name a few. In this work, we cast the problem of human trajectory forecasting as learning a representation of human social interactions. Early works handcrafted this representation based on domain knowledge. However, social interactions in crowded environments are not only diverse but often subtle. Recently, deep learning methods have outperformed their handcrafted counterparts, as they learn about human-human interactions in a more generic data-driven fashion. In this work, we present an in-depth analysis of existing deep learning-based methods for modelling social interactions. We propose two domain-knowledge inspired data-driven methods to effectively capture these social interactions. To objectively compare the performance of these interaction-based forecasting models, we develop a large scale interaction-centric benchmark TrajNet++, a significant yet missing component in the field of human trajectory forecasting. We propose novel performance metrics that evaluate the ability of a model to output socially acceptable trajectories. Experiments on TrajNet++ validate the need for our proposed metrics, and our method outperforms competitive baselines on both real-world and synthetic datasets.",project-academic
10.1109/ICDM50108.2020.00084,2020-11-01,p,IEEE,autonomous graph mining algorithm search with best speed accuracy trade off," Graph data is ubiquitous in academia and industry, from social networks to bioinformatics. The pervasiveness of graphs today has raised the demand for algorithms that can answer various questions: Which products would a user like to purchase given her order list? Which users are buying fake followers to increase their public reputation? Myriads of new graph mining algorithms are proposed every year to answer such questions — each with a distinct problem formulation, computational time, and memory footprint. This lack of unity makes it difficult for a practitioner to compare different algorithms and pick the most suitable one for a specific application. These challenges — even more severe for non-experts — create a gap in which state-of-the-art techniques developed in academic settings fail to be optimally deployed in real-world applications. To bridge this gap, we propose AutoGM, an automated system for graph mining algorithm development. We first define a unified framework UnifiedGM that integrates various message-passing based graph algorithms, ranging from conventional algorithms like PageRank to graph neural networks. Then UnifiedGM defines a search space in which five parameters are required to determine a graph algorithm. Under this search space, AutoGM explicitly optimizes for the optimal parameter set of UnifiedGM using Bayesian Optimization. AutoGM defines a novel budget-aware objective function for the optimization to incorporate a practical issue — finding the best speed-accuracy trade-off under a computation budget - into the graph algorithm generation problem. Experiments on real-world benchmark datasets demonstrate that AutoGM generates novel graph mining algorithms with the best speed/accuracy trade-off compared to existing models with heuristic parameters.",project-academic
,2020-11-26,a,,autonomous graph mining algorithm search with best speed accuracy trade off," Graph data is ubiquitous in academia and industry, from social networks to bioinformatics. The pervasiveness of graphs today has raised the demand for algorithms that can answer various questions: Which products would a user like to purchase given her order list? Which users are buying fake followers to increase their public reputation? Myriads of new graph mining algorithms are proposed every year to answer such questions - each with a distinct problem formulation, computational time, and memory footprint. This lack of unity makes it difficult for a practitioner to compare different algorithms and pick the most suitable one for a specific application. These challenges - even more severe for non-experts - create a gap in which state-of-the-art techniques developed in academic settings fail to be optimally deployed in real-world applications. To bridge this gap, we propose AUTOGM, an automated system for graph mining algorithm development. We first define a unified framework UNIFIEDGM that integrates various message-passing based graph algorithms, ranging from conventional algorithms like PageRank to graph neural networks. Then UNIFIEDGM defines a search space in which five parameters are required to determine a graph algorithm. Under this search space, AUTOGM explicitly optimizes for the optimal parameter set of UNIFIEDGM using Bayesian Optimization. AUTOGM defines a novel budget-aware objective function for the optimization to incorporate a practical issue - finding the best speed-accuracy trade-off under a computation budget - into the graph algorithm generation problem. Experiments on real-world benchmark datasets demonstrate that AUTOGM generates novel graph mining algorithms with the best speed/accuracy trade-off compared to existing models with heuristic parameters.",project-academic
10.1109/TKDE.2018.2854193,2019-06-01,a,IEEE,deepclue visual interpretation of text based deep stock prediction," The recent advance of deep learning has enabled trading algorithms to predict stock price movements more accurately. Unfortunately, there is a significant gap in the real-world deployment of this breakthrough. For example, professional traders in their long-term careers have accumulated numerous trading rules, the myth of which they can understand quite well. On the other hand, deep learning models have been hardly interpretable. This paper presents DeepClue, a system built to bridge text-based deep learning models and end users through visually interpreting the key factors learned in the stock price prediction model. We make three contributions in DeepClue. First, by designing the deep neural network architecture for interpretation and applying an algorithm to extract relevant predictive factors, we provide a useful case on what can be interpreted out of the prediction model for end users. Second, by exploring hierarchies over the extracted factors and displaying these factors in an interactive, hierarchical visualization interface, we shed light on how to effectively communicate the interpreted model to end users. Specially, the interpretation separates the predictables from the unpredictables for stock prediction through the use of intercept model parameters and a risk visualization design. Third, we evaluate the integrated visualization system through two case studies in predicting the stock price with financial news and company-related tweets from social media. Quantitative experiments comparing the proposed neural network architecture with state-of-the-art models and the human baseline are conducted and reported. Feedbacks from an informal user study with domain experts are summarized and discussed in details. The study results demonstrate the effectiveness of DeepClue in helping to complete stock market investment and analysis tasks.",project-academic
10.1016/J.IJROBP.2021.07.167,2021-11-01,a,Elsevier BV,clinical validation of deep learning algorithms for lung cancer radiotherapy targeting," PURPOSE/OBJECTIVE(S) Automated target segmentation for non-small cell lung cancer (NSCLC) patients has the potential to support radiation treatment planning. Artificial intelligence (AI) has demonstrated great promise in medical image segmentation tasks. However, most studies have been confined to in silico validation in small internal cohorts, lacking data on real-world clinical utility. In this study, we developed primary tumor and involved lymph node segmentation algorithms in computed tomography (CT) images. Validation is performed in multiple large multi-institutional cohorts to assess model generalizability. MATERIALS/METHODS Simulation CTs and ground truth annotations were collected from multiple public and private sources (total n = 2584). We employed the following benchmarks: Inter-observer (6 radiation oncologists, n = 20, median volumetric dice 0.83, 95% CI 0.82-0.84) and intra-observer (1 radiation oncologist, 3 reads, n = 21, median volumetric dice 0.88, 95% CI 0.84-0.9). We developed two segmentation algorithms: seed-point assisted and fully automated. Model training data (n = 787) comprised NSCLC-Radiomics (stages I-IIIB, n = 422) and LungRT-1 (stages IA-IV, n = 365). Validation was first performed in an internal dataset annotated by a single thoracic radiation oncologist (LungRT-1, n = 136). Additional validation included: (1) an internal dataset annotated by other radiation oncologists, including generalists, in our center (LungRT-2, n = 1075), (2) an external clinical trial dataset from 185 different institutions (RTOG-0617, n = 403), and (3) a dataset of early-stage surgical patients annotated for diagnostic purposes by radiologists (NSCLC-Radiogenomics, n = 142). Volumetric dice, using expert manual segmentations as ground truth, was used as an evaluation metric. RESULTS The model performance is comparable to the benchmarks when validated on internal data, with degrading performance in cohorts annotated by other radiation oncologists. CONCLUSION The results highlight the importance of assessing segmentation style among annotators and understanding model generalizability in external cohorts, all while cautioning against degrading performance in increasingly external data. Differences between radiologists and radiation oncologists performing the same segmentation task underscore the importance of clinical context in AI model deployment. Further validation includes studying the dosimetric impact of AI-generated segmentations, and conducting human subject experiments to assess AI output acceptance and time savings.",project-academic
10.1145/3110025.3110164,2017-07-31,p,ACM,image4act online social media image processing for disaster response," We present an end-to-end social media image processing system called Image4Act. The system aims at collecting, denoising, and classifying imagery content posted on social media platforms to help humanitarian organizations in gaining situational awareness and launching relief operations. It combines human computation and machine learning techniques to process high-volume social media imagery content in real time during natural and human-made disasters. To cope with the noisy nature of the social media imagery data, we use a deep neural network and perceptual hashing techniques to filter out irrelevant and duplicate images. Furthermore, we present a specific use case to assess the severity of infrastructure damage incurred by a disaster. The evaluations of the system on existing disaster datasets as well as a real-world deployment during a recent cyclone prove the effectiveness of the system.",project-academic
10.1145/3292500.3330654,2019-07-25,p,ACM,deepurbanevent a system for predicting citywide crowd dynamics at big events," Event crowd management has been a significant research topic with high social impact. When some big events happen such as an earthquake, typhoon, and national festival, crowd management becomes the first priority for governments (e.g. police) and public service operators (e.g. subway/bus operator) to protect people's safety or maintain the operation of public infrastructures. However, under such event situations, human behavior will become very different from daily routines, which makes prediction of crowd dynamics at big events become highly challenging, especially at a citywide level. Therefore in this study, we aim to extract the deep trend only from the current momentary observations and generate an accurate prediction for the trend in the short future, which is considered to be an effective way to deal with the event situations. Motivated by these, we build an online system called DeepUrbanEvent which can iteratively take citywide crowd dynamics from the current one hour as input and report the prediction results for the next one hour as output. A novel deep learning architecture built with recurrent neural networks is designed to effectively model these highly-complex sequential data in an analogous manner to video prediction tasks. Experimental results demonstrate the superior performance of our proposed methodology to the existing approaches. Lastly, we apply our prototype system to multiple big real-world events and show that it is highly deployable as an online crowd management system.",project-academic
10.1557/MRC.2019.73,2019-09-01,a,Springer International Publishing,deep materials informatics applications of deep learning in materials science," The growing application of data-driven analytics in materials science has led to the rise of materials informatics. Within the arena of data analytics, deep learning has emerged as a game-changing technique in the last few years, enabling numerous real-world applications, such as self-driving cars. In this paper, the authors present an overview of deep learning, its advantages, challenges, and recent applications on different types of materials data. The increasingly availability of materials databases and big data in general, along with groundbreaking advances in deep learning offers a lot of promise to accelerate the discovery, design, and deployment of next-generation materials.",project-academic
10.1016/J.COSE.2016.05.005,2016-08-01,a,Elsevier Advanced Technology,integro leveraging victim prediction for robust fake account detection in large scale osns," Abstract None None Detecting fake accounts in online social networks (OSNs) protects both OSN operators and their users from various malicious activities. Most detection mechanisms attempt to classify user accounts as real (i.e., benign, honest) or fake (i.e., malicious, Sybil) by analyzing either user-level activities or graph-level structures. These mechanisms, however, are not robust against adversarial attacks in which fake accounts cloak their operation with patterns resembling real user behavior. None In this article, we show that victims – real accounts whose users have accepted friend requests sent by fakes – form a distinct classification category that is useful for designing robust detection mechanisms. In particular, we present Integro – a robust and scalable defense system that leverages victim classification to rank most real accounts higher than fakes, so that OSN operators can take actions against low-ranking fake accounts. Integro starts by identifying potential victims from user-level activities using supervised machine learning. After that, it annotates the graph by assigning lower weights to edges incident to potential victims. Finally, Integro ranks user accounts based on the landing probability of a short random walk that starts from a known real account. As this walk is unlikely to traverse low-weight edges in a few steps and land on fakes, Integro achieves the desired ranking. None We implemented Integro using widely-used, open-source distributed computing platforms, where it scaled nearly linearly. We evaluated Integro against SybilRank, which is the state-of-the-art in fake account detection, using real-world datasets and a large-scale deployment at Tuenti – the largest OSN in Spain with more than 15 million active users. We show that Integro significantly outperforms SybilRank in user ranking quality, with the only requirement that the employed victim classifier is better than random. Moreover, the deployment of Integro at Tuenti resulted in up to an order of magnitude higher precision in fake account detection, as compared to SybilRank.",project-academic
10.23919/ACC45564.2020.9147321,2020-07-01,p,IEEE,fast adaptation of thermal dynamics model for predictive control of hvac and natural ventilation using transfer learning with deep neural networks," Smart buildings and building automation are key components for achieving greater energy efficiency. In order to implement predictive control for the HVAC system and natural ventilation, the model needs to have the capability to predict building thermal responses under various environmental and operational conditions. This task can be accomplished by using a deep neural network, which would capture the effects of complicated physical processes, such as natural ventilation. However, a deep neural network comes with a high demand for training data. In real-world applications, the target building may not have the necessary amount of operational data available. This study demonstrates how transfer learning could solve this dilemma. By freezing partial weights of a deep neural network model that is pretrained on multi-year data from a base case building, it can be quickly deployed to different buildings in other climates. With much fewer trainable parameters, the model can then be well-trained on only 15 days of data from the new target building. The base case and target case can have entirely different floor areas, building materials, and window sizes. This transfer learning model performs significantly better than a comparable model that is only trained on source data or target data, achieving high predicting accuracy on both indoor air temperature and relative humidity in the time horizon from 10 minutes to two hours. This methodology can be applied to the design of the control system in a new building. It has a high sample efficiency and shortens the minimum data collection period for model training.",project-academic
10.2196/19297,2020-05-06,a,JMIR Publications Inc.,agile requirements engineering and software planning for a digital health platform to engage the effects of isolation caused by social distancing case study," Background: Social distancing and shielding measures have been put in place to reduce social interaction and slow the transmission of the coronavirus disease (COVID-19). For older people, self-isolation presents particular challenges for mental health and social relationships. As time progresses, continued social distancing could have a compounding impact on these concerns.
Objective: This project aims to provide a tool for older people and their families and peers to improve their well-being and health during and after regulated social distancing. First, we will evaluate the tool’s feasibility, acceptability, and usability to encourage positive nutrition, enhance physical activity, and enable virtual interaction while social distancing. Second, we will be implementing the app to provide an online community to assist families and peer groups in maintaining contact with older people using goal setting. Anonymized data from the app will be aggregated with other real-world data sources to develop a machine learning algorithm to improve the identification of patients with COVID-19 and track for real time use by health systems.
Methods: Development of this project is occurring at the time of publication, and therefore, a case study design was selected to provide a systematic means of capturing software engineering in progress. The app development framework for software design was based on agile methods. The evaluation of the app’s feasibility, acceptability and usability shall be conducted using Public Health England's guidance on evaluating digital health products, Bandura’s model of health promotion, the Reach Effectiveness Adoption Implementation Maintenance (RE-AIM) framework and the Nonadoption, Abandonment and Challenges to the Scale-up, Spread and Suitability (NASSS) framework.
Results: Making use of a pre-existing software framework for health behavior change, a proof of concept was developed, and a multistage app development and deployment for the solution was created. Grant submissions to fund the project and study execution have been sought at the time of publication, and prediscovery iteration of the solution has begun. Ethical approval for a feasibility study design is being sought.
Conclusions: This case study lays the foundations for future app development to combat mental and societal issues arising from social distancing measures. The app will be tested and evaluated in future studies to allow continuous improvement of the app. This novel contribution will provide an evidence-based exemplar for future app development in the space of social isolation and loneliness.",project-academic
10.1109/ITSC.2017.8317730,2017-10-01,p,IEEE,intelligent traffic light control using distributed multi agent q learning," The combination of Artificial Intelligence (AI) and Internet-of-Things (IoT), which is denoted as AI powered Internet-of-Things (AIoT), is capable of processing huge amount of data generated from large number of devices and handling complex problems in social infrastructures. As AI and IoT technologies are becoming mature, in this paper, we propose to apply AIoT technologies for traffic light control, which is an essential component for intelligent transportation system, to improve the efficiency of smart city's road system. Specifically, various sensors such as surveillance cameras provide real-time information for intelligent traffic light control system to observe the states of both motorized traffic and non-motorized traffic. In this paper, we propose an intelligent traffic light control solution by using distributed multi-agent Q learning, considering the traffic information at the neighboring intersections as well as local motorized and non-motorized traffic, to improve the overall performance of the entire control system. By using the proposed multi-agent Q learning algorithm, our solution is targeting to optimize both the motorized and non-motorized traffic. In addition, we considered many constraints / rules for traffic light control in the real world, and integrate these constraints in the learning algorithm, which can facilitate the proposed solution to be deployed in real operational scenarios. We conducted numerical simulations for a real-world map with real-world traffic data. The simulation results show that our proposed solution outperforms existing solutions in terms of vehicle and pedestrian queue lengths, waiting time at intersections, and many other key performance metrics.",project-academic
,2017-11-29,a,,intelligent traffic light control using distributed multi agent q learning," The combination of Artificial Intelligence (AI) and Internet-of-Things (IoT), which is denoted as AI-powered Internet-of-Things (AIoT), is capable of processing huge amount of data generated from a large number of devices and handling complex problems in social infrastructures. As AI and IoT technologies are becoming mature, in this paper, we propose to apply AIoT technologies for traffic light control, which is an essential component for intelligent transportation system, to improve the efficiency of smart city's road system. Specifically, various sensors such as surveillance cameras provide real-time information for intelligent traffic light control system to observe the states of both motorized traffic and non-motorized traffic. In this paper, we propose an intelligent traffic light control solution by using distributed multi-agent Q learning, considering the traffic information at the neighboring intersections as well as local motorized and non-motorized traffic, to improve the overall performance of the entire control system. By using the proposed multi-agent Q learning algorithm, our solution is targeting to optimize both the motorized and non-motorized traffic. In addition, we considered many constraints/rules for traffic light control in the real world, and integrate these constraints in the learning algorithm, which can facilitate the proposed solution to be deployed in real operational scenarios. We conducted numerical simulations for a real-world map with real-world traffic data. The simulation results show that our proposed solution outperforms existing solutions in terms of vehicle and pedestrian queue lengths, waiting time at intersections, and many other key performance metrics.",project-academic
10.1109/JIOT.2019.2963635,2020-01-01,a,IEEE,toward edge based deep learning in industrial internet of things," As a typical application of the Internet of Things (IoT), the Industrial IoT (IIoT) connects all the related IoT sensing and actuating devices ubiquitously so that the monitoring and control of numerous industrial systems can be realized. Deep learning, as one viable way to carry out big-data-driven modeling and analysis, could be integrated in IIoT systems to aid the automation and intelligence of IIoT systems. As deep learning requires large computation power, it is commonly deployed in cloud servers. Thus, the data collected by IoT devices must be transmitted to the cloud for training process, contributing to network congestion and affecting the IoT network performance as well as the supported applications. To address this issue, in this article, we leverage the fog/edge computing paradigm and propose an edge computing-based deep learning model, which utilizes edge computing to migrate the deep learning process from cloud servers to edge nodes, reducing data transmission demands in the IIoT network and mitigating network congestion. Since edge nodes have limited computation ability compared to servers, we design a mechanism to optimize the deep learning model so that its requirements for computational power can be reduced. To evaluate our proposed solution, we design a testbed implemented in the Google cloud and deploy the proposed convolutional neural network (CNN) model, utilizing a real-world IIoT data set to evaluate our approach. 1 None Our experimental results confirm the effectiveness of our approach, which cannot only reduce the network traffic overhead for IIoT but also maintain the classification accuracy in comparison with several baseline schemes. None 1 None Certain commercial equipment, instruments, or materials are identified in this article in order to specify the experimental procedure adequately. Such identification is not intended to imply recommendation or endorsement by the National Institute of Standards and Technology, nor is it intended to imply that the materials or equipment identified are necessarily the best available for the purpose.",project-academic
10.1016/J.SCS.2020.102582,2021-01-01,a,Elsevier BV,towards the sustainable development of smart cities through mass video surveillance a response to the covid 19 pandemic," Sustainable smart city initiatives around the world have recently had great impact on the lives of citizens and brought significant changes to society. More precisely, data-driven smart applications that efficiently manage sparse resources are offering a futuristic vision of smart, efficient, and secure city operations. However, the ongoing COVID-19 pandemic has revealed the limitations of existing smart city deployment; hence; the development of systems and architectures capable of providing fast and effective mechanisms to limit further spread of the virus has become paramount. An active surveillance system capable of monitoring and enforcing social distancing between people can effectively slow the spread of this deadly virus. In this paper, we propose a data-driven deep learning-based framework for the sustainable development of a smart city, offering a timely response to combat the COVID-19 pandemic through mass video surveillance. To implementing social distancing monitoring, we used three deep learning-based real-time object detection models for the detection of people in videos captured with a monocular camera. We validated the performance of our system using a real-world video surveillance dataset for effective deployment.",project-academic
,2021-07-13,a,,bayesian meta prior learning using empirical bayes," Adding domain knowledge to a learning system is known to improve results. In multi-parameter Bayesian frameworks, such knowledge is incorporated as a prior. On the other hand, various model parameters can have different learning rates in real-world problems, especially with skewed data. Two often-faced challenges in Operation Management and Management Science applications are the absence of informative priors, and the inability to control parameter learning rates. In this study, we propose a hierarchical Empirical Bayes approach that addresses both challenges, and that can generalize to any Bayesian framework. Our method learns empirical meta-priors from the data itself and uses them to decouple the learning rates of first-order and second-order features (or any other given feature grouping) in a Generalized Linear Model. As the first-order features are likely to have a more pronounced effect on the outcome, focusing on learning first-order weights first is likely to improve performance and convergence time. Our Empirical Bayes method clamps features in each group together and uses the deployed model's observed data to empirically compute a hierarchical prior in hindsight. We report theoretical results for the unbiasedness, strong consistency, and optimal frequentist cumulative regret properties of our meta-prior variance estimator. We apply our method to a standard supervised learning optimization problem, as well as an online combinatorial optimization problem in a contextual bandit setting implemented in an Amazon production system. Both during simulations and live experiments, our method shows marked improvements, especially in cases of small traffic. Our findings are promising, as optimizing over sparse data is often a challenge.",project-academic
10.1145/3329486.3329492,2019-06-30,p,ACM Press,osprey weak supervision of imbalanced extraction problems without code," Supervised methods are commonly used for machine-learning based applications but require expensive labeled dataset creation and maintenance. Increasingly, practitioners employ weak supervision approaches, where training labels are pro-grammatically generated in higher-level but noisier ways. However, these approaches require domain experts with programming skills. Additionally, highly imbalanced data is often a significant practical challenge for these approaches. In this work, we propose Osprey, a weak-supervision system suited for highly imbalanced data, built on top of the Snorkel framework. In order to support non-coders, the programmatic labeling is decoupled into a code layer and a configuration one. This decoupling enables a rapid development of end-to-end systems by encoding the business logic into the configuration layer. We apply the resulting system on highly imbalanced (0.05% positive) social-media data using a synthetic data rebalancing and augmentation approach, and a novel technique of ensembling a generative model over the legacy rules with a learned discriminative model. We demonstrate how an existing rule-based model can be transformed easily into a weakly-supervised one. For 3 relation extraction applications based on real-world deployments at Intel, we show that with a fraction of the cost, we achieve gains of 18.5 precision points and 28.5 coverage points over prior traditionally supervised and rule-based approaches.",project-academic
10.1109/SP.2019.00032,2019-05-19,p,IEEE,stealthy porn understanding real world adversarial images for illicit online promotion," Recent years have witnessed the rapid progress in deep learning (DP), which also brings their potential weaknesses to the spotlights of security and machine learning studies. With important discoveries made by adversarial learning research, surprisingly little attention, however, has been paid to the real-world adversarial techniques deployed by the cybercriminal to evade image-based detection. Unlike the adversarial examples that induce misclassification using nearly imperceivable perturbation, real-world adversarial images tend to be less optimal yet equally effective. As a first step to understand the threat, we report in the paper a study on adversarial promotional porn images (APPIs) that are extensively used in underground advertising. We show that the adversary today’s strategically constructs the APPIs to evade explicit content detection while still preserving their sexual appeal, even though the distortions and noise introduced are clearly observable to humans. To understand such real-world adversarial images and the underground business behind them, we develop a novel DP-based methodology called Male`na, which focuses on the regions of an image where sexual content is least obfuscated and therefore visible to the target audience of a promotion. Using this technique, we have discovered over 4,000 APPIs from 4,042,690 images crawled from popular social media, and further brought to light the unique techniques they use to evade popular explicit content detectors (e.g., Google Cloud Vision API, Yahoo Open NSFW model), and the reason that these techniques work. Also studied are the ecosystem of such illicit promotions, including the obfuscated contacts advertised through those images, compromised accounts used to disseminate them, and large APPI campaigns involving thousands of images. Another interesting finding is the apparent attempt made by cybercriminals to steal others’ images for their advertising. The study highlights the importance of the research on real-world adversarial learning and makes the first step towards mitigating the threats it poses.",project-academic
,2010-01-05,b,,computational intelligence paradigms theory applications using matlab," Offering a wide range of programming examples implemented in MATLAB, Computational Intelligence Paradigms: Theory and Applications Using MATLAB presents theoretical concepts and a general framework for computational intelligence (CI) approaches, including artificial neural networks, fuzzy systems, evolutionary computation, genetic algorithms and programming, and swarm intelligence. It covers numerous intelligent computing methodologies and algorithms used in CI research. The book first focuses on neural networks, including common artificial neural networks; neural networks based on data classification, data association, and data conceptualization; and real-world applications of neural networks. It then discusses fuzzy sets, fuzzy rules, applications of fuzzy systems, and different types of fused neuro-fuzzy systems, before providing MATLAB illustrations of ANFIS, classification and regression trees, fuzzy c-means clustering algorithms, fuzzy ART map, and TakagiSugeno inference systems. The authors also describe the history, advantages, and disadvantages of evolutionary computation and include solved MATLAB programs to illustrate the implementation of evolutionary computation in various problems. After exploring the operators and parameters of genetic algorithms, they cover the steps and MATLAB routines of genetic programming. The final chapter introduces swarm intelligence and its applications, particle swarm optimization, and ant colony optimization. Full of worked examples and end-of-chapter questions, this comprehensive book explains how to use MATLAB to implement CI techniques for the solution of biological problems. It will help readers with their work on evolution dynamics, self-organization, natural and artificial morphogenesis, emergent collective behaviors, swarm intelligence, evolutionary strategies, genetic programming, and the evolution of social behaviors.",project-academic
10.1109/ICMLA.2015.152,2015-12-01,p,IEEE,mlaas machine learning as a service," The demand for knowledge extraction has been increasing. With the growing amount of data being generated by global data sources (e.g., social media and mobile apps) and the popularization of context-specific data (e.g., the Internet of Things), companies and researchers need to connect all these data and extract valuable information. Machine learning has been gaining much attention in data mining, leveraging the birth of new solutions. This paper proposes an architecture to create a flexible and scalable machine learning as a service. An open source solution was implemented and presented. As a case study, a forecast of electricity demand was generated using real-world sensor and weather data by running different algorithms at the same time.",project-academic
,2011-04-28,b,,semantic web services," A paradigm shift is taking place in computer science: one generation ago, we learned to abstract from hardware to software, now we are abstracting from software to serviceware implemented through service-oriented computing. Yet ensuring interoperability in open, heterogeneous, and dynamically changing environments, such as the Internet, remains a major challenge for actual machine-to-machine integration. Usually significant problems in aligning data, processes, and protocols appear as soon as a specific piece of functionality is used within a different application context. The Semantic Web Services (SWS) approach is about describing services with metadata on the basis of domain ontologies as a means to enable their automatic location, execution, combination, and use. Fensel and his coauthors provide a comprehensive overview of SWS in line with actual industrial practice. They introduce the main sociotechnological components that ground the SWS vision (like Web Science, Service Science, and service-oriented architectures) and several approaches that realize it, e.g. the Web Service Modeling Framework, OWL-S, and RESTful services. The real-world relevance is emphasized through a series of case studies from large-scale R&D projects and a business-oriented proposition from the SWS technology provider Seekda.Each chapter of the book is structured according to a predefined template, covering both theoretical and practical aspects, and including walk-through examples and hands-on exercises. Additional learning material is available on the book website www.swsbook.org. With its additional features, the book is ideally suited as the basis for courses or self-study in this field, and it may also serve as a reference for researchers looking for a state-of-the-art overview of formalisms, methods, tools, and applications related to SWS.",project-academic
10.1001/JAMAPSYCHIATRY.2018.2530,2018-12-01,a,American Medical Association,the science of prognosis in psychiatry a review," Importance None Prognosis is a venerable component of medical knowledge introduced by Hippocrates (460-377 BC). This educational review presents a contemporary evidence-based approach for how to incorporate clinical risk prediction models in modern psychiatry. The article is organized around key methodological themes most relevant for the science of prognosis in psychiatry. Within each theme, the article highlights key challenges and makes pragmatic recommendations to improve scientific understanding of prognosis in psychiatry. None Observations None The initial step to building clinical risk prediction models that can affect psychiatric care involves designing the model: preparation of the protocol and definition of the outcomes and of the statistical methods (theme 1). Further initial steps involve carefully selecting the predictors, preparing the data, and developing the model in these data. A subsequent step is the validation of the model to accurately test its generalizability (theme 2). The next consideration is that the accuracy of the clinical prediction model is affected by the incidence of the psychiatric condition under investigation (theme 3). Eventually, clinical prediction models need to be implemented in real-world clinical routine, and this is usually the most challenging step (theme 4). Advanced methods such as machine learning approaches can overcome some problems that undermine the previous steps (theme 5). The relevance of each of these themes to current clinical risk prediction modeling in psychiatry is discussed and recommendations are given. None Conclusions and Relevance None Together, these perspectives intend to contribute to an integrative, evidence-based science of prognosis in psychiatry. By focusing on the outcome of the individuals, rather than on the disease, clinical risk prediction modeling can become the cornerstone for a scientific and personalized psychiatry.",project-academic
,2001-01-01,b,,computer vision and fuzzy neural systems," From the Publisher:

New computer vision techniques based on neural networks, fuzzy inference systems, and fuzzy-neural network models
Detailed tutorials, hands-on exercises, real-world examples, and proven algorithms 


CD-ROM: code libraries for the MATLAB neural network, fuzzy logic, and image processing toolboxes, test images from Kodak and Space Imaging, and more. 

The first complete guide to applying fuzzy-neural systems in computer vision. 

Recent advances in neural networks and fuzzy logic are transforming the field of computer vision, making it possible for computer vision applications to learn much as the brain does, and to handle imprecise visual data far more effectively. Now, Dr. Arun D. Kulkarni brings together the field's latest research and applications, presenting the field's first comprehensive tutorial and reference. 

Kulkarni starts by reviewing the fundamentals of computer vision, and the stages of a computer vision system. He shows how these stages have traditionally been implemented via statistical techniques; then introduces approaches that incorporate neural networks, fuzzy inference systems, and fuzzy-neural network models. Coverage includes: 

Preprocessing techniques such as radiometric or geometric corrections
Feature extraction, supervised and unsupervised classification, associative memories, and other techniques for improving accuracy and performance 
Key computer vision applications: remote sensing, medical imaging, compression, data mining, character recognition, stereovision, and more


Computer Vision and Fuzzy-Neural Systems illuminates the state-of-the-art throughhands-on exercises, real-world examples, and proven algorithms. It's an essential resource for every engineer, scientist, and programmer working in computer vision and a wide range of related fields. It can also be used as a textbook for undergraduate- or graduate-level courses in computer vision.
CD-ROM Included
Contains extensive library of MATLAB command files, executable files for some useful programs, and test images from Kodak and Space Imaging.

Author Biography: 
Dr. Arun D. Kulkarni is Professor of Computer Science at The University of Texas at Tyler, Tyler, Texas. His research interests include computer vision, fuzzy-neural systems, data mining, image processing, and artificial intelligence. He has authored a book and published more than 50 referred papers. His awards include the 1984 Fulbright Fellowship award and the 1997 NASA/ASSE Summer Faculty Fellowship. Dr. Kulkarni obtained his Ph.D. from the Indian Institute of Technology, Bombay, and was a post-doctoral fellow at Virginia Tech.",project-academic
,2020-01-01,p,,sample efficient optimization in the latent space of deep generative models via weighted retraining," Many important problems in science and engineering, such as drug design, involve optimizing an expensive black-box objective function over a complex, high-dimensional, and structured input space. Although machine learning techniques have shown promise in solving such problems, existing approaches substantially lack sample efficiency. We introduce an improved method for efficient black-box optimization, which performs the optimization in the low-dimensional, continuous latent manifold learned by a deep generative model. In contrast to previous approaches, we actively steer the generative model to maintain a latent manifold that is highly useful for efficiently optimizing the objective. We achieve this by periodically retraining the generative model on the data points queried along the optimization trajectory, as well as weighting those data points according to their objective function value. This weighted retraining can be easily implemented on top of existing methods, and is empirically shown to significantly improve their efficiency and performance on synthetic and real-world optimization problems.",project-academic
,2018-09-08,a,,fast gradient attack on network embedding," Network embedding maps a network into a low-dimensional Euclidean space, and thus facilitate many network analysis tasks, such as node classification, link prediction and community detection etc, by utilizing machine learning methods. In social networks, we may pay special attention to user privacy, and would like to prevent some target nodes from being identified by such network analysis methods in certain cases. Inspired by successful adversarial attack on deep learning models, we propose a framework to generate adversarial networks based on the gradient information in Graph Convolutional Network (GCN). In particular, we extract the gradient of pairwise nodes based on the adversarial network, and select the pair of nodes with maximum absolute gradient to realize the Fast Gradient Attack (FGA) and update the adversarial network. This process is implemented iteratively and terminated until certain condition is satisfied, i.e., the number of modified links reaches certain predefined value. Comprehensive attacks, including unlimited attack, direct attack and indirect attack, are performed on six well-known network embedding methods. The experiments on real-world networks suggest that our proposed FGA behaves better than some baseline methods, i.e., the network embedding can be easily disturbed using FGA by only rewiring few links, achieving state-of-the-art attack performance.",project-academic
10.1016/S2589-7500(21)00086-8,2021-08-01,a,Elsevier,application of comprehensive artificial intelligence retinal expert care system a national real world evidence study," Summary None None Background None Medical artificial intelligence (AI) has entered the clinical implementation phase, although real-world performance of deep-learning systems (DLSs) for screening fundus disease remains unsatisfactory. Our study aimed to train a clinically applicable DLS for fundus diseases using data derived from the real world, and externally test the model using fundus photographs collected prospectively from the settings in which the model would most likely be adopted. None None None Methods None In this national real-world evidence study, we trained a DLS, the Comprehensive AI Retinal Expert (CARE) system, to identify the 14 most common retinal abnormalities using 207 228 colour fundus photographs derived from 16 clinical settings with different disease distributions. CARE was internally validated using 21 867 photographs and externally tested using 18 136 photographs prospectively collected from 35 real-world settings across China where CARE might be adopted, including eight tertiary hospitals, six community hospitals, and 21 physical examination centres. The performance of CARE was further compared with that of 16 ophthalmologists and tested using datasets with non-Chinese ethnicities and previously unused camera types. This study was registered with None ClinicalTrials.gov , None NCT04213430 , and is currently closed. None None None Findings None The area under the receiver operating characteristic curve (AUC) in the internal validation set was 0·955 (SD 0·046). AUC values in the external test set were 0·965 (0·035) in tertiary hospitals, 0·983 (0·031) in community hospitals, and 0·953 (0·042) in physical examination centres. The performance of CARE was similar to that of ophthalmologists. Large variations in sensitivity were observed among the ophthalmologists in different regions and with varying experience. The system retained strong identification performance when tested using the non-Chinese dataset (AUC 0·960, 95% CI 0·957–0·964 in referable diabetic retinopathy). None None None Interpretation None Our DLS (CARE) showed satisfactory performance for screening multiple retinal abnormalities in real-world settings using prospectively collected fundus photographs, and so could allow the system to be implemented and adopted for clinical care. None None None Funding None This study was funded by the National Key R&D Programme of China, the Science and Technology Planning Projects of Guangdong Province, the National Natural Science Foundation of China, the Natural Science Foundation of Guangdong Province, and the Fundamental Research Funds for the Central Universities. None None None Translation None For the Chinese translation of the abstract see Supplementary Materials section.",project-academic
10.1109/TCSS.2019.2963247,2021-02-01,a,Institute of Electrical and Electronics Engineers (IEEE),information granulation based community detection for social networks," Online social networks (OSNs) have become so popular that it has changed the Internet to a more collaborative environment. Now, a third of the world’s population participates in OSNs, forming communities, and producing and consuming media in different ways. The recent boom of artificial intelligence technologies provides new opportunities to help improve the processing and mining of social data. In this article, an algorithm that can detect communities in the OSNs using the concepts of granular computing in rough sets is proposed. In this information model, a social network as a rough set granular social network (RGSN) is modeled. A new community detection algorithm named granular-based community detection (GBCD) is implemented. This article also defines and uses two measures, namely, a granular community factor and an object community factor. The proposed algorithm is evaluated on four real-world data sets as well as computer-generated data sets. The model is compared with other state-of-the-art community detection algorithms for the values of modularity, normalized mutual information (NMI), Omega index, accuracy, specificity, sensitivity, and None None None $F1$ None None -measure. The cumulative performance of the GBCD algorithm is found to be 3.99, which outperforms other state-of-the-art community detection algorithms.",project-academic
,1996-01-01,b,,computational intelligence pc tools," Computational intelligence is an emerging field in computer science which combines fuzzy logic, neural networks, and genetic algorithms for a flexible yet powerful approach to scientific computing. Because computational intelligence combines three interrelated, mathematically-based tools, it has a wide variety of applications, from engineering and process control to experts systems. This book takes a hands-on, desktop-applications approach to the topic, featuring examples of specific real-world implementations and detailed case studies, with all pertinent code and software included on a floppy disk packaged with the book. Features: * Concise introduction to the concepts of fuzzy logic, neural networks, and genetic algorithms, and how they relate to one another within the context of computational intelligence. * Computational intellignece applications, including self-organizing feature maps, fuzzy calculator, evolutionary programming, and fuzzy neural networks. * Detailed case studies from engineering (F-16 flight system), systems control (mass transit scheduling), and medicine (appendicitis diagnosis). * Windows floppy disk with both source code and executable, self-contained programs for desktop implementation of all of the book's applications.",project-academic
,2006-12-01,a,JMLR.org,large scale multiple kernel learning," While classical kernel-based learning algorithms are based on a single kernel, in practice it is often desirable to use multiple kernels. Lanckriet et al. (2004) considered conic combinations of kernel matrices for classification, leading to a convex quadratically constrained quadratic program. We show that it can be rewritten as a semi-infinite linear program that can be efficiently solved by recycling the standard SVM implementations. Moreover, we generalize the formulation and our method to a larger class of problems, including regression and one-class classification. Experimental results show that the proposed algorithm works for hundred thousands of examples or hundreds of kernels to be combined, and helps for automatic model selection, improving the interpretability of the learning result. In a second part we discuss general speed up mechanism for SVMs, especially when used with sparse feature maps as appear for string kernels, allowing us to train a string kernel SVM on a 10 million real-world splice data set from computational biology. We integrated multiple kernel learning in our machine learning toolbox None SHOGUN None for which the source code is publicly available at None http://www.fml.tuebingen.mpg.de/raetsch/projects/shogun .",project-academic
10.1109/TSMC.2018.2830099,2019-07-01,a,Institute of Electrical and Electronics Engineers (IEEE),efficient deep cnn based fire detection and localization in video surveillance applications," Convolutional neural networks (CNNs) have yielded state-of-the-art performance in image classification and other computer vision tasks. Their application in fire detection systems will substantially improve detection accuracy, which will eventually minimize fire disasters and reduce the ecological and social ramifications. However, the major concern with CNN-based fire detection systems is their implementation in real-world surveillance networks, due to their high memory and computational requirements for inference. In this paper, we propose an original, energy-friendly, and computationally efficient CNN architecture, inspired by the SqueezeNet architecture for fire detection, localization, and semantic understanding of the scene of the fire. It uses smaller convolutional kernels and contains no dense, fully connected layers, which helps keep the computational requirements to a minimum. Despite its low computational needs, the experimental results demonstrate that our proposed solution achieves accuracies that are comparable to other, more complex models, mainly due to its increased depth. Moreover, this paper shows how a tradeoff can be reached between fire detection accuracy and efficiency, by considering the specific characteristics of the problem of interest and the variety of fire data.",project-academic
10.3389/FNINS.2018.00774,2018-10-25,a,Frontiers Media SA,deep learning with spiking neurons opportunities and challenges," Spiking neural networks (SNNs) are inspired by information processing in biology, where sparse and asynchronous binary signals are communicated and processed in a massively parallel fashion. SNNs on neuromorphic hardware exhibit favorable properties such as low power consumption, fast inference, and event-driven information processing. This makes them interesting candidates for the efficient implementation of deep neural networks, the method of choice for many machine learning tasks. In this review, we address the opportunities that deep spiking networks offer and investigate in detail the challenges associated with training SNNs in a way that makes them competitive with conventional deep learning, but simultaneously allows for efficient mapping to hardware. A wide range of training methods for SNNs is presented, ranging from the conversion of conventional deep networks into SNNs, constrained training before conversion, spiking variants of backpropagation, and biologically motivated variants of STDP. The goal of our review is to define a categorization of SNN training methods, and summarize their advantages and drawbacks. We further discuss relationships between SNNs and binary networks, which are becoming popular for efficient digital hardware implementation. Neuromorphic hardware platforms have great potential to enable deep spiking networks in real-world applications. We compare the suitability of various neuromorphic systems that have been developed over the past years, and investigate potential use cases. Neuromorphic approaches and conventional machine learning should not be considered simply two solutions to the same classes of problems, instead it is possible to identify and exploit their task-specific advantages. Deep SNNs offer great opportunities to work with new types of event-based sensors, exploit temporal codes and local on-chip learning, and we have so far just scratched the surface of realizing these advantages in practical applications.",project-academic
,2007-08-10,b,Morgan Kaufmann Publishers Inc.,computational intelligence concepts to implementations," Russ Eberhart and Yuhui Shi have succeeded in integrating various natural and engineering disciplines to establish Computational Intelligence. This is the first comprehensive textbook, including lots of practical examples. -Shun-ichi Amari, RIKEN Brain Science Institute, Japan

This book is an excellent choice on its own, but, as in my case, will form the foundation for our advanced graduate courses in the CI disciplines. -James M. Keller, University of Missouri-Columbia

The excellent new book by Eberhart and Shi asserts that computational intelligence rests on a foundation of evolutionary computation. This refreshing view has set the book apart from other books on computational intelligence. The book has an emphasis on practical applications and computational tools, which are very useful and important for further development of the computational intelligence field. -Xin Yao, The Centre of Excellence for Research in Computational Intelligence and Applications, Birmingham

The ""soft"" analytic tools that comprise the field of computational intelligence have matured to the extent that they can, often in powerful combination with one another, form the foundation for a variety of solutions suitable for use by domain experts without extensive programming experience.

Computational Intelligence: Concepts to Implementations provides the conceptual and practical knowledge necessary to develop solutions of this kind. Focusing on evolutionary computation, neural networks, and fuzzy logic, the authors have constructed an approach to thinking about and working with computational intelligence that has, in their extensive experience, proved highly effective. 

Features
· Moves clearly and efficiently from concepts and paradigms to algorithms and implementation techniques by focusing, in the early chapters, on the specific concepts and paradigms that inform the authors' methodologies.

· Explores a number of key themes, including self-organization, complex adaptive systems, and emergent computation.

· Details the metrics and analytical tools needed to assess the performance of computational intelligence tools.

· Concludes with a series of case studies that illustrate a wide range of successful applications.

· Presents code examples in C and C++.

· Provides, at the end of each chapter, review questions and exercises suitable for graduate students, as well as researchers and practitioners engaged in self-study.

· Makes available, on a companion website, a number of software implementations that can be adapted for real-world applications.

· Moves clearly and efficiently from concepts and paradigms to algorithms and implementation techniques by focusing, in the early chapters, on the specific concepts and paradigms that inform the authors' methodologies.

· Explores a number of key themes, including self-organization, complex adaptive systems, and emergent computation.

· Details the metrics and analytical tools needed to assess the performance of computational intelligence tools.

· Concludes with a series of case studies that illustrate a wide range of successful applications.

· Presents code examples in C and C++.

· Provides, at the end of each chapter, review questions and exercises suitable for graduate students, as well as researchers and practitioners engaged in self-study.

· Makes available, on a companion website, a number of software implementations that can be adapted for real-world applications.",project-academic
10.1109/LRA.2020.2967324,2019-10-15,a,,a hybrid compact neural architecture for visual place recognition," State-of-the-art algorithms for visual place recognition, and related visual navigation systems, can be broadly split into two categories: computer-science-oriented models including deep learning or image retrieval-based techniques with minimal biological plausibility, and neuroscience-oriented dynamical networks that model temporal properties underlying spatial navigation in the brain. In this letter, we propose a new compact and high-performing place recognition model that bridges this divide for the first time. Our approach comprises two key neural models of these categories: (1) FlyNet, a compact, sparse two-layer neural network inspired by brain architectures of fruit flies, Drosophila melanogaster, and (2) a one-dimensional continuous attractor neural network (CANN). The resulting FlyNet+CANN network incorporates the compact pattern recognition capabilities of our FlyNet model with the powerful temporal filtering capabilities of an equally compact CANN, replicating entirely in a hybrid neural implementation the functionality that yields high performance in algorithmic localization approaches like SeqSLAM. We evaluate our model, and compare it to three state-of-the-art methods, on two benchmark real-world datasets with small viewpoint variations and extreme environmental changes - achieving 87% AUC results under day to night transitions compared to 60% for Multi-Process Fusion, 46% for LoST-X and 1% for SeqSLAM, while being 6.5, 310, and 1.5 times faster, respectively.",project-academic
10.1038/S42256-020-0198-X,2020-07-06,a,Nature Publishing Group,quantum approximate bayesian computation for nmr model inference," Recent technological advances may lead to the development of small-scale quantum computers that are capable of solving problems that cannot be tackled with classical computers. A limited number of algorithms have been proposed and their relevance to real-world problems is a subject of active investigation. Analysis of many-body quantum systems is particularly challenging for classical computers due to the exponential scaling of the Hilbert space dimension with the number of particles. Hence, solving the problems relevant to chemistry and condensed-matter physics is expected to be the first successful application of quantum computers. In this Article, we propose another class of problems from the quantum realm that can be solved efficiently on quantum computers: model inference for nuclear magnetic resonance (NMR) spectroscopy, which is important for biological and medical research. Our results are based on three interconnected studies. First, we use methods from classical machine learning to analyse a dataset of NMR spectra of small molecules. We perform stochastic neighbourhood embedding and identify clusters of spectra, and demonstrate that these clusters are correlated with the covalent structure of the molecules. Second, we propose a simple and efficient method, aided by a quantum simulator, to extract the NMR spectrum of any hypothetical molecule described by a parametric Heisenberg model. Third, we propose a simple variational Bayesian inference procedure for estimating the Hamiltonian parameters of experimentally relevant NMR spectra. Currently available quantum hardware is limited by noise, so practical implementations often involve a combination with classical approaches. Sels et al. identify a promising application for such a quantum–classic hybrid approach, namely inferring molecular structure from NMR spectra, by employing a range of machine learning tools in combination with a quantum simulator.",project-academic
10.1002/J.2168-9830.2008.TB00949.X,2008-01-01,a,Blackwell Publishing Ltd,2006 bernard m gordon prize lecture the learning factory industry partnered active learning," On February 21, 2006, the National Academy of Engineering recognized the achievements of the Learning Factory with the Bernard M. Gordon Prize for Innovation in Engineering and Technology Education. The co-founders were commended “for creating the Learning Factory, where multidisciplinary student teams develop engineering leadership skills by working with industry to solve real-world problems.” This paper describes the origins, motivation, philosophy, and implementation of the Learning Factory.

The specific innovations of the Learning Factory partnership were: active learning facilities, called Learning Factories, that provide experiential reinforcement of engineering science, and a realization of its limitations; strong collaborations with industry through advisory boards, engineers in the classroom, and industry-sponsored capstone design projects; practice-based engineering courses integrating analytical and theoretical knowledge with manufacturing, design, business concepts, and professional skills; and dissemination to other academic institutions (domestic and international), government and industry.",project-academic
,2019-07-10,p,,neugraph parallel deep neural network computation on large graphs," Recent deep learning models have moved beyond low dimensional regular grids such as image, video, and speech, to high-dimensional graph-structured data, such as social networks, e-commerce user-item graphs, and knowledge graphs. This evolution has led to large graph-based neural network models that go beyond what existing deep learning frameworks or graph computing systems are designed for. We present NeuGraph, a new framework that bridges the graph and dataflow models to support efficient and scalable parallel neural network computation on graphs. NeuGraph introduces graph computation optimizations into the management of data partitioning, scheduling, and parallelism in dataflow-based deep learning frameworks. Our evaluation shows that, on small graphs that can fit in a single GPU, NeuGraph outperforms state-of-the-art implementations by a significant margin, while scaling to large real-world graphs that none of the existing frameworks can handle directly with GPUs. (Please stay tuned for further updates.)",project-academic
,2021-06-11,a,,global neighbor sampling for mixed cpu gpu training on giant graphs," Graph neural networks (GNNs) are powerful tools for learning from graph data and are widely used in various applications such as social network recommendation, fraud detection, and graph search. The graphs in these applications are typically large, usually containing hundreds of millions of nodes. Training GNN models on such large graphs efficiently remains a big challenge. Despite a number of sampling-based methods have been proposed to enable mini-batch training on large graphs, these methods have not been proved to work on truly industry-scale graphs, which require GPUs or mixed-CPU-GPU training. The state-of-the-art sampling-based methods are usually not optimized for these real-world hardware setups, in which data movement between CPUs and GPUs is a bottleneck. To address this issue, we propose Global Neighborhood Sampling that aims at training GNNs on giant graphs specifically for mixed-CPU-GPU training. The algorithm samples a global cache of nodes periodically for all mini-batches and stores them in GPUs. This global cache allows in-GPU importance sampling of mini-batches, which drastically reduces the number of nodes in a mini-batch, especially in the input layer, to reduce data copy between CPU and GPU and mini-batch computation without compromising the training convergence rate or model accuracy. We provide a highly efficient implementation of this method and show that our implementation outperforms an efficient node-wise neighbor sampling baseline by a factor of 2X-4X on giant graphs. It outperforms an efficient implementation of LADIES with small layers by a factor of 2X-14X while achieving much higher accuracy than LADIES.We also theoretically analyze the proposed algorithm and show that with cached node data of a proper size, it enjoys a comparable convergence rate as the underlying node-wise sampling method.",project-academic
10.1145/3447548.3467437,2021-08-14,p,Association for Computing Machinery,global neighbor sampling for mixed cpu gpu training on giant graphs," Graph neural networks (GNNs) are powerful tools for learning from graph data and are widely used in various applications such as social network recommendation, fraud detection, and graph search. The graphs in these applications are typically large, usually containing hundreds of millions of nodes. Training GNN models on such large graphs efficiently remains a big challenge. Despite a number of sampling-based methods have been proposed to enable mini-batch training on large graphs, these methods have not been proved to work on truly industry-scale graphs, which require GPUs or mixed CPU-GPU training. The state-of-the-art sampling-based methods are usually not optimized for these real-world hardware setups, in which data movement between CPUs and GPUs is a bottleneck. To address this issue, we propose Global Neighborhood Sampling that aims at training GNNs on giant graphs specifically for mixed CPU-GPU training. The algorithm samples a global cache of nodes periodically for all mini-batches and stores them in GPUs. This global cache allows in-GPU importance sampling of mini-batches, which drastically reduces the number of nodes in a mini-batch, especially in the input layer, to reduce data copy between CPU and GPU and mini-batch computation without compromising the training convergence rate or model accuracy. We provide a highly efficient implementation of this method and show that our implementation outperforms an efficient node-wise neighbor sampling baseline by a factor of 2× ~ 4× on giant graphs. It outperforms an efficient implementation of LADIES with small layers by a factor of 2× ~ 14× while achieving much higher accuracy than LADIES. We also theoretically analyze the proposed algorithm and show that with cached node data of a proper size, it enjoys a comparable convergence rate as the underlying node-wise sampling method.",project-academic
10.1103/PHYSREVE.101.042304,2020-04-23,a,American Physical Society,improved mutual information measure for clustering classification and community detection," The information theoretic measure known as mutual information is widely used as a way to quantify the similarity of two different labelings or divisions of the same set of objects, such as arises, for instance, in clustering and classification problems in machine learning or community detection problems in network science. Here we argue that the standard mutual information, as commonly defined, omits a crucial term which can become large under real-world conditions, producing results that can be substantially in error. We derive an expression for this missing term and hence write a corrected mutual information that gives accurate results even in cases where the standard measure fails. We discuss practical implementation of the new measure and give example applications.",project-academic
10.1109/TVCG.2018.2864838,2019-01-01,a,IEEE,vis4ml an ontology for visual analytics assisted machine learning," While many VA workflows make use of machine-learned models to support analytical tasks, VA workflows have become increasingly important in understanding and improving Machine Learning (ML) processes. In this paper, we propose an ontology (VIS4ML) for a subarea of VA, namely “VA-assisted ML”. The purpose of VIS4ML is to describe and understand existing VA workflows used in ML as well as to detect gaps in ML processes and the potential of introducing advanced VA techniques to such processes. Ontologies have been widely used to map out the scope of a topic in biology, medicine, and many other disciplines. We adopt the scholarly methodologies for constructing VIS4ML, including the specification, conceptualization, formalization, implementation, and validation of ontologies. In particular, we reinterpret the traditional VA pipeline to encompass model-development workflows. We introduce necessary definitions, rules, syntaxes, and visual notations for formulating VIS4ML and make use of semantic web technologies for implementing it in the Web Ontology Language (OWL). VIS4ML captures the high-level knowledge about previous workflows where VA is used to assist in ML. It is consistent with the established VA concepts and will continue to evolve along with the future developments in VA and ML. While this ontology is an effort for building the theoretical foundation of VA, it can be used by practitioners in real-world applications to optimize model-development workflows by systematically examining the potential benefits that can be brought about by either machine or human capabilities. Meanwhile, VIS4ML is intended to be extensible and will continue to be updated to reflect future advancements in using VA for building high-quality data-analytical models or for building such models rapidly.",project-academic
,2017-01-01,a,AI Now Institute at New York University,ai now 2017 report," These recommendations reflect the views and research of the AI Now Institute at New York University. We thank the experts who contributed to the AI Now 2017 Symposium and Workshop for informing these perspectives, and our research team for helping shape the AI Now 2017 Report. Artificial intelligence (AI) technologies are in a phase of rapid development, and are being adopted widely. While the concept of artificial intelligence has existed for over sixty years, real-world applications have only accelerated in the last decade due to three concurrent developments: better algorithms, increases in networked computing power and the tech industry’s ability to capture and store massive amounts of data. AI systems are already integrated in everyday technologies like smartphones and personal assistants, making predictions and determinations that help personalize experiences and advertise products. Beyond the familiar, these systems are also being introduced in critical areas like law, finance, policing and the workplace, where they are increasingly used to predict everything from our taste in music to our likelihood of committing a crime to our fitness for a job or an educational opportunity. AI companies promise that the technologies they create can automate the toil of repetitive work, identify subtle behavioral patterns and much more. However, the analysis and understanding of artificial intelligence should not be limited to its technical capabilities. The design and implementation of this next generation of computational tools presents deep normative and ethical challenges for our existing social, economic and political relationships and institutions, and these changes are already underway. Simply put, AI does not exist in a vacuum. We must also ask how broader phenomena like widening inequality, an intensification of concentrated geopolitical power and populist political movements will shape and be shaped by the development and application of AI technologies.",project-academic
10.1109/QSIC.2009.26,2009-08-24,p,IEEE,application of metamorphic testing to supervised classifiers," Many applications in the field of scientific computing - such as computational biology, computational linguistics, and others - depend on Machine Learning algorithms to provide important core functionality to support solutions in the particular problem domains. However, it is difficult to test such applications because often there is no ""test oracle"" to indicate what the correct output should be for arbitrary input. To help address the quality of such software, in this paper we present a technique for testing the implementations of supervised machine learning classification algorithms on which such scientific computing software depends. Our technique is based on an approach called ""metamorphic testing"", which has been shown to be effective in such cases. More importantly, we demonstrate that our technique not only serves the purpose of verification, but also can be applied in validation. In addition to presenting our technique, we describe a case study we performed on a real-world machine learning application framework, and discuss how programmers implementing machine learning algorithms can avoid the common pitfalls discovered in our study. We also discuss how our findings can be of use to other areas outside scientific computing, as well.",project-academic
,2008-01-01,a,,the learning factory industry partnered active learning," On February 21, 2006, the National Academy of Engineering recognized the achievements of the Learning Factory with the Bernard M. Gordon Prize for Innovation in Engineering and Technology Education. The co-founders were commended “for creating the Learning Factory, where multidisciplinary student teams develop engineering leadership skills by working with industry to solve real-world problems.” This paper describes the origins, motivation, philosophy, and implementation of the Learning Factory. The specific innovations of the Learning Factory partnership were: active learning facilities, called Learning Factories, that provide experiential reinforcement of engineering science, and a realization of its limitations; strong collaborations with industry through advisory boards, engineers in the classroom, and industry-sponsored capstone design projects; practice-based engineering courses integrating analytical and theoretical knowledge with manufacturing, design, business concepts, and professional skills; and dissemination to other academic institutions (domestic and international), government and industry.",project-academic
10.1109/CLOUDCOM.2011.21,2011-11-29,p,IEEE,ihadoop asynchronous iterations for mapreduce," MapReduce is a distributed programming framework designed to ease the development of scalable data-intensive applications for large clusters of commodity machines. Most machine learning and data mining applications involve iterative computations over large datasets, such as the Web hyperlink structures and social network graphs. Yet, the MapReduce model does not efficiently support this important class of applications. The architecture of MapReduce, most critically its dataflow techniques and task scheduling, is completely unaware of the nature of iterative applications, tasks are scheduled according to a policy that optimizes the execution for a single iteration which wastes bandwidth, I/O, and CPU cycles when compared with an optimal execution for a consecutive set of iterations. This work presents iHadoop, a modified MapReduce model, and an associated implementation, optimized for iterative computations. The iHadoop model schedules iterations asynchronously. It connects the output of one iteration to the next, allowing both to process their data concurrently. iHadoop's task scheduler exploits inter-iteration data locality by scheduling tasks that exhibit a producer/consumer relation on the same physical machine allowing a fast local data transfer. For those iterative applications that require satisfying certain criteria before termination, iHadoop runs the check concurrently during the execution of the subsequent iteration to further reduce the application's latency. This paper also describes our implementation of the iHadoop model, and evaluates its performance against Hadoop, the widely used open source implementation of MapReduce. Experiments using different data analysis applications over real-world and synthetic datasets show that iHadoop performs better than Hadoop for iterative algorithms, reducing execution time of iterative applications by 25% on average. Furthermore, integrating iHadoop with HaLoop, a variant Hadoop implementation that caches invariant data between iterations, reduces execution time by 38% on average.",project-academic
10.1016/J.JACR.2019.06.009,2019-10-01,a,J Am Coll Radiol,bending the artificial intelligence curve for radiology informatics tools from acr and rsna," Artificial intelligence (AI) will reshape radiology over the coming years. The radiology community has a strong history of embracing new technology for positive change, and AI is no exception. As with any new technology, rapid, successful implementation faces several challenges that will require creation and adoption of new integration technology. Use cases important to real-world application of AI are described, including clinical registries, AI research, AI product validation, and computer assistance for radiology reporting. Furthermore, the informatics technologies required for successful implementation of the use cases are described, including open Computer-Assisted Radiologist Decision Support, ACR Assist, ACR Data Science Institute use cases, common data elements (radelement.org), RadLex (radlex.org), LOINC/RSNA RadLex Playbook (loinc.org), and Radiology Report Templates (radreport.org).",project-academic
10.1016/J.BREAST.2019.10.001,2020-02-01,a,Churchill Livingstone,the ethical legal and social implications of using artificial intelligence systems in breast cancer care," Abstract None None Breast cancer care is a leading area for development of artificial intelligence (AI), with applications including screening and diagnosis, risk calculation, prognostication and clinical decision-support, management planning, and precision medicine. We review the ethical, legal and social implications of these developments. We consider the values encoded in algorithms, the need to evaluate outcomes, and issues of bias and transferability, data ownership, confidentiality and consent, and legal, moral and professional responsibility. We consider potential effects for patients, including on trust in healthcare, and provide some social science explanations for the apparent rush to implement AI solutions. We conclude by anticipating future directions for AI in breast cancer care. Stakeholders in healthcare AI should acknowledge that their enterprise is an ethical, legal and social challenge, not just a technical challenge. Taking these challenges seriously will require broad engagement, imposition of conditions on implementation, and pre-emptive systems of oversight to ensure that development does not run ahead of evaluation and deliberation. Once artificial intelligence becomes institutionalised, it may be difficult to reverse: a proactive role for government, regulators and professional groups will help ensure introduction in robust research contexts, and the development of a sound evidence base regarding real-world effectiveness. Detailed public discussion is required to consider what kind of AI is acceptable rather than simply accepting what is offered, thus optimising outcomes for health systems, professionals, society and those receiving care.",project-academic
10.1145/3020078.3021737,2017-02-22,p,ACM,boosting the performance of fpga based graph processor using hybrid memory cube a case for breadth first search," Large graph processing has gained great attention in recent years due to its broad applicability from machine learning to social science. Large real-world graphs, however, are inherently difficult to process efficiently, not only due to their large memory footprint, but also that most graph algorithms entail memory access patterns with poor locality and a low compute-to-memory access ratio. In this work, we leverage the exceptional random access performance of emerging Hybrid Memory Cube (HMC) technology that stacks multiple DRAM dies on top of a logic layer, combined with the flexibility and efficiency of FPGA to address these challenges. To our best knowledge, this is the first work that implements a graph processing system on a FPGA-HMC platform based on software/hardware co-design and co-optimization. We first present the modifications of algorithm and a platform-aware graph processing architecture to perform level-synchronized breadth first search (BFS) on FPGA-HMC platform. To gain better insights into the potential bottlenecks of proposed implementation, we develop an analytical performance model to quantitatively evaluate the HMC access latency and corresponding BFS performance. Based on the analysis, we propose a two-level bitmap scheme to further reduce memory access and perform optimization on key design parameters (e.g. memory access granularity). Finally, we evaluate the performance of our BFS implementation using the AC-510 development kit from Micron. We achieved 166 million edges traversed per second (MTEPS) using GRAPH500 benchmark on a random graph with a scale of 25 and an edge factor of 16, which significantly outperforms CPU and other FPGA-based large graph processors.",project-academic
10.1136/BMJOPEN-2019-030482,2019-12-11,a,BMJ Open,current state of science in machine learning methods for automatic infant pain evaluation using facial expression information study protocol of a systematic review and meta analysis," Introduction None Infants can experience pain similar to adults, and improperly controlled pain stimuli could have a long-term adverse impact on their cognitive and neurological function development. The biggest challenge of achieving good infant pain control is obtaining objective pain assessment when direct communication is lacking. For years, computer scientists have developed many different facial expression-centred machine learning (ML) methods for automatic infant pain assessment. Many of these ML algorithms showed rather satisfactory performance and have demonstrated good potential to be further enhanced for implementation in real-world clinical settings. To date, there is no prior research that has systematically summarised and compared the performance of these ML algorithms. Our proposed meta-analysis will provide the first comprehensive evidence on this topic to guide further ML algorithm development and clinical implementation. None Methods and analysis None We will search four major public electronic medical and computer science databases including Web of Science, PubMed, Embase and IEEE Xplore Digital Library from January 2008 to present. All the articles will be imported into the Covidence platform for study eligibility screening and inclusion. Study-level extracted data will be stored in the Systematic Review Data Repository online platform. The primary outcome will be the prediction accuracy of the ML model. The secondary outcomes will be model utility measures including generalisability, interpretability and computational efficiency. All extracted outcome data will be imported into RevMan V.5.2.1 software and R V3.3.2 for analysis. Risk of bias will be summarised using the latest Prediction Model Study Risk of Bias Assessment Tool. None Ethics and dissemination None This systematic review and meta-analysis will only use study-level data from public databases, thus formal ethical approval is not required. The results will be disseminated in the form of an official publication in a peer-reviewed journal and/or presentation at relevant conferences. None PROSPERO registration number None CRD42019118784.",project-academic
10.1109/ICDMW51313.2020.00082,2020-11-01,p,IEEE,sync a copula based framework for generating synthetic data from aggregated sources," A synthetic dataset is a data object that is generated programmatically, and it may be valuable to creating a single dataset from multiple sources when direct collection is difficult or costly. Although it is a fundamental step for many data science tasks, an efficient and standard framework is absent. In this paper, we study a specific synthetic data generation task called downscaling, a procedure to infer high-resolution, harder-to-collect information (e.g., individual level records) from many low-resolution, easy-to-collect sources, and propose a multi-stage framework called SynC (Synthetic Data Generation via Gaussian Copula). For given low-resolution datasets, the central idea of SynC is to fit Gaussian copula models to each of the low-resolution datasets in order to correctly capture dependencies and marginal distributions, and then sample from the fitted models to obtain the desired high-resolution subsets. Predictive models are then used to merge sampled subsets into one, and finally, sampled datasets are scaled according to low-resolution marginal constraints. We make four key contributions in this work: 1) propose a novel framework for generating individual level data from aggregated data sources by combining state-of-the-art machine learning and statistical techniques, 2) perform simulation studies to validate SynC's performance as a synthetic data generation algorithm, 3) demonstrate its value as a feature engineering tool, as well as an alternative to data collection in situations where gathering is difficult through two real-world datasets, 4) release an easy-to-use framework implementation for reproducibility and scalability at the production level that easily incorporates new data.",project-academic
,2020-09-20,a,,sync a copula based framework for generating synthetic data from aggregated sources," A synthetic dataset is a data object that is generated programmatically, and it may be valuable to creating a single dataset from multiple sources when direct collection is difficult or costly. Although it is a fundamental step for many data science tasks, an efficient and standard framework is absent. In this paper, we study a specific synthetic data generation task called downscaling, a procedure to infer high-resolution, harder-to-collect information (e.g., individual level records) from many low-resolution, easy-to-collect sources, and propose a multi-stage framework called SYNC (Synthetic Data Generation via Gaussian Copula). For given low-resolution datasets, the central idea of SYNC is to fit Gaussian copula models to each of the low-resolution datasets in order to correctly capture dependencies and marginal distributions, and then sample from the fitted models to obtain the desired high-resolution subsets. Predictive models are then used to merge sampled subsets into one, and finally, sampled datasets are scaled according to low-resolution marginal constraints. We make four key contributions in this work: 1) propose a novel framework for generating individual level data from aggregated data sources by combining state-of-the-art machine learning and statistical techniques, 2) perform simulation studies to validate SYNC's performance as a synthetic data generation algorithm, 3) demonstrate its value as a feature engineering tool, as well as an alternative to data collection in situations where gathering is difficult through two real-world datasets, 4) release an easy-to-use framework implementation for reproducibility and scalability at the production level that easily incorporates new data.",project-academic
10.1109/MCI.2008.930983,2009-02-01,a,IEEE,a successful interdisciplinary course on coputational intelligence," This article presents experiences from the introduction of a new three hour interdisciplinary course on computational intelligence (CI) taught at the Missouri University of Science and Technology, USA at the undergraduate and graduate levels. This course is unique in the sense that it covers five main paradigms of CI and their integration to develop hybrid intelligent systems. The paradigms covered are artificial immune systems (AISs), evolutionary computing (EC), fuzzy systems (FSs), neural networks (NNs) and swarm intelligence (SI). While individual CI paradigms have been applied successfully to solve real-world problems, the current trend is to develop hybrids of these paradigms since no one paradigm is superior to any other for solving all types of problems. In doing so, respective strengths of individual components in a hybrid CI system are capitalized while their weaknesses are eliminated. This CI course is at the introductory level and the objective is to lead students to in-depth courses and specialization in a particular paradigm (AISs, EC, FSs, NNs, SI). The idea of an integrated and interdisciplinary course like this, especially at the undergraduate level, is to expose students to different CI paradigms at an early stage in their degree program and career. The curriculum, assessment, implementation, and impacts of an interdisciplinary CI course are described.",project-academic
10.1007/978-3-642-10701-6,2010-06-04,b,"Springer Publishing Company, Incorporated",computational intelligence in expensive optimization problems," In modern science and engineering, laboratory experiments are replaced by high fidelity and computationally expensive simulations. Using such simulations reduces costs and shortens development times but introduces new challenges to design optimization process. Examples of such challenges include limited computational resource for simulation runs, complicated response surface of the simulation inputs-outputs, and etc. Under such difficulties, classical optimization and analysis methods may perform poorly. This motivates the application of computational intelligence methods such as evolutionary algorithms, neural networks and fuzzy logic, which often perform well in such settings. This is the first book to introduce the emerging field of computational intelligence in expensive optimization problems. Topics covered include: dedicated implementations of evolutionary algorithms, neural networks and fuzzy logic. reduction of expensive evaluations (modelling, variable-fidelity, fitness inheritance), frameworks for optimization (model management, complexity control, model selection), parallelization of algorithms (implementation issues on clusters, grids, parallel machines), incorporation of expert systems and human-system interface, single and multiobjective algorithms, data mining and statistical analysis, analysis of real-world cases (such as multidisciplinary design optimization). The edited book provides both theoretical treatments and real-world insights gained by experience, all contributed by leading researchers in the respective fields. As such, it is a comprehensive reference for researchers, practitioners, and advanced-level students interested in both the theory and practice of using computational intelligence for expensive optimization problems.",project-academic
10.1145/3174243.3174260,2018-02-15,p,ACM,accelerating graph analytics by co optimizing storage and access on an fpga hmc platform," Graph analytics, which explores the relationships among interconnected entities, is becoming increasingly important due to its broad applicability, from machine learning to social sciences. However, due to the irregular data access patterns in graph computations, one major challenge for graph processing systems is performance. The algorithms, softwares, and hardwares that have been tailored for mainstream parallel applications are generally not effective for massive, sparse graphs from the real-world problems, due to their complex and irregular structures. To address the performance issues in large-scale graph analytics, we leverage the exceptional random access performance of the emerging Hybrid Memory Cube (HMC) combined with the flexibility and efficiency of modern FPGAs. In particular, we develop a collaborative software/hardware technique to perform a level-synchronized Breadth First Search (BFS) on a FPGA-HMC platform. From the software perspective, we develop an architecture-aware graph clustering algorithm that exploits the FPGA-HMC platform»s capability to improve data locality and memory access efficiency. From the hardware perspective, we further improve the FPGA-HMC graph processor architecture by designing a memory request merging unit to take advantage of the increased data locality resulting from graph clustering. We evaluate the performance of our BFS implementation using the AC-510 development kit from Micron and achieve $2.8 \times$ average performance improvement compared to the latest FPGA-HMC based graph processing system over a set of benchmarks from a wide range of applications.",project-academic
10.1002/WPS.20883,2021-10-01,a,"John Wiley & Sons, Ltd",the growing field of digital psychiatry current evidence and the future of apps social media chatbots and virtual reality," As the COVID-19 pandemic has largely increased the utilization of telehealth, mobile mental health technologies - such as smartphone apps, vir-tual reality, chatbots, and social media - have also gained attention. These digital health technologies offer the potential of accessible and scalable interventions that can augment traditional care. In this paper, we provide a comprehensive update on the overall field of digital psychiatry, covering three areas. First, we outline the relevance of recent technological advances to mental health research and care, by detailing how smartphones, social media, artificial intelligence and virtual reality present new opportunities for ""digital phenotyping"" and remote intervention. Second, we review the current evidence for the use of these new technological approaches across different mental health contexts, covering their emerging efficacy in self-management of psychological well-being and early intervention, along with more nascent research supporting their use in clinical management of long-term psychiatric conditions - including major depression; anxiety, bipolar and psychotic disorders; and eating and substance use disorders - as well as in child and adolescent mental health care. Third, we discuss the most pressing challenges and opportunities towards real-world implementation, using the Integrated Promoting Action on Research Implementation in Health Services (i-PARIHS) framework to explain how the innovations themselves, the recipients of these innovations, and the context surrounding innovations all must be considered to facilitate their adoption and use in mental health care systems. We conclude that the new technological capabilities of smartphones, artificial intelligence, social media and virtual reality are already changing mental health care in unforeseen and exciting ways, each accompanied by an early but promising evidence base. We point out that further efforts towards strengthening implementation are needed, and detail the key issues at the patient, provider and policy levels which must now be addressed for digital health technologies to truly improve mental health research and treatment in the future.",project-academic
10.1007/S13278-020-00696-X,2020-09-29,a,Springer Vienna,deep learning for misinformation detection on online social networks a survey and new perspectives," Recently, the use of social networks such as Facebook, Twitter, and Sina Weibo has become an inseparable part of our daily lives. It is considered as a convenient platform for users to share personal messages, pictures, and videos. However, while people enjoy social networks, many deceptive activities such as fake news or rumors can mislead users into believing misinformation. Besides, spreading the massive amount of misinformation in social networks has become a global risk. Therefore, misinformation detection (MID) in social networks has gained a great deal of attention and is considered an emerging area of research interest. We find that several studies related to MID have been studied to new research problems and techniques. While important, however, the automated detection of misinformation is difficult to accomplish as it requires the advanced model to understand how related or unrelated the reported information is when compared to real information. The existing studies have mainly focused on three broad categories of misinformation: false information, fake news, and rumor detection. Therefore, related to the previous issues, we present a comprehensive survey of automated misinformation detection on (i) false information, (ii) rumors, (iii) spam, (iv) fake news, and (v) disinformation. We provide a state-of-the-art review on MID where deep learning (DL) is used to automatically process data and create patterns to make decisions not only to extract global features but also to achieve better results. We further show that DL is an effective and scalable technique for the state-of-the-art MID. Finally, we suggest several open issues that currently limit real-world implementation and point to future directions along this dimension.",project-academic
,2021-07-29,b,,an introduction to statistical learning with applications in r," An Introduction to Statistical Learning provides an accessible overview of the field of statistical learning, an essential toolset for making sense of the vast and complex data sets that have emerged in fields ranging from biology to finance to marketing to astrophysics in the past twenty years. This book presents some of the most important modeling and prediction techniques, along with relevant applications. Topics include linear regression, classification, resampling methods, shrinkage approaches, tree-based methods, support vector machines, clustering, and more. Color graphics and real-world examples are used to illustrate the methods presented. Since the goal of this textbook is to facilitate the use of these statistical learning techniques by practitioners in science, industry, and other fields, each chapter contains a tutorial on implementing the analyses and methods presented in R, an extremely popular open source statistical software platform. Two of the authors co-wrote The Elements of Statistical Learning (Hastie, Tibshirani and Friedman, 2nd edition 2009), a popular reference book for statistics and machine learning researchers. An Introduction to Statistical Learning covers many of the same topics, but at a level accessible to a much broader audience. This book is targeted at statisticians and non-statisticians alike who wish to use cutting-edge statistical learning techniques to analyze their data. The text assumes only a previous course in linear regression and no knowledge of matrix algebra.",project-academic
10.1016/S0004-3702(99)00107-1,2000-03-01,a,Elsevier Science Publishers Ltd.,on agent based software engineering," Agent-based computing represents an exciting new synthesis both for Artificial Intelligence (AI) and, more generally, Computer Science. It has the potential to significantly improve the theory and the practice of modeling, designing, and implementing computer systems. Yet, to date, there has been little systematic analysis of what makes the agent-based approach such an appealing and powerful computational model. Moreover, even less effort has been devoted to discussing the inherent disadvantages that stem from adopting an agent-oriented view. Here both sets of issues are explored. The standpoint of this analysis is the role of agent-based software in solving complex, real-world problems. In particular, it will be argued that the development of robust and scalable software systems requires autonomous agents that can complete their objectives while situated in a dynamic and uncertain environment, that can engage in rich, high-level social interactions, and that can operate within flexible organisational structures.",project-academic
,2008-09-09,b,,building intelligent interactive tutors student centered strategies for revolutionizing e learning," Computers have transformed every facet of our culture, most dramatically communication, transportation, finance, science, and the economy. Yet their impact has not been generally felt in education due to lack of hardware, teacher training, and sophisticated software. Another reason is that current instructional software is neither truly responsive to student needs nor flexible enough to emulate teaching. The more instructional software can reason about its own teaching process, know what it is teaching, and which method to use for teaching, the greater is its impact on education. 

Building Intelligent Interactive Tutors discusses educational systems that assess a student's knowledge and are adaptive to a student's learning needs. Dr. Woolf taps into 20 years of research on intelligent tutors to bring designers and developers a broad range of issues and methods that produce the best intelligent learning environments possible, whether for classroom or life-long learning. The book describes multidisciplinary approaches to using computers for teaching, reports on research, development, and real-world experiences, and discusses intelligent tutors, web-based learning systems, adaptive learning systems, intelligent agents and intelligent multimedia.

*Combines both theory and practice to offer most in-depth and up-to-date treatment of intelligent tutoring systems available
*Presents powerful drivers of virtual teaching systems, including cognitive science, artificial intelligence, and the Internet
*Features algorithmic material that enables programmers and researchers to design building components and intelligent systems",project-academic
,2009-12-16,b,,modeling and simulation of systems using matlab and simulink," Not only do modeling and simulation help provide a better understanding of how real-world systems function, they also enable us to predict system behavior before a system is actually built and analyze systems accurately under varying operating conditions. Modeling and Simulation of Systems Using MATLAB and Simulink provides comprehensive, state-of-the-art coverage of all the important aspects of modeling and simulating both physical and conceptual systems. Various real-life examples show how simulation plays a key role in understanding real-world systems. The author also explains how to effectively use MATLAB and Simulink software to successfully apply the modeling and simulation techniques presented. After introducing the underlying philosophy of systems, the book offers step-by-step procedures for modeling different types of systems using modeling techniques, such as the graph-theoretic approach, interpretive structural modeling, and system dynamics modeling. It then explores how simulation evolved from pre-computer days into the current science of today. The text also presents modern soft computing techniques, including artificial neural networks, fuzzy systems, and genetic algorithms, for modeling and simulating complex and nonlinear systems. The final chapter addresses discrete systems modeling. Preparing both undergraduate and graduate students for advanced modeling and simulation courses, this text helps them carry out effective simulation studies. In addition, graduate students should be able to comprehend and conduct simulation research after completing this book. AncillariesAccompanying CD-ROM includes simulation code in MATLAB and Simulink, enabling quick and useful insight into real-world systems. A solutions manual is available for qualifying instructors.",project-academic
10.1126/SCIENCE.AAT8414,2019-06-21,a,American Association for the Advancement of Science,trends and challenges in robot manipulation," BACKGROUND None Humans have a fantastic ability to manipulate objects of various shapes, sizes, and materials and can control the objects’ position in confined spaces with the advanced dexterity capabilities of our hands. Building machines inspired by human hands, with the functionality to autonomously pick up and manipulate objects, has always been an essential component of robotics. The first robot manipulators date back to the 1960s and are some of the first robotic devices ever constructed. In these early days, robotic manipulation consisted of carefully prescribed movement sequences that a robot would execute with no ability to adapt to a changing environment. As time passed, robots gradually gained the ability to automatically generate movement sequences, drawing on artificial intelligence and automated reasoning. Robots would stack boxes according to size, weight, and so forth, extending beyond geometric reasoning. This task also required robots to handle errors and uncertainty in sensing at run time, given that the slightest imprecision in the position and orientation of stacked boxes might cause the entire tower to topple. Methods from control theory also became instrumental for enabling robots to comply with the environment’s natural uncertainty by empowering them to adapt exerted forces upon contact. The ability to stably vary forces upon contact expanded robots’ manipulation repertoire to more-complex tasks, such as inserting pegs in holes or hammering. However, none of these actions truly demonstrated fine or in-hand manipulation capabilities, and they were commonly performed using simple two-fingered grippers. To enable multipurpose fine manipulation, roboticists focused their efforts on designing humanlike hands capable of using tools. Wielding a tool in-hand became a problem of its own, and a variety of advanced algorithms were developed to facilitate stable holding of objects and provide optimality guarantees. Because optimality was difficult to achieve in a stochastic environment, from the 1990s onward researchers aimed to increase the robustness of object manipulation at all levels. These efforts initiated the design of sensors and hardware for improved control of hand–object contacts. Studies that followed were focused on robust perception for coping with object occlusion and noisy measurements, as well as on adaptive control approaches to infer an object’s physical properties, so as to handle objects whose properties are unknown or change as a result of manipulation. None ADVANCES None Roboticists are still working to develop robots capable of sorting and packaging objects, chopping vegetables, and folding clothes in unstructured and dynamic environments. Robots used for modern manufacturing have accomplished some of these tasks in structured settings that still require fences between the robots and human operators to ensure safety. Ideally, robots should be able to work side by side with humans, offering their strength to carry heavy loads while presenting no danger. Over the past decade, robots have gained new levels of dexterity. This enhancement is due to breakthroughs in mechanics with sensors for perceiving touch along a robot’s body and new mechanics for soft actuation to offer natural compliance. Most notably, this development leverages the immense progress in machine learning to encapsulate models of uncertainty and support further advances in adaptive and robust control. Learning to manipulate in real-world settings is costly in terms of both time and hardware. To further elaborate on data-driven methods but avoid generating examples with real, physical systems, many researchers use simulation environments. Still, grasping and dexterous manipulation require a level of reality that existing simulators are not yet able to deliver—for example, in the case of modeling contacts for soft and deformable objects. Two roads are hence pursued: The first draws inspiration from the way humans acquire interaction skills and prompts robots to learn skills from observing humans performing complex manipulation. This allows robots to acquire manipulation capabilities in only a few trials. However, generalizing the acquired knowledge to apply to actions that differ from those previously demonstrated remains difficult. The second road constructs databases of real object manipulation, with the goal to better inform the simulators and generate examples that are as realistic as possible. Yet achieving realistic simulation of friction, material deformation, and other physical properties may not be possible anytime soon, and real experimental evaluation will be unavoidable for learning to manipulate highly deformable objects. None OUTLOOK None Despite many years of software and hardware development, achieving dexterous manipulation capabilities in robots remains an open problem—albeit an interesting one, given that it necessitates improved understanding of human grasping and manipulation techniques. We build robots to automate tasks but also to provide tools for humans to easily perform repetitive and dangerous tasks while avoiding harm. Achieving robust and flexible collaboration between humans and robots is hence the next major challenge. Fences that currently separate humans from robots will gradually disappear, and robots will start manipulating objects jointly with humans. To achieve this objective, robots must become smooth and trustable partners that interpret humans’ intentions and respond accordingly. Furthermore, robots must acquire a better understanding of how humans interact and must attain real-time adaptation capabilities. There is also a need to develop robots that are safe by design, with an emphasis on soft and lightweight structures as well as control and planning methodologies based on multisensory feedback.",project-academic
10.1145/3041008.3041009,2017-07-25,a,,predicting exploitation of disclosed software vulnerabilities using open source data," Each year, thousands of software vulnerabilities are discovered and reported to the public. Unpatched known vulnerabilities are a significant security risk. It is imperative that software vendors quickly provide patches once vulnerabilities are known and users quickly install those patches as soon as they are available. However, most vulnerabilities are never actually exploited. Since writing, testing, and installing software patches can involve considerable resources, it would be desirable to prioritize the remediation of vulnerabilities that are likely to be exploited. Several published research studies have reported moderate success in applying machine learning techniques to the task of predicting whether a vulnerability will be exploited. These approaches typically use features derived from vulnerability databases (such as the summary text describing the vulnerability) or social media posts that mention the vulnerability by name. However, these prior studies share multiple methodological shortcomings that inflate predictive power of these approaches. We replicate key portions of the prior work, compare their approaches, and show how selection of training and test data critically affect the estimated performance of predictive models. The results of this study point to important methodological considerations that should be taken into account so that results reflect real-world utility.",project-academic
10.1093/HUMREP/DEAA013,2020-04-28,a,Oxford Academic,development of an artificial intelligence based assessment model for prediction of embryo viability using static images captured by optical light microscopy during ivf," Study question None Can an artificial intelligence (AI)-based model predict human embryo viability using images captured by optical light microscopy? None Summary answer None We have combined computer vision image processing methods and deep learning techniques to create the non-invasive Life Whisperer AI model for robust prediction of embryo viability, as measured by clinical pregnancy outcome, using single static images of Day 5 blastocysts obtained from standard optical light microscope systems. None What is known already None Embryo selection following IVF is a critical factor in determining the success of ensuing pregnancy. Traditional morphokinetic grading by trained embryologists can be subjective and variable, and other complementary techniques, such as time-lapse imaging, require costly equipment and have not reliably demonstrated predictive ability for the endpoint of clinical pregnancy. AI methods are being investigated as a promising means for improving embryo selection and predicting implantation and pregnancy outcomes. None Study design, size, duration None These studies involved analysis of retrospectively collected data including standard optical light microscope images and clinical outcomes of 8886 embryos from 11 different IVF clinics, across three different countries, between 2011 and 2018. None Participants/materials, setting, methods None The AI-based model was trained using static two-dimensional optical light microscope images with known clinical pregnancy outcome as measured by fetal heartbeat to provide a confidence score for prediction of pregnancy. Predictive accuracy was determined by evaluating sensitivity, specificity and overall weighted accuracy, and was visualized using histograms of the distributions of predictions. Comparison to embryologists' predictive accuracy was performed using a binary classification approach and a 5-band ranking comparison. None Main results and the role of chance None The Life Whisperer AI model showed a sensitivity of 70.1% for viable embryos while maintaining a specificity of 60.5% for non-viable embryos across three independent blind test sets from different clinics. The weighted overall accuracy in each blind test set was >63%, with a combined accuracy of 64.3% across both viable and non-viable embryos, demonstrating model robustness and generalizability beyond the result expected from chance. Distributions of predictions showed clear separation of correctly and incorrectly classified embryos. Binary comparison of viable/non-viable embryo classification demonstrated an improvement of 24.7% over embryologists' accuracy (P = 0.047, n = 2, Student's t test), and 5-band ranking comparison demonstrated an improvement of 42.0% over embryologists (P = 0.028, n = 2, Student's t test). None Limitations, reasons for caution None The AI model developed here is limited to analysis of Day 5 embryos; therefore, further evaluation or modification of the model is needed to incorporate information from different time points. The endpoint described is clinical pregnancy as measured by fetal heartbeat, and this does not indicate the probability of live birth. The current investigation was performed with retrospectively collected data, and hence it will be of importance to collect data prospectively to assess real-world use of the AI model. None Wider implications of the findings None These studies demonstrated an improved predictive ability for evaluation of embryo viability when compared with embryologists' traditional morphokinetic grading methods. The superior accuracy of the Life Whisperer AI model could lead to improved pregnancy success rates in IVF when used in a clinical setting. It could also potentially assist in standardization of embryo selection methods across multiple clinical environments, while eliminating the need for complex time-lapse imaging equipment. Finally, the cloud-based software application used to apply the Life Whisperer AI model in clinical practice makes it broadly applicable and globally scalable to IVF clinics worldwide. None Study funding/competing interest(s) None Life Whisperer Diagnostics, Pty Ltd is a wholly owned subsidiary of the parent company, Presagen Pty Ltd. Funding for the study was provided by Presagen with grant funding received from the South Australian Government: Research, Commercialisation and Startup Fund (RCSF). 'In kind' support and embryology expertise to guide algorithm development were provided by Ovation Fertility. J.M.M.H., D.P. and M.P. are co-owners of Life Whisperer and Presagen. Presagen has filed a provisional patent for the technology described in this manuscript (52985P pending). A.P.M. owns stock in Life Whisperer, and S.M.D., A.J., T.N. and A.P.M. are employees of Life Whisperer.",project-academic
10.1038/S41586-021-03213-Y,2021-02-04,a,Nature Publishing Group,bayesian reaction optimization as a tool for chemical synthesis," Reaction optimization is fundamental to synthetic chemistry, from optimizing the yield of industrial processes to selecting conditions for the preparation of medicinal candidates1. Likewise, parameter optimization is omnipresent in artificial intelligence, from tuning virtual personal assistants to training social media and product recommendation systems2. Owing to the high cost associated with carrying out experiments, scientists in both areas set numerous (hyper)parameter values by evaluating only a small subset of the possible configurations. Bayesian optimization, an iterative response surface-based global optimization algorithm, has demonstrated exceptional performance in the tuning of machine learning models3. Bayesian optimization has also been recently applied in chemistry4–9; however, its application and assessment for reaction optimization in synthetic chemistry has not been investigated. Here we report the development of a framework for Bayesian reaction optimization and an open-source software tool that allows chemists to easily integrate state-of-the-art optimization algorithms into their everyday laboratory practices. We collect a large benchmark dataset for a palladium-catalysed direct arylation reaction, perform a systematic study of Bayesian optimization compared to human decision-making in reaction optimization, and apply Bayesian optimization to two real-world optimization efforts (Mitsunobu and deoxyfluorination reactions). Benchmarking is accomplished via an online game that links the decisions made by expert chemists and engineers to real experiments run in the laboratory. Our findings demonstrate that Bayesian optimization outperforms human decisionmaking in both average optimization efficiency (number of experiments) and consistency (variance of outcome against initially available data). Overall, our studies suggest that adopting Bayesian optimization methods into everyday laboratory practices could facilitate more efficient synthesis of functional chemicals by enabling better-informed, data-driven decisions about which experiments to run. Bayesian optimization is applied in chemical synthesis towards the optimization of various organic reactions and is found to outperform scientists in both average optimization efficiency and consistency.",project-academic
10.1016/S0304-3975(97)00154-0,1998-02-20,a,Elsevier Science Publishers Ltd.,interactive foundations of computing," Abstract None None The claim that interactive systems have richer behavior than algorithms is surprisingly easy to prove. Turing machines cannot model interaction machines (which extend Turing machines with interactive input/output) because interaction is not expressible by a finite initial input string. Interaction machines extend the Chomsky hierarchy, are modeled by interaction grammars, and precisely capture fuzzy concepts like open systems and empirical computer science. Computable functions cannot model real-world behavior because functions are too strong an abstraction, sacrificing the ability to model time and other real-world properties to realize formal tractability. None Part I of this paper examines extensions to interactive models for algorithms, machines, grammars, and semantics, while Part II considers the expressiveness of different forms of interaction. Interactive identity machines are already more powerful than Turing machines, while noninteractive parallelism and distribution are algorithmic. The extension of Turing to interaction machines parallels that of the lambda to the pi calculus. Asynchronous and nonserializable interaction are shown to be more expressive than sequential interaction (multiple streams are more expressive than a single stream). None In Part III, it is shown that interaction machines cannot be described by sound and complete first-order logics (a form of Godel incompleteness), and that incompleteness is inherently necessary to realize greater expressiveness. In the final section the robustness of interactive models in expressing open systems, programming in the large, graphical user interfaces, and agent-oriented artificial intelligence is compared to the robustness of Turing machines. Less technical discussion of these ideas may be found in [25–27]. Applications of interactive models to coordination, objects and components, patterns and frameworks, software engineering, and AI are examined elsewhere [28,29]. None The propositions P1-P36 embody the principal claims, while observations 01 through 040 provide additional insights.",project-academic
,1999-07-31,p,Morgan Kaufmann Publishers Inc.,agent based computing promise and perils," Agent-based computing represents an exciting new synthesis both for Artificial Intelligence (AI) and, more generally, Computer Science. It has the potential to significantly improve the theory and the practice of modelling, designing, and implementing complex systems. Yet, to date, there has been little systematic analysis of what makes an agent such an appealing and powerful conceptual model. Moreover, even less effort has been devoted to exploring the inherent disadvantages that stem from adopting an agent-oriented view. Here both sets of issues are explored. The standpoint of this analysis is the role of agent-based software in solving complex, real-world problems. In particular, it will be argued that the development of robust and scalable software systems requires autonomous agents that can complete their objectives while situated in a dynamic and uncertain environment, that can engage in rich, high-level social interactions, and that can operate within flexible organisational structures.",project-academic
10.1007/S10549-016-3872-2,2016-06-23,a,Springer US,learning from social media utilizing advanced data extraction techniques to understand barriers to breast cancer treatment," Past examinations of breast cancer treatment barriers have typically included registry, claims-based, and smaller survey studies. We examined treatment barriers using a novel, comprehensive, social media analysis of online, candid discussions about breast cancer. Using an innovative toolset to search postings on social networks, message boards, patient communities, and topical sites, we performed a large-scale qualitative analysis. We examined the sentiments and barriers expressed about breast cancer treatments by Internet users during 1 year (2/1/14–1/31/15). We categorized posts based on thematic patterns and examined trends in discussions by race/ethnicity (white/black/Hispanic) when this information was available. We identified 1,024,041 unique posts related to breast cancer treatment. Overall, 57 % of posts expressed negative sentiments. Using machine learning software, we assigned treatment barriers for 387,238 posts (38 %). Barriers included emotional (23 % of posts), preferences and spiritual/religious beliefs (21 %), physical (18 %), resource (15 %), healthcare perceptions (9 %), treatment processes/duration (7 %), and relationships (7 %). Black and Hispanic (vs. white) users more frequently reported barriers related to healthcare perceptions, beliefs, and pre-diagnosis/diagnosis organizational challenges and fewer emotional barriers. Using a novel analysis of diverse social media users, we observed numerous breast cancer treatment barriers that differed by race/ethnicity. Social media is a powerful tool, allowing use of real-world data for qualitative research, capitalizing on the rich discussions occurring spontaneously online. Future research should focus on how to further employ and learn from this type of social intelligence research across all medical disciplines.",project-academic
10.1145/2157689.2157843,2012-03-05,p,ACM,human agent robot teamwork," Teamwork has become a widely accepted metaphor for describing the nature of multi-robot and multi-agent cooperation. By virtue of teamwork models, team members attempt to manage general responsibilities and commitments to each other in a coherent fashion that both enhances performance and facilitates recovery when unanticipated problems arise. Whereas early research on teamwork focused mainly on interaction within groups of autonomous agents or robots, there is a growing interest in leveraging human participation effectively. Unlike autonomous systems designed primarily to take humans out of the loop, many important applications require people, agents, and robots to work together in close and relatively continuous interaction. For software agents and robots to participate in teamwork alongside people in carrying out complex real-world tasks, they must have some of the capabilities that enable natural and effective teamwork among groups of people. Just as important, developers of such systems need tools and methodologies to assure that such systems will work together reliably and safely, even when they have been designed independently.The purpose of the HART workshop is to explore theories, methods, and tools in support of humans, agents and robots working together in teams. Position papers that combine findings from fields such as computer science, artificial intelligence, cognitive science, anthropology, social and organizational psychology, human-computer interaction to address the problem of HART are strongly encouraged. The workshop will formulate perspectives on the current state-of-the-art, identify key challenges and opportunities for future studies, and promote community-building among researchers and practitioners.The workshop will be structured around four two-hour sessions on themes relevant to HART. Each session will consist of presentations and questions on selected position papers, followed by a whole-group discussion of the current state-of-the-art and the key challenges and research opportunities relevant to the theme. During the final hour, the workshop organizers will facilitate a discussion to determine next steps. The workshop will be deemed a success when collaborative scientific projects for the coming year are defined, and publication venues are explored. For example, results from the most recent HART workshop (Lorentz Center, Leiden, The Netherlands, December 2010) will be reflected in a special issue of IEEE Intelligent Systems on HART that is slated to appear in January/February 2012.",project-academic
,2009-12-28,b,Springer-Verlag,intelligence and security informatics for international security information sharing and data mining," On September 11, 2001 the World became completely cognizant of the security challenges it faces on an international scale. With this awareness a commitment has come from the scientific, engineering, and health communities to help meet the world meet an array of security challenges. From these activities the science of ""Intelligence and Security Informatics,"" is emerging, which will influence a new generation of policy makers, practitioners, researchers, and students. INTELLIGENCE AND SECURITY INFORMATICS FOR INTERNATIONAL SECURITY: InformationSharing and Data Mining presents a systematic national security research framework, and within this context, the book discusses IT technical components, and directions. The book synthesizes the research in the field, focusing on information integration and data mining in particular. Integrated in the discussion are a number of real-world case studies illustrating how security technologies are developed and how they can be applied in critical law enforcement, emergency response, intelligence analysis, and terrorism contexts. The book reflects a decade of leading-edge research on intelligence and security informatics from the Artificial Intelligence Laboratory and the NSF COPLINK Center for Homeland Security Information Technology Research. Working in parallel with the research has been its application in real-world community situations by the centers director and the books author, Dr. Hsinchun Chen. The books audience is wide and includes the following: (1) college professors, research scientists, graduate students, and select undergraduate juniors and seniors in computer science, information management, information science, and other related public safety, intelligence analysis, and terrorism research disciplines; (2) researchers, analysts, and policy makers in federal departments, national laboratories, intelligence community, public safety and law enforcement agencies, and the emergency response community; and (3) consultants and practitioners in IT hardware, communication, and software companies, consulting firms, and defence contractors of varying sizes and countries.",project-academic
10.1038/S42256-019-0139-8,2020-01-01,a,Nature Publishing Group,validity of machine learning in biology and medicine increased through collaborations across fields of expertise," Machine learning (ML) has become an essential asset for the life sciences and medicine. We selected 250 articles describing ML applications from 17 journals sampling 26 different fields between 2011 and 2016. Independent evaluation by two readers highlighted three results. First, only half of the articles shared software, 64% shared data and 81% applied any kind of evaluation. Although crucial for ensuring the validity of ML applications, these aspects were met more by publications in lower-ranked journals. Second, the authors’ scientific backgrounds highly influenced how technical aspects were addressed: reproducibility and computational evaluation methods were more prominent with computational co-authors; experimental proofs more with experimentalists. Third, 73% of the ML applications resulted from interdisciplinary collaborations comprising authors from at least two of the three disciplines: computational sciences, biology, and medicine. The results suggested collaborations between computational and experimental scientists to generate more scientifically sound and impactful work integrating knowledge from both domains. Although scientifically more valid solutions and collaborations involving diverse expertise did not correlate with impact factors, such collaborations provide opportunities to both sides: computational scientists are given access to novel and challenging real-world biological data, increasing the scientific impact of their research, and experimentalists benefit from more in-depth computational analyses improving the technical correctness of work. Applications of machine learning in the life sciences and medicine require expertise in computational methods and in scientific subject matter. The authors surveyed articles in the life sciences that included machine learning applications, and found that interdisciplinary collaborations increased the scientific validity of published research.",project-academic
,2017-11-20,a,,the doctor just won t accept that," Calls to arms to build interpretable models express a well-founded discomfort with machine learning. Should a software agent that does not even know what a loan is decide who qualifies for one? Indeed, we ought to be cautious about injecting machine learning (or anything else, for that matter) into applications where there may be a significant risk of causing social harm. However, claims that stakeholders ""just won't accept that!"" do not provide a sufficient foundation for a proposed field of study. For the field of interpretable machine learning to advance, we must ask the following questions: What precisely won't various stakeholders accept? What do they want? Are these desiderata reasonable? Are they feasible? In order to answer these questions, we'll have to give real-world problems and their respective stakeholders greater consideration.",project-academic
,2021-10-28,a,,from machine learning to robotics challenges and opportunities for embodied intelligence," Machine learning has long since become a keystone technology, accelerating science and applications in a broad range of domains. Consequently, the notion of applying learning methods to a particular problem set has become an established and valuable modus operandi to advance a particular field. In this article we argue that such an approach does not straightforwardly extended to robotics -- or to embodied intelligence more generally: systems which engage in a purposeful exchange of energy and information with a physical environment. In particular, the purview of embodied intelligent agents extends significantly beyond the typical considerations of main-stream machine learning approaches, which typically (i) do not consider operation under conditions significantly different from those encountered during training; (ii) do not consider the often substantial, long-lasting and potentially safety-critical nature of interactions during learning and deployment; (iii) do not require ready adaptation to novel tasks while at the same time (iv) effectively and efficiently curating and extending their models of the world through targeted and deliberate actions. In reality, therefore, these limitations result in learning-based systems which suffer from many of the same operational shortcomings as more traditional, engineering-based approaches when deployed on a robot outside a well defined, and often narrow operating envelope. Contrary to viewing embodied intelligence as another application domain for machine learning, here we argue that it is in fact a key driver for the advancement of machine learning technology. In this article our goal is to highlight challenges and opportunities that are specific to embodied intelligence and to propose research directions which may significantly advance the state-of-the-art in robot learning.",project-academic
10.1021/ACSMACROLETT.7B00228,2017-09-15,a,American Chemical Society,polymer informatics opportunities and challenges," We are entering an era where large volumes of scientific data, coupled with algorithmic and computational advances, can reduce both the time and cost of developing new materials. This emerging field known as materials informatics has gained acceptance for a number of classes of materials, including metals and oxides. In the particular case of polymer science, however, there are important challenges that must be addressed before one can start to deploy advanced machine learning approaches for designing new materials. These challenges are primarily related to the manner in which polymeric systems and their properties are reported. In this viewpoint, we discuss the opportunities and challenges for making materials informatics as applied to polymers, or equivalently polymer informatics, a reality.",project-academic
10.1007/S11606-014-3018-3,2015-02-01,a,Springer US,value added medical education engaging future doctors to transform health care delivery today," Medical student education typically aims to increase the supply of future physicians while ignoring current problems in health care delivery. This focus on the future fails to address the pressing challenges of today. These challenges include a shrinking ratio of adult primary care clinicians to population, a failure to address population health and the need for cost reductions, and medical training that prepares students to deal with patients, but not with teams and systems.1 Change is desperately needed to translate education into better health outcomes for all Americans today.

To achieve this change, significant reforms are needed in both practice redesign and medical education. One proposed solution to the capacity issue in primary care is to “share the care” with non-clinician health care team members and learners.1 Within this paradigm, an empowered team comprising of clinicians, non-clinicians (nurses, medical assistants, health educators), and learners share responsibilities so that all team members, operating at their maximum potential, contribute to the health of their patient panel.

We believe that sharing the care goes beyond addressing the capacity-demand problem and may serve as an ideal starting point for building a new vision for the future of medical education. What if every medical student starting in year one is embedded in a primary health care team and engaged in meaningful roles of providing care appropriate to their stage of training? What if, instead of just shadowing a physician in clinic, early medical students serve as health coaches, provide motivational interviewing to assist patients with behavior change, and participate in quality improvement projects? What if, even if they do not have the diagnostic or treatment skills to provide formal patient care yet, early medical students are trained to do population health management and can reach out to patients overdue for routine preventive (pap smears, colonoscopies, mammograms) and chronic care services (foot and eye exams for diabetics)? This model of medical training is not only aligned with many of the recommendations of the Carnegie Foundation report2—outpatient-focused, team-based, and patient-centered—but can add valuable capacity to our health care delivery system without changing the current infrastructure of academic health centers.

We call this “value-added medical education,” where powerful experiential learning experiences can also add value and capacity to our health care delivery system. This can be achieved by training and involving medical students in targeted patient care tasks. Students are eager to engage in care and take on responsibilities as part of the health care team in ways that do not generate duplicative work or consume additional energy from the clinical faculty. The underlying principles of value-added medical education are entirely compatible with the Institute of Medicine’s framework for a Learning Health System, and has been championed by leaders like Thomas Bodenheimer,1 Kevin Grumbach,3 and others who have already piloted a number of innovative programs that combine education and care in creative and synergistic ways.

At the University of California, San Francisco, where much of the pioneering work on value-added medical education has been done, students and faculty members are working together to reboot their curriculum by creating authentic workplace learning experiences that leverage the talents and commitments of every student to add value to the care of patients today. Other schools are also testing new methods of providing real world opportunities for trainees to participate in improving the quality of the health systems in which they work and study. The Health Professions Education Collaborative, sponsored by the Institute for Healthcare Improvement, helps drive curricular changes that promote team-based continuous quality improvement initiatives at 16 academic health centers.

Stanford University School of Medicine is also embedding some first year medical students in community health centers where they provide care through health coaching, motivational interviewing, and patient education delivered via longitudinal primary care community partnerships. Early medical students can participate in a patient navigator program to guide patients hospitalized with congestive heart failure through the discharge process, make follow-up telephone calls after they leave the hospital, and provide a supervised home visit to ensure patient safety and reduce preventable readmissions. In our free clinics, students are serving as referral coordinators and insurance counselors to educate uninsured patients about the Affordable Care Act and place them in medical homes as part of a bridge-to-care initiative. A scribe program designed to turn early trainees into clinical documentation experts has the potential to change our primary care clinics into high-performing practices, while equipping students with the technological skills needed to practice medicine in the electronic medical record era. All these programs are similar in that they balance the often conflicting missions of education and patient care by merging these two objectives into a single pursuit.

We are not the first to advocate for engaging medical students in systems improvement as an essential part of education reform. We echo the call of leaders like Catherine Lucey,4 who has written for all medical schools to “explicitly commit to implementing educational programs that measurably improve health care today while educating the physicians of tomorrow.” We add to her charge by arguing that value-added educational redesign is not only desirable to produce 21st century physicians able to achieve better health outcomes for the American people, but necessary as an immediate solution to address the growing imbalance between population demands for medical care and our capacity to provide care. We need solutions that can work right now, not decades into the future. There are more than 80,000 students in U.S. medical schools today.5 Harnessing their collective engagement and untapped capacity for patient care would be transformative.

Transforming this dream into reality will take leadership, redeployment of resources, and curricular redesign. Leadership is needed to articulate the vision, generate urgency for change, shift resources, provide faculty development, and build partnerships with new clinical training sites. This will require some redirection of existing resources currently devoted to early clinical training, including patient communication and physical exam courses. Value-added education programs have the potential to fuel clinical partners’ interest in having early medical students at their sites, since they are actively engaged in patient care tasks that contribute to the health of their patient populations. These partners may need to adjust workflow and space to accommodate their new medical student team members. At the same time, early clinical training courses will need to prepare their students with the knowledge and skills to take on meaningful responsibilities as part of the health care team, including motivational interviewing, health coaching, and population health management. Medical school course directors, clinical faculty, and community preceptors will need to communicate in order to enable integration between course knowledge and real world skills. Faculty and staff development will be needed in order to reimagine the clinical experience for students and provide the clinic staff with skills to actively involve students in their daily work. Curricular innovation and redesign will be essential to offer opportunity space for these new experiences and skills. However, none of these factors is insurmountable, especially in light of the social good derived from student learning and engagement in improving health care delivery.

A vision for a new kind of medical education is taking shape. We believe that the principles of value-added medical education should and will play an important role in this revolution. These principles include: (1) early integrated workplace learning for all medical students, (2) an interprofessional team-based quality improvement culture to promote understanding and respect of non-clinician providers, (3) collaborative and data driven population health management, (4) optimization of professional roles that are learner-centered and continuously adjusted to changing stages of development, and (5) the fusion of robust experiential learning experiences with the delivery of high-performing, patient-centered primary care. The challenge now is to make value-added training a standard part of the curriculum in every medical school. We can do this by taking lessons learned from successful pilots and implementing them widely with the support of professional organizations and accrediting bodies. It is time to share the care with our future colleagues.",project-academic
,2020-02-01,a,,bail or jail judicial versus algorithmic decision making in the pretrial system," To date, there are approximately 60 risk assessment tools deployed in the criminal justice system. These tools aim to differentiate between low-, medium-, and high-risk defendants and to increase the likelihood that only those who pose a risk to public safety or are likely to flee are detained. Proponents of actuarial tools claim that these tools meant to eliminate human biases and to rationalize the decision-making process by summarizing all relevant information in a more efficient way than the human brain. Opponents of such tools fear that in the name of science, actuarial tools reinforce human biases, harm defendants’ rights and increase racial disparities in the system. The gap between the two camps has widened in the last few years, and policy makers are torn between the promises of technology to contribute to a more just system, and a growing movement that calls for the abolishment of the use of actuarial risk assessment tools in general, and machine learning-based tools in particular.

This paper examines the role that the technology play in this debate, and whether deploying AI in existing risk assessment tools realizes the fears hyped in the media or improves our criminal justice system? It focuses on the pretrial stage and examines in depth the seven most commonly used tools. Five of these tools are based on traditional regression analysis, and two have a certain machine-learning component. The paper concludes that, classifying pretrial risk assessment tools as AI-based tools creates the impression that sophisticated robots are taking over the courts and pushing judges from their jobs, but this is far from reality. Despite the hype, there are more similarities than differences between tools based on traditional regression analysis and tools based on machine learning. Robots have a long way to go before they can replace judges, and this is not the solution that this paper is arguing for. The long list of policy recommendations discussed in the last chapter, highlight the extensive work that needs to be done to ensure that risk assessment tools are both accurate and fair toward all members of society. These recommendations are beneficial regardless of the technique used; and especial attention is dedicated to assessing how machine learning would impact those recommendations. For example, the paper argues that detailing each one of the factors used in the tools, to include multiple options to choose from (not juts yes or no question), will be useful for both regression analysis and machine learning, but if machine learning is used, the final score could be more personalized and meaningful because of the ability to zoom in on the unique details of the individual case.",project-academic
10.7916/STLR.V21I2.6838,2020-07-29,a,,bail or jail judicial versus algorithmic decision making in the pretrial system," To date, there are approximately sixty risk assessment tools deployed in the criminal justice system. These tools aim to differentiate between low-, medium-, and high-risk defendants and to increase the likelihood that only those who pose a risk to public safety or who are likely to flee are detained. Proponents of actuarial tools claim that these tools are meant to eliminate human biases and to rationalize the decision-making process by summarizing all relevant information in a more efficient way than can the human brain. Opponents of such tools fear that in the name of science, actuarial tools reinforce human biases, harm defendants’ rights, and increase racial disparities in the system. The gap between the two camps has widened in the last few years. Policymakers are torn between the promise of technology to contribute to a more just system and a growing movement that calls for the abolishment of the use of actuarial risk assessment tools in general and the use of machine learning-based tools in particular.
This paper examines the role that technology plays in this debate and examines whether deploying artificial intelligence (“AI”) in existing risk assessment tools realizes the fears emphasized by opponents of automation or improves our criminal justice system. It focuses on the pretrial stage and examines in depth the seven most commonly used tools. Five of these tools are based on traditional regression analysis, and two have a machine-learning component. This paper concludes that classifying pretrial risk assessment tools as AI-based tools creates the impression that sophisticated robots are taking over the courts and pushing judges from their jobs, but that impression is far from reality. Despite the hype, there are more similarities than differences between tools based on traditional regression analysis and tools based on machine learning. Robots have a long way to go before they can replace judges, and this paper does not argue for replacement. The long list of policy recommendations discussed in the last chapter highlights the extensive work that needs to be done to ensure that risk assessment tools are both accurate and fair toward all members of society. These recommendations apply regardless of whether machine learning or regression analysis is used. Special attention is paid to assessing how machine learning would impact those recommendations. For example, this paper argues that carefully detailing each of the factors used in the tools and including multiple options to choose from (i.e., not just binary “yes-or-no” questions) will be useful for both regression analysis and machine learning. However, machine learning would likely lead to more personalized and meaningful scoring of criminal defendants because of the ability of machine learning techniques to “zoom in” on the unique details of each individual case.",project-academic
10.1016/J.IJROBP.2021.07.534,2021-11-01,a,Elsevier BV,novel view x ray projection synthesis through geometry integrated deep learning," PURPOSE/OBJECTIVE(S) X-ray imaging is a widely used approach to view the internal structure of a subject for clinical diagnosis, image-guided interventions and decision-making. The X-ray projections acquired at different view angles provide complementary information of patient's anatomy and are required for stereoscopic or volumetric imaging of the subject. In reality, obtaining multiple-view projections inevitably increases radiation dose and complicates clinical workflow. Here we investigate a strategy of obtaining the X-ray projection image at a novel view angle from a given projection image at a specific view angle to alleviate the need for actual projection measurement. Specifically, we propose a deep learning-based geometry-integrated projection synthesis framework for generating novel-view X-ray projections. MATERIALS/METHODS We for the first time investigate the novel-view projection synthesis problem for X-ray imaging. We propose a deep learning-based geometry-integrated projection synthesis model to generate novel-view X-ray projections through feature disentanglement and geometry transformation. The proposed deep learning model extracts geometry and texture features from a source-view projection, and then conducts geometry transformation on the geometry features to accommodate the change of view angle. Then the X-ray projection in target view is synthesized from the transformed geometry and shared texture features via an image generator. We validate the proposed approach by experimenting on one-to-one and multi-to-multi X-ray projection synthesis using lung imaging cases across various patients. We conduct experiments on a public dataset containing 1018 lung CT images, where 80% and 20% data are used for training and testing. The projection images are digitally produced from CT images using geometry consistent with a clinical on-board cone-beam CT system for radiation therapy. RESULTS We deploy the trained model on the held-out testing set for novel-view projection synthesis. The generated results are compared with ground truth qualitatively and quantitatively. For AP-to-lateral and lateral-to-AP view synthesis, average NRMSE / SSIM / PSNR values over all testing data are 0.185 / 0.880 / 22.646 and 0.173 / 0.911 / 23.908, respectively. Promising results are also obtained for multi-to-multi view synthesis. By visualizing the generated projections, we observe the proposed model can generate images closely to the targets despite the various anatomic structures of different patients, indicating the potential of the proposed model for X-ray projection synthesis. CONCLUSION This work investigates the synthesis of novel-view X-ray projections and presents a robust model combining the deep learning and geometry transformation. It is shown that the generated X-ray projections reveal the internal anatomy distribution from new viewpoints, which potentially provides a new paradigm for volumetric imaging with substantially reduced efforts in data acquisition.",project-academic
,2009-03-01,c,AU Press,a model for framing mobile learning," The Framework for the Rational Analysis of Mobile Education (FRAME) model describes mobile learning as a process resulting from the convergence of mobile technologies, human learning capacities, and social interaction. It addresses contemporary pedagogical issues of information overload, knowledge navigation, and collaboration in learning. This model is useful for guiding the development of future mobile devices, the development of learning materials, and the design of teaching and learning strategies for mobile education. Introduction Research in the fi eld of mobile learning is on the rise. Visionaries believe mobile learning offers learners greater access to relevant information, reduced cognitive load, and increased access to other people and systems. It may be argued that wireless, networked mobile devices can help shape culturally sensitive learning experiences and the means to cope with the growing amount 066897_Book.indb 25 3/10/09 9:02:46 AM 26 Marguerite L. Koole of information in the world. Consider, for a moment, an individual who is learning English. There is a myriad of available resources on grammar, vocabulary, and idioms; some resources are accurate and useful; others less so. Equipped with a mobile device, the learner can choose to consult a web page, access audio or video tutorials, send a query via text message to a friend, or phone an expert for practice or guidance. She may use one or several of these techniques. But, how can such a learner take full advantage of the mobile experience? How can practitioners design materials and activities appropriate for mobile access? How can mobile learning be effectively implemented in both formal and informal learning? The Framework for the Rational Analysis of Mobile Education (FRAME) model offers some insights into these issues. The FRAME model takes into consideration the technical characteristics of mobile devices as well as social and personal aspects of learning (Koole 2006). This model refers to concepts similar to those as found in psychological theories such as Activity Theory (Kaptelinin and Nardi 2006) – especially pertaining to Vygotsky’s (1978) work on mediation and the zone of proximal development. However, the FRAME model highlights the role of technology beyond simply an artefact of “cultural-historic” development. In this model, the mobile device is an active component in equal footing to learning and social processes. This model also places more emphasis on constructivism: the word rational refers to the “belief that reason is the primary source of knowledge and that reality is constructed rather than discovered” (Smith and Ragan 1999, 15). The FRAME model describes a mode of learning in which learners may move within different physical and virtual locations and thereby participate and interact with other people, information, or systems – anywhere, anytime. The FRAME Model In the FRAME model, mobile learning experiences are viewed as existing within a context of information. Collectively and individually, learners consume and create information. The interaction with information is mediated through technology. It is through the complexities of this kind of interaction that information becomes meaningful and useful. Within this context of information, the FRAME model is represented by a Venn diagram in which three aspects intersect (Figure 1). 2 2. The nomenclature used in the Venn diagram has been altered from previous publications. Previously the device aspect was called the device usability aspect, the device usability intersection was called the learner context intersection, and the social technology intersection was called the social computing intersection. 066897_Book.indb 26 3/10/09 9:02:47 AM A Model for Framing Mobile Learning 27 The three circles represent the device (D), learner (L), and social (S) aspects. The intersections where two circles overlap contain attributes that belong to both aspects. The attributes of the device usability (DL) and social technology (DS) intersections describe the affordances of mobile technology (Norman 1999). The intersection labelled interaction learning (LS) contains instructional and learning theories with an emphasis on social constructivism. All three aspects overlap at the primary intersection (DLS) in the centre of the Venn diagram. Hypothetically, the primary intersection, a convergence of all three aspects, defi nes an ideal mobile learning situation. By assessing the degree to which all the areas of the FRAME model are utilized within a mobile learning situation, practitioners may use the model to design more effective mobile learning experiences. (DLS) Mobile Learning (DS) Social Technology (LS) Interaction Learning (D) Device Aspect (DL) Device Usability (L) Learner Aspect (S) Social Aspect Information Context FIGURE 1 The FRAME Model 066897_Book.indb 27 3/10/09 9:02:47 AM 28 Marguerite L. Koole Aspects Device Aspect (D) The device aspect (D) refers to the physical, technical, and functional characteristics of a mobile device (Table 1). The physical characteristics include input and output capabilities as well as processes internal to the machine such as storage capabilities, power, processor speed, compatibility, and expandability. These characteristics result from the hardware and software design of the devices and have a signifi cant impact on the physical and psychological comfort levels of the users. It is important to assess these characteristics because mobile learning devices provide the interface between the mobile learner and the learning task(s) as described later in the device usability intersection (DL). TABLE 1 The Device Aspect Criteria Examples & Concepts Comments Physical Characteristics Size, weight, composition, placement of buttons and keys, right/left handed requirements, one or two-hand operability1. Affects how the user can manipulate the device and move around while using the device. Input Capabilities Keyboard, mouse, light pen, pen/stylus, touch screen, trackball, joystick, touchpad, hand/foot control, voice recognition1. Allows selection and positioning of objects or data on the device1. Mobile devices are often criticized for inadequate input mechanisms. Output Capabilities Monitors, speakers or any other visual, auditory, and tactile output mechanisms. Allows the human body to sense changes in the device; allows the user to interact with the device. Mobile devices are often criticized for limitations in output mechanisms such as small",project-academic
10.1145/2696454.2696491,2015-03-02,p,ACM,how robot verbal feedback can improve team performance in human robot task collaborations," We detail an approach to planning effective verbal feedback during pairwise human-robot task collaboration. The approach is motivated by social science literature as well as existing work in robotics and is applicable to a variety of task scenarios. It consists of a dynamic, synthetic task implemented in an augmented reality environment. The result is combined robot task control and speech production, allowing the robot to actively participate and communicate with its teammate. A user study was conducted to experimentally validate the efficacy of the approach on a task in which a single user collaborates with an autonomous robot. The results demonstrate that the approach is capable of improving both objective measures of team performance and the user’s subjective evaluation of both the task and the robot as a teammate. Categories and Subject Descriptors H.1.2 [Models and Principles]: User/Machine Systems—human factors, software psychology; H.5.2 [Information Interfaces and Presentation]: User Interfaces—evaluation/methodology, natural language; I.2.9 [Artificial Intelligence]: Robotics—operator interfaces",project-academic
10.2196/19867,2020-07-28,a,"JMIR Publications Inc., Toronto, Canada",pediatric mental and behavioral health in the period of quarantine and social distancing with covid 19," The coronavirus disease (COVID-19) pandemic has spread rapidly throughout the world and has had a long-term impact. The pandemic has caused great harm to society and caused serious psychological trauma to many people. Children are a vulnerable group in this global public health emergency, as their nervous systems, endocrine systems, and hypothalamic-pituitary-adrenal axes are not well developed. Psychological crises often cause children to produce feelings of abandonment, despair, incapacity, and exhaustion, and even raise the risk of suicide. Children with mental illnesses are especially vulnerable during the quarantine and social distancing period. The inclusion of psychosocial support for children and their families are part of the health responses to disaster and disaster recovery. Based on the biopsychosocial model, some children may have catastrophic thoughts and be prone to experience despair, numbness, flashbacks, and other serious emotional and behavioral reactions. In severe cases, there may be symptoms of psychosis or posttraumatic stress disorder. Timely and appropriate protections are needed to prevent the occurrence of psychological and behavioral problems. The emerging digital applications and health services such as telehealth, social media, mobile health, and remote interactive online education are able to bridge the social distance and support mental and behavioral health for children. Based on the psychological development characteristics of children, this study also illustrates interventions on the psychological impact from the COVID-19 pandemic. Even though the world has been struggling to curb the influences of the pandemic, the quarantine and social distancing policies will have long-term impacts on children. Innovative digital solutions and informatics tools are needed more than ever to mitigate the negative consequences on children. Health care delivery and services should envision and implement innovative paradigms to meet broad well-being needs and child health as the quarantine and social distancing over a longer term becomes a new reality. Future research on children's mental and behavioral health should pay more attention to novel solutions that incorporate cutting edge interactive technologies and digital approaches, leveraging considerable advances in pervasive and ubiquitous computing, human-computer interaction, and health informatics among many others. Digital approaches, health technologies, and informatics are supposed to be designed and implemented to support public health surveillance and critical responses to children's growth and development. For instance, human-computer interactions, augmented reality, and virtual reality could be incorporated to remote psychological supporting service for children's health; mobile technologies could be used to monitor children's mental and behavioral health while protecting their individual privacy; big data and artificial intelligence could be used to support decision making on whether children should go out for physical activities and whether schools should be reopened. Implications to clinical practices, psychological therapeutic practices, and future research directions to address current effort gaps are highlighted in this study.",project-academic
10.1093/OXFORDHB/9780199942237.001.0001,2014-12-02,b,Oxford University Press,the oxford handbook of affective computing," Affective Computing is a growing multidisciplinary field encompassing computer science, engineering, psychology, education, neuroscience, and many other disciplines. It explores how affective factors influence interactions between humans and technology, how affect sensing and affect generation techniques can inform our understanding of human affect, and on the design, implementation, and evaluation of systems that intricately involve affect at their core. The Oxford Handbook of Affective Computing will help both new and experienced researchers identify trends, concepts, methodologies, and applications in this burgeouning field. The volume features 41 chapters divided into five main sections: history and theory, detection, generation, methodologies, and applications. Section One begins with a look at the makings of AC and a historical review of the science of emotion. Chapters discuss the theoretical underpinnings of AC from an interdisciplinary perspective involving the affective, cognitive, social, media, and brain sciences. Section Two focuses on affect detection or affect recognition, which is one of the most commonly investigated areas in AC. Section Three examines aspects of affect generation including the synthesis of emotion and its expression via facial features, speech, postures and gestures. Cultural issues in affect generation are also discussed. Section Four features chapters on methodological issues in AC research, including data collection techniques, multimodal affect databases, emotion representation formats, crowdsourcing techniques, machine learning approaches, affect elicitation techniques, useful AC tools, and ethical issues in AC. Finally, Section Five highlights existing and future applications of AC in domains such as formal and informal learning, games, robotics, virtual reality, autism research, healthcare, cyberpsychology, music, deception, reflective writing, and cyberpsychology.With chapters authored by world leaders in each area, The Oxford Handbook of Affective Computing is suitable for use as a textbook in undergraduate or graduate courses in AC, and will serve as a valuable resource for students, researchers, and practitioners across the globe.",project-academic
10.1186/S41239-020-00193-3,2020-12-01,a,SpringerOpen,prerequisites for artificial intelligence in further education identification of drivers barriers and business models of educational technology companies," The ongoing datafication of our social reality has resulted in the emergence of new data-based business models. This development is also reflected in the education market. An increasing number of educational technology (EdTech) companies are entering the traditional education market with data-based teaching and learning solutions, and they are permanently transforming the market. However, despite the current market dynamics, there are hardly any business models that implement the possibilities of Learning Analytics (LA) and Artificial Intelligence (AI) to create adaptive teaching and learning paths. This paper focuses on EdTech companies and the drivers and barriers that currently affect data-based teaching and learning paths. The results show that LA especially are integrated into the current business models of EdTech companies on three levels, which are as follows: basic Learning Analytics, Learning Analytics and algorithmic or human-based recommendations, and Learning Analytics and adaptive teaching and learning (AI based). The discourse analysis reveals a diametrical relationship between the traditional educational ideal and the futuristic idea of education and knowledge transfer. While the desire for flexibility and individualization drives the debate on AI-based learning systems, a lack of data sovereignty, uncertainty and a lack of understanding of data are holding back the development and implementation of appropriate solutions at the same time.",project-academic
10.1007/S11904-020-00490-6,2020-06-01,a,Springer US,artificial intelligence and machine learning for hiv prevention emerging approaches to ending the epidemic," We review applications of artificial intelligence (AI), including machine learning (ML), in the field of HIV prevention. ML approaches have been used to identify potential candidates for preexposure prophylaxis (PrEP) in healthcare settings in the USA and Denmark and in a population-based research setting in Eastern Africa. Although still in the proof-of-concept stage, other applications include ML with smartphone-collected and social media data to promote real-time HIV risk reduction, virtual reality tools to facilitate HIV serodisclosure, and chatbots for HIV education. ML has also been used for causal inference in HIV prevention studies. ML has strong potential to improve delivery of PrEP, with this approach moving from development to implementation. Development and evaluation of AI and ML strategies for HIV prevention may benefit from an implementation science approach, including qualitative assessments with end users, and should be developed and evaluated with attention to equity.",project-academic
,1997-09-01,b,Simon & Schuster Trade,life on the screen identity in the age of the internet," From the Publisher:
A Question of Identity
Life on the Screen is a fascinating and wide-ranging investigation of the impact of computers and networking on society, peoples' perceptions of themselves, and the individual's relationship to machines. Sherry Turkle, a Professor of the Sociology of Science at MIT and a licensed psychologist, uses Internet MUDs (multi-user domains, or in older gaming parlance multi-user dungeons) as a launching pad for explorations of software design, user interfaces, simulation, artificial intelligence, artificial life, agents, ""bots,"" virtual reality, and ""the on-line way of life."" 
Turkle's discussion of postmodernism is particularly enlightening. She shows how postmodern concepts in art, architecture, and ethics are related to concrete topics much closer to home, for example AI research (Minsky's ""Society of Mind"") and even MUDs (exemplified by students with X-window terminals who are doing homework in one window and simultaneously playing out several different roles in the same MUD in other windows). Those of you who have (like me) been turned off by the shallow, pretentious, meaningless paintings and sculptures that litter our museums of modern art may have a different perspective after hearing what Turkle has to say. 
This is a psychoanalytical book, not a technical one. However, software developers and engineers will find it highly accessible because of the depth of the author's technical understanding and credibility. Unlike most other authors in this genre, Turkle does not constantly jar the technically-literate reader with blatant errors or bogus assertions about how things work. Although I personally don't have time or patience for MUDs,view most of AI as snake-oil, and abhor postmodern architecture, I thought the time spent reading this book was an extremely good investment.",project-academic
10.7554/ELIFE.47994,2019-10-01,a,eLife Sciences Publications Limited,deepposekit a software toolkit for fast and robust animal pose estimation using deep learning," Studying animal behavior can reveal how animals make decisions based on what they sense in their environment, but measuring behavior can be difficult and time-consuming. Computer programs that measure and analyze animal movement have made these studies faster and easier to complete. These tools have also made more advanced behavioral experiments possible, which have yielded new insights about how the brain organizes behavior. Recently, scientists have started using new machine learning tools called deep neural networks to measure animal behavior. These tools learn to measure animal posture – the positions of an animal’s body parts in space – directly from real data, such as images or videos, without being explicitly programmed with instructions to perform the task. This allows deep learning algorithms to automatically track the locations of specific animal body parts in videos faster and more accurately than previous techniques. This ability to learn from images also removes the need to attach physical markers to animals, which may alter their natural behavior. Now, Graving et al. have created a new deep learning toolkit for measuring animal behavior that combines components from previous tools with the latest advances in computer science. Simple modifications to how the algorithms are trained can greatly improve their performance. For example, adding connections between layers, or ‘neurons’, in the deep neural network and training the algorithm to learn the full geometry of the body – by drawing lines between body parts – both enhance its accuracy. As a result of adding these changes, the new toolkit can measure an animal's pose from previously unseen images with high speed and accuracy, after being trained on just 100 examples. Graving et al. tested their model on videos of fruit flies, zebras and locusts, and found that, after training, it was able to accurately track the animals’ movements. The new toolkit has an easy-to-use software interface and is freely available for other scientists to use and build on. The new toolkit may help scientists in many fields including neuroscience and psychology, as well as other computer scientists. For example, companies like Google and Apple use similar algorithms to recognize gestures, so making those algorithms faster and more efficient may make them more suitable for mobile devices like smartphones or virtual-reality headsets. Other possible applications include diagnosing and tracking injuries, or movement-related diseases in humans and livestock.",project-academic
10.1101/164335,2017-07-21,a,Cold Spring Harbor Laboratory,an augmented reality social communication aid for children and adults with autism user and caregiver report of safety and lack of negative effects," Background: Interest has been growing in the use of augmented reality (AR) based social communication interventions in ASD, yet little is known about their safety or negative effects, particularly in head-worn digital smartglasses. Research to understand the safety of smartglasses in people with ASD is crucial given that these individuals may have altered sensory sensitivity, impaired verbal and non-verbal communication, and may experience extreme distress in response to changes in routine or environment.

Methods: None A sequential series of 18 children and adults (mean age 12.2-years-old, range 4.4-21.5-years-old) with clinically diagnosed ASD were given the opportunity to use the BPAS to learn emotion recognition, face-directed gaze, and managing transitions. Users and caregivers were interviewed about perceived negative effects of using BPAS, and had an opportunity to highlight any hardware or software design issues.

Objective: None The objective of this report was to assess the safety and negative effects of the Brain Power Autism System (BPAS), a novel AR smartglasses-based social communication aid for children and adults with ASD. BPAS uses emotion-based artificial intelligence and a smartglasses hardware platform that keeps users engaged in the social world by keeping users looking heads-up, unlike tablet- or phone-based apps.

Results: The majority of users were able to wear and use the BPAS (n=16, 89%). Caregivers reported no perceived negative effects in users during or after use of the BPAS. Three users each reported one temporary negative effect, these were eye strain, dizziness, and nasal discomfort due to the smartglasses nose stabilizers. Most users and caregivers did not have any design concerns regarding the smartglasses hardware or software (users 77.8%, caregivers 88.9%). The only reported design concern was that the smartglasses became warm to the touch during extended use.

Conclusions: It is important to conduct research to understand the feasibility and safety associated with new emerging technologies for vulnerable populations such as ASD. This report found no significant negative effects in using an AR smartglasses based social communication aid across a wide age range of children and adults with ASD. Further research is needed to explore the efficacy and longer term effects of such novel interventions.",project-academic
10.1101/164335,2017-07-22,a,Cold Spring Harbor Laboratory,report of safety and lack of negative effects of augmented reality social communication aid for children and adults with autism," Background None Interest has been growing in the use of augmented reality (AR) based social communication interventions in autism spectrum disorders (ASD), yet little is known about their safety or negative effects, particularly in head-worn digital smartglasses. Research to understand the safety of smartglasses in people with ASD is crucial given that these individuals may have altered sensory sensitivity, impaired verbal and non-verbal communication, and may experience extreme distress in response to changes in routine or environment. None Objective None The objective of this report was to assess the safety and negative effects of the Brain Power Autism System (BPAS), a novel AR smartglasses-based social communication aid for children and adults with ASD. BPAS uses emotion-based artificial intelligence and a smartglasses hardware platform that keeps users engaged in the social world by encouraging “heads-up” interaction, unlike tablet- or phone-based apps. None Methods None A sequential series of 18 children and adults (mean age 12.2-years-old, range 4.4-21.5-years-old) with clinically diagnosed ASD were given the opportunity to use BPAS to learn emotion recognition, face-directed gaze, and managing transitions. Users and caregivers were interviewed about perceived negative effects of using BPAS, and had an opportunity to highlight any hardware or software design issues. None Results None The majority of users were able to wear and use BPAS (n=16, 89%). Caregivers reported no perceived negative effects in users during or after use of BPAS. Two users reported temporary negative effects: eye strain, dizziness, and nasal discomfort due to the smartglasses nose stabilizers. Most users and caregivers did not have any design concerns regarding the smartglasses hardware or software (users 77.8%, caregivers 88.9%). The only reported design concern was that the smartglasses became warm to the touch during extended use. None Conclusions None It is important to conduct research to understand the feasibility and safety associated with new emerging technologies for vulnerable populations such as ASD. This report found no significant negative effects in using an AR smartglasses based social communication aid across a wide age range of children and adults with ASD. Further research is needed to explore the efficacy and longer-term effects of such novel interventions.",project-academic
,2000-12-29,b,,narrative as virtual reality immersion and interactivity in literature and electronic media," From the Publisher:
Is there a significant difference in attitude between immersion in a game and immersion in a movie or novel? What are the new possibilities for representation offered by the emerging technology of virtual reality? As Marie-Laure Ryan demonstrates in Narrative as Virtual Reality, the questions raised by new, interactive technologies have their precursors and echoes in pre-electronic literary and artistic traditions. Formerly a culture of immersive idealsgetting lost in a good book, for examplewe are becoming, Ryan claims, a culture more concerned with interactivity. Approaching the idea of virtual reality as a metaphor for total art, Narrative as Virtual Reality applies the concepts of immersion and interactivity to develop a phenomenology of reading. 
Ryan's analysis encompasses both traditional literary narratives and the new textual genres made possible by the electronic revolution of the past few years, such as hypertext, electronic poetry, interactive movies and drama, digital installation art, and computer role-playing games. Interspersed among the book's chapters are several ""interludes"" that focus exclusively on either key literary texts that foreshadow what we now call ""virtual reality,"" including those of Baudelaire, Huysmans, Ignatius de Loyola, Calvino, and science-fiction author Neal Stephenson, or recent efforts to produce interactive art forms, like the hypertext ""novel"" Twelve Blue, by Michael Joyce, and I'm Your Man, an interactive movie. As Ryan considers the fate of traditional narrative patterns in digital culture, she revisits one of the central issues in modern literary theorythe opposition between a presumably passive reading that is taken over by the world a text represents and an active, deconstructive reading that imaginatively participates in the text's creation. 

About the Author:
Marie-Laure Ryan is an independent scholar and former software consultant. She is the author of Possible Worlds, Artificial Intelligence, and Narrative Theory and the editor of Cyberspace Textuality: Computer Technology and Literary Theory.",project-academic
10.4301/S1807-1775201916001,2019-01-17,a,TECSI Laboratório de Tecnologia e Sistemas de Informação - FEA/USP,the future digital work force robotic process automation rpa," The Robotic Process Automation (RPA) is a new wave of future technologies. Robotic Process Automation is one of the most advanced technologies in the area of computers science, electronic and communications, mechanical engineering, and information technology. It is a combination of both hardware and software, networking and automation for doing things very simple. In this light, the research manuscript investigated the secondary data - which is available on google, academic and research databases. The investigation went for totally 6 months, i.e., 1-1-2018 to 30-6-2018. A very few empirical articles, white papers, blogs and were found RPA and came across to compose this research manuscript. This study is exploratory in nature because of the contemporary phenomenon. The keywords used in searching of the database were Robotic Process Automation, RPA, Robots, Artificial Intelligence, Blue Prism. The study finally discovered that Robots and Robotic Process Automation technologies are becoming compulsory as a part to do business operations in organizations across the globe. Robotic Process Automation can bring immediate value to the core business processes including employee payroll, employee status changes, new hire recruitment, and onboarding, accounts receivable and payable, invoice processing, inventory management, report creation, software installations, data migration, and vendor onboarding etc. to name a few applications. Besides, the Robotic Process Automation has abundant applications including healthcare and pharmaceuticals, financial services, outsourcing, retail, telecom, energy and utilities, real estate and FMCG and many more sectors. To put in the right place of RPA in business operations, their many allied technologies are working at the background level, artificial intelligence, machine learning, deep learning, data analytics, HR analytics, virtual reality (second life), home automation, blockchain technologies, 4D printing etc. Moreover, it covers the content of different start-ups companies and existing companies - their RPA applications used across the world. This manuscript will be a good guideline for the academicians, researchers, students, and practitioners to get an overall idea.",project-academic
10.3233/978-1-60750-050-6-93,2009-01-01,a,Stud Health Technol Inform,using reality mining to improve public health and medicine," We live our lives in digital networks. We wake up in the morning, check our e-mail, make a quick phone call, commute to work, buy lunch. Many of these transactions leave digital breadcrumbs--tiny records of our daily experiences. Reality mining, which pulls together these crumbs using statistical analysis and machine learning methods, offers an increasingly comprehensive picture of our lives, both individually and collectively, with the potential of transforming our understanding of ourselves, our organizations, and our society in a fashion that was barely conceivable just a few years ago. It is for this reason that reality mining was recently identified by Technology Review as one of ""10 emerging technologies that could change the world"". Many everyday devices provide the raw database upon which reality mining builds; sensors in mobile phones, cars, security cameras, RFID ('smart card') readers, and others, all allow for the measurement of human physical and social activity. Computational models based on such data have the potential to dramatically transform the arenas of both individual and community health. Reality mining can provide new opportunities with respect to diagnosis, patient and treatment monitoring, health services planning, surveillance of disease and risk factors, and public health investigation and disease control. Currently, the single most important source of reality mining data is the ubiquitous mobile phone. Every time a person uses a mobile phone, a few bits of information are left behind. The phone pings the nearest mobile-phone towers, revealing its location. The mobile phone service provider records the duration of the call and the number dialed. In the near future, mobile phones and other technologies will collect even more information about their users, recording everything from their physical activity to their conversational cadences. While such data pose a potential threat to individual privacy, they also offer great potential value both to individuals and communities. With the aid of data-mining algorithms, these data could shed light on individual patterns of behavior and even on the well-being of communities, creating new ways to improve public health and medicine. To illustrate, consider two examples of how reality mining may benefit individual health care. By taking advantage of special sensors in mobile phones, such as the microphone or the accelerometers built into newer devices such as Apple's iPhone, important diagnostic data can be captured. Clinical pilot data demonstrate that it may be possible to diagnose depression from the way a person talks--a depressed person tends to speak more slowly, a change that speech analysis software on a phone might recognize more readily than friends or family do. Similarly, monitoring a phone's motion sensors can also reveal small changes in gait, which could be an early indicator of ailments such as Parkinson's disease. Within the next few years reality mining will become more common, thanks in part to the proliferation and increasing sophistication of mobile phones. Many handheld devices now have the processing power of low-end desktop computers, and they can also collect more varied data, due to components such as GPS chips that track location. The Chief Technology Officer of EMC, a large digital storage company, estimates that this sort of personal sensor data will balloon from 10% of all stored information to 90% within the next decade. While the promise of reality mining is great, the idea of collecting so much personal information naturally raises many questions about privacy. It is crucial that behavior-logging technology not be forced on anyone. But legal statutes are lagging behind data collection capabilities, making it particularly important to begin discussing how the technology will and should be used. Therefore, an additional focus of this chapter will be the development of a legal and ethical framework concerning the data used by reality mining techniques.",project-academic
10.1101/2021.06.24.21259307,2021-06-27,a,Cold Spring Harbor Laboratory Press,technology in palliative care tip the identification of digital priorities for palliative care research using a modified delphi method," Background
Developments in digital health (describing technologies which use computing platforms, connectivity, software, and sensors for health care and related purposes) has the potential to transform the delivery of health and social care to help citizens manage their own health. Currently, we lack consensus about digital health research priorities in palliative care and lack theories about how these technologies might improve care outcomes. Global palliative care need is expected to increase due to the consequences of an ageing population; therefore, it is important for healthcare leaders to identify innovations to ensure that an increasingly frail population have appropriate access to palliative care services. Consequently, it is important to articulate research priorities as the first step to determine how we should allocate finite research resources to a field saturated with rapidly developing innovations. 

Aims
To identify research priority areas for digital health in palliative care.

Methods
We selected the digital health trends, most relevant to palliative care, from a list of emerging trends reported by the Future Today Institute. We conducted a modified Delphi process and consensus meeting with palliative care experts to identify research priorities. We used the views of public representatives to gain their perspectives of the agreed priorities.

Results
One hundred and three experts (representing 11 countries) participated in the 1st Delphi round. Fifty-five participated in the 2nd round (53% of 1st round). Eleven experts attended the final consensus meeting. We identified 16 priorities areas, which were summarised into eight themes. These themes were: big data, mobile devices, telehealth and telemedicine, virtual reality, artificial intelligence, the smart home, biotechnology and digital legacy.

Conclusions
The identified priorities in this paper represent a wide range of important emerging areas in field of digital health, personalised medicine, and data science. Human-centred design and robust governance systems should be considered in future research. It is important that the barriers and risks of using these technologies in palliative care are properly addressed to ensure that these tools are used meaningfully, wisely and safely and that do not cause unintentional harm.",project-academic
,2004-01-01,a,,origins of the ieee standard upper ontology," Research and applications in computer science are creating the need for precise definitions of the concepts that make up our world. Web searching is handicapped by the limitations of specifying search criteria in terms of keywords rather than concepts. Automated natural language understanding, both oral and written, is severely limited by the ambiguity of language. Software engineering is limited by the need for engineers to define concepts to model the world. Computers exist in a world similar to Europe in the Middle Ages in which tiny principalities each had their own language or dialect. Worse yet, these dialects are impoverished and they enable the computers to say only very specific and limited things. In order to enable continued progress in ecommerce and software integration, we must give computers a common language with a richness that more closely approaches that of human language. Integrating the meaning (or semantics) of databases and programs is crucial for creating software that is reliable and scalable. The use of ontologies to specify semantics is emerging as a promising technique for software integration. Creators of different components often assume they understand the terms in the same way. The reality is that is rarely the case. Even the bestdocumented code has implicit assumptions and ambiguity in the definition and usage of terms. Research in several areas including computer science, artificial intelligence, philosophy, library science and linguistics are helping to meet these needs. All these fields have experience with creating precise and standard descriptions and terminology for the things that make up our world. (Sowa 2000) Current research is hampered by several issues. Computer scientists and philosophers lack consensus in their communities for creating the very large, wide-coverage ontologies that are needed, although they have the necessary formal languages to do so. Librarians and linguists have the charter to create large ontologies but those ontologies have typically lacked the formal definitions needed for reasoning and decisionmaking. In fact, it is probably fair to claim that to date no group has taken full advantage of the vast body of historical work in this area One group that has developed a large formal ontology is Cycorp. However, ithas released only a small part to the public, retains proprietary rights to the vast bulk of their ontology, (Lenat 1995) and the contents of the ontology have not been subject to extensive peer review",project-academic
,2002-01-01,b,"Stylus Publishing, P.O. Box 605, Herndon, VA 20172-0605 ($29.95 plus $5 shipping). Tel: 800-232-0223 (Toll Free); Web site: http://styluspub.com.",the power of experiential learning a handbook for trainers and educators," Part 1 Unlocking new combinations: Introduction The tumblers An overview of the chapters Conclusion. Part 2 Exploring experiential learning: Introduction Defining experiential learning A meaningful experience Learning is personal Painful learning Detrimental experiential learning Learning from mistakes Formal versus experiential learning The lineage of experience learning Experience as learning styles A chronology of experiential learning Challenging the concept of experiential learning Conclusion. Part 3 The design milieu: Introduction The milieu - activities, methods, techniques and materials Planned or unplanned? Innovation, activities, resources and objects - a simple experiential typology Stimulating intelligence Adventurous activities and journeys - challenge and risk Sequencing the challenges Mind challenges Sensory stimulation Change the rules and create obstacles Constructing and using physical objects Telling the story - using physical objects Conclusion. Part 4 Exploring reality: Introduction What is a real experience? Fantasy Play as experiential learning Suspending reality - drama and role-playing Metaphors and storytelling Management development and cartoons Creating comic strips - suggestions for good practice Using photographic images and computer software Reading and writing - reflections on reality Conclusion. Part 5 Places and elements: Introduction Indoor-outdoor, natural-artificial Artificial urban environments Pedagogy and personal development The sensory power of nature Empathetic strategies and the outdoor ""cure"" Rafts or wildlife projects? Eco-adventure and multiple learning Sustainable development. Part 6 The emotional experience: Introduction Emotion and experiential learning The emotional nerve centre The powerof the emotional state Emotional waves Experiencing emotional calm - sorting time Ecstasy and peak learning Experience, learning and ""identity"" Spiritual feelings Conclusion. Part 7 Working with emotions: Introduction The emotional climate - mood setting and relaxed alertness Overcoming emotion - fear Mapping fears - accessing the inner family Using trilogies in emotional work Using humour and other positive emotions Accessing emotions through popular metaphors Metaphoric intervention Conclusion. Part 8 Good practice and ethics: Introduction The booming business The deliverers Facilitator roles Intruding complicators or enabling animateurs Dysfunctional and indigenous learning Setting the climate and conditions. (Part contents.)",project-academic
,2004-01-01,p,IOS Press,how to make the semantic web more semantic," The Semantic Web is not semantic. It is good for syllogistic reasoning, but there is much more to semantics than syllogisms. I argue that the current Semantic Web is too dependent on symbolic representations of information structures, which limits its representational capacity. As a remedy, I propose conceptual spaces as a tool for expressing more of the semantics. Conceptual spaces are built up from quality dimensions that have geometric or topological structures. With the aid of the dimensions, similarities between objects can easily be represented and it is argued that similarity is a central aspect of semantic content. By sorting the dimensions into domains, I define properties and concepts and show how prototype effects of concepts can be treated with the aid of conceptual spaces. I present an outline of how one can reconstruct most of the taxonomies and other meta-data that are explicitly coded in the current Semantic Web and argue that inference engines on the symbolic level will become largely superfluous. As an example of the semantic power of conceptual spaces, I show how concept combinations can be analysed in a much richer and more accurate way than in the classical logical approach. 1 The dream of the Semantic Web Given the enormous amount of information on the Internet, it is becoming ever more important to find methods for information integration. The Semantic Web is the most well known recent attempt in this direction. In an introductory article, Berners-Lee, Hendler and Lassila [3] write that “the Semantic Web is an extension of the current web in which information is given well-defined meaning, better enabling computers and people to work in cooperation.” The ambition of the Semantic Web is excellent, but, so far, most work has been devoted to developing languages such as RDF for representing information and OWL for expressing ontologies. In my opinion, to enable “computers and people to work in cooperation” one should, above all, take into consideration how humans process concepts. As I shall argue in this article, this will be necessary if we want to put real semantic content into the Semantic Web. The dream of the Semantic Web is to develop one ontology expressed in one language potentially covering everything that exists on the web. Berners-Lee [2] writes: “The Semantic Web is what we will get if we perform the same globalization process to Knowledge Representation that the Web initially did to Hypertext. We remove the centralized concepts of absolute truth, total knowledge, and total provability, and see what we can do with limited knowledge.” As Noy and McGuinness [37] note, there are several excellent reasons for developing ontologies: to share a common understanding of the structure of information among people; to enable reuse of domain knowledge; to make domain assumptions explicit; to separate domain knowledge from the operational knowledge; and to analyse domain knowledge. The question is whether the ontologies as we know them from the current Semantic Web are the best tools to achieve these goals. In reality, the picture is not so beautiful: there are several ontologies in several languages covering partly overlapping subdomains of the web. And the formalisms encounter several kinds of integration problems, including structural heterogeneity, semantic heterogeneity, inconsistency and redundancy problems [47]. Shirky [42] summarizes the state of the art as follows: “The Semantic Web, with its neat ontologies and its syllogistic logic, is a nice vision. However, like many visions that project future benefits but ignore present costs, it requires too much coordination and too much energy to effect in the real world, where deductive logic is less effective and shared world view is harder to create than we often want to admit.” One may even question whether we really need the meta-data provided by the Semantic Web. Uschold [46] points out that sometimes information integration works anyway, for example in web shopping agents. He writes that “[s]hopping agents can work even if there is no automatic processing of semantics; they can work without any formal representations of semantics; they can even work with no explicit representations of semantics at all. The key to enabling shopping agents to automatically use web content is that the meaning of the web content that the agents are expected to encounter can be determined by the human programmers who hardwire it into the web application software.” The reason this can be done is that the terminology involved in web shopping application is comparatively limited and free from ambiguities. 2 The Semantic Web is not semantic My main point of criticism is that the Semantic Web is not very semantic. At best, it is ontological. From a philosopher’s point of view, it is not even ontological, since the formalisms exploited only provide a partial description of what a metaphysician would call an ontology. Berners-Lee, Hendler and Lassila [3] have the following comment: “Artificialintelligence and Web researchers have co-opted the term for their own jargon, and for them an ontology is a document or file that formally defines the relations among terms. The most typical kind of ontology for the Web has a taxonomy and a set of inference rules.” Even if we grant that the Semantic Web is ontological, it contains a number of competing ontologies that make information integration very problematic. Furthermore, the methodology of the Semantic Web puts too much emphasis on symbolic structures. This will be the topic of next section. Why do I say that the Semantic Web is not semantic? Let us consider what can be expressed in, for example OWL (as far as I understand, other tools for the Semantic Web have similar properties). McGuinness and Hamelen [28] write that on top of RDF descriptions “OWL adds more vocabulary for describing properties and classes: among others, relations between classes (for example disjointness), cardinality (for example “exactly one”), equality, richer typing of properties, characteristics of properties (for example symmetry and transitivity), and enumerated classes.” These are admittedly important semantic notions. But they are exactly what is to be expected of a language that has the expressivity of first order logic and that defines all concepts (properties and relations) in terms of sets of objects. However, there is much more to the semantics of concepts. It is an unfortunate dogma of computer science in general, and the Semantic Web in particular, that all semantic contents are reducible to first order logic or to set theory. Berners-Lee, Hendler and Lassila [3] claim that “[f]ortunately, a large majority of the information we want to express is along the lines of 'a hex-head bolt is a type of machine bolt'.” Unfortunately, this is not true. If one considers how humans handle concepts, the class relation structures of the Semantic Web capture only a minute part of our information about concepts. For instance, we often categorize objects according to the similarity between the objects [11, 14]. And similarity is not a notion that can be expressed in a natural way in a web ontology language. Along the same lines, Shirky [42] declares that “the Semantic Web is a machine for creating syllogisms.” He concludes that: “This is the promise of the Semantic Web – it will improve all the areas of your life where you currently use syllogisms. Which is to say, almost nowhere.” He adds, somewhat sarcastically: “The people working on the Semantic Web greatly overestimate the value of deductive reasoning (a persistent theme in Artificial Intelligence projects generally.) The great popularizer of this error was Arthur Conan Doyle, whose Sherlock Holmes stories have done more damage to people's understanding of human intelligence than anyone other than Rene Descartes.” In my book Conceptual Spaces [11], I contrast three basic methodologies within the cognitive sciences for representing information: the symbolic, the associationist and the conceptual. In the symbolic approach, cognition is seen as essentially being computation involving symbol manipulation. The second approach is associationism, where associations between different kinds of information elements carry the main burden of representation. Connectionism is a special case of associationism that models associations using artificial neuron networks. The Semantic Web builds almost entirely on the symbolic methodology. The core of this paper will be to outline the conceptual approach to representations (sections 4-9). I shall argue that if we want to build real content into a system, one should rely on the conceptual methodology. A remarkable feature of human thinking is our ability to combine concepts and, in particular, to understand new combinations of concepts [16]. Nobody has problems grasping the meaning of combinations like pink elephant, striped apple and cubic soapbubble, even if one never will encounter any object with these properties. In all kinds of web applications, for example query answering systems, inputs in form of combinations of concepts are ubiquitous. Consequently, an important criterion for a successful computational model of the semantics of concepts is that it should be able to handle combinations of concepts. In classical logic and in the Semantic Web, combinations of concepts are expressed by conjunctions of properties. This means that the reference of the combination of two concepts is taken to be the intersection of the classes representing of the two individual concepts. However, it turns out there are many everyday combinations of concepts that cannot be analysed in this simplistic manner. For example, tall squirrel, honey bee, stone lion, and white Zinfandel (which happens to be a rose wine) cannot be analysed in terms of intersections of classes. In general, current symbolic methods have serious problems handling concept combinations in the way humans do.",project-academic
10.1002/BMB.21188,2019-01-01,a,"John Wiley & Sons, Ltd",a practical guide to developing virtual and augmented reality exercises for teaching structural biology," Although virtual and augmented reality (VR and AR) techniques have been used extensively in specialized laboratories, only recently did they become affordable, reaching wider consumer markets. With increased availability, it is timely to examine the roles that VR and AR may play in teaching structural biology and in experiencing complex data sets such as macromolecular structures. This guide is suitable for those teachers of structural biology who do not have a deep knowledge of information technologies. This study focuses on three questions: 1) How can teachers of structural biology produce and disseminate VR/AR-ready educational material with established and user-friendly software tools?; 2) What are the positive and negative experiences reported by test participants when performing identical learning tasks in the VR and AR environments?; and 3) How do the test participants perceive prerecorded narration during VR/AR exploration? © 2018 International Union of Biochemistry and Molecular Biology, 47(1):16-24, 2018.",project-academic
10.1186/S12916-019-1426-2,2019-10-29,a,BioMed Central,key challenges for delivering clinical impact with artificial intelligence," Artificial intelligence (AI) research in healthcare is accelerating rapidly, with potential applications being demonstrated across various domains of medicine. However, there are currently limited examples of such techniques being successfully deployed into clinical practice. This article explores the main challenges and limitations of AI in healthcare, and considers the steps required to translate these potentially transformative technologies from research to clinical practice. Key challenges for the translation of AI systems in healthcare include those intrinsic to the science of machine learning, logistical difficulties in implementation, and consideration of the barriers to adoption as well as of the necessary sociocultural or pathway changes. Robust peer-reviewed clinical evaluation as part of randomised controlled trials should be viewed as the gold standard for evidence generation, but conducting these in practice may not always be appropriate or feasible. Performance metrics should aim to capture real clinical applicability and be understandable to intended users. Regulation that balances the pace of innovation with the potential for harm, alongside thoughtful post-market surveillance, is required to ensure that patients are not exposed to dangerous interventions nor deprived of access to beneficial innovations. Mechanisms to enable direct comparisons of AI systems must be developed, including the use of independent, local and representative test sets. Developers of AI algorithms must be vigilant to potential dangers, including dataset shift, accidental fitting of confounders, unintended discriminatory bias, the challenges of generalisation to new populations, and the unintended negative consequences of new algorithms on health outcomes. The safe and timely translation of AI research into clinically validated and appropriately regulated systems that can benefit everyone is challenging. Robust clinical evaluation, using metrics that are intuitive to clinicians and ideally go beyond measures of technical accuracy to include quality of care and patient outcomes, is essential. Further work is required (1) to identify themes of algorithmic bias and unfairness while developing mitigations to address these, (2) to reduce brittleness and improve generalisability, and (3) to develop methods for improved interpretability of machine learning predictions. If these goals can be achieved, the benefits for patients are likely to be transformational.",project-academic
10.1109/MCOM.001.1900103,2020-01-27,a,IEEE,toward an intelligent edge wireless communication meets machine learning," The recent revival of AI is revolutionizing almost every branch of science and technology. Given the ubiquitous smart mobile gadgets and IoT devices, it is expected that a majority of intelligent applications will be deployed at the edge of wireless networks. This trend has generated strong interest in realizing an ""intelligent edge"" to support AI-enabled applications at various edge devices. Accordingly, a new research area, called edge learning, has emerged, which crosses and revolutionizes two disciplines: wireless communication and machine learning. A major theme in edge learning is to overcome the limited computing power, as well as limited data, at each edge device. This is accomplished by leveraging the mobile edge computing platform and exploiting the massive data distributed over a large number of edge devices. In such systems, learning from distributed data and communicating between the edge server and devices are two critical and coupled aspects, and their fusion poses many new research challenges. This article advocates a new set of design guidelines for wireless communication in edge learning, collectively called learning- driven communication. Illustrative examples are provided to demonstrate the effectiveness of these design guidelines. Unique research opportunities are identified.",project-academic
10.1140/EPJDS/S13688-016-0085-1,2016-07-07,a,Springer Berlin Heidelberg,sentibench a benchmark comparison of state of the practice sentiment analysis methods," In the last few years thousands of scientific papers have investigated sentiment analysis, several startups that measure opinions on real data have emerged and a number of innovative products related to this theme have been developed. There are multiple methods for measuring sentiments, including lexical-based and supervised machine learning methods. Despite the vast interest on the theme and wide popularity of some methods, it is unclear which one is better for identifying the polarity (i.e., positive or negative) of a message. Accordingly, there is a strong need to conduct a thorough apple-to-apple comparison of sentiment analysis methods, as they are used in practice, across multiple datasets originated from different data sources. Such a comparison is key for understanding the potential limitations, advantages, and disadvantages of popular methods. This article aims at filling this gap by presenting a benchmark comparison of twenty-four popular sentiment analysis methods (which we call the state-of-the-practice methods). Our evaluation is based on a benchmark of eighteen labeled datasets, covering messages posted on social networks, movie and product reviews, as well as opinions and comments in news articles. Our results highlight the extent to which the prediction performance of these methods varies considerably across datasets. Aiming at boosting the development of this research area, we open the methods’ codes and datasets used in this article, deploying them in a benchmark system, which provides an open API for accessing and comparing sentence-level sentiment analysis methods.",project-academic
10.1007/S12369-015-0310-2,2016-01-01,a,Springer Netherlands,socially adaptive path planning in human environments using inverse reinforcement learning," A key skill for mobile robots is the ability to navigate efficiently through their environment. In the case of social or assistive robots, this involves navigating through human crowds. Typical performance criteria, such as reaching the goal using the shortest path, are not appropriate in such environments, where it is more important for the robot to move in a socially adaptive manner such as respecting comfort zones of the pedestrians. We propose a framework for socially adaptive path planning in dynamic environments, by generating human-like path trajectory. Our framework consists of three modules: a feature extraction module, inverse reinforcement learning (IRL) module, and a path planning module. The feature extraction module extracts features necessary to characterize the state information, such as density and velocity of surrounding obstacles, from a RGB-depth sensor. The inverse reinforcement learning module uses a set of demonstration trajectories generated by an expert to learn the expert’s behaviour when faced with different state features, and represent it as a cost function that respects social variables. Finally, the planning module integrates a three-layer architecture, where a global path is optimized according to a classical shortest-path objective using a global map known a priori, a local path is planned over a shorter distance using the features extracted from a RGB-D sensor and the cost function inferred from IRL module, and a low-level system handles avoidance of immediate obstacles. We evaluate our approach by deploying it on a real robotic wheelchair platform in various scenarios, and comparing the robot trajectories to human trajectories.",project-academic
10.1145/2808769.2808779,2015-10-16,p,ACM,detecting clusters of fake accounts in online social networks," Fake accounts are a preferred means for malicious users of online social networks to send spam, commit fraud, or otherwise abuse the system. A single malicious actor may create dozens to thousands of fake accounts in order to scale their operation to reach the maximum number of legitimate members. Detecting and taking action on these accounts as quickly as possible is imperative in order to protect legitimate members and maintain the trustworthiness of the network. However, any individual fake account may appear to be legitimate on first inspection, for example by having a real-sounding name or a believable profile.In this work we describe a scalable approach to finding groups of fake accounts registered by the same actor. The main technique is a supervised machine learning pipeline for classifying {\em an entire cluster} of accounts as malicious or legitimate. The key features used in the model are statistics on fields of user-generated text such as name, email address, company or university; these include both frequencies of patterns {\em within} the cluster (e.g., do all of the emails share a common letter/digit pattern) and comparison of text frequencies across the entire user base (e.g., are all of the names rare?).We apply our framework to analyze account data on LinkedIn grouped by registration IP address and registration date. Our model achieved AUC 0.98 on a held-out test set and AUC 0.95 on out-of-sample testing data. The model has been productionalized and has identified more than 250,000 fake accounts since deployment.",project-academic
,2016-08-13,p,,proceedings of the 22nd acm sigkdd international conference on knowledge discovery and data mining," It is our great pleasure to welcome you to the 2016 ACM Conference on Knowledge Discovery and Data Mining -- KDD'16. We hope that the content and the professional network at KDD'16 will help you succeed professionally by enabling you to: identify technology trends early; make new/creative contributions; increase your productivity by using newer/better tools, processes or ways of organizing teams; identify new job opportunities; and hire new team members.

We are living in an exciting time for our profession. On the one hand, we are witnessing the industrialization of data science, and the emergence of the industrial assembly line processes characterized by the division of labor, integrated processes/pipelines of work, standards, automation, and repeatability. Data science practitioners are organizing themselves in more sophisticated ways, embedding themselves in larger teams in many industry verticals, improving their productivity substantially, and achieving a much larger scale of social impact. On the other hand we are also witnessing astonishing progress from research in algorithms and systems -- for example the field of deep neural networks has revolutionized speech recognition, NLP, computer vision, image recognition, etc. By facilitating interaction between practitioners at large companies & startups on the one hand, and the algorithm development researchers including leading academics on the other, KDD'16 fosters technological and entrepreneurial innovation in the area of data science.

This year's conference continues its tradition of being the premier forum for presentation of results in the field of data mining, both in the form of cutting edge research, and in the form of insights from the development and deployment of real world applications. Further, the conference continues with its tradition of a strong tutorial and workshop program on leading edge issues of data mining. The mission of this conference has broadened in recent years even as we placed a significant amount of focus on both the research and applied aspects of data mining. As an example of this broadened focus, this year we have introduced a strong hands-on tutorial program nduring the conference in which participants will learn how to use practical tools for data mining. KDD'16 also gives researchers and practitioners a unique opportunity to form professional networks, and to share their perspectives with others interested in the various aspects of data mining. For example, we have introduced office hours for budding entrepreneurs from our community to meet leading Venture Capitalists investing in this area. We hope that KDD 2016 conference will serve as a meeting ground for researchers, practitioners, funding agencies, and investors to help create new algorithms and commercial products.

The call for papers attracted a significant number of submissions from countries all over the world. In particular, the research track attracted 784 submissions and the applied data science track attracted 331 submissions. Papers were accepted either as full papers or as posters. The overall acceptance rate either as full papers or posters was less than 20%. For full papers in the research track, the acceptance rate was lower than 10%. This is consistent with the fact that the KDD Conference is a premier conference in data mining and the acceptance rates historically tend to be low. It is noteworthy that the applied data science track received a larger number of submissions compared to previous years. We view this as an encouraging sign that research in data mining is increasingly becoming relevant to industrial applications. All papers were reviewed by at least three program committee members and then discussed by the PC members in a discussion moderated by a meta-reviewer. Borderline papers were thoroughly reviewed by the program chairs before final decisions were made.",project-academic
,2016-02-22,a,UCL Knowledge Lab,intelligence unleashed an argument for ai in education," This paper on artificial intelligence in education (AIEd) has two aims. The first: to explain to a non-specialist, interested, reader what AIEd is: its goals, how it is built, and how it works. The second: to set out the argument for what AIEd can offer teaching and learning, both now and in the future, with an eye towards improving learning and life outcomes for all. Computer systems that are artificially intelligent interact with the world using capabilities (such as speech recognition) and intelligent behaviours (such as using available information to take the most sensible actions toward a stated goal) that we would think of as essentially human. At the heart of artificial intelligence in education is the scientific goal to make knowledge, which is often left implicit, computationally precise and explicit. In other words, in addition to being the engine behind much ‘smart’ ed tech, AIEd is also designed to be a powerful tool to open up what is sometimes called the ‘black box of learning,’ giving us more fine-grained understandings of how learning actually happens. Although some might find the concept of AIEd alienating, the algorithms and models that underpin ed tech powered by AIEd form the basis of an essentially human endeavor. Using AIEd, teachers will be able to offer learners educational experiences that are more personalised, flexible, inclusive and engaging. Crucially, we do not see a future in which AIEd replaces teachers. What we do see is a future in which the extraordinary expertise of teachers is better leveraged and augmented through the thoughtful deployment of well designed AIEd. We have available, right now, AIEd tools that could support student learning at a scale previously unimaginable by providing one-on-one tutoring to every student, in every subject. Existing technologies also have the capacity to provide intelligent support to learners working in a group, and to create authentic virtual learning environments where students have the right support, at the right time, to tackle real-life problems and puzzles. In the near future, we expect that teaching and learning will increasingly be supported by the thoughtful application of AIEd tools. For example, by lifelong learning companions powered by AI that can accompany and support individual learners throughout their studies - in and beyond school - and new forms of assessment that measure learning while it is taking place, shaping the learning experience in real time. If we are ultimately successful, we predict that AIEd will help us address some of the most intractable problems in education, including achievement gaps and teacher retention. AIEd will also help us respond to the most significant social challenge that AI has already brought - the steady replacement of jobs and occupations with clever algorithms and robots. It is our view that this provides a new innovation imperative in education, which can be expressed simply: as humans live and work alongside increasingly smart machines, our education systems will need to achieve at levels that none have managed to date. True progress will require the development of an AIEd infrastructure. This will not, however, be a single monolithic AIEd system. Instead, it will resemble the marketplace that has developed for smartphone apps: hundreds and then thousands of individual AIEd components, developed in collaboration with educators, conformed to uniform international data standards, and shared with researchers and developers worldwide. These standards will also enable system-level data collation and analysis that will help us to learn much more about learning itself – and how to improve it. Moving forward, we will need to pay close attention to three powerful forces as we map the future of artificial intelligence in education, namely pedagogy, technology, and system change. Paying attention to the pedagogy will mean that the design of new edtech should always start with what we know about learning. It also means that the system for funding this work must be simultaneously opened up and refocused, moving away from isolated pockets of R&D and toward collaborative enterprises that prioritise areas known to make a real difference to teaching and learning. Paying attention to the technology will mean creating smarter demand for commercial grade AIEd products that work. It also means the development of a robust, component-based AIEd infrastructure, similar to the smartphone app marketplace, where researchers and developers can access standardised components that have been developed in collaboration with educators. Paying attention to system change will mean involving teachers, students, and parents in co-designing new tools, so that AIEd will appropriately address the inherent “messiness” of real classroom, university, and workplace learning environments. It also means the development of data standards that promote the safe and ethical use of data. Said succinctly, we need intelligent technologies that embody what we know about great teaching and learning, embodied in enticing consumer grade products, which are then used effectively in real-life settings that combine the best of human and machine. We do not underestimate the new-thinking, inevitable wrong-turns, and effort required to realise these recommendations. However, if we are to properly unleash the intelligence of AIEd, we must do things differently - via new collaborations, sensible funding, and (always) a keen eye on the pedagogy. The potential prize is too great to act otherwise.",project-academic
,2019-11-19,a,,the human body is a black box supporting clinical decision making with deep learning," Machine learning technologies are increasingly developed for use in healthcare. While research communities have focused on creating state-of-the-art models, there has been less focus on real world implementation and the associated challenges to accuracy, fairness, accountability, and transparency that come from actual, situated use. Serious questions remain under examined regarding how to ethically build models, interpret and explain model output, recognize and account for biases, and minimize disruptions to professional expertise and work cultures. We address this gap in the literature and provide a detailed case study covering the development, implementation, and evaluation of Sepsis Watch, a machine learning-driven tool that assists hospital clinicians in the early diagnosis and treatment of sepsis. We, the team that developed and evaluated the tool, discuss our conceptualization of the tool not as a model deployed in the world but instead as a socio-technical system requiring integration into existing social and professional contexts. Rather than focusing on model interpretability to ensure a fair and accountable machine learning, we point toward four key values and practices that should be considered when developing machine learning to support clinical decision-making: rigorously define the problem in context, build relationships with stakeholders, respect professional discretion, and create ongoing feedback loops with stakeholders. Our work has significant implications for future research regarding mechanisms of institutional accountability and considerations for designing machine learning systems. Our work underscores the limits of model interpretability as a solution to ensure transparency, accuracy, and accountability in practice. Instead, our work demonstrates other means and goals to achieve FATML values in design and in practice.",project-academic
10.1145/3351095.3372827,2020-01-27,p,ACM,the human body is a black box supporting clinical decision making with deep learning," Machine learning technologies are increasingly developed for use in healthcare. While research communities have focused on creating state-of-the-art models, there has been less focus on real world implementation and the associated challenges to fairness, transparency, and accountability that come from actual, situated use. Serious questions remain underexamined regarding how to ethically build models, interpret and explain model output, recognize and account for biases, and minimize disruptions to professional expertise and work cultures. We address this gap in the literature and provide a detailed case study covering the development, implementation, and evaluation of Sepsis Watch, a machine learning-driven tool that assists hospital clinicians in the early diagnosis and treatment of sepsis. Sepsis is a severe infection that can lead to organ failure or death if not treated in time and is the leading cause of inpatient deaths in US hospitals. We, the team that developed and evaluated the tool, discuss our conceptualization of the tool not as a model deployed in the world but instead as a socio-technical system requiring integration into existing social and professional contexts. Rather than focusing solely on model interpretability to ensure fair and accountable machine learning, we point toward four key values and practices that should be considered when developing machine learning to support clinical decision-making: rigorously define the problem in context, build relationships with stakeholders, respect professional discretion, and create ongoing feedback loops with stakeholders. Our work has significant implications for future research regarding mechanisms of institutional accountability and considerations for responsibly designing machine learning systems. Our work underscores the limits of model interpretability as a solution to ensure transparency, accuracy, and accountability in practice. Instead, our work demonstrates other means and goals to achieve FATML values in design and in practice.",project-academic
,2021-03-29,a,,layoutparser a unified toolkit for deep learning based document image analysis," Recent advances in document image analysis (DIA) have been primarily driven by the application of neural networks. Ideally, research outcomes could be easily deployed in production and extended for further investigation. However, various factors like loosely organized codebases and sophisticated model configurations complicate the easy reuse of important innovations by a wide audience. Though there have been on-going efforts to improve reusability and simplify deep learning (DL) model development in disciplines like natural language processing and computer vision, none of them are optimized for challenges in the domain of DIA. This represents a major gap in the existing toolkit, as DIA is central to academic research across a wide range of disciplines in the social sciences and humanities. This paper introduces layoutparser, an open-source library for streamlining the usage of DL in DIA research and applications. The core layoutparser library comes with a set of simple and intuitive interfaces for applying and customizing DL models for layout detection, character recognition, and many other document processing tasks. To promote extensibility, layoutparser also incorporates a community platform for sharing both pre-trained models and full document digitization pipelines. We demonstrate that layoutparser is helpful for both lightweight and large-scale digitization pipelines in real-word use cases. The library is publicly available at this https URL.",project-academic
10.1007/978-3-030-86549-8_9,2021-09-05,p,"Springer, Cham",layoutparser a unified toolkit for deep learning based document image analysis," Recent advances in document image analysis (DIA) have been primarily driven by the application of neural networks. Ideally, research outcomes could be easily deployed in production and extended for further investigation. However, various factors like loosely organized codebases and sophisticated model configurations complicate the easy reuse of important innovations by a wide audience. Though there have been on-going efforts to improve reusability and simplify deep learning (DL) model development in disciplines like natural language processing and computer vision, none of them are optimized for challenges in the domain of DIA. This represents a major gap in the existing toolkit, as DIA is central to academic research across a wide range of disciplines in the social sciences and humanities. This paper introduces LayoutParser, an open-source library for streamlining the usage of DL in DIA research and applications. The core LayoutParser library comes with a set of simple and intuitive interfaces for applying and customizing DL models for layout detection, character recognition, and many other document processing tasks. To promote extensibility, LayoutParser also incorporates a community platform for sharing both pre-trained models and full document digitization pipelines. We demonstrate that LayoutParser is helpful for both lightweight and large-scale digitization pipelines in real-word use cases. The library is publicly available at https://layout-parser.github.io.",project-academic
10.1016/J.CMA.2019.112623,2020-01-01,a,North-Holland,machine learning in cardiovascular flows modeling predicting arterial blood pressure from non invasive 4d flow mri data using physics informed neural networks," Abstract None None Advances in computational science offer a principled pipeline for predictive modeling of cardiovascular flows and aspire to provide a valuable tool for monitoring, diagnostics and surgical planning. Such models can be nowadays deployed on large patient-specific topologies of systemic arterial networks and return detailed predictions on flow patterns, wall shear stresses, and pulse wave propagation. However, their success heavily relies on tedious pre-processing and calibration procedures that typically induce a significant computational cost, thus hampering their clinical applicability. In this work we put forth a machine learning framework that enables the seamless synthesis of non-invasive in-vivo measurement techniques and computational flow dynamics models derived from first physical principles. We illustrate this new paradigm by showing how one-dimensional models of pulsatile flow can be used to constrain the output of deep neural networks such that their predictions satisfy the conservation of mass and momentum principles. Once trained on noisy and scattered clinical data of flow and wall displacement, these networks can return physically consistent predictions for velocity, pressure and wall displacement pulse wave propagation, all without the need to employ conventional simulators. A simple post-processing of these outputs can also provide a relatively cheap and effective way for estimating Windkessel model parameters that are required for the calibration of traditional computational models. The effectiveness of the proposed techniques is demonstrated through a series of prototype benchmarks, as well as a realistic clinical case involving in-vivo measurements near the aorta/carotid bifurcation of a healthy human subject.",project-academic
10.2196/26627,2021-04-05,a,JMIR Publications Inc.,artificial intelligence enabled analysis of public attitudes on facebook and twitter toward covid 19 vaccines in the united kingdom and the united states observational study," Background: Global efforts toward the development and deployment of a vaccine for COVID-19 are rapidly advancing. To achieve herd immunity, widespread administration of vaccines is required, which necessitates significant cooperation from the general public. As such, it is crucial that governments and public health agencies understand public sentiments toward vaccines, which can help guide educational campaigns and other targeted policy interventions.
Objective: The aim of this study was to develop and apply an artificial intelligence–based approach to analyze public sentiments on social media in the United Kingdom and the United States toward COVID-19 vaccines to better understand the public attitude and concerns regarding COVID-19 vaccines.
Methods: Over 300,000 social media posts related to COVID-19 vaccines were extracted, including 23,571 Facebook posts from the United Kingdom and 144,864 from the United States, along with 40,268 tweets from the United Kingdom and 98,385 from the United States from March 1 to November 22, 2020. We used natural language processing and deep learning–based techniques to predict average sentiments, sentiment trends, and topics of discussion. These factors were analyzed longitudinally and geospatially, and manual reading of randomly selected posts on points of interest helped identify underlying themes and validated insights from the analysis.
Results: Overall averaged positive, negative, and neutral sentiments were at 58%, 22%, and 17% in the United Kingdom, compared to 56%, 24%, and 18% in the United States, respectively. Public optimism over vaccine development, effectiveness, and trials as well as concerns over their safety, economic viability, and corporation control were identified. We compared our findings to those of nationwide surveys in both countries and found them to correlate broadly.
Conclusions: Artificial intelligence–enabled social media analysis should be considered for adoption by institutions and governments alongside surveys and other conventional methods of assessing public attitude. Such analyses could enable real-time assessment, at scale, of public confidence and trust in COVID-19 vaccines, help address the concerns of vaccine sceptics, and help develop more effective policies and communication strategies to maximize uptake.",project-academic
10.1109/ACCESS.2018.2877919,2018-10-24,a,IEEE,wearable affective robot," With the development of the artificial intelligence (AI), the AI applications have influenced and changed people’s daily life greatly. Here, a wearable affective robot that integrates the affective robot, social robot, brain wearable, and Wearable 2.0 is proposed for the first time. The proposed wearable affective robot is intended for a wide population, and we believe that it can improve the human health on the spirit level, meeting the fashion requirements at the same time. In this paper, the architecture and design of an innovative wearable affective robot, which is dubbed as Fitbot, are introduced in terms of hardware and algorithm’s perspectives. In addition, the important functional component of the robot-brain wearable device is introduced from the aspect of the hardware design, EEG data acquisition and analysis, user behavior perception, and algorithm deployment. Then, the EEG-based cognition of user’s behavior is realized. Through the continuous acquisition of the in-depth, in-breadth data, the Fitbot we present can gradually enrich user’s life modeling and enable the wearable robot to recognize user’s intention and further understand the behavioral motivation behind the user’s emotion. The learning algorithm for the life modeling embedded in Fitbot can achieve better user’s experience of affective social interaction. Finally, the application service scenarios and some challenging issues of a wearable affective robot are discussed.",project-academic
,2018-10-25,a,,wearable affective robot," With the development of the artificial intelligence (AI), the AI applications have influenced and changed people's daily life greatly. Here, a wearable affective robot that integrates the affective robot, social robot, brain wearable, and wearable 2.0 is proposed for the first time. The proposed wearable affective robot is intended for a wide population, and we believe that it can improve the human health on the spirit level, meeting the fashion requirements at the same time. In this paper, the architecture and design of an innovative wearable affective robot, which is dubbed as Fitbot, are introduced in terms of hardware and algorithm's perspectives. In addition, the important functional component of the robot-brain wearable device is introduced from the aspect of the hardware design, EEG data acquisition and analysis, user behavior perception, and algorithm deployment, etc. Then, the EEG based cognition of user's behavior is realized. Through the continuous acquisition of the in-depth, in-breadth data, the Fitbot we present can gradually enrich user's life modeling and enable the wearable robot to recognize user's intention and further understand the behavioral motivation behind the user's emotion. The learning algorithm for the life modeling embedded in Fitbot can achieve better user's experience of affective social interaction. Finally, the application service scenarios and some challenging issues of a wearable affective robot are discussed.",project-academic
,2013-01-29,a,,phishari automatic realtime phishing detection on twitter," With the advent of online social media, phishers have started using social networks like Twitter, Facebook, and Foursquare to spread phishing scams. Twitter is an immensely popular micro-blogging network where people post short messages of 140 characters called tweets. It has over 100 million active users who post about 200 million tweets everyday. Phishers have started using Twitter as a medium to spread phishing because of this vast information dissemination. Further, it is difficult to detect phishing on Twitter unlike emails because of the quick spread of phishing links in the network, short size of the content, and use of URL obfuscation to shorten the URL. Our technique, PhishAri, detects phishing on Twitter in realtime. We use Twitter specific features along with URL features to detect whether a tweet posted with a URL is phishing or not. Some of the Twitter specific features we use are tweet content and its characteristics like length, hashtags, and mentions. Other Twitter features used are the characteristics of the Twitter user posting the tweet such as age of the account, number of tweets, and the follower-followee ratio. These Twitter specific features coupled with URL based features prove to be a strong mechanism to detect phishing tweets. We use machine learning classification techniques and detect phishing tweets with an accuracy of 92.52%. We have deployed our system for end-users by providing an easy to use Chrome browser extension which works in realtime and classifies a tweet as phishing or safe. We show that we are able to detect phishing tweets at zero hour with high accuracy which is much faster than public blacklists and as well as Twitter's own defense mechanism to detect malicious content. To the best of our knowledge, this is the first realtime, comprehensive and usable system to detect phishing on Twitter.",project-academic
10.2196/JMIR.9268,2018-01-30,a,JMIR Publications Inc.,prediction of incident hypertension within the next year prospective study using statewide electronic health records and machine learning," Background: As a high-prevalence health condition, hypertension is clinically costly, difficult to manage, and often leads to severe and life-threatening diseases such as cardiovascular disease (CVD) and stroke. Objective: The aim of this study was to develop and validate prospectively a risk prediction model of incident essential hypertension within the following year. Methods: Data from individual patient electronic health records (EHRs) were extracted from the Maine Health Information Exchange network. Retrospective (N=823,627, calendar year 2013) and prospective (N=680,810, calendar year 2014) cohorts were formed. A machine learning algorithm, XGBoost, was adopted in the process of feature selection and model building. It generated an ensemble of classification trees and assigned a final predictive risk score to each individual. Results: The 1-year incident hypertension risk model attained areas under the curve (AUCs) of 0.917 and 0.870 in the retrospective and prospective cohorts, respectively. Risk scores were calculated and stratified into five risk categories, with 4526 out of 381,544 patients (1.19%) in the lowest risk category (score 0-0.05) and 21,050 out of 41,329 patients (50.93%) in the highest risk category (score 0.4-1) receiving a diagnosis of incident hypertension in the following 1 year. Type 2 diabetes, lipid disorders, CVDs, mental illness, clinical utilization indicators, and socioeconomic determinants were recognized as driving or associated features of incident essential hypertension. The very high risk population mainly comprised elderly (age>50 years) individuals with multiple chronic conditions, especially those receiving medications for mental disorders. Disparities were also found in social determinants, including some community-level factors associated with higher risk and others that were protective against hypertension. Conclusions: With statewide EHR datasets, our study prospectively validated an accurate 1-year risk prediction model for incident essential hypertension. Our real-time predictive analytic model has been deployed in the state of Maine, providing implications in interventions for hypertension and related diseases and hopefully enhancing hypertension care. None [J Med Internet Res 2018;20(1):e22]",project-academic
10.1109/BIGDATA.2016.7841045,2016-12-01,p,hgpu.org,deep learning in the automotive industry applications and tools," Deep Learning refers to a set of machine learning techniques that utilize neural networks with many hidden layers for tasks, such as image classification, speech recognition, language understanding. Deep learning has been proven to be very effective in these domains and is pervasively used by many Internet services. In this paper, we describe different automotive uses cases for deep learning in particular in the domain of computer vision. We surveys the current state-of-the-art in libraries, tools and infrastructures (e. g. GPUs and clouds) for implementing, training and deploying deep neural networks. We particularly focus on convolutional neural networks and computer vision use cases, such as the visual inspection process in manufacturing plants and the analysis of social media data. To train neural networks, curated and labeled datasets are essential. In particular, both the availability and scope of such datasets is typically very limited. A main contribution of this paper is the creation of an automotive dataset, that allows us to learn and automatically recognize different vehicle properties. We describe an end-to-end deep learning application utilizing a mobile app for data collection and process support, and an Amazon-based cloud backend for storage and training. For training we evaluate the use of cloud and on-premises infrastructures (including multiple GPUs) in conjunction with different neural network architectures and frameworks. We assess both the training times as well as the accuracy of the classifier. Finally, we demonstrate the effectiveness of the trained classifier in a real world setting during manufacturing process.",project-academic
10.3390/S19092047,2019-05-02,a,Multidisciplinary Digital Publishing Institute,design and implementation of cloud analytics assisted smart power meters considering advanced artificial intelligence as edge analytics in demand side management for smart homes," In a smart home linked to a smart grid (SG), demand-side management (DSM) has the potential to reduce electricity costs and carbon/chlorofluorocarbon emissions, which are associated with electricity used in today’s modern society. To meet continuously increasing electrical energy demands requested from downstream sectors in an SG, energy management systems (EMS), developed with paradigms of artificial intelligence (AI) across Internet of things (IoT) and conducted in fields of interest, monitor, manage, and analyze industrial, commercial, and residential electrical appliances efficiently in response to demand response (DR) signals as DSM. Usually, a DSM service provided by utilities for consumers in an SG is based on cloud-centered data science analytics. However, such cloud-centered data science analytics service involved for DSM is mostly far away from on-site IoT end devices, such as DR switches/power meters/smart meters, which is usually unacceptable for latency-sensitive user-centric IoT applications in DSM. This implies that, for instance, IoT end devices deployed on-site for latency-sensitive user-centric IoT applications in DSM should be aware of immediately analytical, interpretable, and real-time actionable data insights processed on and identified by IoT end devices at IoT sources. Therefore, this work designs and implements a smart edge analytics-empowered power meter prototype considering advanced AI in DSM for smart homes. The prototype in this work works in a cloud analytics-assisted electrical EMS architecture, which is designed and implemented as edge analytics in the architecture described and developed toward a next-generation smart sensing infrastructure for smart homes. Two different types of AI deployed on-site on the prototype are conducted for DSM and compared in this work. The experimentation reported in this work shows the architecture described with the prototype in this work is feasible and workable.",project-academic
10.1093/COMJNL/BXY082,2019-03-01,a,Oxford Academic,algorithmic government automating public services and supporting civil servants in using data science technologies," The data science technologies of artificial intelligence (AI), Internet of Things (IoT), big data and behavioral/predictive analytics, and blockchain are poised to revolutionize government and create a new generation of GovTech start-ups. The impact from the ‘smartification’ of public services and the national infrastructure will be much more significant in comparison to any other sector given government's function and importance to every institution and individual. Potential GovTech systems include Chatbots and intelligent assistants for public engagement, Robo-advisors to support civil servants, real-time management of the national infrastructure using IoT and blockchain, automated compliance/regulation, public records securely stored in blockchain distributed ledgers, online judicial and dispute resolution systems, and laws/statutes encoded as blockchain smart contracts. Government is potentially the major ‘client’ and also ‘public champion’ for these new data technologies. This review paper uses our simple taxonomy of government services to provide an overview of data science automation being deployed by governments world-wide. The goal of this review paper is to encourage the Computer Science community to engage with government to develop these new systems to transform public services and support the work of civil servants.",project-academic
10.1016/J.COMPELECENG.2017.03.009,2017-03-16,a,Pergamon,applying spark based machine learning model on streaming big data for health status prediction," Abstract None None Machine learning is one of the driving forces of science and commerce, but the proliferation of Big Data demands paradigm shifts from traditional methods in the application of machine learning techniques on this voluminous data having varying velocity. With the availability of large health care datasets and progressions in machine learning techniques, computers are now well equipped in diagnosing many health issues. This work aims at developing a real time remote health status prediction system built around open source Big Data processing engine, the Apache Spark, deployed in the cloud which focus on applying machine learning model on streaming Big Data. In this scalable system, the user tweets his health attributes and the application receives the same in real time, extracts the attributes and applies machine learning model to predict user's health status which is then directly messaged to him/her instantly for taking appropriate action.",project-academic
,2010-06-14,b,,fundamentals of predictive text mining," One consequence of the pervasive use of computers is that most documents originate in digital form. Widespread use of the Internet makes them readily available. Text mining the process of analyzing unstructured natural-language text is concerned with how to extract information from these documents. Developed from the authors highly successful Springer reference on text mining, Fundamentals of Predictive Text Mining is an introductory textbook and guide to this rapidly evolving field. Integrating topics spanning the varied disciplines of data mining, machine learning, databases, and computational linguistics, this uniquely useful book also provides practical advice for text mining. In-depth discussions are presented on issues of document classification, information retrieval, clustering and organizing documents, information extraction, web-based data-sourcing, and prediction and evaluation. Background on data mining is beneficial, but not essential. Where advanced concepts are discussed that require mathematical maturity for a proper understanding, intuitive explanations are also provided for less advanced readers. Topics and features: presents a comprehensive, practical and easy-to-read introduction to text mining; includes chapter summaries, useful historical and bibliographic remarks, and classroom-tested exercises for each chapter; explores the application and utility of each method, as well as the optimum techniques for specific scenarios; provides several descriptive case studies that take readers from problem description to systems deployment in the real world; includes access to industrial-strength text-mining software that runs on any computer; describes methods that rely on basic statistical techniques, thus allowing for relevance to all languages (not just English); contains links to free downloadable software and other supplementary instruction material. Fundamentals of Predictive Text Mining is an essential resource for IT professionals and managers, as well as a key text for advanced undergraduate computer science students and beginning graduate students. Dr. Sholom M. Weiss is a Research Staff Member with the IBM Predictive Modeling group, in Yorktown Heights, New York, and Professor Emeritus of Computer Science at Rutgers University. Dr. Nitin Indurkhya is Professor at the School of Computer Science and Engineering, University of New South Wales, Australia, as well as founder and president of data-mining consulting company Data-Miner Pty Ltd. Dr. Tong Zhang is Associate Professor at the Department of Statistics and Biostatistics at Rutgers University, New Jersey.",project-academic
10.1145/3338469.3358944,2019-11-11,p,ACM Press,ngraph he2 a high throughput framework for neural network inference on encrypted data," In previous work, Boemer et al. introduced nGraph-HE, an extension to the Intel nGraph deep learning (DL) compiler, that enables data scientists to deploy models with popular frameworks such as TensorFlow and PyTorch with minimal code changes. However, the class of supported models was limited to relatively shallow networks with polynomial activations. Here, we introduce nGraph-HE2, which extends nGraph-HE to enable privacy-preserving inference on standard, pre-trained models using their native activation functions and number fields (typically real numbers). The proposed framework leverages the CKKS scheme, whose support for real numbers is friendly to data science, and a client-aided model using a two-party approach to compute activation functions. We first present CKKS-specific optimizations, enabling a 3x-88x runtime speedup for scalar encoding, and doubling the throughput through a novel use of CKKS plaintext packing into complex numbers. Second, we optimize ciphertext-plaintext addition and multiplication, yielding 2.6x-4.2x runtime speedup. Third, we exploit two graph-level optimizations: lazy-rescaling and depth-aware encoding, which allow us to significantly improve performance. Together, these optimizations enable state-of-the-art throughput of 1,998 images/s on the CryptoNets network. Using the client-aided model, we also present homomorphic evaluation of (to our knowledge) the largest network to date, namely, pre-trained MobileNetV2 models on the ImageNet dataset, with 60.4%/82.7% top-1/top-5 accuracy and an amortized runtime of 381 ms/image.",project-academic
,2019-08-12,a,,ngraph he2 a high throughput framework for neural network inference on encrypted data," In previous work, Boemer et al. introduced nGraph-HE, an extension to the Intel nGraph deep learning (DL) compiler, that enables data scientists to deploy models with popular frameworks such as TensorFlow and PyTorch with minimal code changes. However, the class of supported models was limited to relatively shallow networks with polynomial activations. Here, we introduce nGraph-HE2, which extends nGraph-HE to enable privacy-preserving inference on standard, pre-trained models using their native activation functions and number fields (typically real numbers). The proposed framework leverages the CKKS scheme, whose support for real numbers is friendly to data science, and a client-aided model using a two-party approach to compute activation functions. 
We first present CKKS-specific optimizations, enabling a 3x-88x runtime speedup for scalar encoding, and doubling the throughput through a novel use of CKKS plaintext packing into complex numbers. Second, we optimize ciphertext-plaintext addition and multiplication, yielding 2.6x-4.2x runtime speedup. Third, we exploit two graph-level optimizations: lazy rescaling and depth-aware encoding, which allow us to significantly improve performance. 
Together, these optimizations enable state-of-the-art throughput of 1,998 images/s on the CryptoNets network. Using the client-aided model, we also present homomorphic evaluation of (to our knowledge) the largest network to date, namely, pre-trained MobileNetV2 models on the ImageNet dataset, with 60.4\percent/82.7\percent\ top-1/top-5 accuracy and an amortized runtime of 381 ms/image.",project-academic
10.1101/2020.12.08.20246231,2020-12-11,a,Cold Spring Harbor Laboratory Press,artificial intelligence enabled analysis of uk and us public attitudes on facebook and twitter towards covid 19 vaccinations," Abstract None Background None Global efforts towards the development and deployment of a vaccine for SARS-CoV-2 are rapidly advancing. We developed and applied an artificial-intelligence (AI)-based approach to analyse social-media public sentiment in the UK and the US towards COVID-19 vaccinations, to understand public attitude and identify topics of concern. None Methods None Over 300,000 social-media posts related to COVID-19 vaccinations were extracted, including 23,571 Facebook-posts from the UK and 144,864 from the US, along with 40,268 tweets from the UK and 98,385 from the US respectively, from 1st March - 22nd November 2020. We used natural language processing and deep learning based techniques to predict average sentiments, sentiment trends and topics of discussion. These were analysed longitudinally and geo-spatially, and a manual reading of randomly selected posts around points of interest helped identify underlying themes and validated insights from the analysis. None Results None We found overall averaged positive, negative and neutral sentiment in the UK to be 58%, 22% and 17%, compared to 56%, 24% and 18% in the US, respectively. Public optimism over vaccine development, effectiveness and trials as well as concerns over safety, economic viability and corporation control were identified. We compared our findings to national surveys in both countries and found them to correlate broadly. None Conclusions None AI-enabled social-media analysis should be considered for adoption by institutions and governments, alongside surveys and other conventional methods of assessing public attitude. This could enable real-time assessment, at scale, of public confidence and trust in COVID-19 vaccinations, help address concerns of vaccine-sceptics and develop more effective policies and communication strategies to maximise uptake.",project-academic
10.2196/JMIR.8164,2017-10-30,a,"JMIR Publications Inc., Toronto, Canada",discovering cohorts of pregnant women from social media for safety surveillance and analysis," Background: Pregnancy exposure registries are the primary sources of information about the safety of maternal usage of medications during pregnancy. Such registries enroll pregnant women in a voluntary fashion early on in pregnancy and follow them until the end of pregnancy or longer to systematically collect information regarding specific pregnancy outcomes. Although the model of pregnancy registries has distinct advantages over other study designs, they are faced with numerous challenges and limitations such as low enrollment rate, high cost, and selection bias. Objective: The primary objectives of this study were to systematically assess whether social media (Twitter) can be used to discover cohorts of pregnant women and to develop and deploy a natural language processing and machine learning pipeline for the automatic collection of cohort information. In addition, we also attempted to ascertain, in a preliminary fashion, what types of longitudinal information may potentially be mined from the collected cohort information. Methods: Our discovery of pregnant women relies on detecting pregnancy-indicating tweets (PITs), which are statements posted by pregnant women regarding their pregnancies. We used a set of 14 patterns to first detect potential PITs. We manually annotated a sample of 14,156 of the retrieved user posts to distinguish real PITs from false positives and trained a supervised classification system to detect real PITs. We optimized the classification system via cross validation, with features and settings targeted toward optimizing precision for the positive class. For users identified to be posting real PITs via automatic classification, our pipeline collected all their available past and future posts from which other information (eg, medication usage and fetal outcomes) may be mined. Results: Our rule-based PIT detection approach retrieved over 200,000 posts over a period of 18 months. Manual annotation agreement for three annotators was very high at kappa (κ)=.79. On a blind test set, the implemented classifier obtained an overall F1 score of 0.84 (0.88 for the pregnancy class and 0.68 for the nonpregnancy class). Precision for the pregnancy class was 0.93, and recall was 0.84. Feature analysis showed that the combination of dense and sparse vectors for classification achieved optimal performance. Employing the trained classifier resulted in the identification of 71,954 users from the collected posts. Over 250 million posts were retrieved for these users, which provided a multitude of longitudinal information about them. Conclusions: Social media sources such as Twitter can be used to identify large cohorts of pregnant women and to gather longitudinal information via automated processing of their postings. Considering the many drawbacks and limitations of pregnancy registries, social media mining may provide beneficial complementary information. Although the cohort sizes identified over social media are large, future research will have to assess the completeness of the information available through them.",project-academic
,2018-07-09,p,International Foundation for Autonomous Agents and Multiagent Systems,towards a robust interactive and learning social robot," Pepper is a humanoid robot, specifically designed for social interaction, that has been deployed in a variety of public environments. A programmable version of Pepper is also available, enabling our focused research on perception and behavior robustness and capabilities of an interactive social robot. We address Pepper perception by integrating state-of-the-art vision and speech recognition systems and experimentally analyzing their effectiveness. As we recognize limitations of the individual perceptual modalities, we introduce a multi-modality approach to increase the robustness of human social interaction with the robot. We combine vision, gesture, speech, and input from an onboard tablet, a remote mobile phone, and external microphones. Our approach includes the proactive seeking of input from a different modality, adding robustness to the failures of the separate components. We also introduce a learning algorithm to improve communication capabilities over time, updating speech recognition through social interactions. Finally, we realize the rich robot body-sensory data and introduce both a nearest-neighbor and a deep learning approach to enable Pepper to classify and speak up a variety of its own body motions. We view the contributions of our work to be relevant both to Pepper specifically and to other general social robots.",project-academic
,2021-02-23,a,,assigning confidence to molecular property prediction," Introduction: Computational modeling has rapidly advanced over the last decades, especially to predict molecular properties for chemistry, material science and drug design. Recently, machine learning techniques have emerged as a powerful and cost-effective strategy to learn from existing datasets and perform predictions on unseen molecules. Accordingly, the explosive rise of data-driven techniques raises an important question: What confidence can be assigned to molecular property predictions and what techniques can be used for that purpose? 
Areas covered: In this work, we discuss popular strategies for predicting molecular properties relevant to drug design, their corresponding uncertainty sources and methods to quantify uncertainty and confidence. First, our considerations for assessing confidence begin with dataset bias and size, data-driven property prediction and feature design. Next, we discuss property simulation via molecular docking, and free-energy simulations of binding affinity in detail. Lastly, we investigate how these uncertainties propagate to generative models, as they are usually coupled with property predictors. 
Expert opinion: Computational techniques are paramount to reduce the prohibitive cost and timing of brute-force experimentation when exploring the enormous chemical space. We believe that assessing uncertainty in property prediction models is essential whenever closed-loop drug design campaigns relying on high-throughput virtual screening are deployed. Accordingly, considering sources of uncertainty leads to better-informed experimental validations, more reliable predictions and to more realistic expectations of the entire workflow. Overall, this increases confidence in the predictions and designs and, ultimately, accelerates drug design.",project-academic
10.1039/C9TA02356A,2019-07-23,a,The Royal Society of Chemistry,machine learning for renewable energy materials," Achieving the 2016 Paris agreement goal of limiting global warming below 2 °C and securing a sustainable energy future require materials innovations in renewable energy technologies. While the window of opportunity is closing, meeting these goals necessitates deploying new research concepts and strategies to accelerate materials discovery by an order of magnitude. Recent advancements in machine learning have provided the science and engineering community with a flexible and rapid prediction framework, showing a tremendous potential impact. Here we summarize the recent progress in machine learning approaches for developing renewable energy materials. We demonstrate applications of machine learning methods for theoretical approaches in key renewable energy technologies including catalysis, batteries, solar cells, and crystal discovery. We also analyze notable applications resulting in significant real discoveries and discuss critical gaps to further accelerate materials discovery.",project-academic
10.1038/S41746-021-00520-6,2021-10-07,a,Nature Publishing Group,biological data annotation via a human augmenting ai based labeling system," Biology has become a prime area for the deployment of deep learning and artificial intelligence (AI), enabled largely by the massive data sets that the field can generate. Key to most AI tasks is the availability of a sufficiently large, labeled data set with which to train AI models. In the context of microscopy, it is easy to generate image data sets containing millions of cells and structures. However, it is challenging to obtain large-scale high-quality annotations for AI models. Here, we present HALS (Human-Augmenting Labeling System), a human-in-the-loop data labeling AI, which begins uninitialized and learns annotations from a human, in real-time. Using a multi-part AI composed of three deep learning models, HALS learns from just a few examples and immediately decreases the workload of the annotator, while increasing the quality of their annotations. Using a highly repetitive use-case—annotating cell types—and running experiments with seven pathologists—experts at the microscopic analysis of biological specimens—we demonstrate a manual work reduction of 90.60%, and an average data-quality boost of 4.34%, measured across four use-cases and two tissue stain types.",project-academic
10.1109/MTS.2021.3056295,2021-03-15,a,Institute of Electrical and Electronics Engineers (IEEE),toward a more equal world the human rights approach to extending the benefits of artificial intelligence," We are all None aware of the huge potential for artificial intelligence (AI) to bring massive benefits to under-served populations, advancing equal access to public services such as health, education, social assistance, or public transportation, for example. We are equally aware that AI can drive inequality, concentrating wealth, resources, and decision-making power in the hands of a few countries, companies, or citizens. Artificial intelligence for equity (AI4Eq) None [1] None as presented in this magazine, calls upon academics, AI developers, civil society, and government policy-makers to work collaboratively toward a technological transformation that increases the benefits to society, reduces inequality, and aims to leave no one behind. A call for equity rests on the human rights principle of equality and nondiscrimination. AI design, development, and deployment (AI-DDD) can and should be harnessed to reduce inequality and increase the share of the world’s population that is able to live in dignity and fully realize their human potential. This commentary argues, first, that far preferable to an ethics framework, adopting a human rights framework for AI-DDD offers the potential for a robust and enforceable set of guidelines for the pursuit of AI4Eq. Second, the commentary introduces the work of IEEE in proposing practical recommendations for AI4Eq, so that people living in high-income countries (HICs), low- and middle-income countries (LMICs), alike, share AI applications’ widespread benefit to humanity.",project-academic
10.1145/3391403.3399458,2020-07-13,p,ACM,matching algorithms for blood donation," Managing perishable inventory, such as blood stock awaiting use by patients in need, has been a topic of research for decades. This has been investigated across several disciplines: medical and social scientists have investigated who donates blood, how frequently, and why; management science researchers have long studied the blood supply chain from a logistical perspective. Yet global demand for blood still far exceeds supply, and unmet need is greatest in low- and middle-income countries. Both academics and policy experts suggest that large-scale coordination is necessary to alleviate demand for donor blood. Using the recently-deployed Facebook Blood Donation tool, we conduct the first large-scale algorithmic matching of blood donors with donation opportunities. In both simulations and real experiments we match potential donors with opportunities, guided by a machine learning model trained on prior observations of donor behavior. While measuring actual donation rates remains a challenge, we measure donor action (i.e., calling a blood bank or making an appointment) as a proxy for actual donation. Simulations suggest that even a simple matching strategy can increase donor action rate by 10-15%; a pilot experiment with real donors finds a slightly smaller increase of roughly 5%. While overall action rates remain low, even this modest increase among donors in a global network corresponds to many thousands of more potential donors taking action toward donation. Further, observing donor action on a social network can shed light onto donor behavior and response to incentives. Our initial findings align with several observations made in the medical and social science literature regarding donor behavior.",project-academic
10.3905/JFDS.2019.1.1.075,2019-01-31,a,Institutional Investor Journals Umbrella,modeling analysts recommendations via bayesian machine learning," Individual analysts typically publish recommendations several times per year on the handful of stocks they follow within their specialized fields. How should investors interpret this information? How can they factor in the past performance of individual analysts when assessing whether to invest long or short in a stock? This is a complicated problem to model quantitatively: There are thousands of individual analysts, each of whom follows only a small subset of the thousands of stocks available for investment. Overcoming this inherent sparsity naturally raises the question of how to learn an analyst’s forecasting ability by integrating track-record information from all the stocks the analyst follows; in other words, inferring an analyst’s ability on Stock X from track records on both Stock X and stocks other than X. The authors address this topic using a state-of-the-art computationally rapid Bayesian machine learning technique called independent Bayesian classifier combination (IBCC), which has been deployed in the physical and biological sciences. The authors argue that there are many similarities between the analyst forecasting problem and a very successful application of IBCC in astronomy, a study in which it dominates heuristic alternatives including simple or weighted averages and majority voting. The IBCC technique is ideally suited to this particularly sparse problem, enabling computationally efficient inference, dynamic tracking of analyst performance through time, and real-time online forecasting. The results suggest the IBCC technique holds promise in extracting information that can be deployed in active discretionary and quantitative investment management.",project-academic
10.1002/ASI.23596,2016-10-01,p,"John Wiley & Sons, Ltd",map of science with topic modeling comparison of unsupervised learning and human assigned subject classification," The delineation of coordinates is fundamental for the cartography of science, and accurate and credible classification of scientific knowledge presents a persistent challenge in this regard. We present a map of Finnish science based on unsupervised-learning classification, and discuss the advantages and disadvantages of this approach vis-i-vis those generated by human reasoning. We conclude that from theoretical and practical perspectives there exist several challenges for human reasoning-based classification frameworks of scientific knowledge, as they typically try to fit new-to-the-world knowledge into historical models of scientific knowledge, and cannot easily be deployed for new large-scale data sets. Automated classification schemes, in contrast, generate classification models only from the available text corpus, thereby identifying credibly novel bodies of knowledge. They also lend themselves to versatile large-scale data analysis, and enable a range of Big Data possibilities. However, we also argue that it is neither possible nor fruitful to declare one or another method a superior approach in terms of realism to classify scientific knowledge, and we believe that the merits of each approach are dependent on the practical objectives of analysis.",project-academic
10.5555/2936924.2937000,2016-05-09,p,International Foundation for Autonomous Agents and Multiagent Systems,resource abstraction for reinforcement learning in multiagent congestion problems," Real-world congestion problems (e.g. traffic congestion) are typically very complex and large-scale. Multiagent reinforcement learning (MARL) is a promising candidate for dealing with this emerging complexity by providing an autonomous and distributed solution to these problems. However, there are three limiting factors that affect the deployability of MARL approaches to congestion problems. These are learning time, scalability and decentralised coordination i.e. no communication between the learning agents. In this paper we introduce Resource Abstraction, an approach that addresses these challenges by allocating the available resources into abstract groups. This abstraction creates new reward functions that provide a more informative signal to the learning agents and aid the coordination amongst them. Experimental work is conducted on two benchmark domains from the literature, an abstract congestion problem and a realistic traffic congestion problem. The current state-of-the-art for solving multiagent congestion problems is a form of reward shaping called difference rewards. We show that the system using Resource Abstraction significantly improves the learning speed and scalability, and achieves the highest possible or near-highest joint performance/social welfare for both congestion problems in large-scale scenarios involving up to 1000 reinforcement learning agents.",project-academic
,2019-03-13,a,,resource abstraction for reinforcement learning in multiagent congestion problems," Real-world congestion problems (e.g. traffic congestion) are typically very complex and large-scale. Multiagent reinforcement learning (MARL) is a promising candidate for dealing with this emerging complexity by providing an autonomous and distributed solution to these problems. However, there are three limiting factors that affect the deployability of MARL approaches to congestion problems. These are learning time, scalability and decentralised coordination i.e. no communication between the learning agents. In this paper we introduce Resource Abstraction, an approach that addresses these challenges by allocating the available resources into abstract groups. This abstraction creates new reward functions that provide a more informative signal to the learning agents and aid the coordination amongst them. Experimental work is conducted on two benchmark domains from the literature, an abstract congestion problem and a realistic traffic congestion problem. The current state-of-the-art for solving multiagent congestion problems is a form of reward shaping called difference rewards. We show that the system using Resource Abstraction significantly improves the learning speed and scalability, and achieves the highest possible or near-highest joint performance/social welfare for both congestion problems in large-scale scenarios involving up to 1000 reinforcement learning agents.",project-academic
,2019-09-11,a,,ores lowering barriers with participatory machine learning in wikipedia," Algorithmic systems---from rule-based bots to machine learning classifiers---have a long history of supporting the essential work of content moderation and other curation work in peer production projects. From counter-vandalism to task routing, basic machine prediction has allowed open knowledge projects like Wikipedia to scale to the largest encyclopedia in the world, while maintaining quality and consistency. However, conversations about how quality control should work and what role algorithms should play have generally been led by the expert engineers who have the skills and resources to develop and modify these complex algorithmic systems. In this paper, we describe ORES: an algorithmic scoring service that supports real-time scoring of wiki edits using multiple independent classifiers trained on different datasets. ORES decouples several activities that have typically all been performed by engineers: choosing or curating training data, building models to serve predictions, auditing predictions, and developing interfaces or automated agents that act on those predictions. This meta-algorithmic system was designed to open up socio-technical conversations about algorithms in Wikipedia to a broader set of participants. In this paper, we discuss the theoretical mechanisms of social change ORES enables and detail case studies in participatory machine learning around ORES from the 5 years since its deployment.",project-academic
10.1145/3415219,2020-10-14,a,Association for Computing Machinery (ACM),ores lowering barriers with participatory machine learning in wikipedia," Algorithmic systems---from rule-based bots to machine learning classifiers---have a long history of supporting the essential work of content moderation and other curation work in peer production projects. From counter-vandalism to task routing, basic machine prediction has allowed open knowledge projects like Wikipedia to scale to the largest encyclopedia in the world, while maintaining quality and consistency. However, conversations about how quality control should work and what role algorithms should play have generally been led by the expert engineers who have the skills and resources to develop and modify these complex algorithmic systems. In this paper, we describe ORES: an algorithmic scoring service that supports real-time scoring of wiki edits using multiple independent classifiers trained on different datasets. ORES decouples several activities that have typically all been performed by engineers: choosing or curating training data, building models to serve predictions, auditing predictions, and developing interfaces or automated agents that act on those predictions. This meta-algorithmic system was designed to open up socio-technical conversations about algorithms in Wikipedia to a broader set of participants. In this paper, we discuss the theoretical mechanisms of social change ORES enables and detail case studies in participatory machine learning around ORES from the 5 years since its deployment.",project-academic
10.1016/J.IPM.2018.04.011,2019-05-01,a,Pergamon,real time processing of social media with sentinel a syndromic surveillance system incorporating deep learning for health classification," Abstract None None Interest in real-time syndromic surveillance based on social media data has greatly increased in recent years. The ability to detect disease outbreaks earlier than traditional methods would be highly useful for public health officials. This paper describes a software system which is built upon recent developments in machine learning and data processing to achieve this goal. The system is built from reusable modules integrated into data processing pipelines that are easily deployable and configurable. It applies deep learning to the problem of classifying health-related tweets and is able to do so with high accuracy. It has the capability to detect illness outbreaks from Twitter data and then to build up and display information about these outbreaks, including relevant news articles, to provide situational awareness. It also provides nowcasting functionality of current disease levels from previous clinical data combined with Twitter data. None The preliminary results are promising, with the system being able to detect outbreaks of influenza-like illness symptoms which could then be confirmed by existing official sources. The Nowcasting module shows that using social media data can improve prediction for multiple diseases over simply using traditional data sources.",project-academic
10.26434/CHEMRXIV.12609899.V1,2020-07-06,a,,the photoswitch dataset a molecular machine learning benchmark for the advancement of synthetic chemistry," The space of synthesizable molecules is greater than $10^{60}$, meaning only a vanishingly small fraction of these molecules have ever been realized in the lab. In order to prioritize which regions of this space to explore next, synthetic chemists need access to accurate molecular property predictions. While great advances in molecular machine learning have been made, there is a dearth of benchmarks featuring properties that are useful for the synthetic chemist. Focussing directly on the needs of the synthetic chemist, we introduce the Photoswitch Dataset, a new benchmark for molecular machine learning where improvements in model performance can be immediately observed in the throughput of promising molecules synthesized in the lab. Photoswitches are a versatile class of molecule for medical and renewable energy applications where a molecule's efficacy is governed by its electronic transition wavelengths. We demonstrate superior performance in predicting these wavelengths compared to both time-dependent density functional theory (TD-DFT), the incumbent first principles quantum mechanical approach, as well as a panel of human experts. Our baseline models are currently being deployed in the lab as part of the decision process for candidate synthesis. It is our hope that this benchmark can drive real discoveries in photoswitch chemistry and that future benchmarks can be introduced to pivot learning algorithm development to benefit more expansive areas of synthetic chemistry.",project-academic
,2020-06-28,a,,the photoswitch dataset a molecular machine learning benchmark for the advancement of synthetic chemistry," The space of synthesizable molecules is greater than $10^{60}$, meaning only a vanishingly small fraction of these molecules have ever been realized in the lab. In order to prioritize which regions of this space to explore next, synthetic chemists need access to accurate molecular property predictions. While great advances in molecular machine learning have been made, there is a dearth of benchmarks featuring properties that are useful for the synthetic chemist. Focussing directly on the needs of the synthetic chemist, we introduce the Photoswitch Dataset, a new benchmark for molecular machine learning where improvements in model performance can be immediately observed in the throughput of promising molecules synthesized in the lab. Photoswitches are a versatile class of molecule for medical and renewable energy applications where a molecule's efficacy is governed by its electronic transition wavelengths. We demonstrate superior performance in predicting these wavelengths compared to both time-dependent density functional theory (TD-DFT), the incumbent first principles quantum mechanical approach, as well as a panel of human experts. Our baseline models are currently being deployed in the lab as part of the decision process for candidate synthesis. It is our hope that this benchmark can drive real discoveries in photoswitch chemistry and that future benchmarks can be introduced to pivot learning algorithm development to benefit more expansive areas of synthetic chemistry.",project-academic
10.1109/UIC-ATC-SCALCOM.2014.48,2014-12-09,p,IEEE,detecting suicidal ideation in chinese microblogs with psychological lexicons," Suicide is among the leading causes of death in China. However, technical approaches toward preventing suicide are challenging and remaining under development. Recently, several actual suicidal cases were preceded by users who posted micro blogs with suicidal ideation to Sina Weibo, a Chinese social media network akin to Twitter. It would therefore be desirable to detect suicidal ideations from micro blogs in real-time, and immediately alert appropriate support groups, which may lead to successful prevention. In this paper, we propose a real-time suicidal ideation detection system deployed over Weibo, using machine learning and known psychological techniques. Currently, we have identified 53 known suicidal cases who posted suicide notes on Weibo prior to their deaths. We explore linguistic features of these known cases using a psychological lexicon dictionary, and train an effective suicidal Weibo post detection model. 6714 tagged posts and several classifiers are used to verify the model. By combining both machine learning and psychological knowledge, SVM classifier has the best performance of different classifiers, yielding an F-measure of 68:3%, a Precision of 78:9%, and a Recall of 60:3%.",project-academic
10.1186/S42400-020-00064-4,2020-12-01,a,SpringerOpen,sifu a cybersecurity awareness platform with challenge assessment and intelligent coach," Software vulnerabilities, when actively exploited by malicious parties, can lead to catastrophic consequences. Proper handling of software vulnerabilities is essential in the industrial context, particularly when the software is deployed in critical infrastructures. Therefore, several industrial standards mandate secure coding guidelines and industrial software developers’ training, as software quality is a significant contributor to secure software. CyberSecurity Challenges (CSC) form a method that combines serious game techniques with cybersecurity and secure coding guidelines to raise secure coding awareness of software developers in the industry. These cybersecurity awareness events have been used with success in industrial environments. However, until now, these coached events took place on-site. In the present work, we briefly introduce cybersecurity challenges and propose a novel platform that allows these events to take place online. The introduced cybersecurity awareness platform, which the authors call Sifu, performs automatic assessment of challenges in compliance to secure coding guidelines, and uses an artificial intelligence method to provide players with solution-guiding hints. Furthermore, due to its characteristics, the Sifu platform allows for remote (online) learning, in times of social distancing. The CyberSecurity Challenges events based on the Sifu platform were evaluated during four online real-life CSC events. We report on three surveys showing that the Sifu platform’s CSC events are adequate to raise industry software developers awareness on secure coding.",project-academic
10.1109/TKDE.2020.3021256,2020-09-02,a,Institute of Electrical and Electronics Engineers (IEEE),conna addressing name disambiguation on the fly," Name disambiguation is a key and also a very tough problem in many online systems such as social search and academic search. Despite considerable research, a critical issue that has not been systematically studied is disambiguation on the fly--- to complete the disambiguation in the real-time. This is very challenging, as the disambiguation algorithm must be accurate, efficient, and error tolerance. In this paper, we propose a novel framework --- CONNA --- to train a matching component and a decision component jointly via reinforcement learning. The matching component is responsible for finding the top matched candidate for the given paper, and the decision component is responsible for deciding on assigning the top matched person or creating a new person. The two components are intertwined and can be bootstrapped via jointly training. Empirically, we evaluate CONNA on two name disambiguation datasets. Experimental results show that the proposed framework can achieve a 1.21%-19.84% improvement on F1-score using joint training of the matching and the decision components. The proposed CONNA has been successfully deployed on AMiner --- a large online academic search system.",project-academic
,2019-10-27,a,,conna addressing name disambiguation on the fly," Name disambiguation is a key and also a very tough problem in many online systems such as social search and academic search. Despite considerable research, a critical issue that has not been systematically studied is disambiguation on the fly -- to complete the disambiguation in the real-time. This is very challenging, as the disambiguation algorithm must be accurate, efficient, and error tolerance. In this paper, we propose a novel framework -- CONNA -- to train a matching component and a decision component jointly via reinforcement learning. The matching component is responsible for finding the top matched candidate for the given paper, and the decision component is responsible for deciding on assigning the top matched person or creating a new person. The two components are intertwined and can be bootstrapped via jointly training. Empirically, we evaluate CONNA on two name disambiguation datasets. Experimental results show that the proposed framework can achieve a 1.21%-19.84% improvement on F1-score using joint training of the matching and the decision components. The proposed CONNA has been successfully deployed on AMiner -- a large online academic search system.",project-academic
,2015-10-01,p,,topic model for identifying suicidal ideation in chinese microblog," Suicide is one of major public health problems worldwide. Traditionally, suicidal ideation is assessed by surveys or interviews, which lacks of a real-time assessment of personal mental state. Online social networks, with large amount of user-generated data, offer opportunities to gain insights of suicide assessment and prevention. In this paper, we explore potentiality to identify and monitor suicide expressed in microblog on social networks. First, we identify users who have committed suicide and collect millions of microblogs from social networks. Second, we build suicide psychological lexicon by psychological standards and word embedding technique. Third, by leveraging both language styles and online behaviors, we employ Topic Model and other machine learning algorithms to identify suicidal ideation. Our approach achieves the best results on topic-500, yielding F1 − measure of 80.0%, Precision of 87.1%, Recall of 73.9%, and Accuracy of 93.2%. Furthermore, a prototype system for monitoring suicidal ideation on several social networks is deployed.",project-academic
,2009-04-01,a,"International Forum of Educational Technology & Society. Athabasca University, School of Computing & Information Systems, 1 University Drive, Athabasca, AB T9S 3A3, Canada. Tel: 780-675-6812; Fax: 780-675-6973; Web site: http://www.ifets.info",evaluating an intelligent tutoring system for design patterns the depths experience," Introduction Software design patterns (DPs) have been recognized as very important and useful in real software development since they provide an elegant way of getting around problems that often occur. To be more precise, DPs are common conceptual structures that describe successful solutions to common and recurring software design problems. They can be applied over and over again when analyzing, designing, and developing software applications in diverse contexts (Harrer & Devedzic 2002). DPs are aimed at seamless reuse of software designs and architectures that have already proven their effectiveness in practice. In addition, their application guaranties high quality software solutions that are easy to maintain and extend. To make use of the benefits that a DP-based software development offers, one has to master DPs both in theory and practice. Accordingly, it is very important to devise an effective approach for teaching DPs. Our experience in teaching object-oriented programming to software engineering students, suggests that there is no single best way to learn DPs. It depends on the students' knowledge of and experience in the field of software engineering in general and design patterns in particular. For example, beginners in this field are often confused by the number of available patterns, different forms for presenting design patterns and plenty of available sources of information (Raising 1998). It would be best for them to be provided with a step-by-step introduction into the field and strict instructions on how to conduct each step (i.e., what to learn, in which manner, and what resources to use). In this case, sources of information should be carefully selected in order to avoid the threat of using a source providing information that is too difficult for a beginner to understand. On the other hand, for experienced programmers this way of strictly guided learning is not suitable. Rather, exploratory learning would be more appropriate learning strategy to use. It assumes giving a student control over his (since gender-neutral language tends to be imprecise and/or cumbersome, throughout the paper we arbitrarily decided to use masculine pronoun for each generic reference to a student) learning process. In particular, the student chooses on his own what to learn (topics and their order), when to learn and how fast to proceed, at which level of difficulty to learn, and in what way to learn. Such a student has to have access to many different information sources to explore, and thus be enabled to develop his own understanding of the topics explored. Exploratory learners are always intrinsically motivated. It is not possible to force this learning style upon a learner (Holzkamp, 1995). To sum up, students come from different knowledge backgrounds and have different learning styles and preferences. To effectively meet these diversities, an e-learning system should be able to offer different learning experiences to different students. We found that an intelligent tutoring system (ITS) for learning DPs can successfully address these issues, since it can provide different learning strategies for different types of students, as well as, adapt learning content according to the student's performance. ITSs use artificial intelligence's techniques and methods to provide a student with a content that is neither too easy (thus avoiding the drop of concentration), nor above his/her ability to understand (hence not frustrating the student). ITSs make it possible to deploy a course suitable for students with widely varying needs and levels of motivation. We have developed DEPTHS (Design Patterns Teaching Help System), an ITS for learning DPs, as a part of our research in the area of teaching DPs in higher education (Jeremic & Devedzic 2004). Our goal was to provide students with the benefits of one-on-one instruction in a cost effective manner. The system was primarily intended for teaching undergraduate students of Computer Science, but it can be equally successful in other education settings, as well. …",project-academic
10.1109/ICRA.2017.7989184,2017-05-01,p,,rapidly exploring learning trees," Inverse Reinforcement Learning (IRL) for path planning enables robots to learn cost functions for difficult tasks from demonstration, instead of hard-coding them. However, IRL methods face practical limitations that stem from the need to repeat costly planning procedures. In this paper, we propose Rapidly Exploring Learning Trees (RLT∗), which learns the cost functions of Optimal Rapidly Exploring Random Trees (RRT∗) from demonstration, thereby making inverse learning methods applicable to more complex tasks. Our approach extends Maximum Margin Planning to work with RRT∗ cost functions. Furthermore, we propose a caching scheme that greatly reduces the computational cost of this approach. Experimental results on simulated and real-robot data from a social navigation scenario show that RLT∗ achieves better performance at lower computational cost than existing methods. We also successfully deploy control policies learned with RLT∗ on a real telepresence robot.",project-academic
,2008-11-01,b,,distributed intelligent systems a coordination perspective," Distributed Intelligent Systems: A Coordination Perspective addresses and comprehensively answers commonly asked questions about coordination in agent-oriented distributed systems. Characterizing the state-of-the-art research in the field of coordination with regard to the development of distributed agent-oriented systems is a particularly complex endeavour, as the space of available approaches is indeed considerable, and research is independently conducted in a great number of domains. While existing books deal with specific aspects of coordination, the major contribution of this book lies in the attempt to provide an in-depth review covering a wide range of issues regarding multi-agent coordination in Distributed Artificial Intelligence. In addition to reporting various sources of confusion, this book outlines the existence of a plethora of strategies and techniques adapted to different problems, agent-oriented systems, environments, and domains. In short, the current book identifies the absence of a single unified approach in addressing multi-agent coordination problems arising in any system or organization. This book, written by world-class leaders in this field, dedicates itself to providing a state-of-the-art review of current coordination strategies and techniques. The book also describes a broad range of application domains which implement many of the coordination strategies and techniques from the field of multi-agent systems. The application domains include defense, transportation, health care, telecommunication and e-business. Based on current practical deployed applications and existing capabilities, this book also identifies and thoroughly examines trends, challenges, and future agent-oriented research directions. Key features: Unveils the lack of coherence and order that characterizes the area of research pertaining to coordination of distributed intelligent systems Examines coordination models, frameworks, strategies and techniques to enable the development of distributed intelligent agent-oriented systems. Provides specific recommendations to realize more widespread deployment of agent-based systems Distributed Intelligent Systems: A Coordination Perspective is designed for a professional audience composed of practitioners and researchers in industry. This book is also suitable as a reference or secondary textbook for advanced-level students in computer science and engineering.",project-academic
10.1016/J.JOCS.2020.101107,2020-03-12,a,Elsevier,machine learning surrogates for molecular dynamics simulations of soft materials," Abstract None None Molecular dynamics (MD) simulations accelerated by high-performance computing (HPC) methods are powerful tools to investigate and extract the microscopic mechanisms characterizing the properties of soft materials such as self-assembled nanoparticles, virus capsids, confined electrolytes, and polymeric fluids. In this paper, we extend the idea developed in our earlier work of integrating machine learning (ML) methods with HPC-accelerated MD simulations of soft materials in order to enhance their predictive power and advance their applications for research and educational activities. Parallelized MD simulations of self-assembling ions in nanoconfinement are employed to demonstrate our approach. We find that an artificial neural network-based regression model successfully learns nearly all the interesting features associated with the output ionic density profiles over a broad range of ionic system parameters. The ML model generates predictions that are in excellent agreement with the results from MD simulations. The inference time associated with the ML model is over a factor of 10,000 smaller than the corresponding parallel MD simulation time. Through this demonstration, we introduce a “machine learning surrogate” for MD simulations of soft-matter systems. We develop and deploy a web application on nanoHUB to realize the advantages associated with the ML surrogate. The results demonstrate that the performance of MD simulations can be further enhanced by using ML, enabling rapid and accurate simulation-driven exploration of the soft material design space.",project-academic
10.1145/3292500.3330702,2019-07-25,p,ACM,characterizing and detecting malicious accounts in privacy centric mobile social networks a case study," Malicious accounts are one of the biggest threats to the security and privacy of online social networks (OSNs). In this work, we study a new type of OSN, called privacy-centric mobile social network (PC-MSN), such as KakaoTalk and LINE, which has attracted billions of users recently. The design of PC-MSN is inspired to protect their users' privacy from strangers: (1) a stranger is not easy to send a friend request to a user who does not want to make friends with strangers; and (2) strangers cannot view a user's post. Such a design mitigates the security issue of malicious accounts. At the same time, it also brings the battleground between attackers and defenders to an earlier stage, i.e., making friendship, than the one studied in previous works. Also, previous defense proposals mostly rely on certain assumptions on the attacker, which may not be robust in the new PC-MSNs. As a result, previous malicious accounts detection approaches are less effective on a PC-MSN. To mitigate this issue, we study the patterns in friend requests to distinguish malicious accounts, and perform a systematic study over 1 million labeled data from WLink, a real PC-MSN with billions of users, to confirm our hypothesis. Based on the results, we propose dozens of new features and leverage machine learning to detect malicious accounts. We evaluate our method and compare it with existing methods, and the results show that our method achieves a precision of 99.5% and a recall of 98.4%, which significantly outperform previous state-of-the-art methods. Importantly, we qualitatively analyze the robustness of the designed features, and our evaluation shows that using only robust features can achieve the same level of performance as using all features. WLink has deployed our detection method. Our method can detect 0.59 million malicious accounts daily, which is 6 times higher than the previous deployment on WLink, with a precision of over 90%.",project-academic
10.1109/ACII.2019.8925479,2019-08-31,p,IEEE,imputing missing social media data stream in multisensor studies of human behavior," The ubiquitous use of social media enables researchers to obtain self-recorded longitudinal data of individuals in real-time. Because this data can be collected in an inexpensive and unobtrusive way at scale, social media has been adopted as a “passive sensor” to study human behavior. However, such research is impacted by the lack of homogeneity in the use of social media, and the engineering challenges in obtaining such data. This paper proposes a statistical framework to leverage the potential of social media in sensing studies of human behavior, while navigating the challenges associated with its sparsity. Our framework is situated in a large-scale in-situ study concerning the passive assessment of psychological constructs of 757 information workers wherein of four sensing streams was deployed - bluetooth beacons, wearable, smartphone, and social media. Our framework includes principled feature transformation and machine learning models that predict latent social media features from the other passive sensors. We demonstrate the efficacy of this imputation framework via a high correlation of 0.78 between actual and imputed social media features. With the imputed features we test and validate predictions on psychological constructs like personality traits and affect. We find that adding the social media data streams, in their imputed form, improves the prediction of these measures. We discuss how our framework can be valuable in multimodal sensing studies that aim to gather comprehensive signals about an individual's state or situation.",project-academic
,2020-05-11,a,,using computer vision to enhance safety of workforce in manufacturing in a post covid world," The COVID-19 pandemic forced governments across the world to impose lockdowns to prevent virus transmissions. This resulted in the shutdown of all economic activity and accordingly the production at manufacturing plants across most sectors was halted. While there is an urgency to resume production, there is an even greater need to ensure the safety of the workforce at the plant site. Reports indicate that maintaining social distancing and wearing face masks while at work clearly reduces the risk of transmission. We decided to use computer vision on CCTV feeds to monitor worker activity and detect violations which trigger real time voice alerts on the shop floor. This paper describes an efficient and economic approach of using AI to create a safe environment in a manufacturing setup. We demonstrate our approach to build a robust social distancing measurement algorithm using a mix of modern-day deep learning and classic projective geometry techniques. We have deployed our solution at manufacturing plants across the Aditya Birla Group (ABG). We have also described our face mask detection approach which provides a high accuracy across a range of customized masks.",project-academic
10.1002/ADMA.202003018,2020-11-01,a,Wiley,oxide based electrolyte gated transistors for spatiotemporal information processing," Spiking neural networks (SNNs) sharing large similarity with biological nervous systems are promising to process spatiotemporal information and can provide highly time- and energy-efficient computational paradigms for the Internet-of-Things and edge computing. Nonvolatile electrolyte-gated transistors (EGTs) provide prominent analog switching performance, the most critical feature of synaptic element, and have been recently demonstrated as a promising synaptic device. However, high performance, large-scale EGT arrays, and EGT application for spatiotemporal information processing in an SNN are yet to be demonstrated. Here, an oxide-based EGT employing amorphous Nb2 O5 and Lix SiO2 is introduced as the channel and electrolyte gate materials, respectively, and integrated into a 32 × 32 EGT array. The engineered EGTs show a quasi-linear update, good endurance (106 ) and retention, a high switching speed of 100 ns, ultralow readout conductance (<100 nS), and ultralow areal switching energy density (20 fJ µm-2 ). The prominent analog switching performance is leveraged for hardware implementation of an SNN with the capability of spatiotemporal information processing, where spike sequences with different timings are able to be efficiently learned and recognized by the EGT array. Finally, this EGT-based spatiotemporal information processing is deployed to detect moving orientation in a tactile sensing system. These results provide an insight into oxide-based EGT devices for energy-efficient neuromorphic computing to support edge application.",project-academic
10.1109/JSTARS.2020.2991588,2020-05-29,a,IEEE,deep learning driven detection and mapping of rockfalls on mars," The analysis of rockfall distribution and magnitude is a useful tool to study the past and current endogenic and exogenic activity of Mars. At the same time, tracks left by rockfalls provide insights into the mechanical properties of the Martian surface. While a wealth of high-resolution spaceborne image data are available, manual mapping of displaced boulders with tracks is inefficient and slow, resulting in: 1) a small total number of mapped features; 2) inadequate statistics; and 3) a suboptimal utilization of the available big data. This study implements a deep learning-driven approach to automatically detect and map Martian boulders with tracks in high resolution imaging science experiment (HiRISE) imagery. Six off-the-shelf neural networks have been trained either on Martian or lunar rockfall data, or a combination of both, and are able to achieve a maximum overall recall of up to 0.78 and a maximum overall precision of up to 1.0, with a mean average precision of 0.71. The fusion of training data from different planets and sensors results in an increased detection precision, highlighting the value of domain generalization and multidomain learning. Average processing time per HiRISE image is ∼45 s using an NVIDIA Titan Xp, which is more than one order of magnitude faster than a human operator. The developed deep learning-driven infrastructure can be deployed to map Martian rockfalls on a global scale and within a realistic timeframe.",project-academic
10.1007/978-3-319-57454-7_15,2017-05-23,p,"Springer, Cham",personalized deep learning for tag recommendation," Social media services deploy tag recommendation systems to facilitate the process of tagging objects which depends on the information of both the user’s preferences and the tagged object. However, most image tag recommender systems do not consider the additional information provided by the uploaded image but rely only on textual information, or make use of simple low-level image features. In this paper, we propose a personalized deep learning approach for the image tag recommendation that considers the user’s preferences, as well as visual information. We employ Convolutional Neural Networks (CNNs), which already provide excellent performance for image classification and recognition, to obtain visual features from images in a supervised way. We provide empirical evidence that features selected in this fashion improve the capability of tag recommender systems, compared to the current state of the art that is using hand-crafted visual features, or is solely based on the tagging history information. The proposed method yields up to at least two percent accuracy improvement in two real world datasets, namely NUS-WIDE and Flickr-PTR.",project-academic
10.1038/NPHOTON.2014.249,2014-12-01,a,Nature Publishing Group,network of time multiplexed optical parametric oscillators as a coherent ising machine," A network of four degenerate optical parametric oscillators (OPOs) is employed to find the ground state of the Ising Hamiltonian. The good performance of the network reveals the potential of OPOs for many similar problems. Finding the ground states of the Ising Hamiltonian1 maps to various combinatorial optimization problems in biology, medicine, wireless communications, artificial intelligence and social network. So far, no efficient classical and quantum algorithm is known for these problems and intensive research is focused on creating physical systems—Ising machines—capable of finding the absolute or approximate ground states of the Ising Hamiltonian2,3,4,5,6. Here, we report an Ising machine using a network of degenerate optical parametric oscillators (OPOs). Spins are represented with above-threshold binary phases of the OPOs and the Ising couplings are realized by mutual injections7. The network is implemented in a single OPO ring cavity with multiple trains of femtosecond pulses and configurable mutual couplings, and operates at room temperature. We programmed a small non-deterministic polynomial time-hard problem on a 4-OPO Ising machine and in 1,000 runs no computational error was detected.",project-academic
10.1145/2162081.2162089,2012-02-28,p,ACM,walksafe a pedestrian safety app for mobile phone users who walk and talk while crossing roads," Research in social science has shown that mobile phone conversations distract users, presenting a significant impact to pedestrian safety; for example, a mobile phone user deep in conversation while crossing a street is generally more at risk than other pedestrians not engaged in such behavior. We propose WalkSafe, an Android smartphone application that aids people that walk and talk, improving the safety of pedestrian mobile phone users. WalkSafe uses the back camera of the mobile phone to detect vehicles approaching the user, alerting the user of a potentially unsafe situation; more specifically WalkSafe i) uses machine learning algorithms implemented on the phone to detect the front views and back views of moving vehicles and ii) exploits phone APIs to save energy by running the vehicle detection algorithm only during active calls. We present our initial design, implementation and evaluation of the WalkSafe App that is capable of real-time detection of the front and back views of cars, indicating cars are approaching or moving away from the user, respectively. WalkSafe is implemented on Android phones and alerts the user of unsafe conditions using sound and vibration from the phone. WalkSafe is available on Android Market.",project-academic
10.1186/S40537-019-0212-5,2019-12-01,a,SpringerOpen,intelligent video surveillance a review through deep learning techniques for crowd analysis," Big data applications are consuming most of the space in industry and research area. Among the widespread examples of big data, the role of video streams from CCTV cameras is equally important as other sources like social media data, sensor data, agriculture data, medical data and data evolved from space research. Surveillance videos have a major contribution in unstructured big data. CCTV cameras are implemented in all places where security having much importance. Manual surveillance seems tedious and time consuming. Security can be defined in different terms in different contexts like theft identification, violence detection, chances of explosion etc. In crowded public places the term security covers almost all type of abnormal events. Among them violence detection is difficult to handle since it involves group activity. The anomalous or abnormal activity analysis in a crowd video scene is very difficult due to several real world constraints. The paper includes a deep rooted survey which starts from object recognition, action recognition, crowd analysis and finally violence detection in a crowd environment. Majority of the papers reviewed in this survey are based on deep learning technique. Various deep learning methods are compared in terms of their algorithms and models. The main focus of this survey is application of deep learning techniques in detecting the exact count, involved persons and the happened activity in a large crowd at all climate conditions. Paper discusses the underlying deep learning implementation technology involved in various crowd video analysis methods. Real time processing, an important issue which is yet to be explored more in this field is also considered. Not many methods are there in handling all these issues simultaneously. The issues recognized in existing methods are identified and summarized. Also future direction is given to reduce the obstacles identified. The survey provides a bibliographic summary of papers from ScienceDirect, IEEE Xplore and ACM digital library.",project-academic
,2002-07-01,b,"Addison-Wesley Longman Publishing Co., Inc.",xml topic maps creating and using topic maps for the web," From the Book:
A human being is part of a whole, called by us the ""Universe,"" a part limited in time and space. He experiences himself, his thoughts and feelings, as something separated from the resta kind of optical delusion of his consciousness. This delusion is a kind of prison for us, restricting us to our personal desires and to affection for a few persons nearest us. Our task must be to free ourselves from this prison by widening our circles of compassion to embrace all living creatures and the whole of nature in its beauty. Albert Einstein, What I Believe, 1930 

In a former life, I built microprocessor-based data acquisition systems, originally for locating and monitoring wind and solar energy systems. I suppose it is fair to say that I have long been involved in roaming solution space. Along the way, farmers, on whose land the energy systems were often situated, discovered that my monitoring tools would help them form better predictions of fruit frost, irrigation needs, and pesticide needs. My program, which ran on an Apple II that had telephone access to the distributed monitoring stations, printed out large piles of data. Epiphany happened on the day that a manager of one of those monitoring systems came to me and asked ""What else is this data good for?"" That was the day I entered the field of artificial intelligence, looking for ways to organize all that data and mine it for new knowledge. 

A recent issue of a National Public Radio discussion focused on the nature and future of literature. Listening to that discussion while navigating the perils of Palo Alto traffic, I heard two comments that I shall paraphrase, with emphasis placed according to my ownwhims, as follows: 

In the past, we turned to the great works of literature to ponder what is life. Today, we turn to the great works of science to ponder the same issues. 

In some sense, the message I pulled out of that is that we (thatUs the really big we) tend to appeal to science and technology to find comfort and solutions to our daily needs. In that same sense, I found justification for this book and the vision I had when the book was conceived. Make no mistake here, I already had plenty of justification for the vision and the book; as is often pontificated by many, we are engulfed in a kind of information overload that threatens to choke off our ability to solve major problems that face all of humanity. 

No, the vision is not an expression of doom and gloom. Rather, it is an expression of my own deep and optimistic belief that it is through education, through an enriched human intellect that solutions will be found, or at least, the solution space will become a more productive environment in which to operate. The vision expressed here is well grounded in the need to organize and mine data, all part of the solution space. 

While walking along a corridor at an XML conference in San Jose early in the year 2000, I noticed a sign that said Topic Maps, with an arrow pointing to the right. I proceeded immediately to execute a personal ""column right"" command, entered a room, and met Steve Newcomb. The rest all makes sense; while in Paris later that year, I saw the need to take the XTM technology to the public. This book was then conceived at XML2000 in Paris, and several authors signed on immediately. This book came with a larger vision than simply taking XTM to the public. I saw topic maps as an important tool in solution space. The vision included much more; topic maps are just one of many tools in that space. I wanted to start a book series, one that is thematically associated with my view of solution space. 

This book is the first in a book series, flying under the moniker Open Knowledge Systems. By using the word open, I am saying that the series is about making the tools and information required to operate in solution space completely open and available to all who would participate. ""Open"" implies that each book in the series intends to include an Open Source Software project, one that enables all readers to immediately ""play in the sandbox"" and, hopefully, go beyond by extending the software and contributing that new experience to solution space. 

Each contribution to the Open Knowledge Systems series is intended to be a living document, meaning that each work will be available at a web site, the entire content of which will be browsable and supported with an online forum such that topics discussed in the books can be further discussed online. 

This book is about Topic Maps, particularly Topic Maps implemented in the XTM Version 1.0 Standard format, as conceived by the XTM Authoring Group, which was started by an experienced group of individuals along with the vision and guidance of Steven Newcomb and Michel Biezunski, both contributing authors in this book. As with many new technologies, the XTM standard is, in most regards, not yet complete. In fact, a standard like XTM can never be complete simply because such standards must co-evolve with the environment in which they are applied. In the same vein, a book such as this cannot be a coherent work, simply because much of what is evolving now is subject to differing opinions, views, and so forth. 

Because of my view that solution space, itself, is co-evolving along with the participants in that space, I have adopted an editorial management style that I suspect should be explained. My style is based on the understanding that I am combining contributions from many different individuals, each with a potentially different worldview, and each with a different writing style. The content focus of this book is, of course, on Topic Maps, but I believe that it is not necessary to force a coherent worldview on the different authors; it is my hope that readers, and, indeed, solution space will profit by way of exposure to differing views and opinions. There will, by the very nature of this policy, be controversy. Indeed, we are exploring the vast universe of discourse on the topic of knowledge, and there exists plenty of controversy just in that sand box alone. 

There is also the possibility of overlap. Some chapters are likely to offer the same or similar, or even differing points of view on the same point. Case in point: knowledge representation. We have several chapters, one on Ontological Engineering, one on Knowledge Representation, and one on Knowledge Organization. Two talk in some detail about semantic networks, and others go heavily into how people learn. ItUs awfully easy to see just how these can overlap, and they do. My management style has been that which falls out of research in Chaos: use the least amount of central management; let the authors sort it out for themselves. History will tell us if this approach works.",project-academic
10.1038/S41467-020-18098-0,2019-09-14,a,,committee machines a universal method to deal with non idealities in memristor based neural networks," Artificial neural networks are notoriously power- and time-consuming when implemented on conventional von Neumann computing systems. Consequently, recent years have seen an emergence of research in machine learning hardware that strives to bring memory and computing closer together. A popular approach is to realise artificial neural networks in hardware by implementing their synaptic weights using memristive devices. However, various device- and system-level non-idealities usually prevent these physical implementations from achieving high inference accuracy. We suggest applying a well-known concept in computer science -- committee machines -- in the context of memristor-based neural networks. Using simulations and experimental data from three different types of memristive devices, we show that committee machines employing ensemble averaging can successfully increase inference accuracy in physically implemented neural networks that suffer from faulty devices, device-to-device variability, random telegraph noise and line resistance. Importantly, we demonstrate that the accuracy can be improved even without increasing the total number of memristors.",project-academic
10.1093/BIOINFORMATICS/BTZ470,2020-01-01,a,Bioinformatics,scaling tree based automated machine learning to biomedical big data with a feature set selector," Motivation None Automated machine learning (AutoML) systems are helpful data science assistants designed to scan data for novel features, select appropriate supervised learning models and optimize their parameters. For this purpose, Tree-based Pipeline Optimization Tool (TPOT) was developed using strongly typed genetic programing (GP) to recommend an optimized analysis pipeline for the data scientist's prediction problem. However, like other AutoML systems, TPOT may reach computational resource limits when working on big data such as whole-genome expression data. None Results None We introduce two new features implemented in TPOT that helps increase the system's scalability: Feature Set Selector (FSS) and Template. FSS provides the option to specify subsets of the features as separate datasets, assuming the signals come from one or more of these specific data subsets. FSS increases TPOT's efficiency in application on big data by slicing the entire dataset into smaller sets of features and allowing GP to select the best subset in the final pipeline. Template enforces type constraints with strongly typed GP and enables the incorporation of FSS at the beginning of each pipeline. Consequently, FSS and Template help reduce TPOT computation time and may provide more interpretable results. Our simulations show TPOT-FSS significantly outperforms a tuned XGBoost model and standard TPOT implementation. We apply TPOT-FSS to real RNA-Seq data from a study of major depressive disorder. Independent of the previous study that identified significant association with depression severity of two modules, TPOT-FSS corroborates that one of the modules is largely predictive of the clinical diagnosis of each individual. None Availability and implementation None Detailed simulation and analysis code needed to reproduce the results in this study is available at https://github.com/lelaboratoire/tpot-fss. Implementation of the new TPOT operators is available at https://github.com/EpistasisLab/tpot. None Supplementary information None Supplementary data are available at Bioinformatics online.",project-academic
10.1002/MP.12600,2017-12-01,a,Med Phys,a parallel mr imaging method using multilayer perceptron," Purpose
To reconstruct MR images from subsampled data, we propose a fast reconstruction method using the multilayer perceptron (MLP) algorithm.

Methods and Materials
We applied MLP to reduce aliasing artifacts generated by subsampling in k-space. The MLP is learned from training data to map aliased input images into desired alias-free images. The input of the MLP is all voxels in the aliased lines of multi-channel real and imaginary images from the subsampled k-space data, and the desired output is all voxels in the corresponding alias-free line of the root-sum-of-squares of multi-channel images from fully-sampled k-space data. Aliasing artifacts in an image reconstructed from subsampled data were reduced by line-by-line processing of the learned MLP architecture.

Results
Reconstructed images from the proposed method are better than those from compared methods in terms of normalized root-mean-square error. The proposed method can be applied to image reconstruction for any k-space subsampling patterns in a phase encoding direction. Moreover, to further reduce the reconstruction time, it is easily implemented by parallel processing.

Conclusion
We have proposed a reconstruction method using machine learning to accelerate imaging time, which reconstructs high-quality images from subsampled k-space data. It shows flexibility in the use of k-space sampling patterns, and can reconstruct images in real time.

This article is protected by copyright. All rights reserved.",project-academic
,2011-11-30,b,,zhang neural networks and neural dynamic method," The real-time solution to a mathematical problem arises in numerous fields of science, engineering, and business. It is usually an essential part of many solutions, e.g., matrix/vector computation, optimisation, control theory, kinematics, signal processing, and pattern recognition. In recent years, due to the in-depth research on neural networks, numerous recurrent neural networks (RNN) based on the gradient-based method have been developed and investigated. Particularly, some simple neural networks were proposed to solve linear programming problems in real time and implemented on analogue circuits. In this book, ZNN, ZD or ZND theory formalises these problems and solutions in the time-varying context and provides compact models that could solve those dynamic problems.",project-academic
10.1101/2020.10.29.20222547,2020-11-03,a,Cold Spring Harbor Laboratory Press,public mobility data enables covid 19 forecasting and management at local and global scales," Abstract None Policymakers everywhere are working to determine the set of restrictions that will effectively contain the spread of COVID-19 without excessively stifling economic activity. We show that publicly available data on human mobility — collected by Google, Facebook, and other providers — can be used to evaluate the effectiveness of non-pharmaceutical interventions and forecast the spread of COVID-19. This approach relies on simple and transparent statistical models, and involves minimal assumptions about disease dynamics. We demonstrate the effectiveness of this approach using local and regional data from China, France, Italy, South Korea, and the United States, as well as national data from 80 countries around the world. None Summary None Background None Policymakers everywhere are working to determine the set of restrictions that will effectively contain the spread of COVID-19 without excessively stifling economic activity. In some contexts, decision-makers have access to sophisticated epidemiological models and detailed case data. However, a large number of decisions, particularly in low-income and vulnerable communities, are being made with limited or no modeling support. We examine how public human mobility data can be combined with simple statistical models to provide near real-time feedback on non-pharmaceutical policy interventions. Our objective is to provide a simple framework that can be easily implemented and adapted by local decision-makers. None Methods None We develop simple statistical models to measure the effectiveness of non-pharmaceutical interventions (NPIs) and forecast the spread of COVID-19 at local, state, and national levels. The method integrates concepts from econometrics and machine learning, and relies only upon publicly available data on human mobility. The approach does not require explicit epidemiological modeling, and involves minimal assumptions about disease dynamics. We evaluate this approach using local and regional data from China, France, Italy, South Korea, and the United States, as well as national data from 80 countries around the world. None Findings None We find that NPIs are associated with significant reductions in human mobility, and that changes in mobility can be used to forecast COVID-19 infections. The first set of results show the impact of NPIs on human mobility at all geographic scales. While different policies have different effects on different populations, we observed total reductions in mobility between 40 and 84 percent. The second set of results indicate that — even in the absence of other epidemiological information — mobility data substantially improves 10-day case rates forecasts at the county (20.75% error, US), state (21.82 % error, US), and global (15.24% error) level. Finally, for example, country-level results suggest that a shelter-in-place policy targeting a 10% increase in the amount of time spent at home would decrease the propagation of new cases by 32% by the end of a 10 day period. None Interpretation None In rapidly evolving disease outbreaks, decision-makers do not always have immediate access to sophisticated epidemiological models. In such cases, valuable insight can still be derived from simple statistic models and readily-available public data. These models can be quickly fit with a population’s own data and updated over time, thereby capturing social and epidemiological dynamics that are unique to a specific locality or time period. Our results suggest that this approach can effectively support decision-making from local (e.g., city) to national scales.",project-academic
10.17059/2017-4-9,2017-11-01,a,"Centre for Economic Security, Institute of Economics of Ural Branch of Russian Academy of Sciences",forecasting of socio economic development of the russian regions," The regional differentiation makes impossible the sustainable socio-economic development of the subjects of the Russian Federation without the monitoring public governance results in space and time. Despite the comprehensive approach of the current procedure, approved by the federal government, it does not adequately assess the executive authorities effectiveness. Its main problem is the impossibility to assume such important administrative function as forecasting the social and economic development of Russian territorial subjects. The authors propose an alternative methodology on the basis of the system economic theory. This technique is implemented in several consecutive stages. Firstly, we develop the system of 30 indicators. Secondly, we normalize the values of the indicators using the method of pattern. Thirdly, we calculate the index of the social and economic development of Russian regions for 2011â€“2015 assuming that the indicators are equal. Last, we group Russian regions into clusters according to the level of their social and economic development using neural network technologies (Kohonen selforganizing maps). Only 9 in 80 subjects of the Russian Federation (RF) had the degree of realizing the social and economic potential higher than 40 % during the period under consideration. In 2011â€“2015, the most of regions had a low and lower than average level of social and economic development (with an aggregate share about 64.3 %). It means that, under current conditions, the majority of the RF regions have considerable reserves for realizing their social-economic potential. In particular, the absence of the territorial subjects with a high level of social and economic development proves that. The authors have simulated the social and economic situation of the RF subjects by means of an adequate Bayesian neural networks. The obtained results can be used as the basis for further research in the field of evaluating executive authorities effectiveness and forecasting the level of social and economic development of Russian regions.",project-academic
10.1016/J.ENG.2018.11.027,2019-05-08,a,Elsevier,the state of the art of data science and engineering in structural health monitoring," Abstract None None Structural health monitoring (SHM) is a multi-discipline field that involves the automatic sensing of structural loads and response by means of a large number of sensors and instruments, followed by a diagnosis of the structural health based on the collected data. Because an SHM system implemented into a structure automatically senses, evaluates, and warns about structural conditions in real time, massive data are a significant feature of SHM. The techniques related to massive data are referred to as data science and engineering, and include acquisition techniques, transition techniques, management techniques, and processing and mining algorithms for massive data. This paper provides a brief review of the state of the art of data science and engineering in SHM as investigated by these authors, and covers the compressive sampling-based data-acquisition algorithm, the anomaly data diagnosis approach using a deep learning algorithm, crack identification approaches using computer vision techniques, and condition assessment approaches for bridges using machine learning algorithms. Future trends are discussed in the conclusion.",project-academic
10.2196/12214,2019-04-04,a,JMIR Public Health Surveill,improved real time influenza surveillance using internet search data in eight latin american countries," Background: Novel influenza surveillance systems that leverage Internet-based real-time data sources including Internet search frequencies, social-network information, and crowd-sourced flu surveillance tools have shown improved accuracy over the past few years in data-rich countries like the United States. These systems not only track flu activity accurately, but they also report flu estimates a week or more ahead of the publication of reports produced by healthcare-based systems, such as those implemented and managed by the Centers for Disease Control and Prevention. Previous work has shown that the predictive capabilities of novel flu surveillance systems, like Google Flu Trends (GFT), in developing countries in Latin America have not yet delivered acceptable flu estimates. Objective: The aim of this study was to show that recent methodological improvements on the use of Internet search engine information to track diseases can lead to improved retrospective flu estimates in multiple countries in Latin America. Methods: A machine learning-based methodology that uses flu-related Internet search activity and historical information to monitor flu activity, named ARGO (AutoRegression with Google search), was extended to generate flu predictions for 8 Latin American countries (Argentina, Bolivia, Brazil, Chile, Mexico, Paraguay, Peru, and Uruguay) for the time period: January 2012 to December of 2016. These retrospective (out-of-sample) Influenza activity predictions were compared with historically observed flu suspected cases in each country, as reported by Flunet, an influenza surveillance database maintained by the World Health Organization. For a baseline comparison, retrospective (out-of-sample) flu estimates were produced for the same time period using autoregressive models that only leverage historical flu activity information. Results: Our results show that ARGO-like models’ predictive power outperform autoregressive models in 6 out of 8 countries in the 2012-2016 time period. Moreover, ARGO significantly improves on historical flu estimates produced by the now discontinued GFT for the time period of 2012-2015, where GFT information is publicly available. Conclusions: We demonstrate here that a self-correcting machine learning method, leveraging Internet-based disease-related search activity and historical flu trends, has the potential to produce reliable and timely flu estimates in multiple Latin American countries. This methodology may prove helpful to local public health officials who design and implement interventions aimed at mitigating the effects of influenza outbreaks. Our methodology generally outperforms both the now-discontinued tool GFT, and autoregressive methodologies that exploit only historical flu activity to produce future disease estimates.",project-academic
,2000-06-29,p,Morgan Kaufmann Publishers Inc.,acquisition of stand up behavior by a real robot using hierarchical reinforcement learning," In this paper, we propose a hierarchical reinforcement learning architecture that realizes practical learning speed in real hardware control tasks. In order to enable learning in a practical number of trials, we introduce a low-dimensional representation of the state of the robot for higher-level planning. The upper level learns a discrete sequence of sub-goals in a low-dimensional state space for achieving the main goal of the task. The lower-level modules learn local trajectories in the original high-dimensional state space to achieve the sub-goal specified by the upper level. We applied the hierarchical architecture to a three-link, two-joint robot for the task of learning to stand up by trial and error. The upper-level learning was implemented by Q-learning, while the lower-level learning was implemented by a continuous actor–critic method. The robot successfully learned to stand up within 750 trials in simulation and then in an additional 170 trials using real hardware. The effects of the setting of the search steps in the upper level and the use of a supplementary reward for achieving sub-goals are also tested in simulation. © 2001 Elsevier Science B.V. All rights reserved.",project-academic
10.1007/978-3-540-33869-7_1,2006-01-01,a,"Springer, Berlin, Heidelberg",swarm intelligence foundations perspectives and applications," Swarm Intelligence (SI) is an innovative distributed intelligent paradigm for solving optimization problems that originally took its inspiration from the biological examples by swarming, flocking and herding phenomena in vertebrates. Particle Swarm Optimization (PSO) incorporates swarming behaviors observed in flocks of birds, schools of fish, or swarms of bees, and even human social behavior, from which the idea is emerged [14, 7, 22]. PSO is a population-based optimization tool, which could be implemented and applied easily to solve various function optimization problems, or the problems that can be transformed to function optimization problems. As an algorithm, the main strength of PSO is its fast convergence, which compares favorably with many global optimization algorithms like Genetic Algorithms (GA) [13], Simulated Annealing (SA) [20, 27] and other global optimization algorithms. For applying PSO successfully, one of the key issues is finding how to map the problem solution into the PSO particle, which directly affects its feasibility and performance. Ant Colony Optimization (ACO) deals with artificial systems that is inspired from the foraging behavior of real ants, which are used to solve discrete",project-academic
10.3389/FNBOT.2017.00002,2017-01-25,a,Frontiers,connecting artificial brains to robots in a comprehensive simulation framework the neurorobotics platform," Combined efforts in the fields of neuroscience, computer science and biology allowed to design biologically realistic models of the brain based on spiking neural networks. For a proper validation of these models, an embodiment in a dynamic and rich sensory environment, where the model is exposed to a realistic sensory-motor task, is needed. Due to the complexity of these brain models that, at the current stage, cannot deal with real-time constraints, it is not possible to embed them into a real world task. Rather, the embodiment has to be simulated as well. While adequate tools exist to simulate either complex neural networks or robots and their environments, there is so far no tool that allows to easily establish a communication between brain and body models. The Neurorobotics Platform is a new web-based environment that aims to filling this gap by offering scientists and technology developers a software infrastructure allowing them to connect brain models to detailed simulations of robot bodies and environments and to use the resulting neurorobotic systems for in-silico experimentation. In order to simplify the workflow and reduce the level of the required programming skills, the platform provides editors for the specification of experimental sequences and conditions, envi-ronments, robots, and brain-body connectors. In addition to that, a variety of existing robots and environments are provided. This work presents the architecture of the first release of the Neurorobotics Platform developed in subproject 10 “Neurorobotics” of the Human Brain Project (HBP). At the current state, the Neurorobotics Platform allows researchers to design and run basic experiments in neurorobotics using simulated robots and simulated environments linked to simplified versions of brain models. We illustrate the capabilities of the platform with three example experiments: a Braitenberg task implemented on a mobile robot, a sensory-motor learning task based on a robotic controller and a visual tracking embedding a retina model on the iCub humanoid robot. These use-cases allow to assess the applicability of the Neurorobotics Platform for robotic tasks as well as in neuroscientific experiments.",project-academic
10.1140/EPJC/S10052-020-7953-3,2019-10-21,a,,background rejection in atmospheric cherenkov telescopes using recurrent convolutional neural networks," In this work, we present a new, high performance algorithm for background rejection in imaging atmospheric Cherenkov telescopes. We build on the already popular machine-learning techniques used in gamma-ray astronomy by the application of the latest techniques in machine learning, namely recurrent and convolutional neural networks, to the background rejection problem. Use of these machine-learning techniques addresses some of the key challenges encountered in the currently implemented algorithms and helps to significantly increase the background rejection performance at all energies. 
We apply these machine learning techniques to the H.E.S.S. telescope array, first testing their performance on simulated data and then applying the analysis to two well known gamma-ray sources. With real observational data we find significantly improved performance over the current standard methods, with a 20-25\% reduction in the background rate when applying the recurrent neural network analysis. Importantly, we also find that the convolutional neural network results are strongly dependent on the sky brightness in the source region which has important implications for the future implementation of this method in Cherenkov telescope analysis.",project-academic
10.1145/2623330.2630805,2014-08-24,p,,computational epidemiology," As recent pandemics such as SARS and the Swine Flu outbreak have shown, diseases spread very fast in today's interconnected world, making public health an important research area. Some of the basic questions are: How can an outbreak be contained before it becomes an epidemic, and what disease surveillance strategies should be implemented? These problems have been studied traditionally using differential equation methods, which are amenable to analysis and closed form solutions. However, these models are based on complete mixing assumptions, which do not hold for realistic populations, thereby limiting their utility.In this tutorial, we focus on an approach based on diffusion processes on complex networks. This captures more realistic populations, but leads to novel mathematical and computational challenges. The structure of the underlying networks has a significant impact on the dynamical properties, motivating the need for improved network models, and efficient algorithms for computing network and dynamical properties that scale to large networks. We provide an overview of the state of the art in computational epidemiology, which is a multi-disciplinary research area, that overlaps different areas in computer science, including data mining, machine learning, high performance computing and theoretical computer science, as well as mathematics, economics and statistics. Specifically, we will discuss mathematical and computational models, problems of inference, forecasting and state assessment, and epidemic containment.",project-academic
10.1016/J.PHYSA.2019.123174,2020-02-15,a,North-Holland,fake news detection within online social media using supervised artificial intelligence algorithms," Abstract None None Along with the development of the Internet, the emergence and widespread adoption of the social media concept have changed the way news is formed and published. News has become faster, less costly and easily accessible with social media. This change has come along with some disadvantages as well. In particular, beguiling content, such as fake news made by social media users, is becoming increasingly dangerous. The fake news problem, despite being introduced for the first time very recently, has become an important research topic due to the high content of social media. Writing fake comments and news on social media is easy for users. The main challenge is to determine the difference between real and fake news. In this paper, a two-step method for identifying fake news on social media has been proposed, focusing on fake news. In the first step of the method, a number of pre-processing is applied to the data set to convert un-structured data sets into the structured data set. The texts in the data set containing the news are represented by vectors using the obtained TF weighting method and Document-Term Matrix. In the second step, twenty-three supervised artificial intelligence algorithms have been implemented in the data set transformed into the structured format with the text mining methods. In this work, an experimental evaluation of the twenty-three intelligent classification methods has been performed within existing public data sets and these classification models have been compared depending on four evaluation metrics.",project-academic
10.1007/S10916-018-0934-5,2018-05-01,a,Springer US,behavioral modeling for mental health using machine learning algorithms," Mental health is an indicator of emotional, psychological and social well-being of an individual. It determines how an individual thinks, feels and handle situations. Positive mental health helps one to work productively and realize their full potential. Mental health is important at every stage of life, from childhood and adolescence through adulthood. Many factors contribute to mental health problems which lead to mental illness like stress, social anxiety, depression, obsessive compulsive disorder, drug addiction, and personality disorders. It is becoming increasingly important to determine the onset of the mental illness to maintain proper life balance. The nature of machine learning algorithms and Artificial Intelligence (AI) can be fully harnessed for predicting the onset of mental illness. Such applications when implemented in real time will benefit the society by serving as a monitoring tool for individuals with deviant behavior. This research work proposes to apply various machine learning algorithms such as support vector machines, decision trees, na?ve bayes classifier, K-nearest neighbor classifier and logistic regression to identify state of mental health in a target group. The responses obtained from the target group for the designed questionnaire were first subject to unsupervised learning techniques. The labels obtained as a result of clustering were validated by computing the Mean Opinion Score. These cluster labels were then used to build classifiers to predict the mental health of an individual. Population from various groups like high school students, college students and working professionals were considered as target groups. The research presents an analysis of applying the aforementioned machine learning algorithms on the target groups and also suggests directions for future work.",project-academic
10.3389/NEURO.10.025.2009,2009-11-20,a,Frontiers,a framework for modeling the growth and development of neurons and networks," The development of neural tissue is a complex organizing process, in which it is difficult to grasp how the various localized interactions between dividing cells leads relentlessly to global network organization. Simulation is a useful tool for exploring such complex processes because it permits rigorous analysis of observed global behavior in terms of the mechanistic axioms declared in the simulated model. We describe a novel simulation tool, CX3D, for modeling the development of large realistic neural networks such as the neocortex, in a physical 3D space. In CX3D, as in biology, neurons arise by the replication and migration of precursors, which mature into cells able to extend axons and dendrites. Individual neurons are discretized into spherical (for the soma) and cylindrical (for neurites) elements that have appropriate mechanical properties. The growth functions of each neuron are encapsulated in set of pre-defined modules that are automatically distributed across its segments during growth. The extracellular space is also discretized, and allows for the diffusion of extracellular signaling molecules, as well as the physical interactions of the many developing neurons. We demonstrate the utility of CX3D by simulating three interesting developmental processes: neocortical lamination based on mechanical properties of tissues; a growth model of a neocortical pyramidal cell based on layer-specific guidance cues; and the formation of a neural network in vitro by employing neurite fasciculation. We also provide some examples in which previous models from the literature are re-implemented in CX3D. Our results suggest that CX3D is a powerful tool for understanding neural development.",project-academic
,2001-12-01,b,MIT Press,parameterized action representation for virtual human agents," We describe a Parameterized Action Representation (PAR) designed to bridge the gap between natural language instructions and the virtual agents who are to carry them out. The PAR is therefore constructed based jointly on implemented motion capabilities of virtual human figures and linguistic requirements for instruction interpretation. We will illustrate PAR and a real-time execution architecture controlling 3D animated virtual human avatars. Comments Postprint version. Published in American Association for Artificial Intelligence, Spring Synposium, 2000. Author(s) Norman I. Badler, Ramamani Bindiganavale, Juliet C. Bourne, Martha Palmer, Jianping Shi, and William Schuler This journal article is available at ScholarlyCommons: http://repository.upenn.edu/hms/26 A Parameterized Action Representation for Virtual Human Agents Norman Badler, Rama Bindiganavale, Juliet Bourne Martha Palmer, Jianping Shi, William Schuler Center for Human Modeling and Simulation Computer and Information Science Department University of Pennsylvania Philadelphia, PA 19104-6389",project-academic
10.1145/2245276.2245364,2012-03-26,p,ACM,towards building large scale distributed systems for twitter sentiment analysis," In recent years, social networks have become very popular. Twitter, a micro-blogging service, is estimated to have about 200 million registered users and these users create approximately 65 million tweets a day. Twitter users usually show their opinion about topics of their interest. The challenge is that each tweet is limited in 140 characters, and is hence very short. It may contain slang and misspelled words. Thus, it is difficult to apply traditional NLP techniques which are designed for working with formal languages, into Twitter domain. Another challenge is that the total volume of tweets is extremely high, and it takes a long time to process. In this paper, we describe a large-scale distributed system for real-time Twitter sentiment analysis. Our system consists of two components: a lexicon builder and a sentiment classifier. These two components are capable of running on a large-scale distributed system since they are implemented using a MapReduce framework and a distributed database model. Thus, our lexicon builder and sentiment classifier are scalable with the number of machines and the size of data. The experiments also show that our lexicon has a good quality in opinion extraction, and the accuracy of the sentiment classifier can be improved by combining the lexicon with a machine learning technique.",project-academic
10.1145/263479.263481,1997-10-01,a,ACM,a multilevel approach to intelligent information filtering model system and evaluation," In information-filtering environments, uncertainties associated with changing interests of the user and the dynamic document stream must be handled efficiently. In this article, a filtering model is proposed that decomposes the overall task into subsystem functionalities and highlights the need for multiple adaptation techniques to cope with uncertainties. A filtering system, SIFTER, has been implemented based on the model, using established techniques in information retrieval and artificial intelligence. These techniques include document representation by a vector-space model, document classification by unsupervised learning, and user modeling by reinforcement learning. The system can filter information based on content and a user's specific interests. The user's interests are automatically learned with only limited user intervention in the form of optional relevance feedback for documents. We also describe experimental studies conducted with SIFTER to filter computer and information science documents collected from the Internet and commercial database services. The experimental results demonstrate that the system performs very well in filtering documents in a realistic problem setting.",project-academic
,2018-04-10,a,,a real time and unsupervised face re identification system for human robot interaction," In the context of Human-Robot Interaction (HRI), face Re-Identification (face Re-ID) aims to verify if certain detected faces have already been observed by robots. The ability of distinguishing between different users is crucial in social robots as it will enable the robot to tailor the interaction strategy toward the users' individual preferences. So far face recognition research has achieved great success, however little attention has been paid to the realistic applications of Face Re-ID in social robots. In this paper, we present an effective and unsupervised face Re-ID system which simultaneously re-identifies multiple faces for HRI. This Re-ID system employs Deep Convolutional Neural Networks to extract features, and an online clustering algorithm to determine the face's ID. Its performance is evaluated on two datasets: the TERESA video dataset collected by the TERESA robot, and the YouTube Face Dataset (YTF Dataset). We demonstrate that the optimised combination of techniques achieves an overall 93.55% accuracy on TERESA dataset and an overall 90.41% accuracy on YTF dataset. We have implemented the proposed method into a software module in the HCI^2 Framework for it to be further integrated into the TERESA robot, and has achieved real-time performance at 10~26 Frames per second.",project-academic
10.1016/J.PATREC.2018.04.009,2019-12-01,a,North-Holland,a real time and unsupervised face re identification system for human robot interaction," Abstract None None In the context of Human-Robot Interaction (HRI), face Re-Identification (face Re-ID) aims to verify if certain detected faces have already been observed by robots. The ability of distinguishing between different users is crucial in social robots as it will enable the robot to tailor the interaction strategy toward the users’ individual preferences. So far face recognition research has achieved great success, however little attention has been paid to the realistic applications of Face Re-ID in social robots. In this paper, we present an effective and unsupervised face Re-ID system which simultaneously re-identifies multiple faces for HRI. This Re-ID system employs Deep Convolutional Neural Networks to extract features, and an online clustering algorithm to determine the face's ID. Its performance is evaluated on two datasets: the TERESA video dataset collected by the TERESA robot, and the YouTube Face Dataset (YTF Dataset). We demonstrate that the optimised combination of techniques achieves an overall 93.55% accuracy on TERESA dataset and an overall 90.41% accuracy on YTF dataset. We have implemented the proposed method into a software module in the HCI^2 Framework None [1] None for it to be further integrated into the TERESA robot None [2] , and has achieved real-time performance at 10–26 Frames per second.",project-academic
10.1016/J.TRIT.2016.11.005,2016-10-01,a,No longer published by Elsevier,acp based social computing and parallel intelligence societies 5 0 and beyond," Abstract None None Social computing, as the technical foundation of future computational smart societies, has the potential to improve the effectiveness of open-source big data usage, systematically integrate a variety of elements including time, human, resources, scenarios, and organizations in the current cyber-physical-social world, and establish a novel social structure with fair information, equal rights, and a flat configuration. Meanwhile, considering the big modeling gap between the model world and the physical world, the concept of parallel intelligence is introduced. With the help of software-defined everything, parallel intelligence bridges the big modeling gap by means of constructing artificial systems where computational experiments can be implemented to verify social policies, economic strategies, and even military operations. Artificial systems play the role of “social laboratories” in which decisions are computed before they are executed in our physical society. Afterwards, decisions with the expected outputs are executed in parallel in both the artificial and physical systems to interactively sense, compute, evaluate and adjust system behaviors in real-time, leading system behaviors in the physical system converging to those proven to be optimal in the artificial ones. Thus, the smart guidance and management for our society can be achieved.",project-academic
,2012-05-01,a,Frontiers Research Foundation,look at this the neural correlates of initiating and responding to bids for joint attention," When engaging in joint attention, one person directs another person's attention to an object (Initiating Joint Attention, IJA), and the second person's attention follows (Responding to Joint Attention, RJA). As such, joint attention must occur within the context of a social interaction. This ability is critical to language and social development; yet the neural bases for this pivotal skill remain understudied. This paucity of research is likely due to the challenge in acquiring functional MRI data during a naturalistic, contingent social interaction. To examine the neural bases of both IJA and RJA we implemented a dual-video set-up that allowed for a face-to-face interaction between subject and experimenter via video during fMRI data collection. In each trial, participants either followed the experimenter's gaze to a target (RJA) or cued the experimenter to look at the target (IJA). A control condition, solo attention (SA), was included in which the subject shifted gaze to a target while the experimenter closed her eyes. Block and event-related analyses were conducted and revealed common and distinct regions for IJA and RJA. Distinct regions included the ventromedial prefrontal cortex for RJA and intraparietal sulcus and middle frontal gyrus for IJA (as compared to SA). Conjunction analyses revealed overlap in the dorsal medial prefrontal cortex (dMPFC) and right posterior superior temporal sulcus (pSTS) for IJA and RJA (as compared to SA) for the event analyses. Functional connectivity analyses during a resting baseline suggest joint attention processes recruit distinct but interacting networks, including social-cognitive, voluntary attention orienting, and visual networks. This novel experimental set-up allowed for the identification of the neural bases of joint attention during a real-time interaction and findings suggest that whether one is the initiator or responder, the dMPFC and right pSTS, are selectively recruited during periods of joint attention.",project-academic
10.3389/FNHUM.2012.00169,2012-06-22,a,Frontiers Media SA,look at this the neural correlates of initiating and responding to bids for joint attention," When engaging in joint attention, one person directs another person's attention to an object (Initiating Joint Attention, IJA), and the second person's attention follows (Responding to Joint Attention, RJA). As such, joint attention must occur within the context of a social interaction. This ability is critical to language and social development; yet the neural bases for this pivotal skill remain understudied. This paucity of research is likely due to the challenge in acquiring functional MRI data during a naturalistic, contingent social interaction. To examine the neural bases of both IJA and RJA we implemented a dual-video set-up that allowed for a face-to-face interaction between subject and experimenter via video during fMRI data collection. In each trial, participants either followed the experimenter's gaze to a target (RJA) or cued the experimenter to look at the target (IJA). A control condition, solo attention (SA), was included in which the subject shifted gaze to a target while the experimenter closed her eyes. Block and event-related analyses were conducted and revealed common and distinct regions for IJA and RJA. Distinct regions included the ventromedial prefrontal cortex for RJA and intraparietal sulcus and middle frontal gyrus for IJA (as compared to SA). Conjunction analyses revealed overlap in the dorsal medial prefrontal cortex (dMPFC) and right posterior superior temporal sulcus (pSTS) for IJA and RJA (as compared to SA) for the event analyses. Functional connectivity analyses during a resting baseline suggest joint attention processes recruit distinct but interacting networks, including social-cognitive, voluntary attention orienting, and visual networks. This novel experimental set-up allowed for the identification of the neural bases of joint attention during a real-time interaction and findings suggest that whether one is the initiator or responder, the dMPFC and right pSTS, are selectively recruited during periods of joint attention.",project-academic
10.1007/978-3-319-17290-3,2016-01-29,b,"Springer Publishing Company, Incorporated",machine learning in complex networks," This book presents the features and advantages offered by complex networks in the machine learning domain. In the first part, an overview on complex networks and network-based machine learning is presented, offering necessary background material. In the second part, we describe in details some specific techniques based on complex networks for supervised, non-supervised, and semi-supervised learning. Particularly, a stochastic particle competition technique for both non-supervised and semi-supervised learning using a stochastic nonlinear dynamical system is described in details. Moreover, an analytical analysis is supplied, which enables one to predict the behavior of the proposed technique. In addition, data reliability issues are explored in semi-supervised learning. Such matter has practical importance and is not often found in the literature. With the goal of validating these techniques for solving real problems, simulations on broadly accepted databases are conducted. Still in this book, we present a hybrid supervised classification technique that combines both low and high orders of learning. The low level term can be implemented by any classification technique, while the high level term is realized by the extraction of features of the underlying network constructed from the input data. Thus, the former classifies the test instances by their physical features, while the latter measures the compliance of the test instances with the pattern formation of the data. We show that the high level technique can realize classification according to the semantic meaning of the data. This book intends to combine two widely studied research areas, machine learning and complex networks, which in turn will generate broad interests to scientific community, mainly to computer science and engineering areas.",project-academic
10.1109/TMECH.2015.2396114,2015-03-10,a,IEEE,adaptive neural network control of a compact bionic handling arm," In this paper, autonomous control problem of a class of bionic continuum robots named “Compact Bionic Handling Arm” (CBHA) is addressed. These robots can reproduce biological behaviors of trunks, tentacles, or snakes. The modeling problem associated with continuum robots includes nonlinearities, structured and unstructured uncertainties, and the hyperredundancy. In addition to these problems, the CBHA comprises the hysteresis behavior of its actuators and a memory phenomenon related to its structure made of polyamide materials. These undesirable effects make it difficult to design a control system based on quantitative models of the CBHA. Thus, two subcontrollers are proposed in this paper. One, encapsulated in the other, and both implemented in real time allow controlling of the CBHA's end-effector position. The first subcontroller controls the CBHA's kinematics based on a distal supervised learning scheme. The second subcontroller controls the CBHA's kinetics based on an adaptive neural control. These subcontrollers allow a better assessment of the stability of the control architecture while ensuring the convergence of Cartesian errors. The obtained experimental results using a CBHA robot show an accurate tracking of the CBHA's end-effector position.",project-academic
10.1021/ACSNANO.7B08272,2018-01-17,a,American Chemical Society,mimicking biological synaptic functionality with an indium phosphide synaptic device on silicon for scalable neuromorphic computing," Neuromorphic or ""brain-like"" computation is a leading candidate for efficient, fault-tolerant processing of large-scale data as well as real-time sensing and transduction of complex multivariate systems and networks such as self-driving vehicles or Internet of Things applications. In biology, the synapse serves as an active memory unit in the neural system and is the component responsible for learning and memory. Electronically emulating this element via a compact, scalable technology which can be integrated in a three-dimensional (3-D) architecture is critical for future implementations of neuromorphic processors. However, present day 3-D transistor implementations of synapses are typically based on low-mobility semiconductor channels or technologies that are not scalable. Here, we demonstrate a crystalline indium phosphide (InP)-based artificial synapse for spiking neural networks that exhibits elasticity, short-term plasticity, long-term plasticity, metaplasticity, and spike timing-dependent plasticity, emulating the critical behaviors exhibited by biological synapses. Critically, we show that this crystalline InP device can be directly integrated via back-end processing on a Si wafer using a SiO2 buffer without the need for a crystalline seed, enabling neuromorphic devices that can be implemented in a scalable and 3-D architecture. Specifically, the device is a crystalline InP channel field-effect transistor that interacts with neuron spikes by modification of the population of filled traps in the MOS structure itself. Unlike other transistor-based implementations, we show that it is possible to mimic these biological functions without the use of external factors (e.g., surface adsorption of gas molecules) and without the need for the high electric fields necessary for traditional flash-based implementations. Finally, when exposed to neuronal spikes with a waveform similar to that observed in the brain, these devices exhibit the ability to learn without the need for any external potentiating/depressing circuits, mimicking the biological process of Hebbian learning.",project-academic
10.1039/C6TB00152A,2016-05-11,a,The Royal Society of Chemistry,graphene scaffolds in progressive nanotechnology stem cell based tissue engineering of the nervous system," Although graphene/stem cell-based tissue engineering has recently emerged and has promisingly and progressively been utilized for developing one of the most effective regenerative nanomedicines, it suffers from low differentiation efficiency, low hybridization after transplantation and lack of appropriate scaffolds required in implantations without any degrading in functionality of the cells. In fact, recent studies have demonstrated that the unique properties of graphene can successfully resolve all of these challenges. Among various stem cells, neural stem cells (NSCs) and their neural differentiation on graphene have attracted a lot of interest, because graphene-based neuronal tissue engineering can promisingly realize the regenerative therapy of various incurable neurological diseases/disorders and the fabrication of neuronal networks. Hence, in this review, we further focused on the potential bioapplications of graphene-based nanomaterials for the proliferation and differentiation of NSCs. Then, various stimulation techniques (including electrical, pulsed laser, flash photo, near infrared (NIR), chemical and morphological stimuli) which have recently been implemented in graphene-based stem cell differentiations were reviewed. The possibility of degradation of graphene scaffolds (NIR-assisted photodegradation of three-dimensional graphene nanomesh scaffolds) was also discussed based on the latest achievements. The biocompatibility of graphene scaffolds and their probable toxicities (especially after the disintegration of graphene scaffolds and distribution of its platelets in the body), which is still an important challenge, were reviewed and discussed. Finally, the initial recent efforts for fabrication of neuronal networks on graphene materials were presented. Since there has been no in vivo application of graphene in neuronal regenerative medicine, we hope that this review can excite further and concentrated investigations on in vivo (and even in vitro) neural proliferation, stimulation and differentiation of stem cells on biocompatible graphene scaffolds having the potential of degradability for the generation of implantable neuronal networks.",project-academic
10.1063/1.5145177,2020-04-09,a,AIP Publishing LLCAIP Publishing,inverse methods for design of soft materials," Functional soft materials, comprising colloidal and molecular building blocks that self-organize into complex structures as a result of their tunable interactions, enable a wide array of technological applications. Inverse methods provide a systematic means for navigating their inherently high-dimensional design spaces to create materials with targeted properties. While multiple physically motivated inverse strategies have been successfully implemented in silico, their translation to guiding experimental materials discovery has thus far been limited to a handful of proof-of-concept studies. In this perspective, we discuss recent advances in inverse methods for design of soft materials that address two challenges: (1) methodological limitations that prevent such approaches from satisfying design constraints and (2) computational challenges that limit the size and complexity of systems that can be addressed. Strategies that leverage machine learning have proven particularly effective, including methods to discover order parameters that characterize complex structural motifs and schemes to efficiently compute macroscopic properties from the underlying structure. We also highlight promising opportunities to improve the experimental realizability of materials designed computationally, including discovery of materials with functionality at multiple thermodynamic states, design of externally directed assembly protocols that are simple to implement in experiments, and strategies to improve the accuracy and computational efficiency of experimentally relevant models.",project-academic
10.1038/S41467-020-19597-W,2020-11-24,a,Nature Publishing Group,on the fly closed loop materials discovery via bayesian active learning," Active learning—the field of machine learning (ML) dedicated to optimal experiment design—has played a part in science as far back as the 18th century when Laplace used it to guide his discovery of celestial mechanics. In this work, we focus a closed-loop, active learning-driven autonomous system on another major challenge, the discovery of advanced materials against the exceedingly complex synthesis-processes-structure-property landscape. We demonstrate an autonomous materials discovery methodology for functional inorganic compounds which allow scientists to fail smarter, learn faster, and spend less resources in their studies, while simultaneously improving trust in scientific results and machine learning tools. This robot science enables science-over-the-network, reducing the economic impact of scientists being physically separated from their labs. The real-time closed-loop, autonomous system for materials exploration and optimization (CAMEO) is implemented at the synchrotron beamline to accelerate the interconnected tasks of phase mapping and property optimization, with each cycle taking seconds to minutes. We also demonstrate an embodiment of human-machine interaction, where human-in-the-loop is called to play a contributing role within each cycle. This work has resulted in the discovery of a novel epitaxial nanocomposite phase-change memory material. Machine learning driven research holds big promise towards accelerating materials’ discovery. Here the authors demonstrate CAMEO, which integrates active learning Bayesian optimization with practical experiments execution, for the discovery of new phase- change materials using X-ray diffraction experiments.",project-academic
,2018-06-30,a,"MIT Press One Rogers Street, Cambridge, MA 02142-1209 USA journals-info@mit.edu",evolved electrophysiological soft robots," The embodied cognition paradigm emphasizes that both bodies and brains combine to produce complex behaviors, in contrast to the traditional view that the only seat of intelligence is the brain. Despite recent excitement about embodied cognition, brains and bodies remain thought of, and implemented as, two separate entities that merely interface with one another to carry out their respective roles. Previous research co-evolving bodies and brains has simulated the physics of bodies that collect sensory information and pass that information on to disembodied neural networks, which then processes that information and return motor commands. Biological animals, in contrast, produce behavior through physically embedded control structures and a complex and continuous interplay between neural and mechanical forces. In addition to the electrical pulses flowing through the physical wiring of the nervous system, the heart elegantly combines control with actuation, as the physical properties of the tissue itself (or defects therein) determine the actuation of the organ. Inspired by these phenomena from cardiac electrophysiology (the study of the electrical properties of heart tissue), we introduce electrophysiological robots, whose behavior is dictated by electrical signals flowing though the tissue cells of soft robots. Here we describe these robots and how they are evolved. Videos and images of these robots reveal lifelike behaviors despite the added challenge of having physically embedded control structures. We also provide an initial experimental investigation into the impact of different implementation decisions, such as alternatives for sensing, actuation, and locations of central pattern generators. Overall, this paper provides a first step towards removing the chasm between bodies and brains to encourage further research into physically realistic embodied cognition. Introduction and Background The fields of evolutionary robotics and artificial life have seen a great deal of emphasis on embodied cognition in recent years [Cheney et al. (2013); Bongard (2013); Rieffel et al. (2013); Auerbach and Bongard (2012); Hiller and Lipson (2012a); Lehman and Stanley (2011); Auerbach and Bongard (2010a,b); Pfeifer et al. (2007); Hornby et al. (2001); Lipson and Pollack (2000)]. There is even a paradigm called embodied cognition, which argues that the specifics of the embodiment (such as the morphology) are Figure 1: Current flowing through an evolved creature. The legend for voltage within each cell (colors) is given in Fig. 3. vital parts of the resulting behavior of the system: It argues that the co-evolutionary connection between body and brain is more deeply intertwined than the body simply acting as a minimal physical interface between the brain and the environment [Pfeifer and Bongard (2006)]. Recent work in evolutionary robotics has shown that complex behaviors can arise when co-evolving bodies and brains. At one end of the spectrum, Auerbach and Bongard (2010b) demonstrated the evolution of physical structures that had no joints or actuators, and evolved to cover the largest distance in a controlled fall due to gravity. While that work exemplifies the evolution of behavior emerging from morphology alone, it does not co-evolve any actuation or control. Auerbach and Bongard (2010a) then evolved the placement of CPG controlled rotational joints between cellular spheres, thus co-evolving morphology and control. Cheney et al. (2013) evolved locomoting soft robots made of multiple different materials: two passive voxels of differing rigidity and two actuated voxel types that expanded cyclically via out-of-phase central pattern generators (CPGs). While this work added a variety of soft materials and a new type of actuation, the pairing of muscle types directly to a CPG again reflected a focus on evolving morphology rather than sophisticated neural control. Many examples in the literature include the co-evolution of a robot morphology with an artificial neural network controller [Sims (1994); Lipson and Pollack (2000); Hornby et al. (2001); Lehman and Stanley (2011)]. These studies (and many more like them) involve what might be called “ghost” networks: artificial neural networks that provide control to the body, yet do not have any physical embodiALIFE 14: Proceedings of the Fourteenth International Conference on the Synthesis and Simulation of Living Systems Figure 2: An example of complex electrical wave propagation in cardiac modeling [Fenton et al. (2005)]. ment in the system they control. The state of input nodes to these networks is often set by sensors in the robot and output nodes typically signify behavioral outcomes in the actuators, but the computation is done supernaturally, disjoint from the body itself. In the age of 3D printing, it is a realistic goal for robots to physically walk out of a printer. It is thus worthwhile to consider designing robots that can be physically realized: i.e., those whose controllers are accounted for by being physically woven into the design of the robot. While the brains of animals are often a separate module within their bodies, animals also have central and peripheral nervous systems extending throughout their bodies. An extreme example of this is the octopus, which has as much as 90% of its neurons existing outside of its central nervous system [Zullo et al. (2009)]. The distributed and physical layout of the nervous system over space may contribute significantly to neural processing, as the delays and branching in axons (the basis for nerves) are suggested to serve computational functions [Segev and Schneidman (1999)]. Despite the prevalence of embodied, distributed circuitry in nearly all of animal life, the idea of an embodied nervous system has been absent from the field of evolutionary robotics. The sub-field called Evolvable Hardware evolves physical circuits for computer chips [Floreano and Mattiussi (2008)], but such work has not been applied to evolving the circuitry of artificial life organisms. We are unaware of work with virtual creatures that have physically embodied control systems (e.g. where neural circuitry physically runs throughout the body of the creature). We present the first such work in this paper. We propose a very basic model of electrical signal propagation throughout the body of an evolved creature. This embodied controller is based on electrophysiology (specifically at large scales, such as cardiac electrophysiology, Fig. 2). Electrophysiology is the study of the electrical properties of biological cells and tissues [Hoffman et al. (1960)]. In this model, electrical pulses from a single centralized sinusoidal pacemaker (analogous to the sinoatrial node – the pacemaker in the heart [Brown (1982)]) are propagated through the electrically conductive tissue of the creature. The location and patterning of this conductive tissue is described by an evolved Compositional Pattern Producing Network (CPPN) genome. Evolution controls the shape of the body and the electrical pathways within it, which both combine to determine the robot’s behavior. The model involves conductive tissue cells that collect voltage from neighboring cells, causing an action potential (spike) if the collected voltage exceeds the cell’s firing threshold (Fig. 3). Once this threshold is crossed, the cell depolarizes, causing a voltage spike that excites neighboring cells. This voltage spike is followed by a refractory period, during which the cell is temporarily unable to be re-excited. This model allows for the propagation of information through the body of the creature in the form of electrical signals. The structure of this flow is produced entirely by the topology of the creature and the state of each cell’s direct neighbors. In this sense, the model can be seen as a form of distributed information processing. One could draw similarities between this model and a 3D-grid of neurons, where each neuron receives inputs from, and has outputs to, its immediate neighbors. In this analogy, we are evolving where neurons should exist in the grid, what type of material the neuron is housed in, as well as the material type, if any, of grid locations that do not contain neurons. The placement of material, which is under evolutionary control, directly determines the resultant behavior of the organism. Cells that actuate will contract and expand as they depolarize (much like the contraction of cardiac muscles), leading to the locomotion behavior of the creature. In order to control the signal flow throughout the creature, insulator cells are allowed, which are unable to accept and pass on the signal. Evolution can also choose not to fill a voxel with material. The morphology of the simulated robot and tissue type at each cell is determined by a CPPN genome. This model examines the evolution of embodied cognition at a more detailed level of implementation than is typical in the literature – with embodied control circuitry resulting directly from the morphology of the individual creature. While this study only covers the classic problem of locomotion, it is a step towards truly physically embodied robots.",project-academic
10.1162/978-0-262-32621-6-CH037,2014-07-01,p,MIT Press,evolved electrophysiological soft robots," The embodied cognition paradigm emphasizes that both bodies and brains combine to produce complex behaviors, in contrast to the traditional view that the only seat of intelligence is the brain. Despite recent excitement about embodied cognition, brains and bodies remain thought of, and implemented as, two separate entities that merely interface with one another to carry out their respective roles. Previous research co-evolving bodies and brains has simulated the physics of bodies that collect sensory information and pass that information on to disembodied neural networks, which then processes that information and return motor commands. Biological animals, in contrast, produce behavior through physically embedded control structures and a complex and continuous interplay between neural and mechanical forces. In addition to the electrical pulses flowing through the physical wiring of the nervous system, the heart elegantly combines control with actuation, as the physical properties of the tissue itself (or defects therein) determine the actuation of the organ. Inspired by these phenomena from cardiac electrophysiology (the study of the electrical properties of heart tissue), we introduce electrophysiological robots, whose behavior is dictated by electrical signals flowing though the tissue cells of soft robots. Here we describe these robots and how they are evolved. Videos and images of these robots reveal lifelike behaviors despite the added challenge of having physically embedded control structures. We also provide an initial experimental investigation into the impact of different implementation decisions, such as alternatives for sensing, actuation, and locations of central pattern generators. Overall, this paper provides a first step towards removing the chasm between bodies and brains to encourage further research into physically realistic embodied cognition. Introduction and Background The fields of evolutionary robotics and artificial life have seen a great deal of emphasis on embodied cognition in recent years [Cheney et al. (2013); Bongard (2013); Rieffel et al. (2013); Auerbach and Bongard (2012); Hiller and Lipson (2012a); Lehman and Stanley (2011); Auerbach and Bongard (2010a,b); Pfeifer et al. (2007); Hornby et al. (2001); Lipson and Pollack (2000)]. There is even a paradigm called embodied cognition, which argues that the specifics of the embodiment (such as the morphology) are Figure 1: Current flowing through an evolved creature. The legend for voltage within each cell (colors) is given in Fig. 3. vital parts of the resulting behavior of the system: It argues that the co-evolutionary connection between body and brain is more deeply intertwined than the body simply acting as a minimal physical interface between the brain and the environment [Pfeifer and Bongard (2006)]. Recent work in evolutionary robotics has shown that complex behaviors can arise when co-evolving bodies and brains. At one end of the spectrum, Auerbach and Bongard (2010b) demonstrated the evolution of physical structures that had no joints or actuators, and evolved to cover the largest distance in a controlled fall due to gravity. While that work exemplifies the evolution of behavior emerging from morphology alone, it does not co-evolve any actuation or control. Auerbach and Bongard (2010a) then evolved the placement of CPG controlled rotational joints between cellular spheres, thus co-evolving morphology and control. Cheney et al. (2013) evolved locomoting soft robots made of multiple different materials: two passive voxels of differing rigidity and two actuated voxel types that expanded cyclically via out-of-phase central pattern generators (CPGs). While this work added a variety of soft materials and a new type of actuation, the pairing of muscle types directly to a CPG again reflected a focus on evolving morphology rather than sophisticated neural control. Many examples in the literature include the co-evolution of a robot morphology with an artificial neural network controller [Sims (1994); Lipson and Pollack (2000); Hornby et al. (2001); Lehman and Stanley (2011)]. These studies (and many more like them) involve what might be called “ghost” networks: artificial neural networks that provide control to the body, yet do not have any physical embodiALIFE 14: Proceedings of the Fourteenth International Conference on the Synthesis and Simulation of Living Systems Figure 2: An example of complex electrical wave propagation in cardiac modeling [Fenton et al. (2005)]. ment in the system they control. The state of input nodes to these networks is often set by sensors in the robot and output nodes typically signify behavioral outcomes in the actuators, but the computation is done supernaturally, disjoint from the body itself. In the age of 3D printing, it is a realistic goal for robots to physically walk out of a printer. It is thus worthwhile to consider designing robots that can be physically realized: i.e., those whose controllers are accounted for by being physically woven into the design of the robot. While the brains of animals are often a separate module within their bodies, animals also have central and peripheral nervous systems extending throughout their bodies. An extreme example of this is the octopus, which has as much as 90% of its neurons existing outside of its central nervous system [Zullo et al. (2009)]. The distributed and physical layout of the nervous system over space may contribute significantly to neural processing, as the delays and branching in axons (the basis for nerves) are suggested to serve computational functions [Segev and Schneidman (1999)]. Despite the prevalence of embodied, distributed circuitry in nearly all of animal life, the idea of an embodied nervous system has been absent from the field of evolutionary robotics. The sub-field called Evolvable Hardware evolves physical circuits for computer chips [Floreano and Mattiussi (2008)], but such work has not been applied to evolving the circuitry of artificial life organisms. We are unaware of work with virtual creatures that have physically embodied control systems (e.g. where neural circuitry physically runs throughout the body of the creature). We present the first such work in this paper. We propose a very basic model of electrical signal propagation throughout the body of an evolved creature. This embodied controller is based on electrophysiology (specifically at large scales, such as cardiac electrophysiology, Fig. 2). Electrophysiology is the study of the electrical properties of biological cells and tissues [Hoffman et al. (1960)]. In this model, electrical pulses from a single centralized sinusoidal pacemaker (analogous to the sinoatrial node – the pacemaker in the heart [Brown (1982)]) are propagated through the electrically conductive tissue of the creature. The location and patterning of this conductive tissue is described by an evolved Compositional Pattern Producing Network (CPPN) genome. Evolution controls the shape of the body and the electrical pathways within it, which both combine to determine the robot’s behavior. The model involves conductive tissue cells that collect voltage from neighboring cells, causing an action potential (spike) if the collected voltage exceeds the cell’s firing threshold (Fig. 3). Once this threshold is crossed, the cell depolarizes, causing a voltage spike that excites neighboring cells. This voltage spike is followed by a refractory period, during which the cell is temporarily unable to be re-excited. This model allows for the propagation of information through the body of the creature in the form of electrical signals. The structure of this flow is produced entirely by the topology of the creature and the state of each cell’s direct neighbors. In this sense, the model can be seen as a form of distributed information processing. One could draw similarities between this model and a 3D-grid of neurons, where each neuron receives inputs from, and has outputs to, its immediate neighbors. In this analogy, we are evolving where neurons should exist in the grid, what type of material the neuron is housed in, as well as the material type, if any, of grid locations that do not contain neurons. The placement of material, which is under evolutionary control, directly determines the resultant behavior of the organism. Cells that actuate will contract and expand as they depolarize (much like the contraction of cardiac muscles), leading to the locomotion behavior of the creature. In order to control the signal flow throughout the creature, insulator cells are allowed, which are unable to accept and pass on the signal. Evolution can also choose not to fill a voxel with material. The morphology of the simulated robot and tissue type at each cell is determined by a CPPN genome. This model examines the evolution of embodied cognition at a more detailed level of implementation than is typical in the literature – with embodied control circuitry resulting directly from the morphology of the individual creature. While this study only covers the classic problem of locomotion, it is a step towards truly physically embodied robots.",project-academic
10.1186/S12880-019-0307-7,2019-02-28,a,BioMed Central,automated detection of nonmelanoma skin cancer using digital images a systematic review," Computer-aided diagnosis of skin lesions is a growing area of research, but its application to nonmelanoma skin cancer (NMSC) is relatively under-studied. The purpose of this review is to synthesize the research that has been conducted on automated detection of NMSC using digital images and to assess the quality of evidence for the diagnostic accuracy of these technologies. Eight databases (PubMed, Google Scholar, Embase, IEEE Xplore, Web of Science, SpringerLink, ScienceDirect, and the ACM Digital Library) were searched to identify diagnostic studies of NMSC using image-based machine learning models. Two reviewers independently screened eligible articles. The level of evidence of each study was evaluated using a five tier rating system, and the applicability and risk of bias of each study was assessed using the Quality Assessment of Diagnostic Accuracy Studies tool. Thirty-nine studies were reviewed. Twenty-four models were designed to detect basal cell carcinoma, two were designed to detect squamous cell carcinoma, and thirteen were designed to detect both. All studies were conducted in silico. The overall diagnostic accuracy of the classifiers, defined as concordance with histopathologic diagnosis, was high, with reported accuracies ranging from 72 to 100% and areas under the receiver operating characteristic curve ranging from 0.832 to 1. Most studies had substantial methodological limitations, but several were robustly designed and presented a high level of evidence. Most studies of image-based NMSC classifiers report performance greater than or equal to the reported diagnostic accuracy of the average dermatologist, but relatively few studies have presented a high level of evidence. Clinical studies are needed to assess whether these technologies can feasibly be implemented as a real-time aid for clinical diagnosis of NMSC.",project-academic
10.1145/2522848.2522879,2013-12-09,p,ACM,how can i help you comparing engagement classification strategies for a robot bartender," A robot agent existing in the physical world must be able to understand the social states of the human users it interacts with in order to respond appropriately. We compared two implemented methods for estimating the engagement state of customers for a robot bartender based on low-level sensor data: a rule-based version derived from the analysis of human behaviour in real bars, and a trained version using supervised learning on a labelled multimodal corpus. We first compared the two implementations using cross-validation on real sensor data and found that nearly all classifier types significantly outperformed the rule-based classifier. We also carried out feature selection to see which sensor features were the most informative for the classification task, and found that the position of the head and hands were relevant, but that the torso orientation was not. Finally, we performed a user study comparing the ability of the two classifiers to detect the intended user engagement of actual customers of the robot bartender; this study found that the trained classifier was faster at detecting initial intended user engagement, but that the rule-based classifier was more stable.",project-academic
10.1145/2287076.2287111,2012-06-18,p,ACM,distributed approximate spectral clustering for large scale datasets," Data-intensive applications are becoming important in many science and engineering fields, because of the high rates in which data are being generated and the numerous opportunities offered by the sheer amount of these data. Large-scale datasets, however, are challenging to process using many of the current machine learning algorithms due to their high time and space complexities. In this paper, we propose a novel approximation algorithm that enables kernel-based machine learning algorithms to efficiently process very large-scale datasets. While important in many applications, current kernel-based algorithms suffer from a scalability problem as they require computing a kernel matrix which takes O(N2) in time and space to compute and store. The proposed algorithm yields substantial reduction in computation and memory overhead required to compute the kernel matrix, and it does not significantly impact the accuracy of the results. In addition, the level of approximation can be controlled to tradeoff some accuracy of the results with the required computing resources. The algorithm is designed such that it is independent of the subsequently used kernel-based machine learning algorithm, and thus can be used with many of them. To illustrate the effect of the approximation algorithm, we developed a variant of the spectral clustering algorithm on top of it. Furthermore, we present the design of a MapReduce-based implementation of the proposed algorithm. We have implemented this design and run it on our own Hadoop cluster as well as on the Amazon Elastic MapReduce service. Experimental results on synthetic and real datasets demonstrate that significant time and memory savings can be achieved using our algorithm.",project-academic
10.3390/SU13073851,2021-03-31,a,Multidisciplinary Digital Publishing Institute,usage of real time machine vision in rolling mill," This article deals with the issue of computer vision on a rolling mill. The main goal of this article is to describe the designed and implemented algorithm for the automatic identification of the character string of billets on the rolling mill. The algorithm allows the conversion of image information from the front of the billet, which enters the rolling process, into a string of characters, which is further used to control the technological process. The purpose of this identification is to prevent the input pieces from being confused because different parameters of the rolling process are set for different pieces. In solving this task, it was necessary to design the optimal technical equipment for image capture, choose the appropriate lighting, search for text and recognize individual symbols, and insert them into the control system. The research methodology is based on the empirical-quantitative principle, the basis of which is the analysis of experimentally obtained data (photographs of billet faces) in real operating conditions leading to their interpretation (transformation into the shape of a digital chain). The first part of the article briefly describes the billet identification system from the point of view of technology and hardware resources. The next parts are devoted to the main parts of the algorithm of automatic identification—optical recognition of strings and recognition of individual characters of the chain using artificial intelligence. The method of optical character recognition using artificial neural networks is the basic algorithm of the system of automatic identification of billets and eliminates ambiguities during their further processing. Successful implementation of the automatic inspection system will increase the share of operation automation and lead to ensuring automatic inspection of steel billets according to the production plan. This issue is related to the trend of digitization of individual technological processes in metallurgy and also to the social sustainability of processes, which means the elimination of human errors in the management of the billet rolling process.",project-academic
10.1021/ACS.EST.9B04678,2020-02-04,a,American Chemical Society,significant changes in chemistry of fine particles in wintertime beijing from 2007 to 2017 impact of clean air actions," The Beijing government implemented a number of clean air action plans to improve air quality in the last 10 years, which contributed to changes in the concentration of fine particles and their compositions. However, quantifying the impacts of these interventions is challenging as meteorology masks the real changes in observed concentrations. Here, we applied a machine learning technique to decouple the effect of meteorology and evaluate the changes in the chemistry of nonrefractory PM1 (particulate matter less than 1 μm) in winter 2007, 2016, and 2017 as a result of the clean air actions. The observed mass concentrations of PM1 were 74.6, 90.2, and 36.1 μg m-3 in the three winters, while the deweathered concentrations were 74.2, 78.7, and 46.3 μg m-3, respectively. The deweathered concentrations of PM1, organics, sulfate, ammonium, chloride, SO2, NO2, and CO decreased by -38, -46, -59, -24, -51, -89, -16, and -52% in 2017 in comparison to 2007. On the contrary, the deweathered concentration of nitrates increased by 4%. Our results indicate that the clean air actions implemented in 2017 were highly effective in reducing ambient concentrations of SO2, CO, and PM1 organics, sulfate, ammonium, and chloride, but the control of nitrate and PM1 organics remains a major challenge.",project-academic
10.1371/JOURNAL.PCBI.1007549,2020-01-15,a,Public Library of Science,brainiak tutorials user friendly learning materials for advanced fmri analysis," Advanced brain imaging analysis methods, including multivariate pattern analysis (MVPA), functional connectivity, and functional alignment, have become powerful tools in cognitive neuroscience over the past decade. These tools are implemented in custom code and separate packages, often requiring different software and language proficiencies. Although usable by expert researchers, novice users face a steep learning curve. These difficulties stem from the use of new programming languages (e.g., Python), learning how to apply machine-learning methods to high-dimensional fMRI data, and minimal documentation and training materials. Furthermore, most standard fMRI analysis packages (e.g., AFNI, FSL, SPM) focus on preprocessing and univariate analyses, leaving a gap in how to integrate with advanced tools. To address these needs, we developed BrainIAK (brainiak.org), an open-source Python software package that seamlessly integrates several cutting-edge, computationally efficient techniques with other Python packages (e.g., Nilearn, Scikit-learn) for file handling, visualization, and machine learning. To disseminate these powerful tools, we developed user-friendly tutorials (in Jupyter format; https://brainiak.org/tutorials/) for learning BrainIAK and advanced fMRI analysis in Python more generally. These materials cover techniques including: MVPA (pattern classification and representational similarity analysis); parallelized searchlight analysis; background connectivity; full correlation matrix analysis; inter-subject correlation; inter-subject functional connectivity; shared response modeling; event segmentation using hidden Markov models; and real-time fMRI. For long-running jobs or large memory needs we provide detailed guidance on high-performance computing clusters. These notebooks were successfully tested at multiple sites, including as problem sets for courses at Yale and Princeton universities and at various workshops and hackathons. These materials are freely shared, with the hope that they become part of a pool of open-source software and educational materials for large-scale, reproducible fMRI analysis and accelerated discovery.",project-academic
10.1016/J.IJMULTIPHASEFLOW.2019.103194,2020-05-01,a,Pergamon,bubble patterns recognition using neural networks application to the analysis of a two phase bubbly jet," Abstract None None Gas-liquid two-phase bubbly flows are found in different areas of science and technology such as nuclear energy, chemical industry, or piping systems. Optical diagnostics of two-phase bubbly flows with modern panoramic techniques makes it possible to capture simultaneously instantaneous characteristics of both continuous and dispersed phases with a high spatial resolution. In this paper, we introduce a novel approach based on neural networks to recognize bubble patterns in images and identify their geometric parameters. The originality of the proposed method consists in training of a neural network ensemble using synthetic images that resemble real photographs gathered in experiment. The use of neural networks in combination with automatically generated data allowed us to detect overlapping, blurred, and non-spherical bubbles in a broad range of volume gas fractions. Experiments on a turbulent bubbly jet proved that the implemented method increases the identification accuracy, reducing errors of various kinds, and lowers the processing time compared to conventional recognition methods. Furthermore, utilizing the new method of bubbles recognition, the primary physical parameters of a dispersed phase, such as bubble size distribution and local gas content, were calculated in a near-to-nozzle region of the bubbly jet. The obtained results and integral experimental parameters, especially volume gas fraction, are in good agreement with each other.",project-academic
10.1016/J.CPC.2019.107064,2020-05-01,a,Mendeley Data,pyfitit the software for quantitative analysis of xanes spectra using machine learning algorithms," Abstract None None X-ray absorption near-edge spectroscopy (XANES) is becoming an extremely popular tool for material science thanks to the development of new synchrotron radiation light sources. It provides information about charge state and local geometry around atoms of interest in operando and extreme conditions. However, in contrast to X-ray diffraction, a quantitative analysis of XANES spectra is rarely performed in the research papers. The reason must be found in the larger amount of time required for the calculation of a single spectrum compared to a diffractogram. For such time-consuming calculations, in the space of several structural parameters, we developed an interpolation approach proposed originally by Smolentsev and Soldatov (2007). The current version of this software, named PyFitIt, is a major upgrade version of FitIt and it is based on machine learning algorithms. We have chosen Jupyter Notebook framework to be friendly for users and at the same time being available for remastering. The analytical work is divided into two steps. First, the series of experimental spectra are analyzed statistically and decomposed into principal components. Second, pure spectral profiles, recovered by principal components, are fitted by theoretical interpolated spectra. We implemented different schemes of choice of nodes for approximation and learning algorithms including Gradient Boosting of Random Trees, Radial Basis Functions and Neural Networks. The fitting procedure can be performed both for a XANES spectrum or for a difference spectrum, thus minimizing the systematic errors of theoretical simulations. The problem of several local minima is addressed in the framework of direct and indirect approaches. None None None Program summary None Program title: PyFitIt. None Program Files doi: None http://dx.doi.org/10.17632/ydkgfdc38t.1 None None Licensing provisions: GNU General Public License 3. None Programming language: Python, Jupyter Notebook framework. None Nature of problem: Quantitative structural refinements of the X-ray absorption near-edge structure spectra (XANES). Identification of the pure spectral and concentration profiles associated with an experimental XANES dataset. None Solution method: The fitting procedure of the experimental XANES spectra or of their differences is realized by means of the inverse and direct approaches based on the training set and approximation machine learning algorithms. The spectral resolution method is based on the PCA technique involving the usage of a target transformation matrix. None Additional comments including restrictions and unusual features: The current version is compatible with the free FDMNES program package for XANES simulations. However, users can prepare their own matrices of spectra calculated by an arbitrary software and the corresponding structural parameters to perform the fitting procedure in PyFitIt. The complete set of examples is distributed along with the program. None References: PyFitIt web page: None http://hpc.nano.sfedu.ru/pyfitit/",project-academic
10.31812/EDUCDIM.V53I1.3831,2019-12-19,a,Видавничий центр Криворiзького державного педагогiчного унiверситету,theoretical and methodical aspects of the organization of students independent study activities together with the use of ict and tools," In the article the possibilities and classification of ICTs and tools that can be used in organizing students’ independent study activities of higher education institutions has been explored.
It is determined the students’ independent study activities is individual, group, collective activity and is implemented within the process of education under the condition of no pedagogy’s direct involvement. It complies with the requirements of the curriculum and syllabus and is aimed at students’ acquisition of some social experiences in line with the learning objectives of vocational training.
The analysis of the latest information and technological approaches to the organization of students’ independent study activities made it possible to determine the means of realization of the leading forms of organization for this activity (independent and research work, lectures, consultations and non-formal education), to characterize and classify the ICTs and tools that support presentation of teaching materials, electronic communication, mastering of learning material, monitoring of students’ learning and cognitive activity, such as ones that serve for the sake of development and support of automated training courses, systems of remote virtual education with elements of artificial intelligence, which implement the principle of adaptive management of learning and the organization of students’ independent study activities. 
The paper provides the insight into the essence of the conducted investigation on the assesses of the effectiveness of ICTs and tools in the process of organizing students’ independent study activities.",project-academic
10.0253/TUPRINTS-00003014,2012-06-19,a,,accelerometer based biometric gait recognition for authentication on smartphones," The authentication via accelerometer-based biometric gait recognition offers a user-friendly alternative to common authentication methods on smartphones. It has the great advantage that the authentication can be performed without user interaction. When the user is walking, his walk-pattern can be extracted from the accelerations measured using the integrated sensors of the smartphone. This pattern can be used for authentication. A study showed that users often deactivate the authentication methods of their mobile devices because they consume too much time. Because all steps necessary to perform biometric gait recognition can be executed in the background, no user interaction is necessary for the presented technique. Performing a continuous authentication while the user is walking, an up-to-date authentication result is available at any point in time. During log-in, no calculations are necessary anymore, hence there is no delay. Only in cases where the user is not walking, an alternative authentication method has to be used. This is a great benefit for the user because he has the advantages of a phone which is protected by authentication but without the disadvantages common methods impose. This high user-friendliness is likely to increase the number of smartphones for which the screen-lock is linked with an authentication. Therefore, a higher security of the data stored in smartphones can be achieved. A misuse of the stored information by an unauthorized user can be prevented. Due to the growing distribution of powerful smartphones, the number of available applications is increasing as well. These applications result in a growing amount of data stored on the devices, which make the protection of the device necessary. These data comprise e.g. addresses, appointments or GPS-information. Additionally, some applications, e.g. of e-mail-providers or social networks, require the user to authenticate himself. Often these credentials are stored by the user on the phone, such that it is not necessary to enter them each time. In case an unauthorized person has access to such a phone he can use these services without restrictions and therefore substantially harm the user. The objective of this thesis was to develop methods for accelerometer-based biometric gait recognition which achieve sufficient low error rates, as well as to demonstrate that their computational effort is low and allows for an execution on current smartphones. Because the basis of existing methods is the extraction of gait cycles (i.e. two steps) from the accelerometer data, a cycle-based method was developed and evaluated in a scenario test. This method uses raw data of the gait cycles as feature vectors and accomplishes the classification using distance functions. In addition, a further approach was selected, which does not need the time-costly and error-prone gait cycle extraction. Instead, it is using overlapping segments of a fixed time length. Several features are extracted from these segments and combined to feature vectors. Machine learning algorithms are used for classification. A benchmark of the approaches on a challenging database showed that these methods yield low equal error rates between 6% and 7% and are outperforming the cycle-based methods. These error rates were achieved under the realistic conditions that training and probe data are not collected on the same day. It was shown that five minutes of gait data are sufficient to thoroughly train the models. It should be regarded that the training data contain the different walking velocities at which the user should be recognized later on. To obtain low false rejection rates, the classification should be based on around three minutes walk data. Two of the developed methods were implemented on a smartphone. It was shown that both methods are able to perform the classification fast enough to allow for an authentication without delay for the user.",project-academic
10.5555/1295109.1295123,2008-01-01,a,Consortium for Computing Sciences in Colleges,detecting outsourced student programming assignments," The task of writing computer programs outside of class is the most realistic experience students have in a programming class and hence can be the most accurate evaluation of their ability. However some students hire outside parties to produce these programs. We present a data mining and machine learning approach that can provide objective evidence for detecting such instances. Based on programs submitted by students across two lower-level CS (Computer Science) courses, we extract some basic programming style metrics. A decision tree model built on the collected measurements yields relatively good detection accuracy. In addition, an investigation into relative importance of the basic style metrics was performed which indicated Lines of Code, Number of Variables, and Number of Comments as important attributes. The methods are being implemented in a software analysis tool that instructors could possibly use for detecting outsourced program submissions.",project-academic
10.4108/EAI.7-8-2017.152984,2017-08-07,p,"ICST (Institute for Computer Sciences, Social-Informatics and Telecommunications Engineering)",iot enabled electric vehicle s battery monitoring system," The Internet of Things (IoT) technology has immense potential for application in improvement and development of Smart Grid. The rising number of distributed generation, aging of present grid infrastructure and appeal for the transformation of networks have sparked the interest in smart grid. The need for energy storage system primarily the electrical energy storage systems is growing as the prospects for their usage is becoming more compelling. Dynamic electrical energy storage system viz., Electric Vehicles (EVs) are relatively standard due to their excellent electrical properties and flexibility but the possibility of damage to their batteries is there in case of overcharging or deep discharging and their mass penetration profoundly impacts the grid. To circumvent the possibility of damage, EVs’ batteries need a precise state of charge estimation to increase their lifespan and to protect the equipment they power. Based on ease of implementation and less overall complexity, this paper proposes a real-time Battery Monitoring System (BMS) using coulomb counting method for SoC estimation and messaging based MQTT as the communication protocol. The proposed BMS is implemented on hardware platform using appropriate sensing technology, central processor, interfacing devices and the Node-RED environment. An optimization model aimed at maximizing the trade revenue for EVs’ aggregator is presented aimed at enabling the smart charging.

[9] L. Lu, et.al., “A review on the key issues for lithium-ion battery management in electric vehicles,” Journal of Power Sources, vol. 226. pp. 272–288, 2013.

[10] M. Charkhgard and M. Farrokhi, “State-of-charge estimation for lithium-ion batteries using neural networks and EKF,” IEEE Trans. Ind. Electron., vol. 57, no. 12, pp. 4178–4187, 2010.

[11] G. Y. Y. Ding-xuan, “SOC estimation of Lithium-ion battery based on Kalman filter algorithm,” in 2nd International Conference on Computer Science and Electronics Engineering (ICCSEE), 2013.

[12] C. M. and Y. H. K.S. Ng, Y.F Huang, “An enhanced coulomb counting method for estimating state-of-charge and state-of-health of lead-acid batteries,” in 31st International Telecommunications Energy Conference (INTELEC 2009).

[13] Y. M. Jeong, Y. K. Cho, J. H. Ahn, S. H. Ryu, and B. K. Lee, “Enhanced coulomb counting method with adaptive SOC reset time for estimating OCV,” in 2014 IEEE Energy Conversion Congress and Exposition, ECCE 2014, 2014, pp. 4313–4318.

[14] K. S. Ng, et.al., “Enhanced coulomb counting method for estimating state-of-charge and state-of-health of lithium-ion batteries,” Appl. Energy, vol. 86, no. 9, pp. 1506–1511, 2009.

[15] Node-Red.: http://nodered.org/.

[16] MQTT Publish & Subscribe.: http://www.hivemq.com/blog/mqtt-essentials-part2-publish-subscribe.",project-academic
10.1109/ICCAD45719.2019.8942103,2019-06-28,p,IEEE,tucker tensor decomposition on fpga," Tensor computation has emerged as a powerful mathematical tool for solving high-dimensional and/or extreme-scale problems in science and engineering. The last decade has witnessed tremendous advancement of tensor computation and its applications in machine learning and big data. However, its hardware optimization on resource-constrained devices remains an (almost) unexplored field. This paper presents an hardware accelerator for a classical tensor computation framework, Tucker decomposition. We study three modules of this architecture: tensor-times-matrix (TTM), matrix singular value decomposition (SVD), and tensor permutation, and implemented them on Xilinx FPGA for prototyping. In order to further reduce the computing time, a warm-start algorithm for the Jacobi iterations in SVD is proposed. A fixed-point simulator is used to evaluate the performance of our design. Some synthetic data sets and a real MRI data set are used to validate the design and evaluate its performance. We compare our work with state-of-the-art software toolboxes running on both CPU and GPU, and our work shows 2.16 – 30.2× speedup on the cardiac MRI data set.",project-academic
,2019-06-28,a,,tucker tensor decomposition on fpga," Tensor computation has emerged as a powerful mathematical tool for solving high-dimensional and/or extreme-scale problems in science and engineering. The last decade has witnessed tremendous advancement of tensor computation and its applications in machine learning and big data. However, its hardware optimization on resource-constrained devices remains an (almost) unexplored field. This paper presents an hardware accelerator for a classical tensor computation framework, Tucker decomposition. We study three modules of this architecture: tensor-times-matrix (TTM), matrix singular value decomposition (SVD), and tensor permutation, and implemented them on Xilinx FPGA for prototyping. In order to further reduce the computing time, a warm-start algorithm for the Jacobi iterations in SVD is proposed. A fixed-point simulator is used to evaluate the performance of our design. Some synthetic data sets and a real MRI data set are used to validate the design and evaluate its performance. We compare our work with state-of-the-art software toolboxes running on both CPU and GPU, and our work shows 2.16 - 30.2x speedup on the cardiac MRI data set.",project-academic
10.1007/S10845-018-1433-8,2020-01-01,a,Springer US,literature review of industry 4 0 and related technologies," Manufacturing industry profoundly impact economic and societal progress. As being a commonly accepted term for research centers and universities, the Industry 4.0 initiative has received a splendid attention of the business and research community. Although the idea is not new and was on the agenda of academic research in many years with different perceptions, the term “Industry 4.0” is just launched and well accepted to some extend not only in academic life but also in the industrial society as well. While academic research focuses on understanding and defining the concept and trying to develop related systems, business models and respective methodologies, industry, on the other hand, focuses its attention on the change of industrial machine suits and intelligent products as well as potential customers on this progress. It is therefore important for the companies to primarily understand the features and content of the Industry 4.0 for potential transformation from machine dominant manufacturing to digital manufacturing. In order to achieve a successful transformation, they should clearly review their positions and respective potentials against basic requirements set forward for Industry 4.0 standard. This will allow them to generate a well-defined road map. There has been several approaches and discussions going on along this line, a several road maps are already proposed. Some of those are reviewed in this paper. However, the literature clearly indicates the lack of respective assessment methodologies. Since the implementation and applications of related theorems and definitions outlined for the 4th industrial revolution is not mature enough for most of the reel life implementations, a systematic approach for making respective assessments and evaluations seems to be urgently required for those who are intending to speed this transformation up. It is now main responsibility of the research community to developed technological infrastructure with physical systems, management models, business models as well as some well-defined Industry 4.0 scenarios in order to make the life for the practitioners easy. It is estimated by the experts that the Industry 4.0 and related progress along this line will have an enormous effect on social life. As outlined in the introduction, some social transformation is also expected. It is assumed that the robots will be more dominant in manufacturing, implanted technologies, cooperating and coordinating machines, self-decision-making systems, autonom problem solvers, learning machines, 3D printing etc. will dominate the production process. Wearable internet, big data analysis, sensor based life, smart city implementations or similar applications will be the main concern of the community. This social transformation will naturally trigger the manufacturing society to improve their manufacturing suits to cope with the customer requirements and sustain competitive advantage. A summary of the potential progress along this line is reviewed in introduction of the paper. It is so obvious that the future manufacturing systems will have a different vision composed of products, intelligence, communications and information network. This will bring about new business models to be dominant in industrial life. Another important issue to take into account is that the time span of this so-called revolution will be so short triggering a continues transformation process to yield some new industrial areas to emerge. This clearly puts a big pressure on manufacturers to learn, understand, design and implement the transformation process. Since the main motivation for finding the best way to follow this transformation, a comprehensive literature review will generate a remarkable support. This paper presents such a review for highlighting the progress and aims to help improve the awareness on the best experiences. It is intended to provide a clear idea for those wishing to generate a road map for digitizing the respective manufacturing suits. By presenting this review it is also intended to provide a hands-on library of Industry 4.0 to both academics as well as industrial practitioners. The top 100 headings, abstracts and key words (i.e. a total of 619 publications of any kind) for each search term were independently analyzed in order to ensure the reliability of the review process. Note that, this exhaustive literature review provides a concrete definition of Industry 4.0 and defines its six design principles such as interoperability, virtualization, local, real-time talent, service orientation and modularity. It seems that these principles have taken the attention of the scientists to carry out more variety of research on the subject and to develop implementable and appropriate scenarios. A comprehensive taxonomy of Industry 4.0 can also be developed through analyzing the results of this review.",project-academic
10.1016/J.MFGLET.2018.09.002,2018-10-01,a,Elsevier,industrial artificial intelligence for industry 4 0 based manufacturing systems," Abstract None None The recent White House report on Artificial Intelligence (AI) (Lee, 2016) highlights the significance of AI and the necessity of a clear roadmap and strategic investment in this area. As AI emerges from science fiction to become the frontier of world-changing technologies, there is an urgent need for systematic development and implementation of AI to see its real impact in the next generation of industrial systems, namely Industry 4.0. Within the 5C architecture previously proposed in Lee et al. (2015), this paper provides an insight into the current state of AI technologies and the eco-system required to harness the power of AI in industrial applications.",project-academic
10.1007/978-0-387-68832-9,2007-04-01,b,Springer-Verlag,cognitive wireless communication networks," The proposed book includes a set of research and survey articles featuring the recent advances in theory and applications of cognitive radio technology for the next generation (e.g., fourth generation) wireless communication networks. Cognitive radio has emerged as a promising technology for maximizing the utilization of the limited radio bandwidth while accommodating the increasing amount of services and applications in the wireless networks. A cognitive radio transceiver is able to adapt to the dynamic radio environment and the network parameters to maximize the utilization of the limited radio resources while at the same time providing flexibility in wireless access. Development of cognitive radio technology has to deal with technical and practical considerations as well as regulatory requirements, and therefore, there is an increasing interest on this technology among the researchers and the spectrum policy makers. The contributed articles cover both the theoretical concepts (e.g., information-theoretic analysis) and system-level implementation issues. Therefore, the book provides a unified view on the state of the art of cognitive radio technology. The topics include information-theoretic analysis of cognitive radio systems, challenges and issues in designing cognitive radio systems, architectures and protocols for cognitive wireless networks, distributed adaptation and optimization methods, real-time spectrum sensing and channel allocation, cognitive machine learning techniques, interoperability and co-existence issues, spectrum awareness and dynamic channel selection, cross-layer optimization of cognitive radio systems, cognitive radio test-beds and hardware prototypes, regulatory issues on spectrum sharing, and applications of cognitive radio networks. The book starts with the essential background on cognitive radio techniques and systems (through one/two survey articles), and then it presents advanced level materials in a step-by-step fashion so that the readers can follow the book easily. The rich set of references in each of the articles will be invaluable to the researchers. The book is useful to both researchers and practitioners in this area. Also, it can be adopted as a graduate-level textbook for an advanced course on wireless communication networks.",project-academic
10.1103/PHYSREVD.97.044039,2018-02-26,a,American Physical Society,deep neural networks to enable real time multimessenger astrophysics," Gravitational wave astronomy has set in motion a scientific revolution. To further enhance the science reach of this emergent field of research, there is a pressing need to increase the depth and speed of the algorithms used to enable these ground-breaking discoveries. We introduce Deep Filtering---a new scalable machine learning method for end-to-end time-series signal processing. Deep Filtering is based on deep learning with two deep convolutional neural networks, which are designed for classification and regression, to detect gravitational wave signals in highly noisy time-series data streams and also estimate the parameters of their sources in real time. Acknowledging that some of the most sensitive algorithms for the detection of gravitational waves are based on implementations of matched filtering, and that a matched filter is the optimal linear filter in Gaussian noise, the application of Deep Filtering using whitened signals in Gaussian noise is investigated in this foundational article. The results indicate that Deep Filtering outperforms conventional machine learning techniques, achieves similar performance compared to matched filtering, while being several orders of magnitude faster, allowing real-time signal processing with minimal resources. Furthermore, we demonstrate that Deep Filtering can detect and characterize waveform signals emitted from new classes of eccentric or spin-precessing binary black holes, even when trained with data sets of only quasicircular binary black hole waveforms. The results presented in this article, and the recent use of deep neural networks for the identification of optical transients in telescope data, suggests that deep learning can facilitate real-time searches of gravitational wave sources and their electromagnetic and astroparticle counterparts. In the subsequent article, the framework introduced herein is directly applied to identify and characterize gravitational wave events in real LIGO data.",project-academic
10.1038/NATURE23307,2017-08-17,a,Nature Research,chaotic dynamics in nanoscale nbo 2 mott memristors for analogue computing," A relaxation oscillator incorporating nanoscale niobium dioxide memristors that exhibit both a current- and a temperature-controlled negative differential resistance produces chaotic dynamics that aid biomimetic computing. In recent years, grids of memristor devices, with their synapse-like dynamics and adaptable conductivity, have demonstrated neural-network-type implementations of analogue (non-Boolean) computing. Suhas Kumar et al. now explore the possibility of exploiting chaotic dynamics in highly nonlinear niobium dioxide memristor devices. This idea is inspired by the theory that biological neurons operate in a regime called 'the edge of chaos', which is thought to be key to the ability of the human brain to tackle complex information processing tasks with high efficiency. The authors demonstrate a controllable regime of chaotic self-oscillations in their devices and simulate a memristor grid that can solve a typical computationally hard task—a travelling salesman problem—with higher accuracy and efficiency than an approach that does not incorporate chaotic elements. Building artificial neural networks with chaotic oscillators based on single electronic devices provides an exciting direction for unconventional analogue computing. At present, machine learning systems use simplified neuron models that lack the rich nonlinear phenomena observed in biological systems, which display spatio-temporal cooperative dynamics. There is evidence that neurons operate in a regime called the edge of chaos1 that may be central to complexity, learning efficiency, adaptability and analogue (non-Boolean) computation in brains2,3,4,5,6,7. Neural networks have exhibited enhanced computational complexity when operated at the edge of chaos2, and networks of chaotic elements have been proposed for solving combinatorial or global optimization problems8. Thus, a source of controllable chaotic behaviour that can be incorporated into a neural-inspired circuit may be an essential component of future computational systems. Such chaotic elements have been simulated using elaborate transistor circuits that simulate known equations of chaos9,10,11,12, but an experimental realization of chaotic dynamics from a single scalable electronic device has been lacking5,6,13. Here we describe niobium dioxide (NbO2) Mott memristors each less than 100 nanometres across that exhibit both a nonlinear-transport-driven current-controlled negative differential resistance and a Mott-transition-driven temperature-controlled negative differential resistance. Mott materials have a temperature-dependent metal–insulator transition that acts as an electronic switch, which introduces a history-dependent resistance into the device. We incorporate these memristors into a relaxation oscillator14 and observe a tunable range of periodic and chaotic self-oscillations15. We show that the nonlinear current transport coupled with thermal fluctuations at the nanoscale generates chaotic oscillations. Such memristors could be useful in certain types of neural-inspired computation by introducing a pseudo-random signal that prevents global synchronization and could also assist in finding a global minimum during a constrained search. We specifically demonstrate that incorporating such memristors into the hardware of a Hopfield computing network can greatly improve the efficiency and accuracy of converging to a solution for computationally difficult problems.",project-academic
10.1016/S0169-023X(99)00044-0,2000-04-01,p,Elsevier Science Publishers B. V.,semint a tool for identifying attribute correspondences in heterogeneous databases using neural networks, One step in interoperating among heterogeneous databases is semantic integration: Identifying relationships between attributes or classes in diAerent database schemas. SEMantic INTegrator (SEMINT) is a tool based on neural networks to assist in identifying attribute correspondences in heterogeneous databases. SEMINT supports access to a variety of database systems and utilizes both schema information and data contents to produce rules for matching corresponding attributes automatically. This paper provides theoretical background and implementation details of SEMINT. Experimental results from large and complex real databases are presented. We discuss the eAectiveness of SEMINT and our experiences with attribute correspondence identification in various environments. ” 2000 Elsevier Science B.V. All rights reserved.,project-academic
,2011-04-18,b,,data mining and statistics for decision making," Data mining is the process of automatically searching large volumes of data for models and patterns using computational techniques from statistics, machine learning and information theory; it is the ideal tool for such an extraction of knowledge. Data mining is usually associated with a business or an organization's need to identify trends and profiles, allowing, for example, retailers to discover patterns on which to base marketing objectives. This book looks at both classical and recent techniques of data mining, such as clustering, discriminant analysis, logistic regression, generalized linear models, regularized regression, PLS regression, decision trees, neural networks, support vector machines, Vapnik theory, naive Bayesian classifier, ensemble learning and detection of association rules. They are discussed along with illustrative examples throughout the book to explain the theory of these methods, as well as their strengths and limitations. Key Features: * Presents a comprehensive introduction to all techniques used in data mining and statistical learning, from classical to latest techniques. * Starts from basic principles up to advanced concepts. * Includes many step-by-step examples with the main software (R, SAS, IBM SPSS) as well as a thorough discussion and comparison of those software. * Gives practical tips for data mining implementation to solve real world problems. * Looks at a range of tools and applications, such as association rules, web mining and text mining, with a special focus on credit scoring. * Supported by an accompanying website hosting datasets and user analysis. Statisticians and business intelligence analysts, students as well as computer science, biology, marketing and financial risk professionals in both commercial and government organizations across all business and industry sectors will benefit from this book.",project-academic
,2021-02-18,p,,therapeutics data commons machine learning datasets and tasks for drug discovery and development," Therapeutics machine learning is an emerging field with incredible opportunities for innovatiaon and impact. However, advancement in this field requires formulation of meaningful learning tasks and careful curation of datasets. Here, we introduce Therapeutics Data Commons (TDC), the first unifying platform to systematically access and evaluate machine learning across the entire range of therapeutics. To date, TDC includes 66 AI-ready datasets spread across 22 learning tasks and spanning the discovery and development of safe and effective medicines. TDC also provides an ecosystem of tools and community resources, including 33 data functions and types of meaningful data splits, 23 strategies for systematic model evaluation, 17 molecule generation oracles, and 29 public leaderboards. All resources are integrated and accessible via an open Python library. We carry out extensive experiments on selected datasets, demonstrating that even the strongest algorithms fall short of solving key therapeutics challenges, including real dataset distributional shifts, multi-scale modeling of heterogeneous data, and robust generalization to novel data points. We envision that TDC can facilitate algorithmic and scientific advances and considerably accelerate machine-learning model development, validation and transition into biomedical and clinical implementation. TDC is an open-science initiative available at this https URL.",project-academic
10.1093/BIB/BBV118,2017-01-01,a,Oxford University Press,translational bioinformatics in the era of real time biomedical health care and wellness data streams," Monitoring and modeling biomedical, health care and wellness data from individuals and converging data on a population scale have tremendous potential to improve understanding of the transition to the healthy state of human physiology to disease setting. Wellness monitoring devices and companion software applications capable of generating alerts and sharing data with health care providers or social networks are now available. The accessibility and clinical utility of such data for disease or wellness research are currently limited. Designing methods for streaming data capture, real-time data aggregation, machine learning, predictive analytics and visualization solutions to integrate wellness or health monitoring data elements with the electronic medical records (EMRs) maintained by health care providers permits better utilization. Integration of population-scale biomedical, health care and wellness data would help to stratify patients for active health management and to understand clinically asymptomatic patients and underlying illness trajectories. In this article, we discuss various health-monitoring devices, their ability to capture the unique state of health represented in a patient and their application in individualized diagnostics, prognosis, clinical or wellness intervention. We also discuss examples of translational bioinformatics approaches to integrating patient-generated data with existing EMRs, personal health records, patient portals and clinical data repositories. Briefly, translational bioinformatics methods, tools and resources are at the center of these advances in implementing real-time biomedical and health care analytics in the clinical setting. Furthermore, these advances are poised to play a significant role in clinical decision-making and implementation of data-driven medicine and wellness care.",project-academic
10.1038/S41563-018-0248-5,2019-02-01,a,Nature Publishing Group,ionic modulation and ionic coupling effects in mos2 devices for neuromorphic computing," Coupled ionic-electronic effects present intriguing opportunities for device and circuit development. In particular, layered two-dimensional materials such as MoS2 offer highly anisotropic ionic transport properties, facilitating controlled ion migration and efficient ionic coupling among devices. Here, we report reversible modulation of MoS2 films that is consistent with local 2H-1T' phase transitions by controlling the migration of Li+ ions with an electric field, where an increase/decrease in the local Li+ ion concentration leads to the transition between the 2H (semiconductor) and 1T' (metal) phases. The resulting devices show excellent memristive behaviour and can be directly coupled with each other through local ionic exchange, naturally leading to synaptic competition and synaptic cooperation effects observed in biology. These results demonstrate the potential of direct modulation of two-dimensional materials through field-driven ionic processes, and can lead to future electronic and energy devices based on coupled ionic-electronic effects and biorealistic implementation of artificial neural networks.",project-academic
10.1145/2330601.2330629,2012-04-29,p,ACM,learning dispositions and transferable competencies pedagogy modelling and learning analytics," Theoretical and empirical evidence in the learning sciences substantiates the view that deep engagement in learning is a function of a complex combination of learners' identities, dispositions, values, attitudes and skills. When these are fragile, learners struggle to achieve their potential in conventional assessments, and critically, are not prepared for the novelty and complexity of the challenges they will meet in the workplace, and the many other spheres of life which require personal qualities such as resilience, critical thinking and collaboration skills. To date, the learning analytics research and development communities have not addressed how these complex concepts can be modelled and analysed, and how more traditional social science data analysis can support and be enhanced by learning analytics. We report progress in the design and implementation of learning analytics based on a research validated multidimensional construct termed ""learning power"". We describe, for the first time, a learning analytics infrastructure for gathering data at scale, managing stakeholder permissions, the range of analytics that it supports from real time summaries to exploratory research, and a particular visual analytic which has been shown to have demonstrable impact on learners. We conclude by summarising the ongoing research and development programme and identifying the challenges of integrating traditional social science research, with learning analytics and modelling.",project-academic
10.3389/FDGTH.2021.564906,2021-03-29,a,Front Digit Health,covid 19 and computer audition an overview on what speech sound analysis could contribute in the sars cov 2 corona crisis," At the time of writing this article, the world population is suffering from more than 2 million registered COVID-19 disease epidemic-induced deaths since the outbreak of the corona virus, which is now officially known as SARS-CoV-2. However, tremendous efforts have been made worldwide to counter-steer and control the epidemic by now labelled as pandemic. In this contribution, we provide an overview on the potential for computer audition (CA), i.e., the usage of speech and sound analysis by artificial intelligence to help in this scenario. We first survey which types of related or contextually significant phenomena can be automatically assessed from speech or sound. These include the automatic recognition and monitoring of COVID-19 directly or its symptoms such as breathing, dry, and wet coughing or sneezing sounds, speech under cold, eating behaviour, sleepiness, or pain to name but a few. Then, we consider potential use-cases for exploitation. These include risk assessment and diagnosis based on symptom histograms and their development over time, as well as monitoring of spread, social distancing and its effects, treatment and recovery, and patient well-being. We quickly guide further through challenges that need to be faced for real-life usage and limitations also in comparison with non-audio solutions. We come to the conclusion that CA appears ready for implementation of (pre-)diagnosis and monitoring tools, and more generally provides rich and significant, yet so far untapped potential in the fight against COVID-19 spread.",project-academic
,2020-03-24,a,,covid 19 and computer audition an overview on what speech sound analysis could contribute in the sars cov 2 corona crisis," At the time of writing, the world population is suffering from more than 10,000 registered COVID-19 disease epidemic induced deaths since the outbreak of the Corona virus more than three months ago now officially known as SARS-CoV-2. Since, tremendous efforts have been made worldwide to counter-steer and control the epidemic by now labelled as pandemic. In this contribution, we provide an overview on the potential for computer audition (CA), i.e., the usage of speech and sound analysis by artificial intelligence to help in this scenario. We first survey which types of related or contextually significant phenomena can be automatically assessed from speech or sound. These include the automatic recognition and monitoring of breathing, dry and wet coughing or sneezing sounds, speech under cold, eating behaviour, sleepiness, or pain to name but a few. Then, we consider potential use-cases for exploitation. These include risk assessment and diagnosis based on symptom histograms and their development over time, as well as monitoring of spread, social distancing and its effects, treatment and recovery, and patient wellbeing. We quickly guide further through challenges that need to be faced for real-life usage. We come to the conclusion that CA appears ready for implementation of (pre-)diagnosis and monitoring tools, and more generally provides rich and significant, yet so far untapped potential in the fight against COVID-19 spread.",project-academic
10.3390/E17042367,2015-04-20,a,Multidisciplinary Digital Publishing Institute,an entropy based network anomaly detection method," Data mining is an interdisciplinary subfield of computer science involving methods at the intersection of artificial intelligence, machine learning and statistics. One of the data mining tasks is anomaly detection which is the analysis of large quantities of data to identify items, events or observations which do not conform to an expected pattern. Anomaly detection is applicable in a variety of domains, e.g., fraud detection, fault detection, system health monitoring but this article focuses on application of anomaly detection in the field of network intrusion detection.The main goal of the article is to prove that an entropy-based approach is suitable to detect modern botnet-like malware based on anomalous patterns in network. This aim is achieved by realization of the following points: (i) preparation of a concept of original entropy-based network anomaly detection method, (ii) implementation of the method, (iii) preparation of original dataset, (iv) evaluation of the method.",project-academic
10.1145/3461702.3462612,2021-05-04,a,,envisioning communities a participatory approach towards ai for social good," Research in artificial intelligence (AI) for social good presupposes some definition of social good, but potential definitions have been seldom suggested and never agreed upon. The normative question of what AI for social good research should be ""for"" is not thoughtfully elaborated, or is frequently addressed with a utilitarian outlook that prioritizes the needs of the majority over those who have been historically marginalized, brushing aside realities of injustice and inequity. We argue that AI for social good ought to be assessed by the communities that the AI system will impact, using as a guide the capabilities approach, a framework to measure the ability of different policies to improve human welfare equity. Furthermore, we lay out how AI research has the potential to catalyze social progress by expanding and equalizing capabilities. We show how the capabilities approach aligns with a participatory approach for the design and implementation of AI for social good research in a framework we introduce called PACT, in which community members affected should be brought in as partners and their input prioritized throughout the project. We conclude by providing an incomplete set of guiding questions for carrying out such participatory AI research in a way that elicits and respects a community's own definition of social good.",project-academic
10.1051/0004-6361/201833390,2018-10-16,a,EDP Sciences,a new method for unveiling open clusters in gaia new nearby open clusters confirmed by dr2," Context . The publication of the Gaia None Data Release 2 (Gaia None DR2) opens a new era in astronomy. It includes precise astrometric data (positions, proper motions, and parallaxes) for more than 1.3 billion sources, mostly stars. To analyse such a vast amount of new data, the use of data-mining techniques and machine-learning algorithms is mandatory.Aims . A great example of the application of such techniques and algorithms is the search for open clusters (OCs), groups of stars that were born and move together, located in the disc. Our aim is to develop a method to automatically explore the data space, requiring minimal manual intervention.Methods . We explore the performance of a density-based clustering algorithm, DBSCAN, to find clusters in the data together with a supervised learning method such as an artificial neural network (ANN) to automatically distinguish between real OCs and statistical clusters.Results . The development and implementation of this method in a five-dimensional space (l , b , ϖ , μ α * None , μ δ None ) with the Tycho-Gaia Astrometric Solution (TGAS) data, and a posterior validation using Gaia None DR2 data, lead to the proposal of a set of new nearby OCs. Conclusions . We have developed a method to find OCs in astrometric data, designed to be applied to the full Gaia None DR2 archive.",project-academic
10.1038/S41699-020-0137-Z,2019-10-28,a,,deep learning based image segmentation integrated with optical microscopy for automatically searching for two dimensional materials," Deep-learning algorithms enable precise image recognition based on high-dimensional hierarchical image features. Here, we report the development and implementation of a deep-learning-based image segmentation algorithm in an autonomous robotic system to search for two-dimensional (2D) materials. We trained the neural network based on Mask-RCNN on annotated optical microscope images of 2D materials (graphene, hBN, MoS2, and WTe2). The inference algorithm is run on a 1024 x 1024 px2 optical microscope images for 200 ms, enabling the real-time detection of 2D materials. The detection process is robust against changes in the microscopy conditions, such as illumination and color balance, which obviates the parameter-tuning process required for conventional rule-based detection algorithms. Integrating the algorithm with a motorized optical microscope enables the automated searching and cataloging of 2D materials. This development will allow researchers to utilize unlimited amounts of 2D materials simply by exfoliating and running the automated searching process.",project-academic
,2020-03-02,a,American Physical Society,deep learning based image segmentation integrated with optical microscopy for automatically searching for two dimensional materials," Deep-learning algorithms enable precise image recognition based on high-dimensional hierarchical image features. Here, we report the development and implementation of a deep-learning-based image segmentation algorithm in an autonomous robotic system to search for two-dimensional (2D) materials. We trained the neural network based on Mask-RCNN on annotated optical microscope images of 2D materials (graphene, hBN, MoS2, and WTe2). The inference algorithm is run on a 1024 × 1024 px2 optical microscope images for 200 ms, enabling the real-time detection of 2D materials. The detection process is robust against changes in the microscopy conditions, such as illumination and color balance, which obviates the parameter-tuning process required for conventional rule-based detection algorithms. Integrating the algorithm with a motorized optical microscope enables the automated searching and cataloging of 2D materials. This development will allow researchers to utilize a large number of 2D materials simply by exfoliating and running the automated searching process. To facilitate research, we make the training codes, dataset, and model weights publicly available.",project-academic
,2000-01-01,b,,safe and sound artificial intelligence in hazardous applications," Computer science and artificial intelligence are increasingly used in the hazardous and uncertain realms of medical decision making, where small faults or errors can spell human catastrophe. This book describes, from both practical and theoretical perspectives, an AI technology for supporting sound clinical decision making and safe patient management. The technology combines techniques from conventional software engineering with a systematic method for building intelligent agents. Although the focus is on medicine, many of the ideas can be applied to AI systems in other hazardous settings. The book also covers a number of general AI problems, including knowledge representation and expertise modeling, reasoning and decision making under uncertainty, planning and scheduling, and the design and implementation of intelligent agents.The book, written in an informal style, begins with the medical background and motivations, technical challenges, and proposed solutions. It then turns to a wide-ranging discussion of intelligent and autonomous agents, with particular reference to safety and hazard management. The final section provides a detailed discussion of the knowledge representation and other aspects of the agent model developed in the book, along with a formal logical semantics for the language.",project-academic
10.2139/SSRN.3175792,2018-05-09,a,,china s social credit system an evolving practice of control," The Social Credit System (SCS) is perhaps the most prominent manifestation of the Chinese government's intention to reinforce legal, regulatory and policy processes through the application of information technology. Yet its organizational specifics have not yet received academic scrutiny. This paper will identify the objectives, perspectives and mechanisms through which the Chinese government has sought to realise its vision of ""social credit"". Reviewing the system's historical evolution, institutional structure, central and local implementation, and relationship with the private sector, this paper concludes that it is perhaps more accurate to conceive of the SCS as an ecosystem of initiatives broadly sharing a similar underlying logic, than a fully unified and integrated machine for social control. It also finds that, intentions with regards to big data and artificial intelligence notwithstanding, the SCS remains a relatively crude tool. This may change in the future, and this paper suggests the dimensions to be studied in order to assess this evolution.",project-academic
10.1038/S41565-021-00882-8,2020-08-29,a,,electrical tuning of phase change antennas and metasurfaces," The success of semiconductor electronics is built on the creation of compact, low-power switching elements that offer routing, logic, and memory functions. The availability of nanoscale optical switches could have a similarly transformative impact on the development of dynamic and programmable metasurfaces, optical neural networks, and quantum information processing. Phase change materials are uniquely suited to enable their creation as they offer high-speed electrical switching between amorphous and crystalline states with notably different optical properties. Their high refractive index has also been harnessed to fashion them into compact optical antennas. Here, we take the next important step by realizing electrically-switchable phase change antennas and metasurfaces that offer strong, reversible, non-volatile, multi-phase switching and spectral tuning of light scattering in the visible and near-infrared spectral ranges. Their successful implementation relies on a careful joint thermal and optical optimization of the antenna elements that comprise an Ag strip that simultaneously serves as a plasmonic resonator and a miniature heating stage.",project-academic
,2020-01-01,a,"Darmstadt Technical University, Department of Business Administration, Economics and Law, Institute for Business Studies (BWL)",ai based chatbots in customer service and their effects on user compliance," Communicating with customers through live chat interfaces has become an increasingly popular means to provide real-time customer service in many e-commerce settings. Today, human chat service agents are frequently replaced by conversational software agents or chatbots, which are systems designed to communicate with human users by means of natural language often based on artificial intelligence (AI). Though cost- and time-saving opportunities triggered a widespread implementation of AI-based chatbots, they still frequently fail to meet customer expectations, potentially resulting in users being less inclined to comply with requests made by the chatbot. Drawing on social response and commitment-consistency theory, we empirically examine through a randomized online experiment how verbal anthropomorphic design cues and the foot-in-the-door technique affect user request compliance. Our results demonstrate that both anthropomorphism as well as the need to stay consistent significantly increase the likelihood that users comply with a chatbot’s request for service feedback. Moreover, the results show that social presence mediates the effect of anthropomorphic design cues on user compliance.",project-academic
10.1007/S12525-020-00414-7,2020-03-17,a,Springer Berlin Heidelberg,ai based chatbots in customer service and their effects on user compliance," Communicating with customers through live chat interfaces has become an increasingly popular means to provide real-time customer service in many e-commerce settings. Today, human chat service agents are frequently replaced by conversational software agents or chatbots, which are systems designed to communicate with human users by means of natural language often based on artificial intelligence (AI). Though cost- and time-saving opportunities triggered a widespread implementation of AI-based chatbots, they still frequently fail to meet customer expectations, potentially resulting in users being less inclined to comply with requests made by the chatbot. Drawing on social response and commitment-consistency theory, we empirically examine through a randomized online experiment how verbal anthropomorphic design cues and the foot-in-the-door technique affect user request compliance. Our results demonstrate that both anthropomorphism as well as the need to stay consistent significantly increase the likelihood that users comply with a chatbot’s request for service feedback. Moreover, the results show that social presence mediates the effect of anthropomorphic design cues on user compliance.",project-academic
,2017-09-08,,,personalized recommendation system based on deep learning under social network," The invention discloses a personalized recommendation system based on deep learning under a social network. The system mainly comprises an offline learning module and an online recommendation module. The offline learning module firstly generates a training sample seat to construct a deep convolutional neural network learning module with an attention mechanism and carries out iterative optimization on parameters in the learning module; and the online recommendation module carries out real-time item recommendation on a newly-registered user based on the learning model obtained through training. Compared with the prior art, the system has the advantages of high accuracy, fast speed and simplicity and easiness in implementation and can be effectively applied to the fields, such as electronic commerce, public opinion monitoring, intelligent transportation and medical treatment and health.",project-academic
10.1016/J.JENVMAN.2011.01.020,2011-06-01,a,J Environ Manage,system dynamics modeling for municipal water demand estimation in an urban region under uncertain economic impacts," Accurate prediction of municipal water demand is critically important to water utilities in fast-growing urban regions for drinking water system planning, design, and water utility asset management. Achieving the desired prediction accuracy is challenging, however, because the forecasting model must simultaneously consider a variety of factors associated with climate changes, economic development, population growth and migration, and even consumer behavioral patterns. Traditional forecasting models such as multivariate regression and time series analysis, as well as advanced modeling techniques (e.g., expert systems and artificial neural networks), are often applied for either short- or long-term water demand projections, yet few can adequately manage the dynamics of a water supply system because of the limitations in modeling structures. Potential challenges also arise from a lack of long and continuous historical records of water demand and its dependent variables. The objectives of this study were to (1) thoroughly review water demand forecasting models over the past five decades, and (2) propose a new system dynamics model to reflect the intrinsic relationship between water demand and macroeconomic environment using out-of-sample estimation for long-term municipal water demand forecasts in a fast-growing urban region. This system dynamics model is based on a coupled modeling structure that takes into account the interactions among economic and social dimensions, offering a realistic platform for practical use. Practical implementation of this water demand forecasting tool was assessed by using a case study under the most recent alternate fluctuations of economic boom and downturn environments.",project-academic
10.1097/ACO.0B013E328337339C,2010-04-01,a,Curr Opin Anaesthesiol,anesthesia 2 0 internet based information resources and web 2 0 applications in anesthesia education," PURPOSE OF REVIEW Informatics is a broad field encompassing artificial intelligence, cognitive science, computer science, information science, and social science. The goal of this review is to illustrate how Web 2.0 information technologies could be used to improve anesthesia education. RECENT FINDINGS Educators in all specialties of medicine are increasingly studying Web 2.0 technologies to maximize postgraduate medical education of housestaff. These technologies include microblogging, blogs, really simple syndication (RSS) feeds, podcasts, wikis, and social bookmarking and networking. 'Anesthesia 2.0' reflects our expectation that these technologies will foster innovation and interactivity in anesthesia-related web resources which embraces the principles of openness, sharing, and interconnectedness that represent the Web 2.0 movement. Although several recent studies have shown benefits of implementing these systems into medical education, much more investigation is needed. SUMMARY Although direct practice and observation in the operating room are essential, Web 2.0 technologies hold great promise to innovate anesthesia education and clinical practice such that the resident learner need not be in a classroom for a didactic talk, or even in the operating room to see how an arterial line is properly placed. Thoughtful research to maximize implementation of these technologies should be a priority for development by academic anesthesiology departments. Web 2.0 and advanced informatics resources will be part of physician lifelong learning and clinical practice.",project-academic
10.1007/978-3-030-30493-5_53,2019-09-17,p,"Springer, Cham",signed graph attention networks," Graph or network data is ubiquitous in the real world, including social networks, information networks, traffic networks, biological networks and various technical networks. The non-Euclidean nature of graph data poses the challenge for modeling and analyzing graph data. Recently, Graph Neural Network (GNN) is proposed as a general and powerful framework to handle tasks on graph data, e.g., node embedding, link prediction and node classification. As a representative implementation of GNNs, Graph Attention Networks (GAT) is successfully applied in a variety of tasks on real datasets. However, GAT is designed to networks with only positive links and fails to handle signed networks which contain both positive and negative links. In this paper, we propose Signed Graph Attention Networks (SiGAT), generalizing GAT to signed networks. SiGAT incorporates graph motifs into GAT to capture two well-known theories in signed network research, i.e., balance theory and status theory. In SiGAT, motifs offer us the flexible structural pattern to aggregate and propagate messages on the signed network to generate node embeddings. We evaluate the proposed SiGAT method by applying it to the signed link prediction task. Experimental results on three real datasets demonstrate that SiGAT outperforms feature-based method, network embedding method and state-of-the-art GNN-based methods like signed graph convolutional networks (SGCN).",project-academic
,2019-06-26,a,,signed graph attention networks," Graph or network data is ubiquitous in the real world, including social networks, information networks, traffic networks, biological networks and various technical networks. The non-Euclidean nature of graph data poses the challenge for modeling and analyzing graph data. Recently, Graph Neural Network (GNNs) are proposed as a general and powerful framework to handle tasks on graph data, e.g., node embedding, link prediction and node classification. As a representative implementation of GNNs, Graph Attention Networks (GATs) are successfully applied in a variety of tasks on real datasets. However, GAT is designed to networks with only positive links and fails to handle signed networks which contain both positive and negative links. In this paper, we propose Signed Graph Attention Networks (SiGATs), generalizing GAT to signed networks. SiGAT incorporates graph motifs into GAT to capture two well-known theories in signed network research, i.e., balance theory and status theory. In SiGAT, motifs offer us the flexible structural pattern to aggregate and propagate messages on the signed network to generate node embeddings. We evaluate the proposed SiGAT method by applying it to the signed link prediction task. Experimental results on three real datasets demonstrate that SiGAT outperforms feature-based method, network embedding method and state-of-the-art GNN-based methods like signed graph convolutional network (SGCN).",project-academic
10.1093/JAS/SKY014,2018-04-14,a,American Society of Animal Science,big data analytics and precision animal agriculture symposium machine learning and data mining advance predictive big data analysis in precision animal agriculture," Precision animal agriculture is poised to rise to prominence in the livestock enterprise in the domains of management, production, welfare, sustainability, health surveillance, and environmental footprint. Considerable progress has been made in the use of tools to routinely monitor and collect information from animals and farms in a less laborious manner than before. These efforts have enabled the animal sciences to embark on information technology-driven discoveries to improve animal agriculture. However, the growing amount and complexity of data generated by fully automated, high-throughput data recording or phenotyping platforms, including digital images, sensor and sound data, unmanned systems, and information obtained from real-time noninvasive computer vision, pose challenges to the successful implementation of precision animal agriculture. The emerging fields of machine learning and data mining are expected to be instrumental in helping meet the daunting challenges facing global agriculture. Yet, their impact and potential in ""big data"" analysis have not been adequately appreciated in the animal science community, where this recognition has remained only fragmentary. To address such knowledge gaps, this article outlines a framework for machine learning and data mining and offers a glimpse into how they can be applied to solve pressing problems in animal sciences.",project-academic
10.1093/JAMIA/OCZ152,2020-02-01,a,J Am Med Inform Assoc,consumer health information and question answering helping consumers find answers to their health related information needs," Objective None Consumers increasingly turn to the internet in search of health-related information; and they want their questions answered with short and precise passages, rather than needing to analyze lists of relevant documents returned by search engines and reading each document to find an answer. We aim to answer consumer health questions with information from reliable sources. None Materials and methods None We combine knowledge-based, traditional machine and deep learning approaches to understand consumers' questions and select the best answers from consumer-oriented sources. We evaluate the end-to-end system and its components on simple questions generated in a pilot development of MedlinePlus Alexa skill, as well as the short and long real-life questions submitted to the National Library of Medicine by consumers. None Results None Our system achieves 78.7% mean average precision and 87.9% mean reciprocal rank on simple Alexa questions, and 44.5% mean average precision and 51.6% mean reciprocal rank on real-life questions submitted by National Library of Medicine consumers. None Discussion None The ensemble of deep learning, domain knowledge, and traditional approaches recognizes question type and focus well in the simple questions, but it leaves room for improvement on the real-life consumers' questions. Information retrieval approaches alone are sufficient for finding answers to simple Alexa questions. Answering real-life questions, however, benefits from a combination of information retrieval and inference approaches. None Conclusion None A pilot practical implementation of research needed to help consumers find reliable answers to their health-related questions demonstrates that for most questions the reliable answers exist and can be found automatically with acceptable accuracy.",project-academic
10.1038/S41467-018-07330-7,2018-12-04,a,Nature Publishing Group,self limited single nanowire systems combining all in one memristive and neuromorphic functionalities," The ability for artificially reproducing human brain type signals' processing is one of the main challenges in modern information technology, being one of the milestones for developing global communicating networks and artificial intelligence. Electronic devices termed memristors have been proposed as effective artificial synapses able to emulate the plasticity of biological counterparts. Here we report for the first time a single crystalline nanowire based model system capable of combining all memristive functions - non-volatile bipolar memory, multilevel switching, selector and synaptic operations imitating Ca2+ dynamics of biological synapses. Besides underlying common electrochemical fundamentals of biological and artificial redox-based synapses, a detailed analysis of the memristive mechanism revealed the importance of surfaces and interfaces in crystalline materials. Our work demonstrates the realization of self-assembled, self-limited devices feasible for implementation via bottom up approach, as an attractive solution for the ultimate system miniaturization needed for the hardware realization of brain-inspired systems.",project-academic
10.1016/J.IPM.2020.102370,2020-11-01,a,Pergamon,an incentive aware blockchain based solution for internet of fake media things," Abstract None None The concept of Fake Media or Internet of Fake Media Things (IoFMT) has emerged in different domains of digital society such as politics, news, and social media. Due to the integrity of the media being compromised quite frequently, revolutionary changes must be taken to avoid further and more widespread IoFMT. With today’s advancements in Artificial Intelligence (AI) and Deep Learning (DL), such compromises may be profoundly limited. Providing proof of authenticity to outline the authorship and integrity for digital content has been a pressing need. Blockchain, a promising new decentralized secure platform, has been advocated to help combat the authenticity aspect of fake media in a context where resistance to the modification of data is important. Although some methods around blockchain have been proposed to take on authentication problems, most current studies are built on unrealistic assumptions with the after-the-incident type of mechanisms. In this article, we propose a preventative approach using a novel blockchain-based solution suited for IoFMT incorporated with a gamification component. More specifically, the proposed approach uses concepts of a customized Proof-of-Authority consensus algorithm, along with a weighted-ranking algorithm, serving as an incentive mechanism in the gamification component to determine the integrity of fake news. Although our approach focuses on fake news, the framework could be very well extended for other types of digital content as well. A proof of concept implementation is developed to outline the advantage of the proposed solution.",project-academic
10.1038/S41467-020-20062-X,2020-02-19,a,,phase imaging with computational specificity pics for measuring dry mass changes in sub cellular compartments," Due to its specificity, fluorescence microscopy (FM) has become a quintessential imaging tool in cell biology. However, photobleaching, phototoxicity, and related artifacts continue to limit FM's utility. Recently, it has been shown that artificial intelligence (AI) can transform one form of contrast into another. We present PICS, a combination of quantitative phase imaging and AI, which provides information about unlabeled live cells with high specificity. Our imaging system allows for automatic training, while inference is built into the acquisition software and runs in real-time. Applying the computed fluorescence maps back to the QPI data, we measured the growth of both nuclei and cytoplasm independently, over many days, without loss of viability. Using a QPI method that suppresses multiple scattering, we measured the dry mass content of individual cell nuclei within spheroids. In its current implementation, PICS offers a versatile quantitative technique for continuous simultaneous monitoring of individual cellular components in biological applications where long-term label-free imaging is desirable.",project-academic
10.1002/PLD3.252,2020-08-01,a,"John Wiley & Sons, Ltd",plant science decadal vision 2020 2030 reimagining the potential of plants for a healthy and sustainable future," Plants, and the biological systems around them, are key to the future health of the planet and its inhabitants. The Plant Science Decadal Vision 2020-2030 frames our ability to perform vital and far-reaching research in plant systems sciences, essential to how we value participants and apply emerging technologies. We outline a comprehensive vision for addressing some of our most pressing global problems through discovery, practical applications, and education. The Decadal Vision was developed by the participants at the Plant Summit 2019, a community event organized by the Plant Science Research Network. The Decadal Vision describes a holistic vision for the next decade of plant science that blends recommendations for research, people, and technology. Going beyond discoveries and applications, we, the plant science community, must implement bold, innovative changes to research cultures and training paradigms in this era of automation, virtualization, and the looming shadow of climate change. Our vision and hopes for the next decade are encapsulated in the phrase reimagining the potential of plants for a healthy and sustainable future. The Decadal Vision recognizes the vital intersection of human and scientific elements and demands an integrated implementation of strategies for research (Goals 1-4), people (Goals 5 and 6), and technology (Goals 7 and 8). This report is intended to help inspire and guide the research community, scientific societies, federal funding agencies, private philanthropies, corporations, educators, entrepreneurs, and early career researchers over the next 10 years. The research encompass experimental and computational approaches to understanding and predicting ecosystem behavior; novel production systems for food, feed, and fiber with greater crop diversity, efficiency, productivity, and resilience that improve ecosystem health; approaches to realize the potential for advances in nutrition, discovery and engineering of plant-based medicines, and ""green infrastructure."" Launching the Transparent Plant will use experimental and computational approaches to break down the phytobiome into a ""parts store"" that supports tinkering and supports query, prediction, and rapid-response problem solving. Equity, diversity, and inclusion are indispensable cornerstones of realizing our vision. We make recommendations around funding and systems that support customized professional development. Plant systems are frequently taken for granted therefore we make recommendations to improve plant awareness and community science programs to increase understanding of scientific research. We prioritize emerging technologies, focusing on non-invasive imaging, sensors, and plug-and-play portable lab technologies, coupled with enabling computational advances. Plant systems science will benefit from data management and future advances in automation, machine learning, natural language processing, and artificial intelligence-assisted data integration, pattern identification, and decision making. Implementation of this vision will transform plant systems science and ripple outwards through society and across the globe. Beyond deepening our biological understanding, we envision entirely new applications. We further anticipate a wave of diversification of plant systems practitioners while stimulating community engagement, underpinning increasing entrepreneurship. This surge of engagement and knowledge will help satisfy and stoke people's natural curiosity about the future, and their desire to prepare for it, as they seek fuller information about food, health, climate and ecological systems.",project-academic
10.1136/BMJOPEN-2021-052287,2021-07-28,a,British Medical Journal Publishing Group,diverse experts perspectives on ethical issues of using machine learning to predict hiv aids risk in sub saharan africa a modified delphi study," Objective None To better understand diverse experts’ views about the ethical implications of ongoing research funded by the National Institutes of Health that uses machine learning to predict HIV/AIDS risk in sub-Saharan Africa (SSA) based on publicly available Demographic and Health Surveys data. None Design None Three rounds of semi-structured surveys in an online expert panel using a modified Delphi approach. None Participants None Experts in informatics, African public health and HIV/AIDS and bioethics were invited to participate. None Measures None Perceived importance of or agreement about relevance of ethical issues on 5-point unipolar Likert scales. Qualitative data analysis identified emergent themes related to ethical issues and development of an ethical framework and recommendations for open-ended questions. None Results None Of the 35 invited experts, 22 participated in the online expert panel (63%). Emergent themes were the inclusion of African researchers in all aspects of study design, analysis and dissemination to identify and address local contextual issues, as well as engagement of communities. Experts focused on engagement with health and science professionals to address risks, benefits and communication of findings. Respondents prioritised the mitigation of stigma to research participants but recognised trade-offs between privacy and the need to disseminate findings to realise public health benefits. Strategies for responsible communication of results were suggested, including careful word choice in presentation of results and limited dissemination to need-to-know stakeholders such as public health planners. None Conclusion None Experts identified ethical issues specific to the African context and to research on sensitive, publicly available data and strategies for addressing these issues. These findings can be used to inform an ethical implementation framework with research stage-specific recommendations on how to use publicly available data for machine learning-based predictive analytics to predict HIV/AIDS risk in SSA.",project-academic
10.1103/PHYSREVLETT.118.080501,2017-02-21,a,American Physical Society,quantum machine learning over infinite dimensions," Machine learning is a fascinating and exciting field within computer science. Recently, this excitement has been transferred to the quantum information realm. Currently, all proposals for the quantum version of machine learning utilize the finite-dimensional substrate of discrete variables. Here we generalize quantum machine learning to the more complex, but still remarkably practical, infinite-dimensional systems. We present the critical subroutines of quantum machine learning algorithms for an all-photonic continuous-variable quantum computer that can lead to exponential speedups in situations where classical algorithms scale polynomially. Finally, we also map out an experimental implementation which can be used as a blueprint for future photonic demonstrations.",project-academic
10.1038/S41746-020-00318-Y,2020-08-21,a,Nature Publishing Group,developing a delivery science for artificial intelligence in healthcare," Artificial Intelligence (AI) has generated a large amount of excitement in healthcare, mostly driven by the emergence of increasingly accurate machine learning models. However, the promise of AI delivering scalable and sustained value for patient care in the real world setting has yet to be realized. In order to safely and effectively bring AI into use in healthcare, there needs to be a concerted effort around not just the creation, but also the delivery of AI. This AI ""delivery science"" will require a broader set of tools, such as design thinking, process improvement, and implementation science, as well as a broader definition of what AI will look like in practice, which includes not just machine learning models and their predictions, but also the new systems for care delivery that they enable. The careful design, implementation, and evaluation of these AI enabled systems will be important in the effort to understand how AI can improve healthcare.",project-academic
,2009-01-01,b,,principles of synthetic intelligence psi an architecture of motivated cognition," From the Foreword: ""In this book Joscha Bach introduces Dietrich Dorner's PSI architecture and Joscha's implementation of the MicroPSI architecture. These architectures and their implementation have several lessons for other architectures and models. Most notably, the PSI architecture includes drives and thus directly addresses questions of emotional behavior. An architecture including drives helps clarify how emotions could arise. It also changes the way that the architecture works on a fundamental level, providing an architecture more suited for behaving autonomously in a simulated world. PSI includes three types of drives, physiological (e.g., hunger), social (i.e., affiliation needs), and cognitive (i.e., reduction of uncertainty and expression of competency). These drives routinely influence goal formation and knowledge selection and application. The resulting architecture generates new kinds of behaviors, including context dependent memories, socially motivated behavior, and internally motivated task switching. This architecture illustrates how emotions and physical drives can be included in an embodied cognitive architecture. The PSI architecture, while including perceptual, motor, learning, and cognitive processing components, also includes several novel knowledge representations: temporal structures, spatial memories, and several new information processing mechanisms and behaviors, including progress through types of knowledge sources when problem solving (the Rasmussen ladder), and knowledge-based hierarchical active vision. These mechanisms and representations suggest ways for making other architectures more realistic, more accurate, and easier to use. The architecture is demonstrated in the Island simulated environment. While it may look like a simple game, it was carefully designed to allow multiple tasks to be pursued and provides ways to satisfy the multiple drives. It would be useful in its own right for developing other architectures interested in multi-tasking, long-term learning, social interaction, embodied architectures, and related aspects of behavior that arise in a complex but tractable real-time environment. The resulting models are not presented as validated cognitive models, but as theoretical explorations in the space of architectures for generating behavior. The sweep of the architecture can thus be larger-it presents a new cognitive architecture attempting to provide a unified theory of cognition. It attempts to cover perhaps the largest number of phenomena to date. This is not a typical cognitive modeling work, but one that I believe that we can learn much from."" --Frank E. Ritter, Series Editor Although computational models of cognition have become very popular, these models are relatively limited in their coverage of cognition-- they usually only emphasize problem solving and reasoning, or treat perception and motivation as isolated modules. The first architecture to cover cognition more broadly is PSI theory, developed by Dietrich Dorner. By integrating motivation and emotion with perception and reasoning, and including grounded neuro-symbolic representations, PSI contributes significantly to an integrated understanding of the mind. It provides a conceptual framework that highlights the relationships between perception and memory, language and mental representation, reasoning and motivation, emotion and cognition, autonomy and social behavior. It is, however, unfortunate that PSI's origin in psychology, its methodology, and its lack of documentation have limited its impact. The proposed book adapts Psi theory to cognitive science and artificial intelligence, by elucidating both its theoretical and technical frameworks, and clarifying its contribution to how we have come to understand cognition.",project-academic
10.1093/BIOINFORMATICS/BTY845,2019-05-01,a,Oxford University Press (OUP),lion lbd a literature based discovery system for cancer biology," Motivation None The overwhelming size and rapid growth of the biomedical literature make it impossible for scientists to read all studies related to their work, potentially leading to missed connections and wasted time and resources. Literature-based discovery (LBD) aims to alleviate these issues by identifying implicit links between disjoint parts of the literature. While LBD has been studied in depth since its introduction three decades ago, there has been limited work making use of recent advances in biomedical text processing methods in LBD. None Results None We present LION LBD, a literature-based discovery system that enables researchers to navigate published information and supports hypothesis generation and testing. The system is built with a particular focus on the molecular biology of cancer using state-of-the-art machine learning and natural language processing methods, including named entity recognition and grounding to domain ontologies covering a wide range of entity types and a novel approach to detecting references to the hallmarks of cancer in text. LION LBD implements a broad selection of co-occurrence based metrics for analyzing the strength of entity associations, and its design allows real-time search to discover indirect associations between entities in a database of tens of millions of publications while preserving the ability of users to explore each mention in its original context in the literature. Evaluations of the system demonstrate its ability to identify undiscovered links and rank relevant concepts highly among potential connections. None Availability and implementation None The LION LBD system is available via a web-based user interface and a programmable API, and all components of the system are made available under open licenses from the project home page http://lbd.lionproject.net. None Supplementary information None Supplementary data are available at Bioinformatics online.",project-academic
,2009-12-12,a,,comme il faut a system for simulating social games between autonomous characters," Modern video games have highly developed computational models of physical space, which allow sophisticated play in the physical realm. However, computational models of social interaction are rare, offer limited social play, and require a large amount of authoring to create. We believe that a computational model of social interaction inspired by appropriate humanities and social science concepts could help alleviate these problems and open up new areas of social play. In this paper, we describe a playable model called Comme il Faut that uses a social artificial intelligence system particularly inspired by Goffman’s dramaturgical analysis and Berne’s psychological games, constructed for authoring power rather than fidelity with the everyday world. Our theoretical basis, the system’s relation to other digital media and games, and its implementation are presented to explain Comme il Faut and our approach to enabling social play.",project-academic
10.3390/S19173760,2019-08-30,a,Multidisciplinary Digital Publishing Institute,review on smart gas sensing technology," With the development of the Internet-of-Things (IoT) technology, the applications of gas sensors in the fields of smart homes, wearable devices, and smart mobile terminals have developed by leaps and bounds. In such complex sensing scenarios, the gas sensor shows the defects of cross sensitivity and low selectivity. Therefore, smart gas sensing methods have been proposed to address these issues by adding sensor arrays, signal processing, and machine learning techniques to traditional gas sensing technologies. This review introduces the reader to the overall framework of smart gas sensing technology, including three key points; gas sensor arrays made of different materials, signal processing for drift compensation and feature extraction, and gas pattern recognition including Support Vector Machine (SVM), Artificial Neural Network (ANN), and other techniques. The implementation, evaluation, and comparison of the proposed solutions in each step have been summarized covering most of the relevant recently published studies. This review also highlights the challenges facing smart gas sensing technology represented by repeatability and reusability, circuit integration and miniaturization, and real-time sensing. Besides, the proposed solutions, which show the future directions of smart gas sensing, are explored. Finally, the recommendations for smart gas sensing based on brain-like sensing are provided in this paper.",project-academic
10.1016/J.TECHFORE.2016.03.022,2017-01-01,a,North-Holland,the global brain and the emerging economy of abundance mutualism open collaboration exchange networks and the automated commons," Abstract None None The emergence of artificial general intelligence and the global brain provides new opportunities for realizing humanity's long quest for a more utopian existence. One possibility is a more successful implementation of the state socialist vision of a centrally managed economy, possibly controlled by an AGI “Nanny” instead of a central committee of politicians. An alternative outcome, more in keeping with the original Marxist vision of the withering away of the state, may be the mutualist vision of organizing economic and social life along voluntary lines. A number of recent developments and new ideas may facilitate this outcome. The institution of the commons, in the past available only to small geographical communities, can now be used by global communities. Open collaboration and exchange networks facilitate voluntary cooperative activity by people at dispersed physical locations. Open Production Networks can make the most complicated economic exchanges transparent to consumers, allowing them to factor ethical and sociological considerations into their purchasing decisions. Offer Networks can help people with similar interests and complementary abilities to organize joint projects and organizations. Blockchain technologies could be used to create transparent currencies in which transactions can be done openly. These and other related technologies have the potential to humanize global economic interactions, giving them more emotional resonance, as increasing affluence lessens individual and societal preoccupation with maximizing economic gain.",project-academic
10.1016/J.JCP.2017.09.057,2018-01-01,a,Academic Press,sparsity enabled cluster reduced order models for control," Abstract None None Characterizing and controlling nonlinear, multi-scale phenomena are central goals in science and engineering. Cluster-based reduced-order modeling (CROM) was introduced to exploit the underlying low-dimensional dynamics of complex systems. CROM builds a data-driven discretization of the Perron–Frobenius operator, resulting in a probabilistic model for ensembles of trajectories. A key advantage of CROM is that it embeds nonlinear dynamics in a linear framework, which enables the application of standard linear techniques to the nonlinear system. CROM is typically computed on high-dimensional data; however, access to and computations on this full-state data limit the online implementation of CROM for prediction and control. Here, we address this key challenge by identifying a small subset of critical measurements to learn an efficient CROM, referred to as sparsity-enabled CROM. In particular, we leverage compressive measurements to faithfully embed the cluster geometry and preserve the probabilistic dynamics. Further, we show how to identify fewer optimized sensor locations tailored to a specific problem that outperform random measurements. Both of these sparsity-enabled sensing strategies significantly reduce the burden of data acquisition and processing for low-latency in-time estimation and control. We illustrate this unsupervised learning approach on three different high-dimensional nonlinear dynamical systems from fluids with increasing complexity, with one application in flow control. Sparsity-enabled CROM is a critical facilitator for real-time implementation on high-dimensional systems where full-state information may be inaccessible.",project-academic
,2016-12-31,a,,sparsity enabled cluster reduced order models for control," Characterizing and controlling nonlinear, multi-scale phenomena play important roles in science and engineering. Cluster-based reduced-order modeling (CROM) was introduced to exploit the underlying low-dimensional dynamics of complex systems. CROM builds a data-driven discretization of the Perron-Frobenius operator, resulting in a probabilistic model for ensembles of trajectories. A key advantage of CROM is that it embeds nonlinear dynamics in a linear framework, and uncertainty can be managed with data assimilation. CROM is typically computed on high-dimensional data, however, access to and computations on this full-state data limit the online implementation of CROM for prediction and control. Here, we address this key challenge by identifying a small subset of critical measurements to learn an efficient CROM, referred to as sparsity-enabled CROM. In particular, we leverage compressive measurements to faithfully embed the cluster geometry and preserve the probabilistic dynamics. Further, we show how to identify fewer optimized sensor locations tailored to a specific problem that outperform random measurements. Both of these sparsity-enabled sensing strategies significantly reduce the burden of data acquisition and processing for low-latency in-time estimation and control. We illustrate this unsupervised learning approach on three different high-dimensional nonlinear dynamical systems from fluids with increasing complexity, with one application in flow control. Sparsity-enabled CROM is a critical facilitator for real-time implementation on high-dimensional systems where full-state information may be inaccessible.",project-academic
10.1016/J.ASCOM.2015.01.002,2015-04-01,a,Elsevier,the overlooked potential of generalized linear models in astronomy ii gamma regression and photometric redshifts," Abstract None None Machine learning techniques offer a precious tool box for use within astronomy to solve problems involving so-called big data. They provide a means to make accurate predictions about a particular system without prior knowledge of the underlying physical processes of the data. In this article, and the companion papers of this series, we present the set of Generalized Linear Models (GLMs) as a fast alternative method for tackling general astronomical problems, including the ones related to the machine learning paradigm. To demonstrate the applicability of GLMs to inherently positive and continuous physical observables, we explore their use in estimating the photometric redshifts of galaxies from their multi-wavelength photometry. Using the gamma family with a log link function we predict redshifts from the PHoto-z Accuracy Testing simulated catalogue and a subset of the Sloan Digital Sky Survey from Data Release 10. We obtain fits that result in catastrophic outlier rates as low as ∼1% for simulated and ∼2% for real data. Moreover, we can easily obtain such levels of precision within a matter of seconds on a normal desktop computer and with training sets that contain merely thousands of galaxies. Our software is made publicly available as a user-friendly package developed in None Python , None R None and via an interactive web application. This software allows users to apply a set of GLMs to their own photometric catalogues and generates publication quality plots with minimum effort. By facilitating their ease of use to the astronomical community, this paper series aims to make GLMs widely known and to encourage their implementation in future large-scale projects, such as the Large Synoptic Survey Telescope.",project-academic
10.1126/SCIENCE.ABB1466,2020-03-06,a,American Association for the Advancement of Science,learning to deal with dual use," Biological research is profoundly valuable but can carry profound risks. The coronavirus outbreak reminds us of our vulnerability to biological threats and that research on pathogens is vital to threat mitigation. But such research can lead to catastrophic safety and security incidents. A global proliferation of tools and capabilities, driven by economic and national security interests, is also generating risks that fall outside current governance frameworks. We must learn to manage risks as quickly as we learn to manipulate life, but it remains unclear how well we are doing. One opportunity to learn is found in a new charge to the U.S. National Science Advisory Board for Biosecurity (NSABB), which reconvened in January after a 2-year hiatus, to assess the effectiveness of U.S. dual-use research oversight policies. To meet the charge and suggest improvements, the NSABB must address a neglected need for an evidence base for adaptive risk management. Building this evidence base will require revisiting success criteria, creating data infrastructures, and fostering adaptive policies and testbeds.

Clear criteria for success form the foundation of an evidence base. Criteria for policies designed to prevent rare incidents can be challenging to develop, but there are leading indicators that can validate design goals and assumptions. These can range from enhanced stakeholder participation and expressions of trust to reductions in near misses. Such criteria form the basis for sound policy design and implementation and must be regularly revisited.

The next step is to collect and analyze data to assess whether goals were met. A 2011 U.S. government–commissioned risk and benefit analysis of gain-of-function research exposed the dearth of data available to inform many risk management decisions. The NSABB and other groups have called—to no avail—for systems to collect and analyze data on incidents and near-misses as well as oversight. Failure of the U.S. government to quickly act on these recommendations means that the NSABB will again confront this data gap. Moreover, biosafety and biosecurity research, which seeks to understand and improve practices for risk management, remains chronically underfunded despite being called out as a priority in the U.S. National Biodefense Strategy. This research should be supported as an essential part of all major biological research programs so that it evolves alongside and is integrated with the latest advances.

Unfortunately, even when limited data are collected, they are often not shared lest they expose vulnerabilities or tarnish reputations. Legitimate security concerns abound, but an unknown and growing risk exposure is also dangerous. Many organizations do not fully exercise their ability to share details of their risk management process; they should do so to promote cross-organizational learning and to avoid repeating the same mistakes.

Policies designed to adapt to emerging evidence can provide standards and incentives for gathering data. Yet dual-use policies are typically narrowly scoped to prioritize limiting oversight burden over learning where concerns arise. This priority is misplaced in preparing for the future. As the scope of research broadens, our definitions of dual use will change, and we need to continue moving toward functional definitions of activities that could be hazardous that aren't reliant on quickly outdated lists. In many instances where there is uncertainty over appropriate scope, we should update policies regularly and prioritize observing activities over restricting them to gain visibility into the evolving landscape.

Communities of practitioners need testbeds to experiment with policy approaches and to learn from each other in real time. One example is the international Genetically Engineered Machine (iGEM) competition, where thousands of students from dozens of countries work on hundreds of projects that can often expose policy gaps. The competition has becomes a nexus to test strategies for managing dual-use technologies across many cases with yearly iteration. Such testbeds should be cultivated at many scales—from individual labs, to universities, to regional and national networks.

Biological science and technology will only become more essential to our societies, and we need strategies to learn to manage their power. Investing in an evidence base for adaptive risk management is essential to ensure that the future of the life sciences is one in which we want to live.",project-academic
10.1177/0022034519875450,2019-10-21,a,J Dent Res,evolution of aesthetic dentistry," One of the main goals of dental treatment is to mimic teeth and design smiles in a most natural and aesthetic manner, based on the individual and specific needs of the patient. Possibilities to reach that goal have significantly improved over the last decade through new and specific treatment modalities, steadily enhanced and more aesthetic dental materials, and novel techniques and technologies. This article gives an overview of the evolution of aesthetic dentistry over the past 100 y from a historical point of view and highlights advances in the development of dental research and clinical interventions that have contributed the science and art of aesthetic dentistry. Among the most noteworthy advancements over the past decade are the establishment of universal aesthetic rules and guidelines based on the assessment of natural aesthetic parameters, anatomy, and physiognomy; the development of tooth whitening and advanced restorative as well as prosthetic materials and techniques, supported by the pioneering discovery of dental adhesion; the significant progress in orthodontics and periodontal as well as oral and maxillofacial surgery; and, most recently, the implementation of digital technologies in the 3-dimensional planning and realization of truly natural, individual, and aesthetic smiles. In the future, artificial intelligence and machine learning will likely lead to automation of aesthetic evaluation, smile design, and treatment-planning processes.",project-academic
,2015-01-25,p,AAAI Press,a reduction of the elastic net to support vector machines with an application to gpu computing," Algorithmic reductions are one of the corner stones of theoretical computer science. Surprisingly, to-date, they have only played a limited role in machine learning. In this paper we introduce a formal and practical reduction between two of the most widely used machine learning algorithms: from the Elastic Net (and the Lasso as a special case) to the Support Vector Machine. First, we derive the reduction and summarize it in only 11 lines of MATLABTM. Then, we demonstrate its high impact potential by translating recent advances in parallelizing SVM solvers directly to the Elastic Net. The resulting algorithm is a parallel solver for the Elastic Net (and Lasso) that naturally utilizes GPU and multi-core CPUs. We evaluate it on twelve real world data sets, and show that it yields identical results as the popular (and highly optimized) glmnet implementation but is up-to two orders of magnitude faster.",project-academic
10.1039/D0LC00521E,2020-08-26,a,The Royal Society of Chemistry,ai on a chip," Artificial intelligence (AI) has dramatically changed the landscape of science, industry, defence, and medicine in the last several years. Supported by considerably enhanced computational power and cloud storage, the field of AI has shifted from mostly theoretical studies in the discipline of computer science to diverse real-life applications such as drug design, material discovery, speech recognition, self-driving cars, advertising, finance, medical imaging, and astronomical observation, where AI-produced outcomes have been proven to be comparable or even superior to the performance of human experts. In these applications, what is essentially important for the development of AI is the data needed for machine learning. Despite its prominent importance, the very first process of the AI development, namely data collection and data preparation, is typically the most laborious task and is often a limiting factor of constructing functional AI algorithms. Lab-on-a-chip technology, in particular microfluidics, is a powerful platform for both the construction and implementation of AI in a large-scale, cost-effective, high-throughput, automated, and multiplexed manner, thereby overcoming the above bottleneck. On this platform, high-throughput imaging is a critical tool as it can generate high-content information (e.g., size, shape, structure, composition, interaction) of objects on a large scale. High-throughput imaging can also be paired with sorting and DNA/RNA sequencing to conduct a massive survey of phenotype-genotype relations whose data is too complex to analyze with traditional computational tools, but is analyzable with the power of AI. In addition to its function as a data provider, lab-on-a-chip technology can also be employed to implement the developed AI for accurate identification, characterization, classification, and prediction of objects in mixed, heterogeneous, or unknown samples. In this review article, motivated by the excellent synergy between AI and lab-on-a-chip technology, we outline fundamental elements, recent advances, future challenges, and emerging opportunities of AI with lab-on-a-chip technology or ""AI on a chip"" for short.",project-academic
10.1109/ISSC.2018.8585369,2018-06-20,p,IEEE,bayesi chain intelligent identity authentication," In a bid to stamp out fraudulent crime, there is increased pressure on individuals to provide evidence that they possess a ‘real’ identity. Counterfeiting and fake identities have reduced confidence in traditional paper documentation as proof of identity, this has created a demand for an intelligent digital alternative. Recent government implementations and identity trends have also improved the popularity of digital forms of identification.Authentication of identity is a salient issue in the current climate where identity theft through record duplication is on the rise. Identity resolution techniques have proven effective in filtering duplicated and fake records in identity management systems. These techniques have been further improved by the implementation of machine learning techniques which are capable of revealing patterns and links that have formerly gone undetected. Research has also suggested that incorporating non-standard attributes in the form of social contextual data can increase the efficiency and success-rate of these fraud detection methods.In the digital age where individuals are creating large digital footprints, online accounts and activities can prove to be a valuable source of information that may contribute to ‘proof’ that an asserted identity is genuine. Online social contextual data – or ‘Digital identities’ -- pertaining to real people are built over time and bolstered by associated accounts, relationships and attributes. This data is difficult to fake and therefore may have the capacity to provide proof of a ‘real’ identity.This paper outlines the design and initial development of a solution that utilizes data sourced from an individual’s digital footprint to assess the likelihood that it pertains to a ‘real’ identity. This is achieved through application of machine learning and Bayesian probabilistic modelling techniques. Where identity sources are considered reliable, a secure and intelligent digital identification artefact will be created. This artefact will emulate a blockchain-inspired ledger and may subsequently be used to prove identity in place of traditional paper documentation.",project-academic
10.2196/16222,2019-10-28,a,J Med Internet Res,trust me i m a chatbot how artificial intelligence in health care fails the turing test," Over the next decade, one issue which will dominate sociotechnical studies in health informatics is the extent to which the promise of artificial intelligence in health care will be realized, along with the social and ethical issues which accompany it. A useful thought experiment is the application of the Turing test to user-facing artificial intelligence systems in health care (such as chatbots or conversational agents). In this paper I argue that many medical decisions require value judgements and the doctor-patient relationship requires empathy and understanding to arrive at a shared decision, often handling large areas of uncertainty and balancing competing risks. Arguably, medicine requires wisdom more than intelligence, artificial or otherwise. Artificial intelligence therefore needs to supplement rather than replace medical professionals, and identifying the complementary positioning of artificial intelligence in medical consultation is a key challenge for the future. In health care, artificial intelligence needs to pass the implementation game, not the imitation game.",project-academic
10.1109/MIS.2019.2942836,2020-01-01,a,IEEE,research on road traffic situation awareness system based on image big data," Road traffic is an important component of the national economy and social life. Promoting intelligent and Informa ionization construction in the field of road traffic is conducive to the construction of smart cities and the formulation of macro strategies and construction plans for urban traffic development. Aiming at the shortcomings of the current road traffic system, this article, on the basis of combining convolution neural network, situational awareness technology, database and other technologies, takes the road traffic situational awareness system as the research object, and analyzes the information collection, processing, and analysis process of road traffic situational awareness system. Convolutional neural networks (CNN), region-CNN (R-CNN), fast R-CNN, and faster R-CNN are used for vehicle class classification and location identification in road image big data. The deep convolutional neural network model based on road traffic image big data was further established, and the system requirements analysis and system framework design and implementation were carried out. Through the analysis and trial of actual cases, the results show the application effect of the realized road traffic situational awareness system, which provides a scientific reference and basis for the establishment of modern intelligent transportation system.",project-academic
10.1021/ACS.ACCOUNTS.0C00785,2021-02-02,a,American Chemical Society,data driven strategies for accelerated materials design," The ongoing revolution of the natural sciences by the advent of machine learning and artificial intelligence sparked significant interest in the material science community in recent years. The intrinsically high dimensionality of the space of realizable materials makes traditional approaches ineffective for large-scale explorations. Modern data science and machine learning tools developed for increasingly complicated problems are an attractive alternative. An imminent climate catastrophe calls for a clean energy transformation by overhauling current technologies within only several years of possible action available. Tackling this crisis requires the development of new materials at an unprecedented pace and scale. For example, organic photovoltaics have the potential to replace existing silicon-based materials to a large extent and open up new fields of application. In recent years, organic light-emitting diodes have emerged as state-of-the-art technology for digital screens and portable devices and are enabling new applications with flexible displays. Reticular frameworks allow the atom-precise synthesis of nanomaterials and promise to revolutionize the field by the potential to realize multifunctional nanoparticles with applications from gas storage, gas separation, and electrochemical energy storage to nanomedicine. In the recent decade, significant advances in all these fields have been facilitated by the comprehensive application of simulation and machine learning for property prediction, property optimization, and chemical space exploration enabled by considerable advances in computing power and algorithmic efficiency.In this Account, we review the most recent contributions of our group in this thriving field of machine learning for material science. We start with a summary of the most important material classes our group has been involved in, focusing on small molecules as organic electronic materials and crystalline materials. Specifically, we highlight the data-driven approaches we employed to speed up discovery and derive material design strategies. Subsequently, our focus lies on the data-driven methodologies our group has developed and employed, elaborating on high-throughput virtual screening, inverse molecular design, Bayesian optimization, and supervised learning. We discuss the general ideas, their working principles, and their use cases with examples of successful implementations in data-driven material discovery and design efforts. Furthermore, we elaborate on potential pitfalls and remaining challenges of these methods. Finally, we provide a brief outlook for the field as we foresee increasing adaptation and implementation of large scale data-driven approaches in material discovery and design campaigns.",project-academic
10.3389/FEVO.2018.00237,2019-01-15,a,Frontiers,decision making in agent based modeling a current review and future prospectus," All basic processes of ecological populations involve decisions; when and where to move, when and what to eat, and whether to fight or flee. Yet decisions and the underlying principles of decision-making have been difficult to integrate into the classical population-level models of ecology. Certainly, there is a long history of modeling individuals’ searching behavior, diet selection, or conflict dynamics within social interactions. When all individuals are given certain simple rules to govern their decision-making processes, the resultant population–level models have yielded important generalizations and theory. But it is also recognized that such models do not represent the way real individuals decide on actions. Factors that influence a decision include the organism’s environment with its dynamic rewards and risks, the complex internal state of the organism, and its imperfect knowledge of the environment. In the case of animals, it may also involve complex social factors, and experience and learning, which vary among individuals. The way that all factors are weighed and processed to lead to decisions is a major area of behavioral theory. While classic population-level modeling is limited in its ability to integrate decision-making in its actual complexity, the development of individual- or agent-based models (IBM/ABMs) (we use ABM throughout to designate both ‘agent-based modeling’ and an ‘agent-based model’) has opened the possibility of describing the way that decisions are made, and their effects, in minute detail. Over the years, these models have increased in size and complexity. Current ABMs can simulate thousands of individuals in realistic environments, and with highly detailed internal physiology, perception and ability to process the perceptions and make decisions based on those and their internal states. The implementation of decision-making in ABMs ranges from fairly simple to highly complex; the process of an individual deciding on an action can occur through the use of logical and simple (if-then) rules to more sophisticated neural networks and genetic algorithms. The purpose of this paper is to give an overview of the ways in which decisions are integrated into a variety of ABMs and to give a prospectus on the future of modeling of decisions in ABMs.",project-academic
,2016-07-27,b,,form versus function theory and models for neuronal substrates," The quest for endowing form with function represents the fundamental motivation behind all neural network modeling. In this thesis, we discuss various functional neuronal architectures and their implementation in silico, both on conventional computer systems and on neuromorpic devices. Necessarily, such casting to a particular substrate will constrain their form, either by requiring a simplified description of neuronal dynamics and interactions or by imposing physical limitations on important characteristics such as network connectivity or parameter precision. While our main focus lies on the computational properties of the studied models, we augment our discussion with rigorous mathematical formalism. We start by investigating the behavior of point neurons under synaptic bombardment and provide analytical predictions of single-unit and ensemble statistics. These considerations later become useful when moving to the functional network level, where we study the effects of an imperfect physical substrate on the computational properties of several cortical networks. Finally, we return to the single neuron level to discuss a novel interpretation of spiking activity in the context of probabilistic inference through sampling. We provide analytical derivations for the translation of this ``neural sampling'' framework to networks of biologically plausible and hardware-compatible neurons and later take this concept beyond the realm of brain science when we discuss applications in machine learning and analogies to solid-state systems.",project-academic
10.1101/19003913,2019-08-21,a,Cold Spring Harbor Laboratory Press,research on artificial intelligence and primary care a scoping review," ABSTRACT None Objective None The purpose of this study was to assess the nature and extent of the body of research on artificial intelligence (AI) and primary care. None Methods None We performed a scoping review, searching 11 published and grey literature databases with subject headings and key words pertaining to the concepts of 1) AI and 2) primary care: MEDLINE, EMBASE, Cinahl, Cochrane Library, Web of Science, Scopus, IEEE Xplore, ACM Digital Library, MathSciNet, AAAI, arXiv. Screening included title and abstract and then full text stages. Final inclusion criteria: 1) research study of any design, 2) developed or used AI, 3) used primary care data and/or study conducted in a primary care setting and/or explicit mention of study applicability to primary care; exclusion criteria: 1) narrative, editorial, or textbook chapter, 2) not applicable to primary care population or settings, 3) full text inaccessible in the English Language. We extracted and summarized seven key characteristics of included studies: overall study purpose(s), author appointments, primary care functions, author intended target end user(s), target health condition(s), location of data source(s) (if any), subfield(s) of AI. None Results None Of 5,515 non-duplicate documents, 405 met our eligibility criteria. The body of literature is primarily focused on creating novel AI methods or modifying existing AI methods to support physician diagnostic or treatment recommendations, for chronic conditions, using data from higher income countries. Meaningfully more studies had at least one author with a technology, engineering, or math appointment than with a primary care appointment (57 (14%) compared to 217 (54%)). Predominant AI subfields were supervised machine learning and expert systems. None Discussion None Overall, AI research associated with primary care is at an early stage of maturity with respect to widespread implementation in practice settings. For the field to progress, more interdisciplinary research teams with end-user engagement and evaluation studies are needed. None SUMMARY BOXES None Section 1: What is already known on this topic None Advancements in technology and the availability of health data have increased opportunities for artificial intelligence to be used for primary care purposes. None No comprehensive review of research on artificial intelligence associated with primary care has been performed. None Section 2: What this study adds None The body of research on artificial intelligence and primary care is driven by authors without appointments in primary care departments and is focused on developing artificial intelligence methods to support diagnostic and treatment decisions. None There is a need for more interdisciplinary research teams and evaluation of artificial intelligence projects in ‘real world’ practice settings.",project-academic
,2019-05-24,a,,deep learning based high resolution incoherent x ray imaging with a single pixel detector," X-ray ""ghost"" imaging has drawn great attention for its potential to lower radiation dose in medical diagnosis. For practical implementation, however, the efficiency and image quality have to be greatly improved. Here we demonstrate a computational ghost imaging scheme where a bucket detector and specially designed modulation masks are used, together with a new robust deep learning algorithm in which a compressed set of Hadamard matrices is incorporated into a multi-level wavelet convolutional neural network. By this means we have obtained an image of a real object from only 18.75% of the Nyquist sampling rate, using a portable tabletop incoherent x-ray source of ~37 {\mu}m diameter. A high imaging resolution of ~10 {\mu}m is achieved, which represents a concrete step towards the realization of a practical low cost x-ray ghost imaging camera for applications in biomedicine, archeology, material science, and so forth.",project-academic
,2020-09-01,a,,unsupervised machine learning for clustering in political and social research," In the age of data-driven problem-solving, the ability to apply cutting edge computational tools for explaining substantive phenomena in a digestible way to a wide audience is an increasingly valuable skill. Such skills are no less important in political and social research. Yet, application of quantitative methods often assumes an understanding of the data, structure, patterns, and concepts that directly influence the broader research program. It is often the case that researchers may not be entirely aware of the precise structure and nature of their data or what to expect of their data when approaching analysis. Further, in teaching social science research methods, it is often overlooked that the process of exploring data is a key stage in applied research, which precedes predictive modeling and hypothesis testing. These tasks, though, require knowledge of appropriate methods for exploring and understanding data in the service of discerning patterns, which contribute to development of theories and testable expectations. This Element seeks to fill this gap by offering researchers and instructors an introduction clustering, which is a prominent class of unsupervised machine learning for exploring, mining, and understanding data. I detail several widely used clustering techniques, and pair each with R code and real data to facilitate interaction with the concepts. Three unsupervised clustering algorithms are introduced: agglomerative hierarchical clustering, k-means clustering, and Gaussian mixture models. I conclude by offering a high level look at three advanced methods: fuzzy C-means, DBSCAN, and partitioning around medoids clustering. The goal is to bring applied researchers into the world of unsupervised machine learning, both theoretically as well as practically. All code can be interactively run on the cloud computing platform Code Ocean to guide readers through implementation of the algorithms and techniques.",project-academic
,2016-09-28,b,Packt Publishing Ltd,big data analytics," Key FeaturesThis book is based on the latest 2.0 version of Apache Spark and 2.7 version of Hadoop integrated with most commonly used tools. Learn all Spark stack components including latest topics such as DataFrames, DataSets, GraphFrames, Structured Streaming, DataFrame based ML Pipelines and SparkR. Integrations with frameworks such as HDFS, YARN and tools such as Jupyter, Zeppelin, NiFi, Mahout, HBase Spark Connector, GraphFrames, H2O and Hivemall. Book DescriptionBig Data Analytics book aims at providing the fundamentals of Apache Spark and Hadoop. All Spark components Spark Core, Spark SQL, DataFrames, Data sets, Conventional Streaming, Structured Streaming, MLlib, Graphx and Hadoop core components HDFS, MapReduce and Yarn are explored in greater depth with implementation examples on Spark + Hadoop clusters. It is moving away from MapReduce to Spark. So, advantages of Spark over MapReduce are explained at great depth to reap benefits of in-memory speeds. DataFrames API, Data Sources API and new Data set API are explained for building Big Data analytical applications. Real-time data analytics using Spark Streaming with Apache Kafka and HBase is covered to help building streaming applications. New Structured streaming concept is explained with an IOT (Internet of Things) use case. Machine learning techniques are covered using MLLib, ML Pipelines and SparkR and Graph Analytics are covered with GraphX and GraphFrames components of Spark. Readers will also get an opportunity to get started with web based notebooks such as Jupyter, Apache Zeppelin and data flow tool Apache NiFi to analyze and visualize data. What you will learnFind out and implement the tools and techniques of big data analytics using Spark on Hadoop clusters with wide variety of tools used with Spark and HadoopUnderstand all the Hadoop and Spark ecosystem componentsGet to know all the Spark components: Spark Core, Spark SQL, DataFrames, DataSets, Conventional and Structured Streaming, MLLib, ML Pipelines and GraphxSee batch and real-time data analytics using Spark Core, Spark SQL, and Conventional and Structured StreamingGet to grips with data science and machine learning using MLLib, ML Pipelines, H2O, Hivemall, Graphx, SparkR and Hivemall. About the AuthorVenkat Ankam has over 18 years of IT experience and over 5 years in big data technologies, working with customers to design and develop scalable big data applications. Having worked with multiple clients globally, he has tremendous experience in big data analytics using Hadoop and Spark. He is a Cloudera Certified Hadoop Developer and Administrator and also a Databricks Certified Spark Developer. He is the founder and presenter of a few Hadoop and Spark meetup groups globally and loves to share knowledge with the community. Venkat has delivered hundreds of trainings, presentations, and white papers in the big data sphere. While this is his first attempt at writing a book, many more books are in the pipeline.",project-academic
10.1016/S0893-6080(02)00056-4,2002-06-01,a,Elsevier Science Ltd.,control of exploitation exploration meta parameter in reinforcement learning," In reinforcement learning (RL), the duality between exploitation and exploration has long been an important issue. This paper presents a new method that controls the balance between exploitation and exploration. Our learning scheme is based on model-based RL, in which the Bayes inference with forgetting effect estimates the state-transition probability of the environment. The balance parameter, which corresponds to the randomness in action selection, is controlled based on variation of action results and perception of environmental change. When applied to maze tasks, our method successfully obtains good controls by adapting to environmental changes. Recently, Usher et al. [Science 283 (1999) 549] has suggested that noradrenergic neurons in the locus coeruleus may control the exploitation-exploration balance in a real brain and that the balance may correspond to the level of animal's selective attention. According to this scenario, we also discuss a possible implementation in the brain.",project-academic
10.1145/3351095.3372843,2020-01-27,p,ACM,bias in word embeddings," Word embeddings are a widely used set of natural language processing techniques that map words to vectors of real numbers. These vectors are used to improve the quality of generative and predictive models. Recent studies demonstrate that word embeddings contain and amplify biases present in data, such as stereotypes and prejudice. In this study, we provide a complete overview of bias in word embeddings. We develop a new technique for bias detection for gendered languages and use it to compare bias in embeddings trained on Wikipedia and on political social media data. We investigate bias diffusion and prove that existing biases are transferred to further machine learning models. We test two techniques for bias mitigation and show that the generally proposed methodology for debiasing models at the embeddings level is insufficient. Finally, we employ biased word embeddings and illustrate that they can be used for the detection of similar biases in new data. Given that word embeddings are widely used by commercial companies, we discuss the challenges and required actions towards fair algorithmic implementations and applications.",project-academic
,1989-08-01,b,MIT Press,methods in neuronal modeling from synapses to networks," ""Methods in Neuronal Modeling"" is the first technical handbook on computational neuroscience. Written for researchers and theoreticians alike, it outlines methods and techniques used for simulating on digital computers the functional properties of single neurons from synapses, dendrites, single cells; and small invertebrate networks to large scale neural networks in the mammalian nervous system.The use of new experimental tools such as selective staining methods, membrane patch electrodes, voltage and calcium-dependent dyes, and multielectrode recordings, together with the, advent of universally available powerful computing, makes it possible to construct detailed and realistic models of neuronal systems. ""Methods in Neuronal Modeling ""addresses such questions as what can and should be simulated and what techniques should be used; what experimental parameters are crucial for such simulations, and whether these models may be verified experimentally.Chapters cover simulation of passive dendritic trees, compartmental models of single cells including neurons with a number of different ionic channels, calcium current dynamics, simulations of small invertebrate networks, simulations of the mammalian cortex, connectionists' models, and the use of parallel computers in modeling neural networks. Although the chapters were written by several authors, they are uniform in structure and notation. Detailed examples are given to clarify the different approaches. Each chapter concludes with a description of the model discussed and the details of its implementation on the computer.Christof Koch is an Assistant Professor of Computation and Neural Systems at the California Institute of Technology. Idan Segev is a Lecturer in Neurobiology at the Institute of Life Science, Hebrew University of Jerusalem. ""Methods in Neuronal Modeling ""inaugurates the new series in Computational Neuroscience, edited by Terrence J. Sejnowski and Tomaso Poggio. A Bradford Book.",project-academic
10.1101/2019.12.13.874909,2019-12-13,a,Cold Spring Harbor Laboratory,survey of public assay data opportunities and challenges to understanding antimicrobial resistance," ABSTRACT None Computational learning methods allow researchers to make predictions, draw inferences, and automate generation of mathematical models. These models are crucial to solving real world problems, such as antimicrobial resistance, pathogen detection, and protein evolution. Machine learning methods depend upon ground truth data to achieve specificity and sensitivity. Since the data is limited in this case, as we will show during the course of this paper, and as the size of available data increases super-linearly, it is of paramount importance to understand the distribution of ground truth data and the analyses it is suited and where it may have limitations that bias downstream learning methods. In this paper, we focus on training data required to model antimicrobial resistance (AR). We report an analysis of bacterial biochemical assay data associated with whole genome sequencing (WGS) from the National Center for Biotechnology Information (NCBI), and discuss important implications when making use of assay data, utilizing genetic features as training data for machine learning models. Complete discussion of machine learning model implementation is outside the scope of this paper and the subject to a later publication. None The antimicrobial assay data was obtained from NCBI BioSample, which contains descriptive information about the physical biological specimen from which experimental data is obtained and the results of those experiments themselves.[1] Assay data includes minimum inhibitory concentrations (MIC) of antibiotics, links to associated microbial WGS data, and treatment of a particular microorganism with antibiotics. None We observe that there is minimal microbial data available for many antibiotics and for targeted taxonomic groups. The antibiotics with the highest number of assays have less than 1500 measurements each. Corresponding bias in available assays makes machine learning problematic for some important microbes and for building more advanced models that can work across microbial genera. In this study we focus, therefore, on the antibiotic with most assay data (tetracycline) and the corresponding genus with the most available sequence (Acinetobacter with 14000 measurements across 49 antibiotic compounds). Using this data for training and testing, we observed contradictions in the distribution of assay outcomes and report methods to identify and resolve such conflicts. Per antibiotic, we find that there can be up to 30% of (resolvable) conflicting measurements. As more data becomes available, automated training data curation will be an important part of creating useful machine learning models to predict antibiotic resistance. None CCS CONCEPTS None • Applied computing → Computational biology; Computational genomics; Bioinformatics;",project-academic
,2020-07-16,a,,fast neural models for symbolic regression at scale," Deep learning owes much of its success to the astonishing expressiveness of neural networks. However, this comes at the cost of complex, black-boxed models that extrapolate poorly beyond the domain of the training dataset, conflicting with goals of finding analytic expressions to describe science, engineering and real world data. Under the hypothesis that the hierarchical modularity of such laws can be captured by training a neural network, we introduce OccamNet, a neural network model that finds interpretable, compact, and sparse solutions for fitting data, a la Occam's razor. Our model defines a probability distribution over a non-differentiable function space. We introduce a two-step optimization method that samples functions and updates the weights with backpropagation based on cross-entropy matching in an evolutionary strategy: we train by biasing the probability mass toward better fitting solutions. OccamNet is able to fit a variety of symbolic laws including simple analytic functions, recursive programs, implicit functions, simple image classification, and can outperform noticeably state-of-the-art symbolic regression methods on real world regression datasets. Our method requires minimal memory footprint, does not require AI accelerators for efficient training, fits complicated functions in minutes of training on a single CPU, and demonstrates significant performance gains when scaled on a GPU. Our implementation, demonstrations and instructions for reproducing the experiments are available at this https URL.",project-academic
,2016-11-09,,,intelligent cargo delivering and collecting device of unmanned aerial vehicle and implementation method," The invention provides an intelligent cargo delivering and collecting device of an unmanned aerial vehicle and an implementation method, which sufficiently utilize the modern network and the artificial intelligence technology. The intelligent cargo delivering and collecting device comprises a delivering aerial vehicle, a dropping cargo delivering and collecting intelligent device assembly and an intelligent cargo delivering and collecting implementation method; the delivering aerial vehicle is suspended in the mid air to intelligently carry out cargo packet delivering or collecting operation and repeated landing and taking-off working manners of the unmanned aerial vehicle are changed; the device is more convenient, more efficient and safer and realizes intelligent direct delivery; the investment of manpower and material resources of secondary handover, transportation and home delivery of cargo packets can also be greatly and effectively reduced, so that the cost is saved and resources are saved. Especially, aiming at current situations of social development and modern living manners, the delivering aerial vehicle is suspended in the air and the cargo packets can be rapidly and directly transferred and can be easily, conveniently, directly and intelligently collected and delivered; the intelligent cargo delivering and collecting device can give better services to old people, dwelling people and people working and living in high-rise buildings, and can also be adaptive to outdoor consumption requirements of people at anytime and anywhere and meet the fashion psychology of pursuing high-tech content and following time trend of young people.",project-academic
10.12958/1817-3772-2019-3(57)-189-216,2019-01-01,a,State University Luhansk Taras Shevchenko National University,industry 4 0 the directions for attracting investment from the perspective of the interests of domestic producers," The modern world is changing rapidly, the convergence of globalization and localization processes, a combination of competition and cooperation, interaction and rivalry in the relations of economic agents are taking place. A new challenge for all countries of the world is becoming Industry 4.0, based on such important elements of the modern type of production as cyber-physical systems, big data, artificial intelligence, 3D printing, which can increase productivity, speed and flexibility of production, improve the quality of goods and, as a result, ensure industry competitiveness. For Ukraine, readiness for Industry 4.0 is an important aspect of its secure future. It is defined, that Ukraine has almost lost an industrial platform upon which an innovative economy can develop, and industrial regions have inherited its basic structural problems. The traditional production of Donbass also requires an innovative push, the current economic situation of which is defined as a disaster. It is proved that this catastrophe has social, economic, environmental and scientific-technical dimensions. The stages and system of measures for reloading the economy of Donbass and turning it from coal and metallurgical region into a region of a creative economy have been developed. It would be based on knowledge, new development models, new types of social relations, a new cultural paradigm. A vision of the strategic future of Donbass as a region of high technological culture, which should retain the role of the industrial center of Ukraine, is proposed. In doing so, the concept of the industry should be enriched with neo-industrial content based on the introduction of domestic and world achievements in science and technology. Special attention has been devoted to the problem of a high level of Ukraine’s illicit economy, which hampers the realization of structural and technological transformations in industrial regions and in the Donbass, in particular. It was evaluated the level of smartization of the domestic industry. It is established that Ukraine remains considerably behind the processes of smart industrialization that ensued in developed Western and Eastern countries because investment in machinery and equipment is not accompanied by adequate investment in creating a software environment for the smart industry. It is suggested to legislate on the ideology of industrial development at the state and regional level by developing and adopting the Law of Ukraine «On the modernization of industrial regions» which will create a favorable regulatory conditions for business entities and authorities in the implementation of structural and technological modernization of the economy and its industrial sector.",project-academic
,2017-12-04,p,,coded distributed computing for inverse problems," Computationally intensive distributed and parallel computing is often bottlenecked by a small set of slow workers known as stragglers. In this paper, we utilize the emerging idea of ""coded computation"" to design a novel error-correcting-code inspired technique for solving linear inverse problems under specific iterative methods in a parallelized implementation affected by stragglers. Example machine-learning applications include inverse problems such as personalized PageRank and sampling on graphs. We provably show that our coded-computation technique can reduce the mean-squared error under a computational deadline constraint. In fact, the ratio of mean-squared error of replication-based and coded techniques diverges to infinity as the deadline increases. Our experiments for personalized PageRank performed on real systems and real social networks show that this ratio can be as large as 104. Further, unlike coded-computation techniques proposed thus far, our strategy combines outputs of all workers, including the stragglers, to produce more accurate estimates at the computational deadline. This also ensures that the accuracy degrades ""gracefully"" in the event that the number of stragglers is large.",project-academic
,2016-07-01,a,,the rise of social bots," Bots (short for software robots) have been around since the early days of computers. One compelling example of bots is chatbots, algorithms designed to hold a conversation with a human, as envisioned by Alan Turing in the 1950s. The dream of designing a computer algorithm that passes the Turing test has driven artificial intelligence research for decades, as witnessed by initiatives like the Loebner Prize, awarding progress in natural language processing. Many things have changed since the early days of AI, when bots like Joseph Weizenbaum's ELIZA, mimicking a Rogerian psychotherapist, were developed as demonstrations or for delight. 
Today, social media ecosystems populated by hundreds of millions of individuals present real incentives—including economic and political ones—to design algorithms that exhibit human-like behavior. Such ecosystems also raise the bar of the challenge, as they introduce new dimensions to emulate in addition to content, including the social network, temporal activity, diffusion patterns, and sentiment expression. A social bot is a computer algorithm that automatically produces content and interacts with humans on social media, trying to emulate and possibly alter their behavior. Social bots have inhabited social media platforms for the past few years.",project-academic
,2018-06-04,a,,relational inductive biases deep learning and graph networks," Artificial intelligence (AI) has undergone a renaissance recently, making major progress in key domains such as vision, language, control, and decision-making. This has been due, in part, to cheap data and cheap compute resources, which have fit the natural strengths of deep learning. However, many defining characteristics of human intelligence, which developed under much different pressures, remain out of reach for current approaches. In particular, generalizing beyond one's experiences--a hallmark of human intelligence from infancy--remains a formidable challenge for modern AI. 
The following is part position paper, part review, and part unification. We argue that combinatorial generalization must be a top priority for AI to achieve human-like abilities, and that structured representations and computations are key to realizing this objective. Just as biology uses nature and nurture cooperatively, we reject the false choice between ""hand-engineering"" and ""end-to-end"" learning, and instead advocate for an approach which benefits from their complementary strengths. We explore how using relational inductive biases within deep learning architectures can facilitate learning about entities, relations, and rules for composing them. We present a new building block for the AI toolkit with a strong relational inductive bias--the graph network--which generalizes and extends various approaches for neural networks that operate on graphs, and provides a straightforward interface for manipulating structured knowledge and producing structured behaviors. We discuss how graph networks can support relational reasoning and combinatorial generalization, laying the foundation for more sophisticated, interpretable, and flexible patterns of reasoning. As a companion to this paper, we have released an open-source software library for building graph networks, with demonstrations of how to use them in practice.",project-academic
,2002-09-01,b,"John Wiley & Sons, Inc.",adaptive blind signal and image processing learning algorithms and applications," From the Publisher:
With solid theoretical foundations and numerous potential applications, Blind Signal Processing (BSP) is one of the hottest emerging areas in Signal Processing. This volume unifies and extends the theories of adaptive blind signal and image processing and provides practical and efficient algorithms for blind source separation, Independent, Principal, Minor Component Analysis, and Multichannel Blind Deconvolution (MBD) and Equalization. Containing over 1400 references and mathematical expressions Adaptive Blind Signal and Image Processing delivers an unprecedented collection of useful techniques for adaptive blind signal/image separation, extraction, decomposition and filtering of multi-variable signals and data.
Offers a broad coverage of blind signal processing techniques and algorithms both from a theoretical and practical point of viewPresents more than 50 simple algorithms that can be easily modified to suit the reader's specific real world problemsProvides a guide to fundamental mathematics of multi-input, multi-output and multi-sensory systemsIncludes illustrative worked examples, computer simulations, tables, detailed graphs and conceptual models within self contained chapters to assist self studyAccompanying CD-ROM features an electronic, interactive version of the book with fully coloured figures and text. C and MATLAB user-friendly software packages are also provided
MATLAB is a registered trademark of The MathWorks, Inc.
By providing a detailed introduction to BSP, as well as presenting new results and recent developments, this informative and inspiring work will appeal to researchers, postgraduate students, engineers and scientists working in biomedical engineering, communications, electronics, computer science, optimisations, finance, geophysics and neural networks.",project-academic
,2002-08-01,b,Morgan Kaufmann Publishers,how to build a digital library," From the Publisher:
Given modern society's need to control its ever-increasing body of information, digital libraries will be among the most important and influential institutions of this century. With their versatility, accessibility, and economy, these focused collections of everything digital are fast becoming the ""banks"" in which the world's wealth of information is stored.
How to Build a Digital Library is the only book that offers all the knowledge and tools needed to construct and maintain a digital library-no matter how large or small. Two internationally recognized experts provide a fully developed, step-by-step method, as well as the software that makes it all possible. How to Build a Digital Library is the perfectly self-contained resource for individuals, agencies, and institutions wishing to put this powerful tool to work in their burgeoning information treasuries. 
Features
Sketches the history of libraries-both traditional and digital-and their impact on present practices and future directions Offers in-depth coverage of today's practical standards used to represent and store information digitally Uses Greenstone, freely accessible open-source software-available with interfaces in the world's major languages (including Spanish, Chinese, and Arabic) Written for both technical and non-technical audiences Web-enhanced with software documentation, color illustrations, full-text index, source code, and more

Author Biography: Ian H. Witten is a professor of computer science at the University of Waikato in New Zealand. He directs the New Zealand Digital Library research project. His research interests include information retrieval, machine learning, text compression, and programming by demonstration. He received an MA in Mathematics from Cambridge University, England; an MSc in Computer Science from the University of Calgary, Canada; and a PhD in Electrical Engineering from Essex University, England. He is a fellow of the ACM and of the Royal Society of New Zealand. He has published widely on digital libraries, machine learning, text compression, hypertext, speech synthesis and signal processing, and computer typography. He has written several books, the latest being Managing Gigabytes (1999) and Data Mining (2000), both from Morgan Kaufmann. 
David Bainbridge is a senior lecturer in Computer Science at the University of Waikato, New Zealand. He holds a PhD in Optical Music Recognition from the University of Canterbury, New Zealand where he studied as a Commonwealth Scholar. Since moving to Waikato in 1996 he has continued to broadened his interest in digital media, while retaining a particular emphasis on music. An active member of the New Zealand Digital Library project, he manages the group's digital music library, Meldex, and has collaborated with several United Nations Agencies, the BBC and various public libraries. David has also worked as a research engineer for Thorn EMI in the area of photo-realistic imaging and graduated from the University of Edinburgh in 1991 as the class medalist in Computer Science.",project-academic
10.1126/SCIENCE.AAX1566,2019-08-09,a,American Association for the Advancement of Science,a robotic platform for flow synthesis of organic compounds informed by ai planning," INTRODUCTION None The ability to synthesize complex organic molecules is essential to the discovery and manufacture of functional compounds, including small-molecule medicines. Despite advances in laboratory automation, the identification and development of synthetic routes remain a manual process and experimental synthesis platforms must be manually configured to suit the type of chemistry to be performed, requiring time and effort investment from expert chemists. The ideal automated synthesis platform would be capable of planning its own synthetic routes and executing them under conditions that facilitate scale-up to production goals. Individual elements of the chemical development process (design, route development, experimental configuration, and execution) have been streamlined in previous studies, but none has presented a path toward integration of computer-aided synthesis planning (CASP), expert refined chemical recipe generation, and robotically executed chemical synthesis. None RATIONALE None We describe an approach toward automated, scalable synthesis that combines techniques in artificial intelligence (AI) for planning and robotics for execution. Millions of previously published reactions inform the computational design of synthetic routes; expert-refined chemical recipe files (CRFs) are run on a robotic flow chemistry platform for scalable, reproducible synthesis. This development strategy augments a chemist’s ability to approach target-oriented flow synthesis while substantially reducing the necessary information gathering and manual effort. None RESULTS None We developed an open source software suite for CASP trained on millions of reactions from the Reaxys database and the U.S. Patent and Trademark Office. The software was designed to generalize known chemical reactions to new substrates by learning to apply retrosynthetic transformations, to identify suitable reaction conditions, and to evaluate whether reactions are likely to be successful when attempted experimentally. Suggested routes partially populate CRFs, which require additional details from chemist users to define residence times, stoichiometries, and concentrations that are compatible with continuous flow. To execute these syntheses, a robotic arm assembles modular process units (reactors and separators) into a continuous flow path according to the desired process configuration defined in the CRF. The robot also connects reagent lines and computer-controlled pumps to reactor inlets through a fluidic switchboard. When that is completed, the system primes the lines and starts the synthesis. After a specified synthesis time, the system flushes the lines with a cleaning solvent, and the robotic arm disconnects reagent lines and removes process modules to their appropriate storage locations. None This paradigm of flow chemistry development was demonstrated for a suite of 15 medicinally relevant small molecules. In order of increasing complexity, we investigated the synthesis of aspirin and secnidazole run back to back; lidocaine and diazepam run back to back to use a common feedstock; (S)-warfarin and safinamide to demonstrate the planning program’s stereochemical awareness; and two compound libraries: a family of five ACE inhibitors including quinapril and a family of four nonsteroidal anti-inflammatory drugs including celecoxib. These targets required a total of eight particular retrosynthetic routes and nine specific process configurations. None CONCLUSION None The software and platform herein represent a milestone on the path toward fully autonomous chemical synthesis, where routes still require human input and process development. Over time, the results generated by this and similar automated experimental platforms may reduce our reliance on historical reaction data, particularly in combination with smaller-scale flow-screening platforms. Increased availability of reaction data will further enable robotically realized syntheses based on AI recommendations, relieving expert chemists of manual tasks so that they may focus on new ideas.",project-academic
,2013-11-01,b,,information and influence propagation in social networks," Research on social networks has exploded over the last decade. To a large extent, this has been fueled by the spectacular growth of social media and online social networking sites, which continue growing at a very fast pace, as well as by the increasing availability of very large social network datasets for purposes of research. A rich body of this research has been devoted to the analysis of the propagation of information, influence, innovations, infections, practices and customs through networks. Can we build models to explain the way these propagations occur? How can we validate our models against any available real datasets consisting of a social network and propagation traces that occurred in the past? These are just some questions studied by researchers in this area. Information propagation models find applications in viral marketing, outbreak detection, finding key blog posts to read in order to catch important stories, finding leaders or trendsetters, information feed ranking, etc. A number of algorithmic problems arising in these applications have been abstracted and studied extensively by researchers under the garb of influence maximization. This book starts with a detailed description of well-established diffusion models, including the independent cascade model and the linear threshold model, that have been successful at explaining propagation phenomena. We describe their properties as well as numerous extensions to them, introducing aspects such as competition, budget, and time-criticality, among many others. We delve deep into the key problem of influence maximization, which selects key individuals to activate in order to influence a large fraction of a network. Influence maximization in classic diffusion models including both the independent cascade and the linear threshold models is computationally intractable, more precisely #P-hard, and we describe several approximation algorithms and scalable heuristics that have been proposed in the literature. Finally, we also deal with key issues that need to be tackled in order to turn this research into practice, such as learning the strength with which individuals in a network influence each other, as well as the practical aspects of this research including the availability of datasets and software tools for facilitating research. We conclude with a discussion of various research problems that remain open, both from a technical perspective and from the viewpoint of transferring the results of research into industry strength applications. Table of Contents: Acknowledgments / Introduction / Stochastic Diffusion Models / Influence Maximization / Extensions to Diffusion Modeling and Influence Maximization / Learning Propagation Models / Data and Software for Information/Influence: Propagation Research / Conclusion and Challenges / Bibliography / Authors' Biographies / Index",project-academic
10.1088/1538-3873/AB936E,2020-03-20,a,,design and operation of the atlas transient science server," The Asteroid Terrestrial impact Last Alert System (ATLAS) system consists of two 0.5m Schmidt telescopes with cameras covering 29 square degrees at plate scale of 1.86 arcsec per pixel. Working in tandem, the telescopes routinely survey the whole sky visible from Hawaii (above $\delta > -50^{\circ}$) every two nights, exposing four times per night, typically reaching $o < 19$ magnitude per exposure when the moon is illuminated and $c < 19.5$ per exposure in dark skies. Construction is underway of two further units to be sited in Chile and South Africa which will result in an all-sky daily cadence from 2021. Initially designed for detecting potentially hazardous near earth objects, the ATLAS data enable a range of astrophysical time domain science. To extract transients from the data stream requires a computing system to process the data, assimilate detections in time and space and associate them with known astrophysical sources. Here we describe the hardware and software infrastructure to produce a stream of clean, real, astrophysical transients in real time. This involves machine learning and boosted decision tree algorithms to identify extragalactic and Galactic transients. Typically we detect 10-15 supernova candidates per night which we immediately announce publicly. The ATLAS discoveries not only enable rapid follow-up of interesting sources but will provide complete statistical samples within the local volume of 100 Mpc. A simple comparison of the detected supernova rate within 100 Mpc, with no corrections for completeness, is already significantly higher (factor 1.5 to 2) than the current accepted rates.",project-academic
,2004-01-01,b,,head first design patterns," You're not alone. At any given moment, somewhere in the world someone struggles with the same software design problems you have. You know you don't want to reinvent the wheel (or worse, a flat tire), so you look to Design Patterns--the lessons learned by those who've faced the same problems. With Design Patterns, you get to take advantage of the best practices and experience of others, so that you can spend your time on...something else. Something more challenging. Something more complex. Something more fun. You want to learn about the patterns that matter--why to use them, when to use them, how to use them (and when NOT to use them). But you don't just want to see how patterns look in a book, you want to know how they look ""in the wild"". In their native environment. In other words, in real world applications. You also want to learn how patterns are used in the Java API, and how to exploit Java's built-in pattern support in your own code. You want to learn the real OO design principles and why everything your boss told you about inheritance might be wrong (and what to do instead). You want to learn how those principles will help the next time you're up a creek without a design pattern. Most importantly, you want to learn the ""secret language"" of Design Patterns so that you can hold your own with your co-worker (and impress cocktail party guests) when he casually mentions his stunningly clever use of Command, Facade, Proxy, and Factory in between sips of a martini. You'll easily counter with your deep understanding of why Singleton isn't as simple as it sounds, how the Factory is so often misunderstood, or on the real relationship between Decorator, Facade and Adapter. With Head First Design Patterns, you'll avoid the embarrassment of thinking Decorator is something from the ""Trading Spaces"" show. Best of all, in a way that won't put you to sleep! We think your time is too important (and too short) to spend it struggling with academic texts. If you've read a Head First book, you know what to expect--a visually rich format designed for the way your brain works. Using the latest research in neurobiology, cognitive science, and learning theory, Head First Design Patterns will load patterns into your brain in a way that sticks. In a way that lets you put them to work immediately. In a way that makes you better at solving software design problems, and better at speaking the language of patterns with others on your team.",project-academic
10.1088/1538-3873/AB3E82,2019-11-05,a,IOP Publishing,the breakthrough listen search for intelligent life public data formats reduction and archiving," Breakthrough Listen is the most comprehensive and sensitive search for extraterrestrial intelligence (SETI) to date, employing a collection of international observational facilities including both radio and optical telescopes. During the first three years of the Listen program, thousands of targets have been observed with the Green Bank Telescope (GBT), Parkes Telescope and Automated Planet Finder. At GBT and Parkes, observations have been performed ranging from 700 MHz to 26 GHz, with raw data volumes averaging over 1PB / day. A pseudo-real time software spectroscopy suite is used to produce multi-resolution spectrograms amounting to approximately 400 GB hr^-1 GHz^-1 beam^-1. For certain targets, raw baseband voltage data is also preserved. Observations with the Automated Planet Finder produce both 2-dimensional and 1-dimensional high resolution (R~10^5) echelle spectral data. 
Although the primary purpose of Listen data acquisition is for SETI, a range of secondary science has also been performed with these data, including studies of fast radio bursts. Other current and potential research topics include spectral line studies, searches for certain kinds of dark matter, probes of interstellar scattering, pulsar searches, radio transient searches and investigations of stellar activity. Listen data are also being used in the development of algorithms, including machine learning approaches to modulation scheme classification and outlier detection, that have wide applicability not just for astronomical research but for a broad range of science and engineering. 
In this paper, we describe the hardware and software pipeline used for collection, reduction, archival, and public dissemination of Listen data. We describe the data formats and tools, and present Breakthrough Listen Data Release 1.0 (BLDR 1.0), a defined set of publicly-available raw and reduced data totalling 1 PB.",project-academic
,2000-09-04,b,,computational explorations in cognitive neuroscience understanding the mind by simulating the brain," From the Publisher:
The goal of computational cognitive neuroscience is to understand how the brain embodies the mind by using biologically based computational models comprising networks of neuronlike units. This text, based on a course taught by Randall O'Reilly and Yuko Munakata over the past several years, provides an in-depth introduction to the main ideas in the field. The neural units in the simulations use equations based directly on the ion channels that govern the behavior of real neurons, and the neural networks incorporate anatomical and physiological properties of the neocortex. Thus the text provides the student with knowledge of the basic biology of the brain as well as the computational skills needed to simulate large-scale cognitive phenomena. 
The text consists of two parts. The first part covers basic neural computation mechanisms: individual neurons, neural networks, and learning mechanisms. The second part covers large-scale brain area organization and cognitive phenomena: perception and attention, memory, language, and higher-level cognition. The second part is relatively self-contained and can be used separately for mechanistically oriented cognitive neuroscience courses. Integrated throughout the text are more than forty different simulation models, many of them full-scale research-grade models, with friendly interfaces and accompanying exercises. The simulation software (PDP++, available for all major platforms) and simulations can be downloaded free of charge from the Web. Exercise solutions are available, and the text includes full information on the software.",project-academic
10.1109/TMM.2019.2893549,2019-01-16,a,IEEE,hybrid deep learning based anomaly detection scheme for suspicious flow detection in sdn a social multimedia perspective," The continuous development and usage of multi-media-based applications and services have contributed to the exponential growth of social multimedia traffic. In this context, secure transmission of data plays a critical role in realizing all of the key requirements of social multimedia networks such as reliability, scalability, quality of information, and quality of service (QoS). Thus, a trust-based paradigm for multimedia analytics is highly desired to meet the increasing user requirements and deliver more timely and actionable insights. In this regard, software-defined networks (SDNs) play a vital role; however, several factors such as as-runtime security, and energy-aware networking limit its capabilities to facilitate efficient network control and management. Thus, with the view to enhance the reliability of the SDN, a hybrid deep-learning-based anomaly detection scheme for suspicious flow detection in the context of social multimedia is proposed. It consists of the following two modules: 1) an anomaly detection module that leverages improved restricted Boltzmann machine and gradient descent-based support vector machine to detect the abnormal activities, and 2) an end-to-end data delivery module to satisfy strict QoS requirements of the SDN, that is, high bandwidth and low latency. Finally, the proposed scheme has been experimentally evaluated on both real-time and benchmark datasets to prove its effectiveness and efficiency in terms of anomaly detection and data delivery essential for social multimedia. Further, a large-scale analysis over a Carnegie Mellon University (CMU)-based insider threat dataset has been conducted to identify its performance in terms of detecting malicious events such as-Identity theft, profile cloning, confidential data collection, etc.",project-academic
10.1088/2632-2153/ABEDC8,2021-03-11,a,IOP Publishing,olympus a benchmarking framework for noisy optimization and experiment planning," Research challenges encountered across science, engineering, and economics can frequently be formulated as optimization tasks. In chemistry and materials science, recent growth in laboratory digitization and automation has sparked interest in optimization-guided autonomous discovery and closed-loop experimentation. Experiment planning strategies based on off-the-shelf optimization algorithms can be employed in fully autonomous research platforms to achieve desired experimentation goals with the minimum number of trials. However, the experiment planning strategy that is most suitable to a scientific discovery task is a priori unknown while rigorous comparisons of different strategies are highly time and resource demanding. As optimization algorithms are typically benchmarked on low-dimensional synthetic functions, it is unclear how their performance would translate to noisy, higher-dimensional experimental tasks encountered in chemistry and materials science. We introduce Olympus, a software package that provides a consistent and easy-to-use framework for benchmarking optimization algorithms against realistic experiments emulated via probabilistic deep-learning models. Olympus includes a collection of experimentally derived bench- mark sets from chemistry and materials science and a suite of experiment planning strategies that can be easily accessed via a user-friendly python interface. Furthermore, Olympus facilitates the integration, testing, and sharing of custom algorithms and user-defined datasets. In brief, Olympus mitigates the barriers associated with benchmarking optimization algorithms on realistic experimental scenarios, promoting data sharing and the creation of a standard framework for evaluating the performance of experiment planning strategies.",project-academic
10.1088/0067-0049/212/1/5,2013-08-22,a,,the third gravitational lensing accuracy testing great3 challenge handbook," The GRavitational lEnsing Accuracy Testing 3 (GREAT3) challenge is the third in a series of image analysis challenges, with a goal of testing and facilitating the development of methods for analyzing astronomical images that will be used to measure weak gravitational lensing. This measurement requires extremely precise estimation of very small galaxy shape distortions, in the presence of far larger intrinsic galaxy shapes and distortions due to the blurring kernel caused by the atmosphere, telescope optics, and instrumental effects. The GREAT3 challenge is posed to the astronomy, machine learning, and statistics communities, and includes tests of three specific effects that are of immediate relevance to upcoming weak lensing surveys, two of which have never been tested in a community challenge before. These effects include realistically complex galaxy models based on high-resolution imaging from space; spatially varying, physically-motivated blurring kernel; and combination of multiple different exposures. To facilitate entry by people new to the field, and for use as a diagnostic tool, the simulation software for the challenge is publicly available, though the exact parameters used for the challenge are blinded. Sample scripts to analyze the challenge data using existing methods will also be provided. See this http URL and this http URL for more information.",project-academic
,2015-04-06,b,,social sensing building reliable systems on unreliable data," Increasingly, human beings are sensors engaging directly with the mobile Internet. Individuals can now share real-time experiences at an unprecedented scale. Social Sensing: Building Reliable Systems on Unreliable Data looks at recent advances in the emerging field of social sensing, emphasizing the key problem faced by application designers: how to extract reliable information from data collected from largely unknown and possibly unreliable sources. The book explains how a myriad of societal applications can be derived from this massive amount of data collected and shared by average individuals. The title offers theoretical foundations to support emerging data-driven cyber-physical applications and touches on key issues such as privacy. The authors present solutions based on recent research and novel ideas that leverage techniques from cyber-physical systems, sensor networks, machine learning, data mining, and information fusion. Offers a unique interdisciplinary perspective bridging social networks, big data, cyber-physical systems, and reliability Presents novel theoretical foundations for assured social sensing and modeling humans as sensors Includes case studies and application examples based on real data sets Supplemental material includes sample datasets and fact-finding software that implements the main algorithms described in the book",project-academic
,2017-11-10,a,,neural symbolic learning and reasoning a survey and interpretation," The study and understanding of human behaviour is relevant to computer science, artificial intelligence, neural computation, cognitive science, philosophy, psychology, and several other areas. Presupposing cognition as basis of behaviour, among the most prominent tools in the modelling of behaviour are computational-logic systems, connectionist models of cognition, and models of uncertainty. Recent studies in cognitive science, artificial intelligence, and psychology have produced a number of cognitive models of reasoning, learning, and language that are underpinned by computation. In addition, efforts in computer science research have led to the development of cognitive computational systems integrating machine learning and automated reasoning. Such systems have shown promise in a range of applications, including computational biology, fault diagnosis, training and assessment in simulators, and software verification. This joint survey reviews the personal ideas and views of several researchers on neural-symbolic learning and reasoning. The article is organised in three parts: Firstly, we frame the scope and goals of neural-symbolic computation and have a look at the theoretical foundations. We then proceed to describe the realisations of neural-symbolic computation, systems, and applications. Finally we present the challenges facing the area and avenues for further research.",project-academic
10.1016/J.ARTMED.2007.11.006,2008-02-01,a,Elsevier Science Publishers Ltd.,medic medical embedded device for individualized care," Objective: Presented work highlights the development and initial validation of a medical embedded device for individualized care (MEDIC), which is based on a novel software architecture, enabling sensor management and disease prediction capabilities, and commercially available microelectronic components, sensors and conventional personal digital assistant (PDA) (or a cell phone). Methods and materials: In this paper, we present a general architecture for a wearable sensor system that can be customized to an individual patient's needs. This architecture is based on embedded artificial intelligence that permits autonomous operation, sensor management and inference, and may be applied to a general purpose wearable medical diagnostics. Results: A prototype of the system has been developed based on a standard PDA and wireless sensor nodes equipped with commercially available Bluetooth radio components, permitting real-time streaming of high-bandwidth data from various physiological and contextual sensors. We also present the results of abnormal gait diagnosis using the complete system from our evaluation, and illustrate how the wearable system and its operation can be remotely configured and managed by either enterprise systems or medical personnel at centralized locations. Conclusion: By using commercially available hardware components and software architecture presented in this paper, the MEDIC system can be rapidly configured, providing medical researchers with broadband sensor data from remote patients and platform access to best adapt operation for diagnostic operation objectives.",project-academic
10.1145/273133.274326,1998-03-01,p,ACM,a robot laboratory for teaching artificial intelligence," There is a growing consensus among computer science faculty that it is quite difficult to teach the introductory course on Artificial Intelligence well [4, 6]. In part this is because AI lacks a unified methodology, overlaps with many other disciplines, and involves a wide range of skills from very applied to quite formal. In the funded project described here we have addressed these problems by"" Offering a unifying theme that draws together the disparate topics of AI;"" Focusing the course syllabus on the role AI plays in the core computer science curriculum; and"" Motivating the students to learn by using concrete, hands-on laboratory exercises.Our approach is to conceive of topics in AI as robotics tasks. In the laboratory, students build their own robots and program them to accomplish the tasks. By constructing a physical entity in conjunction with the code to control it, students have a unique opportunity to directly tackle many central issues of computer science including the interaction between hardware and software, space complexity in terms of the memory limitations of the robot's controller, and time complexity in terms of the speed of the robot's action decisions. More importantly, the robot theme provides a strong incentive towards learning because students want to see their inventions succeed.This robot-centered approach is an extension of the agent-centered approach adopted by Russell and Norvig in their recent text book [11]. Taking the agent perspective, the problem of AI is seen as describing and building agents that receive perceptions as input and then output appropriate actions based on them. As a result the study of AI centers around how best to implement this mapping from perceptions to actions. The robot perspective takes this approach one step further; rather than studying software agents in a simulated environment, we embed physical agents in the real world. This adds a dimension of complexity as well as excitement to the AI course. The complexity has to do with additional demands of learning robot building techniques but can be overcome by the introduction of kits that are easy to assemble. Additionally, they are lightweight, inexpensive to maintain, programmable through the standard interfaces provided on most computers, and yet, offer sufficient extensibility to create and experiment with a wide range of agent behaviors. At the same time, using robots also leads the students to an important conclusion about scalability: the real world is very different from a simulated world, which has been a long standing criticism of many well-known AI techniques.We proposed a plan to develop identical robot building laboratories at both Bryn Mawr and Swarthmore Colleges that would allow us to integrate the construction of robots into our introductory AI courses. Furthermore, we hoped that these laboratories would encourage our undergraduate students to pursue honors theses and research projects dealing with the building of physical agents.",project-academic
10.3389/FPUBH.2019.00081,2019-04-12,a,Frontiers Media SA,crowdbreaks tracking health trends using public social media data and crowdsourcing," In the past decade, tracking health trends using social media data has shown great promise, due to a powerful combination of massive adoption of social media around the world, and increasingly potent hardware and software that enables us to work with these new big data streams. At the same time, many challenging problems have been identified. First, there is often a mismatch between how rapidly online data can change, and how rapidly algorithms are updated, which means that there is limited reusability for algorithms trained on past data as their performance decreases over time. Second, much of the work is focusing on specific issues during a specific past period in time, even though public health institutions would need flexible tools to assess multiple evolving situations in real time. Third, most tools providing such capabilities are proprietary systems with little algorithmic or data transparency, and thus little buy-in from the global public health and research community. Here, we introduce Crowdbreaks, an open platform which allows tracking of health trends by making use of continuous crowdsourced labelling of public social media content. The system is built in a way which automatizes the typical workflow from data collection, filtering, labelling and training of machine learning classifiers and therefore can greatly accelerate the research process in the public health domain. This work describes the technical aspects of the platform, thereby covering the functionalities at its current state and exploring its future use cases and extensions.",project-academic
10.1186/S13673-019-0205-6,2020-01-01,a,SpringerOpen,developing an online hate classifier for multiple social media platforms," The proliferation of social media enables people to express their opinions widely online. However, at the same time, this has resulted in the emergence of conflict and hate, making online environments uninviting for users. Although researchers have found that hate is a problem across multiple platforms, there is a lack of models for online hate detection using multi-platform data. To address this research gap, we collect a total of 197,566 comments from four platforms: YouTube, Reddit, Wikipedia, and Twitter, with 80% of the comments labeled as non-hateful and the remaining 20% labeled as hateful. We then experiment with several classification algorithms (Logistic Regression, Naive Bayes, Support Vector Machines, XGBoost, and Neural Networks) and feature representations (Bag-of-Words, TF-IDF, Word2Vec, BERT, and their combination). While all the models significantly outperform the keyword-based baseline classifier, XGBoost using all features performs the best (F1 = 0.92). Feature importance analysis indicates that BERT features are the most impactful for the predictions. Findings support the generalizability of the best model, as the platform-specific results from Twitter and Wikipedia are comparable to their respective source papers. We make our code publicly available for application in real software systems as well as for further development by online hate researchers.",project-academic
,2021-02-28,a,,software training in hep," Long term sustainability of the high energy physics (HEP) research software ecosystem is essential for the field. With upgrades and new facilities coming online throughout the 2020s this will only become increasingly relevant throughout this decade. Meeting this sustainability challenge requires a workforce with a combination of HEP domain knowledge and advanced software skills. The required software skills fall into three broad groups. The first is fundamental and generic software engineering (e.g. Unix, version control,C++, continuous integration). The second is knowledge of domain specific HEP packages and practices (e.g., the ROOT data format and analysis framework). The third is more advanced knowledge involving more specialized techniques. These include parallel programming, machine learning and data science tools, and techniques to preserve software projects at all scales. This paper dis-cusses the collective software training program in HEP and its activities led by the HEP Software Foundation (HSF) and the Institute for Research and Innovation in Software in HEP (IRIS-HEP). The program equips participants with an array of software skills that serve as ingredients from which solutions to the computing challenges of HEP can be formed. Beyond serving the community by ensuring that members are able to pursue research goals, this program serves individuals by providing intellectual capital and transferable skills that are becoming increasingly important to careers in the realm of software and computing, whether inside or outside HEP",project-academic
10.1007/S41781-021-00069-9,2021-01-01,a,Springer Science and Business Media LLC,software training in hep," The long-term sustainability of the high-energy physics (HEP) research software ecosystem is essential to the field. With new facilities and upgrades coming online throughout the 2020s, this will only become increasingly important. Meeting the sustainability challenge requires a workforce with a combination of HEP domain knowledge and advanced software skills. The required software skills fall into three broad groups. The first is fundamental and generic software engineering (e.g., Unix, version control, C++, and continuous integration). The second is knowledge of domain-specific HEP packages and practices (e.g., the ROOT data format and analysis framework). The third is more advanced knowledge involving specialized techniques, including parallel programming, machine learning and data science tools, and techniques to maintain software projects at all scales. This paper discusses the collective software training program in HEP led by the HEP Software Foundation (HSF) and the Institute for Research and Innovation in Software in HEP (IRIS-HEP). The program equips participants with an array of software skills that serve as ingredients for the solution of HEP computing challenges. Beyond serving the community by ensuring that members are able to pursue research goals, the program serves individuals by providing intellectual capital and transferable skills important to careers in the realm of software and computing, inside or outside HEP.",project-academic
,2021-07-28,a,,survival stacking casting survival analysis as a classification problem," While there are many well-developed data science methods for classification and regression, there are relatively few methods for working with right-censored data. Here, we present ""survival stacking"": a method for casting survival analysis problems as classification problems, thereby allowing the use of general classification methods and software in a survival setting. Inspired by the Cox partial likelihood, survival stacking collects features and outcomes of survival data in a large data frame with a binary outcome. We show that survival stacking with logistic regression is approximately equivalent to the Cox proportional hazards model. We further recommend methods for evaluating model performance in the survival stacked setting, and we illustrate survival stacking on real and simulated data. By reframing survival problems as classification problems, we make it possible for data scientists to use well-known learning algorithms (including random forests, gradient boosting machines and neural networks) in a survival setting, and lower the barrier for flexible survival modeling.",project-academic
10.2196/10410,2018-07-04,a,JMIR Publications Inc.,health care robotics qualitative exploration of key challenges and future directions," Background: The emergence of robotics is transforming industries around the world. Robot technologies are evolving exponentially, particularly as they converge with other functionalities such as artificial intelligence to learn from their environment, from each other, and from humans. Objective: The goal of the research was to understand the emerging role of robotics in health care and identify existing and likely future challenges to maximize the benefits associated with robotics and related convergent technologies. Methods: We conducted qualitative semistructured one-to-one interviews exploring the role of robotic applications in health care contexts. Using purposive sampling, we identified a diverse range of stakeholders involved in conceiving, procuring, developing, and using robotics in a range of national and international health care settings. Interviews were digitally recorded, transcribed verbatim, and analyzed thematically, supported by NVivo 10 (QSR International) software. Theoretically, this work was informed by the sociotechnical perspective, where social and technical systems are understood as being interdependent. Results: We conducted 21 interviews and these accounts suggested that there are significant opportunities for improving the safety, quality, and efficiency of health care through robotics, but our analysis identified 4 major barriers that need to be effectively negotiated to realize these: (1) no clear pull from professionals and patients, (2) appearance of robots and associated expectations and concerns, (3) disruption of the way work is organized and distributed, and (4) new ethical and legal challenges requiring flexible liability and ethical frameworks. Conclusions: Sociotechnical challenges associated with the effective integration of robotic applications in health care settings are likely to be significant, particularly for patient-facing functions. These need to be identified and addressed for effective innovation and adoption.",project-academic
,2008-08-15,b,,bioconductor case studies," Bioconductor software has become a standard tool for the analysis and comprehension of data from high-throughput genomics experiments. Its application spans a broad field of technologies used in contemporary molecular biology. In this volume, the authors present a collection of cases to apply Bioconductor tools in the analysis of microarray gene expression data. Topics covered include * import and preprocessing of data from various sources * statistical modeling of differential gene expression * biological metadata * application of graphs and graph rendering * machine learning for clustering and classification problems * gene set enrichment analysis Each chapter of this book describes an analysis of real data using hands-on example driven approaches. Short exercises help in the learning process and invitemore advanced considerations of key topics. The book is a dynamic document. All the code shown can be executed on a local computer, and readers are able to reproduce every computation, figure, and table.",project-academic
10.3389/FNBOT.2018.00035,2018-07-06,a,Frontiers,a survey of robotics control based on learning inspired spiking neural networks," Biological intelligence processes information using impulses or spikes, which makes those living creatures able to perceive and act in the real world exceptionally well and outperform state-of-the-art robots in almost every aspect of life. To make up the deficit, emerging hardware technologies and software knowledge in the fields of neuroscience, electronics, and computer science have made it possible to design biologically realistic robots controlled by spiking neural networks (SNNs), inspired by the mechanism of brains. However, a comprehensive review on controlling robots based on SNNs is still missing. In this paper, we survey the developments of the past decade in the field of spiking neural networks for control tasks, with particular focus on the fast emerging robotics-related applications. We first highlight the primary impetuses of SNN-based robotics tasks in terms of speed, energy efficiency, and computation capabilities. We then classify those SNN-based robotic applications according to different learning rules and explicate those learning rules with their corresponding robotic applications. We also briefly present some existing platforms that offer an interaction between SNNs and robotics simulations for exploration and exploitation. Finally, we conclude our survey with a forecast of future challenges and some associated potential research topics in terms of controlling robots based on SNNs.",project-academic
10.1371/JOURNAL.PONE.0205844,2018-10-17,a,Public Library of Science,qflow lite dataset a machine learning approach to the charge states in quantum dot experiments," Background 
Over the past decade, machine learning techniques have revolutionized how research and science are done, from designing new materials and predicting their properties to data mining and analysis to assisting drug discovery to advancing cybersecurity. Recently, we added to this list by showing how a machine learning algorithm (a so-called learner) combined with an optimization routine can assist experimental efforts in the realm of tuning semiconductor quantum dot (QD) devices. Among other applications, semiconductor quantum dots are a candidate system for building quantum computers. In order to employ QDs, one needs to tune the devices into a desirable configuration suitable for quantum computing. While current experiments adjust the control parameters heuristically, such an approach does not scale with the increasing size of the quantum dot arrays required for even near-term quantum computing demonstrations. Establishing a reliable protocol for tuning QD devices that does not rely on the gross-scale heuristics developed by experimentalists is thus of great importance.


Materials and methods 
To implement the machine learning-based approach, we constructed a dataset of simulated QD device characteristics, such as the conductance and the charge sensor response versus the applied electrostatic gate voltages. The gate voltages are the experimental ‘knobs’ for tuning the device into useful regimes. Here, we describe the methodology for generating the dataset, as well as its validation in training convolutional neural networks.


Results and discussion 
From 200 training sets sampled randomly from the full dataset, we show that the learner’s accuracy in recognizing the state of a device is ≈ 96.5% when using either current-based or charge-sensor-based training. The spread in accuracy over our 200 training sets is 0.5% and 1.8% for current- and charge-sensor-based data, respectively. In addition, we also introduce a tool that enables other researchers to use this approach for further research: QFlow lite—a Python-based mini-software suite that uses the dataset to train neural networks to recognize the state of a device and differentiate between states in experimental data. This work gives the definitive reference for the new dataset that will help enable researchers to use it in their experiments or to develop new machine learning approaches and concepts.",project-academic
,2001-10-09,b,,fuzzy reasoning in decision making and optimization," This book starts with the basic concepts of fuzzy arithmetics and progresses through the analysis of sup-t-norm-extended arithmetic operations, possibilistic linear systems and fuzzy reasoning approaches to fuzzy optimization. Four applications of (interdependent) fuzzy optimization and fuzzy reasoning to strategic planning, project management with real options, strategic management and supply chain management are presented and carefully discussed. The book ends with a detailed description of some intelligent software agents, where fuzzy reasoning schemes are used to enhance their functionality. It can be useful for researchers and students working in soft computing, applied mathematics, operations research, management science, information systems, intelligent agents and artificial intelligence.",project-academic
10.1109/INFVIS.2004.43,2004-10-10,p,IEEE Computer Society,interactive visualization of small world graphs," Many real world graphs have small world characteristics, that is, they have a small diameter compared to the number of nodes and exhibit a local cluster structure. Examples are social networks, software structures, bibliographic references and biological neural nets. Their high connectivity makes both finding a pleasing layout and a suitable clustering hard. In this paper we present a method to create scalable, interactive visualizations of small world graphs, allowing the user to inspect local clusters while maintaining a global overview of the entire structure. The visualization method uses a combination of both semantical and geometrical distortions, while the layout is generated by a spring embedder algorithm using recently developed force model. We use a cross referenced database of 500 artists as a running example",project-academic
,2009-02-11,,,device for real time monitoring mine roof rock formation or concrete structure stability," The invention relates to a device for monitoring the stability of roof strata of a mine or concrete engineering in real time. The device mainly comprises a microseism signal conduction rod, a microseism signal sensing element and a microseism signal collection and analysis host computer. The microseism signal collection and analysis host computer is provided with a filter circuit for filtering the affection of ambient noise signal, a rapid data sampling circuit and a professional diagnosis and analysis software for analyzing the stability of the roof strata of the mine or the concrete structure in real time by utilizing artificial intelligence technology. The device is suitable for coal mines or other mines, can be used for the stability monitoring of underground construction which adopting concrete bar and concrete as the materials, such as metro stations or tunnels, and can also be used for the stability monitoring of dams, bridges, tall buildings and mountain landslides. Through a set of technology of digital filtering recognition and collection for the microseism signal, the recognition rate for acoustic signal released during rock breaking can be improved above 80 percent, and the purpose of monitoring the stability of the roof strata of the mine or the concrete structure is well completed.",project-academic
,2021-01-21,a,,noisy intermediate scale quantum nisq algorithms," A universal fault-tolerant quantum computer that can solve efficiently problems such as integer factorization and unstructured database search requires millions of qubits with low error rates and long coherence times. While the experimental advancement towards realizing such devices will potentially take decades of research, noisy intermediate-scale quantum (NISQ) computers already exist. These computers are composed of hundreds of noisy qubits, i.e. qubits that are not error-corrected, and therefore perform imperfect operations in a limited coherence time. In the search for quantum advantage with these devices, algorithms have been proposed for applications in various disciplines spanning physics, machine learning, quantum chemistry and combinatorial optimization. The goal of such algorithms is to leverage the limited available resources to perform classically challenging tasks. In this review, we provide a thorough summary of NISQ computational paradigms and algorithms. We discuss the key structure of these algorithms, their limitations, and advantages. We additionally provide a comprehensive overview of various benchmarking and software tools useful for programming and testing NISQ devices.",project-academic
,2013-01-01,a,,comparative analysis of classification function techniques for heart disease prediction," 2 ABSTRACT: The data mining can be referred as discovery of relationships in large databases automatically and in some cases it is used for predicting relationships based on the results discovered. Data mining plays an important role in various applications such as business organizations, e-commerce, health care industry, scientific and engineering. In the health care industry, the data mining is mainly used for Disease Prediction. Various data mining techniques are available for predicting diseases namely clustering, classification, association rules, regression and etc. This paper analyses the performance of various classification function techniques in data mining for predicting the heart disease from the heart disease data set. The classification function algorithms used and tested in this work are Logistics, Multi Layer Perception and Sequential Minimal Optimization algorithms. Comparative analysis is done by using Waikato Environment for Knowledge Analysis or in short, WEKA. It is open source software which consists of a collection of machine learning algorithms for data mining tasks. The performance factors used for analysing the efficiency of algorithms are clustering accuracy and error rate. The result shows that logistics classification function efficiency is better than multi layer perception and sequential minimal optimization. Data mining can be defined as the extraction of useful knowledge from large data repositories. Compared with other data mining application fields, medical data mining plays a vital role and it has some unique characteristics. Data mining techniques are the result of a long process of research and product development. This evolution began when business data was first stored on computers, continued with improvements in data access, and more recently, generated technologies that allow users to navigate through their data in real time. Data mining takes this evolutionary process beyond retrospective data access and navigation to prospective and proactive information delivery. Data mining is ready for application in the business community because it is supported by three technologies that are now sufficiently mature: Massive data collection, Powerful multiprocessor computers and Data mining algorithms The medical data mining has the high potential in medical domain for extracting the hidden patterns in the datasets (3). These patterns are used for clinical diagnosis and prognosis. The medical data are widely distributed, heterogeneous, voluminous in nature. The data should be integrated and collected to provide a user oriented approach to novel and hidden patterns of the data. A major problem in medical science or bioinformatics analysis is in attaining the correct diagnosis of certain important information. For an ultimate diagnosis, normally, many tests generally involve the classification or clustering of large scale data. The test procedures are said to be necessary in order to reach the ultimate diagnosis. However, on the other hand, too many tests could complicate the main diagnosis process and lead to the difficulty in obtaining the end results, particularly in the case of finding disease many tests are should be performed. This kind of difficulty could be resolved with the aid of machine learning which could be used directly to obtain the end result with the aid of several artificial intelligent algorithms which perform the role as classifiers. Classification is one of the most important techniques in data mining. If a categorization process is to be done, the data is to be classified, and/or codified, and then it can be placed into chunks that are manageable by a human (12). This paper describes classification function algorithms and it also analyzes the performance of these algorithms. The performance factors used for analysis are accuracy and error measures. The accuracy measures are True Positive (TP) rate, F Measure, ROC area and Kappa Statistics. The error measures are Mean Absolute Error (M.A.E), Root Mean Squared Error (R.M.S.E), Relative Absolute Error (R.A.E) and Relative Root Squared Error (R.R.S.E).",project-academic
10.1109/WACV.2017.83,2017-03-24,p,IEEE,x ray scattering image classification using deep learning," Visual inspection of x-ray scattering images is a powerful technique for probing the physical structure of materials at the molecular scale. In this paper, we explore the use of deep learning to develop methods for automatically analyzing x-ray scattering images. In particular, we apply Convolutional Neural Networks and Convolutional Autoencoders for x-ray scattering image classification. To acquire enough training data for deep learning, we use simulation software to generate synthetic x-ray scattering images. Experiments show that deep learning methods outperform previously published methods by 10% on synthetic and real datasets.",project-academic
,2016-11-10,a,,x ray scattering image classification using deep learning," Visual inspection of x-ray scattering images is a powerful technique for probing the physical structure of materials at the molecular scale. In this paper, we explore the use of deep learning to develop methods for automatically analyzing x-ray scattering images. In particular, we apply Convolutional Neural Networks and Convolutional Autoencoders for x-ray scattering image classification. To acquire enough training data for deep learning, we use simulation software to generate synthetic x-ray scattering images. Experiments show that deep learning methods outperform previously published methods by 10\% on synthetic and real datasets.",project-academic
10.1145/2484838.2484884,2013-07-29,p,ACM,making sense of big data with the berkeley data analytics stack," The Berkeley AMPLab was founded on the idea that the challenges of emerging Big Data applications require a new approach to analytics systems. Launching in early 2011, the project set out to rethink the traditional analytics stack, breaking down technical and intellectual barriers that had arisen during decades of evolutionary development. The vision of the lab is to seamlessly integrate the three main resources available for making sense of data at scale: Algorithms (such as machine learning and statistical techniques), Machines (in the form of scalable clusters and elastic cloud computing), and People (both individually as analysts and en masse, as with crowd-sourced human computation). To pursue this goal, we assembled a research team with diverse interests across computer science, forged relationships with domain experts on campus and elsewhere, and obtained the support of leading industry partners and major government sponsors. The lab is realizing its ideas through the development of a freely-available Open Source software stack called BDAS: the Berkeley Data Analytics Stack. In the nearly three years the lab has been in operation, we've released major components of BDAS. Several of these components have gained significant traction in industry and elsewhere: the Mesos cluster resource manager, the Spark in-memory computation framework, and the Shark query processing system. In this talk I'll describe the current state of BDAS with an emphasis on the key components that have been released to date. I'll then discuss ongoing efforts on machine learning scalability and ease of use, including the MLbase system, as our focus moves higher up the stack. Finally I will present our longer-term views of how all the pieces will fit together to form a system that can adaptively bring the right resources to bear on a given data-driven question to meet time, cost and quality requirements throughout the analytics lifecycle.",project-academic
10.1016/J.CHB.2017.02.064,2017-12-01,a,Pergamon,shopping with a robotic companion," In this paper, we present a robotic shopping assistant, designed with a cognitive architecture, grounded in machine learning systems, in order to study how the human-robot interaction (HRI) is changing the shopping behavior in smart technological stores. In the software environment of the NAO robot, connected to the Internet with cloud services, we designed a social-like interaction where the robot carries out actions with the customer. In particular, we focused our design on two main skills the robot has to learn: the first is the ability to acquire social input communicated by relevant clues that humans provide about their emotional state (emotions, emotional speech), or collected in the Social Media (such as, information on the customer's tastes, cultural background, etc.). The second is the skill to express in turn its own emotional state, so that it can affect the customer buying decision, refining in the user the sense of interacting with a human-like companion. By combining social robotics and machine learning systems the potential of robotics to assist people in real life situations will increase, providing a gentle customers' acceptance of advanced technologies. We designed an assistant humanoid robot to help customers in the shop activity.The robot's architecture is a complex and synchronized machine learning system.The customer-robot interaction has been evaluated in the shopping scenario.The robot's learns the tasks in order to behave as a social companion.The robot assistant provides a gentle customers' acceptance of advanced technologies.",project-academic
10.1039/D0LC00080A,2020-06-30,a,Royal Society of Chemistry,intelligent image activated cell sorting 2 0," The advent of intelligent image-activated cell sorting (iIACS) has enabled high-throughput intelligent image-based sorting of single live cells from heterogeneous populations. iIACS is an on-chip microfluidic technology that builds on a seamless integration of a high-throughput fluorescence microscope, cell focuser, cell sorter, and deep neural network on a hybrid software-hardware data management architecture, thereby providing the combined merits of optical microscopy, fluorescence-activated cell sorting (FACS), and deep learning. Here we report an iIACS machine that far surpasses the state-of-the-art iIACS machine in system performance in order to expand the range of applications and discoveries enabled by the technology. Specifically, it provides a high throughput of ∼2000 events per second and a high sensitivity of ∼50 molecules of equivalent soluble fluorophores (MESFs), both of which are 20 times superior to those achieved in previous reports. This is made possible by employing (i) an image-sensor-based optomechanical flow imaging method known as virtual-freezing fluorescence imaging and (ii) a real-time intelligent image processor on an 8-PC server equipped with 8 multi-core CPUs and GPUs for intelligent decision-making, in order to significantly boost the imaging performance and computational power of the iIACS machine. We characterize the iIACS machine with fluorescent particles and various cell types and show that the performance of the iIACS machine is close to its achievable design specification. Equipped with the improved capabilities, this new generation of the iIACS technology holds promise for diverse applications in immunology, microbiology, stem cell biology, cancer biology, pathology, and synthetic biology.",project-academic
10.1007/S11663-016-0735-5,2016-09-13,a,Springer US,digitalizing the circular economy," Metallurgy is a key enabler of a circular economy (CE), its digitalization is the metallurgical Internet of Things (m-IoT). In short: Metallurgy is at the heart of a CE, as metals all have strong intrinsic recycling potentials. Process metallurgy, as a key enabler for a CE, will help much to deliver its goals. The first-principles models of process engineering help quantify the resource efficiency (RE) of the CE system, connecting all stakeholders via digitalization. This provides well-argued and first-principles environmental information to empower a tax paying consumer society, policy, legislators, and environmentalists. It provides the details of capital expenditure and operational expenditure estimates. Through this path, the opportunities and limits of a CE, recycling, and its technology can be estimated. The true boundaries of sustainability can be determined in addition to the techno-economic evaluation of RE. The integration of metallurgical reactor technology and systems digitally, not only on one site but linking different sites globally via hardware, is the basis for describing CE systems as dynamic feedback control loops, i.e., the m-IoT. It is the linkage of the global carrier metallurgical processing system infrastructure that maximizes the recovery of all minor and technology elements in its associated refining metallurgical infrastructure. This will be illustrated through the following: (1) System optimization models for multimetal metallurgical processing. These map large-scale m-IoT systems linked to computer-aided design tools of the original equipment manufacturers and then establish a recycling index through the quantification of RE. (2) Reactor optimization and industrial system solutions to realize the “CE (within a) Corporation—CEC,” realizing the CE of society. (3) Real-time measurement of ore and scrap properties in intelligent plant structures, linked to the modeling, simulation, and optimization of industrial extractive process metallurgical reactors and plants for both primary and secondary materials processing. (4) Big-data analysis and process control of industrial metallurgical systems, processes, and reactors by the application of, among others, artificial intelligence techniques and computer-aided engineering. (5) Minerals processing and process metallurgical theory, technology, simulation, and analytical tools, which are all key enablers of the CE. (6) Visualizing the results of all the tools used for estimating the RE of the CE system in a form that the consumer and general public can understand. (7) The smart integration of tools and methods that quantify RE and deliver sustainable solutions, named in this article as circular economy engineering. In view of space limitations, this message will be colored in by various publications also with students and colleagues, referring to (often commercial) software that acts as a conduit to capture and formalize the research of the large body of work in the literature by distinguished metallurgical engineers and researchers and realized in innovative industrial solutions. The author stands humbly on the shoulders of these developments and their distinguished developers. This award lecture article implicitly also refers to work done while working for Ausmelt (Australia), Outotec (Finland and Australia), Mintek (South Africa), and Anglo American Corporation (South Africa), honoring the many colleagues the author has worked with over the years.",project-academic
10.1016/J.IJROBP.2018.08.032,2018-11-15,a,Elsevier Science,a deep look into the future of quantitative imaging in oncology a statement of working principles and proposal for change," The adoption of enterprise digital imaging, along with the development of quantitative imaging methods and the re-emergence of statistical learning, has opened the opportunity for more personalized cancer treatments through transformative data science research. In the last 5 years, accumulating evidence has indicated that noninvasive advanced imaging analytics (i.e., radiomics) can reveal key components of tumor phenotype for multiple lesions at multiple time points over the course of treatment. Many groups using homegrown software have extracted engineered and deep quantitative features on 3-dimensional medical images for better spatial and longitudinal understanding of tumor biology and for the prediction of diverse outcomes. These developments could augment patient stratification and prognostication, buttressing emerging targeted therapeutic approaches. Unfortunately, the rapid growth in popularity of this immature scientific discipline has resulted in many early publications that miss key information or use underpowered patient data sets, without production of generalizable results. Quantitative imaging research is complex, and key principles should be followed to realize its full potential. The fields of quantitative imaging and radiomics in particular require a renewed focus on optimal study design and reporting practices, standardization, interpretability, data sharing, and clinical trials. Standardization of image acquisition, feature calculation, and statistical analysis (i.e., machine learning) are required for the field to move forward. A new data-sharing paradigm enacted among open and diverse participants (medical institutions, vendors and associations) should be embraced for faster development and comprehensive clinical validation of imaging biomarkers. In this review and critique of the field, we propose working principles and fundamental changes to the current scientific approach, with the goal of high-impact research and development of actionable prediction models that will yield more meaningful applications of precision cancer medicine.",project-academic
,2014-09-24,b,,multiword expressions acquisition a generic and open framework," This book is an excellent introduction to multiword expressions. It provides a unique, comprehensive and up-to-date overview of this exciting topic in computational linguistics. The first part describes the diversity and richness of multiword expressions, including many examples in several languages. These constructions are not only complex and arbitrary, but also much more frequent than one would guess, making them a real nightmare for natural language processing applications. The second part introduces a new generic framework for automatic acquisition of multiword expressions from texts. Furthermore, it describes the accompanying free software tool, the mwetoolkit, which comes in handy when looking for expressions in texts (regardless of the language). Evaluation is greatly emphasized, underlining the fact that results depend on parameters like corpus size, language, MWE type, etc. The last part contains solid experimental results and evaluates the mwetoolkit, demonstrating its usefulness for computer-assisted lexicography and machine translation. This is the first book to cover the whole pipeline of multiword expression acquisition in a single volume. It is addresses the needs of students and researchers in computational and theoretical linguistics, cognitive sciences, artificial intelligence and computer science. Its good balance between computational and linguistic views make it the perfect starting point for anyone interested in multiword expressions, language and text processing in general.",project-academic
10.1093/BIOINFORMATICS/BTI345,2005-05-15,a,Oxford University Press,collateral missing value imputation a new robust missing value estimation algorithm for microarray data," Motivation: Microarray data are used in a range of application areas in biology, although often it contains considerable numbers of missing values. These missing values can significantly affect subsequent statistical analysis and machine learning algorithms so there is a strong motivation to estimate these values as accurately as possible before using these algorithms. While many imputation algorithms have been proposed, more robust techniques need to be developed so that further analysis of biological data can be accurately undertaken. In this paper, an innovative missing value imputation algorithm called collateral missing value estimation (CMVE) is presented which uses multiple covariance-based imputation matrices for the final prediction of missing values. The matrices are computed and optimized using least square regression and linear programming methods.

Results: The new CMVE algorithm has been compared with existing estimation techniques including Bayesian principal component analysis imputation (BPCA), least square impute (LSImpute) and K-nearest neighbour (KNN). All these methods were rigorously tested to estimate missing values in three separate non-time series (ovarian cancer based) and one time series (yeast sporulation) dataset. Each method was quantitatively analyzed using the normalized root mean square (NRMS) error measure, covering a wide range of randomly introduced missing value probabilities from 0.01 to 0.2. Experiments were also undertaken on the yeast dataset, which comprised 1.7% actual missing values, to test the hypothesis that CMVE performed better not only for randomly occurring but also for a real distribution of missing values. The results confirmed that CMVE consistently demonstrated superior and robust estimation capability of missing values compared with other methods for both series types of data, for the same order of computational complexity. A concise theoretical framework has also been formulated to validate the improved performance of the CMVE algorithm.

Availability: The CMVE software is available upon request from the authors.

Contact: Shoaib.Sehgal@infotech.monash.edu.au",project-academic
10.7554/ELIFE.55502,2020-07-27,a,eLife Sciences Publications Limited,revealing architectural order with quantitative label free imaging and deep learning," Microscopy is central to biological research and has enabled scientist to study the structure and dynamics of cells and their components within. Often, fluorescent dyes or trackers are used that can be detected under the microscope. However, this procedure can sometimes interfere with the biological processes being studied. Now, Guo, Yeh, Folkesson et al. have developed a new approach to examine structures within tissues and cells without the need for a fluorescent label. The technique, called QLIPP, uses the phase and polarization of the light passing through the sample to get information about its makeup. A computational model was used to decode the characteristics of the light and to provide information about the density and orientation of molecules in live cells and brain tissue samples of mice and human. This way, Guo et al. were able to reveal details that conventional microscopy would have missed. Then, a type of machine learning, known as ‘deep learning’, was used to translate the density and orientation images into fluorescence images, which enabled the researchers to predict specific structures in human brain tissue sections. QLIPP can be added as a module to a microscope and its software is available open source. Guo et al. hope that this approach can be used across many fields of biology, for example, to map the connectivity of nerve cells in the human brain or to identify how cells respond to infection. However, further work in automating other aspects, such as sample preparation and analysis, will be needed to realize the full benefits.",project-academic
10.1111/1755-0998.13407,2021-05-24,a,"John Wiley & Sons, Ltd",assessment of current taxonomic assignment strategies for metabarcoding eukaryotes," The effective use of metabarcoding in biodiversity science has brought important analytical challenges due to the need to generate accurate taxonomic assignments. The assignment of sequences to genus or species level is critical for biodiversity surveys and biomonitoring, but it is particularly challenging as researchers must select the approach that best recovers information on species composition. This study evaluates the performance and accuracy of seven methods in recovering the species composition of mock communities by using COI barcode fragments. The mock communities varied in species number and specimen abundance, while upstream molecular and bioinformatic variables were held constant, and using a set of COI fragments. We evaluated the impact of parameter optimization on the quality of the predictions. Our results indicate that BLAST top hit competes well with more complex approaches if optimized for the mock community under study. For example, the two machine learning methods that were benchmarked proved more sensitive to reference database heterogeneity and completeness than methods based on sequence similarity. The accuracy of assignments was impacted by both species and specimen counts (query compositional heterogeneity) which ultimately influence the selection of appropriate software. We urge researchers to: (i) use realistic mock communities to allow optimization of parameters, regardless of the taxonomic assignment method employed; (ii) carefully choose and curate the reference databases including completeness; and (iii) use QIIME, BLAST or LCA methods, in conjunction with parameter tuning to better assign taxonomy to diverse communities, especially when information on species diversity is lacking for the area under study.",project-academic
10.1101/2020.07.21.214270,2020-07-22,a,Cold Spring Harbor Laboratory,assessment of current taxonomic assignment strategies for metabarcoding eukaryotes," The effective use of metabarcoding in biodiversity science has brought important analytical challenges due to the need to generate accurate taxonomic assignments. The assignment of sequences to a generic or species level is critical for biodiversity surveys and biomonitoring, but it is particularly challenging. Researchers must select the approach that best recovers information on species composition. This study evaluates the performance and accuracy of seven methods in recovering the species composition of mock communities which vary in species number and specimen abundance, while holding upstream molecular and bioinformatic variables constant. It also evaluates the impact of parameter optimization on the quality of the predictions. Despite the general belief that BLAST top hit underperforms newer methods, our results indicate that it competes well with more complex approaches if optimized for the mock community under study. For example, the two machine learning methods that were benchmarked proved more sensitive to the reference database heterogeneity and completeness than methods based on sequence similarity. The accuracy of assignments was impacted by both species and specimen counts which will influence the selection of appropriate software. We urge the usage of realistic mock communities to allow optimization of parameters, regardless of the taxonomic assignment method used.",project-academic
,2009-01-01,a,Massachusetts Institute of Technology,natively probabilistic computation," I introduce a new set of natively probabilistic computing abstractions, including probabilistic generalizations of Boolean circuits, backtracking search and pure Lisp. I show how these tools let one compactly specify probabilistic generative models, generalize and parallelize widely used sampling algorithms like rejection sampling and Markov chain Monte Carlo, and solve difficult Bayesian inference problems. 
I first introduce Church, a probabilistic programming language for describing probabilistic generative processes that induce distributions, which generalizes Lisp, a language for describing deterministic procedures that induce functions. I highlight the ways randomness meshes with the reflectiveness of Lisp to support the representation of structured, uncertain knowledge, including nonparametric Bayesian models from the current literature, programs for decision making under uncertainty, and programs that learn very simple programs from data. I then introduce systematic stochastic search, a recursive algorithm for exact and approximate sampling that generalizes a popular form of backtracking search to the broader setting of stochastic simulation and recovers widely used particle filters as a special case. I use it to solve probabilistic reasoning problems from statistical physics, causal reasoning and stereo vision. Finally, I introduce stochastic digital circuits that model the probability algebra just as traditional Boolean circuits model the Boolean algebra. I show how these circuits can be used to build massively parallel, fault-tolerant machines for sampling and allow one to efficiently run Markov chain Monte Carlo methods on models with hundreds of thousands of variables in real time. 
I emphasize the ways in which these ideas fit together into a coherent software and hardware stack for natively probabilistic computing, organized around distributions and samplers rather than deterministic functions. I argue that by building uncertainty and randomness into the foundations of our programming languages and computing machines, we may arrive at ones that are more powerful, flexible and efficient than deterministic designs, and are in better alignment with the needs of computational science, statistics and artificial intelligence. (Copies available exclusively from MIT Libraries, Rm. 14-0551, Cambridge, MA 02139-4307. Ph. 617-253-5668; Fax 617-253-1690.)",project-academic
10.1007/978-0-387-35602-0_3,2002-08-25,p,"Springer, Boston, MA",agent based computing," Agent-based computing represents an exciting new synthesis for both Artificial Intelligence and, more generally, Computer Science. It has the potential to improve the theory and the practice of modelling, designing and implementing complex computer systems. Yet, to date, there has been little systematic analysis of what makes the agent-based approach such an appealing and powerful computational model. To rectify this situation, this paper aims to tackle exactly this issue. The standpoint of this analysis is the role of agent-based software in solving complex, realworld problems. In particular, it will be argued that the development of robust and scalable software systems requires autonomous agents that can complete their objectives while situated in a dynamic and uncertain environment, that can engage in rich, high-level interactions, and that can operate within flexible organisational structures.",project-academic
10.1109/IPDPSW.2019.00142,2019-05-20,p,IEEE,workflow driven distributed machine learning in chase ci a cognitive hardware and software ecosystem community infrastructure," The advances in data, computing and networking over the last two decades led to a shift in many application domains that includes machine learning on big data as a part of the scientific process, requiring new capabilities for integrated and distributed hardware and software infrastructure. This paper contributes a workflow-driven approach for dynamic data-driven application development on top of a new kind of networked Cyberinfrastructure called CHASE-CI. In particular, we present: 1) The architecture for CHASE-CI, a network of distributed fast GPU appliances for machine learning and storage managed through Kubernetes on the high-speed (10-100Gbps) Pacific Research Platform (PRP); 2) A machine learning software containerization approach and libraries required for turning such a network into a distributed computer for big data analysis; 3) An atmospheric science case study that can only be made scalable with an infrastructure like CHASE-CI; 4) Capabilities for virtual cluster management for data communication and analysis in a dynamically scalable fashion, and visualization across the network in specialized visualization facilities in near real-time; and, 5) A step-by-step workflow and performance measurement approach that enables taking advantage of the dynamic architecture of the CHASE-CI network and container management infrastructure.",project-academic
,2019-02-26,a,,workflow driven distributed machine learning in chase ci a cognitive hardware and software ecosystem community infrastructure," The advances in data, computing and networking over the last two decades led to a shift in many application domains that includes machine learning on big data as a part of the scientific process, requiring new capabilities for integrated and distributed hardware and software infrastructure. This paper contributes a workflow-driven approach for dynamic data-driven application development on top of a new kind of networked Cyberinfrastructure called CHASE-CI. In particular, we present: 1) The architecture for CHASE-CI, a network of distributed fast GPU appliances for machine learning and storage managed through Kubernetes on the high-speed (10-100Gbps) Pacific Research Platform (PRP); 2) A machine learning software containerization approach and libraries required for turning such a network into a distributed computer for big data analysis; 3) An atmospheric science case study that can only be made scalable with an infrastructure like CHASE-CI; 4) Capabilities for virtual cluster management for data communication and analysis in a dynamically scalable fashion, and visualization across the network in specialized visualization facilities in near real-time; and, 5) A step-by-step workflow and performance measurement approach that enables taking advantage of the dynamic architecture of the CHASE-CI network and container management infrastructure.",project-academic
10.3390/S20236936,2020-12-04,a,Multidisciplinary Digital Publishing Institute,mining and tailings dam detection in satellite imagery using deep learning," This work explores the combination of free cloud computing, free open-source software, and deep learning methods to analyze a real, large-scale problem: the automatic country-wide identification and classification of surface mines and mining tailings dams in Brazil. Locations of officially registered mines and dams were obtained from the Brazilian government open data resource. Multispectral Sentinel-2 satellite imagery, obtained and processed at the Google Earth Engine platform, was used to train and test deep neural networks using the TensorFlow 2 application programming interface (API) and Google Colaboratory (Colab) platform. Fully convolutional neural networks were used in an innovative way to search for unregistered ore mines and tailing dams in large areas of the Brazilian territory. The efficacy of the approach is demonstrated by the discovery of 263 mines that do not have an official mining concession. This exploratory work highlights the potential of a set of new technologies, freely available, for the construction of low cost data science tools that have high social impact. At the same time, it discusses and seeks to suggest practical solutions for the complex and serious problem of illegal mining and the proliferation of tailings dams, which pose high risks to the population and the environment, especially in developing countries.",project-academic
,2020-07-02,a,,mining and tailings dam detection in satellite imagery using deep learning," This work explores the combination of free cloud computing, free open-source software, and deep learning methods to analyse a real, large-scale problem: the automatic country-wide identification and classification of surface mines and mining tailings dams in Brazil. Locations of officially registered mines and dams were obtained from the Brazilian government open data resource. Multispectral Sentinel-2 satellite imagery, obtained and processed at the Google Earth Engine platform, was used to train and test deep neural networks using the TensorFlow 2 API and Google Colab platform. Fully Convolutional Neural Networks were used in an innovative way, to search for unregistered ore mines and tailing dams in large areas of the Brazilian territory. The efficacy of the approach is demonstrated by the discovery of 263 mines that do not have an official mining concession. This exploratory work highlights the potential of a set of new technologies, freely available, for the construction of low cost data science tools that have high social impact. At the same time, it discusses and seeks to suggest practical solutions for the complex and serious problem of illegal mining and the proliferation of tailings dams, which pose high risks to the population and the environment, especially in developing countries. Code is made publicly available at: this https URL.",project-academic
10.2196/RESPROT.7757,2017-08-29,a,JMIR Res Protoc,automating construction of machine learning models with clinical big data proposal rationale and methods," Background: To improve health outcomes and cut health care costs, we often need to conduct prediction/classification using large clinical datasets (aka, clinical big data), for example, to identify high-risk patients for preventive interventions. Machine learning has been proposed as a key technology for doing this. Machine learning has won most data science competitions and could support many clinical activities, yet only 15% of hospitals use it for even limited purposes. Despite familiarity with data, health care researchers often lack machine learning expertise to directly use clinical big data, creating a hurdle in realizing value from their data. Health care researchers can work with data scientists with deep machine learning knowledge, but it takes time and effort for both parties to communicate effectively. Facing a shortage in the United States of data scientists and hiring competition from companies with deep pockets, health care systems have difficulty recruiting data scientists. Building and generalizing a machine learning model often requires hundreds to thousands of manual iterations by data scientists to select the following: (1) hyper-parameter values and complex algorithms that greatly affect model accuracy and (2) operators and periods for temporally aggregating clinical attributes (eg, whether a patient’s weight kept rising in the past year). This process becomes infeasible with limited budgets. Objective: This study’s goal is to enable health care researchers to directly use clinical big data, make machine learning feasible with limited budgets and data scientist resources, and realize value from data. Methods: This study will allow us to achieve the following: (1) finish developing the new software, Automated Machine Learning (Auto-ML), to automate model selection for machine learning with clinical big data and validate Auto-ML on seven benchmark modeling problems of clinical importance; (2) apply Auto-ML and novel methodology to two new modeling problems crucial for care management allocation and pilot one model with care managers; and (3) perform simulations to estimate the impact of adopting Auto-ML on US patient outcomes. Results: We are currently writing Auto-ML’s design document. We intend to finish our study by around the year 2022. Conclusions: Auto-ML will generalize to various clinical prediction/classification problems. With minimal help from data scientists, health care researchers can use Auto-ML to quickly build high-quality models. This will boost wider use of machine learning in health care and improve patient outcomes. None [JMIR Res Protoc 2017;6(8):e175]",project-academic
10.1007/S12152-018-9371-X,2021-04-01,a,Springer Netherlands,big brain data on the responsible use of brain data from clinical and consumer directed neurotechnological devices," The focus of this paper are the ethical, legal and social challenges for ensuring the responsible use of “big brain data”—the recording, collection and analysis of individuals’ brain data on a large scale with clinical and consumer-directed neurotechnological devices. First, I highlight the benefits of big data and machine learning analytics in neuroscience for basic and translational research. Then, I describe some of the technological, social and psychological barriers for securing brain data from unwarranted access. In this context, I then examine ways in which safeguards at the hardware and software level, as well as increasing “data literacy” in society, may enhance the security of neurotechnological devices and protect the privacy of personal brain data. Regarding ethical and legal ramifications of big brain data, I first discuss effects on the autonomy, the sense of agency and authenticity, as well as the self that may result from the interaction between users and intelligent, particularly closed-loop, neurotechnological devices. I then discuss the impact of the “datafication” in basic and clinical neuroscience research on the just distribution of resources and access to these transformative technologies. In the legal realm, I examine possible legal consequences that arises from the increasing abilities to decode brain states and their corresponding subjective phenomenological experiences on the hitherto inaccessible privacy of these information. Finally, I discuss the implications of big brain data for national and international regulatory policies and models of good data governance.",project-academic
10.1063/1.5084708,2019-01-16,p,AIP Publishing LLC AIP Publishing,ultrafast processing of pixel detector data with machine learning frameworks," Modern photon science performed at high repetition rate free-electron laser (FEL) facilities and beyond relies on 2D pixel detectors operating at increasing frequencies (towards 100 kHz at LCLS-II) and producing rapidly increasing amounts of data (towards TB/s). This data must be rapidly stored for offline analysis and summarized in real time for online feedback to the scientists. While at LCLS all raw data has been stored, at LCLS-II this would lead to a prohibitive cost; instead, enabling real time processing of pixel detector data (dark, gain, common mode, background, charge summing, subpixel position, photon counting, data summarization) allows reducing the size and cost of online processing, offline processing and storage by orders of magnitude while preserving full photon information. This could be achieved by taking advantage of the compressibility of sparse data typical for LCLS-II applications. Faced with a similar big data challenge a decade ago, computer vision stimulated revolutionary advances in machine learning hardware and software. We investigated if these developments are useful in the field of data processing for high speed pixel detectors and found that typical deep learning models and autoencoder architectures failed to yield useful noise reduction while preserving full photon information, presumably because of the very different statistics and feature sets in computer vision and radiation imaging. However, the raw performance of modern frameworks like Tensorflow inspired us to redesign in Tensorflow mathematically equivalent versions of the state-of-the-art, “classical” algorithms used at LCLS. The novel Tensorflow models resulted in elegant, compact and hardware agnostic code, gaining 1 to 2 orders of magnitude faster processing on an inexpensive consumer GPU, reducing by 3 orders of magnitude the projected cost of online analysis and compression without photon loss at LCLS-II. The novel Tensorflow models also enabled ongoing development of a pipelined hardware system expected to yield an additional 3 to 4 orders of magnitude speedup, necessary for meeting the data acquisition and storage requirements at LCLS-II, potentially enabling acquiring every single FEL pulse at full speed. Computer vision a decade ago was dominated by hand-crafted filters; their structure inspired the deep learning revolution resulting in modern deep convolutional networks; similarly, our novel Tensorflow filters provide inspiration for designing future deep learning models for ultrafast and efficient processing and classification of pixel detector images at FEL facilities.",project-academic
,2017-04-05,,,system and method for machine learning based user application," The invention synthesizes a social network, electronic commerce (including performance based advertisement and electronic payment), a mobile internet device and a machine learning algorithm(s), utilizing a classical computer or a quantum computer enhanced machine learning algorithm(s), utilizing a quantum computer. The synthesized social commerce further dynamically integrates stored information, real time information and real time information/data/image(s) from an object/array of objects (Internet of Things (IoT)). The machine learning algorithm(s), utilizing a classical computer can include a software agent, a fuzzy logic algorithm, a predictive algorithm, an intelligence rendering algorithm and a self-learning (including relearning) algorithm.",project-academic
10.1101/2020.05.04.20090258,2020-05-08,a,Cold Spring Harbor Laboratory Press,safe blues a method for estimation and control in the fight against covid 19," Abstract None How do fine modifications to social distancing measures really affect COVID-19 spread? A major problem for health authorities is that we do not know. None In an imaginary world, we might develop a harmless biological virus that spreads just like COVID-19, but is traceable via a cheap and reliable diagnosis. By introducing such an imaginary virus into the population and observing how it spreads, we would have a way of learning about COVID-19 because the benign virus would respond to population behaviour and social distancing measures in a similar manner. Such a benign biological virus does not exist. Instead, we propose a safe and privacy-preserving digital alternative. None Our solution is to mimic the benign virus by passing virtual tokens between electronic devices when they move into close proximity. As Bluetooth transmission is the most likely method used for such inter-device communication, and as our suggested “virtual viruses” do not harm individuals’ software or intrude on privacy, we call these Safe Blues. None In contrast to many app-based methods that inform individuals or governments about actual COVID-19 patients or hazards, Safe Blues does not provide information about individuals’ locations or contacts. Hence the privacy concerns associated with Safe Blues are much lower than other methods. However, from the point of view of data collection, Safe Blues has two major advantages: None Data about the spread of Safe Blues is uploaded to a central server in real time, which can give authorities a more up-to-date picture in comparison to actual COVID-19 data, which is only available retrospectively. None Sampling of Safe Blues data is not biased by being applied only to people who have shown symptoms or who have come into contact with known positive cases. None These features mean that there would be real statistical value in introducing Safe Blues. In the medium term and end game of COVID-19, information from Safe Blues could aid health authorities to make informed decisions with respect to social distancing and other measures. None In this paper we outline the general principles of Safe Blues and we illustrate how Safe Blues data together with neural networks may be used to infer characteristics of the progress of the COVID-19 pandemic in real time. Further information is on the Safe Blues website: https://safeblues.org/.",project-academic
10.1109/ACCESS.2015.2442680,2015-06-22,a,IEEE,challenges and opportunities in game artificial intelligence education using angry birds," Games have been an important tool for motivating undergraduate students majoring in computer science and engineering. However, it is difficult to build an entire game for education from scratch, because the task requires high-level programming skills and expertise to understand the graphics and physics. Recently, there have been many different game artificial intelligence (AI) competitions, ranging from board games to the state-of-the-art video games (car racing, mobile games, first-person shooting games, real-time strategy games, and so on). The competitions have been designed such that participants develop their own AI module on top of public/commercial games. Because the materials are open to the public, it is quite useful to adopt them for an undergraduate course project. In this paper, we report our experiences using the Angry Birds AI Competition for such a project-based course. In the course, teams of students consider computer vision, strategic decision-making, resource management, and bug-free coding for their outcome. To promote understanding of game contents generation and extensive testing on the generalization abilities of the student’s AI program, we developed software to help them create user-created levels. Students actively participated in the project and the final outcome was comparable with that of successful entries in the 2013 International Angry Birds AI Competition. Furthermore, it leads to the development of a new parallelized Angry Birds AI Competition platform with undergraduate students aiming to use advanced optimization algorithms for their controllers.",project-academic
10.1016/J.JMSY.2020.06.012,2021-01-01,a,Elsevier,a digital twin to train deep reinforcement learning agent for smart manufacturing plants environment interfaces and intelligence," Abstract None None Filling the gaps between virtual and physical systems will open new doors in Smart Manufacturing. This work proposes a data-driven approach to utilize digital transformation methods to automate smart manufacturing systems. This is fundamentally enabled by using a digital twin to represent manufacturing cells, simulate system behaviors, predict process faults, and adaptively control manipulated variables. First, the manufacturing cell is accommodated to environments such as computer-aided applications, industrial Product Lifecycle Management solutions, and control platforms for automation systems. Second, a network of interfaces between the environments is designed and implemented to enable communication between the digital world and physical manufacturing plant, so that near-synchronous controls can be achieved. Third, capabilities of some members in the family of Deep Reinforcement Learning (DRL) are discussed with manufacturing features within the context of Smart Manufacturing. Trained results for Deep Q Learning algorithms are finally presented in this work as a case study to incorporate DRL-based artificial intelligence to the industrial control process. As a result, developed control methodology, named Digital Engine, is expected to acquire process knowledges, schedule manufacturing tasks, identify optimal actions, and demonstrate control robustness. The authors show that integrating a smart agent into the industrial platforms further expands the usage of the system-level digital twin, where intelligent control algorithms are trained and verified upfront before deployed to the physical world for implementation. Moreover, DRL approach to automated manufacturing control problems under facile optimization environments will be a novel combination between data science and manufacturing industries.",project-academic
,2021-04-27,a,,geometric deep learning grids groups graphs geodesics and gauges," The last decade has witnessed an experimental revolution in data science and machine learning, epitomised by deep learning methods. Indeed, many high-dimensional learning tasks previously thought to be beyond reach -- such as computer vision, playing Go, or protein folding -- are in fact feasible with appropriate computational scale. Remarkably, the essence of deep learning is built from two simple algorithmic principles: first, the notion of representation or feature learning, whereby adapted, often hierarchical, features capture the appropriate notion of regularity for each task, and second, learning by local gradient-descent type methods, typically implemented as backpropagation. While learning generic functions in high dimensions is a cursed estimation problem, most tasks of interest are not generic, and come with essential pre-defined regularities arising from the underlying low-dimensionality and structure of the physical world. This text is concerned with exposing these regularities through unified geometric principles that can be applied throughout a wide spectrum of applications. Such a 'geometric unification' endeavour, in the spirit of Felix Klein's Erlangen Program, serves a dual purpose: on one hand, it provides a common mathematical framework to study the most successful neural network architectures, such as CNNs, RNNs, GNNs, and Transformers. On the other hand, it gives a constructive procedure to incorporate prior physical knowledge into neural architectures and provide principled way to build future architectures yet to be invented.",project-academic
10.1016/J.PROCS.2018.05.053,2018-01-01,a,Elsevier,gender recognition through face using deep learning," Abstract None None Automatic gender recognition has now pertinent to an extension of its usage in various software and hardware, particularly because of the growth of online social networking websites and social media. However the performance of already exist system with the physical world face pictures, images are somewhat not excellent, particularly in comparison with the result of task related to face recognition. Within this paper, we have explored that by doing learn and classification method and with the utilization of Deep Convolutional Neural Networks (D-CNN) technique, a satisfied growth in performance can be achieved on such gender classification tasks that is a reason why we decided to propose an efficient convolutional network VGGnet architecture which can be used in extreme case when the amount of training data used to learn D-CNN based on VGGNet architecture is limited. We examine our related work on the current unfiltered image of the face for gender recognition and display it to dramatics outplay current advance updated methods.",project-academic
