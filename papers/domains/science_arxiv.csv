id,updated,published,title,summary,database
http://arxiv.org/abs/1910.10045v2,2019-12-26T08:09:25Z,2019-10-22T15:27:30Z,"Explainable Artificial Intelligence (XAI): Concepts, Taxonomies,
  Opportunities and Challenges toward Responsible AI","In the last years, Artificial Intelligence (AI) has achieved a notable
momentum that may deliver the best of expectations over many application
sectors across the field. For this to occur, the entire community stands in
front of the barrier of explainability, an inherent problem of AI techniques
brought by sub-symbolism (e.g. ensembles or Deep Neural Networks) that were not
present in the last hype of AI. Paradigms underlying this problem fall within
the so-called eXplainable AI (XAI) field, which is acknowledged as a crucial
feature for the practical deployment of AI models. This overview examines the
existing literature in the field of XAI, including a prospect toward what is
yet to be reached. We summarize previous efforts to define explainability in
Machine Learning, establishing a novel definition that covers prior conceptual
propositions with a major focus on the audience for which explainability is
sought. We then propose and discuss about a taxonomy of recent contributions
related to the explainability of different Machine Learning models, including
those aimed at Deep Learning methods for which a second taxonomy is built. This
literature analysis serves as the background for a series of challenges faced
by XAI, such as the crossroads between data fusion and explainability. Our
prospects lead toward the concept of Responsible Artificial Intelligence,
namely, a methodology for the large-scale implementation of AI methods in real
organizations with fairness, model explainability and accountability at its
core. Our ultimate goal is to provide newcomers to XAI with a reference
material in order to stimulate future research advances, but also to encourage
experts and professionals from other disciplines to embrace the benefits of AI
in their activity sectors, without any prior bias for its lack of
interpretability.",arxiv
http://arxiv.org/abs/1909.13343v2,2019-10-01T16:06:39Z,2019-09-29T19:15:08Z,"ISTHMUS: Secure, Scalable, Real-time and Robust Machine Learning
  Platform for Healthcare","In recent times, machine learning (ML) and artificial intelligence (AI) based
systems have evolved and scaled across different industries such as finance,
retail, insurance, energy utilities, etc. Among other things, they have been
used to predict patterns of customer behavior, to generate pricing models, and
to predict the return on investments. But the successes in deploying machine
learning models at scale in those industries have not translated into the
healthcare setting. There are multiple reasons why integrating ML models into
healthcare has not been widely successful, but from a technical perspective,
general-purpose commercial machine learning platforms are not a good fit for
healthcare due to complexities in handling data quality issues, mandates to
demonstrate clinical relevance, and a lack of ability to monitor performance in
a highly regulated environment with stringent security and privacy needs. In
this paper, we describe Isthmus, a turnkey, cloud-based platform which
addresses the challenges above and reduces time to market for operationalizing
ML/AI in healthcare. Towards the end, we describe three case studies which shed
light on Isthmus capabilities. These include (1) supporting an end-to-end
lifecycle of a model which predicts trauma survivability at hospital trauma
centers, (2) bringing in and harmonizing data from disparate sources to create
a community data platform for inferring population as well as patient level
insights for Social Determinants of Health (SDoH), and (3) ingesting
live-streaming data from various IoT sensors to build models, which can
leverage real-time and longitudinal information to make advanced time-sensitive
predictions.",arxiv
http://arxiv.org/abs/2103.13452v1,2021-03-24T19:11:58Z,2021-03-24T19:11:58Z,"A Portable, Self-Contained Neuroprosthetic Hand with Deep Learning-Based
  Finger Control","Objective: Deep learning-based neural decoders have emerged as the prominent
approach to enable dexterous and intuitive control of neuroprosthetic hands.
Yet few studies have materialized the use of deep learning in clinical settings
due to its high computational requirements. Methods: Recent advancements of
edge computing devices bring the potential to alleviate this problem. Here we
present the implementation of a neuroprosthetic hand with embedded deep
learning-based control. The neural decoder is designed based on the recurrent
neural network (RNN) architecture and deployed on the NVIDIA Jetson Nano - a
compacted yet powerful edge computing platform for deep learning inference.
This enables the implementation of the neuroprosthetic hand as a portable and
self-contained unit with real-time control of individual finger movements.
Results: The proposed system is evaluated on a transradial amputee using
peripheral nerve signals (ENG) with implanted intrafascicular microelectrodes.
The experiment results demonstrate the system's capabilities of providing
robust, high-accuracy (95-99%) and low-latency (50-120 msec) control of
individual finger movements in various laboratory and real-world environments.
Conclusion: Modern edge computing platforms enable the effective use of deep
learning-based neural decoders for neuroprosthesis control as an autonomous
system. Significance: This work helps pioneer the deployment of deep neural
networks in clinical applications underlying a new class of wearable biomedical
devices with embedded artificial intelligence.",arxiv
http://arxiv.org/abs/2005.01557v1,2020-05-04T15:16:30Z,2020-05-04T15:16:30Z,"Off-the-shelf deep learning is not enough: parsimony, Bayes and
  causality","Deep neural networks (""deep learning"") have emerged as a technology of choice
to tackle problems in natural language processing, computer vision, speech
recognition and gameplay, and in just a few years has led to superhuman level
performance and ushered in a new wave of ""AI."" Buoyed by these successes,
researchers in the physical sciences have made steady progress in incorporating
deep learning into their respective domains. However, such adoption brings
substantial challenges that need to be recognized and confronted. Here, we
discuss both opportunities and roadblocks to implementation of deep learning
within materials science, focusing on the relationship between correlative
nature of machine learning and causal hypothesis driven nature of physical
sciences. We argue that deep learning and AI are now well positioned to
revolutionize fields where causal links are known, as is the case for
applications in theory. When confounding factors are frozen or change only
weakly, this leaves open the pathway for effective deep learning solutions in
experimental domains. Similarly, these methods offer a pathway towards
understanding the physics of real-world systems, either via deriving reduced
representations, deducing algorithmic complexity, or recovering generative
physical models. However, extending deep learning and ""AI"" for models with
unclear causal relationship can produce misleading and potentially incorrect
results. Here, we argue the broad adoption of Bayesian methods incorporating
prior knowledge, development of DL solutions with incorporated physical
constraints, and ultimately adoption of causal models, offers a path forward
for fundamental and applied research. Most notably, while these advances can
change the way science is carried out in ways we cannot imagine, machine
learning is not going to substitute science any time soon.",arxiv
http://arxiv.org/abs/1711.06517v2,2018-05-22T05:54:17Z,2017-11-17T12:59:22Z,Wikipedia for Smart Machines and Double Deep Machine Learning,"Very important breakthroughs in data centric deep learning algorithms led to
impressive performance in transactional point applications of Artificial
Intelligence (AI) such as Face Recognition, or EKG classification. With all due
appreciation, however, knowledge blind data only machine learning algorithms
have severe limitations for non-transactional AI applications, such as medical
diagnosis beyond the EKG results. Such applications require deeper and broader
knowledge in their problem solving capabilities, e.g. integrating anatomy and
physiology knowledge with EKG results and other patient findings. Following a
review and illustrations of such limitations for several real life AI
applications, we point at ways to overcome them. The proposed Wikipedia for
Smart Machines initiative aims at building repositories of software structures
that represent humanity science & technology knowledge in various parts of
life; knowledge that we all learn in schools, universities and during our
professional life. Target readers for these repositories are smart machines;
not human. AI software developers will have these Reusable Knowledge structures
readily available, hence, the proposed name ReKopedia. Big Data is by now a
mature technology, it is time to focus on Big Knowledge. Some will be derived
from data, some will be obtained from mankind gigantic repository of knowledge.
Wikipedia for smart machines along with the new Double Deep Learning approach
offer a paradigm for integrating datacentric deep learning algorithms with
algorithms that leverage deep knowledge, e.g. evidential reasoning and
causality reasoning. For illustration, a project is described to produce
ReKopedia knowledge modules for medical diagnosis of about 1,000 disorders.
Data is important, but knowledge deep, basic, and commonsense is equally
important.",arxiv
http://arxiv.org/abs/1803.04873v2,2018-03-14T15:30:00Z,2018-03-13T15:17:30Z,"Using Convolutional Neural Networks for Determining Reticulocyte
  Percentage in Cats","Recent advances in artificial intelligence (AI), specifically in computer
vision (CV) and deep learning (DL), have created opportunities for novel
systems in many fields. In the last few years, deep learning applications have
demonstrated impressive results not only in fields such as autonomous driving
and robotics, but also in the field of medicine, where they have, in some
cases, even exceeded human-level performance. However, despite the huge
potential, adoption of deep learning-based methods is still slow in many areas,
especially in veterinary medicine, where we haven't been able to find any
research papers using modern convolutional neural networks (CNNs) in medical
image processing. We believe that using deep learning-based medical imaging can
enable more accurate, faster and less expensive diagnoses in veterinary
medicine. In order to do so, however, these methods have to be accessible to
everyone in this field, not just to computer scientists. To show the potential
of this technology, we present results on a real-world task in veterinary
medicine that is usually done manually: feline reticulocyte percentage. Using
an open source Keras implementation of the Single-Shot MultiBox Detector (SSD)
model architecture and training it on only 800 labeled images, we achieve an
accuracy of 98.7% at predicting the correct number of aggregate reticulocytes
in microscope images of cat blood smears. The main motivation behind this paper
is to show not only that deep learning can approach or even exceed human-level
performance on a task like this, but also that anyone in the field can
implement it, even without a background in computer science.",arxiv
http://arxiv.org/abs/1901.10281v1,2019-01-29T13:43:57Z,2019-01-29T13:43:57Z,Structural Material Property Tailoring Using Deep Neural Networks,"Advances in robotics, artificial intelligence, and machine learning are
ushering in a new age of automation, as machines match or outperform human
performance. Machine intelligence can enable businesses to improve performance
by reducing errors, improving sensitivity, quality and speed, and in some cases
achieving outcomes that go beyond current resource capabilities. Relevant
applications include new product architecture design, rapid material
characterization, and life-cycle management tied with a digital strategy that
will enable efficient development of products from cradle to grave. In
addition, there are also challenges to overcome that must be addressed through
a major, sustained research effort that is based solidly on both inferential
and computational principles applied to design tailoring of functionally
optimized structures. Current applications of structural materials in the
aerospace industry demand the highest quality control of material
microstructure, especially for advanced rotational turbomachinery in aircraft
engines in order to have the best tailored material property. In this paper,
deep convolutional neural networks were developed to accurately predict
processing-structure-property relations from materials microstructures images,
surpassing current best practices and modeling efforts. The models
automatically learn critical features, without the need for manual
specification and/or subjective and expensive image analysis. Further, in
combination with generative deep learning models, a framework is proposed to
enable rapid material design space exploration and property identification and
optimization. The implementation must take account of real-time decision cycles
and the trade-offs between speed and accuracy.",arxiv
http://arxiv.org/abs/1911.08448v4,2020-03-17T16:06:58Z,2019-11-19T18:14:58Z,Artificial intelligence approach to momentum risk-taking,"We propose a mathematical model of momentum risk-taking, which is essentially
real-time risk management focused on short-term volatility of stock markets.
Its implementation, our fully automated momentum equity trading system
presented systematically, proved to be successful in extensive historical and
real-time experiments. Momentum risk-taking is one of the key components of
general decision-making, a challenge for artificial intelligence and machine
learning with deep roots in cognitive science; its variants beyond stock
markets are discussed. We begin with a new algebraic-type theory of news impact
on share-prices, which describes well their power growth, periodicity, and the
market phenomena like price targets and profit-taking. This theory generally
requires Bessel and hypergeometric functions. Its discretization results in
some tables of bids, which are basically expected returns for main investment
horizons, the key in our trading system. The ML procedures we use are similar
to those in neural networking. A preimage of our approach is the new contract
card game provided at the end, a combination of bridge and poker. Relations to
random processes and the fractional Brownian motion are outlined.",arxiv
http://arxiv.org/abs/1705.00346v1,2017-04-30T17:17:44Z,2017-04-30T17:17:44Z,Deep Learning in the Automotive Industry: Applications and Tools,"Deep Learning refers to a set of machine learning techniques that utilize
neural networks with many hidden layers for tasks, such as image
classification, speech recognition, language understanding. Deep learning has
been proven to be very effective in these domains and is pervasively used by
many Internet services. In this paper, we describe different automotive uses
cases for deep learning in particular in the domain of computer vision. We
surveys the current state-of-the-art in libraries, tools and infrastructures
(e.\,g.\ GPUs and clouds) for implementing, training and deploying deep neural
networks. We particularly focus on convolutional neural networks and computer
vision use cases, such as the visual inspection process in manufacturing plants
and the analysis of social media data. To train neural networks, curated and
labeled datasets are essential. In particular, both the availability and scope
of such datasets is typically very limited. A main contribution of this paper
is the creation of an automotive dataset, that allows us to learn and
automatically recognize different vehicle properties. We describe an end-to-end
deep learning application utilizing a mobile app for data collection and
process support, and an Amazon-based cloud backend for storage and training.
For training we evaluate the use of cloud and on-premises infrastructures
(including multiple GPUs) in conjunction with different neural network
architectures and frameworks. We assess both the training times as well as the
accuracy of the classifier. Finally, we demonstrate the effectiveness of the
trained classifier in a real world setting during manufacturing process.",arxiv
http://arxiv.org/abs/2002.00763v1,2020-01-31T02:28:35Z,2020-01-31T02:28:35Z,Two-path Deep Semi-supervised Learning for Timely Fake News Detection,"News in social media such as Twitter has been generated in high volume and
speed. However, very few of them are labeled (as fake or true news) by
professionals in near real time. In order to achieve timely detection of fake
news in social media, a novel framework of two-path deep semi-supervised
learning is proposed where one path is for supervised learning and the other is
for unsupervised learning. The supervised learning path learns on the limited
amount of labeled data while the unsupervised learning path is able to learn on
a huge amount of unlabeled data. Furthermore, these two paths implemented with
convolutional neural networks (CNN) are jointly optimized to complete
semi-supervised learning. In addition, we build a shared CNN to extract the low
level features on both labeled data and unlabeled data to feed them into these
two paths. To verify this framework, we implement a Word CNN based
semi-supervised learning model and test it on two datasets, namely, LIAR and
PHEME. Experimental results demonstrate that the model built on the proposed
framework can recognize fake news effectively with very few labeled data.",arxiv
http://arxiv.org/abs/2110.04249v1,2021-10-08T16:58:57Z,2021-10-08T16:58:57Z,How Can AI Recognize Pain and Express Empathy,"Sensory and emotional experiences such as pain and empathy are relevant to
mental and physical health. The current drive for automated pain recognition is
motivated by a growing number of healthcare requirements and demands for social
interaction make it increasingly essential. Despite being a trending area, they
have not been explored in great detail. Over the past decades, behavioral
science and neuroscience have uncovered mechanisms that explain the
manifestations of pain. Recently, also artificial intelligence research has
allowed empathic machine learning methods to be approachable. Generally, the
purpose of this paper is to review the current developments for computational
pain recognition and artificial empathy implementation. Our discussion covers
the following topics: How can AI recognize pain from unimodality and
multimodality? Is it necessary for AI to be empathic? How can we create an AI
agent with proactive and reactive empathy? This article explores the challenges
and opportunities of real-world multimodal pain recognition from a
psychological, neuroscientific, and artificial intelligence perspective.
Finally, we identify possible future implementations of artificial empathy and
analyze how humans might benefit from an AI agent equipped with empathy.",arxiv
http://arxiv.org/abs/1912.02102v1,2019-12-03T02:11:50Z,2019-12-03T02:11:50Z,"Artificial Intelligence for Low-Resource Communities: Influence
  Maximization in an Uncertain World","The potential of Artificial Intelligence (AI) to tackle challenging problems
that afflict society is enormous, particularly in the areas of healthcare,
conservation and public safety and security. Many problems in these domains
involve harnessing social networks of under-served communities to enable
positive change, e.g., using social networks of homeless youth to raise
awareness about Human Immunodeficiency Virus (HIV) and other STDs.
Unfortunately, most of these real-world problems are characterized by
uncertainties about social network structure and influence models, and previous
research in AI fails to sufficiently address these uncertainties. This thesis
addresses these shortcomings by advancing the state-of-the-art to a new
generation of algorithms for interventions in social networks. In particular,
this thesis describes the design and development of new influence maximization
algorithms which can handle various uncertainties that commonly exist in
real-world social networks. These algorithms utilize techniques from sequential
planning problems and social network theory to develop new kinds of AI
algorithms. Further, this thesis also demonstrates the real-world impact of
these algorithms by describing their deployment in three pilot studies to
spread awareness about HIV among actual homeless youth in Los Angeles. This
represents one of the first-ever deployments of computer science based
influence maximization algorithms in this domain. Our results show that our AI
algorithms improved upon the state-of-the-art by 160% in the real-world. We
discuss research and implementation challenges faced in deploying these
algorithms, and lessons that can be gleaned for future deployment of such
algorithms. The positive results from these deployments illustrate the enormous
potential of AI in addressing societally relevant problems.",arxiv
http://arxiv.org/abs/2012.01356v1,2020-12-02T17:56:44Z,2020-12-02T17:56:44Z,"Coinbot: Intelligent Robotic Coin Bag Manipulation Using Deep
  Reinforcement Learning And Machine Teaching","Given the laborious difficulty of moving heavy bags of physical currency in
the cash center of the bank, there is a large demand for training and deploying
safe autonomous systems capable of conducting such tasks in a collaborative
workspace. However, the deformable properties of the bag along with the large
quantity of rigid-body coins contained within it, significantly increases the
challenges of bag detection, grasping and manipulation by a robotic gripper and
arm. In this paper, we apply deep reinforcement learning and machine learning
techniques to the task of controlling a collaborative robot to automate the
unloading of coin bags from a trolley. To accomplish the task-specific process
of gripping flexible materials like coin bags where the center of the mass
changes during manipulation, a special gripper was implemented in simulation
and designed in physical hardware. Leveraging a depth camera and object
detection using deep learning, a bag detection and pose estimation has been
done for choosing the optimal point of grasping. An intelligent approach based
on deep reinforcement learning has been introduced to propose the best
configuration of the robot end-effector to maximize successful grasping. A
boosted motion planning is utilized to increase the speed of motion planning
during robot operation. Real-world trials with the proposed pipeline have
demonstrated success rates over 96\% in a real-world setting.",arxiv
http://arxiv.org/abs/2103.10975v1,2021-03-19T18:21:38Z,2021-03-19T18:21:38Z,Accelerating GMRES with Deep Learning in Real-Time,"GMRES is a powerful numerical solver used to find solutions to extremely
large systems of linear equations. These systems of equations appear in many
applications in science and engineering. Here we demonstrate a real-time
machine learning algorithm that can be used to accelerate the time-to-solution
for GMRES. Our framework is novel in that is integrates the deep learning
algorithm in an in situ fashion: the AI-accelerator gradually learns how to
optimizes the time to solution without requiring user input (such as a
pre-trained data set). We describe how our algorithm collects data and
optimizes GMRES. We demonstrate our algorithm by implementing an accelerated
(MLGMRES) solver in Python. We then use MLGMRES to accelerate a solver for the
Poisson equation -- a class of linear problems that appears in may
applications.
  Informed by the properties of formal solutions to the Poisson equation, we
test the performance of different neural networks. Our key takeaway is that
networks which are capable of learning non-local relationships perform well,
without needing to be scaled with the input problem size, making them good
candidates for the extremely large problems encountered in high-performance
computing. For the inputs studied, our method provides a roughly 2$\times$
acceleration.",arxiv
http://arxiv.org/abs/2104.09164v1,2021-04-19T09:41:32Z,2021-04-19T09:41:32Z,"HEAR: Human Action Recognition via Neural Networks on Homomorphically
  Encrypted Data","Remote monitoring to support ""aging in place"" is an active area of research.
Advanced computer vision technology based on deep learning can provide near
real-time home monitoring to detect falling and symptoms related to seizure,
and stroke. Affordable webcams, together with cloud computing services (to run
machine learning algorithms), can potentially bring significant social and
health benefits. However, it has not been deployed in practice because of
privacy and security concerns. People may feel uncomfortable sending their
videos of daily activities (with potentially sensitive private information) to
a computing service provider (e.g., on a commercial cloud). In this paper, we
propose a novel strategy to resolve this dilemma by applying fully homomorphic
encryption (FHE) to an alternative representation of human actions (i.e.,
skeleton joints), which guarantees information confidentiality while retaining
high-performance action detection at a low cost. We design an FHE-friendly
neural network for action recognition and present a secure neural network
evaluation strategy to achieve near real-time action detection. Our framework
for private inference achieves an 87.99% recognition accuracy (86.21%
sensitivity and 99.14% specificity in detecting falls) with a latency of 3.1
seconds on real-world datasets. Our evaluation shows that our elaborated and
fine-tuned method reduces the inference latency by 23.81%~74.67% over a
straightforward implementation.",arxiv
http://arxiv.org/abs/1812.00825v2,2018-12-04T05:36:36Z,2018-11-21T21:02:50Z,"Microscope 2.0: An Augmented Reality Microscope with Real-time
  Artificial Intelligence Integration","The brightfield microscope is instrumental in the visual examination of both
biological and physical samples at sub-millimeter scales. One key clinical
application has been in cancer histopathology, where the microscopic assessment
of the tissue samples is used for the diagnosis and staging of cancer and thus
guides clinical therapy. However, the interpretation of these samples is
inherently subjective, resulting in significant diagnostic variability.
Moreover, in many regions of the world, access to pathologists is severely
limited due to lack of trained personnel. In this regard, Artificial
Intelligence (AI) based tools promise to improve the access and quality of
healthcare. However, despite significant advances in AI research, integration
of these tools into real-world cancer diagnosis workflows remains challenging
because of the costs of image digitization and difficulties in deploying AI
solutions. Here we propose a cost-effective solution to the integration of AI:
the Augmented Reality Microscope (ARM). The ARM overlays AI-based information
onto the current view of the sample through the optical pathway in real-time,
enabling seamless integration of AI into the regular microscopy workflow. We
demonstrate the utility of ARM in the detection of lymph node metastases in
breast cancer and the identification of prostate cancer with a latency that
supports real-time workflows. We anticipate that ARM will remove barriers
towards the use of AI in microscopic analysis and thus improve the accuracy and
efficiency of cancer diagnosis. This approach is applicable to other microscopy
tasks and AI algorithms in the life sciences and beyond.",arxiv
http://arxiv.org/abs/1908.02150v3,2019-10-22T02:23:42Z,2019-08-04T05:19:43Z,Industrial Artificial Intelligence,"Artificial Intelligence (AI) is a cognitive science to enables human to
explore many intelligent ways to model our sensing and reasoning processes.
Industrial AI is a systematic discipline to enable engineers to systematically
develop and deploy AI algorithms with repeating and consistent successes. In
this paper, the key enablers for this transformative technology along with
their significant advantages are discussed. In addition, this research explains
Lighthouse Factories as an emerging status applying to the top manufacturers
that have implemented Industrial AI in their manufacturing ecosystem and gained
significant financial benefits. It is believed that this research will work as
a guideline and roadmap for researchers and industries towards the real-world
implementation of Industrial AI.",arxiv
http://arxiv.org/abs/1902.00522v1,2019-02-01T19:02:18Z,2019-02-01T19:02:18Z,"Deep Learning for Multi-Messenger Astrophysics: A Gateway for Discovery
  in the Big Data Era","This report provides an overview of recent work that harnesses the Big Data
Revolution and Large Scale Computing to address grand computational challenges
in Multi-Messenger Astrophysics, with a particular emphasis on real-time
discovery campaigns. Acknowledging the transdisciplinary nature of
Multi-Messenger Astrophysics, this document has been prepared by members of the
physics, astronomy, computer science, data science, software and
cyberinfrastructure communities who attended the NSF-, DOE- and NVIDIA-funded
""Deep Learning for Multi-Messenger Astrophysics: Real-time Discovery at Scale""
workshop, hosted at the National Center for Supercomputing Applications,
October 17-19, 2018. Highlights of this report include unanimous agreement that
it is critical to accelerate the development and deployment of novel,
signal-processing algorithms that use the synergy between artificial
intelligence (AI) and high performance computing to maximize the potential for
scientific discovery with Multi-Messenger Astrophysics. We discuss key aspects
to realize this endeavor, namely (i) the design and exploitation of scalable
and computationally efficient AI algorithms for Multi-Messenger Astrophysics;
(ii) cyberinfrastructure requirements to numerically simulate astrophysical
sources, and to process and interpret Multi-Messenger Astrophysics data; (iii)
management of gravitational wave detections and triggers to enable
electromagnetic and astro-particle follow-ups; (iv) a vision to harness future
developments of machine and deep learning and cyberinfrastructure resources to
cope with the scale of discovery in the Big Data Era; (v) and the need to build
a community that brings domain experts together with data scientists on equal
footing to maximize and accelerate discovery in the nascent field of
Multi-Messenger Astrophysics.",arxiv
http://arxiv.org/abs/2110.13041v1,2021-10-25T15:25:25Z,2021-10-25T15:25:25Z,Applications and Techniques for Fast Machine Learning in Science,"In this community review report, we discuss applications and techniques for
fast machine learning (ML) in science -- the concept of integrating power ML
methods into the real-time experimental data processing loop to accelerate
scientific discovery. The material for the report builds on two workshops held
by the Fast ML for Science community and covers three main areas: applications
for fast ML across a number of scientific domains; techniques for training and
implementing performant and resource-efficient ML algorithms; and computing
architectures, platforms, and technologies for deploying these algorithms. We
also present overlapping challenges across the multiple scientific domains
where common solutions can be found. This community report is intended to give
plenty of examples and inspiration for scientific discovery through integrated
and accelerated ML solutions. This is followed by a high-level overview and
organization of technical advances, including an abundance of pointers to
source material, which can enable these breakthroughs.",arxiv
http://arxiv.org/abs/2108.03713v1,2021-08-08T19:12:04Z,2021-08-08T19:12:04Z,"On the Difficulty of Generalizing Reinforcement Learning Framework for
  Combinatorial Optimization","Combinatorial optimization problems (COPs) on the graph with real-life
applications are canonical challenges in Computer Science. The difficulty of
finding quality labels for problem instances holds back leveraging supervised
learning across combinatorial problems. Reinforcement learning (RL) algorithms
have recently been adopted to solve this challenge automatically. The
underlying principle of this approach is to deploy a graph neural network (GNN)
for encoding both the local information of the nodes and the graph-structured
data in order to capture the current state of the environment. Then, it is
followed by the actor to learn the problem-specific heuristics on its own and
make an informed decision at each state for finally reaching a good solution.
Recent studies on this subject mainly focus on a family of combinatorial
problems on the graph, such as the travel salesman problem, where the proposed
model aims to find an ordering of vertices that optimizes a given objective
function. We use the security-aware phone clone allocation in the cloud as a
classical quadratic assignment problem (QAP) to investigate whether or not deep
RL-based model is generally applicable to solve other classes of such hard
problems. Extensive empirical evaluation shows that existing RL-based model may
not generalize to QAP.",arxiv
http://arxiv.org/abs/2104.13478v2,2021-05-02T16:16:03Z,2021-04-27T21:09:51Z,"Geometric Deep Learning: Grids, Groups, Graphs, Geodesics, and Gauges","The last decade has witnessed an experimental revolution in data science and
machine learning, epitomised by deep learning methods. Indeed, many
high-dimensional learning tasks previously thought to be beyond reach -- such
as computer vision, playing Go, or protein folding -- are in fact feasible with
appropriate computational scale. Remarkably, the essence of deep learning is
built from two simple algorithmic principles: first, the notion of
representation or feature learning, whereby adapted, often hierarchical,
features capture the appropriate notion of regularity for each task, and
second, learning by local gradient-descent type methods, typically implemented
as backpropagation.
  While learning generic functions in high dimensions is a cursed estimation
problem, most tasks of interest are not generic, and come with essential
pre-defined regularities arising from the underlying low-dimensionality and
structure of the physical world. This text is concerned with exposing these
regularities through unified geometric principles that can be applied
throughout a wide spectrum of applications.
  Such a 'geometric unification' endeavour, in the spirit of Felix Klein's
Erlangen Program, serves a dual purpose: on one hand, it provides a common
mathematical framework to study the most successful neural network
architectures, such as CNNs, RNNs, GNNs, and Transformers. On the other hand,
it gives a constructive procedure to incorporate prior physical knowledge into
neural architectures and provide principled way to build future architectures
yet to be invented.",arxiv
http://arxiv.org/abs/2010.14866v1,2020-10-28T10:25:05Z,2020-10-28T10:25:05Z,"Deterministic and probabilistic deep learning models for inverse design
  of broadband acoustic cloak","Concealing an object from incoming waves (light and/or sound) remained
science fiction for a long time due to the absence of wave-shielding materials
in nature. Yet, the invention of artificial materials and new physical
principles for optical and sound wave manipulation translated this abstract
concept into reality by making an object acoustically invisible. Here, we
present the notion of a machine learning-driven acoustic cloak and demonstrate
an example of such a cloak with a multilayered core-shell configuration.
Importantly, we develop deterministic and probabilistic deep learning models
based on autoencoder-like neural network structure to retrieve the structural
and material properties of the cloaking shell surrounding the object that
suppresses scattering of sound in a broad spectral range, as if it was not
there. The probabilistic model enhances the generalization ability of design
procedure and uncovers the sensitivity of the cloak parameters on the spectral
response for practical implementation. This proposal opens up new avenues to
expedite the design of intelligent cloaking devices for tailored spectral
response and offers a feasible solution for inverse scattering problems.",arxiv
http://arxiv.org/abs/1811.02213v1,2018-11-06T08:05:24Z,2018-11-06T08:05:24Z,"Hybrid Approach to Automation, RPA and Machine Learning: a Method for
  the Human-centered Design of Software Robots","One of the more prominent trends within Industry 4.0 is the drive to employ
Robotic Process Automation (RPA), especially as one of the elements of the Lean
approach. The full implementation of RPA is riddled with challenges relating
both to the reality of everyday business operations, from SMEs to SSCs and
beyond, and the social effects of the changing job market. To successfully
address these points there is a need to develop a solution that would adjust to
the existing business operations and at the same time lower the negative social
impact of the automation process.
  To achieve these goals we propose a hybrid, human-centered approach to the
development of software robots. This design and implementation method combines
the Living Lab approach with empowerment through participatory design to
kick-start the co-development and co-maintenance of hybrid software robots
which, supported by variety of AI methods and tools, including interactive and
collaborative ML in the cloud, transform menial job posts into higher-skilled
positions, allowing former employees to stay on as robot co-designers and
maintainers, i.e. as co-programmers who supervise the machine learning
processes with the use of tailored high-level RPA Domain Specific Languages
(DSLs) to adjust the functioning of the robots and maintain operational
flexibility.",arxiv
http://arxiv.org/abs/1805.03045v2,2018-06-12T08:10:11Z,2018-05-08T14:15:46Z,"A new method for unveiling Open Clusters in Gaia: new nearby Open
  Clusters confirmed by DR2","The publication of the Gaia Data Release 2 (Gaia DR2) opens a new era in
Astronomy. It includes precise astrometric data (positions, proper motions and
parallaxes) for more than $1.3$ billion sources, mostly stars. To analyse such
a vast amount of new data, the use of data mining techniques and machine
learning algorithms are mandatory. The search for Open Clusters, groups of
stars that were born and move together, located in the disk, is a great example
for the application of these techniques. Our aim is to develop a method to
automatically explore the data space, requiring minimal manual intervention. We
explore the performance of a density based clustering algorithm, DBSCAN, to
find clusters in the data together with a supervised learning method such as an
Artificial Neural Network (ANN) to automatically distinguish between real Open
Clusters and statistical clusters. The development and implementation of this
method to a $5$-Dimensional space ($l$, $b$, $\varpi$, $\mu_{\alpha^*}$,
$\mu_\delta$) to the Tycho-Gaia Astrometric Solution (TGAS) data, and a
posterior validation using Gaia DR2 data, lead to the proposal of a set of new
nearby Open Clusters. We have developed a method to find OCs in astrometric
data, designed to be applied to the full Gaia DR2 archive.",arxiv
http://arxiv.org/abs/2108.09862v1,2021-08-22T22:44:28Z,2021-08-22T22:44:28Z,"Explainable Machine Learning using Real, Synthetic and Augmented Fire
  Tests to Predict Fire Resistance and Spalling of RC Columns","This paper presents the development of systematic machine learning (ML)
approach to enable explainable and rapid assessment of fire resistance and
fire-induced spalling of reinforced concrete (RC) columns. The developed
approach comprises of an ensemble of three novel ML algorithms namely; random
forest (RF), extreme gradient boosted trees (ExGBT), and deep learning (DL).
These algorithms are trained to account for a wide collection of geometric
characteristics and material properties, as well as loading conditions to
examine fire performance of normal and high strength RC columns by analyzing a
comprehensive database of fire tests comprising of over 494 observations. The
developed ensemble is also capable of presenting quantifiable insights to ML
predictions; thus, breaking free from the notion of 'blackbox' ML and
establishing a solid step towards transparent and explainable ML. Most
importantly, this work tackles the scarcity of available fire tests by
proposing new techniques to leverage the use of real, synthetic and augmented
fire test observations. The developed ML ensemble has been calibrated and
validated for standard and design fire exposures and for one, two, three and
four-sided fire exposures thus; covering a wide range of practical scenarios
present during fire incidents. When fully deployed, the developed ensemble can
analyze over 5,000 RC columns in under 60 seconds thus, providing an attractive
solution for researchers and practitioners. The presented approach can also be
easily extended for evaluating fire resistance and spalling of other structural
members and under varying fire scenarios and loading conditions and hence paves
the way to modernize the state of this research area and practice.",arxiv
http://arxiv.org/abs/2007.10784v2,2021-07-10T17:50:27Z,2020-07-16T21:14:45Z,Fast Neural Models for Symbolic Regression at Scale,"Deep learning owes much of its success to the astonishing expressiveness of
neural networks. However, this comes at the cost of complex, black-boxed models
that extrapolate poorly beyond the domain of the training dataset, conflicting
with goals of finding analytic expressions to describe science, engineering and
real world data. Under the hypothesis that the hierarchical modularity of such
laws can be captured by training a neural network, we introduce OccamNet, a
neural network model that finds interpretable, compact, and sparse solutions
for fitting data, \`{a} la Occam's razor. Our model defines a probability
distribution over a non-differentiable function space. We introduce a two-step
optimization method that samples functions and updates the weights with
backpropagation based on cross-entropy matching in an evolutionary strategy: we
train by biasing the probability mass toward better fitting solutions. OccamNet
is able to fit a variety of symbolic laws including simple analytic functions,
recursive programs, implicit functions, simple image classification, and can
outperform noticeably state-of-the-art symbolic regression methods on real
world regression datasets. Our method requires minimal memory footprint, does
not require AI accelerators for efficient training, fits complicated functions
in minutes of training on a single CPU, and demonstrates significant
performance gains when scaled on a GPU. Our implementation, demonstrations and
instructions for reproducing the experiments are available at
https://github.com/druidowm/OccamNet_Public.",arxiv
http://arxiv.org/abs/2007.03578v2,2020-07-08T22:53:16Z,2020-07-07T15:55:50Z,"A Vision-based Social Distancing and Critical Density Detection System
  for COVID-19","Social distancing has been proven as an effective measure against the spread
of the infectious COronaVIrus Disease 2019 (COVID-19). However, individuals are
not used to tracking the required 6-feet (2-meters) distance between themselves
and their surroundings. An active surveillance system capable of detecting
distances between individuals and warning them can slow down the spread of the
deadly disease. Furthermore, measuring social density in a region of interest
(ROI) and modulating inflow can decrease social distancing violation occurrence
chance.
  On the other hand, recording data and labeling individuals who do not follow
the measures will breach individuals' rights in free-societies. Here we propose
an Artificial Intelligence (AI) based real-time social distancing detection and
warning system considering four important ethical factors: (1) the system
should never record/cache data, (2) the warnings should not target the
individuals, (3) no human supervisor should be in the detection/warning loop,
and (4) the code should be open-source and accessible to the public. Against
this backdrop, we propose using a monocular camera and deep learning-based
real-time object detectors to measure social distancing. If a violation is
detected, a non-intrusive audio-visual warning signal is emitted without
targeting the individual who breached the social distancing measure. Also, if
the social density is over a critical value, the system sends a control signal
to modulate inflow into the ROI. We tested the proposed method across
real-world datasets to measure its generality and performance. The proposed
method is ready for deployment, and our code is open-sourced.",arxiv
http://arxiv.org/abs/1911.04469v1,2019-11-09T19:59:17Z,2019-11-09T19:59:17Z,"A Proposed Artificial intelligence Model for Real-Time Human Action
  Localization and Tracking","In recent years, artificial intelligence (AI) based on deep learning (DL) has
sparked tremendous global interest. DL is widely used today and has expanded
into various interesting areas. It is becoming more popular in cross-subject
research, such as studies of smart city systems, which combine computer science
with engineering applications. Human action detection is one of these areas.
Human action detection is an interesting challenge due to its stringent
requirements in terms of computing speed and accuracy. High-accuracy real-time
object tracking is also considered a significant challenge. This paper
integrates the YOLO detection network, which is considered a state-of-the-art
tool for real-time object detection, with motion vectors and the Coyote
Optimization Algorithm (COA) to construct a real-time human action localization
and tracking system. The proposed system starts with the extraction of motion
information from a compressed video stream and the extraction of appearance
information from RGB frames using an object detector. Then, a fusion step
between the two streams is performed, and the results are fed into the proposed
action tracking model. The COA is used in object tracking due to its accuracy
and fast convergence. The basic foundation of the proposed model is the
utilization of motion vectors, which already exist in a compressed video bit
stream and provide sufficient information to improve the localization of the
target action without requiring high consumption of computational resources
compared with other popular methods of extracting motion information, such as
optical flows. This advantage allows the proposed approach to be implemented in
challenging environments where the computational resources are limited, such as
Internet of Things (IoT) systems.",arxiv
http://arxiv.org/abs/2110.04080v1,2021-10-03T10:52:19Z,2021-10-03T10:52:19Z,Landslide Detection in Real-Time Social Media Image Streams,"Lack of global data inventories obstructs scientific modeling of and response
to landslide hazards which are oftentimes deadly and costly. To remedy this
limitation, new approaches suggest solutions based on citizen science that
requires active participation. However, as a non-traditional data source,
social media has been increasingly used in many disaster response and
management studies in recent years. Inspired by this trend, we propose to
capitalize on social media data to mine landslide-related information
automatically with the help of artificial intelligence (AI) techniques.
Specifically, we develop a state-of-the-art computer vision model to detect
landslides in social media image streams in real time. To that end, we create a
large landslide image dataset labeled by experts and conduct extensive model
training experiments. The experimental results indicate that the proposed model
can be deployed in an online fashion to support global landslide susceptibility
maps and emergency response.",arxiv
http://arxiv.org/abs/2104.13386v1,2021-04-27T18:00:02Z,2021-04-27T18:00:02Z,"Deep physical neural networks enabled by a backpropagation algorithm for
  arbitrary physical systems","Deep neural networks have become a pervasive tool in science and engineering.
However, modern deep neural networks' growing energy requirements now
increasingly limit their scaling and broader use. We propose a radical
alternative for implementing deep neural network models: Physical Neural
Networks. We introduce a hybrid physical-digital algorithm called Physics-Aware
Training to efficiently train sequences of controllable physical systems to act
as deep neural networks. This method automatically trains the functionality of
any sequence of real physical systems, directly, using backpropagation, the
same technique used for modern deep neural networks. To illustrate their
generality, we demonstrate physical neural networks with three diverse physical
systems-optical, mechanical, and electrical. Physical neural networks may
facilitate unconventional machine learning hardware that is orders of magnitude
faster and more energy efficient than conventional electronic processors.",arxiv
http://arxiv.org/abs/2004.05953v1,2020-04-13T14:09:21Z,2020-04-13T14:09:21Z,"Software-Defined Network for End-to-end Networked Science at the
  Exascale","Domain science applications and workflow processes are currently forced to
view the network as an opaque infrastructure into which they inject data and
hope that it emerges at the destination with an acceptable Quality of
Experience. There is little ability for applications to interact with the
network to exchange information, negotiate performance parameters, discover
expected performance metrics, or receive status/troubleshooting information in
real time. The work presented here is motivated by a vision for a new smart
network and smart application ecosystem that will provide a more deterministic
and interactive environment for domain science workflows. The Software-Defined
Network for End-to-end Networked Science at Exascale (SENSE) system includes a
model-based architecture, implementation, and deployment which enables
automated end-to-end network service instantiation across administrative
domains. An intent based interface allows applications to express their
high-level service requirements, an intelligent orchestrator and resource
control systems allow for custom tailoring of scalability and real-time
responsiveness based on individual application and infrastructure operator
requirements. This allows the science applications to manage the network as a
first-class schedulable resource as is the current practice for instruments,
compute, and storage systems. Deployment and experiments on production networks
and testbeds have validated SENSE functions and performance. Emulation based
testing verified the scalability needed to support research and education
infrastructures. Key contributions of this work include an architecture
definition, reference implementation, and deployment. This provides the basis
for further innovation of smart network services to accelerate scientific
discovery in the era of big data, cloud computing, machine learning and
artificial intelligence.",arxiv
http://arxiv.org/abs/2002.08361v2,2020-03-13T20:47:01Z,2020-02-19T17:05:50Z,"Phase Imaging with Computational Specificity (PICS) for measuring dry
  mass changes in sub-cellular compartments","Due to its specificity, fluorescence microscopy (FM) has become a
quintessential imaging tool in cell biology. However, photobleaching,
phototoxicity, and related artifacts continue to limit FM's utility. Recently,
it has been shown that artificial intelligence (AI) can transform one form of
contrast into another. We present PICS, a combination of quantitative phase
imaging and AI, which provides information about unlabeled live cells with high
specificity. Our imaging system allows for automatic training, while inference
is built into the acquisition software and runs in real-time. Applying the
computed fluorescence maps back to the QPI data, we measured the growth of both
nuclei and cytoplasm independently, over many days, without loss of viability.
Using a QPI method that suppresses multiple scattering, we measured the dry
mass content of individual cell nuclei within spheroids. In its current
implementation, PICS offers a versatile quantitative technique for continuous
simultaneous monitoring of individual cellular components in biological
applications where long-term label-free imaging is desirable.",arxiv
http://arxiv.org/abs/2105.01774v2,2021-06-18T15:21:10Z,2021-05-04T21:40:04Z,"Envisioning Communities: A Participatory Approach Towards AI for Social
  Good","Research in artificial intelligence (AI) for social good presupposes some
definition of social good, but potential definitions have been seldom suggested
and never agreed upon. The normative question of what AI for social good
research should be ""for"" is not thoughtfully elaborated, or is frequently
addressed with a utilitarian outlook that prioritizes the needs of the majority
over those who have been historically marginalized, brushing aside realities of
injustice and inequity. We argue that AI for social good ought to be assessed
by the communities that the AI system will impact, using as a guide the
capabilities approach, a framework to measure the ability of different policies
to improve human welfare equity. Furthermore, we lay out how AI research has
the potential to catalyze social progress by expanding and equalizing
capabilities. We show how the capabilities approach aligns with a participatory
approach for the design and implementation of AI for social good research in a
framework we introduce called PACT, in which community members affected should
be brought in as partners and their input prioritized throughout the project.
We conclude by providing an incomplete set of guiding questions for carrying
out such participatory AI research in a way that elicits and respects a
community's own definition of social good.",arxiv
http://arxiv.org/abs/2103.16323v2,2021-04-08T09:28:14Z,2021-03-30T13:15:48Z,"Thermal Neural Networks: Lumped-Parameter Thermal Modeling With
  State-Space Machine Learning","With electric power systems becoming more compact and increasingly powerful,
the relevance of thermal stress especially during overload operation is
expected to increase ceaselessly. Whenever critical temperatures cannot be
measured economically on a sensor base, a thermal model lends itself to
estimate those unknown quantities. Thermal models for electric power systems
are usually required to be both, real-time capable and of high estimation
accuracy. Moreover, ease of implementation and time to production play an
increasingly important role. In this work, the thermal neural network (TNN) is
introduced, which unifies both, consolidated knowledge in the form of
heat-transfer-based lumped-parameter models, and data-driven nonlinear function
approximation with supervised machine learning. A quasi-linear
parameter-varying system is identified solely from empirical data, where
relationships between scheduling variables and system matrices are inferred
statistically and automatically. At the same time, a TNN has physically
interpretable states through its state-space representation, is end-to-end
trainable -- similar to deep learning models -- with automatic differentiation,
and requires no material, geometry, nor expert knowledge for its design.
Experiments on an electric motor data set show that a TNN achieves higher
temperature estimation accuracies than previous white-/grey- or black-box
models with a mean squared error of $3.18~\text{K}^2$ and a worst-case error of
$5.84~\text{K}$ at 64 model parameters.",arxiv
http://arxiv.org/abs/2104.02214v1,2021-04-06T01:04:28Z,2021-04-06T01:04:28Z,"Intelligent Building Control Systems for Thermal Comfort and
  Energy-Efficiency: A Systematic Review of Artificial Intelligence-Assisted
  Techniques","Building operations represent a significant percentage of the total primary
energy consumed in most countries due to the proliferation of Heating,
Ventilation and Air-Conditioning (HVAC) installations in response to the
growing demand for improved thermal comfort. Reducing the associated energy
consumption while maintaining comfortable conditions in buildings are
conflicting objectives and represent a typical optimization problem that
requires intelligent system design. Over the last decade, different
methodologies based on the Artificial Intelligence (AI) techniques have been
deployed to find the sweet spot between energy use in HVAC systems and suitable
indoor comfort levels to the occupants. This paper performs a comprehensive and
an in-depth systematic review of AI-based techniques used for building control
systems by assessing the outputs of these techniques, and their implementations
in the reviewed works, as well as investigating their abilities to improve the
energy-efficiency, while maintaining thermal comfort conditions. This enables a
holistic view of (1) the complexities of delivering thermal comfort to users
inside buildings in an energy-efficient way, and (2) the associated
bibliographic material to assist researchers and experts in the field in
tackling such a challenge. Among the 20 AI tools developed for both energy
consumption and comfort control, functions such as identification and
recognition patterns, optimization, predictive control. Based on the findings
of this work, the application of AI technology in building control is a
promising area of research and still an ongoing, i.e., the performance of
AI-based control is not yet completely satisfactory. This is mainly due in part
to the fact that these algorithms usually need a large amount of high-quality
real-world data, which is lacking in the building or, more precisely, the
energy sector.",arxiv
http://arxiv.org/abs/2109.13476v1,2021-09-28T04:21:07Z,2021-09-28T04:21:07Z,Fake News Detection using Semi-Supervised Graph Convolutional Network,"Social media becomes the central way for people to obtain and utilise news,
due to its rapidness and inexpensive value of data distribution. Though, such
features of social media platforms also present it a root cause of fake news
distribution, causing adverse consequences on both people and culture. Hence,
detecting fake news has become a significant research interest for bringing
feasible real time solutions to the problem. Most current techniques of fake
news disclosure are supervised, that need large cost in terms of time and
effort to make a certainly interpreted dataset. The proposed framework
concentrates on the text-based detection of fake news items while considering
that only limited number of labels are available. Graphs are functioned
extensively under several purposes of real-world problems on the strength of
their property to structure things easily. Deep neural networks are used to
generate great results within tasks that utilizes graph classification. The
Graph Convolution Network works as a deep learning paradigm which works on
graphs. Our proposed framework deals with limited amount of labelled data; we
go for a semi-supervised learning method. We come up with a semi-supervised
fake news detection technique based on GCN (Graph Convolutional Networks). The
recommended architecture comprises of three basic components: collecting word
embeddings from the news articles in datasets utilising GloVe, building
similarity graph using Word Movers Distance (WMD) and finally applying Graph
Convolution Network (GCN) for binary classification of news articles in
semi-supervised paradigm. The implemented technique is validated on three
different datasets by varying the volume of labelled data achieving 95.27 %
highest accuracy on Real or Fake dataset. Comparison with other contemporary
techniques also reinforced the supremacy of the proposed framework.",arxiv
http://arxiv.org/abs/1907.10323v1,2019-07-24T09:27:11Z,2019-07-24T09:27:11Z,Fairness in Reinforcement Learning,"Decision support systems (e.g., for ecological conservation) and autonomous
systems (e.g., adaptive controllers in smart cities) start to be deployed in
real applications. Although their operations often impact many users or
stakeholders, no fairness consideration is generally taken into account in
their design, which could lead to completely unfair outcomes for some users or
stakeholders. To tackle this issue, we advocate for the use of social welfare
functions that encode fairness and present this general novel problem in the
context of (deep) reinforcement learning, although it could possibly be
extended to other machine learning tasks.",arxiv
http://arxiv.org/abs/2002.05648v3,2020-04-26T04:59:52Z,2020-02-01T01:15:39Z,Politics of Adversarial Machine Learning,"In addition to their security properties, adversarial machine-learning
attacks and defenses have political dimensions. They enable or foreclose
certain options for both the subjects of the machine learning systems and for
those who deploy them, creating risks for civil liberties and human rights. In
this paper, we draw on insights from science and technology studies,
anthropology, and human rights literature, to inform how defenses against
adversarial attacks can be used to suppress dissent and limit attempts to
investigate machine learning systems. To make this concrete, we use real-world
examples of how attacks such as perturbation, model inversion, or membership
inference can be used for socially desirable ends. Although the predictions of
this analysis may seem dire, there is hope. Efforts to address human rights
concerns in the commercial spyware industry provide guidance for similar
measures to ensure ML systems serve democratic, not authoritarian ends",arxiv
http://arxiv.org/abs/2107.01019v1,2021-06-25T20:24:41Z,2021-06-25T20:24:41Z,"Toward 6G: From New Hardware Design to Wireless Semantic and
  Goal-Oriented Communication Paradigms","Several speculative visions are conjecturing on what 6G services will be able
to offer at the horizon of 2030. Nevertheless, the 6G design process is at its
preliminary stages. The reality today is that hardware, technologies and new
materials required to effectively meet the unprecedented performance targets
required for future 6G services and network operation, have not been designed,
tested or even do not exist yet. Today, a solid vision on the cost-benefit
trade-offs of machine learning and artificial intelligence support for 6G
network and services operation optimization is missing. This includes the
possible support from hardware efficiency, operation effectiveness and, the
immeasurable cost due to data acquisition-transfer-processing. The contribution
of this paper is three-fold. This is the first paper deriving crucial 6G key
performance indicators on hardware and technology design. Second, we present a
new hardware technologies design methodology conceived to enable the effective
software-hardware components integration required to meet the challenging
performance envisioned for future 6G networks. Third, we suggest a paradigm
shift towards goal-oriented and semantic communications, in which a totally new
opportunity of joint design of hardware, artificial intelligence and effective
communication is offered. The proposed vision is consolidated by our recent
results on hardware, technology and machine learning performance.",arxiv
http://arxiv.org/abs/1910.09435v1,2019-10-21T15:12:32Z,2019-10-21T15:12:32Z,"Background Rejection in Atmospheric Cherenkov Telescopes using Recurrent
  Convolutional Neural Networks","In this work, we present a new, high performance algorithm for background
rejection in imaging atmospheric Cherenkov telescopes. We build on the already
popular machine-learning techniques used in gamma-ray astronomy by the
application of the latest techniques in machine learning, namely recurrent and
convolutional neural networks, to the background rejection problem. Use of
these machine-learning techniques addresses some of the key challenges
encountered in the currently implemented algorithms and helps to significantly
increase the background rejection performance at all energies.
  We apply these machine learning techniques to the H.E.S.S. telescope array,
first testing their performance on simulated data and then applying the
analysis to two well known gamma-ray sources. With real observational data we
find significantly improved performance over the current standard methods, with
a 20-25\% reduction in the background rate when applying the recurrent neural
network analysis. Importantly, we also find that the convolutional neural
network results are strongly dependent on the sky brightness in the source
region which has important implications for the future implementation of this
method in Cherenkov telescope analysis.",arxiv
http://arxiv.org/abs/1810.02688v2,2018-10-19T13:07:57Z,2018-09-28T08:27:59Z,Wikistat 2.0: Educational Resources for Artificial Intelligence,"Big data, data science, deep learning, artificial intelligence are the key
words of intense hype related with a job market in full evolution, that impose
to adapt the contents of our university professional trainings. Which
artificial intelligence is mostly concerned by the job offers? Which
methodologies and technologies should be favored in the training programs?
Which objectives, tools and educational resources do we needed to put in place
to meet these pressing needs? We answer these questions in describing the
contents and operational resources in the Data Science orientation of the
specialty Applied Mathematics at INSA Toulouse. We focus on basic mathematics
training (Optimization, Probability, Statistics), associated with the practical
implementation of the most performing statistical learning algorithms, with the
most appropriate technologies and on real examples. Considering the huge
volatility of the technologies, it is imperative to train students in
seft-training, this will be their technological watch tool when they will be in
professional activity. This explains the structuring of the educational site
github.com/wikistat into a set of tutorials. Finally, to motivate the thorough
practice of these tutorials, a serious game is organized each year in the form
of a prediction contest between students of Master degrees in Applied
Mathematics for IA.",arxiv
http://arxiv.org/abs/2005.05024v1,2020-04-15T19:10:12Z,2020-04-15T19:10:12Z,Intelligent Tutoring Systems for Generation Z's Addiction,"As generation Z's big data is flooding the Internet through social nets,
neural network based data processing is turning an important cornerstone,
showing significant potential for fast extraction of data patterns. Online
course delivery and associated tutoring are transforming into customizable,
on-demand services driven by the learner. Besides automated grading, strong
potential exists for the development and deployment of next generation
intelligent tutoring software agents. Self-adaptive, online tutoring agents
exhibiting ""intelligent-like"" behavior, being capable ""to learn"" from the
learner, will become the next educational superstars. Over the past decade,
computer-based tutoring agents were deployed in a variety of extended reality
environments, from patient rehabilitation to psychological trauma healing. Most
of these agents are driven by a set of conditional control statements and a
large answers/questions pairs dataset. This article provides a brief
introduction on Generation Z's addiction to digital information, highlights
important efforts for the development of intelligent dialogue systems, and
explains the main components and important design decisions for Intelligent
Tutoring System.",arxiv
http://arxiv.org/abs/2012.02298v2,2021-06-15T06:28:13Z,2020-11-25T17:23:52Z,"Exploration in Online Advertising Systems with Deep Uncertainty-Aware
  Learning","Modern online advertising systems inevitably rely on personalization methods,
such as click-through rate (CTR) prediction. Recent progress in CTR prediction
enjoys the rich representation capabilities of deep learning and achieves great
success in large-scale industrial applications. However, these methods can
suffer from lack of exploration. Another line of prior work addresses the
exploration-exploitation trade-off problem with contextual bandit methods,
which are recently less studied in the industry due to the difficulty in
extending their flexibility with deep models. In this paper, we propose a novel
Deep Uncertainty-Aware Learning (DUAL) method to learn CTR models based on
Gaussian processes, which can provide predictive uncertainty estimations while
maintaining the flexibility of deep neural networks. DUAL can be easily
implemented on existing models and deployed in real-time systems with minimal
extra computational overhead. By linking the predictive uncertainty estimation
ability of DUAL to well-known bandit algorithms, we further present DUAL-based
Ad-ranking strategies to boost up long-term utilities such as the social
welfare in advertising systems. Experimental results on several public datasets
demonstrate the effectiveness of our methods. Remarkably, an online A/B test
deployed in the Alibaba display advertising platform shows an 8.2% social
welfare improvement and an 8.0% revenue lift.",arxiv
http://arxiv.org/abs/1611.03313v1,2016-11-10T14:32:24Z,2016-11-10T14:32:24Z,X-ray Scattering Image Classification Using Deep Learning,"Visual inspection of x-ray scattering images is a powerful technique for
probing the physical structure of materials at the molecular scale. In this
paper, we explore the use of deep learning to develop methods for automatically
analyzing x-ray scattering images. In particular, we apply Convolutional Neural
Networks and Convolutional Autoencoders for x-ray scattering image
classification. To acquire enough training data for deep learning, we use
simulation software to generate synthetic x-ray scattering images. Experiments
show that deep learning methods outperform previously published methods by 10\%
on synthetic and real datasets.",arxiv
http://arxiv.org/abs/1609.02664v1,2016-09-09T06:04:17Z,2016-09-09T06:04:17Z,"Machine Learning with Guarantees using Descriptive Complexity and SMT
  Solvers","Machine learning is a thriving part of computer science. There are many
efficient approaches to machine learning that do not provide strong theoretical
guarantees, and a beautiful general learning theory. Unfortunately, machine
learning approaches that give strong theoretical guarantees have not been
efficient enough to be applicable. In this paper we introduce a logical
approach to machine learning. Models are represented by tuples of logical
formulas and inputs and outputs are logical structures. We present our
framework together with several applications where we evaluate it using SAT and
SMT solvers. We argue that this approach to machine learning is particularly
suited to bridge the gap between efficiency and theoretical soundness. We
exploit results from descriptive complexity theory to prove strong theoretical
guarantees for our approach. To show its applicability, we present experimental
results including learning complexity-theoretic reductions rules for board
games. We also explain how neural networks fit into our framework, although the
current implementation does not scale to provide guarantees for real-world
neural networks.",arxiv
http://arxiv.org/abs/2003.07689v1,2020-02-28T05:00:44Z,2020-02-28T05:00:44Z,Braitenberg Vehicles as Developmental Neurosimulation,"The connection between brain and behavior is a longstanding issue in the
areas of behavioral science, artificial intelligence, and neurobiology.
Particularly in artificial intelligence research, behavior is generated by a
black box approximating the brain. As is standard among models of artificial
and biological neural networks, an analogue of the fully mature brain is
presented as a blank slate. This model generates outputs and behaviors from a
priori associations, yet this does not consider the realities of biological
development and developmental learning. Our purpose is to model the development
of an artificial organism that exhibits complex behaviors. We will introduce
our approach, which is to use Braitenberg Vehicles (BVs) to model the
development of an artificial nervous system. The resulting developmental BVs
will generate behaviors that range from stimulus responses to group behavior
that resembles collective motion. Next, we will situate this work in the domain
of artificial brain networks. Then we will focus on broader themes such as
embodied cognition, feedback, and emergence. Our perspective will then be
exemplified by three software instantiations that demonstrate how a BV-genetic
algorithm hybrid model, multisensory Hebbian learning model, and multi-agent
approaches can be used to approach BV development. We introduce use cases such
as optimized spatial cognition (vehicle-genetic algorithm hybrid model), hinges
connecting behavioral and neural models (multisensory Hebbian learning model),
and cumulative classification (multi-agent approaches). In conclusion, we will
revisit concepts related to our approach and how they might guide future
development.",arxiv
http://arxiv.org/abs/2105.06442v1,2021-05-13T17:33:28Z,2021-05-13T17:33:28Z,"An Empirical Comparison of Bias Reduction Methods on Real-World Problems
  in High-Stakes Policy Settings","Applications of machine learning (ML) to high-stakes policy settings -- such
as education, criminal justice, healthcare, and social service delivery -- have
grown rapidly in recent years, sparking important conversations about how to
ensure fair outcomes from these systems. The machine learning research
community has responded to this challenge with a wide array of proposed
fairness-enhancing strategies for ML models, but despite the large number of
methods that have been developed, little empirical work exists evaluating these
methods in real-world settings. Here, we seek to fill this research gap by
investigating the performance of several methods that operate at different
points in the ML pipeline across four real-world public policy and social good
problems. Across these problems, we find a wide degree of variability and
inconsistency in the ability of many of these methods to improve model
fairness, but post-processing by choosing group-specific score thresholds
consistently removes disparities, with important implications for both the ML
research community and practitioners deploying machine learning to inform
consequential policy decisions.",arxiv
http://arxiv.org/abs/2105.01636v1,2021-05-04T17:27:59Z,2021-05-04T17:27:59Z,Learning 3D Granular Flow Simulations,"Recently, the application of machine learning models has gained momentum in
natural sciences and engineering, which is a natural fit due to the abundance
of data in these fields. However, the modeling of physical processes from
simulation data without first principle solutions remains difficult. Here, we
present a Graph Neural Networks approach towards accurate modeling of complex
3D granular flow simulation processes created by the discrete element method
LIGGGHTS and concentrate on simulations of physical systems found in real world
applications like rotating drums and hoppers. We discuss how to implement Graph
Neural Networks that deal with 3D objects, boundary conditions, particle -
particle, and particle - boundary interactions such that an accurate modeling
of relevant physical quantities is made possible. Finally, we compare the
machine learning based trajectories to LIGGGHTS trajectories in terms of
particle flows and mixing entropies.",arxiv
http://arxiv.org/abs/2111.12142v1,2021-11-23T20:28:07Z,2021-11-23T20:28:07Z,"Phenomenological classification of the Zwicky Transient Facility
  astronomical event alerts","The Zwicky Transient Facility (ZTF), a state-of-the-art optical robotic sky
survey, registers on the order of a million transient events - such as
supernova explosions, changes in brightness of variable sources, or moving
object detections - every clear night, and generates associated real-time
alerts. We present Alert-Classifying Artificial Intelligence (ACAI), an
open-source deep-learning framework for the phenomenological classification of
ZTF alerts. ACAI uses a set of five binary classifiers to characterize objects
which, in combination with the auxiliary/contextual event information available
from alert brokers, provides a powerful tool for alert stream filtering
tailored to different science cases, including early identification of
supernova-like and anomalous transient events. We report on the performance of
ACAI during the first months of deployment in a production setting.",arxiv
http://arxiv.org/abs/1911.06636v2,2020-06-16T09:13:58Z,2019-11-15T13:57:35Z,"Catch & Carry: Reusable Neural Controllers for Vision-Guided Whole-Body
  Tasks","We address the longstanding challenge of producing flexible, realistic
humanoid character controllers that can perform diverse whole-body tasks
involving object interactions. This challenge is central to a variety of
fields, from graphics and animation to robotics and motor neuroscience. Our
physics-based environment uses realistic actuation and first-person perception
-- including touch sensors and egocentric vision -- with a view to producing
active-sensing behaviors (e.g. gaze direction), transferability to real robots,
and comparisons to the biology. We develop an integrated neural-network based
approach consisting of a motor primitive module, human demonstrations, and an
instructed reinforcement learning regime with curricula and task variations. We
demonstrate the utility of our approach for several tasks, including
goal-conditioned box carrying and ball catching, and we characterize its
behavioral robustness. The resulting controllers can be deployed in real-time
on a standard PC. See overview video, https://youtu.be/2rQAW-8gQQk .",arxiv
http://arxiv.org/abs/1707.04826v1,2017-07-16T05:58:40Z,2017-07-16T05:58:40Z,Machine learning application in the life time of materials,"Materials design and development typically takes several decades from the
initial discovery to commercialization with the traditional trial and error
development approach. With the accumulation of data from both experimental and
computational results, data based machine learning becomes an emerging field in
materials discovery, design and property prediction. This manuscript reviews
the history of materials science as a disciplinary the most common machine
learning method used in materials science, and specifically how they are used
in materials discovery, design, synthesis and even failure detection and
analysis after materials are deployed in real application. Finally, the
limitations of machine learning for application in materials science and
challenges in this emerging field is discussed.",arxiv
http://arxiv.org/abs/2104.03961v1,2021-04-08T17:59:07Z,2021-04-08T17:59:07Z,Generalized Approach to Matched Filtering using Neural Networks,"Gravitational wave science is a pioneering field with rapidly evolving data
analysis methodology currently assimilating and inventing deep learning
techniques. The bulk of the sophisticated flagship searches of the field rely
on the time-tested matched filtering principle within their core. In this
paper, we make a key observation on the relationship between the emerging deep
learning and the traditional techniques: matched filtering is formally
equivalent to a particular neural network. This means that a neural network can
be constructed analytically to exactly implement matched filtering, and can be
further trained on data or boosted with additional complexity for improved
performance. This fundamental equivalence allows us to define a ""complexity
standard candle"" allowing us to characterize the relative complexity of the
different approaches to gravitational wave signals in a common framework.
Additionally it also provides a glimpse of an intriguing symmetry that could
provide clues on how neural networks approach the problem of finding signals in
overwhelming noise. Moreover, we show that the proposed neural network
architecture can outperform matched filtering, both with or without knowledge
of a prior on the parameter distribution. When a prior is given, the proposed
neural network can approach the statistically optimal performance. We also
propose and investigate two different neural network architectures MNet-Shallow
and MNet-Deep, both of which implement matched filtering at initialization and
can be trained on data. MNet-Shallow has simpler structure, while MNet-Deep is
more flexible and can deal with a wider range of distributions. Our theoretical
findings are corroborated by experiments using real LIGO data and synthetic
injections. Finally, our results suggest new perspectives on the role of deep
learning in gravitational wave detection.",arxiv
http://arxiv.org/abs/2105.06457v1,2021-05-13T17:56:04Z,2021-05-13T17:56:04Z,Conversational AI Systems for Social Good: Opportunities and Challenges,"Conversational artificial intelligence (ConvAI) systems have attracted much
academic and commercial attention recently, making significant progress on both
fronts. However, little existing work discusses how these systems can be
developed and deployed for social good. In this paper, we briefly review the
progress the community has made towards better ConvAI systems and reflect on
how existing technologies can help advance social good initiatives from various
angles that are unique for ConvAI, or not yet become common knowledge in the
community. We further discuss about the challenges ahead for ConvAI systems to
better help us achieve these goals and highlight the risks involved in their
development and deployment in the real world.",arxiv
http://arxiv.org/abs/2012.08174v2,2021-03-29T17:15:00Z,2020-12-15T09:49:22Z,"Towards open and expandable cognitive AI architectures for large-scale
  multi-agent human-robot collaborative learning","Learning from Demonstration (LfD) constitutes one of the most robust
methodologies for constructing efficient cognitive robotic systems. Despite the
large body of research works already reported, current key technological
challenges include those of multi-agent learning and long-term autonomy.
Towards this direction, a novel cognitive architecture for multi-agent LfD
robotic learning is introduced, targeting to enable the reliable deployment of
open, scalable and expandable robotic systems in large-scale and complex
environments. In particular, the designed architecture capitalizes on the
recent advances in the Artificial Intelligence (AI) field, by establishing a
Federated Learning (FL)-based framework for incarnating a multi-human
multi-robot collaborative learning environment. The fundamental
conceptualization relies on employing multiple AI-empowered cognitive processes
(implementing various robotic tasks) that operate at the edge nodes of a
network of robotic platforms, while global AI models (underpinning the
aforementioned robotic tasks) are collectively created and shared among the
network, by elegantly combining information from a large number of human-robot
interaction instances. Regarding pivotal novelties, the designed cognitive
architecture a) introduces a new FL-based formalism that extends the
conventional LfD learning paradigm to support large-scale multi-agent
operational settings, b) elaborates previous FL-based self-learning robotic
schemes so as to incorporate the human in the learning loop and c) consolidates
the fundamental principles of FL with additional sophisticated AI-enabled
learning methodologies for modelling the multi-level inter-dependencies among
the robotic tasks. The applicability of the proposed framework is explained
using an example of a real-world industrial case study for agile
production-based Critical Raw Materials (CRM) recovery.",arxiv
http://arxiv.org/abs/2106.13219v1,2021-06-24T17:52:43Z,2021-06-24T17:52:43Z,Towards Understanding and Mitigating Social Biases in Language Models,"As machine learning methods are deployed in real-world settings such as
healthcare, legal systems, and social science, it is crucial to recognize how
they shape social biases and stereotypes in these sensitive decision-making
processes. Among such real-world deployments are large-scale pretrained
language models (LMs) that can be potentially dangerous in manifesting
undesirable representational biases - harmful biases resulting from
stereotyping that propagate negative generalizations involving gender, race,
religion, and other social constructs. As a step towards improving the fairness
of LMs, we carefully define several sources of representational biases before
proposing new benchmarks and metrics to measure them. With these tools, we
propose steps towards mitigating social biases during text generation. Our
empirical results and human evaluation demonstrate effectiveness in mitigating
bias while retaining crucial contextual information for high-fidelity text
generation, thereby pushing forward the performance-fairness Pareto frontier.",arxiv
http://arxiv.org/abs/1606.03212v1,2016-06-10T07:17:00Z,2016-06-10T07:17:00Z,"Discovery of Latent Factors in High-dimensional Data Using Tensor
  Methods","Unsupervised learning aims at the discovery of hidden structure that drives
the observations in the real world. It is essential for success in modern
machine learning. Latent variable models are versatile in unsupervised learning
and have applications in almost every domain. Training latent variable models
is challenging due to the non-convexity of the likelihood objective. An
alternative method is based on the spectral decomposition of low order moment
tensors. This versatile framework is guaranteed to estimate the correct model
consistently. My thesis spans both theoretical analysis of tensor decomposition
framework and practical implementation of various applications. This thesis
presents theoretical results on convergence to globally optimal solution of
tensor decomposition using the stochastic gradient descent, despite
non-convexity of the objective. This is the first work that gives global
convergence guarantees for the stochastic gradient descent on non-convex
functions with exponentially many local minima and saddle points. This thesis
also presents large-scale deployment of spectral methods carried out on various
platforms. Dimensionality reduction techniques such as random projection are
incorporated for a highly parallel and scalable tensor decomposition algorithm.
We obtain a gain in both accuracies and in running times by several orders of
magnitude compared to the state-of-art variational methods. To solve real world
problems, more advanced models and learning algorithms are proposed. This
thesis discusses generalization of LDA model to mixed membership stochastic
block model for learning user communities in social network, convolutional
dictionary model for learning word-sequence embeddings, hierarchical tensor
decomposition and latent tree structure model for learning disease hierarchy,
and spatial point process mixture model for detecting cell types in
neuroscience.",arxiv
http://arxiv.org/abs/1904.07633v1,2019-04-16T13:02:01Z,2019-04-16T13:02:01Z,"HARK Side of Deep Learning -- From Grad Student Descent to Automated
  Machine Learning","Recent advancements in machine learning research, i.e., deep learning,
introduced methods that excel conventional algorithms as well as humans in
several complex tasks, ranging from detection of objects in images and speech
recognition to playing difficult strategic games. However, the current
methodology of machine learning research and consequently, implementations of
the real-world applications of such algorithms, seems to have a recurring
HARKing (Hypothesizing After the Results are Known) issue. In this work, we
elaborate on the algorithmic, economic and social reasons and consequences of
this phenomenon. We present examples from current common practices of
conducting machine learning research (e.g. avoidance of reporting negative
results) and failure of generalization ability of the proposed algorithms and
datasets in actual real-life usage. Furthermore, a potential future trajectory
of machine learning research and development from the perspective of
accountable, unbiased, ethical and privacy-aware algorithmic decision making is
discussed. We would like to emphasize that with this discussion we neither
claim to provide an exhaustive argumentation nor blame any specific institution
or individual on the raised issues. This is simply a discussion put forth by
us, insiders of the machine learning field, reflecting on us.",arxiv
http://arxiv.org/abs/2104.01757v1,2021-04-05T03:49:45Z,2021-04-05T03:49:45Z,Predicting Mergers and Acquisitions using Graph-based Deep Learning,"The graph data structure is a staple in mathematics, yet graph-based machine
learning is a relatively green field within the domain of data science. Recent
advances in graph-based ML and open source implementations of relevant
algorithms are allowing researchers to apply methods created in academia to
real-world datasets. The goal of this project was to utilize a popular graph
machine learning framework, GraphSAGE, to predict mergers and acquisitions
(M&A) of enterprise companies. The results were promising, as the model
predicted with 81.79% accuracy on a validation dataset. Given the abundance of
data sources and algorithmic decision making within financial data science,
graph-based machine learning offers a performant, yet non-traditional approach
to generating alpha.",arxiv
http://arxiv.org/abs/2102.10398v3,2021-02-28T00:47:42Z,2021-02-20T17:35:23Z,All-Chalcogenide Programmable All-Optical Deep Neural Networks,"Deeplearning algorithms are revolutionising many aspects of modern life.
Typically, they are implemented in CMOS-based hardware with severely limited
memory access times and inefficient data-routing. All-optical neural networks
without any electro-optic conversions could alleviate these shortcomings.
However, an all-optical nonlinear activation function, which is a vital
building block for optical neural networks, needs to be developed efficiently
on-chip. Here, we introduce and demonstrate both optical synapse weighting and
all-optical nonlinear thresholding using two different effects in a
chalcogenide material photonic platform. We show how the structural phase
transitions in a wide-bandgap phase-change material enables storing the neural
network weights via non-volatile photonic memory, whilst resonant bond
destabilisation is used as a nonlinear activation threshold without changing
the material. These two different transitions within chalcogenides enable
programmable neural networks with near-zero static power consumption once
trained, in addition to picosecond delays performing inference tasks not
limited by wire charging that limit electrical circuits; for instance, we show
that nanosecond-order weight programming and near-instantaneous weight updates
enable accurate inference tasks within 20 picoseconds in a 3-layer all-optical
neural network. Optical neural networks that bypass electro-optic conversion
altogether hold promise for network-edge machine learning applications where
decision-making in real-time are critical, such as for autonomous vehicles or
navigation systems such as signal pre-processing of LIDAR systems.",arxiv
http://arxiv.org/abs/2010.04687v2,2021-01-18T19:52:07Z,2020-10-09T17:16:29Z,"A Series of Unfortunate Counterfactual Events: the Role of Time in
  Counterfactual Explanations","Counterfactual explanations are a prominent example of post-hoc
interpretability methods in the explainable Artificial Intelligence research
domain. They provide individuals with alternative scenarios and a set of
recommendations to achieve a sought-after machine learning model outcome.
Recently, the literature has identified desiderata of counterfactual
explanations, such as feasibility, actionability and sparsity that should
support their applicability in real-world contexts. However, we show that the
literature has neglected the problem of the time dependency of counterfactual
explanations. We argue that, due to their time dependency and because of the
provision of recommendations, even feasible, actionable and sparse
counterfactual explanations may not be appropriate in real-world applications.
This is due to the possible emergence of what we call ""unfortunate
counterfactual events."" These events may occur due to the retraining of machine
learning models whose outcomes have to be explained via counterfactual
explanation. Series of unfortunate counterfactual events frustrate the efforts
of those individuals who successfully implemented the recommendations of
counterfactual explanations. This negatively affects people's trust in the
ability of institutions to provide machine learning-supported decisions
consistently. We introduce an approach to address the problem of the emergence
of unfortunate counterfactual events that makes use of histories of
counterfactual explanations. In the final part of the paper we propose an
ethical analysis of two distinct strategies to cope with the challenge of
unfortunate counterfactual events. We show that they respond to an ethically
responsible imperative to preserve the trustworthiness of credit lending
organizations, the decision models they employ, and the social-economic
function of credit lending.",arxiv
http://arxiv.org/abs/1906.04450v2,2019-08-14T17:52:37Z,2019-06-11T09:02:35Z,"Quantifying Intrinsic Uncertainty in Classification via Deep Dirichlet
  Mixture Networks","With the widespread success of deep neural networks in science and
technology, it is becoming increasingly important to quantify the uncertainty
of the predictions produced by deep learning. In this paper, we introduce a new
method that attaches an explicit uncertainty statement to the probabilities of
classification using deep neural networks. Precisely, we view that the
classification probabilities are sampled from an unknown distribution, and we
propose to learn this distribution through the Dirichlet mixture that is
flexible enough for approximating any continuous distribution on the simplex.
We then construct credible intervals from the learned distribution to assess
the uncertainty of the classification probabilities. Our approach is easy to
implement, computationally efficient, and can be coupled with any deep neural
network architecture. Our method leverages the crucial observation that, in
many classification applications such as medical diagnosis, more than one class
labels are available for each observational unit. We demonstrate the usefulness
of our approach through simulations and a real data example.",arxiv
http://arxiv.org/abs/2002.04716v1,2020-02-11T22:18:28Z,2020-02-11T22:18:28Z,"Robust multi-scale multi-feature deep learning for atomic and defect
  identification in Scanning Tunneling Microscopy on H-Si(100) 2x1 surface","The nature of the atomic defects on the hydrogen passivated Si (100) surface
is analyzed using deep learning and scanning tunneling microscopy (STM). A
robust deep learning framework capable of identifying atomic species, defects,
in the presence of non-resolved contaminates, step edges, and noise is
developed. The automated workflow, based on the combination of several networks
for image assessment, atom-finding and defect finding, is developed to perform
the analysis at different levels of description and is deployed on an
operational STM platform. This is further extended to unsupervised
classification of the extracted defects using the mean-shift clustering
algorithm, which utilizes features automatically engineered from the combined
output of neural networks. This combined approach allows the identification of
localized and extended defects on the topographically non-uniform surfaces or
real materials. Our approach is universal in nature and can be applied to other
surfaces for building comprehensive libraries of atomic defects in quantum
materials.",arxiv
http://arxiv.org/abs/2110.10655v1,2021-10-20T16:49:26Z,2021-10-20T16:49:26Z,"Adversarial Socialbot Learning via Multi-Agent Deep Hierarchical
  Reinforcement Learning","Socialbots are software-driven user accounts on social platforms, acting
autonomously (mimicking human behavior), with the aims to influence the
opinions of other users or spread targeted misinformation for particular goals.
As socialbots undermine the ecosystem of social platforms, they are often
considered harmful. As such, there have been several computational efforts to
auto-detect the socialbots. However, to our best knowledge, the adversarial
nature of these socialbots has not yet been studied. This begs a question ""can
adversaries, controlling socialbots, exploit AI techniques to their advantage?""
To this question, we successfully demonstrate that indeed it is possible for
adversaries to exploit computational learning mechanism such as reinforcement
learning (RL) to maximize the influence of socialbots while avoiding being
detected. We first formulate the adversarial socialbot learning as a
cooperative game between two functional hierarchical RL agents. While one agent
curates a sequence of activities that can avoid the detection, the other agent
aims to maximize network influence by selectively connecting with right users.
Our proposed policy networks train with a vast amount of synthetic graphs and
generalize better than baselines on unseen real-life graphs both in terms of
maximizing network influence (up to +18%) and sustainable stealthiness (up to
+40% undetectability) under a strong bot detector (with 90% detection
accuracy). During inference, the complexity of our approach scales linearly,
independent of a network's structure and the virality of news. This makes our
approach a practical adversarial attack when deployed in a real-life setting.",arxiv
http://arxiv.org/abs/2108.12430v1,2021-08-27T18:00:00Z,2021-08-27T18:00:00Z,"Hardware-accelerated Inference for Real-Time Gravitational-Wave
  Astronomy","The field of transient astronomy has seen a revolution with the first
gravitational-wave detections and the arrival of multi-messenger observations
they enabled. Transformed by the first detection of binary black hole and
binary neutron star mergers, computational demands in gravitational-wave
astronomy are expected to grow by at least a factor of two over the next five
years as the global network of kilometer-scale interferometers are brought to
design sensitivity. With the increase in detector sensitivity, real-time
delivery of gravitational-wave alerts will become increasingly important as an
enabler of multi-messenger followup. In this work, we report a novel
implementation and deployment of deep learning inference for real-time
gravitational-wave data denoising and astrophysical source identification. This
is accomplished using a generic Inference-as-a-Service model that is capable of
adapting to the future needs of gravitational-wave data analysis. Our
implementation allows seamless incorporation of hardware accelerators and also
enables the use of commercial or private (dedicated) as-a-service computing.
Based on our results, we propose a paradigm shift in low-latency and offline
computing in gravitational-wave astronomy. Such a shift can address key
challenges in peak-usage, scalability and reliability, and provide a data
analysis platform particularly optimized for deep learning applications. The
achieved sub-millisecond scale latency will also be relevant for any machine
learning-based real-time control systems that may be invoked in the operation
of near-future and next generation ground-based laser interferometers, as well
as the front-end collection, distribution and processing of data from such
instruments.",arxiv
http://arxiv.org/abs/2106.04008v2,2021-06-09T16:58:52Z,2021-06-07T23:31:47Z,Widening Access to Applied Machine Learning with TinyML,"Broadening access to both computational and educational resources is critical
to diffusing machine-learning (ML) innovation. However, today, most ML
resources and experts are siloed in a few countries and organizations. In this
paper, we describe our pedagogical approach to increasing access to applied ML
through a massive open online course (MOOC) on Tiny Machine Learning (TinyML).
We suggest that TinyML, ML on resource-constrained embedded devices, is an
attractive means to widen access because TinyML both leverages low-cost and
globally accessible hardware, and encourages the development of complete,
self-contained applications, from data collection to deployment. To this end, a
collaboration between academia (Harvard University) and industry (Google)
produced a four-part MOOC that provides application-oriented instruction on how
to develop solutions using TinyML. The series is openly available on the edX
MOOC platform, has no prerequisites beyond basic programming, and is designed
for learners from a global variety of backgrounds. It introduces pupils to
real-world applications, ML algorithms, data-set engineering, and the ethical
considerations of these technologies via hands-on programming and deployment of
TinyML applications in both the cloud and their own microcontrollers. To
facilitate continued learning, community building, and collaboration beyond the
courses, we launched a standalone website, a forum, a chat, and an optional
course-project competition. We also released the course materials publicly,
hoping they will inspire the next generation of ML practitioners and educators
and further broaden access to cutting-edge ML technologies.",arxiv
http://arxiv.org/abs/1912.09621v1,2019-12-20T02:57:05Z,2019-12-20T02:57:05Z,"Understanding Deep Neural Network Predictions for Medical Imaging
  Applications","Computer-aided detection has been a research area attracting great interest
in the past decade. Machine learning algorithms have been utilized extensively
for this application as they provide a valuable second opinion to the doctors.
Despite several machine learning models being available for medical imaging
applications, not many have been implemented in the real-world due to the
uninterpretable nature of the decisions made by the network. In this paper, we
investigate the results provided by deep neural networks for the detection of
malaria, diabetic retinopathy, brain tumor, and tuberculosis in different
imaging modalities. We visualize the class activation mappings for all the
applications in order to enhance the understanding of these networks. This type
of visualization, along with the corresponding network performance metrics,
would aid the data science experts in better understanding of their models as
well as assisting doctors in their decision-making process.",arxiv
http://arxiv.org/abs/1711.10941v1,2017-11-29T16:23:38Z,2017-11-29T16:23:38Z,"Intelligent Traffic Light Control Using Distributed Multi-agent Q
  Learning","The combination of Artificial Intelligence (AI) and Internet-of-Things (IoT),
which is denoted as AI-powered Internet-of-Things (AIoT), is capable of
processing huge amount of data generated from a large number of devices and
handling complex problems in social infrastructures. As AI and IoT technologies
are becoming mature, in this paper, we propose to apply AIoT technologies for
traffic light control, which is an essential component for intelligent
transportation system, to improve the efficiency of smart city's road system.
Specifically, various sensors such as surveillance cameras provide real-time
information for intelligent traffic light control system to observe the states
of both motorized traffic and non-motorized traffic. In this paper, we propose
an intelligent traffic light control solution by using distributed multi-agent
Q learning, considering the traffic information at the neighboring
intersections as well as local motorized and non-motorized traffic, to improve
the overall performance of the entire control system. By using the proposed
multi-agent Q learning algorithm, our solution is targeting to optimize both
the motorized and non-motorized traffic. In addition, we considered many
constraints/rules for traffic light control in the real world, and integrate
these constraints in the learning algorithm, which can facilitate the proposed
solution to be deployed in real operational scenarios. We conducted numerical
simulations for a real-world map with real-world traffic data. The simulation
results show that our proposed solution outperforms existing solutions in terms
of vehicle and pedestrian queue lengths, waiting time at intersections, and
many other key performance metrics.",arxiv
http://arxiv.org/abs/1910.12750v1,2019-10-28T15:21:48Z,2019-10-28T15:21:48Z,"Deep-Learning-Based Image Segmentation Integrated with Optical
  Microscopy for Automatically Searching for Two-Dimensional Materials","Deep-learning algorithms enable precise image recognition based on
high-dimensional hierarchical image features. Here, we report the development
and implementation of a deep-learning-based image segmentation algorithm in an
autonomous robotic system to search for two-dimensional (2D) materials. We
trained the neural network based on Mask-RCNN on annotated optical microscope
images of 2D materials (graphene, hBN, MoS2, and WTe2). The inference algorithm
is run on a 1024 x 1024 px2 optical microscope images for 200 ms, enabling the
real-time detection of 2D materials. The detection process is robust against
changes in the microscopy conditions, such as illumination and color balance,
which obviates the parameter-tuning process required for conventional
rule-based detection algorithms. Integrating the algorithm with a motorized
optical microscope enables the automated searching and cataloging of 2D
materials. This development will allow researchers to utilize unlimited amounts
of 2D materials simply by exfoliating and running the automated searching
process.",arxiv
http://arxiv.org/abs/2103.16010v1,2021-03-30T00:49:40Z,2021-03-30T00:49:40Z,"Theory-Guided Machine Learning for Process Simulation of Advanced
  Composites","Science-based simulation tools such as Finite Element (FE) models are
routinely used in scientific and engineering applications. While their success
is strongly dependent on our understanding of underlying governing physical
laws, they suffer inherent limitations including trade-off between
fidelity/accuracy and speed. The recent rise of Machine Learning (ML) proposes
a theory-agnostic paradigm. In complex multi-physics problems, however,
creating large enough datasets for successful training of ML models has proven
to be challenging. One promising strategy to bridge the divide between these
approaches and take advantage of their respective strengths is Theory-Guided
Machine Learning (TGML) which aims to integrate physical laws into ML
algorithms. In this paper, three case studies on thermal management during
processing of advanced composites are presented and studied using FE, ML and
TGML. A structured approach to incrementally adding increasingly complex
physics to training of TGML model is presented. The benefits of TGML over ML
models are seen in more accurate predictions, particularly outside the training
region, and ability to train with small datasets. One benefit of TGML over FE
is significant speed improvement to potentially develop real-time feedback
systems. A recent successful implementation of a TGML model to assess
producibility of aerospace composite parts is presented.",arxiv
http://arxiv.org/abs/2001.01861v2,2020-07-30T16:58:22Z,2020-01-07T02:39:02Z,Vamsa: Automated Provenance Tracking in Data Science Scripts,"There has recently been a lot of ongoing research in the areas of fairness,
bias and explainability of machine learning (ML) models due to the self-evident
or regulatory requirements of various ML applications. We make the following
observation: All of these approaches require a robust understanding of the
relationship between ML models and the data used to train them. In this work,
we introduce the ML provenance tracking problem: the fundamental idea is to
automatically track which columns in a dataset have been used to derive the
features/labels of an ML model. We discuss the challenges in capturing such
information in the context of Python, the most common language used by data
scientists. We then present Vamsa, a modular system that extracts provenance
from Python scripts without requiring any changes to the users' code. Using 26K
real data science scripts, we verify the effectiveness of Vamsa in terms of
coverage, and performance. We also evaluate Vamsa's accuracy on a smaller
subset of manually labeled data. Our analysis shows that Vamsa's precision and
recall range from 90.4% to 99.1% and its latency is in the order of
milliseconds for average size scripts. Drawing from our experience in deploying
ML models in production, we also present an example in which Vamsa helps
automatically identify models that are affected by data corruption issues.",arxiv
http://arxiv.org/abs/2106.12605v1,2021-06-23T18:08:07Z,2021-06-23T18:08:07Z,Deep Fake Detection: Survey of Facial Manipulation Detection Solutions,"Deep Learning as a field has been successfully used to solve a plethora of
complex problems, the likes of which we could not have imagined a few decades
back. But as many benefits as it brings, there are still ways in which it can
be used to bring harm to our society. Deep fakes have been proven to be one
such problem, and now more than ever, when any individual can create a fake
image or video simply using an application on the smartphone, there need to be
some countermeasures, with which we can detect if the image or video is a fake
or real and dispose of the problem threatening the trustworthiness of online
information. Although the Deep fakes created by neural networks, may seem to be
as real as a real image or video, it still leaves behind spatial and temporal
traces or signatures after moderation, these signatures while being invisible
to a human eye can be detected with the help of a neural network trained to
specialize in Deep fake detection. In this paper, we analyze several such
states of the art neural networks (MesoNet, ResNet-50, VGG-19, and Xception
Net) and compare them against each other, to find an optimal solution for
various scenarios like real-time deep fake detection to be deployed in online
social media platforms where the classification should be made as fast as
possible or for a small news agency where the classification need not be in
real-time but requires utmost accuracy.",arxiv
http://arxiv.org/abs/2007.01076v1,2020-07-02T13:08:39Z,2020-07-02T13:08:39Z,"Mining and Tailings Dam Detection In Satellite Imagery Using Deep
  Learning","This work explores the combination of free cloud computing, free open-source
software, and deep learning methods to analyse a real, large-scale problem: the
automatic country-wide identification and classification of surface mines and
mining tailings dams in Brazil. Locations of officially registered mines and
dams were obtained from the Brazilian government open data resource.
Multispectral Sentinel-2 satellite imagery, obtained and processed at the
Google Earth Engine platform, was used to train and test deep neural networks
using the TensorFlow 2 API and Google Colab platform. Fully Convolutional
Neural Networks were used in an innovative way, to search for unregistered ore
mines and tailing dams in large areas of the Brazilian territory. The efficacy
of the approach is demonstrated by the discovery of 263 mines that do not have
an official mining concession. This exploratory work highlights the potential
of a set of new technologies, freely available, for the construction of low
cost data science tools that have high social impact. At the same time, it
discusses and seeks to suggest practical solutions for the complex and serious
problem of illegal mining and the proliferation of tailings dams, which pose
high risks to the population and the environment, especially in developing
countries. Code is made publicly available at:
https://github.com/remis/mining-discovery-with-deep-learning.",arxiv
http://arxiv.org/abs/1905.10364v1,2019-05-24T09:19:42Z,2019-05-24T09:19:42Z,"Deep learning based high-resolution incoherent x-ray imaging with a
  single-pixel detector","X-ray ""ghost"" imaging has drawn great attention for its potential to lower
radiation dose in medical diagnosis. For practical implementation, however, the
efficiency and image quality have to be greatly improved. Here we demonstrate a
computational ghost imaging scheme where a bucket detector and specially
designed modulation masks are used, together with a new robust deep learning
algorithm in which a compressed set of Hadamard matrices is incorporated into a
multi-level wavelet convolutional neural network. By this means we have
obtained an image of a real object from only 18.75% of the Nyquist sampling
rate, using a portable tabletop incoherent x-ray source of ~37 {\mu}m diameter.
A high imaging resolution of ~10 {\mu}m is achieved, which represents a
concrete step towards the realization of a practical low cost x-ray ghost
imaging camera for applications in biomedicine, archeology, material science,
and so forth.",arxiv
http://arxiv.org/abs/2001.00487v1,2020-01-02T15:22:36Z,2020-01-02T15:22:36Z,"Using CNNs For Users Segmentation In Video See-Through Augmented
  Virtuality","In this paper, we present preliminary results on the use of deep learning
techniques to integrate the users self-body and other participants into a
head-mounted video see-through augmented virtuality scenario. It has been
previously shown that seeing users bodies in such simulations may improve the
feeling of both self and social presence in the virtual environment, as well as
user performance. We propose to use a convolutional neural network for real
time semantic segmentation of users bodies in the stereoscopic RGB video
streams acquired from the perspective of the user. We describe design issues as
well as implementation details of the system and demonstrate the feasibility of
using such neural networks for merging users bodies in an augmented virtuality
simulation.",arxiv
http://arxiv.org/abs/1806.03600v2,2018-06-12T11:16:34Z,2018-06-10T07:29:41Z,"ML + FV = $\heartsuit$? A Survey on the Application of Machine Learning
  to Formal Verification","Formal Verification (FV) and Machine Learning (ML) can seem incompatible due
to their opposite mathematical foundations and their use in real-life problems:
FV mostly relies on discrete mathematics and aims at ensuring correctness; ML
often relies on probabilistic models and consists of learning patterns from
training data. In this paper, we postulate that they are complementary in
practice, and explore how ML helps FV in its classical approaches: static
analysis, model-checking, theorem-proving, and SAT solving. We draw a landscape
of the current practice and catalog some of the most prominent uses of ML
inside FV tools, thus offering a new perspective on FV techniques that can help
researchers and practitioners to better locate the possible synergies. We
discuss lessons learned from our work, point to possible improvements and offer
visions for the future of the domain in the light of the science of software
and systems modeling.",arxiv
http://arxiv.org/abs/1801.00091v2,2018-02-21T22:25:38Z,2017-12-30T06:56:37Z,"PrivySense: $\underline{Pri}$ce $\underline{V}$olatilit$\underline{y}$
  based $\underline{Sen}$timent$\underline{s}$ $\underline{E}$stimation from
  Financial News using Machine Learning","As machine learning ascends the peak of computer science zeitgeist, the usage
and experimentation with sentiment analysis using various forms of textual data
seems pervasive. The effect is especially pronounced in formulating securities
trading strategies, due to a plethora of reasons including the relative ease of
implementation and the abundance of academic research suggesting automated
sentiment analysis can be productively used in trading strategies. The source
data for such analyzers ranges a broad spectrum like social media feeds,
micro-blogs, real-time news feeds, ex-post financial data etc. The abstract
technique underlying these analyzers involve supervised learning of sentiment
classification where the classifier is trained on annotated source corpus, and
accuracy is measured by testing how well the classifiers generalizes on unseen
test data from the corpus. Post training, and validation of fitted models, the
classifiers are used to execute trading strategies, and the corresponding
returns are compared with appropriate benchmark returns (for e.g., the S&P500
returns).
  In this paper, we introduce $\underline{a\ novel\ technique\ of\ using\
price\ volatilities\ to\ empirically\ determine\ the\ sentiment\ in\ news\
data}$, instead of the traditional reverse approach. We also perform meta
sentiment analysis by evaluating the efficacy of existing sentiment classifiers
and the precise definition of sentiment from securities trading context. We
scrutinize the efficacy of using human-annotated sentiment classification and
the tacit assumptions that introduces subjective bias in existing financial
news sentiment classifiers.",arxiv
http://arxiv.org/abs/1902.08638v1,2019-02-22T19:16:32Z,2019-02-22T19:16:32Z,MPP: Model Performance Predictor,"Operations is a key challenge in the domain of machine learning pipeline
deployments involving monitoring and management of real-time prediction
quality. Typically, metrics like accuracy, RMSE etc., are used to track the
performance of models in deployment. However, these metrics cannot be
calculated in production due to the absence of labels. We propose using an ML
algorithm, Model Performance Predictor (MPP), to track the performance of the
models in deployment. We argue that an ensemble of such metrics can be used to
create a score representing the prediction quality in production. This in turn
facilitates formulation and customization of ML alerts, that can be escalated
by an operations team to the data science team. Such a score automates
monitoring and enables ML deployments at scale.",arxiv
http://arxiv.org/abs/2103.00959v2,2021-07-23T03:29:00Z,2021-03-01T12:35:16Z,CogDL: Toolkit for Deep Learning on Graphs,"Deep learning on graphs has attracted tremendous attention from the graph
learning community in recent years. It has been widely used in several
real-world applications such as social network analysis and recommender
systems. In this paper, we introduce CogDL, an extensive toolkit for deep
learning on graphs that allows researchers and developers to easily conduct
experiments and build applications. It provides standard training and
evaluation for the most important tasks in the graph domain, including node
classification, graph classification, etc. For each task, it provides
implementations of state-of-the-art models. The models in our toolkit are
divided into two major parts, graph embedding methods and graph neural
networks. Most of the graph embedding methods learn node-level or graph-level
representations in an unsupervised way and preserves the graph properties such
as structural information, while graph neural networks capture node features
and work in semi-supervised or self-supervised settings. All models implemented
in our toolkit can be easily reproducible for leaderboard results. Most models
in CogDL are developed on top of PyTorch, and users can leverage the advantages
of PyTorch to implement their own models. Furthermore, we demonstrate the
effectiveness of CogDL for real-world applications in AMiner, a large academic
mining system.",arxiv
http://arxiv.org/abs/1903.04548v1,2019-03-08T08:04:52Z,2019-03-08T08:04:52Z,"A Novel Approach for Protection of Accounts' Names against Hackers
  Combining Cluster Analysis and Chaotic Theory","The last years of the 20 th century and the beginning of the 21 th mark the
facilitation trend of our real life due to the big development and progress of
the computers and other intelligent devices. Algorithms based on artificial
intelligence are basically a part of the software. The transmitted information
by Internet or LAN arises continuously and it is expected that the protection
of the data has been ensured. The aim of the present paper is to reveal false
names of users' accounts as a result of hackers' attacks. The probability a
given account to be either false or actual is calculated using a novel approach
combining machine learning analysis (especially clusters' analysis) with chaos
theory. The suspected account will be used as a pattern and by classification
techniques clusters will be formed with a respective probability this name to
be false. This investigation puts two main purposes: First, to determine if
there exists a trend of appearance of the similar usernames, which arises
during the creation of new accounts. Second, to detect the false usernames and
to discriminate those from the real ones, independently of that if two types of
accounts are generated with the same speed. These security systems are applied
in different areas, where the security of the data in users' accounts is
strictly required. For example, they can be used in on-line voting for
balloting, in studying the social opinion by inquiries, in protection of the
information in different user accounts of given system etc.",arxiv
http://arxiv.org/abs/1901.06242v1,2018-12-01T13:40:03Z,2018-12-01T13:40:03Z,"Data-driven Air Quality Characterisation for Urban Environments: a Case
  Study","The economic and social impact of poor air quality in towns and cities is
increasingly being recognised, together with the need for effective ways of
creating awareness of real-time air quality levels and their impact on human
health. With local authority maintained monitoring stations being
geographically sparse and the resultant datasets also featuring missing labels,
computational data-driven mechanisms are needed to address the data sparsity
challenge. In this paper, we propose a machine learning-based method to
accurately predict the Air Quality Index (AQI), using environmental monitoring
data together with meteorological measurements. To do so, we develop an air
quality estimation framework that implements a neural network that is enhanced
with a novel Non-linear Autoregressive neural network with exogenous input
(NARX), especially designed for time series prediction. The framework is
applied to a case study featuring different monitoring sites in London, with
comparisons against other standard machine-learning based predictive algorithms
showing the feasibility and robust performance of the proposed method for
different kinds of areas within an urban region.",arxiv
http://arxiv.org/abs/2010.12751v1,2020-10-24T03:09:37Z,2020-10-24T03:09:37Z,"Model Extraction Attacks on Graph Neural Networks: Taxonomy and
  Realization","Graph neural networks (GNNs) have been widely used to analyze the
graph-structured data in various application domains, e.g., social networks,
molecular biology, and anomaly detection. With great power, the GNN models,
usually as valuable Intellectual Properties of their owners, also become
attractive targets of the attacker. Recent studies show that machine learning
models are facing a severe threat called Model Extraction Attacks, where a
well-trained private model owned by a service provider can be stolen by the
attacker pretending as a client. Unfortunately, existing works focus on the
models trained on the Euclidean space, e.g., images and texts, while how to
extract a GNN model that contains a graph structure and node features is yet to
be explored. In this paper, we explore and develop model extraction attacks
against GNN models. Given only black-box access to a target GNN model, the
attacker aims to reconstruct a duplicated one via several nodes he obtained
(called attacker nodes). We first systematically formalise the threat modeling
in the context of GNN model extraction and classify the adversarial threats
into seven categories by considering different background knowledge of the
attacker, e.g., attributes and/or neighbor connectives of the attacker nodes.
Then we present the detailed methods which utilize the accessible knowledge in
each threat to implement the attacks. By evaluating over three real-world
datasets, our attacks are shown to extract duplicated models effectively, i.e.,
more than 89% inputs in the target domain have the same output predictions as
the victim model.",arxiv
http://arxiv.org/abs/2010.07634v3,2021-01-24T10:00:18Z,2020-10-15T10:09:09Z,"Towards Reflectivity profile inversion through Artificial Neural
  Networks","The goal of Specular Neutron and X-ray Reflectometry is to infer materials
Scattering Length Density (SLD) profiles from experimental reflectivity curves.
This paper focuses on investigating an original approach to the ill-posed
non-invertible problem which involves the use of Artificial Neural Networks
(ANN). In particular, the numerical experiments described here deal with large
data sets of simulated reflectivity curves and SLD profiles, and aim to assess
the applicability of Data Science and Machine Learning technology to the
analysis of data generated at neutron scattering large scale facilities. It is
demonstrated that, under certain circumstances, properly trained Deep Neural
Networks are capable of correctly recovering plausible SLD profiles when
presented with never-seen-before simulated reflectivity curves. When the
necessary conditions are met, a proper implementation of the described approach
would offer two main advantages over traditional fitting methods when dealing
with real experiments, namely, 1. sample physical models are described under a
new paradigm: detailed layer-by-layer descriptions (SLDs, thicknesses,
roughnesses) are replaced by parameter free curves $\rho(z)$, allowing a-priori
assumptions to be fed in terms of the sample family to which a given sample
belongs (e.g. ""thin film"", ""lamellar structure"", etc.) 2. the time-to-solution
is shrunk by orders of magnitude, enabling faster batch analyses for large
datasets.",arxiv
http://arxiv.org/abs/2103.15348v2,2021-06-21T16:24:36Z,2021-03-29T05:55:08Z,"LayoutParser: A Unified Toolkit for Deep Learning Based Document Image
  Analysis","Recent advances in document image analysis (DIA) have been primarily driven
by the application of neural networks. Ideally, research outcomes could be
easily deployed in production and extended for further investigation. However,
various factors like loosely organized codebases and sophisticated model
configurations complicate the easy reuse of important innovations by a wide
audience. Though there have been on-going efforts to improve reusability and
simplify deep learning (DL) model development in disciplines like natural
language processing and computer vision, none of them are optimized for
challenges in the domain of DIA. This represents a major gap in the existing
toolkit, as DIA is central to academic research across a wide range of
disciplines in the social sciences and humanities. This paper introduces
layoutparser, an open-source library for streamlining the usage of DL in DIA
research and applications. The core layoutparser library comes with a set of
simple and intuitive interfaces for applying and customizing DL models for
layout detection, character recognition, and many other document processing
tasks. To promote extensibility, layoutparser also incorporates a community
platform for sharing both pre-trained models and full document digitization
pipelines. We demonstrate that layoutparser is helpful for both lightweight and
large-scale digitization pipelines in real-word use cases. The library is
publicly available at https://layout-parser.github.io/.",arxiv
http://arxiv.org/abs/2009.05835v3,2021-04-03T15:08:50Z,2020-09-12T17:37:36Z,"How Much Can We Really Trust You? Towards Simple, Interpretable Trust
  Quantification Metrics for Deep Neural Networks","A critical step to building trustworthy deep neural networks is trust
quantification, where we ask the question: How much can we trust a deep neural
network? In this study, we take a step towards simple, interpretable metrics
for trust quantification by introducing a suite of metrics for assessing the
overall trustworthiness of deep neural networks based on their behaviour when
answering a set of questions. We conduct a thought experiment and explore two
key questions about trust in relation to confidence: 1) How much trust do we
have in actors who give wrong answers with great confidence? and 2) How much
trust do we have in actors who give right answers hesitantly? Based on insights
gained, we introduce the concept of question-answer trust to quantify
trustworthiness of an individual answer based on confident behaviour under
correct and incorrect answer scenarios, and the concept of trust density to
characterize the distribution of overall trust for an individual answer
scenario. We further introduce the concept of trust spectrum for representing
overall trust with respect to the spectrum of possible answer scenarios across
correctly and incorrectly answered questions. Finally, we introduce
NetTrustScore, a scalar metric summarizing overall trustworthiness. The suite
of metrics aligns with past social psychology studies that study the
relationship between trust and confidence. Leveraging these metrics, we
quantify the trustworthiness of several well-known deep neural network
architectures for image recognition to get a deeper understanding of where
trust breaks down. The proposed metrics are by no means perfect, but the hope
is to push the conversation towards better metrics to help guide practitioners
and regulators in producing, deploying, and certifying deep learning solutions
that can be trusted to operate in real-world, mission-critical scenarios.",arxiv
http://arxiv.org/abs/1611.00315v1,2016-08-28T20:04:25Z,2016-08-28T20:04:25Z,"Rapid Prototyping of a Text Mining Application for Cryptocurrency Market
  Intelligence","Blockchain represents a technology for establishing a shared, immutable
version of the truth between a network of participants that do not trust one
another, and therefore has the potential to disrupt any financial or other
industries that rely on third-parties to establish trust. Recent trends in
computing including: prevalence of Free and Open Source Software (FOSS); easy
access to High Performance Computing (HPC i.e. 'The Cloud'); and increasingly
advanced analytics capabilities such as Natural Language Processing (NLP) and
Machine Learning (ML) allow for rapidly prototyping applications for analysis
of trends in the emergence of Blockchain technology. A scaleable
proof-of-concept pipeline that lays the groundwork for analysis of multiple
streams of semi-structured data posted on social media is demonstrated.
Preliminary analysis and performance metrics are presented and discussed.
Future work is described that will scale the system to cloud-based, real-time,
analysis of multiple data streams, with Information Extraction (IE) (ex.
sentiment analysis) and Machine Learning capability.",arxiv
http://arxiv.org/abs/1908.10398v1,2019-08-27T18:30:49Z,2019-08-27T18:30:49Z,"A Data-Efficient Deep Learning Approach for Deployable Multimodal Social
  Robots","The deep supervised and reinforcement learning paradigms (among others) have
the potential to endow interactive multimodal social robots with the ability of
acquiring skills autonomously. But it is still not very clear yet how they can
be best deployed in real world applications. As a step in this direction, we
propose a deep learning-based approach for efficiently training a humanoid
robot to play multimodal games---and use the game of `Noughts & Crosses' with
two variants as a case study. Its minimum requirements for learning to perceive
and interact are based on a few hundred example images, a few example
multimodal dialogues and physical demonstrations of robot manipulation, and
automatic simulations. In addition, we propose novel algorithms for robust
visual game tracking and for competitive policy learning with high winning
rates, which substantially outperform DQN-based baselines. While an automatic
evaluation shows evidence that the proposed approach can be easily extended to
new games with competitive robot behaviours, a human evaluation with 130 humans
playing with the Pepper robot confirms that highly accurate visual perception
is required for successful game play.",arxiv
http://arxiv.org/abs/2110.00840v1,2021-10-02T16:52:28Z,2021-10-02T16:52:28Z,"Induction, Popper, and machine learning","Francis Bacon popularized the idea that science is based on a process of
induction by which repeated observations are, in some unspecified way,
generalized to theories based on the assumption that the future resembles the
past. This idea was criticized by Hume and others as untenable leading to the
famous problem of induction. It wasn't until the work of Karl Popper that this
problem was solved, by demonstrating that induction is not the basis for
science and that the development of scientific knowledge is instead based on
the same principles as biological evolution. Today, machine learning is also
taught as being rooted in induction from big data. Solomonoff induction
implemented in an idealized Bayesian agent (Hutter's AIXI) is widely discussed
and touted as a framework for understanding AI algorithms, even though
real-world attempts to implement something like AIXI immediately encounter
fatal problems. In this paper, we contrast frameworks based on induction with
Donald T. Campbell's universal Darwinism. We show that most AI algorithms in
use today can be understood as using an evolutionary trial and error process
searching over a solution space. In this work we argue that a universal
Darwinian framework provides a better foundation for understanding AI systems.
Moreover, at a more meta level the process of development of all AI algorithms
can be understood under the framework of universal Darwinism.",arxiv
http://arxiv.org/abs/2009.07632v1,2020-08-26T08:58:29Z,2020-08-26T08:58:29Z,"Helping Users Tackle Algorithmic Threats on Social Media: A Multimedia
  Research Agenda","Participation on social media platforms has many benefits but also poses
substantial threats. Users often face an unintended loss of privacy, are
bombarded with mis-/disinformation, or are trapped in filter bubbles due to
over-personalized content. These threats are further exacerbated by the rise of
hidden AI-driven algorithms working behind the scenes to shape users' thoughts,
attitudes, and behavior. We investigate how multimedia researchers can help
tackle these problems to level the playing field for social media users. We
perform a comprehensive survey of algorithmic threats on social media and use
it as a lens to set a challenging but important research agenda for effective
and real-time user nudging. We further implement a conceptual prototype and
evaluate it with experts to supplement our research agenda. This paper calls
for solutions that combat the algorithmic threats on social media by utilizing
machine learning and multimedia content analysis techniques but in a
transparent manner and for the benefit of the users.",arxiv
http://arxiv.org/abs/1707.06600v2,2017-09-06T17:32:44Z,2017-07-20T16:35:02Z,"A multi-agent reinforcement learning model of common-pool resource
  appropriation","Humanity faces numerous problems of common-pool resource appropriation. This
class of multi-agent social dilemma includes the problems of ensuring
sustainable use of fresh water, common fisheries, grazing pastures, and
irrigation systems. Abstract models of common-pool resource appropriation based
on non-cooperative game theory predict that self-interested agents will
generally fail to find socially positive equilibria---a phenomenon called the
tragedy of the commons. However, in reality, human societies are sometimes able
to discover and implement stable cooperative solutions. Decades of behavioral
game theory research have sought to uncover aspects of human behavior that make
this possible. Most of that work was based on laboratory experiments where
participants only make a single choice: how much to appropriate. Recognizing
the importance of spatial and temporal resource dynamics, a recent trend has
been toward experiments in more complex real-time video game-like environments.
However, standard methods of non-cooperative game theory can no longer be used
to generate predictions for this case. Here we show that deep reinforcement
learning can be used instead. To that end, we study the emergent behavior of
groups of independently learning agents in a partially observed Markov game
modeling common-pool resource appropriation. Our experiments highlight the
importance of trial-and-error learning in common-pool resource appropriation
and shed light on the relationship between exclusion, sustainability, and
inequality.",arxiv
http://arxiv.org/abs/2102.09548v2,2021-08-28T19:59:03Z,2021-02-18T18:50:31Z,"Therapeutics Data Commons: Machine Learning Datasets and Tasks for Drug
  Discovery and Development","Therapeutics machine learning is an emerging field with incredible
opportunities for innovatiaon and impact. However, advancement in this field
requires formulation of meaningful learning tasks and careful curation of
datasets. Here, we introduce Therapeutics Data Commons (TDC), the first
unifying platform to systematically access and evaluate machine learning across
the entire range of therapeutics. To date, TDC includes 66 AI-ready datasets
spread across 22 learning tasks and spanning the discovery and development of
safe and effective medicines. TDC also provides an ecosystem of tools and
community resources, including 33 data functions and types of meaningful data
splits, 23 strategies for systematic model evaluation, 17 molecule generation
oracles, and 29 public leaderboards. All resources are integrated and
accessible via an open Python library. We carry out extensive experiments on
selected datasets, demonstrating that even the strongest algorithms fall short
of solving key therapeutics challenges, including real dataset distributional
shifts, multi-scale modeling of heterogeneous data, and robust generalization
to novel data points. We envision that TDC can facilitate algorithmic and
scientific advances and considerably accelerate machine-learning model
development, validation and transition into biomedical and clinical
implementation. TDC is an open-science initiative available at
https://tdcommons.ai.",arxiv
http://arxiv.org/abs/1911.03848v1,2019-11-10T04:36:59Z,2019-11-10T04:36:59Z,Embedded Neural Networks for Robot Autonomy,"We present a library to automatically embed signal processing and neural
network predictions into the material robots are made of. Deep and shallow
neural network models are first trained offline using state-of-the-art machine
learning tools and then transferred onto general purpose microcontrollers that
are co-located with a robot's sensors and actuators. We validate this approach
using multiple examples: a smart robotic tire for terrain classification, a
robotic finger sensor for load classification and a smart composite capable of
regressing impact source localization. In each example, sensing and computation
are embedded inside the material, creating artifacts that serve as stand-in
replacement for otherwise inert conventional parts. The open source software
library takes as inputs trained model files from higher level learning
software, such as Tensorflow/Keras, and outputs code that is readable in a
microcontroller that supports C. We compare the performance of this approach
for various embedded platforms. In particular, we show that low-cost
off-the-shelf microcontrollers can match the accuracy of a desktop computer,
while being fast enough for real-time applications at different neural network
configurations. We provide means to estimate the maximum number of parameters
that the hardware will support based on the microcontroller's specifications.",arxiv
http://arxiv.org/abs/1908.08998v2,2019-10-23T14:39:47Z,2019-08-13T10:15:39Z,AIBench: An Industry Standard Internet Service AI Benchmark Suite,"Today's Internet Services are undergoing fundamental changes and shifting to
an intelligent computing era where AI is widely employed to augment services.
In this context, many innovative AI algorithms, systems, and architectures are
proposed, and thus the importance of benchmarking and evaluating them rises.
However, modern Internet services adopt a microservice-based architecture and
consist of various modules. The diversity of these modules and complexity of
execution paths, the massive scale and complex hierarchy of datacenter
infrastructure, the confidential issues of data sets and workloads pose great
challenges to benchmarking. In this paper, we present the first
industry-standard Internet service AI benchmark suite---AIBench with seventeen
industry partners, including several top Internet service providers. AIBench
provides a highly extensible, configurable, and flexible benchmark framework
that contains loosely coupled modules. We identify sixteen prominent AI problem
domains like learning to rank, each of which forms an AI component benchmark,
from three most important Internet service domains: search engine, social
network, and e-commerce, which is by far the most comprehensive AI benchmarking
effort. On the basis of the AIBench framework, abstracting the real-world data
sets and workloads from one of the top e-commerce providers, we design and
implement the first end-to-end Internet service AI benchmark, which contains
the primary modules in the critical paths of an industry scale application and
is scalable to deploy on different cluster scales. The specifications, source
code, and performance numbers are publicly available from the benchmark council
web site http://www.benchcouncil.org/AIBench/index.html.",arxiv
http://arxiv.org/abs/2110.15245v1,2021-10-28T16:04:01Z,2021-10-28T16:04:01Z,"From Machine Learning to Robotics: Challenges and Opportunities for
  Embodied Intelligence","Machine learning has long since become a keystone technology, accelerating
science and applications in a broad range of domains. Consequently, the notion
of applying learning methods to a particular problem set has become an
established and valuable modus operandi to advance a particular field. In this
article we argue that such an approach does not straightforwardly extended to
robotics -- or to embodied intelligence more generally: systems which engage in
a purposeful exchange of energy and information with a physical environment. In
particular, the purview of embodied intelligent agents extends significantly
beyond the typical considerations of main-stream machine learning approaches,
which typically (i) do not consider operation under conditions significantly
different from those encountered during training; (ii) do not consider the
often substantial, long-lasting and potentially safety-critical nature of
interactions during learning and deployment; (iii) do not require ready
adaptation to novel tasks while at the same time (iv) effectively and
efficiently curating and extending their models of the world through targeted
and deliberate actions. In reality, therefore, these limitations result in
learning-based systems which suffer from many of the same operational
shortcomings as more traditional, engineering-based approaches when deployed on
a robot outside a well defined, and often narrow operating envelope. Contrary
to viewing embodied intelligence as another application domain for machine
learning, here we argue that it is in fact a key driver for the advancement of
machine learning technology. In this article our goal is to highlight
challenges and opportunities that are specific to embodied intelligence and to
propose research directions which may significantly advance the
state-of-the-art in robot learning.",arxiv
http://arxiv.org/abs/1911.08089v2,2019-12-07T03:42:06Z,2019-11-19T04:28:47Z,"""The Human Body is a Black Box"": Supporting Clinical Decision-Making
  with Deep Learning","Machine learning technologies are increasingly developed for use in
healthcare. While research communities have focused on creating
state-of-the-art models, there has been less focus on real world implementation
and the associated challenges to accuracy, fairness, accountability, and
transparency that come from actual, situated use. Serious questions remain
under examined regarding how to ethically build models, interpret and explain
model output, recognize and account for biases, and minimize disruptions to
professional expertise and work cultures. We address this gap in the literature
and provide a detailed case study covering the development, implementation, and
evaluation of Sepsis Watch, a machine learning-driven tool that assists
hospital clinicians in the early diagnosis and treatment of sepsis. We, the
team that developed and evaluated the tool, discuss our conceptualization of
the tool not as a model deployed in the world but instead as a socio-technical
system requiring integration into existing social and professional contexts.
Rather than focusing on model interpretability to ensure a fair and accountable
machine learning, we point toward four key values and practices that should be
considered when developing machine learning to support clinical
decision-making: rigorously define the problem in context, build relationships
with stakeholders, respect professional discretion, and create ongoing feedback
loops with stakeholders. Our work has significant implications for future
research regarding mechanisms of institutional accountability and
considerations for designing machine learning systems. Our work underscores the
limits of model interpretability as a solution to ensure transparency,
accuracy, and accountability in practice. Instead, our work demonstrates other
means and goals to achieve FATML values in design and in practice.",arxiv
http://arxiv.org/abs/1804.07886v1,2018-04-21T04:16:46Z,2018-04-21T04:16:46Z,Social Bots for Online Public Health Interventions,"According to the Center for Disease Control and Prevention, in the United
States hundreds of thousands initiate smoking each year, and millions live with
smoking-related dis- eases. Many tobacco users discuss their habits and
preferences on social media. This work conceptualizes a framework for targeted
health interventions to inform tobacco users about the consequences of tobacco
use. We designed a Twitter bot named Notobot (short for No-Tobacco Bot) that
leverages machine learning to identify users posting pro-tobacco tweets and
select individualized interventions to address their interest in tobacco use.
We searched the Twitter feed for tobacco-related keywords and phrases, and
trained a convolutional neural network using over 4,000 tweets dichotomously
manually labeled as either pro- tobacco or not pro-tobacco. This model achieves
a 90% recall rate on the training set and 74% on test data. Users posting pro-
tobacco tweets are matched with former smokers with similar interests who
posted anti-tobacco tweets. Algorithmic matching, based on the power of peer
influence, allows for the systematic delivery of personalized interventions
based on real anti-tobacco tweets from former smokers. Experimental evaluation
suggests that our system would perform well if deployed. This research offers
opportunities for public health researchers to increase health awareness at
scale. Future work entails deploying the fully operational Notobot system in a
controlled experiment within a public health campaign.",arxiv
http://arxiv.org/abs/1204.1653v1,2012-04-07T16:34:20Z,2012-04-07T16:34:20Z,Machine Cognition Models: EPAM and GPS,"Through history, the human being tried to relay its daily tasks to other
creatures, which was the main reason behind the rise of civilizations. It
started with deploying animals to automate tasks in the field of
agriculture(bulls), transportation (e.g. horses and donkeys), and even
communication (pigeons). Millenniums after, come the Golden age with
""Al-jazari"" and other Muslim inventors, which were the pioneers of automation,
this has given birth to industrial revolution in Europe, centuries after. At
the end of the nineteenth century, a new era was to begin, the computational
era, the most advanced technological and scientific development that is driving
the mankind and the reason behind all the evolutions of science; such as
medicine, communication, education, and physics. At this edge of technology
engineers and scientists are trying to model a machine that behaves the same as
they do, which pushed us to think about designing and implementing ""Things
that-Thinks"", then artificial intelligence was. In this work we will cover each
of the major discoveries and studies in the field of machine cognition, which
are the ""Elementary Perceiver and Memorizer""(EPAM) and ""The General Problem
Solver""(GPS). The First one focus mainly on implementing the human-verbal
learning behavior, while the second one tries to model an architecture that is
able to solve problems generally (e.g. theorem proving, chess playing, and
arithmetic). We will cover the major goals and the main ideas of each model, as
well as comparing their strengths and weaknesses, and finally giving their
fields of applications. And Finally, we will suggest a real life implementation
of a cognitive machine.",arxiv
http://arxiv.org/abs/1812.03078v1,2018-12-07T15:57:54Z,2018-12-07T15:57:54Z,"Evolutionary Games, Complex Networks and Nonlinear Analysis for
  Epileptic Seizures Forecasting","Epileptic seizures detection and forecasting is nowadays widely recognized as
a problem of great significance and social resonance, and still remains an
open, grand challenge. Furthermore, the development of mobile warning systems
and wearable, non invasive, advisory devices are increasingly and strongly
requested, from the patient community and their families and also from
institutional stakeholders. According to the many recent studies, exploiting
machine learning capabilities upon intracranial EEG (iEEG), in this work we
investigate a combination of novel game theory dynamical model on networks for
brain electrical activity and nonlinear time series analysis based on
recurrences quantification. These two methods are then melted together within a
supervised learning scheme and finally, prediction performances are assessed
using EEG scalp datasets, specifically recorded for this study. Our study
achieved mean sensitivity of 70.9% and a mean time in warning of 20.3%, thus
showing an increase of the improvement over chance metric from 42%, reported in
the most recent study, to 50.5%. Moreover, the real time implementation of the
proposed approach is currently under development on a prototype of a wearable
device.",arxiv
http://arxiv.org/abs/2105.12123v1,2021-05-25T09:45:59Z,2021-05-25T09:45:59Z,Photonic extreme learning machine by free-space optical propagation,"Photonic brain-inspired platforms are emerging as novel analog computing
devices, enabling fast and energy-efficient operations for machine learning.
These artificial neural networks generally require tailored optical elements,
such as integrated photonic circuits, engineered diffractive layers,
nanophotonic materials, or time-delay schemes, which are challenging to train
or stabilize. Here we present a neuromorphic photonic scheme - photonic extreme
learning machines - that can be implemented simply by using an optical encoder
and coherent wave propagation in free space. We realize the concept through
spatial light modulation of a laser beam, with the far field that acts as
feature mapping space. We experimentally demonstrated learning from data on
various classification and regression tasks, achieving accuracies comparable to
digital extreme learning machines. Our findings point out an optical machine
learning device that is easy-to-train, energetically efficient, scalable and
fabrication-constraint free. The scheme can be generalized to a plethora of
photonic systems, opening the route to real-time neuromorphic processing of
optical data.",arxiv
http://arxiv.org/abs/2101.09577v1,2021-01-23T20:23:31Z,2021-01-23T20:23:31Z,"ReliefE: Feature Ranking in High-dimensional Spaces via Manifold
  Embeddings","Feature ranking has been widely adopted in machine learning applications such
as high-throughput biology and social sciences. The approaches of the popular
Relief family of algorithms assign importances to features by iteratively
accounting for nearest relevant and irrelevant instances. Despite their high
utility, these algorithms can be computationally expensive and not-well suited
for high-dimensional sparse input spaces. In contrast, recent embedding-based
methods learn compact, low-dimensional representations, potentially
facilitating down-stream learning capabilities of conventional learners. This
paper explores how the Relief branch of algorithms can be adapted to benefit
from (Riemannian) manifold-based embeddings of instance and target spaces,
where a given embedding's dimensionality is intrinsic to the dimensionality of
the considered data set. The developed ReliefE algorithm is faster and can
result in better feature rankings, as shown by our evaluation on 20 real-life
data sets for multi-class and multi-label classification tasks. The utility of
ReliefE for high-dimensional data sets is ensured by its implementation that
utilizes sparse matrix algebraic operations. Finally, the relation of ReliefE
to other ranking algorithms is studied via the Fuzzy Jaccard Index.",arxiv
http://arxiv.org/abs/2010.14000v2,2020-12-08T18:04:58Z,2020-10-27T02:19:40Z,"Graph-based Reinforcement Learning for Active Learning in Real Time: An
  Application in Modeling River Networks","Effective training of advanced ML models requires large amounts of labeled
data, which is often scarce in scientific problems given the substantial human
labor and material cost to collect labeled data. This poses a challenge on
determining when and where we should deploy measuring instruments (e.g.,
in-situ sensors) to collect labeled data efficiently. This problem differs from
traditional pool-based active learning settings in that the labeling decisions
have to be made immediately after we observe the input data that come in a time
series. In this paper, we develop a real-time active learning method that uses
the spatial and temporal contextual information to select representative query
samples in a reinforcement learning framework. To reduce the need for large
training data, we further propose to transfer the policy learned from
simulation data which is generated by existing physics-based models. We
demonstrate the effectiveness of the proposed method by predicting streamflow
and water temperature in the Delaware River Basin given a limited budget for
collecting labeled data. We further study the spatial and temporal distribution
of selected samples to verify the ability of this method in selecting
informative samples over space and time.",arxiv
http://arxiv.org/abs/1910.06840v3,2020-01-19T09:18:47Z,2019-10-15T14:58:54Z,A Hybrid Compact Neural Architecture for Visual Place Recognition,"State-of-the-art algorithms for visual place recognition, and related visual
navigation systems, can be broadly split into two categories:
computer-science-oriented models including deep learning or image
retrieval-based techniques with minimal biological plausibility, and
neuroscience-oriented dynamical networks that model temporal properties
underlying spatial navigation in the brain. In this letter, we propose a new
compact and high-performing place recognition model that bridges this divide
for the first time. Our approach comprises two key neural models of these
categories: (1) FlyNet, a compact, sparse two-layer neural network inspired by
brain architectures of fruit flies, Drosophila melanogaster, and (2) a
one-dimensional continuous attractor neural network (CANN). The resulting
FlyNet+CANN network incorporates the compact pattern recognition capabilities
of our FlyNet model with the powerful temporal filtering capabilities of an
equally compact CANN, replicating entirely in a hybrid neural implementation
the functionality that yields high performance in algorithmic localization
approaches like SeqSLAM. We evaluate our model, and compare it to three
state-of-the-art methods, on two benchmark real-world datasets with small
viewpoint variations and extreme environmental changes - achieving 87% AUC
results under day to night transitions compared to 60% for Multi-Process
Fusion, 46% for LoST-X and 1% for SeqSLAM, while being 6.5, 310, and 1.5 times
faster, respectively.",arxiv
http://arxiv.org/abs/1806.03342v1,2018-06-08T20:11:05Z,2018-06-08T20:11:05Z,Discovering Signals from Web Sources to Predict Cyber Attacks,"Cyber attacks are growing in frequency and severity. Over the past year alone
we have witnessed massive data breaches that stole personal information of
millions of people and wide-scale ransomware attacks that paralyzed critical
infrastructure of several countries. Combating the rising cyber threat calls
for a multi-pronged strategy, which includes predicting when these attacks will
occur. The intuition driving our approach is this: during the planning and
preparation stages, hackers leave digital traces of their activities on both
the surface web and dark web in the form of discussions on platforms like
hacker forums, social media, blogs and the like. These data provide predictive
signals that allow anticipating cyber attacks. In this paper, we describe
machine learning techniques based on deep neural networks and autoregressive
time series models that leverage external signals from publicly available Web
sources to forecast cyber attacks. Performance of our framework across ground
truth data over real-world forecasting tasks shows that our methods yield a
significant lift or increase of F1 for the top signals on predicted cyber
attacks. Our results suggest that, when deployed, our system will be able to
provide an effective line of defense against various types of targeted cyber
attacks.",arxiv
http://arxiv.org/abs/2103.06450v2,2021-05-21T18:52:44Z,2021-03-11T04:37:29Z,Full Page Handwriting Recognition via Image to Sequence Extraction,"We present a Neural Network based Handwritten Text Recognition (HTR) model
architecture that can be trained to recognize full pages of handwritten or
printed text without image segmentation. Being based on Image to Sequence
architecture, it can extract text present in an image and then sequence it
correctly without imposing any constraints regarding orientation, layout and
size of text and non-text. Further, it can also be trained to generate
auxiliary markup related to formatting, layout and content. We use character
level vocabulary, thereby enabling language and terminology of any subject. The
model achieves a new state-of-art in paragraph level recognition on the IAM
dataset. When evaluated on scans of real world handwritten free form test
answers - beset with curved and slanted lines, drawings, tables, math,
chemistry and other symbols - it performs better than all commercially
available HTR cloud APIs. It is deployed in production as part of a commercial
web application.",arxiv
http://arxiv.org/abs/1907.06011v3,2020-09-14T19:04:01Z,2019-07-13T05:21:05Z,"Extracting Interpretable Physical Parameters from Spatiotemporal Systems
  using Unsupervised Learning","Experimental data is often affected by uncontrolled variables that make
analysis and interpretation difficult. For spatiotemporal systems, this problem
is further exacerbated by their intricate dynamics. Modern machine learning
methods are particularly well-suited for analyzing and modeling complex
datasets, but to be effective in science, the result needs to be interpretable.
We demonstrate an unsupervised learning technique for extracting interpretable
physical parameters from noisy spatiotemporal data and for building a
transferable model of the system. In particular, we implement a
physics-informed architecture based on variational autoencoders that is
designed for analyzing systems governed by partial differential equations
(PDEs). The architecture is trained end-to-end and extracts latent parameters
that parameterize the dynamics of a learned predictive model for the system. To
test our method, we train our model on simulated data from a variety of PDEs
with varying dynamical parameters that act as uncontrolled variables. Numerical
experiments show that our method can accurately identify relevant parameters
and extract them from raw and even noisy spatiotemporal data (tested with
roughly 10% added noise). These extracted parameters correlate well (linearly
with $R^2 > 0.95$) with the ground truth physical parameters used to generate
the datasets. We then apply this method to nonlinear fiber propagation data,
generated by an ab-initio simulation, to demonstrate its capabilities on a more
realistic dataset. Our method for discovering interpretable latent parameters
in spatiotemporal systems will allow us to better analyze and understand
real-world phenomena and datasets, which often have unknown and uncontrolled
variables that alter the system dynamics and cause varying behaviors that are
difficult to disentangle.",arxiv
http://arxiv.org/abs/2002.05147v1,2020-02-12T18:46:48Z,2020-02-12T18:46:48Z,"Multi-Agent Reinforcement Learning and Human Social Factors in Climate
  Change Mitigation","Many complex real-world problems, such as climate change mitigation, are
intertwined with human social factors. Climate change mitigation, a social
dilemma made difficult by the inherent complexities of human behavior, has an
impact at a global scale. We propose applying multi-agent reinforcement
learning (MARL) in this setting to develop intelligent agents that can
influence the social factors at play in climate change mitigation. There are
ethical, practical, and technical challenges that must be addressed when
deploying MARL in this way. In this paper, we present these challenges and
outline an approach to address them. Understanding how intelligent agents can
be used to impact human social factors is important to prevent their abuse and
can be beneficial in furthering our knowledge of these complex problems as a
whole. The challenges we present are not limited to our specific application
but are applicable to broader MARL. Thus, developing MARL for social factors in
climate change mitigation helps address general problems hindering MARL's
applicability to other real-world problems while also motivating discussion on
the social implications of MARL deployment.",arxiv
http://arxiv.org/abs/2007.13127v1,2020-07-26T13:27:05Z,2020-07-26T13:27:05Z,What Government by Algorithm Might Look Like,"Algocracy is the rule by algorithms. This paper summarises technologies
useful to create algocratic social machines and presents idealistic examples of
their application. In particular, it describes smart contracts and their
implementations, challenges of behaviour mining and prediction, as well as
game-theoretic and AI approaches to mechanism design. The presented idealistic
examples of new algocratic solutions are picked from the reality of a modern
state. The examples are science funding, trade by organisations, regulation of
rental agreements, ranking of significance and sortition. Artificial General
Intelligence is not in the scope of this feasibility study.",arxiv
http://arxiv.org/abs/2008.13369v1,2020-08-31T05:12:57Z,2020-08-31T05:12:57Z,"Introducing Representations of Facial Affect in Automated Multimodal
  Deception Detection","Automated deception detection systems can enhance health, justice, and
security in society by helping humans detect deceivers in high-stakes
situations across medical and legal domains, among others. This paper presents
a novel analysis of the discriminative power of dimensional representations of
facial affect for automated deception detection, along with interpretable
features from visual, vocal, and verbal modalities. We used a video dataset of
people communicating truthfully or deceptively in real-world, high-stakes
courtroom situations. We leveraged recent advances in automated emotion
recognition in-the-wild by implementing a state-of-the-art deep neural network
trained on the Aff-Wild database to extract continuous representations of
facial valence and facial arousal from speakers. We experimented with unimodal
Support Vector Machines (SVM) and SVM-based multimodal fusion methods to
identify effective features, modalities, and modeling approaches for detecting
deception. Unimodal models trained on facial affect achieved an AUC of 80%, and
facial affect contributed towards the highest-performing multimodal approach
(adaptive boosting) that achieved an AUC of 91% when tested on speakers who
were not part of training sets. This approach achieved a higher AUC than
existing automated machine learning approaches that used interpretable visual,
vocal, and verbal features to detect deception in this dataset, but did not use
facial affect. Across all videos, deceptive and truthful speakers exhibited
significant differences in facial valence and facial arousal, contributing
computational support to existing psychological theories on affect and
deception. The demonstrated importance of facial affect in our models informs
and motivates the future development of automated, affect-aware machine
learning approaches for modeling and detecting deception and other social
behaviors in-the-wild.",arxiv
http://arxiv.org/abs/1704.02012v1,2017-04-06T20:25:36Z,2017-04-06T20:25:36Z,"A Software-equivalent SNN Hardware using RRAM-array for Asynchronous
  Real-time Learning","Spiking Neural Network (SNN) naturally inspires hardware implementation as it
is based on biology. For learning, spike time dependent plasticity (STDP) may
be implemented using an energy efficient waveform superposition on memristor
based synapse. However, system level implementation has three challenges.
First, a classic dilemma is that recognition requires current reading for short
voltage$-$spikes which is disturbed by large voltage$-$waveforms that are
simultaneously applied on the same memristor for real$-$time learning i.e. the
simultaneous read$-$write dilemma. Second, the hardware needs to exactly
replicate software implementation for easy adaptation of algorithm to hardware.
Third, the devices used in hardware simulations must be realistic. In this
paper, we present an approach to address the above concerns. First, the
learning and recognition occurs in separate arrays simultaneously in
real$-$time, asynchronously $-$ avoiding non$-$biomimetic clocking based
complex signal management. Second, we show that the hardware emulates software
at every stage by comparison of SPICE (circuit$-$simulator) with MATLAB
(mathematical SNN algorithm implementation in software) implementations. As an
example, the hardware shows 97.5 per cent accuracy in classification which is
equivalent to software for a Fisher$-$Iris dataset. Third, the STDP is
implemented using a model of synaptic device implemented using HfO2 memristor.
We show that an increasingly realistic memristor model slightly reduces the
hardware performance (85 per cent), which highlights the need to engineer RRAM
characteristics specifically for SNN.",arxiv
http://arxiv.org/abs/1809.02797v2,2018-09-15T08:31:50Z,2018-09-08T13:08:26Z,Fast Gradient Attack on Network Embedding,"Network embedding maps a network into a low-dimensional Euclidean space, and
thus facilitate many network analysis tasks, such as node classification, link
prediction and community detection etc, by utilizing machine learning methods.
In social networks, we may pay special attention to user privacy, and would
like to prevent some target nodes from being identified by such network
analysis methods in certain cases. Inspired by successful adversarial attack on
deep learning models, we propose a framework to generate adversarial networks
based on the gradient information in Graph Convolutional Network (GCN). In
particular, we extract the gradient of pairwise nodes based on the adversarial
network, and select the pair of nodes with maximum absolute gradient to realize
the Fast Gradient Attack (FGA) and update the adversarial network. This process
is implemented iteratively and terminated until certain condition is satisfied,
i.e., the number of modified links reaches certain predefined value.
Comprehensive attacks, including unlimited attack, direct attack and indirect
attack, are performed on six well-known network embedding methods. The
experiments on real-world networks suggest that our proposed FGA behaves better
than some baseline methods, i.e., the network embedding can be easily disturbed
using FGA by only rewiring few links, achieving state-of-the-art attack
performance.",arxiv
http://arxiv.org/abs/2103.00740v3,2021-03-03T03:26:02Z,2021-03-01T04:13:21Z,"Towards Enhancing Database Education: Natural Language Generation Meets
  Query Execution Plans","The database systems course is offered as part of an undergraduate computer
science degree program in many major universities. A key learning goal of
learners taking such a course is to understand how SQL queries are processed in
a RDBMS in practice. Since a query execution plan (QEP) describes the execution
steps of a query, learners can acquire the understanding by perusing the QEPs
generated by a RDBMS. Unfortunately, in practice, it is often daunting for a
learner to comprehend these QEPs containing vendor-specific implementation
details, hindering her learning process. In this paper, we present a novel,
end-to-end, generic system called lantern that generates a natural language
description of a qep to facilitate understanding of the query execution steps.
It takes as input an SQL query and its QEP, and generates a natural language
description of the execution strategy deployed by the underlying RDBMS.
Specifically, it deploys a declarative framework called pool that enables
subject matter experts to efficiently create and maintain natural language
descriptions of physical operators used in QEPs. A rule-based framework called
RULE-LANTERN is proposed that exploits pool to generate natural language
descriptions of QEPs. Despite the high accuracy of RULE-LANTERN, our engagement
with learners reveal that, consistent with existing psychology theories,
perusing such rule-based descriptions lead to boredom due to repetitive
statements across different QEPs. To address this issue, we present a novel
deep learning-based language generation framework called NEURAL-LANTERN that
infuses language variability in the generated description by exploiting a set
of paraphrasing tools and word embedding. Our experimental study with real
learners shows the effectiveness of lantern in facilitating comprehension of
QEPs.",arxiv
http://arxiv.org/abs/1906.10910v2,2019-07-01T09:03:34Z,2019-06-26T08:37:44Z,"Creating A Neural Pedagogical Agent by Jointly Learning to Review and
  Assess","Machine learning plays an increasing role in intelligent tutoring systems as
both the amount of data available and specialization among students grow.
Nowadays, these systems are frequently deployed on mobile applications. Users
on such mobile education platforms are dynamic, frequently being added,
accessing the application with varying levels of focus, and changing while
using the service. The education material itself, on the other hand, is often
static and is an exhaustible resource whose use in tasks such as problem
recommendation must be optimized. The ability to update user models with
respect to educational material in real-time is thus essential; however,
existing approaches require time-consuming re-training of user features
whenever new data is added. In this paper, we introduce a neural pedagogical
agent for real-time user modeling in the task of predicting user response
correctness, a central task for mobile education applications. Our model,
inspired by work in natural language processing on sequence modeling and
machine translation, updates user features in real-time via bidirectional
recurrent neural networks with an attention mechanism over embedded
question-response pairs. We experiment on the mobile education application
SantaTOEIC, which has 559k users, 66M response data points as well as a set of
10k study problems each expert-annotated with topic tags and gathered since
2016. Our model outperforms existing approaches over several metrics in
predicting user response correctness, notably out-performing other methods on
new users without large question-response histories. Additionally, our
attention mechanism and annotated tag set allow us to create an interpretable
education platform, with a smart review system that addresses the
aforementioned issue of varied user attention and problem exhaustion.",arxiv
http://arxiv.org/abs/2005.05287v2,2020-05-25T12:16:12Z,2020-05-11T17:40:58Z,"Using Computer Vision to enhance Safety of Workforce in Manufacturing in
  a Post COVID World","The COVID-19 pandemic forced governments across the world to impose lockdowns
to prevent virus transmissions. This resulted in the shutdown of all economic
activity and accordingly the production at manufacturing plants across most
sectors was halted. While there is an urgency to resume production, there is an
even greater need to ensure the safety of the workforce at the plant site.
Reports indicate that maintaining social distancing and wearing face masks
while at work clearly reduces the risk of transmission. We decided to use
computer vision on CCTV feeds to monitor worker activity and detect violations
which trigger real time voice alerts on the shop floor. This paper describes an
efficient and economic approach of using AI to create a safe environment in a
manufacturing setup. We demonstrate our approach to build a robust social
distancing measurement algorithm using a mix of modern-day deep learning and
classic projective geometry techniques. We have deployed our solution at
manufacturing plants across the Aditya Birla Group (ABG). We have also
described our face mask detection approach which provides a high accuracy
across a range of customized masks.",arxiv
http://arxiv.org/abs/2006.06865v1,2020-06-11T22:58:36Z,2020-06-11T22:58:36Z,Exploring Algorithmic Fairness in Robust Graph Covering Problems,"Fueled by algorithmic advances, AI algorithms are increasingly being deployed
in settings subject to unanticipated challenges with complex social effects.
Motivated by real-world deployment of AI driven, social-network based suicide
prevention and landslide risk management interventions, this paper focuses on
robust graph covering problems subject to group fairness constraints. We show
that, in the absence of fairness constraints, state-of-the-art algorithms for
the robust graph covering problem result in biased node coverage: they tend to
discriminate individuals (nodes) based on membership in traditionally
marginalized groups. To mitigate this issue, we propose a novel formulation of
the robust graph covering problem with group fairness constraints and a
tractable approximation scheme applicable to real-world instances. We provide a
formal analysis of the price of group fairness (PoF) for this problem, where we
show that uncertainty can lead to greater PoF. We demonstrate the effectiveness
of our approach on several real-world social networks. Our method yields
competitive node coverage while significantly improving group fairness relative
to state-of-the-art methods.",arxiv
http://arxiv.org/abs/2108.11579v1,2021-08-26T05:00:27Z,2021-08-26T05:00:27Z,Modeling Item Response Theory with Stochastic Variational Inference,"Item Response Theory (IRT) is a ubiquitous model for understanding human
behaviors and attitudes based on their responses to questions. Large modern
datasets offer opportunities to capture more nuances in human behavior,
potentially improving psychometric modeling leading to improved scientific
understanding and public policy. However, while larger datasets allow for more
flexible approaches, many contemporary algorithms for fitting IRT models may
also have massive computational demands that forbid real-world application. To
address this bottleneck, we introduce a variational Bayesian inference
algorithm for IRT, and show that it is fast and scalable without sacrificing
accuracy. Applying this method to five large-scale item response datasets from
cognitive science and education yields higher log likelihoods and higher
accuracy in imputing missing data than alternative inference algorithms. Using
this new inference approach we then generalize IRT with expressive Bayesian
models of responses, leveraging recent advances in deep learning to capture
nonlinear item characteristic curves (ICC) with neural networks. Using an
eigth-grade mathematics test from TIMSS, we show our nonlinear IRT models can
capture interesting asymmetric ICCs. The algorithm implementation is
open-source, and easily usable.",arxiv
http://arxiv.org/abs/1908.04387v3,2019-09-10T17:46:09Z,2019-08-05T02:59:18Z,"Mass Estimation from Images using Deep Neural Network and Sparse Ground
  Truth","Supervised learning is the workhorse for regression and classification tasks,
but the standard approach presumes ground truth for every measurement. In real
world applications, limitations due to expense or general in-feasibility due to
the specific application are common. In the context of agriculture
applications, yield monitoring is one such example where simple-physics based
measurements such as volume or force-impact have been used to quantify mass
flow, which incur error due to sensor calibration. By utilizing semi-supervised
deep learning with gradient aggregation and a sequence of images, in this work
we can accurately estimate a physical quantity (mass) with complex data
structures and sparse ground truth. Using a vision system capturing images of a
sugarcane elevator and running bamboo under controlled testing as a surrogate
material to harvesting sugarcane, mass is accurately predicted from images by
training a DNN using only final load weights. The DNN succeeds in capturing the
complex density physics of random stacking of slender rods internally as part
of the mass prediction model, and surpasses older volumetric-based methods for
mass prediction. Furthermore, by incorporating knowledge about the system
physics through the DNN architecture and penalty terms, improvements in
prediction accuracy and stability, as well as faster learning are obtained. It
is shown that the classic nonlinear regression optimization can be reformulated
with an aggregation term with some independence assumptions to achieve this
feat. Since the number of images for any given run are too large to fit on
typical GPU vRAM, an implementation is shown that compensates for the limited
memory but still achieve fast training times. The same approach presented
herein could be applied to other applications like yield monitoring on grain
combines or other harvesters using vision or other instrumentation.",arxiv
http://arxiv.org/abs/2002.01129v3,2021-07-12T21:18:32Z,2020-02-04T05:08:17Z,Bayesian Meta-Prior Learning Using Empirical Bayes,"Adding domain knowledge to a learning system is known to improve results. In
multi-parameter Bayesian frameworks, such knowledge is incorporated as a prior.
On the other hand, various model parameters can have different learning rates
in real-world problems, especially with skewed data. Two often-faced challenges
in Operation Management and Management Science applications are the absence of
informative priors, and the inability to control parameter learning rates. In
this study, we propose a hierarchical Empirical Bayes approach that addresses
both challenges, and that can generalize to any Bayesian framework. Our method
learns empirical meta-priors from the data itself and uses them to decouple the
learning rates of first-order and second-order features (or any other given
feature grouping) in a Generalized Linear Model. As the first-order features
are likely to have a more pronounced effect on the outcome, focusing on
learning first-order weights first is likely to improve performance and
convergence time. Our Empirical Bayes method clamps features in each group
together and uses the deployed model's observed data to empirically compute a
hierarchical prior in hindsight. We report theoretical results for the
unbiasedness, strong consistency, and optimal frequentist cumulative regret
properties of our meta-prior variance estimator. We apply our method to a
standard supervised learning optimization problem, as well as an online
combinatorial optimization problem in a contextual bandit setting implemented
in an Amazon production system. Both during simulations and live experiments,
our method shows marked improvements, especially in cases of small traffic. Our
findings are promising, as optimizing over sparse data is often a challenge.",arxiv
http://arxiv.org/abs/2012.11696v2,2021-06-19T00:16:56Z,2020-12-21T21:48:18Z,"Image Captioning as an Assistive Technology: Lessons Learned from VizWiz
  2020 Challenge","Image captioning has recently demonstrated impressive progress largely owing
to the introduction of neural network algorithms trained on curated dataset
like MS-COCO. Often work in this field is motivated by the promise of
deployment of captioning systems in practical applications. However, the
scarcity of data and contexts in many competition datasets renders the utility
of systems trained on these datasets limited as an assistive technology in
real-world settings, such as helping visually impaired people navigate and
accomplish everyday tasks. This gap motivated the introduction of the novel
VizWiz dataset, which consists of images taken by the visually impaired and
captions that have useful, task-oriented information. In an attempt to help the
machine learning computer vision field realize its promise of producing
technologies that have positive social impact, the curators of the VizWiz
dataset host several competitions, including one for image captioning. This
work details the theory and engineering from our winning submission to the 2020
captioning competition. Our work provides a step towards improved assistive
image captioning systems.",arxiv
http://arxiv.org/abs/2001.00660v1,2020-01-02T22:56:15Z,2020-01-02T22:56:15Z,A Parallel Sparse Tensor Benchmark Suite on CPUs and GPUs,"Tensor computations present significant performance challenges that impact a
wide spectrum of applications ranging from machine learning, healthcare
analytics, social network analysis, data mining to quantum chemistry and signal
processing. Efforts to improve the performance of tensor computations include
exploring data layout, execution scheduling, and parallelism in common tensor
kernels. This work presents a benchmark suite for arbitrary-order sparse tensor
kernels using state-of-the-art tensor formats: coordinate (COO) and
hierarchical coordinate (HiCOO) on CPUs and GPUs. It presents a set of
reference tensor kernel implementations that are compatible with real-world
tensors and power law tensors extended from synthetic graph generation
techniques. We also propose Roofline performance models for these kernels to
provide insights of computer platforms from sparse tensor view.",arxiv
http://arxiv.org/abs/2107.13473v2,2021-07-30T20:49:01Z,2021-07-28T16:29:58Z,"The Portiloop: a deep learning-based open science tool for closed-loop
  brain stimulation","Electroencephalography (EEG) is a method of measuring the brain's electrical
activity, using non-invasive scalp electrodes. In this article, we propose the
Portiloop, a deep learning-based portable and low-cost device enabling the
neuroscience community to capture EEG, process it in real time, detect patterns
of interest, and respond with precisely-timed stimulation. The core of the
Portiloop is a System on Chip composed of an Analog to Digital Converter (ADC)
and a Field-Programmable Gate Array (FPGA). After being converted to digital by
the ADC, the EEG signal is processed in the FPGA. The FPGA contains an ad-hoc
Artificial Neural Network (ANN) with convolutional and recurrent units,
directly implemented in hardware. The output of the ANN is then used to trigger
the user-defined feedback. We use the Portiloop to develop a real-time sleep
spindle stimulating application, as a case study. Sleep spindles are a specific
type of transient oscillation ($\sim$2.5 s, 12-16 Hz) that are observed in EEG
recordings, and are related to memory consolidation during sleep. We tested the
Portiloop's capacity to detect and stimulate sleep spindles in real time using
an existing database of EEG sleep recordings. With 71% for both precision and
recall as compared with expert labels, the system is able to stimulate spindles
within $\sim$300 ms of their onset, enabling experimental manipulation of early
the entire spindle. The Portiloop can be extended to detect and stimulate other
neural events in EEG. It is fully available to the research community as an
open science project.",arxiv
http://arxiv.org/abs/1908.10407v2,2019-09-02T10:43:50Z,2019-08-27T18:50:50Z,"An Energy Approach to the Solution of Partial Differential Equations in
  Computational Mechanics via Machine Learning: Concepts, Implementation and
  Applications","Partial Differential Equations (PDE) are fundamental to model different
phenomena in science and engineering mathematically. Solving them is a crucial
step towards a precise knowledge of the behaviour of natural and engineered
systems. In general, in order to solve PDEs that represent real systems to an
acceptable degree, analytical methods are usually not enough. One has to resort
to discretization methods. For engineering problems, probably the best known
option is the finite element method (FEM). However, powerful alternatives such
as mesh-free methods and Isogeometric Analysis (IGA) are also available. The
fundamental idea is to approximate the solution of the PDE by means of
functions specifically built to have some desirable properties. In this
contribution, we explore Deep Neural Networks (DNNs) as an option for
approximation. They have shown impressive results in areas such as visual
recognition. DNNs are regarded here as function approximation machines. There
is great flexibility to define their structure and important advances in the
architecture and the efficiency of the algorithms to implement them make DNNs a
very interesting alternative to approximate the solution of a PDE. We
concentrate in applications that have an interest for Computational Mechanics.
Most contributions that have decided to explore this possibility have adopted a
collocation strategy. In this contribution, we concentrate in mechanical
problems and analyze the energetic format of the PDE. The energy of a
mechanical system seems to be the natural loss function for a machine learning
method to approach a mechanical problem. As proofs of concept, we deal with
several problems and explore the capabilities of the method for applications in
engineering.",arxiv
http://arxiv.org/abs/2110.06196v1,2021-10-12T17:49:46Z,2021-10-12T17:49:46Z,GraPE: fast and scalable Graph Processing and Embedding,"Graph Representation Learning methods have enabled a wide range of learning
problems to be addressed for data that can be represented in graph form.
Nevertheless, several real world problems in economy, biology, medicine and
other fields raised relevant scaling problems with existing methods and their
software implementation, due to the size of real world graphs characterized by
millions of nodes and billions of edges. We present GraPE, a software resource
for graph processing and random walk based embedding, that can scale with large
and high-degree graphs and significantly speed up-computation. GraPE comprises
specialized data structures, algorithms, and a fast parallel implementation
that displays everal orders of magnitude improvement in empirical space and
time complexity compared to state of the art software resources, with a
corresponding boost in the performance of machine learning methods for edge and
node label prediction and for the unsupervised analysis of graphs.GraPE is
designed to run on laptop and desktop computers, as well as on high performance
computing clusters",arxiv
http://arxiv.org/abs/2001.05982v2,2020-06-04T15:13:47Z,2020-01-16T18:32:19Z,"A Common Operating Picture Framework Leveraging Data Fusion and Deep
  Learning","Organizations are starting to realize of the combined power of data and
data-driven algorithmic models to gain insights, situational awareness, and
advance their mission. A common challenge to gaining insights is connecting
inherently different datasets. These datasets (e.g. geocoded features, video
streams, raw text, social network data, etc.) per separate they provide very
narrow answers; however collectively they can provide new capabilities. In this
work, we present a data fusion framework for accelerating solutions for
Processing, Exploitation, and Dissemination (PED). Our platform is a collection
of services that extract information from several data sources (per separate)
by leveraging deep learning and other means of processing. This information is
fused by a set of analytical engines that perform data correlations, searches,
and other modeling operations to combine information from the disparate data
sources. As a result, events of interest are detected, geolocated, logged, and
presented into a common operating picture. This common operating picture allows
the user to visualize in real time all the data sources, per separate and their
collective cooperation. In addition, forensic activities have been implemented
and made available through the framework. Users can review archived results and
compare them to the most recent snapshot of the operational environment. In our
first iteration we have focused on visual data (FMV, WAMI, CCTV/PTZ-Cameras,
open source video, etc.) and AIS data streams (satellite and terrestrial
sources). As a proof-of-concept, in our experiments we show how FMV detections
can be combined with vessel tracking signals from AIS sources to confirm
identity, tip-and-cue aerial reconnaissance, and monitor vessel activity in an
area.",arxiv
http://arxiv.org/abs/2004.00104v1,2020-03-31T20:58:14Z,2020-03-31T20:58:14Z,"Improvement of electronic Governance and mobile Governance in
  Multilingual Countries with Digital Etymology using Sanskrit Grammar","With huge improvement of digital connectivity (Wifi,3G,4G) and digital
devices access to internet has reached in the remotest corners now a days.
Rural people can easily access web or apps from PDAs, laptops, smartphones etc.
This is an opportunity of the Government to reach to the citizen in large
number, get their feedback, associate them in policy decision with e governance
without deploying huge man, material or resourses. But the Government of
multilingual countries face a lot of problem in successful implementation of
Government to Citizen (G2C) and Citizen to Government (C2G) governance as the
rural people tend and prefer to interact in their native languages. Presenting
equal experience over web or app to different language group of speakers is a
real challenge. In this research we have sorted out the problems faced by Indo
Aryan speaking netizens which is in general also applicable to any language
family groups or subgroups. Then we have tried to give probable solutions using
Etymology. Etymology is used to correlate the words using their ROOT forms. In
5th century BC Panini wrote Astadhyayi where he depicted sutras or rules -- how
a word is changed according to person,tense,gender,number etc. Later this book
was followed in Western countries also to derive their grammar of comparatively
new languages. We have trained our system for automatic root extraction from
the surface level or morphed form of words using Panian Gramatical rules. We
have tested our system over 10000 bengali Verbs and extracted the root form
with 98% accuracy. We are now working to extend the program to successfully
lemmatize any words of any language and correlate them by applying those rule
sets in Artificial Neural Network.",arxiv
http://arxiv.org/abs/1903.05575v1,2019-03-13T16:19:27Z,2019-03-13T16:19:27Z,"On the Efficacy and High-Performance Implementation of Quaternion Matrix
  Multiplication","Quaternion symmetry is ubiquitous in the physical sciences. As such, much
work has been afforded over the years to the development of efficient schemes
to exploit this symmetry using real and complex linear algebra. Recent years
have also seen many advances in the formal theoretical development of
explicitly quaternion linear algebra with promising applications in image
processing and machine learning. Despite these advances, there do not currently
exist optimized software implementations of quaternion linear algebra. The
leverage of optimized linear algebra software is crucial in the achievement of
high levels of performance on modern computing architectures, and thus provides
a central tool in the development of high-performance scientific software. In
this work, a case will be made for the efficacy of high-performance quaternion
linear algebra software for appropriate problems. In this pursuit, an optimized
software implementation of quaternion matrix multiplication will be presented
and will be shown to outperform a vendor tuned implementation for the analogous
complex matrix operation. The results of this work pave the path for further
development of high-performance quaternion linear algebra software which will
improve the performance of the next generation of applicable scientific
applications.",arxiv
http://arxiv.org/abs/2012.07938v1,2020-12-14T20:55:48Z,2020-12-14T20:55:48Z,NVIDIA SimNet^{TM}: an AI-accelerated multi-physics simulation framework,"We present SimNet, an AI-driven multi-physics simulation framework, to
accelerate simulations across a wide range of disciplines in science and
engineering. Compared to traditional numerical solvers, SimNet addresses a wide
range of use cases - coupled forward simulations without any training data,
inverse and data assimilation problems. SimNet offers fast turnaround time by
enabling parameterized system representation that solves for multiple
configurations simultaneously, as opposed to the traditional solvers that solve
for one configuration at a time. SimNet is integrated with parameterized
constructive solid geometry as well as STL modules to generate point clouds.
Furthermore, it is customizable with APIs that enable user extensions to
geometry, physics and network architecture. It has advanced network
architectures that are optimized for high-performance GPU computing, and offers
scalable performance for multi-GPU and multi-Node implementation with
accelerated linear algebra as well as FP32, FP64 and TF32 computations. In this
paper we review the neural network solver methodology, the SimNet architecture,
and the various features that are needed for effective solution of the PDEs. We
present real-world use cases that range from challenging forward multi-physics
simulations with turbulence and complex 3D geometries, to industrial design
optimization and inverse problems that are not addressed efficiently by the
traditional solvers. Extensive comparisons of SimNet results with open source
and commercial solvers show good correlation.",arxiv
http://arxiv.org/abs/1809.07763v4,2020-05-26T15:15:19Z,2018-09-19T19:14:46Z,"auditor: an R Package for Model-Agnostic Visual Validation and
  Diagnostics","Machine learning models have spread to almost every area of life. They are
successfully applied in biology, medicine, finance, physics, and other fields.
With modern software it is easy to train even a~complex model that fits the
training data and results in high accuracy on the test set. The problem arises
when models fail confronted with real-world data.
  This paper describes methodology and tools for model-agnostic audit.
Introduced techniques facilitate assessing and comparing the goodness of fit
and performance of models. In~addition, they may be used for the analysis of
the similarity of residuals and for identification of~outliers and influential
observations. The examination is carried out by diagnostic scores and visual
verification.
  Presented methods were implemented in the auditor package for R. Due to
flexible and~consistent grammar, it is simple to validate models of any
classes.",arxiv
http://arxiv.org/abs/2105.13598v3,2021-08-17T06:31:58Z,2021-05-28T05:54:59Z,End-to-End Deep Fault Tolerant Control,"Ideally, accurate sensor measurements are needed to achieve a good
performance in the closed-loop control of mechatronic systems. As a
consequence, sensor faults will prevent the system from working correctly,
unless a fault-tolerant control (FTC) architecture is adopted. As model-based
FTC algorithms for nonlinear systems are often challenging to design, this
paper focuses on a new method for FTC in the presence of sensor faults, based
on deep learning. The considered approach replaces the phases of fault
detection and isolation and controller design with a single recurrent neural
network, which has the value of past sensor measurements in a given time window
as input, and the current values of the control variables as output. This
end-to-end deep FTC method is applied to a mechatronic system composed of a
spherical inverted pendulum, whose configuration is changed via reaction
wheels, in turn actuated by electric motors. The simulation and experimental
results show that the proposed method can handle abrupt faults occurring in
link position/velocity sensors. The provided supplementary material includes a
video of real-world experiments and the software source code.",arxiv
http://arxiv.org/abs/1603.06212v1,2016-03-20T13:32:27Z,2016-03-20T13:32:27Z,"Evaluation of a Tree-based Pipeline Optimization Tool for Automating
  Data Science","As the field of data science continues to grow, there will be an
ever-increasing demand for tools that make machine learning accessible to
non-experts. In this paper, we introduce the concept of tree-based pipeline
optimization for automating one of the most tedious parts of machine
learning---pipeline design. We implement an open source Tree-based Pipeline
Optimization Tool (TPOT) in Python and demonstrate its effectiveness on a
series of simulated and real-world benchmark data sets. In particular, we show
that TPOT can design machine learning pipelines that provide a significant
improvement over a basic machine learning analysis while requiring little to no
input nor prior knowledge from the user. We also address the tendency for TPOT
to design overly complex pipelines by integrating Pareto optimization, which
produces compact pipelines without sacrificing classification accuracy. As
such, this work represents an important step toward fully automating machine
learning pipeline design.",arxiv
http://arxiv.org/abs/2004.02569v1,2020-04-06T11:32:37Z,2020-04-06T11:32:37Z,"Gradient-Based Training and Pruning of Radial Basis Function Networks
  with an Application in Materials Physics","Many applications, especially in physics and other sciences, call for easily
interpretable and robust machine learning techniques. We propose a fully
gradient-based technique for training radial basis function networks with an
efficient and scalable open-source implementation. We derive novel closed-form
optimization criteria for pruning the models for continuous as well as binary
data which arise in a challenging real-world material physics problem. The
pruned models are optimized to provide compact and interpretable versions of
larger models based on informed assumptions about the data distribution.
Visualizations of the pruned models provide insight into the atomic
configurations that determine atom-level migration processes in solid matter;
these results may inform future research on designing more suitable descriptors
for use with machine learning algorithms.",arxiv
http://arxiv.org/abs/1107.5462v1,2011-07-27T13:07:39Z,2011-07-27T13:07:39Z,HyFlex: A Benchmark Framework for Cross-domain Heuristic Search,"Automating the design of heuristic search methods is an active research field
within computer science, artificial intelligence and operational research. In
order to make these methods more generally applicable, it is important to
eliminate or reduce the role of the human expert in the process of designing an
effective methodology to solve a given computational search problem.
Researchers developing such methodologies are often constrained on the number
of problem domains on which to test their adaptive, self-configuring
algorithms; which can be explained by the inherent difficulty of implementing
their corresponding domain specific software components.
  This paper presents HyFlex, a software framework for the development of
cross-domain search methodologies. The framework features a common software
interface for dealing with different combinatorial optimisation problems, and
provides the algorithm components that are problem specific. In this way, the
algorithm designer does not require a detailed knowledge the problem domains,
and thus can concentrate his/her efforts in designing adaptive general-purpose
heuristic search algorithms. Four hard combinatorial problems are fully
implemented (maximum satisfiability, one dimensional bin packing, permutation
flow shop and personnel scheduling), each containing a varied set of instance
data (including real-world industrial applications) and an extensive set of
problem specific heuristics and search operators. The framework forms the basis
for the first International Cross-domain Heuristic Search Challenge (CHeSC),
and it is currently in use by the international research community. In summary,
HyFlex represents a valuable new benchmark of heuristic search generality, with
which adaptive cross-domain algorithms are being easily developed, and reliably
compared.",arxiv
http://arxiv.org/abs/1910.11779v1,2019-10-25T15:03:11Z,2019-10-25T15:03:11Z,"Toward a better trade-off between performance and fairness with
  kernel-based distribution matching","As recent literature has demonstrated how classifiers often carry unintended
biases toward some subgroups, deploying machine learned models to users demands
careful consideration of the social consequences. How should we address this
problem in a real-world system? How should we balance core performance and
fairness metrics? In this paper, we introduce a MinDiff framework for
regularizing classifiers toward different fairness metrics and analyze a
technique with kernel-based statistical dependency tests. We run a thorough
study on an academic dataset to compare the Pareto frontier achieved by
different regularization approaches, and apply our kernel-based method to two
large-scale industrial systems demonstrating real-world improvements.",arxiv
http://arxiv.org/abs/1804.10134v2,2018-07-28T10:33:47Z,2018-04-26T16:04:30Z,Detection-Tracking for Efficient Person Analysis: The DetTA Pipeline,"In the past decade many robots were deployed in the wild, and people
detection and tracking is an important component of such deployments. On top of
that, one often needs to run modules which analyze persons and extract higher
level attributes such as age and gender, or dynamic information like gaze and
pose. The latter ones are especially necessary for building a reactive, social
robot-person interaction.
  In this paper, we combine those components in a fully modular
detection-tracking-analysis pipeline, called DetTA. We investigate the benefits
of such an integration on the example of head and skeleton pose, by using the
consistent track ID for a temporal filtering of the analysis modules'
observations, showing a slight improvement in a challenging real-world
scenario. We also study the potential of a so-called ""free-flight"" mode, where
the analysis of a person attribute only relies on the filter's predictions for
certain frames. Here, our study shows that this boosts the runtime
dramatically, while the prediction quality remains stable. This insight is
especially important for reducing power consumption and sharing precious
(GPU-)memory when running many analysis components on a mobile platform,
especially so in the era of expensive deep learning methods.",arxiv
http://arxiv.org/abs/2012.10610v3,2021-02-16T17:31:15Z,2020-12-19T07:00:09Z,"SpaceML: Distributed Open-source Research with Citizen Scientists for
  the Advancement of Space Technology for NASA","Traditionally, academic labs conduct open-ended research with the primary
focus on discoveries with long-term value, rather than direct products that can
be deployed in the real world. On the other hand, research in the industry is
driven by its expected commercial return on investment, and hence focuses on a
real world product with short-term timelines. In both cases, opportunity is
selective, often available to researchers with advanced educational
backgrounds. Research often happens behind closed doors and may be kept
confidential until either its publication or product release, exacerbating the
problem of AI reproducibility and slowing down future research by others in the
field. As many research organizations tend to exclusively focus on specific
areas, opportunities for interdisciplinary research reduce. Undertaking
long-term bold research in unexplored fields with non-commercial yet great
public value is hard due to factors including the high upfront risk, budgetary
constraints, and a lack of availability of data and experts in niche fields.
Only a few companies or well-funded research labs can afford to do such
long-term research. With research organizations focused on an exploding array
of fields and resources spread thin, opportunities for the maturation of
interdisciplinary research reduce. Apart from these exigencies, there is also a
need to engage citizen scientists through open-source contributors to play an
active part in the research dialogue. We present a short case study of SpaceML,
an extension of the Frontier Development Lab, an AI accelerator for NASA.
SpaceML distributes open-source research and invites volunteer citizen
scientists to partake in development and deployment of high social value
products at the intersection of space and AI.",arxiv
http://arxiv.org/abs/1910.07083v1,2019-09-29T22:04:19Z,2019-09-29T22:04:19Z,"Occurence of A Cyber Security Eco-System: A Nature Oriented Project and
  Evaluation of An Indirect Social Experiment","Because of todays technological developments and the influence of digital
systems into every aspect of our lives, importance of cyber security improves
more and more day-by-day. Projects, educational processes and seminars realized
for this aim create and improve awareness among individuals and provide useful
tools for growing equipped generations. The aim of this study is to focus on a
cyber security eco-system, which was self-occurred within the interactive
educational environment designed under the scope of TUBITAK 4004 Nature
Education and Science Schools Projects (with the name of A Cyber Security
Adventure) with the use of important technologies such as virtual reality,
augmented reality, and artificial intelligence. The eco-system occurred within
the interactive educational process where high school students took place
caused both students and the project team to experience an indirect social
experiment environment. In this sense, it is thought that the findings and
comments presented in the study will give important ideas to everyone involved
in cyber security education, life-long learning processes, and the technology
use in software oriented educational tools.",arxiv
http://arxiv.org/abs/1601.07925v1,2016-01-28T21:45:55Z,2016-01-28T21:45:55Z,"Automating biomedical data science through tree-based pipeline
  optimization","Over the past decade, data science and machine learning has grown from a
mysterious art form to a staple tool across a variety of fields in academia,
business, and government. In this paper, we introduce the concept of tree-based
pipeline optimization for automating one of the most tedious parts of machine
learning---pipeline design. We implement a Tree-based Pipeline Optimization
Tool (TPOT) and demonstrate its effectiveness on a series of simulated and
real-world genetic data sets. In particular, we show that TPOT can build
machine learning pipelines that achieve competitive classification accuracy and
discover novel pipeline operators---such as synthetic feature
constructors---that significantly improve classification accuracy on these data
sets. We also highlight the current challenges to pipeline optimization, such
as the tendency to produce pipelines that overfit the data, and suggest future
research paths to overcome these challenges. As such, this work represents an
early step toward fully automating machine learning pipeline design.",arxiv
http://arxiv.org/abs/2003.10548v1,2020-03-23T21:04:43Z,2020-03-23T21:04:43Z,spsurv: An R package for semi-parametric survival analysis,"Software development innovations and advances in computing have enabled more
complex and less costly computations in medical research (survival analysis),
engineering studies (reliability analysis), and social sciences event analysis
(historical analysis). As a result, many semi-parametric modeling efforts
emerged when it comes to time-to-event data analysis. In this context, this
work presents a flexible Bernstein polynomial (BP) based framework for survival
data modeling. This innovative approach is applied to existing families of
models such as proportional hazards (PH), proportional odds (PO), and
accelerated failure time (AFT) models to estimate unknown baseline functions.
Along with this contribution, this work also presents new automated routines in
R, taking advantage of algorithms available in Stan. The proposed computation
routines are tested and explored through simulation studies based on artificial
datasets. The tools implemented to fit the proposed statistical models are
combined and organized in an R package. Also, the BP based proportional hazards
(BPPH), proportional odds (BPPO), and accelerated failure time (BPAFT) models
are illustrated in real applications related to cancer trial data using maximum
likelihood (ML) estimation and Markov chain Monte Carlo (MCMC) methods.",arxiv
http://arxiv.org/abs/2010.11884v1,2020-10-22T17:20:38Z,2020-10-22T17:20:38Z,"AEGIS: A real-time multimodal augmented reality computer vision based
  system to assist facial expression recognition for individuals with autism
  spectrum disorder","The ability to interpret social cues comes naturally for most people, but for
those living with Autism Spectrum Disorder (ASD), some experience a deficiency
in this area. This paper presents the development of a multimodal augmented
reality (AR) system which combines the use of computer vision and deep
convolutional neural networks (CNN) in order to assist individuals with the
detection and interpretation of facial expressions in social settings. The
proposed system, which we call AEGIS (Augmented-reality Expression Guided
Interpretation System), is an assistive technology deployable on a variety of
user devices including tablets, smartphones, video conference systems, or
smartglasses, showcasing its extreme flexibility and wide range of use cases,
to allow integration into daily life with ease. Given a streaming video camera
source, each real-world frame is passed into AEGIS, processed for facial
bounding boxes, and then fed into our novel deep convolutional time windowed
neural network (TimeConvNet). We leverage both spatial and temporal information
in order to provide an accurate expression prediction, which is then converted
into its corresponding visualization and drawn on top of the original video
frame. The system runs in real-time, requires minimal set up and is simple to
use. With the use of AEGIS, we can assist individuals living with ASD to learn
to better identify expressions and thus improve their social experiences.",arxiv
http://arxiv.org/abs/2106.02964v1,2021-06-05T21:15:34Z,2021-06-05T21:15:34Z,"A Review of Machine Learning Classification Using Quantum Annealing for
  Real-world Applications","Optimizing the training of a machine learning pipeline helps in reducing
training costs and improving model performance. One such optimizing strategy is
quantum annealing, which is an emerging computing paradigm that has shown
potential in optimizing the training of a machine learning model. The
implementation of a physical quantum annealer has been realized by D-Wave
systems and is available to the research community for experiments. Recent
experimental results on a variety of machine learning applications using
quantum annealing have shown interesting results where the performance of
classical machine learning techniques is limited by limited training data and
high dimensional features. This article explores the application of D-Wave's
quantum annealer for optimizing machine learning pipelines for real-world
classification problems. We review the application domains on which a physical
quantum annealer has been used to train machine learning classifiers. We
discuss and analyze the experiments performed on the D-Wave quantum annealer
for applications such as image recognition, remote sensing imagery,
computational biology, and particle physics. We discuss the possible advantages
and the problems for which quantum annealing is likely to be advantageous over
classical computation.",arxiv
http://arxiv.org/abs/1810.03032v1,2018-10-06T17:55:26Z,2018-10-06T17:55:26Z,"Constructing Graph Node Embeddings via Discrimination of Similarity
  Distributions","The problem of unsupervised learning node embeddings in graphs is one of the
important directions in modern network science. In this work we propose a novel
framework, which is aimed to find embeddings by \textit{discriminating
distributions of similarities (DDoS)} between nodes in the graph. The general
idea is implemented by maximizing the \textit{earth mover distance} between
distributions of decoded similarities of similar and dissimilar nodes. The
resulting algorithm generates embeddings which give a state-of-the-art
performance in the problem of link prediction in real-world graphs.",arxiv
http://arxiv.org/abs/2011.11081v1,2020-11-22T18:30:23Z,2020-11-22T18:30:23Z,"Deep learning model trained on mobile phone-acquired frozen section
  images effectively detects basal cell carcinoma","Background: Margin assessment of basal cell carcinoma using the frozen
section is a common task of pathology intraoperative consultation. Although
frequently straight-forward, the determination of the presence or absence of
basal cell carcinoma on the tissue sections can sometimes be challenging. We
explore if a deep learning model trained on mobile phone-acquired frozen
section images can have adequate performance for future deployment. Materials
and Methods: One thousand two hundred and forty-one (1241) images of frozen
sections performed for basal cell carcinoma margin status were acquired using
mobile phones. The photos were taken at 100x magnification (10x objective). The
images were downscaled from a 4032 x 3024 pixel resolution to 576 x 432 pixel
resolution. Semantic segmentation algorithm Deeplab V3 with Xception backbone
was used for model training. Results: The model uses an image as input and
produces a 2-dimensional black and white output of prediction of the same
dimension; the areas determined to be basal cell carcinoma were displayed with
white color, in a black background. Any output with the number of white pixels
exceeding 0.5% of the total number of pixels is deemed positive for basal cell
carcinoma. On the test set, the model achieves area under curve of 0.99 for
receiver operator curve and 0.97 for precision-recall curve at the pixel level.
The accuracy of classification at the slide level is 96%. Conclusions: The deep
learning model trained with mobile phone images shows satisfactory performance
characteristics, and thus demonstrates the potential for deploying as a mobile
phone app to assist in frozen section interpretation in real time.",arxiv
http://arxiv.org/abs/1810.11063v1,2018-10-25T18:42:01Z,2018-10-25T18:42:01Z,Sorry: Ambient Tactical Deception Via Malware-Based Social Engineering,"In this paper we argue, drawing from the perspectives of cybersecurity and
social psychology, that Internet-based manipulation of an individual or group
reality using ambient tactical deception is possible using only software and
changing words in a web browser. We call this attack Ambient Tactical Deception
(ATD). Ambient, in artificial intelligence, describes software that is
""unobtrusive,"" and completely integrated into a user's life. Tactical deception
is an information warfare term for the use of deception on an opposing force.
We suggest that an ATD attack could change the sentiment of text in a web
browser. This could alter the victim's perception of reality by providing
disinformation. Within the limit of online communication, even a pause in
replying to a text can affect how people perceive each other. The outcomes of
an ATD attack could include alienation, upsetting a victim, and influencing
their feelings about an election, a spouse, or a corporation.",arxiv
http://arxiv.org/abs/2003.05861v1,2020-03-12T15:52:49Z,2020-03-12T15:52:49Z,"The Chef's Hat Simulation Environment for Reinforcement-Learning-Based
  Agents","To achieve social interactions within Human-Robot Interaction (HRI)
environments is a very challenging task. Most of the current research focuses
on Wizard-of-Oz approaches, which neglect the recent development of intelligent
robots. On the other hand, real-world scenarios usually do not provide the
necessary control and reproducibility which are needed for learning algorithms.
In this paper, we propose a virtual simulation environment that implements the
Chef's Hat card game, designed to be used in HRI scenarios, to provide a
controllable and reproducible scenario for reinforcement-learning algorithms.",arxiv
http://arxiv.org/abs/1906.01974v3,2020-03-05T16:58:35Z,2019-06-03T22:43:00Z,"Willump: A Statistically-Aware End-to-end Optimizer for Machine Learning
  Inference","Systems for ML inference are widely deployed today, but they typically
optimize ML inference workloads using techniques designed for conventional data
serving workloads and miss critical opportunities to leverage the statistical
nature of ML. In this paper, we present Willump, an optimizer for ML inference
that introduces two statistically-motivated optimizations targeting ML
applications whose performance bottleneck is feature computation. First,
Willump automatically cascades feature computation for classification queries:
Willump classifies most data inputs using only high-value, low-cost features
selected through empirical observations of ML model performance, improving
query performance by up to 5x without statistically significant accuracy loss.
Second, Willump accurately approximates ML top-K queries, discarding
low-scoring inputs with an automatically constructed approximate model and then
ranking the remainder with a more powerful model, improving query performance
by up to 10x with minimal accuracy loss. Willump automatically tunes these
optimizations' parameters to maximize query performance while meeting an
accuracy target. Moreover, Willump complements these statistical optimizations
with compiler optimizations to automatically generate fast inference code for
ML applications. We show that Willump improves the end-to-end performance of
real-world ML inference pipelines curated from major data science competitions
by up to 16x without statistically significant loss of accuracy.",arxiv
http://arxiv.org/abs/1409.7699v3,2018-12-30T17:43:46Z,2014-09-26T20:00:14Z,"The Overlooked Potential of Generalized Linear Models in Astronomy-II:
  Gamma regression and photometric redshifts","Machine learning techniques offer a precious tool box for use within
astronomy to solve problems involving so-called big data. They provide a means
to make accurate predictions about a particular system without prior knowledge
of the underlying physical processes of the data. In this article, and the
companion papers of this series, we present the set of Generalized Linear
Models (GLMs) as a fast alternative method for tackling general astronomical
problems, including the ones related to the machine learning paradigm. To
demonstrate the applicability of GLMs to inherently positive and continuous
physical observables, we explore their use in estimating the photometric
redshifts of galaxies from their multi-wavelength photometry. Using the gamma
family with a log link function we predict redshifts from the PHoto-z Accuracy
Testing simulated catalogue and a subset of the Sloan Digital Sky Survey from
Data Release 10. We obtain fits that result in catastrophic outlier rates as
low as ~1% for simulated and ~2% for real data. Moreover, we can easily obtain
such levels of precision within a matter of seconds on a normal desktop
computer and with training sets that contain merely thousands of galaxies. Our
software is made publicly available as an user-friendly package developed in
Python, R and via an interactive web application
(https://cosmostatisticsinitiative.shinyapps.io/CosmoPhotoz). This software
allows users to apply a set of GLMs to their own photometric catalogues and
generates publication quality plots with minimum effort from the user. By
facilitating their ease of use to the astronomical community, this paper series
aims to make GLMs widely known and to encourage their implementation in future
large-scale projects, such as the Large Synoptic Survey Telescope.",arxiv
http://arxiv.org/abs/1702.01780v1,2017-02-06T20:10:10Z,2017-02-06T20:10:10Z,"Toward the automated analysis of complex diseases in genome-wide
  association studies using genetic programming","Machine learning has been gaining traction in recent years to meet the demand
for tools that can efficiently analyze and make sense of the ever-growing
databases of biomedical data in health care systems around the world. However,
effectively using machine learning methods requires considerable domain
expertise, which can be a barrier of entry for bioinformaticians new to
computational data science methods. Therefore, off-the-shelf tools that make
machine learning more accessible can prove invaluable for bioinformaticians. To
this end, we have developed an open source pipeline optimization tool
(TPOT-MDR) that uses genetic programming to automatically design machine
learning pipelines for bioinformatics studies. In TPOT-MDR, we implement
Multifactor Dimensionality Reduction (MDR) as a feature construction method for
modeling higher-order feature interactions, and combine it with a new expert
knowledge-guided feature selector for large biomedical data sets. We
demonstrate TPOT-MDR's capabilities using a combination of simulated and real
world data sets from human genetics and find that TPOT-MDR significantly
outperforms modern machine learning methods such as logistic regression and
eXtreme Gradient Boosting (XGBoost). We further analyze the best pipeline
discovered by TPOT-MDR for a real world problem and highlight TPOT-MDR's
ability to produce a high-accuracy solution that is also easily interpretable.",arxiv
http://arxiv.org/abs/2008.03226v1,2020-06-28T20:59:03Z,2020-06-28T20:59:03Z,"The Photoswitch Dataset: A Molecular Machine Learning Benchmark for the
  Advancement of Synthetic Chemistry","The space of synthesizable molecules is greater than $10^{60}$, meaning only
a vanishingly small fraction of these molecules have ever been realized in the
lab. In order to prioritize which regions of this space to explore next,
synthetic chemists need access to accurate molecular property predictions.
While great advances in molecular machine learning have been made, there is a
dearth of benchmarks featuring properties that are useful for the synthetic
chemist. Focussing directly on the needs of the synthetic chemist, we introduce
the Photoswitch Dataset, a new benchmark for molecular machine learning where
improvements in model performance can be immediately observed in the throughput
of promising molecules synthesized in the lab. Photoswitches are a versatile
class of molecule for medical and renewable energy applications where a
molecule's efficacy is governed by its electronic transition wavelengths. We
demonstrate superior performance in predicting these wavelengths compared to
both time-dependent density functional theory (TD-DFT), the incumbent first
principles quantum mechanical approach, as well as a panel of human experts.
Our baseline models are currently being deployed in the lab as part of the
decision process for candidate synthesis. It is our hope that this benchmark
can drive real discoveries in photoswitch chemistry and that future benchmarks
can be introduced to pivot learning algorithm development to benefit more
expansive areas of synthetic chemistry.",arxiv
http://arxiv.org/abs/1803.03191v1,2018-03-08T16:33:09Z,2018-03-08T16:33:09Z,"A Bayesian and Machine Learning approach to estimating Influence Model
  parameters for IM-RO","The rise of Online Social Networks (OSNs) has caused an insurmountable amount
of interest from advertisers and researchers seeking to monopolize on its
features. Researchers aim to develop strategies for determining how information
is propagated among users within an OSN that is captured by diffusion or
influence models. We consider the influence models for the IM-RO problem, a
novel formulation to the Influence Maximization (IM) problem based on
implementing Stochastic Dynamic Programming (SDP). In contrast to existing
approaches involving influence spread and the theory of submodular functions,
the SDP method focuses on optimizing clicks and ultimately revenue to
advertisers in OSNs. Existing approaches to influence maximization have been
actively researched over the past decade, with applications to multiple fields,
however, our approach is a more practical variant to the original IM problem.
In this paper, we provide an analysis on the influence models of the IM-RO
problem by conducting experiments on synthetic and real-world datasets. We
propose a Bayesian and Machine Learning approach for estimating the parameters
of the influence models for the (Influence Maximization- Revenue Optimization)
IM-RO problem. We present a Bayesian hierarchical model and implement the
well-known Naive Bayes classifier (NBC), Decision Trees classifier (DTC) and
Random Forest classifier (RFC) on three real-world datasets. Compared to
previous approaches to estimating influence model parameters, our strategy has
the great advantage of being directly implementable in standard software
packages such as WinBUGS/OpenBUGS/JAGS and Apache Spark. We demonstrate the
efficiency and usability of our methods in terms of spreading information and
generating revenue for advertisers in the context of OSNs.",arxiv
http://arxiv.org/abs/2010.11411v2,2020-11-21T08:45:37Z,2020-10-22T03:27:19Z,"Value Cards: An Educational Toolkit for Teaching Social Impacts of
  Machine Learning through Deliberation","Recently, there have been increasing calls for computer science curricula to
complement existing technical training with topics related to Fairness,
Accountability, Transparency, and Ethics. In this paper, we present Value Card,
an educational toolkit to inform students and practitioners of the social
impacts of different machine learning models via deliberation. This paper
presents an early use of our approach in a college-level computer science
course. Through an in-class activity, we report empirical data for the initial
effectiveness of our approach. Our results suggest that the use of the Value
Cards toolkit can improve students' understanding of both the technical
definitions and trade-offs of performance metrics and apply them in real-world
contexts, help them recognize the significance of considering diverse social
values in the development of deployment of algorithmic systems, and enable them
to communicate, negotiate and synthesize the perspectives of diverse
stakeholders. Our study also demonstrates a number of caveats we need to
consider when using the different variants of the Value Cards toolkit. Finally,
we discuss the challenges as well as future applications of our approach.",arxiv
http://arxiv.org/abs/2011.05373v1,2020-11-10T20:06:19Z,2020-11-10T20:06:19Z,"Emergent Reciprocity and Team Formation from Randomized Uncertain Social
  Preferences","Multi-agent reinforcement learning (MARL) has shown recent success in
increasingly complex fixed-team zero-sum environments. However, the real world
is not zero-sum nor does it have fixed teams; humans face numerous social
dilemmas and must learn when to cooperate and when to compete. To successfully
deploy agents into the human world, it may be important that they be able to
understand and help in our conflicts. Unfortunately, selfish MARL agents
typically fail when faced with social dilemmas. In this work, we show evidence
of emergent direct reciprocity, indirect reciprocity and reputation, and team
formation when training agents with randomized uncertain social preferences
(RUSP), a novel environment augmentation that expands the distribution of
environments agents play in. RUSP is generic and scalable; it can be applied to
any multi-agent environment without changing the original underlying game
dynamics or objectives. In particular, we show that with RUSP these behaviors
can emerge and lead to higher social welfare equilibria in both classic
abstract social dilemmas like Iterated Prisoner's Dilemma as well in more
complex intertemporal environments.",arxiv
http://arxiv.org/abs/2106.10131v1,2021-06-18T13:47:56Z,2021-06-18T13:47:56Z,Enhancing user creativity: Semantic measures for idea generation,"Human creativity generates novel ideas to solve real-world problems. This
thereby grants us the power to transform the surrounding world and extend our
human attributes beyond what is currently possible. Creative ideas are not just
new and unexpected, but are also successful in providing solutions that are
useful, efficient and valuable. Thus, creativity optimizes the use of available
resources and increases wealth. The origin of human creativity, however, is
poorly understood, and semantic measures that could predict the success of
generated ideas are currently unknown. Here, we analyze a dataset of design
problem-solving conversations in real-world settings by using 49 semantic
measures based on WordNet 3.1 and demonstrate that a divergence of semantic
similarity, an increased information content, and a decreased polysemy predict
the success of generated ideas. The first feedback from clients also enhances
information content and leads to a divergence of successful ideas in creative
problem solving. These results advance cognitive science by identifying
real-world processes in human problem solving that are relevant to the success
of produced solutions and provide tools for real-time monitoring of problem
solving, student training and skill acquisition. A selected subset of
information content (IC S\'anchez-Batet) and semantic similarity
(Lin/S\'anchez-Batet) measures, which are both statistically powerful and
computationally fast, could support the development of technologies for
computer-assisted enhancements of human creativity or for the implementation of
creativity in machines endowed with general artificial intelligence.",arxiv
http://arxiv.org/abs/2012.14325v1,2020-12-22T09:54:04Z,2020-12-22T09:54:04Z,Digital me ontology and ethics,"This paper addresses ontology and ethics of an AI agent called digital me. We
define digital me as autonomous, decision-making, and learning agent,
representing an individual and having practically immortal own life. It is
assumed that digital me is equipped with the big-five personality model,
ensuring that it provides a model of some aspects of a strong AI:
consciousness, free will, and intentionality. As computer-based personality
judgments are more accurate than those made by humans, digital me can judge the
personality of the individual represented by the digital me, other individuals'
personalities, and other digital me-s. We describe seven ontological qualities
of digital me: a) double-layer status of Digital Being versus digital me, b)
digital me versus real me, c) mind-digital me and body-digital me, d) digital
me versus doppelganger (shadow digital me), e) non-human time concept, f)
social quality, g) practical immortality. We argue that with the advancement of
AI's sciences and technologies, there exist two digital me thresholds. The
first threshold defines digital me having some (rudimentarily) form of
consciousness, free will, and intentionality. The second threshold assumes that
digital me is equipped with moral learning capabilities, implying that, in
principle, digital me could develop their own ethics which significantly
differs from human's understanding of ethics. Finally we discuss the
implications of digital me metaethics, normative and applied ethics, the
implementation of the Golden Rule in digital me-s, and we suggest two sets of
normative principles for digital me: consequentialist and duty based digital me
principles.",arxiv
http://arxiv.org/abs/2101.02000v1,2021-01-06T13:15:21Z,2021-01-06T13:15:21Z,Weakly-Supervised Multi-Face 3D Reconstruction,"3D face reconstruction plays a very important role in many real-world
multimedia applications, including digital entertainment, social media,
affection analysis, and person identification. The de-facto pipeline for
estimating the parametric face model from an image requires to firstly detect
the facial regions with landmarks, and then crop each face to feed the deep
learning-based regressor. Comparing to the conventional methods performing
forward inference for each detected instance independently, we suggest an
effective end-to-end framework for multi-face 3D reconstruction, which is able
to predict the model parameters of multiple instances simultaneously using
single network inference. Our proposed approach not only greatly reduces the
computational redundancy in feature extraction but also makes the deployment
procedure much easier using the single network model. More importantly, we
employ the same global camera model for the reconstructed faces in each image,
which makes it possible to recover the relative head positions and orientations
in the 3D scene. We have conducted extensive experiments to evaluate our
proposed approach on the sparse and dense face alignment tasks. The
experimental results indicate that our proposed approach is very promising on
face alignment tasks without fully-supervision and pre-processing like
detection and crop. Our implementation is publicly available at
\url{https://github.com/kalyo-zjl/WM3DR}.",arxiv
http://arxiv.org/abs/2109.05821v1,2021-09-13T09:46:50Z,2021-09-13T09:46:50Z,Cyber-Security in the Emerging World of Smart Everything,"The fourth industrial revolution (4IR) is a revolution many authors believe
have come to stay. It is a revolution that has been fast blurring the line
between physical, digital and biological technologies. These disruptive
technologies largely rely on high-speed internet connectivity, Cloud
technologies, Augmented Reality, Additive Manufacturing, Data science and
Artificial Intelligence. Most developed economies have embraced the it while
the developing economies are struggling to adopt 4IR because they lack the
requisite skills, knowledge and technology. Thus, this study investigates
Nigeria as one of the developing economies to understand her readiness for 4IR
and the level of preparedness to mitigate the sophisticated cyber-attacks that
comes with it. The investigation adopted quantitative research approach and
developed an online questionnaire that was shared amongst the population of
interest that includes academic, industry experts and relevant stakeholders.
The questionnaire returned 116 valid responses which were analysed with
descriptive statistical tools in SPSS. Results suggest that 60 of the
respondents opined that Nigerian government at are not showing enough evidence
to demonstrate her preparedness to leverage these promised potentials by
developing 4IR relevant laws, strong institutional frameworks and policies.
They lack significant development capacity to mitigate risks associated with
digital ecosystem and cyber ecosystem that are ushered in by the 4IR. In the
universities, 52 of the courses offered at the undergraduate and 42 at the
post-graduate levels are relevant in the development of skills required in the
revolution. The study recommends that the government at all levels make
adequate efforts in developing the countrys intangible assets. In all, this
paper posits that successful implementation of these could equip Nigeria to
embrace the 4IR in all its aspects.",arxiv
http://arxiv.org/abs/2107.06882v1,2021-07-14T17:55:28Z,2021-07-14T17:55:28Z,"Conservative Objective Models for Effective Offline Model-Based
  Optimization","Computational design problems arise in a number of settings, from synthetic
biology to computer architectures. In this paper, we aim to solve data-driven
model-based optimization (MBO) problems, where the goal is to find a design
input that maximizes an unknown objective function provided access to only a
static dataset of prior experiments. Such data-driven optimization procedures
are the only practical methods in many real-world domains where active data
collection is expensive (e.g., when optimizing over proteins) or dangerous
(e.g., when optimizing over aircraft designs). Typical methods for MBO that
optimize the design against a learned model suffer from distributional shift:
it is easy to find a design that ""fools"" the model into predicting a high
value. To overcome this, we propose conservative objective models (COMs), a
method that learns a model of the objective function that lower bounds the
actual value of the ground-truth objective on out-of-distribution inputs, and
uses it for optimization. Structurally, COMs resemble adversarial training
methods used to overcome adversarial examples. COMs are simple to implement and
outperform a number of existing methods on a wide range of MBO problems,
including optimizing protein sequences, robot morphologies, neural network
weights, and superconducting materials.",arxiv
http://arxiv.org/abs/1804.03547v2,2018-04-11T15:20:31Z,2018-04-10T14:07:45Z,"A real-time and unsupervised face Re-Identification system for
  Human-Robot Interaction","In the context of Human-Robot Interaction (HRI), face Re-Identification (face
Re-ID) aims to verify if certain detected faces have already been observed by
robots. The ability of distinguishing between different users is crucial in
social robots as it will enable the robot to tailor the interaction strategy
toward the users' individual preferences. So far face recognition research has
achieved great success, however little attention has been paid to the realistic
applications of Face Re-ID in social robots. In this paper, we present an
effective and unsupervised face Re-ID system which simultaneously re-identifies
multiple faces for HRI. This Re-ID system employs Deep Convolutional Neural
Networks to extract features, and an online clustering algorithm to determine
the face's ID. Its performance is evaluated on two datasets: the TERESA video
dataset collected by the TERESA robot, and the YouTube Face Dataset (YTF
Dataset). We demonstrate that the optimised combination of techniques achieves
an overall 93.55% accuracy on TERESA dataset and an overall 90.41% accuracy on
YTF dataset. We have implemented the proposed method into a software module in
the HCI^2 Framework for it to be further integrated into the TERESA robot, and
has achieved real-time performance at 10~26 Frames per second.",arxiv
http://arxiv.org/abs/2004.09608v2,2020-04-22T05:23:40Z,2020-04-20T20:14:00Z,"Flow-based Algorithms for Improving Clusters: A Unifying Framework,
  Software, and Performance","Clustering points in a vector space or nodes in a graph is a ubiquitous
primitive in statistical data analysis, and it is commonly used for exploratory
data analysis. In practice, it is often of interest to ""refine"" or ""improve"" a
given cluster that has been obtained by some other method. In this survey, we
focus on principled algorithms for this cluster improvement problem. Many such
cluster improvement algorithms are flow-based methods, by which we mean that
operationally they require the solution of a sequence of maximum flow problems
on a (typically implicitly) modified data graph. These cluster improvement
algorithms are powerful, both in theory and in practice, but they have not been
widely adopted for problems such as community detection, local graph
clustering, semi-supervised learning, etc. Possible reasons for this are: the
steep learning curve for these algorithms; the lack of efficient and easy to
use software; and the lack of detailed numerical experiments on real-world data
that demonstrate their usefulness. Our objective here is to address these
issues. To do so, we guide the reader through the whole process of
understanding how to implement and apply these powerful algorithms. We present
a unifying fractional programming optimization framework that permits us to
distill out in a simple way the crucial components of all these algorithms. It
also makes apparent similarities and differences between related methods.
Viewing these cluster improvement algorithms via a fractional programming
framework suggests directions for future algorithm development. Finally, we
develop efficient implementations of these algorithms in our
LocalGraphClustering python package, and we perform extensive numerical
experiments to demonstrate the performance of these methods on social networks
and image-based data graphs.",arxiv
http://arxiv.org/abs/2104.04148v2,2021-04-12T03:06:05Z,2021-04-09T01:54:58Z,"Individual Explanations in Machine Learning Models: A Case Study on
  Poverty Estimation","Machine learning methods are being increasingly applied in sensitive societal
contexts, where decisions impact human lives. Hence it has become necessary to
build capabilities for providing easily-interpretable explanations of models'
predictions. Recently in academic literature, a vast number of explanations
methods have been proposed. Unfortunately, to our knowledge, little has been
documented about the challenges machine learning practitioners most often face
when applying them in real-world scenarios. For example, a typical procedure
such as feature engineering can make some methodologies no longer applicable.
The present case study has two main objectives. First, to expose these
challenges and how they affect the use of relevant and novel explanations
methods. And second, to present a set of strategies that mitigate such
challenges, as faced when implementing explanation methods in a relevant
application domain -- poverty estimation and its use for prioritizing access to
social policies.",arxiv
http://arxiv.org/abs/1802.06108v3,2019-12-31T20:08:44Z,2018-02-16T20:22:41Z,"Modeling the Formation of Social Conventions from Embodied Real-Time
  Interactions","What is the role of real-time control and learning in the formation of social
conventions? To answer this question, we propose a computational model that
matches human behavioral data in a social decision-making game that was
analyzed both in discrete-time and continuous-time setups. Furthermore, unlike
previous approaches, our model takes into account the role of sensorimotor
control loops in embodied decision-making scenarios. For this purpose, we
introduce the Control-based Reinforcement Learning (CRL) model. CRL is grounded
in the Distributed Adaptive Control (DAC) theory of mind and brain, where
low-level sensorimotor control is modulated through perceptual and behavioral
learning in a layered structure. CRL follows these principles by implementing a
feedback control loop handling the agent's reactive behaviors (pre-wired
reflexes), along with an adaptive layer that uses reinforcement learning to
maximize long-term reward. We test our model in a multi-agent game-theoretic
task in which coordination must be achieved to find an optimal solution. We
show that CRL is able to reach human-level performance on standard
game-theoretic metrics such as efficiency in acquiring rewards and fairness in
reward distribution.",arxiv
http://arxiv.org/abs/1411.0778v1,2014-11-04T03:48:20Z,2014-11-04T03:48:20Z,"Detecting Suicidal Ideation in Chinese Microblogs with Psychological
  Lexicons","Suicide is among the leading causes of death in China. However, technical
approaches toward preventing suicide are challenging and remaining under
development. Recently, several actual suicidal cases were preceded by users who
posted microblogs with suicidal ideation to Sina Weibo, a Chinese social media
network akin to Twitter. It would therefore be desirable to detect suicidal
ideations from microblogs in real-time, and immediately alert appropriate
support groups, which may lead to successful prevention. In this paper, we
propose a real-time suicidal ideation detection system deployed over Weibo,
using machine learning and known psychological techniques. Currently, we have
identified 53 known suicidal cases who posted suicide notes on Weibo prior to
their deaths.We explore linguistic features of these known cases using a
psychological lexicon dictionary, and train an effective suicidal Weibo post
detection model. 6714 tagged posts and several classifiers are used to verify
the model. By combining both machine learning and psychological knowledge, SVM
classifier has the best performance of different classifiers, yielding an
F-measure of 68:3%, a Precision of 78:9%, and a Recall of 60:3%.",arxiv
http://arxiv.org/abs/1907.01522v1,2019-06-28T22:22:41Z,2019-06-28T22:22:41Z,Tucker Tensor Decomposition on FPGA,"Tensor computation has emerged as a powerful mathematical tool for solving
high-dimensional and/or extreme-scale problems in science and engineering. The
last decade has witnessed tremendous advancement of tensor computation and its
applications in machine learning and big data. However, its hardware
optimization on resource-constrained devices remains an (almost) unexplored
field. This paper presents an hardware accelerator for a classical tensor
computation framework, Tucker decomposition. We study three modules of this
architecture: tensor-times-matrix (TTM), matrix singular value decomposition
(SVD), and tensor permutation, and implemented them on Xilinx FPGA for
prototyping. In order to further reduce the computing time, a warm-start
algorithm for the Jacobi iterations in SVD is proposed. A fixed-point simulator
is used to evaluate the performance of our design. Some synthetic data sets and
a real MRI data set are used to validate the design and evaluate its
performance. We compare our work with state-of-the-art software toolboxes
running on both CPU and GPU, and our work shows 2.16 - 30.2x speedup on the
cardiac MRI data set.",arxiv
http://arxiv.org/abs/2006.07333v1,2020-06-12T17:17:01Z,2020-06-12T17:17:01Z,Targeting Learning: Robust Statistics for Reproducible Research,"Targeted Learning is a subfield of statistics that unifies advances in causal
inference, machine learning and statistical theory to help answer
scientifically impactful questions with statistical confidence. Targeted
Learning is driven by complex problems in data science and has been implemented
in a diversity of real-world scenarios: observational studies with missing
treatments and outcomes, personalized interventions, longitudinal settings with
time-varying treatment regimes, survival analysis, adaptive randomized trials,
mediation analysis, and networks of connected subjects. In contrast to the
(mis)application of restrictive modeling strategies that dominate the current
practice of statistics, Targeted Learning establishes a principled standard for
statistical estimation and inference (i.e., confidence intervals and p-values).
This multiply robust approach is accompanied by a guiding roadmap and a
burgeoning software ecosystem, both of which provide guidance on the
construction of estimators optimized to best answer the motivating question.
The roadmap of Targeted Learning emphasizes tailoring statistical procedures so
as to minimize their assumptions, carefully grounding them only in the
scientific knowledge available. The end result is a framework that honestly
reflects the uncertainty in both the background knowledge and the available
data in order to draw reliable conclusions from statistical analyses -
ultimately enhancing the reproducibility and rigor of scientific findings.",arxiv
http://arxiv.org/abs/2010.16356v1,2020-10-30T16:25:26Z,2020-10-30T16:25:26Z,Cooperation dynamics of generalized reciprocity on complex networks,"Recent studies suggest that the emergence of cooperative behavior can be
explained by generalized reciprocity, a behavioral mechanism based on the
principle of ""help anyone if helped by someone"". In complex systems, the
cooperative dynamics is largely determined by the network structure which
dictates the interactions among neighboring individuals. Despite an abundance
of studies, the role of the network structure in in promoting cooperation
through generalized reciprocity remains an under-explored phenomenon. In this
doctoral thesis, we utilize basic tools from the dynamical systems theory, and
develop a unifying framework for investigating the cooperation dynamics of
generalized reciprocity on complex networks. We use this framework to present a
theoretical overview on the role of generalized reciprocity in promoting
cooperation in three distinct interaction structures: i) social dilemmas, ii)
multidimensional networks, and iii) fluctuating environments. The results
suggest that cooperation through generalized reciprocity always emerges as the
unique attractor in which the overall level of cooperation is maximized, while
simultaneously exploitation of the participating individuals is prevented. The
effect of the network structure is captured by a local centrality measure which
uniquely quantifies the propensity of the network structure to cooperation, by
dictating the degree of cooperation displayed both at microscopic and
macroscopic level. As a consequence, the implementation of our results may go
beyond explaining the evolution of cooperation. In particular, they can be
directly applied in domains that deal with the development of artificial
systems able to adequately mimic reality, such as reinforcement learning.",arxiv
http://arxiv.org/abs/1810.03190v1,2018-10-07T17:59:49Z,2018-10-07T17:59:49Z,"Scalable Solutions for Automated Single Pulse Identification and
  Classification in Radio Astronomy","Data collection for scientific applications is increasing exponentially and
is forecasted to soon reach peta- and exabyte scales. Applications which
process and analyze scientific data must be scalable and focus on execution
performance to keep pace. In the field of radio astronomy, in addition to
increasingly large datasets, tasks such as the identification of transient
radio signals from extrasolar sources are computationally expensive. We present
a scalable approach to radio pulsar detection written in Scala that
parallelizes candidate identification to take advantage of in-memory task
processing using Apache Spark on a YARN distributed system. Furthermore, we
introduce a novel automated multiclass supervised machine learning technique
that we combine with feature selection to reduce the time required for
candidate classification. Experimental testing on a Beowulf cluster with 15
data nodes shows that the parallel implementation of the identification
algorithm offers a speedup of up to 5X that of a similar multithreaded
implementation. Further, we show that the combination of automated multiclass
classification and feature selection speeds up the execution performance of the
RandomForest machine learning algorithm by an average of 54% with less than a
2% average reduction in the algorithm's ability to correctly classify pulsars.
The generalizability of these results is demonstrated by using two real-world
radio astronomy data sets.",arxiv
http://arxiv.org/abs/1907.00498v4,2020-07-08T22:48:11Z,2019-06-30T23:46:30Z,"Proof of Witness Presence: Blockchain Consensus for Augmented Democracy
  in Smart Cities","Smart Cities evolve into complex and pervasive urban environments with a
citizens' mandate to meet sustainable development goals. Repositioning
democratic values of citizens' choices in these complex ecosystems has turned
out to be imperative in an era of social media filter bubbles, fake news and
opportunities for manipulating electoral results with such means. This paper
introduces a new paradigm of augmented democracy that promises actively
engaging citizens in a more informed decision-making augmented into public
urban space. The proposed concept is inspired by a digital revive of the
Ancient Agora of Athens, an arena of public discourse, a Polis where citizens
assemble to actively deliberate and collectively decide about public matters.
The core contribution of the proposed paradigm is the concept of proving
witness presence: making decision-making subject of providing secure evidence
and testifying for choices made in the physical space. This paper shows how the
challenge of proving witness presence can be tackled with blockchain consensus
to empower citizens' trust and overcome security vulnerabilities of GPS
localization. Moreover, a novel platform for collective decision-making and
crowd-sensing in urban space is introduced: Smart Agora. It is shown how
real-time collective measurements over citizens' choices can be made in a fully
decentralized and privacy-preserving way. Witness presence is tested by
deploying a decentralized system for crowd-sensing the sustainable use of
transport means. Furthermore, witness presence of cycling risk is validated
using official accident data from public authorities, which are compared
against wisdom of the crowd. The paramount role of dynamic consensus,
self-governance and ethically aligned artificial intelligence in the augmented
democracy paradigm is outlined.",arxiv
http://arxiv.org/abs/2003.11117v1,2020-03-24T21:17:44Z,2020-03-24T21:17:44Z,"COVID-19 and Computer Audition: An Overview on What Speech & Sound
  Analysis Could Contribute in the SARS-CoV-2 Corona Crisis","At the time of writing, the world population is suffering from more than
10,000 registered COVID-19 disease epidemic induced deaths since the outbreak
of the Corona virus more than three months ago now officially known as
SARS-CoV-2. Since, tremendous efforts have been made worldwide to counter-steer
and control the epidemic by now labelled as pandemic. In this contribution, we
provide an overview on the potential for computer audition (CA), i.e., the
usage of speech and sound analysis by artificial intelligence to help in this
scenario. We first survey which types of related or contextually significant
phenomena can be automatically assessed from speech or sound. These include the
automatic recognition and monitoring of breathing, dry and wet coughing or
sneezing sounds, speech under cold, eating behaviour, sleepiness, or pain to
name but a few. Then, we consider potential use-cases for exploitation. These
include risk assessment and diagnosis based on symptom histograms and their
development over time, as well as monitoring of spread, social distancing and
its effects, treatment and recovery, and patient wellbeing. We quickly guide
further through challenges that need to be faced for real-life usage. We come
to the conclusion that CA appears ready for implementation of (pre-)diagnosis
and monitoring tools, and more generally provides rich and significant, yet so
far untapped potential in the fight against COVID-19 spread.",arxiv
http://arxiv.org/abs/2010.08600v2,2020-11-16T06:26:16Z,2020-10-16T19:40:08Z,"Robot Navigation in Constrained Pedestrian Environments using
  Reinforcement Learning","Navigating fluently around pedestrians is a necessary capability for mobile
robots deployed in human environments, such as buildings and homes. While
research on social navigation has focused mainly on the scalability with the
number of pedestrians in open spaces, typical indoor environments present the
additional challenge of constrained spaces such as corridors and doorways that
limit maneuverability and influence patterns of pedestrian interaction. We
present an approach based on reinforcement learning (RL) to learn policies
capable of dynamic adaptation to the presence of moving pedestrians while
navigating between desired locations in constrained environments. The policy
network receives guidance from a motion planner that provides waypoints to
follow a globally planned trajectory, whereas RL handles the local
interactions. We explore a compositional principle for multi-layout training
and find that policies trained in a small set of geometrically simple layouts
successfully generalize to more complex unseen layouts that exhibit composition
of the structural elements available during training. Going beyond walls-world
like domains, we show transfer of the learned policy to unseen 3D
reconstructions of two real environments. These results support the
applicability of the compositional principle to navigation in real-world
buildings and indicate promising usage of multi-agent simulation within
reconstructed environments for tasks that involve interaction.",arxiv
http://arxiv.org/abs/2102.03616v1,2021-02-06T17:11:09Z,2021-02-06T17:11:09Z,"A Data Augmented Bayesian Network for Node Failure Prediction in Optical
  Networks","Failures in optical network backbone can cause significant interruption in
internet data traffic. Hence, it is very important to reduce such network
outages. Prediction of such failures would be a step forward to avoid such
disruption of internet services for users as well as operators. Several
research proposals are available in the literature which are applications of
data science and machine learning techniques. Most of the techniques rely on
significant amount of real time data collection. Network devices are assumed to
be equipped to collect data and these are then analysed by different algorithms
to predict failures. Every network element which is already deployed in the
field may not have these data gathering or analysis techniques designed into
them initially. However, such mechanisms become necessary later when they are
already deployed in the field. This paper proposes a Bayesian network based
failure prediction of network nodes, e.g., routers etc., using very basic
information from the log files of the devices and applying power law based data
augmentation to complement for scarce real time information. Numerical results
show that network node failure prediction can be performed with high accuracy
using the proposed mechanism.",arxiv
http://arxiv.org/abs/1711.08037v2,2017-11-24T07:02:24Z,2017-11-20T04:19:49Z,The Doctor Just Won't Accept That!,"Calls to arms to build interpretable models express a well-founded discomfort
with machine learning. Should a software agent that does not even know what a
loan is decide who qualifies for one? Indeed, we ought to be cautious about
injecting machine learning (or anything else, for that matter) into
applications where there may be a significant risk of causing social harm.
However, claims that stakeholders ""just won't accept that!"" do not provide a
sufficient foundation for a proposed field of study. For the field of
interpretable machine learning to advance, we must ask the following questions:
What precisely won't various stakeholders accept? What do they want? Are these
desiderata reasonable? Are they feasible? In order to answer these questions,
we'll have to give real-world problems and their respective stakeholders
greater consideration.",arxiv
http://arxiv.org/abs/2012.05996v1,2020-12-10T21:48:26Z,2020-12-10T21:48:26Z,"How to enhance quantum generative adversarial learning of noisy
  information","Quantum Machine Learning is where nowadays machine learning meets quantum
information science. In order to implement this new paradigm for novel quantum
technologies, we still need a much deeper understanding of its underlying
mechanisms, before proposing new algorithms to feasibly address real problems.
In this context, quantum generative adversarial learning is a promising
strategy to use quantum devices for quantum estimation or generative machine
learning tasks. However, the convergence behaviours of its training process,
which is crucial for its practical implementation on quantum processors, have
not been investigated in detail yet. Indeed here we show how different training
problems may occur during the optimization process, such as the emergence of
limit cycles. The latter may remarkably extend the convergence time in the
scenario of mixed quantum states playing a crucial role in the already
available noisy intermediate scale quantum devices. Then, we propose new
strategies to achieve a faster convergence in any operating regime. Our results
pave the way for new experimental demonstrations of such hybrid
classical-quantum protocols allowing to evaluate the potential advantages over
their classical counterparts.",arxiv
http://arxiv.org/abs/1808.04511v1,2018-08-14T03:00:29Z,2018-08-14T03:00:29Z,A Record Linkage Model Incorporating Relational Data,"In this paper we introduce a novel Bayesian approach for linking multiple
social networks in order to discover the same real world person having
different accounts across networks. In particular, we develop a latent model
that allow us to jointly characterize the network and linkage structures
relying in both relational and profile data. In contrast to other existing
approaches in the machine learning literature, our Bayesian implementation
naturally provides uncertainty quantification via posterior probabilities for
the linkage structure itself or any function of it. Our findings clearly
suggest that our methodology can produce accurate point estimates of the
linkage structure even in the absence of profile information, and also, in an
identity resolution setting, our results confirm that including relational data
into the matching process improves the linkage accuracy. We illustrate our
methodology using real data from popular social networks such as Twitter,
Facebook, and YouTube.",arxiv
http://arxiv.org/abs/1811.08270v2,2019-02-25T06:50:27Z,2018-11-11T14:15:23Z,Graph Convolutional Neural Networks via Motif-based Attention,"Many real-world problems can be represented as graph-based learning problems.
In this paper, we propose a novel framework for learning spatial and
attentional convolution neural networks on arbitrary graphs. Different from
previous convolutional neural networks on graphs, we first design a
motif-matching guided subgraph normalization method to capture neighborhood
information. Then we implement subgraph-level self-attentional layers to learn
different importances from different subgraphs to solve graph classification
problems. Analogous to image-based attentional convolution networks that
operate on locally connected and weighted regions of the input, we also extend
graph normalization from one-dimensional node sequence to two-dimensional node
grid by leveraging motif-matching, and design self-attentional layers without
requiring any kinds of cost depending on prior knowledge of the graph
structure. Our results on both bioinformatics and social network datasets show
that we can significantly improve graph classification benchmarks over
traditional graph kernel and existing deep models.",arxiv
http://arxiv.org/abs/2010.09814v1,2020-10-19T19:47:38Z,2020-10-19T19:47:38Z,"Social Hierarchy-based Distributed Economic Model Predictive Control of
  Floating Offshore Wind Farms","This paper implements a recently developed social hierarchy-based distributed
economic model predictive control (DEMPC) algorithm in floating offshore wind
farms for the purpose of power maximization. The controller achieves this
objective using the concept of yaw and induction-based turbine repositioning
(YITuR), which minimizes the overlap areas between adjacent floating wind
turbine rotors in real-time to minimize the wake effect. Floating wind farm
dynamics and performance are predicted numerically using FOWFSim-Dyn. To ensure
fast decision-making by the DEMPC algorithm, feed-forward neural networks are
used to estimate floating wind turbine dynamics during the process of dynamic
optimization. For simulated wind farms with layouts ranging from 1-by-2 to
1-by-5, an increase of 20% in energy production is predicted when using YITuR
instead of greedy operation. Increased variability in wind speed and direction
is also studied and is shown to diminish controller performance due to rising
errors in neural network predictions.",arxiv
http://arxiv.org/abs/2109.09343v1,2021-09-20T07:54:09Z,2021-09-20T07:54:09Z,"Latexify Math: Mathematical Formula Markup Revision to Assist
  Collaborative Editing in Math Q&A Sites","Collaborative editing questions and answers plays an important role in
quality control of Mathematics Stack Exchange which is a math Q&A Site. Our
study of post edits in Mathematics Stack Exchange shows that there is a large
number of math-related edits about latexifying formulas, revising LaTeX and
converting the blurred math formula screenshots to LaTeX sequence. Despite its
importance, manually editing one math-related post especially those with
complex mathematical formulas is time-consuming and error-prone even for
experienced users. To assist post owners and editors to do this editing, we
have developed an edit-assistance tool, MathLatexEdit for formula
latexification, LaTeX revision and screenshot transcription. We formulate this
formula editing task as a translation problem, in which an original post is
translated to a revised post. MathLatexEdit implements a deep learning based
approach including two encoder-decoder models for textual and visual LaTeX edit
recommendation with math-specific inference. The two models are trained on
large-scale historical original-edited post pairs and synthesized
screenshot-formula pairs. Our evaluation of MathLatexEdit not only demonstrates
the accuracy of our model, but also the usefulness of MathLatexEdit in editing
real-world posts which are accepted in Mathematics Stack Exchange.",arxiv
http://arxiv.org/abs/1910.12202v4,2020-09-14T02:38:16Z,2019-10-27T07:42:15Z,CONNA: Addressing Name Disambiguation on The Fly,"Name disambiguation is a key and also a very tough problem in many online
systems such as social search and academic search. Despite considerable
research, a critical issue that has not been systematically studied is
disambiguation on the fly -- to complete the disambiguation in the real-time.
This is very challenging, as the disambiguation algorithm must be accurate,
efficient, and error tolerance. In this paper, we propose a novel framework --
CONNA -- to train a matching component and a decision component jointly via
reinforcement learning. The matching component is responsible for finding the
top matched candidate for the given paper, and the decision component is
responsible for deciding on assigning the top matched person or creating a new
person. The two components are intertwined and can be bootstrapped via jointly
training. Empirically, we evaluate CONNA on two name disambiguation datasets.
Experimental results show that the proposed framework can achieve a
1.21%-19.84% improvement on F1-score using joint training of the matching and
the decision components. The proposed CONNA has been successfully deployed on
AMiner -- a large online academic search system.",arxiv
http://arxiv.org/abs/2104.02541v1,2021-04-06T14:31:23Z,2021-04-06T14:31:23Z,"Instantaneous Stereo Depth Estimation of Real-World Stimuli with a
  Neuromorphic Stereo-Vision Setup","The stereo-matching problem, i.e., matching corresponding features in two
different views to reconstruct depth, is efficiently solved in biology. Yet, it
remains the computational bottleneck for classical machine vision approaches.
By exploiting the properties of event cameras, recently proposed Spiking Neural
Network (SNN) architectures for stereo vision have the potential of simplifying
the stereo-matching problem. Several solutions that combine event cameras with
spike-based neuromorphic processors already exist. However, they are either
simulated on digital hardware or tested on simplified stimuli. In this work, we
use the Dynamic Vision Sensor 3D Human Pose Dataset (DHP19) to validate a
brain-inspired event-based stereo-matching architecture implemented on a
mixed-signal neuromorphic processor with real-world data. Our experiments show
that this SNN architecture, composed of coincidence detectors and disparity
sensitive neurons, is able to provide a coarse estimate of the input disparity
instantaneously, thereby detecting the presence of a stimulus moving in depth
in real-time.",arxiv
http://arxiv.org/abs/1907.07958v1,2019-07-18T09:58:27Z,2019-07-18T09:58:27Z,Transfer Learning Across Simulated Robots With Different Sensors,"For a robot to learn a good policy, it often requires expensive equipment
(such as sophisticated sensors) and a prepared training environment conducive
to learning. However, it is seldom possible to perfectly equip robots for
economic reasons, nor to guarantee ideal learning conditions, when deployed in
real-life environments. A solution would be to prepare the robot in the lab
environment, when all necessary material is available to learn a good policy.
After training in the lab, the robot should be able to get by without the
expensive equipment that used to be available to it, and yet still be
guaranteed to perform well on the field. The transition between the lab
(source) and the real-world environment (target) is related to transfer
learning, where the state-space between the source and target tasks differ. We
tackle a simulated task with continuous states and discrete actions presenting
this challenge, using Bootstrapped Dual Policy Iteration, a model-free
actor-critic reinforcement learning algorithm, and Policy Shaping.
Specifically, we train a BDPI agent, embodied by a virtual robot performing a
task in the V-Rep simulator, sensing its environment through several proximity
sensors. The resulting policy is then used by a second agent learning the same
task in the same environment, but with camera images as input. The goal is to
obtain a policy able to perform the task relying on merely camera images.",arxiv
http://arxiv.org/abs/1106.0134v1,2011-06-01T09:56:49Z,2011-06-01T09:56:49Z,"ProDiGe: PRioritization Of Disease Genes with multitask machine learning
  from positive and unlabeled examples","Elucidating the genetic basis of human diseases is a central goal of genetics
and molecular biology. While traditional linkage analysis and modern
high-throughput techniques often provide long lists of tens or hundreds of
disease gene candidates, the identification of disease genes among the
candidates remains time-consuming and expensive. Efficient computational
methods are therefore needed to prioritize genes within the list of candidates,
by exploiting the wealth of information available about the genes in various
databases. Here we propose ProDiGe, a novel algorithm for Prioritization of
Disease Genes. ProDiGe implements a novel machine learning strategy based on
learning from positive and unlabeled examples, which allows to integrate
various sources of information about the genes, to share information about
known disease genes across diseases, and to perform genome-wide searches for
new disease genes. Experiments on real data show that ProDiGe outperforms
state-of-the-art methods for the prioritization of genes in human diseases.",arxiv
http://arxiv.org/abs/1708.04664v1,2017-08-05T13:15:36Z,2017-08-05T13:15:36Z,"A Novel data Pre-processing method for multi-dimensional and non-uniform
  data","We are in the era of data analytics and data science which is on full bloom.
There is abundance of all kinds of data for example biometrics based data,
satellite images data, chip-seq data, social network data, sensor based data
etc. from a variety of sources. This data abundance is the result of the fact
that storage cost is getting cheaper day by day, so people as well as almost
all business or scientific organizations are storing more and more data. Most
of the real data is multi-dimensional, non-uniform, and big in size, such that
it requires a unique pre-processing before analyzing it. In order to make data
useful for any kind of analysis, pre-processing is a very important step. This
paper presents a unique and novel pre-processing method for multi-dimensional
and non-uniform data with the aim of making it uniform and reduced in size
without losing much of its value. We have chosen biometric signature data to
demonstrate the proposed method as it qualifies for the attributes of being
multi-dimensional, non-uniform and big in size. Biometric signature data does
not only captures the structural characteristics of a signature but also its
behavioral characteristics that are captured using a dynamic signature capture
device. These features like pen pressure, pen tilt angle, time taken to sign a
document when collected in real-time turn out to be of varying dimensions. This
feature data set along with the structural data needs to be pre-processed in
order to use it to train a machine learning based model for signature
verification purposes. We demonstrate the success of the proposed method over
other methods using experimental results for biometric signature data but the
same can be implemented for any other data with similar properties from a
different domain.",arxiv
http://arxiv.org/abs/2102.10997v1,2021-02-03T10:52:02Z,2021-02-03T10:52:02Z,"Trust Computational Heuristic for Social Internet of Things: A Machine
  Learning-based Approach","The Internet of Things (IoT) is an evolving network of billions of
interconnected physical objects, such as numerous sensors, smartphones,
wearables, and embedded devices. These physical objects, generally referred to
as the smart objects, when deployed in the real-world aggregates useful
information from their surrounding environment. As-of-late, this notion of IoT
has been extended to incorporate the social networking facets which have led to
the promising paradigm of the `Social Internet of Things' (SIoT). In SIoT, the
devices operate as an autonomous agent and provide an exchange of information
and service discovery in an intelligent manner by establishing social
relationships among them with respect to their owners. Trust plays an important
role in establishing trustworthy relationships among the physical objects and
reduces probable risks in the decision-making process. In this paper, a trust
computational model is proposed to extract individual trust features in a SIoT
environment. Furthermore, a machine learning-based heuristic is used to
aggregate all the trust features in order to ascertain an aggregate trust
score. Simulation results illustrate that the proposed trust-based model
isolates the trustworthy and untrustworthy nodes within the network in an
efficient manner.",arxiv
http://arxiv.org/abs/2010.06425v1,2020-10-13T14:38:40Z,2020-10-13T14:38:40Z,"Temporal Collaborative Filtering with Graph Convolutional Neural
  Networks","Temporal collaborative filtering (TCF) methods aim at modelling non-static
aspects behind recommender systems, such as the dynamics in users' preferences
and social trends around items. State-of-the-art TCF methods employ recurrent
neural networks (RNNs) to model such aspects. These methods deploy
matrix-factorization-based (MF-based) approaches to learn the user and item
representations. Recently, graph-neural-network-based (GNN-based) approaches
have shown improved performance in providing accurate recommendations over
traditional MF-based approaches in non-temporal CF settings. Motivated by this,
we propose a novel TCF method that leverages GNNs to learn user and item
representations, and RNNs to model their temporal dynamics. A challenge with
this method lies in the increased data sparsity, which negatively impacts
obtaining meaningful quality representations with GNNs. To overcome this
challenge, we train a GNN model at each time step using a set of observed
interactions accumulated time-wise. Comprehensive experiments on real-world
data show the improved performance obtained by our method over several
state-of-the-art temporal and non-temporal CF models.",arxiv
http://arxiv.org/abs/2101.06448v3,2021-01-21T18:16:41Z,2021-01-16T14:20:32Z,"Self-Supervised Multi-Channel Hypergraph Convolutional Network for
  Social Recommendation","Social relations are often used to improve recommendation quality when
user-item interaction data is sparse in recommender systems. Most existing
social recommendation models exploit pairwise relations to mine potential user
preferences. However, real-life interactions among users are very complicated
and user relations can be high-order. Hypergraph provides a natural way to
model complex high-order relations, while its potentials for improving social
recommendation are under-explored. In this paper, we fill this gap and propose
a multi-channel hypergraph convolutional network to enhance social
recommendation by leveraging high-order user relations. Technically, each
channel in the network encodes a hypergraph that depicts a common high-order
user relation pattern via hypergraph convolution. By aggregating the embeddings
learned through multiple channels, we obtain comprehensive user representations
to generate recommendation results. However, the aggregation operation might
also obscure the inherent characteristics of different types of high-order
connectivity information. To compensate for the aggregating loss, we
innovatively integrate self-supervised learning into the training of the
hypergraph convolutional network to regain the connectivity information with
hierarchical mutual information maximization. The experimental results on
multiple real-world datasets show that the proposed model outperforms the SOTA
methods, and the ablation study verifies the effectiveness of the multi-channel
setting and the self-supervised task. The implementation of our model is
available via https://github.com/Coder-Yu/RecQ.",arxiv
http://arxiv.org/abs/1710.02595v2,2017-10-10T05:05:58Z,2017-10-06T21:42:15Z,Intelligent Pothole Detection and Road Condition Assessment,"Poor road conditions are a public nuisance, causing passenger discomfort,
damage to vehicles, and accidents. In the U.S., road-related conditions are a
factor in 22,000 of the 42,000 traffic fatalities each year. Although we often
complain about bad roads, we have no way to detect or report them at scale. To
address this issue, we developed a system to detect potholes and assess road
conditions in real-time. Our solution is a mobile application that captures
data on a car's movement from gyroscope and accelerometer sensors in the phone.
To assess roads using this sensor data, we trained SVM models to classify road
conditions with 93% accuracy and potholes with 92% accuracy, beating the base
rate for both problems. As the user drives, the models use the sensor data to
classify whether the road is good or bad, and whether it contains potholes.
Then, the classification results are used to create data-rich maps that
illustrate road conditions across the city. Our system will empower civic
officials to identify and repair damaged roads which inconvenience passengers
and cause accidents. This paper details our data science process for collecting
training data on real roads, transforming noisy sensor data into useful
signals, training and evaluating machine learning models, and deploying those
models to production through a real-time classification app. It also highlights
how cities can use our system to crowdsource data and deliver road repair
resources to areas in need.",arxiv
http://arxiv.org/abs/2106.07178v4,2021-10-11T10:02:11Z,2021-06-14T06:04:57Z,A Comprehensive Survey on Graph Anomaly Detection with Deep Learning,"Anomalies represent rare observations (e.g., data records or events) that
deviate significantly from others. Over several decades, research on anomaly
mining has received increasing interests due to the implications of these
occurrences in a wide range of disciplines. Anomaly detection, which aims to
identify rare observations, is among the most vital tasks in the world, and has
shown its power in preventing detrimental events, such as financial fraud,
network intrusion, and social spam. The detection task is typically solved by
identifying outlying data points in the feature space and inherently overlooks
the relational information in real-world data. Graphs have been prevalently
used to represent the structural information, which raises the graph anomaly
detection problem - identifying anomalous graph objects (i.e., nodes, edges and
sub-graphs) in a single graph, or anomalous graphs in a database/set of graphs.
However, conventional anomaly detection techniques cannot tackle this problem
well because of the complexity of graph data. For the advent of deep learning,
graph anomaly detection with deep learning has received a growing attention
recently. In this survey, we aim to provide a systematic and comprehensive
review of the contemporary deep learning techniques for graph anomaly
detection. We compile open-sourced implementations, public datasets, and
commonly-used evaluation metrics to provide affluent resources for future
studies. More importantly, we highlight twelve extensive future research
directions according to our survey results covering unsolved and emerging
research problems and real-world applications. With this survey, our goal is to
create a ""one-stop-shop"" that provides a unified understanding of the problem
categories and existing approaches, publicly available hands-on resources, and
high-impact open challenges for graph anomaly detection using deep learning.",arxiv
http://arxiv.org/abs/2003.01207v1,2020-03-02T21:55:35Z,2020-03-02T21:55:35Z,"BARD: A structured technique for group elicitation of Bayesian networks
  to support analytic reasoning","In many complex, real-world situations, problem solving and decision making
require effective reasoning about causation and uncertainty. However, human
reasoning in these cases is prone to confusion and error. Bayesian networks
(BNs) are an artificial intelligence technology that models uncertain
situations, supporting probabilistic and causal reasoning and decision making.
However, to date, BN methodologies and software require significant upfront
training, do not provide much guidance on the model building process, and do
not support collaboratively building BNs. BARD (Bayesian ARgumentation via
Delphi) is both a methodology and an expert system that utilises (1) BNs as the
underlying structured representations for better argument analysis, (2) a
multi-user web-based software platform and Delphi-style social processes to
assist with collaboration, and (3) short, high-quality e-courses on demand, a
highly structured process to guide BN construction, and a variety of helpful
tools to assist in building and reasoning with BNs, including an automated
explanation tool to assist effective report writing. The result is an
end-to-end online platform, with associated online training, for groups without
prior BN expertise to understand and analyse a problem, build a model of its
underlying probabilistic causal structure, validate and reason with the causal
model, and use it to produce a written analytic report. Initial experimental
results demonstrate that BARD aids in problem solving, reasoning and
collaboration.",arxiv
http://arxiv.org/abs/2005.08337v1,2020-05-17T18:46:23Z,2020-05-17T18:46:23Z,A Survey on Unknown Presentation Attack Detection for Fingerprint,"Fingerprint recognition systems are widely deployed in various real-life
applications as they have achieved high accuracy. The widely used applications
include border control, automated teller machine (ATM), and attendance
monitoring systems. However, these critical systems are prone to spoofing
attacks (a.k.a presentation attacks (PA)). PA for fingerprint can be performed
by presenting gummy fingers made from different materials such as silicone,
gelatine, play-doh, ecoflex, 2D printed paper, 3D printed material, or latex.
Biometrics Researchers have developed Presentation Attack Detection (PAD)
methods as a countermeasure to PA. PAD is usually done by training a machine
learning classifier for known attacks for a given dataset, and they achieve
high accuracy in this task. However, generalizing to unknown attacks is an
essential problem from applicability to real-world systems, mainly because
attacks cannot be exhaustively listed in advance. In this survey paper, we
present a comprehensive survey on existing PAD algorithms for fingerprint
recognition systems, specifically from the standpoint of detecting unknown PAD.
We categorize PAD algorithms, point out their advantages/disadvantages, and
future directions for this area.",arxiv
http://arxiv.org/abs/adap-org/9807003v1,1998-07-17T01:07:50Z,1998-07-17T01:07:50Z,Development and Evolution of Neural Networks in an Artificial Chemistry,"We present a model of decentralized growth for Artificial Neural Networks
(ANNs) inspired by the development and the physiology of real nervous systems.
In this model, each individual artificial neuron is an autonomous unit whose
behavior is determined only by the genetic information it harbors and local
concentrations of substrates modeled by a simple artificial chemistry. Gene
expression is manifested as axon and dendrite growth, cell division and
differentiation, substrate production and cell stimulation. We demonstrate the
model's power with a hand-written genome that leads to the growth of a simple
network which performs classical conditioning. To evolve more complex
structures, we implemented a platform-independent, asynchronous, distributed
Genetic Algorithm (GA) that allows users to participate in evolutionary
experiments via the World Wide Web.",arxiv
http://arxiv.org/abs/1701.00038v1,2016-12-31T00:27:42Z,2016-12-31T00:27:42Z,Sparsity enabled cluster reduced-order models for control,"Characterizing and controlling nonlinear, multi-scale phenomena play
important roles in science and engineering. Cluster-based reduced-order
modeling (CROM) was introduced to exploit the underlying low-dimensional
dynamics of complex systems. CROM builds a data-driven discretization of the
Perron-Frobenius operator, resulting in a probabilistic model for ensembles of
trajectories. A key advantage of CROM is that it embeds nonlinear dynamics in a
linear framework, and uncertainty can be managed with data assimilation. CROM
is typically computed on high-dimensional data, however, access to and
computations on this full-state data limit the online implementation of CROM
for prediction and control. Here, we address this key challenge by identifying
a small subset of critical measurements to learn an efficient CROM, referred to
as sparsity-enabled CROM. In particular, we leverage compressive measurements
to faithfully embed the cluster geometry and preserve the probabilistic
dynamics. Further, we show how to identify fewer optimized sensor locations
tailored to a specific problem that outperform random measurements. Both of
these sparsity-enabled sensing strategies significantly reduce the burden of
data acquisition and processing for low-latency in-time estimation and control.
We illustrate this unsupervised learning approach on three different
high-dimensional nonlinear dynamical systems from fluids with increasing
complexity, with one application in flow control. Sparsity-enabled CROM is a
critical facilitator for real-time implementation on high-dimensional systems
where full-state information may be inaccessible.",arxiv
http://arxiv.org/abs/1911.09281v1,2019-11-21T04:19:16Z,2019-11-21T04:19:16Z,"Event Detection in Noisy Streaming Data with Combination of
  Corroborative and Probabilistic Sources","Global physical event detection has traditionally relied on dense coverage of
physical sensors around the world; while this is an expensive undertaking,
there have not been alternatives until recently. The ubiquity of social
networks and human sensors in the field provides a tremendous amount of
real-time, live data about true physical events from around the world. However,
while such human sensor data have been exploited for retrospective large-scale
event detection, such as hurricanes or earthquakes, they has been limited to no
success in exploiting this rich resource for general physical event detection.
  Prior implementation approaches have suffered from the concept drift
phenomenon, where real-world data exhibits constant, unknown, unbounded changes
in its data distribution, making static machine learning models ineffective in
the long term. We propose and implement an end-to-end collaborative drift
adaptive system that integrates corroborative and probabilistic sources to
deliver real-time predictions. Furthermore, out system is adaptive to concept
drift and performs automated continuous learning to maintain high performance.
We demonstrate our approach in a real-time demo available online for landslide
disaster detection, with extensibility to other real-world physical events such
as flooding, wildfires, hurricanes, and earthquakes.",arxiv
http://arxiv.org/abs/1903.05431v1,2019-03-13T11:54:04Z,2019-03-13T11:54:04Z,"Resource Abstraction for Reinforcement Learning in Multiagent Congestion
  Problems","Real-world congestion problems (e.g. traffic congestion) are typically very
complex and large-scale. Multiagent reinforcement learning (MARL) is a
promising candidate for dealing with this emerging complexity by providing an
autonomous and distributed solution to these problems. However, there are three
limiting factors that affect the deployability of MARL approaches to congestion
problems. These are learning time, scalability and decentralised coordination
i.e. no communication between the learning agents. In this paper we introduce
Resource Abstraction, an approach that addresses these challenges by allocating
the available resources into abstract groups. This abstraction creates new
reward functions that provide a more informative signal to the learning agents
and aid the coordination amongst them. Experimental work is conducted on two
benchmark domains from the literature, an abstract congestion problem and a
realistic traffic congestion problem. The current state-of-the-art for solving
multiagent congestion problems is a form of reward shaping called difference
rewards. We show that the system using Resource Abstraction significantly
improves the learning speed and scalability, and achieves the highest possible
or near-highest joint performance/social welfare for both congestion problems
in large-scale scenarios involving up to 1000 reinforcement learning agents.",arxiv
http://arxiv.org/abs/1908.04172v2,2019-08-29T19:16:10Z,2019-08-12T14:30:13Z,"nGraph-HE2: A High-Throughput Framework for Neural Network Inference on
  Encrypted Data","In previous work, Boemer et al. introduced nGraph-HE, an extension to the
Intel nGraph deep learning (DL) compiler, that enables data scientists to
deploy models with popular frameworks such as TensorFlow and PyTorch with
minimal code changes. However, the class of supported models was limited to
relatively shallow networks with polynomial activations. Here, we introduce
nGraph-HE2, which extends nGraph-HE to enable privacy-preserving inference on
standard, pre-trained models using their native activation functions and number
fields (typically real numbers). The proposed framework leverages the CKKS
scheme, whose support for real numbers is friendly to data science, and a
client-aided model using a two-party approach to compute activation functions.
  We first present CKKS-specific optimizations, enabling a 3x-88x runtime
speedup for scalar encoding, and doubling the throughput through a novel use of
CKKS plaintext packing into complex numbers. Second, we optimize
ciphertext-plaintext addition and multiplication, yielding 2.6x-4.2x runtime
speedup. Third, we exploit two graph-level optimizations: lazy rescaling and
depth-aware encoding, which allow us to significantly improve performance.
  Together, these optimizations enable state-of-the-art throughput of 1,998
images/s on the CryptoNets network. Using the client-aided model, we also
present homomorphic evaluation of (to our knowledge) the largest network to
date, namely, pre-trained MobileNetV2 models on the ImageNet dataset, with
60.4\percent/82.7\percent\ top-1/top-5 accuracy and an amortized runtime of 381
ms/image.",arxiv
http://arxiv.org/abs/2106.11686v1,2021-06-22T11:40:00Z,2021-06-22T11:40:00Z,Simulation-Driven COVID-19 Epidemiological Modeling with Social Media,"Modern Bayesian approaches and workflows emphasize in how simulation is
important in the context of model developing. Simulation can help researchers
understand how the model behaves in a controlled setting and can be used to
stress the model in different ways before it is exposed to any real data. This
improved understanding could be beneficial in epidemiological models, specially
when dealing with COVID-19. Unfortunately, few researchers perform any
simulations. We present a simulation algorithm that implements a simple
agent-based model for disease transmission that works with a standard
compartment epidemiological model for COVID-19. Our algorithm can be applied in
different parameterizations to reflect several plausible epidemic scenarios.
Additionally, we also model how social media information in the form of daily
symptom mentions can be incorporate into COVID-19 epidemiological models. We
test our social media COVID-19 model with two experiments. The first using
simulated data from our agent-based simulation algorithm and the second with
real data using a machine learning tweet classifier to identify tweets that
mention symptoms from noise. Our results shows how a COVID-19 model can be (1)
used to incorporate social media data and (2) assessed and evaluated with
simulated and real data.",arxiv
http://arxiv.org/abs/2006.09191v2,2020-10-25T23:11:44Z,2020-06-16T14:34:40Z,"Sample-Efficient Optimization in the Latent Space of Deep Generative
  Models via Weighted Retraining","Many important problems in science and engineering, such as drug design,
involve optimizing an expensive black-box objective function over a complex,
high-dimensional, and structured input space. Although machine learning
techniques have shown promise in solving such problems, existing approaches
substantially lack sample efficiency. We introduce an improved method for
efficient black-box optimization, which performs the optimization in the
low-dimensional, continuous latent manifold learned by a deep generative model.
In contrast to previous approaches, we actively steer the generative model to
maintain a latent manifold that is highly useful for efficiently optimizing the
objective. We achieve this by periodically retraining the generative model on
the data points queried along the optimization trajectory, as well as weighting
those data points according to their objective function value. This weighted
retraining can be easily implemented on top of existing methods, and is
empirically shown to significantly improve their efficiency and performance on
synthetic and real-world optimization problems.",arxiv
http://arxiv.org/abs/1512.01818v5,2016-07-14T22:51:39Z,2015-12-06T18:52:51Z,"SentiBench - a benchmark comparison of state-of-the-practice sentiment
  analysis methods","In the last few years thousands of scientific papers have investigated
sentiment analysis, several startups that measure opinions on real data have
emerged and a number of innovative products related to this theme have been
developed. There are multiple methods for measuring sentiments, including
lexical-based and supervised machine learning methods. Despite the vast
interest on the theme and wide popularity of some methods, it is unclear which
one is better for identifying the polarity (i.e., positive or negative) of a
message. Accordingly, there is a strong need to conduct a thorough
apple-to-apple comparison of sentiment analysis methods, \textit{as they are
used in practice}, across multiple datasets originated from different data
sources. Such a comparison is key for understanding the potential limitations,
advantages, and disadvantages of popular methods. This article aims at filling
this gap by presenting a benchmark comparison of twenty-four popular sentiment
analysis methods (which we call the state-of-the-practice methods). Our
evaluation is based on a benchmark of eighteen labeled datasets, covering
messages posted on social networks, movie and product reviews, as well as
opinions and comments in news articles. Our results highlight the extent to
which the prediction performance of these methods varies considerably across
datasets. Aiming at boosting the development of this research area, we open the
methods' codes and datasets used in this article, deploying them in a benchmark
system, which provides an open API for accessing and comparing sentence-level
sentiment analysis methods.",arxiv
http://arxiv.org/abs/2011.02838v1,2020-10-11T15:04:34Z,2020-10-11T15:04:34Z,"Real-time parameter inference in reduced-order flame models with
  heteroscedastic Bayesian neural network ensembles","The estimation of model parameters with uncertainties from observed data is a
ubiquitous inverse problem in science and engineering. In this paper, we
suggest an inexpensive and easy to implement parameter estimation technique
that uses a heteroscedastic Bayesian Neural Network trained using anchored
ensembling. The heteroscedastic aleatoric error of the network models the
irreducible uncertainty due to parameter degeneracies in our inverse problem,
while the epistemic uncertainty of the Bayesian model captures uncertainties
which may arise from an input observation's out-of-distribution nature. We use
this tool to perform real-time parameter inference in a 6 parameter G-equation
model of a ducted, premixed flame from observations of acoustically excited
flames. We train our networks on a library of 2.1 million simulated flame
videos. Results on the test dataset of simulated flames show that the network
recovers flame model parameters, with the correlation coefficient between
predicted and true parameters ranging from 0.97 to 0.99, and well-calibrated
uncertainty estimates. The trained neural networks are then used to infer model
parameters from real videos of a premixed Bunsen flame captured using a
high-speed camera in our lab. Re-simulation using inferred parameters shows
excellent agreement between the real and simulated flames. Compared to Ensemble
Kalman Filter-based tools that have been proposed for this problem in the
combustion literature, our neural network ensemble achieves better
data-efficiency and our sub-millisecond inference times represent a savings on
computational costs by several orders of magnitude. This allows us to calibrate
our reduced-order flame model in real-time and predict the thermoacoustic
instability behaviour of the flame more accurately.",arxiv
http://arxiv.org/abs/2005.10224v2,2021-06-05T22:47:16Z,2020-05-20T17:41:40Z,The Random Feature Model for Input-Output Maps between Banach Spaces,"Well known to the machine learning community, the random feature model is a
parametric approximation to kernel interpolation or regression methods. It is
typically used to approximate functions mapping a finite-dimensional input
space to the real line. In this paper, we instead propose a methodology for use
of the random feature model as a data-driven surrogate for operators that map
an input Banach space to an output Banach space. Although the methodology is
quite general, we consider operators defined by partial differential equations
(PDEs); here, the inputs and outputs are themselves functions, with the input
parameters being functions required to specify the problem, such as initial
data or coefficients, and the outputs being solutions of the problem. Upon
discretization, the model inherits several desirable attributes from this
infinite-dimensional viewpoint, including mesh-invariant approximation error
with respect to the true PDE solution map and the capability to be trained at
one mesh resolution and then deployed at different mesh resolutions. We view
the random feature model as a non-intrusive data-driven emulator, provide a
mathematical framework for its interpretation, and demonstrate its ability to
efficiently and accurately approximate the nonlinear parameter-to-solution maps
of two prototypical PDEs arising in physical science and engineering
applications: viscous Burgers' equation and a variable coefficient elliptic
equation.",arxiv
http://arxiv.org/abs/2101.04086v1,2021-01-11T18:29:50Z,2021-01-11T18:29:50Z,"System Design for a Data-driven and Explainable Customer Sentiment
  Monitor","The most important goal of customer services is to keep the customer
satisfied. However, service resources are always limited and must be
prioritized. Therefore, it is important to identify customers who potentially
become unsatisfied and might lead to escalations. Today this prioritization of
customers is often done manually. Data science on IoT data (esp. log data) for
machine health monitoring, as well as analytics on enterprise data for customer
relationship management (CRM) have mainly been researched and applied
independently. In this paper, we present a framework for a data-driven decision
support system which combines IoT and enterprise data to model customer
sentiment. Such decision support systems can help to prioritize customers and
service resources to effectively troubleshoot problems or even avoid them. The
framework is applied in a real-world case study with a major medical device
manufacturer. This includes a fully automated and interpretable machine
learning pipeline designed to meet the requirements defined with domain experts
and end users. The overall framework is currently deployed, learns and
evaluates predictive models from terabytes of IoT and enterprise data to
actively monitor the customer sentiment for a fleet of thousands of high-end
medical devices. Furthermore, we provide an anonymized industrial benchmark
dataset for the research community.",arxiv
http://arxiv.org/abs/1907.09209v1,2019-07-22T10:04:22Z,2019-07-22T10:04:22Z,"Automatic Calibration of Artificial Neural Networks for Zebrafish
  Collective Behaviours using a Quality Diversity Algorithm","During the last two decades, various models have been proposed for fish
collective motion. These models are mainly developed to decipher the biological
mechanisms of social interaction between animals. They consider very simple
homogeneous unbounded environments and it is not clear that they can simulate
accurately the collective trajectories. Moreover when the models are more
accurate, the question of their scalability to either larger groups or more
elaborate environments remains open. This study deals with learning how to
simulate realistic collective motion of collective of zebrafish, using
real-world tracking data. The objective is to devise an agent-based model that
can be implemented on an artificial robotic fish that can blend into a
collective of real fish. We present a novel approach that uses Quality
Diversity algorithms, a class of algorithms that emphasise exploration over
pure optimisation. In particular, we use CVT-MAP-Elites, a variant of the
state-of-the-art MAP-Elites algorithm for high dimensional search space.
Results show that Quality Diversity algorithms not only outperform classic
evolutionary reinforcement learning methods at the macroscopic level (i.e.
group behaviour), but are also able to generate more realistic biomimetic
behaviours at the microscopic level (i.e. individual behaviour).",arxiv
http://arxiv.org/abs/1810.08517v1,2018-10-19T14:21:14Z,2018-10-19T14:21:14Z,"Developing a seismic pattern interpretation network (SpiNet) for
  automated seismic interpretation","Seismic interpretation is now serving as a fundamental tool for depicting
subsurface geology and assisting activities in various domains, such as
environmental engineering and petroleum exploration. However, most of the
existing interpretation techniques are designed for interpreting a certain
seismic pattern (e.g., faults and salt domes) in a given seismic dataset at one
time; correspondingly, the rest patterns would be ignored. Interpreting all the
important seismic patterns becomes feasible with the aid of multiple
classification techniques. When implementing them into the seismic domain,
however, the major drawback is the low efficiency particularly for a large
dataset, since the classification need to be repeated at every seismic sample.
To resolve such limitation, this study first present a seismic pattern
interpretation dataset (SpiDat), which tentatively categorizes 12
commonly-observed seismic patterns based on their signal intensity and lateral
geometry, including these of important geologic implications such as faults,
salt domes, gas chimneys, and depositional sequences. Then we propose a seismic
pattern interpretation network (SpiNet) based on the state-of-the-art
deconvolutional neural network, which is capable of automatically recognizing
and annotating the 12 defined seismic patterns in real time. The impacts of the
proposed SpiNet come in two folds. First, applying the SpiNet to a seismic cube
allows interpreters to quickly identify the important seismic patterns as input
to advanced interpretation and modeling. Second, the SpiNet paves the
foundation for deriving more task-oriented seismic interpretation networks,
such as fault detection. It is concluded that the proposed SpiNet holds great
potentials for assisting the major seismic interpretation challenges and
advancing it further towards cognitive seismic data analysis.",arxiv
http://arxiv.org/abs/2105.11914v2,2021-08-24T13:58:11Z,2021-05-25T13:17:35Z,Theory and Design of Super-resolution Haptic Skins,"Haptic feedback is important to make robots more dexterous and effective in
unstructured environments. High-resolution haptic sensors are still not widely
available, and their application is often bound by the resolution-robustness
dilemma. A route towards high-resolution and robust skin embeds a few sensor
units (taxels) into a flexible surface material and uses signal processing to
achieve sensing with super-resolution accuracy. We propose a theory for
geometric super-resolution to guide the development of haptic sensors of this
kind and link it to machine learning techniques for signal processing. This
theory is based on sensor isolines and allows us to predict force sensitivity
and accuracy in contact position and force magnitude as a spatial quantity. We
evaluate the influence of different factors, such as elastic properties of the
material, structure design, and transduction methods, using finite element
simulations and by implementing real sensors. We empirically determine sensor
isolines and validate the theory in two custom-built sensors with barometric
units for 1D and 2D measurement surfaces. Using machine learning methods for
the inference of contact information, our sensors obtain an unparalleled
average super-resolution factor of over 100 and 1200, respectively. Our theory
can guide future haptic sensor designs and inform various design choices.",arxiv
http://arxiv.org/abs/2110.06674v1,2021-10-13T12:18:09Z,2021-10-13T12:18:09Z,Truthful AI: Developing and governing AI that does not lie,"In many contexts, lying -- the use of verbal falsehoods to deceive -- is
harmful. While lying has traditionally been a human affair, AI systems that
make sophisticated verbal statements are becoming increasingly prevalent. This
raises the question of how we should limit the harm caused by AI ""lies"" (i.e.
falsehoods that are actively selected for). Human truthfulness is governed by
social norms and by laws (against defamation, perjury, and fraud). Differences
between AI and humans present an opportunity to have more precise standards of
truthfulness for AI, and to have these standards rise over time. This could
provide significant benefits to public epistemics and the economy, and mitigate
risks of worst-case AI futures.
  Establishing norms or laws of AI truthfulness will require significant work
to: (1) identify clear truthfulness standards; (2) create institutions that can
judge adherence to those standards; and (3) develop AI systems that are
robustly truthful.
  Our initial proposals for these areas include: (1) a standard of avoiding
""negligent falsehoods"" (a generalisation of lies that is easier to assess); (2)
institutions to evaluate AI systems before and after real-world deployment; and
(3) explicitly training AI systems to be truthful via curated datasets and
human interaction.
  A concerning possibility is that evaluation mechanisms for eventual
truthfulness standards could be captured by political interests, leading to
harmful censorship and propaganda. Avoiding this might take careful attention.
And since the scale of AI speech acts might grow dramatically over the coming
decades, early truthfulness standards might be particularly important because
of the precedents they set.",arxiv
http://arxiv.org/abs/2004.00622v1,2020-04-01T17:59:59Z,2020-04-01T17:59:59Z,Evading Deepfake-Image Detectors with White- and Black-Box Attacks,"It is now possible to synthesize highly realistic images of people who don't
exist. Such content has, for example, been implicated in the creation of
fraudulent social-media profiles responsible for dis-information campaigns.
Significant efforts are, therefore, being deployed to detect
synthetically-generated content. One popular forensic approach trains a neural
network to distinguish real from synthetic content.
  We show that such forensic classifiers are vulnerable to a range of attacks
that reduce the classifier to near-0% accuracy. We develop five attack case
studies on a state-of-the-art classifier that achieves an area under the ROC
curve (AUC) of 0.95 on almost all existing image generators, when only trained
on one generator. With full access to the classifier, we can flip the lowest
bit of each pixel in an image to reduce the classifier's AUC to 0.0005; perturb
1% of the image area to reduce the classifier's AUC to 0.08; or add a single
noise pattern in the synthesizer's latent space to reduce the classifier's AUC
to 0.17. We also develop a black-box attack that, with no access to the target
classifier, reduces the AUC to 0.22. These attacks reveal significant
vulnerabilities of certain image-forensic classifiers.",arxiv
http://arxiv.org/abs/1903.06802v1,2019-02-26T02:09:31Z,2019-02-26T02:09:31Z,"Workflow-Driven Distributed Machine Learning in CHASE-CI: A Cognitive
  Hardware and Software Ecosystem Community Infrastructure","The advances in data, computing and networking over the last two decades led
to a shift in many application domains that includes machine learning on big
data as a part of the scientific process, requiring new capabilities for
integrated and distributed hardware and software infrastructure. This paper
contributes a workflow-driven approach for dynamic data-driven application
development on top of a new kind of networked Cyberinfrastructure called
CHASE-CI. In particular, we present: 1) The architecture for CHASE-CI, a
network of distributed fast GPU appliances for machine learning and storage
managed through Kubernetes on the high-speed (10-100Gbps) Pacific Research
Platform (PRP); 2) A machine learning software containerization approach and
libraries required for turning such a network into a distributed computer for
big data analysis; 3) An atmospheric science case study that can only be made
scalable with an infrastructure like CHASE-CI; 4) Capabilities for virtual
cluster management for data communication and analysis in a dynamically
scalable fashion, and visualization across the network in specialized
visualization facilities in near real-time; and, 5) A step-by-step workflow and
performance measurement approach that enables taking advantage of the dynamic
architecture of the CHASE-CI network and container management infrastructure.",arxiv
http://arxiv.org/abs/2001.03855v1,2020-01-12T05:25:02Z,2020-01-12T05:25:02Z,"Hyperparameters optimization for Deep Learning based emotion prediction
  for Human Robot Interaction","To enable humanoid robots to share our social space we need to develop
technology for easy interaction with the robots using multiple modes such as
speech, gestures and share our emotions with them. We have targeted this
research towards addressing the core issue of emotion recognition problem which
would require less computation resources and much lesser number of network
hyperparameters which will be more adaptive to be computed on low resourced
social robots for real time communication. More specifically, here we have
proposed an Inception module based Convolutional Neural Network Architecture
which has achieved improved accuracy of upto 6% improvement over the existing
network architecture for emotion classification when combinedly tested over
multiple datasets when tried over humanoid robots in real - time. Our proposed
model is reducing the trainable Hyperparameters to an extent of 94% as compared
to vanilla CNN model which clearly indicates that it can be used in real time
based application such as human robot interaction. Rigorous experiments have
been performed to validate our methodology which is sufficiently robust and
could achieve high level of accuracy. Finally, the model is implemented in a
humanoid robot, NAO in real time and robustness of the model is evaluated.",arxiv
http://arxiv.org/abs/2004.12161v1,2020-04-25T14:52:11Z,2020-04-25T14:52:11Z,"DAN-SNR: A Deep Attentive Network for Social-Aware Next
  Point-of-Interest Recommendation","Next (or successive) point-of-interest (POI) recommendation has attracted
increasing attention in recent years. Most of the previous studies attempted to
incorporate the spatiotemporal information and sequential patterns of user
check-ins into recommendation models to predict the target user's next move.
However, none of these approaches utilized the social influence of each user's
friends. In this study, we discuss a new topic of next POI recommendation and
present a deep attentive network for social-aware next POI recommendation
called DAN-SNR. In particular, the DAN-SNR makes use of the self-attention
mechanism instead of the architecture of recurrent neural networks to model
sequential influence and social influence in a unified manner. Moreover, we
design and implement two parallel channels to capture short-term user
preference and long-term user preference as well as social influence,
respectively. By leveraging multi-head self-attention, the DAN-SNR can model
long-range dependencies between any two historical check-ins efficiently and
weigh their contributions to the next destination adaptively. Also, we carried
out a comprehensive evaluation using large-scale real-world datasets collected
from two popular location-based social networks, namely Gowalla and Brightkite.
Experimental results indicate that the DAN-SNR outperforms seven competitive
baseline approaches regarding recommendation performance and is of high
efficiency among six neural-network- and attention-based methods.",arxiv
http://arxiv.org/abs/2007.03639v3,2021-01-11T11:02:34Z,2020-07-07T17:19:56Z,Human Trajectory Forecasting in Crowds: A Deep Learning Perspective,"Since the past few decades, human trajectory forecasting has been a field of
active research owing to its numerous real-world applications: evacuation
situation analysis, deployment of intelligent transport systems, traffic
operations, to name a few. Early works handcrafted this representation based on
domain knowledge. However, social interactions in crowded environments are not
only diverse but often subtle. Recently, deep learning methods have
outperformed their handcrafted counterparts, as they learned about human-human
interactions in a more generic data-driven fashion. In this work, we present an
in-depth analysis of existing deep learning-based methods for modelling social
interactions. We propose two knowledge-based data-driven methods to effectively
capture these social interactions. To objectively compare the performance of
these interaction-based forecasting models, we develop a large scale
interaction-centric benchmark TrajNet++, a significant yet missing component in
the field of human trajectory forecasting. We propose novel performance metrics
that evaluate the ability of a model to output socially acceptable
trajectories. Experiments on TrajNet++ validate the need for our proposed
metrics, and our method outperforms competitive baselines on both real-world
and synthetic datasets.",arxiv
http://arxiv.org/abs/0810.4945v1,2008-10-27T21:39:04Z,2008-10-27T21:39:04Z,New Approaches to Object Classification in Synoptic Sky Surveys,"Digital synoptic sky surveys pose several new object classification
challenges. In surveys where real-time detection and classification of
transient events is a science driver, there is a need for an effective
elimination of instrument-related artifacts which can masquerade as transient
sources in the detection pipeline, e.g., unremoved large cosmic rays,
saturation trails, reflections, crosstalk artifacts, etc. We have implemented
such an Artifact Filter, using a supervised neural network, for the real-time
processing pipeline in the Palomar-Quest (PQ) survey. After the training phase,
for each object it takes as input a set of measured morphological parameters
and returns the probability of it being a real object. Despite the relatively
low number of training cases for many kinds of artifacts, the overall artifact
classification rate is around 90%, with no genuine transients misclassified
during our real-time scans. Another question is how to assign an optimal
star-galaxy classification in a multi-pass survey, where seeing and other
conditions change between different epochs, potentially producing inconsistent
classifications for the same object. We have implemented a star/galaxy
multipass classifier that makes use of external and a priori knowledge to find
the optimal classification from the individually derived ones. Both these
techniques can be applied to other, similar surveys and data sets.",arxiv
http://arxiv.org/abs/2104.09650v1,2021-04-19T21:32:44Z,2021-04-19T21:32:44Z,"Mapping the Internet: Modelling Entity Interactions in Complex
  Heterogeneous Networks","Even though machine learning algorithms already play a significant role in
data science, many current methods pose unrealistic assumptions on input data.
The application of such methods is difficult due to incompatible data formats,
or heterogeneous, hierarchical or entirely missing data fragments in the
dataset. As a solution, we propose a versatile, unified framework called
`HMill' for sample representation, model definition and training. We review in
depth a multi-instance paradigm for machine learning that the framework builds
on and extends. To theoretically justify the design of key components of HMill,
we show an extension of the universal approximation theorem to the set of all
functions realized by models implemented in the framework. The text also
contains a detailed discussion on technicalities and performance improvements
in our implementation, which is published for download under the MIT License.
The main asset of the framework is its flexibility, which makes modelling of
diverse real-world data sources with the same tool possible. Additionally to
the standard setting in which a set of attributes is observed for each object
individually, we explain how message-passing inference in graphs that represent
whole systems of objects can be implemented in the framework. To support our
claims, we solve three different problems from the cybersecurity domain using
the framework. The first use case concerns IoT device identification from raw
network observations. In the second problem, we study how malicious binary
files can be classified using a snapshot of the operating system represented as
a directed graph. The last provided example is a task of domain blacklist
extension through modelling interactions between entities in the network. In
all three problems, the solution based on the proposed framework achieves
performance comparable to specialized approaches.",arxiv
http://arxiv.org/abs/1607.02168v1,2016-07-07T20:47:05Z,2016-07-07T20:47:05Z,Discovering Boolean Gates in Slime Mould,"Slime mould of Physarum polycephalum is a large cell exhibiting rich spatial
non-linear electrical characteristics. We exploit the electrical properties of
the slime mould to implement logic gates using a flexible hardware platform
designed for investigating the electrical properties of a substrate (MECOBO).
We apply arbitrary electrical signals to `configure' the slime mould, i.e.
change shape of its body and, measure the slime mould's electrical response. We
show that it is possible to find configurations that allow the Physarum to act
as any 2-input Boolean gate. The occurrence frequency of the gates discovered
in the slime was analysed and compared to complexity hierarchies of logical
gates obtained in other unconventional materials. The search for gates was
performed by both sweeping across configurations in the real material as well
as training a neural network-based model and searching the gates therein using
gradient descent.",arxiv
http://arxiv.org/abs/2105.06631v4,2021-09-15T14:46:28Z,2021-05-14T03:49:59Z,Ordering-Based Causal Discovery with Reinforcement Learning,"It is a long-standing question to discover causal relations among a set of
variables in many empirical sciences. Recently, Reinforcement Learning (RL) has
achieved promising results in causal discovery from observational data.
However, searching the space of directed graphs and enforcing acyclicity by
implicit penalties tend to be inefficient and restrict the existing RL-based
method to small scale problems. In this work, we propose a novel RL-based
approach for causal discovery, by incorporating RL into the ordering-based
paradigm. Specifically, we formulate the ordering search problem as a
multi-step Markov decision process, implement the ordering generating process
with an encoder-decoder architecture, and finally use RL to optimize the
proposed model based on the reward mechanisms designed for~each ordering. A
generated ordering would then be processed using variable selection to obtain
the final causal graph. We analyze the consistency and computational complexity
of the proposed method, and empirically show that a pretrained model can be
exploited to accelerate training. Experimental results on both synthetic and
real data sets shows that the proposed method achieves a much improved
performance over existing RL-based method.",arxiv
http://arxiv.org/abs/1901.05356v1,2019-01-16T15:56:19Z,2019-01-16T15:56:19Z,"How to Host a Data Competition: Statistical Advice for Design and
  Analysis of a Data Competition","Data competitions rely on real-time leaderboards to rank competitor entries
and stimulate algorithm improvement. While such competitions have become quite
popular and prevalent, particularly in supervised learning formats, their
implementations by the host are highly variable. Without careful planning, a
supervised learning competition is vulnerable to overfitting, where the winning
solutions are so closely tuned to the particular set of provided data that they
cannot generalize to the underlying problem of interest to the host. This paper
outlines some important considerations for strategically designing relevant and
informative data sets to maximize the learning outcome from hosting a
competition based on our experience. It also describes a post-competition
analysis that enables robust and efficient assessment of the strengths and
weaknesses of solutions from different competitors, as well as greater
understanding of the regions of the input space that are well-solved. The
post-competition analysis, which complements the leaderboard, uses exploratory
data analysis and generalized linear models (GLMs). The GLMs not only expand
the range of results we can explore, they also provide more detailed analysis
of individual sub-questions including similarities and differences between
algorithms across different types of scenarios, universally easy or hard
regions of the input space, and different learning objectives. When coupled
with a strategically planned data generation approach, the methods provide
richer and more informative summaries to enhance the interpretation of results
beyond just the rankings on the leaderboard. The methods are illustrated with a
recently completed competition to evaluate algorithms capable of detecting,
identifying, and locating radioactive materials in an urban environment.",arxiv
http://arxiv.org/abs/2103.09054v1,2021-03-07T14:59:12Z,2021-03-07T14:59:12Z,Sentiment Analysis for Troll Detection on Weibo,"The impact of social media on the modern world is difficult to overstate.
Virtually all companies and public figures have social media accounts on
popular platforms such as Twitter and Facebook. In China, the micro-blogging
service provider, Sina Weibo, is the most popular such service. To influence
public opinion, Weibo trolls -- the so called Water Army -- can be hired to
post deceptive comments. In this paper, we focus on troll detection via
sentiment analysis and other user activity data on the Sina Weibo platform. We
implement techniques for Chinese sentence segmentation, word embedding, and
sentiment score calculation. In recent years, troll detection and sentiment
analysis have been studied, but we are not aware of previous research that
considers troll detection based on sentiment analysis. We employ the resulting
techniques to develop and test a sentiment analysis approach for troll
detection, based on a variety of machine learning strategies. Experimental
results are generated and analyzed. A Chrome extension is presented that
implements our proposed technique, which enables real-time troll detection when
a user browses Sina Weibo.",arxiv
http://arxiv.org/abs/2106.12372v2,2021-06-25T08:09:48Z,2021-06-23T13:09:58Z,Real-time Neural Radiance Caching for Path Tracing,"We present a real-time neural radiance caching method for path-traced global
illumination. Our system is designed to handle fully dynamic scenes, and makes
no assumptions about the lighting, geometry, and materials. The data-driven
nature of our approach sidesteps many difficulties of caching algorithms, such
as locating, interpolating, and updating cache points. Since pretraining neural
networks to handle novel, dynamic scenes is a formidable generalization
challenge, we do away with pretraining and instead achieve generalization via
adaptation, i.e. we opt for training the radiance cache while rendering. We
employ self-training to provide low-noise training targets and simulate
infinite-bounce transport by merely iterating few-bounce training updates. The
updates and cache queries incur a mild overhead -- about 2.6ms on full HD
resolution -- thanks to a streaming implementation of the neural network that
fully exploits modern hardware. We demonstrate significant noise reduction at
the cost of little induced bias, and report state-of-the-art, real-time
performance on a number of challenging scenarios.",arxiv
http://arxiv.org/abs/2001.09938v1,2019-10-22T15:57:20Z,2019-10-22T15:57:20Z,"Autonomous discovery of battery electrolytes with robotic
  experimentation and machine-learning","Innovations in batteries take years to formulate and commercialize, requiring
extensive experimentation during the design and optimization phases. We
approached the design and selection of a battery electrolyte through a
black-box optimization algorithm directly integrated into a robotic test-stand.
We report here the discovery of a novel battery electrolyte by this experiment
completely guided by the machine-learning software without human intervention.
Motivated by the recent trend toward super-concentrated aqueous electrolytes
for high-performance batteries, we utilize Dragonfly - a Bayesian
machine-learning software package - to search mixtures of commonly used lithium
and sodium salts for super-concentrated aqueous electrolytes with wide
electrochemical stability windows. Dragonfly autonomously managed the robotic
test-stand, recommending electrolyte designs to test and receiving experimental
feedback in real time. In 40 hours of continuous experimentation over a
four-dimensional design space with millions of potential candidates, Dragonfly
discovered a novel, mixed-anion aqueous sodium electrolyte with a wider
electrochemical stability window than state-of-the-art sodium electrolyte. A
human-guided design process may have missed this optimal electrolyte. This
result demonstrates the possibility of integrating robotics with
machine-learning to rapidly and autonomously discover novel battery materials.",arxiv
http://arxiv.org/abs/2103.06109v2,2021-04-29T09:01:45Z,2021-03-10T15:03:48Z,Session-based Social and Dependency-aware Software Recommendation,"With the increase of complexity of modern software, social collaborative
coding and reuse of open source software packages become more and more popular,
which thus greatly enhances the development efficiency and software quality.
However, the explosive growth of open source software packages exposes
developers to the challenge of information overload. While this can be
addressed by conventional recommender systems, they usually do not consider
particular constraints of social coding such as social influence among
developers and dependency relations among software packages. In this paper, we
aim to model the dynamic interests of developers with both social influence and
dependency constraints, and propose the Session-based Social and
Dependency-aware software Recommendation (SSDRec) model. This model integrates
recurrent neural network (RNN) and graph attention network (GAT) into a unified
framework. A RNN is employed to model the short-term dynamic interests of
developers in each session and two GATs are utilized to capture social
influence from friends and dependency constraints from dependent software
packages, respectively. Extensive experiments are conducted on real-world
datasets and the results demonstrate that our model significantly outperforms
the competitive baselines.",arxiv
http://arxiv.org/abs/2001.07427v1,2020-01-21T10:17:39Z,2020-01-21T10:17:39Z,Circuit Implementation of a Four-Dimensional Topological Insulator,"The classification of topological insulators predicts the existence of
high-dimensional topological phases that cannot occur in real materials, as
these are limited to three or fewer spatial dimensions. We use electric
circuits to experimentally implement a four-dimensional (4D) topological
lattice. The lattice dimensionality is established by circuit connections, and
not by mapping to a lower-dimensional system. On the lattice's
three-dimensional surface, we observe topological surface states that are
associated with a nonzero second Chern number but vanishing first Chern
numbers. The 4D lattice belongs to symmetry class AI, which refers to
time-reversal-invariant and spinless systems with no special spatial symmetry.
Class AI is topologically trivial in one to three spatial dimensions, so 4D is
the lowest possible dimension for achieving a topological insulator in this
class. This work paves the way to the use of electric circuits for exploring
high-dimensional topological models.",arxiv
http://arxiv.org/abs/2102.04871v1,2021-02-09T15:14:27Z,2021-02-09T15:14:27Z,The Factory Must Grow: Automation in Factorio,"Efficient optimization of resources is paramount to success in many problems
faced today. In the field of operational research the efficient scheduling of
employees; packing of vans; routing of vehicles; logistics of airlines and
transport of materials can be the difference between emission reduction or
excess, profits or losses and feasibility or unworkable solutions. The video
game Factorio, by Wube Software, has a myriad of problems which are analogous
to such real-world problems, and is a useful simulator for developing solutions
for these problems. In this paper we define the logistic transport belt problem
and define mathematical integer programming model of it. We developed an
interface to allow optimizers in any programming language to interact with
Factorio, and we provide an initial benchmark of logistic transport belt
problems. We present results for Simulated Annealing, quick Genetic Programming
and Evolutionary Reinforcement Learning, three different meta-heuristic
techniques to optimize this novel problem.",arxiv
http://arxiv.org/abs/2110.12749v1,2021-10-25T09:24:00Z,2021-10-25T09:24:00Z,"A GPU based single-pulse search pipeline (GSP) with database and its
  application to the commensal radio astronomy FAST survey (CRAFTS)","We developed a GPU based single-pulse search pipeline (GSP) with
candidate-archiving database. Largely based upon the infrastructure of Open
source pulsar search and analysis toolkit (PRESTO), GSP implements GPU
acceleration of the de-dispersion and integrates a candidate-archiving
database. We applied GSP to the data streams from the commensal radio astronomy
FAST survey (CRAFTS), which resulted in a quasi-real-time processing. The
integrated candidate database facilitates synergistic usage of multiple
machine-learning tools and thus improves efficient identification of radio
pulsars such as rotating radio transients (RRATs) and Fast Radio Bursts (FRBs).
We first tested GSP on pilot CRAFTS observations with the FAST Ultra-Wide Band
(UWB) receiver. GSP detected all pulsars known from the the Parkes multibeam
pulsar survey in the respective sky area covered by the FAST-UWB. GSP also
discovered 13 new pulsars. We measured the computational efficiency of GSP to
be ~120 times faster than the original PRESTO and ~60 times faster than a
MPI-parallelized version of PRESTO.",arxiv
http://arxiv.org/abs/2011.14925v1,2020-11-26T02:37:39Z,2020-11-26T02:37:39Z,"Autonomous Graph Mining Algorithm Search with Best Speed/Accuracy
  Trade-off","Graph data is ubiquitous in academia and industry, from social networks to
bioinformatics. The pervasiveness of graphs today has raised the demand for
algorithms that can answer various questions: Which products would a user like
to purchase given her order list? Which users are buying fake followers to
increase their public reputation? Myriads of new graph mining algorithms are
proposed every year to answer such questions - each with a distinct problem
formulation, computational time, and memory footprint. This lack of unity makes
it difficult for a practitioner to compare different algorithms and pick the
most suitable one for a specific application. These challenges - even more
severe for non-experts - create a gap in which state-of-the-art techniques
developed in academic settings fail to be optimally deployed in real-world
applications. To bridge this gap, we propose AUTOGM, an automated system for
graph mining algorithm development. We first define a unified framework
UNIFIEDGM that integrates various message-passing based graph algorithms,
ranging from conventional algorithms like PageRank to graph neural networks.
Then UNIFIEDGM defines a search space in which five parameters are required to
determine a graph algorithm. Under this search space, AUTOGM explicitly
optimizes for the optimal parameter set of UNIFIEDGM using Bayesian
Optimization. AUTOGM defines a novel budget-aware objective function for the
optimization to incorporate a practical issue - finding the best speed-accuracy
trade-off under a computation budget - into the graph algorithm generation
problem. Experiments on real-world benchmark datasets demonstrate that AUTOGM
generates novel graph mining algorithms with the best speed/accuracy trade-off
compared to existing models with heuristic parameters.",arxiv
http://arxiv.org/abs/1608.01987v1,2016-08-05T19:55:57Z,2016-08-05T19:55:57Z,Human collective intelligence as distributed Bayesian inference,"Collective intelligence is believed to underly the remarkable success of
human society. The formation of accurate shared beliefs is one of the key
components of human collective intelligence. How are accurate shared beliefs
formed in groups of fallible individuals? Answering this question requires a
multiscale analysis. We must understand both the individual decision mechanisms
people use, and the properties and dynamics of those mechanisms in the
aggregate. As of yet, mathematical tools for such an approach have been
lacking. To address this gap, we introduce a new analytical framework: We
propose that groups arrive at accurate shared beliefs via distributed Bayesian
inference. Distributed inference occurs through information processing at the
individual level, and yields rational belief formation at the group level. We
instantiate this framework in a new model of human social decision-making,
which we validate using a dataset we collected of over 50,000 users of an
online social trading platform where investors mimic each others' trades using
real money in foreign exchange and other asset markets. We find that in this
setting people use a decision mechanism in which popularity is treated as a
prior distribution for which decisions are best to make. This mechanism is
boundedly rational at the individual level, but we prove that in the aggregate
implements a type of approximate ""Thompson sampling""---a well-known and highly
effective single-agent Bayesian machine learning algorithm for sequential
decision-making. The perspective of distributed Bayesian inference therefore
reveals how collective rationality emerges from the boundedly rational decision
mechanisms people use.",arxiv
http://arxiv.org/abs/1706.00163v3,2017-06-05T14:54:55Z,2017-06-01T04:35:33Z,Coding Method for Parallel Iterative Linear Solver,"Computationally intensive distributed and parallel computing is often
bottlenecked by a small set of slow workers known as stragglers. In this paper,
we utilize the emerging idea of ""coded computation"" to design a novel
error-correcting-code inspired technique for solving linear inverse problems
under specific iterative methods in a parallelized implementation affected by
stragglers. Example applications include inverse problems in machine learning
on graphs, such as personalized PageRank and sampling on graphs. We provably
show that our coded-computation technique can reduce the mean-squared error
under a computational deadline constraint. In fact, the ratio of mean-squared
error of replication-based and coded techniques diverges to infinity as the
deadline increases. Our experiments for personalized PageRank performed on real
systems and real social networks show that this ratio can be as large as
$10^4$. Further, unlike coded-computation techniques proposed thus far, our
strategy combines outputs of all workers, including the stragglers, to produce
more accurate estimates at the computational deadline. This also ensures that
the accuracy degrades ""gracefully"" in the event that the number of stragglers
is large.",arxiv
http://arxiv.org/abs/1906.10958v3,2019-09-05T02:16:56Z,2019-06-26T10:33:24Z,Signed Graph Attention Networks,"Graph or network data is ubiquitous in the real world, including social
networks, information networks, traffic networks, biological networks and
various technical networks. The non-Euclidean nature of graph data poses the
challenge for modeling and analyzing graph data. Recently, Graph Neural Network
(GNNs) are proposed as a general and powerful framework to handle tasks on
graph data, e.g., node embedding, link prediction and node classification. As a
representative implementation of GNNs, Graph Attention Networks (GATs) are
successfully applied in a variety of tasks on real datasets. However, GAT is
designed to networks with only positive links and fails to handle signed
networks which contain both positive and negative links. In this paper, we
propose Signed Graph Attention Networks (SiGATs), generalizing GAT to signed
networks. SiGAT incorporates graph motifs into GAT to capture two well-known
theories in signed network research, i.e., balance theory and status theory. In
SiGAT, motifs offer us the flexible structural pattern to aggregate and
propagate messages on the signed network to generate node embeddings. We
evaluate the proposed SiGAT method by applying it to the signed link prediction
task. Experimental results on three real datasets demonstrate that SiGAT
outperforms feature-based method, network embedding method and state-of-the-art
GNN-based methods like signed graph convolutional network (SGCN).",arxiv
http://arxiv.org/abs/1407.3502v1,2014-07-13T19:09:30Z,2014-07-13T19:09:30Z,"Automated Real-Time Classification and Decision Making in Massive Data
  Streams from Synoptic Sky Surveys","The nature of scientific and technological data collection is evolving
rapidly: data volumes and rates grow exponentially, with increasing complexity
and information content, and there has been a transition from static data sets
to data streams that must be analyzed in real time. Interesting or anomalous
phenomena must be quickly characterized and followed up with additional
measurements via optimal deployment of limited assets. Modern astronomy
presents a variety of such phenomena in the form of transient events in digital
synoptic sky surveys, including cosmic explosions (supernovae, gamma ray
bursts), relativistic phenomena (black hole formation, jets), potentially
hazardous asteroids, etc. We have been developing a set of machine learning
tools to detect, classify and plan a response to transient events for astronomy
applications, using the Catalina Real-time Transient Survey (CRTS) as a
scientific and methodological testbed. The ability to respond rapidly to the
potentially most interesting events is a key bottleneck that limits the
scientific returns from the current and anticipated synoptic sky surveys.
Similar challenge arise in other contexts, from environmental monitoring using
sensor networks to autonomous spacecraft systems. Given the exponential growth
of data rates, and the time-critical response, we need a fully automated and
robust approach. We describe the results obtained to date, and the possible
future developments.",arxiv
http://arxiv.org/abs/1601.04385v1,2016-01-18T02:06:45Z,2016-01-18T02:06:45Z,Real-Time Data Mining of Massive Data Streams from Synoptic Sky Surveys,"The nature of scientific and technological data collection is evolving
rapidly: data volumes and rates grow exponentially, with increasing complexity
and information content, and there has been a transition from static data sets
to data streams that must be analyzed in real time. Interesting or anomalous
phenomena must be quickly characterized and followed up with additional
measurements via optimal deployment of limited assets. Modern astronomy
presents a variety of such phenomena in the form of transient events in digital
synoptic sky surveys, including cosmic explosions (supernovae, gamma ray
bursts), relativistic phenomena (black hole formation, jets), potentially
hazardous asteroids, etc. We have been developing a set of machine learning
tools to detect, classify and plan a response to transient events for astronomy
applications, using the Catalina Real-time Transient Survey (CRTS) as a
scientific and methodological testbed. The ability to respond rapidly to the
potentially most interesting events is a key bottleneck that limits the
scientific returns from the current and anticipated synoptic sky surveys.
Similar challenge arise in other contexts, from environmental monitoring using
sensor networks to autonomous spacecraft systems. Given the exponential growth
of data rates, and the time-critical response, we need a fully automated and
robust approach. We describe the results obtained to date, and the possible
future developments.",arxiv
http://arxiv.org/abs/2104.10891v1,2021-04-22T06:43:02Z,2021-04-22T06:43:02Z,"Computer Vision-based Social Distancing Surveillance Solution with
  Optional Automated Camera Calibration for Large Scale Deployment","Social distancing has been suggested as one of the most effective measures to
break the chain of viral transmission in the current COVID-19 pandemic. We
herein describe a computer vision-based AI-assisted solution to aid compliance
with social distancing norms. The solution consists of modules to detect and
track people and to identify distance violations. It provides the flexibility
to choose between a tool-based mode or an automated mode of camera calibration,
making the latter suitable for large-scale deployments. In this paper, we
discuss different metrics to assess the risk associated with social distancing
violations and how we can differentiate between transient or persistent
violations. Our proposed solution performs satisfactorily under different test
scenarios, processes video feed at real-time speed as well as addresses data
privacy regulations by blurring faces of detected people, making it ideal for
deployments.",arxiv
http://arxiv.org/abs/1909.05189v3,2020-08-20T14:35:49Z,2019-09-11T16:40:03Z,ORES: Lowering Barriers with Participatory Machine Learning in Wikipedia,"Algorithmic systems---from rule-based bots to machine learning
classifiers---have a long history of supporting the essential work of content
moderation and other curation work in peer production projects. From
counter-vandalism to task routing, basic machine prediction has allowed open
knowledge projects like Wikipedia to scale to the largest encyclopedia in the
world, while maintaining quality and consistency. However, conversations about
how quality control should work and what role algorithms should play have
generally been led by the expert engineers who have the skills and resources to
develop and modify these complex algorithmic systems. In this paper, we
describe ORES: an algorithmic scoring service that supports real-time scoring
of wiki edits using multiple independent classifiers trained on different
datasets. ORES decouples several activities that have typically all been
performed by engineers: choosing or curating training data, building models to
serve predictions, auditing predictions, and developing interfaces or automated
agents that act on those predictions. This meta-algorithmic system was designed
to open up socio-technical conversations about algorithms in Wikipedia to a
broader set of participants. In this paper, we discuss the theoretical
mechanisms of social change ORES enables and detail case studies in
participatory machine learning around ORES from the 5 years since its
deployment.",arxiv
http://arxiv.org/abs/1810.02648v3,2019-01-25T15:37:49Z,2018-10-05T12:39:37Z,LiveCap: Real-time Human Performance Capture from Monocular Video,"We present the first real-time human performance capture approach that
reconstructs dense, space-time coherent deforming geometry of entire humans in
general everyday clothing from just a single RGB video. We propose a novel
two-stage analysis-by-synthesis optimization whose formulation and
implementation are designed for high performance. In the first stage, a skinned
template model is jointly fitted to background subtracted input video, 2D and
3D skeleton joint positions found using a deep neural network, and a set of
sparse facial landmark detections. In the second stage, dense non-rigid 3D
deformations of skin and even loose apparel are captured based on a novel
real-time capable algorithm for non-rigid tracking using dense photometric and
silhouette constraints. Our novel energy formulation leverages automatically
identified material regions on the template to model the differing non-rigid
deformation behavior of skin and apparel. The two resulting non-linear
optimization problems per-frame are solved with specially-tailored
data-parallel Gauss-Newton solvers. In order to achieve real-time performance
of over 25Hz, we design a pipelined parallel architecture using the CPU and two
commodity GPUs. Our method is the first real-time monocular approach for
full-body performance capture. Our method yields comparable accuracy with
off-line performance capture techniques, while being orders of magnitude
faster.",arxiv
http://arxiv.org/abs/2110.05762v2,2021-11-15T18:31:11Z,2021-10-12T06:31:54Z,"Detecting Damage Building Using Real-time Crowdsourced Images and
  Transfer Learning","After significant earthquakes, we can see images posted on social media
platforms by individuals and media agencies owing to the mass usage of
smartphones these days. These images can be utilized to provide information
about the shaking damage in the earthquake region both to the public and
research community, and potentially to guide rescue work. This paper presents
an automated way to extract the damaged building images after earthquakes from
social media platforms such as Twitter and thus identify the particular user
posts containing such images. Using transfer learning and ~6500 manually
labelled images, we trained a deep learning model to recognize images with
damaged buildings in the scene. The trained model achieved good performance
when tested on newly acquired images of earthquakes at different locations and
ran in near real-time on Twitter feed after the 2020 M7.0 earthquake in Turkey.
Furthermore, to better understand how the model makes decisions, we also
implemented the Grad-CAM method to visualize the important locations on the
images that facilitate the decision.",arxiv
http://arxiv.org/abs/1901.05147v1,2019-01-16T06:10:45Z,2019-01-16T06:10:45Z,The Winning Solution to the IEEE CIG 2017 Game Data Mining Competition,"Machine learning competitions such as those organized by Kaggle or KDD
represent a useful benchmark for data science research. In this work, we
present our winning solution to the Game Data Mining competition hosted at the
2017 IEEE Conference on Computational Intelligence and Games (CIG 2017). The
contest consisted of two tracks, and participants (more than 250, belonging to
both industry and academia) were to predict which players would stop playing
the game, as well as their remaining lifetime. The data were provided by a
major worldwide video game company, NCSoft, and came from their successful
massively multiplayer online game Blade and Soul. Here, we describe the long
short-term memory approach and conditional inference survival ensemble model
that made us win both tracks of the contest, as well as the validation
procedure that we followed in order to prevent overfitting. In particular,
choosing a survival method able to deal with censored data was crucial to
accurately predict the moment in which each player would leave the game, as
censoring is inherent in churn. The selected models proved to be robust against
evolving conditions---since there was a change in the business model of the
game (from subscription-based to free-to-play) between the two sample datasets
provided---and efficient in terms of time cost. Thanks to these features and
also to their a ability to scale to large datasets, our models could be readily
implemented in real business settings.",arxiv
http://arxiv.org/abs/1904.07998v2,2019-11-11T01:48:59Z,2019-04-16T22:10:19Z,"SynC: A Unified Framework for Generating Synthetic Population with
  Gaussian Copula","Synthetic population generation is the process of combining multiple
socioeconomic and demographic datasets from different sources and/or
granularity levels, and downscaling them to an individual level. Although it is
a fundamental step for many data science tasks, an efficient and standard
framework is absent. In this study, we propose a multi-stage framework called
SynC (Synthetic Population via Gaussian Copula) to fill the gap. SynC first
removes potential outliers in the data and then fits the filtered data with a
Gaussian copula model to correctly capture dependencies and marginal
distributions of sampled survey data. Finally, SynC leverages predictive models
to merge datasets into one and then scales them accordingly to match the
marginal constraints. We make three key contributions in this work: 1) propose
a novel framework for generating individual level data from aggregated data
sources by combining state-of-the-art machine learning and statistical
techniques, 2) demonstrate its value as a feature engineering tool, as well as
an alternative to data collection in situations where gathering is difficult
through two real-world datasets, 3) release an easy-to-use framework
implementation for reproducibility, and 4) ensure the methodology is scalable
at the production level and can easily incorporate new data.",arxiv
http://arxiv.org/abs/1907.12817v2,2019-07-31T07:14:32Z,2019-07-30T10:00:52Z,"Increasing Scalability of Process Mining using Event Dataframes: How
  Data Structure Matters","Process Mining is a branch of Data Science that aims to extract
process-related information from event data contained in information systems,
that is steadily increasing in amount. Many algorithms, and a general-purpose
open source framework (ProM 6), have been developed in the last years for
process discovery, conformance checking, machine learning on event data.
However, in very few cases scalability has been a target, prioritizing the
quality of the output over the execution speed and the optimization of
resources. This is making progressively more difficult to apply process mining
with mainstream workstations on real-life event data with any open source
process mining framework. Hence, exploring more scalable storage techniques,
in-memory data structures, more performant algorithms is a strictly incumbent
need. In this paper, we propose the usage of mainstream columnar storages and
dataframes to increase the scalability of process mining. These can replace the
classic event log structures in most tasks, but require completely different
implementations with regards to mainstream process mining algorithms.
Dataframes will be defined, some algorithms on such structures will be
presented and their complexity will be calculated.",arxiv
http://arxiv.org/abs/2104.11360v1,2021-04-23T00:46:35Z,2021-04-23T00:46:35Z,"Normalized multivariate time series causality analysis and causal graph
  reconstruction","Causality analysis is an important problem lying at the heart of science, and
is of particular importance in data science and machine learning. An endeavor
during the past 16 years viewing causality as real physical notion so as to
formulate it from first principles, however, seems to go unnoticed. This study
introduces to the community this line of work, with a long-due generalization
of the information flow-based bivariate time series causal inference to
multivariate series, based on the recent advance in theoretical development.
The resulting formula is transparent, and can be implemented as a
computationally very efficient algorithm for application. It can be normalized,
and tested for statistical significance. Different from the previous work along
this line where only information flows are estimated, here an algorithm is also
implemented to quantify the influence of a unit to itself. While this forms a
challenge in some causal inferences, here it comes naturally, and hence the
identification of self-loops in a causal graph is fulfilled automatically as
the causalities along edges are inferred.
  To demonstrate the power of the approach, presented here are two applications
in extreme situations. The first is a network of multivariate processes buried
in heavy noises (with the noise-to-signal ratio exceeding 100), and the second
a network with nearly synchronized chaotic oscillators. In both graphs,
confounding processes exist. While it seems to be a huge challenge to
reconstruct from given series these causal graphs, an easy application of the
algorithm immediately reveals the desideratum. Particularly, the confounding
processes have been accurately differentiated. Considering the surge of
interest in the community, this study is very timely.",arxiv
http://arxiv.org/abs/1905.04815v1,2019-05-13T00:20:31Z,2019-05-13T00:20:31Z,"Programmable Spectrometry -- Per-pixel Classification of Materials using
  Learned Spectral Filters","Many materials have distinct spectral profiles. This facilitates estimation
of the material composition of a scene at each pixel by first acquiring its
hyperspectral image, and subsequently filtering it using a bank of spectral
profiles. This process is inherently wasteful since only a set of linear
projections of the acquired measurements contribute to the classification task.
We propose a novel programmable camera that is capable of producing images of a
scene with an arbitrary spectral filter. We use this camera to optically
implement the spectral filtering of the scene's hyperspectral image with the
bank of spectral profiles needed to perform per-pixel material classification.
This provides gains both in terms of acquisition speed --- since only the
relevant measurements are acquired --- and in signal-to-noise ratio --- since
we invariably avoid narrowband filters that are light inefficient. Given
training data, we use a range of classical and modern techniques including SVMs
and neural networks to identify the bank of spectral profiles that facilitate
material classification. We verify the method in simulations on standard
datasets as well as real data using a lab prototype of the camera.",arxiv
http://arxiv.org/abs/1511.09120v4,2017-12-18T14:04:48Z,2015-11-30T00:44:41Z,Coresets for Kinematic Data: From Theorems to Real-Time Systems,"A coreset (or core-set) of a dataset is its semantic compression with respect
to a set of queries, such that querying the (small) coreset provably yields an
approximate answer to querying the original (full) dataset. In the last decade,
coresets provided breakthroughs in theoretical computer science for
approximation algorithms, and more recently, in the machine learning community
for learning ""Big data"". However, we are not aware of real-time systems that
compute coresets in a rate of dozens of frames per second. In this paper we
suggest a framework to turn theorems to such systems using coresets. We begin
with a proof of independent interest, that any set of $n$ matrices in
$\mathbb{R}^{d\times d}$ whose sum is $S$, has a positively weighted subset
whose sum has the same center of mass (mean) and orientation (left+right
singular vectors) as $S$, and consists of $O(dr)$ matrices (independent of
$n$), where $r\leq d$ is the rank of $S$. We provide an algorithm that computes
this (core) set in one pass over possibly infinite stream of matrices in
$d^{O(1)}$ time per matrix insertion. By maintaining such a coreset for
kinematic (moving) set of $n$ points, we can run pose-estimation algorithms,
such as Kabsch or PnP, on the small coresets, instead of the $n$ points, in
real-time using weak devices, while obtaining the same results. This enabled us
to implement a low-cost ($<\$100$) IoT wireless system that tracks a toy (and
harmless) quadcopter which guides guests to a desired room (in a hospital,
mall, hotel, museum, etc.) with no help of additional human or remote
controller. We hope that our framework will encourage researchers outside the
theoretical community to design and use coresets in future systems and papers.
To this end, we provide extensive experimental results on both synthetic and
real data, as well as a link to the open code of our system and algorithms.",arxiv
http://arxiv.org/abs/1808.06277v1,2018-08-20T00:54:29Z,2018-08-20T00:54:29Z,An Efficient Approach for Geo-Multimedia Cross-Modal Retrieval,"Due to the rapid development of mobile Internet techniques, cloud computation
and popularity of online social networking and location-based services, massive
amount of multimedia data with geographical information is generated and
uploaded to the Internet. In this paper, we propose a novel type of cross-modal
multimedia retrieval called geo-multimedia cross-modal retrieval which aims to
search out a set of geo-multimedia objects based on geographical distance
proximity and semantic similarity between different modalities. Previous
studies for cross-modal retrieval and spatial keyword search cannot address
this problem effectively because they do not consider multimedia data with
geo-tags and do not focus on this type of query. In order to address this
problem efficiently, we present the definition of $k$NN geo-multimedia
cross-modal query at the first time and introduce relevant conceptions such as
cross-modal semantic representation space. To bridge the semantic gap between
different modalities, we propose a method named cross-modal semantic matching
which contains two important component, i.e., CorrProj and LogsTran, which aims
to construct a common semantic representation space for cross-modal semantic
similarity measurement. Besides, we designed a framework based on deep learning
techniques to implement common semantic representation space construction. In
addition, a novel hybrid indexing structure named GMR-Tree combining
geo-multimedia data and R-Tree is presented and a efficient $k$NN search
algorithm called $k$GMCMS is designed. Comprehensive experimental evaluation on
real and synthetic dataset clearly demonstrates that our solution outperforms
the-state-of-the-art methods.",arxiv
http://arxiv.org/abs/2010.10346v2,2021-02-27T18:46:18Z,2020-10-20T15:12:30Z,"Deep Importance Sampling based on Regression for Model Inversion and
  Emulation","Understanding systems by forward and inverse modeling is a recurrent topic of
research in many domains of science and engineering. In this context, Monte
Carlo methods have been widely used as powerful tools for numerical inference
and optimization. They require the choice of a suitable proposal density that
is crucial for their performance. For this reason, several adaptive importance
sampling (AIS) schemes have been proposed in the literature. We here present an
AIS framework called Regression-based Adaptive Deep Importance Sampling
(RADIS). In RADIS, the key idea is the adaptive construction via regression of
a non-parametric proposal density (i.e., an emulator), which mimics the
posterior distribution and hence minimizes the mismatch between proposal and
target densities. RADIS is based on a deep architecture of two (or more) nested
IS schemes, in order to draw samples from the constructed emulator. The
algorithm is highly efficient since employs the posterior approximation as
proposal density, which can be improved adding more support points. As a
consequence, RADIS asymptotically converges to an exact sampler under mild
conditions. Additionally, the emulator produced by RADIS can be in turn used as
a cheap surrogate model for further studies. We introduce two specific RADIS
implementations that use Gaussian Processes (GPs) and Nearest Neighbors (NN)
for constructing the emulator. Several numerical experiments and comparisons
show the benefits of the proposed schemes. A real-world application in remote
sensing model inversion and emulation confirms the validity of the approach.",arxiv
http://arxiv.org/abs/2012.13968v1,2020-12-27T16:03:32Z,2020-12-27T16:03:32Z,"Detecting Medical Misinformation on Social Media Using Multimodal Deep
  Learning","In 2019, outbreaks of vaccine-preventable diseases reached the highest number
in the US since 1992. Medical misinformation, such as antivaccine content
propagating through social media, is associated with increases in vaccine delay
and refusal. Our overall goal is to develop an automatic detector for
antivaccine messages to counteract the negative impact that antivaccine
messages have on the public health. Very few extant detection systems have
considered multimodality of social media posts (images, texts, and hashtags),
and instead focus on textual components, despite the rapid growth of
photo-sharing applications (e.g., Instagram). As a result, existing systems are
not sufficient for detecting antivaccine messages with heavy visual components
(e.g., images) posted on these newer platforms. To solve this problem, we
propose a deep learning network that leverages both visual and textual
information. A new semantic- and task-level attention mechanism was created to
help our model to focus on the essential contents of a post that signal
antivaccine messages. The proposed model, which consists of three branches, can
generate comprehensive fused features for predictions. Moreover, an ensemble
method is proposed to further improve the final prediction accuracy. To
evaluate the proposed model's performance, a real-world social media dataset
that consists of more than 30,000 samples was collected from Instagram between
January 2016 and October 2019. Our 30 experiment results demonstrate that the
final network achieves above 97% testing accuracy and outperforms other
relevant models, demonstrating that it can detect a large amount of antivaccine
messages posted daily. The implementation code is available at
https://github.com/wzhings/antivaccine_detection.",arxiv
http://arxiv.org/abs/1707.05223v1,2017-07-17T15:18:36Z,2017-07-17T15:18:36Z,A transient search using combined human and machine classifications,"Large modern surveys require efficient review of data in order to find
transient sources such as supernovae, and to distinguish such sources from
artefacts and noise. Much effort has been put into the development of automatic
algorithms, but surveys still rely on human review of targets. This paper
presents an integrated system for the identification of supernovae in data from
Pan-STARRS1, combining classifications from volunteers participating in a
citizen science project with those from a convolutional neural network. The
unique aspect of this work is the deployment, in combination, of both human and
machine classifications for near real-time discovery in an astronomical
project. We show that the combination of the two methods outperforms either one
used individually. This result has important implications for the future
development of transient searches, especially in the era of LSST and other
large-throughput surveys.",arxiv
http://arxiv.org/abs/1806.06671v1,2018-06-18T13:43:51Z,2018-06-18T13:43:51Z,"Where to Go Next: A Spatio-temporal LSTM model for Next POI
  Recommendation","Next Point-of-Interest (POI) recommendation is of great value for both
location-based service providers and users. Recently Recurrent Neural Networks
(RNNs) have been proved to be effective on sequential recommendation tasks.
However, existing RNN solutions rarely consider the spatio-temporal intervals
between neighbor check-ins, which are essential for modeling user check-in
behaviors in next POI recommendation. In this paper, we propose a new variant
of LSTM, named STLSTM, which implements time gates and distance gates into LSTM
to capture the spatio-temporal relation between successive check-ins.
Specifically, one-time gate and one distance gate are designed to control
short-term interest update, and another time gate and distance gate are
designed to control long-term interest update. Furthermore, to reduce the
number of parameters and improve efficiency, we further integrate coupled input
and forget gates with our proposed model. Finally, we evaluate the proposed
model using four real-world datasets from various location-based social
networks. Our experimental results show that our model significantly
outperforms the state-of-the-art approaches for next POI recommendation.",arxiv
http://arxiv.org/abs/2007.13004v1,2020-07-25T20:07:28Z,2020-07-25T20:07:28Z,Learning Attribute-Structure Co-Evolutions in Dynamic Graphs,"Most graph neural network models learn embeddings of nodes in static
attributed graphs for predictive analysis. Recent attempts have been made to
learn temporal proximity of the nodes. We find that real dynamic attributed
graphs exhibit complex co-evolution of node attributes and graph structure.
Learning node embeddings for forecasting change of node attributes and birth
and death of links over time remains an open problem. In this work, we present
a novel framework called CoEvoGNN for modeling dynamic attributed graph
sequence. It preserves the impact of earlier graphs on the current graph by
embedding generation through the sequence. It has a temporal self-attention
mechanism to model long-range dependencies in the evolution. Moreover, CoEvoGNN
optimizes model parameters jointly on two dynamic tasks, attribute inference
and link prediction over time. So the model can capture the co-evolutionary
patterns of attribute change and link formation. This framework can adapt to
any graph neural algorithms so we implemented and investigated three methods
based on it: CoEvoGCN, CoEvoGAT, and CoEvoSAGE. Experiments demonstrate the
framework (and its methods) outperform strong baselines on predicting an entire
unseen graph snapshot of personal attributes and interpersonal links in dynamic
social graphs and financial graphs.",arxiv
http://arxiv.org/abs/2110.08760v1,2021-10-17T08:41:21Z,2021-10-17T08:41:21Z,"Adapting Membership Inference Attacks to GNN for Graph Classification:
  Approaches and Implications","Graph Neural Networks (GNNs) are widely adopted to analyse non-Euclidean
data, such as chemical networks, brain networks, and social networks, modelling
complex relationships and interdependency between objects. Recently, Membership
Inference Attack (MIA) against GNNs raises severe privacy concerns, where
training data can be leaked from trained GNN models. However, prior studies
focus on inferring the membership of only the components in a graph, e.g., an
individual node or edge. How to infer the membership of an entire graph record
is yet to be explored.
  In this paper, we take the first step in MIA against GNNs for graph-level
classification. Our objective is to infer whether a graph sample has been used
for training a GNN model. We present and implement two types of attacks, i.e.,
training-based attacks and threshold-based attacks from different adversarial
capabilities. We perform comprehensive experiments to evaluate our attacks in
seven real-world datasets using five representative GNN models. Both our
attacks are shown effective and can achieve high performance, i.e., reaching
over 0.7 attack F1 scores in most cases. Furthermore, we analyse the
implications behind the MIA against GNNs. Our findings confirm that GNNs can be
even more vulnerable to MIA than the models with non-graph structures. And
unlike the node-level classifier, MIAs on graph-level classification tasks are
more co-related with the overfitting level of GNNs rather than the statistic
property of their training graphs.",arxiv
http://arxiv.org/abs/2008.02321v2,2021-02-25T03:04:37Z,2020-08-05T19:00:36Z,"Can I Pour into It? Robot Imagining Open Containability Affordance of
  Previously Unseen Objects via Physical Simulations","Open containers, i.e., containers without covers, are an important and
ubiquitous class of objects in human life. In this letter, we propose a novel
method for robots to ""imagine"" the open containability affordance of a
previously unseen object via physical simulations. We implement our imagination
method on a UR5 manipulator. The robot autonomously scans the object with an
RGB-D camera. The scanned 3D model is used for open containability imagination
which quantifies the open containability affordance by physically simulating
dropping particles onto the object and counting how many particles are retained
in it. This quantification is used for open-container vs. non-open-container
binary classification (hereafter referred to as open container classification).
If the object is classified as an open container, the robot further imagines
pouring into the object, again using physical simulations, to obtain the
pouring position and orientation for real robot autonomous pouring. We evaluate
our method on open container classification and autonomous pouring of granular
material on a dataset containing 130 previously unseen objects with 57 object
categories. Although our proposed method uses only 11 objects for simulation
calibration (training), its open container classification aligns well with
human judgements. In addition, our method endows the robot with the capability
to autonomously pour into the 55 containers in the dataset with a very high
success rate. We also compare to a deep learning method. Results show that our
method achieves the same performance as the deep learning method on open
container classification and outperforms it on autonomous pouring. Moreover,
our method is fully explainable.",arxiv
http://arxiv.org/abs/1810.06637v2,2019-05-02T15:03:20Z,2018-10-15T19:46:00Z,"Nonlinear System Identification of Soft Robot Dynamics Using Koopman
  Operator Theory","Soft robots are challenging to model due in large part to the nonlinear
properties of soft materials. Fortunately, this softness makes it possible to
safely observe their behavior under random control inputs, making them amenable
to large-scale data collection and system identification. This paper implements
and evaluates a system identification method based on Koopman operator theory
in which models of nonlinear dynamical systems are constructed via linear
regression of observed data by exploiting the fact that every nonlinear system
has a linear representation in the infinite-dimensional space of real-valued
functions called observables. The approach does not suffer from some of the
shortcomings of other nonlinear system identification methods, which typically
require the manual tuning of training parameters and have limited convergence
guarantees. A dynamic model of a pneumatic soft robot arm is constructed via
this method, and used to predict the behavior of the real system. The total
normalized-root-mean-square error (NRMSE) of its predictions is lower than that
of several other identified models including a neural network, NLARX, nonlinear
Hammerstein-Wiener, and linear state space model.",arxiv
http://arxiv.org/abs/2106.06150v1,2021-06-11T03:30:25Z,2021-06-11T03:30:25Z,Global Neighbor Sampling for Mixed CPU-GPU Training on Giant Graphs,"Graph neural networks (GNNs) are powerful tools for learning from graph data
and are widely used in various applications such as social network
recommendation, fraud detection, and graph search. The graphs in these
applications are typically large, usually containing hundreds of millions of
nodes. Training GNN models on such large graphs efficiently remains a big
challenge. Despite a number of sampling-based methods have been proposed to
enable mini-batch training on large graphs, these methods have not been proved
to work on truly industry-scale graphs, which require GPUs or mixed-CPU-GPU
training. The state-of-the-art sampling-based methods are usually not optimized
for these real-world hardware setups, in which data movement between CPUs and
GPUs is a bottleneck. To address this issue, we propose Global Neighborhood
Sampling that aims at training GNNs on giant graphs specifically for
mixed-CPU-GPU training. The algorithm samples a global cache of nodes
periodically for all mini-batches and stores them in GPUs. This global cache
allows in-GPU importance sampling of mini-batches, which drastically reduces
the number of nodes in a mini-batch, especially in the input layer, to reduce
data copy between CPU and GPU and mini-batch computation without compromising
the training convergence rate or model accuracy. We provide a highly efficient
implementation of this method and show that our implementation outperforms an
efficient node-wise neighbor sampling baseline by a factor of 2X-4X on giant
graphs. It outperforms an efficient implementation of LADIES with small layers
by a factor of 2X-14X while achieving much higher accuracy than LADIES.We also
theoretically analyze the proposed algorithm and show that with cached node
data of a proper size, it enjoys a comparable convergence rate as the
underlying node-wise sampling method.",arxiv
http://arxiv.org/abs/2103.06327v1,2021-03-10T20:14:41Z,2021-03-10T20:14:41Z,"Closed Loop Predictive Control of Adaptive Optics Systems with
  Convolutional Neural Networks","Predictive wavefront control is an important and rapidly developing field of
adaptive optics (AO). Through the prediction of future wavefront effects, the
inherent AO system servo-lag caused by the measurement, computation, and
application of the wavefront correction can be significantly mitigated. This
lag can impact the final delivered science image, including reduced strehl and
contrast, and inhibits our ability to reliably use faint guidestars. We
summarize here a novel method for training deep neural networks for predictive
control based on an adversarial prior. Unlike previous methods in the
literature, which have shown results based on previously generated data or for
open-loop systems, we demonstrate our network's performance simulated in closed
loop. Our models are able to both reduce effects induced by servo-lag and push
the faint end of reliable control with natural guidestars, improving K-band
Strehl performance compared to classical methods by over 55% for 16th magnitude
guide stars on an 8-meter telescope. We further show that LSTM based approaches
may be better suited in high-contrast scenarios where servo-lag error is most
pronounced, while traditional feed forward models are better suited for high
noise scenarios. Finally, we discuss future strategies for implementing our
system in real-time and on astronomical telescope systems.",arxiv
http://arxiv.org/abs/2103.16584v1,2021-03-30T18:01:06Z,2021-03-30T18:01:06Z,"Parameterized Hypercomplex Graph Neural Networks for Graph
  Classification","Despite recent advances in representation learning in hypercomplex (HC)
space, this subject is still vastly unexplored in the context of graphs.
Motivated by the complex and quaternion algebras, which have been found in
several contexts to enable effective representation learning that inherently
incorporates a weight-sharing mechanism, we develop graph neural networks that
leverage the properties of hypercomplex feature transformation. In particular,
in our proposed class of models, the multiplication rule specifying the algebra
itself is inferred from the data during training. Given a fixed model
architecture, we present empirical evidence that our proposed model
incorporates a regularization effect, alleviating the risk of overfitting. We
also show that for fixed model capacity, our proposed method outperforms its
corresponding real-formulated GNN, providing additional confirmation for the
enhanced expressivity of HC embeddings. Finally, we test our proposed
hypercomplex GNN on several open graph benchmark datasets and show that our
models reach state-of-the-art performance while consuming a much lower memory
footprint with 70& fewer parameters. Our implementations are available at
https://github.com/bayer-science-for-a-better-life/phc-gnn.",arxiv
http://arxiv.org/abs/1906.05658v1,2019-06-07T08:10:16Z,2019-06-07T08:10:16Z,EKT: Exercise-aware Knowledge Tracing for Student Performance Prediction,"For offering proactive services to students in intelligent education, one of
the fundamental tasks is predicting their performance (e.g., scores) on future
exercises, where it is necessary to track each student's knowledge acquisition
during her exercising activities. However, existing approaches can only exploit
the exercising records of students, and the problem of extracting rich
information existed in the exercise's materials (e.g., knowledge concepts,
exercise content) to achieve both precise predictions of student performance
and interpretable analysis of knowledge acquisition remains underexplored. In
this paper, we present a holistic study of student performance prediction. To
directly achieve the primary goal of prediction, we first propose a general
Exercise-Enhanced Recurrent Neural Network (EERNN) framework by exploring both
student's records and the exercise contents. In EERNN, we simply summarize each
student's state into an integrated vector and trace it with a recurrent neural
network, where we design a bidirectional LSTM to learn the encoding of each
exercise's content. For making predictions, we propose two implementations
under EERNN with different strategies, i.e., EERNNM with Markov property and
EERNNA with Attention mechanism. Then, to explicitly track student's knowledge
acquisition on multiple knowledge concepts, we extend EERNN to an explainable
Exercise-aware Knowledge Tracing (EKT) by incorporating the knowledge concept
effects, where the student's integrated state vector is extended to a knowledge
state matrix. In EKT, we further develop a memory network for quantifying how
much each exercise can affect the mastery of students on concepts during the
exercising process. Finally, we conduct extensive experiments on large-scale
real-world data. The results demonstrate the prediction effectiveness of two
frameworks as well as the superior interpretability of EKT.",arxiv
http://arxiv.org/abs/1805.05491v1,2018-05-14T22:59:56Z,2018-05-14T22:59:56Z,"Crowdbreaks: Tracking Health Trends using Public Social Media Data and
  Crowdsourcing","In the past decade, tracking health trends using social media data has shown
great promise, due to a powerful combination of massive adoption of social
media around the world, and increasingly potent hardware and software that
enables us to work with these new big data streams. At the same time, many
challenging problems have been identified. First, there is often a mismatch
between how rapidly online data can change, and how rapidly algorithms are
updated, which means that there is limited reusability for algorithms trained
on past data as their performance decreases over time. Second, much of the work
is focusing on specific issues during a specific past period in time, even
though public health institutions would need flexible tools to assess multiple
evolving situations in real time. Third, most tools providing such capabilities
are proprietary systems with little algorithmic or data transparency, and thus
little buy-in from the global public health and research community. Here, we
introduce Crowdbreaks, an open platform which allows tracking of health trends
by making use of continuous crowdsourced labelling of public social media
content. The system is built in a way which automatizes the typical workflow
from data collection, filtering, labelling and training of machine learning
classifiers and therefore can greatly accelerate the research process in the
public health domain. This work introduces the technical aspects of the
platform and explores its future use cases.",arxiv
http://arxiv.org/abs/2105.08841v1,2021-05-18T21:43:51Z,2021-05-18T21:43:51Z,A Deep Learning Method for AGILE-GRID GRB Detection,"The follow-up of external science alerts received from Gamma-Ray Bursts (GRB)
and Gravitational Waves (GW) detectors is one of the AGILE Team's current major
activities. The AGILE team developed an automated real-time analysis pipeline
to analyse AGILE Gamma-Ray Imaging Detector (GRID) data to detect possible
counterparts in the energy range 0.1-10 GeV. This work presents a new approach
for detecting GRBs using a Convolutional Neural Network (CNN) to classify the
AGILE-GRID intensity maps improving the GRBs detection capability over the
Li&Ma method, currently used by the AGILE team. The CNN is trained with large
simulated datasets of intensity maps. The AGILE complex observing pattern due
to the so-called 'spinning mode' is studied to prepare datasets to test and
evaluate the CNN. A GRB emission model is defined from the Second Fermi-LAT GRB
catalogue and convoluted with the AGILE observing pattern. Different p-value
distributions are calculated evaluating with the CNN millions of
background-only maps simulated varying the background level. The CNN is then
used on real data to analyse the AGILE-GRID data archive, searching for GRB
detections using the trigger time and position taken from the Swift-BAT,
Fermi-GBM, and Fermi-LAT GRB catalogues. From these catalogues, the CNN detects
21 GRBs with a significance $\geq 3 \sigma$, while the Li&Ma method detects
only two GRBs. The results shown in this work demonstrate that the CNN is more
effective in detecting GRBs than the Li&Ma method in this context and can be
implemented into the AGILE-GRID real-time analysis pipeline.",arxiv
http://arxiv.org/abs/1902.01580v1,2019-02-05T08:09:33Z,2019-02-05T08:09:33Z,PUTWorkbench: Analysing Privacy in AI-intensive Systems,"AI intensive systems that operate upon user data face the challenge of
balancing data utility with privacy concerns. We propose the idea and present
the prototype of an open-source tool called Privacy Utility Trade-off (PUT)
Workbench which seeks to aid software practitioners to take such crucial
decisions. We pick a simple privacy model that doesn't require any background
knowledge in Data Science and show how even that can achieve significant
results over standard and real-life datasets. The tool and the source code is
made freely available for extensions and usage.",arxiv
http://arxiv.org/abs/2107.13480v1,2021-07-28T16:42:09Z,2021-07-28T16:42:09Z,Survival stacking: casting survival analysis as a classification problem,"While there are many well-developed data science methods for classification
and regression, there are relatively few methods for working with
right-censored data. Here, we present ""survival stacking"": a method for casting
survival analysis problems as classification problems, thereby allowing the use
of general classification methods and software in a survival setting. Inspired
by the Cox partial likelihood, survival stacking collects features and outcomes
of survival data in a large data frame with a binary outcome. We show that
survival stacking with logistic regression is approximately equivalent to the
Cox proportional hazards model. We further recommend methods for evaluating
model performance in the survival stacked setting, and we illustrate survival
stacking on real and simulated data. By reframing survival problems as
classification problems, we make it possible for data scientists to use
well-known learning algorithms (including random forests, gradient boosting
machines and neural networks) in a survival setting, and lower the barrier for
flexible survival modeling.",arxiv
http://arxiv.org/abs/2012.15005v2,2021-05-30T01:31:39Z,2020-12-30T02:03:25Z,"Infer-AVAE: An Attribute Inference Model Based on Adversarial
  Variational Autoencoder","User attributes, such as gender and education, face severe incompleteness in
social networks. In order to make this kind of valuable data usable for
downstream tasks like user profiling and personalized recommendation, attribute
inference aims to infer users' missing attribute labels based on observed data.
Recently, variational autoencoder (VAE), an end-to-end deep generative model,
has shown promising performance by handling the problem in a semi-supervised
way. However, VAEs can easily suffer from over-fitting and over-smoothing when
applied to attribute inference. To be specific, VAE implemented with
multi-layer perceptron (MLP) can only reconstruct input data but fail in
inferring missing parts. While using the trending graph neural networks (GNNs)
as encoder has the problem that GNNs aggregate redundant information from
neighborhood and generate indistinguishable user representations, which is
known as over-smoothing. In this paper, we propose an attribute
\textbf{Infer}ence model based on \textbf{A}dversarial \textbf{VAE}
(Infer-AVAE) to cope with these issues. Specifically, to overcome
over-smoothing, Infer-AVAE unifies MLP and GNNs in encoder to learn positive
and negative latent representations respectively. Meanwhile, an adversarial
network is trained to distinguish the two representations and GNNs are trained
to aggregate less noise for more robust representations through adversarial
training. Finally, to relieve over-fitting, mutual information constraint is
introduced as a regularizer for decoder, so that it can make better use of
auxiliary information in representations and generate outputs not limited by
observations. We evaluate our model on 4 real-world social network datasets,
experimental results demonstrate that our model averagely outperforms baselines
by 7.0$\%$ in accuracy.",arxiv
http://arxiv.org/abs/2102.10477v1,2021-02-20T23:45:24Z,2021-02-20T23:45:24Z,"Neural Sampling Machine with Stochastic Synapse allows Brain-like
  Learning and Inference","Many real-world mission-critical applications require continual online
learning from noisy data and real-time decision making with a defined
confidence level. Probabilistic models and stochastic neural networks can
explicitly handle uncertainty in data and allow adaptive learning-on-the-fly,
but their implementation in a low-power substrate remains a challenge. Here, we
introduce a novel hardware fabric that implements a new class of stochastic NN
called Neural-Sampling-Machine that exploits stochasticity in synaptic
connections for approximate Bayesian inference. Harnessing the inherent
non-linearities and stochasticity occurring at the atomic level in emerging
materials and devices allows us to capture the synaptic stochasticity occurring
at the molecular level in biological synapses. We experimentally demonstrate
in-silico hybrid stochastic synapse by pairing a ferroelectric field-effect
transistor -based analog weight cell with a two-terminal stochastic selector
element. Such a stochastic synapse can be integrated within the
well-established crossbar array architecture for compute-in-memory. We
experimentally show that the inherent stochastic switching of the selector
element between the insulator and metallic state introduces a multiplicative
stochastic noise within the synapses of NSM that samples the conductance states
of the FeFET, both during learning and inference. We perform network-level
simulations to highlight the salient automatic weight normalization feature
introduced by the stochastic synapses of the NSM that paves the way for
continual online learning without any offline Batch Normalization. We also
showcase the Bayesian inferencing capability introduced by the stochastic
synapse during inference mode, thus accounting for uncertainty in data. We
report 98.25%accuracy on standard image classification task as well as
estimation of data uncertainty in rotated samples.",arxiv
http://arxiv.org/abs/2009.09471v1,2020-09-20T16:36:25Z,2020-09-20T16:36:25Z,"SYNC: A Copula based Framework for Generating Synthetic Data from
  Aggregated Sources","A synthetic dataset is a data object that is generated programmatically, and
it may be valuable to creating a single dataset from multiple sources when
direct collection is difficult or costly. Although it is a fundamental step for
many data science tasks, an efficient and standard framework is absent. In this
paper, we study a specific synthetic data generation task called downscaling, a
procedure to infer high-resolution, harder-to-collect information (e.g.,
individual level records) from many low-resolution, easy-to-collect sources,
and propose a multi-stage framework called SYNC (Synthetic Data Generation via
Gaussian Copula). For given low-resolution datasets, the central idea of SYNC
is to fit Gaussian copula models to each of the low-resolution datasets in
order to correctly capture dependencies and marginal distributions, and then
sample from the fitted models to obtain the desired high-resolution subsets.
Predictive models are then used to merge sampled subsets into one, and finally,
sampled datasets are scaled according to low-resolution marginal constraints.
We make four key contributions in this work: 1) propose a novel framework for
generating individual level data from aggregated data sources by combining
state-of-the-art machine learning and statistical techniques, 2) perform
simulation studies to validate SYNC's performance as a synthetic data
generation algorithm, 3) demonstrate its value as a feature engineering tool,
as well as an alternative to data collection in situations where gathering is
difficult through two real-world datasets, 4) release an easy-to-use framework
implementation for reproducibility and scalability at the production level that
easily incorporates new data.",arxiv
http://arxiv.org/abs/1612.06699v3,2017-06-12T21:38:17Z,2016-12-20T15:04:38Z,Unsupervised Perceptual Rewards for Imitation Learning,"Reward function design and exploration time are arguably the biggest
obstacles to the deployment of reinforcement learning (RL) agents in the real
world. In many real-world tasks, designing a reward function takes considerable
hand engineering and often requires additional sensors to be installed just to
measure whether the task has been executed successfully. Furthermore, many
interesting tasks consist of multiple implicit intermediate steps that must be
executed in sequence. Even when the final outcome can be measured, it does not
necessarily provide feedback on these intermediate steps. To address these
issues, we propose leveraging the abstraction power of intermediate visual
representations learned by deep models to quickly infer perceptual reward
functions from small numbers of demonstrations. We present a method that is
able to identify key intermediate steps of a task from only a handful of
demonstration sequences, and automatically identify the most discriminative
features for identifying these steps. This method makes use of the features in
a pre-trained deep model, but does not require any explicit specification of
sub-goals. The resulting reward functions can then be used by an RL agent to
learn to perform the task in real-world settings. To evaluate the learned
reward, we present qualitative results on two real-world tasks and a
quantitative evaluation against a human-designed reward function. We also show
that our method can be used to learn a real-world door opening skill using a
real robot, even when the demonstration used for reward learning is provided by
a human using their own hand. To our knowledge, these are the first results
showing that complex robotic manipulation skills can be learned directly and
without supervised labels from a video of a human performing the task.
Supplementary material and data are available at
https://sermanet.github.io/rewards",arxiv
http://arxiv.org/abs/1707.08015v1,2017-07-25T14:40:18Z,2017-07-25T14:40:18Z,"Predicting Exploitation of Disclosed Software Vulnerabilities Using
  Open-source Data","Each year, thousands of software vulnerabilities are discovered and reported
to the public. Unpatched known vulnerabilities are a significant security risk.
It is imperative that software vendors quickly provide patches once
vulnerabilities are known and users quickly install those patches as soon as
they are available. However, most vulnerabilities are never actually exploited.
Since writing, testing, and installing software patches can involve
considerable resources, it would be desirable to prioritize the remediation of
vulnerabilities that are likely to be exploited. Several published research
studies have reported moderate success in applying machine learning techniques
to the task of predicting whether a vulnerability will be exploited. These
approaches typically use features derived from vulnerability databases (such as
the summary text describing the vulnerability) or social media posts that
mention the vulnerability by name. However, these prior studies share multiple
methodological shortcomings that inflate predictive power of these approaches.
We replicate key portions of the prior work, compare their approaches, and show
how selection of training and test data critically affect the estimated
performance of predictive models. The results of this study point to important
methodological considerations that should be taken into account so that results
reflect real-world utility.",arxiv
http://arxiv.org/abs/1909.11822v1,2019-09-25T23:52:57Z,2019-09-25T23:52:57Z,"DisCo: Physics-Based Unsupervised Discovery of Coherent Structures in
  Spatiotemporal Systems","Extracting actionable insight from complex unlabeled scientific data is an
open challenge and key to unlocking data-driven discovery in science.
Complementary and alternative to supervised machine learning approaches,
unsupervised physics-based methods based on behavior-driven theories hold great
promise. Due to computational limitations, practical application on real-world
domain science problems has lagged far behind theoretical development. We
present our first step towards bridging this divide - DisCo - a
high-performance distributed workflow for the behavior-driven local causal
state theory. DisCo provides a scalable unsupervised physics-based
representation learning method that decomposes spatiotemporal systems into
their structurally relevant components, which are captured by the latent local
causal state variables. Complex spatiotemporal systems are generally highly
structured and organize around a lower-dimensional skeleton of coherent
structures, and in several firsts we demonstrate the efficacy of DisCo in
capturing such structures from observational and simulated scientific data. To
the best of our knowledge, DisCo is also the first application software
developed entirely in Python to scale to over 1000 machine nodes, providing
good performance along with ensuring domain scientists' productivity. We
developed scalable, performant methods optimized for Intel many-core processors
that will be upstreamed to open-source Python library packages. Our capstone
experiment, using newly developed DisCo workflow and libraries, performs
unsupervised spacetime segmentation analysis of CAM5.1 climate simulation data,
processing an unprecedented 89.5 TB in 6.6 minutes end-to-end using 1024 Intel
Haswell nodes on the Cori supercomputer obtaining 91% weak-scaling and 64%
strong-scaling efficiency.",arxiv
http://arxiv.org/abs/2003.00205v2,2020-06-03T08:57:51Z,2020-02-29T08:06:19Z,The GWAC Data Processing and Management System,"GWAC will have been built an integrated FOV of 5,000 $degree^2$ and have
already built 1,800 square $degree^2$. The limit magnitude of a 10-second
exposure image in the moonless night is 16R. In each observation night, GWAC
produces about 0.7TB of raw data, and the data processing pipeline generates
millions of single frame alerts. We describe the GWAC Data Processing and
Management System (GPMS), including hardware architecture, database,
detection-filtering-validation of transient candidates, data archiving, and
user interfaces for the check of transient and the monitor of the system. GPMS
combines general technology and software in astronomy and computer field, and
use some advanced technologies such as deep learning. Practical results show
that GPMS can fully meet the scientific data processing requirement of GWAC. It
can online accomplish the detection, filtering and validation of millions of
transient candidates, and feedback the final results to the astronomer in
real-time. During the observation from October of 2018 to December of 2019, we
have already found 102 transients.",arxiv
http://arxiv.org/abs/2007.14432v1,2020-07-28T18:47:21Z,2020-07-28T18:47:21Z,"A Convolutional Neural Network for gaze preference detection: A
  potential tool for diagnostics of autism spectrum disorder in children","Early diagnosis of autism spectrum disorder (ASD) is known to improve the
quality of life of affected individuals. However, diagnosis is often delayed
even in wealthier countries including the US, largely due to the fact that gold
standard diagnostic tools such as the Autism Diagnostic Observation Schedule
(ADOS) and the Autism Diagnostic Interview-Revised (ADI-R) are time consuming
and require expertise to administer. This trend is even more pronounced lower
resources settings due to a lack of trained experts. As a result, alternative,
less technical methods that leverage the unique ways in which children with ASD
react to visual stimulation in a controlled environment have been developed to
help facilitate early diagnosis. Previous studies have shown that, when exposed
to a video that presents both social and abstract scenes side by side, a child
with ASD will focus their attention towards the abstract images on the screen
to a greater extent than a child without ASD. Such differential responses make
it possible to implement an algorithm for the rapid diagnosis of ASD based on
eye tracking against different visual stimuli. Here we propose a convolutional
neural network (CNN) algorithm for gaze prediction using images extracted from
a one-minute stimulus video. Our model achieved a high accuracy rate and
robustness for prediction of gaze direction with independent persons and
employing a different camera than the one used during testing. In addition to
this, the proposed algorithm achieves a fast response time, providing a near
real-time evaluation of ASD. Thereby, by applying the proposed method, we could
significantly reduce the diagnosis time and facilitate the diagnosis of ASD in
low resource regions.",arxiv
http://arxiv.org/abs/2107.09822v2,2021-07-22T05:48:12Z,2021-07-21T00:43:32Z,"Bayesian Controller Fusion: Leveraging Control Priors in Deep
  Reinforcement Learning for Robotics","We present Bayesian Controller Fusion (BCF): a hybrid control strategy that
combines the strengths of traditional hand-crafted controllers and model-free
deep reinforcement learning (RL). BCF thrives in the robotics domain, where
reliable but suboptimal control priors exist for many tasks, but RL from
scratch remains unsafe and data-inefficient. By fusing uncertainty-aware
distributional outputs from each system, BCF arbitrates control between them,
exploiting their respective strengths. We study BCF on two real-world robotics
tasks involving navigation in a vast and long-horizon environment, and a
complex reaching task that involves manipulability maximisation. For both these
domains, there exist simple handcrafted controllers that can solve the task at
hand in a risk-averse manner but do not necessarily exhibit the optimal
solution given limitations in analytical modelling, controller miscalibration
and task variation. As exploration is naturally guided by the prior in the
early stages of training, BCF accelerates learning, while substantially
improving beyond the performance of the control prior, as the policy gains more
experience. More importantly, given the risk-aversity of the control prior, BCF
ensures safe exploration and deployment, where the control prior naturally
dominates the action distribution in states unknown to the policy. We
additionally show BCF's applicability to the zero-shot sim-to-real setting and
its ability to deal with out-of-distribution states in the real-world. BCF is a
promising approach for combining the complementary strengths of deep RL and
traditional robotic control, surpassing what either can achieve independently.
The code and supplementary video material are made publicly available at
https://krishanrana.github.io/bcf.",arxiv
http://arxiv.org/abs/2110.00468v1,2021-10-01T15:03:03Z,2021-10-01T15:03:03Z,"New Evolutionary Computation Models and their Applications to Machine
  Learning","Automatic Programming is one of the most important areas of computer science
research today. Hardware speed and capability have increased exponentially, but
the software is years behind. The demand for software has also increased
significantly, but it is still written in old fashion: by using humans.
  There are multiple problems when the work is done by humans: cost, time,
quality. It is costly to pay humans, it is hard to keep them satisfied for a
long time, it takes a lot of time to teach and train them and the quality of
their output is in most cases low (in software, mostly due to bugs).
  The real advances in human civilization appeared during the industrial
revolutions. Before the first revolution, most people worked in agriculture.
Today, very few percent of people work in this field.
  A similar revolution must appear in the computer programming field.
Otherwise, we will have so many people working in this field as we had in the
past working in agriculture.
  How do people know how to write computer programs? Very simple: by learning.
Can we do the same for software? Can we put the software to learn how to write
software?
  It seems that is possible (to some degree) and the term is called Machine
Learning. It was first coined in 1959 by the first person who made a computer
perform a serious learning task, namely, Arthur Samuel.
  However, things are not so easy as in humans (well, truth to be said - for
some humans it is impossible to learn how to write software). So far we do not
have software that can learn perfectly to write software. We have some
particular cases where some programs do better than humans, but the examples
are sporadic at best. Learning from experience is difficult for computer
programs. Instead of trying to simulate how humans teach humans how to write
computer programs, we can simulate nature.",arxiv
http://arxiv.org/abs/1411.0440v8,2020-04-19T19:58:37Z,2014-11-03T11:50:19Z,Modelling serendipity in a computational context,"The term serendipity describes a creative process that develops, in context,
with the active participation of a creative agent, but not entirely within that
agent's control. While a system cannot be made to perform serendipitously on
demand, we argue that its $\mathit{serendipity\ potential}$ can be increased by
means of a suitable system architecture and other design choices. We distil a
unified description of serendipitous occurrences from historical theorisations
of serendipity and creativity. This takes the form of a framework with six
phases: $\mathit{perception}$, $\mathit{attention}$, $\mathit{interest}$,
$\mathit{explanation}$, $\mathit{bridge}$, and $\mathit{valuation}$. We then
use this framework to organise a survey of literature in cognitive science,
philosophy, and computing, which yields practical definitions of the six
phases, along with heuristics for implementation. We use the resulting model to
evaluate the serendipity potential of four existing systems developed by
others, and two systems previously developed by two of the authors. Most
existing research that considers serendipity in a computing context deals with
serendipity as a service; here we relate theories of serendipity to the
development of autonomous systems and computational creativity practice. We
argue that serendipity is not teleologically blind, and outline representative
directions for future applications of our model. We conclude that it is
feasible to equip computational systems with the potential for serendipity, and
that this could be beneficial in varied computational creativity/AI
applications, particularly those designed to operate responsively in real-world
contexts.",arxiv
http://arxiv.org/abs/1809.02270v5,2020-11-26T09:40:25Z,2018-09-07T01:33:13Z,"Learning Embeddings of Directed Networks with Text-Associated
  Nodes---with Applications in Software Package Dependency Networks","A network embedding consists of a vector representation for each node in the
network. Its usefulness has been shown in many real-world application domains,
such as social networks and web networks. Directed networks with text
associated with each node, such as software package dependency networks, are
commonplace. However, to the best of our knowledge, their embeddings have
hitherto not been specifically studied. In this paper, we propose PCTADW-1 and
PCTADW-2, two algorithms based on neural networks that learn embeddings of
directed networks with text associated with each node. We create two new
node-labeled such networks: The package dependency networks in two popular
GNU/Linux distributions, Debian and Fedora. We experimentally demonstrate that
the embeddings produced by our algorithms resulted in node classification with
better quality than those of various baselines on these two networks. We
observe that there exist systematic presence of analogies (similar to those in
word embeddings) in the network embeddings of software package dependency
networks. To the best of our knowledge, this is the first time that such
systematic presence of analogies is observed in network and document
embeddings. We further demonstrate that these network embeddings can be novelly
used for better understanding software attributes, such as the development
process and user interface of software, etc.",arxiv
http://arxiv.org/abs/2011.09780v1,2020-11-19T11:37:40Z,2020-11-19T11:37:40Z,Kernel Phase and Coronagraphy with Automatic Differentiation,"The accumulation of aberrations along the optical path in a telescope
produces distortions and speckles in the resulting images, limiting the
performance of cameras at high angular resolution. It is important to achieve
the highest possible sensitivity to faint sources such as planets, using both
hardware and data analysis software. While analytic methods are efficient, real
systems are better-modelled numerically, but such models with many parameters
can be hard to understand, optimize and apply. Automatic differentiation
software developed for machine learning now makes calculating derivatives with
respect to aberrations straightforward for arbitrary optical systems. We apply
this powerful new tool to enhance high-angular-resolution astronomical imaging.
Self-calibrating observables such as the 'closure phase' or 'bispectrum' have
been widely used in optical and radio astronomy to mitigate optical aberrations
and achieve high-fidelity imagery. Kernel phases are a generalization of
closure phases in the limit of small phase errors. Using automatic
differentiation, we reproduce existing kernel phase theory within this
framework and demonstrate an extension to the Lyot coronagraph, finding
self-calibrating combinations of speckles which are resistant to phase noise,
but only in the very high-wavefront-quality regime. As an illustrative example,
we reanalyze Palomar adaptive optics observations of the binary alpha Ophiuchi,
finding consistency between the new pipeline and the existing standard. We
present a new Python package 'morphine' that incorporates these ideas, with an
interface similar to the popular package poppy, for optical simulation with
automatic differentiation. These methods may be useful for designing improved
astronomical optical systems by gradient descent.",arxiv
http://arxiv.org/abs/2003.09052v2,2020-06-02T15:31:08Z,2020-03-20T00:20:36Z,Design and operation of the ATLAS Transient Science Server,"The Asteroid Terrestrial impact Last Alert System (ATLAS) system consists of
two 0.5m Schmidt telescopes with cameras covering 29 square degrees at plate
scale of 1.86 arcsec per pixel. Working in tandem, the telescopes routinely
survey the whole sky visible from Hawaii (above $\delta > -50^{\circ}$) every
two nights, exposing four times per night, typically reaching $o < 19$
magnitude per exposure when the moon is illuminated and $c < 19.5$ per exposure
in dark skies. Construction is underway of two further units to be sited in
Chile and South Africa which will result in an all-sky daily cadence from 2021.
Initially designed for detecting potentially hazardous near earth objects, the
ATLAS data enable a range of astrophysical time domain science. To extract
transients from the data stream requires a computing system to process the
data, assimilate detections in time and space and associate them with known
astrophysical sources. Here we describe the hardware and software
infrastructure to produce a stream of clean, real, astrophysical transients in
real time. This involves machine learning and boosted decision tree algorithms
to identify extragalactic and Galactic transients. Typically we detect 10-15
supernova candidates per night which we immediately announce publicly. The
ATLAS discoveries not only enable rapid follow-up of interesting sources but
will provide complete statistical samples within the local volume of 100 Mpc. A
simple comparison of the detected supernova rate within 100 Mpc, with no
corrections for completeness, is already significantly higher (factor 1.5 to 2)
than the current accepted rates.",arxiv
http://arxiv.org/abs/2003.06705v1,2020-03-14T21:11:02Z,2020-03-14T21:11:02Z,Identifying Individual Dogs in Social Media Images,"We present the results of an initial study focused on developing a visual AI
solution able to recognize individual dogs in unconstrained (wild) images
occurring on social media.
  The work described here is part of joint project done with Pet2Net, a social
network focused on pets and their owners. In order to detect and recognize
individual dogs we combine transfer learning and object detection approaches on
Inception v3 and SSD Inception v2 architectures respectively and evaluate the
proposed pipeline using a new data set containing real data that the users
uploaded to Pet2Net platform. We show that it can achieve 94.59% accuracy in
identifying individual dogs. Our approach has been designed with simplicity in
mind and the goal of easy deployment on all the images uploaded to Pet2Net
platform.
  A purely visual approach to identifying dogs in images, will enhance Pet2Net
features aimed at finding lost dogs, as well as form the basis of future work
focused on identifying social relationships between dogs, which cannot be
inferred from other data collected by the platform.",arxiv
http://arxiv.org/abs/1906.07391v3,2020-12-29T21:20:09Z,2019-06-18T05:51:57Z,"The Breakthrough Listen Search for Intelligent Life: Public Data,
  Formats, Reduction and Archiving","Breakthrough Listen is the most comprehensive and sensitive search for
extraterrestrial intelligence (SETI) to date, employing a collection of
international observational facilities including both radio and optical
telescopes. During the first three years of the Listen program, thousands of
targets have been observed with the Green Bank Telescope (GBT), Parkes
Telescope and Automated Planet Finder. At GBT and Parkes, observations have
been performed ranging from 700 MHz to 26 GHz, with raw data volumes averaging
over 1PB / day. A pseudo-real time software spectroscopy suite is used to
produce multi-resolution spectrograms amounting to approximately 400 GB hr^-1
GHz^-1 beam^-1. For certain targets, raw baseband voltage data is also
preserved. Observations with the Automated Planet Finder produce both
2-dimensional and 1-dimensional high resolution (R~10^5) echelle spectral data.
  Although the primary purpose of Listen data acquisition is for SETI, a range
of secondary science has also been performed with these data, including studies
of fast radio bursts. Other current and potential research topics include
spectral line studies, searches for certain kinds of dark matter, probes of
interstellar scattering, pulsar searches, radio transient searches and
investigations of stellar activity. Listen data are also being used in the
development of algorithms, including machine learning approaches to modulation
scheme classification and outlier detection, that have wide applicability not
just for astronomical research but for a broad range of science and
engineering.
  In this paper, we describe the hardware and software pipeline used for
collection, reduction, archival, and public dissemination of Listen data. We
describe the data formats and tools, and present Breakthrough Listen Data
Release 1.0 (BLDR 1.0), a defined set of publicly-available raw and reduced
data totalling 1 PB.",arxiv
http://arxiv.org/abs/2007.10243v1,2020-07-20T16:32:27Z,2020-07-20T16:32:27Z,Inter-Homines: Distance-Based Risk Estimation for Human Safety,"In this document, we report our proposal for modeling the risk of possible
contagiousity in a given area monitored by RGB cameras where people freely move
and interact. Our system, called Inter-Homines, evaluates in real-time the
contagion risk in a monitored area by analyzing video streams: it is able to
locate people in 3D space, calculate interpersonal distances and predict risk
levels by building dynamic maps of the monitored area. Inter-Homines works both
indoor and outdoor, in public and private crowded areas. The software is
applicable to already installed cameras or low-cost cameras on industrial PCs,
equipped with an additional embedded edge-AI system for temporary measurements.
From the AI-side, we exploit a robust pipeline for real-time people detection
and localization in the ground plane by homographic transformation based on
state-of-the-art computer vision algorithms; it is a combination of a people
detector and a pose estimator. From the risk modeling side, we propose a
parametric model for a spatio-temporal dynamic risk estimation, that, validated
by epidemiologists, could be useful for safety monitoring the acceptance of
social distancing prevention measures by predicting the risk level of the
scene.",arxiv
http://arxiv.org/abs/2009.01654v1,2020-09-03T13:40:13Z,2020-09-03T13:40:13Z,Indoor Localization Techniques Within a Home Monitoring Platform,"This paper details a number of indoor localization techniques developed for
real-time monitoring of older adults. These were developed within the framework
of the i-Light research project that was funded by the European Union. The
project targeted the development and initial evaluation of a configurable and
cost-effective cyber-physical system for monitoring the safety of older adults
who are living in their own homes. Localization hardware consists of a number
of custom-developed devices that replace existing luminaires. In addition to
lighting capabilities, they measure the strength of a Bluetooth Low Energy
signal emitted by a wearable device on the user. Readings are recorded in real
time and sent to a software server for analysis. We present a comparative
evaluation of the accuracy achieved by several server-side algorithms,
including Kalman filtering, a look-back heuristic as well as a neural
network-based approach. It is known that approaches based on measuring signal
strength are sensitive to the placement of walls, construction materials used,
the presence of doors as well as existing furniture. As such, we evaluate the
proposed approaches in two separate locations having distinct building
characteristics. We show that the proposed techniques improve the accuracy of
localization. As the final step, we evaluate our results against comparable
existing approaches.",arxiv
http://arxiv.org/abs/2104.14210v1,2021-04-29T08:59:36Z,2021-04-29T08:59:36Z,"Biased Edge Dropout for Enhancing Fairness in Graph Representation
  Learning","Graph representation learning has become a ubiquitous component in many
scenarios, ranging from social network analysis to energy forecasting in smart
grids. In several applications, ensuring the fairness of the node (or graph)
representations with respect to some protected attributes is crucial for their
correct deployment. Yet, fairness in graph deep learning remains
under-explored, with few solutions available. In particular, the tendency of
similar nodes to cluster on several real-world graphs (i.e., homophily) can
dramatically worsen the fairness of these procedures. In this paper, we propose
a biased edge dropout algorithm (FairDrop) to counter-act homophily and improve
fairness in graph representation learning. FairDrop can be plugged in easily on
many existing algorithms, is efficient, adaptable, and can be combined with
other fairness-inducing solutions. After describing the general algorithm, we
demonstrate its application on two benchmark tasks, specifically, as a random
walk model for producing node embeddings, and to a graph convolutional network
for link prediction. We prove that the proposed algorithm can successfully
improve the fairness of all models up to a small or negligible drop in
accuracy, and compares favourably with existing state-of-the-art solutions. In
an ablation study, we demonstrate that our algorithm can flexibly interpolate
between biasing towards fairness and an unbiased edge dropout. Furthermore, to
better evaluate the gains, we propose a new dyadic group definition to measure
the bias of a link prediction task when paired with group-based fairness
metrics. In particular, we extend the metric used to measure the bias in the
node embeddings to take into account the graph structure.",arxiv
http://arxiv.org/abs/2012.12305v2,2021-07-22T16:53:43Z,2020-12-22T19:27:11Z,"Confronting Abusive Language Online: A Survey from the Ethical and Human
  Rights Perspective","The pervasiveness of abusive content on the internet can lead to severe
psychological and physical harm. Significant effort in Natural Language
Processing (NLP) research has been devoted to addressing this problem through
abusive content detection and related sub-areas, such as the detection of hate
speech, toxicity, cyberbullying, etc. Although current technologies achieve
high classification performance in research studies, it has been observed that
the real-life application of this technology can cause unintended harms, such
as the silencing of under-represented groups. We review a large body of NLP
research on automatic abuse detection with a new focus on ethical challenges,
organized around eight established ethical principles: privacy, accountability,
safety and security, transparency and explainability, fairness and
non-discrimination, human control of technology, professional responsibility,
and promotion of human values. In many cases, these principles relate not only
to situational ethical codes, which may be context-dependent, but are in fact
connected to universal human rights, such as the right to privacy, freedom from
discrimination, and freedom of expression. We highlight the need to examine the
broad social impacts of this technology, and to bring ethical and human rights
considerations to every stage of the application life-cycle, from task
formulation and dataset design, to model training and evaluation, to
application deployment. Guided by these principles, we identify several
opportunities for rights-respecting, socio-technical solutions to detect and
confront online abuse, including `nudging', `quarantining', value sensitive
design, counter-narratives, style transfer, and AI-driven public education
applications.",arxiv
