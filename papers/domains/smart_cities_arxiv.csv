id,updated,published,title,summary,database
http://arxiv.org/abs/1810.04107v1,2018-10-09T16:27:46Z,2018-10-09T16:27:46Z,"Enabling Cognitive Smart Cities Using Big Data and Machine Learning:
  Approaches and Challenges","The development of smart cities and their fast-paced deployment is resulting
in the generation of large quantities of data at unprecedented rates.
Unfortunately, most of the generated data is wasted without extracting
potentially useful information and knowledge because of the lack of established
mechanisms and standards that benefit from the availability of such data.
Moreover, the high dynamical nature of smart cities calls for new generation of
machine learning approaches that are flexible and adaptable to cope with the
dynamicity of data to perform analytics and learn from real-time data. In this
article, we shed the light on the challenge of under utilizing the big data
generated by smart cities from a machine learning perspective. Especially, we
present the phenomenon of wasting unlabeled data. We argue that
semi-supervision is a must for smart city to address this challenge. We also
propose a three-level learning framework for smart cities that matches the
hierarchical nature of big data generated by smart cities with a goal of
providing different levels of knowledge abstractions. The proposed framework is
scalable to meet the needs of smart city services. Fundamentally, the framework
benefits from semi-supervised deep reinforcement learning where a small amount
of data that has users' feedback serves as labeled data while a larger amount
is without such users' feedback serves as unlabeled data. This paper also
explores how deep reinforcement learning and its shift toward semi-supervision
can handle the cognitive side of smart city services and improve their
performance by providing several use cases spanning the different domains of
smart cities. We also highlight several challenges as well as promising future
research directions for incorporating machine learning and high-level
intelligence into smart city services.",arxiv
http://arxiv.org/abs/2007.13233v1,2020-07-26T22:39:33Z,2020-07-26T22:39:33Z,Cyber Threat Intelligence for Secure Smart City,"Smart city improved the quality of life for the citizens by implementing
information communication technology (ICT) such as the internet of things
(IoT). Nevertheless, the smart city is a critical environment that needs to
secure it is network and data from intrusions and attacks. This work proposes a
hybrid deep learning (DL) model for cyber threat intelligence (CTI) to improve
threats classification performance based on convolutional neural network (CNN)
and quasi-recurrent neural network (QRNN). We use QRNN to provide a real-time
threat classification model. The evaluation results of the proposed model
compared to the state-of-the-art models show that the proposed model
outperformed the other models. Therefore, it will help in classifying the smart
city threats in a reasonable time.",arxiv
http://arxiv.org/abs/2001.00269v2,2020-04-01T05:40:23Z,2020-01-01T21:21:21Z,"A Smart, Efficient, and Reliable Parking Surveillance System with Edge
  Artificial Intelligence on IoT Devices","Cloud computing has been a main-stream computing service for years. Recently,
with the rapid development in urbanization, massive video surveillance data are
produced at an unprecedented speed. A traditional solution to deal with the big
data would require a large amount of computing and storage resources. With the
advances in Internet of things (IoT), artificial intelligence, and
communication technologies, edge computing offers a new solution to the problem
by processing the data partially or wholly on the edge of a surveillance
system. In this study, we investigate the feasibility of using edge computing
for smart parking surveillance tasks, which is a key component of Smart City.
The system processing pipeline is carefully designed with the consideration of
flexibility, online surveillance, data transmission, detection accuracy, and
system reliability. It enables artificial intelligence at the edge by
implementing an enhanced single shot multibox detector (SSD). A few more
algorithms are developed on both the edge and the server targeting optimal
system efficiency and accuracy. Thorough field tests were conducted in the
Angle Lake parking garage for three months. The experimental results are
promising that the final detection method achieves over 95% accuracy in
real-world scenarios with high efficiency and reliability. The proposed smart
parking surveillance system can be a solid foundation for future applications
of intelligent transportation systems.",arxiv
http://arxiv.org/abs/1807.00464v1,2018-07-02T05:17:15Z,2018-07-02T05:17:15Z,"Leveraging the Channel as a Sensor: Real-time Vehicle Classification
  Using Multidimensional Radio-fingerprinting","Upcoming Intelligent Transportation Systems (ITSs) will transform roads from
static resources to dynamic Cyber Physical Systems (CPSs) in order to satisfy
the requirements of future vehicular traffic in smart city environments.
Up-to-date information serves as the basis for changing street directions as
well as guiding individual vehicles to a fitting parking slot. In this context,
not only abstract indicators like traffic flow and density are required, but
also data about mobility parameters and class information of individual
vehicles. Consequently, accurate and reliable systems that are capable of
providing these kinds of information in real-time are highly demanded. In this
paper, we present a system for classifying vehicles based on their
radio-fingerprints which applies cutting-edge machine learning models and can
be non-intrusively installed into the existing road infrastructure in an ad-hoc
manner. In contrast to other approaches, it is able to provide accurate
classification results without causing privacy-violations or being vulnerable
to challenging weather conditions. Moreover, it is a promising candidate for
large-scale city deployments due to its cost-efficient installation and
maintenance properties. The proposed system is evaluated in a comprehensive
field evaluation campaign within an experimental live deployment on a German
highway, where it is able to achieve a binary classification success ratio of
more than 99% and an overall accuracy of 89.15% for a fine-grained
classification task with nine different classes.",arxiv
http://arxiv.org/abs/1807.02098v2,2018-08-14T23:31:16Z,2018-07-05T17:35:33Z,"MAT-CNN-SOPC: Motionless Analysis of Traffic Using Convolutional Neural
  Networks on System-On-a-Programmable-Chip","Intelligent Transportation Systems (ITS) have become an important pillar in
modern ""smart city"" framework which demands intelligent involvement of
machines. Traffic load recognition can be categorized as an important and
challenging issue for such systems. Recently, Convolutional Neural Network
(CNN) models have drawn considerable amount of interest in many areas such as
weather classification, human rights violation detection through images, due to
its accurate prediction capabilities. This work tackles real-life traffic load
recognition problem on System-On-a-Programmable-Chip (SOPC) platform and coin
it as MAT-CNN- SOPC, which uses an intelligent re-training mechanism of the CNN
with known environments. The proposed methodology is capable of enhancing the
efficacy of the approach by 2.44x in comparison to the state-of-art and proven
through experimental analysis. We have also introduced a mathematical equation,
which is capable of quantifying the suitability of using different CNN models
over the other for a particular application based implementation.",arxiv
http://arxiv.org/abs/2001.05703v1,2020-01-16T09:13:31Z,2020-01-16T09:13:31Z,"A Markerless Deep Learning-based 6 Degrees of Freedom PoseEstimation for
  with Mobile Robots using RGB Data","Augmented Reality has been subject to various integration efforts within
industries due to its ability to enhance human machine interaction and
understanding. Neural networks have achieved remarkable results in areas of
computer vision, which bear great potential to assist and facilitate an
enhanced Augmented Reality experience. However, most neural networks are
computationally intensive and demand huge processing power thus, are not
suitable for deployment on Augmented Reality devices. In this work we propose a
method to deploy state of the art neural networks for real time 3D object
localization on augmented reality devices. As a result, we provide a more
automated method of calibrating the AR devices with mobile robotic systems. To
accelerate the calibration process and enhance user experience, we focus on
fast 2D detection approaches which are extracting the 3D pose of the object
fast and accurately by using only 2D input. The results are implemented into an
Augmented Reality application for intuitive robot control and sensor data
visualization. For the 6D annotation of 2D images, we developed an annotation
tool, which is, to our knowledge, the first open source tool to be available.
We achieve feasible results which are generally applicable to any AR device
thus making this work promising for further research in combining high
demanding neural networks with Internet of Things devices.",arxiv
http://arxiv.org/abs/1907.10323v1,2019-07-24T09:27:11Z,2019-07-24T09:27:11Z,Fairness in Reinforcement Learning,"Decision support systems (e.g., for ecological conservation) and autonomous
systems (e.g., adaptive controllers in smart cities) start to be deployed in
real applications. Although their operations often impact many users or
stakeholders, no fairness consideration is generally taken into account in
their design, which could lead to completely unfair outcomes for some users or
stakeholders. To tackle this issue, we advocate for the use of social welfare
functions that encode fairness and present this general novel problem in the
context of (deep) reinforcement learning, although it could possibly be
extended to other machine learning tasks.",arxiv
http://arxiv.org/abs/2111.02845v1,2021-11-04T13:10:33Z,2021-11-04T13:10:33Z,"Attacking Deep Reinforcement Learning-Based Traffic Signal Control
  Systems with Colluding Vehicles","The rapid advancements of Internet of Things (IoT) and artificial
intelligence (AI) have catalyzed the development of adaptive traffic signal
control systems (ATCS) for smart cities. In particular, deep reinforcement
learning (DRL) methods produce the state-of-the-art performance and have great
potentials for practical applications. In the existing DRL-based ATCS, the
controlled signals collect traffic state information from nearby vehicles, and
then optimal actions (e.g., switching phases) can be determined based on the
collected information. The DRL models fully ""trust"" that vehicles are sending
the true information to the signals, making the ATCS vulnerable to adversarial
attacks with falsified information. In view of this, this paper first time
formulates a novel task in which a group of vehicles can cooperatively send
falsified information to ""cheat"" DRL-based ATCS in order to save their total
travel time. To solve the proposed task, we develop CollusionVeh, a generic and
effective vehicle-colluding framework composed of a road situation encoder, a
vehicle interpreter, and a communication mechanism. We employ our method to
attack established DRL-based ATCS and demonstrate that the total travel time
for the colluding vehicles can be significantly reduced with a reasonable
number of learning episodes, and the colluding effect will decrease if the
number of colluding vehicles increases. Additionally, insights and suggestions
for the real-world deployment of DRL-based ATCS are provided. The research
outcomes could help improve the reliability and robustness of the ATCS and
better protect the smart mobility systems.",arxiv
http://arxiv.org/abs/1711.10941v1,2017-11-29T16:23:38Z,2017-11-29T16:23:38Z,"Intelligent Traffic Light Control Using Distributed Multi-agent Q
  Learning","The combination of Artificial Intelligence (AI) and Internet-of-Things (IoT),
which is denoted as AI-powered Internet-of-Things (AIoT), is capable of
processing huge amount of data generated from a large number of devices and
handling complex problems in social infrastructures. As AI and IoT technologies
are becoming mature, in this paper, we propose to apply AIoT technologies for
traffic light control, which is an essential component for intelligent
transportation system, to improve the efficiency of smart city's road system.
Specifically, various sensors such as surveillance cameras provide real-time
information for intelligent traffic light control system to observe the states
of both motorized traffic and non-motorized traffic. In this paper, we propose
an intelligent traffic light control solution by using distributed multi-agent
Q learning, considering the traffic information at the neighboring
intersections as well as local motorized and non-motorized traffic, to improve
the overall performance of the entire control system. By using the proposed
multi-agent Q learning algorithm, our solution is targeting to optimize both
the motorized and non-motorized traffic. In addition, we considered many
constraints/rules for traffic light control in the real world, and integrate
these constraints in the learning algorithm, which can facilitate the proposed
solution to be deployed in real operational scenarios. We conducted numerical
simulations for a real-world map with real-world traffic data. The simulation
results show that our proposed solution outperforms existing solutions in terms
of vehicle and pedestrian queue lengths, waiting time at intersections, and
many other key performance metrics.",arxiv
http://arxiv.org/abs/1902.05377v1,2019-02-06T08:22:18Z,2019-02-06T08:22:18Z,UrbanFM: Inferring Fine-Grained Urban Flows,"Urban flow monitoring systems play important roles in smart city efforts
around the world. However, the ubiquitous deployment of monitoring devices,
such as CCTVs, induces a long-lasting and enormous cost for maintenance and
operation. This suggests the need for a technology that can reduce the number
of deployed devices, while preventing the degeneration of data accuracy and
granularity. In this paper, we aim to infer the real-time and fine-grained
crowd flows throughout a city based on coarse-grained observations. This task
is challenging due to two reasons: the spatial correlations between coarse- and
fine-grained urban flows, and the complexities of external impacts. To tackle
these issues, we develop a method entitled UrbanFM based on deep neural
networks. Our model consists of two major parts: 1) an inference network to
generate fine-grained flow distributions from coarse-grained inputs by using a
feature extraction module and a novel distributional upsampling module; 2) a
general fusion subnet to further boost the performance by considering the
influences of different external factors. Extensive experiments on two
real-world datasets, namely TaxiBJ and HappyValley, validate the effectiveness
and efficiency of our method compared to seven baselines, demonstrating the
state-of-the-art performance of our approach on the fine-grained urban flow
inference problem.",arxiv
http://arxiv.org/abs/1907.00498v4,2020-07-08T22:48:11Z,2019-06-30T23:46:30Z,"Proof of Witness Presence: Blockchain Consensus for Augmented Democracy
  in Smart Cities","Smart Cities evolve into complex and pervasive urban environments with a
citizens' mandate to meet sustainable development goals. Repositioning
democratic values of citizens' choices in these complex ecosystems has turned
out to be imperative in an era of social media filter bubbles, fake news and
opportunities for manipulating electoral results with such means. This paper
introduces a new paradigm of augmented democracy that promises actively
engaging citizens in a more informed decision-making augmented into public
urban space. The proposed concept is inspired by a digital revive of the
Ancient Agora of Athens, an arena of public discourse, a Polis where citizens
assemble to actively deliberate and collectively decide about public matters.
The core contribution of the proposed paradigm is the concept of proving
witness presence: making decision-making subject of providing secure evidence
and testifying for choices made in the physical space. This paper shows how the
challenge of proving witness presence can be tackled with blockchain consensus
to empower citizens' trust and overcome security vulnerabilities of GPS
localization. Moreover, a novel platform for collective decision-making and
crowd-sensing in urban space is introduced: Smart Agora. It is shown how
real-time collective measurements over citizens' choices can be made in a fully
decentralized and privacy-preserving way. Witness presence is tested by
deploying a decentralized system for crowd-sensing the sustainable use of
transport means. Furthermore, witness presence of cycling risk is validated
using official accident data from public authorities, which are compared
against wisdom of the crowd. The paramount role of dynamic consensus,
self-governance and ethically aligned artificial intelligence in the augmented
democracy paradigm is outlined.",arxiv
http://arxiv.org/abs/2110.01842v1,2021-10-05T06:34:58Z,2021-10-05T06:34:58Z,Dataset: Large-scale Urban IoT Activity Data for DDoS Attack Emulation,"As IoT deployments grow in scale for applications such as smart cities, they
face increasing cyber-security threats. In particular, as evidenced by the
famous Mirai incident and other ongoing threats, large-scale IoT device
networks are particularly susceptible to being hijacked and used as botnets to
launch distributed denial of service (DDoS) attacks. Real large-scale datasets
are needed to train and evaluate the use of machine learning algorithms such as
deep neural networks to detect and defend against such DDoS attacks. We present
a dataset from an urban IoT deployment of 4060 nodes describing their
spatio-temporal activity under benign conditions. We also provide a synthetic
DDoS attack generator that injects attack activity into the dataset based on
tunable parameters such as number of nodes attacked and duration of attack. We
discuss some of the features of the dataset. We also demonstrate the utility of
the dataset as well as our synthetic DDoS attack generator by using them for
the training and evaluation of a simple multi-label feed-forward neural network
that aims to identify which nodes are under attack and when.",arxiv
http://arxiv.org/abs/1802.06305v1,2018-02-17T22:37:17Z,2018-02-17T22:37:17Z,Machine learning for Internet of Things data analysis: A survey,"Rapid developments in hardware, software, and communication technologies have
allowed the emergence of Internet-connected sensory devices that provide
observation and data measurement from the physical world. By 2020, it is
estimated that the total number of Internet-connected devices being used will
be between 25 and 50 billion. As the numbers grow and technologies become more
mature, the volume of data published will increase. Internet-connected devices
technology, referred to as Internet of Things (IoT), continues to extend the
current Internet by providing connectivity and interaction between the physical
and cyber worlds. In addition to increased volume, the IoT generates Big Data
characterized by velocity in terms of time and location dependency, with a
variety of multiple modalities and varying data quality. Intelligent processing
and analysis of this Big Data is the key to developing smart IoT applications.
This article assesses the different machine learning methods that deal with the
challenges in IoT data by considering smart cities as the main use case. The
key contribution of this study is presentation of a taxonomy of machine
learning algorithms explaining how different techniques are applied to the data
in order to extract higher level information. The potential and challenges of
machine learning for IoT data analytics will also be discussed. A use case of
applying Support Vector Machine (SVM) on Aarhus Smart City traffic data is
presented for a more detailed exploration.",arxiv
http://arxiv.org/abs/2001.06202v1,2020-01-17T09:02:36Z,2020-01-17T09:02:36Z,"FedVision: An Online Visual Object Detection Platform Powered by
  Federated Learning","Visual object detection is a computer vision-based artificial intelligence
(AI) technique which has many practical applications (e.g., fire hazard
monitoring). However, due to privacy concerns and the high cost of transmitting
video data, it is highly challenging to build object detection models on
centrally stored large training datasets following the current approach.
Federated learning (FL) is a promising approach to resolve this challenge.
Nevertheless, there currently lacks an easy to use tool to enable computer
vision application developers who are not experts in federated learning to
conveniently leverage this technology and apply it in their systems. In this
paper, we report FedVision - a machine learning engineering platform to support
the development of federated learning powered computer vision applications. The
platform has been deployed through a collaboration between WeBank and Extreme
Vision to help customers develop computer vision-based safety monitoring
solutions in smart city applications. Over four months of usage, it has
achieved significant efficiency improvement and cost reduction while removing
the need to transmit sensitive data for three major corporate customers. To the
best of our knowledge, this is the first real application of FL in computer
vision-based tasks.",arxiv
http://arxiv.org/abs/1708.03854v2,2021-04-13T14:48:16Z,2017-08-13T04:58:49Z,IoT Data Analytics Using Deep Learning,"Deep learning is a popular machine learning approach which has achieved a lot
of progress in all traditional machine learning areas. Internet of thing (IoT)
and Smart City deployments are generating large amounts of time-series sensor
data in need of analysis. Applying deep learning to these domains has been an
important topic of research. The Long-Short Term Memory (LSTM) network has been
proven to be well suited for dealing with and predicting important events with
long intervals and delays in the time series. LTSM networks have the ability to
maintain long-term memory. In an LTSM network, a stacked LSTM hidden layer also
makes it possible to learn a high level temporal feature without the need of
any fine tuning and preprocessing which would be required by other techniques.
In this paper, we construct a long-short term memory (LSTM) recurrent neural
network structure, use the normal time series training set to build the
prediction model. And then we use the predicted error from the prediction model
to construct a Gaussian naive Bayes model to detect whether the original sample
is abnormal. This method is called LSTM-Gauss-NBayes for short. We use three
real-world data sets, each of which involve long-term time-dependence or
short-term time-dependence, even very weak time dependence. The experimental
results show that LSTM-Gauss-NBayes is an effective and robust model.",arxiv
http://arxiv.org/abs/1903.02503v1,2019-03-06T17:24:11Z,2019-03-06T17:24:11Z,The AI Driving Olympics at NeurIPS 2018,"Despite recent breakthroughs, the ability of deep learning and reinforcement
learning to outperform traditional approaches to control physically embodied
robotic agents remains largely unproven. To help bridge this gap, we created
the 'AI Driving Olympics' (AI-DO), a competition with the objective of
evaluating the state of the art in machine learning and artificial intelligence
for mobile robotics. Based on the simple and well specified autonomous driving
and navigation environment called 'Duckietown', AI-DO includes a series of
tasks of increasing complexity -- from simple lane-following to fleet
management. For each task, we provide tools for competitors to use in the form
of simulators, logs, code templates, baseline implementations and low-cost
access to robotic hardware. We evaluate submissions in simulation online, on
standardized hardware environments, and finally at the competition event. The
first AI-DO, AI-DO 1, occurred at the Neural Information Processing Systems
(NeurIPS) conference in December 2018. The results of AI-DO 1 highlight the
need for better benchmarks, which are lacking in robotics, as well as improved
mechanisms to bridge the gap between simulation and reality.",arxiv
http://arxiv.org/abs/1911.04469v1,2019-11-09T19:59:17Z,2019-11-09T19:59:17Z,"A Proposed Artificial intelligence Model for Real-Time Human Action
  Localization and Tracking","In recent years, artificial intelligence (AI) based on deep learning (DL) has
sparked tremendous global interest. DL is widely used today and has expanded
into various interesting areas. It is becoming more popular in cross-subject
research, such as studies of smart city systems, which combine computer science
with engineering applications. Human action detection is one of these areas.
Human action detection is an interesting challenge due to its stringent
requirements in terms of computing speed and accuracy. High-accuracy real-time
object tracking is also considered a significant challenge. This paper
integrates the YOLO detection network, which is considered a state-of-the-art
tool for real-time object detection, with motion vectors and the Coyote
Optimization Algorithm (COA) to construct a real-time human action localization
and tracking system. The proposed system starts with the extraction of motion
information from a compressed video stream and the extraction of appearance
information from RGB frames using an object detector. Then, a fusion step
between the two streams is performed, and the results are fed into the proposed
action tracking model. The COA is used in object tracking due to its accuracy
and fast convergence. The basic foundation of the proposed model is the
utilization of motion vectors, which already exist in a compressed video bit
stream and provide sufficient information to improve the localization of the
target action without requiring high consumption of computational resources
compared with other popular methods of extracting motion information, such as
optical flows. This advantage allows the proposed approach to be implemented in
challenging environments where the computational resources are limited, such as
Internet of Things (IoT) systems.",arxiv
http://arxiv.org/abs/2101.03889v2,2021-02-16T13:44:56Z,2020-12-21T10:25:27Z,A Comprehensive Survey of 6G Wireless Communications,"While fifth-generation (5G) communications are being rolled out worldwide,
sixth-generation (6G) communications have attracted much attention from both
the industry and the academia. Compared with 5G, 6G will have a wider frequency
band, higher transmission rate, spectrum efficiency, greater connection
capacity, shorter delay, broader coverage, and more robust anti-interference
capability to satisfy various network requirements. This survey presents an
insightful understanding of 6G wireless communications by introducing
requirements, features, critical technologies, challenges, and applications.
First, we give an overview of 6G from perspectives of technologies, security
and privacy, and applications. Subsequently, we introduce various 6G
technologies and their existing challenges in detail, e.g., artificial
intelligence (AI), intelligent surfaces, THz, space-air-ground-sea integrated
network, cell-free massive MIMO, etc. Because of these technologies, 6G is
expected to outperform existing wireless communication systems regarding the
transmission rate, latency, global coverage, etc. Next, we discuss security and
privacy techniques that can be applied to protect data in 6G. Since edge
devices are expected to gain popularity soon, the vast amount of generated data
and frequent data exchange make the leakage of data easily. Finally, we predict
real-world applications built on the technologies and features of 6G; for
example, smart healthcare, smart city, and smart manufacturing will be
implemented by taking advantage of AI.",arxiv
http://arxiv.org/abs/2002.04597v1,2020-02-11T18:41:57Z,2020-02-11T18:41:57Z,WatchDog: Real-time Vehicle Tracking on Geo-distributed Edge Nodes,"Vehicle tracking, a core application to smart city video analytics, is
becoming more widely deployed than ever before thanks to the increasing number
of traffic cameras and recent advances of computer vision and machine learning.
Due to the constraints of bandwidth, latency, and privacy concerns, tracking
tasks are more preferable to run on edge devices sitting close to the cameras.
However, edge devices are provisioned with a fixed amount of compute budget,
making them incompetent to adapt to time-varying tracking workloads caused by
traffic dynamics. In coping with this challenge, we propose WatchDog, a
real-time vehicle tracking system fully utilizes edge nodes across the road
network. WatchDog leverages computer vision tasks with different
resource-accuracy trade-offs, and decompose and schedule tracking tasks
judiciously across edge devices based on the current workload to maximize the
number of tasks while ensuring a provable response time bound at each edge
device. Extensive evaluations have been conducted using real-world city-wide
vehicle trajectory datasets, showing a 100% tracking coverage with real-time
guarantee.",arxiv
http://arxiv.org/abs/2011.09747v2,2021-09-29T15:26:04Z,2020-11-19T09:53:27Z,"Energy Aware Deep Reinforcement Learning Scheduling for Sensors
  Correlated in Time and Space","Millions of battery-powered sensors deployed for monitoring purposes in a
multitude of scenarios, e.g., agriculture, smart cities, industry, etc.,
require energy-efficient solutions to prolong their lifetime. When these
sensors observe a phenomenon distributed in space and evolving in time, it is
expected that collected observations will be correlated in time and space. In
this paper, we propose a Deep Reinforcement Learning (DRL) based scheduling
mechanism capable of taking advantage of correlated information. We design our
solution using the Deep Deterministic Policy Gradient (DDPG) algorithm. The
proposed mechanism is capable of determining the frequency with which sensors
should transmit their updates, to ensure accurate collection of observations,
while simultaneously considering the energy available. To evaluate our
scheduling mechanism, we use multiple datasets containing environmental
observations obtained in multiple real deployments. The real observations
enable us to model the environment with which the mechanism interacts as
realistically as possible. We show that our solution can significantly extend
the sensors' lifetime. We compare our mechanism to an idealized, all-knowing
scheduler to demonstrate that its performance is near-optimal. Additionally, we
highlight the unique feature of our design, energy-awareness, by displaying the
impact of sensors' energy levels on the frequency of updates.",arxiv
http://arxiv.org/abs/2101.02780v1,2021-01-07T22:01:30Z,2021-01-07T22:01:30Z,"SHARKS: Smart Hacking Approaches for RisK Scanning in Internet-of-Things
  and Cyber-Physical Systems based on Machine Learning","Cyber-physical systems (CPS) and Internet-of-Things (IoT) devices are
increasingly being deployed across multiple functionalities, ranging from
healthcare devices and wearables to critical infrastructures, e.g., nuclear
power plants, autonomous vehicles, smart cities, and smart homes. These devices
are inherently not secure across their comprehensive software, hardware, and
network stacks, thus presenting a large attack surface that can be exploited by
hackers. In this article, we present an innovative technique for detecting
unknown system vulnerabilities, managing these vulnerabilities, and improving
incident response when such vulnerabilities are exploited. The novelty of this
approach lies in extracting intelligence from known real-world CPS/IoT attacks,
representing them in the form of regular expressions, and employing machine
learning (ML) techniques on this ensemble of regular expressions to generate
new attack vectors and security vulnerabilities. Our results show that 10 new
attack vectors and 122 new vulnerability exploits can be successfully generated
that have the potential to exploit a CPS or an IoT ecosystem. The ML
methodology achieves an accuracy of 97.4% and enables us to predict these
attacks efficiently with an 87.2% reduction in the search space. We demonstrate
the application of our method to the hacking of the in-vehicle network of a
connected car. To defend against the known attacks and possible novel exploits,
we discuss a defense-in-depth mechanism for various classes of attacks and the
classification of data targeted by such attacks. This defense mechanism
optimizes the cost of security measures based on the sensitivity of the
protected resource, thus incentivizing its adoption in real-world CPS/IoT by
cybersecurity practitioners.",arxiv
http://arxiv.org/abs/1910.00497v1,2019-09-02T06:41:28Z,2019-09-02T06:41:28Z,Intelligent Metasurface Imager and Recognizer,"It is ever-increasingly demanded to remotely monitor people in daily life
using radio-frequency probing signals. However, conventional systems can hardly
be deployed in real-world settings since they typically require objects to
either deliberately cooperate or carry a wireless active device or
identification tag. To accomplish the complicated successive tasks using a
single device in real time, we propose a smart metasurface imager and
recognizer simultaneously, empowered by a network of artificial neural networks
(ANNs) for adaptively controlling data flow. Here, three ANNs are employed in
an integrated hierarchy: transforming measured microwave data into images of
whole human body; classifying the specifically designated spots (hand and
chest) within the whole image; and recognizing human hand signs instantly at
Wi-Fi frequency of 2.4 GHz. Instantaneous in-situ imaging of full scene and
adaptive recognition of hand signs and vital signs of multiple non-cooperative
people have been experimentally demonstrated. We also show that the proposed
intelligent metasurface system work well even when it is passively excited by
stray Wi-Fi signals that ubiquitously exist in our daily lives. The reported
strategy could open a new avenue for future smart cities, smart homes,
human-device interactive interfaces, healthy monitoring, and safety screening
free of visual privacy issues.",arxiv
http://arxiv.org/abs/2007.05828v1,2020-07-11T18:41:47Z,2020-07-11T18:41:47Z,Understanding Object Detection Through An Adversarial Lens,"Deep neural networks based object detection models have revolutionized
computer vision and fueled the development of a wide range of visual
recognition applications. However, recent studies have revealed that deep
object detectors can be compromised under adversarial attacks, causing a victim
detector to detect no object, fake objects, or mislabeled objects. With object
detection being used pervasively in many security-critical applications, such
as autonomous vehicles and smart cities, we argue that a holistic approach for
an in-depth understanding of adversarial attacks and vulnerabilities of deep
object detection systems is of utmost importance for the research community to
develop robust defense mechanisms. This paper presents a framework for
analyzing and evaluating vulnerabilities of the state-of-the-art object
detectors under an adversarial lens, aiming to analyze and demystify the attack
strategies, adverse effects, and costs, as well as the cross-model and
cross-resolution transferability of attacks. Using a set of quantitative
metrics, extensive experiments are performed on six representative deep object
detectors from three popular families (YOLOv3, SSD, and Faster R-CNN) with two
benchmark datasets (PASCAL VOC and MS COCO). We demonstrate that the proposed
framework can serve as a methodical benchmark for analyzing adversarial
behaviors and risks in real-time object detection systems. We conjecture that
this framework can also serve as a tool to assess the security risks and the
adversarial robustness of deep object detectors to be deployed in real-world
applications.",arxiv
http://arxiv.org/abs/2006.05873v1,2020-06-10T14:57:58Z,2020-06-10T14:57:58Z,WasteNet: Waste Classification at the Edge for Smart Bins,"Smart Bins have become popular in smart cities and campuses around the world.
These bins have a compaction mechanism that increases the bins' capacity as
well as automated real-time collection notifications. In this paper, we propose
WasteNet, a waste classification model based on convolutional neural networks
that can be deployed on a low power device at the edge of the network, such as
a Jetson Nano. The problem of segregating waste is a big challenge for many
countries around the world. Automated waste classification at the edge allows
for fast intelligent decisions in smart bins without needing access to the
cloud. Waste is classified into six categories: paper, cardboard, glass, metal,
plastic and other. Our model achieves a 97\% prediction accuracy on the test
dataset. This level of classification accuracy will help to alleviate some
common smart bin problems, such as recycling contamination, where different
types of waste become mixed with recycling waste causing the bin to be
contaminated. It also makes the bins more user friendly as citizens do not have
to worry about disposing their rubbish in the correct bin as the smart bin will
be able to make the decision for them.",arxiv
http://arxiv.org/abs/2012.05410v1,2020-12-10T02:08:47Z,2020-12-10T02:08:47Z,Artificial Intelligence at the Edge,"The Internet of Things (IoT) and edge computing applications aim to support a
variety of societal needs, including the global pandemic situation that the
entire world is currently experiencing and responses to natural disasters.
  The need for real-time interactive applications such as immersive video
conferencing, augmented/virtual reality, and autonomous vehicles, in education,
healthcare, disaster recovery and other domains, has never been higher. At the
same time, there have been recent technological breakthroughs in highly
relevant fields such as artificial intelligence (AI)/machine learning (ML),
advanced communication systems (5G and beyond), privacy-preserving
computations, and hardware accelerators. 5G mobile communication networks
increase communication capacity, reduce transmission latency and error, and
save energy -- capabilities that are essential for new applications. The
envisioned future 6G technology will integrate many more technologies,
including for example visible light communication, to support groundbreaking
applications, such as holographic communications and high precision
manufacturing. Many of these applications require computations and analytics
close to application end-points: that is, at the edge of the network, rather
than in a centralized cloud. AI techniques applied at the edge have tremendous
potential both to power new applications and to need more efficient operation
of edge infrastructure. However, it is critical to understand where to deploy
AI systems within complex ecosystems consisting of advanced applications and
the specific real-time requirements towards AI systems.",arxiv
http://arxiv.org/abs/1905.03418v2,2019-06-07T15:31:48Z,2019-05-09T02:39:37Z,"Deep Learning Acceleration Techniques for Real Time Mobile Vision
  Applications","Deep Learning (DL) has become a crucial technology for Artificial
Intelligence (AI). It is a powerful technique to automatically extract
high-level features from complex data which can be exploited for applications
such as computer vision, natural language processing, cybersecurity,
communications, and so on. For the particular case of computer vision, several
algorithms like object detection in real time videos have been proposed and
they work well on Desktop GPUs and distributed computing platforms. However
these algorithms are still heavy for mobile and embedded visual applications.
The rapid spreading of smart portable devices and the emerging 5G network are
introducing new smart multimedia applications in mobile environments. As a
consequence, the possibility of implementing deep neural networks to mobile
environments has attracted a lot of researchers. This paper presents emerging
deep learning acceleration techniques that can enable the delivery of real time
visual recognition into the hands of end users, anytime and anywhere.",arxiv
http://arxiv.org/abs/2106.15021v1,2021-06-21T11:23:12Z,2021-06-21T11:23:12Z,"How to Reach Real-Time AI on Consumer Devices? Solutions for
  Programmable and Custom Architectures","The unprecedented performance of deep neural networks (DNNs) has led to large
strides in various Artificial Intelligence (AI) inference tasks, such as object
and speech recognition. Nevertheless, deploying such AI models across commodity
devices faces significant challenges: large computational cost, multiple
performance objectives, hardware heterogeneity and a common need for high
accuracy, together pose critical problems to the deployment of DNNs across the
various embedded and mobile devices in the wild. As such, we have yet to
witness the mainstream usage of state-of-the-art deep learning algorithms
across consumer devices. In this paper, we provide preliminary answers to this
potentially game-changing question by presenting an array of design techniques
for efficient AI systems. We start by examining the major roadblocks when
targeting both programmable processors and custom accelerators. Then, we
present diverse methods for achieving real-time performance following a
cross-stack approach. These span model-, system- and hardware-level techniques,
and their combination. Our findings provide illustrative examples of AI systems
that do not overburden mobile hardware, while also indicating how they can
improve inference accuracy. Moreover, we showcase how custom ASIC- and
FPGA-based accelerators can be an enabling factor for next-generation AI
applications, such as multi-DNN systems. Collectively, these results highlight
the critical need for further exploration as to how the various cross-stack
solutions can be best combined in order to bring the latest advances in deep
learning close to users, in a robust and efficient manner.",arxiv
http://arxiv.org/abs/1910.01092v2,2020-02-04T18:47:28Z,2019-10-02T17:17:22Z,"A Machine Learning framework for Sleeping Cell Detection in a Smart-city
  IoT Telecommunications Infrastructure","The smooth operation of largely deployed Internet of Things (IoT)
applications will depend on, among other things, effective infrastructure
failure detection. Access failures in wireless network Base Stations (BSs)
produce a phenomenon called ""sleeping cells"", which can render a cell catatonic
without triggering any alarms or provoking immediate effects on cell
performance, making them difficult to discover. To detect this kind of failure,
we propose a Machine Learning (ML) framework based on the use of Key
Performance Indicator (KPI) statistics from the BS under study, as well as
those of the neighboring BSs with propensity to have their performance affected
by the failure. A simple way to define neighbors is to use adjacency in Voronoi
diagrams. In this paper, we propose a much more realistic approach based on the
nature of radio-propagation and the way devices choose the BS to which they
send access requests. We gather data from large-scale simulators that use real
location data for BSs and IoT devices and pose the detection problem as a
supervised binary classification problem. We measure the effects on the
detection performance by the size of time aggregations of the data, the level
of traffic and the parameters of the neighborhood definition. The Extra Trees
and Naive Bayes classifiers achieve Receiver Operating Characteristic (ROC)
Area Under the Curve (AUC) scores of 0.996 and 0.993, respectively, with False
Positive Rate (FPR) under 5 %. The proposed framework holds potential for other
pattern recognition tasks in smart-city wireless infrastructures, that would
enable the monitoring, prediction and improvement of the Quality of Service
(QoS) experienced by IoT applications.",arxiv
http://arxiv.org/abs/2104.06826v1,2021-04-14T12:57:40Z,2021-04-14T12:57:40Z,Towards Unsupervised Fine-Tuning for Edge Video Analytics,"Judging by popular and generic computer vision challenges, such as the
ImageNet or PASCAL VOC, neural networks have proven to be exceptionally
accurate in recognition tasks. However, state-of-the-art accuracy often comes
at a high computational price, requiring equally state-of-the-art and high-end
hardware acceleration to achieve anything near real-time performance. At the
same time, use cases such as smart cities or autonomous vehicles require an
automated analysis of images from fixed cameras in real-time. Due to the huge
and constant amount of network bandwidth these streams would generate, we
cannot rely on offloading compute to the omnipresent and omnipotent cloud.
Therefore, a distributed Edge Cloud must be in charge to process images
locally. However, the Edge Cloud is, by nature, resource-constrained, which
puts a limit on the computational complexity of the models executed in the
edge. Nonetheless, there is a need for a meeting point between the Edge Cloud
and accurate real-time video analytics. In this paper, we propose a method for
improving accuracy of edge models without any extra compute cost by means of
automatic model specialization. First, we show how the sole assumption of
static cameras allows us to make a series of considerations that greatly
simplify the scope of the problem. Then, we present Edge AutoTuner, a framework
that implements and brings these considerations together to automate the
end-to-end fine-tuning of models. Finally, we show that complex neural networks
- able to generalize better - can be effectively used as teachers to annotate
datasets for the fine-tuning of lightweight neural networks and tailor them to
the specific edge context, which boosts accuracy at constant computational
cost, and do so without any human interaction. Results show that our method can
automatically improve accuracy of pre-trained models by an average of 21%.",arxiv
http://arxiv.org/abs/2009.00351v1,2020-09-01T11:10:13Z,2020-09-01T11:10:13Z,"Advancing from Predictive Maintenance to Intelligent Maintenance with AI
  and IIoT","As Artificial Intelligent (AI) technology advances and increasingly large
amounts of data become readily available via various Industrial Internet of
Things (IIoT) projects, we evaluate the state of the art of predictive
maintenance approaches and propose our innovative framework to improve the
current practice. The paper first reviews the evolution of reliability
modelling technology in the past 90 years and discusses major technologies
developed in industry and academia. We then introduce the next generation
maintenance framework - Intelligent Maintenance, and discuss its key
components. This AI and IIoT based Intelligent Maintenance framework is
composed of (1) latest machine learning algorithms including probabilistic
reliability modelling with deep learning, (2) real-time data collection,
transfer, and storage through wireless smart sensors, (3) Big Data
technologies, (4) continuously integration and deployment of machine learning
models, (5) mobile device and AR/VR applications for fast and better
decision-making in the field. Particularly, we proposed a novel probabilistic
deep learning reliability modelling approach and demonstrate it in the Turbofan
Engine Degradation Dataset.",arxiv
http://arxiv.org/abs/2004.14619v1,2020-04-30T07:47:14Z,2020-04-30T07:47:14Z,The 4th AI City Challenge,"The AI City Challenge was created to accelerate intelligent video analysis
that helps make cities smarter and safer. Transportation is one of the largest
segments that can benefit from actionable insights derived from data captured
by sensors, where computer vision and deep learning have shown promise in
achieving large-scale practical deployment. The 4th annual edition of the AI
City Challenge has attracted 315 participating teams across 37 countries, who
leveraged city-scale real traffic data and high-quality synthetic data to
compete in four challenge tracks. Track 1 addressed video-based automatic
vehicle counting, where the evaluation is conducted on both algorithmic
effectiveness and computational efficiency. Track 2 addressed city-scale
vehicle re-identification with augmented synthetic data to substantially
increase the training set for the task. Track 3 addressed city-scale
multi-target multi-camera vehicle tracking. Track 4 addressed traffic anomaly
detection. The evaluation system shows two leader boards, in which a general
leader board shows all submitted results, and a public leader board shows
results limited to our contest participation rules, that teams are not allowed
to use external data in their work. The public leader board shows results more
close to real-world situations where annotated data are limited. Our results
show promise that AI technology can enable smarter and safer transportation
systems.",arxiv
http://arxiv.org/abs/2008.01257v1,2020-08-04T00:44:54Z,2020-08-04T00:44:54Z,Reinforced Epidemic Control: Saving Both Lives and Economy,"Saving lives or economy is a dilemma for epidemic control in most cities
while smart-tracing technology raises people's privacy concerns. In this paper,
we propose a solution for the life-or-economy dilemma that does not require
private data. We bypass the private-data requirement by suppressing epidemic
transmission through a dynamic control on inter-regional mobility that only
relies on Origin-Designation (OD) data. We develop DUal-objective
Reinforcement-Learning Epidemic Control Agent (DURLECA) to search
mobility-control policies that can simultaneously minimize infection spread and
maximally retain mobility. DURLECA hires a novel graph neural network, namely
Flow-GNN, to estimate the virus-transmission risk induced by urban mobility.
The estimated risk is used to support a reinforcement learning agent to
generate mobility-control actions. The training of DURLECA is guided with a
well-constructed reward function, which captures the natural trade-off relation
between epidemic control and mobility retaining. Besides, we design two
exploration strategies to improve the agent's searching efficiency and help it
get rid of local optimums. Extensive experimental results on a real-world OD
dataset show that DURLECA is able to suppress infections at an extremely low
level while retaining 76\% of the mobility in the city. Our implementation is
available at https://github.com/anyleopeace/DURLECA/.",arxiv
http://arxiv.org/abs/2102.12165v1,2021-02-24T09:36:39Z,2021-02-24T09:36:39Z,"Efficient Low-Latency Dynamic Licensing for Deep Neural Network
  Deployment on Edge Devices","Along with the rapid development in the field of artificial intelligence,
especially deep learning, deep neural network applications are becoming more
and more popular in reality. To be able to withstand the heavy load from
mainstream users, deployment techniques are essential in bringing neural
network models from research to production. Among the two popular computing
topologies for deploying neural network models in production are
cloud-computing and edge-computing. Recent advances in communication
technologies, along with the great increase in the number of mobile devices,
has made edge-computing gradually become an inevitable trend. In this paper, we
propose an architecture to solve deploying and processing deep neural networks
on edge-devices by leveraging their synergy with the cloud and the
access-control mechanisms of the database. Adopting this architecture allows
low-latency DNN model updates on devices. At the same time, with only one model
deployed, we can easily make different versions of it by setting access
permissions on the model weights. This method allows for dynamic model
licensing, which benefits commercial applications.",arxiv
http://arxiv.org/abs/2109.13963v1,2021-09-28T18:09:29Z,2021-09-28T18:09:29Z,"Smart at what cost? Characterising Mobile Deep Neural Networks in the
  wild","With smartphones' omnipresence in people's pockets, Machine Learning (ML) on
mobile is gaining traction as devices become more powerful. With applications
ranging from visual filters to voice assistants, intelligence on mobile comes
in many forms and facets. However, Deep Neural Network (DNN) inference remains
a compute intensive workload, with devices struggling to support intelligence
at the cost of responsiveness.On the one hand, there is significant research on
reducing model runtime requirements and supporting deployment on embedded
devices. On the other hand, the strive to maximise the accuracy of a task is
supported by deeper and wider neural networks, making mobile deployment of
state-of-the-art DNNs a moving target.
  In this paper, we perform the first holistic study of DNN usage in the wild
in an attempt to track deployed models and match how these run on widely
deployed devices. To this end, we analyse over 16k of the most popular apps in
the Google Play Store to characterise their DNN usage and performance across
devices of different capabilities, both across tiers and generations.
Simultaneously, we measure the models' energy footprint, as a core cost
dimension of any mobile deployment. To streamline the process, we have
developed gaugeNN, a tool that automates the deployment, measurement and
analysis of DNNs on devices, with support for different frameworks and
platforms. Results from our experience study paint the landscape of deep
learning deployments on smartphones and indicate their popularity across app
developers. Furthermore, our study shows the gap between bespoke techniques and
real-world deployments and the need for optimised deployment of deep learning
models in a highly dynamic and heterogeneous ecosystem.",arxiv
http://arxiv.org/abs/1705.00346v1,2017-04-30T17:17:44Z,2017-04-30T17:17:44Z,Deep Learning in the Automotive Industry: Applications and Tools,"Deep Learning refers to a set of machine learning techniques that utilize
neural networks with many hidden layers for tasks, such as image
classification, speech recognition, language understanding. Deep learning has
been proven to be very effective in these domains and is pervasively used by
many Internet services. In this paper, we describe different automotive uses
cases for deep learning in particular in the domain of computer vision. We
surveys the current state-of-the-art in libraries, tools and infrastructures
(e.\,g.\ GPUs and clouds) for implementing, training and deploying deep neural
networks. We particularly focus on convolutional neural networks and computer
vision use cases, such as the visual inspection process in manufacturing plants
and the analysis of social media data. To train neural networks, curated and
labeled datasets are essential. In particular, both the availability and scope
of such datasets is typically very limited. A main contribution of this paper
is the creation of an automotive dataset, that allows us to learn and
automatically recognize different vehicle properties. We describe an end-to-end
deep learning application utilizing a mobile app for data collection and
process support, and an Amazon-based cloud backend for storage and training.
For training we evaluate the use of cloud and on-premises infrastructures
(including multiple GPUs) in conjunction with different neural network
architectures and frameworks. We assess both the training times as well as the
accuracy of the classifier. Finally, we demonstrate the effectiveness of the
trained classifier in a real world setting during manufacturing process.",arxiv
http://arxiv.org/abs/2009.11722v1,2020-09-23T09:23:29Z,2020-09-23T09:23:29Z,"Cloud2Edge Elastic AI Framework for Prototyping and Deployment of AI
  Inference Engines in Autonomous Vehicles","Self-driving cars and autonomous vehicles are revolutionizing the automotive
sector, shaping the future of mobility altogether. Although the integration of
novel technologies such as Artificial Intelligence (AI) and Cloud/Edge
computing provides golden opportunities to improve autonomous driving
applications, there is the need to modernize accordingly the whole prototyping
and deployment cycle of AI components. This paper proposes a novel framework
for developing so-called AI Inference Engines for autonomous driving
applications based on deep learning modules, where training tasks are deployed
elastically over both Cloud and Edge resources, with the purpose of reducing
the required network bandwidth, as well as mitigating privacy issues. Based on
our proposed data driven V-Model, we introduce a simple yet elegant solution
for the AI components development cycle, where prototyping takes place in the
cloud according to the Software-in-the-Loop (SiL) paradigm, while deployment
and evaluation on the target ECUs (Electronic Control Units) is performed as
Hardware-in-the-Loop (HiL) testing. The effectiveness of the proposed framework
is demonstrated using two real-world use-cases of AI inference engines for
autonomous vehicles, that is environment perception and most probable path
prediction.",arxiv
http://arxiv.org/abs/2004.14850v1,2020-04-30T15:02:08Z,2020-04-30T15:02:08Z,6G White Paper on Edge Intelligence,"In this white paper we provide a vision for 6G Edge Intelligence. Moving
towards 5G and beyond the future 6G networks, intelligent solutions utilizing
data-driven machine learning and artificial intelligence become crucial for
several real-world applications including but not limited to, more efficient
manufacturing, novel personal smart device environments and experiences, urban
computing and autonomous traffic settings. We present edge computing along with
other 6G enablers as a key component to establish the future 2030 intelligent
Internet technologies as shown in this series of 6G White Papers.
  In this white paper, we focus in the domains of edge computing infrastructure
and platforms, data and edge network management, software development for edge,
and real-time and distributed training of ML/AI algorithms, along with
security, privacy, pricing, and end-user aspects. We discuss the key enablers
and challenges and identify the key research questions for the development of
the Intelligent Edge services. As a main outcome of this white paper, we
envision a transition from Internet of Things to Intelligent Internet of
Intelligent Things and provide a roadmap for development of 6G Intelligent
Edge.",arxiv
http://arxiv.org/abs/2005.08076v1,2020-05-16T19:42:16Z,2020-05-16T19:42:16Z,"A Deep Learning based Wearable Healthcare IoT Device for AI-enabled
  Hearing Assistance Automation","With the recent booming of artificial intelligence (AI), particularly deep
learning techniques, digital healthcare is one of the prevalent areas that
could gain benefits from AI-enabled functionality. This research presents a
novel AI-enabled Internet of Things (IoT) device operating from the ESP-8266
platform capable of assisting those who suffer from impairment of hearing or
deafness to communicate with others in conversations. In the proposed solution,
a server application is created that leverages Google's online speech
recognition service to convert the received conversations into texts, then
deployed to a micro-display attached to the glasses to display the conversation
contents to deaf people, to enable and assist conversation as normal with the
general population. Furthermore, in order to raise alert of traffic or
dangerous scenarios, an 'urban-emergency' classifier is developed using a deep
learning model, Inception-v4, with transfer learning to detect/recognize
alerting/alarming sounds, such as a horn sound or a fire alarm, with texts
generated to alert the prospective user. The training of Inception-v4 was
carried out on a consumer desktop PC and then implemented into the AI based IoT
application. The empirical results indicate that the developed prototype system
achieves an accuracy rate of 92% for sound recognition and classification with
real-time performance.",arxiv
http://arxiv.org/abs/1901.04985v1,2019-01-12T06:00:05Z,2019-01-12T06:00:05Z,"NNStreamer: Stream Processing Paradigm for Neural Networks, Toward
  Efficient Development and Execution of On-Device AI Applications","We propose nnstreamer, a software system that handles neural networks as
filters of stream pipelines, applying the stream processing paradigm to neural
network applications. A new trend with the wide-spread of deep neural network
applications is on-device AI; i.e., processing neural networks directly on
mobile devices or edge/IoT devices instead of cloud servers. Emerging privacy
issues, data transmission costs, and operational costs signifies the need for
on-device AI especially when a huge number of devices with real-time data
processing are deployed. Nnstreamer efficiently handles neural networks with
complex data stream pipelines on devices, improving the overall performance
significantly with minimal efforts. Besides, nnstreamer simplifies the neural
network pipeline implementations and allows reusing off-shelf multimedia stream
filters directly; thus it reduces the developmental costs significantly.
Nnstreamer is already being deployed with a product releasing soon and is open
source software applicable to a wide range of hardware architectures and
software platforms.",arxiv
http://arxiv.org/abs/2109.09165v1,2021-09-19T16:59:01Z,2021-09-19T16:59:01Z,Traffic-Net: 3D Traffic Monitoring Using a Single Camera,"Computer Vision has played a major role in Intelligent Transportation Systems
(ITS) and traffic surveillance. Along with the rapidly growing automated
vehicles and crowded cities, the automated and advanced traffic management
systems (ATMS) using video surveillance infrastructures have been evolved by
the implementation of Deep Neural Networks. In this research, we provide a
practical platform for real-time traffic monitoring, including 3D
vehicle/pedestrian detection, speed detection, trajectory estimation,
congestion detection, as well as monitoring the interaction of vehicles and
pedestrians, all using a single CCTV traffic camera. We adapt a custom YOLOv5
deep neural network model for vehicle/pedestrian detection and an enhanced SORT
tracking algorithm. For the first time, a hybrid satellite-ground based inverse
perspective mapping (SG-IPM) method for camera auto-calibration is also
developed which leads to an accurate 3D object detection and visualisation. We
also develop a hierarchical traffic modelling solution based on short- and
long-term temporal video data stream to understand the traffic flow,
bottlenecks, and risky spots for vulnerable road users. Several experiments on
real-world scenarios and comparisons with state-of-the-art are conducted using
various traffic monitoring datasets, including MIO-TCD, UA-DETRAC and GRAM-RTM
collected from highways, intersections, and urban areas under different
lighting and weather conditions.",arxiv
http://arxiv.org/abs/1911.03878v2,2019-12-08T09:39:28Z,2019-11-10T08:59:21Z,"An Overview of Data-Importance Aware Radio Resource Management for Edge
  Machine Learning","The 5G network connecting billions of Internet-of-Things (IoT) devices will
make it possible to harvest an enormous amount of real-time mobile data.
Furthermore, the 5G virtualization architecture will enable cloud computing at
the (network) edge. The availability of both rich data and computation power at
the edge has motivated Internet companies to deploy artificial intelligence
(AI) there, creating the hot area of edge-AI. Edge learning, the theme of this
project, concerns training edge-AI models, which endow on IoT devices
intelligence for responding to real-time events. However, the transmission of
high-dimensional data from many edge devices to servers can result in excessive
communication latency, creating a bottleneck for edge learning. Traditional
wireless techniques deigned for only radio access are ineffective in tackling
the challenge. Attempts to overcome the communication bottleneck has led to the
development of a new class of techniques for intelligent radio resource
management (RRM), called data-importance aware RRM. Their designs feature the
interplay of active machine learning and wireless communication. Specifically,
the metrics that measure data importance in active learning (e.g.,
classification uncertainty and data diversity) are applied to RRM for efficient
acquisition of distributed data in wireless networks to train AI models at
servers. This article aims at providing an introduction to the emerging area of
importance-aware RRM. To this end, we will introduce the design principles,
survey recent advancements in the area, discuss some design examples, and
suggest some promising research opportunities.",arxiv
http://arxiv.org/abs/2109.07571v1,2021-09-14T15:41:13Z,2021-09-14T15:41:13Z,"Secure Your Ride: Real-time Matching Success Rate Prediction for
  Passenger-Driver Pairs","In recent years, online ride-hailing platforms have become an indispensable
part of urban transportation. After a passenger is matched up with a driver by
the platform, both the passenger and the driver have the freedom to simply
accept or cancel a ride with one click. Hence, accurately predicting whether a
passenger-driver pair is a good match turns out to be crucial for ride-hailing
platforms to devise instant order assignments. However, since the users of
ride-hailing platforms consist of two parties, decision-making needs to
simultaneously account for the dynamics from both the driver and the passenger
sides. This makes it more challenging than traditional online advertising
tasks. Moreover, the amount of available data is severely imbalanced across
different cities, creating difficulties for training an accurate model for
smaller cities with scarce data. Though a sophisticated neural network
architecture can help improve the prediction accuracy under data scarcity, the
overly complex design will impede the model's capacity of delivering timely
predictions in a production environment. In the paper, to accurately predict
the MSR of passenger-driver, we propose the Multi-View model (MV) which
comprehensively learns the interactions among the dynamic features of the
passenger, driver, trip order, as well as context. Regarding the data imbalance
problem, we further design the Knowledge Distillation framework (KD) to
supplement the model's predictive power for smaller cities using the knowledge
from cities with denser data and also generate a simple model to support
efficient deployment. Finally, we conduct extensive experiments on real-world
datasets from several different cities, which demonstrates the superiority of
our solution.",arxiv
http://arxiv.org/abs/2111.02149v1,2021-11-03T11:37:11Z,2021-11-03T11:37:11Z,"Deployment Optimization for Shared e-Mobility Systems with Multi-agent
  Deep Neural Search","Shared e-mobility services have been widely tested and piloted in cities
across the globe, and already woven into the fabric of modern urban planning.
This paper studies a practical yet important problem in those systems: how to
deploy and manage their infrastructure across space and time, so that the
services are ubiquitous to the users while sustainable in profitability.
However, in real-world systems evaluating the performance of different
deployment strategies and then finding the optimal plan is prohibitively
expensive, as it is often infeasible to conduct many iterations of
trial-and-error. We tackle this by designing a high-fidelity simulation
environment, which abstracts the key operation details of the shared e-mobility
systems at fine-granularity, and is calibrated using data collected from the
real-world. This allows us to try out arbitrary deployment plans to learn the
optimal given specific context, before actually implementing any in the
real-world systems. In particular, we propose a novel multi-agent neural search
approach, in which we design a hierarchical controller to produce tentative
deployment plans. The generated deployment plans are then tested using a
multi-simulation paradigm, i.e., evaluated in parallel, where the results are
used to train the controller with deep reinforcement learning. With this closed
loop, the controller can be steered to have higher probability of generating
better deployment plans in future iterations. The proposed approach has been
evaluated extensively in our simulation environment, and experimental results
show that it outperforms baselines e.g., human knowledge, and state-of-the-art
heuristic-based optimization approaches in both service coverage and net
revenue.",arxiv
http://arxiv.org/abs/2003.06700v3,2020-05-14T21:05:24Z,2020-03-14T20:53:05Z,"CoCoPIE: Making Mobile AI Sweet As PIE --Compression-Compilation
  Co-Design Goes a Long Way","Assuming hardware is the major constraint for enabling real-time mobile
intelligence, the industry has mainly dedicated their efforts to developing
specialized hardware accelerators for machine learning and inference. This
article challenges the assumption. By drawing on a recent real-time AI
optimization framework CoCoPIE, it maintains that with effective
compression-compiler co-design, it is possible to enable real-time artificial
intelligence on mainstream end devices without special hardware. CoCoPIE is a
software framework that holds numerous records on mobile AI: the first
framework that supports all main kinds of DNNs, from CNNs to RNNs, transformer,
language models, and so on; the fastest DNN pruning and acceleration framework,
up to 180X faster compared with current DNN pruning on other frameworks such as
TensorFlow-Lite; making many representative AI applications able to run in
real-time on off-the-shelf mobile devices that have been previously regarded
possible only with special hardware support; making off-the-shelf mobile
devices outperform a number of representative ASIC and FPGA solutions in terms
of energy efficiency and/or performance.",arxiv
http://arxiv.org/abs/2009.01657v1,2020-08-27T20:53:26Z,2020-08-27T20:53:26Z,"A free web service for fast COVID-19 classification of chest X-Ray
  images","The coronavirus outbreak became a major concern for society worldwide.
Technological innovation and ingenuity are essential to fight COVID-19 pandemic
and bring us one step closer to overcome it. Researchers over the world are
working actively to find available alternatives in different fields, such as
the Healthcare System, pharmaceutic, health prevention, among others. With the
rise of artificial intelligence (AI) in the last 10 years, IA-based
applications have become the prevalent solution in different areas because of
its higher capability, being now adopted to help combat against COVID-19. This
work provides a fast detection system of COVID-19 characteristics in X-Ray
images based on deep learning (DL) techniques. This system is available as a
free web deployed service for fast patient classification, alleviating the high
demand for standards method for COVID-19 diagnosis. It is constituted of two
deep learning models, one to differentiate between X-Ray and non-X-Ray images
based on Mobile-Net architecture, and another one to identify chest X-Ray
images with characteristics of COVID-19 based on the DenseNet architecture. For
real-time inference, it is provided a pair of dedicated GPUs, which reduce the
computational time. The whole system can filter out non-chest X-Ray images, and
detect whether the X-Ray presents characteristics of COVID-19, highlighting the
most sensitive regions.",arxiv
http://arxiv.org/abs/1901.00858v1,2019-01-03T03:39:12Z,2019-01-03T03:39:12Z,"HG-Caffe: Mobile and Embedded Neural Network GPU (OpenCL) Inference
  Engine with FP16 Supporting","Breakthroughs in the fields of deep learning and mobile system-on-chips are
radically changing the way we use our smartphones. However, deep neural
networks inference is still a challenging task for edge AI devices due to the
computational overhead on mobile CPUs and a severe drain on the batteries. In
this paper, we present a deep neural network inference engine named HG-Caffe,
which supports GPUs with half precision. HG-Caffe provides up to 20 times
speedup with GPUs compared to the original implementations. In addition to the
speedup, the peak memory usage is also reduced to about 80%. With HG-Caffe,
more innovative and fascinating mobile applications will be turned into
reality.",arxiv
http://arxiv.org/abs/1611.02416v2,2019-02-27T09:24:09Z,2016-11-08T07:41:54Z,"An Efficient Approach to Boosting Performance of Deep Spiking Network
  Training","Nowadays deep learning is dominating the field of machine learning with
state-of-the-art performance in various application areas. Recently, spiking
neural networks (SNNs) have been attracting a great deal of attention, notably
owning to their power efficiency, which can potentially allow us to implement a
low-power deep learning engine suitable for real-time/mobile applications.
However, implementing SNN-based deep learning remains challenging, especially
gradient-based training of SNNs by error backpropagation. We cannot simply
propagate errors through SNNs in conventional way because of the property of
SNNs that process discrete data in the form of a series. Consequently, most of
the previous studies employ a workaround technique, which first trains a
conventional weighted-sum deep neural network and then maps the learning
weights to the SNN under training, instead of training SNN parameters directly.
In order to eliminate this workaround, recently proposed is a new class of SNN
named deep spiking networks (DSNs), which can be trained directly (without a
mapping from conventional deep networks) by error backpropagation with
stochastic gradient descent. In this paper, we show that the initialization of
the membrane potential on the backward path is an important step in DSN
training, through diverse experiments performed under various conditions.
Furthermore, we propose a simple and efficient method that can improve DSN
training by controlling the initial membrane potential on the backward path. In
our experiments, adopting the proposed approach allowed us to boost the
performance of DSN training in terms of converging time and accuracy.",arxiv
http://arxiv.org/abs/1901.02144v1,2019-01-08T03:32:31Z,2019-01-08T03:32:31Z,"Guidelines and Benchmarks for Deployment of Deep Learning Models on
  Smartphones as Real-Time Apps","Deep learning solutions are being increasingly used in mobile applications.
Although there are many open-source software tools for the development of deep
learning solutions, there are no guidelines in one place in a unified manner
for using these tools towards real-time deployment of these solutions on
smartphones. From the variety of available deep learning tools, the most suited
ones are used in this paper to enable real-time deployment of deep learning
inference networks on smartphones. A uniform flow of implementation is devised
for both Android and iOS smartphones. The advantage of using multi-threading to
achieve or improve real-time throughputs is also showcased. A benchmarking
framework consisting of accuracy, CPU/GPU consumption and real-time throughput
is considered for validation purposes. The developed deployment approach allows
deep learning models to be turned into real-time smartphone apps with ease
based on publicly available deep learning and smartphone software tools. This
approach is applied to six popular or representative convolutional neural
network models and the validation results based on the benchmarking metrics are
reported.",arxiv
http://arxiv.org/abs/2003.04544v3,2020-06-30T01:47:27Z,2020-03-10T05:52:15Z,"Joint Parameter-and-Bandwidth Allocation for Improving the Efficiency of
  Partitioned Edge Learning","To leverage data and computation capabilities of mobile devices, machine
learning algorithms are deployed at the network edge for training artificial
intelligence (AI) models, resulting in the new paradigm of edge learning. In
this paper, we consider the framework of partitioned edge learning for
iteratively training a large-scale model using many resource-constrained
devices (called workers). To this end, in each iteration, the model is
dynamically partitioned into parametric blocks, which are downloaded to worker
groups for updating using data subsets. Then, the local updates are uploaded to
and cascaded by the server for updating a global model. To reduce resource
usage by minimizing the total learning-and-communication latency, this work
focuses on the novel joint design of parameter (computation load) allocation
and bandwidth allocation (for downloading and uploading). Two design approaches
are adopted. First, a practical sequential approach, called partially
integrated parameter-and-bandwidth allocation (PABA), yields two schemes,
namely bandwidth aware parameter allocation and parameter aware bandwidth
allocation. The former minimizes the load for the slowest (in computing) of
worker groups, each training a same parametric block. The latter allocates the
largest bandwidth to the worker being the latency bottleneck. Second, PABA are
jointly optimized. Despite its being a nonconvex problem, an efficient and
optimal solution algorithm is derived by intelligently nesting a bisection
search and solving a convex problem. Experimental results using real data
demonstrate that integrating PABA can substantially improve the performance of
partitioned edge learning in terms of latency (by e.g., 46%) and accuracy (by
e.g., 4%).",arxiv
http://arxiv.org/abs/1901.06242v1,2018-12-01T13:40:03Z,2018-12-01T13:40:03Z,"Data-driven Air Quality Characterisation for Urban Environments: a Case
  Study","The economic and social impact of poor air quality in towns and cities is
increasingly being recognised, together with the need for effective ways of
creating awareness of real-time air quality levels and their impact on human
health. With local authority maintained monitoring stations being
geographically sparse and the resultant datasets also featuring missing labels,
computational data-driven mechanisms are needed to address the data sparsity
challenge. In this paper, we propose a machine learning-based method to
accurately predict the Air Quality Index (AQI), using environmental monitoring
data together with meteorological measurements. To do so, we develop an air
quality estimation framework that implements a neural network that is enhanced
with a novel Non-linear Autoregressive neural network with exogenous input
(NARX), especially designed for time series prediction. The framework is
applied to a case study featuring different monitoring sites in London, with
comparisons against other standard machine-learning based predictive algorithms
showing the feasibility and robust performance of the proposed method for
different kinds of areas within an urban region.",arxiv
http://arxiv.org/abs/1910.05316v1,2019-10-04T00:53:44Z,2019-10-04T00:53:44Z,"Edge AI: On-Demand Accelerating Deep Neural Network Inference via Edge
  Computing","As a key technology of enabling Artificial Intelligence (AI) applications in
5G era, Deep Neural Networks (DNNs) have quickly attracted widespread
attention. However, it is challenging to run computation-intensive DNN-based
tasks on mobile devices due to the limited computation resources. What's worse,
traditional cloud-assisted DNN inference is heavily hindered by the significant
wide-area network latency, leading to poor real-time performance as well as low
quality of user experience. To address these challenges, in this paper, we
propose Edgent, a framework that leverages edge computing for DNN collaborative
inference through device-edge synergy. Edgent exploits two design knobs: (1)
DNN partitioning that adaptively partitions computation between device and edge
for purpose of coordinating the powerful cloud resource and the proximal edge
resource for real-time DNN inference; (2) DNN right-sizing that further reduces
computing latency via early exiting inference at an appropriate intermediate
DNN layer. In addition, considering the potential network fluctuation in
real-world deployment, Edgentis properly design to specialize for both static
and dynamic network environment. Specifically, in a static environment where
the bandwidth changes slowly, Edgent derives the best configurations with the
assist of regression-based prediction models, while in a dynamic environment
where the bandwidth varies dramatically, Edgent generates the best execution
plan through the online change point detection algorithm that maps the current
bandwidth state to the optimal configuration. We implement Edgent prototype
based on the Raspberry Pi and the desktop PC and the extensive experimental
evaluations demonstrate Edgent's effectiveness in enabling on-demand
low-latency edge intelligence.",arxiv
http://arxiv.org/abs/2011.06144v1,2020-11-12T01:06:17Z,2020-11-12T01:06:17Z,I-POST: Intelligent Point of Sale and Transaction System,"We propose a novel solution for the cashier problem. Current cashier
system/Point of Sale (POS) terminals can be inefficient, cumbersome and
time-consuming for the users. There is a need for a solution dependent on
modern technology and ubiquitous computing resources. We present I-POST
(Intelligent Point of Sale and Transaction) as a software system that uses
smart devices, mobile phone and state of the art machine learning algorithms to
process the user transactions in automated and real time manner. I-POST is an
automated checkout system that allows the user to walk in a store, collect his
items and exit the store. There is no need to stand and wait in a queue. The
system uses object detection and facial recognition algorithm to process the
authentication of the client and the state of the object. At point of exit, the
classifier sends the data to the backend server which execute the payments. The
system uses Convolution Neural Network (CNN) for the image recognition and
processing. CNN is a supervised learning model that has found major application
in pattern recognition problem. The current implementation uses two classifiers
that work intrinsically to authenticate the user and track the items. The model
accuracy for object recognition is 97%, the loss is 9.3%. We expect that such
systems can bring efficiency to the market and has the potential for broad and
diverse applications.",arxiv
http://arxiv.org/abs/2007.07122v2,2020-07-15T14:03:29Z,2020-07-14T15:42:55Z,"Energy-Efficient Resource Management for Federated Edge Learning with
  CPU-GPU Heterogeneous Computing","Edge machine learning involves the deployment of learning algorithms at the
network edge to leverage massive distributed data and computation resources to
train artificial intelligence (AI) models. Among others, the framework of
federated edge learning (FEEL) is popular for its data-privacy preservation.
FEEL coordinates global model training at an edge server and local model
training at edge devices that are connected by wireless links. This work
contributes to the energy-efficient implementation of FEEL in wireless networks
by designing joint computation-and-communication resource management
($\text{C}^2$RM). The design targets the state-of-the-art heterogeneous mobile
architecture where parallel computing using both a CPU and a GPU, called
heterogeneous computing, can significantly improve both the performance and
energy efficiency. To minimize the sum energy consumption of devices, we
propose a novel $\text{C}^2$RM framework featuring multi-dimensional control
including bandwidth allocation, CPU-GPU workload partitioning and speed scaling
at each device, and $\text{C}^2$ time division for each link. The key component
of the framework is a set of equilibriums in energy rates with respect to
different control variables that are proved to exist among devices or between
processing units at each device. The results are applied to designing efficient
algorithms for computing the optimal $\text{C}^2$RM policies faster than the
standard optimization tools. Based on the equilibriums, we further design
energy-efficient schemes for device scheduling and greedy spectrum sharing that
scavenges ""spectrum holes"" resulting from heterogeneous $\text{C}^2$ time
divisions among devices. Using a real dataset, experiments are conducted to
demonstrate the effectiveness of $\text{C}^2$RM on improving the energy
efficiency of a FEEL system.",arxiv
http://arxiv.org/abs/1911.13178v1,2019-11-29T16:23:41Z,2019-11-29T16:23:41Z,"Short Term Prediction of Parking Area states Using Real Time Data and
  Machine Learning Techniques","Public road authorities and private mobility service providers need
information derived from the current and predicted traffic states to act upon
the daily urban system and its spatial and temporal dynamics. In this research,
a real-time parking area state (occupancy, in- and outflux) prediction model
(up to 60 minutes ahead) has been developed using publicly available historic
and real time data sources. Based on a case study in a real-life scenario in
the city of Arnhem, a Neural Network-based approach outperforms a Random
Forest-based one on all assessed performance measures, although the differences
are small. Both are outperforming a naive seasonal random walk model. Although
the performance degrades with increasing prediction horizon, the model shows a
performance gain of over 150% at a prediction horizon of 60 minutes compared
with the naive model. Furthermore, it is shown that predicting the in- and
outflux is a far more difficult task (i.e. performance gains of 30%) which
needs more training data, not based exclusively on occupancy rate. However, the
performance of predicting in- and outflux is less sensitive to the prediction
horizon. In addition, it is shown that real-time information of current
occupancy rate is the independent variable with the highest contribution to the
performance, although time, traffic flow and weather variables also deliver a
significant contribution. During real-time deployment, the model performs three
times better than the naive model on average. As a result, it can provide
valuable information for proactive traffic management as well as mobility
service providers.",arxiv
http://arxiv.org/abs/2007.00263v1,2020-07-01T06:19:12Z,2020-07-01T06:19:12Z,"Mobile Botnet Detection: A Deep Learning Approach Using Convolutional
  Neural Networks","Android, being the most widespread mobile operating systems is increasingly
becoming a target for malware. Malicious apps designed to turn mobile devices
into bots that may form part of a larger botnet have become quite common, thus
posing a serious threat. This calls for more effective methods to detect
botnets on the Android platform. Hence, in this paper, we present a deep
learning approach for Android botnet detection based on Convolutional Neural
Networks (CNN). Our proposed botnet detection system is implemented as a
CNN-based model that is trained on 342 static app features to distinguish
between botnet apps and normal apps. The trained botnet detection model was
evaluated on a set of 6,802 real applications containing 1,929 botnets from the
publicly available ISCX botnet dataset. The results show that our CNN-based
approach had the highest overall prediction accuracy compared to other popular
machine learning classifiers. Furthermore, the performance results observed
from our model were better than those reported in previous studies on machine
learning based Android botnet detection.",arxiv
http://arxiv.org/abs/2005.06342v1,2020-05-09T05:54:28Z,2020-05-09T05:54:28Z,"sCrop: A Internet-of-Agro-Things (IoAT) Enabled Solar Powered Smart
  Device for Automatic Plant Disease Prediction","Internet-of-Things (IoT) is omnipresent, ranging from home solutions to
turning wheels for the fourth industrial revolution. This article presents the
novel concept of Internet-of-Agro-Things (IoAT) with an example of automated
plant disease prediction. It consists of solar enabled sensor nodes which help
in continuous sensing and automating agriculture. The existing solutions have
implemented a battery powered sensor node. On the contrary, the proposed system
has adopted the use of an energy efficient way of powering using solar energy.
It is observed that around 80% of the crops are attacked with microbial
diseases in traditional agriculture. To prevent this, a health maintenance
system is integrated with the sensor node, which captures the image of the crop
and performs an analysis with the trained Convolutional Neural Network (CNN)
model. The deployment of the proposed system is demonstrated in a real-time
environment using a microcontroller, solar sensor nodes with a camera module,
and an mobile application for the farmers visualization of the farms. The
deployed prototype was deployed for two months and has achieved a robust
performance by sustaining in varied weather conditions and continued to remain
rust-free. The proposed deep learning framework for plant disease prediction
has achieved an accuracy of 99.2% testing accuracy.",arxiv
http://arxiv.org/abs/2111.10810v1,2021-11-21T12:53:50Z,2021-11-21T12:53:50Z,"Vulcan: Solving the Steiner Tree Problem with Graph Neural Networks and
  Deep Reinforcement Learning","Steiner Tree Problem (STP) in graphs aims to find a tree of minimum weight in
the graph that connects a given set of vertices. It is a classic NP-hard
combinatorial optimization problem and has many real-world applications (e.g.,
VLSI chip design, transportation network planning and wireless sensor
networks). Many exact and approximate algorithms have been developed for STP,
but they suffer from high computational complexity and weak worst-case solution
guarantees, respectively. Heuristic algorithms are also developed. However,
each of them requires application domain knowledge to design and is only
suitable for specific scenarios. Motivated by the recently reported observation
that instances of the same NP-hard combinatorial problem may maintain the same
or similar combinatorial structure but mainly differ in their data, we
investigate the feasibility and benefits of applying machine learning
techniques to solving STP. To this end, we design a novel model Vulcan based on
novel graph neural networks and deep reinforcement learning. The core of Vulcan
is a novel, compact graph embedding that transforms highdimensional graph
structure data (i.e., path-changed information) into a low-dimensional vector
representation. Given an STP instance, Vulcan uses this embedding to encode its
pathrelated information and sends the encoded graph to a deep reinforcement
learning component based on a double deep Q network (DDQN) to find solutions.
In addition to STP, Vulcan can also find solutions to a wide range of NP-hard
problems (e.g., SAT, MVC and X3C) by reducing them to STP. We implement a
prototype of Vulcan and demonstrate its efficacy and efficiency with extensive
experiments using real-world and synthetic datasets.",arxiv
http://arxiv.org/abs/1807.04587v2,2018-11-20T14:26:44Z,2018-07-12T12:53:50Z,"Assessing the Scalability of Biologically-Motivated Deep Learning
  Algorithms and Architectures","The backpropagation of error algorithm (BP) is impossible to implement in a
real brain. The recent success of deep networks in machine learning and AI,
however, has inspired proposals for understanding how the brain might learn
across multiple layers, and hence how it might approximate BP. As of yet, none
of these proposals have been rigorously evaluated on tasks where BP-guided deep
learning has proved critical, or in architectures more structured than simple
fully-connected networks. Here we present results on scaling up biologically
motivated models of deep learning on datasets which need deep networks with
appropriate architectures to achieve good performance. We present results on
the MNIST, CIFAR-10, and ImageNet datasets and explore variants of
target-propagation (TP) and feedback alignment (FA) algorithms, and explore
performance in both fully- and locally-connected architectures. We also
introduce weight-transport-free variants of difference target propagation (DTP)
modified to remove backpropagation from the penultimate layer. Many of these
algorithms perform well for MNIST, but for CIFAR and ImageNet we find that TP
and FA variants perform significantly worse than BP, especially for networks
composed of locally connected units, opening questions about whether new
architectures and algorithms are required to scale these approaches. Our
results and implementation details help establish baselines for biologically
motivated deep learning schemes going forward.",arxiv
http://arxiv.org/abs/2005.02544v1,2020-05-06T00:30:29Z,2020-05-06T00:30:29Z,"AutoScale: Optimizing Energy Efficiency of End-to-End Edge Inference
  under Stochastic Variance","Deep learning inference is increasingly run at the edge. As the programming
and system stack support becomes mature, it enables acceleration opportunities
within a mobile system, where the system performance envelope is scaled up with
a plethora of programmable co-processors. Thus, intelligent services designed
for mobile users can choose between running inference on the CPU or any of the
co-processors on the mobile system, or exploiting connected systems, such as
the cloud or a nearby, locally connected system. By doing so, the services can
scale out the performance and increase the energy efficiency of edge mobile
systems. This gives rise to a new challenge - deciding when inference should
run where. Such execution scaling decision becomes more complicated with the
stochastic nature of mobile-cloud execution, where signal strength variations
of the wireless networks and resource interference can significantly affect
real-time inference performance and system energy efficiency. To enable
accurate, energy-efficient deep learning inference at the edge, this paper
proposes AutoScale. AutoScale is an adaptive and light-weight execution scaling
engine built upon the custom-designed reinforcement learning algorithm. It
continuously learns and selects the most energy-efficient inference execution
target by taking into account characteristics of neural networks and available
systems in the collaborative cloud-edge execution environment while adapting to
the stochastic runtime variance. Real system implementation and evaluation,
considering realistic execution scenarios, demonstrate an average of 9.8 and
1.6 times energy efficiency improvement for DNN edge inference over the
baseline mobile CPU and cloud offloading, while meeting the real-time
performance and accuracy requirement.",arxiv
http://arxiv.org/abs/1909.10614v1,2019-09-23T20:55:18Z,2019-09-23T20:55:18Z,"Acceptable Planning: Influencing Individual Behavior to Reduce
  Transportation Energy Expenditure of a City","Our research aims at developing intelligent systems to reduce the
transportation-related energy expenditure of a large city by influencing
individual behavior. We introduce COPTER - an intelligent travel assistant that
evaluates multi-modal travel alternatives to find a plan that is acceptable to
a person given their context and preferences. We propose a formulation for
acceptable planning that brings together ideas from AI, machine learning, and
economics. This formulation has been incorporated in COPTER that produces
acceptable plans in real-time. We adopt a novel empirical evaluation framework
that combines human decision data with a high fidelity multi-modal
transportation simulation to demonstrate a 4\% energy reduction and 20\% delay
reduction in a realistic deployment scenario in Los Angeles, California, USA.",arxiv
http://arxiv.org/abs/2102.10323v1,2021-02-20T12:17:20Z,2021-02-20T12:17:20Z,"Unavailable Transit Feed Specification: Making it Available with
  Recurrent Neural Networks","Studies on public transportation in Europe suggest that European inhabitants
use buses in ca. 56% of all public transport travels. One of the critical
factors affecting such a percentage and more, in general, the demand for public
transport services, with an increasing reluctance to use them, is their
quality. End-users can perceive quality from various perspectives, including
the availability of information, i.e., the access to details about the transit
and the provided services. The approach proposed in this paper, using
innovative methodologies resorting on data mining and machine learning
techniques, aims to make available the unavailable data about public transport.
In particular, by mining GPS traces, we manage to reconstruct the complete
transit graph of public transport. The approach has been successfully validated
on a real dataset collected from the local bus system of the city of L'Aquila
(Italy). The experimental results demonstrate that the proposed approach and
implemented framework are both effective and efficient, thus being ready for
deployment.",arxiv
http://arxiv.org/abs/1903.09254v4,2019-04-05T22:52:17Z,2019-03-21T22:03:25Z,"CityFlow: A City-Scale Benchmark for Multi-Target Multi-Camera Vehicle
  Tracking and Re-Identification","Urban traffic optimization using traffic cameras as sensors is driving the
need to advance state-of-the-art multi-target multi-camera (MTMC) tracking.
This work introduces CityFlow, a city-scale traffic camera dataset consisting
of more than 3 hours of synchronized HD videos from 40 cameras across 10
intersections, with the longest distance between two simultaneous cameras being
2.5 km. To the best of our knowledge, CityFlow is the largest-scale dataset in
terms of spatial coverage and the number of cameras/videos in an urban
environment. The dataset contains more than 200K annotated bounding boxes
covering a wide range of scenes, viewing angles, vehicle models, and urban
traffic flow conditions. Camera geometry and calibration information are
provided to aid spatio-temporal analysis. In addition, a subset of the
benchmark is made available for the task of image-based vehicle
re-identification (ReID). We conducted an extensive experimental evaluation of
baselines/state-of-the-art approaches in MTMC tracking, multi-target
single-camera (MTSC) tracking, object detection, and image-based ReID on this
dataset, analyzing the impact of different network architectures, loss
functions, spatio-temporal models and their combinations on task effectiveness.
An evaluation server is launched with the release of our benchmark at the 2019
AI City Challenge (https://www.aicitychallenge.org/) that allows researchers to
compare the performance of their newest techniques. We expect this dataset to
catalyze research in this field, propel the state-of-the-art forward, and lead
to deployed traffic optimization(s) in the real world.",arxiv
http://arxiv.org/abs/2002.11045v1,2020-02-22T14:38:11Z,2020-02-22T14:38:11Z,"Deep Learning for Ultra-Reliable and Low-Latency Communications in 6G
  Networks","In the future 6th generation networks, ultra-reliable and low-latency
communications (URLLC) will lay the foundation for emerging mission-critical
applications that have stringent requirements on end-to-end delay and
reliability. Existing works on URLLC are mainly based on theoretical models and
assumptions. The model-based solutions provide useful insights, but cannot be
directly implemented in practice. In this article, we first summarize how to
apply data-driven supervised deep learning and deep reinforcement learning in
URLLC, and discuss some open problems of these methods. To address these open
problems, we develop a multi-level architecture that enables device
intelligence, edge intelligence, and cloud intelligence for URLLC. The basic
idea is to merge theoretical models and real-world data in analyzing the
latency and reliability and training deep neural networks (DNNs). Deep transfer
learning is adopted in the architecture to fine-tune the pre-trained DNNs in
non-stationary networks. Further considering that the computing capacity at
each user and each mobile edge computing server is limited, federated learning
is applied to improve the learning efficiency. Finally, we provide some
experimental and simulation results and discuss some future directions.",arxiv
http://arxiv.org/abs/1905.07681v3,2020-12-11T14:26:53Z,2019-05-16T10:45:18Z,Spatial Positioning Token (SPToken) for Smart Mobility,"We introduce a permissioned distributed ledger technology (DLT) design for
crowdsourced smart mobility applications. This architecture is based on a
directed acyclic graph architecture (similar to the IOTA tangle) and uses both
Proof-of-Work and Proof-of-Position mechanisms to provide protection against
spam attacks and malevolent actors. In addition to enabling individuals to
retain ownership of their data and to monetize it, the architecture also is
suitable for distributed privacy-preserving machine learning algorithms, is
lightweight, and can be implemented in simple internet-of-things (IoT) devices.
To demonstrate its efficacy, we apply this framework to reinforcement learning
settings where a third party is interested in acquiring information from
agents. In particular, one may be interested in sampling an unknown vehicular
traffic flow in a city, using a DLT-type architecture and without perturbing
the density, with the idea of realizing a set of virtual tokens as surrogates
of real vehicles to explore geographical areas of interest. These tokens, whose
authenticated position determines write access to the ledger, are thus used to
emulate the probing actions of commanded (real) vehicles on a given planned
route by ""jumping"" from a passing-by vehicle to another to complete the planned
trajectory. Consequently, the environment stays unaffected (i.e., the autonomy
of participating vehicles is not influenced by the algorithm), regardless of
the number of emitted tokens. The design of such a DLT architecture is
presented, and numerical results from large-scale simulations are provided to
validate the proposed approach.",arxiv
http://arxiv.org/abs/2111.00340v1,2021-10-30T21:10:56Z,2021-10-30T21:10:56Z,"Identifying and mitigating bias in algorithms used to manage patients in
  a pandemic","Numerous COVID-19 clinical decision support systems have been developed.
However many of these systems do not have the merit for validity due to
methodological shortcomings including algorithmic bias. Methods Logistic
regression models were created to predict COVID-19 mortality, ventilator status
and inpatient status using a real-world dataset consisting of four hospitals in
New York City and analyzed for biases against race, gender and age. Simple
thresholding adjustments were applied in the training process to establish more
equitable models. Results Compared to the naively trained models, the
calibrated models showed a 57% decrease in the number of biased trials, while
predictive performance, measured by area under the receiver/operating curve
(AUC), remained unchanged. After calibration, the average sensitivity of the
predictive models increased from 0.527 to 0.955. Conclusion We demonstrate that
naively training and deploying machine learning models on real world data for
predictive analytics of COVID-19 has a high risk of bias. Simple implemented
adjustments or calibrations during model training can lead to substantial and
sustained gains in fairness on subsequent deployment.",arxiv
http://arxiv.org/abs/1712.08083v1,2017-12-21T16:56:27Z,2017-12-21T16:56:27Z,"Long-Term Mobile Traffic Forecasting Using Deep Spatio-Temporal Neural
  Networks","Forecasting with high accuracy the volume of data traffic that mobile users
will consume is becoming increasingly important for precision traffic
engineering, demand-aware network resource allocation, as well as public
transportation. Measurements collection in dense urban deployments is however
complex and expensive, and the post-processing required to make predictions is
highly non-trivial, given the intricate spatio-temporal variability of mobile
traffic due to user mobility. To overcome these challenges, in this paper we
harness the exceptional feature extraction abilities of deep learning and
propose a Spatio-Temporal neural Network (STN) architecture purposely designed
for precise network-wide mobile traffic forecasting. We present a mechanism
that fine tunes the STN and enables its operation with only limited ground
truth observations. We then introduce a Double STN technique (D-STN), which
uniquely combines the STN predictions with historical statistics, thereby
making faithful long-term mobile traffic projections. Experiments we conduct
with real-world mobile traffic data sets, collected over 60 days in both urban
and rural areas, demonstrate that the proposed (D-)STN schemes perform up to
10-hour long predictions with remarkable accuracy, irrespective of the time of
day when they are triggered. Specifically, our solutions achieve up to 61%
smaller prediction errors as compared to widely used forecasting approaches,
while operating with up to 600 times shorter measurement intervals.",arxiv
http://arxiv.org/abs/2107.00401v1,2021-07-01T12:20:48Z,2021-07-01T12:20:48Z,"CarSNN: An Efficient Spiking Neural Network for Event-Based Autonomous
  Cars on the Loihi Neuromorphic Research Processor","Autonomous Driving (AD) related features provide new forms of mobility that
are also beneficial for other kind of intelligent and autonomous systems like
robots, smart transportation, and smart industries. For these applications, the
decisions need to be made fast and in real-time. Moreover, in the quest for
electric mobility, this task must follow low power policy, without affecting
much the autonomy of the mean of transport or the robot. These two challenges
can be tackled using the emerging Spiking Neural Networks (SNNs). When deployed
on a specialized neuromorphic hardware, SNNs can achieve high performance with
low latency and low power consumption. In this paper, we use an SNN connected
to an event-based camera for facing one of the key problems for AD, i.e., the
classification between cars and other objects. To consume less power than
traditional frame-based cameras, we use a Dynamic Vision Sensor (DVS). The
experiments are made following an offline supervised learning rule, followed by
mapping the learnt SNN model on the Intel Loihi Neuromorphic Research Chip. Our
best experiment achieves an accuracy on offline implementation of 86%, that
drops to 83% when it is ported onto the Loihi Chip. The Neuromorphic Hardware
implementation has maximum 0.72 ms of latency for every sample, and consumes
only 310 mW. To the best of our knowledge, this work is the first
implementation of an event-based car classifier on a Neuromorphic Chip.",arxiv
http://arxiv.org/abs/1807.08775v1,2018-07-23T18:19:57Z,2018-07-23T18:19:57Z,CNN-based Facial Affect Analysis on Mobile Devices,"This paper focuses on the design, deployment and evaluation of Convolutional
Neural Network (CNN) architectures for facial affect analysis on mobile
devices. Unlike traditional CNN approaches, models deployed to mobile devices
must minimise storage requirements while retaining high performance. We
therefore propose three variants of established CNN architectures and
comparatively evaluate them on a large, in-the-wild benchmark dataset of facial
images. Our results show that the proposed architectures retain similar
performance to the dataset baseline while minimising storage requirements:
achieving 58% accuracy for eight-class emotion classification and average RMSE
of 0.39 for valence/arousal prediction. To demonstrate the feasibility of
deploying these models for real-world applications, we implement a music
recommendation interface based on predicted user affect. Although the CNN
models were not trained in the context of music recommendation, our case study
shows that: (i) the trained models achieve similar prediction performance to
the benchmark dataset, and (ii) users tend to positively rate the song
recommendations provided by the interface. Average runtime of the deployed
models on an iPhone 6S equates to ~45 fps, suggesting that the proposed
architectures are also well suited for real-time deployment on video streams.",arxiv
http://arxiv.org/abs/2010.09635v1,2020-10-19T16:20:45Z,2020-10-19T16:20:45Z,"Deep Reinforcement Learning with Population-Coded Spiking Neural Network
  for Continuous Control","The energy-efficient control of mobile robots is crucial as the complexity of
their real-world applications increasingly involves high-dimensional
observation and action spaces, which cannot be offset by limited on-board
resources. An emerging non-Von Neumann model of intelligence, where spiking
neural networks (SNNs) are run on neuromorphic processors, is regarded as an
energy-efficient and robust alternative to the state-of-the-art real-time
robotic controllers for low dimensional control tasks. The challenge now for
this new computing paradigm is to scale so that it can keep up with real-world
tasks. To do so, SNNs need to overcome the inherent limitations of their
training, namely the limited ability of their spiking neurons to represent
information and the lack of effective learning algorithms. Here, we propose a
population-coded spiking actor network (PopSAN) trained in conjunction with a
deep critic network using deep reinforcement learning (DRL). The population
coding scheme dramatically increased the representation capacity of the network
and the hybrid learning combined the training advantages of deep networks with
the energy-efficient inference of spiking networks. To show the general
applicability of our approach, we integrated it with a spectrum of both
on-policy and off-policy DRL algorithms. We deployed the trained PopSAN on
Intel's Loihi neuromorphic chip and benchmarked our method against the
mainstream DRL algorithms for continuous control. To allow for a fair
comparison among all methods, we validated them on OpenAI gym tasks. Our
Loihi-run PopSAN consumed 140 times less energy per inference when compared
against the deep actor network on Jetson TX2, and had the same level of
performance. Our results support the efficiency of neuromorphic controllers and
suggest our hybrid RL as an alternative to deep learning, when both
energy-efficiency and robustness are important.",arxiv
http://arxiv.org/abs/2009.09926v1,2020-09-17T02:36:52Z,2020-09-17T02:36:52Z,"Cross-Modal Alignment with Mixture Experts Neural Network for
  Intral-City Retail Recommendation","In this paper, we introduce Cross-modal Alignment with mixture experts Neural
Network (CameNN) recommendation model for intral-city retail industry, which
aims to provide fresh foods and groceries retailing within 5 hours delivery
service arising for the outbreak of Coronavirus disease (COVID-19) pandemic
around the world. We propose CameNN, which is a multi-task model with three
tasks including Image to Text Alignment (ITA) task, Text to Image Alignment
(TIA) task and CVR prediction task. We use pre-trained BERT to generate the
text embedding and pre-trained InceptionV4 to generate image patch embedding
(each image is split into small patches with the same pixels and treat each
patch as an image token). Softmax gating networks follow to learn the weight of
each transformer expert output and choose only a subset of experts conditioned
on the input. Then transformer encoder is applied as the share-bottom layer to
learn all input features' shared interaction. Next, mixture of transformer
experts (MoE) layer is implemented to model different aspects of tasks. At top
of the MoE layer, we deploy a transformer layer for each task as task tower to
learn task-specific information. On the real word intra-city dataset,
experiments demonstrate CameNN outperform baselines and achieve significant
improvements on the image and text representation. In practice, we applied
CameNN on CVR prediction in our intra-city recommender system which is one of
the leading intra-city platforms operated in China.",arxiv
http://arxiv.org/abs/2110.10887v1,2021-10-21T04:17:35Z,2021-10-21T04:17:35Z,"A Real-Time Energy and Cost Efficient Vehicle Route Assignment Neural
  Recommender System","This paper presents a neural network recommender system algorithm for
assigning vehicles to routes based on energy and cost criteria. In this work,
we applied this new approach to efficiently identify the most cost-effective
medium and heavy duty truck (MDHDT) powertrain technology, from a total cost of
ownership (TCO) perspective, for given trips. We employ a machine learning
based approach to efficiently estimate the energy consumption of various
candidate vehicles over given routes, defined as sequences of links (road
segments), with little information known about internal dynamics, i.e using
high level macroscopic route information. A complete recommendation logic is
then developed to allow for real-time optimum assignment for each route,
subject to the operational constraints of the fleet. We show how this framework
can be used to (1) efficiently provide a single trip recommendation with a
top-$k$ vehicles star ranking system, and (2) engage in more general assignment
problems where $n$ vehicles need to be deployed over $m \leq n$ trips. This new
assignment system has been deployed and integrated into the POLARIS
Transportation System Simulation Tool for use in research conducted by the
Department of Energy's Systems and Modeling for Accelerated Research in
Transportation (SMART) Mobility Consortium",arxiv
http://arxiv.org/abs/1807.03043v5,2019-02-25T21:06:08Z,2018-07-09T11:12:16Z,Convolutional Recurrent Neural Networks for Glucose Prediction,"Control of blood glucose is essential for diabetes management. Current
digital therapeutic approaches for subjects with Type 1 diabetes mellitus
(T1DM) such as the artificial pancreas and insulin bolus calculators leverage
machine learning techniques for predicting subcutaneous glucose for improved
control. Deep learning has recently been applied in healthcare and medical
research to achieve state-of-the-art results in a range of tasks including
disease diagnosis, and patient state prediction among others. In this work, we
present a deep learning model that is capable of forecasting glucose levels
with leading accuracy for simulated patient cases (RMSE = 9.38$\pm$0.71 [mg/dL]
over a 30-minute horizon, RMSE = 18.87$\pm$2.25 [mg/dL] over a 60-minute
horizon) and real patient cases (RMSE = 21.07$\pm$2.35 [mg/dL] for 30-minute,
RMSE = 33.27$\pm$4.79\% for 60-minute). In addition, the model provides
competitive performance in providing effective prediction horizon ($PH_{eff}$)
with minimal time lag both in a simulated patient dataset ($PH_{eff}$ =
29.0$\pm$0.7 for 30-min and $PH_{eff}$ = 49.8$\pm$2.9 for 60-min) and in a real
patient dataset ($PH_{eff}$ = 19.3$\pm$3.1 for 30-min and $PH_{eff}$ =
29.3$\pm$9.4 for 60-min). This approach is evaluated on a dataset of 10
simulated cases generated from the UVa/Padova simulator and a clinical dataset
of 10 real cases each containing glucose readings, insulin bolus, and meal
(carbohydrate) data. Performance of the recurrent convolutional neural network
is benchmarked against four algorithms. The proposed algorithm is implemented
on an Android mobile phone, with an execution time of $6$ms on a phone compared
to an execution time of $780$ms on a laptop.",arxiv
http://arxiv.org/abs/1905.11669v1,2019-05-28T08:24:58Z,2019-05-28T08:24:58Z,"CompactNet: Platform-Aware Automatic Optimization for Convolutional
  Neural Networks","Convolutional Neural Network (CNN) based Deep Learning (DL) has achieved
great progress in many real-life applications. Meanwhile, due to the complex
model structures against strict latency and memory restriction, the
implementation of CNN models on the resource-limited platforms is becoming more
challenging. This work proposes a solution, called CompactNet\footnote{Project
URL: \url{https://github.com/CompactNet/CompactNet}}, which automatically
optimizes a pre-trained CNN model on a specific resource-limited platform given
a specific target of inference speedup. Guided by a simulator of the target
platform, CompactNet progressively trims a pre-trained network by removing
certain redundant filters until the target speedup is reached and generates an
optimal platform-specific model while maintaining the accuracy. We evaluate our
work on two platforms of a mobile ARM CPU and a machine learning accelerator
NPU (Cambricon-1A ISA) on a Huawei Mate10 smartphone. For the state-of-the-art
slim CNN model made for the embedded platform, MobileNetV2, CompactNet achieves
up to a 1.8x kernel computation speedup with equal or even higher accuracy for
image classification tasks on the Cifar-10 dataset.",arxiv
http://arxiv.org/abs/2006.09892v1,2020-06-08T09:47:55Z,2020-06-08T09:47:55Z,"STAD: Spatio-Temporal Adjustment of Traffic-Oblivious Travel-Time
  Estimation","Travel time estimation is an important component in modern transportation
applications. The state of the art techniques for travel time estimation use
GPS traces to learn the weights of a road network, often modeled as a directed
graph, then apply Dijkstra-like algorithms to find shortest paths. Travel time
is then computed as the sum of edge weights on the returned path. In order to
enable time-dependency, existing systems compute multiple weighted graphs
corresponding to different time windows. These graphs are often optimized
offline before they are deployed into production routing engines, causing a
serious engineering overhead. In this paper, we present STAD, a system that
adjusts - on the fly - travel time estimates for any trip request expressed in
the form of origin, destination, and departure time. STAD uses machine learning
and sparse trips data to learn the imperfections of any basic routing engine,
before it turns it into a full-fledged time-dependent system capable of
adjusting travel times to real traffic conditions in a city. STAD leverages the
spatio-temporal properties of traffic by combining spatial features such as
departing and destination geographic zones with temporal features such as
departing time and day to significantly improve the travel time estimates of
the basic routing engine. Experiments on real trip datasets from Doha, New York
City, and Porto show a reduction in median absolute errors of 14% in the first
two cities and 29% in the latter. We also show that STAD performs better than
different commercial and research baselines in all three cities.",arxiv
http://arxiv.org/abs/1805.00361v1,2018-04-30T17:36:14Z,2018-04-30T17:36:14Z,"Ultra Power-Efficient CNN Domain Specific Accelerator with 9.3TOPS/Watt
  for Mobile and Embedded Applications","Computer vision performances have been significantly improved in recent years
by Convolutional Neural Networks(CNN). Currently, applications using CNN
algorithms are deployed mainly on general purpose hardwares, such as CPUs, GPUs
or FPGAs. However, power consumption, speed, accuracy, memory footprint, and
die size should all be taken into consideration for mobile and embedded
applications. Domain Specific Architecture (DSA) for CNN is the efficient and
practical solution for CNN deployment and implementation. We designed and
produced a 28nm Two-Dimensional CNN-DSA accelerator with an ultra
power-efficient performance of 9.3TOPS/Watt and with all processing done in the
internal memory instead of outside DRAM. It classifies 224x224 RGB image inputs
at more than 140fps with peak power consumption at less than 300mW and an
accuracy comparable to the VGG benchmark. The CNN-DSA accelerator is
reconfigurable to support CNN model coefficients of various layer sizes and
layer types, including convolution, depth-wise convolution, short-cut
connections, max pooling, and ReLU. Furthermore, in order to better support
real-world deployment for various application scenarios, especially with
low-end mobile and embedded platforms and MCUs (Microcontroller Units), we also
designed algorithms to fully utilize the CNN-DSA accelerator efficiently by
reducing the dependency on external accelerator computation resources,
including implementation of Fully-Connected (FC) layers within the accelerator
and compression of extracted features from the CNN-DSA accelerator. Live demos
with our CNN-DSA accelerator on mobile and embedded systems show its
capabilities to be widely and practically applied in the real world.",arxiv
http://arxiv.org/abs/1812.03639v2,2019-02-14T01:31:38Z,2018-12-10T05:59:29Z,"Crossfire Attack Detection using Deep Learning in Software Defined ITS
  Networks","Recent developments in intelligent transport systems (ITS) based on smart
mobility significantly improves safety and security over roads and highways.
ITS networks are comprised of the Internet-connected vehicles (mobile nodes),
roadside units (RSU), cellular base stations and conventional core network
routers to create a complete data transmission platform that provides real-time
traffic information and enable prediction of future traffic conditions.
However, the heterogeneity and complexity of the underlying ITS networks raise
new challenges in intrusion prevention of mobile network nodes and detection of
security attacks due to such highly vulnerable mobile nodes. In this paper, we
consider a new type of security attack referred to as crossfire attack, which
involves a large number of compromised nodes that generate low-intensity
traffic in a temporally coordinated fashion such that target links or hosts
(victims) are disconnected from the rest of the network. Detection of such
attacks is challenging since the attacking traffic flows are indistinguishable
from the legitimate flows. With the support of software-defined networking that
enables dynamic network monitoring and traffic characteristic extraction, we
develop a machine learning model that can learn the temporal correlation among
traffic flows traversing in the ITS network, thus differentiating legitimate
flows from coordinated attacking flows. We use different deep learning
algorithms to train the model and study the performance using Mininet-WiFi
emulation platform. The results show that our approach achieves a detection
accuracy of at least 80%.",arxiv
http://arxiv.org/abs/1809.03428v3,2019-01-05T11:21:17Z,2018-09-10T16:09:58Z,"Not Just Privacy: Improving Performance of Private Deep Learning in
  Mobile Cloud","The increasing demand for on-device deep learning services calls for a highly
efficient manner to deploy deep neural networks (DNNs) on mobile devices with
limited capacity. The cloud-based solution is a promising approach to enabling
deep learning applications on mobile devices where the large portions of a DNN
are offloaded to the cloud. However, revealing data to the cloud leads to
potential privacy risk. To benefit from the cloud data center without the
privacy risk, we design, evaluate, and implement a cloud-based framework ARDEN
which partitions the DNN across mobile devices and cloud data centers. A simple
data transformation is performed on the mobile device, while the
resource-hungry training and the complex inference rely on the cloud data
center. To protect the sensitive information, a lightweight privacy-preserving
mechanism consisting of arbitrary data nullification and random noise addition
is introduced, which provides strong privacy guarantee. A rigorous privacy
budget analysis is given. Nonetheless, the private perturbation to the original
data inevitably has a negative impact on the performance of further inference
on the cloud side. To mitigate this influence, we propose a noisy training
method to enhance the cloud-side network robustness to perturbed data. Through
the sophisticated design, ARDEN can not only preserve privacy but also improve
the inference performance. To validate the proposed ARDEN, a series of
experiments based on three image datasets and a real mobile application are
conducted. The experimental results demonstrate the effectiveness of ARDEN.
Finally, we implement ARDEN on a demo system to verify its practicality.",arxiv
http://arxiv.org/abs/2002.02897v2,2020-02-15T14:34:48Z,2020-02-07T16:55:21Z,"MDLdroid: a ChainSGD-reduce Approach to Mobile Deep Learning for
  Personal Mobile Sensing","Personal mobile sensing is fast permeating our daily lives to enable activity
monitoring, healthcare and rehabilitation. Combined with deep learning, these
applications have achieved significant success in recent years. Different from
conventional cloud-based paradigms, running deep learning on devices offers
several advantages including data privacy preservation and low-latency response
for both model inference and update. Since data collection is costly in
reality, Google's Federated Learning offers not only complete data privacy but
also better model robustness based on multiple user data. However, personal
mobile sensing applications are mostly user-specific and highly affected by
environment. As a result, continuous local changes may seriously affect the
performance of a global model generated by Federated Learning. In addition,
deploying Federated Learning on a local server, e.g., edge server, may quickly
reach the bottleneck due to resource constraint and serious failure by attacks.
Towards pushing deep learning on devices, we present MDLdroid, a novel
decentralized mobile deep learning framework to enable resource-aware on-device
collaborative learning for personal mobile sensing applications. To address
resource limitation, we propose a ChainSGD-reduce approach which includes a
novel chain-directed Synchronous Stochastic Gradient Descent algorithm to
effectively reduce overhead among multiple devices. We also design an
agent-based multi-goal reinforcement learning mechanism to balance resources in
a fair and efficient manner. Our evaluations show that our model training on
off-the-shelf mobile devices achieves 2x to 3.5x faster than single-device
training, and 1.5x faster than the master-slave approach.",arxiv
http://arxiv.org/abs/1710.02595v2,2017-10-10T05:05:58Z,2017-10-06T21:42:15Z,Intelligent Pothole Detection and Road Condition Assessment,"Poor road conditions are a public nuisance, causing passenger discomfort,
damage to vehicles, and accidents. In the U.S., road-related conditions are a
factor in 22,000 of the 42,000 traffic fatalities each year. Although we often
complain about bad roads, we have no way to detect or report them at scale. To
address this issue, we developed a system to detect potholes and assess road
conditions in real-time. Our solution is a mobile application that captures
data on a car's movement from gyroscope and accelerometer sensors in the phone.
To assess roads using this sensor data, we trained SVM models to classify road
conditions with 93% accuracy and potholes with 92% accuracy, beating the base
rate for both problems. As the user drives, the models use the sensor data to
classify whether the road is good or bad, and whether it contains potholes.
Then, the classification results are used to create data-rich maps that
illustrate road conditions across the city. Our system will empower civic
officials to identify and repair damaged roads which inconvenience passengers
and cause accidents. This paper details our data science process for collecting
training data on real roads, transforming noisy sensor data into useful
signals, training and evaluating machine learning models, and deploying those
models to production through a real-time classification app. It also highlights
how cities can use our system to crowdsource data and deliver road repair
resources to areas in need.",arxiv
http://arxiv.org/abs/2002.04349v1,2020-02-11T12:41:01Z,2020-02-11T12:41:01Z,Robot Navigation with Map-Based Deep Reinforcement Learning,"This paper proposes an end-to-end deep reinforcement learning approach for
mobile robot navigation with dynamic obstacles avoidance. Using experience
collected in a simulation environment, a convolutional neural network (CNN) is
trained to predict proper steering actions of a robot from its egocentric local
occupancy maps, which accommodate various sensors and fusion algorithms. The
trained neural network is then transferred and executed on a real-world mobile
robot to guide its local path planning. The new approach is evaluated both
qualitatively and quantitatively in simulation and real-world robot
experiments. The results show that the map-based end-to-end navigation model is
easy to be deployed to a robotic platform, robust to sensor noise and
outperforms other existing DRL-based models in many indicators.",arxiv
http://arxiv.org/abs/2004.05898v1,2020-04-10T14:26:00Z,2020-04-10T14:26:00Z,Exposing Hardware Building Blocks to Machine Learning Frameworks,"There are a plethora of applications that demand high throughput and low
latency algorithms leveraging machine learning methods. This need for real time
processing can be seen in industries ranging from developing neural network
based pre-distortors for enhanced mobile broadband to designing FPGA-based
triggers in major scientific efforts by CERN for particle physics. In this
thesis, we explore how niche domains can benefit vastly if we look at neurons
as a unique boolean function of the form $f:B^{I} \rightarrow B^{O}$, where $B
= \{0,1\}$. We focus on how to design topologies that complement such a view of
neurons, how to automate such a strategy of neural network design, and
inference of such networks on Xilinx FPGAs. Major hardware borne constraints
arise when designing topologies that view neurons as unique boolean functions.
Fundamentally, realizing such topologies on hardware asserts a strict limit on
the 'fan-in' bits of a neuron due to the doubling of permutations possible with
every increment in input bit-length. We address this limit by exploring
different methods of implementing sparsity and explore activation quantization.
Further, we develop a library that supports training a neural network with
custom sparsity and quantization. This library also supports conversion of
trained Sparse Quantized networks from PyTorch to VERILOG code which is then
synthesized using Vivado, all of which is part of the LogicNet tool-flow. To
aid faster prototyping, we also support calculation of the worst-case hardware
cost of any given topology. We hope that our insights into the behavior of
extremely sparse quantized neural networks are of use to the research community
and by extension allow people to use the LogicNet design flow to deploy highly
efficient neural networks.",arxiv
http://arxiv.org/abs/1902.01506v3,2019-06-24T07:19:55Z,2019-02-05T00:59:44Z,"Learning to Prescribe Interventions for Tuberculosis Patients Using
  Digital Adherence Data","Digital Adherence Technologies (DATs) are an increasingly popular method for
verifying patient adherence to many medications. We analyze data from one city
served by 99DOTS, a phone-call-based DAT deployed for Tuberculosis (TB)
treatment in India where nearly 3 million people are afflicted with the disease
each year. The data contains nearly 17,000 patients and 2.1M dose records. We
lay the groundwork for learning from this real-world data, including a method
for avoiding the effects of unobserved interventions in training data used for
machine learning. We then construct a deep learning model, demonstrate its
interpretability, and show how it can be adapted and trained in different
clinical scenarios to better target and improve patient care. In the real-time
risk prediction setting our model could be used to proactively intervene with
21% more patients and before 76% more missed doses than current heuristic
baselines. For outcome prediction, our model performs 40% better than baseline
methods, allowing cities to target more resources to clinics with a heavier
burden of patients at risk of failure. Finally, we present a case study
demonstrating how our model can be trained in an end-to-end decision focused
learning setting to achieve 15% better solution quality in an example decision
problem faced by health workers.",arxiv
http://arxiv.org/abs/2002.12654v1,2020-02-28T11:16:26Z,2020-02-28T11:16:26Z,"Real time Smart Contracts for IoT using Blockchain and Collaborative
  Intelligence based Dynamic Pricing for the next generation Smart Toll
  Application","The confluence of Internet of Things(IoT) , Blockchain(BC) and Artificial
Intelligence(AI) acts as a key accelerator for enabling Machine Economy. To be
ready for future businesses these technologies needs to be adapted by extending
the IoT capabilities to Economy of Things (EoT) capabilities. In this paper we
focus on one such implementation experience for Smart Toll Transaction
application in the domain of mobility. Our paper showcases a possible solution
by leveraging negotiations, decision making, distributed learning capabilities
at the devices level using AI-enabled Multi-Agent Systems and the real-time
smart contracts between the Cars and Tolls using Blockchain. This solution also
showcases the monetization of real time data coming from various IoT devices
which are part of vehicles and infrastructure. While blockchain secures the
privacy of the participants it also acts as an economic transactional layer and
governance layer between the devices in the networ",arxiv
http://arxiv.org/abs/2012.02417v2,2020-12-07T02:34:05Z,2020-12-04T06:02:26Z,"Autonomous Navigation with Mobile Robots using Deep Learning and the
  Robot Operating System","Autonomous navigation is a long-standing field of robotics research, which
provides an essential capability for mobile robots to execute a series of tasks
on the same environments performed by human everyday. In this chapter, we
present a set of algorithms to train and deploy deep networks for autonomous
navigation of mobile robots using the Robot Operation System (ROS). We describe
three main steps to tackle this problem: i) collecting data in simulation
environments using ROS and Gazebo; ii) designing deep network for autonomous
navigation, and iii) deploying the learned policy on mobile robots in both
simulation and real-world. Theoretically, we present deep learning
architectures for robust navigation in normal environments (e.g., man-made
houses, roads) and complex environments (e.g., collapsed cities, or natural
caves). We further show that the use of visual modalities such as RGB, Lidar,
and point cloud is essential to improve the autonomy of mobile robots. Our
project website and demonstration video can be found at
https://sites.google.com/site/autonomousnavigationros.",arxiv
http://arxiv.org/abs/1904.06933v3,2019-04-21T13:39:00Z,2019-04-15T09:47:38Z,"Learning to Navigate in Indoor Environments: from Memorizing to
  Reasoning","Autonomous navigation is an essential capability of smart mobility for mobile
robots. Traditional methods must have the environment map to plan a
collision-free path in workspace. Deep reinforcement learning (DRL) is a
promising technique to realize the autonomous navigation task without a map,
with which deep neural network can fit the mapping from observation to
reasonable action through explorations. It should not only memorize the trained
target, but more importantly, the planner can reason out the unseen goal. We
proposed a new motion planner based on deep reinforcement learning that can
arrive at new targets that have not been trained before in the indoor
environment with RGB image and odometry only. The model has a structure of
stacked Long Short-Term memory (LSTM). Finally, experiments were implemented in
both simulated and real environments. The source code is available:
https://github.com/marooncn/navbot.",arxiv
http://arxiv.org/abs/1910.04335v2,2020-03-02T10:24:13Z,2019-10-10T02:34:34Z,"CityLearn: Diverse Real-World Environments for Sample-Efficient
  Navigation Policy Learning","Visual navigation tasks in real-world environments often require both
self-motion and place recognition feedback. While deep reinforcement learning
has shown success in solving these perception and decision-making problems in
an end-to-end manner, these algorithms require large amounts of experience to
learn navigation policies from high-dimensional data, which is generally
impractical for real robots due to sample complexity. In this paper, we address
these problems with two main contributions. We first leverage place recognition
and deep learning techniques combined with goal destination feedback to
generate compact, bimodal image representations that can then be used to
effectively learn control policies from a small amount of experience. Second,
we present an interactive framework, CityLearn, that enables for the first time
training and deployment of navigation algorithms across city-sized, realistic
environments with extreme visual appearance changes. CityLearn features more
than 10 benchmark datasets, often used in visual place recognition and
autonomous driving research, including over 100 recorded traversals across 60
cities around the world. We evaluate our approach on two CityLearn
environments, training our navigation policy on a single traversal. Results
show our method can be over 2 orders of magnitude faster than when using raw
images, and can also generalize across extreme visual changes including day to
night and summer to winter transitions.",arxiv
http://arxiv.org/abs/2101.06896v1,2021-01-18T06:29:30Z,2021-01-18T06:29:30Z,"DeepPayload: Black-box Backdoor Attack on Deep Learning Models through
  Neural Payload Injection","Deep learning models are increasingly used in mobile applications as critical
components. Unlike the program bytecode whose vulnerabilities and threats have
been widely-discussed, whether and how the deep learning models deployed in the
applications can be compromised are not well-understood since neural networks
are usually viewed as a black box. In this paper, we introduce a highly
practical backdoor attack achieved with a set of reverse-engineering techniques
over compiled deep learning models. The core of the attack is a neural
conditional branch constructed with a trigger detector and several operators
and injected into the victim model as a malicious payload. The attack is
effective as the conditional logic can be flexibly customized by the attacker,
and scalable as it does not require any prior knowledge from the original
model. We evaluated the attack effectiveness using 5 state-of-the-art deep
learning models and real-world samples collected from 30 users. The results
demonstrated that the injected backdoor can be triggered with a success rate of
93.5%, while only brought less than 2ms latency overhead and no more than 1.4%
accuracy decrease. We further conducted an empirical study on real-world mobile
deep learning apps collected from Google Play. We found 54 apps that were
vulnerable to our attack, including popular and security-critical ones. The
results call for the awareness of deep learning application developers and
auditors to enhance the protection of deployed models.",arxiv
http://arxiv.org/abs/2105.01401v1,2021-05-04T10:27:20Z,2021-05-04T10:27:20Z,"A Review of Confidentiality Threats Against Embedded Neural Network
  Models","Utilization of Machine Learning (ML) algorithms, especially Deep Neural
Network (DNN) models, becomes a widely accepted standard in many domains more
particularly IoT-based systems. DNN models reach impressive performances in
several sensitive fields such as medical diagnosis, smart transport or security
threat detection, and represent a valuable piece of Intellectual Property. Over
the last few years, a major trend is the large-scale deployment of models in a
wide variety of devices. However, this migration to embedded systems is slowed
down because of the broad spectrum of attacks threatening the integrity,
confidentiality and availability of embedded models. In this review, we cover
the landscape of attacks targeting the confidentiality of embedded DNN models
that may have a major impact on critical IoT systems, with a particular focus
on model extraction and data leakage. We highlight the fact that Side-Channel
Analysis (SCA) is a relatively unexplored bias by which model's confidentiality
can be compromised. Input data, architecture or parameters of a model can be
extracted from power or electromagnetic observations, testifying a real need
from a security point of view.",arxiv
http://arxiv.org/abs/1812.02030v2,2019-03-19T15:06:12Z,2018-12-05T15:05:01Z,"Wireless Data Acquisition for Edge Learning: Data-Importance Aware
  Retransmission","By deploying machine-learning algorithms at the network edge, edge learning
can leverage the enormous real-time data generated by billions of mobile
devices to train AI models, which enable intelligent mobile applications. In
this emerging research area, one key direction is to efficiently utilize radio
resources for wireless data acquisition to minimize the latency of executing a
learning task at an edge server. Along this direction, we consider the specific
problem of retransmission decision in each communication round to ensure both
reliability and quantity of those training data for accelerating model
convergence. To solve the problem, a new retransmission protocol called
data-importance aware automatic-repeat-request (importance ARQ) is proposed.
Unlike the classic ARQ focusing merely on reliability, importance ARQ
selectively retransmits a data sample based on its uncertainty which helps
learning and can be measured using the model under training. Underpinning the
proposed protocol is a derived elegant communication-learning relation between
two corresponding metrics, i.e., signal-to-noise ratio (SNR) and data
uncertainty. This relation facilitates the design of a simple threshold based
policy for importance ARQ. The policy is first derived based on the classic
classifier model of support vector machine (SVM), where the uncertainty of a
data sample is measured by its distance to the decision boundary. The policy is
then extended to the more complex model of convolutional neural networks (CNN)
where data uncertainty is measured by entropy. Extensive experiments have been
conducted for both the SVM and CNN using real datasets with balanced and
imbalanced distributions. Experimental results demonstrate that importance ARQ
effectively copes with channel fading and noise in wireless data acquisition to
achieve faster model convergence than the conventional channel-aware ARQ.",arxiv
http://arxiv.org/abs/2103.15908v2,2021-03-31T00:46:39Z,2021-03-29T19:38:04Z,"pH-RL: A personalization architecture to bring reinforcement learning to
  health practice","While reinforcement learning (RL) has proven to be the approach of choice for
tackling many complex problems, it remains challenging to develop and deploy RL
agents in real-life scenarios successfully. This paper presents pH-RL
(personalization in e-Health with RL) a general RL architecture for
personalization to bring RL to health practice. pH-RL allows for various levels
of personalization in health applications and allows for online and batch
learning. Furthermore, we provide a general-purpose implementation framework
that can be integrated with various healthcare applications. We describe a
step-by-step guideline for the successful deployment of RL policies in a mobile
application. We implemented our open-source RL architecture and integrated it
with the MoodBuster mobile application for mental health to provide messages to
increase daily adherence to the online therapeutic modules. We then performed a
comprehensive study with human participants over a sustained period. Our
experimental results show that the developed policies learn to select
appropriate actions consistently using only a few days' worth of data.
Furthermore, we empirically demonstrate the stability of the learned policies
during the study.",arxiv
http://arxiv.org/abs/1702.02676v1,2017-02-09T02:02:27Z,2017-02-09T02:02:27Z,Energy Saving Additive Neural Network,"In recent years, machine learning techniques based on neural networks for
mobile computing become increasingly popular. Classical multi-layer neural
networks require matrix multiplications at each stage. Multiplication operation
is not an energy efficient operation and consequently it drains the battery of
the mobile device. In this paper, we propose a new energy efficient neural
network with the universal approximation property over space of Lebesgue
integrable functions. This network, called, additive neural network, is very
suitable for mobile computing. The neural structure is based on a novel vector
product definition, called ef-operator, that permits a multiplier-free
implementation. In ef-operation, the ""product"" of two real numbers is defined
as the sum of their absolute values, with the sign determined by the sign of
the product of the numbers. This ""product"" is used to construct a vector
product in $R^N$. The vector product induces the $l_1$ norm. The proposed
additive neural network successfully solves the XOR problem. The experiments on
MNIST dataset show that the classification performances of the proposed
additive neural networks are very similar to the corresponding multi-layer
perceptron and convolutional neural networks (LeNet).",arxiv
http://arxiv.org/abs/1804.09364v3,2018-12-13T15:42:35Z,2018-04-25T06:20:12Z,Driving Policy Transfer via Modularity and Abstraction,"End-to-end approaches to autonomous driving have high sample complexity and
are difficult to scale to realistic urban driving. Simulation can help
end-to-end driving systems by providing a cheap, safe, and diverse training
environment. Yet training driving policies in simulation brings up the problem
of transferring such policies to the real world. We present an approach to
transferring driving policies from simulation to reality via modularity and
abstraction. Our approach is inspired by classic driving systems and aims to
combine the benefits of modular architectures and end-to-end deep learning
approaches. The key idea is to encapsulate the driving policy such that it is
not directly exposed to raw perceptual input or low-level vehicle dynamics. We
evaluate the presented approach in simulated urban environments and in the real
world. In particular, we transfer a driving policy trained in simulation to a
1/5-scale robotic truck that is deployed in a variety of conditions, with no
finetuning, on two continents. The supplementary video can be viewed at
https://youtu.be/BrMDJqI6H5U",arxiv
http://arxiv.org/abs/1803.04311v3,2019-01-30T12:54:32Z,2018-03-12T15:30:04Z,Deep Learning in Mobile and Wireless Networking: A Survey,"The rapid uptake of mobile devices and the rising popularity of mobile
applications and services pose unprecedented demands on mobile and wireless
networking infrastructure. Upcoming 5G systems are evolving to support
exploding mobile traffic volumes, agile management of network resource to
maximize user experience, and extraction of fine-grained real-time analytics.
Fulfilling these tasks is challenging, as mobile environments are increasingly
complex, heterogeneous, and evolving. One potential solution is to resort to
advanced machine learning techniques to help managing the rise in data volumes
and algorithm-driven applications. The recent success of deep learning
underpins new and powerful tools that tackle problems in this space.
  In this paper we bridge the gap between deep learning and mobile and wireless
networking research, by presenting a comprehensive survey of the crossovers
between the two areas. We first briefly introduce essential background and
state-of-the-art in deep learning techniques with potential applications to
networking. We then discuss several techniques and platforms that facilitate
the efficient deployment of deep learning onto mobile systems. Subsequently, we
provide an encyclopedic review of mobile and wireless networking research based
on deep learning, which we categorize by different domains. Drawing from our
experience, we discuss how to tailor deep learning to mobile environments. We
complete this survey by pinpointing current challenges and open future
directions for research.",arxiv
http://arxiv.org/abs/2109.13602v1,2021-09-28T10:23:46Z,2021-09-28T10:23:46Z,"SafetyNet: Safe planning for real-world self-driving vehicles using
  machine-learned policies","In this paper we present the first safe system for full control of
self-driving vehicles trained from human demonstrations and deployed in
challenging, real-world, urban environments. Current industry-standard
solutions use rule-based systems for planning. Although they perform reasonably
well in common scenarios, the engineering complexity renders this approach
incompatible with human-level performance. On the other hand, the performance
of machine-learned (ML) planning solutions can be improved by simply adding
more exemplar data. However, ML methods cannot offer safety guarantees and
sometimes behave unpredictably. To combat this, our approach uses a simple yet
effective rule-based fallback layer that performs sanity checks on an ML
planner's decisions (e.g. avoiding collision, assuring physical feasibility).
This allows us to leverage ML to handle complex situations while still assuring
the safety, reducing ML planner-only collisions by 95%. We train our ML planner
on 300 hours of expert driving demonstrations using imitation learning and
deploy it along with the fallback layer in downtown San Francisco, where it
takes complete control of a real vehicle and navigates a wide variety of
challenging urban driving scenarios.",arxiv
http://arxiv.org/abs/2104.15094v1,2021-04-30T16:20:27Z,2021-04-30T16:20:27Z,"QoS-Aware Placement of Deep Learning Services on the Edge with Multiple
  Service Implementations","Mobile edge computing pushes computationally-intensive services closer to the
user to provide reduced delay due to physical proximity. This has led many to
consider deploying deep learning models on the edge -- commonly known as edge
intelligence (EI). EI services can have many model implementations that provide
different QoS. For instance, one model can perform inference faster than
another (thus reducing latency) while achieving less accuracy when evaluated.
In this paper, we study joint service placement and model scheduling of EI
services with the goal to maximize Quality-of-Servcice (QoS) for end users
where EI services have multiple implementations to serve user requests, each
with varying costs and QoS benefits. We cast the problem as an integer linear
program and prove that it is NP-hard. We then prove the objective is equivalent
to maximizing a monotone increasing, submodular set function and thus can be
solved greedily while maintaining a (1-1/e)-approximation guarantee. We then
propose two greedy algorithms: one that theoretically guarantees this
approximation and another that empirically matches its performance with greater
efficiency. Finally, we thoroughly evaluate the proposed algorithm for making
placement and scheduling decisions in both synthetic and real-world scenarios
against the optimal solution and some baselines. In the real-world case, we
consider real machine learning models using the ImageNet 2012 data-set for
requests. Our numerical experiments empirically show that our more efficient
greedy algorithm is able to approximate the optimal solution with a 0.904
approximation on average, while the next closest baseline achieves a 0.607
approximation on average.",arxiv
http://arxiv.org/abs/2107.01996v1,2021-05-31T19:22:53Z,2021-05-31T19:22:53Z,"Explainability via Interactivity? Supporting Nonexperts' Sensemaking of
  Pretrained CNN by Interacting with Their Daily Surroundings","Current research on Explainable AI (XAI) heavily targets on expert users
(data scientists or AI developers). However, increasing importance has been
argued for making AI more understandable to nonexperts, who are expected to
leverage AI techniques, but have limited knowledge about AI. We present a
mobile application to support nonexperts to interactively make sense of
Convolutional Neural Networks (CNN); it allows users to play with a pretrained
CNN by taking pictures of their surrounding objects. We use an up-to-date XAI
technique (Class Activation Map) to intuitively visualize the model's decision
(the most important image regions that lead to a certain result). Deployed in a
university course, this playful learning tool was found to support design
students to gain vivid understandings about the capabilities and limitations of
pretrained CNNs in real-world environments. Concrete examples of students'
playful explorations are reported to characterize their sensemaking processes
reflecting different depths of thought.",arxiv
http://arxiv.org/abs/1801.01444v2,2018-10-15T20:34:00Z,2017-12-06T02:45:04Z,"Deep Anticipation: Light Weight Intelligent Mobile Sensing in IoT by
  Recurrent Architecture","The rapid growth of IoT era is shaping the future of mobile services.
Advanced communication technology enables a heterogeneous connectivity where
mobile devices broadcast information to everything. Mobile applications such as
robotics and vehicles connecting to cloud and surroundings transfer the
short-range on-board sensor perception system to long-range mobile-sensing
perception system. However, the mobile sensing perception brings new challenges
for how to efficiently analyze and intelligently interpret the deluge of IoT
data in mission- critical services. In this article, we model the challenges as
latency, packet loss and measurement noise which severely deteriorate the
reliability and quality of IoT data. We integrate the artificial intelligence
into IoT to tackle these challenges. We propose a novel architecture that
leverages recurrent neural networks (RNN) and Kalman filtering to anticipate
motions and interac- tions between objects. The basic idea is to learn
environment dynamics by recurrent networks. To improve the robustness of IoT
communication, we use the idea of Kalman filtering and deploy a prediction and
correction step. In this way, the architecture learns to develop a biased
belief between prediction and measurement in the different situation. We
demonstrate our approach with synthetic and real-world datasets with noise that
mimics the challenges of IoT communications. Our method brings a new level of
IoT intelligence. It is also lightweight compared to other state-of-the-art
convolutional recurrent architecture and is ideally suitable for the
resource-limited mobile applications.",arxiv
http://arxiv.org/abs/2108.00974v1,2021-08-02T15:22:05Z,2021-08-02T15:22:05Z,"Evaluating Federated Learning for Intrusion Detection in Internet of
  Things: Review and Challenges","The application of Machine Learning (ML) techniques to the well-known
intrusion detection systems (IDS) is key to cope with increasingly
sophisticated cybersecurity attacks through an effective and efficient
detection process. In the context of the Internet of Things (IoT), most
ML-enabled IDS approaches use centralized approaches where IoT devices share
their data with data centers for further analysis. To mitigate privacy concerns
associated with centralized approaches, in recent years the use of Federated
Learning (FL) has attracted a significant interest in different sectors,
including healthcare and transport systems. However, the development of
FL-enabled IDS for IoT is in its infancy, and still requires research efforts
from various areas, in order to identify the main challenges for the deployment
in real-world scenarios. In this direction, our work evaluates a FL-enabled IDS
approach based on a multiclass classifier considering different data
distributions for the detection of different attacks in an IoT scenario. In
particular, we use three different settings that are obtained by partitioning
the recent ToN\_IoT dataset according to IoT devices' IP address and types of
attack. Furthermore, we evaluate the impact of different aggregation functions
according to such setting by using the recent IBMFL framework as FL
implementation. Additionally, we identify a set of challenges and future
directions based on the existing literature and the analysis of our evaluation
results.",arxiv
http://arxiv.org/abs/2007.13404v2,2020-10-29T16:23:12Z,2020-07-27T09:50:11Z,"YOLOpeds: Efficient Real-Time Single-Shot Pedestrian Detection for Smart
  Camera Applications","Deep Learning-based object detectors can enhance the capabilities of smart
camera systems in a wide spectrum of machine vision applications including
video surveillance, autonomous driving, robots and drones, smart factory, and
health monitoring. Pedestrian detection plays a key role in all these
applications and deep learning can be used to construct accurate
state-of-the-art detectors. However, such complex paradigms do not scale easily
and are not traditionally implemented in resource-constrained smart cameras for
on-device processing which offers significant advantages in situations when
real-time monitoring and robustness are vital. Efficient neural networks can
not only enable mobile applications and on-device experiences but can also be a
key enabler of privacy and security allowing a user to gain the benefits of
neural networks without needing to send their data to the server to be
evaluated. This work addresses the challenge of achieving a good trade-off
between accuracy and speed for efficient deployment of deep-learning-based
pedestrian detection in smart camera applications. A computationally efficient
architecture is introduced based on separable convolutions and proposes
integrating dense connections across layers and multi-scale feature fusion to
improve representational capacity while decreasing the number of parameters and
operations. In particular, the contributions of this work are the following: 1)
An efficient backbone combining multi-scale feature operations, 2) a more
elaborate loss function for improved localization, 3) an anchor-less approach
for detection, The proposed approach called YOLOpeds is evaluated using the
PETS2009 surveillance dataset on 320x320 images. Overall, YOLOpeds provides
real-time sustained operation of over 30 frames per second with detection rates
in the range of 86% outperforming existing deep learning models.",arxiv
http://arxiv.org/abs/1804.00706v1,2018-03-28T16:02:45Z,2018-03-28T16:02:45Z,"Synergy: A HW/SW Framework for High Throughput CNNs on Embedded
  Heterogeneous SoC","Convolutional Neural Networks (CNN) have been widely deployed in diverse
application domains. There has been significant progress in accelerating both
their training and inference using high-performance GPUs, FPGAs, and custom
ASICs for datacenter-scale environments. The recent proliferation of mobile and
IoT devices have necessitated real-time, energy-efficient deep neural network
inference on embedded-class, resource-constrained platforms. In this context,
we present {\em Synergy}, an automated, hardware-software co-designed,
pipelined, high-throughput CNN inference framework on embedded heterogeneous
system-on-chip (SoC) architectures (Xilinx Zynq). {\em Synergy} leverages,
through multi-threading, all the available on-chip resources, which includes
the dual-core ARM processor along with the FPGA and the NEON SIMD engines as
accelerators. Moreover, {\em Synergy} provides a unified abstraction of the
heterogeneous accelerators (FPGA and NEON) and can adapt to different network
configurations at runtime without changing the underlying hardware accelerator
architecture by balancing workload across accelerators through work-stealing.
{\em Synergy} achieves 7.3X speedup, averaged across seven CNN models, over a
well-optimized software-only solution. {\em Synergy} demonstrates substantially
better throughput and energy-efficiency compared to the contemporary CNN
implementations on the same SoC architecture.",arxiv
http://arxiv.org/abs/2010.07029v2,2021-02-27T19:06:48Z,2020-09-29T21:32:28Z,"Basic principles and concept design of a real-time clinical decision
  support system for managing medical emergencies on missions to Mars","Space agencies and private companies prepare the beginning of human space
exploration for the 2030s with missions to put the first human on the Mars
surface. The absence of gravity and radiation, along with distance, isolation
and hostile environments, are expected to increase medical events where
previously unseen manifestations may arise. The current healthcare strategy
based on telemedicine and the possibility to stabilize and transport the
injured crewmember to a terrestrial definitive medical facility is not
applicable in exploration class missions. Therefore, the need for deploying the
full autonomous capability to solve medical emergencies may guide the design of
future onboard healthcare systems. We present ten basic principles and concept
design of a software suite to bring onboard decision support to help the crew
dealing with medical emergencies taking into consideration physiological
disturbances in space and spaceflight restrictions. 1) give real-time support
for emergency medical decision making, 2) give patient-specific advice for
executive problem-solving, 3) take into account available information from life
support and monitoring of crewmembers, 4) be fully autonomous from remote
facilities, 5) continuously adapt predictions to physiological disturbance and
changing conditions, 6) optimize emergency medical decision making in terms of
mission fundamental priorities, 7) take into account medical supplies and
equipment on board, 8) apply health standards for the level of care V, 9)
implement ethics responsibilities for spaceflights, and 10) apply ethical
standards for artificial intelligence. Based on these principles, we propose an
autonomous clinical decision support system (CDSS) to provide real-time advice
for emergency medical interventions on board of space exploration missions.",arxiv
http://arxiv.org/abs/2106.00171v1,2021-06-01T01:39:10Z,2021-06-01T01:39:10Z,"AIRIS: Artificial Intelligence Enhanced Signal Processing in
  Reconfigurable Intelligent Surface Communications","Reconfigurable intelligent surface (RIS) is an emerging meta-surface that can
provide additional communications links through reflecting the signals, and has
been recognized as a strong candidate of 6G mobile communications systems.
Meanwhile, it has been recently admitted that implementing artificial
intelligence (AI) into RIS communications will extensively benefit the
reconfiguration capacity and enhance the robustness to complicated transmission
environments. Besides the conventional model-driven approaches, AI can also
deal with the existing signal processing problems in a data-driven manner via
digging the inherent characteristic from the real data. Hence, AI is
particularly suitable for the signal processing problems over RIS networks
under unideal scenarios like modeling mismatching, insufficient resource,
hardware impairment, as well as dynamical transmissions. As one of the earliest
survey papers, we will introduce the merging of AI and RIS, called AIRIS, over
various signal processing topics, including environmental sensing, channel
acquisition, beamforming design, and resource scheduling, etc. We will also
discuss the challenges of AIRIS and present some interesting future directions.",arxiv
http://arxiv.org/abs/1707.04610v2,2018-04-15T17:48:20Z,2017-07-14T19:05:50Z,Cloud-based or On-device: An Empirical Study of Mobile Deep Inference,"Modern mobile applications are benefiting significantly from the advancement
in deep learning, e.g., implementing real-time image recognition and
conversational system. Given a trained deep learning model, applications
usually need to perform a series of matrix operations based on the input data,
in order to infer possible output values. Because of computational complexity
and size constraints, these trained models are often hosted in the cloud. To
utilize these cloud-based models, mobile apps will have to send input data over
the network. While cloud-based deep learning can provide reasonable response
time for mobile apps, it restricts the use case scenarios, e.g. mobile apps
need to have network access. With mobile specific deep learning optimizations,
it is now possible to employ on-device inference. However, because mobile
hardware, such as GPU and memory size, can be very limited when compared to its
desktop counterpart, it is important to understand the feasibility of this new
on-device deep learning inference architecture. In this paper, we empirically
evaluate the inference performance of three Convolutional Neural Networks
(CNNs) using a benchmark Android application we developed. Our measurement and
analysis suggest that on-device inference can cost up to two orders of
magnitude greater response time and energy when compared to cloud-based
inference, and that loading model and computing probability are two performance
bottlenecks for on-device deep inferences.",arxiv
http://arxiv.org/abs/2008.06448v1,2020-08-14T16:17:54Z,2020-08-14T16:17:54Z,"Loghub: A Large Collection of System Log Datasets towards Automated Log
  Analytics","Logs have been widely adopted in software system development and maintenance
because of the rich system runtime information they contain. In recent years,
the increase of software size and complexity leads to the rapid growth of the
volume of logs. To handle these large volumes of logs efficiently and
effectively, a line of research focuses on intelligent log analytics powered by
AI (artificial intelligence) techniques. However, only a small fraction of
these techniques have reached successful deployment in industry because of the
lack of public log datasets and necessary benchmarking upon them. To fill this
significant gap between academia and industry and also facilitate more research
on AI-powered log analytics, we have collected and organized loghub, a large
collection of log datasets. In particular, loghub provides 17 real-world log
datasets collected from a wide range of systems, including distributed systems,
supercomputers, operating systems, mobile systems, server applications, and
standalone software. In this paper, we summarize the statistics of these
datasets, introduce some practical log usage scenarios, and present a case
study on anomaly detection to demonstrate how loghub facilitates the research
and practice in this field. Up to the time of this paper writing, loghub
datasets have been downloaded over 15,000 times by more than 380 organizations
from both industry and academia.",arxiv
http://arxiv.org/abs/2011.02738v1,2020-11-05T10:16:54Z,2020-11-05T10:16:54Z,"Switching Scheme: A Novel Approach for Handling Incremental Concept
  Drift in Real-World Data Sets","Machine learning models nowadays play a crucial role for many applications in
business and industry. However, models only start adding value as soon as they
are deployed into production. One challenge of deployed models is the effect of
changing data over time, which is often described with the term concept drift.
Due to their nature, concept drifts can severely affect the prediction
performance of a machine learning system. In this work, we analyze the effects
of concept drift in the context of a real-world data set. For efficient concept
drift handling, we introduce the switching scheme which combines the two
principles of retraining and updating of a machine learning model. Furthermore,
we systematically analyze existing regular adaptation as well as triggered
adaptation strategies. The switching scheme is instantiated on New York City
taxi data, which is heavily influenced by changing demand patterns over time.
We can show that the switching scheme outperforms all other baselines and
delivers promising prediction results.",arxiv
http://arxiv.org/abs/1801.02190v1,2018-01-07T13:46:03Z,2018-01-07T13:46:03Z,Approximate FPGA-based LSTMs under Computation Time Constraints,"Recurrent Neural Networks and in particular Long Short-Term Memory (LSTM)
networks have demonstrated state-of-the-art accuracy in several emerging
Artificial Intelligence tasks. However, the models are becoming increasingly
demanding in terms of computational and memory load. Emerging latency-sensitive
applications including mobile robots and autonomous vehicles often operate
under stringent computation time constraints. In this paper, we address the
challenge of deploying computationally demanding LSTMs at a constrained time
budget by introducing an approximate computing scheme that combines iterative
low-rank compression and pruning, along with a novel FPGA-based LSTM
architecture. Combined in an end-to-end framework, the approximation method's
parameters are optimised and the architecture is configured to address the
problem of high-performance LSTM execution in time-constrained applications.
Quantitative evaluation on a real-life image captioning application indicates
that the proposed methods required up to 6.5x less time to achieve the same
application-level accuracy compared to a baseline method, while achieving an
average of 25x higher accuracy under the same computation time constraints.",arxiv
http://arxiv.org/abs/2007.12640v1,2020-07-24T16:50:41Z,2020-07-24T16:50:41Z,"Autonomous Exploration Under Uncertainty via Deep Reinforcement Learning
  on Graphs","We consider an autonomous exploration problem in which a range-sensing mobile
robot is tasked with accurately mapping the landmarks in an a priori unknown
environment efficiently in real-time; it must choose sensing actions that both
curb localization uncertainty and achieve information gain. For this problem,
belief space planning methods that forward-simulate robot sensing and
estimation may often fail in real-time implementation, scaling poorly with
increasing size of the state, belief and action spaces. We propose a novel
approach that uses graph neural networks (GNNs) in conjunction with deep
reinforcement learning (DRL), enabling decision-making over graphs containing
exploration information to predict a robot's optimal sensing action in belief
space. The policy, which is trained in different random environments without
human intervention, offers a real-time, scalable decision-making process whose
high-performance exploratory sensing actions yield accurate maps and high rates
of information gain.",arxiv
http://arxiv.org/abs/2005.13857v1,2020-05-28T09:15:14Z,2020-05-28T09:15:14Z,"Deep Reinforcement learning for real autonomous mobile robot navigation
  in indoor environments","Deep Reinforcement Learning has been successfully applied in various computer
games [8]. However, it is still rarely used in real-world applications,
especially for the navigation and continuous control of real mobile robots
[13]. Previous approaches lack safety and robustness and/or need a structured
environment. In this paper we present our proof of concept for autonomous
self-learning robot navigation in an unknown environment for a real robot
without a map or planner. The input for the robot is only the fused data from a
2D laser scanner and a RGB-D camera as well as the orientation to the goal. The
map of the environment is unknown. The output actions of an Asynchronous
Advantage Actor-Critic network (GA3C) are the linear and angular velocities for
the robot. The navigator/controller network is pretrained in a high-speed,
parallel, and self-implemented simulation environment to speed up the learning
process and then deployed to the real robot. To avoid overfitting, we train
relatively small networks, and we add random Gaussian noise to the input laser
data. The sensor data fusion with the RGB-D camera allows the robot to navigate
in real environments with real 3D obstacle avoidance and without the need to
fit the environment to the sensory capabilities of the robot. To further
increase the robustness, we train on environments of varying difficulties and
run 32 training instances simultaneously. Video: supplementary File / YouTube,
Code: GitHub",arxiv
http://arxiv.org/abs/1805.09994v2,2018-06-13T13:15:03Z,2018-05-25T06:18:01Z,Safe learning-based optimal motion planning for automated driving,"This paper presents preliminary work on learning the search heuristic for the
optimal motion planning for automated driving in urban traffic. Previous work
considered search-based optimal motion planning framework (SBOMP) that utilized
numerical or model-based heuristics that did not consider dynamic obstacles.
Optimal solution was still guaranteed since dynamic obstacles can only increase
the cost. However, significant variations in the search efficiency are observed
depending whether dynamic obstacles are present or not. This paper introduces
machine learning (ML) based heuristic that takes into account dynamic
obstacles, thus adding to the performance consistency for achieving real-time
implementation.",arxiv
http://arxiv.org/abs/1611.09065v1,2016-11-28T11:00:56Z,2016-11-28T11:00:56Z,"DrivingStyles: A mobile platform for driving styles and fuel consumption
  characterization","Intelligent Transportation Systems (ITS) rely on connected vehicle
applications to address real-world problems. Research is currently being
conducted to support safety, mobility and environmental applications. This
paper presents the DrivingStyles architecture, which adopts data mining
techniques and neural networks to analyze and generate a classification of
driving styles and fuel consumption based on driver characterization. In
particular, we have implemented an algorithm that is able to characterize the
degree of aggressiveness of each driver. We have also developed a methodology
to calculate, in real-time, the consumption and environmental impact of spark
ignition and diesel vehicles from a set of variables obtained from the
vehicle's Electronic Control Unit (ECU). In this paper, we demonstrate the
impact of the driving style on fuel consumption, as well as its correlation
with the greenhouse gas emissions generated by each vehicle. Overall, our
platform is able to assist drivers in correcting their bad driving habits,
while offering helpful tips to improve fuel economy and driving safety.",arxiv
http://arxiv.org/abs/2108.11482v1,2021-08-25T21:28:54Z,2021-08-25T21:28:54Z,ETA Prediction with Graph Neural Networks in Google Maps,"Travel-time prediction constitutes a task of high importance in
transportation networks, with web mapping services like Google Maps regularly
serving vast quantities of travel time queries from users and enterprises
alike. Further, such a task requires accounting for complex spatiotemporal
interactions (modelling both the topological properties of the road network and
anticipating events -- such as rush hours -- that may occur in the future).
Hence, it is an ideal target for graph representation learning at scale. Here
we present a graph neural network estimator for estimated time of arrival (ETA)
which we have deployed in production at Google Maps. While our main
architecture consists of standard GNN building blocks, we further detail the
usage of training schedule methods such as MetaGradients in order to make our
model robust and production-ready. We also provide prescriptive studies:
ablating on various architectural decisions and training regimes, and
qualitative analyses on real-world situations where our model provides a
competitive edge. Our GNN proved powerful when deployed, significantly reducing
negative ETA outcomes in several regions compared to the previous production
baseline (40+% in cities like Sydney).",arxiv
http://arxiv.org/abs/2006.03259v2,2020-06-13T07:55:34Z,2020-06-05T07:06:42Z,"Real-time Human Activity Recognition Using Conditionally Parametrized
  Convolutions on Mobile and Wearable Devices","Recently, deep learning has represented an important research trend in human
activity recognition (HAR). In particular, deep convolutional neural networks
(CNNs) have achieved state-of-the-art performance on various HAR datasets. For
deep learning, improvements in performance have to heavily rely on increasing
model size or capacity to scale to larger and larger datasets, which inevitably
leads to the increase of operations. A high number of operations in deep
leaning increases computational cost and is not suitable for real-time HAR
using mobile and wearable sensors. Though shallow learning techniques often are
lightweight, they could not achieve good performance. Therefore, deep learning
methods that can balance the trade-off between accuracy and computation cost is
highly needed, which to our knowledge has seldom been researched. In this
paper, we for the first time propose a computation efficient CNN using
conditionally parametrized convolution for real-time HAR on mobile and wearable
devices. We evaluate the proposed method on four public benchmark HAR datasets
consisting of WISDM dataset, PAMAP2 dataset, UNIMIB-SHAR dataset, and
OPPORTUNITY dataset, achieving state-of-the-art accuracy without compromising
computation cost. Various ablation experiments are performed to show how such a
network with large capacity is clearly preferable to baseline while requiring a
similar amount of operations. The method can be used as a drop-in replacement
for the existing deep HAR architectures and easily deployed onto mobile and
wearable devices for real-time HAR applications.",arxiv
http://arxiv.org/abs/1910.08683v1,2019-10-19T02:49:50Z,2019-10-19T02:49:50Z,"ELSA: A Throughput-Optimized Design of an LSTM Accelerator for
  Energy-Constrained Devices","The next significant step in the evolution and proliferation of artificial
intelligence technology will be the integration of neural network (NN) models
within embedded and mobile systems. This calls for the design of compact,
energy efficient NN models in silicon. In this paper, we present a scalable
ASIC design of an LSTM accelerator named ELSA, that is suitable for
energy-constrained devices. It includes several architectural innovations to
achieve small area and high energy efficiency. To reduce the area and power
consumption of the overall design, the compute-intensive units of ELSA employ
approximate multiplications and still achieve high performance and accuracy.
The performance is further improved through efficient synchronization of the
elastic pipeline stages to maximize the utilization. The paper also includes a
performance model of ELSA, as a function of the hidden nodes and time steps,
permitting its use for the evaluation of any LSTM application. ELSA was
implemented in RTL and was synthesized and placed and routed in 65nm
technology. Its functionality is demonstrated for language modeling-a common
application of LSTM. ELSA is compared against a baseline implementation of an
LSTM accelerator with standard functional units and without any of the
architectural innovations of ELSA. The paper demonstrates that ELSA can achieve
significant improvements in power, area and energy-efficiency when compared to
the baseline design and several ASIC implementations reported in the
literature, making it suitable for use in embedded systems and real-time
applications.",arxiv
http://arxiv.org/abs/2105.07986v2,2021-05-18T07:15:56Z,2021-05-17T16:10:58Z,Learning to Automatically Catch Potholes in Worldwide Road Scene Images,"Among several road hazards that are present in any paved way in the world,
potholes are one of the most annoying and also involving higher maintenance
costs. There exists an increasing interest on the automated detection of these
hazards enabled by technological and research progress. Our research work
tackled the challenge of pothole detection from images of real world road
scenes. The main novelty resides on the application of the latest progress in
AI to learn the visual appearance of potholes. We built a large dataset of
images with pothole annotations. They contained road scenes from different
cities in the world, taken with different cameras, vehicles and viewpoints
under varied environmental conditions. Then, we fine-tuned four different
object detection models based on Faster R-CNN and SSD deep neural networks. We
achieved high average precision and the pothole detector was tested on the
Nvidia DrivePX2 platform with GPGPU capability, which can be embedded on
vehicles. Moreover, it was deployed on a real vehicle to notify the detected
potholes to a given IoT platform as part of AUTOPILOT H2020 project.",arxiv
http://arxiv.org/abs/2109.11661v1,2021-09-23T21:55:12Z,2021-09-23T21:55:12Z,Learning-Based Path Planning for Long-Range Autonomous Valet Parking,"In this paper, to reduce the congestion rate at the city center and increase
the quality of experience (QoE) of each user, the framework of long-range
autonomous valet parking (LAVP) is presented, where an Electric Autonomous
Vehicle (EAV) is deployed in the city, which can pick up, drop off users at
their required spots, and then drive to the car park out of city center
autonomously. In this framework, we aim to minimize the overall distance of the
EAV, while guarantee all users are served, i.e., picking up, and dropping off
users at their required spots through optimizing the path planning of the EAV
and number of serving time slots. To this end, we first propose a learning
based algorithm, which is named as Double-Layer Ant Colony Optimization
(DL-ACO) algorithm to solve the above problem in an iterative way. Then, to
make the real-time decision, while consider the dynamic environment (i.e., the
EAV may pick up and drop off users from different locations), we further
present a deep reinforcement learning (DRL) based algorithm, which is known as
deep Q network (DQN). The experimental results show that the DL-ACO and
DQN-based algorithms both achieve the considerable performance.",arxiv
http://arxiv.org/abs/1610.07862v2,2016-10-26T02:32:30Z,2016-10-24T02:15:46Z,Intelligence in Artificial Intelligence,"The elusive quest for intelligence in artificial intelligence prompts us to
consider that instituting human-level intelligence in systems may be (still) in
the realm of utopia. In about a quarter century, we have witnessed the winter
of AI (1990) being transformed and transported to the zenith of tabloid fodder
about AI (2015). The discussion at hand is about the elements that constitute
the canonical idea of intelligence. The delivery of intelligence as a
pay-per-use-service, popping out of an app or from a shrink-wrapped software
defined point solution, is in contrast to the bio-inspired view of intelligence
as an outcome, perhaps formed from a tapestry of events, cross-pollinated by
instances, each with its own microcosm of experiences and learning, which may
not be discrete all-or-none functions but continuous, over space and time. The
enterprise world may not require, aspire or desire such an engaged solution to
improve its services for enabling digital transformation through the deployment
of digital twins, for example. One might ask whether the ""work-flow on
steroids"" version of decision support may suffice for intelligence? Are we
harking back to the era of rule based expert systems? The image conjured by the
publicity machines offers deep solutions with human-level AI and preposterous
claims about capturing the ""brain in a box"" by 2020. Even emulating insects may
be difficult in terms of real progress. Perhaps we can try to focus on worms
(Caenorhabditis elegans) which may be better suited for what business needs to
quench its thirst for so-called intelligence in AI.",arxiv
http://arxiv.org/abs/2005.07460v1,2020-05-15T10:34:51Z,2020-05-15T10:34:51Z,"Collective Risk Minimization via a Bayesian Model for Statistical
  Software Testing","In the last four years, the number of distinct autonomous vehicles platforms
deployed in the streets of California increased 6-fold, while the reported
accidents increased 12-fold. This can become a trend with no signs of subsiding
as it is fueled by a constant stream of innovations in hardware sensors and
machine learning software. Meanwhile, if we expect the public and regulators to
trust the autonomous vehicle platforms, we need to find better ways to solve
the problem of adding technological complexity without increasing the risk of
accidents. We studied this problem from the perspective of reliability
engineering in which a given risk of an accident has severity and probability
of occurring. Timely information on accidents is important for engineers to
anticipate and reuse previous failures to approximate the risk of accidents in
a new city. However, this is challenging in the context of autonomous vehicles
because of the sparse nature of data on the operational scenarios (driving
trajectories in a new city). Our approach was to mitigate data sparsity by
reducing the state space through monitoring of multiple-vehicles operations. We
then minimized the risk of accidents by determining proper allocation of tests
for each equivalence class. Our contributions comprise (1) a set of strategies
to monitor the operational data of multiple autonomous vehicles, (2) a Bayesian
model that estimates changes in the risk of accidents, and (3) a feedback
control-loop that minimizes these risks by reallocating test effort. Our
results are promising in the sense that we were able to measure and control
risk for a diversity of changes in the operational scenarios. We evaluated our
models with data from two real cities with distinct traffic patterns and made
the data available for the community.",arxiv
http://arxiv.org/abs/2005.06612v1,2020-05-05T12:01:12Z,2020-05-05T12:01:12Z,"An Investigation of COVID-19 Spreading Factors with Explainable AI
  Techniques","Since COVID-19 was first identified in December 2019, various public health
interventions have been implemented across the world. As different measures are
implemented at different countries at different times, we conduct an assessment
of the relative effectiveness of the measures implemented in 18 countries and
regions using data from 22/01/2020 to 02/04/2020. We compute the top one and
two measures that are most effective for the countries and regions studied
during the period. Two Explainable AI techniques, SHAP and ECPI, are used in
our study; such that we construct (machine learning) models for predicting the
instantaneous reproduction number ($R_t$) and use the models as surrogates to
the real world and inputs that the greatest influence to our models are seen as
measures that are most effective. Across-the-board, city lockdown and contact
tracing are the two most effective measures. For ensuring $R_t<1$, public
wearing face masks is also important. Mass testing alone is not the most
effective measure although when paired with other measures, it can be
effective. Warm temperature helps for reducing the transmission.",arxiv
http://arxiv.org/abs/1910.05765v1,2019-10-13T15:01:56Z,2019-10-13T15:01:56Z,"Real-Time and Embedded Deep Learning on FPGA for RF Signal
  Classification","We designed and implemented a deep learning based RF signal classifier on the
Field Programmable Gate Array (FPGA) of an embedded software-defined radio
platform, DeepRadio, that classifies the signals received through the RF front
end to different modulation types in real time and with low power. This
classifier implementation successfully captures complex characteristics of
wireless signals to serve critical applications in wireless security and
communications systems such as identifying spoofing signals in signal
authentication systems, detecting target emitters and jammers in electronic
warfare (EW) applications, discriminating primary and secondary users in
cognitive radio networks, interference hunting, and adaptive modulation.
Empowered by low-power and low-latency embedded computing, the deep neural
network runs directly on the FPGA fabric of DeepRadio, while maintaining
classifier accuracy close to the software performance. We evaluated the
performance when another SDR (USRP) transmits signals with different modulation
types at different power levels and DeepRadio receives the signals and
classifies them in real time on its FPGA. A smartphone with a mobile app is
connected to DeepRadio to initiate the experiment and visualize the
classification results. With real radio transmissions over the air, we show
that the classifier implemented on DeepRadio achieves high accuracy with low
latency (microsecond per sample) and low energy consumption (microJoule per
sample), and this performance is not matched by other embedded platforms such
as embedded graphics processing unit (GPU).",arxiv
http://arxiv.org/abs/1909.04824v2,2020-03-01T21:34:13Z,2019-09-10T13:00:35Z,A Machine Learning Method for Prediction of Multipath Channels,"In this paper, a machine learning method for predicting the evolution of a
mobile communication channel based on a specific type of convolutional neural
network is developed and evaluated in a simulated multipath transmission
scenario. The simulation and channel estimation are designed to replicate
real-world scenarios and common measurements supported by reference signals in
modern cellular networks. The capability of the predictor meets the
requirements that a deployment of the developed method in a radio resource
scheduler of a base station poses. Possible applications of the method are
discussed.",arxiv
http://arxiv.org/abs/1901.00748v1,2018-12-26T15:59:18Z,2018-12-26T15:59:18Z,The Impact of Countdown Clocks on Subway Ridership in New York City,"Protecting the passengers' safety and increasing ridership are two never
ending pursuits of public transit agencies. One of the proposed methods to
achieve both goals for subway service is to implement real time train arriving
countdown clocks in subway stations. Metropolitan Transportation Authority
(MTA) of New York City (NYC) chose to install such countdown clocks in their
stations starting from 2007 on a selection of subway lines. Due to the recent
development of Bluetooth Beacon technology, the MTA could now install countdown
clocks and train trackers in a non intrusive manner with much faster speed. As
a result, the MTA is aiming to install countdown clocks in every subway station
on every line. However, with such an aggressive plan, the impact of countdown
clocks on subway ridership has not been fully studied. This paper proposes
using Panel Regression methods, specifically, Random Effect (RE) model and
Fixed Effect (FE) model to quantify the impact of countdown clocks on subway
ridership. Machine Learning methods, namely Random Forest (RF) with AdaBoost
and Decision Tree (DT) Regression, are also used as alternative data driven
approaches for the FE and RE model. The results show that for the G line
service, which runs between Brooklyn and Queens, the introduction of countdown
clocks could increase weekly ridership by about 1783 per station. The study
also found that the machine learning methods provide better accuracy in
predicting the ridership than RE and FE models.",arxiv
http://arxiv.org/abs/2108.08910v1,2021-08-18T06:47:31Z,2021-08-18T06:47:31Z,"Achieving on-Mobile Real-Time Super-Resolution with Neural Architecture
  and Pruning Search","Though recent years have witnessed remarkable progress in single image
super-resolution (SISR) tasks with the prosperous development of deep neural
networks (DNNs), the deep learning methods are confronted with the computation
and memory consumption issues in practice, especially for resource-limited
platforms such as mobile devices. To overcome the challenge and facilitate the
real-time deployment of SISR tasks on mobile, we combine neural architecture
search with pruning search and propose an automatic search framework that
derives sparse super-resolution (SR) models with high image quality while
satisfying the real-time inference requirement. To decrease the search cost, we
leverage the weight sharing strategy by introducing a supernet and decouple the
search problem into three stages, including supernet construction,
compiler-aware architecture and pruning search, and compiler-aware pruning
ratio search. With the proposed framework, we are the first to achieve
real-time SR inference (with only tens of milliseconds per frame) for
implementing 720p resolution with competitive image quality (in terms of PSNR
and SSIM) on mobile platforms (Samsung Galaxy S20).",arxiv
http://arxiv.org/abs/2107.12943v1,2021-07-27T16:59:00Z,2021-07-27T16:59:00Z,"Learning-based Prediction, Rendering and Transmission for Interactive
  Virtual Reality in RIS-Assisted Terahertz Networks","The quality of experience (QoE) requirements of wireless Virtual Reality (VR)
can only be satisfied with high data rate, high reliability, and low VR
interaction latency. This high data rate over short transmission distances may
be achieved via abundant bandwidth in the terahertz (THz) band. However, THz
waves suffer from severe signal attenuation, which may be compensated by the
reconfigurable intelligent surface (RIS) technology with programmable
reflecting elements. Meanwhile, the low VR interaction latency may be achieved
with the mobile edge computing (MEC) network architecture due to its high
computation capability. Motivated by these considerations, in this paper, we
propose a MEC-enabled and RIS-assisted THz VR network in an indoor scenario, by
taking into account the uplink viewpoint prediction and position transmission,
MEC rendering, and downlink transmission. We propose two methods, which are
referred to as centralized online Gated Recurrent Unit (GRU) and distributed
Federated Averaging (FedAvg), to predict the viewpoints of VR users. In the
uplink, an algorithm that integrates online Long-short Term Memory (LSTM) and
Convolutional Neural Networks (CNN) is deployed to predict the locations and
the line-of-sight and non-line-of-sight statuses of the VR users over time. In
the downlink, we further develop a constrained deep reinforcement learning
algorithm to select the optimal phase shifts of the RIS under latency
constraints. Simulation results show that our proposed learning architecture
achieves near-optimal QoE as that of the genie-aided benchmark algorithm, and
about two times improvement in QoE compared to the random phase shift selection
scheme.",arxiv
http://arxiv.org/abs/2005.04970v3,2020-09-03T08:23:17Z,2020-05-11T10:01:56Z,"A Performance-Sensitive Malware Detection System Using Deep Learning on
  Mobile Devices","Currently, Android malware detection is mostly performed on server side
against the increasing number of malware. Powerful computing resource provides
more exhaustive protection for app markets than maintaining detection by a
single user. However, apart from the applications provided by the official
market, apps from unofficial markets and third-party resources are always
causing serious security threats to end-users. Meanwhile, it is a
time-consuming task if the app is downloaded first and then uploaded to the
server side for detection, because the network transmission has a lot of
overhead. In addition, the uploading process also suffers from the security
threats of attackers. Consequently, a last line of defense on mobile devices is
necessary and much-needed. In this paper, we propose an effective Android
malware detection system, MobiTive, leveraging customized deep neural networks
to provide a real-time and responsive detection environment on mobile devices.
MobiTive is a preinstalled solution rather than an app scanning and monitoring
engine using after installation, which is more practical and secure. Original
deep learning models cannot be directly deployed and executed on mobile devices
due to various performance limitations, such as computation power, memory size,
and energy. Therefore, we evaluate and investigate the following key points:(1)
the performance of different feature extraction methods based on source code or
binary code;(2) the performance of different feature type selections for deep
learning on mobile devices;(3) the detection accuracy of different deep neural
networks on mobile devices;(4) the real-time detection performance and accuracy
on different mobile devices;(5) the potential based on the evolution trend of
mobile devices' specifications; and finally we further propose a practical
solution (MobiTive) to detect Android malware on mobile devices.",arxiv
http://arxiv.org/abs/1305.7432v1,2013-05-31T14:50:36Z,2013-05-31T14:50:36Z,"Real-world Transfer of Evolved Artificial Immune System Behaviours
  between Small and Large Scale Robotic Platforms","In mobile robotics, a solid test for adaptation is the ability of a control
system to function not only in a diverse number of physical environments, but
also on a number of different robotic platforms. This paper demonstrates that a
set of behaviours evolved in simulation on a miniature robot (epuck) can be
transferred to a much larger-scale platform (Pioneer), both in simulation and
in the real world. The chosen architecture uses artificial evolution of epuck
behaviours to obtain a genetic sequence, which is then employed to seed an
idiotypic, artificial immune system (AIS) on the Pioneers. Despite numerous
hardware and software differences between the platforms, navigation and
target-finding experiments show that the evolved behaviours transfer very well
to the larger robot when the idiotypic AIS technique is used. In contrast,
transferability is poor when reinforcement learning alone is used, which
validates the adaptability of the chosen architecture.",arxiv
http://arxiv.org/abs/2008.05255v1,2020-08-12T12:03:27Z,2020-08-12T12:03:27Z,"Identity-Aware Attribute Recognition via Real-Time Distributed Inference
  in Mobile Edge Clouds","With the development of deep learning technologies, attribute recognition and
person re-identification (re-ID) have attracted extensive attention and
achieved continuous improvement via executing computing-intensive deep neural
networks in cloud datacenters. However, the datacenter deployment cannot meet
the real-time requirement of attribute recognition and person re-ID, due to the
prohibitive delay of backhaul networks and large data transmissions from
cameras to datacenters. A feasible solution thus is to employ mobile edge
clouds (MEC) within the proximity of cameras and enable distributed inference.
In this paper, we design novel models for pedestrian attribute recognition with
re-ID in an MEC-enabled camera monitoring system. We also investigate the
problem of distributed inference in the MEC-enabled camera network. To this
end, we first propose a novel inference framework with a set of distributed
modules, by jointly considering the attribute recognition and person re-ID. We
then devise a learning-based algorithm for the distributions of the modules of
the proposed distributed inference framework, considering the dynamic
MEC-enabled camera network with uncertainties. We finally evaluate the
performance of the proposed algorithm by both simulations with real datasets
and system implementation in a real testbed. Evaluation results show that the
performance of the proposed algorithm with distributed inference framework is
promising, by reaching the accuracies of attribute recognition and person
identification up to 92.9% and 96.6% respectively, and significantly reducing
the inference delay by at least 40.6% compared with existing methods.",arxiv
http://arxiv.org/abs/2110.15127v1,2021-10-28T14:02:16Z,2021-10-28T14:02:16Z,"Lightweight Mobile Automated Assistant-to-physician for Global
  Lower-resource Areas","Importance: Lower-resource areas in Africa and Asia face a unique set of
healthcare challenges: the dual high burden of communicable and
non-communicable diseases; a paucity of highly trained primary healthcare
providers in both rural and densely populated urban areas; and a lack of
reliable, inexpensive internet connections. Objective: To address these
challenges, we designed an artificial intelligence assistant to help primary
healthcare providers in lower-resource areas document demographic and medical
sign/symptom data and to record and share diagnostic data in real-time with a
centralized database. Design: We trained our system using multiple data sets,
including US-based electronic medical records (EMRs) and open-source medical
literature and developed an adaptive, general medical assistant system based on
machine learning algorithms. Main outcomes and Measure: The application
collects basic information from patients and provides primary care providers
with diagnoses and prescriptions suggestions. The application is unique from
existing systems in that it covers a wide range of common diseases, signs, and
medication typical in lower-resource countries; the application works with or
without an active internet connection. Results: We have built and implemented
an adaptive learning system that assists trained primary care professionals by
means of an Android smartphone application, which interacts with a central
database and collects real-time data. The application has been tested by dozens
of primary care providers. Conclusions and Relevance: Our application would
provide primary healthcare providers in lower-resource areas with a tool that
enables faster and more accurate documentation of medical encounters. This
application could be leveraged to automatically populate local or national EMR
systems.",arxiv
http://arxiv.org/abs/2004.00438v1,2020-04-01T13:30:05Z,2020-04-01T13:30:05Z,"Handling Concept Drifts in Regression Problems -- the Error Intersection
  Approach","Machine learning models are omnipresent for predictions on big data. One
challenge of deployed models is the change of the data over time, a phenomenon
called concept drift. If not handled correctly, a concept drift can lead to
significant mispredictions. We explore a novel approach for concept drift
handling, which depicts a strategy to switch between the application of simple
and complex machine learning models for regression tasks. We assume that the
approach plays out the individual strengths of each model, switching to the
simpler model if a drift occurs and switching back to the complex model for
typical situations. We instantiate the approach on a real-world data set of
taxi demand in New York City, which is prone to multiple drifts, e.g. the
weather phenomena of blizzards, resulting in a sudden decrease of taxi demand.
We are able to show that our suggested approach outperforms all regarded
baselines significantly.",arxiv
http://arxiv.org/abs/2110.04953v1,2021-10-11T01:23:07Z,2021-10-11T01:23:07Z,"Compact CNN Models for On-device Ocular-based User Recognition in Mobile
  Devices","A number of studies have demonstrated the efficacy of deep learning
convolutional neural network (CNN) models for ocular-based user recognition in
mobile devices. However, these high-performing networks have enormous space and
computational complexity due to the millions of parameters and computations
involved. These requirements make the deployment of deep learning models to
resource-constrained mobile devices challenging. To this end, only a handful of
studies based on knowledge distillation and patch-based models have been
proposed to obtain compact size CNN models for ocular recognition in the mobile
environment. In order to further advance the state-of-the-art, this study for
the first time evaluates five neural network pruning methods and compares them
with the knowledge distillation method for on-device CNN inference and mobile
user verification using ocular images. Subject-independent analysis on VISOB
and UPFR-Periocular datasets suggest the efficacy of layerwise magnitude-based
pruning at a compression rate of 8 for mobile ocular-based authentication using
ResNet50 as the base model. Further, comparison with the knowledge distillation
suggests the efficacy of knowledge distillation over pruning methods in terms
of verification accuracy and the real-time inference measured as deep feature
extraction time on five mobile devices, namely, iPhone 6, iPhone X, iPhone XR,
iPad Air 2 and iPad 7th Generation.",arxiv
http://arxiv.org/abs/1912.05313v1,2019-12-01T17:55:51Z,2019-12-01T17:55:51Z,"Flow Rate Control in Smart District Heating Systems Using Deep
  Reinforcement Learning","At high latitudes, many cities adopt a centralized heating system to improve
the energy generation efficiency and to reduce pollution. In multi-tier
systems, so-called district heating, there are a few efficient approaches for
the flow rate control during the heating process. In this paper, we describe
the theoretical methods to solve this problem by deep reinforcement learning
and propose a cloud-based heating control system for implementation. A
real-world case study shows the effectiveness and practicability of the
proposed system controlled by humans, and the simulated experiments for deep
reinforcement learning show about 1985.01 gigajoules of heat quantity and
42276.45 tons of water are saved per hour compared with manual control.",arxiv
http://arxiv.org/abs/2107.01001v2,2021-07-08T13:19:43Z,2021-06-03T08:35:10Z,"Feeling of Presence Maximization: mmWave-Enabled Virtual Reality Meets
  Deep Reinforcement Learning","This paper investigates the problem of providing ultra-reliable and
energy-efficient virtual reality (VR) experiences for wireless mobile users. To
ensure reliable ultra-high-definition (UHD) video frame delivery to mobile
users and enhance their immersive visual experiences, a coordinated multipoint
(CoMP) transmission technique and millimeter wave (mmWave) communications are
exploited. Owing to user movement and time-varying wireless channels, the
wireless VR experience enhancement problem is formulated as a
sequence-dependent and mixed-integer problem with a goal of maximizing users'
feeling of presence (FoP) in the virtual world, subject to power consumption
constraints on access points (APs) and users' head-mounted displays (HMDs). The
problem, however, is hard to be directly solved due to the lack of users'
accurate tracking information and the sequence-dependent and mixed-integer
characteristics. To overcome this challenge, we develop a parallel echo state
network (ESN) learning method to predict users' tracking information by
training fresh and historical tracking samples separately collected by APs.
With the learnt results, we propose a deep reinforcement learning (DRL) based
optimization algorithm to solve the formulated problem. In this algorithm, we
implement deep neural networks (DNNs) as a scalable solution to produce integer
decision variables and solving a continuous power control problem to criticize
the integer decision variables. Finally, the performance of the proposed
algorithm is compared with various benchmark algorithms, and the impact of
different design parameters is also discussed. Simulation results demonstrate
that the proposed algorithm is more 4.14% energy-efficient than the benchmark
algorithms.",arxiv
http://arxiv.org/abs/1702.01182v1,2017-02-03T21:57:13Z,2017-02-03T21:57:13Z,Uncertainty-Aware Reinforcement Learning for Collision Avoidance,"Reinforcement learning can enable complex, adaptive behavior to be learned
automatically for autonomous robotic platforms. However, practical deployment
of reinforcement learning methods must contend with the fact that the training
process itself can be unsafe for the robot. In this paper, we consider the
specific case of a mobile robot learning to navigate an a priori unknown
environment while avoiding collisions. In order to learn collision avoidance,
the robot must experience collisions at training time. However, high-speed
collisions, even at training time, could damage the robot. A successful
learning method must therefore proceed cautiously, experiencing only low-speed
collisions until it gains confidence. To this end, we present an
uncertainty-aware model-based learning algorithm that estimates the probability
of collision together with a statistical estimate of uncertainty. By
formulating an uncertainty-dependent cost function, we show that the algorithm
naturally chooses to proceed cautiously in unfamiliar environments, and
increases the velocity of the robot in settings where it has high confidence.
Our predictive model is based on bootstrapped neural networks using dropout,
allowing it to process raw sensory inputs from high-bandwidth sensors such as
cameras. Our experimental evaluation demonstrates that our method effectively
minimizes dangerous collisions at training time in an obstacle avoidance task
for a simulated and real-world quadrotor, and a real-world RC car. Videos of
the experiments can be found at https://sites.google.com/site/probcoll.",arxiv
http://arxiv.org/abs/2110.00836v1,2021-10-02T16:21:00Z,2021-10-02T16:21:00Z,"AI Back-End as a Service for Learning Switching of Mobile Apps between
  the Fog and the Cloud","Given that cloud servers are usually remotely located from the devices of
mobile apps, the end-users of the apps can face delays. The Fog has been
introduced to augment the apps with machines located at the network edge close
to the end-users. However, edge machines are usually resource constrained.
Thus, the execution of online data-analytics on edge machines may not be
feasible if the time complexity of the data-analytics algorithm is high. To
overcome this, multiple instances of the back-end should be deployed on edge
and remote machines. In this case, the research question is how the switching
of the app among the instances of the back-end can be dynamically decided based
on the response time of the service instances. To answer this, we contribute an
AI approach that trains machine-learning models of the response time of service
instances. Our approach extends a back-end as a service into an AI
self-back-end as a service that self-decides at runtime the right edge/remote
instance that achieves the lowest response-time. We evaluate the accuracy and
the efficiency of our approach by using real-word machine-learning datasets on
an existing auction app.",arxiv
http://arxiv.org/abs/2103.05056v2,2021-06-16T16:27:51Z,2021-03-08T20:19:37Z,"LCDNet: Deep Loop Closure Detection and Point Cloud Registration for
  LiDAR SLAM","Loop closure detection is an essential component of Simultaneous Localization
and Mapping (SLAM) systems, which reduces the drift accumulated over time. Over
the years, several deep learning approaches have been proposed to address this
task, however their performance has been subpar compared to handcrafted
techniques, especially while dealing with reverse loops. In this paper, we
introduce the novel LCDNet that effectively detects loop closures in LiDAR
point clouds by simultaneously identifying previously visited places and
estimating the 6-DoF relative transformation between the current scan and the
map. LCDNet is composed of a shared encoder, a place recognition head that
extracts global descriptors, and a relative pose head that estimates the
transformation between two point clouds. We introduce a novel relative pose
head based on the unbalanced optimal transport theory that we implement in a
differentiable manner to allow for end-to-end training. Extensive evaluations
of LCDNet on multiple real-world autonomous driving datasets show that our
approach outperforms state-of-the-art loop closure detection and point cloud
registration techniques by a large margin, especially while dealing with
reverse loops. Moreover, we integrate our proposed loop closure detection
approach into a LiDAR SLAM library to provide a complete mapping system and
demonstrate the generalization ability using different sensor setup in an
unseen city.",arxiv
http://arxiv.org/abs/2107.14585v1,2021-07-27T14:16:59Z,2021-07-27T14:16:59Z,"Dynamic optimal congestion pricing in multi-region urban networks by
  application of a Multi-Layer-Neural network","Traffic management by applying congestion pricing is a measure for mitigating
congestion in protected city corridors. As a promising tool, pricing improves
the level of service in a network and reduces travel delays. However,
real-world implementations are restricted to static pricing, i.e., the price is
fixed and not responsive to the prevailing regional traffic conditions. Dynamic
pricing overcomes these limitations but also affects the users route choices.
This work uses dynamic pricing's influence and predicts pricing functions to
aim for a system optimal traffic distribution. The framework models a
large-scale network where every region is considered homogeneous, allowing for
the Macroscopic Fundamental Diagram (MFD) application. We compute Dynamic
System Optimum (DSO) and a Quasi Dynamic User Equilibrium (QDUE) of the
macroscopic model by formulating a linear optimization problem and utilizing
the Dijkstra algorithm and a Multinomial Logit model (MNL), respectively. The
equilibria allow us to find an optimal pricing methodology by training
Multi-Layer-Neural (MLN) network models. We test our framework on a case study
in Zurich, Switzerland, and showcase that (a) our neural network model learns
the complex user behavior and (b) allows predicting optimal pricing functions.
Results show a significant performance improvement when operating a
transportation network in the DSO and highlight how dynamic pricing influences
the user's route choice behavior towards the system optimal equilibrium.",arxiv
http://arxiv.org/abs/2106.10352v1,2021-06-18T20:54:18Z,2021-06-18T20:54:18Z,"Semi-supervised Optimal Transport with Self-paced Ensemble for
  Cross-hospital Sepsis Early Detection","The utilization of computer technology to solve problems in medical scenarios
has attracted considerable attention in recent years, which still has great
potential and space for exploration. Among them, machine learning has been
widely used in the prediction, diagnosis and even treatment of Sepsis. However,
state-of-the-art methods require large amounts of labeled medical data for
supervised learning. In real-world applications, the lack of labeled data will
cause enormous obstacles if one hospital wants to deploy a new Sepsis detection
system. Different from the supervised learning setting, we need to use known
information (e.g., from another hospital with rich labeled data) to help build
a model with acceptable performance, i.e., transfer learning. In this paper, we
propose a semi-supervised optimal transport with self-paced ensemble framework
for Sepsis early detection, called SPSSOT, to transfer knowledge from the other
that has rich labeled data. In SPSSOT, we first extract the same clinical
indicators from the source domain (e.g., hospital with rich labeled data) and
the target domain (e.g., hospital with little labeled data), then we combine
the semi-supervised domain adaptation based on optimal transport theory with
self-paced under-sampling to avoid a negative transfer possibly caused by
covariate shift and class imbalance. On the whole, SPSSOT is an end-to-end
transfer learning method for Sepsis early detection which can automatically
select suitable samples from two domains respectively according to the number
of iterations and align feature space of two domains. Extensive experiments on
two open clinical datasets demonstrate that comparing with other methods, our
proposed SPSSOT, can significantly improve the AUC values with only 1% labeled
data in the target domain in two transfer learning scenarios, MIMIC
$rightarrow$ Challenge and Challenge $rightarrow$ MIMIC.",arxiv
http://arxiv.org/abs/2106.14739v1,2021-06-28T14:11:48Z,2021-06-28T14:11:48Z,"Real-Time Human Pose Estimation on a Smart Walker using Convolutional
  Neural Networks","Rehabilitation is important to improve quality of life for mobility-impaired
patients. Smart walkers are a commonly used solution that should embed
automatic and objective tools for data-driven human-in-the-loop control and
monitoring. However, present solutions focus on extracting few specific metrics
from dedicated sensors with no unified full-body approach. We investigate a
general, real-time, full-body pose estimation framework based on two RGB+D
camera streams with non-overlapping views mounted on a smart walker equipment
used in rehabilitation. Human keypoint estimation is performed using a
two-stage neural network framework. The 2D-Stage implements a detection module
that locates body keypoints in the 2D image frames. The 3D-Stage implements a
regression module that lifts and relates the detected keypoints in both cameras
to the 3D space relative to the walker. Model predictions are low-pass filtered
to improve temporal consistency. A custom acquisition method was used to obtain
a dataset, with 14 healthy subjects, used for training and evaluating the
proposed framework offline, which was then deployed on the real walker
equipment. An overall keypoint detection error of 3.73 pixels for the 2D-Stage
and 44.05mm for the 3D-Stage were reported, with an inference time of 26.6ms
when deployed on the constrained hardware of the walker. We present a novel
approach to patient monitoring and data-driven human-in-the-loop control in the
context of smart walkers. It is able to extract a complete and compact body
representation in real-time and from inexpensive sensors, serving as a common
base for downstream metrics extraction solutions, and Human-Robot interaction
applications. Despite promising results, more data should be collected on users
with impairments, to assess its performance as a rehabilitation tool in
real-world scenarios.",arxiv
http://arxiv.org/abs/2011.09359v1,2020-11-18T15:56:22Z,2020-11-18T15:56:22Z,FLaaS: Federated Learning as a Service,"Federated Learning (FL) is emerging as a promising technology to build
machine learning models in a decentralized, privacy-preserving fashion. Indeed,
FL enables local training on user devices, avoiding user data to be transferred
to centralized servers, and can be enhanced with differential privacy
mechanisms. Although FL has been recently deployed in real systems, the
possibility of collaborative modeling across different 3rd-party applications
has not yet been explored. In this paper, we tackle this problem and present
Federated Learning as a Service (FLaaS), a system enabling different scenarios
of 3rd-party application collaborative model building and addressing the
consequent challenges of permission and privacy management, usability, and
hierarchical model training. FLaaS can be deployed in different operational
environments. As a proof of concept, we implement it on a mobile phone setting
and discuss practical implications of results on simulated and real devices
with respect to on-device training CPU cost, memory footprint and power
consumed per FL model round. Therefore, we demonstrate FLaaS's feasibility in
building unique or joint FL models across applications for image object
detection in a few hours, across 100 devices.",arxiv
http://arxiv.org/abs/1604.04384v2,2016-10-14T08:13:52Z,2016-04-15T07:35:32Z,The STRANDS Project: Long-Term Autonomy in Everyday Environments,"Thanks to the efforts of the robotics and autonomous systems community,
robots are becoming ever more capable. There is also an increasing demand from
end-users for autonomous service robots that can operate in real environments
for extended periods. In the STRANDS project we are tackling this demand
head-on by integrating state-of-the-art artificial intelligence and robotics
research into mobile service robots, and deploying these systems for long-term
installations in security and care environments. Over four deployments, our
robots have been operational for a combined duration of 104 days autonomously
performing end-user defined tasks, covering 116km in the process. In this
article we describe the approach we have used to enable long-term autonomous
operation in everyday environments, and how our robots are able to use their
long run times to improve their own performance.",arxiv
http://arxiv.org/abs/2101.01329v1,2021-01-05T03:21:07Z,2021-01-05T03:21:07Z,A Trainable Reconciliation Method for Hierarchical Time-Series,"In numerous applications, it is required to produce forecasts for multiple
time-series at different hierarchy levels. An obvious example is given by the
supply chain in which demand forecasting may be needed at a store, city, or
country level. The independent forecasts typically do not add up properly
because of the hierarchical constraints, so a reconciliation step is needed. In
this paper, we propose a new general, flexible, and easy-to-implement
reconciliation strategy based on an encoder-decoder neural network. By testing
our method on four real-world datasets, we show that it can consistently reach
or surpass the performance of existing methods in the reconciliation setting.",arxiv
http://arxiv.org/abs/2105.05504v1,2021-05-12T08:28:12Z,2021-05-12T08:28:12Z,"An Empirical Experiment on Deep Learning Models for Predicting Traffic
  Data","To tackle ever-increasing city traffic congestion problems, researchers have
proposed deep learning models to aid decision-makers in the traffic control
domain. Although the proposed models have been remarkably improved in recent
years, there are still questions that need to be answered before deploying
models. For example, it is difficult to figure out which models provide
state-of-the-art performance, as recently proposed models have often been
evaluated with different datasets and experiment environments. It is also
difficult to determine which models would work when traffic conditions change
abruptly (e.g., rush hour). In this work, we conduct two experiments to answer
the two questions. In the first experiment, we conduct an experiment with the
state-of-the-art models and the identical public datasets to compare model
performance under a consistent experiment environment. We then extract a set of
temporal regions in the datasets, whose speeds change abruptly and use these
regions to explore model performance with difficult intervals. The experiment
results indicate that Graph-WaveNet and GMAN show better performance in
general. We also find that prediction models tend to have varying performances
with data and intervals, which calls for in-depth analysis of models on
difficult intervals for real-world deployment.",arxiv
http://arxiv.org/abs/1806.07840v4,2018-12-27T11:49:55Z,2018-06-20T16:56:54Z,"Edge Intelligence: On-Demand Deep Learning Model Co-Inference with
  Device-Edge Synergy","As the backbone technology of machine learning, deep neural networks (DNNs)
have have quickly ascended to the spotlight. Running DNNs on
resource-constrained mobile devices is, however, by no means trivial, since it
incurs high performance and energy overhead. While offloading DNNs to the cloud
for execution suffers unpredictable performance, due to the uncontrolled long
wide-area network latency. To address these challenges, in this paper, we
propose Edgent, a collaborative and on-demand DNN co-inference framework with
device-edge synergy. Edgent pursues two design knobs: (1) DNN partitioning that
adaptively partitions DNN computation between device and edge, in order to
leverage hybrid computation resources in proximity for real-time DNN inference.
(2) DNN right-sizing that accelerates DNN inference through early-exit at a
proper intermediate DNN layer to further reduce the computation latency. The
prototype implementation and extensive evaluations based on Raspberry Pi
demonstrate Edgent's effectiveness in enabling on-demand low-latency edge
intelligence.",arxiv
http://arxiv.org/abs/2003.12881v1,2020-03-28T19:57:55Z,2020-03-28T19:57:55Z,"Streamlined Empirical Bayes Fitting of Linear Mixed Models in Mobile
  Health","To effect behavior change a successful algorithm must make high-quality
decisions in real-time. For example, a mobile health (mHealth) application
designed to increase physical activity must make contextually relevant
suggestions to motivate users. While machine learning offers solutions for
certain stylized settings, such as when batch data can be processed offline,
there is a dearth of approaches which can deliver high-quality solutions under
the specific constraints of mHealth. We propose an algorithm which provides
users with contextualized and personalized physical activity suggestions. This
algorithm is able to overcome a challenge critical to mHealth that complex
models be trained efficiently. We propose a tractable streamlined empirical
Bayes procedure which fits linear mixed effects models in large-data settings.
Our procedure takes advantage of sparsity introduced by hierarchical random
effects to efficiently learn the posterior distribution of a linear mixed
effects model. A key contribution of this work is that we provide explicit
updates in order to learn both fixed effects, random effects and
hyper-parameter values. We demonstrate the success of this approach in a mobile
health (mHealth) reinforcement learning application, a domain in which fast
computations are crucial for real time interventions. Not only is our approach
computationally efficient, it is also easily implemented with closed form
matrix algebraic updates and we show improvements over state of the art
approaches both in speed and accuracy of up to 99% and 56% respectively.",arxiv
http://arxiv.org/abs/2110.01863v1,2021-10-05T07:55:19Z,2021-10-05T07:55:19Z,"DeepEdge: A Deep Reinforcement Learning based Task Orchestrator for Edge
  Computing","The improvements in the edge computing technology pave the road for
diversified applications that demand real-time interaction. However, due to the
mobility of the end-users and the dynamic edge environment, it becomes
challenging to handle the task offloading with high performance. Moreover,
since each application in mobile devices has different characteristics, a task
orchestrator must be adaptive and have the ability to learn the dynamics of the
environment. For this purpose, we develop a deep reinforcement learning based
task orchestrator, DeepEdge, which learns to meet different task requirements
without needing human interaction even under the heavily-loaded stochastic
network conditions in terms of mobile users and applications. Given the dynamic
offloading requests and time-varying communication conditions, we successfully
model the problem as a Markov process and then apply the Double Deep Q-Network
(DDQN) algorithm to implement DeepEdge. To evaluate the robustness of DeepEdge,
we experiment with four different applications including image rendering,
infotainment, pervasive health, and augmented reality in the network under
various loads. Furthermore, we compare the performance of our agent with the
four different task offloading approaches in the literature. Our results show
that DeepEdge outperforms its competitors in terms of the percentage of
satisfactorily completed tasks.",arxiv
http://arxiv.org/abs/2004.11003v1,2020-04-23T07:03:20Z,2020-04-23T07:03:20Z,Adaptive Techniques in Practical Quantum Key Distribution,"Quantum Key Distribution (QKD) can provide information-theoretically secure
communications and is a strong candidate for the next generation of
cryptography. However, in practice, the performance of QKD is limited by
""practical imperfections"" in realistic sources, channels, and detectors (such
as multi-photon components or imperfect encoding from the sources, losses and
misalignment in the channels, or dark counts in detectors). Addressing such
practical imperfections is a crucial part of implementing QKD protocols with
good performance in reality. There are two highly important future directions
for QKD: (1) QKD over free space, which can allow secure communications between
mobile platforms such as handheld systems, drones, planes, and even satellites,
and (2) fibre-based QKD networks, which can simultaneously provide QKD service
to numerous users at arbitrary locations. These directions are both highly
promising, but so far they are limited by practical imperfections in the
channels and devices, which pose huge challenges and limit their performance.
In this thesis, we develop adaptive techniques with innovative protocol and
algorithm design, as well as novel techniques such as machine learning, to
address some of these key challenges, including (a) atmospheric turbulence in
channels for free-space QKD, (b) asymmetric losses in channels for QKD network,
and (c) efficient parameter optimization in real time, which is important for
both free-space QKD and QKD networks. We believe that this work will pave the
way to important implementations of free-space QKD and fibre-based QKD networks
in the future.",arxiv
http://arxiv.org/abs/1910.09998v3,2020-06-03T07:15:15Z,2019-10-22T14:15:20Z,Learning Resilient Behaviors for Navigation Under Uncertainty,"Deep reinforcement learning has great potential to acquire complex, adaptive
behaviors for autonomous agents automatically. However, the underlying neural
network polices have not been widely deployed in real-world applications,
especially in these safety-critical tasks (e.g., autonomous driving). One of
the reasons is that the learned policy cannot perform flexible and resilient
behaviors as traditional methods to adapt to diverse environments. In this
paper, we consider the problem that a mobile robot learns adaptive and
resilient behaviors for navigating in unseen uncertain environments while
avoiding collisions. We present a novel approach for uncertainty-aware
navigation by introducing an uncertainty-aware predictor to model the
environmental uncertainty, and we propose a novel uncertainty-aware navigation
network to learn resilient behaviors in the prior unknown environments. To
train the proposed uncertainty-aware network more stably and efficiently, we
present the temperature decay training paradigm, which balances exploration and
exploitation during the training process. Our experimental evaluation
demonstrates that our approach can learn resilient behaviors in diverse
environments and generate adaptive trajectories according to environmental
uncertainties.",arxiv
http://arxiv.org/abs/2003.01157v2,2020-07-31T22:25:13Z,2020-03-02T19:39:16Z,"Reinforcement co-Learning of Deep and Spiking Neural Networks for
  Energy-Efficient Mapless Navigation with Neuromorphic Hardware","Energy-efficient mapless navigation is crucial for mobile robots as they
explore unknown environments with limited on-board resources. Although the
recent deep reinforcement learning (DRL) approaches have been successfully
applied to navigation, their high energy consumption limits their use in
several robotic applications. Here, we propose a neuromorphic approach that
combines the energy-efficiency of spiking neural networks with the optimality
of DRL and benchmark it in learning control policies for mapless navigation.
Our hybrid framework, spiking deep deterministic policy gradient (SDDPG),
consists of a spiking actor network (SAN) and a deep critic network, where the
two networks were trained jointly using gradient descent. The co-learning
enabled synergistic information exchange between the two networks, allowing
them to overcome each other's limitations through a shared representation
learning. To evaluate our approach, we deployed the trained SAN on Intel's
Loihi neuromorphic processor. When validated on simulated and real-world
complex environments, our method on Loihi consumed 75 times less energy per
inference as compared to DDPG on Jetson TX2, and also exhibited a higher rate
of successful navigation to the goal, which ranged from 1% to 4.2% and depended
on the forward-propagation timestep size. These results reinforce our ongoing
efforts to design brain-inspired algorithms for controlling autonomous robots
with neuromorphic hardware.",arxiv
http://arxiv.org/abs/1805.04902v2,2018-05-18T15:41:47Z,2018-05-13T15:55:33Z,LMNet: Real-time Multiclass Object Detection on CPU using 3D LiDAR,"This paper describes an optimized single-stage deep convolutional neural
network to detect objects in urban environments, using nothing more than point
cloud data. This feature enables our method to work regardless the time of the
day and the lighting conditions.The proposed network structure employs dilated
convolutions to gradually increase the perceptive field as depth increases,
this helps to reduce the computation time by about 30%. The network input
consists of five perspective representations of the unorganized point cloud
data. The network outputs an objectness map and the bounding box offset values
for each point. Our experiments showed that using reflection, range, and the
position on each of the three axes helped to improve the location and
orientation of the output bounding box. We carried out quantitative evaluations
with the help of the KITTI dataset evaluation server. It achieved the fastest
processing speed among the other contenders, making it suitable for real-time
applications. We implemented and tested it on a real vehicle with a Velodyne
HDL-64 mounted on top of it. We achieved execution times as fast as 50 FPS
using desktop GPUs, and up to 10 FPS on a single Intel Core i5 CPU. The deploy
implementation is open-sourced and it can be found as a feature branch inside
the autonomous driving framework Autoware. Code is available at:
https://github.com/CPFL/Autoware/tree/feature/cnn_lidar_detection",arxiv
http://arxiv.org/abs/2104.09968v3,2021-05-04T12:10:54Z,2021-04-19T10:36:23Z,"SALAD: Self-Adaptive Lightweight Anomaly Detection for Real-time
  Recurrent Time Series","Real-world time series data often present recurrent or repetitive patterns
and it is often generated in real time, such as transportation passenger
volume, network traffic, system resource consumption, energy usage, and human
gait. Detecting anomalous events based on machine learning approaches in such
time series data has been an active research topic in many different areas.
However, most machine learning approaches require labeled datasets, offline
training, and may suffer from high computation complexity, consequently
hindering their applicability. Providing a lightweight self-adaptive approach
that does not need offline training in advance and meanwhile is able to detect
anomalies in real time could be highly beneficial. Such an approach could be
immediately applied and deployed on any commodity machine to provide timely
anomaly alerts. To facilitate such an approach, this paper introduces SALAD,
which is a Self-Adaptive Lightweight Anomaly Detection approach based on a
special type of recurrent neural networks called Long Short-Term Memory (LSTM).
Instead of using offline training, SALAD converts a target time series into a
series of average absolute relative error (AARE) values on the fly and predicts
an AARE value for every upcoming data point based on short-term historical AARE
values. If the difference between a calculated AARE value and its corresponding
forecast AARE value is higher than a self-adaptive detection threshold, the
corresponding data point is considered anomalous. Otherwise, the data point is
considered normal. Experiments based on two real-world open-source time series
datasets demonstrate that SALAD outperforms five other state-of-the-art anomaly
detection approaches in terms of detection accuracy. In addition, the results
also show that SALAD is lightweight and can be deployed on a commodity machine.",arxiv
http://arxiv.org/abs/1808.07647v4,2020-06-04T15:33:19Z,2018-08-23T07:06:41Z,"Machine Learning at the Edge: A Data-Driven Architecture with
  Applications to 5G Cellular Networks","The fifth generation of cellular networks (5G) will rely on edge cloud
deployments to satisfy the ultra-low latency demand of future applications. In
this paper, we argue that such deployments can also be used to enable advanced
data-driven and Machine Learning (ML) applications in mobile networks. We
propose an edge-controller-based architecture for cellular networks and
evaluate its performance with real data from hundreds of base stations of a
major U.S. operator. In this regard, we will provide insights on how to
dynamically cluster and associate base stations and controllers, according to
the global mobility patterns of the users. Then, we will describe how the
controllers can be used to run ML algorithms to predict the number of users in
each base station, and a use case in which these predictions are exploited by a
higher-layer application to route vehicular traffic according to network Key
Performance Indicators (KPIs). We show that the prediction accuracy improves
when based on machine learning algorithms that rely on the controllers' view
and, consequently, on the spatial correlation introduced by the user mobility,
with respect to when the prediction is based only on the local data of each
single base station.",arxiv
http://arxiv.org/abs/2012.12104v1,2020-12-09T05:08:41Z,2020-12-09T05:08:41Z,"A Deep Reinforcement Learning Approach for Ramp Metering Based on
  Traffic Video Data","Ramp metering that uses traffic signals to regulate vehicle flows from the
on-ramps has been widely implemented to improve vehicle mobility of the
freeway. Previous studies generally update signal timings in real-time based on
predefined traffic measures collected by point detectors, such as traffic
volumes and occupancies. Comparing with point detectors, traffic cameras-which
have been increasingly deployed on road networks-could cover larger areas and
provide more detailed traffic information. In this work, we propose a deep
reinforcement learning (DRL) method to explore the potential of traffic video
data in improving the efficiency of ramp metering. The proposed method uses
traffic video frames as inputs and learns the optimal control strategies
directly from the high-dimensional visual inputs. A real-world case study
demonstrates that, in comparison with a state-of-the-practice method, the
proposed DRL method results in 1) lower travel times in the mainline, 2)
shorter vehicle queues at the on-ramp, and 3) higher traffic flows downstream
of the merging area. The results suggest that the proposed method is able to
extract useful information from the video data for better ramp metering
controls.",arxiv
http://arxiv.org/abs/2004.03718v1,2020-04-07T21:17:45Z,2020-04-07T21:17:45Z,Mobile-Based Deep Learning Models for Banana Diseases Detection,"Smallholder farmers in Tanzania are challenged on the lack of tools for early
detection of banana diseases. This study aimed at developing a mobile
application for early detection of Fusarium wilt race 1 and black Sigatoka
banana diseases using deep learning. We used a dataset of 3000 banana leaves
images. We pre-trained our model on Resnet152 and Inceptionv3 Convolution
Neural Network architectures. The Resnet152 achieved an accuracy of 99.2% and
Inceptionv3 an accuracy of 95.41%. On deployment using Android mobile phones,
we chose Inceptionv3 since it has lower memory requirements compared to
Resnet152. The mobile application on real environment detected the two diseases
with a confidence level of 99% of the captured leaf area. This result indicates
the potential in improving the yield of bananas by smallholder farmers using a
tool for early detection of diseases.",arxiv
http://arxiv.org/abs/1711.02666v1,2017-11-07T17:10:28Z,2017-11-07T17:10:28Z,"Tensor-Generative Adversarial Network with Two-dimensional Sparse
  Coding: Application to Real-time Indoor Localization","Localization technology is important for the development of indoor
location-based services (LBS). Global Positioning System (GPS) becomes invalid
in indoor environments due to the non-line-of-sight issue, so it is urgent to
develop a real-time high-accuracy localization approach for smartphones.
However, accurate localization is challenging due to issues such as real-time
response requirements, limited fingerprint samples and mobile device storage.
To address these problems, we propose a novel deep learning architecture:
Tensor-Generative Adversarial Network (TGAN).
  We first introduce a transform-based 3D tensor to model fingerprint samples.
Instead of those passive methods that construct a fingerprint database as a
prior, our model applies artificial neural network with deep learning to train
network classifiers and then gives out estimations. Then we propose a novel
tensor-based super-resolution scheme using the generative adversarial network
(GAN) that adopts sparse coding as the generator network and a residual
learning network as the discriminator. Further, we analyze the performance of
tensor-GAN and implement a trace-based localization experiment, which achieves
better performance. Compared to existing methods for smartphones indoor
positioning, that are energy-consuming and high demands on devices, TGAN can
give out an improved solution in localization accuracy, response time and
implementation complexity.",arxiv
http://arxiv.org/abs/2106.11118v3,2021-11-08T03:42:16Z,2021-06-21T13:55:57Z,"SODA10M: A Large-Scale 2D Self/Semi-Supervised Object Detection Dataset
  for Autonomous Driving","Aiming at facilitating a real-world, ever-evolving and scalable autonomous
driving system, we present a large-scale dataset for standardizing the
evaluation of different self-supervised and semi-supervised approaches by
learning from raw data, which is the first and largest dataset to date.
Existing autonomous driving systems heavily rely on `perfect' visual perception
models (i.e., detection) trained using extensive annotated data to ensure
safety. However, it is unrealistic to elaborately label instances of all
scenarios and circumstances (i.e., night, extreme weather, cities) when
deploying a robust autonomous driving system. Motivated by recent advances of
self-supervised and semi-supervised learning, a promising direction is to learn
a robust detection model by collaboratively exploiting large-scale unlabeled
data and few labeled data. Existing datasets either provide only a small amount
of data or covers limited domains with full annotation, hindering the
exploration of large-scale pre-trained models. Here, we release a Large-Scale
2D Self/semi-supervised Object Detection dataset for Autonomous driving, named
as SODA10M, containing 10 million unlabeled images and 20K images labeled with
6 representative object categories. To improve diversity, the images are
collected within 27833 driving hours under different weather conditions,
periods and location scenes of 32 different cities. We provide extensive
experiments and deep analyses of existing popular self/semi-supervised
approaches, and give some interesting findings in autonomous driving scope.
Experiments show that SODA10M can serve as a promising pre-training dataset for
different self-supervised learning methods, which gives superior performance
when fine-tuning with different downstream tasks (i.e., detection,
semantic/instance segmentation) in autonomous driving domain. More information
can refer to https://soda-2d.github.io.",arxiv
http://arxiv.org/abs/2011.12715v1,2020-11-23T00:34:56Z,2020-11-23T00:34:56Z,"Resonance: Replacing Software Constants with Context-Aware Models in
  Real-time Communication","Large software systems tune hundreds of 'constants' to optimize their runtime
performance. These values are commonly derived through intuition, lab tests, or
A/B tests. A 'one-size-fits-all' approach is often sub-optimal as the best
value depends on runtime context. In this paper, we provide an experimental
approach to replace constants with learned contextual functions for Skype - a
widely used real-time communication (RTC) application. We present Resonance, a
system based on contextual bandits (CB). We describe experiences from three
real-world experiments: applying it to the audio, video, and transport
components in Skype. We surface a unique and practical challenge of performing
machine learning (ML) inference in large software systems written using
encapsulation principles. Finally, we open-source FeatureBroker, a library to
reduce the friction in adopting ML models in such development environments",arxiv
http://arxiv.org/abs/1810.00162v2,2018-10-02T20:07:32Z,2018-09-29T06:56:33Z,"NICE: Noise Injection and Clamping Estimation for Neural Network
  Quantization","Convolutional Neural Networks (CNN) are very popular in many fields including
computer vision, speech recognition, natural language processing, to name a
few. Though deep learning leads to groundbreaking performance in these domains,
the networks used are very demanding computationally and are far from real-time
even on a GPU, which is not power efficient and therefore does not suit low
power systems such as mobile devices. To overcome this challenge, some
solutions have been proposed for quantizing the weights and activations of
these networks, which accelerate the runtime significantly. Yet, this
acceleration comes at the cost of a larger error. The \uniqname method proposed
in this work trains quantized neural networks by noise injection and a learned
clamping, which improve the accuracy. This leads to state-of-the-art results on
various regression and classification tasks, e.g., ImageNet classification with
architectures such as ResNet-18/34/50 with low as 3-bit weights and
activations. We implement the proposed solution on an FPGA to demonstrate its
applicability for low power real-time applications. The implementation of the
paper is available at https://github.com/Lancer555/NICE",arxiv
http://arxiv.org/abs/2105.02613v1,2021-05-06T12:40:28Z,2021-05-06T12:40:28Z,"Challenges and Obstacles Towards Deploying Deep Learning Models on
  Mobile Devices","From computer vision and speech recognition to forecasting trajectories in
autonomous vehicles, deep learning approaches are at the forefront of so many
domains. Deep learning models are developed using plethora of high-level,
generic frameworks and libraries. Running those models on the mobile devices
require hardware-aware optimizations and in most cases converting the models to
other formats or using a third-party framework. In reality, most of the
developed models need to undergo a process of conversion, adaptation, and, in
some cases, full retraining to match the requirements and features of the
framework that is deploying the model on the target platform. Variety of
hardware platforms with heterogeneous computing elements, from wearable devices
to high-performance GPU clusters are used to run deep learning models. In this
paper, we present the existing challenges, obstacles, and practical solutions
towards deploying deep learning models on mobile devices.",arxiv
http://arxiv.org/abs/1812.11652v1,2018-12-18T06:02:15Z,2018-12-18T06:02:15Z,"Using Machine Learning for Handover Optimization in Vehicular Fog
  Computing","Smart mobility management would be an important prerequisite for future fog
computing systems. In this research, we propose a learning-based handover
optimization for the Internet of Vehicles that would assist the smooth
transition of device connections and offloaded tasks between fog nodes. To
accomplish this, we make use of machine learning algorithms to learn from
vehicle interactions with fog nodes. Our approach uses a three-layer
feed-forward neural network to predict the correct fog node at a given location
and time with 99.2 % accuracy on a test set. We also implement a dual stacked
recurrent neural network (RNN) with long short-term memory (LSTM) cells capable
of learning the latency, or cost, associated with these service requests. We
create a simulation in JAMScript using a dataset of real-world vehicle
movements to create a dataset to train these networks. We further propose the
use of this predictive system in a smarter request routing mechanism to
minimize the service interruption during handovers between fog nodes and to
anticipate areas of low coverage through a series of experiments and test the
models' performance on a test set.",arxiv
http://arxiv.org/abs/1812.03078v1,2018-12-07T15:57:54Z,2018-12-07T15:57:54Z,"Evolutionary Games, Complex Networks and Nonlinear Analysis for
  Epileptic Seizures Forecasting","Epileptic seizures detection and forecasting is nowadays widely recognized as
a problem of great significance and social resonance, and still remains an
open, grand challenge. Furthermore, the development of mobile warning systems
and wearable, non invasive, advisory devices are increasingly and strongly
requested, from the patient community and their families and also from
institutional stakeholders. According to the many recent studies, exploiting
machine learning capabilities upon intracranial EEG (iEEG), in this work we
investigate a combination of novel game theory dynamical model on networks for
brain electrical activity and nonlinear time series analysis based on
recurrences quantification. These two methods are then melted together within a
supervised learning scheme and finally, prediction performances are assessed
using EEG scalp datasets, specifically recorded for this study. Our study
achieved mean sensitivity of 70.9% and a mean time in warning of 20.3%, thus
showing an increase of the improvement over chance metric from 42%, reported in
the most recent study, to 50.5%. Moreover, the real time implementation of the
proposed approach is currently under development on a prototype of a wearable
device.",arxiv
http://arxiv.org/abs/2002.06619v1,2020-02-16T17:02:59Z,2020-02-16T17:02:59Z,CRL: Class Representative Learning for Image Classification,"Building robust and real-time classifiers with diverse datasets are one of
the most significant challenges to deep learning researchers. It is because
there is a considerable gap between a model built with training (seen) data and
real (unseen) data in applications. Recent works including Zero-Shot Learning
(ZSL), have attempted to deal with this problem of overcoming the apparent gap
through transfer learning. In this paper, we propose a novel model, called
Class Representative Learning Model (CRL), that can be especially effective in
image classification influenced by ZSL. In the CRL model, first, the learning
step is to build class representatives to represent classes in datasets by
aggregating prominent features extracted from a Convolutional Neural Network
(CNN). Second, the inferencing step in CRL is to match between the class
representatives and new data. The proposed CRL model demonstrated superior
performance compared to the current state-of-the-art research in ZSL and mobile
deep learning. The proposed CRL model has been implemented and evaluated in a
parallel environment, using Apache Spark, for both distributed learning and
recognition. An extensive experimental study on the benchmark datasets,
ImageNet-1K, CalTech-101, CalTech-256, CIFAR-100, shows that CRL can build a
class distribution model with drastic improvement in learning and recognition
performance without sacrificing accuracy compared to the state-of-the-art
performances in image classification.",arxiv
http://arxiv.org/abs/1204.1653v1,2012-04-07T16:34:20Z,2012-04-07T16:34:20Z,Machine Cognition Models: EPAM and GPS,"Through history, the human being tried to relay its daily tasks to other
creatures, which was the main reason behind the rise of civilizations. It
started with deploying animals to automate tasks in the field of
agriculture(bulls), transportation (e.g. horses and donkeys), and even
communication (pigeons). Millenniums after, come the Golden age with
""Al-jazari"" and other Muslim inventors, which were the pioneers of automation,
this has given birth to industrial revolution in Europe, centuries after. At
the end of the nineteenth century, a new era was to begin, the computational
era, the most advanced technological and scientific development that is driving
the mankind and the reason behind all the evolutions of science; such as
medicine, communication, education, and physics. At this edge of technology
engineers and scientists are trying to model a machine that behaves the same as
they do, which pushed us to think about designing and implementing ""Things
that-Thinks"", then artificial intelligence was. In this work we will cover each
of the major discoveries and studies in the field of machine cognition, which
are the ""Elementary Perceiver and Memorizer""(EPAM) and ""The General Problem
Solver""(GPS). The First one focus mainly on implementing the human-verbal
learning behavior, while the second one tries to model an architecture that is
able to solve problems generally (e.g. theorem proving, chess playing, and
arithmetic). We will cover the major goals and the main ideas of each model, as
well as comparing their strengths and weaknesses, and finally giving their
fields of applications. And Finally, we will suggest a real life implementation
of a cognitive machine.",arxiv
http://arxiv.org/abs/1909.06727v1,2019-09-15T04:01:39Z,2019-09-15T04:01:39Z,"An Empirical Study towards Characterizing Deep Learning Development and
  Deployment across Different Frameworks and Platforms","Deep Learning (DL) has recently achieved tremendous success. A variety of DL
frameworks and platforms play a key role to catalyze such progress. However,
the differences in architecture designs and implementations of existing
frameworks and platforms bring new challenges for DL software development and
deployment. Till now, there is no study on how various mainstream frameworks
and platforms influence both DL software development and deployment in
practice. To fill this gap, we take the first step towards understanding how
the most widely-used DL frameworks and platforms support the DL software
development and deployment. We conduct a systematic study on these frameworks
and platforms by using two types of DNN architectures and three popular
datasets. (1) For development process, we investigate the prediction accuracy
under the same runtime training configuration or same model weights/biases. We
also study the adversarial robustness of trained models by leveraging the
existing adversarial attack techniques. The experimental results show that the
computing differences across frameworks could result in an obvious prediction
accuracy decline, which should draw the attention of DL developers. (2) For
deployment process, we investigate the prediction accuracy and performance
(refers to time cost and memory consumption) when the trained models are
migrated/quantized from PC to real mobile devices and web browsers. The DL
platform study unveils that the migration and quantization still suffer from
compatibility and reliability issues. Meanwhile, we find several DL software
bugs by using the results as a benchmark. We further validate the results
through bug confirmation from stakeholders and industrial positive feedback to
highlight the implications of our study. Through our study, we summarize
practical guidelines, identify challenges and pinpoint new research directions.",arxiv
http://arxiv.org/abs/1808.01977v6,2020-07-15T07:35:49Z,2018-08-06T16:13:47Z,"Deep Reinforcement Learning for Online Computation Offloading in
  Wireless Powered Mobile-Edge Computing Networks","Wireless powered mobile-edge computing (MEC) has recently emerged as a
promising paradigm to enhance the data processing capability of low-power
networks, such as wireless sensor networks and internet of things (IoT). In
this paper, we consider a wireless powered MEC network that adopts a binary
offloading policy, so that each computation task of wireless devices (WDs) is
either executed locally or fully offloaded to an MEC server. Our goal is to
acquire an online algorithm that optimally adapts task offloading decisions and
wireless resource allocations to the time-varying wireless channel conditions.
This requires quickly solving hard combinatorial optimization problems within
the channel coherence time, which is hardly achievable with conventional
numerical optimization methods. To tackle this problem, we propose a Deep
Reinforcement learning-based Online Offloading (DROO) framework that implements
a deep neural network as a scalable solution that learns the binary offloading
decisions from the experience. It eliminates the need of solving combinatorial
optimization problems, and thus greatly reduces the computational complexity
especially in large-size networks. To further reduce the complexity, we propose
an adaptive procedure that automatically adjusts the parameters of the DROO
algorithm on the fly. Numerical results show that the proposed algorithm can
achieve near-optimal performance while significantly decreasing the computation
time by more than an order of magnitude compared with existing optimization
methods. For example, the CPU execution latency of DROO is less than $0.1$
second in a $30$-user network, making real-time and optimal offloading truly
viable even in a fast fading environment.",arxiv
http://arxiv.org/abs/1906.03040v1,2019-05-14T05:26:19Z,2019-05-14T05:26:19Z,FASTER: Fusion AnalyticS for public Transport Event Response,"Increasing urban concentration raises operational challenges that can benefit
from integrated monitoring and decision support. Such complex systems need to
leverage the full stack of analytical methods, from state estimation using
multi-sensor fusion for situational awareness, to prediction and computation of
optimal responses. The FASTER platform that we describe in this work, deployed
at nation scale and handling 1.5 billion public transport trips a year, offers
such a full stack of techniques for this large-scale, real-time problem. FASTER
provides fine-grained situational awareness and real-time decision support with
the objective of improving the public transport commuter experience. The
methods employed range from statistical machine learning to agent-based
simulation and mixed-integer optimization. In this work we present an overview
of the challenges and methods involved, with details of the commuter movement
prediction module, as well as a discussion of open problems.",arxiv
http://arxiv.org/abs/2008.06655v1,2020-08-15T05:15:00Z,2020-08-15T05:15:00Z,Object Detection in the Context of Mobile Augmented Reality,"In the past few years, numerous Deep Neural Network (DNN) models and
frameworks have been developed to tackle the problem of real-time object
detection from RGB images. Ordinary object detection approaches process
information from the images only, and they are oblivious to the camera pose
with regard to the environment and the scale of the environment. On the other
hand, mobile Augmented Reality (AR) frameworks can continuously track a
camera's pose within the scene and can estimate the correct scale of the
environment by using Visual-Inertial Odometry (VIO). In this paper, we propose
a novel approach that combines the geometric information from VIO with semantic
information from object detectors to improve the performance of object
detection on mobile devices. Our approach includes three components: (1) an
image orientation correction method, (2) a scale-based filtering approach, and
(3) an online semantic map. Each component takes advantage of the different
characteristics of the VIO-based AR framework. We implemented the AR-enhanced
features using ARCore and the SSD Mobilenet model on Android phones. To
validate our approach, we manually labeled objects in image sequences taken
from 12 room-scale AR sessions. The results show that our approach can improve
on the accuracy of generic object detectors by 12% on our dataset.",arxiv
http://arxiv.org/abs/1910.13232v1,2019-10-29T12:50:40Z,2019-10-29T12:50:40Z,Detecting motorcycle helmet use with deep learning,"The continuous motorization of traffic has led to a sustained increase in the
global number of road related fatalities and injuries. To counter this,
governments are focusing on enforcing safe and law-abiding behavior in traffic.
However, especially in developing countries where the motorcycle is the main
form of transportation, there is a lack of comprehensive data on the
safety-critical behavioral metric of motorcycle helmet use. This lack of data
prohibits targeted enforcement and education campaigns which are crucial for
injury prevention. Hence, we have developed an algorithm for the automated
registration of motorcycle helmet usage from video data, using a deep learning
approach. Based on 91,000 annotated frames of video data, collected at multiple
observation sites in 7 cities across the country of Myanmar, we trained our
algorithm to detect active motorcycles, the number and position of riders on
the motorcycle, as well as their helmet use. An analysis of the algorithm's
accuracy on an annotated test data set, and a comparison to available
human-registered helmet use data reveals a high accuracy of our approach. Our
algorithm registers motorcycle helmet use rates with an accuracy of -4.4% and
+2.1% in comparison to a human observer, with minimal training for individual
observation sites. Without observation site specific training, the accuracy of
helmet use detection decreases slightly, depending on a number of factors. Our
approach can be implemented in existing roadside traffic surveillance
infrastructure and can facilitate targeted data-driven injury prevention
campaigns with real-time speed. Implications of the proposed method, as well as
measures that can further improve detection accuracy are discussed.",arxiv
http://arxiv.org/abs/1909.09910v2,2020-01-14T15:58:08Z,2019-09-21T22:28:00Z,"Deep learning approach to control of prosthetic hands with
  electromyography signals","Natural muscles provide mobility in response to nerve impulses.
Electromyography (EMG) measures the electrical activity of muscles in response
to a nerve's stimulation. In the past few decades, EMG signals have been used
extensively in the identification of user intention to potentially control
assistive devices such as smart wheelchairs, exoskeletons, and prosthetic
devices. In the design of conventional assistive devices, developers optimize
multiple subsystems independently. Feature extraction and feature description
are essential subsystems of this approach. Therefore, researchers proposed
various hand-crafted features to interpret EMG signals. However, the
performance of conventional assistive devices is still unsatisfactory. In this
paper, we propose a deep learning approach to control prosthetic hands with raw
EMG signals. We use a novel deep convolutional neural network to eschew the
feature-engineering step. Removing the feature extraction and feature
description is an important step toward the paradigm of end-to-end
optimization. Fine-tuning and personalization are additional advantages of our
approach. The proposed approach is implemented in Python with TensorFlow deep
learning library, and it runs in real-time in general-purpose graphics
processing units of NVIDIA Jetson TX2 developer kit. Our results demonstrate
the ability of our system to predict fingers position from raw EMG signals. We
anticipate our EMG-based control system to be a starting point to design more
sophisticated prosthetic hands. For example, a pressure measurement unit can be
added to transfer the perception of the environment to the user. Furthermore,
our system can be modified for other prosthetic devices.",arxiv
http://arxiv.org/abs/2103.15245v3,2021-04-03T14:55:20Z,2021-03-28T23:36:56Z,"Game Theory Based Privacy Preserving Approach for Collaborative Deep
  Learning in IoT","The exponential growth of Internet of Things (IoT) has become a transcending
force in creating innovative smart devices and connected domains including
smart homes, healthcare, transportation and manufacturing. With billions of IoT
devices, there is a huge amount of data continuously being generated,
transmitted, and stored at various points in the IoT architecture. Deep
learning is widely being used in IoT applications to extract useful insights
from IoT data. However, IoT users have security and privacy concerns and prefer
not to share their personal data with third party applications or stakeholders.
In order to address user privacy concerns, Collaborative Deep Learning (CDL)
has been largely employed in data-driven applications which enables multiple
IoT devices to train their models locally on edge gateways. In this chapter, we
first discuss different types of deep learning approaches and how these
approaches can be employed in the IoT domain. We present a privacy-preserving
collaborative deep learning approach for IoT devices which can achieve benefits
from other devices in the system. This learning approach is analyzed from the
behavioral perspective of mobile edge devices using a game-theoretic model. We
analyze the Nash Equilibrium in N-player static game model. We further present
a novel fair collaboration strategy among edge IoT devices using cluster based
approach to solve the CDL game, which enforces mobile edge devices for
cooperation. We also present implementation details and evaluation analysis in
a real-world smart home deployment.",arxiv
http://arxiv.org/abs/1804.10334v3,2019-02-22T08:15:42Z,2018-04-27T04:07:49Z,"Deep Learning Coordinated Beamforming for Highly-Mobile Millimeter Wave
  Systems","Supporting high mobility in millimeter wave (mmWave) systems enables a wide
range of important applications such as vehicular communications and wireless
virtual/augmented reality. Realizing this in practice, though, requires
overcoming several challenges. First, the use of narrow beams and the
sensitivity of mmWave signals to blockage greatly impact the coverage and
reliability of highly-mobile links. Second, highly-mobile users in dense mmWave
deployments need to frequently hand-off between base stations (BSs), which is
associated with critical control and latency overhead. Further, identifying the
optimal beamforming vectors in large antenna array mmWave systems requires
considerable training overhead, which significantly affects the efficiency of
these mobile systems. In this paper, a novel integrated machine learning and
coordinated beamforming solution is developed to overcome these challenges and
enable highly-mobile mmWave applications. In the proposed solution, a number of
distributed yet coordinating BSs simultaneously serve a mobile user. This user
ideally needs to transmit only one uplink training pilot sequence that will be
jointly received at the coordinating BSs using omni or quasi-omni beam
patterns. These received signals draw a defining signature not only for the
user location, but also for its interaction with the surrounding environment.
The developed solution then leverages a deep learning model that learns how to
use these signatures to predict the beamforming vectors at the BSs. This
renders a comprehensive solution that supports highly-mobile mmWave
applications with reliable coverage, low latency, and negligible training
overhead. Simulation results show that the proposed deep-learning coordinated
beamforming strategy approaches the achievable rate of the genie-aided solution
that knows the optimal beamforming vectors with no training overhead.",arxiv
http://arxiv.org/abs/1805.12085v1,2018-05-30T17:01:30Z,2018-05-30T17:01:30Z,"MPDCompress - Matrix Permutation Decomposition Algorithm for Deep Neural
  Network Compression","Deep neural networks (DNNs) have become the state-of-the-art technique for
machine learning tasks in various applications. However, due to their size and
the computational complexity, large DNNs are not readily deployable on edge
devices in real-time. To manage complexity and accelerate computation, network
compression techniques based on pruning and quantization have been proposed and
shown to be effective in reducing network size. However, such network
compression can result in irregular matrix structures that are mismatched with
modern hardware-accelerated platforms, such as graphics processing units (GPUs)
designed to perform the DNN matrix multiplications in a structured
(block-based) way. We propose MPDCompress, a DNN compression algorithm based on
matrix permutation decomposition via random mask generation. In-training
application of the masks molds the synaptic weight connection matrix to a
sub-graph separation format. Aided by the random permutations, a
hardware-desirable block matrix is generated, allowing for a more efficient
implementation and compression of the network. To show versatility, we
empirically verify MPDCompress on several network models, compression rates,
and image datasets. On the LeNet 300-100 model (MNIST dataset), Deep MNIST, and
CIFAR10, we achieve 10 X network compression with less than 1% accuracy loss
compared to non-compressed accuracy performance. On AlexNet for the full
ImageNet ILSVRC-2012 dataset, we achieve 8 X network compression with less than
1% accuracy loss, with top-5 and top-1 accuracies of 79.6% and 56.4%,
respectively. Finally, we observe that the algorithm can offer inference
speedups across various hardware platforms, with 4 X faster operation achieved
on several mobile GPUs.",arxiv
http://arxiv.org/abs/2107.13869v2,2021-07-30T13:55:57Z,2021-07-29T10:11:36Z,"Autonomous UAV Base Stations for Next Generation Wireless Networks: A
  Deep Learning Approach","To address the ever-growing connectivity demands of wireless communications,
the adoption of ingenious solutions, such as Unmanned Aerial Vehicles (UAVs) as
mobile Base Stations (BSs), is imperative. In general, the location of a UAV
Base Station (UAV-BS) is determined by optimization algorithms, which have high
computationally complexities and place heavy demands on UAV resources. In this
paper, we show that a Convolutional Neural Network (CNN) model can be trained
to infer the location of a UAV-BS in real time. In so doing, we create a
framework to determine the UAV locations that considers the deployment of
Mobile Users (MUs) to generate labels by using the data obtained from an
optimization algorithm. Performance evaluations reveal that once the CNN model
is trained with the given labels and locations of MUs, the proposed approach is
capable of approximating the results given by the adopted optimization
algorithm with high fidelity, outperforming Reinforcement Learning (RL)-based
approaches. We also explore future research challenges and highlight key
issues.",arxiv
http://arxiv.org/abs/2105.01803v1,2021-05-05T00:08:17Z,2021-05-05T00:08:17Z,"DeepRT: A Soft Real Time Scheduler for Computer Vision Applications on
  the Edge","The ubiquity of smartphone cameras and IoT cameras, together with the recent
boom of deep learning and deep neural networks, proliferate various computer
vision driven mobile and IoT applications deployed on the edge. This paper
focuses on applications which make soft real time requests to perform inference
on their data - they desire prompt responses within designated deadlines, but
occasional deadline misses are acceptable. Supporting soft real time
applications on a multi-tenant edge server is not easy, since the requests
sharing the limited GPU computing resources of an edge server interfere with
each other. In order to tackle this problem, we comprehensively evaluate how
latency and throughput respond to different GPU execution plans. Based on this
analysis, we propose a GPU scheduler, DeepRT, which provides latency guarantee
to the requests while maintaining high overall system throughput. The key
component of DeepRT, DisBatcher, batches data from different requests as much
as possible while it is proven to provide latency guarantee for requests
admitted by an Admission Control Module. DeepRT also includes an Adaptation
Module which tackles overruns. Our evaluation results show that DeepRT
outperforms state-of-the-art works in terms of the number of deadline misses
and throughput.",arxiv
http://arxiv.org/abs/1911.07919v1,2019-11-15T18:44:25Z,2019-11-15T18:44:25Z,ASV: Accelerated Stereo Vision System,"Estimating depth from stereo vision cameras, i.e., ""depth from stereo"", is
critical to emerging intelligent applications deployed in energy- and
performance-constrained devices, such as augmented reality headsets and mobile
autonomous robots. While existing stereo vision systems make trade-offs between
accuracy, performance and energy-efficiency, we describe ASV, an accelerated
stereo vision system that simultaneously improves both performance and
energy-efficiency while achieving high accuracy. The key to ASV is to exploit
unique characteristics inherent to stereo vision, and apply stereo-specific
optimizations, both algorithmically and computationally. We make two
contributions. Firstly, we propose a new stereo algorithm, invariant-based
stereo matching (ISM), that achieves significant speedup while retaining high
accuracy. The algorithm combines classic ""hand-crafted"" stereo algorithms with
recent developments in Deep Neural Networks (DNNs), by leveraging the
correspondence invariant unique to stereo vision systems. Secondly, we observe
that the bottleneck of the ISM algorithm is the DNN inference, and in
particular the deconvolution operations that introduce massive
compute-inefficiencies. We propose a set of software optimizations that
mitigate these inefficiencies. We show that with less than 0.5% hardware area
overhead, these algorithmic and computational optimizations can be effectively
integrated within a conventional DNN accelerator. Overall, ASV achieves 5x
speedup and 85% energy saving with 0.02% accuracy loss compared to today
DNN-based stereo vision systems.",arxiv
http://arxiv.org/abs/1906.10910v2,2019-07-01T09:03:34Z,2019-06-26T08:37:44Z,"Creating A Neural Pedagogical Agent by Jointly Learning to Review and
  Assess","Machine learning plays an increasing role in intelligent tutoring systems as
both the amount of data available and specialization among students grow.
Nowadays, these systems are frequently deployed on mobile applications. Users
on such mobile education platforms are dynamic, frequently being added,
accessing the application with varying levels of focus, and changing while
using the service. The education material itself, on the other hand, is often
static and is an exhaustible resource whose use in tasks such as problem
recommendation must be optimized. The ability to update user models with
respect to educational material in real-time is thus essential; however,
existing approaches require time-consuming re-training of user features
whenever new data is added. In this paper, we introduce a neural pedagogical
agent for real-time user modeling in the task of predicting user response
correctness, a central task for mobile education applications. Our model,
inspired by work in natural language processing on sequence modeling and
machine translation, updates user features in real-time via bidirectional
recurrent neural networks with an attention mechanism over embedded
question-response pairs. We experiment on the mobile education application
SantaTOEIC, which has 559k users, 66M response data points as well as a set of
10k study problems each expert-annotated with topic tags and gathered since
2016. Our model outperforms existing approaches over several metrics in
predicting user response correctness, notably out-performing other methods on
new users without large question-response histories. Additionally, our
attention mechanism and annotated tag set allow us to create an interpretable
education platform, with a smart review system that addresses the
aforementioned issue of varied user attention and problem exhaustion.",arxiv
http://arxiv.org/abs/1903.03882v1,2019-03-09T22:30:17Z,2019-03-09T22:30:17Z,"DeepPool: Distributed Model-free Algorithm for Ride-sharing using Deep
  Reinforcement Learning","The success of modern ride-sharing platforms crucially depends on the profit
of the ride-sharing fleet operating companies, and how efficiently the
resources are managed. Further, ride-sharing allows sharing costs and, hence,
reduces the congestion and emission by making better use of vehicle capacities.
In this work, we develop a distributed model-free, DeepPool, that uses deep
Q-network (DQN) techniques to learn optimal dispatch policies by interacting
with the environment. Further, DeepPool efficiently incorporates travel demand
statistics and deep learning models to manage dispatching vehicles for improved
ride sharing services. Using real-world dataset of taxi trip records in New
York City, DeepPool performs better than other strategies, proposed in the
literature, that do not consider ride sharing or do not dispatch the vehicles
to regions where the future demand is anticipated. Finally, DeepPool can adapt
rapidly to dynamic environments since it is implemented in a distributed manner
in which each vehicle solves its own DQN individually without coordination.",arxiv
http://arxiv.org/abs/2101.09336v1,2021-01-22T21:13:46Z,2021-01-22T21:13:46Z,A Comprehensive Survey on Hardware-Aware Neural Architecture Search,"Neural Architecture Search (NAS) methods have been growing in popularity.
These techniques have been fundamental to automate and speed up the time
consuming and error-prone process of synthesizing novel Deep Learning (DL)
architectures. NAS has been extensively studied in the past few years. Arguably
their most significant impact has been in image classification and object
detection tasks where the state of the art results have been obtained. Despite
the significant success achieved to date, applying NAS to real-world problems
still poses significant challenges and is not widely practical. In general, the
synthesized Convolution Neural Network (CNN) architectures are too complex to
be deployed in resource-limited platforms, such as IoT, mobile, and embedded
systems. One solution growing in popularity is to use multi-objective
optimization algorithms in the NAS search strategy by taking into account
execution latency, energy consumption, memory footprint, etc. This kind of NAS,
called hardware-aware NAS (HW-NAS), makes searching the most efficient
architecture more complicated and opens several questions.
  In this survey, we provide a detailed review of existing HW-NAS research and
categorize them according to four key dimensions: the search space, the search
strategy, the acceleration technique, and the hardware cost estimation
strategies. We further discuss the challenges and limitations of existing
approaches and potential future directions. This is the first survey paper
focusing on hardware-aware NAS. We hope it serves as a valuable reference for
the various techniques and algorithms discussed and paves the road for future
research towards hardware-aware NAS.",arxiv
http://arxiv.org/abs/2002.00831v1,2020-02-03T15:39:56Z,2020-02-03T15:39:56Z,An Actor-Critic-Based UAV-BSs Deployment Method for Dynamic Environments,"In this paper, the real-time deployment of unmanned aerial vehicles (UAVs) as
flying base stations (BSs) for optimizing the throughput of mobile users is
investigated for UAV networks. This problem is formulated as a time-varying
mixed-integer non-convex programming (MINP) problem, which is challenging to
find an optimal solution in a short time with conventional optimization
techniques. Hence, we propose an actor-critic-based (AC-based) deep
reinforcement learning (DRL) method to find near-optimal UAV positions at every
moment. In the proposed method, the process searching for the solution
iteratively at a particular moment is modeled as a Markov decision process
(MDP). To handle infinite state and action spaces and improve the robustness of
the decision process, two powerful neural networks (NNs) are configured to
evaluate the UAV position adjustments and make decisions, respectively.
Compared with the heuristic algorithm, sequential least-squares programming and
fixed UAVs methods, simulation results have shown that the proposed method
outperforms these three benchmarks in terms of the throughput at every moment
in UAV networks.",arxiv
http://arxiv.org/abs/2004.08170v1,2020-04-17T11:07:25Z,2020-04-17T11:07:25Z,"Deep Echo State Networks for Short-Term Traffic Forecasting: Performance
  Comparison and Statistical Assessment","In short-term traffic forecasting, the goal is to accurately predict future
values of a traffic parameter of interest occurring shortly after the
prediction is queried. The activity reported in this long-standing research
field has been lately dominated by different Deep Learning approaches, yielding
overly complex forecasting models that in general achieve accuracy gains of
questionable practical utility. In this work we elaborate on the performance of
Deep Echo State Networks for this particular task. The efficient learning
algorithm and simpler parametric configuration of these alternative modeling
approaches make them emerge as a competitive traffic forecasting method for
real ITS applications deployed in devices and systems with stringently limited
computational resources. An extensive comparison benchmark is designed with
real traffic data captured over the city of Madrid (Spain), amounting to more
than 130 automatic Traffic Readers (ATRs) and several shallow learning,
ensembles and Deep Learning models. Results from this comparison benchmark and
the analysis of the statistical significance of the reported performance gaps
are decisive: Deep Echo State Networks achieve more accurate traffic forecasts
than the rest of considered modeling counterparts.",arxiv
http://arxiv.org/abs/1812.01813v1,2018-12-05T04:41:10Z,2018-12-05T04:41:10Z,"Machine-learned epidemiology: real-time detection of foodborne illness
  at scale","Machine learning has become an increasingly powerful tool for solving complex
problems, and its application in public health has been underutilized. The
objective of this study is to test the efficacy of a machine-learned model of
foodborne illness detection in a real-world setting. To this end, we built
FINDER, a machine-learned model for real-time detection of foodborne illness
using anonymous and aggregated web search and location data. We computed the
fraction of people who visited a particular restaurant and later searched for
terms indicative of food poisoning to identify potentially unsafe restaurants.
We used this information to focus restaurant inspections in two cities and
demonstrated that FINDER improves the accuracy of health inspections;
restaurants identified by FINDER are 3.1 times as likely to be deemed unsafe
during the inspection as restaurants identified by existing methods.
Additionally, FINDER enables us to ascertain previously intractable
epidemiological information, for example, in 38% of cases the restaurant
potentially causing food poisoning was not the last one visited, which may
explain the lower precision of complaint-based inspections. We found that
FINDER is able to reliably identify restaurants that have an active lapse in
food safety, allowing for implementation of corrective actions that would
prevent the potential spread of foodborne illness.",arxiv
http://arxiv.org/abs/1908.03057v2,2019-08-09T15:37:31Z,2019-08-08T13:25:00Z,"Sim-to-Real Learning for Casualty Detection from Ground Projected Point
  Cloud Data","This paper addresses the problem of human body detection---particularly a
human body lying on the ground (a.k.a. casualty)---using point cloud data. This
ability to detect a casualty is one of the most important features of mobile
rescue robots, in order for them to be able to operate autonomously. We propose
a deep-learning-based casualty detection method using a deep convolutional
neural network (CNN). This network is trained to be able to detect a casualty
using a point-cloud data input. In the method we propose, the point cloud input
is pre-processed to generate a depth image-like ground-projected heightmap.
This heightmap is generated based on the projected distance of each point onto
the detected ground plane within the point cloud data. The generated heightmap
-- in image form -- is then used as an input for the CNN to detect a human body
lying on the ground. To train the neural network, we propose a novel
sim-to-real approach, in which the network model is trained using synthetic
data obtained in simulation and then tested on real sensor data. To make the
model transferable to real data implementations, during the training we adopt
specific data augmentation strategies with the synthetic training data. The
experimental results show that data augmentation introduced during the training
process is essential for improving the performance of the trained model on real
data. More specifically, the results demonstrate that the data augmentations on
raw point-cloud data have contributed to a considerable improvement of the
trained model performance.",arxiv
http://arxiv.org/abs/1808.00430v1,2018-08-01T17:23:36Z,2018-08-01T17:23:36Z,Tackling Android Stego Apps in the Wild,"Digital image forensics is a young but maturing field, encompassing key areas
such as camera identification, detection of forged images, and steganalysis.
However, large gaps exist between academic results and applications used by
practicing forensic analysts. To move academic discoveries closer to real-world
implementations, it is important to use data that represent ""in the wild""
scenarios. For detection of stego images created from steganography apps,
images generated from those apps are ideal to use. In this paper, we present
our work to perform steg detection on images from mobile apps using two
different approaches: ""signature"" detection, and machine learning methods. A
principal challenge of the ML task is to create a great many of stego images
from different apps with certain embedding rates. One of our main contributions
is a procedure for generating a large image database by using Android emulators
and reverse engineering techniques. We develop algorithms and tools for
signature detection on stego apps, and provide solutions to issues encountered
when creating ML classifiers.",arxiv
http://arxiv.org/abs/2001.00048v1,2019-12-31T19:41:59Z,2019-12-31T19:41:59Z,"MIR-Vehicle: Cost-Effective Research Platform for Autonomous Vehicle
  Applications","This paper illustrates the MIR (Mobile Intelligent Robotics) Vehicle: a
feasible option of transforming an electric ride-on-car into a modular Graphics
Processing Unit (GPU) powered autonomous platform equipped with the capability
that supports test and deployment of various intelligent autonomous vehicles
algorithms. To use a platform for research, two components must be provided:
perception and control. The sensors such as incremental encoders, an Inertial
Measurement Unit (IMU), a camera, and a LIght Detection And Ranging (LIDAR)
must be able to be installed on the platform to add the capability of
environmental perception. A microcontroller-powered control box is designed to
properly respond to the environmental changes by regulating drive and steering
motors. This drive-by-wire capability is controlled by a GPU powered laptop
computer where high-level perception algorithms are processed and complex
actions are generated by various methods including behavior cloning using deep
neural networks. The main goal of this paper is to provide an adequate and
comprehensive approach for fabricating a cost-effective platform that would
contribute to the research quality from the wider community. The proposed
platform is to use a modular and hierarchical software architecture where the
lower and simpler motor controls are taken care of by microcontroller programs,
and the higher and complex algorithms are processed by a GPU powered laptop
computer. The platform uses the Robot Operating System (ROS) as middleware to
maintain the modularity of the perceptions and decision-making modules. It is
expected that the level three and above autonomous vehicle systems and Advanced
Driver Assistance Systems (ADAS) can be tested on and deployed to the platform
with a decent real-time system behavior due to the capabilities and
affordability of the proposed platform.",arxiv
http://arxiv.org/abs/1904.03814v2,2019-11-18T06:16:42Z,2019-04-08T03:21:11Z,Temporal Convolution for Real-time Keyword Spotting on Mobile Devices,"Keyword spotting (KWS) plays a critical role in enabling speech-based user
interactions on smart devices. Recent developments in the field of deep
learning have led to wide adoption of convolutional neural networks (CNNs) in
KWS systems due to their exceptional accuracy and robustness. The main
challenge faced by KWS systems is the trade-off between high accuracy and low
latency. Unfortunately, there has been little quantitative analysis of the
actual latency of KWS models on mobile devices. This is especially concerning
since conventional convolution-based KWS approaches are known to require a
large number of operations to attain an adequate level of performance. In this
paper, we propose a temporal convolution for real-time KWS on mobile devices.
Unlike most of the 2D convolution-based KWS approaches that require a deep
architecture to fully capture both low- and high-frequency domains, we exploit
temporal convolutions with a compact ResNet architecture. In Google Speech
Command Dataset, we achieve more than \textbf{385x} speedup on Google Pixel 1
and surpass the accuracy compared to the state-of-the-art model. In addition,
we release the implementation of the proposed and the baseline models including
an end-to-end pipeline for training models and evaluating them on mobile
devices.",arxiv
http://arxiv.org/abs/2109.02270v1,2021-09-06T07:38:24Z,2021-09-06T07:38:24Z,"Encryption and Real Time Decryption for protecting Machine Learning
  models in Android Applications","With the Increasing use of Machine Learning in Android applications, more
research and efforts are being put into developing better-performing machine
learning algorithms with a vast amount of data. Along with machine learning for
mobile phones, the threat of extraction of trained machine learning models from
application packages (APK) through reverse engineering exists. Currently, there
are ways to protect models in mobile applications such as name obfuscation,
cloud deployment, last layer isolation. Still, they offer less security, and
their implementation requires more effort. This paper gives an algorithm to
protect trained machine learning models inside android applications with high
security and low efforts to implement it. The algorithm ensures security by
encrypting the model and real-time decrypting it with 256-bit Advanced
Encryption Standard (AES) inside the running application. It works efficiently
with big model files without interrupting the User interface (UI) Thread. As
compared to other methods, it is fast, more secure, and involves fewer efforts.
This algorithm provides the developers and researchers a way to secure their
actions and making the results available to all without any concern.",arxiv
http://arxiv.org/abs/1909.04760v1,2019-09-10T21:20:59Z,2019-09-10T21:20:59Z,"Deep Reinforcement Learning Algorithm for Dynamic Pricing of Express
  Lanes with Multiple Access Locations","This article develops a deep reinforcement learning (Deep-RL) framework for
dynamic pricing on managed lanes with multiple access locations and
heterogeneity in travelers' value of time, origin, and destination. This
framework relaxes assumptions in the literature by considering multiple origins
and destinations, multiple access locations to the managed lane, en route
diversion of travelers, partial observability of the sensor readings, and
stochastic demand and observations. The problem is formulated as a partially
observable Markov decision process (POMDP) and policy gradient methods are used
to determine tolls as a function of real-time observations. Tolls are modeled
as continuous and stochastic variables, and are determined using a feedforward
neural network. The method is compared against a feedback control method used
for dynamic pricing. We show that Deep-RL is effective in learning toll
policies for maximizing revenue, minimizing total system travel time, and other
joint weighted objectives, when tested on real-world transportation networks.
The Deep-RL toll policies outperform the feedback control heuristic for the
revenue maximization objective by generating revenues up to 9.5% higher than
the heuristic and for the objective minimizing total system travel time (TSTT)
by generating TSTT up to 10.4% lower than the heuristic. We also propose reward
shaping methods for the POMDP to overcome the undesired behavior of toll
policies, like the jam-and-harvest behavior of revenue-maximizing policies.
Additionally, we test transferability of the algorithm trained on one set of
inputs for new input distributions and offer recommendations on real-time
implementations of Deep-RL algorithms. The source code for our experiments is
available online at https://github.com/venktesh22/ExpressLanes_Deep-RL",arxiv
http://arxiv.org/abs/1405.4378v2,2016-03-03T02:43:03Z,2014-05-17T10:31:22Z,Area Coverage Under Low Sensor Density,"This paper presents a solution to the problem of monitoring a region of
interest (RoI) using a set of nodes that is not sufficient to achieve the
required degree of monitoring coverage. In particular, sensing coverage of
wireless sensor networks (WSNs) is a crucial issue in projects due to failure
of sensors. The lack of sensor equipment resources hinders the traditional
method of using mobile robots to move around the RoI to collect readings.
Instead, our solution employs supervised neural networks to produce the values
of the uncovered locations by extracting the non-linear relation among randomly
deployed sensor nodes throughout the area. Moreover, we apply a hybrid
backpropagation method to accelerate the learning convergence speed to a local
minimum solution. We use a real-world data set from meteorological deployment
for experimental validation and analysis.",arxiv
http://arxiv.org/abs/2007.09072v1,2020-07-15T09:40:13Z,2020-07-15T09:40:13Z,"Joint Multi-User DNN Partitioning and Computational Resource Allocation
  for Collaborative Edge Intelligence","Mobile Edge Computing (MEC) has emerged as a promising supporting
architecture providing a variety of resources to the network edge, thus acting
as an enabler for edge intelligence services empowering massive mobile and
Internet of Things (IoT) devices with AI capability. With the assistance of
edge servers, user equipments (UEs) are able to run deep neural network (DNN)
based AI applications, which are generally resource-hungry and
compute-intensive, such that an individual UE can hardly afford by itself in
real time. However the resources in each individual edge server are typically
limited. Therefore, any resource optimization involving edge servers is by
nature a resource-constrained optimization problem and needs to be tackled in
such realistic context. Motivated by this observation, we investigate the
optimization problem of DNN partitioning (an emerging DNN offloading scheme) in
a realistic multi-user resource-constrained condition that rarely considered in
previous works. Despite the extremely large solution space, we reveal several
properties of this specific optimization problem of joint multi-UE DNN
partitioning and computational resource allocation. We propose an algorithm
called Iterative Alternating Optimization (IAO) that can achieve the optimal
solution in polynomial time. In addition, we present rigorous theoretic
analysis of our algorithm in terms of time complexity and performance under
realistic estimation error. Moreover, we build a prototype that implements our
framework and conduct extensive experiments using realistic DNN models, whose
results demonstrate its effectiveness and efficiency.",arxiv
http://arxiv.org/abs/2004.02822v1,2020-04-06T17:09:50Z,2020-04-06T17:09:50Z,"LaNet: Real-time Lane Identification by Learning Road
  SurfaceCharacteristics from Accelerometer Data","The resolution of GPS measurements, especially in urban areas, is
insufficient for identifying a vehicle's lane. In this work, we develop a deep
LSTM neural network model LaNet that determines the lane vehicles are on by
periodically classifying accelerometer samples collected by vehicles as they
drive in real time. Our key finding is that even adjacent patches of road
surfaces contain characteristics that are sufficiently unique to differentiate
between lanes, i.e., roads inherently exhibit differing bumps, cracks,
potholes, and surface unevenness. Cars can capture this road surface
information as they drive using inexpensive, easy-to-install accelerometers
that increasingly come fitted in cars and can be accessed via the CAN-bus. We
collect an aggregate of 60 km driving data and synthesize more based on this
that capture factors such as variable driving speed, vehicle suspensions, and
accelerometer noise. Our formulated LSTM-based deep learning model, LaNet,
learns lane-specific sequences of road surface events (bumps, cracks etc.) and
yields 100% lane classification accuracy with 200 meters of driving data,
achieving over 90% with just 100 m (correspondingly to roughly one minute of
driving). We design the LaNet model to be practical for use in real-time lane
classification and show with extensive experiments that LaNet yields high
classification accuracy even on smooth roads, on large multi-lane roads, and on
drives with frequent lane changes. Since different road surfaces have different
inherent characteristics or entropy, we excavate our neural network model and
discover a mechanism to easily characterize the achievable classification
accuracies in a road over various driving distances by training the model just
once. We present LaNet as a low-cost, easily deployable and highly accurate way
to achieve fine-grained lane identification.",arxiv
http://arxiv.org/abs/1910.12980v1,2019-10-28T21:43:22Z,2019-10-28T21:43:22Z,Learning Transferable Graph Exploration,"This paper considers the problem of efficient exploration of unseen
environments, a key challenge in AI. We propose a `learning to explore'
framework where we learn a policy from a distribution of environments. At test
time, presented with an unseen environment from the same distribution, the
policy aims to generalize the exploration strategy to visit the maximum number
of unique states in a limited number of steps. We particularly focus on
environments with graph-structured state-spaces that are encountered in many
important real-world applications like software testing and map building. We
formulate this task as a reinforcement learning problem where the `exploration'
agent is rewarded for transitioning to previously unseen environment states and
employ a graph-structured memory to encode the agent's past trajectory.
Experimental results demonstrate that our approach is extremely effective for
exploration of spatial maps; and when applied on the challenging problems of
coverage-guided software-testing of domain-specific programs and real-world
mobile applications, it outperforms methods that have been hand-engineered by
human experts.",arxiv
http://arxiv.org/abs/1803.06077v2,2018-08-31T09:16:33Z,2018-03-16T05:29:12Z,"Real-time Detection, Tracking, and Classification of Moving and
  Stationary Objects using Multiple Fisheye Images","The ability to detect pedestrians and other moving objects is crucial for an
autonomous vehicle. This must be done in real-time with minimum system
overhead. This paper discusses the implementation of a surround view system to
identify moving as well as static objects that are close to the ego vehicle.
The algorithm works on 4 views captured by fisheye cameras which are merged
into a single frame. The moving object detection and tracking solution uses
minimal system overhead to isolate regions of interest (ROIs) containing moving
objects. These ROIs are then analyzed using a deep neural network (DNN) to
categorize the moving object. With deployment and testing on a real car in
urban environments, we have demonstrated the practical feasibility of the
solution. The video demos of our algorithm have been uploaded to Youtube:
https://youtu.be/vpoCfC724iA, https://youtu.be/2X4aqH2bMBs",arxiv
http://arxiv.org/abs/1804.06882v3,2019-01-18T05:46:59Z,2018-04-18T19:27:27Z,Pelee: A Real-Time Object Detection System on Mobile Devices,"An increasing need of running Convolutional Neural Network (CNN) models on
mobile devices with limited computing power and memory resource encourages
studies on efficient model design. A number of efficient architectures have
been proposed in recent years, for example, MobileNet, ShuffleNet, and
MobileNetV2. However, all these models are heavily dependent on depthwise
separable convolution which lacks efficient implementation in most deep
learning frameworks. In this study, we propose an efficient architecture named
PeleeNet, which is built with conventional convolution instead. On ImageNet
ILSVRC 2012 dataset, our proposed PeleeNet achieves a higher accuracy and over
1.8 times faster speed than MobileNet and MobileNetV2 on NVIDIA TX2. Meanwhile,
PeleeNet is only 66% of the model size of MobileNet. We then propose a
real-time object detection system by combining PeleeNet with Single Shot
MultiBox Detector (SSD) method and optimizing the architecture for fast speed.
Our proposed detection system2, named Pelee, achieves 76.4% mAP (mean average
precision) on PASCAL VOC2007 and 22.4 mAP on MS COCO dataset at the speed of
23.6 FPS on iPhone 8 and 125 FPS on NVIDIA TX2. The result on COCO outperforms
YOLOv2 in consideration of a higher precision, 13.6 times lower computational
cost and 11.3 times smaller model size.",arxiv
http://arxiv.org/abs/2001.05277v1,2020-01-15T12:50:09Z,2020-01-15T12:50:09Z,Model-Driven Beamforming Neural Networks,"Beamforming is evidently a core technology in recent generations of mobile
communication networks. Nevertheless, an iterative process is typically
required to optimize the parameters, making it ill-placed for real-time
implementation due to high complexity and computational delay. Heuristic
solutions such as zero-forcing (ZF) are simpler but at the expense of
performance loss. Alternatively, deep learning (DL) is well understood to be a
generalizing technique that can deliver promising results for a wide range of
applications at much lower complexity if it is sufficiently trained. As a
consequence, DL may present itself as an attractive solution to beamforming. To
exploit DL, this article introduces general data- and model-driven beamforming
neural networks (BNNs), presents various possible learning strategies, and also
discusses complexity reduction for the DL-based BNNs. We also offer enhancement
methods such as training-set augmentation and transfer learning in order to
improve the generality of BNNs, accompanied by computer simulation results and
testbed results showing the performance of such BNN solutions.",arxiv
http://arxiv.org/abs/2012.01153v1,2020-12-02T12:53:19Z,2020-12-02T12:53:19Z,Towards Intelligent Reconfigurable Wireless Physical Layer (PHY),"Next-generation wireless networks are getting significant attention because
they promise 10-factor enhancement in mobile broadband along with the potential
to enable new heterogeneous services. Services include massive machine type
communications desired for Industrial 4.0 along with ultra-reliable low latency
services for remote healthcare and vehicular communications. In this paper, we
present the design of an intelligent and reconfigurable physical layer (PHY) to
bring these services to reality. First, we design and implement the
reconfigurable PHY via a hardware-software co-design approach on system-on-chip
consisting of the ARM processor and field-programmable gate array (FPGA). The
reconfigurable PHY is then made intelligent by augmenting it with online
machine learning (OML) based decision-making algorithm. Such PHY can learn the
environment (for example, wireless channel) and dynamically adapt the
transceivers' configuration (i.e., modulation scheme, word-length) and select
the wireless channel on-the-fly. Since the environment is unknown and changes
with time, we make the OML architecture reconfigurable to enable dynamic switch
between various OML algorithms on-the-fly. We have demonstrated the functional
correctness of the proposed architecture for different environments and
word-lengths. The detailed throughput, latency, and complexity analysis
validate the feasibility and importance of the proposed intelligent and
reconfigurable PHY in next-generation networks.",arxiv
http://arxiv.org/abs/1907.06740v1,2019-07-15T20:39:15Z,2019-07-15T20:39:15Z,Real-time Hair Segmentation and Recoloring on Mobile GPUs,"We present a novel approach for neural network-based hair segmentation from a
single camera input specifically designed for real-time, mobile application.
Our relatively small neural network produces a high-quality hair segmentation
mask that is well suited for AR effects, e.g. virtual hair recoloring. The
proposed model achieves real-time inference speed on mobile GPUs (30-100+ FPS,
depending on the device) with high accuracy. We also propose a very realistic
hair recoloring scheme. Our method has been deployed in major AR application
and is used by millions of users.",arxiv
http://arxiv.org/abs/2005.05069v1,2020-05-08T10:53:49Z,2020-05-08T10:53:49Z,"Transfer Learning and Online Learning for Traffic Forecasting under
  Different Data Availability Conditions: Alternatives and Pitfalls","This work aims at unveiling the potential of Transfer Learning (TL) for
developing a traffic flow forecasting model in scenarios of absent data.
Knowledge transfer from high-quality predictive models becomes feasible under
the TL paradigm, enabling the generation of new proper models with few data. In
order to explore this capability, we identify three different levels of data
absent scenarios, where TL techniques are applied among Deep Learning (DL)
methods for traffic forecasting. Then, traditional batch learning is compared
against TL based models using real traffic flow data, collected by deployed
loops managed by the City Council of Madrid (Spain). In addition, we apply
Online Learning (OL) techniques, where model receives an update after each
prediction, in order to adapt to traffic flow trend changes and incrementally
learn from new incoming traffic data. The obtained experimental results shed
light on the advantages of transfer and online learning for traffic flow
forecasting, and draw practical insights on their interplay with the amount of
available training data at the location of interest.",arxiv
http://arxiv.org/abs/1812.11494v3,2019-01-16T07:11:30Z,2018-12-30T09:09:11Z,"Broadband Analog Aggregation for Low-Latency Federated Edge Learning
  (Extended Version)","The popularity of mobile devices results in the availability of enormous data
and computational resources at the network edge. To leverage the data and
resources, a new machine learning paradigm, called edge learning, has emerged
where learning algorithms are deployed at the edge for providing fast and
intelligent services to mobile users. While computing speeds are advancing
rapidly, the communication latency is becoming the bottleneck of fast edge
learning. To address this issue, this work is focused on designing a low
latency multi-access scheme for edge learning. We consider a popular framework,
federated edge learning (FEEL), where edge-server and on-device learning are
synchronized to train a model without violating user-data privacy. It is
proposed that model updates simultaneously transmitted by devices over
broadband channels should be analog aggregated ""over-the-air"" by exploiting the
superposition property of a multi-access channel. Thereby, ""interference"" is
harnessed to provide fast implementation of the model aggregation. This results
in dramatical latency reduction compared with the traditional orthogonal access
(i.e., OFDMA). In this work, the performance of FEEL is characterized targeting
a single-cell random network. First, due to power alignment between devices as
required for aggregation, a fundamental tradeoff is shown to exist between the
update-reliability and the expected update-truncation ratio. This motivates the
design of an opportunistic scheduling scheme for FEEL that selects devices
within a distance threshold. This scheme is shown using real datasets to yield
satisfactory learning performance in the presence of high mobility. Second,
both the multi-access latency of the proposed analog aggregation and the OFDMA
scheme are analyzed. Their ratio, which quantifies the latency reduction of the
former, is proved to scale almost linearly with device population.",arxiv
http://arxiv.org/abs/2103.05225v3,2021-03-29T20:57:58Z,2021-03-09T05:06:47Z,A Scavenger Hunt for Service Robots,"Creating robots that can perform general-purpose service tasks in a
human-populated environment has been a longstanding grand challenge for AI and
Robotics research. One particularly valuable skill that is relevant to a wide
variety of tasks is the ability to locate and retrieve objects upon request.
This paper models this skill as a Scavenger Hunt (SH) game, which we formulate
as a variation of the NP-hard stochastic traveling purchaser problem. In this
problem, the goal is to find a set of objects as quickly as possible, given
probability distributions of where they may be found. We investigate the
performance of several solution algorithms for the SH problem, both in
simulation and on a real mobile robot. We use Reinforcement Learning (RL) to
train an agent to plan a minimal cost path, and show that the RL agent can
outperform a range of heuristic algorithms, achieving near optimal performance.
In order to stimulate research on this problem, we introduce a publicly
available software stack and associated website that enable users to upload
scavenger hunts which robots can download, perform, and learn from to
continually improve their performance on future hunts.",arxiv
http://arxiv.org/abs/2102.02638v1,2021-02-02T18:50:06Z,2021-02-02T18:50:06Z,"Autodidactic Neurosurgeon: Collaborative Deep Inference for Mobile Edge
  Intelligence via Online Learning","Recent breakthroughs in deep learning (DL) have led to the emergence of many
intelligent mobile applications and services, but in the meanwhile also pose
unprecedented computing challenges on resource-constrained mobile devices. This
paper builds a collaborative deep inference system between a
resource-constrained mobile device and a powerful edge server, aiming at
joining the power of both on-device processing and computation offloading. The
basic idea of this system is to partition a deep neural network (DNN) into a
front-end part running on the mobile device and a back-end part running on the
edge server, with the key challenge being how to locate the optimal partition
point to minimize the end-to-end inference delay. Unlike existing efforts on
DNN partitioning that rely heavily on a dedicated offline profiling stage to
search for the optimal partition point, our system has a built-in online
learning module, called Autodidactic Neurosurgeon (ANS), to automatically learn
the optimal partition point on-the-fly. Therefore, ANS is able to closely
follow the changes of the system environment by generating new knowledge for
adaptive decision making. The core of ANS is a novel contextual bandit learning
algorithm, called $\mu$LinUCB, which not only has provable theoretical learning
performance guarantee but also is ultra-lightweight for easy real-world
implementation. We implement our system on a video stream object detection
testbed to validate the design of ANS and evaluate its performance. The
experiments show that ANS significantly outperforms state-of-the-art benchmarks
in terms of tracking system changes and reducing the end-to-end inference
delay.",arxiv
http://arxiv.org/abs/1808.00362v1,2018-08-01T15:13:48Z,2018-08-01T15:13:48Z,Deep Appearance Models for Face Rendering,"We introduce a deep appearance model for rendering the human face. Inspired
by Active Appearance Models, we develop a data-driven rendering pipeline that
learns a joint representation of facial geometry and appearance from a
multiview capture setup. Vertex positions and view-specific textures are
modeled using a deep variational autoencoder that captures complex nonlinear
effects while producing a smooth and compact latent representation.
View-specific texture enables the modeling of view-dependent effects such as
specularity. In addition, it can also correct for imperfect geometry stemming
from biased or low resolution estimates. This is a significant departure from
the traditional graphics pipeline, which requires highly accurate geometry as
well as all elements of the shading model to achieve realism through
physically-inspired light transport. Acquiring such a high level of accuracy is
difficult in practice, especially for complex and intricate parts of the face,
such as eyelashes and the oral cavity. These are handled naturally by our
approach, which does not rely on precise estimates of geometry. Instead, the
shading model accommodates deficiencies in geometry though the flexibility
afforded by the neural network employed. At inference time, we condition the
decoding network on the viewpoint of the camera in order to generate the
appropriate texture for rendering. The resulting system can be implemented
simply using existing rendering engines through dynamic textures with flat
lighting. This representation, together with a novel unsupervised technique for
mapping images to facial states, results in a system that is naturally suited
to real-time interactive settings such as Virtual Reality (VR).",arxiv
http://arxiv.org/abs/2010.13246v1,2020-10-25T23:01:13Z,2020-10-25T23:01:13Z,MixNet for Generalized Face Presentation Attack Detection,"The non-intrusive nature and high accuracy of face recognition algorithms
have led to their successful deployment across multiple applications ranging
from border access to mobile unlocking and digital payments. However, their
vulnerability against sophisticated and cost-effective presentation attack
mediums raises essential questions regarding its reliability. In the
literature, several presentation attack detection algorithms are presented;
however, they are still far behind from reality. The major problem with
existing work is the generalizability against multiple attacks both in the seen
and unseen setting. The algorithms which are useful for one kind of attack
(such as print) perform unsatisfactorily for another type of attack (such as
silicone masks). In this research, we have proposed a deep learning-based
network termed as \textit{MixNet} to detect presentation attacks in
cross-database and unseen attack settings. The proposed algorithm utilizes
state-of-the-art convolutional neural network architectures and learns the
feature mapping for each attack category. Experiments are performed using
multiple challenging face presentation attack databases such as SMAD and Spoof
In the Wild (SiW-M) databases. Extensive experiments and comparison with
existing state of the art algorithms show the effectiveness of the proposed
algorithm.",arxiv
http://arxiv.org/abs/1902.05577v2,2020-03-15T13:25:50Z,2019-02-14T19:43:10Z,"A Scalable Platform for Distributed Object Tracking across a Many-camera
  Network","Advances in deep neural networks (DNN) and computer vision (CV) algorithms
have made it feasible to extract meaningful insights from large-scale
deployments of urban cameras. Tracking an object of interest across the camera
network in near real-time is a canonical problem. However, current tracking
platforms have two key limitations: 1) They are monolithic, proprietary and
lack the ability to rapidly incorporate sophisticated tracking models; and 2)
They are less responsive to dynamism across wide-area computing resources that
include edge, fog and cloud abstractions. We address these gaps using Anveshak,
a runtime platform for composing and coordinating distributed tracking
applications. It provides a domain-specific dataflow programming model to
intuitively compose a tracking application, supporting contemporary CV advances
like query fusion and re-identification, and enabling dynamic scoping of the
camera network's search space to avoid wasted computation. We also offer
tunable batching and data-dropping strategies for dataflow blocks deployed on
distributed resources to respond to network and compute variability. These
balance the tracking accuracy, its real-time performance and the active
camera-set size. We illustrate the concise expressiveness of the programming
model for $4$ tracking applications. Our detailed experiments for a network of
1000 camera-feeds on modest resources exhibit the tunable scalability,
performance and quality trade-offs enabled by our dynamic tracking, batching
and dropping strategies.",arxiv
http://arxiv.org/abs/2104.13295v1,2021-04-27T16:08:49Z,2021-04-27T16:08:49Z,Metamorphic Detection of Repackaged Malware,"Machine learning-based malware detection systems are often vulnerable to
evasion attacks, in which a malware developer manipulates their malicious
software such that it is misclassified as benign. Such software hides some
properties of the real class or adopts some properties of a different class by
applying small perturbations. A special case of evasive malware hides by
repackaging a bonafide benign mobile app to contain malware in addition to the
original functionality of the app, thus retaining most of the benign properties
of the original app. We present a novel malware detection system based on
metamorphic testing principles that can detect such benign-seeming malware
apps. We apply metamorphic testing to the feature representation of the mobile
app rather than to the app itself. That is, the source input is the original
feature vector for the app and the derived input is that vector with selected
features removed. If the app was originally classified benign and is indeed
benign, the output for the source and derived inputs should be the same class,
i.e., benign, but if they differ, then the app is exposed as likely malware.
Malware apps originally classified as malware should retain that classification
since only features prevalent in benign apps are removed. This approach enables
the machine learning model to classify repackaged malware with reasonably few
false negatives and false positives. Our training pipeline is simpler than many
existing ML-based malware detection methods, as the network is trained
end-to-end to learn appropriate features and perform classification. We
pre-trained our classifier model on 3 million apps collected from the
widely-used AndroZoo dataset. We perform an extensive study on other publicly
available datasets to show our approach's effectiveness in detecting repackaged
malware with more than94% accuracy, 0.98 precision, 0.95 recall, and 0.96 F1
score.",arxiv
http://arxiv.org/abs/2102.08936v2,2021-04-02T20:06:57Z,2021-02-17T18:51:51Z,"Deep Learning Anomaly Detection for Cellular IoT with Applications in
  Smart Logistics","The number of connected Internet of Things (IoT) devices within
cyber-physical infrastructure systems grows at an increasing rate. This poses
significant device management and security challenges to current IoT networks.
Among several approaches to cope with these challenges, data-based methods
rooted in deep learning (DL) are receiving an increased interest. In this
paper, motivated by the upcoming surge of 5G IoT connectivity in industrial
environments, we propose to integrate a DL-based anomaly detection (AD) as a
service into the 3GPP mobile cellular IoT architecture. The proposed
architecture embeds autoencoder based anomaly detection modules both at the IoT
devices (ADM-EDGE) and in the mobile core network (ADM-FOG), thereby balancing
between the system responsiveness and accuracy. We design, integrate,
demonstrate and evaluate a testbed that implements the above service in a
real-world deployment integrated within the 3GPP Narrow-Band IoT (NB-IoT)
mobile operator network.",arxiv
http://arxiv.org/abs/1801.06827v3,2018-07-29T08:06:19Z,2018-01-21T14:26:03Z,Artificial Impostors for Location Privacy Preservation,"The progress of location-based services has led to serious concerns on
location privacy leakage. For effective and efficient location privacy
preservation (LPP), existing methods are still not fully competent. They are
often vulnerable under the identification attack with side information, or hard
to be implemented due to the high computational complexity. In this paper, we
pursue the high protection efficacy and low computational complexity
simultaneously. We propose a scalable LPP method based on the paradigm of
counterfeiting locations. To make fake locations extremely plausible, we forge
them through synthesizing artificial impostors (AIs). The AIs refer to the
synthesized traces which have similar semantic features to the actual traces,
and do not contain any target location. Two dedicated techniques are devised:
the sampling-based synthesis method and population-level semantic model. They
play significant roles in two critical steps of synthesizing AIs. We conduct
experiments on real datasets in two cities (Shanghai, China and Asturias,
Spain) to validate the high efficacy and scalability of the proposed method. In
these two datasets, the experimental results show that our method achieves the
preservation efficacy of $97.65\%$ and $96.12\%$, and its run time of building
the generators is only $230.47$ and $215.92$ seconds, respectively. This study
would give the research community new insights into improving the practicality
of the state-of-the-art LPP paradigm via counterfeiting locations.",arxiv
http://arxiv.org/abs/2107.01025v1,2021-06-29T18:11:41Z,2021-06-29T18:11:41Z,"Structure-aware reinforcement learning for node-overload protection in
  mobile edge computing","Mobile Edge Computing (MEC) refers to the concept of placing computational
capability and applications at the edge of the network, providing benefits such
as reduced latency in handling client requests, reduced network congestion, and
improved performance of applications. The performance and reliability of MEC
are degraded significantly when one or several edge servers in the cluster are
overloaded. Especially when a server crashes due to the overload, it causes
service failures in MEC. In this work, an adaptive admission control policy to
prevent edge node from getting overloaded is presented. This approach is based
on a recently-proposed low complexity RL (Reinforcement Learning) algorithm
called SALMUT (Structure-Aware Learning for Multiple Thresholds), which
exploits the structure of the optimal admission control policy in multi-class
queues for an average-cost setting. We extend the framework to work for node
overload-protection problem in a discounted-cost setting. The proposed solution
is validated using several scenarios mimicking real-world deployments in two
different settings - computer simulations and a docker testbed. Our empirical
evaluations show that the total discounted cost incurred by SALMUT is similar
to state-of-the-art deep RL algorithms such as PPO (Proximal Policy
Optimization) and A2C (Advantage Actor Critic) but requires an order of
magnitude less time to train, outputs easily interpretable policy, and can be
deployed in an online manner.",arxiv
http://arxiv.org/abs/2012.08015v2,2021-08-26T16:29:07Z,2020-12-15T00:09:37Z,Active Learning for Deep Gaussian Process Surrogates,"Deep Gaussian processes (DGPs) are increasingly popular as predictive models
in machine learning (ML) for their non-stationary flexibility and ability to
cope with abrupt regime changes in training data. Here we explore DGPs as
surrogates for computer simulation experiments whose response surfaces exhibit
similar characteristics. In particular, we transport a DGP's automatic warping
of the input space and full uncertainty quantification (UQ), via a novel
elliptical slice sampling (ESS) Bayesian posterior inferential scheme, through
to active learning (AL) strategies that distribute runs non-uniformly in the
input space -- something an ordinary (stationary) GP could not do. Building up
the design sequentially in this way allows smaller training sets, limiting both
expensive evaluation of the simulator code and mitigating cubic costs of DGP
inference. When training data sizes are kept small through careful acquisition,
and with parsimonious layout of latent layers, the framework can be both
effective and computationally tractable. Our methods are illustrated on
simulation data and two real computer experiments of varying input
dimensionality. We provide an open source implementation in the ""deepgp""
package on CRAN.",arxiv
http://arxiv.org/abs/2101.11800v1,2021-01-28T03:30:04Z,2021-01-28T03:30:04Z,"AdaSpring: Context-adaptive and Runtime-evolutionary Deep Model
  Compression for Mobile Applications","There are many deep learning (e.g., DNN) powered mobile and wearable
applications today continuously and unobtrusively sensing the ambient
surroundings to enhance all aspects of human lives. To enable robust and
private mobile sensing, DNN tends to be deployed locally on the
resource-constrained mobile devices via model compression. The current practice
either hand-crafted DNN compression techniques, i.e., for optimizing
DNN-relative performance (e.g., parameter size), or on-demand DNN compression
methods, i.e., for optimizing hardware-dependent metrics (e.g., latency),
cannot be locally online because they require offline retraining to ensure
accuracy. Also, none of them have correlated their efforts with runtime
adaptive compression to consider the dynamic nature of the deployment context
of mobile applications. To address those challenges, we present AdaSpring, a
context-adaptive and self-evolutionary DNN compression framework. It enables
the runtime adaptive DNN compression locally online. Specifically, it presents
the ensemble training of a retraining-free and self-evolutionary network to
integrate multiple alternative DNN compression configurations (i.e., compressed
architectures and weights). It then introduces the runtime search strategy to
quickly search for the most suitable compression configurations and evolve the
corresponding weights. With evaluation on five tasks across three platforms and
a real-world case study, experiment outcomes show that AdaSpring obtains up to
3.1x latency reduction, 4.2 x energy efficiency improvement in DNNs, compared
to hand-crafted compression techniques, while only incurring <= 6.2ms
runtime-evolution latency.",arxiv
http://arxiv.org/abs/1701.05130v1,2017-01-18T16:17:35Z,2017-01-18T16:17:35Z,"On the Performance of Network Parallel Training in Artificial Neural
  Networks","Artificial Neural Networks (ANNs) have received increasing attention in
recent years with applications that span a wide range of disciplines including
vital domains such as medicine, network security and autonomous transportation.
However, neural network architectures are becoming increasingly complex and
with an increasing need to obtain real-time results from such models, it has
become pivotal to use parallelization as a mechanism for speeding up network
training and deployment. In this work we propose an implementation of Network
Parallel Training through Cannon's Algorithm for matrix multiplication. We show
that increasing the number of processes speeds up training until the point
where process communication costs become prohibitive; this point varies by
network complexity. We also show through empirical efficiency calculations that
the speedup obtained is superlinear.",arxiv
http://arxiv.org/abs/1910.09806v3,2020-01-29T08:44:37Z,2019-10-22T07:51:27Z,"A low-power end-to-end hybrid neuromorphic framework for surveillance
  applications","With the success of deep learning, object recognition systems that can be
deployed for real-world applications are becoming commonplace. However,
inference that needs to largely take place on the `edge' (not processed on
servers), is a highly computational and memory intensive workload, making it
intractable for low-power mobile nodes and remote security applications. To
address this challenge, this paper proposes a low-power (5W) end-to-end
neuromorphic framework for object tracking and classification using event-based
cameras that possess desirable properties such as low power consumption (5-14
mW) and high dynamic range (120 dB). Nonetheless, unlike traditional approaches
of using event-by-event processing, this work uses a mixed frame and event
approach to get energy savings with high performance. Using a frame-based
region proposal method based on the density of foreground events, a
hardware-friendly object tracking is implemented using the apparent object
velocity while tackling occlusion scenarios. For low-power classification of
the tracked objects, the event camera is interfaced to IBM TrueNorth, which is
time-multiplexed to tackle up to eight instances for a traffic monitoring
application. The frame-based object track input is converted back to spikes for
Truenorth classification via the energy efficient deep network (EEDN) pipeline.
Using originally collected datasets, we train the TrueNorth model on the
hardware track outputs, instead of using ground truth object locations as
commonly done, and demonstrate the efficacy of our system to handle practical
surveillance scenarios. Finally, we compare the proposed methodologies to
state-of-the-art event-based systems for object tracking and classification,
and demonstrate the use case of our neuromorphic approach for low-power
applications without sacrificing on performance.",arxiv
http://arxiv.org/abs/1707.01662v1,2017-07-06T07:39:06Z,2017-07-06T07:39:06Z,An Embedded Deep Learning based Word Prediction,"Recent developments in deep learning with application to language modeling
have led to success in tasks of text processing, summarizing and machine
translation. However, deploying huge language models for mobile device such as
on-device keyboards poses computation as a bottle-neck due to their puny
computation capacities. In this work we propose an embedded deep learning based
word prediction method that optimizes run-time memory and also provides a real
time prediction environment. Our model size is 7.40MB and has average
prediction time of 6.47 ms. We improve over the existing methods for word
prediction in terms of key stroke savings and word prediction rate.",arxiv
http://arxiv.org/abs/1805.08695v1,2018-05-06T21:56:33Z,2018-05-06T21:56:33Z,"SqueezeJet: High-level Synthesis Accelerator Design for Deep
  Convolutional Neural Networks","Deep convolutional neural networks have dominated the pattern recognition
scene by providing much more accurate solutions in computer vision problems
such as object recognition and object detection. Most of these solutions come
at a huge computational cost, requiring billions of multiply-accumulate
operations and, thus, making their use quite challenging in real-time
applications that run on embedded mobile (resource-power constrained) hardware.
This work presents the architecture, the high-level synthesis design, and the
implementation of SqueezeJet, an FPGA accelerator for the inference phase of
the SqueezeNet DCNN architecture, which is designed specifically for use in
embedded systems. Results show that SqueezeJet can achieve 15.16 times speed-up
compared to the software implementation of SqueezeNet running on an embedded
mobile processor with less than 1% drop in top-5 accuracy.",arxiv
http://arxiv.org/abs/2101.04930v2,2021-02-10T15:12:25Z,2021-01-13T08:19:50Z,"An Empirical Study on Deployment Faults of Deep Learning Based Mobile
  Applications","Deep Learning (DL) is finding its way into a growing number of mobile
software applications. These software applications, named as DL based mobile
applications (abbreviated as mobile DL apps) integrate DL models trained using
large-scale data with DL programs. A DL program encodes the structure of a
desirable DL model and the process by which the model is trained using training
data. Due to the increasing dependency of current mobile apps on DL, software
engineering (SE) for mobile DL apps has become important. However, existing
efforts in SE research community mainly focus on the development of DL models
and extensively analyze faults in DL programs. In contrast, faults related to
the deployment of DL models on mobile devices (named as deployment faults of
mobile DL apps) have not been well studied. Since mobile DL apps have been used
by billions of end users daily for various purposes including for
safety-critical scenarios, characterizing their deployment faults is of
enormous importance. To fill the knowledge gap, this paper presents the first
comprehensive study on the deployment faults of mobile DL apps. We identify 304
real deployment faults from Stack Overflow and GitHub, two commonly used data
sources for studying software faults. Based on the identified faults, we
construct a fine-granularity taxonomy consisting of 23 categories regarding to
fault symptoms and distill common fix strategies for different fault types.
Furthermore, we suggest actionable implications and research avenues that could
further facilitate the deployment of DL models on mobile devices.",arxiv
http://arxiv.org/abs/2105.15041v1,2021-05-31T15:26:09Z,2021-05-31T15:26:09Z,"Scorpion detection and classification systems based on computer vision
  and deep learning for health security purposes","In this paper, two novel automatic and real-time systems for the detection
and classification of two genera of scorpions found in La Plata city
(Argentina) were developed using computer vision and deep learning techniques.
The object detection technique was implemented with two different methods, YOLO
(You Only Look Once) and MobileNet, based on the shape features of the
scorpions. High accuracy values of 88% and 91%, and high recall values of 90%
and 97%, have been achieved for both models, respectively, which guarantees
that they can successfully detect scorpions. In addition, the MobileNet method
has been shown to have excellent performance to detect scorpions within an
uncontrolled environment and to perform multiple detections. The MobileNet
model was also used for image classification in order to successfully
distinguish between dangerous scorpion (Tityus) and non-dangerous scorpion
(Bothriurus) with the purpose of providing a health security tool. Applications
for smartphones were developed, with the advantage of the portability of the
systems, which can be used as a help tool for emergency services, or for
biological research purposes. The developed systems can be easily scalable to
other genera and species of scorpions to extend the region where these
applications can be used.",arxiv
http://arxiv.org/abs/1910.07360v1,2019-10-16T14:11:24Z,2019-10-16T14:11:24Z,"Conservation AI: Live Stream Analysis for the Detection of Endangered
  Species Using Convolutional Neural Networks and Drone Technology","Many different species are adversely affected by poaching. In response to
this escalating crisis, efforts to stop poaching using hidden cameras, drones
and DNA tracking have been implemented with varying degrees of success. Limited
resources, costs and logistical limitations are often the cause of most
unsuccessful poaching interventions. The study presented in this paper outlines
a flexible and interoperable framework for the automatic detection of animals
and poaching activity to facilitate early intervention practices. Using a
robust deep learning pipeline, a convolutional neural network is trained and
implemented to detect rhinos and cars (considered an important tool in poaching
for fast access and artefact transportation in natural habitats) in the study,
that are found within live video streamed from drones Transfer learning with
the Faster RCNN Resnet 101 is performed to train a custom model with 350 images
of rhinos and 350 images of cars. Inference is performed using a frame sampling
technique to address the required trade-off control precision and processing
speed and maintain synchronisation with the live feed. Inference models are
hosted on a web platform using flask web serving, OpenCV and TensorFlow 1.13.
Video streams are transmitted from a DJI Mavic Pro 2 drone using the Real-Time
Messaging Protocol (RMTP). The best trained Faster RCNN model achieved a mAP of
0.83 @IOU 0.50 and 0.69 @IOU 0.75 respectively. In comparison an
SSD-mobilenetmodel trained under the same experimental conditions achieved a
mAP of 0.55 @IOU .50 and 0.27 @IOU 0.75.The results demonstrate that using a
FRCNN and off-the-shelf drones is a promising and scalable option for a range
of conservation projects.",arxiv
http://arxiv.org/abs/2011.11081v1,2020-11-22T18:30:23Z,2020-11-22T18:30:23Z,"Deep learning model trained on mobile phone-acquired frozen section
  images effectively detects basal cell carcinoma","Background: Margin assessment of basal cell carcinoma using the frozen
section is a common task of pathology intraoperative consultation. Although
frequently straight-forward, the determination of the presence or absence of
basal cell carcinoma on the tissue sections can sometimes be challenging. We
explore if a deep learning model trained on mobile phone-acquired frozen
section images can have adequate performance for future deployment. Materials
and Methods: One thousand two hundred and forty-one (1241) images of frozen
sections performed for basal cell carcinoma margin status were acquired using
mobile phones. The photos were taken at 100x magnification (10x objective). The
images were downscaled from a 4032 x 3024 pixel resolution to 576 x 432 pixel
resolution. Semantic segmentation algorithm Deeplab V3 with Xception backbone
was used for model training. Results: The model uses an image as input and
produces a 2-dimensional black and white output of prediction of the same
dimension; the areas determined to be basal cell carcinoma were displayed with
white color, in a black background. Any output with the number of white pixels
exceeding 0.5% of the total number of pixels is deemed positive for basal cell
carcinoma. On the test set, the model achieves area under curve of 0.99 for
receiver operator curve and 0.97 for precision-recall curve at the pixel level.
The accuracy of classification at the slide level is 96%. Conclusions: The deep
learning model trained with mobile phone images shows satisfactory performance
characteristics, and thus demonstrates the potential for deploying as a mobile
phone app to assist in frozen section interpretation in real time.",arxiv
http://arxiv.org/abs/2001.10632v1,2020-01-28T23:13:12Z,2020-01-28T23:13:12Z,IoT Behavioral Monitoring via Network Traffic Analysis,"Smart homes, enterprises, and cities are increasingly being equipped with a
plethora of Internet of Things (IoT), ranging from smart-lights to security
cameras. While IoT networks have the potential to benefit our lives, they
create privacy and security challenges not seen with traditional IT networks.
Due to the lack of visibility, operators of such smart environments are not
often aware of their IoT assets, let alone whether each IoT device is
functioning properly safe from cyber-attacks. This thesis is the culmination of
our efforts to develop techniques to profile the network behavioral pattern of
IoTs, automate IoT classification, deduce their operating context, and detect
anomalous behavior indicative of cyber-attacks.
  We begin this thesis by surveying IoT ecosystem, while reviewing current
approaches to vulnerability assessments, intrusion detection, and behavioral
monitoring. For our first contribution, we collect traffic traces and
characterize the network behavior of IoT devices via attributes from traffic
patterns. We develop a robust machine learning-based inference engine trained
with these attributes and demonstrate real-time classification of 28 IoT
devices with over 99% accuracy. Our second contribution enhances the
classification by reducing the cost of attribute extraction while also
identifying IoT device states. Prototype implementation and evaluation
demonstrate the ability of our supervised machine learning method to detect
behavioral changes for five IoT devices. Our third and final contribution
develops a modularized unsupervised inference engine that dynamically
accommodates the addition of new IoT devices and/or updates to existing ones,
without requiring system-wide retraining of the model. We demonstrate via
experiments that our model can automatically detect attacks and firmware
changes in ten IoT devices with over 94% accuracy.",arxiv
http://arxiv.org/abs/1907.00594v1,2019-07-01T08:01:26Z,2019-07-01T08:01:26Z,"Fingerprint-based Localization using Commercial LTE Signals: A
  Field-Trial Study","Wireless localization for mobile device has attracted more and more interests
by increasing the demand for location based services. Fingerprint-based
localization is promising, especially in non-Line-of-Sight (NLoS) or rich
scattering environments, such as urban areas and indoor scenarios. In this
paper, we propose a novel fingerprint-based localization technique based on
deep learning framework under commercial long term evolution (LTE) systems.
Specifically, we develop a software defined user equipment to collect the real
time channel state information (CSI) knowledge from LTE base stations and
extract the intrinsic features among CSI observations. On top of that, we
propose a time domain fusion approach to assemble multiple positioning
estimations. Experimental results demonstrated that the proposed localization
technique can significantly improve the localization accuracy and robustness,
e.g. achieves Mean Distance Error (MDE) of 0.47 meters for indoor and of 19.9
meters for outdoor scenarios, respectively.",arxiv
http://arxiv.org/abs/1808.01356v1,2018-07-31T10:33:09Z,2018-07-31T10:33:09Z,"Deep Learning-Based Multiple Object Visual Tracking on Embedded System
  for IoT and Mobile Edge Computing Applications","Compute and memory demands of state-of-the-art deep learning methods are
still a shortcoming that must be addressed to make them useful at IoT
end-nodes. In particular, recent results depict a hopeful prospect for image
processing using Convolutional Neural Netwoks, CNNs, but the gap between
software and hardware implementations is already considerable for IoT and
mobile edge computing applications due to their high power consumption. This
proposal performs low-power and real time deep learning-based multiple object
visual tracking implemented on an NVIDIA Jetson TX2 development kit. It
includes a camera and wireless connection capability and it is battery powered
for mobile and outdoor applications. A collection of representative sequences
captured with the on-board camera, dETRUSC video dataset, is used to exemplify
the performance of the proposed algorithm and to facilitate benchmarking. The
results in terms of power consumption and frame rate demonstrate the
feasibility of deep learning algorithms on embedded platforms although more
effort to joint algorithm and hardware design of CNNs is needed.",arxiv
http://arxiv.org/abs/1806.07761v3,2019-10-13T21:13:22Z,2018-06-20T14:28:04Z,"Quick and Plenty: Achieving Low Delay and High Rate in 802.11ac Edge
  Networks","We consider transport layer approaches for achieving high rate, low delay
communication over edge paths where the bottleneck is an 802.11ac WLAN. We
first show that by regulating send rate so as to maintain a target aggregation
level it is possible to realise high rate, low delay communication over
802.11ac WLANs. We then address two important practical issues arising in
production networks, namely that (i) many client devices are non-rooted mobile
handsets/tablets and (ii) the bottleneck may lie in the backhaul rather than
the WLAN, or indeed vary between the two over time. We show that both these
issues can be resolved by use of simple and robust machine learning techniques.
We present a prototype transport layer implementation of our low delay rate
allocation approach and use this to evaluate performance under real radio
conditions.",arxiv
http://arxiv.org/abs/1809.04966v1,2018-09-13T13:53:01Z,2018-09-13T13:53:01Z,"Real-Time Lightweight Chaotic Encryption for 5G IoT Enabled Lip-Reading
  Driven Secure Hearing-Aid","Existing audio-only hearing-aids are known to perform poorly in noisy
situations where overwhelming noise is present. Next-generation audio-visual
(lip-reading driven) hearing-aids stand as a major enabler to realise more
intelligible audio. However, high data rate, low latency, low computational
complexity, and privacy are some of the major bottlenecks to the successful
deployment of such advanced hearing aids. To address these challenges, we
envision an integration of 5G Cloud-Radio Access Network, Internet of Things
(IoT), and strong privacy algorithms to fully benefit from the possibilities
these technologies have to offer. The envisioned 5G IoT enabled secure
audio-visual (AV) hearing-aid transmits the encrypted compressed AV information
and receives encrypted enhanced reconstructed speech in real-time which fully
addresses cybersecurity attacks such as location privacy and eavesdropping. For
security implementation, a real-time lightweight AV encryption is utilized. For
speech enhancement, the received AV information in the cloud is used to filter
noisy audio using both deep learning and analytical acoustic modelling
(filtering based approach). To offload the computational complexity and
real-time optimization issues, the framework runs deep learning and big data
optimization processes in the background on the cloud. Specifically, in this
work, three key contributions are reported: (1) 5G IoT enabled secure
audio-visual hearing-aid framework that aims to achieve a round-trip latency up
to 5ms with 100 Mbps datarate (2) Real-time lightweight audio-visual encryption
(3) Lip-reading driven deep learning approach for speech enhancement in the
cloud. The critical analysis in terms of both speech enhancement and AV
encryption demonstrate the potential of the envisioned technology in acquiring
high-quality speech reconstruction and secure mobile AV hearing aid
communication.",arxiv
http://arxiv.org/abs/2106.15202v2,2021-11-21T08:43:15Z,2021-06-29T09:39:34Z,"Inconspicuous Adversarial Patches for Fooling Image Recognition Systems
  on Mobile Devices","Deep learning based image recognition systems have been widely deployed on
mobile devices in today's world. In recent studies, however, deep learning
models are shown vulnerable to adversarial examples. One variant of adversarial
examples, called adversarial patch, draws researchers' attention due to its
strong attack abilities. Though adversarial patches achieve high attack success
rates, they are easily being detected because of the visual inconsistency
between the patches and the original images. Besides, it usually requires a
large amount of data for adversarial patch generation in the literature, which
is computationally expensive and time-consuming. To tackle these challenges, we
propose an approach to generate inconspicuous adversarial patches with one
single image. In our approach, we first decide the patch locations basing on
the perceptual sensitivity of victim models, then produce adversarial patches
in a coarse-to-fine way by utilizing multiple-scale generators and
discriminators. The patches are encouraged to be consistent with the background
images with adversarial training while preserving strong attack abilities. Our
approach shows the strong attack abilities in white-box settings and the
excellent transferability in black-box settings through extensive experiments
on various models with different architectures and training methods. Compared
to other adversarial patches, our adversarial patches hold the most negligible
risks to be detected and can evade human observations, which is supported by
the illustrations of saliency maps and results of user evaluations. Lastly, we
show that our adversarial patches can be applied in the physical world.",arxiv
http://arxiv.org/abs/2002.10853v1,2020-02-25T13:36:15Z,2020-02-25T13:36:15Z,Learning Machines from Simulation to Real World,"Learning Machines is developing a flexible, cross-industry, advanced
analytics platform, targeted during stealth-stage at a limited number of
specific vertical applications. In this paper, we aim to integrate a general
machine system to learn a variant of tasks from simulation to real world. In
such a machine system, it involves real-time robot vision, sensor fusion, and
learning algorithms (reinforcement learning). To this end, we demonstrate the
general machine system on three fundamental tasks including obstacle avoidance,
foraging, and predator-prey robot. The proposed solutions are implemented on
Robobo robots with mobile device (smartphone with camera) as interface and
built-in infrared (IR) sensors. The agent is trained in a virtual environment.
In order to assess its performance, the learned agent is tested in the virtual
environment and reproduce the same results in a real environment. The results
show that the reinforcement learning algorithm can be reliably used for a
variety of tasks in unknown environments.",arxiv
http://arxiv.org/abs/1702.06329v1,2017-02-21T11:07:27Z,2017-02-21T11:07:27Z,"Towards a Common Implementation of Reinforcement Learning for Multiple
  Robotic Tasks","Mobile robots are increasingly being employed for performing complex tasks in
dynamic environments. Reinforcement learning (RL) methods are recognized to be
promising for specifying such tasks in a relatively simple manner. However, the
strong dependency between the learning method and the task to learn is a
well-known problem that restricts practical implementations of RL in robotics,
often requiring major modifications of parameters and adding other techniques
for each particular task. In this paper we present a practical core
implementation of RL which enables the learning process for multiple robotic
tasks with minimal per-task tuning or none. Based on value iteration methods,
this implementation includes a novel approach for action selection, called
Q-biased softmax regression (QBIASSR), which avoids poor performance of the
learning process when the robot reaches new unexplored states. Our approach
takes advantage of the structure of the state space by attending the physical
variables involved (e.g., distances to obstacles, X,Y,{\theta} pose, etc.),
thus experienced sets of states may favor the decision-making process of
unexplored or rarely-explored states. This improvement has a relevant role in
reducing the tuning of the algorithm for particular tasks. Experiments with
real and simulated robots, performed with the software framework also
introduced here, show that our implementation is effectively able to learn
different robotic tasks without tuning the learning method. Results also
suggest that the combination of true online SARSA({\lambda}) with QBIASSR can
outperform the existing RL core algorithms in low-dimensional robotic tasks.",arxiv
http://arxiv.org/abs/1804.10134v2,2018-07-28T10:33:47Z,2018-04-26T16:04:30Z,Detection-Tracking for Efficient Person Analysis: The DetTA Pipeline,"In the past decade many robots were deployed in the wild, and people
detection and tracking is an important component of such deployments. On top of
that, one often needs to run modules which analyze persons and extract higher
level attributes such as age and gender, or dynamic information like gaze and
pose. The latter ones are especially necessary for building a reactive, social
robot-person interaction.
  In this paper, we combine those components in a fully modular
detection-tracking-analysis pipeline, called DetTA. We investigate the benefits
of such an integration on the example of head and skeleton pose, by using the
consistent track ID for a temporal filtering of the analysis modules'
observations, showing a slight improvement in a challenging real-world
scenario. We also study the potential of a so-called ""free-flight"" mode, where
the analysis of a person attribute only relies on the filter's predictions for
certain frames. Here, our study shows that this boosts the runtime
dramatically, while the prediction quality remains stable. This insight is
especially important for reducing power consumption and sharing precious
(GPU-)memory when running many analysis components on a mobile platform,
especially so in the era of expensive deep learning methods.",arxiv
http://arxiv.org/abs/2006.04271v1,2020-06-07T21:18:36Z,2020-06-07T21:18:36Z,"Multi-Task Reinforcement Learning based Mobile Manipulation Control for
  Dynamic Object Tracking and Grasping","Agile control of mobile manipulator is challenging because of the high
complexity coupled by the robotic system and the unstructured working
environment. Tracking and grasping a dynamic object with a random trajectory is
even harder. In this paper, a multi-task reinforcement learning-based mobile
manipulation control framework is proposed to achieve general dynamic object
tracking and grasping. Several basic types of dynamic trajectories are chosen
as the task training set. To improve the policy generalization in practice,
random noise and dynamics randomization are introduced during the training
process. Extensive experiments show that our policy trained can adapt to unseen
random dynamic trajectories with about 0.1m tracking error and 75\% grasping
success rate of dynamic objects. The trained policy can also be successfully
deployed on a real mobile manipulator.",arxiv
http://arxiv.org/abs/2103.14225v1,2021-03-26T02:25:42Z,2021-03-26T02:25:42Z,SD-VEC: Software-Defined Vehicular Edge Computing with Ultra-Low Latency,"New paradigm shifts and 6G technological revolution in vehicular services
have emerged toward unmanned driving, automated transportation, and
self-driving vehicles. As the technology for autonomous vehicles becomes
mature, real challenges come from reliable, safe, real-time connected
transportation operations to achieve ubiquitous and prompt information
exchanges with massive connected and autonomous vehicles. This article aims at
introducing novel wireless distributed architectures that embed the edge
computing capability inside software-defined vehicular networking
infrastructure. Such edge networks consist of open-loop grant-free
communications and computing-based control frameworks, which enable dynamic
eco-routing with ultra-low latency and mobile data-driven orchestration. Thus,
this work advances the frontiers of machine learning potentials and
next-generation mobile system realization in vehicular networking applications.",arxiv
http://arxiv.org/abs/2108.00505v1,2021-08-01T17:33:04Z,2021-08-01T17:33:04Z,"DeepTrack: Lightweight Deep Learning for Vehicle Path Prediction in
  Highways","Vehicle trajectory prediction is an essential task for enabling many
intelligent transportation systems. While there have been some promising
advances in the field, there is a need for new agile algorithms with smaller
model sizes and lower computational requirements. This article presents
DeepTrack, a novel deep learning algorithm customized for real-time vehicle
trajectory prediction in highways. In contrast to previous methods, the vehicle
dynamics are encoded using Agile Temporal Convolutional Networks (ATCNs) to
provide more robust time prediction with less computation. ATCN also uses
depthwise convolution, which reduces the complexity of models compared to
existing approaches in terms of model size and operations. Overall, our
experimental results demonstrate that DeepTrack achieves comparable accuracy to
state-of-the-art trajectory prediction models but with smaller model sizes and
lower computational complexity, making it more suitable for real-world
deployment.",arxiv
http://arxiv.org/abs/1205.6910v1,2012-05-31T08:22:19Z,2012-05-31T08:22:19Z,"A New Architecture of a Ubiquitous Health Monitoring System: A Prototype
  Of Cloud Mobile Health Monitoring System","Wireless Body Area Sensor Networks (WBASN) is an emerging technology which
uses wireless sensors to implement real-time wearable health monitoring of
patients to enhance independent living. In this paper we propose a prototype of
cloud mobile health monitoring system. The system uses WBASN and Smartphone
application that uses cloud computing, location data and a neural network to
determine the state of patients.",arxiv
http://arxiv.org/abs/2009.14756v1,2020-09-30T15:43:44Z,2020-09-30T15:43:44Z,"A Plausibility-based Fault Detection Method for High-level Fusion
  Perception Systems","Trustworthy environment perception is the fundamental basis for the safe
deployment of automated agents such as self-driving vehicles or intelligent
robots. The problem remains that such trust is notoriously difficult to
guarantee in the presence of systematic faults, e.g. non-traceable errors
caused by machine learning functions. One way to tackle this issue without
making rather specific assumptions about the perception process is plausibility
checking. Similar to the reasoning of human intuition, the final outcome of a
complex black-box procedure is verified against given expectations of an
object's behavior. In this article, we apply and evaluate collaborative,
sensor-generic plausibility checking as a mean to detect empirical perception
faults from their statistical fingerprints. Our real use case is
next-generation automated driving that uses a roadside sensor infrastructure
for perception augmentation, represented here by test scenarios at a German
highway and a city intersection. The plausibilization analysis is integrated
naturally in the object fusion process, and helps to diagnose known and
possibly yet unknown faults in distributed sensing systems.",arxiv
http://arxiv.org/abs/2104.11146v1,2021-04-22T15:59:56Z,2021-04-22T15:59:56Z,"An Efficient One-Class SVM for Anomaly Detection in the Internet of
  Things","Insecure Internet of things (IoT) devices pose significant threats to
critical infrastructure and the Internet at large; detecting anomalous behavior
from these devices remains of critical importance, but fast, efficient,
accurate anomaly detection (also called ""novelty detection"") for these classes
of devices remains elusive. One-Class Support Vector Machines (OCSVM) are one
of the state-of-the-art approaches for novelty detection (or anomaly detection)
in machine learning, due to their flexibility in fitting complex nonlinear
boundaries between {normal} and {novel} data. IoT devices in smart homes and
cities and connected building infrastructure present a compelling use case for
novelty detection with OCSVM due to the variety of devices, traffic patterns,
and types of anomalies that can manifest in such environments. Much previous
research has thus applied OCSVM to novelty detection for IoT. Unfortunately,
conventional OCSVMs introduce significant memory requirements and are
computationally expensive at prediction time as the size of the train set
grows, requiring space and time that scales with the number of training points.
These memory and computational constraints can be prohibitive in practical,
real-world deployments, where large training sets are typically needed to
develop accurate models when fitting complex decision boundaries. In this work,
we extend so-called Nystr\""om and (Gaussian) Sketching approaches to OCSVM, by
combining these methods with clustering and Gaussian mixture models to achieve
significant speedups in prediction time and space in various IoT settings,
without sacrificing detection accuracy.",arxiv
http://arxiv.org/abs/2108.00332v2,2021-10-03T20:45:19Z,2021-07-31T22:24:40Z,"Edge Intelligence in Softwarized 6G: Deep Learning-enabled Network
  Traffic Predictions","The 6G vision is envisaged to enable agile network expansion and rapid
deployment of new on-demand microservices (e.g., visibility services for data
traffic management, mobile edge computing services) closer to the network's
edge IoT devices. However, providing one of the critical features of network
visibility services, i.e., data flow prediction in the network, is challenging
at the edge devices within a dynamic cloud-native environment as the traffic
flow characteristics are random and sporadic. To provide the AI-native services
for the 6G vision, we propose a novel edge-native framework to provide an
intelligent prognosis technique for data traffic management in this paper. The
prognosis model uses long short-term memory (LSTM)-based encoder-decoder deep
learning, which we train on real time-series multivariate data records
collected from the edge $\mu$-boxes of a selected testbed network. Our result
accurately predicts the statistical characteristics of data traffic and
verifies the trained model against the ground truth observations. Moreover, we
validate our novel framework with two performance metrics for each feature of
the multivariate data.",arxiv
http://arxiv.org/abs/1502.07402v2,2015-06-15T13:55:15Z,2015-02-25T23:40:19Z,"Real-time capable first principle based modelling of tokamak turbulent
  transport","A real-time capable core turbulence tokamak transport model is developed.
This model is constructed from the regularized nonlinear regression of
quasilinear gyrokinetic transport code output. The regression is performed with
a multilayer perceptron neural network. The transport code input for the neural
network training set consists of five dimensions, and is limited to adiabatic
electrons. The neural network model successfully reproduces transport fluxes
predicted by the original quasilinear model, while gaining five orders of
magnitude in computation time. The model is implemented in a real-time capable
tokamak simulator, and simulates a 300s ITER discharge in 10s. This
proof-of-principle for regression based transport models anticipates a
significant widening of input space dimensionality and physics realism for
future training sets. This aims to provide unprecedented computational speed
coupled with first-principle based physics for real-time control and integrated
modelling applications.",arxiv
http://arxiv.org/abs/2108.05962v1,2021-08-12T21:03:44Z,2021-08-12T21:03:44Z,DRQN-based 3D Obstacle Avoidance with a Limited Field of View,"In this paper, we propose a map-based end-to-end DRL approach for
three-dimensional (3D) obstacle avoidance in a partially observed environment,
which is applied to achieve autonomous navigation for an indoor mobile robot
using a depth camera with a narrow field of view. We first train a neural
network with LSTM units in a 3D simulator of mobile robots to approximate the
Q-value function in double DRQN. We also use a curriculum learning strategy to
accelerate and stabilize the training process. Then we deploy the trained model
to a real robot to perform 3D obstacle avoidance in its navigation. We evaluate
the proposed approach both in the simulated environment and on a robot in the
real world. The experimental results show that the approach is efficient and
easy to be deployed, and it performs well for 3D obstacle avoidance with a
narrow observation angle, which outperforms other existing DRL-based models by
15.5% on success rate.",arxiv
http://arxiv.org/abs/1805.12208v2,2020-06-01T16:02:17Z,2018-05-30T20:18:27Z,"Profiling presence patterns and segmenting user locations from cell
  phone data","The dynamic monitoring of commuting flows is crucial for improving transit
systems in fast-developing cities around the world. However, existing
methodology to infer commuting originations and destinations have to either
rely on large-scale survey data, which is inherently expensive to implement, or
on Call Detail Records but based on ad-hoc heuristic assignment rules based on
the frequency of appearance at given locations. In this paper, we proposed a
novel method to accurately infer the point of origin and destinations of
commuting flows based on individual's spatial-temporal patterns inferred from
Call Detail Records. Our project significantly improves the accuracy upon the
heuristic assignment rules popularly adopted in the literature. Starting with
the historical data of geo-temporal travel patterns for a panel of individuals,
we create, for each person-location, a vector of probability distribution
capturing the likelihood that the person will appear in that location for a
given the time of day. Stacked in this way, the matrix of historical
geo-temporal data enables us to apply Eigen-decomposition and use unsupervised
machine learning techniques to extract commonalities across locations for the
different groups of travelers, which ultimately allows us to make inferences
and create labels, such as home and work, on specific locations. Testing the
methodology on real-world data with known location labels shows that our method
identifies home and workplaces with significant accuracy, improving upon the
most commonly used methods in the literature by 79% and 34%, respectively. Most
importantly, our methodology does not bear any significant computation burden
and is easily scalable and easily expanded to other real-world data with
historical tracking.",arxiv
http://arxiv.org/abs/1908.00149v1,2019-07-31T23:53:50Z,2019-07-31T23:53:50Z,"Response time optimization for drone-delivered automated external
  defibrillators","Out-of-hospital cardiac arrest (OHCA) claims over 400,000 lives each year in
North America and is one of the most time-sensitive medical emergencies.
Drone-delivered automated external defibrillators (AEDs) have the potential to
be a transformative innovation in the provision of emergency care for OHCA. In
this paper, we propose a simulation-optimization framework to minimize the
total number of drones required to meet a pre-specified response time goal,
while guaranteeing a sufficient number of drones are located at each base. To
do this, we develop a location-queuing model that is based on the p-median
architecture, where each base constitutes an explicit M/M/d queue, and that
incorporates estimated baseline response times to the demand points. We then
develop a reformulation technique that exploits the baseline response times,
allowing us to solve real-world instances to optimality using an off-the-shelf
solver. To test our model, we develop a two-stage machine learning approach to
simulate both the locations and baseline response times for future OHCAs. We
demonstrate the application of our framework using eight years of real data
from an area covering 26,000 square kilometres around Toronto, Canada. A modest
number of drones are required to significantly reduce response times in all
regions. Furthermore, an objective function focused on improving the 90th
percentile is well-suited for use in practice because the model reduces the
entire response time distribution, while providing equitable coverage in both
cities and rural areas. Overall, this paper provides a realistic framework that
can be leveraged by healthcare providers seeking to implement a drone network.",arxiv
http://arxiv.org/abs/2010.06333v3,2021-04-13T13:44:33Z,2020-10-13T12:31:13Z,Capacitated spatial clustering with multiple constraints and attributes,"Capacitated spatial clustering, a type of unsupervised machine learning
method, is often used to tackle problems in compressing, classifying, logistic
optimization and infrastructure optimization. Depending on the application at
hand, a wide set of extensions may be necessary in clustering.
  In this article we propose a number of novel extensions to PACK that is a
novel capacitated spatial clustering method. These extensions are relocation
and location preference of cluster centers, outliers, and non-spatial
attributes. The strength of PACK is that it can consider all of these
extensions jointly. We demonstrate the usefulness PACK with a real world
example in edge computing server placement for a city region with various
different set ups, where we take into consideration outliers, center placement,
and non-spatial attributes. Different setups are evaluated with summary
statistics on spatial proximity and attribute similarity. As a result, the
similarity of the clusters was improved at best by 53%, while simultaneously
the proximity degraded only 18%. In alternate scenarios, both proximity and
similarity were improved. The different extensions proved to provide a valuable
way to include non-spatial information into the cluster analysis, and attain
better overall proximity and similarity. Furthermore, we provide easy-to-use
software tools (rpack) for conducting clustering analyses.",arxiv
http://arxiv.org/abs/1904.01576v2,2019-04-11T16:00:14Z,2019-04-02T01:46:38Z,"BARISTA: Efficient and Scalable Serverless Serving System for Deep
  Learning Prediction Services","Pre-trained deep learning models are increasingly being used to offer a
variety of compute-intensive predictive analytics services such as fitness
tracking, speech and image recognition. The stateless and highly parallelizable
nature of deep learning models makes them well-suited for serverless computing
paradigm. However, making effective resource management decisions for these
services is a hard problem due to the dynamic workloads and diverse set of
available resource configurations that have their deployment and management
costs. To address these challenges, we present a distributed and scalable
deep-learning prediction serving system called Barista and make the following
contributions. First, we present a fast and effective methodology for
forecasting workloads by identifying various trends. Second, we formulate an
optimization problem to minimize the total cost incurred while ensuring bounded
prediction latency with reasonable accuracy. Third, we propose an efficient
heuristic to identify suitable compute resource configurations. Fourth, we
propose an intelligent agent to allocate and manage the compute resources by
horizontal and vertical scaling to maintain the required prediction latency.
Finally, using representative real-world workloads for urban transportation
service, we demonstrate and validate the capabilities of Barista.",arxiv
http://arxiv.org/abs/1712.02427v1,2017-12-06T22:27:36Z,2017-12-06T22:27:36Z,High performance ultra-low-precision convolutions on mobile devices,"Many applications of mobile deep learning, especially real-time computer
vision workloads, are constrained by computation power. This is particularly
true for workloads running on older consumer phones, where a typical device
might be powered by a single- or dual-core ARMv7 CPU. We provide an open-source
implementation and a comprehensive analysis of (to our knowledge) the state of
the art ultra-low-precision (<4 bit precision) implementation of the core
primitives required for modern deep learning workloads on ARMv7 devices, and
demonstrate speedups of 4x-20x over our additional state-of-the-art float32 and
int8 baselines.",arxiv
http://arxiv.org/abs/1308.3015v1,2013-08-14T02:30:40Z,2013-08-14T02:30:40Z,"On Generalized Bayesian Data Fusion with Complex Models in Large Scale
  Networks","Recent advances in communications, mobile computing, and artificial
intelligence have greatly expanded the application space of intelligent
distributed sensor networks. This in turn motivates the development of
generalized Bayesian decentralized data fusion (DDF) algorithms for robust and
efficient information sharing among autonomous agents using probabilistic
belief models. However, DDF is significantly challenging to implement for
general real-world applications requiring the use of dynamic/ad hoc network
topologies and complex belief models, such as Gaussian mixtures or hybrid
Bayesian networks. To tackle these issues, we first discuss some new key
mathematical insights about exact DDF and conservative approximations to DDF.
These insights are then used to develop novel generalized DDF algorithms for
complex beliefs based on mixture pdfs and conditional factors. Numerical
examples motivated by multi-robot target search demonstrate that our methods
lead to significantly better fusion results, and thus have great potential to
enhance distributed intelligent reasoning in sensor networks.",arxiv
http://arxiv.org/abs/1811.07738v3,2019-04-23T07:51:28Z,2018-11-19T14:51:56Z,"M2U-Net: Effective and Efficient Retinal Vessel Segmentation for
  Resource-Constrained Environments","In this paper, we present a novel neural network architecture for retinal
vessel segmentation that improves over the state of the art on two benchmark
datasets, is the first to run in real time on high resolution images, and its
small memory and processing requirements make it deployable in mobile and
embedded systems. The M2U-Net has a new encoder-decoder architecture that is
inspired by the U-Net. It adds pretrained components of MobileNetV2 in the
encoder part and novel contractive bottleneck blocks in the decoder part that,
combined with bilinear upsampling, drastically reduce the parameter count to
0.55M compared to 31.03M in the original U-Net. We have evaluated its
performance against a wide body of previously published results on three public
datasets. On two of them, the M2U-Net achieves new state-of-the-art performance
by a considerable margin. When implemented on a GPU, our method is the first to
achieve real-time inference speeds on high-resolution fundus images. We also
implemented our proposed network on an ARM-based embedded system where it
segments images in between 0.6 and 15 sec, depending on the resolution. Thus,
the M2U-Net enables a number of applications of retinal vessel structure
extraction, such as early diagnosis of eye diseases, retinal biometric
authentication systems, and robot assisted microsurgery.",arxiv
http://arxiv.org/abs/1906.07052v1,2019-06-17T14:27:24Z,2019-06-17T14:27:24Z,Towards Real-Time Action Recognition on Mobile Devices Using Deep Models,"Action recognition is a vital task in computer vision, and many methods are
developed to push it to the limit. However, current action recognition models
have huge computational costs, which cannot be deployed to real-world tasks on
mobile devices. In this paper, we first illustrate the setting of real-time
action recognition, which is different from current action recognition
inference settings. Under the new inference setting, we investigate
state-of-the-art action recognition models on the Kinetics dataset empirically.
Our results show that designing efficient real-time action recognition models
is different from designing efficient ImageNet models, especially in weight
initialization. We show that pre-trained weights on ImageNet improve the
accuracy under the real-time action recognition setting. Finally, we use the
hand gesture recognition task as a case study to evaluate our compact real-time
action recognition models in real-world applications on mobile phones. Results
show that our action recognition models, being 6x faster and with similar
accuracy as state-of-the-art, can roughly meet the real-time requirements on
mobile devices. To our best knowledge, this is the first paper that deploys
current deep learning action recognition models on mobile devices.",arxiv
http://arxiv.org/abs/2001.01043v2,2020-04-06T03:26:24Z,2020-01-04T06:05:23Z,"SurveilEdge: Real-time Video Query based on Collaborative Cloud-Edge
  Deep Learning","The real-time query of massive surveillance video data plays a fundamental
role in various smart urban applications such as public safety and intelligent
transportation. Traditional cloud-based approaches are not applicable because
of high transmission latency and prohibitive bandwidth cost, while edge devices
are often incapable of executing complex vision algorithms with low latency and
high accuracy due to restricted resources. Given the infeasibility of both
cloud-only and edge-only solutions, we present SurveilEdge, a collaborative
cloud-edge system for real-time queries of large-scale surveillance video
streams. Specifically, we design a convolutional neural network (CNN) training
scheme to reduce the training time with high accuracy, and an intelligent task
allocator to balance the load among different computing nodes and to achieve
the latency-accuracy tradeoff for real-time queries. We implement SurveilEdge
on a prototype with multiple edge devices and a public Cloud, and conduct
extensive experiments using realworld surveillance video datasets. Evaluation
results demonstrate that SurveilEdge manages to achieve up to 7x less bandwidth
cost and 5.4x faster query response time than the cloud-only solution; and can
improve query accuracy by up to 43.9% and achieve 15.8x speedup respectively,
in comparison with edge-only approaches.",arxiv
http://arxiv.org/abs/1807.08931v1,2018-07-24T07:16:54Z,2018-07-24T07:16:54Z,"CReaM: Condensed Real-time Models for Depth Prediction using
  Convolutional Neural Networks","Since the resurgence of CNNs the robotic vision community has developed a
range of algorithms that perform classification, semantic segmentation and
structure prediction (depths, normals, surface curvature) using neural
networks. While some of these models achieve state-of-the art results and super
human level performance, deploying these models in a time critical robotic
environment remains an ongoing challenge. Real-time frameworks are of paramount
importance to build a robotic society where humans and robots integrate
seamlessly. To this end, we present a novel real-time structure prediction
framework that predicts depth at 30fps on an NVIDIA-TX2. At the time of
writing, this is the first piece of work to showcase such a capability on a
mobile platform. We also demonstrate with extensive experiments that neural
networks with very large model capacities can be leveraged in order to train
accurate condensed model architectures in a ""from teacher to student"" style
knowledge transfer.",arxiv
http://arxiv.org/abs/2101.03944v1,2021-01-11T15:08:07Z,2021-01-11T15:08:07Z,"Impact of Interventional Policies Including Vaccine on Covid-19
  Propagation and Socio-Economic Factors","A novel coronavirus disease has emerged (later named COVID-19) and caused the
world to enter a new reality, with many direct and indirect factors influencing
it. Some are human-controllable (e.g. interventional policies, mobility and the
vaccine); some are not (e.g. the weather). We have sought to test how a change
in these human-controllable factors might influence two measures: the number of
daily cases against economic impact. If applied at the right level and with
up-to-date data to measure, policymakers would be able to make targeted
interventions and measure their cost. This study aims to provide a predictive
analytics framework to model, predict and simulate COVID-19 propagation and the
socio-economic impact of interventions intended to reduce the spread of the
disease such as policy and/or vaccine. It allows policymakers, government
representatives and business leaders to make better-informed decisions about
the potential effect of various interventions with forward-looking views via
scenario planning. We have leveraged a recently launched open-source COVID-19
big data platform and used published research to find potentially relevant
variables (features) and leveraged in-depth data quality checks and analytics
for feature selection and predictions. An advanced machine learning pipeline
has been developed armed with a self-evolving model, deployed on a modern
machine learning architecture. It has high accuracy for trend prediction
(back-tested with r-squared) and is augmented with interpretability for deeper
insights.",arxiv
http://arxiv.org/abs/1612.06259v1,2016-12-19T16:41:24Z,2016-12-19T16:41:24Z,"Photo-Quality Evaluation based on Computational Aesthetics: Review of
  Feature Extraction Techniques","Researchers try to model the aesthetic quality of photographs into low and
high- level features, drawing inspiration from art theory, psychology and
marketing. We attempt to describe every feature extraction measure employed in
the above process. The contribution of this literature review is the taxonomy
of each feature by its implementation complexity, considering real-world
applications and integration in mobile apps and digital cameras. Also, we
discuss the machine learning results along with some unexplored research areas
as future work.",arxiv
http://arxiv.org/abs/1803.09492v1,2018-03-26T09:49:03Z,2018-03-26T09:49:03Z,"Latency and Throughput Characterization of Convolutional Neural Networks
  for Mobile Computer Vision","We study performance characteristics of convolutional neural networks (CNN)
for mobile computer vision systems. CNNs have proven to be a powerful and
efficient approach to implement such systems. However, the system performance
depends largely on the utilization of hardware accelerators, which are able to
speed up the execution of the underlying mathematical operations tremendously
through massive parallelism. Our contribution is performance characterization
of multiple CNN-based models for object recognition and detection with several
different hardware platforms and software frameworks, using both local
(on-device) and remote (network-side server) computation. The measurements are
conducted using real workloads and real processing platforms. On the platform
side, we concentrate especially on TensorFlow and TensorRT. Our measurements
include embedded processors found on mobile devices and high-performance
processors that can be used on the network side of mobile systems. We show that
there exists significant latency--throughput trade-offs but the behavior is
very complex. We demonstrate and discuss several factors that affect the
performance and yield this complex behavior.",arxiv
http://arxiv.org/abs/2108.02804v1,2021-08-05T18:40:23Z,2021-08-05T18:40:23Z,"Configuring Antenna System to Enhance the Downlink Performance of High
  Velocity Users in 5G MU-MIMO Networks","An exponential increase in the data rate demand prompted several technical
innovations. Multi User Multiple Input Multiple Output (MU-MIMO) is one of the
most promising schemes. This has been evolved into Massive MIMO technology in
5G to further stretch the network throughput. Massive MIMO tackles the rising
data rate with the increase in the number of antenna. This comes at the price
of a higher energy consumption. Moreover the high velocity users in MU-MIMO
scheme experiences a frequent unpredictable change in the channel condition
that degrade its downlink performance. Therefore a proper number of antenna
selection is of paramount importance. This issue has been addressed using
machine learning techniques and Channel State Information (CSI) but only for
static users. In this study we propose to introduce antenna diversity in
spatial multiplexing MU-MIMO transmission scheme by operating more number of
reception antenna compare to the number of transmission antenna. The diversity
improves the downlink performance of high velocity users. In general our
results can be interpreted for large scale antenna systems like Massive MIMO.
The proposed method can be easily implemented in the existing network
architectures with minimal complexity. Also it has the potential for solving
real-life problems like call drops and low data rate to be experienced by
cellular users traveling through high-speed transportation systems like Dhaka
MRT project",arxiv
http://arxiv.org/abs/1808.04752v2,2018-12-16T08:26:57Z,2018-08-13T14:11:43Z,A Survey on Methods and Theories of Quantized Neural Networks,"Deep neural networks are the state-of-the-art methods for many real-world
tasks, such as computer vision, natural language processing and speech
recognition. For all its popularity, deep neural networks are also criticized
for consuming a lot of memory and draining battery life of devices during
training and inference. This makes it hard to deploy these models on mobile or
embedded devices which have tight resource constraints. Quantization is
recognized as one of the most effective approaches to satisfy the extreme
memory requirements that deep neural network models demand. Instead of adopting
32-bit floating point format to represent weights, quantized representations
store weights using more compact formats such as integers or even binary
numbers. Despite a possible degradation in predictive performance, quantization
provides a potential solution to greatly reduce the model size and the energy
consumption. In this survey, we give a thorough review of different aspects of
quantized neural networks. Current challenges and trends of quantized neural
networks are also discussed.",arxiv
http://arxiv.org/abs/2001.09452v1,2020-01-26T13:14:04Z,2020-01-26T13:14:04Z,"Towards Cooperative Data Rate Prediction for Future Mobile and Vehicular
  6G Networks","Machine learning-based data rate prediction is one of the key drivers for
anticipatory mobile networking with applications such as dynamic Radio Access
Technology (RAT) selection, opportunistic data transfer, and predictive
caching. User Equipment (UE)-based prediction approaches that rely on passive
measurements of network quality indicators have successfully been applied to
forecast the throughput of vehicular data transmissions. However, the
achievable prediction accuracy is limited as the UE is unaware of the current
network load. To overcome this issue, we propose a cooperative data rate
prediction approach which brings together knowledge from the client and network
domains. In a real world proof-of-concept evaluation, we utilize the Software
Defined Radio (SDR)-based control channel sniffer FALCON in order to mimic the
behavior of a possible network-assisted information provisioning within future
6G networks. The results show that the proposed cooperative prediction approach
is able to reduce the average prediction error by up to 30%. With respect to
the ongoing standardization efforts regarding the implementation of
intelligence for network management, we argue that future 6G networks should go
beyond network-focused approaches and actively provide load information to the
UEs in order to fuel pervasive machine learning and catalyze UE-based network
optimization techniques.",arxiv
http://arxiv.org/abs/2007.03639v3,2021-01-11T11:02:34Z,2020-07-07T17:19:56Z,Human Trajectory Forecasting in Crowds: A Deep Learning Perspective,"Since the past few decades, human trajectory forecasting has been a field of
active research owing to its numerous real-world applications: evacuation
situation analysis, deployment of intelligent transport systems, traffic
operations, to name a few. Early works handcrafted this representation based on
domain knowledge. However, social interactions in crowded environments are not
only diverse but often subtle. Recently, deep learning methods have
outperformed their handcrafted counterparts, as they learned about human-human
interactions in a more generic data-driven fashion. In this work, we present an
in-depth analysis of existing deep learning-based methods for modelling social
interactions. We propose two knowledge-based data-driven methods to effectively
capture these social interactions. To objectively compare the performance of
these interaction-based forecasting models, we develop a large scale
interaction-centric benchmark TrajNet++, a significant yet missing component in
the field of human trajectory forecasting. We propose novel performance metrics
that evaluate the ability of a model to output socially acceptable
trajectories. Experiments on TrajNet++ validate the need for our proposed
metrics, and our method outperforms competitive baselines on both real-world
and synthetic datasets.",arxiv
http://arxiv.org/abs/2109.08710v1,2021-09-17T18:31:31Z,2021-09-17T18:31:31Z,On-device neural speech synthesis,"Recent advances in text-to-speech (TTS) synthesis, such as Tacotron and
WaveRNN, have made it possible to construct a fully neural network based TTS
system, by coupling the two components together. Such a system is conceptually
simple as it only takes grapheme or phoneme input, uses Mel-spectrogram as an
intermediate feature, and directly generates speech samples. The system
achieves quality equal or close to natural speech. However, the high
computational cost of the system and issues with robustness have limited their
usage in real-world speech synthesis applications and products. In this paper,
we present key modeling improvements and optimization strategies that enable
deploying these models, not only on GPU servers, but also on mobile devices.
The proposed system can generate high-quality 24 kHz speech at 5x faster than
real time on server and 3x faster than real time on mobile devices.",arxiv
http://arxiv.org/abs/2111.01616v1,2021-11-02T14:25:58Z,2021-11-02T14:25:58Z,OnSlicing: Online End-to-End Network Slicing with Reinforcement Learning,"Network slicing allows mobile network operators to virtualize infrastructures
and provide customized slices for supporting various use cases with
heterogeneous requirements. Online deep reinforcement learning (DRL) has shown
promising potential in solving network problems and eliminating the
simulation-to-reality discrepancy. Optimizing cross-domain resources with
online DRL is, however, challenging, as the random exploration of DRL violates
the service level agreement (SLA) of slices and resource constraints of
infrastructures. In this paper, we propose OnSlicing, an online end-to-end
network slicing system, to achieve minimal resource usage while satisfying
slices' SLA. OnSlicing allows individualized learning for each slice and
maintains its SLA by using a novel constraint-aware policy update method and
proactive baseline switching mechanism. OnSlicing complies with resource
constraints of infrastructures by using a unique design of action modification
in slices and parameter coordination in infrastructures. OnSlicing further
mitigates the poor performance of online learning during the early learning
stage by offline imitating a rule-based solution. Besides, we design four new
domain managers to enable dynamic resource configuration in radio access,
transport, core, and edge networks, respectively, at a timescale of subseconds.
We implement OnSlicing on an end-to-end slicing testbed designed based on
OpenAirInterface with both 4G LTE and 5G NR, OpenDayLight SDN platform, and
OpenAir-CN core network. The experimental results show that OnSlicing achieves
61.3% usage reduction as compared to the rule-based solution and maintains
nearly zero violation (0.06%) throughout the online learning phase. As online
learning is converged, OnSlicing reduces 12.5% usage without any violations as
compared to the state-of-the-art online DRL solution.",arxiv
http://arxiv.org/abs/2003.02838v1,2020-03-05T21:34:22Z,2020-03-05T21:34:22Z,Accelerator-aware Neural Network Design using AutoML,"While neural network hardware accelerators provide a substantial amount of
raw compute throughput, the models deployed on them must be co-designed for the
underlying hardware architecture to obtain the optimal system performance. We
present a class of computer vision models designed using hardware-aware neural
architecture search and customized to run on the Edge TPU, Google's neural
network hardware accelerator for low-power, edge devices. For the Edge TPU in
Coral devices, these models enable real-time image classification performance
while achieving accuracy typically seen only with larger, compute-heavy models
running in data centers. On Pixel 4's Edge TPU, these models improve the
accuracy-latency tradeoff over existing SoTA mobile models.",arxiv
http://arxiv.org/abs/2104.05485v2,2021-10-13T15:59:39Z,2021-04-12T14:10:25Z,"Predicting Pedestrian Crossing Intention with Feature Fusion and
  Spatio-Temporal Attention","Predicting vulnerable road user behavior is an essential prerequisite for
deploying Automated Driving Systems (ADS) in the real-world. Pedestrian
crossing intention should be recognized in real-time, especially for urban
driving. Recent works have shown the potential of using vision-based deep
neural network models for this task. However, these models are not robust and
certain issues still need to be resolved. First, the global spatio-temproal
context that accounts for the interaction between the target pedestrian and the
scene has not been properly utilized. Second, the optimum strategy for fusing
different sensor data has not been thoroughly investigated. This work addresses
the above limitations by introducing a novel neural network architecture to
fuse inherently different spatio-temporal features for pedestrian crossing
intention prediction. We fuse different phenomena such as sequences of RGB
imagery, semantic segmentation masks, and ego-vehicle speed in an optimum way
using attention mechanisms and a stack of recurrent neural networks. The
optimum architecture was obtained through exhaustive ablation and comparison
studies. Extensive comparative experiments on the JAAD pedestrian action
prediction benchmark demonstrate the effectiveness of the proposed method,
where state-of-the-art performance was achieved. Our code is open-source and
publicly available.",arxiv
http://arxiv.org/abs/2108.11033v1,2021-08-25T03:50:46Z,2021-08-25T03:50:46Z,"GRIM: A General, Real-Time Deep Learning Inference Framework for Mobile
  Devices based on Fine-Grained Structured Weight Sparsity","It is appealing but challenging to achieve real-time deep neural network
(DNN) inference on mobile devices because even the powerful modern mobile
devices are considered as ``resource-constrained'' when executing large-scale
DNNs. It necessitates the sparse model inference via weight pruning, i.e., DNN
weight sparsity, and it is desirable to design a new DNN weight sparsity scheme
that can facilitate real-time inference on mobile devices while preserving a
high sparse model accuracy. This paper designs a novel mobile inference
acceleration framework GRIM that is General to both convolutional neural
networks (CNNs) and recurrent neural networks (RNNs) and that achieves
Real-time execution and high accuracy, leveraging fine-grained structured
sparse model Inference and compiler optimizations for Mobiles. We start by
proposing a new fine-grained structured sparsity scheme through the Block-based
Column-Row (BCR) pruning. Based on this new fine-grained structured sparsity,
our GRIM framework consists of two parts: (a) the compiler optimization and
code generation for real-time mobile inference; and (b) the BCR pruning
optimizations for determining pruning hyperparameters and performing weight
pruning. We compare GRIM with Alibaba MNN, TVM, TensorFlow-Lite, a sparse
implementation based on CSR, PatDNN, and ESE (a representative FPGA inference
acceleration framework for RNNs), and achieve up to 14.08x speedup.",arxiv
http://arxiv.org/abs/2004.13094v1,2020-04-27T18:54:36Z,2020-04-27T18:54:36Z,Compact retail shelf segmentation for mobile deployment,"The recent surge of automation in the retail industries has rapidly increased
demand for applying deep learning models on mobile devices. To make the deep
learning models real-time on-device, a compact efficient network becomes
inevitable. In this paper, we work on one such common problem in the retail
industries - Shelf segmentation. Shelf segmentation can be interpreted as a
pixel-wise classification problem, i.e., each pixel is classified as to whether
they belong to visible shelf edges or not. The aim is not just to segment shelf
edges, but also to deploy the model on mobile devices. As there is no standard
solution for such dense classification problem on mobile devices, we look at
semantic segmentation architectures which can be deployed on edge. We modify
low-footprint semantic segmentation architectures to perform shelf
segmentation. In addressing this issue, we modified the famous U-net
architecture in certain aspects to make it fit for on-devices without impacting
significant drop in accuracy and also with 15X fewer parameters. In this paper,
we proposed Light Weight Segmentation Network (LWSNet), a small compact model
able to run fast on devices with limited memory and can train with less amount
(~ 100 images) of labeled data.",arxiv
http://arxiv.org/abs/2101.07996v1,2021-01-20T06:47:41Z,2021-01-20T06:47:41Z,SplitSR: An End-to-End Approach to Super-Resolution on Mobile Devices,"Super-resolution (SR) is a coveted image processing technique for mobile apps
ranging from the basic camera apps to mobile health. Existing SR algorithms
rely on deep learning models with significant memory requirements, so they have
yet to be deployed on mobile devices and instead operate in the cloud to
achieve feasible inference time. This shortcoming prevents existing SR methods
from being used in applications that require near real-time latency. In this
work, we demonstrate state-of-the-art latency and accuracy for on-device
super-resolution using a novel hybrid architecture called SplitSR and a novel
lightweight residual block called SplitSRBlock. The SplitSRBlock supports
channel-splitting, allowing the residual blocks to retain spatial information
while reducing the computation in the channel dimension. SplitSR has a hybrid
design consisting of standard convolutional blocks and lightweight residual
blocks, allowing people to tune SplitSR for their computational budget. We
evaluate our system on a low-end ARM CPU, demonstrating both higher accuracy
and up to 5 times faster inference than previous approaches. We then deploy our
model onto a smartphone in an app called ZoomSR to demonstrate the first-ever
instance of on-device, deep learning-based SR. We conducted a user study with
15 participants to have them assess the perceived quality of images that were
post-processed by SplitSR. Relative to bilinear interpolation -- the existing
standard for on-device SR -- participants showed a statistically significant
preference when looking at both images (Z=-9.270, p<0.01) and text (Z=-6.486,
p<0.01).",arxiv
http://arxiv.org/abs/1806.01248v2,2018-06-08T22:01:12Z,2018-06-04T17:39:58Z,"Dynamically Hierarchy Revolution: DirNet for Compressing Recurrent
  Neural Network on Mobile Devices","Recurrent neural networks (RNNs) achieve cutting-edge performance on a
variety of problems. However, due to their high computational and memory
demands, deploying RNNs on resource constrained mobile devices is a challenging
task. To guarantee minimum accuracy loss with higher compression rate and
driven by the mobile resource requirement, we introduce a novel model
compression approach DirNet based on an optimized fast dictionary learning
algorithm, which 1) dynamically mines the dictionary atoms of the projection
dictionary matrix within layer to adjust the compression rate 2) adaptively
changes the sparsity of sparse codes cross the hierarchical layers.
Experimental results on language model and an ASR model trained with a 1000h
speech dataset demonstrate that our method significantly outperforms prior
approaches. Evaluated on off-the-shelf mobile devices, we are able to reduce
the size of original model by eight times with real-time model inference and
negligible accuracy loss.",arxiv
http://arxiv.org/abs/2004.05828v2,2020-04-14T03:48:28Z,2020-04-13T09:00:26Z,"Hybrid Attention Networks for Flow and Pressure Forecasting in Water
  Distribution Systems","Multivariate geo-sensory time series prediction is challenging because of the
complex spatial and temporal correlation. In urban water distribution systems
(WDS), numerous spatial-correlated sensors have been deployed to continuously
collect hydraulic data. Forecasts of monitored flow and pressure time series
are of vital importance for operational decision making, alerts and anomaly
detection. To address this issue, we proposed a hybrid dual-stage
spatial-temporal attention-based recurrent neural networks (hDS-RNN). Our model
consists of two stages: a spatial attention-based encoder and a temporal
attention-based decoder. Specifically, a hybrid spatial attention mechanism
that employs inputs along temporal and spatial axes is proposed. Experiments on
a real-world dataset are conducted and demonstrate that our model outperformed
9 baseline models in flow and pressure series prediction in WDS.",arxiv
http://arxiv.org/abs/1908.11319v2,2019-08-30T01:08:54Z,2019-08-29T16:04:38Z,"Machine Learning and the Internet of Things Enable Steam Flood
  Optimization for Improved Oil Production","Recently developed machine learning techniques, in association with the
Internet of Things (IoT) allow for the implementation of a method of increasing
oil production from heavy-oil wells. Steam flood injection, a widely used
enhanced oil recovery technique, uses thermal and gravitational potential to
mobilize and dilute heavy oil in situ to increase oil production. In contrast
to traditional steam flood simulations based on principles of classic physics,
we introduce here an approach using cutting-edge machine learning techniques
that have the potential to provide a better way to describe the performance of
steam flood. We propose a workflow to address a category of time-series data
that can be analyzed with supervised machine learning algorithms and IoT. We
demonstrate the effectiveness of the technique for forecasting oil production
in steam flood scenarios. Moreover, we build an optimization system that
recommends an optimal steam allocation plan, and show that it leads to a 3%
improvement in oil production. We develop a minimum viable product on a cloud
platform that can implement real-time data collection, transfer, and storage,
as well as the training and implementation of a cloud-based machine learning
model. This workflow also offers an applicable solution to other problems with
similar time-series data structures, like predictive maintenance.",arxiv
http://arxiv.org/abs/2105.08374v1,2021-05-18T09:01:44Z,2021-05-18T09:01:44Z,"Online Multimodal Transportation Planning using Deep Reinforcement
  Learning","In this paper we propose a Deep Reinforcement Learning approach to solve a
multimodal transportation planning problem, in which containers must be
assigned to a truck or to trains that will transport them to their destination.
While traditional planning methods work ""offline"" (i.e., they take decisions
for a batch of containers before the transportation starts), the proposed
approach is ""online"", in that it can take decisions for individual containers,
while transportation is being executed. Planning transportation online helps to
effectively respond to unforeseen events that may affect the original
transportation plan, thus supporting companies in lowering transportation
costs. We implemented different container selection heuristics within the
proposed Deep Reinforcement Learning algorithm and we evaluated its performance
for each heuristic using data that simulate a realistic scenario, designed on
the basis of a real case study at a logistics company. The experimental results
revealed that the proposed method was able to learn effective patterns of
container assignment. It outperformed tested competitors in terms of total
transportation costs and utilization of train capacity by 20.48% to 55.32% for
the cost and by 7.51% to 20.54% for the capacity. Furthermore, it obtained
results within 2.7% for the cost and 0.72% for the capacity of the optimal
solution generated by an Integer Linear Programming solver in an offline
setting.",arxiv
http://arxiv.org/abs/2002.07443v1,2020-02-18T09:28:14Z,2020-02-18T09:28:14Z,"An Evaluation of Monte Carlo-Based Hyper-Heuristic for Interaction
  Testing of Industrial Embedded Software Applications","Hyper-heuristic is a new methodology for the adaptive hybridization of
meta-heuristic algorithms to derive a general algorithm for solving
optimization problems. This work focuses on the selection type of
hyper-heuristic, called the Exponential Monte Carlo with Counter (EMCQ).
Current implementations rely on the memory-less selection that can be
counterproductive as the selected search operator may not (historically) be the
best performing operator for the current search instance. Addressing this
issue, we propose to integrate the memory into EMCQ for combinatorial t-wise
test suite generation using reinforcement learning based on the Q-learning
mechanism, called Q-EMCQ. The limited application of combinatorial test
generation on industrial programs can impact the use of such techniques as
Q-EMCQ. Thus, there is a need to evaluate this kind of approach against
relevant industrial software, with a purpose to show the degree of interaction
required to cover the code as well as finding faults. We applied Q-EMCQ on 37
real-world industrial programs written in Function Block Diagram (FBD)
language, which is used for developing a train control management system at
Bombardier Transportation Sweden AB. The results of this study show that Q-EMCQ
is an efficient technique for test case generation. Additionally, unlike the
t-wise test suite generation, which deals with the minimization problem, we
have also subjected Q-EMCQ to a maximization problem involving the general
module clustering to demonstrate the effectiveness of our approach.",arxiv
http://arxiv.org/abs/2007.02351v1,2020-07-05T14:24:24Z,2020-07-05T14:24:24Z,Offline Model Guard: Secure and Private ML on Mobile Devices,"Performing machine learning tasks in mobile applications yields a challenging
conflict of interest: highly sensitive client information (e.g., speech data)
should remain private while also the intellectual property of service providers
(e.g., model parameters) must be protected. Cryptographic techniques offer
secure solutions for this, but have an unacceptable overhead and moreover
require frequent network interaction. In this work, we design a practically
efficient hardware-based solution. Specifically, we build Offline Model Guard
(OMG) to enable privacy-preserving machine learning on the predominant mobile
computing platform ARM - even in offline scenarios. By leveraging a trusted
execution environment for strict hardware-enforced isolation from other system
components, OMG guarantees privacy of client data, secrecy of provided models,
and integrity of processing algorithms. Our prototype implementation on an ARM
HiKey 960 development board performs privacy-preserving keyword recognition
using TensorFlow Lite for Microcontrollers in real time.",arxiv
http://arxiv.org/abs/2111.08587v1,2021-11-11T11:31:20Z,2021-11-11T11:31:20Z,Offline Contextual Bandits for Wireless Network Optimization,"The explosion in mobile data traffic together with the ever-increasing
expectations for higher quality of service call for the development of AI
algorithms for wireless network optimization. In this paper, we investigate how
to learn policies that can automatically adjust the configuration parameters of
every cell in the network in response to the changes in the user demand. Our
solution combines existent methods for offline learning and adapts them in a
principled way to overcome crucial challenges arising in this context.
Empirical results suggest that our proposed method will achieve important
performance gains when deployed in the real network while satisfying practical
constrains on computational efficiency.",arxiv
http://arxiv.org/abs/2009.06488v2,2020-10-20T15:23:20Z,2020-09-14T14:48:40Z,"Fast Implementation of 4-bit Convolutional Neural Networks for Mobile
  Devices","Quantized low-precision neural networks are very popular because they require
less computational resources for inference and can provide high performance,
which is vital for real-time and embedded recognition systems. However, their
advantages are apparent for FPGA and ASIC devices, while general-purpose
processor architectures are not always able to perform low-bit integer
computations efficiently. The most frequently used low-precision neural network
model for mobile central processors is an 8-bit quantized network. However, in
a number of cases, it is possible to use fewer bits for weights and
activations, and the only problem is the difficulty of efficient
implementation. We introduce an efficient implementation of 4-bit matrix
multiplication for quantized neural networks and perform time measurements on a
mobile ARM processor. It shows 2.9 times speedup compared to standard
floating-point multiplication and is 1.5 times faster than 8-bit quantized one.
We also demonstrate a 4-bit quantized neural network for OCR recognition on the
MIDV-500 dataset. 4-bit quantization gives 95.0% accuracy and 48% overall
inference speedup, while an 8-bit quantized network gives 95.4% accuracy and
39% speedup. The results show that 4-bit quantization perfectly suits mobile
devices, yielding good enough accuracy and low inference time.",arxiv
http://arxiv.org/abs/2006.10214v1,2020-06-18T00:19:13Z,2020-06-18T00:19:13Z,MediaPipe Hands: On-device Real-time Hand Tracking,"We present a real-time on-device hand tracking pipeline that predicts hand
skeleton from single RGB camera for AR/VR applications. The pipeline consists
of two models: 1) a palm detector, 2) a hand landmark model. It's implemented
via MediaPipe, a framework for building cross-platform ML solutions. The
proposed model and pipeline architecture demonstrates real-time inference speed
on mobile GPUs and high prediction quality. MediaPipe Hands is open sourced at
https://mediapipe.dev.",arxiv
http://arxiv.org/abs/1808.09945v2,2020-12-03T15:51:08Z,2018-08-29T17:51:39Z,"Fixed-Point Convolutional Neural Network for Real-Time Video Processing
  in FPGA","Modern mobile neural networks with a reduced number of weights and parameters
do a good job with image classification tasks, but even they may be too complex
to be implemented in an FPGA for video processing tasks. The article proposes
neural network architecture for the practical task of recognizing images from a
camera, which has several advantages in terms of speed. This is achieved by
reducing the number of weights, moving from a floating-point to a fixed-point
arithmetic, and due to a number of hardware-level optimizations associated with
storing weights in blocks, a shift register, and an adjustable number of
convolutional blocks that work in parallel. The article also proposed methods
for adapting the existing data set for solving a different task. As the
experiments showed, the proposed neural network copes well with real-time video
processing even on the cheap FPGAs.",arxiv
http://arxiv.org/abs/1809.11086v2,2019-01-24T19:14:18Z,2018-09-28T15:27:29Z,Learning Recurrent Binary/Ternary Weights,"Recurrent neural networks (RNNs) have shown excellent performance in
processing sequence data. However, they are both complex and memory intensive
due to their recursive nature. These limitations make RNNs difficult to embed
on mobile devices requiring real-time processes with limited hardware
resources. To address the above issues, we introduce a method that can learn
binary and ternary weights during the training phase to facilitate hardware
implementations of RNNs. As a result, using this approach replaces all
multiply-accumulate operations by simple accumulations, bringing significant
benefits to custom hardware in terms of silicon area and power consumption. On
the software side, we evaluate the performance (in terms of accuracy) of our
method using long short-term memories (LSTMs) on various sequential models
including sequence classification and language modeling. We demonstrate that
our method achieves competitive results on the aforementioned tasks while using
binary/ternary weights during the runtime. On the hardware side, we present
custom hardware for accelerating the recurrent computations of LSTMs with
binary/ternary weights. Ultimately, we show that LSTMs with binary/ternary
weights can achieve up to 12x memory saving and 10x inference speedup compared
to the full-precision implementation on an ASIC platform.",arxiv
http://arxiv.org/abs/1903.02358v1,2019-03-06T13:20:36Z,2019-03-06T13:20:36Z,"Compressing complex convolutional neural network based on an improved
  deep compression algorithm","Although convolutional neural network (CNN) has made great progress, large
redundant parameters restrict its deployment on embedded devices, especially
mobile devices. The recent compression works are focused on real-value
convolutional neural network (Real CNN), however, to our knowledge, there is no
attempt for the compression of complex-value convolutional neural network
(Complex CNN). Compared with the real-valued network, the complex-value neural
network is easier to optimize, generalize, and has better learning potential.
This paper extends the commonly used deep compression algorithm from real
domain to complex domain and proposes an improved deep compression algorithm
for the compression of Complex CNN. The proposed algorithm compresses the
network about 8 times on CIFAR-10 dataset with less than 3% accuracy loss. On
the ImageNet dataset, our method compresses the model about 16 times and the
accuracy loss is about 2% without retraining.",arxiv
http://arxiv.org/abs/2002.01618v1,2020-02-05T03:20:25Z,2020-02-05T03:20:25Z,Crowdsourcing the Perception of Machine Teaching,"Teachable interfaces can empower end-users to attune machine learning systems
to their idiosyncratic characteristics and environment by explicitly providing
pertinent training examples. While facilitating control, their effectiveness
can be hindered by the lack of expertise or misconceptions. We investigate how
users may conceptualize, experience, and reflect on their engagement in machine
teaching by deploying a mobile teachable testbed in Amazon Mechanical Turk.
Using a performance-based payment scheme, Mechanical Turkers (N = 100) are
called to train, test, and re-train a robust recognition model in real-time
with a few snapshots taken in their environment. We find that participants
incorporate diversity in their examples drawing from parallels to how humans
recognize objects independent of size, viewpoint, location, and illumination.
Many of their misconceptions relate to consistency and model capabilities for
reasoning. With limited variation and edge cases in testing, the majority of
them do not change strategies on a second training attempt.",arxiv
http://arxiv.org/abs/2011.04128v1,2020-11-09T01:17:58Z,2020-11-09T01:17:58Z,"Stable predictions for health related anticausal prediction tasks
  affected by selection biases: the need to deconfound the test set features","In health related machine learning applications, the training data often
corresponds to a non-representative sample from the target populations where
the learners will be deployed. In anticausal prediction tasks, selection biases
often make the associations between confounders and the outcome variable
unstable across different target environments. As a consequence, the
predictions from confounded learners are often unstable, and might fail to
generalize in shifted test environments. Stable prediction approaches aim to
solve this problem by producing predictions that are stable across unknown test
environments. These approaches, however, are sometimes applied to the training
data alone with the hope that training an unconfounded model will be enough to
generate stable predictions in shifted test sets. Here, we show that this is
insufficient, and that improved stability can be achieved by deconfounding the
test set features as well. We illustrate these observations using both
synthetic data and real world data from a mobile health study.",arxiv
http://arxiv.org/abs/2103.13997v1,2021-03-25T17:34:59Z,2021-03-25T17:34:59Z,Real-time low-resource phoneme recognition on edge devices,"While speech recognition has seen a surge in interest and research over the
last decade, most machine learning models for speech recognition either require
large training datasets or lots of storage and memory. Combined with the
prominence of English as the number one language in which audio data is
available, this means most other languages currently lack good speech
recognition models.
  The method presented in this paper shows how to create and train models for
speech recognition in any language which are not only highly accurate, but also
require very little storage, memory and training data when compared with
traditional models. This allows training models to recognize any language and
deploying them on edge devices such as mobile phones or car displays for fast
real-time speech recognition.",arxiv
http://arxiv.org/abs/2111.07574v1,2021-11-15T07:43:43Z,2021-11-15T07:43:43Z,"Vision-Position Multi-Modal Beam Prediction Using Real Millimeter Wave
  Datasets","Enabling highly-mobile millimeter wave (mmWave) and terahertz (THz) wireless
communication applications requires overcoming the critical challenges
associated with the large antenna arrays deployed at these systems. In
particular, adjusting the narrow beams of these antenna arrays typically incurs
high beam training overhead that scales with the number of antennas. To address
these challenges, this paper proposes a multi-modal machine learning based
approach that leverages positional and visual (camera) data collected from the
wireless communication environment for fast beam prediction. The developed
framework has been tested on a real-world vehicular dataset comprising
practical GPS, camera, and mmWave beam training data. The results show the
proposed approach achieves more than $\approx$ 75\% top-1 beam prediction
accuracy and close to 100\% top-3 beam prediction accuracy in realistic
communication scenarios.",arxiv
http://arxiv.org/abs/1805.08692v1,2018-05-04T15:07:29Z,2018-05-04T15:07:29Z,"Assessing a mobile-based deep learning model for plant disease
  surveillance","Convolutional neural network models (CNNs) have made major advances in
computer vision tasks in the last five years. Given the challenge in collecting
real world datasets, most studies report performance metrics based on available
research datasets. In scenarios where CNNs are to be deployed on images or
videos from mobile devices, models are presented with new challenges due to
lighting, angle, and camera specifications, which are not accounted for in
research datasets. It is essential for assessment to also be conducted on real
world datasets if such models are to be reliably integrated with products and
services in society. Plant disease datasets can be used to test CNNs in real
time and gain insight into real world performance. We train a CNN object
detection model to identify foliar symptoms of diseases (or lack thereof) in
cassava (Manihot esculenta Crantz). We then deploy the model on a mobile app
and test its performance on mobile images and video of 720 diseased leaflets in
an agricultural field in Tanzania. Within each disease category we test two
levels of severity of symptoms - mild and pronounced, to assess the model
performance for early detection of symptoms. In both severities we see a
decrease in the F-1 score for real world images and video. The F-1 score
dropped by 32% for pronounced symptoms in real world images (the closest data
to the training data) due to a drop in model recall. If the potential of
smartphone CNNs are to be realized our data suggest it is crucial to consider
tuning precision and recall performance in order to achieve the desired
performance in real world settings. In addition, the varied performance related
to different input data (image or video) is an important consideration for the
design of CNNs in real world applications.",arxiv
http://arxiv.org/abs/2004.08340v2,2020-05-13T10:19:29Z,2020-04-17T16:44:46Z,"Data-driven Flood Emulation: Speeding up Urban Flood Predictions by Deep
  Convolutional Neural Networks","Computational complexity has been the bottleneck of applying physically-based
simulations on large urban areas with high spatial resolution for efficient and
systematic flooding analyses and risk assessments. To address this issue of
long computational time, this paper proposes that the prediction of maximum
water depth rasters can be considered as an image-to-image translation problem
where the results are generated from input elevation rasters using the
information learned from data rather than by conducting simulations, which can
significantly accelerate the prediction process. The proposed approach was
implemented by a deep convolutional neural network trained on flood simulation
data of 18 designed hyetographs on three selected catchments. Multiple tests
with both designed and real rainfall events were performed and the results show
that the flood predictions by neural network uses only 0.5 % of time comparing
with physically-based approaches, with promising accuracy and ability of
generalizations. The proposed neural network can also potentially be applied to
different but relevant problems including flood predictions for urban layout
planning.",arxiv
http://arxiv.org/abs/2103.08022v1,2021-03-14T20:13:06Z,2021-03-14T20:13:06Z,"Success Weighted by Completion Time: A Dynamics-Aware Evaluation
  Criteria for Embodied Navigation","We present Success weighted by Completion Time (SCT), a new metric for
evaluating navigation performance for mobile robots. Several related works on
navigation have used Success weighted by Path Length (SPL) as the primary
method of evaluating the path an agent makes to a goal location, but SPL is
limited in its ability to properly evaluate agents with complex dynamics. In
contrast, SCT explicitly takes the agent's dynamics model into consideration,
and aims to accurately capture how well the agent has approximated the fastest
navigation behavior afforded by its dynamics. While several embodied navigation
works use point-turn dynamics, we focus on unicycle-cart dynamics for our
agent, which better exemplifies the dynamics model of popular mobile robotics
platforms (e.g., LoCoBot, TurtleBot, Fetch, etc.). We also present
RRT*-Unicycle, an algorithm for unicycle dynamics that estimates the fastest
collision-free path and completion time from a starting pose to a goal location
in an environment containing obstacles. We experiment with deep reinforcement
learning and reward shaping to train and compare the navigation performance of
agents with different dynamics models. In evaluating these agents, we show that
in contrast to SPL, SCT is able to capture the advantages in navigation speed a
unicycle model has over a simpler point-turn model of dynamics. Lastly, we show
that we can successfully deploy our trained models and algorithms outside of
simulation in the real world. We embody our agents in an real robot to navigate
an apartment, and show that they can generalize in a zero-shot manner.",arxiv
http://arxiv.org/abs/2004.11823v1,2020-04-08T03:12:49Z,2020-04-08T03:12:49Z,Facial Expression Recognition with Deep Learning,"One of the most universal ways that people communicate is through facial
expressions. In this paper, we take a deep dive, implementing multiple deep
learning models for facial expression recognition (FER). Our goals are twofold:
we aim not only to maximize accuracy, but also to apply our results to the
real-world. By leveraging numerous techniques from recent research, we
demonstrate a state-of-the-art 75.8% accuracy on the FER2013 test set,
outperforming all existing publications. Additionally, we showcase a mobile web
app which runs our FER models on-device in real time.",arxiv
http://arxiv.org/abs/2101.08391v1,2021-01-21T01:39:42Z,2021-01-21T01:39:42Z,"Deep Reinforcement Learning with Spatio-temporal Traffic Forecasting for
  Data-Driven Base Station Sleep Control","To meet the ever increasing mobile traffic demand in 5G era, base stations
(BSs) have been densely deployed in radio access networks (RANs) to increase
the network coverage and capacity. However, as the high density of BSs is
designed to accommodate peak traffic, it would consume an unnecessarily large
amount of energy if BSs are on during off-peak time. To save the energy
consumption of cellular networks, an effective way is to deactivate some idle
base stations that do not serve any traffic demand. In this paper, we develop a
traffic-aware dynamic BS sleep control framework, named DeepBSC, which presents
a novel data-driven learning approach to determine the BS active/sleep modes
while meeting lower energy consumption and satisfactory Quality of Service
(QoS) requirements. Specifically, the traffic demands are predicted by the
proposed GS-STN model, which leverages the geographical and semantic
spatial-temporal correlations of mobile traffic. With accurate mobile traffic
forecasting, the BS sleep control problem is cast as a Markov Decision Process
that is solved by Actor-Critic reinforcement learning methods. To reduce the
variance of cost estimation in the dynamic environment, we propose a benchmark
transformation method that provides robust performance indicator for policy
update. To expedite the training process, we adopt a Deep Deterministic Policy
Gradient (DDPG) approach, together with an explorer network, which can
strengthen the exploration further. Extensive experiments with a real-world
dataset corroborate that our proposed framework significantly outperforms the
existing methods.",arxiv
http://arxiv.org/abs/2111.01946v1,2021-11-02T23:41:09Z,2021-11-02T23:41:09Z,"Robust Dynamic Bus Control: A Distributional Multi-agent Reinforcement
  Learning Approach","Bus system is a critical component of sustainable urban transportation.
However, the operation of a bus fleet is unstable in nature, and bus bunching
has become a common phenomenon that undermines the efficiency and reliability
of bus systems. Recently research has demonstrated the promising application of
multi-agent reinforcement learning (MARL) to achieve efficient vehicle holding
control to avoid bus bunching. However, existing studies essentially overlook
the robustness issue resulting from various events, perturbations and anomalies
in a transit system, which is of utmost importance when transferring the models
for real-world deployment/application. In this study, we integrate implicit
quantile network and meta-learning to develop a distributional MARL framework
-- IQNC-M -- to learn continuous control. The proposed IQNC-M framework
achieves efficient and reliable control decisions through better handling
various uncertainties/events in real-time transit operations. Specifically, we
introduce an interpretable meta-learning module to incorporate global
information into the distributional MARL framework, which is an effective
solution to circumvent the credit assignment issue in the transit system. In
addition, we design a specific learning procedure to train each agent within
the framework to pursue a robust control policy. We develop simulation
environments based on real-world bus services and passenger demand data and
evaluate the proposed framework against both traditional holding control models
and state-of-the-art MARL models. Our results show that the proposed IQNC-M
framework can effectively handle the various extreme events, such as traffic
state perturbations, service interruptions, and demand surges, thus improving
both efficiency and reliability of the system.",arxiv
http://arxiv.org/abs/1909.05073v4,2020-03-04T19:39:06Z,2019-09-06T03:58:29Z,"PCONV: The Missing but Desirable Sparsity in DNN Weight Pruning for
  Real-time Execution on Mobile Devices","Model compression techniques on Deep Neural Network (DNN) have been widely
acknowledged as an effective way to achieve acceleration on a variety of
platforms, and DNN weight pruning is a straightforward and effective method.
There are currently two mainstreams of pruning methods representing two
extremes of pruning regularity: non-structured, fine-grained pruning can
achieve high sparsity and accuracy, but is not hardware friendly; structured,
coarse-grained pruning exploits hardware-efficient structures in pruning, but
suffers from accuracy drop when the pruning rate is high. In this paper, we
introduce PCONV, comprising a new sparsity dimension, -- fine-grained pruning
patterns inside the coarse-grained structures. PCONV comprises two types of
sparsities, Sparse Convolution Patterns (SCP) which is generated from
intra-convolution kernel pruning and connectivity sparsity generated from
inter-convolution kernel pruning. Essentially, SCP enhances accuracy due to its
special vision properties, and connectivity sparsity increases pruning rate
while maintaining balanced workload on filter computation. To deploy PCONV, we
develop a novel compiler-assisted DNN inference framework and execute PCONV
models in real-time without accuracy compromise, which cannot be achieved in
prior work. Our experimental results show that, PCONV outperforms three
state-of-art end-to-end DNN frameworks, TensorFlow-Lite, TVM, and Alibaba
Mobile Neural Network with speedup up to 39.2x, 11.4x, and 6.3x, respectively,
with no accuracy loss. Mobile devices can achieve real-time inference on
large-scale DNNs.",arxiv
http://arxiv.org/abs/2012.00938v2,2021-03-30T04:52:40Z,2020-12-02T02:49:53Z,"Improving Accuracy of Binary Neural Networks using Unbalanced Activation
  Distribution","Binarization of neural network models is considered as one of the promising
methods to deploy deep neural network models on resource-constrained
environments such as mobile devices. However, Binary Neural Networks (BNNs)
tend to suffer from severe accuracy degradation compared to the full-precision
counterpart model. Several techniques were proposed to improve the accuracy of
BNNs. One of the approaches is to balance the distribution of binary
activations so that the amount of information in the binary activations becomes
maximum. Based on extensive analysis, in stark contrast to previous work, we
argue that unbalanced activation distribution can actually improve the accuracy
of BNNs. We also show that adjusting the threshold values of binary activation
functions results in the unbalanced distribution of the binary activation,
which increases the accuracy of BNN models. Experimental results show that the
accuracy of previous BNN models (e.g. XNOR-Net and Bi-Real-Net) can be improved
by simply shifting the threshold values of binary activation functions without
requiring any other modification.",arxiv
http://arxiv.org/abs/1908.05858v1,2019-08-16T06:07:57Z,2019-08-16T06:07:57Z,"daBNN: A Super Fast Inference Framework for Binary Neural Networks on
  ARM devices","It is always well believed that Binary Neural Networks (BNNs) could
drastically accelerate the inference efficiency by replacing the arithmetic
operations in float-valued Deep Neural Networks (DNNs) with bit-wise
operations. Nevertheless, there has not been open-source implementation in
support of this idea on low-end ARM devices (e.g., mobile phones and embedded
devices). In this work, we propose daBNN --- a super fast inference framework
that implements BNNs on ARM devices. Several speed-up and memory refinement
strategies for bit-packing, binarized convolution, and memory layout are
uniquely devised to enhance inference efficiency. Compared to the recent
open-source BNN inference framework, BMXNet, our daBNN is
$7\times$$\sim$$23\times$ faster on a single binary convolution, and about
$6\times$ faster on Bi-Real Net 18 (a BNN variant of ResNet-18). The daBNN is a
BSD-licensed inference framework, and its source code, sample projects and
pre-trained models are available on-line: https://github.com/JDAI-CV/dabnn.",arxiv
http://arxiv.org/abs/1810.01140v2,2018-10-08T08:40:40Z,2018-10-02T09:45:15Z,"Training compact deep learning models for video classification using
  circulant matrices","In real world scenarios, model accuracy is hardly the only factor to
consider. Large models consume more memory and are computationally more
intensive, which makes them difficult to train and to deploy, especially on
mobile devices. In this paper, we build on recent results at the crossroads of
Linear Algebra and Deep Learning which demonstrate how imposing a structure on
large weight matrices can be used to reduce the size of the model. We propose
very compact models for video classification based on state-of-the-art network
architectures such as Deep Bag-of-Frames, NetVLAD and NetFisherVectors. We then
conduct thorough experiments using the large YouTube-8M video classification
dataset. As we will show, the circulant DBoF embedding achieves an excellent
trade-off between size and accuracy.",arxiv
http://arxiv.org/abs/1505.06125v1,2015-05-22T15:39:52Z,2015-05-22T15:39:52Z,"Machine Learning for Indoor Localization Using Mobile Phone-Based
  Sensors","In this paper we investigate the problem of localizing a mobile device based
on readings from its embedded sensors utilizing machine learning methodologies.
We consider a real-world environment, collect a large dataset of 3110
datapoints, and examine the performance of a substantial number of machine
learning algorithms in localizing a mobile device. We have found algorithms
that give a mean error as accurate as 0.76 meters, outperforming other indoor
localization systems reported in the literature. We also propose a hybrid
instance-based approach that results in a speed increase by a factor of ten
with no loss of accuracy in a live deployment over standard instance-based
methods, allowing for fast and accurate localization. Further, we determine how
smaller datasets collected with less density affect accuracy of localization,
important for use in real-world environments. Finally, we demonstrate that
these approaches are appropriate for real-world deployment by evaluating their
performance in an online, in-motion experiment.",arxiv
http://arxiv.org/abs/1702.01243v3,2017-07-17T07:56:21Z,2017-02-04T06:34:31Z,Wide-Residual-Inception Networks for Real-time Object Detection,"Since convolutional neural network(CNN)models emerged,several tasks in
computer vision have actively deployed CNN models for feature extraction.
However,the conventional CNN models have a high computational cost and require
high memory capacity, which is impractical and unaffordable for commercial
applications such as real-time on-road object detection on embedded boards or
mobile platforms. To tackle this limitation of CNN models, this paper proposes
a wide-residual-inception (WR-Inception) network, which constructs the
architecture based on a residual inception unit that captures objects of
various sizes on the same feature map, as well as shallower and wider layers,
compared to state-of-the-art networks like ResNet. To verify the proposed
networks, this paper conducted two experiments; one is a classification task on
CIFAR-10/100 and the other is an on-road object detection task using a
Single-Shot Multi-box Detector(SSD) on the KITTI dataset.",arxiv
http://arxiv.org/abs/1810.11846v2,2019-02-19T05:08:46Z,2018-10-28T17:59:57Z,LPCNet: Improving Neural Speech Synthesis Through Linear Prediction,"Neural speech synthesis models have recently demonstrated the ability to
synthesize high quality speech for text-to-speech and compression applications.
These new models often require powerful GPUs to achieve real-time operation, so
being able to reduce their complexity would open the way for many new
applications. We propose LPCNet, a WaveRNN variant that combines linear
prediction with recurrent neural networks to significantly improve the
efficiency of speech synthesis. We demonstrate that LPCNet can achieve
significantly higher quality than WaveRNN for the same network size and that
high quality LPCNet speech synthesis is achievable with a complexity under 3
GFLOPS. This makes it easier to deploy neural synthesis applications on
lower-power devices, such as embedded systems and mobile phones.",arxiv
http://arxiv.org/abs/2008.11383v1,2020-08-26T05:44:07Z,2020-08-26T05:44:07Z,"Applying Surface Normal Information in Drivable Area and Road Anomaly
  Detection for Ground Mobile Robots","The joint detection of drivable areas and road anomalies is a crucial task
for ground mobile robots. In recent years, many impressive semantic
segmentation networks, which can be used for pixel-level drivable area and road
anomaly detection, have been developed. However, the detection accuracy still
needs improvement. Therefore, we develop a novel module named the Normal
Inference Module (NIM), which can generate surface normal information from
dense depth images with high accuracy and efficiency. Our NIM can be deployed
in existing convolutional neural networks (CNNs) to refine the segmentation
performance. To evaluate the effectiveness and robustness of our NIM, we embed
it in twelve state-of-the-art CNNs. The experimental results illustrate that
our NIM can greatly improve the performance of the CNNs for drivable area and
road anomaly detection. Furthermore, our proposed NIM-RTFNet ranks 8th on the
KITTI road benchmark and exhibits a real-time inference speed.",arxiv
http://arxiv.org/abs/1712.06107v1,2017-12-17T13:00:25Z,2017-12-17T13:00:25Z,Railway Track Specific Traffic Signal Selection Using Deep Learning,"With the railway transportation Industry moving actively towards automation,
accurate location and inventory of wayside track assets like traffic signals,
crossings, switches, mileposts, etc. is of extreme importance. With the new
Positive Train Control (PTC) regulation coming into effect, many railway safety
rules will be tied directly to location of assets like mileposts and signals.
Newer speed regulations will be enforced based on location of the Train with
respect to a wayside asset. Hence it is essential for the railroads to have an
accurate database of the types and locations of these assets. This paper talks
about a real-world use-case of detecting railway signals from a camera mounted
on a moving locomotive and tracking their locations. The camera is engineered
to withstand the environment factors on a moving train and provide a consistent
steady image at around 30 frames per second. Using advanced image analysis and
deep learning techniques, signals are detected in these camera images and a
database of their locations is created. Railway signals differ a lot from road
signals in terms of shapes and rules for placement with respect to track. Due
to space constraint and traffic densities in urban areas signals are not placed
on the same side of the track and multiple lines can run in parallel. Hence
there is need to associate signal detected with the track on which the train
runs. We present a method to associate the signals to the specific track they
belong to using a video feed from the front facing camera mounted on the lead
locomotive. A pipeline of track detection, region of interest selection, signal
detection has been implemented which gives an overall accuracy of 94.7% on a
route covering 150km with 247 signals.",arxiv
http://arxiv.org/abs/1904.01735v1,2019-04-03T01:29:48Z,2019-04-03T01:29:48Z,"Multi-Modal Generative Adversarial Network for Short Product Title
  Generation in Mobile E-Commerce","Nowadays, more and more customers browse and purchase products in favor of
using mobile E-Commerce Apps such as Taobao and Amazon. Since merchants are
usually inclined to describe redundant and over-informative product titles to
attract attentions from customers, it is important to concisely display short
product titles on limited screen of mobile phones. To address this discrepancy,
previous studies mainly consider textual information of long product titles and
lacks of human-like view during training and evaluation process. In this paper,
we propose a Multi-Modal Generative Adversarial Network (MM-GAN) for short
product title generation in E-Commerce, which innovatively incorporates image
information and attribute tags from product, as well as textual information
from original long titles. MM-GAN poses short title generation as a
reinforcement learning process, where the generated titles are evaluated by the
discriminator in a human-like view. Extensive experiments on a large-scale
E-Commerce dataset demonstrate that our algorithm outperforms other
state-of-the-art methods. Moreover, we deploy our model into a real-world
online E-Commerce environment and effectively boost the performance of click
through rate and click conversion rate by 1.66% and 1.87%, respectively.",arxiv
http://arxiv.org/abs/2101.03273v2,2021-03-29T02:53:58Z,2021-01-09T02:26:14Z,"Robust and Scalable Routing with Multi-Agent Deep Reinforcement Learning
  for MANETs","Highly dynamic mobile ad-hoc networks (MANETs) are continuing to serve as one
of the most challenging environments to develop and deploy robust, efficient,
and scalable routing protocols. In this paper, we present DeepCQ+ routing
which, in a novel manner, integrates emerging multi-agent deep reinforcement
learning (MADRL) techniques into existing Q-learning-based routing protocols
and their variants, and achieves persistently higher performance across a wide
range of MANET configurations while training only on a limited range of network
parameters and conditions. Quantitatively, DeepCQ+ shows consistently higher
end-to-end throughput with lower overhead compared to its Q-learning-based
counterparts with the overall gain of 10-15% in its efficiency. Qualitatively
and more significantly, DeepCQ+ maintains remarkably similar performance gains
under many scenarios that it was not trained for in terms of network sizes,
mobility conditions, and traffic dynamics. To the best of our knowledge, this
is the first successful demonstration of MADRL for the MANET routing problem
that achieves and maintains a high degree of scalability and robustness even in
the environments that are outside the trained range of scenarios. This implies
that the proposed hybrid design approach of DeepCQ+ that combines MADRL and
Q-learning significantly increases its practicality and explainability because
the real-world MANET environment will likely vary outside the trained range of
MANET scenarios.",arxiv
http://arxiv.org/abs/2010.08600v2,2020-11-16T06:26:16Z,2020-10-16T19:40:08Z,"Robot Navigation in Constrained Pedestrian Environments using
  Reinforcement Learning","Navigating fluently around pedestrians is a necessary capability for mobile
robots deployed in human environments, such as buildings and homes. While
research on social navigation has focused mainly on the scalability with the
number of pedestrians in open spaces, typical indoor environments present the
additional challenge of constrained spaces such as corridors and doorways that
limit maneuverability and influence patterns of pedestrian interaction. We
present an approach based on reinforcement learning (RL) to learn policies
capable of dynamic adaptation to the presence of moving pedestrians while
navigating between desired locations in constrained environments. The policy
network receives guidance from a motion planner that provides waypoints to
follow a globally planned trajectory, whereas RL handles the local
interactions. We explore a compositional principle for multi-layout training
and find that policies trained in a small set of geometrically simple layouts
successfully generalize to more complex unseen layouts that exhibit composition
of the structural elements available during training. Going beyond walls-world
like domains, we show transfer of the learned policy to unseen 3D
reconstructions of two real environments. These results support the
applicability of the compositional principle to navigation in real-world
buildings and indicate promising usage of multi-agent simulation within
reconstructed environments for tasks that involve interaction.",arxiv
http://arxiv.org/abs/2011.02373v1,2020-11-04T16:05:07Z,2020-11-04T16:05:07Z,"Moving Forward in Formation: A Decentralized Hierarchical Learning
  Approach to Multi-Agent Moving Together","Multi-agent path finding in formation has many potential real-world
applications like mobile warehouse robots. However, previous multi-agent path
finding (MAPF) methods hardly take formation into consideration. Furthermore,
they are usually centralized planners and require the whole state of the
environment. Other decentralized partially observable approaches to MAPF are
reinforcement learning (RL) methods. However, these RL methods encounter
difficulties when learning path finding and formation problem at the same time.
In this paper, we propose a novel decentralized partially observable RL
algorithm that uses a hierarchical structure to decompose the multi objective
task into unrelated ones. It also calculates a theoretical weight that makes
every task reward has equal influence on the final RL value function.
Additionally, we introduce a communication method that helps agents cooperate
with each other. Experiments in simulation show that our method outperforms
other end-to-end RL methods and our method can naturally scale to large world
sizes where centralized planner struggles. We also deploy and validate our
method in a real world scenario.",arxiv
http://arxiv.org/abs/2011.09902v1,2020-11-17T04:11:31Z,2020-11-17T04:11:31Z,"Low-latency Federated Learning and Blockchain for Edge Association in
  Digital Twin empowered 6G Networks","Emerging technologies such as digital twins and 6th Generation mobile
networks (6G) have accelerated the realization of edge intelligence in
Industrial Internet of Things (IIoT). The integration of digital twin and 6G
bridges the physical system with digital space and enables robust instant
wireless connectivity. With increasing concerns on data privacy, federated
learning has been regarded as a promising solution for deploying distributed
data processing and learning in wireless networks. However, unreliable
communication channels, limited resources, and lack of trust among users,
hinder the effective application of federated learning in IIoT. In this paper,
we introduce the Digital Twin Wireless Networks (DTWN) by incorporating digital
twins into wireless networks, to migrate real-time data processing and
computation to the edge plane. Then, we propose a blockchain empowered
federated learning framework running in the DTWN for collaborative computing,
which improves the reliability and security of the system, and enhances data
privacy. Moreover, to balance the learning accuracy and time cost of the
proposed scheme, we formulate an optimization problem for edge association by
jointly considering digital twin association, training data batch size, and
bandwidth allocation. We exploit multi-agent reinforcement learning to find an
optimal solution to the problem. Numerical results on real-world dataset show
that the proposed scheme yields improved efficiency and reduced cost compared
to benchmark learning method.",arxiv
http://arxiv.org/abs/2002.00577v2,2020-05-19T01:55:13Z,2020-02-03T06:40:04Z,"Prophet: Proactive Candidate-Selection for Federated Learning by
  Predicting the Qualities of Training and Reporting Phases","Although the challenge of the device connection is much relieved in 5G
networks, the training latency is still an obstacle preventing Federated
Learning (FL) from being largely adopted. One of the most fundamental problems
that lead to large latency is the bad candidate-selection for FL. In the
dynamic environment, the mobile devices selected by the existing reactive
candidate-selection algorithms very possibly fail to complete the training and
reporting phases of FL, because the FL parameter server only knows the
currently-observed resources of all candidates. To this end, we study the
proactive candidate-selection for FL in this paper. We first let each candidate
device predict the qualities of both its training and reporting phases locally
using LSTM. Then, the proposed candidateselection algorithm is implemented by
the Deep Reinforcement Learning (DRL) framework. Finally, the real-world
trace-driven experiments prove that the proposed approach outperforms the
existing reactive algorithms",arxiv
http://arxiv.org/abs/2007.14545v2,2020-07-30T07:13:17Z,2020-07-29T01:09:27Z,"Learning Object-conditioned Exploration using Distributed Soft Actor
  Critic","Object navigation is defined as navigating to an object of a given label in a
complex, unexplored environment. In its general form, this problem poses
several challenges for Robotics: semantic exploration of unknown environments
in search of an object and low-level control. In this work we study
object-guided exploration and low-level control, and present an end-to-end
trained navigation policy achieving a success rate of 0.68 and SPL of 0.58 on
unseen, visually complex scans of real homes. We propose a highly scalable
implementation of an off-policy Reinforcement Learning algorithm, distributed
Soft Actor Critic, which allows the system to utilize 98M experience steps in
24 hours on 8 GPUs. Our system learns to control a differential drive mobile
base in simulation from a stack of high dimensional observations commonly used
on robotic platforms. The learned policy is capable of object-guided
exploratory behaviors and low-level control learned from pure experiences in
realistic environments.",arxiv
http://arxiv.org/abs/2103.02649v1,2021-03-03T19:31:29Z,2021-03-03T19:31:29Z,"Self-play Learning Strategies for Resource Assignment in Open-RAN
  Networks","Open Radio Access Network (ORAN) is being developed with an aim to
democratise access and lower the cost of future mobile data networks,
supporting network services with various QoS requirements, such as massive IoT
and URLLC. In ORAN, network functionality is dis-aggregated into remote units
(RUs), distributed units (DUs) and central units (CUs), which allows flexible
software on Commercial-Off-The-Shelf (COTS) deployments. Furthermore, the
mapping of variable RU requirements to local mobile edge computing centres for
future centralized processing would significantly reduce the power consumption
in cellular networks. In this paper, we study the RU-DU resource assignment
problem in an ORAN system, modelled as a 2D bin packing problem. A deep
reinforcement learning-based self-play approach is proposed to achieve
efficient RU-DU resource management, with AlphaGo Zero inspired neural
Monte-Carlo Tree Search (MCTS). Experiments on representative 2D bin packing
environment and real sites data show that the self-play learning strategy
achieves intelligent RU-DU resource assignment for different network
conditions.",arxiv
http://arxiv.org/abs/1803.00680v2,2019-03-17T01:34:35Z,2018-03-02T01:34:06Z,"A Tutorial on UAVs for Wireless Networks: Applications, Challenges, and
  Open Problems","The use of flying platforms such as unmanned aerial vehicles (UAVs),
popularly known as drones, is rapidly growing. In particular, with their
inherent attributes such as mobility, flexibility, and adaptive altitude, UAVs
admit several key potential applications in wireless systems. On the one hand,
UAVs can be used as aerial base stations to enhance coverage, capacity,
reliability, and energy efficiency of wireless networks. On the other hand,
UAVs can operate as flying mobile terminals within a cellular network. Such
cellular-connected UAVs can enable several applications ranging from real-time
video streaming to item delivery. In this paper, a comprehensive tutorial on
the potential benefits and applications of UAVs in wireless communications is
presented. Moreover, the important challenges and the fundamental tradeoffs in
UAV-enabled wireless networks are thoroughly investigated. In particular, the
key UAV challenges such as three-dimensional deployment, performance analysis,
channel modeling, and energy efficiency are explored along with representative
results. Then, open problems and potential research directions pertaining to
UAV communications are introduced. Finally, various analytical frameworks and
mathematical tools such as optimization theory, machine learning, stochastic
geometry, transport theory, and game theory are described. The use of such
tools for addressing unique UAV problems is also presented. In a nutshell, this
tutorial provides key guidelines on how to analyze, optimize, and design
UAV-based wireless communication systems.",arxiv
http://arxiv.org/abs/2003.04816v1,2020-02-21T07:29:15Z,2020-02-21T07:29:15Z,"Data Freshness and Energy-Efficient UAV Navigation Optimization: A Deep
  Reinforcement Learning Approach","In this paper, we design a navigation policy for multiple unmanned aerial
vehicles (UAVs) where mobile base stations (BSs) are deployed to improve the
data freshness and connectivity to the Internet of Things (IoT) devices. First,
we formulate an energy-efficient trajectory optimization problem in which the
objective is to maximize the energy efficiency by optimizing the UAV-BS
trajectory policy. We also incorporate different contextual information such as
energy and age of information (AoI) constraints to ensure the data freshness at
the ground BS. Second, we propose an agile deep reinforcement learning with
experience replay model to solve the formulated problem concerning the
contextual constraints for the UAV-BS navigation. Moreover, the proposed
approach is well-suited for solving the problem, since the state space of the
problem is extremely large and finding the best trajectory policy with useful
contextual features is too complex for the UAV-BSs. By applying the proposed
trained model, an effective real-time trajectory policy for the UAV-BSs
captures the observable network states over time. Finally, the simulation
results illustrate the proposed approach is 3.6% and 3.13% more energy
efficient than those of the greedy and baseline deep Q Network (DQN)
approaches.",arxiv
http://arxiv.org/abs/2007.10129v1,2020-07-15T21:32:43Z,2020-07-15T21:32:43Z,"Information Freshness-Aware Task Offloading in Air-Ground Integrated
  Edge Computing Systems","This paper studies the problem of information freshness-aware task offloading
in an air-ground integrated multi-access edge computing system, which is
deployed by an infrastructure provider (InP). A third-party real-time
application service provider provides computing services to the subscribed
mobile users (MUs) with the limited communication and computation resources
from the InP based on a long-term business agreement. Due to the dynamic
characteristics, the interactions among the MUs are modelled by a
non-cooperative stochastic game, in which the control policies are coupled and
each MU aims to selfishly maximize its own expected long-term payoff. To
address the Nash equilibrium solutions, we propose that each MU behaves in
accordance with the local system states and conjectures, based on which the
stochastic game is transformed into a single-agent Markov decision process.
Moreover, we derive a novel online deep reinforcement learning (RL) scheme that
adopts two separate double deep Q-networks for each MU to approximate the
Q-factor and the post-decision Q-factor. Using the proposed deep RL scheme,
each MU in the system is able to make decisions without a priori statistical
knowledge of dynamics. Numerical experiments examine the potentials of the
proposed scheme in balancing the age of information and the energy consumption.",arxiv
http://arxiv.org/abs/2006.06443v1,2020-06-11T13:53:18Z,2020-06-11T13:53:18Z,"Convolutional neural networks compression with low rank and sparse
  tensor decompositions","Convolutional neural networks show outstanding results in a variety of
computer vision tasks. However, a neural network architecture design usually
faces a trade-off between model performance and computational/memory
complexity. For some real-world applications, it is crucial to develop models,
which can be fast and light enough to run on edge systems and mobile devices.
However, many modern architectures that demonstrate good performance don't
satisfy inference time and storage limitation requirements. Thus, arises a
problem of neural network compression to obtain a smaller and faster model,
which is on par with the initial one.
  In this work, we consider a neural network compression method based on tensor
decompositions. Namely, we propose to approximate the convolutional layer
weight with a tensor, which can be represented as a sum of low-rank and sparse
components. The motivation for such approximation is based on the assumption
that low-rank and sparse terms allow eliminating two different types of
redundancy and thus yield a better compression rate. An efficient CPU
implementation for the proposed method has been developed. Our algorithm has
demonstrated up to 3.5x CPU layer speedup and 11x layer size reduction when
compressing Resnet50 architecture for the image classification task.",arxiv
http://arxiv.org/abs/1811.05320v3,2018-12-31T05:32:23Z,2018-11-12T03:30:03Z,T-GCN: A Temporal Graph ConvolutionalNetwork for Traffic Prediction,"Accurate and real-time traffic forecasting plays an important role in the
Intelligent Traffic System and is of great significance for urban traffic
planning, traffic management, and traffic control. However, traffic forecasting
has always been considered an open scientific issue, owing to the constraints
of urban road network topological structure and the law of dynamic change with
time, namely, spatial dependence and temporal dependence. To capture the
spatial and temporal dependence simultaneously, we propose a novel neural
network-based traffic forecasting method, the temporal graph convolutional
network (T-GCN) model, which is in combination with the graph convolutional
network (GCN) and gated recurrent unit (GRU). Specifically, the GCN is used to
learn complex topological structures to capture spatial dependence and the
gated recurrent unit is used to learn dynamic changes of traffic data to
capture temporal dependence. Then, the T-GCN model is employed to traffic
forecasting based on the urban road network. Experiments demonstrate that our
T-GCN model can obtain the spatio-temporal correlation from traffic data and
the predictions outperform state-of-art baselines on real-world traffic
datasets. Our tensorflow implementation of the T-GCN is available at
https://github.com/lehaifeng/T-GCN.",arxiv
http://arxiv.org/abs/1606.01581v1,2016-06-05T23:30:36Z,2016-06-05T23:30:36Z,Big Data Caching for Networking: Moving from Cloud to Edge,"In order to cope with the relentless data tsunami in $5G$ wireless networks,
current approaches such as acquiring new spectrum, deploying more base stations
(BSs) and increasing nodes in mobile packet core networks are becoming
ineffective in terms of scalability, cost and flexibility. In this regard,
context-aware $5$G networks with edge/cloud computing and exploitation of
\emph{big data} analytics can yield significant gains to mobile operators. In
this article, proactive content caching in $5$G wireless networks is
investigated in which a big data-enabled architecture is proposed. In this
practical architecture, vast amount of data is harnessed for content popularity
estimation and strategic contents are cached at the BSs to achieve higher
users' satisfaction and backhaul offloading. To validate the proposed solution,
we consider a real-world case study where several hours of mobile data traffic
is collected from a major telecom operator in Turkey and a big data-enabled
analysis is carried out leveraging tools from machine learning. Based on the
available information and storage capacity, numerical studies show that several
gains are achieved both in terms of users' satisfaction and backhaul
offloading. For example, in the case of $16$ BSs with $30\%$ of content ratings
and $13$ Gbyte of storage size ($78\%$ of total library size), proactive
caching yields $100\%$ of users' satisfaction and offloads $98\%$ of the
backhaul.",arxiv
http://arxiv.org/abs/2001.05097v1,2020-01-15T01:31:01Z,2020-01-15T01:31:01Z,"Lightweight 3D Human Pose Estimation Network Training Using
  Teacher-Student Learning","We present MoVNect, a lightweight deep neural network to capture 3D human
pose using a single RGB camera. To improve the overall performance of the
model, we apply the teacher-student learning method based knowledge
distillation to 3D human pose estimation. Real-time post-processing makes the
CNN output yield temporally stable 3D skeletal information, which can be used
in applications directly. We implement a 3D avatar application running on
mobile in real-time to demonstrate that our network achieves both high accuracy
and fast inference time. Extensive evaluations show the advantages of our
lightweight model with the proposed training method over previous 3D pose
estimation methods on the Human3.6M dataset and mobile devices.",arxiv
http://arxiv.org/abs/1808.05444v1,2018-08-16T12:30:52Z,2018-08-16T12:30:52Z,"DRLGENCERT: Deep Learning-based Automated Testing of Certificate
  Verification in SSL/TLS Implementations","The Secure Sockets Layer (SSL) and Transport Layer Security (TLS) protocols
are the foundation of network security. The certificate verification in SSL/TLS
implementations is vital and may become the weak link in the whole network
ecosystem. In previous works, some research focused on the automated testing of
certificate verification, and the main approaches rely on generating massive
certificates through randomly combining parts of seed certificates for fuzzing.
Although the generated certificates could meet the semantic constraints, the
cost is quite heavy, and the performance is limited due to the randomness. To
fill this gap, in this paper, we propose DRLGENCERT, the first framework of
applying deep reinforcement learning to the automated testing of certificate
verification in SSL/TLS implementations. DRLGENCERT accepts ordinary
certificates as input and outputs newly generated certificates which could
trigger discrepancies with high efficiency. Benefited by the deep reinforcement
learning, when generating certificates, our framework could choose the best
next action according to the result of a previous modification, instead of
simple random combinations. At the same time, we developed a set of new
techniques to support the overall design, like new feature extraction method
for X.509 certificates, fine-grained differential testing, and so forth. Also,
we implemented a prototype of DRLGENCERT and carried out a series of real-world
experiments. The results show DRLGENCERT is quite efficient, and we obtained
84,661 discrepancy-triggering certificates from 181,900 certificate seeds, say
around 46.5% effectiveness. Also, we evaluated six popular SSL/TLS
implementations, including GnuTLS, MatrixSSL, MbedTLS, NSS, OpenSSL, and
wolfSSL. DRLGENCERT successfully discovered 23 serious certificate verification
flaws, and most of them were previously unknown.",arxiv
http://arxiv.org/abs/1908.07466v2,2021-08-20T02:10:42Z,2019-08-15T07:44:51Z,"Secure Computation Offloading in Blockchain based IoT Networks with Deep
  Reinforcement Learning","For current and future Internet of Things (IoT) networks, mobile edge-cloud
computation offloading (MECCO) has been regarded as a promising means to
support delay-sensitive IoT applications. However, offloading mobile tasks to
the cloud is vulnerable to security issues due to malicious mobile devices
(MDs). How to implement offloading to alleviate computation burdens at MDs
while guaranteeing high security in mobile edge cloud is a challenging problem.
In this paper, we investigate simultaneously the security and computation
offloading problems in a multi-user MECCO system with blockchain. First, to
improve the offloading security, we propose a trustworthy access control using
blockchain, which can protect cloud resources against illegal offloading
behaviours. Then, to tackle the computation management of authorized MDs, we
formulate a computation offloading problem by jointly optimizing the offloading
decisions, the allocation of computing resource and radio bandwidth, and smart
contract usage. This optimization problem aims to minimize the long-term system
costs of latency, energy consumption and smart contract fee among all MDs. To
solve the proposed offloading problem, we develop an advanced deep
reinforcement learning algorithm using a double-dueling Q-network. Evaluation
results from real experiments and numerical simulations demonstrate the
significant advantages of our scheme over existing approaches.",arxiv
http://arxiv.org/abs/1708.04728v2,2018-01-10T23:41:57Z,2017-08-16T00:45:22Z,"DeepRebirth: Accelerating Deep Neural Network Execution on Mobile
  Devices","Deploying deep neural networks on mobile devices is a challenging task.
Current model compression methods such as matrix decomposition effectively
reduce the deployed model size, but still cannot satisfy real-time processing
requirement. This paper first discovers that the major obstacle is the
excessive execution time of non-tensor layers such as pooling and normalization
without tensor-like trainable parameters. This motivates us to design a novel
acceleration framework: DeepRebirth through ""slimming"" existing consecutive and
parallel non-tensor and tensor layers. The layer slimming is executed at
different substructures: (a) streamline slimming by merging the consecutive
non-tensor and tensor layer vertically; (b) branch slimming by merging
non-tensor and tensor branches horizontally. The proposed optimization
operations significantly accelerate the model execution and also greatly reduce
the run-time memory cost since the slimmed model architecture contains less
hidden layers. To maximally avoid accuracy loss, the parameters in new
generated layers are learned with layer-wise fine-tuning based on both
theoretical analysis and empirical verification. As observed in the experiment,
DeepRebirth achieves more than 3x speed-up and 2.5x run-time memory saving on
GoogLeNet with only 0.4% drop of top-5 accuracy on ImageNet. Furthermore, by
combining with other model compression techniques, DeepRebirth offers an
average of 65ms inference time on the CPU of Samsung Galaxy S6 with 86.5% top-5
accuracy, 14% faster than SqueezeNet which only has a top-5 accuracy of 80.5%.",arxiv
http://arxiv.org/abs/1011.6129v1,2010-11-29T05:05:38Z,2010-11-29T05:05:38Z,Toward a Push-Scalable Global Internet,"Push message delivery, where a client maintains an ``always-on'' connection
with a server in order to be notified of a (asynchronous) message arrival in
real-time, is increasingly being used in Internet services. The key message in
this paper is that push message delivery on the World Wide Web is not scalable
for servers, intermediate network elements, and battery-operated mobile device
clients. We present a measurement analysis of a commercially deployed WWW push
email service to highlight some of these issues. Next, we suggest content-based
optimization to reduce the always-on connection requirement of push messaging.
Our idea is based on exploiting the periodic nature of human-to-human
messaging. We show how machine learning can accurately model the times of a day
or week when messages are least likely to arrive; and turn off always-on
connections these times. We apply our approach to a real email data set and our
experiments demonstrate that the number of hours of active always-on
connections can be cut by half while still achieving real-time message delivery
for up to 90% of all messages.",arxiv
http://arxiv.org/abs/2103.08538v1,2021-03-15T17:03:22Z,2021-03-15T17:03:22Z,"UrbanVCA: a vector-based cellular automata framework to simulate the
  urban land-use change at the land-parcel level","Vector-based cellular automata (CA) based on real land-parcel has become an
important trend in current urban development simulation studies. Compared with
raster-based and parcel-based CA models, vector CA models are difficult to be
widely used because of their complex data structures and technical
difficulties. The UrbanVCA, a brand-new vector CA-based urban development
simulation framework was proposed in this study, which supports multiple
machine-learning models. To measure the simulation accuracy better, this study
also first proposes a vector-based landscape index (VecLI) model based on the
real land-parcels. Using Shunde, Guangdong as the study area, the UrbanVCA
simulates multiple types of urban land-use changes at the land-parcel level
have achieved a high accuracy (FoM=0.243) and the landscape index similarity
reaches 87.3%. The simulation results in 2030 show that the eco-protection
scenario can promote urban agglomeration and reduce ecological aggression and
loss of arable land by at least 60%. Besides, we have developed and released
UrbanVCA software for urban planners and researchers.",arxiv
http://arxiv.org/abs/2009.05780v1,2020-09-12T12:38:47Z,2020-09-12T12:38:47Z,"EdgeLoc: An Edge-IoT Framework for Robust Indoor Localization Using
  Capsule Networks","With the unprecedented demand for location-based services in indoor
scenarios, wireless indoor localization has become essential for mobile users.
While GPS is not available at indoor spaces, WiFi RSS fingerprinting has become
popular with its ubiquitous accessibility. However, it is challenging to
achieve robust and efficient indoor localization with two major challenges.
First, the localization accuracy can be degraded by the random signal
fluctuations, which would influence conventional localization algorithms that
simply learn handcrafted features from raw fingerprint data. Second, mobile
users are sensitive to the localization delay, but conventional indoor
localization algorithms are computation-intensive and time-consuming. In this
paper, we propose EdgeLoc, an edge-IoT framework for efficient and robust
indoor localization using capsule networks. We develop a deep learning model
with the CapsNet to efficiently extract hierarchical information from WiFi
fingerprint data, thereby significantly improving the localization accuracy.
Moreover, we implement an edge-computing prototype system to achieve a nearly
real-time localization process, by enabling mobile users with the deep-learning
model that has been well-trained by the edge server. We conduct a real-world
field experimental study with over 33,600 data points and an extensive
synthetic experiment with the open dataset, and the experimental results
validate the effectiveness of EdgeLoc. The best trade-off of the EdgeLoc system
achieves 98.5% localization accuracy within an average positioning time of only
2.31 ms in the field experiment.",arxiv
http://arxiv.org/abs/2005.00953v1,2020-05-03T00:12:38Z,2020-05-03T00:12:38Z,"Deep Generative Adversarial Residual Convolutional Networks for
  Real-World Super-Resolution","Most current deep learning based single image super-resolution (SISR) methods
focus on designing deeper / wider models to learn the non-linear mapping
between low-resolution (LR) inputs and the high-resolution (HR) outputs from a
large number of paired (LR/HR) training data. They usually take as assumption
that the LR image is a bicubic down-sampled version of the HR image. However,
such degradation process is not available in real-world settings i.e. inherent
sensor noise, stochastic noise, compression artifacts, possible mismatch
between image degradation process and camera device. It reduces significantly
the performance of current SISR methods due to real-world image corruptions. To
address these problems, we propose a deep Super-Resolution Residual
Convolutional Generative Adversarial Network (SRResCGAN) to follow the
real-world degradation settings by adversarial training the model with
pixel-wise supervision in the HR domain from its generated LR counterpart. The
proposed network exploits the residual learning by minimizing the energy-based
objective function with powerful image regularization and convex optimization
techniques. We demonstrate our proposed approach in quantitative and
qualitative experiments that generalize robustly to real input and it is easy
to deploy for other down-scaling operators and mobile/embedded devices.",arxiv
http://arxiv.org/abs/1812.04480v3,2020-07-01T04:28:23Z,2018-12-09T06:38:00Z,"A Hybrid Distribution Feeder Long-Term Load Forecasting Method Based on
  Sequence Prediction","Distribution feeder long-term load forecast (LTLF) is a critical task many
electric utility companies perform on an annual basis. The goal of this task is
to forecast the annual load of distribution feeders. The previous top-down and
bottom-up LTLF methods are unable to incorporate different levels of
information. This paper proposes a hybrid modeling method using sequence
prediction for this classic and important task. The proposed method can
seamlessly integrate top-down, bottom-up and sequential information hidden in
multi-year data. Two advanced sequence prediction models Long Short-Term Memory
(LSTM) and Gated Recurrent Unit (GRU) networks are investigated in this paper.
They successfully solve the vanishing and exploding gradient problems a
standard recurrent neural network has. This paper firstly explains the theories
of LSTM and GRU networks and then discusses the steps of feature selection,
feature engineering and model implementation in detail. In the end, a
real-world application example for a large urban grid in West Canada is
provided. LSTM and GRU networks under different sequential configurations and
traditional models including bottom-up, ARIMA and feed-forward neural network
are all implemented and compared in detail. The proposed method demonstrates
superior performance and great practicality.",arxiv
http://arxiv.org/abs/1707.02880v2,2017-08-22T19:26:08Z,2017-07-10T14:34:06Z,Deep Bilateral Learning for Real-Time Image Enhancement,"Performance is a critical challenge in mobile image processing. Given a
reference imaging pipeline, or even human-adjusted pairs of images, we seek to
reproduce the enhancements and enable real-time evaluation. For this, we
introduce a new neural network architecture inspired by bilateral grid
processing and local affine color transforms. Using pairs of input/output
images, we train a convolutional neural network to predict the coefficients of
a locally-affine model in bilateral space. Our architecture learns to make
local, global, and content-dependent decisions to approximate the desired image
transformation. At runtime, the neural network consumes a low-resolution
version of the input image, produces a set of affine transformations in
bilateral space, upsamples those transformations in an edge-preserving fashion
using a new slicing node, and then applies those upsampled transformations to
the full-resolution image. Our algorithm processes high-resolution images on a
smartphone in milliseconds, provides a real-time viewfinder at 1080p
resolution, and matches the quality of state-of-the-art approximation
techniques on a large class of image operators. Unlike previous work, our model
is trained off-line from data and therefore does not require access to the
original operator at runtime. This allows our model to learn complex,
scene-dependent transformations for which no reference implementation is
available, such as the photographic edits of a human retoucher.",arxiv
http://arxiv.org/abs/1808.04490v1,2018-08-13T23:21:43Z,2018-08-13T23:21:43Z,"Mitigating Location Privacy Attacks on Mobile Devices using Dynamic App
  Sandboxing","We present the design, implementation and evaluation of a system, called
MATRIX, developed to protect the privacy of mobile device users from location
inference and sensor side-channel attacks. MATRIX gives users control and
visibility over location and sensor (e.g., Accelerometers and Gyroscopes)
accesses by mobile apps. It implements a PrivoScope service that audits all
location and sensor accesses by apps on the device and generates real-time
notifications and graphs for visualizing these accesses; and a Synthetic
Location service to enable users to provide obfuscated or synthetic location
trajectories or sensor traces to apps they find useful, but do not trust with
their private information. The services are designed to be extensible and easy
for users, hiding all of the underlying complexity from them. MATRIX also
implements a Location Provider component that generates realistic
privacy-preserving synthetic identities and trajectories for users by
incorporating traffic information using historical data from Google Maps
Directions API, and accelerations using statistical information from user
driving experiments. The random traffic patterns are generated by
modeling/solving user schedule using a randomized linear program and
modeling/solving for user driving behavior using a quadratic program. We
extensively evaluated MATRIX using user studies, popular location-driven apps
and machine learning techniques, and demonstrate that it is portable to most
Android devices globally, is reliable, has low-overhead, and generates
synthetic trajectories that are difficult to differentiate from real mobility
trajectories by an adversary.",arxiv
http://arxiv.org/abs/1606.02147v1,2016-06-07T14:09:27Z,2016-06-07T14:09:27Z,"ENet: A Deep Neural Network Architecture for Real-Time Semantic
  Segmentation","The ability to perform pixel-wise semantic segmentation in real-time is of
paramount importance in mobile applications. Recent deep neural networks aimed
at this task have the disadvantage of requiring a large number of floating
point operations and have long run-times that hinder their usability. In this
paper, we propose a novel deep neural network architecture named ENet
(efficient neural network), created specifically for tasks requiring low
latency operation. ENet is up to 18$\times$ faster, requires 75$\times$ less
FLOPs, has 79$\times$ less parameters, and provides similar or better accuracy
to existing models. We have tested it on CamVid, Cityscapes and SUN datasets
and report on comparisons with existing state-of-the-art methods, and the
trade-offs between accuracy and processing time of a network. We present
performance measurements of the proposed architecture on embedded systems and
suggest possible software improvements that could make ENet even faster.",arxiv
http://arxiv.org/abs/1901.05356v1,2019-01-16T15:56:19Z,2019-01-16T15:56:19Z,"How to Host a Data Competition: Statistical Advice for Design and
  Analysis of a Data Competition","Data competitions rely on real-time leaderboards to rank competitor entries
and stimulate algorithm improvement. While such competitions have become quite
popular and prevalent, particularly in supervised learning formats, their
implementations by the host are highly variable. Without careful planning, a
supervised learning competition is vulnerable to overfitting, where the winning
solutions are so closely tuned to the particular set of provided data that they
cannot generalize to the underlying problem of interest to the host. This paper
outlines some important considerations for strategically designing relevant and
informative data sets to maximize the learning outcome from hosting a
competition based on our experience. It also describes a post-competition
analysis that enables robust and efficient assessment of the strengths and
weaknesses of solutions from different competitors, as well as greater
understanding of the regions of the input space that are well-solved. The
post-competition analysis, which complements the leaderboard, uses exploratory
data analysis and generalized linear models (GLMs). The GLMs not only expand
the range of results we can explore, they also provide more detailed analysis
of individual sub-questions including similarities and differences between
algorithms across different types of scenarios, universally easy or hard
regions of the input space, and different learning objectives. When coupled
with a strategically planned data generation approach, the methods provide
richer and more informative summaries to enhance the interpretation of results
beyond just the rankings on the leaderboard. The methods are illustrated with a
recently completed competition to evaluate algorithms capable of detecting,
identifying, and locating radioactive materials in an urban environment.",arxiv
http://arxiv.org/abs/1910.09667v1,2019-10-21T21:44:15Z,2019-10-21T21:44:15Z,"Combining Benefits from Trajectory Optimization and Deep Reinforcement
  Learning","Recent breakthroughs both in reinforcement learning and trajectory
optimization have made significant advances towards real world robotic system
deployment. Reinforcement learning (RL) can be applied to many problems without
needing any modeling or intuition about the system, at the cost of high sample
complexity and the inability to prove any metrics about the learned policies.
Trajectory optimization (TO) on the other hand allows for stability and
robustness analyses on generated motions and trajectories, but is only as good
as the often over-simplified derived model, and may have prohibitively
expensive computation times for real-time control. This paper seeks to combine
the benefits from these two areas while mitigating their drawbacks by (1)
decreasing RL sample complexity by using existing knowledge of the problem with
optimal control, and (2) providing an upper bound estimate on the
time-to-arrival of the combined learned-optimized policy, allowing online
policy deployment at any point in the training process by using the TO as a
worst-case scenario action. This method is evaluated for a car model, with
applicability to any mobile robotic system. A video showing policy execution
comparisons can be found at https://youtu.be/mv2xw83NyWU .",arxiv
http://arxiv.org/abs/2010.05842v2,2021-01-15T13:41:37Z,2020-10-12T16:46:40Z,Remote Electrical Tilt Optimization via Safe Reinforcement Learning,"Remote Electrical Tilt (RET) optimization is an efficient method for
adjusting the vertical tilt angle of Base Stations (BSs) antennas in order to
optimize Key Performance Indicators (KPIs) of the network. Reinforcement
Learning (RL) provides a powerful framework for RET optimization because of its
self-learning capabilities and adaptivity to environmental changes. However, an
RL agent may execute unsafe actions during the course of its interaction, i.e.,
actions resulting in undesired network performance degradation. Since the
reliability of services is critical for Mobile Network Operators (MNOs), the
prospect of performance degradation has prohibited the real-world deployment of
RL methods for RET optimization. In this work, we model the RET optimization
problem in the Safe Reinforcement Learning (SRL) framework with the goal of
learning a tilt control strategy providing performance improvement guarantees
with respect to a safe baseline. We leverage a recent SRL method, namely Safe
Policy Improvement through Baseline Bootstrapping (SPIBB), to learn an improved
policy from an offline dataset of interactions collected by the safe baseline.
Our experiments show that the proposed approach is able to learn a safe and
improved tilt update policy, providing a higher degree of reliability and
potential for real-world network deployment.",arxiv
http://arxiv.org/abs/2009.03693v1,2020-09-07T11:11:18Z,2020-09-07T11:11:18Z,"Deep Cyclic Generative Adversarial Residual Convolutional Networks for
  Real Image Super-Resolution","Recent deep learning based single image super-resolution (SISR) methods
mostly train their models in a clean data domain where the low-resolution (LR)
and the high-resolution (HR) images come from noise-free settings (same domain)
due to the bicubic down-sampling assumption. However, such degradation process
is not available in real-world settings. We consider a deep cyclic network
structure to maintain the domain consistency between the LR and HR data
distributions, which is inspired by the recent success of CycleGAN in the
image-to-image translation applications. We propose the Super-Resolution
Residual Cyclic Generative Adversarial Network (SRResCycGAN) by training with a
generative adversarial network (GAN) framework for the LR to HR domain
translation in an end-to-end manner. We demonstrate our proposed approach in
the quantitative and qualitative experiments that generalize well to the real
image super-resolution and it is easy to deploy for the mobile/embedded
devices. In addition, our SR results on the AIM 2020 Real Image SR Challenge
datasets demonstrate that the proposed SR approach achieves comparable results
as the other state-of-art methods.",arxiv
http://arxiv.org/abs/2107.10146v1,2021-07-21T15:23:52Z,2021-07-21T15:23:52Z,A Deep Reinforcement Learning Approach for Fair Traffic Signal Control,"Traffic signal control is one of the most effective methods of traffic
management in urban areas. In recent years, traffic control methods based on
deep reinforcement learning (DRL) have gained attention due to their ability to
exploit real-time traffic data, which is often poorly used by the traditional
hand-crafted methods. While most recent DRL-based methods have focused on
maximizing the throughput or minimizing the average travel time of the
vehicles, the fairness of the traffic signal controllers has often been
neglected. This is particularly important as neglecting fairness can lead to
situations where some vehicles experience extreme waiting times, or where the
throughput of a particular traffic flow is highly impacted by the fluctuations
of another conflicting flow at the intersection. In order to address these
issues, we introduce two notions of fairness: delay-based and throughput-based
fairness, which correspond to the two issues mentioned above. Furthermore, we
propose two DRL-based traffic signal control methods for implementing these
fairness notions, that can achieve a high throughput as well. We evaluate the
performance of our proposed methods using three traffic arrival distributions,
and find that our methods outperform the baselines in the tested scenarios.",arxiv
http://arxiv.org/abs/1411.7910v1,2014-11-28T15:38:23Z,2014-11-28T15:38:23Z,"A Flexible Framework for Accurate Simulation of Cloud In-Memory Data
  Stores","In-memory (transactional) data stores are recognized as a first-class data
management technology for cloud platforms, thanks to their ability to match the
elasticity requirements imposed by the pay-as-you-go cost model. On the other
hand, defining the well-suited amount of cache servers to be deployed, and the
degree of in-memory replication of slices of data, in order to optimize
reliability/availability and performance tradeoffs, is far from being a trivial
task. Yet, it is an essential aspect of the provisioning process of cloud
platforms, given that it has an impact on how well cloud resources are actually
exploited. To cope with the issue of determining optimized configurations of
cloud in-memory data stores, in this article we present a flexible simulation
framework offering skeleton simulation models that can be easily specialized in
order to capture the dynamics of diverse data grid systems, such as those
related to the specific protocol used to provide data consistency and/or
transactional guarantees. Besides its flexibility, another peculiar aspect of
the framework lies in that it integrates simulation and machine-learning
(black-box) techniques, the latter being essentially used to capture the
dynamics of the data-exchange layer (e.g. the message passing layer) across the
cache servers. This is a relevant aspect when considering that the actual
data-transport/networking infrastructure on top of which the data grid is
deployed might be unknown, hence being not feasible to be modeled via white-box
(namely purely simulative) approaches. We also provide an extended experimental
study aimed at validating instances of simulation models supported by our
framework against execution dynamics of real data grid systems deployed on top
of either private or public cloud infrastructures.",arxiv
http://arxiv.org/abs/2111.05199v1,2021-11-09T15:18:03Z,2021-11-09T15:18:03Z,"Deep diffusion-based forecasting of COVID-19 by incorporating
  network-level mobility information","Modeling the spatiotemporal nature of the spread of infectious diseases can
provide useful intuition in understanding the time-varying aspect of the
disease spread and the underlying complex spatial dependency observed in
people's mobility patterns. Besides, the county level multiple related time
series information can be leveraged to make a forecast on an individual time
series. Adding to this challenge is the fact that real-time data often deviates
from the unimodal Gaussian distribution assumption and may show some complex
mixed patterns. Motivated by this, we develop a deep learning-based time-series
model for probabilistic forecasting called Auto-regressive Mixed Density
Dynamic Diffusion Network(ARM3Dnet), which considers both people's mobility and
disease spread as a diffusion process on a dynamic directed graph. The Gaussian
Mixture Model layer is implemented to consider the multimodal nature of the
real-time data while learning from multiple related time series. We show that
our model, when trained with the best combination of dynamic covariate features
and mixture components, can outperform both traditional statistical and deep
learning models in forecasting the number of Covid-19 deaths and cases at the
county level in the United States.",arxiv
http://arxiv.org/abs/2104.01036v1,2021-04-02T13:17:11Z,2021-04-02T13:17:11Z,"Hybrid Policy Learning for Energy-Latency Tradeoff in MEC-Assisted VR
  Video Service","Virtual reality (VR) is promising to fundamentally transform a broad spectrum
of industry sectors and the way humans interact with virtual content. However,
despite unprecedented progress, current networking and computing
infrastructures are incompetent to unlock VR's full potential. In this paper,
we consider delivering the wireless multi-tile VR video service over a mobile
edge computing (MEC) network. The primary goal is to minimize the system
latency/energy consumption and to arrive at a tradeoff thereof. To this end, we
first cast the time-varying view popularity as a model-free Markov chain to
effectively capture its dynamic characteristics. After jointly assessing the
caching and computing capacities on both the MEC server and the VR playback
device, a hybrid policy is then implemented to coordinate the dynamic caching
replacement and the deterministic offloading, so as to fully utilize the system
resources. The underlying multi-objective problem is reformulated as a
partially observable Markov decision process, and a deep deterministic policy
gradient algorithm is proposed to iteratively learn its solution, where a long
short-term memory neural network is embedded to continuously predict the
dynamics of the unobservable popularity. Simulation results demonstrate the
superiority of the proposed scheme in achieving a trade-off between the energy
efficiency and the latency reduction over the baseline methods.",arxiv
http://arxiv.org/abs/1611.01235v1,2016-11-04T01:10:07Z,2016-11-04T01:10:07Z,"A Self-Driving Robot Using Deep Convolutional Neural Networks on
  Neuromorphic Hardware","Neuromorphic computing is a promising solution for reducing the size, weight
and power of mobile embedded systems. In this paper, we introduce a realization
of such a system by creating the first closed-loop battery-powered
communication system between an IBM TrueNorth NS1e and an autonomous
Android-Based Robotics platform. Using this system, we constructed a dataset of
path following behavior by manually driving the Android-Based robot along steep
mountain trails and recording video frames from the camera mounted on the robot
along with the corresponding motor commands. We used this dataset to train a
deep convolutional neural network implemented on the TrueNorth NS1e. The NS1e,
which was mounted on the robot and powered by the robot's battery, resulted in
a self-driving robot that could successfully traverse a steep mountain path in
real time. To our knowledge, this represents the first time the TrueNorth NS1e
neuromorphic chip has been embedded on a mobile platform under closed-loop
control.",arxiv
http://arxiv.org/abs/2006.10748v1,2020-06-17T22:03:31Z,2020-06-17T22:03:31Z,"Genetic Programming visitation scheduling solution can deliver a less
  austere COVID-19 pandemic population lockdown","A computational methodology is introduced to minimize infection opportunities
for people suffering some degree of lockdown in response to a pandemic, as is
the 2020 COVID-19 pandemic. Persons use their mobile phone or computational
device to request trips to places of their need or interest indicating a rough
time of day: `morning', `afternoon', `night' or `any time' when they would like
to undertake these outings as well as the desired place to visit. An artificial
intelligence methodology which is a variant of Genetic Programming studies all
requests and responds with specific time allocations for such visits that
minimize the overall risks of infection, hospitalization and death of people. A
number of alternatives for this computation are presented and results of
numerical experiments involving over 230 people of various ages and background
health levels in over 1700 visits that take place over three consecutive days.
A novel partial infection model is introduced to discuss these proof of concept
solutions which are compared to round robin uninformed time scheduling for
visits to places. The computations indicate vast improvements with far fewer
dead and hospitalized. These auger well for a more realistic study using
accurate infection models with the view to test deployment in the real world.
The input that drives the infection model is the degree of infection by
taxonomic class, such as the information that may arise from population testing
for COVID-19 or, alternatively, any contamination model. The taxonomy class
assumed in the computations is the likely level of infection by age group.",arxiv
http://arxiv.org/abs/1711.07480v1,2017-11-20T17:58:10Z,2017-11-20T17:58:10Z,E-PUR: An Energy-Efficient Processing Unit for Recurrent Neural Networks,"Recurrent Neural Networks (RNNs) are a key technology for emerging
applications such as automatic speech recognition, machine translation or image
description. Long Short Term Memory (LSTM) networks are the most successful RNN
implementation, as they can learn long term dependencies to achieve high
accuracy. Unfortunately, the recurrent nature of LSTM networks significantly
constrains the amount of parallelism and, hence, multicore CPUs and many-core
GPUs exhibit poor efficiency for RNN inference. In this paper, we present
E-PUR, an energy-efficient processing unit tailored to the requirements of LSTM
computation. The main goal of E-PUR is to support large recurrent neural
networks for low-power mobile devices. E-PUR provides an efficient hardware
implementation of LSTM networks that is flexible to support diverse
applications. One of its main novelties is a technique that we call Maximizing
Weight Locality (MWL), which improves the temporal locality of the memory
accesses for fetching the synaptic weights, reducing the memory requirements by
a large extent. Our experimental results show that E-PUR achieves real-time
performance for different LSTM networks, while reducing energy consumption by
orders of magnitude with respect to general-purpose processors and GPUs, and it
requires a very small chip area. Compared to a modern mobile SoC, an NVIDIA
Tegra X1, E-PUR provides an average energy reduction of 92x.",arxiv
http://arxiv.org/abs/2109.09435v1,2021-09-20T11:33:09Z,2021-09-20T11:33:09Z,Incremental Learning Techniques for Online Human Activity Recognition,"Unobtrusive and smart recognition of human activities using smartphones
inertial sensors is an interesting topic in the field of artificial
intelligence acquired tremendous popularity among researchers, especially in
recent years. A considerable challenge that needs more attention is the
real-time detection of physical activities, since for many real-world
applications such as health monitoring and elderly care, it is required to
recognize users' activities immediately to prevent severe damages to
individuals' wellness. In this paper, we propose a human activity recognition
(HAR) approach for the online prediction of physical movements, benefiting from
the capabilities of incremental learning algorithms. We develop a HAR system
containing monitoring software and a mobile application that collects
accelerometer and gyroscope data and send them to a remote server via the
Internet for classification and recognition operations. Six incremental
learning algorithms are employed and evaluated in this work and compared with
several batch learning algorithms commonly used for developing offline HAR
systems. The Final results indicated that considering all performance
evaluation metrics, Incremental K-Nearest Neighbors and Incremental Naive
Bayesian outperformed other algorithms, exceeding a recognition accuracy of 95%
in real-time.",arxiv
http://arxiv.org/abs/1902.02380v1,2019-02-06T19:49:22Z,2019-02-06T19:49:22Z,Compression of Recurrent Neural Networks for Efficient Language Modeling,"Recurrent neural networks have proved to be an effective method for
statistical language modeling. However, in practice their memory and run-time
complexity are usually too large to be implemented in real-time offline mobile
applications. In this paper we consider several compression techniques for
recurrent neural networks including Long-Short Term Memory models. We make
particular attention to the high-dimensional output problem caused by the very
large vocabulary size. We focus on effective compression methods in the context
of their exploitation on devices: pruning, quantization, and matrix
decomposition approaches (low-rank factorization and tensor train
decomposition, in particular). For each model we investigate the trade-off
between its size, suitability for fast inference and perplexity. We propose a
general pipeline for applying the most suitable methods to compress recurrent
neural networks for language modeling. It has been shown in the experimental
study with the Penn Treebank (PTB) dataset that the most efficient results in
terms of speed and compression-perplexity balance are obtained by matrix
decomposition techniques.",arxiv
http://arxiv.org/abs/2005.07649v2,2021-09-23T01:42:58Z,2020-05-15T17:09:10Z,"Convolutional Neural Network for emotion recognition to assist
  psychiatrists and psychologists during the COVID-19 pandemic: experts opinion","A web application with real-time emotion recognition for psychologists and
psychiatrists is presented. Mental health effects during COVID-19 quarantine
need to be handled because society is being emotionally impacted. The human
micro-expressions can describe genuine emotions that can be captured by
Convolutional Neural Networks (CNN) models. But the challenge is to implement
it under the poor performance of a part of society computers and the low speed
of internet connection, i.e., improve the computational efficiency and reduce
the data transfer. To validate the computational efficiency premise, we compare
CNN architectures results, collecting the floating-point operations per second
(FLOPS), the Number of Parameters (NP) and accuracy from the MobileNet,
PeleeNet, Extended Deep Neural Network (EDNN), Inception- Based Deep Neural
Network (IDNN) and our proposed Residual mobile-based Network model (ResmoNet).
Also, we compare the trained models results in terms of Main Memory Utilization
(MMU) and Response Time to complete the Emotion (RTE) recognition. Besides, we
design a data transfer that includes the raw data of emotions and the basic
patient information. The web application was evaluated with the System
Usability Scale (SUS) and a utility questionnaire by psychologists and
psychiatrists. ResmoNet model generated the most reduced NP, FLOPS, and MMU
results, only EDNN overcomes ResmoNet in 0.01sec in RTE. The optimizations to
our model impacted the accuracy, therefore IDNN and EDNN are 0.02 and 0.05 more
accurate than our model respectively. Finally, according to psychologists and
psychiatrists, the web application has good usability (73.8 of 100) and utility
(3.94 of 5).",arxiv
http://arxiv.org/abs/1512.02972v1,2015-12-09T18:08:59Z,2015-12-09T18:08:59Z,Get More With Less: Near Real-Time Image Clustering on Mobile Phones,"Machine learning algorithms, in conjunction with user data, hold the promise
of revolutionizing the way we interact with our phones, and indeed their
widespread adoption in the design of apps bear testimony to this promise.
However, currently, the computationally expensive segments of the learning
pipeline, such as feature extraction and model training, are offloaded to the
cloud, resulting in an over-reliance on the network and under-utilization of
computing resources available on mobile platforms. In this paper, we show that
by combining the computing power distributed over a number of phones, judicious
optimization choices, and contextual information it is possible to execute the
end-to-end pipeline entirely on the phones at the edge of the network,
efficiently. We also show that by harnessing the power of this combination, it
is possible to execute a computationally expensive pipeline at near real-time.
  To demonstrate our approach, we implement an end-to-end image-processing
pipeline -- that includes feature extraction, vocabulary learning,
vectorization, and image clustering -- on a set of mobile phones. Our results
show a 75% improvement over the standard, full pipeline implementation running
on the phones without modification -- reducing the time to one minute under
certain conditions. We believe that this result is a promising indication that
fully distributed, infrastructure-less computing is possible on networks of
mobile phones; enabling a new class of mobile applications that are less
reliant on the cloud.",arxiv
http://arxiv.org/abs/2107.01784v1,2021-07-05T04:34:51Z,2021-07-05T04:34:51Z,"Learning a Model for Inferring a Spatial Road Lane Network Graph using
  Self-Supervision","Interconnected road lanes are a central concept for navigating urban roads.
Currently, most autonomous vehicles rely on preconstructed lane maps as
designing an algorithmic model is difficult. However, the generation and
maintenance of such maps is costly and hinders large-scale adoption of
autonomous vehicle technology. This paper presents the first self-supervised
learning method to train a model to infer a spatially grounded lane-level road
network graph based on a dense segmented representation of the road scene
generated from onboard sensors. A formal road lane network model is presented
and proves that any structured road scene can be represented by a directed
acyclic graph of at most depth three while retaining the notion of intersection
regions, and that this is the most compressed representation. The formal model
is implemented by a hybrid neural and search-based model, utilizing a novel
barrier function loss formulation for robust learning from partial labels.
Experiments are conducted for all common road intersection layouts. Results
show that the model can generalize to new road layouts, unlike previous
approaches, demonstrating its potential for real-world application as a
practical learning-based lane-level map generator.",arxiv
http://arxiv.org/abs/2102.04871v1,2021-02-09T15:14:27Z,2021-02-09T15:14:27Z,The Factory Must Grow: Automation in Factorio,"Efficient optimization of resources is paramount to success in many problems
faced today. In the field of operational research the efficient scheduling of
employees; packing of vans; routing of vehicles; logistics of airlines and
transport of materials can be the difference between emission reduction or
excess, profits or losses and feasibility or unworkable solutions. The video
game Factorio, by Wube Software, has a myriad of problems which are analogous
to such real-world problems, and is a useful simulator for developing solutions
for these problems. In this paper we define the logistic transport belt problem
and define mathematical integer programming model of it. We developed an
interface to allow optimizers in any programming language to interact with
Factorio, and we provide an initial benchmark of logistic transport belt
problems. We present results for Simulated Annealing, quick Genetic Programming
and Evolutionary Reinforcement Learning, three different meta-heuristic
techniques to optimize this novel problem.",arxiv
http://arxiv.org/abs/2108.06721v2,2021-11-19T21:04:10Z,2021-08-15T11:20:10Z,"Training for the Future: A Simple Gradient Interpolation Loss to
  Generalize Along Time","In several real world applications, machine learning models are deployed to
make predictions on data whose distribution changes gradually along time,
leading to a drift between the train and test distributions. Such models are
often re-trained on new data periodically, and they hence need to generalize to
data not too far into the future. In this context, there is much prior work on
enhancing temporal generalization, e.g. continuous transportation of past data,
kernel smoothed time-sensitive parameters and more recently, adversarial
learning of time-invariant features. However, these methods share several
limitations, e.g, poor scalability, training instability, and dependence on
unlabeled data from the future. Responding to the above limitations, we propose
a simple method that starts with a model with time-sensitive parameters but
regularizes its temporal complexity using a Gradient Interpolation (GI) loss.
GI allows the decision boundary to change along time and can still prevent
overfitting to the limited training time snapshots by allowing task-specific
control over changes along time. We compare our method to existing baselines on
multiple real-world datasets, which show that GI outperforms more complicated
generative and adversarial approaches on the one hand, and simpler gradient
regularization methods on the other.",arxiv
http://arxiv.org/abs/2007.01542v1,2020-07-03T08:03:45Z,2020-07-03T08:03:45Z,Strategies for Using Proximal Policy Optimization in Mobile Puzzle Games,"While traditionally a labour intensive task, the testing of game content is
progressively becoming more automated. Among the many directions in which this
automation is taking shape, automatic play-testing is one of the most promising
thanks also to advancements of many supervised and reinforcement learning (RL)
algorithms. However these type of algorithms, while extremely powerful, often
suffer in production environments due to issues with reliability and
transparency in their training and usage.
  In this research work we are investigating and evaluating strategies to apply
the popular RL method Proximal Policy Optimization (PPO) in a casual mobile
puzzle game with a specific focus on improving its reliability in training and
generalization during game playing.
  We have implemented and tested a number of different strategies against a
real-world mobile puzzle game (Lily's Garden from Tactile Games). We isolated
the conditions that lead to a failure in either training or generalization
during testing and we identified a few strategies to ensure a more stable
behaviour of the algorithm in this game genre.",arxiv
http://arxiv.org/abs/1902.00159v1,2019-02-01T03:24:26Z,2019-02-01T03:24:26Z,Compressing GANs using Knowledge Distillation,"Generative Adversarial Networks (GANs) have been used in several machine
learning tasks such as domain transfer, super resolution, and synthetic data
generation. State-of-the-art GANs often use tens of millions of parameters,
making them expensive to deploy for applications in low SWAP (size, weight, and
power) hardware, such as mobile devices, and for applications with real time
capabilities. There has been no work found to reduce the number of parameters
used in GANs. Therefore, we propose a method to compress GANs using knowledge
distillation techniques, in which a smaller ""student"" GAN learns to mimic a
larger ""teacher"" GAN. We show that the distillation methods used on MNIST,
CIFAR-10, and Celeb-A datasets can compress teacher GANs at ratios of 1669:1,
58:1, and 87:1, respectively, while retaining the quality of the generated
image. From our experiments, we observe a qualitative limit for GAN's
compression. Moreover, we observe that, with a fixed parameter budget,
compressed GANs outperform GANs trained using standard training methods. We
conjecture that this is partially owing to the optimization landscape of
over-parameterized GANs which allows efficient training using alternating
gradient descent. Thus, training an over-parameterized GAN followed by our
proposed compression scheme provides a high quality generative model with a
small number of parameters.",arxiv
http://arxiv.org/abs/1808.07840v1,2018-08-23T16:55:53Z,2018-08-23T16:55:53Z,Learning to Importance Sample in Primary Sample Space,"Importance sampling is one of the most widely used variance reduction
strategies in Monte Carlo rendering. In this paper, we propose a novel
importance sampling technique that uses a neural network to learn how to sample
from a desired density represented by a set of samples. Our approach considers
an existing Monte Carlo rendering algorithm as a black box. During a
scene-dependent training phase, we learn to generate samples with a desired
density in the primary sample space of the rendering algorithm using maximum
likelihood estimation. We leverage a recent neural network architecture that
was designed to represent real-valued non-volume preserving ('Real NVP')
transformations in high dimensional spaces. We use Real NVP to non-linearly
warp primary sample space and obtain desired densities. In addition, Real NVP
efficiently computes the determinant of the Jacobian of the warp, which is
required to implement the change of integration variables implied by the warp.
A main advantage of our approach is that it is agnostic of underlying light
transport effects, and can be combined with many existing rendering techniques
by treating them as a black box. We show that our approach leads to effective
variance reduction in several practical scenarios.",arxiv
http://arxiv.org/abs/1602.01208v3,2016-05-07T11:59:51Z,2016-02-03T06:56:51Z,"Spatial Concept Acquisition for a Mobile Robot that Integrates
  Self-Localization and Unsupervised Word Discovery from Spoken Sentences","In this paper, we propose a novel unsupervised learning method for the
lexical acquisition of words related to places visited by robots, from human
continuous speech signals. We address the problem of learning novel words by a
robot that has no prior knowledge of these words except for a primitive
acoustic model. Further, we propose a method that allows a robot to effectively
use the learned words and their meanings for self-localization tasks. The
proposed method is nonparametric Bayesian spatial concept acquisition method
(SpCoA) that integrates the generative model for self-localization and the
unsupervised word segmentation in uttered sentences via latent variables
related to the spatial concept. We implemented the proposed method SpCoA on
SIGVerse, which is a simulation environment, and TurtleBot2, which is a mobile
robot in a real environment. Further, we conducted experiments for evaluating
the performance of SpCoA. The experimental results showed that SpCoA enabled
the robot to acquire the names of places from speech sentences. They also
revealed that the robot could effectively utilize the acquired spatial concepts
and reduce the uncertainty in self-localization.",arxiv
http://arxiv.org/abs/2007.15215v1,2020-07-30T03:54:32Z,2020-07-30T03:54:32Z,"Learner's Dilemma: IoT Devices Training Strategies in Collaborative Deep
  Learning","With the growth of Internet of Things (IoT) and mo-bile edge computing,
billions of smart devices are interconnected to develop applications used in
various domains including smart homes, healthcare and smart manufacturing. Deep
learning has been extensively utilized in various IoT applications which
require huge amount of data for model training. Due to privacy requirements,
smart IoT devices do not release data to a remote third party for their use. To
overcome this problem, collaborative approach to deep learning, also known as
Collaborative DeepLearning (CDL) has been largely employed in data-driven
applications. This approach enables multiple edge IoT devices to train their
models locally on mobile edge devices. In this paper,we address IoT device
training problem in CDL by analyzing the behavior of mobile edge devices using
a game-theoretic model,where each mobile edge device aims at maximizing the
accuracy of its local model at the same time limiting the overhead of
participating in CDL. We analyze the Nash Equilibrium in anN-player static game
model. We further present a novel cluster-based fair strategy to approximately
solve the CDL game to enforce mobile edge devices for cooperation. Our
experimental results and evaluation analysis in a real-world smart home
deployment show that 80% mobile edge devices are ready to cooperate in CDL,
while 20% of them do not train their local models collaboratively.",arxiv
http://arxiv.org/abs/2110.01659v2,2021-10-06T17:21:46Z,2021-10-04T18:49:51Z,Cross-Modal Virtual Sensing for Combustion Instability Monitoring,"In many cyber-physical systems, imaging can be an important but expensive or
'difficult to deploy' sensing modality. One such example is detecting
combustion instability using flame images, where deep learning frameworks have
demonstrated state-of-the-art performance. The proposed frameworks are also
shown to be quite trustworthy such that domain experts can have sufficient
confidence to use these models in real systems to prevent unwanted incidents.
However, flame imaging is not a common sensing modality in engine combustors
today. Therefore, the current roadblock exists on the hardware side regarding
the acquisition and processing of high-volume flame images. On the other hand,
the acoustic pressure time series is a more feasible modality for data
collection in real combustors. To utilize acoustic time series as a sensing
modality, we propose a novel cross-modal encoder-decoder architecture that can
reconstruct cross-modal visual features from acoustic pressure time series in
combustion systems. With the ""distillation"" of cross-modal features, the
results demonstrate that the detection accuracy can be enhanced using the
virtual visual sensing modality. By providing the benefit of cross-modal
reconstruction, our framework can prove to be useful in different domains well
beyond the power generation and transportation industries.",arxiv
http://arxiv.org/abs/2004.06049v2,2020-09-09T06:09:20Z,2020-04-09T06:36:18Z,"A Prospective Look: Key Enabling Technologies, Applications and Open
  Research Topics in 6G Networks","The fifth generation (5G) mobile networks are envisaged to enable a plethora
of breakthrough advancements in wireless technologies, providing support of a
diverse set of services over a single platform. While the deployment of 5G
systems is scaling up globally, it is time to look ahead for beyond 5G systems.
This is driven by the emerging societal trends, calling for fully automated
systems and intelligent services supported by extended reality and haptics
communications. To accommodate the stringent requirements of their prospective
applications, which are data-driven and defined by extremely low-latency,
ultra-reliable, fast and seamless wireless connectivity, research initiatives
are currently focusing on a progressive roadmap towards the sixth generation
(6G) networks. In this article, we shed light on some of the major enabling
technologies for 6G, which are expected to revolutionize the fundamental
architectures of cellular networks and provide multiple homogeneous artificial
intelligence-empowered services, including distributed communications, control,
computing, sensing, and energy, from its core to its end nodes. Particularly,
this paper aims to answer several 6G framework related questions: What are the
driving forces for the development of 6G? How will the enabling technologies of
6G differ from those in 5G? What kind of applications and interactions will
they support which would not be supported by 5G? We address these questions by
presenting a profound study of the 6G vision and outlining five of its
disruptive technologies, i.e., terahertz communications, programmable
metasurfaces, drone-based communications, backscatter communications and
tactile internet, as well as their potential applications. Then, by leveraging
the state-of-the-art literature surveyed for each technology, we discuss their
requirements, key challenges, and open research problems.",arxiv
http://arxiv.org/abs/2006.01674v2,2020-06-07T10:04:40Z,2020-06-02T14:48:34Z,"A network paradigm for very high capacity mobile and fixed
  telecommunications ecosystem sustainable evolution","For very high capacity networks (VHC), the main objective is to improve the
quality of the end-user experience. This implies compliance with key
performance indicators (KPIs) required by applications. Key performance
indicators at the application level are throughput, download time, round trip
time, and video delay. They depend on the end-to-end connection between the
server and the end-user device. For VHC networks, Telco operators must provide
the required application quality. Moreover, they must meet the objectives of
economic sustainability. Today, Telco operators rarely achieve the above
objectives, mainly due to the push to increase the bit-rate of access networks
without considering the end-to-end KPIs of the applications. The main
contribution of this paper concerns the definition of a deployment framework to
address performance and cost issues for VHC networks. We show three actions on
which it is necessary to focus. First, limiting bit-rate through video
compression. Second, contain the rate of packet loss through artificial
intelligence algorithms for line stabilization. Third, reduce latency (i.e.,
round-trip time) with edge-cloud computing. The concerted and gradual
application of these measures can allow a Telco to get out of the
ultra-broadband ""trap"" of the access network, as defined in the paper. We
propose to work on end-to-end optimization of the bandwidth utilization ratio.
This leads to a better performance experienced by the end-user. It also allows
a Telco operator to create new business models and obtain new revenue streams
at a sustainable cost. To give a clear example, we describe how to realize
mobile virtual and augmented reality, which is one of the most challenging
future services.",arxiv
http://arxiv.org/abs/2110.01864v1,2021-10-05T08:00:46Z,2021-10-05T08:00:46Z,"Mobile authentication of copy detection patterns: how critical is to
  know fakes?","Protection of physical objects against counterfeiting is an important task
for the modern economies. In recent years, the high-quality counterfeits appear
to be closer to originals thanks to the rapid advancement of digital
technologies. To combat these counterfeits, an anti-counterfeiting technology
based on hand-crafted randomness implemented in a form of copy detection
patterns (CDP) is proposed enabling a link between the physical and digital
worlds and being used in various brand protection applications. The modern
mobile phone technologies make the verification process of CDP easier and
available to the end customers. Besides a big interest and attractiveness, the
CDP authentication based on the mobile phone imaging remains insufficiently
studied. In this respect, in this paper we aim at investigating the CDP
authentication under the real-life conditions with the codes printed on an
industrial printer and enrolled via a modern mobile phone under the regular
light conditions. The authentication aspects of the obtained CDP are
investigated with respect to the four types of copy fakes. The impact of fakes'
type used for training of authentication classifier is studied in two
scenarios: (i) supervised binary classification under various assumptions about
the fakes and (ii) one-class classification under unknown fakes. The obtained
results show that the modern machine-learning approaches and the technical
capacity of modern mobile phones allow to make the CDP authentication under
unknown fakes feasible with respect to the considered types of fakes and code
design.",arxiv
http://arxiv.org/abs/1711.05734v2,2018-02-20T21:43:55Z,2017-11-15T10:15:44Z,"Chipmunk: A Systolically Scalable 0.9 mm${}^2$, 3.08 Gop/s/mW @ 1.2 mW
  Accelerator for Near-Sensor Recurrent Neural Network Inference","Recurrent neural networks (RNNs) are state-of-the-art in voice
awareness/understanding and speech recognition. On-device computation of RNNs
on low-power mobile and wearable devices would be key to applications such as
zero-latency voice-based human-machine interfaces. Here we present Chipmunk, a
small (<1 mm${}^2$) hardware accelerator for Long-Short Term Memory RNNs in UMC
65 nm technology capable to operate at a measured peak efficiency up to 3.08
Gop/s/mW at 1.24 mW peak power. To implement big RNN models without incurring
in huge memory transfer overhead, multiple Chipmunk engines can cooperate to
form a single systolic array. In this way, the Chipmunk architecture in a 75
tiles configuration can achieve real-time phoneme extraction on a demanding RNN
topology proposed by Graves et al., consuming less than 13 mW of average power.",arxiv
http://arxiv.org/abs/1803.06312v2,2018-04-17T02:26:35Z,2018-03-16T16:59:47Z,EVA$^2$: Exploiting Temporal Redundancy in Live Computer Vision,"Hardware support for deep convolutional neural networks (CNNs) is critical to
advanced computer vision in mobile and embedded devices. Current designs,
however, accelerate generic CNNs; they do not exploit the unique
characteristics of real-time vision. We propose to use the temporal redundancy
in natural video to avoid unnecessary computation on most frames. A new
algorithm, activation motion compensation, detects changes in the visual input
and incrementally updates a previously-computed output. The technique takes
inspiration from video compression and applies well-known motion estimation
techniques to adapt to visual changes. We use an adaptive key frame rate to
control the trade-off between efficiency and vision quality as the input
changes. We implement the technique in hardware as an extension to existing
state-of-the-art CNN accelerator designs. The new unit reduces the average
energy per frame by 54.2%, 61.7%, and 87.6% for three CNNs with less than 1%
loss in vision accuracy.",arxiv
http://arxiv.org/abs/2101.10808v2,2021-04-23T05:14:54Z,2021-01-12T09:40:40Z,Fast Facial Landmark Detection and Applications: A Survey,"In this paper we survey and analyze modern neural-network-based facial
landmark detection algorithms. We focus on approaches that have led to a
significant increase in quality over the past few years on datasets with large
pose and emotion variability, high levels of face occlusions - all of which are
typical in real-world scenarios. We summarize the improvements into categories,
provide quality comparison on difficult and modern in-the-wild datasets: 300-W,
AFLW, WFLW, COFW. Additionally, we compare algorithm speed on CPU, GPU and
Mobile devices. For completeness, we also briefly touch on established methods
with open implementations available. Besides, we cover applications and
vulnerabilities of the landmark detection algorithms. Based on which, we raise
problems that as we hope will lead to further algorithm improvements.",arxiv
http://arxiv.org/abs/2105.12931v1,2021-05-27T03:54:38Z,2021-05-27T03:54:38Z,YOLO5Face: Why Reinventing a Face Detector,"Tremendous progress has been made on face detection in recent years using
convolutional neural networks. While many face detectors use designs designated
for the detection of face, we treat face detection as a general object
detection task. We implement a face detector based on YOLOv5 object detector
and call it YOLO5Face. We add a five-point landmark regression head into it and
use the Wing loss function. We design detectors with different model sizes,
from a large model to achieve the best performance, to a super small model for
real-time detection on an embedded or mobile device. Experiment results on the
WiderFace dataset show that our face detectors can achieve state-of-the-art
performance in almost all the Easy, Medium, and Hard subsets, exceeding the
more complex designated face detectors. The code is available at
\url{https://www.github.com/deepcam-cn/yolov5-face}.",arxiv
http://arxiv.org/abs/2002.12597v1,2020-02-28T08:46:12Z,2020-02-28T08:46:12Z,"An Efficient Method of Training Small Models for Regression Problems
  with Knowledge Distillation","Compressing deep neural network (DNN) models becomes a very important and
necessary technique for real-world applications, such as deploying those models
on mobile devices. Knowledge distillation is one of the most popular methods
for model compression, and many studies have been made on developing this
technique. However, those studies mainly focused on classification problems,
and very few attempts have been made on regression problems, although there are
many application of DNNs on regression problems. In this paper, we propose a
new formalism of knowledge distillation for regression problems. First, we
propose a new loss function, teacher outlier rejection loss, which rejects
outliers in training samples using teacher model predictions. Second, we
consider a multi-task network with two outputs: one estimates training labels
which is in general contaminated by noisy labels; And the other estimates
teacher model's output which is expected to modify the noise labels following
the memorization effects. By considering the multi-task network, training of
the feature extraction of student models becomes more effective, and it allows
us to obtain a better student model than one trained from scratch. We performed
comprehensive evaluation with one simple toy model: sinusoidal function, and
two open datasets: MPIIGaze, and Multi-PIE. Our results show consistent
improvement in accuracy regardless of the annotation error level in the
datasets.",arxiv
http://arxiv.org/abs/2003.03603v3,2020-08-10T12:56:06Z,2020-03-07T16:38:34Z,Generative Low-bitwidth Data Free Quantization,"Neural network quantization is an effective way to compress deep models and
improve their execution latency and energy efficiency, so that they can be
deployed on mobile or embedded devices. Existing quantization methods require
original data for calibration or fine-tuning to get better performance.
However, in many real-world scenarios, the data may not be available due to
confidential or private issues, thereby making existing quantization methods
not applicable. Moreover, due to the absence of original data, the recently
developed generative adversarial networks (GANs) cannot be applied to generate
data. Although the full-precision model may contain rich data information, such
information alone is hard to exploit for recovering the original data or
generating new meaningful data. In this paper, we investigate a
simple-yet-effective method called Generative Low-bitwidth Data Free
Quantization (GDFQ) to remove the data dependence burden. Specifically, we
propose a knowledge matching generator to produce meaningful fake data by
exploiting classification boundary knowledge and distribution information in
the pre-trained model. With the help of generated data, we can quantize a model
by learning knowledge from the pre-trained model. Extensive experiments on
three data sets demonstrate the effectiveness of our method. More critically,
our method achieves much higher accuracy on 4-bit quantization than the
existing data free quantization method. Code is available at
https://github.com/xushoukai/GDFQ.",arxiv
http://arxiv.org/abs/2009.09940v1,2020-09-08T02:08:20Z,2020-09-08T02:08:20Z,CNNPruner: Pruning Convolutional Neural Networks with Visual Analytics,"Convolutional neural networks (CNNs) have demonstrated extraordinarily good
performance in many computer vision tasks. The increasing size of CNN models,
however, prevents them from being widely deployed to devices with limited
computational resources, e.g., mobile/embedded devices. The emerging topic of
model pruning strives to address this problem by removing less important
neurons and fine-tuning the pruned networks to minimize the accuracy loss.
Nevertheless, existing automated pruning solutions often rely on a numerical
threshold of the pruning criteria, lacking the flexibility to optimally balance
the trade-off between model size and accuracy. Moreover, the complicated
interplay between the stages of neuron pruning and model fine-tuning makes this
process opaque, and therefore becomes difficult to optimize. In this paper, we
address these challenges through a visual analytics approach, named CNNPruner.
It considers the importance of convolutional filters through both instability
and sensitivity, and allows users to interactively create pruning plans
according to a desired goal on model size or accuracy. Also, CNNPruner
integrates state-of-the-art filter visualization techniques to help users
understand the roles that different filters played and refine their pruning
plans. Through comprehensive case studies on CNNs with real-world sizes, we
validate the effectiveness of CNNPruner.",arxiv
http://arxiv.org/abs/2103.15263v2,2021-03-30T14:17:16Z,2021-03-29T01:33:34Z,Zero-shot Adversarial Quantization,"Model quantization is a promising approach to compress deep neural networks
and accelerate inference, making it possible to be deployed on mobile and edge
devices. To retain the high performance of full-precision models, most existing
quantization methods focus on fine-tuning quantized model by assuming training
datasets are accessible. However, this assumption sometimes is not satisfied in
real situations due to data privacy and security issues, thereby making these
quantization methods not applicable. To achieve zero-short model quantization
without accessing training data, a tiny number of quantization methods adopt
either post-training quantization or batch normalization statistics-guided data
generation for fine-tuning. However, both of them inevitably suffer from low
performance, since the former is a little too empirical and lacks training
support for ultra-low precision quantization, while the latter could not fully
restore the peculiarities of original data and is often low efficient for
diverse data generation. To address the above issues, we propose a zero-shot
adversarial quantization (ZAQ) framework, facilitating effective discrepancy
estimation and knowledge transfer from a full-precision model to its quantized
model. This is achieved by a novel two-level discrepancy modeling to drive a
generator to synthesize informative and diverse data examples to optimize the
quantized model in an adversarial learning fashion. We conduct extensive
experiments on three fundamental vision tasks, demonstrating the superiority of
ZAQ over the strong zero-shot baselines and validating the effectiveness of its
main components. Code is available at <https://git.io/Jqc0y>.",arxiv
http://arxiv.org/abs/1601.02781v2,2017-05-19T11:23:33Z,2016-01-12T09:38:44Z,BAMCloud: A Cloud Based Mobile Biometric Authentication Framework,"With an exponential increase in number of users switching to mobile banking,
various countries are adopting biometric solutions as security measures. The
main reason for biometric technologies becoming more common in the everyday
lives of consumers is because of the facility to easily capture biometric data
in real time, using their mobile phones. Biometric technologies are providing
the potential security framework to make banking more convenient and secure
than it has ever been. At the same time, the exponential growth of enrollment
in the biometric system produces massive amount of high dimensionality data
that leads to degradation in the performance of the mobile banking systems.
Therefore, in order to overcome the performance issues arising due to this data
deluge, this paper aims to propose a distributed mobile biometric system based
on a high performance cluster Cloud. High availability, better time efficiency
and scalability are some of the added advantages of using the proposed system.
In this paper a Cloud based mobile biometric authentication framework
(BAMCloud) is proposed that uses dynamic signatures and performs
authentication. It includes the steps involving data capture using any handheld
mobile device, then storage, preprocessing and training the system in a
distributed manner over Cloud. For this purpose we have implemented it using
MapReduce on Hadoop platform and for training Levenberg-Marquardt
backpropagation neural network has been used. Moreover, the methodology adopted
is very novel as it achieves a speedup of 8.5x and a performance of 96.23%.
Furthermore, the cost benefit analysis of the implemented system shows that the
cost of implementation and execution of the system is lesser than the existing
ones. The experiments demonstrate that the better performance is achieved by
proposed framework as compared to the other methods used in the recent
literature.",arxiv
http://arxiv.org/abs/2102.06336v1,2021-02-12T03:07:06Z,2021-02-12T03:07:06Z,"Dancing along Battery: Enabling Transformer with Run-time
  Reconfigurability on Mobile Devices","A pruning-based AutoML framework for run-time reconfigurability, namely RT3,
is proposed in this work. This enables Transformer-based large Natural Language
Processing (NLP) models to be efficiently executed on resource-constrained
mobile devices and reconfigured (i.e., switching models for dynamic hardware
conditions) at run-time. Such reconfigurability is the key to save energy for
battery-powered mobile devices, which widely use dynamic voltage and frequency
scaling (DVFS) technique for hardware reconfiguration to prolong battery life.
In this work, we creatively explore a hybrid block-structured pruning (BP) and
pattern pruning (PP) for Transformer-based models and first attempt to combine
hardware and software reconfiguration to maximally save energy for
battery-powered mobile devices. Specifically, RT3 integrates two-level
optimizations: First, it utilizes an efficient BP as the first-step compression
for resource-constrained mobile devices; then, RT3 heuristically generates a
shrunken search space based on the first level optimization and searches
multiple pattern sets with diverse sparsity for PP via reinforcement learning
to support lightweight software reconfiguration, which corresponds to available
frequency levels of DVFS (i.e., hardware reconfiguration). At run-time, RT3 can
switch the lightweight pattern sets within 45ms to guarantee the required
real-time constraint at different frequency levels. Results further show that
RT3 can prolong battery life over 4x improvement with less than 1% accuracy
loss for Transformer and 1.5% score decrease for DistilBERT.",arxiv
http://arxiv.org/abs/2010.10903v1,2020-10-21T11:22:30Z,2020-10-21T11:22:30Z,"Visual Navigation in Real-World Indoor Environments Using End-to-End
  Deep Reinforcement Learning","Visual navigation is essential for many applications in robotics, from
manipulation, through mobile robotics to automated driving. Deep reinforcement
learning (DRL) provides an elegant map-free approach integrating image
processing, localization, and planning in one module, which can be trained and
therefore optimized for a given environment. However, to date, DRL-based visual
navigation was validated exclusively in simulation, where the simulator
provides information that is not available in the real world, e.g., the robot's
position or image segmentation masks. This precludes the use of the learned
policy on a real robot. Therefore, we propose a novel approach that enables a
direct deployment of the trained policy on real robots. We have designed visual
auxiliary tasks, a tailored reward scheme, and a new powerful simulator to
facilitate domain randomization. The policy is fine-tuned on images collected
from real-world environments. We have evaluated the method on a mobile robot in
a real office environment. The training took ~30 hours on a single GPU. In 30
navigation experiments, the robot reached a 0.3-meter neighborhood of the goal
in more than 86.7% of cases. This result makes the proposed method directly
applicable to tasks like mobile manipulation.",arxiv
http://arxiv.org/abs/2102.03286v1,2021-02-05T16:56:21Z,2021-02-05T16:56:21Z,"Stable Online Computation Offloading via Lyapunov-guided Deep
  Reinforcement Learning","In this paper, we consider a multi-user mobile-edge computing (MEC) network
with time-varying wireless channels and stochastic user task data arrivals in
sequential time frames. In particular, we aim to design an online computation
offloading algorithm to maximize the network data processing capability subject
to the long-term data queue stability and average power constraints. The online
algorithm is practical in the sense that the decisions for each time frame are
made without the assumption of knowing future channel conditions and data
arrivals. We formulate the problem as a multi-stage stochastic mixed integer
non-linear programming (MINLP) problem that jointly determines the binary
offloading (each user computes the task either locally or at the edge server)
and system resource allocation decisions in sequential time frames. To address
the coupling in the decisions of different time frames, we propose a novel
framework, named LyDROO, that combines the advantages of Lyapunov optimization
and deep reinforcement learning (DRL). Specifically, LyDROO first applies
Lyapunov optimization to decouple the multi-stage stochastic MINLP into
deterministic per-frame MINLP subproblems of much smaller size. Then, it
integrates model-based optimization and model-free DRL to solve the per-frame
MINLP problems with very low computational complexity. Simulation results show
that the proposed LyDROO achieves optimal computation performance while
satisfying all the long-term constraints. Besides, it induces very low
execution latency that is particularly suitable for real-time implementation in
fast fading environments.",arxiv
http://arxiv.org/abs/2106.06592v1,2021-06-11T19:43:47Z,2021-06-11T19:43:47Z,"Diseño y desarrollo de aplicación móvil para la clasificación de
  flora nativa chilena utilizando redes neuronales convolucionales","Introduction: Mobile apps, through artificial vision, are capable of
recognizing vegetable species in real time. However, the existing species
recognition apps do not take in consideration the wide variety of endemic and
native (Chilean) species, which leads to wrong species predictions. This study
introduces the development of a chilean species dataset and an optimized
classification model implemented to a mobile app. Method: the data set was
built by putting together pictures of several species captured on the field and
by selecting some pictures available from other datasets available online.
Convolutional neural networks were used in order to develop the images
prediction models. The networks were trained by performing a sensitivity
analysis, validating with k-fold cross validation and performing tests with
different hyper-parameters, optimizers, convolutional layers, and learning
rates in order to identify and choose the best models and then put them
together in one classification model. Results: The final data set was
compounded by 46 species, including native species, endemic and exotic from
Chile, with 6120 training pictures and 655 testing pictures. The best models
were implemented on a mobile app, obtaining a 95% correct prediction rate with
respect to the set of tests. Conclusion: The app developed in this study is
capable of classifying species with a high level of accuracy, depending on the
state of the art of the artificial vision and it can also show relevant
information related to the classified species.",arxiv
http://arxiv.org/abs/1807.01726v1,2018-07-04T18:05:04Z,2018-07-04T18:05:04Z,LaneNet: Real-Time Lane Detection Networks for Autonomous Driving,"Lane detection is to detect lanes on the road and provide the accurate
location and shape of each lane. It severs as one of the key techniques to
enable modern assisted and autonomous driving systems. However, several unique
properties of lanes challenge the detection methods. The lack of distinctive
features makes lane detection algorithms tend to be confused by other objects
with similar local appearance. Moreover, the inconsistent number of lanes on a
road as well as diverse lane line patterns, e.g. solid, broken, single, double,
merging, and splitting lines further hamper the performance. In this paper, we
propose a deep neural network based method, named LaneNet, to break down the
lane detection into two stages: lane edge proposal and lane line localization.
Stage one uses a lane edge proposal network for pixel-wise lane edge
classification, and the lane line localization network in stage two then
detects lane lines based on lane edge proposals. Please note that the goal of
our LaneNet is built to detect lane line only, which introduces more
difficulties on suppressing the false detections on the similar lane marks on
the road like arrows and characters. Despite all the difficulties, our lane
detection is shown to be robust to both highway and urban road scenarios method
without relying on any assumptions on the lane number or the lane line
patterns. The high running speed and low computational cost endow our LaneNet
the capability of being deployed on vehicle-based systems. Experiments validate
that our LaneNet consistently delivers outstanding performances on real world
traffic scenarios.",arxiv
http://arxiv.org/abs/1910.06540v1,2019-10-15T05:44:28Z,2019-10-15T05:44:28Z,"Real-time monitoring of driver drowsiness on mobile platforms using 3D
  neural networks","Driver drowsiness increases crash risk, leading to substantial road trauma
each year. Drowsiness detection methods have received considerable attention,
but few studies have investigated the implementation of a detection approach on
a mobile phone. Phone applications reduce the need for specialised hardware and
hence, enable a cost-effective roll-out of the technology across the driving
population. While it has been shown that three-dimensional (3D) operations are
more suitable for spatiotemporal feature learning, current methods for
drowsiness detection commonly use frame-based, multi-step approaches. However,
computationally expensive techniques that achieve superior results on action
recognition benchmarks (e.g. 3D convolutions, optical flow extraction) create
bottlenecks for real-time, safety-critical applications on mobile devices.
Here, we show how depthwise separable 3D convolutions, combined with an early
fusion of spatial and temporal information, can achieve a balance between high
prediction accuracy and real-time inference requirements. In particular,
increased accuracy is achieved when assessment requires motion information, for
example, when sunglasses conceal the eyes. Further, a custom TensorFlow-based
smartphone application shows the true impact of various approaches on inference
times and demonstrates the effectiveness of real-time monitoring based on
out-of-sample data to alert a drowsy driver. Our model is pre-trained on
ImageNet and Kinetics and fine-tuned on a publicly available Driver Drowsiness
Detection dataset. Fine-tuning on large naturalistic driving datasets could
further improve accuracy to obtain robust in-vehicle performance. Overall, our
research is a step towards practical deep learning applications, potentially
preventing micro-sleeps and reducing road trauma.",arxiv
http://arxiv.org/abs/1801.05132v1,2018-01-16T06:42:15Z,2018-01-16T06:42:15Z,"Learning to Navigate: Exploiting Deep Networks to Inform Sample-Based
  Planning During Vision-Based Navigation","Recent applications of deep learning to navigation have generated end-to-end
navigation solutions whereby visual sensor input is mapped to control signals
or to motion primitives. The resulting visual navigation strategies work very
well at collision avoidance and have performance that matches traditional
reactive navigation algorithms while operating in real-time. It is accepted
that these solutions cannot provide the same level of performance as a global
planner. However, it is less clear how such end-to-end systems should be
integrated into a full navigation pipeline. We evaluate the typical end-to-end
solution within a full navigation pipeline in order to expose its weaknesses.
Doing so illuminates how to better integrate deep learning methods into the
navigation pipeline. In particular, we show that they are an efficient means to
provide informed samples for sample-based planners. Controlled simulations with
comparison against traditional planners show that the number of samples can be
reduced by an order of magnitude while preserving navigation performance.
Implementation on a mobile robot matches the simulated performance outcomes.",arxiv
http://arxiv.org/abs/1906.08227v1,2019-06-19T17:16:54Z,2019-06-19T17:16:54Z,"Local Bures-Wasserstein Transport: A Practical and Fast Mapping
  Approximation","Optimal transport (OT)-based methods have a wide range of applications and
have attracted a tremendous amount of attention in recent years. However, most
of the computational approaches of OT do not learn the underlying transport
map. Although some algorithms have been proposed to learn this map, they rely
on kernel-based methods, which makes them prohibitively slow when the number of
samples increases. Here, we propose a way to learn an approximate transport map
and a parametric approximation of the Wasserstein barycenter. We build an
approximated transport mapping by leveraging the closed-form of Gaussian
(Bures-Wasserstein) transport; we compute local transport plans between matched
pairs of the Gaussian components of each density. The learned map generalizes
to out-of-sample examples. We provide experimental results on simulated and
real data, comparing our proposed method with other mapping estimation
algorithms. Preliminary experiments suggest that our proposed method is not
only faster, with a factor 80 overall running time, but it also requires fewer
components than state-of-the-art methods to recover the support of the
barycenter. From a practical standpoint, it is straightforward to implement and
can be used with a conventional machine learning pipeline.",arxiv
http://arxiv.org/abs/2106.12372v2,2021-06-25T08:09:48Z,2021-06-23T13:09:58Z,Real-time Neural Radiance Caching for Path Tracing,"We present a real-time neural radiance caching method for path-traced global
illumination. Our system is designed to handle fully dynamic scenes, and makes
no assumptions about the lighting, geometry, and materials. The data-driven
nature of our approach sidesteps many difficulties of caching algorithms, such
as locating, interpolating, and updating cache points. Since pretraining neural
networks to handle novel, dynamic scenes is a formidable generalization
challenge, we do away with pretraining and instead achieve generalization via
adaptation, i.e. we opt for training the radiance cache while rendering. We
employ self-training to provide low-noise training targets and simulate
infinite-bounce transport by merely iterating few-bounce training updates. The
updates and cache queries incur a mild overhead -- about 2.6ms on full HD
resolution -- thanks to a streaming implementation of the neural network that
fully exploits modern hardware. We demonstrate significant noise reduction at
the cost of little induced bias, and report state-of-the-art, real-time
performance on a number of challenging scenarios.",arxiv
http://arxiv.org/abs/2105.12899v1,2021-05-27T01:16:00Z,2021-05-27T01:16:00Z,Learning to Optimize Industry-Scale Dynamic Pickup and Delivery Problems,"The Dynamic Pickup and Delivery Problem (DPDP) is aimed at dynamically
scheduling vehicles among multiple sites in order to minimize the cost when
delivery orders are not known a priori. Although DPDP plays an important role
in modern logistics and supply chain management, state-of-the-art DPDP
algorithms are still limited on their solution quality and efficiency. In
practice, they fail to provide a scalable solution as the numbers of vehicles
and sites become large. In this paper, we propose a data-driven approach,
Spatial-Temporal Aided Double Deep Graph Network (ST-DDGN), to solve
industry-scale DPDP. In our method, the delivery demands are first forecast
using spatial-temporal prediction method, which guides the neural network to
perceive spatial-temporal distribution of delivery demand when dispatching
vehicles. Besides, the relationships of individuals such as vehicles are
modelled by establishing a graph-based value function. ST-DDGN incorporates
attention-based graph embedding with Double DQN (DDQN). As such, it can make
the inference across vehicles more efficiently compared with traditional
methods. Our method is entirely data driven and thus adaptive, i.e., the
relational representation of adjacent vehicles can be learned and corrected by
ST-DDGN from data periodically. We have conducted extensive experiments over
real-world data to evaluate our solution. The results show that ST-DDGN reduces
11.27% number of the used vehicles and decreases 13.12% total transportation
cost on average over the strong baselines, including the heuristic algorithm
deployed in our UAT (User Acceptance Test) environment and a variety of vanilla
DRL methods. We are due to fully deploy our solution into our online logistics
system and it is estimated that millions of USD logistics cost can be saved per
year.",arxiv
http://arxiv.org/abs/2012.09812v2,2021-03-26T11:12:28Z,2020-12-17T18:22:32Z,ViNG: Learning Open-World Navigation with Visual Goals,"We propose a learning-based navigation system for reaching visually indicated
goals and demonstrate this system on a real mobile robot platform. Learning
provides an appealing alternative to conventional methods for robotic
navigation: instead of reasoning about environments in terms of geometry and
maps, learning can enable a robot to learn about navigational affordances,
understand what types of obstacles are traversable (e.g., tall grass) or not
(e.g., walls), and generalize over patterns in the environment. However, unlike
conventional planning algorithms, it is harder to change the goal for a learned
policy during deployment. We propose a method for learning to navigate towards
a goal image of the desired destination. By combining a learned policy with a
topological graph constructed out of previously observed data, our system can
determine how to reach this visually indicated goal even in the presence of
variable appearance and lighting. Three key insights, waypoint proposal, graph
pruning and negative mining, enable our method to learn to navigate in
real-world environments using only offline data, a setting where prior methods
struggle. We instantiate our method on a real outdoor ground robot and show
that our system, which we call ViNG, outperforms previously-proposed methods
for goal-conditioned reinforcement learning, including other methods that
incorporate reinforcement learning and search. We also study how \sysName
generalizes to unseen environments and evaluate its ability to adapt to such an
environment with growing experience. Finally, we demonstrate ViNG on a number
of real-world applications, such as last-mile delivery and warehouse
inspection. We encourage the reader to visit the project website for videos of
our experiments and demonstrations sites.google.com/view/ving-robot.",arxiv
http://arxiv.org/abs/2009.05668v1,2020-09-11T21:48:39Z,2020-09-11T21:48:39Z,KSM: Fast Multiple Task Adaption via Kernel-wise Soft Mask Learning,"Deep Neural Networks (DNN) could forget the knowledge about earlier tasks
when learning new tasks, and this is known as \textit{catastrophic forgetting}.
While recent continual learning methods are capable of alleviating the
catastrophic problem on toy-sized datasets, some issues still remain to be
tackled when applying them in real-world problems. Recently, the fast
mask-based learning method (e.g. piggyback \cite{mallya2018piggyback}) is
proposed to address these issues by learning only a binary element-wise mask in
a fast manner, while keeping the backbone model fixed. However, the binary mask
has limited modeling capacity for new tasks. A more recent work
\cite{hung2019compacting} proposes a compress-grow-based method (CPG) to
achieve better accuracy for new tasks by partially training backbone model, but
with order-higher training cost, which makes it infeasible to be deployed into
popular state-of-the-art edge-/mobile-learning. The primary goal of this work
is to simultaneously achieve fast and high-accuracy multi task adaption in
continual learning setting. Thus motivated, we propose a new training method
called \textit{kernel-wise Soft Mask} (KSM), which learns a kernel-wise hybrid
binary and real-value soft mask for each task, while using the same backbone
model. Such a soft mask can be viewed as a superposition of a binary mask and a
properly scaled real-value tensor, which offers a richer representation
capability without low-level kernel support to meet the objective of low
hardware overhead. We validate KSM on multiple benchmark datasets against
recent state-of-the-art methods (e.g. Piggyback, Packnet, CPG, etc.), which
shows good improvement in both accuracy and training cost.",arxiv
http://arxiv.org/abs/1704.07854v4,2019-02-20T13:27:28Z,2017-04-25T18:21:42Z,Generating Liquid Simulations with Deformation-aware Neural Networks,"We propose a novel approach for deformation-aware neural networks that learn
the weighting and synthesis of dense volumetric deformation fields. Our method
specifically targets the space-time representation of physical surfaces from
liquid simulations. Liquids exhibit highly complex, non-linear behavior under
changing simulation conditions such as different initial conditions. Our
algorithm captures these complex phenomena in two stages: a first neural
network computes a weighting function for a set of pre-computed deformations,
while a second network directly generates a deformation field for refining the
surface. Key for successful training runs in this setting is a suitable loss
function that encodes the effect of the deformations, and a robust calculation
of the corresponding gradients. To demonstrate the effectiveness of our
approach, we showcase our method with several complex examples of flowing
liquids with topology changes. Our representation makes it possible to rapidly
generate the desired implicit surfaces. We have implemented a mobile
application to demonstrate that real-time interactions with complex liquid
effects are possible with our approach.",arxiv
http://arxiv.org/abs/1804.00497v3,2018-10-03T16:11:25Z,2018-03-28T01:32:59Z,"MicronNet: A Highly Compact Deep Convolutional Neural Network
  Architecture for Real-time Embedded Traffic Sign Classification","Traffic sign recognition is a very important computer vision task for a
number of real-world applications such as intelligent transportation
surveillance and analysis. While deep neural networks have been demonstrated in
recent years to provide state-of-the-art performance traffic sign recognition,
a key challenge for enabling the widespread deployment of deep neural networks
for embedded traffic sign recognition is the high computational and memory
requirements of such networks. As a consequence, there are significant benefits
in investigating compact deep neural network architectures for traffic sign
recognition that are better suited for embedded devices. In this paper, we
introduce MicronNet, a highly compact deep convolutional neural network for
real-time embedded traffic sign recognition designed based on macroarchitecture
design principles (e.g., spectral macroarchitecture augmentation, parameter
precision optimization, etc.) as well as numerical microarchitecture
optimization strategies. The resulting overall architecture of MicronNet is
thus designed with as few parameters and computations as possible while
maintaining recognition performance, leading to optimized information density
of the proposed network. The resulting MicronNet possesses a model size of just
~1MB and ~510,000 parameters (~27x fewer parameters than state-of-the-art)
while still achieving a human performance level top-1 accuracy of 98.9% on the
German traffic sign recognition benchmark. Furthermore, MicronNet requires just
~10 million multiply-accumulate operations to perform inference, and has a
time-to-compute of just 32.19 ms on a Cortex-A53 high efficiency processor.
These experimental results show that highly compact, optimized deep neural
network architectures can be designed for real-time traffic sign recognition
that are well-suited for embedded scenarios.",arxiv
http://arxiv.org/abs/2003.00819v1,2020-02-19T07:18:46Z,2020-02-19T07:18:46Z,"RiskOracle: A Minute-level Citywide Traffic Accident Forecasting
  Framework","Real-time traffic accident forecasting is increasingly important for public
safety and urban management (e.g., real-time safe route planning and emergency
response deployment). Previous works on accident forecasting are often
performed on hour levels, utilizing existed neural networks with static
region-wise correlations taken into account. However, it is still challenging
when the granularity of forecasting step improves as the highly dynamic nature
of road network and inherent rareness of accident records in one training
sample, which leads to biased results and zero-inflated issue. In this work, we
propose a novel framework RiskOracle, to improve the prediction granularity to
minute levels. Specifically, we first transform the zero-risk values in labels
to fit the training network. Then, we propose the Differential Time-varying
Graph neural network (DTGN) to capture the immediate changes of traffic status
and dynamic inter-subregion correlations. Furthermore, we adopt multi-task and
region selection schemes to highlight citywide most-likely accident subregions,
bridging the gap between biased risk values and sporadic accident distribution.
Extensive experiments on two real-world datasets demonstrate the effectiveness
and scalability of our RiskOracle framework.",arxiv
http://arxiv.org/abs/1802.03515v5,2018-11-11T04:53:47Z,2018-02-10T03:56:19Z,Vehicle Pose and Shape Estimation through Multiple Monocular Vision,"In this paper, we present an accurate approach to estimate vehicles' pose and
shape from off-board multiview images. The images are taken by monocular
cameras and have small overlaps. We utilize state-of-the-art convolutional
neural networks (CNNs) to extract vehicles' semantic keypoints and introduce a
Cross Projection Optimization (CPO) method to estimate the 3D pose. During the
iterative CPO process, an adaptive shape adjustment method named Hierarchical
Wireframe Constraint (HWC) is implemented to estimate the shape. Our approach
is evaluated under both simulated and real-world scenes for performance
verification. It's shown that our algorithm outperforms other existing
monocular and stereo methods for vehicles' pose and shape estimation. This
approach provides a new and robust solution for off-board visual vehicle
localization and tracking, which can be applied to massive surveillance camera
networks for intelligent transportation.",arxiv
http://arxiv.org/abs/1904.11985v1,2019-04-26T14:09:41Z,2019-04-26T14:09:41Z,Transmission of natural scene images through a multimode fibre,"The optical transport of images through a multimode fibre remains an
outstanding challenge with applications ranging from optical communications to
neuro-imaging. State of the art approaches either involve measurement and
control of the full complex field transmitted through the fibre or, more
recently, training of artificial neural networks that however, are typically
limited to image classes belong to the same class as the training data set.
Here we implement a method that statistically reconstructs the inverse
transformation matrix for the fibre. We demonstrate imaging at high frame
rates, high resolutions and in full colour of natural scenes, thus
demonstrating general-purpose imaging capability. Real-time imaging over long
fibre lengths opens alternative routes to exploitation for example for secure
communication systems, novel remote imaging devices, quantum state control
processing and endoscopy.",arxiv
http://arxiv.org/abs/1404.1905v1,2014-04-04T17:31:17Z,2014-04-04T17:31:17Z,Developing a 21st Century Global Library for Mathematics Research,"Developing a 21st Century Global Library for Mathematics Research discusses
how information about what the mathematical literature contains can be
formalized and made easier to express, encode, and explore. Many of the tools
necessary to make this information system a reality will require much more than
indexing and will instead depend on community input paired with machine
learning, where mathematicians' expertise can fill the gaps of automatization.
This report proposes the establishment of an organization; the development of a
set of platforms, tools, and services; the deployment of an ongoing applied
research program to complement the development work; and the mobilization and
coordination of the mathematical community to take the first steps toward these
capabilities. The report recommends building on the extensive work done by many
dedicated individuals under the rubric of the World Digital Mathematical
Library, as well as many other community initiatives. Developing a 21st Century
Global Library for Mathematics envisions a combination of machine learning
methods and community-based editorial effort that makes a significantly greater
portion of the information and knowledge in the global mathematical corpus
available to researchers as linked open data through a central organizational
entity-referred to in the report as the Digital Mathematics Library. This
report describes how such a library might operate - discussing development and
research needs, role in facilitating discover and interaction, and establishing
partnerships with publishers.",arxiv
http://arxiv.org/abs/2104.03657v1,2021-04-08T10:18:52Z,2021-04-08T10:18:52Z,"Dynamic Object Aware LiDAR SLAM based on Automatic Generation of
  Training Data","Highly dynamic environments, with moving objects such as cars or humans, can
pose a performance challenge for LiDAR SLAM systems that assume largely static
scenes. To overcome this challenge and support the deployment of robots in real
world scenarios, we propose a complete solution for a dynamic object aware
LiDAR SLAM algorithm. This is achieved by leveraging a real-time capable neural
network that can detect dynamic objects, thus allowing our system to deal with
them explicitly. To efficiently generate the necessary training data which is
key to our approach, we present a novel end-to-end occupancy grid based
pipeline that can automatically label a wide variety of arbitrary dynamic
objects. Our solution can thus generalize to different environments without the
need for expensive manual labeling and at the same time avoids assumptions
about the presence of a predefined set of known objects in the scene. Using
this technique, we automatically label over 12000 LiDAR scans collected in an
urban environment with a large amount of pedestrians and use this data to train
a neural network, achieving an average segmentation IoU of 0.82. We show that
explicitly dealing with dynamic objects can improve the LiDAR SLAM odometry
performance by 39.6% while yielding maps which better represent the
environments. A supplementary video as well as our test data are available
online.",arxiv
http://arxiv.org/abs/2002.05509v1,2020-02-13T14:22:39Z,2020-02-13T14:22:39Z,Replacing Mobile Camera ISP with a Single Deep Learning Model,"As the popularity of mobile photography is growing constantly, lots of
efforts are being invested now into building complex hand-crafted camera ISP
solutions. In this work, we demonstrate that even the most sophisticated ISP
pipelines can be replaced with a single end-to-end deep learning model trained
without any prior knowledge about the sensor and optics used in a particular
device. For this, we present PyNET, a novel pyramidal CNN architecture designed
for fine-grained image restoration that implicitly learns to perform all ISP
steps such as image demosaicing, denoising, white balancing, color and contrast
correction, demoireing, etc. The model is trained to convert RAW Bayer data
obtained directly from mobile camera sensor into photos captured with a
professional high-end DSLR camera, making the solution independent of any
particular mobile ISP implementation. To validate the proposed approach on the
real data, we collected a large-scale dataset consisting of 10 thousand
full-resolution RAW-RGB image pairs captured in the wild with the Huawei P20
cameraphone (12.3 MP Sony Exmor IMX380 sensor) and Canon 5D Mark IV DSLR. The
experiments demonstrate that the proposed solution can easily get to the level
of the embedded P20's ISP pipeline that, unlike our approach, is combining the
data from two (RGB + B/W) camera sensors. The dataset, pre-trained models and
codes used in this paper are available on the project website.",arxiv
http://arxiv.org/abs/2104.04076v1,2021-04-01T21:05:26Z,2021-04-01T21:05:26Z,"An artificial intelligence and Internet of things based automated
  irrigation system","It is not hard to see that the need for clean water is growing by considering
the decrease of the water sources day by day in the world. Potable fresh water
is also used for irrigation, so it should be planned to decrease freshwater
wastage. With the development of technology and the availability of cheaper and
more effective solutions, the efficiency of irrigation increased and the water
loss can be reduced. In particular, Internet of things (IoT) devices has begun
to be used in all areas. We can easily and precisely collect temperature,
humidity and mineral values from the irrigation field with the IoT devices and
sensors. Most of the operations and decisions about irrigation are carried out
by people. For people, it is hard to have all the real-time data such as
temperature, moisture and mineral levels in the decision-making process and
make decisions by considering them. People usually make decisions with their
experience. In this study, a wide range of information from the irrigation
field was obtained by using IoT devices and sensors. Data collected from IoT
devices and sensors sent via communication channels and stored on MongoDB. With
the help of Weka software, the data was normalized and the normalized data was
used as a learning set. As a result of the examinations, a decision tree (J48)
algorithm with the highest accuracy was chosen and an artificial intelligence
model was created. Decisions are used to manage operations such as starting,
maintaining and stopping the irrigation. The accuracy of the decisions was
evaluated and the irrigation system was tested with the results. There are
options to manage, view the system remotely and manually and also see the
system s decisions with the created mobile application.",arxiv
http://arxiv.org/abs/1902.01084v2,2021-01-15T20:47:41Z,2019-02-04T08:51:50Z,Paracosm: A Language and Tool for Testing Autonomous Driving Systems,"Systematic testing of autonomous vehicles operating in complex real-world
scenarios is a difficult and expensive problem. We present Paracosm, a reactive
language for writing test scenarios for autonomous driving systems. Paracosm
allows users to programmatically describe complex driving situations with
specific visual features, e.g., road layout in an urban environment, as well as
reactive temporal behaviors of cars and pedestrians. Paracosm programs are
executed on top of a game engine that provides realistic physics simulation and
visual rendering. The infrastructure allows systematic exploration of the state
space, both for visual features (lighting, shadows, fog) and for reactive
interactions with the environment (pedestrians, other traffic). We define a
notion of test coverage for Paracosm configurations based on combinatorial
testing and low dispersion sequences. Paracosm comes with an automatic test
case generator that uses random sampling for discrete parameters and
deterministic quasi-Monte Carlo generation for continuous parameters. Through
an empirical evaluation, we demonstrate the modeling and testing capabilities
of Paracosm on a suite of autonomous driving systems implemented using deep
neural networks developed in research and education. We show how Paracosm can
expose incorrect behaviors or degraded performance.",arxiv
http://arxiv.org/abs/1905.11299v1,2019-05-27T15:32:59Z,2019-05-27T15:32:59Z,"ImgSensingNet: UAV Vision Guided Aerial-Ground Air Quality Sensing
  System","Given the increasingly serious air pollution problem, the monitoring of air
quality index (AQI) in urban areas has drawn considerable attention. This paper
presents ImgSensingNet, a vision guided aerial-ground sensing system, for
fine-grained air quality monitoring and forecasting using the fusion of haze
images taken by the unmanned-aerial-vehicle (UAV) and the AQI data collected by
an on-ground three-dimensional (3D) wireless sensor network (WSN).
Specifically, ImgSensingNet first leverages the computer vision technique to
tell the AQI scale in different regions from the taken haze images, where
haze-relevant features and a deep convolutional neural network (CNN) are
designed for direct learning between haze images and corresponding AQI scale.
Based on the learnt AQI scale, ImgSensingNet determines whether to wake up
on-ground wireless sensors for small-scale AQI monitoring and inference, which
can greatly reduce the energy consumption of the system. An entropy-based model
is employed for accurate real-time AQI inference at unmeasured locations and
future air quality distribution forecasting. We implement and evaluate
ImgSensingNet on two university campuses since Feb. 2018, and has collected
17,630 photos and 2.6 millions of AQI data samples. Experimental results
confirm that ImgSensingNet can achieve higher inference accuracy while greatly
reduce the energy consumption, compared to state-of-the-art AQI monitoring
approaches.",arxiv
http://arxiv.org/abs/1411.3895v1,2014-11-14T13:11:32Z,2014-11-14T13:11:32Z,"Learning Fuzzy Controllers in Mobile Robotics with Embedded
  Preprocessing","The automatic design of controllers for mobile robots usually requires two
stages. In the first stage,sensorial data are preprocessed or transformed into
high level and meaningful values of variables whichare usually defined from
expert knowledge. In the second stage, a machine learning technique is applied
toobtain a controller that maps these high level variables to the control
commands that are actually sent tothe robot. This paper describes an algorithm
that is able to embed the preprocessing stage into the learningstage in order
to get controllers directly starting from sensorial raw data with no expert
knowledgeinvolved. Due to the high dimensionality of the sensorial data, this
approach uses Quantified Fuzzy Rules(QFRs), that are able to transform
low-level input variables into high-level input variables, reducingthe
dimensionality through summarization. The proposed learning algorithm, called
Iterative QuantifiedFuzzy Rule Learning (IQFRL), is based on genetic
programming. IQFRL is able to learn rules with differentstructures, and can
manage linguistic variables with multiple granularities. The algorithm has been
testedwith the implementation of the wall-following behavior both in several
realistic simulated environmentswith different complexity and on a Pioneer 3-AT
robot in two real environments. Results have beencompared with several
well-known learning algorithms combined with different data
preprocessingtechniques, showing that IQFRL exhibits a better and statistically
significant performance. Moreover,three real world applications for which IQFRL
plays a central role are also presented: path and objecttracking with static
and moving obstacles avoidance.",arxiv
http://arxiv.org/abs/2003.11009v2,2020-11-03T01:48:59Z,2020-03-24T17:50:53Z,Learning-based Handover in Mobile Millimeter-wave Networks,"Millimeter-wave (mmWave) communication is considered as a key enabler of
ultra-high data rates in the future cellular and wireless networks. The need
for directional communication between base stations (BSs) and users in mmWave
systems, that is achieved through beamforming, increases the complexity of the
channel estimation. Moreover, in order to provide better coverage, dense
deployment of BSs is required which causes frequent handovers and increased
association overhead. In this paper, we present an approach that jointly
addresses the beamforming and handover problems. Our solution entails an
efficient beamforming method with a minimum number of pilots and a
learning-based handover method supporting mobile scenarios. We use
reinforcement learning algorithm to learn the optimal choices of the backup BSs
in different locations of a mobile user. We show that our method provides high
rate and reliability in all locations of the user's trajectory with a minimal
number of handovers. Simulation results in an outdoor environment based on
geometric mmWave channel modeling and real building map data show the superior
performance of our proposed solution in achievable instantaneous rate and
trajectory rate.",arxiv
http://arxiv.org/abs/2108.10205v1,2021-08-02T13:09:53Z,2021-08-02T13:09:53Z,"Power transformer faults diagnosis using undestructive methods (Roger
  and IEC) and artificial neural network for dissolved gas analysis applied on
  the functional transformer in the Algerian north-eastern: a comparative study","Nowadays, power transformer aging and failures are viewed with great
attention in power transmission industry. Dissolved gas analysis (DGA) is
classified among the biggest widely used methods used within the context of
asset management policy to detect the incipient faults in their earlier stage
in power transformers. Up to now, several procedures have been employed for the
lecture of DGA results. Among these useful means, we find Key Gases, Rogers
Ratios, IEC Ratios, the historical technique less used today Doernenburg
Ratios, the two types of Duval Pentagons methods, several versions of the Duval
Triangles method and Logarithmic Nomograph. Problem. DGA data extracted from
different units in service served to verify the ability and reliability of
these methods in assessing the state of health of the power transformer. Aim.
An improving the quality of diagnostics of electrical power transformer by
artificial neural network tools based on two conventional methods in the case
of a functional power transformer at S\'etif province in East North of Algeria.
Methodology. Design an inelegant tool for power transformer diagnosis using
neural networks based on traditional methods IEC and Rogers, which allows to
early detection faults, to increase the reliability, of the entire electrical
energy system from transport to consumers and improve a continuity and quality
of service. Results. The solution of the problem was carried out by using
feed-forward back-propagation neural networks implemented in MATLAB-Simulink
environment. Four real power transformers working under different environment
and climate conditions such as: desert, humid, cold were taken into account.
The practical results of the diagnosis of these power transformers by the DGA
are presented. Practical value.....",arxiv
http://arxiv.org/abs/1807.05211v1,2018-07-11T11:05:12Z,2018-07-11T11:05:12Z,"Learning Deployable Navigation Policies at Kilometer Scale from a Single
  Traversal","Model-free reinforcement learning has recently been shown to be effective at
learning navigation policies from complex image input. However, these
algorithms tend to require large amounts of interaction with the environment,
which can be prohibitively costly to obtain on robots in the real world. We
present an approach for efficiently learning goal-directed navigation policies
on a mobile robot, from only a single coverage traversal of recorded data. The
navigation agent learns an effective policy over a diverse action space in a
large heterogeneous environment consisting of more than 2km of travel, through
buildings and outdoor regions that collectively exhibit large variations in
visual appearance, self-similarity, and connectivity. We compare pretrained
visual encoders that enable precomputation of visual embeddings to achieve a
throughput of tens of thousands of transitions per second at training time on a
commodity desktop computer, allowing agents to learn from millions of
trajectories of experience in a matter of hours. We propose multiple forms of
computationally efficient stochastic augmentation to enable the learned policy
to generalise beyond these precomputed embeddings, and demonstrate successful
deployment of the learned policy on the real robot without fine tuning, despite
environmental appearance differences at test time. The dataset and code
required to reproduce these results and apply the technique to other datasets
and robots is made publicly available at rl-navigation.github.io/deployable.",arxiv
http://arxiv.org/abs/2012.15545v2,2021-10-14T20:19:22Z,2020-12-31T11:15:10Z,"Vehicular Network Slicing for Reliable Access and Deadline-Constrained
  Data Offloading: A Multi-Agent On-Device Learning Approach","Efficient data offloading plays a pivotal role in computational-intensive
platforms as data rate over wireless channels is fundamentally limited. On top
of that, high mobility adds an extra burden in vehicular edge networks (VENs),
bolstering the desire for efficient user-centric solutions. Therefore, unlike
the legacy inflexible network-centric approach, this paper exploits a
software-defined flexible, open, and programmable networking platform for an
efficient user-centric, fast, reliable, and deadline-constrained offloading
solution in VENs. In the proposed model, each active vehicle user (VU) is
served from multiple low-powered access points (APs) by creating a noble
virtual cell (VC). A joint node association, power allocation, and distributed
resource allocation problem is formulated. As centralized learning is not
practical in many real-world problems, following the distributed nature of
autonomous VUs, each VU is considered an edge learning agent. To that end,
considering practical location-aware node associations, a joint radio and power
resource allocation non-cooperative stochastic game is formulated. Leveraging
reinforcement learning's (RL) efficacy, a multi-agent RL (MARL) solution is
proposed where the edge learners aim to learn the Nash equilibrium (NE)
strategies to solve the game efficiently. Besides, real-world map data, with a
practical microscopic mobility model, are used for the simulation. Results
suggest that the proposed user-centric approach can deliver remarkable
performances in VENs. Moreover, the proposed MARL solution delivers
near-optimal performances with approximately 3% collision probabilities in case
of distributed random access in the uplink.",arxiv
http://arxiv.org/abs/1611.05128v4,2017-04-18T19:49:29Z,2016-11-16T03:00:40Z,"Designing Energy-Efficient Convolutional Neural Networks using
  Energy-Aware Pruning","Deep convolutional neural networks (CNNs) are indispensable to
state-of-the-art computer vision algorithms. However, they are still rarely
deployed on battery-powered mobile devices, such as smartphones and wearable
gadgets, where vision algorithms can enable many revolutionary real-world
applications. The key limiting factor is the high energy consumption of CNN
processing due to its high computational complexity. While there are many
previous efforts that try to reduce the CNN model size or amount of
computation, we find that they do not necessarily result in lower energy
consumption, and therefore do not serve as a good metric for energy cost
estimation.
  To close the gap between CNN design and energy consumption optimization, we
propose an energy-aware pruning algorithm for CNNs that directly uses energy
consumption estimation of a CNN to guide the pruning process. The energy
estimation methodology uses parameters extrapolated from actual hardware
measurements that target realistic battery-powered system setups. The proposed
layer-by-layer pruning algorithm also prunes more aggressively than previously
proposed pruning methods by minimizing the error in output feature maps instead
of filter weights. For each layer, the weights are first pruned and then
locally fine-tuned with a closed-form least-square solution to quickly restore
the accuracy. After all layers are pruned, the entire network is further
globally fine-tuned using back-propagation. With the proposed pruning method,
the energy consumption of AlexNet and GoogLeNet are reduced by 3.7x and 1.6x,
respectively, with less than 1% top-5 accuracy loss. Finally, we show that
pruning the AlexNet with a reduced number of target classes can greatly
decrease the number of weights but the energy reduction is limited.
  Energy modeling tool and energy-aware pruned models available at
http://eyeriss.mit.edu/energy.html",arxiv
http://arxiv.org/abs/2010.02778v1,2020-10-06T14:49:22Z,2020-10-06T14:49:22Z,"Compressing Deep Convolutional Neural Networks by Stacking
  Low-dimensional Binary Convolution Filters","Deep Convolutional Neural Networks (CNN) have been successfully applied to
many real-life problems. However, the huge memory cost of deep CNN models poses
a great challenge of deploying them on memory-constrained devices (e.g., mobile
phones). One popular way to reduce the memory cost of deep CNN model is to
train binary CNN where the weights in convolution filters are either 1 or -1
and therefore each weight can be efficiently stored using a single bit.
However, the compression ratio of existing binary CNN models is upper bounded
by around 32. To address this limitation, we propose a novel method to compress
deep CNN model by stacking low-dimensional binary convolution filters. Our
proposed method approximates a standard convolution filter by selecting and
stacking filters from a set of low-dimensional binary convolution filters. This
set of low-dimensional binary convolution filters is shared across all filters
for a given convolution layer. Therefore, our method will achieve much larger
compression ratio than binary CNN models. In order to train our proposed model,
we have theoretically shown that our proposed model is equivalent to select and
stack intermediate feature maps generated by low-dimensional binary filters.
Therefore, our proposed model can be efficiently trained using the
split-transform-merge strategy. We also provide detailed analysis of the memory
and computation cost of our model in model inference. We compared the proposed
method with other five popular model compression techniques on two benchmark
datasets. Our experimental results have demonstrated that our proposed method
achieves much higher compression ratio than existing methods while maintains
comparable accuracy.",arxiv
http://arxiv.org/abs/2102.05449v3,2021-10-09T15:10:17Z,2021-02-10T14:12:10Z,"Adaptive Processor Frequency Adjustment for Mobile Edge Computing with
  Intermittent Energy Supply","With astonishing speed, bandwidth, and scale, Mobile Edge Computing (MEC) has
played an increasingly important role in the next generation of connectivity
and service delivery. Yet, along with the massive deployment of MEC servers,
the ensuing energy issue is now on an increasingly urgent agenda. In the
current context, the large scale deployment of renewable-energy-supplied MEC
servers is perhaps the most promising solution for the incoming energy issue.
Nonetheless, as a result of the intermittent nature of their power sources,
these special design MEC server must be more cautious about their energy usage,
in a bid to maintain their service sustainability as well as service standard.
Targeting optimization on a single-server MEC scenario, we in this paper
propose NAFA, an adaptive processor frequency adjustment solution, to enable an
effective plan of the server's energy usage. By learning from the historical
data revealing request arrival and energy harvest pattern, the deep
reinforcement learning-based solution is capable of making intelligent
schedules on the server's processor frequency, so as to strike a good balance
between service sustainability and service quality. The superior performance of
NAFA is substantiated by real-data-based experiments, wherein NAFA demonstrates
up to 20% increase in average request acceptance ratio and up to 50% reduction
in average request processing time.",arxiv
http://arxiv.org/abs/2108.12118v1,2021-08-27T04:58:45Z,2021-08-27T04:58:45Z,"Densely-Populated Traffic Detection using YOLOv5 and Non-Maximum
  Suppression Ensembling","Vehicular object detection is the heart of any intelligent traffic system. It
is essential for urban traffic management. R-CNN, Fast R-CNN, Faster R-CNN and
YOLO were some of the earlier state-of-the-art models. Region based CNN methods
have the problem of higher inference time which makes it unrealistic to use the
model in real-time. YOLO on the other hand struggles to detect small objects
that appear in groups. In this paper, we propose a method that can locate and
classify vehicular objects from a given densely crowded image using YOLOv5. The
shortcoming of YOLO was solved my ensembling 4 different models. Our proposed
model performs well on images taken from both top view and side view of the
street in both day and night. The performance of our proposed model was
measured on Dhaka AI dataset which contains densely crowded vehicular images.
Our experiment shows that our model achieved mAP@0.5 of 0.458 with inference
time of 0.75 sec which outperforms other state-of-the-art models on
performance. Hence, the model can be implemented in the street for real-time
traffic detection which can be used for traffic control and data collection.",arxiv
http://arxiv.org/abs/2010.01370v3,2021-07-07T01:58:34Z,2020-10-03T14:49:55Z,"Lyapunov-guided Deep Reinforcement Learning for Stable Online
  Computation Offloading in Mobile-Edge Computing Networks","Opportunistic computation offloading is an effective method to improve the
computation performance of mobile-edge computing (MEC) networks under dynamic
edge environment. In this paper, we consider a multi-user MEC network with
time-varying wireless channels and stochastic user task data arrivals in
sequential time frames. In particular, we aim to design an online computation
offloading algorithm to maximize the network data processing capability subject
to the long-term data queue stability and average power constraints. The online
algorithm is practical in the sense that the decisions for each time frame are
made without the assumption of knowing future channel conditions and data
arrivals. We formulate the problem as a multi-stage stochastic mixed integer
non-linear programming (MINLP) problem that jointly determines the binary
offloading (each user computes the task either locally or at the edge server)
and system resource allocation decisions in sequential time frames. To address
the coupling in the decisions of different time frames, we propose a novel
framework, named LyDROO, that combines the advantages of Lyapunov optimization
and deep reinforcement learning (DRL). Specifically, LyDROO first applies
Lyapunov optimization to decouple the multi-stage stochastic MINLP into
deterministic per-frame MINLP subproblems. By doing so, it guarantees to
satisfy all the long-term constraints by solving the per-frame subproblems that
are much smaller in size. Then, LyDROO integrates model-based optimization and
model-free DRL to solve the per-frame MINLP problems with low computational
complexity. Simulation results show that under various network setups, the
proposed LyDROO achieves optimal computation performance while stabilizing all
queues in the system. Besides, it induces very low execution latency that is
particularly suitable for real-time implementation in fast fading environments.",arxiv
http://arxiv.org/abs/2007.07132v1,2020-07-14T15:51:52Z,2020-07-14T15:51:52Z,"A Deep Learning Approach for Low-Latency Packet Loss Concealment of
  Audio Signals in Networked Music Performance Applications","Networked Music Performance (NMP) is envisioned as a potential game changer
among Internet applications: it aims at revolutionizing the traditional concept
of musical interaction by enabling remote musicians to interact and perform
together through a telecommunication network. Ensuring realistic conditions for
music performance, however, constitutes a significant engineering challenge due
to extremely strict requirements in terms of audio quality and, most
importantly, network delay. To minimize the end-to-end delay experienced by the
musicians, typical implementations of NMP applications use un-compressed,
bidirectional audio streams and leverage UDP as transport protocol. Being
connection less and unreliable,audio packets transmitted via UDP which become
lost in transit are not re-transmitted and thus cause glitches in the receiver
audio playout. This article describes a technique for predicting lost packet
content in real-time using a deep learning approach. The ability of concealing
errors in real time can help mitigate audio impairments caused by packet
losses, thus improving the quality of audio playout in real-world scenarios.",arxiv
http://arxiv.org/abs/1508.01292v3,2015-11-23T20:01:06Z,2015-08-06T07:01:55Z,Compact Convolutional Neural Network Cascade for Face Detection,"The problem of faces detection in images or video streams is a classical
problem of computer vision. The multiple solutions of this problem have been
proposed, but the question of their optimality is still open. Many algorithms
achieve a high quality face detection, but at the cost of high computational
complexity. This restricts their application in the real-time systems. This
paper presents a new solution of the frontal face detection problem based on
compact convolutional neural networks cascade. The test results on FDDB dataset
show that it is competitive with state-of-the-art algorithms. This proposed
detector is implemented using three technologies: SSE/AVX/AVX2 instruction sets
for Intel CPUs, Nvidia CUDA, OpenCL. The detection speed of our approach
considerably exceeds all the existing CPU-based and GPU-based algorithms.
Because of high computational efficiency, our detector can processing 4K Ultra
HD video stream in real time (up to 27 fps) on mobile platforms (Intel Ivy
Bridge CPUs and Nvidia Kepler GPUs) in searching objects with the dimension
60x60 pixels or higher. At the same time its performance weakly dependent on
the background and number of objects in scene. This is achieved by the
asynchronous computation of stages in the cascade.",arxiv
http://arxiv.org/abs/1907.07210v1,2019-07-16T18:33:20Z,2019-07-16T18:33:20Z,Real-time Vision-based Depth Reconstruction with NVidia Jetson,"Vision-based depth reconstruction is a challenging problem extensively
studied in computer vision but still lacking universal solution. Reconstructing
depth from single image is particularly valuable to mobile robotics as it can
be embedded to the modern vision-based simultaneous localization and mapping
(vSLAM) methods providing them with the metric information needed to construct
accurate maps in real scale. Typically, depth reconstruction is done nowadays
via fully-convolutional neural networks (FCNNs). In this work we experiment
with several FCNN architectures and introduce a few enhancements aimed at
increasing both the effectiveness and the efficiency of the inference. We
experimentally determine the solution that provides the best
performance/accuracy tradeoff and is able to run on NVidia Jetson with the
framerates exceeding 16FPS for 320 x 240 input. We also evaluate the suggested
models by conducting monocular vSLAM of unknown indoor environment on NVidia
Jetson TX2 in real-time. Open-source implementation of the models and the
inference node for Robot Operating System (ROS) are available at
https://github.com/CnnDepth/tx2_fcnn_node.",arxiv
http://arxiv.org/abs/1910.14540v1,2019-10-31T15:36:34Z,2019-10-31T15:36:34Z,"Team NCTU: Toward AI-Driving for Autonomous Surface Vehicles -- From
  Duckietown to RobotX","Robotic software and hardware systems of autonomous surface vehicles have
been developed in transportation, military, and ocean researches for decades.
Previous efforts in RobotX Challenges 2014 and 2016 facilitates the
developments for important tasks such as obstacle avoidance and docking. Team
NCTU is motivated by the AI Driving Olympics (AI-DO) developed by the
Duckietown community, and adopts the principles to RobotX challenge. With the
containerization (Docker) and uniformed AI agent (with observations and
actions), we could better 1) integrate solutions developed in different
middlewares (ROS and MOOS), 2) develop essential functionalities of from
simulation (Gazebo) to real robots (either miniaturized or full-sized WAM-V),
and 3) compare different approaches either from classic model-based or
learning-based. Finally, we setup an outdoor on-surface platform with
localization services for evaluation. Some of the preliminary results will be
presented for the Team NCTU participations of the RobotX competition in Hawaii
in 2018.",arxiv
http://arxiv.org/abs/1308.2903v2,2014-06-05T15:36:00Z,2013-08-13T15:57:51Z,"ConXsense - Automated Context Classification for Context-Aware Access
  Control","We present ConXsense, the first framework for context-aware access control on
mobile devices based on context classification. Previous context-aware access
control systems often require users to laboriously specify detailed policies or
they rely on pre-defined policies not adequately reflecting the true
preferences of users. We present the design and implementation of a
context-aware framework that uses a probabilistic approach to overcome these
deficiencies. The framework utilizes context sensing and machine learning to
automatically classify contexts according to their security and privacy-related
properties. We apply the framework to two important smartphone-related use
cases: protection against device misuse using a dynamic device lock and
protection against sensory malware. We ground our analysis on a sociological
survey examining the perceptions and concerns of users related to contextual
smartphone security and analyze the effectiveness of our approach with
real-world context data. We also demonstrate the integration of our framework
with the FlaskDroid architecture for fine-grained access control enforcement on
the Android platform.",arxiv
http://arxiv.org/abs/1611.04049v1,2016-11-12T22:08:15Z,2016-11-12T22:08:15Z,Prognostics of Surgical Site Infections using Dynamic Health Data,"Surgical Site Infection (SSI) is a national priority in healthcare research.
Much research attention has been attracted to develop better SSI risk
prediction models. However, most of the existing SSI risk prediction models are
built on static risk factors such as comorbidities and operative factors. In
this paper, we investigate the use of the dynamic wound data for SSI risk
prediction. There have been emerging mobile health (mHealth) tools that can
closely monitor the patients and generate continuous measurements of many
wound-related variables and other evolving clinical variables. Since existing
prediction models of SSI have quite limited capacity to utilize the evolving
clinical data, we develop the corresponding solution to equip these mHealth
tools with decision-making capabilities for SSI prediction with a seamless
assembly of several machine learning models to tackle the analytic challenges
arising from the spatial-temporal data. The basic idea is to exploit the
low-rank property of the spatial-temporal data via the bilinear formulation,
and further enhance it with automatic missing data imputation by the matrix
completion technique. We derive efficient optimization algorithms to implement
these models and demonstrate the superior performances of our new predictive
model on a real-world dataset of SSI, compared to a range of state-of-the-art
methods.",arxiv
http://arxiv.org/abs/2004.06298v1,2020-04-14T04:38:14Z,2020-04-14T04:38:14Z,Budget Learning via Bracketing,"Conventional machine learning applications in the mobile/IoT setting transmit
data to a cloud-server for predictions. Due to cost considerations (power,
latency, monetary), it is desirable to minimise device-to-server transmissions.
The budget learning (BL) problem poses the learner's goal as minimising use of
the cloud while suffering no discernible loss in accuracy, under the constraint
that the methods employed be edge-implementable.
  We propose a new formulation for the BL problem via the concept of
bracketings. Concretely, we propose to sandwich the cloud's prediction, $g,$
via functions $h^-, h^+$ from a `simple' class so that $h^- \le g \le h^+$
nearly always. On an instance $x$, if $h^+(x)=h^-(x)$, we leverage local
processing, and bypass the cloud. We explore theoretical aspects of this
formulation, providing PAC-style learnability definitions; associating the
notion of budget learnability to approximability via brackets; and giving
VC-theoretic analyses of their properties. We empirically validate our theory
on real-world datasets, demonstrating improved performance over prior gating
based methods.",arxiv
http://arxiv.org/abs/1909.05776v1,2019-09-12T16:14:37Z,2019-09-12T16:14:37Z,"I-SAFE: Instant Suspicious Activity identiFication at the Edge using
  Fuzzy Decision Making","Urban imagery usually serves as forensic analysis and by design is available
for incident mitigation. As more imagery collected, it is harder to narrow down
to certain frames among thousands of video clips to a specific incident. A
real-time, proactive surveillance system is desirable, which could instantly
detect dubious personnel, identify suspicious activities, or raise momentous
alerts. The recent proliferation of the edge computing paradigm allows more
data-intensive tasks to be accomplished by smart edge devices with lightweight
but powerful algorithms. This paper presents a forensic surveillance strategy
by introducing an Instant Suspicious Activity identiFication at the Edge
(I-SAFE) using fuzzy decision making. A fuzzy control system is proposed to
mimic the decision-making process of a security officer. Decisions are made
based on video features extracted by a lightweight Deep Machine Learning (DML)
model. Based on the requirements from the first-line law enforcement officers,
several features are selected and fuzzified to cope with the state of
uncertainty that exists in the officers' decision-making process. Using
features in the edge hierarchy minimizes the communication delay such that
instant alerting is achieved. Additionally, leveraging the Microservices
architecture, the I-SAFE scheme possesses good scalability given the increasing
complexities at the network edge. Implemented as an edge-based application and
tested using exemplary and various labeled dataset surveillance videos, the
I-SAFE scheme raises alerts by identifying the suspicious activity in an
average of 0.002 seconds. Compared to four other state-of-the-art methods over
two other data sets, the experimental study verified the superiority of the
I-SAFE decentralized method.",arxiv
http://arxiv.org/abs/1801.05997v4,2018-12-18T14:00:40Z,2018-01-18T13:04:53Z,"An Energy-Efficient FPGA-based Deconvolutional Neural Networks
  Accelerator for Single Image Super-Resolution","Convolutional neural networks (CNNs) demonstrate excellent performance in
various computer vision applications. In recent years, FPGA-based CNN
accelerators have been proposed for optimizing performance and power
efficiency. Most accelerators are designed for object detection and recognition
algorithms that are performed on low-resolution (LR) images. However, real-time
image super-resolution (SR) cannot be implemented on a typical accelerator
because of the long execution cycles required to generate high-resolution (HR)
images, such as those used in ultra-high-definition (UHD) systems. In this
paper, we propose a novel CNN accelerator with efficient parallelization
methods for SR applications. First, we propose a new methodology for optimizing
the deconvolutional neural networks (DCNNs) used for increasing feature maps.
Secondly, we propose a novel method to optimize CNN dataflow so that the SR
algorithm can be driven at low power in display applications. Finally, we
quantize and compress a DCNN-based SR algorithm into an optimal model for
efficient inference using on-chip memory. We present an energy-efficient
architecture for SR and validate our architecture on a mobile panel with
quad-high-definition (QHD) resolution. Our experimental results show that, with
the same hardware resources, the proposed DCNN accelerator achieves a
throughput up to 108 times greater than that of a conventional DCNN
accelerator. In addition, our SR system achieves an energy efficiency of 144.9
GOPS/W, 293.0 GOPS/W, and 500.2 GOPS/W at SR scale factors of 2, 3, and 4,
respectively. Furthermore, we demonstrate that our system can restore HR images
to a high quality while greatly reducing the data bit-width and the number of
parameters compared to conventional SR algorithms.",arxiv
http://arxiv.org/abs/1909.10972v2,2020-03-11T05:46:37Z,2019-09-24T14:55:00Z,"Residual Reactive Navigation: Combining Classical and Learned Navigation
  Strategies For Deployment in Unknown Environments","In this work we focus on improving the efficiency and generalisation of
learned navigation strategies when transferred from its training environment to
previously unseen ones. We present an extension of the residual reinforcement
learning framework from the robotic manipulation literature and adapt it to the
vast and unstructured environments that mobile robots can operate in. The
concept is based on learning a residual control effect to add to a typical
sub-optimal classical controller in order to close the performance gap, whilst
guiding the exploration process during training for improved data efficiency.
We exploit this tight coupling and propose a novel deployment strategy,
switching Residual Reactive Navigation (sRRN), which yields efficient
trajectories whilst probabilistically switching to a classical controller in
cases of high policy uncertainty. Our approach achieves improved performance
over end-to-end alternatives and can be incorporated as part of a complete
navigation stack for cluttered indoor navigation tasks in the real world. The
code and training environment for this project is made publicly available at
https://sites.google.com/view/srrn/home.",arxiv
http://arxiv.org/abs/2108.06091v1,2021-08-13T06:52:08Z,2021-08-13T06:52:08Z,"BESS Aided Reconfigurable Energy Supply using Deep Reinforcement
  Learning for 5G and Beyond","The year of 2020 has witnessed the unprecedented development of 5G networks,
along with the widespread deployment of 5G base stations (BSs). Nevertheless,
the enormous energy consumption of BSs and the incurred huge energy cost have
become significant concerns for the mobile operators. As the continuous decline
of the renewable energy cost, equipping the power-hungry BSs with renewable
energy generators could be a sustainable solution. In this work, we propose an
energy storage aided reconfigurable renewable energy supply solution for the
BS, which could supply clean energy to the BS and store surplus energy for
backup usage. Specifically, to flexibly reconfigure the battery's
discharging/charging operations, we propose a deep reinforcement learning based
reconfiguring policy, which can adapt to the dynamical renewable energy
generations as well as the varying power demands. Our experiments using the
real-world data on renewable energy generations and power demands demonstrate
that, our reconfigurable power supply solution can achieve an energy saving
ratio of 74.8%, compared to the case with traditional power grid supply.",arxiv
http://arxiv.org/abs/2012.10706v4,2021-07-30T13:36:36Z,2020-12-19T14:53:56Z,Siamese Anchor Proposal Network for High-Speed Aerial Tracking,"In the domain of visual tracking, most deep learning-based trackers highlight
the accuracy but casting aside efficiency. Therefore, their real-world
deployment on mobile platforms like the unmanned aerial vehicle (UAV) is
impeded. In this work, a novel two-stage Siamese network-based method is
proposed for aerial tracking, \textit{i.e.}, stage-1 for high-quality anchor
proposal generation, stage-2 for refining the anchor proposal. Different from
anchor-based methods with numerous pre-defined fixed-sized anchors, our
no-prior method can 1) increase the robustness and generalization to different
objects with various sizes, especially to small, occluded, and fast-moving
objects, under complex scenarios in light of the adaptive anchor generation, 2)
make calculation feasible due to the substantial decrease of anchor numbers. In
addition, compared to anchor-free methods, our framework has better performance
owing to refinement at stage-2. Comprehensive experiments on three benchmarks
have proven the superior performance of our approach, with a speed of around
200 frames/s.",arxiv
http://arxiv.org/abs/1706.06696v1,2017-06-20T22:53:16Z,2017-06-20T22:53:16Z,"The NAO Backpack: An Open-hardware Add-on for Fast Software Development
  with the NAO Robot","We present an open-source accessory for the NAO robot, which enables to test
computationally demanding algorithms in an external platform while preserving
robot's autonomy and mobility. The platform has the form of a backpack, which
can be 3D printed and replicated, and holds an ODROID XU4 board to process
algorithms externally with ROS compatibility. We provide also a software bridge
between the B-Human's framework and ROS to have access to the robot's sensors
close to real-time. We tested the platform in several robotics applications
such as data logging, visual SLAM, and robot vision with deep learning
techniques. The CAD model, hardware specifications and software are available
online for the benefit of the community:
https://github.com/uchile-robotics/nao-backpack",arxiv
http://arxiv.org/abs/2010.07848v1,2020-10-15T16:06:53Z,2020-10-15T16:06:53Z,Towards a Flexible Framework for Algorithmic Fairness,"Increasingly, scholars seek to integrate legal and technological insights to
combat bias in AI systems. In recent years, many different definitions for
ensuring non-discrimination in algorithmic decision systems have been put
forward. In this paper, we first briefly describe the EU law framework covering
cases of algorithmic discrimination. Second, we present an algorithm that
harnesses optimal transport to provide a flexible framework to interpolate
between different fairness definitions. Third, we show that important normative
and legal challenges remain for the implementation of algorithmic fairness
interventions in real-world scenarios. Overall, the paper seeks to contribute
to the quest for flexible technical frameworks that can be adapted to varying
legal and normative fairness constraints.",arxiv
http://arxiv.org/abs/1902.06824v2,2019-06-13T19:14:27Z,2019-02-18T22:31:09Z,"Autonomous Airline Revenue Management: A Deep Reinforcement Learning
  Approach to Seat Inventory Control and Overbooking","Revenue management can enable airline corporations to maximize the revenue
generated from each scheduled flight departing in their transportation network
by means of finding the optimal policies for differential pricing, seat
inventory control and overbooking. As different demand segments in the market
have different Willingness-To-Pay (WTP), airlines use differential pricing,
booking restrictions, and service amenities to determine different fare classes
or products targeted at each of these demand segments. Because seats are
limited for each flight, airlines also need to allocate seats for each of these
fare classes to prevent lower fare class passengers from displacing higher fare
class ones and set overbooking limits in anticipation of cancellations and
no-shows such that revenue is maximized. Previous work addresses these problems
using optimization techniques or classical Reinforcement Learning methods. This
paper focuses on the latter problem - the seat inventory control problem -
casting it as a Markov Decision Process to be able to find the optimal policy.
Multiple fare classes, concurrent continuous arrival of passengers of different
fare classes, overbooking and random cancellations that are independent of
class have been considered in the model. We have addressed this problem using
Deep Q-Learning with the goal of maximizing the reward for each flight
departure. The implementation of this technique allows us to employ large
continuous state space but also presents the potential opportunity to test on
real time airline data. To generate data and train the agent, a basic
air-travel market simulator was developed. The performance of the agent in
different simulated market scenarios was compared against theoretically optimal
solutions and was found to be nearly close to the expected optimal revenue.",arxiv
http://arxiv.org/abs/2109.14797v2,2021-10-02T00:05:08Z,2021-09-30T01:54:44Z,"Emergency Vehicles Audio Detection and Localization in Autonomous
  Driving","Emergency vehicles in service have right-of-way over all other vehicles.
Hence, all other vehicles are supposed to take proper actions to yield
emergency vehicles with active sirens. As this task requires the cooperation
between ears and eyes for human drivers, it also needs audio detection as a
supplement to vision-based algorithms for fully autonomous driving vehicles. In
urban driving scenarios, we need to know both the existence of emergency
vehicles and their relative positions to us to decide the proper actions. We
present a novel system from collecting the real-world siren data to the
deployment of models using only two cost-efficient microphones. We are able to
achieve promising performance for each task separately, especially within the
crucial 10m to 50m distance range to react (the size of our ego vehicle is
around 5m in length and 2m in width). The recall rate to determine the
existence of sirens is 99.16% , the median and mean angle absolute error is
9.64{\deg} and 19.18{\deg} respectively, and the median and mean distance
absolute error of 9.30m and 10.58m respectively within that range. We also
benchmark various machine learning approaches that can determine the siren
existence and sound source localization which includes direction and distance
simultaneously within 50ms of latency.",arxiv
http://arxiv.org/abs/2011.07607v2,2021-11-18T14:10:57Z,2020-11-15T19:19:40Z,"Deep Ordinal Regression using Optimal Transport Loss and Unimodal Output
  Probabilities","It is often desired that ordinal regression models yield unimodal
predictions. However, in many recent works this characteristic is either
absent, or implemented using soft targets, which do not guarantee unimodal
outputs at inference. In addition, we argue that the standard maximum
likelihood objective is not suitable for ordinal regression problems, and that
optimal transport is better suited for this task, as it naturally captures the
order of the classes. In this work, we propose a framework for deep ordinal
regression, based on unimodal output distribution and optimal transport loss.
Inspired by the well-known Proportional Odds model, we propose to modify its
design by using an architectural mechanism which guarantees that the model
output distribution will be unimodal. We empirically analyze the different
components of our proposed approach and demonstrate their contribution to the
performance of the model. Experimental results on eight real-world datasets
demonstrate that our proposed approach consistently performs on par with and
often better than several recently proposed deep learning approaches for deep
ordinal regression with unimodal output probabilities, while having guarantee
on the output unimodality. In addition, we demonstrate that proposed approach
is less overconfident than current baselines.",arxiv
http://arxiv.org/abs/1808.06277v1,2018-08-20T00:54:29Z,2018-08-20T00:54:29Z,An Efficient Approach for Geo-Multimedia Cross-Modal Retrieval,"Due to the rapid development of mobile Internet techniques, cloud computation
and popularity of online social networking and location-based services, massive
amount of multimedia data with geographical information is generated and
uploaded to the Internet. In this paper, we propose a novel type of cross-modal
multimedia retrieval called geo-multimedia cross-modal retrieval which aims to
search out a set of geo-multimedia objects based on geographical distance
proximity and semantic similarity between different modalities. Previous
studies for cross-modal retrieval and spatial keyword search cannot address
this problem effectively because they do not consider multimedia data with
geo-tags and do not focus on this type of query. In order to address this
problem efficiently, we present the definition of $k$NN geo-multimedia
cross-modal query at the first time and introduce relevant conceptions such as
cross-modal semantic representation space. To bridge the semantic gap between
different modalities, we propose a method named cross-modal semantic matching
which contains two important component, i.e., CorrProj and LogsTran, which aims
to construct a common semantic representation space for cross-modal semantic
similarity measurement. Besides, we designed a framework based on deep learning
techniques to implement common semantic representation space construction. In
addition, a novel hybrid indexing structure named GMR-Tree combining
geo-multimedia data and R-Tree is presented and a efficient $k$NN search
algorithm called $k$GMCMS is designed. Comprehensive experimental evaluation on
real and synthetic dataset clearly demonstrates that our solution outperforms
the-state-of-the-art methods.",arxiv
http://arxiv.org/abs/1710.08299v1,2017-09-26T05:20:12Z,2017-09-26T05:20:12Z,An In-field Automatic Wheat Disease Diagnosis System,"Crop diseases are responsible for the major production reduction and economic
losses in agricultural industry world- wide. Monitoring for health status of
crops is critical to control the spread of diseases and implement effective
management. This paper presents an in-field automatic wheat disease diagnosis
system based on a weakly super- vised deep learning framework, i.e. deep
multiple instance learning, which achieves an integration of identification for
wheat diseases and localization for disease areas with only image-level
annotation for training images in wild conditions. Furthermore, a new in-field
image dataset for wheat disease, Wheat Disease Database 2017 (WDD2017), is
collected to verify the effectiveness of our system. Under two different
architectures, i.e. VGG-FCN-VD16 and VGG-FCN-S, our system achieves the mean
recognition accuracies of 97.95% and 95.12% respectively over 5-fold
cross-validation on WDD2017, exceeding the results of 93.27% and 73.00% by two
conventional CNN frameworks, i.e. VGG-CNN-VD16 and VGG-CNN-S. Experimental
results demonstrate that the proposed system outperforms conventional CNN
architectures on recognition accuracy under the same amount of parameters,
meanwhile main- taining accurate localization for corresponding disease areas.
Moreover, the proposed system has been packed into a real-time mobile app to
provide support for agricultural disease diagnosis.",arxiv
http://arxiv.org/abs/1909.08991v1,2019-09-19T13:43:31Z,2019-09-19T13:43:31Z,"Road Damage Detection Acquisition System based on Deep Neural Networks
  for Physical Asset Management","Research on damage detection of road surfaces has been an active area of
re-search, but most studies have focused so far on the detection of the
presence of damages. However, in real-world scenarios, road managers need to
clearly understand the type of damage and its extent in order to take effective
action in advance or to allocate the necessary resources. Moreover, currently
there are few uniform and openly available road damage datasets, leading to a
lack of a common benchmark for road damage detection. Such dataset could be
used in a great variety of applications; herein, it is intended to serve as the
acquisition component of a physical asset management tool which can aid
governments agencies for planning purposes, or by infrastructure mainte-nance
companies. In this paper, we make two contributions to address these issues.
First, we present a large-scale road damage dataset, which includes a more
balanced and representative set of damages. This dataset is composed of 18,034
road damage images captured with a smartphone, with 45,435 in-stances road
surface damages. Second, we trained different types of object detection
methods, both traditional (an LBP-cascaded classifier) and deep learning-based,
specifically, MobileNet and RetinaNet, which are amenable for embedded and
mobile and implementations with an acceptable perfor-mance for many
applications. We compare the accuracy and inference time of all these models
with others in the state of the art.",arxiv
http://arxiv.org/abs/2007.11404v1,2020-07-21T07:11:27Z,2020-07-21T07:11:27Z,"A Hybrid Neuromorphic Object Tracking and Classification Framework for
  Real-time Systems","Deep learning inference that needs to largely take place on the 'edge' is a
highly computational and memory intensive workload, making it intractable for
low-power, embedded platforms such as mobile nodes and remote security
applications. To address this challenge, this paper proposes a real-time,
hybrid neuromorphic framework for object tracking and classification using
event-based cameras that possess properties such as low-power consumption (5-14
mW) and high dynamic range (120 dB). Nonetheless, unlike traditional approaches
of using event-by-event processing, this work uses a mixed frame and event
approach to get energy savings with high performance. Using a frame-based
region proposal method based on the density of foreground events, a
hardware-friendly object tracking scheme is implemented using the apparent
object velocity while tackling occlusion scenarios. The object track input is
converted back to spikes for TrueNorth classification via the energy-efficient
deep network (EEDN) pipeline. Using originally collected datasets, we train the
TrueNorth model on the hardware track outputs, instead of using ground truth
object locations as commonly done, and demonstrate the ability of our system to
handle practical surveillance scenarios. As an optional paradigm, to exploit
the low latency and asynchronous nature of neuromorphic vision sensors (NVS),
we also propose a continuous-time tracker with C++ implementation where each
event is processed individually. Thereby, we extensively compare the proposed
methodologies to state-of-the-art event-based and frame-based methods for
object tracking and classification, and demonstrate the use case of our
neuromorphic approach for real-time and embedded applications without
sacrificing performance. Finally, we also showcase the efficacy of the proposed
system to a standard RGB camera setup when evaluated over several hours of
traffic recordings.",arxiv
http://arxiv.org/abs/2012.03213v2,2021-02-14T11:28:46Z,2020-12-06T08:29:13Z,"Reinforcement Learning Based Dynamic Function Splitting in Disaggregated
  Green Open RANs","With the growing momentum around Open RAN (O-RAN) initiatives, performing
dynamic Function Splitting (FS) in disaggregated and virtualized Radio Access
Networks (vRANs), in an efficient way, is becoming highly important. An equally
important efficiency demand is emerging from the energy consumption dimension
of the RAN hardware and software. Supplying the RAN with Renewable Energy
Sources (RESs) promises to boost the energy-efficiency. Yet, FS in such a
dynamic setting, calls for intelligent mechanisms that can adapt to the varying
conditions of the RES supply and the traffic load on the mobile network. In
this paper, we propose a reinforcement learning (RL)-based dynamic function
splitting (RLDFS) technique that decides on the function splits in an O-RAN to
make the best use of RES supply and minimize operator costs. We also formulate
an operational expenditure minimization problem. We evaluate the performance of
the proposed approach on a real data set of solar irradiation and traffic rate
variations. Our results show that the proposed RLDFS method makes effective use
of RES and reduces the cost of an MNO. We also investigate the impact of the
size of solar panels and batteries which may guide MNOs to decide on proper RES
and battery sizing for their networks.",arxiv
http://arxiv.org/abs/1108.0123v1,2011-07-31T01:55:21Z,2011-07-31T01:55:21Z,Lean Algebraic Multigrid (LAMG): Fast Graph Laplacian Linear Solver,"Laplacian matrices of graphs arise in large-scale computational applications
such as machine learning; spectral clustering of images, genetic data and web
pages; transportation network flows; electrical resistor circuits; and elliptic
partial differential equations discretized on unstructured grids with finite
elements. A Lean Algebraic Multigrid (LAMG) solver of the linear system Ax=b is
presented, where A is a graph Laplacian. LAMG's run time and storage are linear
in the number of graph edges. LAMG consists of a setup phase, in which a
sequence of increasingly-coarser Laplacian systems is constructed, and an
iterative solve phase using multigrid cycles. General graphs pose algorithmic
challenges not encountered in traditional applications of algebraic multigrid.
LAMG combines a lean piecewise-constant interpolation, judicious node
aggregation based on a new node proximity definition, and an energy correction
of the coarse-level systems. This results in fast convergence and substantial
overhead and memory savings. A serial LAMG implementation scaled linearly for a
diverse set of 1666 real-world graphs with up to six million edges. This
multilevel methodology can be fully parallelized and extended to eigenvalue
problems and other graph computations.",arxiv
http://arxiv.org/abs/1108.1310v2,2012-06-08T12:20:21Z,2011-08-05T12:04:00Z,"Lean Algebraic Multigrid (LAMG): Fast Graph Laplacian Linear Solver
  (Journal Version)","Laplacian matrices of graphs arise in large-scale computational applications
such as semi-supervised machine learning; spectral clustering of images,
genetic data and web pages; transportation network flows; electrical resistor
circuits; and elliptic partial differential equations discretized on
unstructured grids with finite elements. A Lean Algebraic Multigrid (LAMG)
solver of the symmetric linear system Ax=b is presented, where A is a graph
Laplacian. LAMG's run time and storage are empirically demonstrated to scale
linearly with the number of edges.
  LAMG consists of a setup phase during which a sequence of
increasingly-coarser Laplacian systems is constructed, and an iterative solve
phase using multigrid cycles. General graphs pose algorithmic challenges not
encountered in traditional multigrid applications. LAMG combines a lean
piecewise-constant interpolation, judicious node aggregation based on a new
node proximity measure (the affinity), and an energy correction of coarse-level
systems. This results in fast convergence and substantial setup and memory
savings. A serial LAMG implementation scaled linearly for a diverse set of 3774
real-world graphs with up to 47 million edges, with no parameter tuning. LAMG
was more robust than the UMFPACK direct solver and Combinatorial Multigrid
(CMG), although CMG was faster than LAMG on average. Our methodology is
extensible to eigenproblems and other graph computations.",arxiv
http://arxiv.org/abs/1811.10869v1,2018-11-27T08:28:52Z,2018-11-27T08:28:52Z,"Efficient non-uniform quantizer for quantized neural network targeting
  reconfigurable hardware","Convolutional Neural Networks (CNN) has become more popular choice for
various tasks such as computer vision, speech recognition and natural language
processing. Thanks to their large computational capability and throughput, GPUs
,which are not power efficient and therefore does not suit low power systems
such as mobile devices, are the most common platform for both training and
inferencing tasks. Recent studies has shown that FPGAs can provide a good
alternative to GPUs as a CNN accelerator, due to their re-configurable nature,
low power and small latency. In order for FPGA-based accelerators outperform
GPUs in inference task, both the parameters of the network and the activations
must be quantized. While most works use uniform quantizers for both parameters
and activations, it is not always the optimal one, and a non-uniform quantizer
need to be considered. In this work we introduce a custom hardware-friendly
approach to implement non-uniform quantizers. In addition, we use a single
scale integer representation of both parameters and activations, for both
training and inference. The combined method yields a hardware efficient
non-uniform quantizer, fit for real-time applications. We have tested our
method on CIFAR-10 and CIFAR-100 image classification datasets with ResNet-18
and VGG-like architectures, and saw little degradation in accuracy.",arxiv
http://arxiv.org/abs/1709.04731v1,2017-09-14T12:30:41Z,2017-09-14T12:30:41Z,"Binary-decomposed DCNN for accelerating computation and compressing
  model without retraining","Recent trends show recognition accuracy increasing even more profoundly.
Inference process of Deep Convolutional Neural Networks (DCNN) has a large
number of parameters, requires a large amount of computation, and can be very
slow. The large number of parameters also require large amounts of memory. This
is resulting in increasingly long computation times and large model sizes. To
implement mobile and other low performance devices incorporating DCNN, model
sizes must be compressed and computation must be accelerated. To that end, this
paper proposes Binary-decomposed DCNN, which resolves these issues without the
need for retraining. Our method replaces real-valued inner-product computations
with binary inner-product computations in existing network models to accelerate
computation of inference and decrease model size without the need for
retraining. Binary computations can be done at high speed using logical
operators such as XOR and AND, together with bit counting. In tests using
AlexNet with the ImageNet classification task, speed increased by a factor of
1.79, models were compressed by approximately 80%, and increase in error rate
was limited to 1.20%. With VGG-16, speed increased by a factor of 2.07, model
sizes decreased by 81%, and error increased by only 2.16%.",arxiv
http://arxiv.org/abs/1907.02526v1,2019-07-03T21:25:21Z,2019-07-03T21:25:21Z,"Convolutional Neural Network-based Speech Enhancement for Cochlear
  Implant Recipients","Attempts to develop speech enhancement algorithms with improved speech
intelligibility for cochlear implant (CI) users have met with limited success.
To improve speech enhancement methods for CI users, we propose to perform
speech enhancement in a cochlear filter-bank feature space, a feature-set
specifically designed for CI users based on CI auditory stimuli. We leverage a
convolutional neural network (CNN) to extract both stationary and
non-stationary components of environmental acoustics and speech. We propose
three CNN architectures: (1) vanilla CNN that directly generates the enhanced
signal; (2) spectral-subtraction-style CNN (SS-CNN) that first predicts noise
and then generates the enhanced signal by subtracting noise from the noisy
signal; (3) Wiener-style CNN (Wiener-CNN) that generates an optimal mask for
suppressing noise. An important problem of the proposed networks is that they
introduce considerable delays, which limits their real-time application for CI
users. To address this, this study also considers causal variations of these
networks. Our experiments show that the proposed networks (both causal and
non-causal forms) achieve significant improvement over existing baseline
systems. We also found that causal Wiener-CNN outperforms other networks, and
leads to the best overall envelope coefficient measure (ECM). The proposed
algorithms represent a viable option for implementation on the CCi-MOBILE
research platform as a pre-processor for CI users in naturalistic environments.",arxiv
http://arxiv.org/abs/2105.01777v1,2021-05-04T21:48:18Z,2021-05-04T21:48:18Z,"PathBench: A Benchmarking Platform for Classical and Learned Path
  Planning Algorithms","Path planning is a key component in mobile robotics. A wide range of path
planning algorithms exist, but few attempts have been made to benchmark the
algorithms holistically or unify their interface. Moreover, with the recent
advances in deep neural networks, there is an urgent need to facilitate the
development and benchmarking of such learning-based planning algorithms. This
paper presents PathBench, a platform for developing, visualizing, training,
testing, and benchmarking of existing and future, classical and learned 2D and
3D path planning algorithms, while offering support for Robot Oper-ating System
(ROS). Many existing path planning algorithms are supported; e.g. A*,
wavefront, rapidly-exploring random tree, value iteration networks, gated path
planning networks; and integrating new algorithms is easy and clearly
specified. We demonstrate the benchmarking capability of PathBench by comparing
implemented classical and learned algorithms for metrics, such as path length,
success rate, computational time and path deviation. These evaluations are done
on built-in PathBench maps and external path planning environments from video
games and real world databases. PathBench is open source.",arxiv
http://arxiv.org/abs/2108.13475v1,2021-08-18T13:39:50Z,2021-08-18T13:39:50Z,"An Analysis Of Entire Space Multi-Task Models For Post-Click Conversion
  Prediction","Industrial recommender systems are frequently tasked with approximating
probabilities for multiple, often closely related, user actions. For example,
predicting if a user will click on an advertisement and if they will then
purchase the advertised product. The conceptual similarity between these tasks
has promoted the use of multi-task learning: a class of algorithms that aim to
bring positive inductive transfer from related tasks. Here, we empirically
evaluate multi-task learning approaches with neural networks for an online
advertising task. Specifically, we consider approximating the probability of
post-click conversion events (installs) (CVR) for mobile app advertising on a
large-scale advertising platform, using the related click events (CTR) as an
auxiliary task. We use an ablation approach to systematically study recent
approaches that incorporate both multitask learning and ""entire space modeling""
which train the CVR on all logged examples rather than learning a conditional
likelihood of conversion given clicked. Based on these results we show that
several different approaches result in similar levels of positive transfer from
the data-abundant CTR task to the CVR task and offer some insight into how the
multi-task design choices address the two primary problems affecting the CVR
task: data sparsity and data bias. Our findings add to the growing body of
evidence suggesting that standard multi-task learning is a sensible approach to
modelling related events in real-world large-scale applications and suggest the
specific multitask approach can be guided by ease of implementation in an
existing system.",arxiv
http://arxiv.org/abs/2003.00637v3,2020-03-16T04:27:33Z,2020-03-02T03:04:13Z,"A Novel Recurrent Encoder-Decoder Structure for Large-Scale Multi-view
  Stereo Reconstruction from An Open Aerial Dataset","A great deal of research has demonstrated recently that multi-view stereo
(MVS) matching can be solved with deep learning methods. However, these efforts
were focused on close-range objects and only a very few of the deep
learning-based methods were specifically designed for large-scale 3D urban
reconstruction due to the lack of multi-view aerial image benchmarks. In this
paper, we present a synthetic aerial dataset, called the WHU dataset, we
created for MVS tasks, which, to our knowledge, is the first large-scale
multi-view aerial dataset. It was generated from a highly accurate 3D digital
surface model produced from thousands of real aerial images with precise camera
parameters. We also introduce in this paper a novel network, called RED-Net,
for wide-range depth inference, which we developed from a recurrent
encoder-decoder structure to regularize cost maps across depths and a 2D fully
convolutional network as framework. RED-Net's low memory requirements and high
performance make it suitable for large-scale and highly accurate 3D Earth
surface reconstruction. Our experiments confirmed that not only did our method
exceed the current state-of-the-art MVS methods by more than 50% mean absolute
error (MAE) with less memory and computational cost, but its efficiency as
well. It outperformed one of the best commercial software programs based on
conventional methods, improving their efficiency 16 times over. Moreover, we
proved that our RED-Net model pre-trained on the synthetic WHU dataset can be
efficiently transferred to very different multi-view aerial image datasets
without any fine-tuning. Dataset are available at http://gpcv.whu.edu.cn/data.",arxiv
http://arxiv.org/abs/2110.02582v1,2021-10-06T08:50:33Z,2021-10-06T08:50:33Z,"FADNet++: Real-Time and Accurate Disparity Estimation with Configurable
  Networks","Deep neural networks (DNNs) have achieved great success in the area of
computer vision. The disparity estimation problem tends to be addressed by DNNs
which achieve much better prediction accuracy than traditional hand-crafted
feature-based methods. However, the existing DNNs hardly serve both efficient
computation and rich expression capability, which makes them difficult for
deployment in real-time and high-quality applications, especially on mobile
devices. To this end, we propose an efficient, accurate, and configurable deep
network for disparity estimation named FADNet++. Leveraging several liberal
network design and training techniques, FADNet++ can boost its accuracy with a
fast model inference speed for real-time applications. Besides, it enables
users to easily configure different sizes of models for balancing accuracy and
inference efficiency. We conduct extensive experiments to demonstrate the
effectiveness of FADNet++ on both synthetic and realistic datasets among six
GPU devices varying from server to mobile platforms. Experimental results show
that FADNet++ and its variants achieve state-of-the-art prediction accuracy,
and run at a significant order of magnitude faster speed than existing 3D
models. With the constraint of running at above 15 frames per second (FPS) on a
mobile GPU, FADNet++ achieves a new state-of-the-art result for the SceneFlow
dataset.",arxiv
http://arxiv.org/abs/2103.03572v1,2021-03-05T10:17:36Z,2021-03-05T10:17:36Z,SDR-based Testbed for Real-time CQI Prediction for URLLC,"Ultra-reliable Low-Latency Communication (URLLC) is a key feature of 5G
systems. The quality of service (QoS) requirements imposed by URLLC are less
than 10ms delay and less than $10^{-5}$ packet loss rate (PLR). To satisfy such
strict requirements with minimal channel resource consumption, the devices need
to accurately predict the channel quality and select Modulation and Coding
Scheme (MCS) for URLLC in a proper way.
  This paper presents a novel real-time channel prediction system based on
Software-Defined Radio that uses a neural network. The paper also describes and
shares an open channel measurement dataset that can be used to compare various
channel prediction approaches in different mobility scenarios in future
research on URLLC",arxiv
http://arxiv.org/abs/2106.14861v2,2021-06-29T01:46:13Z,2021-06-28T17:28:28Z,"Doing good by fighting fraud: Ethical anti-fraud systems for mobile
  payments","App builders commonly use security challenges, a form of step-up
authentication, to add security to their apps. However, the ethical
implications of this type of architecture has not been studied previously. In
this paper, we present a large-scale measurement study of running an existing
anti-fraud security challenge, Boxer, in real apps running on mobile devices.
We find that although Boxer does work well overall, it is unable to scan
effectively on devices that run its machine learning models at less than one
frame per second (FPS), blocking users who use inexpensive devices. With the
insights from our study, we design Daredevil, anew anti-fraud system for
scanning payment cards that work swell across the broad range of performance
characteristics and hardware configurations found on modern mobile devices.
Daredevil reduces the number of devices that run at less than one FPS by an
order of magnitude compared to Boxer, providing a more equitable system for
fighting fraud. In total, we collect data from 5,085,444 real devices spread
across 496 real apps running production software and interacting with real
users.",arxiv
http://arxiv.org/abs/1707.06959v1,2017-07-21T16:15:31Z,2017-07-21T16:15:31Z,"A Framework for Easing the Development of Applications Embedding Answer
  Set Programming","Answer Set Programming (ASP) is a well-established declarative problem
solving paradigm which became widely used in AI and recognized as a powerful
tool for knowledge representation and reasoning (KRR), especially for its high
expressiveness and the ability to deal also with incomplete knowledge.
  Recently, thanks to the availability of a number of robust and efficient
implementations, ASP has been increasingly employed in a number of different
domains, and used for the development of industrial-level and enterprise
applications. This made clear the need for proper development tools and
interoperability mechanisms for easing interaction and integration with
external systems in the widest range of real-world scenarios, including mobile
applications and educational contexts.
  In this work we present a framework for integrating the KRR capabilities of
ASP into generic applications. We show the use of the framework by illustrating
proper specializations for some relevant ASP systems over different platforms,
including the mobile setting; furthermore, the potential of the framework for
educational purposes is illustrated by means of the development of several
ASP-based applications.",arxiv
http://arxiv.org/abs/1911.12093v1,2019-11-27T11:48:11Z,2019-11-27T11:48:11Z,"Multi-Range Attentive Bicomponent Graph Convolutional Network for
  Traffic Forecasting","Traffic forecasting is of great importance to transportation management and
public safety, and very challenging due to the complicated spatial-temporal
dependency and essential uncertainty brought about by the road network and
traffic conditions. Latest studies mainly focus on modeling the spatial
dependency by utilizing graph convolutional networks (GCNs) throughout a fixed
weighted graph. However, edges, i.e., the correlations between pair-wise nodes,
are much more complicated and interact with each other. In this paper, we
propose the Multi-Range Attentive Bicomponent GCN (MRA-BGCN), a novel deep
learning model for traffic forecasting. We first build the node-wise graph
according to the road network distance and the edge-wise graph according to
various edge interaction patterns. Then, we implement the interactions of both
nodes and edges using bicomponent graph convolution. The multi-range attention
mechanism is introduced to aggregate information in different neighborhood
ranges and automatically learn the importance of different ranges. Extensive
experiments on two real-world road network traffic datasets, METR-LA and
PEMS-BAY, show that our MRA-BGCN achieves the state-of-the-art results.",arxiv
http://arxiv.org/abs/1905.07451v1,2019-05-17T19:30:42Z,2019-05-17T19:30:42Z,Graph-based Semi-Supervised & Active Learning for Edge Flows,"We present a graph-based semi-supervised learning (SSL) method for learning
edge flows defined on a graph. Specifically, given flow measurements on a
subset of edges, we want to predict the flows on the remaining edges. To this
end, we develop a computational framework that imposes certain constraints on
the overall flows, such as (approximate) flow conservation. These constraints
render our approach different from classical graph-based SSL for vertex labels,
which posits that tightly connected nodes share similar labels and leverages
the graph structure accordingly to extrapolate from a few vertex labels to the
unlabeled vertices. We derive bounds for our method's reconstruction error and
demonstrate its strong performance on synthetic and real-world flow networks
from transportation, physical infrastructure, and the Web. Furthermore, we
provide two active learning algorithms for selecting informative edges on which
to measure flow, which has applications for optimal sensor deployment. The
first strategy selects edges to minimize the reconstruction error bound and
works well on flows that are approximately divergence-free. The second approach
clusters the graph and selects bottleneck edges that cross cluster-boundaries,
which works well on flows with global trends.",arxiv
http://arxiv.org/abs/1803.10988v1,2018-03-29T09:51:26Z,2018-03-29T09:51:26Z,"A real-time warning system for rear-end collision based on random forest
  classifier","Rear-end collision warning system has a great role to enhance the driving
safety. In this system some measures are used to estimate the dangers and the
system warns drivers to be more cautious. The real-time processes should be
executed in such system, to remain enough time and distance to avoid collision
with the front vehicle. To this end, in this paper a new system is developed by
using random forest classifier. To evaluate the performance of the proposed
system, vehicles trajectory data of 100 car's database from Virginia tech
transportation institute are used and the methods are compared based on their
accuracy and their processing time. By using TOPSIS multi-criteria selection
method, we show that the results of the implemented classifier is better than
the results of different classifiers including Bayesian network, naive Bayes,
MLP neural network, support vector machine, nearest neighbor, rule-based
methods and decision tree. The presented experiments reveals that the random
forest is an acceptable algorithm for the proposed driver assistant system with
88.4% accuracy for detecting warning situations and 94.7% for detecting safe
situations.",arxiv
http://arxiv.org/abs/2009.05317v1,2020-09-11T10:00:47Z,2020-09-11T10:00:47Z,"SoFAr: Shortcut-based Fractal Architectures for Binary Convolutional
  Neural Networks","Binary Convolutional Neural Networks (BCNNs) can significantly improve the
efficiency of Deep Convolutional Neural Networks (DCNNs) for their deployment
on resource-constrained platforms, such as mobile and embedded systems.
However, the accuracy degradation of BCNNs is still considerable compared with
their full precision counterpart, impeding their practical deployment. Because
of the inevitable binarization error in the forward propagation and gradient
mismatch problem in the backward propagation, it is nontrivial to train BCNNs
to achieve satisfactory accuracy. To ease the difficulty of training, the
shortcut-based BCNNs, such as residual connection-based Bi-real ResNet and
dense connection-based BinaryDenseNet, introduce additional shortcuts in
addition to the shortcuts already present in their full precision counterparts.
Furthermore, fractal architectures have been also been used to improve the
training process of full-precision DCNNs since the fractal structure triggers
effects akin to deep supervision and lateral student-teacher information flow.
Inspired by the shortcuts and fractal architectures, we propose two
Shortcut-based Fractal Architectures (SoFAr) specifically designed for BCNNs:
1. residual connection-based fractal architectures for binary ResNet, and 2.
dense connection-based fractal architectures for binary DenseNet. Our proposed
SoFAr combines the adoption of shortcuts and the fractal architectures in one
unified model, which is helpful in the training of BCNNs. Results show that our
proposed SoFAr achieves better accuracy compared with shortcut-based BCNNs.
Specifically, the Top-1 accuracy of our proposed RF-c4d8 ResNet37(41) and
DRF-c2d2 DenseNet51(53) on ImageNet outperforms Bi-real ResNet18(64) and
BinaryDenseNet51(32) by 3.29% and 1.41%, respectively, with the same
computational complexity overhead.",arxiv
http://arxiv.org/abs/1610.01585v1,2016-10-05T19:41:12Z,2016-10-05T19:41:12Z,"Caching in the Sky: Proactive Deployment of Cache-Enabled Unmanned
  Aerial Vehicles for Optimized Quality-of-Experience","In this paper, the problem of proactive deployment of cache-enabled unmanned
aerial vehicles (UAVs) for optimizing the quality-of-experience (QoE) of
wireless devices in a cloud radio access network (CRAN) is studied. In the
considered model, the network can leverage human-centric information such as
users' visited locations, requested contents, gender, job, and device type to
predict the content request distribution and mobility pattern of each user.
Then, given these behavior predictions, the proposed approach seeks to find the
user-UAV associations, the optimal UAVs' locations, and the contents to cache
at UAVs. This problem is formulated as an optimization problem whose goal is to
maximize the users' QoE while minimizing the transmit power used by the UAVs.
To solve this problem, a novel algorithm based on the machine learning
framework of conceptor-based echo state networks (ESNs) is proposed. Using
ESNs, the network can effectively predict each user's content request
distribution and its mobility pattern when limited information on the states of
users and the network is available. Based on the predictions of the users'
content request distribution and their mobility patterns, we derive the optimal
user-UAV association, optimal locations of the UAVs as well as the content to
cache at UAVs. Simulation results using real pedestrian mobility patterns from
BUPT and actual content transmission data from Youku show that the proposed
algorithm can yield 40% and 61% gains, respectively, in terms of the average
transmit power and the percentage of the users with satisfied QoE compared to a
benchmark algorithm without caching and a benchmark solution without UAVs.",arxiv
http://arxiv.org/abs/1606.05814v1,2016-06-18T23:53:54Z,2016-06-18T23:53:54Z,Eye Tracking for Everyone,"From scientific research to commercial applications, eye tracking is an
important tool across many domains. Despite its range of applications, eye
tracking has yet to become a pervasive technology. We believe that we can put
the power of eye tracking in everyone's palm by building eye tracking software
that works on commodity hardware such as mobile phones and tablets, without the
need for additional sensors or devices. We tackle this problem by introducing
GazeCapture, the first large-scale dataset for eye tracking, containing data
from over 1450 people consisting of almost 2.5M frames. Using GazeCapture, we
train iTracker, a convolutional neural network for eye tracking, which achieves
a significant reduction in error over previous approaches while running in real
time (10-15fps) on a modern mobile device. Our model achieves a prediction
error of 1.71cm and 2.53cm without calibration on mobile phones and tablets
respectively. With calibration, this is reduced to 1.34cm and 2.12cm. Further,
we demonstrate that the features learned by iTracker generalize well to other
datasets, achieving state-of-the-art results. The code, data, and models are
available at http://gazecapture.csail.mit.edu.",arxiv
http://arxiv.org/abs/1811.08955v1,2018-11-21T21:20:24Z,2018-11-21T21:20:24Z,"Integrating Task-Motion Planning with Reinforcement Learning for Robust
  Decision Making in Mobile Robots","Task-motion planning (TMP) addresses the problem of efficiently generating
executable and low-cost task plans in a discrete space such that the (initially
unknown) action costs are determined by motion plans in a corresponding
continuous space. However, a task-motion plan can be sensitive to unexpected
domain uncertainty and changes, leading to suboptimal behaviors or execution
failures. In this paper, we propose a novel framework, TMP-RL, which is an
integration of TMP and reinforcement learning (RL) from the execution
experience, to solve the problem of robust task-motion planning in dynamic and
uncertain domains. TMP-RL features two nested planning-learning loops. In the
inner TMP loop, the robot generates a low-cost, feasible task-motion plan by
iteratively planning in the discrete space and updating relevant action costs
evaluated by the motion planner in continuous space. In the outer loop, the
plan is executed, and the robot learns from the execution experience via
model-free RL, to further improve its task-motion plans. RL in the outer loop
is more accurate to the current domain but also more expensive, and using less
costly task and motion planning leads to a jump-start for learning in the real
world. Our approach is evaluated on a mobile service robot conducting
navigation tasks in an office area. Results show that TMP-RL approach
significantly improves adaptability and robustness (in comparison to TMP
methods) and leads to rapid convergence (in comparison to task planning (TP)-RL
methods). We also show that TMP-RL can reuse learned values to smoothly adapt
to new scenarios during long-term deployments.",arxiv
http://arxiv.org/abs/2009.10031v1,2020-09-21T17:12:33Z,2020-09-21T17:12:33Z,Training Production Language Models without Memorizing User Data,"This paper presents the first consumer-scale next-word prediction (NWP) model
trained with Federated Learning (FL) while leveraging the Differentially
Private Federated Averaging (DP-FedAvg) technique. There has been prior work on
building practical FL infrastructure, including work demonstrating the
feasibility of training language models on mobile devices using such
infrastructure. It has also been shown (in simulations on a public corpus) that
it is possible to train NWP models with user-level differential privacy using
the DP-FedAvg algorithm. Nevertheless, training production-quality NWP models
with DP-FedAvg in a real-world production environment on a heterogeneous fleet
of mobile phones requires addressing numerous challenges. For instance, the
coordinating central server has to keep track of the devices available at the
start of each round and sample devices uniformly at random from them, while
ensuring \emph{secrecy of the sample}, etc. Unlike all prior privacy-focused FL
work of which we are aware, for the first time we demonstrate the deployment of
a differentially private mechanism for the training of a production neural
network in FL, as well as the instrumentation of the production training
infrastructure to perform an end-to-end empirical measurement of unintended
memorization.",arxiv
http://arxiv.org/abs/2105.12834v1,2021-05-10T08:23:50Z,2021-05-10T08:23:50Z,"Sense-Bandits: AI-based Adaptation of Sensing Thresholds for
  Heterogeneous-technology Coexistence Over Unlicensed Bands","In this paper, we present Sense-Bandits, an AI-based framework for
distributed adaptation of the sensing thresholds (STs) over shared spectrum.
This framework specifically targets the coexistence of heterogenous
technologies, e.g., Wi-Fi, 4G Licensed-Assisted Access (LAA), and 5G New Radio
Unlicensed (NR-U), over unlicensed channels. To access the channel, a device
compares the measured power with a predefined ST value and accordingly decides
if the channel is idle or not. Improper setting of the ST values creates
asymmetric sensing floors, resulting in collisions due to hidden terminals
and/or reduction in the spatial reuse due to exposed terminals. Optimal ST
setting is challenging because it requires global knowledge of mobility,
traffic loads, and channel access behavior of all contending devices. Sense-
Bandits tackles this problem by employing a clustering-based multi-armed bandit
(MAB) algorithm, which adapts its learning behavior based on network dynamics.
Clustering allows the algorithm to track network changes in real-time, ensuring
fast learning of the best ST values by classifying the state and dynamics of
coexisting networks. We develop a C++-based network simulator that implements
Sense-Bandits and we apply it to evaluate the coexistence of Wi-Fi and 5G NR-U
systems over the unlicensed 5 GHz U NII bands. Our simulation results indicate
that ST-adaptive devices employing Sense-Bandits do not harm neighboring
devices that adopt a fixed ST value.",arxiv
http://arxiv.org/abs/1810.09729v1,2018-10-23T08:51:54Z,2018-10-23T08:51:54Z,"Design Challenges of Multi-UAV Systems in Cyber-Physical Applications: A
  Comprehensive Survey, and Future Directions","Unmanned Aerial Vehicles (UAVs) have recently rapidly grown to facilitate a
wide range of innovative applications that can fundamentally change the way
cyber-physical systems (CPSs) are designed. CPSs are a modern generation of
systems with synergic cooperation between computational and physical potentials
that can interact with humans through several new mechanisms. The main
advantages of using UAVs in CPS application is their exceptional features,
including their mobility, dynamism, effortless deployment, adaptive altitude,
agility, adjustability, and effective appraisal of real-world functions anytime
and anywhere. Furthermore, from the technology perspective, UAVs are predicted
to be a vital element of the development of advanced CPSs. Therefore, in this
survey, we aim to pinpoint the most fundamental and important design challenges
of multi-UAV systems for CPS applications. We highlight key and versatile
aspects that span the coverage and tracking of targets and infrastructure
objects, energy-efficient navigation, and image analysis using machine learning
for fine-grained CPS applications. Key prototypes and testbeds are also
investigated to show how these practical technologies can facilitate CPS
applications. We present and propose state-of-the-art algorithms to address
design challenges with both quantitative and qualitative methods and map these
challenges with important CPS applications to draw insightful conclusions on
the challenges of each application. Finally, we summarize potential new
directions and ideas that could shape future research in these areas.",arxiv
http://arxiv.org/abs/2111.11053v1,2021-11-22T08:49:33Z,2021-11-22T08:49:33Z,DAPPER: Performance Estimation of Domain Adaptation in Mobile Sensing,"Many applications that utilize sensors in mobile devices and apply machine
learning to provide novel services have emerged. However, various factors such
as different users, devices, environments, and hyperparameters, affect the
performance for such applications, thus making the domain shift (i.e.,
distribution shift of a target user from the training source dataset) an
important problem. Although recent domain adaptation techniques attempt to
solve this problem, the complex interplay between the diverse factors often
limits their effectiveness. We argue that accurately estimating the performance
in untrained domains could significantly reduce performance uncertainty. We
present DAPPER (Domain AdaPtation Performance EstimatoR) that estimates the
adaptation performance in a target domain with only unlabeled target data. Our
intuition is that the outputs of a model on the target data provide clues for
the model's actual performance in the target domain. DAPPER does not require
expensive labeling costs nor involve additional training after deployment. Our
evaluation with four real-world sensing datasets compared against four
baselines shows that DAPPER outperforms the baselines by on average 17% in
estimation accuracy. Moreover, our on-device experiment shows that DAPPER
achieves up to 216X less computation overhead compared with the baselines.",arxiv
http://arxiv.org/abs/2002.04700v4,2020-03-15T03:27:52Z,2020-02-11T21:42:22Z,"A Single RGB Camera Based Gait Analysis with a Mobile Tele-Robot for
  Healthcare","With the increasing awareness of high-quality life, there is a growing need
for health monitoring devices running robust algorithms in home environment.
Health monitoring technologies enable real-time analysis of users' health
status, offering long-term healthcare support and reducing hospitalization
time. The purpose of this work is twofold, the software focuses on the analysis
of gait, which is widely adopted for joint correction and assessing any lower
limb or spinal problem. On the hardware side, we design a novel marker-less
gait analysis device using a low-cost RGB camera mounted on a mobile
tele-robot. As gait analysis with a single camera is much more challenging
compared to previous works utilizing multi-cameras, a RGB-D camera or wearable
sensors, we propose using vision-based human pose estimation approaches. More
specifically, based on the output of two state-of-the-art human pose estimation
models (Openpose and VNect), we devise measurements for four bespoke gait
parameters: inversion/eversion, dorsiflexion/plantarflexion, ankle and foot
progression angles. We thereby classify walking patterns into normal,
supination, pronation and limp. We also illustrate how to run the purposed
machine learning models in low-resource environments such as a single
entry-level CPU. Experiments show that our single RGB camera method achieves
competitive performance compared to state-of-the-art methods based on depth
cameras or multi-camera motion capture system, at smaller hardware costs.",arxiv
http://arxiv.org/abs/2010.09028v1,2020-10-18T16:52:06Z,2020-10-18T16:52:06Z,Characterizing and Taming Model Instability Across Edge Devices,"The same machine learning model running on different edge devices may produce
highly-divergent outputs on a nearly-identical input. Possible reasons for the
divergence include differences in the device sensors, the device's signal
processing hardware and software, and its operating system and processors. This
paper presents the first methodical characterization of the variations in model
prediction across real-world mobile devices. We demonstrate that accuracy is
not a useful metric to characterize prediction divergence, and introduce a new
metric, instability, which captures this variation. We characterize different
sources for instability, and show that differences in compression formats and
image signal processing account for significant instability in object
classification models. Notably, in our experiments, 14-17% of images produced
divergent classifications across one or more phone models. We evaluate three
different techniques for reducing instability. In particular, we adapt prior
work on making models robust to noise in order to fine-tune models to be robust
to variations across edge devices. We demonstrate our fine-tuning techniques
reduce instability by 75%.",arxiv
http://arxiv.org/abs/1804.00736v1,2018-04-02T21:38:56Z,2018-04-02T21:38:56Z,"Deep Spatiotemporal Models for Robust Proprioceptive Terrain
  Classification","Terrain classification is a critical component of any autonomous mobile robot
system operating in unknown real-world environments. Over the years, several
proprioceptive terrain classification techniques have been introduced to
increase robustness or act as a fallback for traditional vision based
approaches. However, they lack widespread adaptation due to various factors
that include inadequate accuracy, robustness and slow run-times. In this paper,
we use vehicle-terrain interaction sounds as a proprioceptive modality and
propose a deep Long-Short Term Memory (LSTM) based recurrent model that
captures both the spatial and temporal dynamics of such a problem, thereby
overcoming these past limitations. Our model consists of a new Convolution
Neural Network (CNN) architecture that learns deep spatial features,
complemented with LSTM units that learn complex temporal dynamics. Experiments
on two extensive datasets collected with different microphones on various
indoor and outdoor terrains demonstrate state-of-the-art performance compared
to existing techniques. We additionally evaluate the performance in adverse
acoustic conditions with high ambient noise and propose a noise-aware training
scheme that enables learning of more generalizable models that are essential
for robust real-world deployments.",arxiv
http://arxiv.org/abs/1901.10584v1,2019-01-29T22:07:41Z,2019-01-29T22:07:41Z,"Trading-off Accuracy and Energy of Deep Inference on Embedded Systems: A
  Co-Design Approach","Deep neural networks have seen tremendous success for different modalities of
data including images, videos, and speech. This success has led to their
deployment in mobile and embedded systems for real-time applications. However,
making repeated inferences using deep networks on embedded systems poses
significant challenges due to constrained resources (e.g., energy and computing
power). To address these challenges, we develop a principled co-design
approach. Building on prior work, we develop a formalism referred to as
Coarse-to-Fine Networks (C2F Nets) that allow us to employ classifiers of
varying complexity to make predictions. We propose a principled optimization
algorithm to automatically configure C2F Nets for a specified trade-off between
accuracy and energy consumption for inference. The key idea is to select a
classifier on-the-fly whose complexity is proportional to the hardness of the
input example: simple classifiers for easy inputs and complex classifiers for
hard inputs. We perform comprehensive experimental evaluation using four
different C2F Net architectures on multiple real-world image classification
tasks. Our results show that optimized C2F Net can reduce the Energy Delay
Product (EDP) by 27 to 60 percent with no loss in accuracy when compared to the
baseline solution, where all predictions are made using the most complex
classifier in C2F Net.",arxiv
http://arxiv.org/abs/2108.01884v1,2021-08-04T07:30:04Z,2021-08-04T07:30:04Z,"Adaptive Path Planning for UAV-based Multi-Resolution Semantic
  Segmentation","In this paper, we address the problem of adaptive path planning for accurate
semantic segmentation of terrain using unmanned aerial vehicles (UAVs). The
usage of UAVs for terrain monitoring and remote sensing is rapidly gaining
momentum due to their high mobility, low cost, and flexible deployment.
However, a key challenge is planning missions to maximize the value of acquired
data in large environments given flight time limitations. To address this, we
propose an online planning algorithm which adapts the UAV paths to obtain
high-resolution semantic segmentations necessary in areas on the terrain with
fine details as they are detected in incoming images. This enables us to
perform close inspections at low altitudes only where required, without wasting
energy on exhaustive mapping at maximum resolution. A key feature of our
approach is a new accuracy model for deep learning-based architectures that
captures the relationship between UAV altitude and semantic segmentation
accuracy. We evaluate our approach on the application of crop/weed segmentation
in precision agriculture using real-world field data.",arxiv
http://arxiv.org/abs/2110.07206v1,2021-10-14T08:03:33Z,2021-10-14T08:03:33Z,"Task-Driven Deep Image Enhancement Network for Autonomous Driving in Bad
  Weather","Visual perception in autonomous driving is a crucial part of a vehicle to
navigate safely and sustainably in different traffic conditions. However, in
bad weather such as heavy rain and haze, the performance of visual perception
is greatly affected by several degrading effects. Recently, deep learning-based
perception methods have addressed multiple degrading effects to reflect
real-world bad weather cases but have shown limited success due to 1) high
computational costs for deployment on mobile devices and 2) poor relevance
between image enhancement and visual perception in terms of the model ability.
To solve these issues, we propose a task-driven image enhancement network
connected to the high-level vision task, which takes in an image corrupted by
bad weather as input. Specifically, we introduce a novel low memory network to
reduce most of the layer connections of dense blocks for less memory and
computational cost while maintaining high performance. We also introduce a new
task-driven training strategy to robustly guide the high-level task model
suitable for both high-quality restoration of images and highly accurate
perception. Experiment results demonstrate that the proposed method improves
the performance among lane and 2D object detection, and depth estimation
largely under adverse weather in terms of both low memory and accuracy.",arxiv
http://arxiv.org/abs/1908.02427v1,2019-08-07T03:04:38Z,2019-08-07T03:04:38Z,"Strengthening the Case for a Bayesian Approach to Car-following Model
  Calibration and Validation using Probabilistic Programming","Compute and memory constraints have historically prevented traffic simulation
software users from fully utilizing the predictive models underlying them. When
calibrating car-following models, particularly, accommodations have included 1)
using sensitivity analysis to limit the number of parameters to be calibrated,
and 2) identifying only one set of parameter values using data collected from
multiple car-following instances across multiple drivers. Shortcuts are further
motivated by insufficient data set sizes, for which a driver may have too few
instances to fully account for the variation in their driving behavior. In this
paper, we demonstrate that recent technological advances can enable
transportation researchers and engineers to overcome these constraints and
produce calibration results that 1) outperform industry standard approaches,
and 2) allow for a unique set of parameters to be estimated for each driver in
a data set, even given a small amount of data. We propose a novel calibration
procedure for car-following models based on Bayesian machine learning and
probabilistic programming, and apply it to real-world data from a naturalistic
driving study. We also discuss how this combination of mathematical and
software tools can offer additional benefits such as more informative model
validation and the incorporation of true-to-data uncertainty into simulation
traces.",arxiv
http://arxiv.org/abs/1904.01080v4,2020-02-27T20:23:40Z,2019-04-01T19:38:56Z,"Learning Matchable Image Transformations for Long-term Metric Visual
  Localization","Long-term metric self-localization is an essential capability of autonomous
mobile robots, but remains challenging for vision-based systems due to
appearance changes caused by lighting, weather, or seasonal variations. While
experience-based mapping has proven to be an effective technique for bridging
the `appearance gap,' the number of experiences required for reliable metric
localization over days or months can be very large, and methods for reducing
the necessary number of experiences are needed for this approach to scale.
Taking inspiration from color constancy theory, we learn a nonlinear
RGB-to-grayscale mapping that explicitly maximizes the number of inlier feature
matches for images captured under different lighting and weather conditions,
and use it as a pre-processing step in a conventional single-experience
localization pipeline to improve its robustness to appearance change. We train
this mapping by approximating the target non-differentiable localization
pipeline with a deep neural network, and find that incorporating a learned
low-dimensional context feature can further improve cross-appearance feature
matching. Using synthetic and real-world datasets, we demonstrate substantial
improvements in localization performance across day-night cycles, enabling
continuous metric localization over a 30-hour period using a single mapping
experience, and allowing experience-based localization to scale to long
deployments with dramatically reduced data requirements.",arxiv
http://arxiv.org/abs/2003.12228v1,2020-03-27T04:10:06Z,2020-03-27T04:10:06Z,Mechanism Design for Wireless Powered Spatial Crowdsourcing Networks,"Wireless power transfer (WPT) is a promising technology to prolong the
lifetime of the sensors and communication devices, i.e., workers, in completing
crowdsourcing tasks by providing continuous and cost-effective energy supplies.
In this paper, we propose a wireless powered spatial crowdsourcing framework
which consists of two mutually dependent phases: task allocation phase and data
crowdsourcing phase. In the task allocation phase, we propose a Stackelberg
game based mechanism for the spatial crowdsourcing platform to efficiently
allocate spatial tasks and wireless charging power to each worker. In the data
crowdsourcing phase, the workers may have an incentive to misreport its real
working location to improve its utility, which causes adverse effects to the
spatial crowdsourcing platform. To address this issue, we present three
strategyproof deployment mechanisms for the spatial crowdsourcing platform to
place a mobile base station, e.g., vehicle or robot, which is responsible for
transferring the wireless power and collecting the crowdsourced data. As the
benchmark, we first apply the classical median mechanism and evaluate its
worst-case performance. Then, we design a conventional strategyproof deployment
mechanism to improve the expected utility of the spatial crowdsourcing platform
under the condition that the workers' locations follow a known geographical
distribution. For a more general case with only the historical location data
available, we propose a deep learning based strategyproof deployment mechanism
to maximize the spatial crowdsourcing platform's utility. Extensive
experimental results based on synthetic and real-world datasets reveal the
effectiveness of the proposed framework in allocating tasks and charging power
to workers while avoiding the dishonest worker's manipulation.",arxiv
http://arxiv.org/abs/1806.04847v2,2018-12-11T04:00:24Z,2018-06-13T05:10:17Z,"Android Malware Detection using Large-scale Network Representation
  Learning","With the growth of mobile devices and applications, the number of malicious
software, or malware, is rapidly increasing in recent years, which calls for
the development of advanced and effective malware detection approaches.
Traditional methods such as signature-based ones cannot defend users from an
increasing number of new types of malware or rapid malware behavior changes. In
this paper, we propose a new Android malware detection approach based on deep
learning and static analysis. Instead of using Application Programming
Interfaces (APIs) only, we further analyze the source code of Android
applications and create their higher-level graphical semantics, which makes it
harder for attackers to evade detection. In particular, we use a call graph
from method invocations in an Android application to represent the application,
and further analyze method attributes to form a structured Program
Representation Graph (PRG) with node attributes. Then, we use a graph
convolutional network (GCN) to yield a graph representation of the application
by embedding the entire graph into a dense vector, and classify whether it is a
malware or not. To efficiently train such a graph convolutional network, we
propose a batch training scheme that allows multiple heterogeneous graphs to be
input as a batch. To the best of our knowledge, this is the first work to use
graph representation learning for malware detection. We conduct extensive
experiments from real-world sample collections and demonstrate that our
developed system outperforms multiple other existing malware detection
techniques.",arxiv
http://arxiv.org/abs/1703.01006v1,2017-03-03T01:12:38Z,2017-03-03T01:12:38Z,"Scalable Deep Traffic Flow Neural Networks for Urban Traffic Congestion
  Prediction","Tracking congestion throughout the network road is a critical component of
Intelligent transportation network management systems. Understanding how the
traffic flows and short-term prediction of congestion occurrence due to
rush-hour or incidents can be beneficial to such systems to effectively manage
and direct the traffic to the most appropriate detours. Many of the current
traffic flow prediction systems are designed by utilizing a central processing
component where the prediction is carried out through aggregation of the
information gathered from all measuring stations. However, centralized systems
are not scalable and fail provide real-time feedback to the system whereas in a
decentralized scheme, each node is responsible to predict its own short-term
congestion based on the local current measurements in neighboring nodes.
  We propose a decentralized deep learning-based method where each node
accurately predicts its own congestion state in real-time based on the
congestion state of the neighboring stations. Moreover, historical data from
the deployment site is not required, which makes the proposed method more
suitable for newly installed stations. In order to achieve higher performance,
we introduce a regularized Euclidean loss function that favors high congestion
samples over low congestion samples to avoid the impact of the unbalanced
training dataset. A novel dataset for this purpose is designed based on the
traffic data obtained from traffic control stations in northern California.
Extensive experiments conducted on the designed benchmark reflect a successful
congestion prediction.",arxiv
http://arxiv.org/abs/1701.06507v2,2017-02-02T17:59:10Z,2017-01-23T17:05:22Z,Plausible Shading Decomposition For Layered Photo Retouching,"Photographers routinely compose multiple manipulated photos of the same scene
(layers) into a single image, which is better than any individual photo could
be alone. Similarly, 3D artists set up rendering systems to produce layered
images to contain only individual aspects of the light transport, which are
composed into the final result in post-production. Regrettably, both approaches
either take considerable time to capture, or remain limited to synthetic
scenes. In this paper, we suggest a system to allow decomposing a single image
into a plausible shading decomposition (PSD) that approximates effects such as
shadow, diffuse illumination, albedo, and specular shading. This decomposition
can then be manipulated in any off-the-shelf image manipulation software and
recomposited back. We perform such a decomposition by learning a convolutional
neural network trained using synthetic data. We demonstrate the effectiveness
of our decomposition on synthetic (i.e., rendered) and real data (i.e.,
photographs), and use them for common photo manipulation, which are nearly
impossible to perform otherwise from single images.",arxiv
http://arxiv.org/abs/2109.03011v1,2021-09-07T11:57:07Z,2021-09-07T11:57:07Z,Understanding Model Drift in a Large Cellular Network,"Operational networks are increasingly using machine learning models for a
variety of tasks, including detecting anomalies, inferring application
performance, and forecasting demand. Accurate models are important, yet
accuracy can degrade over time due to concept drift, whereby either the
characteristics of the data change over time (data drift) or the relationship
between the features and the target predictor change over time (model drift).
Drift is important to detect because changes in properties of the underlying
data or relationships to the target prediction can require model retraining,
which can be time-consuming and expensive. Concept drift occurs in operational
networks for a variety of reasons, ranging from software upgrades to
seasonality to changes in user behavior. Yet, despite the prevalence of drift
in networks, its extent and effects on prediction accuracy have not been
extensively studied. This paper presents an initial exploration into concept
drift in a large cellular network in the United States for a major metropolitan
area in the context of demand forecasting. We find that concept drift arises
largely due to data drift, and it appears across different key performance
indicators (KPIs), models, training set sizes, and time intervals. We identify
the sources of concept drift for the particular problem of forecasting downlink
volume. Weekly and seasonal patterns introduce both high and low-frequency
model drift, while disasters and upgrades result in sudden drift due to
exogenous shocks. Regions with high population density, lower traffic volumes,
and higher speeds also tend to correlate with more concept drift. The features
that contribute most significantly to concept drift are User Equipment (UE)
downlink packets, UE uplink packets, and Real-time Transport Protocol (RTP)
total received packets.",arxiv
